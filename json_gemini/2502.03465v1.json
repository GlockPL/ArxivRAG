{"title": "Seeing World Dynamics in a Nutshell", "authors": ["Qiuhong Shen", "Xuanyu Yi", "Mingbao Lin", "Hanwang Zhang", "Shuicheng Yan", "Xinchao Wang"], "abstract": "We consider the problem of efficiently representing casually captured monocular videos in a spatially- and temporally-coherent manner. While existing approaches predominantly rely on 2D/2.5D techniques treating videos as collections of spatiotemporal pixels, they struggle with complex motions, occlusions, and geometric consistency due to absence of temporal coherence and explicit 3D structure. Drawing inspiration from monocular video as a projection of the dynamic 3D world, we explore representing videos in their intrinsic 3D form through continuous flows of Gaussian primitives in space-time. In this paper, we propose NutWorld, a novel framework that efficiently transforms monocular videos into dynamic 3D Gaussian representations in a single forward pass. At its core, NutWorld introduces a structured spatial-temporal aligned Gaussian (STAG) representation, enabling optimization-free scene modeling with effective depth and flow regularization. Through comprehensive experiments, we demonstrate that NutWorld achieves high-fidelity video reconstruction quality while enabling various downstream applications in real-time.", "sections": [{"title": "1. Introduction", "content": "Our natural world exhibits inherent dynamism --from rustling leaves swaying in the wind to clouds drifting across the sky and ocean waves rolling along the shore-where objects maintain their structural integrity while undergoing continuous spatiotemporal evolution. A fundamental objective in video processing [5, 65] is to enable machines to interpret such visual information, allowing recovery of both object geometry and motion patterns while preserving the spatiotemporal coherence intrinsic to human perception. This capability is crucial for numerous applications, ranging from autonomous driving [20, 34] and robotics [24, 25, 43] to augmented reality and content creation [2, 31, 61, 85], where an accurate understanding of dynamic scenes directly impacts system performance and user experience.\nThe current neural-based video representation [44, 58, 82] focuses predominantly on 2D and 2.5D techniques, treating videos as collections of spatiotemporal pixels [17, 91]. Although these discrete representations enable basic temporal modeling through pixel matching [60] and tracking [27, 81], they struggle to capture complex scene dynamics and maintain geometric consistency, particularly in challenging scenarios involving occlusions and non-rigid deformations [42, 44]. Moreover, these approaches inherently lack explicit 3D understanding, leading to unreliable spatial arrangements and limitations in novel view synthesis [40].\nDrawing inspiration from monocular video as a projection of the dynamic 3D world, we investigate whether videos can be represented in their intrinsic 3D form without per-scene optimization. Recent advances in dynamic Gaussian Splatting [29] have shown unprecedented capabilities in dynamic scene reconstruction [16, 62, 75, 80], achieving high-fidelity rendering with explicit 3D representation and superior efficiency. By modeling videos as continuous flows of Gaussian primitives over time, we address the limitations of 2D representations while enabling efficient video processing. As shown in Figure 1, this paradigm treats space-time as an entirety and offers key advantages: each Gaussian functions as a flexible building block adapting to local geometric structures, enabling seamless representation of complex scenes across scales. Moreover, when embraced with dynamic attributes, these structured Gaussians naturally approximate the underlying spatiotemporal 4D volume of dynamic scenes, capturing intrinsic motions with demonstrated temporal consistency and facilitating various downstream tasks including novel view synthesis [40, 86], video editing [7, 49], and frame interpolation [14, 39].\nHowever, transforming casually captured monocular videos to dynamic Gaussian representations instantly (sec. /frame) presents several challenges: (1) Unposed inputs. Gaussian Splatting [29] and its variants [23, 87, 88] heavily rely on accurate camera poses obtained through Structure-from-Motion (SfM) [54, 66], which are typically unavailable for casual monocular videos. Without such pose guidance, current methods [4, 19, 35, 73] fail to disentangle camera motion from scene dynamics, resulting in deteriorated rendering quality and even collapsed reconstruction. (2) Nonstructural Nature. Most existing dynamic Gaussian Splatting either leverage per-scene optimized deformation networks [16, 28, 32, 75, 79, 80] or adopt per-frame Gaussian generation [53] for dynamics modeling, both incompatible with our feed-forward prediction paradigm. The spatially unstructured property of Gaussian primitives further makes them prone to local minima in inverse rendering [10, 95], impeding feed-forward modeling of dynamic 3D scenes. (3) Spatial Ambiguity. The absence of multi-view supervision and initialization from SfM points significantly limits the spatial modeling capability of Gaussian Splatting, leading to ambiguous scale, depth collapse, and inconsistent spatial arrangements in reconstructed scenes.\nTo address the above challenges, we introduce NutWorld to efficiently transform monocular videos into dynamic Gaussian Splatting representations in a single forward pass. Our approach comprises three key components: (1) A structured spatial-temporal aligned Gaussian (STAG) representation in canonical space (Section 4.1), which enables feed-forward prediction while ensuring pose-free and scale-invariant modeling. (2) An optimization-free pipeline (Section 4.2) that learns spatial-temporal correspondence and continuous temporal dynamics across video frames, efficiently converting them into STAG representations. (3) Depth and flow regularization (Section 4.3) that incorporates calibrated monocular depth [78] and optical flow priors [76] to resolve spatial ambiguity and motion-appearance entanglement in the ill-posed monocular video setting [62, 69]. Through large-scale pre-training, NutWorld could process arbitrarily long videos while maintaining spatial-temporal consistency via segment-based inference (Section 4.4).\nWe conducted qualitative and quantitative experiments on RealEstate10K [94] and MiraData [26] to verify the representation efficacy of our proposed NutWorld in terms of video reconstruction. Moreover, our approach demonstrates compelling flexibility while maintaining real-time inference speed across various video downstream applications, including novel view synthesis, consistent depth estimation, video segmentation, as well as video editing and frame interpolation, suggesting its potential as a general-purpose video representation framework. The contributions and novelties of our paper are summarized as follows:\n\u2022 We present the first framework to efficiently represent world dynamics in casually captured monocular videos as dynamic Gaussian Splatting in a single forward pass.\n\u2022 Our NutWorld framework incorporates the STAG representation, elaborated network for feed-forward reconstruction, and effective regularization strategies for spatial and temporal coherent recovery from casual videos.\n\u2022 Extensive experiments on video reconstruction and various downstream tasks demonstrate the spatial-temporal coherence and the versatility of NutWorld."}, {"title": "2. Related Work", "content": "Dynamic 3D scene reconstruction. Dynamic scenes, with moving objects and varying lighting conditions, require models that can handle temporal variations and motion in 3D space. While early works extended NeRF [40] to dynamic scenes [46\u201348] through temporal representations [48] and deformable fields [6], they remain computationally intensive due to MLPs and volume rendering [41]. Recent dynamic 3D Gaussian Splatting addresses this limitation by using deformable 3D Gaussian [28, 32, 75, 80] or anisotropic 4D Gaussian [16, 79], enabling efficient rendering through tile-based rasterization [29]. Among them, concurrent works [33, 62, 69, 71] have demonstrated dynamic scene reconstruction from unposed monocular videos. However, they rely on computationally intensive per-scene optimization, making them impractical for real-world applications. In contrast, our NutWorld focuses on neural video representation (NVR) without the availability of camera poses, specifically addressing the challenge of explicitly representing monocular videos rather than dynamic 3D scene reconstruction. Moreover, it employs a feed-forward approach that enables fast inference with high reconstruction quality and temporal consistency, distinguishing itself from tedious optimization-based 3D reconstruction paradigms.\nFeed-Forward Gaussian Splatting. Recent advance in large-scale 3D scene datasets [36, 37, 94] has enabled feed-forward Gaussian approaches [8, 9, 55, 74, 84, 90, 92], which excel in efficiency and sparse view reconstruction. PixelSplat [8] and LatentSplat [74] employ the epipolar transformer [21, 72] to establish cross-view correspondences and predict 3D Gaussians from multi-view image features, while MVSplat [9] utilizes cost volumes to jointly predict depth and Gaussian parameters. Alternatively, GS-LRM [92] and Long-LRM [96] consider Gaussian Splatting reconstruction as a sequence-to-sequence translation task, employing transformer-based [67] or hybrid [12] architectures to regress Gaussian primitives. Although these methods address static 3D reconstruction, NutWorld is the first to represent dynamic scenes from unposed monocular videos."}, {"title": "3. Preliminary: Dynamic Gaussian Splatting", "content": "Dynamic Gaussian Splatting [16, 28, 32, 75, 79, 80] is an explicit 4D neural representation for reconstructing dynamic 3D scenes from multi-view videos through differentiable rasterization [29]. Dynamic Gaussian {Gi, Di(t)} decouples dynamic scenes into a static canonical 3D Gaussian Gi and a deformation motion field Di to account for temporal variations in 3D space. Specifically, the static 3D Gaussian Gi is composed of a 3D center $\\mu \\in \\mathbb{R}^3$, 3D scale $s \\in \\mathbb{R}^3$, associated color $c \\in \\mathbb{R}^3$, opacity $a \\in \\mathbb{R}$, and rotation quaternion $q \\in \\mathbb{R}^4$. For the deformation fields Di(t) = {pi(t), qi (t), ai(t)}, the deformable attributes and"}, {"title": "4. Methodology", "content": "In this section, we present a framework for efficiently representing world dynamics from monocular video in a feed-forward manner. As shown in Figure 3, we first introduce our Spatial-Temporal Aligned Gaussian Splatting (STAG) representation (Section 4.1). To enable the mapping of videos to STAG in a single forward pass, we detail our transformer-based network (Section 4.2), which operates with calibrated depth and flow priors (Section 4.3). Finally, we discuss the overall training objectives and protocols for processing long video segments (Section 4.4)."}, {"title": "4.1. Spatial-Temporal Aligned Gaussian", "content": "Canonical camera space. Given an unposed monocular video, we employ an orthographic camera coordinate system instead of an absolute 3D world coordinate system. This choice is motivated by two key challenges: the difficulty of obtaining coherent camera trajectories in dynamic scenes [45, 54, 66, 68, 70], and the inherent scale ambiguity in feed-forward 3D reconstruction models [8, 9, 90], where perspective projection couples object scale with its distance from camera. By adopting orthographic projection with a fixed pose along the z axis, we eliminate the necessity for explicit camera pose estimation while modeling both camera and object motion without scale ambiguity in a unified canonical space. We provide a detailed implementation of the orthographic rasterization pipeline in the Appendix.\nStructured Dynamic Gaussian. To overcome the unstructured nature in dynamic Gaussian Splatting and facilitate neural network integration, we introduce Spatial-Temporal Aligned Gaussian Splatting (STAG) in the canonical cam-"}, {"title": "4.2. Encapsulate Dynamics within \u201cNutshell\"", "content": "In this section, we introduce the transformer-based model in NutWorld to transform unposed monocular videos into the proposed STAG. Formally, given an input sequence of K video frames {Fk,tk}, where each frame Fk \u2208 RH\u00d7W\u00d73 is associated with a normalized timestamp tk \u2208 [0,1], we define NutWorld as an inverse mapping function \u0398:\n$\\left\\{G_{i}, \\mu_{i}(t)\\right\\}=\\Theta\\left(\\left\\{F_{k}, t_{k}\\right\\}\\right)$.\nThis mapping generates a set of STAGs {Gi, pi(t)} that can be rendered into arbitrary frames (M > K) through temporal interpolation of the deformation field. Specifically,\nTransformer-based Encoder. For each input frame Fk \u2208 RH\u00d7W\u00d73, we augment pixels by concatenating their RGB values with corresponding depth coordinates de along the channel dimension, obtained from a pre-trained depth estimation model [78]. The augmented frames are first split into non-overlapping patches of size p\u00d7 p using convolutions, which are then linearly transformed and concatenated across all frames to generate transformer input tokens. Notably, our architecture eliminates the need for explicit positional embeddings as used in ViT [1, 15], since depth coordinates inherently encode spatial information. The transformer blocks, comprising self-attention [67] and MLP layers, process these concatenated tokens to capture spatiotemporal correspondence and produce encoded features Eo \u2208 RK\u00d7h\u00d7w\u00d7C, where h = H/p and w = W/p denote the spatial resolution and C denotes the feature dimension. To ensure sufficient STAGs for casual videos, we leverage a hierarchical upsampling network [77, 92] that progressively expands the spatial resolution of the encoded"}, {"title": "4.3. Calibrated 2D Priors Regularization", "content": "Learning spatially-aware STAGs solely from monocular videos is inherently ill-posed due to depth ambiguity and motion uncertainty. Therefore, we leverage off-the-shelf foundation models [76, 78] to recover spatial relationships and temporal motion through calibrated depth and flow priors, respectively. Under the orthographic projection, the movement of STAG along the xy-coordinates directly corresponds to optical flow magnitude, whereas depth-related loss exclusively affects the z-coordinate, further facilitating the incorporation of those 2D priors.\nDepth Regularization. To enhance robustness against scale and shift variations in depth rendering, we employ a scale and shift invariant loss [3, 52]. This loss function computes optimal scaling \u03b2 and shifting \u03b3 factors that align the rendered depth d with the pseudo depth d* estimated by the depth prior model [78]. The optimal values for \u03b2 and \u03b3 are obtained by minimizing the squared error between the scaled rendered depth and the pseudo depth, as follows:\n$\\beta, \\gamma = \\arg \\min _{\\beta, \\gamma} \\sum_{H \\times W} M_{i} \\cdot\\left(\\beta \\cdot d_{i}+\\gamma-d_{i}^{*}\\right)^{2}$,\n$L_{\\text {depth }}=\\sum \\frac{\\left|d_{i}-d_{i}^{*}\\right|}{\\sum M_{i}}, d_{i}=\\beta \\cdot d_{i}+\\gamma$.\nHere, M is an outlier mask where Mi = 0 for the top 10% values in each depth map and Mi = 1 otherwise, mitigating noise in the estimated pseudo depth d*. This depth supervision effectively regularizes the training, making it robust for predicting relative depth in scenes.\nFlow Regularization. We extract global STAG trajectories by leveraging frame-to-frame optical flow associations [76]. In contrast to previous methods that solely employ iterative optimization between adjacent frames, our feed-forward framework utilizes global trajectory supervision to ensure consistent motion in a single forward pass.\nFor each STAG, we define its estimated pseudo-trajectory *(t) across K video frames. This trajectory is derived by sequential queries to the pre-computed optical flow field between adjacent frames, represented by a global flow matrix F:\n$\\mathbf{F}=\\left(\\begin{array}{ccccc}f_{0 \\rightarrow 0} & f_{1 \\rightarrow 0} & f_{2 \\rightarrow 0} & \\cdots & f_{K-1 \\rightarrow 0} \\\\(0, \\ldots, 0) & f_{1 \\rightarrow 1} & f_{2 \\rightarrow 1} & \\cdots & f_{K-1 \\rightarrow 1} \\\\(0, \\ldots, 0) & (0, \\ldots, 0) & f_{2 \\rightarrow 2} & \\cdots & f_{K-1 \\rightarrow 2} \\\\\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\(0, \\ldots, 0) & (0, \\ldots, 0) & (0, \\ldots, 0) & \\cdots & f_{K-1 \\rightarrow K-1}\\end{array}\\right)$.\nThe global flow matrix F is structured as a K \u00d7 K matrix, where each entry Fi,j is structured as a vector of length (U \u00d7 V) represents the 2D cumulative motion of each Gaussian from frame j to frame i. The matrix structure is asymmetric: upper triangular entries encode cumulative backward flow fk k\u22121(Hi), while lower triangular entries contain cumulative forward flow fk\u2192k+1(\u03bci). Here, i denotes the projected 2D coordinates of the i-th Gaussian in frame k. For notational clarity, we omit the explicit flow query operations in the matrix entries.\nUsing the global flow matrix, we regularize the deformation field \u00b5(t) by comparing its 2D projection (t) against the estimated global trajectories \u03bc*(t) across K frames:\n$L_{\\text {flow }}=\\sum_{i=1}^{K \\times U \\times V} \\sum_{k=0}^{K-1}\\left|\\mu_{i}\\left(t_{k}\\right)-\\mu_{i}^{*}\\left(t_{k}\\right)\\right| \\|_{1}$,\nwhere || ||1 denotes the L1 norm. An outlier filtering strat-"}, {"title": "4.4. Training and Inference", "content": "Overall objective. During the training phase, we render RGB frames from K = 6 sparsely sampled frames and interpolate M = 10 intermediate frames for dense temporal supervision. Our training objective comprises three terms:\n$\\mathcal{L}=\\mathcal{L}_{M S E}+\\lambda_{\\text {flow }} \\mathcal{L}_{\\text {flow }}+\\lambda_{\\text {depth }} \\mathcal{L}_{\\text {depth }}$,\nwhere LMSE is the mean square error loss between the rendered and ground truth RGB frames. Lflow and Ldepth denote the calibrated regularization term, respectively. The coefficients flow and Adepth balance the contribution of each term.\nSegment-based long video inference. To handle casual videos with hundreds of frames, we propose a simple but effective segment-based strategy during inference. The input video is divided into overlapping segments, where the adjacent segments share one frame. Due to our pixel-level spatial-temporal representation, Gaussian trajectories can be seamlessly propagated across segments through these shared frames, enabling NutWorld to process arbitrarily long videos while maintaining spatial-temporal consistency."}, {"title": "5. Experiment", "content": "5.1. Experimental Setup\nTraining Dataset. NutWorld is pre-trained on MiraData [26] and RealEstate10K [94]. MiraData is a high-quality video dataset consisting primarily of 3D engine-generated scenes and movie clips with diverse motion patterns. The RealEstate10K dataset contains indoor house tour videos that showcase various architectural scenes and camera movements \u2020. During pre-processing, we segment the original videos into video cubes, each containing 10 consecutive frames as the basic processing unit.\nImplementation Details. NutWorld is trained on 32 NVIDIA A100 (80GB) GPUs with a batch size of 256 for around 4 days. To improve computational efficiency, we integrate Flash-Attention-v2 [11, 13], gradient checkpointing [59], and mixed-precision training with BF16 [89]. The orthographic camera coordinates are bounded to [-1,1] along the x and y axes and to [0, 1] along the z axis. The input frames are resized to 512 \u00d7 288 to preserve the aspect ratio. We adopt a two-phase training strategy: a static phase using single frames (K = 1) with window size W = 576, followed by a dynamic phase where we initialize from the"}, {"title": "5.2. Video Reconstruction", "content": "Experimental Protocol. We evaluated the video reconstruction performance of NutWorld on 50 randomly selected test video clips from RealEstate10K and MiraData, both with a default length of 90 frames via standard reconstruction quality metrics (PSNR, SSIM, and LPIPS [93]). As there are currently no other feed-forward dynamic Gaussian approaches, we compared with optimization-based methods including Splatter-a-Video (SaV) [62], 4DGS [75], RoDynRF [38] and CoDeF [44] as the most relevant baselines. For fair comparison, all methods incorporate the confined canonical space, depth and flow supervision. We used the official implementations for most methods while SaV was reproduced according to the implementation details provided in their paper.\nComparison with Baselines. We evaluate NutWorld's representation effectiveness through both qualitative and quantitative experiments on video reconstruction."}, {"title": "5.3. Video Downstream Tasks", "content": "Large-scale pretrained NutWorld empowers video applications including object segmentation, frame interpolation, video editing, novel view synthesis, and consistent depth prediction."}, {"title": "6. Ablation Study", "content": "We analyze NutWorld's design choices through ablation studies on the 50 selected video clips. As shown in Table 2, our experiments demonstrate that discarding any component from the multi-component pipeline leads to significant performance degradation. Specifically:\nAblations on STAG representation. To validate the effectiveness of STAG representation, we perform an ablation by loosening its positional constraints. Following [64], we implement a less constrained variant where Gaussian positions are predicted with only tanh activation, limiting their range to [-1,1]. As shown in Table 2, this loosened constraint leads to significantly degraded performance by 10dB decrease in PSNR, with slower convergence, blurred artifacts and unstable optimization behavior. These results demonstrate the necessity of structured positional constraints, as"}, {"title": "7. Conclusion", "content": "In this paper, we introduce NutWorld, a novel framework for efficiently representing casual monocular videos through dynamic Gaussian Splatting. By introducing the structured STAG representation and incorporating effective depth and flow regularization, our approach successfully tackles several fundamental challenges in monocular video representation, achieving both spatial and temporal coherence without per-scene optimization. Comprehensive experiments demonstrate that NutWorld not only achieves high-fidelity video reconstruction in real-time but also enables various downstream applications. In the future, distilling rich visual features (e.g., SAM, CLIP) into our STAG representation and adapting our representation paradigm for video generation tasks are promising directions to explore."}]}