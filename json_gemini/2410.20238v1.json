{"title": "A Survey of Large Language Models for Arabic Language and its Dialects", "authors": ["Malak Mashaabi", "Shahad Al-Khalifa", "Hend Al-Khalifa"], "abstract": "This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of Arabic LLMs based on factors, such as source code availability, training data, model weights, and documentation. The survey highlights the need for more diverse dialectal datasets and attributes the importance of openness for research reproducibility and transparency. It concludes by identifying key challenges and opportunities for future research and stressing the need for more inclusive and representative models.", "sections": [{"title": "1. Introduction", "content": "Large-language models (LLMs) have gained significant attention owing to their high performance in a wide range of natural language tasks. These models learn to understand and generate language by training billions of parameters on vast volumes of textual data. Despite being a relatively new field, LLM research is advancing rapidly in various directions, including the development of models for different languages and dialects.\nArabic, the fifth most spoken language in the world, presents unique challenges and opportunities in the field of natural language processing (NLP). The Arabic language is characterized by its"}, {"title": "2. Method", "content": "In this section, we detail our method for gathering research papers for our survey on LLMs for the Arabic language and its dialects. Our process consists of three main stages: keyword-based search and data collection, data verification, and data analysis. The overall process is illustrated in Figure 1. In the following subsections, we describe these three stages in detail."}, {"title": "Stage One: Search and Data Collection", "content": "In this stage, we aim to gather a comprehensive collection of research papers relevant to LLMs for the Arabic language and its dialects. Our initial search query is customized to capture a broad range of Arabic LLMs by using keywords such as \"Arabic LLM\", \"Arabic large language model\", and \"Arabic dialect LLM\". These keywords were chosen to ensure the inclusion of both general LLMs, focusing on different forms of Arabic, and those addressing regional dialects. We restricted our search to LLMs that have a technical paper associated with them, excluding models that are mentioned only on platforms such as HuggingFace or in news without a published paper, these include: Noor [5], Noon [6], SILMA [7], Mulhem [8], Huawei's Arabic LLM [9], and Fanar [10]. The time frame for collecting papers was up to October 2024 to capture the most recent advancements in Arabic LLMs.\nThe data collection process utilizes widely used academic search engines, such as Google Scholar and Semantic Scholar. In addition, we conducted a supplementary search using Litmaps\u00b9, which tracks citation networks, allowing us to further expand our dataset. By outlining the connections between the cited works in each paper, we minimize the possibility of missing relevant studies."}, {"title": "Stage Two: Data Verification", "content": "To ensure the relevance and quality of the collected papers, we implemented a verification process designed to maintain the integrity of our survey. Each paper's abstract and conclusion were carefully reviewed to assess whether the research aligned with our specific focus on Arabic LLMs"}, {"title": "Stage three: Data Analysis", "content": "Our analysis process was designed to ensure a thorough understanding of the Arabic LLMs. The process involved the following steps:\nEach paper was carefully read and analyzed to extract relevant information related to Arabic LLMs. We focused on understanding the methodologies, data sources, and results of each study.\nTo ensure that no relevant papers were missed, we identified additional Arabic LLMs referenced in the related work sections of the papers that we initially collected. By tracing citations and references, we discovered and included further studies that were not part of our original search results, thereby broadening the scope of our survey and ensuring a more complete representation of the topic.\nFor each identified Arabic LLM, detailed information was extracted, including the model's architecture, dataset used for training, tasks it was applied to, and evaluation metrics reported. This step was crucial for comparing different models and identifying trends and patterns in Arabic LLMs research."}, {"title": "3. Arabic Large Language Models Architectures", "content": "This section explores the various architectures employed by Arabic language models, focusing on their designs and functionalities. The architectures of these models typically fall into three main categories: encoder, decoder, and encoder-decoder models. Each of these architectures plays a unique role in NLP tasks, affecting how models understand and generate Arabic text. Table 1 provides a detailed comparison of various Arabic LLMs categorized according to architecture type. This overview highlights the diversity in model design, reflecting specific use cases and strengths across different NLP applications."}, {"title": "3.1 Encoder-only Models", "content": "These models only use an encoder network. They were first developed for tasks that require understanding text, such as predicting categories for input text in text classification and next"}, {"title": "3.2 Decoder-only Models", "content": "Decoder models are designed to produce or recreate output data from internal representations, turning abstract information into a more accessible format [16]. They are particularly effective in translating complex data structures, such as latent features or encoded information, into coherent outputs. These models are frequently used in tasks such as natural language generation, image and video creation, and conversion of encoded data into readable formats. They play a crucial role in language modeling, content creation, and machine translation, where sequential logical output is necessary. Models like Generative Pretrained Transformer (GPT) represent decoder-only architectures that generate text using a unidirectional method, predicting the next word in a sequence by focusing solely on the preceding context. This makes them particularly suited for text creation and language processing tasks [17]. Notable examples include GPT2, GPT3, LLAMA, and Gemma, as described below."}, {"title": "3.3 Encoder-decoder Models", "content": "Encoder-decoder models are well-suited for tasks that require understanding and generating responses to the input text. A prominent example of this architecture is the Text-to-Text Transfer Transformer (T5) [27] that employs a unified text-to-text training framework for a wide range of NLP tasks. It implements a unique approach to MLM by masking the entire length of consecutive words rather than single tokens, which accelerates training by dealing with shorter sequences. Additionally, T5 optimizes its performance using adapter modules that fine-tune the model for specific NLP tasks during the fine-tuning phase. This allows the model to adjust efficiently to task- specific requirements without the need to retrain the entire model. AraMUS [28] and AraT5 [29] are examples of Arabic models that are based on this architecture."}, {"title": "4. Datasets for Arabic LLMs Pretraining", "content": "LLMs are typically pre-trained on vast amounts of text from the Internet, including websites, books, and articles. The quality and variety of these data directly affect the performance and capabilities of LLMs. This section provides an overview of the resources used to train Arabic LLMs."}, {"title": "4.1 Overview of Arabic Language Datasets", "content": "Arabic datasets for the development of advanced LLMs are growing rapidly, offering increasingly diverse datasets. These datasets help in improving natural language processing tasks in the Arabic language domain. In addition, they play a key role in enhancing the quality of LLMs in understanding and generating Arabic text. This will contribute to better communication technologies and digital solutions for Arabic-speaking communities and beyond. In the following sections, we provide details of the resources used for Arabic LLMs pre-training, categorized by Arabic forms. We begin with the CA form, followed by the MSA form, the DA form, and finally the combinations of different Arabic forms of resources."}, {"title": "4.2 Classical Arabic (CA) Resources", "content": "Classical Arabic (CA) is the language of the Quran and a standardized literary form of Arabic that has been used since the 7th century [33]. Despite its historical nature, CA remains relevant for various applications including religious studies, historical research, and cultural preservation. CA is primarily used in religious and historical contexts and plays a significant role in developing specialized Arabic LLMs despite its limited use in everyday life. The limited availability of digitized CA compared with MSA and DA presents challenges for LLMs training. However, projects such as"}, {"title": "4.3 Modern Standard Arabic (MSA) Resources", "content": "Modern Standard Arabic (MSA) is the standardized and literary form of Arabic used in writing and formal speech throughout the Arab world that developed in the late 19th and early 20th centuries [49]. It serves as the foundation for most Arabic NLP tasks, such as news analysis and official document processing. This is critical for LLMs development due to its formal and standardized nature. MSA's importance derives from its role as a formal language in the Arab world, bridging the gap between various Arabic dialects and serving as the primary language for written media, literature, and formal communication. Recent years have seen significant growth in MSA resources, reflecting the increasing demand for Arabic language technologies. These datasets typically consist of web-crawled data, news articles, Wikipedia content, and other formal Arabic texts."}, {"title": "4.4 Dialectal Arabic (DA) Resources", "content": "Dialectal Arabic (DA) refers to the diverse spoken forms of Arabic that are used in everyday communication throughout the Arab world. These dialects differ significantly from MSA across several linguistic dimensions, including phonology, morphology, orthography, and syntax [50]. DA is typically categorized by region into major groups, such as Egyptian, North African, Levantine, Gulf, and Yemeni dialects. Each of these groups contains further sub-varieties, including specific dialects such as Tunisian, Algerian, Lebanese, Syrian, Jordanian, Saudi, and Qatari [51].\nGiven the widespread use of DA in daily conversations, integrating DA resources into the training of LLMs is essential for developing models capable of understanding and generating Arabic, as it is spoken in real-life contexts."}, {"title": "4.5 Combination of Arabic Forms Resources", "content": "Although many Arabic LLMs focus on specific forms of Arabic, such as MSA or DA, some models leverage a combination of these forms to create more adaptable and realistic language models. This approach mirrors real-world language use, where Arabic speakers often transition between different forms, depending on the context. For instance, in social media conversations, users may alternate between MSA and DA, while CA, although less frequently used, might appear in religious or classical citations. In addition, in multicultural settings, speakers sometimes switch between different dialects, reflecting the diverse linguistic landscape of the Arab world. By combining multiple forms of Arabic, these models aim to handle a broader range of tasks and more accurately represent the complex interplay between linguistic forms in daily life. This approach aligns with the natural way in which Arabic is used in real-life scenarios."}, {"title": "4.6 Discussion", "content": "The datasets used for pre-training Arabic LLMs reflect the rich diversity of the Arabic language and its dialects. However, challenges remain, particularly in representing DA and CA forms. As shown in Figure 2, existing LLMs primarily focus on MSA, which forms the foundation of many formal texts across the Arab world, including news articles, academic works, and official documents. Datasets such as the 1.5 billion Words Corpus, the OSIAN Corpus, and the Arabic Wikipedia dump form the core resources for MSA-based LLMs, as well as for models that incorporate combinations of MSA with other forms of Arabic. Figure 3 illustrates the frequency with which different datasets are used across models, showing the critical role of these resources in LLM development."}, {"title": "5. Arabic LLMS", "content": "This section provides an overview of monolingual, bilingual, and multilingual LLMs for the Arabic language and its dialects."}, {"title": "5.1 Monolingual Models", "content": "The development of monolingual Arabic language models has seen significant progress since 2020, with models based on various architectures, such as BERT, GPT-2, ELECTRA, T5, and RoBERTa. These models were trained exclusively on Arabic text, allowing them to learn the nuances of a language's morphology, syntax, semantics, and dialectical variations. This section provides a comprehensive overview of the 26 monolingual Arabic models, discussing their architecture, the variety of downstream tasks they are designed to handle, and the baseline models used for performance evaluation."}, {"title": "5.1.1 BERT-Based Models", "content": "AraBERT, introduced in 2020, was the first known BERT-based model pre-trained for the Arabic language. The model followed the same BERT architecture with approximately 110M parameters and was trained on 70M Arabic sentences. The model has two variants: AraBERT-v.01, which does not require sub-word unit segmentation during training, and AraBERT-v1, which was trained using sub-word segmentation. Both variants were fine-tuned for Sequence Classification, Named Entity Recognition (NER), and QA, and were evaluated on Sentiment Analysis (SA), NER, and QA tasks and compared to the multilingual BERT, namely mBERT [11]. The results showed that both versions of AraBERT outperformed mBERT on SA and NER tasks, particularly in dialects, noting that AraBERT was trained on MSA data only. For the QA task, AraBERT had a better F1 score and sentence match, but a lower score on the exact match metric.\nThe success of AraBERT has paved the way for subsequent BERT-based models tailored for Arabic. MARBERT and ARBERT, released later in 2020, share the same BERT-based architecture but differ in their training data. ARBERT focused on MSA, while MARBERT incorporated both MSA and dialectal Arabic data. For the evaluation, the authors compared both models to the multilingual models (mBERT and XLM [52]) and AraBERT across five downstream tasks. The results revealed MARBERT's superiority in SA, Social Meaning, and Dialect Identification, whereas ARBERT excelled in Topic Classification. Both models performed equally well in the NER tasks. Overall, the evaluation demonstrated that ARBERT excelled in formal Arabic tasks, whereas MARBERT showed superior performance in dialectal Arabic and social media-related tasks."}, {"title": "5.1.2 Dialectical BERT-Based Models", "content": "Linguistic diversity across the Arab world has prompted the development of dialect-specific BERT- based models to address the unique characteristics of various Arabic dialects. For instance, SudaBERT focused on Sudanese Arabic, utilizing the Arabic-BERT architecture, and trained on 13 million sentences in the Sudanese dialect. The model was compared with Arabic-BERT on SA and NER tasks, and the results showed that SudaBERT outperformed Arabic-BERT in the SA task in the Sudanese dialect, whereas Arabic-BERT performed better in SA and NER in MSA.\nAraRoBERTa is a dialect-specific language model trained using the RoBERTa-base configuration [55] for seven dialects, Saudi, Egyptian, Kuwaiti, Omani, Lebanese, Joradanian, and Algerian, where each dialect is used to train a separate model. The baseline models used to compare the performance of the models were mBERT, XLM-R, and AraBERT. The models were trained for dialect classification using fully, semi-supervised, and weakly supervised approaches. For the fully supervised approach, in addition to the baselines, the model was compared to a traditional logistic regression model, which yielded the best results in the Kuwaiti, Lebanese, Jordanian, and Algerian dialects, whereas AraRoBERTa performed best in the Saudi and Egyptian dialects because of their larger data size compared to other dialects. The semi-supervised approach obtained better results than the supervised approach in Egyptian, Omani, Lebanese, and Algerian dialects. However, the weakly supervised approach performed poorly compared with the other approaches.\nThe DziriBERT is specializes in Algerian Arabic, trained on a dataset of 1.1 million tweets in the Algerian dialect and has the same architecture as the BERT-base model. The authors have compared DziriBERT to multilingual transformers (mBERT and XLM-R) and multiple standard and dialectal Arabic models (AraBERT, QARIB, CAMELBERT-DA, CAMELBERT-mix, and MARBERT). The results show that DziriBERT obtained the best results in SA, Emotion Classification, and Topic Classification tasks. The MARBERT model was the second-best in SA and Emotion Classification.\nTunBERT targets Tunisian Arabic using a corpus of 500k sentences extracted from Tunisian social media, blogs, and websites. The authors implemented TunBERT with two approaches: the first approach, namely TunBERT-P, relies on BERT Pytorch implementation, and the second approach, TunBERT-T, relies on Tensorflow implementation. The authors have compared TunBERT to mBERT, AraBERT, GigaBERT, and MARBERT on SA, Dialect Identification, and Reading Comprehension QA tasks. The results show that TunBERT-P achieved the best performance in SA and dialect identification but failed to perform better than AraBERT and GigaBERT in the Reading Comprehension QA task. In addition, TunBERT-P performed better than TunBERT-T for all three tasks."}, {"title": "5.1.3 GPT-Based Models", "content": "Following the emergence of encoder-only BERT-based models, decoder-only models, namely, GPT, have also been adapted for Arabic, offering new capabilities in text generation and completion tasks. These models have shown promise in various applications ranging from question-answering to language understanding.\nThe first published work that leveraged the GPT-2 architecture for Arabic is AraGPT-2, with four size variants: base (135 million parameters), medium (370 million parameters), large (792 million parameters), and mega (1.46 billion parameters), noting that the large and mega variants utilized the GROVER model [58]. All variants were trained on a substantial dataset comprising of 8.8 billion words. The perplexity score, which measures the degree of uncertainty of the model when assigning probabilities to the text, was used to evaluate the model. In zero-shot QA and translation downstream tasks, AraGPT-2 performed well in zero-shot QA related to countries, birth and death years, and geography but struggled with questions involving quantities. The model's performance in English-to-Arabic translation was poor. Notably, a human evaluation revealed that AraGPT-2- mega could fool 60% of the subjects, with longer passages being more convincing than shorter ones.\nBuilding upon AraGPT-2, AraQA was developed as an Arabic generative question-answering model for religious Arabic text, which was fine-tuned using the AraGPT-2 architecture on 88.6 thousand question-answer pairs. Although the authors evaluated the model using perplexity and cross- entropy loss, they did not compare it to existing Arabic models or benchmarks, limiting the context for its performance.\nJASMINE, based on the GPT-3 architecture, represents a significant scaling up of Arabic language models, with four size variants and a number of parameters varying between 350 million and 6.7 billion parameters. The dataset that the model was trained on comprises of 71.5 billion tokens of classical, modern, and dialectical Arabic. Two evaluation strategies were followed: Intrinsic Evaluation where the Perplexity metric was used, and Extrinsic Evaluation, where zero-shot, one- shot, and few-shots were used to evaluate the model on five tasks: language modeling, autocompletion, common sense inference, word manipulation, and natural language understanding. The authors compared JASMINE with AraGPT2 and mGPT [59] for the perplexity score, and the results showed that JASMINE-6.7B obtained the best score. As for the downstream task results with different shot settings, it was concluded that JASMINE-6.7B performed best in autocompletion, word manipulation, and natural language understanding, whereas JASMINE-2.7B performed best in common sense inference."}, {"title": "5.1.4 Other Models", "content": "While BERT-based models dominate the landscape of Arabic language models, several other architectures have been explored to push the boundaries of Arabic language models. These include models based on ELECTRA, T5, BART[60], and RoBERTa, each of which has unique strengths in the field.\nAraELECTRA was the first Arabic language model based on the ELECTRA architecture, which is larger than BERT, consisting of 136 million parameters. The model was trained on a dataset of 8.8 billion words and evaluated on three downstream tasks: QA, SA, and NER. The authors compared their model against AraBERT, Arabic-BERT, and ARBERT. The evaluation results showed that AraELECTRA achieved the highest performance in all tasks, except for one QA task, where the much larger AraBERTv0.2-large maintained an edge.\nAraBART, which utilizes the BART-Base architecture, with 139M parameters, was trained using the same corpus as AraBERT. The model focused on abstractive summarization and was compared to"}, {"title": "5.2 Bilingual Models", "content": "Along with monolingual Arabic models, the field has witnessed the emergence of bilingual language models specifically trained on parallel corpora comprising both Arabic and English data. These bilingual Arabic-English models leverage the strengths and linguistic characteristics of both languages, enabling enhanced cross-lingual capabilities. This section delves into the architectural approaches, training methodologies, and evaluation benchmarks employed in developing bilingual Arabic-English language models, shedding light on their ability to bridge the linguistic and cultural gaps between these two widely spoken languages.\nGigaBERT is a BERT-based bilingual model designed for Information Extraction tasks in English and Arabic that focuses on NER, POS Tagging, Argument Labeling, and Relation Extraction. It comes in five variants, each trained on progressively larger datasets of English and Arabic data. Compared to baseline Arabic and multilingual models such as AraBERT, mBERT, XLM-R, and GigaXLM-R, GigaBERT variants demonstrated superior performance in most tasks across English, Arabic, and zero-shot transfer scenarios. However, XLM-R outperformed GigaBERT in POS tagging and Argument Labeling for English and zero-shot transfer tasks.\nJais is a 13B parameter Arabic-English decoder-only language model based on GPT-3 architecture, trained on 348B tokens (116B Arabic, 232B English), with the Arabic dataset augmented from an initial 55B tokens. Its instruction-tuned variant, Jais-chat, was fine-tuned on 9.6M instruction- response pairs. Both models were evaluated against baselines like AraT5, AraBART, mT0, BLOOM, BLOOMz, LLAMA, LLaMA2, and LLaMA2-chat on tasks including World Knowledge, Commonsense Reasoning, and Misinformation and Bias in Arabic and English. Jais and Jais-chat outperformed these baselines, establishing themselves as state-of-the-art for Arabic LLMs, particularly in terms of knowledge acquisition and commonsense reasoning. BLOOMz emerged as the best baseline for Arabic, while instruction-tuning further improved performance, with Jais-chat achieving the highest scores. Remarkably, Jais-chat excelled even in English tasks, surpassing dedicated English models despite having less training data and a smaller parameter size.\nAceGPT is an LLaMA2-based model, with AceGPT-chat being a version that has undergone supervised fine-tuning and reinforcement learning from Al feedback. The model was trained using two versions of LLaMA2: LLAMA2-7B and LLaMA2-13B. The 7B version was trained with 30B tokens (19.2B in Arabic and 10.8B in English), while the 13B version was trained with 10B tokens"}, {"title": "5.3 Multilingual Models", "content": "In recent years, there has been a surge in the development of LLMs; however, most of them are multilingual, supporting multiple languages, including Arabic. ArabicBERT is an Arabic model that uses a combination of BERT and Convolution Neural Network (CNN) layers. The model was trained on three languages: Arabic, Greek, and English. ArabicBERT has four variants of different sizes trained on the same data. The ArabicBERT model was compared to SVM with TF-IDF, multilingual BERT, Bi-LSTM, CNN-Text, and BERT, for Offensive Language Detection in downstream tasks. The results show that ArabicBERT outperformed other baseline models in all languages except Turkish, where BERT performed the best.\nAraT5 was the first sequence-to-sequence Arabic language model using a T5-Base encoder- decoder architecture. AraT5 has three models, each of which is trained on a different type of data. The first model is AraT5-MSA, trained on MSA data; the second model is AraT5-TW, trained on Twitter data; and lastly, AraT5, which was trained on both MSA and Twitter data. The authors compared their models to a vanilla sequence-to-sequence transformer (S2S) [65] and mT5 on the seven downstream tasks, namely: MT, code-switched translation (CST), text summarization (TS), news title generation (NGT), question generation (QG), transliteration (TR), and paraphrasing"}, {"title": "5.4 Discussion", "content": "Figure 5 illustrates the geographic distribution of Arabic LLMs, showing how different models are trained and developed across regions, offering insight into regional disparities and concentrations in Arabic language model development.\nBased on our previous overview of Arabic LLMs, the landscape of Arabic language models encompasses a diverse range of architectures tailored to different use cases and capabilities. As summarized in Table 6, monolingual models focus exclusively on the Arabic language, allowing a nuanced understanding of dialects, syntax, and other linguistic features. Architecturally, BERT- based encoders continue to dominate, but incremental innovations with models such as AraGPT, AraELECTRA, and AraMUS have brought new capabilities, establishing architectures such as GPT, ELECTRA, and T5. Downstream performance has surged on tasks such as SA, NER, QA, and dialect identification, often approaching or exceeding multilingual baselines. Evaluation metrics vary widely across models, with accuracy and F1 score emerging as the most commonly used metrics. Addressing these gaps presents an opportunity to widen accessibility and enhance the understanding of Arabic language modeling struggles. With computing resources expansion and accelerating model scaling, the future is bright for monolingual Arabic models that address a wide range of applications."}, {"title": "6. Arabic LLMs Openness", "content": "In recent years, there has been an increase in language models claiming to be open, but how open are they really? This question raises the demand for creating an openness assessment to explore different dimensions of the openness of language models. Based on the literature [73], [74], free of availability is not equal to openness and transparency. In addition, making the weights of the models available in the name of openness while maintaining the architecture and how the system is built under wraps aligns with the practice of openwashing [75]. This section discusses 12 elements of openness, including the availability of the code and weights, clear and scientific documentation, and access methods, based on the framework of Liesenfeld and Dingemanse [76], which analyzes European LLMs and their openness alignment with the EU AI Act [77]. The EU AI Act was chosen to validate the openness of Arabic LLMs because of the unavailability of rules and regulations for the openness of Al in Arabic-speaking countries."}, {"title": "6.1 Key Elements of Openness", "content": "The framework follows the composite and graded approach, which entails that the framework comprises of multiple elements for assessments, where each element is graded as open, partial, or closed, to provide a well-informed and systematic judgment of the openness of the models. This section discusses three dimensions\u2014availability, documentation, and access\u2014and each element under the dimension that is used for grading."}, {"title": "6.2 Availability", "content": "The availability dimension contains six elements: open code, data, weights, instruction-tuning (RL) data, RL weights, and license. The open code indicates the availability of source code for training, fine-tuning, and running the model [78]. We can see that 16 of the 36 models fully satisfy the open code element, whereas six models partially make their codes available. Although ARBERT and MARBERT come from the same paper, the authors only provided code for fine-tuning MARBERT for sentiment analysis. The same applies to JABER and SABER, where the authors provided a code for running and fine-tuning JABER only, owing to the large size of the SABER. Some authors have not provided the code but are willing to provide it per request, such as the JASMINE model.\nThe availability of the data element refers to the public release of datasets, which enables access and reuse [79]. We can see that 17 models have their trained data fully available and 17 models have their data partially available, particularly their own collected data. The RL data element refers to the data used for instruction-tuning of the model. While they are not utilized for BERT-based models, GPT- and Llama-based models have utilized instruction-tuning, particularity AraStories, and Atlas-Chat, where they make the RL data fully available. However, AceGPT, JAIS, and ALLAM make their RL data partially available.\nThe availability of the model weights enables the reconstruction and production of the full model. The majority of published models on huggingface have model weights available for usage. Some models, namely QARiB and AraQA, have their source code fully available, but the weight of the model has not been published, indicating that users can retrain the model from scratch using their own datasets; however, they cannot replicate the exact performance and outputs of the original trained model. RL weights refer to the weights of the model after instruction-tuning. We can see that only JAIS and AceGPT made their instruction-tuned model weights available."}, {"title": "6.3 Documentation", "content": "The documentation dimension contains four elements: code, architecture, paper, and model card. The documentation of the code aligns with the availability of the open code element, such that when the code is available, it is most likely well-documented with the addition of comments and explanations to the source code. However, in some models, such as AraQA, AraPoemBERT, and SaudiBERT, the code is fully available, but the code documentation lacks some details. The JAIS and ArabicBERT models have their source code fully available but with no code documentation, making code understanding challenging for users.\nThe documentation of the models' architecture is usually fully described in the paper. Most of the models provide a full description of the model's architecture, including the base model, number of parameters, and layers. However, some models such as ARBERT, QARIB, and SudaBERT lack architectural details such as vocabulary and context size.\nThe documentation of models through research papers and model cards achieves full transparency and extensibility [78]. While most models are documented in research papers, Noor, Noon, and SILMA are exceptions because of their paid service nature. Noor has a research paper documenting its base model and parameter size but does not contain information about its training data or the specific training procedures used. Model cards provide metrics, usage guidance, and details regarding the model [81]. Although most models have full details documented in research papers, many are missing the model card document. Only 12 out of 36 models had their model cards fully documented, reflecting a gap in the standardization of model documentation. The absence of model cards for many models suggests a need for improved documentation practices to enhance user understanding and transparency."}, {"title": "6.4 Access", "content": "The access dimension includes two elements: access via an API and downloadable software package. Most models are available in HuggingFace and can be accessed through a pipeline API call for model usage or an installed library package via a public code repository from GitHub. The only closed-source models with no available access are SudaBERT, SABER, AraLegal-BERT,"}, {"title": "7. Conclusion", "content": "This survey provides a comprehensive overview of the current state of LLMs in Arabic NLP. We explored various aspects of Arabic LLMs including their architecture, training datasets, evaluation metrics, and openness.\nOur analysis of the datasets used for pre-training Arabic LLMs highlights the diversity of available resources, including CA, MSA, and DA. We found that, while MSA resources are relatively abundant, there is a need for more datasets representing CA and a wide range of Arabic dialects. Addressing this data imbalance is crucial for the development of more inclusive and representative Arabic LLMs.\nWe have also delved into the realm of monolingual, bilingual, and multilingual LLMs for the Arabic language and its dialects. Our review of these models shows the variety of architectures employed, including encoder-only models (e.g., BERT, ELECTRA, and RoBERTa), decoder-only (e.g., GPT), and encoder-decoder models (e.g., T5). These LLMs have been applied to a wide range of downstream tasks such as sentiment analysis, named entity recognition, question answering, and machine translation. Evaluations against baseline models demonstrated the significant progress made in Arabic NLP using LLMs.\nFurthermore, we assessed the openness of Arabic LLMs using a comprehensive framework that considered factors such as the availability of source code, training data, model weights, and documentation. Our findings highlight the importance of openness for reproducibility and transparency and facilitate further research and development in the field.\nDespite the advancements in Arabic LLMs, there are still several challenges and opportunities for future research. A key direction is the development of more diverse and representative datasets, particularly for CA and dialectal variations. This will enable the creation of LLMs that can better capture the richness and nuances of the Arabic language across its various forms.\nAnother important avenue for future research is the exploration of novel architectures and training techniques specifically tailored to the unique characteristics of Arabic such as its complex morphology and syntax. This could involve the development of Arabic-specific tokenization methods, attention mechanisms, or pre-training objectives that consider the linguistic properties of a language.\nIn addition, there is a need for more standardized evaluation benchmarks and metrics for Arabic LLMs. The establishment of common datasets and tasks will facilitate the comparison and assessment of different models, thereby driving further progress in the field.\nIn conclusion, this survey provides a valuable resource for researchers, practitioners, and enthusiasts interested in the current state and future direction of Arabic LLMs. By highlighting the achievements, challenges, and opportunities in this field, we hope to inspire further research and development efforts to advance Arabic NLP and unlock the full potential of language technologies for Arabic-speaking communities worldwide."}]}