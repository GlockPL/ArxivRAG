{"title": "MODEL AGNOSTIC HYBRID SHARDING FOR HETEROGENEOUS DISTRIBUTED INFERENCE", "authors": ["Claudio Angione", "Yue Zhao", "Harry Yang", "Ahmad Farhan", "Fielding Johnston", "James Buban", "Patrick Colangelo"], "abstract": "The rapid growth of large-scale AI models, particularly large language models has brought significant challenges in data privacy, computational resources, and accessibility. Traditional centralized architectures often struggle to meet required data security and scalability needs which hinders the democratization of AI systems. Nesa introduces a model-agnostic sharding framework designed for decentralized AI inference. Our framework uses blockchain-based sequential deep neural network sharding to distribute computational tasks across a diverse network of nodes based on a personalised heuristic and routing mechanism. This enables efficient distributed training and inference for recent large-scale models even on consumer-grade hardware. We use compression techniques like dynamic blockwise quantization and mixed matrix decomposition to reduce data transfer and memory needs. We also integrate robust security measures, including hardware-based trusted execution environments to ensure data integrity and confidentiality. Evaluating our system across various natural language processing and vision tasks shows that these compression strategies do not compromise model accuracy. Our results highlight the potential to democratize access to cutting-edge AI technologies by enabling secure and efficient inference on a decentralized network.", "sections": [{"title": "INTRODUCTION", "content": "Centralized AI inference poses significant risks related to data privacy, computational bottlenecks, deterministic output, and single points of failure. The prohibitive cost and scarcity of high performance computing resources prevent the mass adoption of the only counter option to centralization, which is open-source decentralized AI. These challenges further limit the ability to contribute to training, fine-tuning, and AI inference at scale. This severely prohibits the adoption of state-of-the-art AI models for enterprises and developers, as well as shared research around the world.\nRecent large language models are available with more than 100 billion parameters [1, 2, 3, 4], which makes it important to train them on powerful and costly accelerated hardware such as GPUs and TPUs. [5]. Several approaches can be used to make these models more accessible, especially during the fine-training and inference phase. Using the APIs is one possible approach, which allows quicker inference passes from pre-trained models, but offers little customisation capacity, and no options to change or optimise the training process [2, 6]. A second solution is offloading, where the model components are moved to slower memory (e.g. RAM or SSD) [7]. Then, only the relevant portion of the model is iteratively moved to the available GPU, allowing it to run on less expensive hardware. However, this process involves frequent data transfers and can be extremely slow for larger models.\nCollaborative efforts to AI execution today are further hindered by the challenges around security [8]. Techniques like continuous and domain adaptation offer partial solutions by enabling model sharing without direct data exchange [9]. However, these methods are prone to backdoor attacks and often result in sub-optimal model performance due to the limitations of semi-supervised or unsupervised fine-tuning. Collectively, these issues have real-world implications for businesses requiring critical"}, {"title": "DISTRIBUTED TRAINING AND INFERENCE", "content": "Distributed inference and training across multiple nodes are essential due to the exponential increase in the size of models, their complexity and the scarcity of computational resources available to process them [17]. The essence of both of these tasks lies in the necessity to handle vast computational loads more efficiently and to reduce the latency involved in generating predictions and updating model parameters [18]. Our approach uses the collective computational resources and memory available across several processing nodes. This makes it possible to handle larger models or increased batch sizes without a proportional increase in inference or training time. One of the critical aspects of efficient distributed inference and training is partitioning the computational graph of any neural network. This allows the nodes with limited resources to only handle a segment of the model during the inference or training phase. The computational graph represents all the operations and data flows within the model from input to output. By Partitioning this graph we divide the model's"}, {"title": "NESA'S MODEL PARTITIONING APPROACH", "content": "Due to the novelty of running deep learning algorithms on the blockchain, Nesa's partitioning mechanism aims to minimize the amount of data that must be transferred between different nodes, thus reducing the latency and bandwidth requirements for both inference and training phases. The overall throughput of the inference and training tasks is limited by the slowest part of the system, known as the bottleneck stage [22]. We therefore introduce mechanisms to balance the workload and avoid bottlenecks. The partitioning must balance the computational load across all nodes to maximize throughput. This balance ensures that all parts of the system work at their full potential without any single stage becoming a drag on performance.\nWe prioritize fast memory like Static Random-Access Memory (SRAM) for distributed inference and training across multiple nodes which is essential for storing intermediate model outputs, i.e., activations, and parameter weights [23, 24]. SRAM is significantly faster than conventional memory solutions but is also more expensive and limited in size. Each node in the network contributes its SRAM, which multiplies the available fast memory during the distributed inference and training session. This increase in capacity allows for caching more model parameters in fast memory."}, {"title": "BLOCKCHAIN-BASED SEQUENTIAL DEEP NEURAL NETWORK SHARDING", "content": "Nesa developed a new approach for network topology-informed model sharding, named Blockchain-based Sequential deep neural network Sharding (BSNS). Our approach establishes and solves an optimization problem that can find a sequence of nodes able to run an inference request. Each node will be typically a distributed machine on the network, which will normally run a block or a contiguous sequence of blocks of a deep learning architecture. It is crucial that for each inference session involving block sharding, a chain of nodes is found that can collectively reconstruct the layers of the full model [25].\nThe key innovation of our approach is that the selection of nodes for sharding is informed by: (i) the topology of the network; (ii) our persistent homology metrics based on graph embeddings and (iii) network-based variables, including latency and geographical distance between the nodes. Taken together, this will constitute a full end-to-end neural network that performs inference across the blockchain at the optimal speed, fully embedding the topology and the security components of the blockchain."}, {"title": "SWARM CREATION AND DYNAMIC REBALANCING", "content": "The BSNS framework allows arbitrary deep neural networks to be distributed for training, but with specific focus given to LLMs and transformer-based architectures. The swarm is formed based on a heuristic that selects the optimal set from the pool of available nodes. Each node inside the swarm processes a shard of the network architecture and communicates through a sequence of remote procedure calls (RPC). Each $N_i$ denotes a node within the network which is responsible for the i-th shard of the model. Whereas, p represents the total number of shards distributed across the swarm. The selection process heuristic considers different network topology features, such as node and edge structure, bandwidth, and computational capabilities, therefore ensuring optimal efficiency in data processing across the swarm. This has strong practical benefits for our clients, as distance and latency heavily affect the performance when computing using a blockchain.\nGiven a network A with n nodes that need to execute $p < n$ shards of a model, and assuming that each block is held by one network node, the task involves finding a sequence S of nodes\n$S: (A_i)_{i\\in1,...,p}.$ (1)"}, {"title": "CACHE OPTIMIZATION TO ENHANCE EFFICIENCY", "content": "BSNS uses a Key Value (KV) cache for scaling LLMs across a distributed network. This minimizes the computational overhead associated with token generation in LLMs when dealing with models that operate on a token-by-token basis [26, 27]. This mechanism uses the inherent characteristics of transformer models by caching the key and value vectors after their initial computation to prevent redundant calculations in subsequent generations. This also decreases the load on the network's nodes and allows the processing of longer sequence lengths in LLMs [28].\nCaching improves efficiency by ensuring that each node within a swarm can generate tokens by utilizing pre-computed key and value vectors. This is a core factor that increases the overall throughput of the system. Moreover, it enables the system to scale to models with extensive context sizes by removing the memory and computational constraints that typically hinder the operation of large LLMs."}, {"title": "BSNS WITH PARAMETER-EFFICIENT FINE-TUNING VIA ADAPTERS", "content": "BSNS uses Low-rank adaption techniques to enable fine-tuning capabilities for arbitrary language models [14]. The adapters are inserted between the transformer layers which allows learning the downstream tasks without retraining the entire network as it can be computationally expensive[14, 30]."}, {"title": "OPERATIONAL FRAMEWORK FOR ADAPTERS IN NESA", "content": "Adapters are small neural network modules inserted between the layers of a pre-trained model. They allow task-specific training with minimal parameter updates [14]. The operation of an adapter within a transformer layer is given as:\n$h_i' = LayerNorm(h_i + W_2 \\sigma(W_1 h_i)),$ (5)\nwhere $h_i$ is the input to the adapter at layer i, $W_1$ and $W_2$ are trainable weights of the adapter, $\\sigma$ represents a non-linear activation function, and $h_i'$ is the output of the adapter [31].\nThe adaptation of a weight matrix W in a transformer can be modeled as\n$W' = W + BA,$ (6)\nwhere W is the original weight matrix of the model, $A \\in R^{r\\times n}$ and $B \\in R^{n\\times r}$ are the low-rank matrices introduced by LoRA with $r < n$, and W' is the adapted weight matrix [14]. Upon initiating a fine-tuning session, each participating node initializes adapter modules according to the specified configuration aligned with the shard of the LLM it is responsible for."}, {"title": "NODE SYNCHRONIZATION", "content": "Nodes collaboratively fine-tune their adapter modules using gradients from task-specific data processed through their portion of the LLM. The process includes three main steps:\n1.  Forward pass. Nodes perform a forward pass through transformer layers and adapters which generates activations based on the input tensors.\n2.  Backward pass. From the final layer of a large language model, gradients are propagated by comparing predictions with true labels. Gradients for corresponding adapters are used to perform the gradient descent update.\n3.  Synchronization and update. Nodes synchronize their updates across the network to ensure consistency and convergence\nThe parameter updates for the adapters, denoted as $\\theta_i$ for the i-th node, follow the rule:\n$\\theta_i^{(t+1)} = \\theta_i^{(t)} - \\eta \\nabla_{\\theta_i^{(t)}} L(\\theta_i^{(t)})$ (7)\nwhere t is the current iteration, $\\eta$ is the learning rate, and $L$ is the loss function.\nAlong with these synchronized update rules, BSNS uses a consensus mechanism for agreeing on fine-tuning objectives, data distribution, and synchronization intervals. This setup allows for the sharing and reuse of fine-tuned adapters and allows nodes to build on existing adaptations for new tasks."}, {"title": "DYNAMIC SHARDING OF NEURAL NETWORKS", "content": "The BSNS method works well for LLMs as they have common blocks that can be distributed on a sequence of nodes. However, for arbitrary layered neural networks like diffusion or sequence"}, {"title": "ENHANCED MTPP SLICING OF TOPOLOGICAL ORDER", "content": "The MTPP algorithm is key in the optimization of computation graphs for neural network inference across multiple systems. It is known to be NP-hard, as shown by its relation to the minimum makespan scheduling problem [35]. This makes fully polynomial approximations unlikely. Instead, we use a heuristic approach to handle this complexity and maintain high throughput. For non-transformers based architectures, our method combines Kahn's algorithm [36] for topological sorting with dynamic programming and segment trees. Kahn's algorithm ensures correct topological order for acyclic partitioning. Dynamic programming then calculates the best partitioning and Segment trees quickly adjust partitions during optimization. This combination helps in distributing the computational load evenly and reduce communication delays [37, 38].\nOur approach centres around the segment cost data structure which calculates costs based on computation and communication for each segment. We start with a computation graph G and a topological order \u03c0. The SliceGraph algorithm uses this to divide the graph into throughput-optimized segments. It dynamically splits the graph into blocks which distributes workload evenly and reduces communication between nodes."}, {"title": "BIASED RANDOM-KEY GENETIC ALGORITHM", "content": "BRKGA improves our approach by using stochastic and evolutionary methods which overcomes the limitations of heuristic-based techniques [40, 41]. This algorithm enables broad exploration of partition configurations, essential for solving the NP-hard MTPP. Over generations, BRKGA evolves partitioning strategies to discover solutions that balance computational load and reduce communication overhead and achieves optimal throughput in distributed deep neural network inference [20].\nBRKGA evolves node priority vectors $x \\in [0, 1]^n$ and links these priorities directly to partitioning quality for adjustments. The process starts by creating a population of random vectors. These vectors help setup orders in the SliceGraph algorithm. Each vector's quality is assessed for throughput optimization and latency reduction which determines its fitness. BRKGA refines the strategies until convergence criteria for node priorities is met. This process involves selection, crossover $\\text{new} = \\alpha \\text{parent}_1 + (1 - \\alpha) \\text{parent}_2$, and mutation to generate near optimal offsprings.\nThe BRKGA approach offers many advantages over sequential sharding methods for partitioning neural networks. Sequential methods follow a predetermined path which perhaps can miss more efficient configurations that could improve computational throughput and reduce communication latency. This adaptability is important in distributed environments with varying network topologies and node capabilities."}, {"title": "SWARM TOPOLOGY", "content": "Due to the dynamic topology of the swarm network, we further extend our computational efficiency by monitoring and classifying changes and the current state of our swarm topology over time. In conjunction, we build several pre-computed sharding schemas based on the commonly observed swarm topologies as we monitor our networks. In doing so, we can quickly select among sharding schemes to find one that is near-optimal for the current swarm topology. In the following section, we outline how we monitor and analyze the swarm topology."}, {"title": "TOPOLOGICAL FORECASTING AND MONITORING", "content": "Dynamic sharding is computationally expensive as finding an optimal sharding scheme is known to be NP-hard [20]. We mitigate this expense by precomputing highly efficient sharding schemas for different topological states of our network on a per-model basis. As the network topology itself is dynamic, we periodically monitor our network using techniques from (weighted) persistent homology [42, 43], to give snapshots of topological characteristics to guide the applications of our pre-computed sharding schemas. The main goal of this section will be to introduce many of the concepts behind persistent homology (which is not a mainstream topic) to guide intuition on how this computational tool can be leveraged to make dynamic sharding optimization much more tractable.\nAs a basic introduction to the implementation of this framework, we briefly illustrate the mathematical technique of simplicial homology. This uses the simplicial complex to calculate the homology groups these groups are formally Abelian groups but they can be thought of as vector spaces. A simplicial complex is a collection of simplices that are the convex hull of a group of vertices. More explicitly, an n-simplex $\\Delta^n$ contains n + 1 vertices {$v_0, v_1, . . . , v_n$} and is the collection of points in $R^n$ such that:\n$\\Delta^n = \\{k_0 v_0 + ... + k_n v_n | \\sum_{i=1}^{n} k_i = 1 \\text{ and } k_i \\geq 0 \\text{ for } 0 \\leq i \\leq n\\}$ (12)\nThus, a 0-simplex $\\Delta^0$ is a point, a 1-simplex $\\Delta^1$ is a line segment, a 2-simplex $\\Delta^2$ is a triangle, and so on. To build a simplicial complex out of a collection of simplices, we need to define how to glue the pieces together [44]. Mathematically it is given as (linear) boundary maps $\\partial_n: C^n \\rightarrow C^{n-1}$ which map each n-simplex $\\Delta$ to its boundary which lives in one dimension lower (if it exists). Here, were are using the notation $C^n$ to indicate the collection of all n-simplices in our complex, sums of which are often referred to as n-chains or simply chains. In general, we can (abstractly) represent the entire data of the simplicial complex by:\n$O_{n+2}, C^{n+1} \\stackrel{\\partial_{n+1}}{\\rightarrow} C^{n} \\stackrel{\\partial_{n}}{\\rightarrow} C^{n-1} \\stackrel{\\partial_{n-1}}{\\rightarrow} ...$ (13)\nReferring back to the boundary maps $\\partial_n: C^n \\rightarrow C^{n-1}$, the image of $\\partial_1(\\Delta_0^1)$ will be the (simplicial) boundary of $\\Delta$. We assign an orientation to the simplices to make boundary calculations consistent. For example, with the 1-simple as the convex hull of {$v_0, v_1$} the image of the boundary map would be defined as $v_1 - v_0$, with such an orientation induced by imaging the 1-simple is a directed edge from the point $v_0$ to the point $v_1$. More explicitly:\n$\\partial_1(\\Delta_{v_0}^{v_1}) = v_1 - v_0$, (14)\nwhere we have intentionally denoted $\\Delta_{v_0}^{v_1}$ as the simplex constructed from the vertices {$v_0, v_1$}. Looking at Figure 3, and considering the triangle at the left, the collection of 1-chains contains three edges, which are the convex hulls of the sets, {$v_0, v_1$}, {$v_1, v_2$} and {$v_0, v_2$}. The arrows give the orientation of each edge, which guides the boundary map calculation. We already calculated the boundary map for the first edge. The remaining two edges, $\\Delta_1$ and $\\Delta_2$ map can be calculated as:\n$\\partial_1(\\Delta_1) = v_2 - v_1$\n$\\partial_1(\\Delta_2) = v_0 - v_2$ (15)\nBy linearity, we can compute the boundary for the full chain, $\\Delta_0^1 + \\Delta_1 + \\Delta_2$ as:\n$\\partial_1(\\Delta_0^1 + \\Delta_1 + \\Delta_2) = \\partial_1(\\Delta_0^1) + \\partial_1(\\Delta_1) + \\partial_1(\\Delta_2)$ \n$= (v_1 - v_0) + (v_2 - v_1) + (v_0 - v_2)$ \n$= 0.$ (16)\nThus, the chain $\\Delta_0^1 + \\Delta_1 + \\Delta_2$ has no formal boundary (to be more clear this chain is what most people identify as the triangle itself). More importantly. This example demonstrates that a cycle (a chain without a boundary) has a trivial image. This will correlate with some topological holes in the simplicial complex. Thus, cycles correspond to elements of the kernel of the boundary map (a kernel"}, {"title": "HOMOLOGY ENABLED ROUTING MECHANISM", "content": "Nesa's inference framework is based on a Distributed Hash Table (DHT) for routing data and tasks on the distributed blockchain network [47]. This is a critical factor in task allocation, and fault tolerance as well as ensuring node information can be accessed globally. The DHT is immediately updated when a node becomes unreliable to change and update the heuristic parameters that allow the network the ability to redeploy without intervention. In the BSNS framework, this becomes even more important as tasks are distributed across a swarm of nodes.\nThe effective routing of data and tasks is crucial in a distributed blockchain network designed for neural network computations. The routing decision specifies a subset of nodes that are chosen to"}, {"title": "EVOLUTIONARY ALGORITHM FOR PARAMETER TUNING", "content": "The parameters \u03b3, \u03b2, and a are critical in balancing the cost components in the routing mechanism. We use a genetic algorithm [48] to optimize these parameters which allows routing to adapt to varying network conditions and workloads.\n1.  Initialization. The algorithm starts with a population of candidate parameter sets, each represented by a tuple (\u03b3, \u03b2, \u03b1). These candidates are initialized randomly in a predefined range for a diverse starting point for the optimization process.\n2.  Fitness Evaluation. The fitness of each candidate is evaluated based on the total cost $F(\\gamma, \\beta, \\alpha)$. The fitness function is inversely related to the cost where lower costs indicate higher fitness. This step involves calculating the objective function for each candidate, using real-time data from the network.\n$F(\\gamma, \\beta, \\alpha) = \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\{\\gamma [(\\frac{D_{j,A}}{B_{j,A}})^{\\gamma} + (\\frac{L_{j,A}}{B_{j,A}})^{\\gamma}] + \\beta (\\frac{G_A}{C_A})^\\beta + \\alpha (\\frac{1}{R_{A+1}})^\\alpha\\}$ (19)\nWhere:"}, {"title": "COMPRESSION TECHNIQUES", "content": "Fast inference of large models like Mixtral [1] and Llama [50] variants present scalability challenges due to their high memory demands. Consumer-grade GPUs often lack sufficient memory to store models with over 100 billion parameters, such as Llama-3.1. This accentuates the need for distributed inference and efficient management to minimize communication overhead. We implemented two main compression strategies to optimize hardware use\n1.  Dynamic Quantization. This technique compresses the hidden states exchanged between nodes during the model inference process. By applying dynamic blockwise quantization, we can significantly reduce the data that needs to be transmitted and halve the bandwidth requirements. This reduction is important for maintaining efficient operation, especially in environments with constrained bandwidth or increased latency due to the geographical distribution of nodes. This results in an efficient data transfer process, which reduces overall system latency and enhances throughput.\n2.  Mixed matrix decomposition. We use a mixed matrix decomposition method to quantize the model weights from 16-bit to 8-bit precision which further reduces the memory footprint. This method involves decomposing the weight matrices into 8-bit values while retaining a small fraction of critical weights at 16-bit precision. For example, for Mixtral 8x22B, it's compute-intensive to store all layers at full precision and can be significantly downsized in terms of GPU requirements by a factor of 30% with this quantization."}, {"title": "BENCHMARKS", "content": "Currently, our framework enables running a large number of machine learning models, which comprehensively span a variety of learning tasks. The models we host on our platform range from standard deep learning to large language-based models and vision transformers. Overall, tasks can be performed with multiple models, depending on the user requirements. We currently support:\n1.  Language-related tasks including token classification, question answering, translation, text classification, and summarization.\n2.  Machine vision tasks like instance segmentation, object detection, panoptic segmentation, and image classification.\n3.  General purpose machine learning tasks including feature extraction, causal generation.\nNesa's distributed inference system can also handle state-of-the-art LLMs and their variants, such as Mixtral, Llama Bloom etc. even on consumer-grade GPUs. This flexibility ensures broader accessibility and scalability, enabling deployments across diverse environments without the need for high-end, specialized hardware."}, {"title": "IMPACT OF COMPRESSION ON PERFORMANCE", "content": "Our results, detailed in Table 1, demonstrate that reducing precision from 16-bit to 8-bit has a minimal impact on task performance. For instance, in the HellaSwag task, Mixtral 56B performed equally well under both precisions which indicates that reasoning and understanding quality was kept intact at a lesser memory cost. Tasks such as Lambada and Causal Judgement are relatively unaffected as well, demonstrating that the compression operations do little to compromise the model's abilities.\nTable 2 shows the impact of network conditions and batch sizes on two LLMs including Mixtral and LLama. Round-trip time and bandwidth directly impact the tokens per second speed, which is expected over a distributed swarm. Both LLMs have 32 transformer blocks, served by 6 nodes chosen based on the previously discussed routing mechanism. Three out of six machines are using Nvidia L24 GPUs, while the other three are using a consumer-grade GPU i.e. Nvidia GTX 1650. Nesa supports inference on both CPUs and GPUs where slightly more latency is expected on the CPU. We can observe that token generation speed increases as batch size increases, demonstrating that parallel computation enhances network throughput; this behavior is crucial for large-scale, fast inference.\nIn addition to evaluating LLMs, we also focus on text-to-image architectures including diffusion models, Anime generators and debiasing generation models [32, 55]. These models are assessed across various metrics to evaluate their effectiveness and reliability. We consider the following metrics [56]:\n1.  Fairness evaluates if the image models are biased towards a gender or a specific ethnicity.\n2.  Quality measures the ability of models to generate aesthetic images. This metric focuses on different aspects including clarity and detail."}, {"title": "SECURITY AND PRIVACY IN NESA'S SYSTEM", "content": "Our BSNS distributed inference protocol offers major advantages for distributed inference, but it also requires cutting-edge approaches to security and privacy (S&P) enhancement [57]. Specifically, hardware-based and software/algorithm-based solutions need to be integrated to achieve co-optimization, each selected and optimized for varying scenarios within our ecosystem.\nPotential concerns linked to distributed BSNS inference may appear in different forms. For instance:\n\u2022 Users may wish to protect their input data and the inference results.\n\u2022 Node owners might seek to protect the confidentiality of their model parameters in certain cases.\n\u2022 Meanwhile, the users want to ensure that the models executed by the nodes are verifiable namely, the designated ML models need to generate the inference results without unexpected changes.\nWe list below the key steps we carry out towards ensuring the security of the BSNS protocol. The reader is referred to the companion Security paper for more details on Nesa's implementation, which ensures that both user input for AI inference and private model parameters are protected during the execution of distributed models."}, {"title": "S&P REQUIREMENTS", "content": "In summary, there are two core S&P aspects we identify in decentralized, private inference: (i) model verification to prove the nodes execute the designated models, e.g., LLaMA 3 [15], for a user, where adversarial participants may return random results or even malicious results and (ii) data encryption [58] to protect the user's data from being revealed during the inference. Based on these requirements, we develop a suite of solutions to ensure S&P in Nesa's system."}, {"title": "OVERVIEW OF OUR HARDWARE-SOFTWARE CO-OPTIMIZATION SOLUTION", "content": "To address both model verification and data encryption jointly, we design an integrated approach to achieve leading S&P performance in our system. Specifically, we design a combined strategy of the robust, hardware-centric protections of Trusted Execution Environments (TEEs) [59] and the advanced algorithmic approaches, including Zero-Knowledge Machine Learning (ZKML) [60] and Consensus-based Distribution Verification (CDV) [61] for verification and Split-Learning (SL) [62] for encryption, to provide the highest level of S&P in our system.\nIn short, TEEs provide a secure area within a processor that ensures the confidentiality and integrity of the code and data loaded within it, thus supplying robustness from the hardware level. On the software/algorithm side, ZKML can provide the means to confirm the authenticity and integrity of the models run by nodes without revealing any other information. Due to its computational cost, we currently leverage it for private, small models requiring the highest security level. As an efficient alternative for public, large models, CDV is a novel algorithm we propose that ensures that the inference nodes execute the correct model by measuring their output distribution consensus, while SL protects user data by only transferring the intermediate computational embeddings other than the raw data. Collectively, this hardware-software integrated solution guarantees high S&P in Nesa's system."}, {"title": "BSNS-COMPATIBLE DATA ENCRYPTION", "content": "In decentralized inference systems like Nesa's BSNS, it is crucial to protect user data. BSNS distributes computational tasks across various nodes, each potentially operated by different entities, which in principle increases the risk of exposing sensitive user data during the inference process intensifies. To protect against this, we have designed an innovative encryption method called Sequential Vector Encryption (SVE), developed as a more efficient version of homomorphic encryption, and applied on select linear layers rather than the full model based on a sequence of vector space operators. SVE randomly transforms the outputs of every operator so that the intermediate vector representations can no longer be interpretable using a given method developed for the original model's vector representations. These representations are then transformed to the original representation space before feeding to the next operator to render the original model still usable."}, {"title": "SPLIT LEARNING", "content": "Recognizing the challenges posed by encrypting data for use in decentralized inference systems, Nesa adopts Split Learning (SL) as a pragmatic solution to facilitate secure and efficient computation on encrypted data [63, 64]. Traditional encryption methods, while securing data at rest and in transit, render it unusable for direct computation by obscuring its format and structure. This limitation is particularly problematic for processing with LLMs within a decentralized framework, where data privacy cannot be compromised.\nSplit Learning [62] addresses these concerns by partitioning the computational model, allowing for data to be processed in parts without revealing sensitive information. In essence, the user data is protected by not being directly transmitted to any nodes \u2013 only the data embeddings are being passed around, and each node will only be accessing the embeddings of certain layers.\nConsider a neural network model N, such as Llama 2 [15] composed of a sequence of 32 layers {$L_1, L_2,..., L_{32}$}, each with its own set of parameters $\\Theta_i$ and activation function $\\sigma_i$. The input to the network is X, and the output of the i-th layer, given input $x_i$, can be mathematically described as:\n$a_i = L_i(x_i; \\Theta_i) = \\sigma_i(W_i x_i + b_i)$ (21)"}, {"title": "CONCLUSION", "content": "Our paper introduces a model-agnostic hybrid sharding method, complemented by a comprehensive security and privacy framework designed for distributed AI inference. This approach strategically distributes computational tasks across a decentralized network, enabling scalable AI execution through the sequential consumption of shards on low-powered nodes. First, our BSNS optimizes network topology. Equipped with dynamic network rebalancing and KV caching mechanisms, the BSNS design is adept at facilitating scalable operations for large models and provides standardization across all variants where the base architecture is the same.\nNext, we proposed an integrated security and privacy framework featuring co-optimized hardware-based TEEs with CDV and SL. Long-term techniques for the protocol are also explored which could potentially have significant impacts in this field. Collectively, our system removes the prohibitive costs associated with needing to own GPUs, making AI inference execution accessible to the general population. Democratizing AI requires technology like BSNS in order to provide the opportunity for participation to anyone with a computer at home, while upholding an essential commitment to safety and privacy throughout the inference execution process."}]}