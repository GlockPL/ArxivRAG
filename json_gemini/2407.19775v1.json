{"title": "MODEL AGNOSTIC HYBRID SHARDING FOR HETEROGENEOUS DISTRIBUTED INFERENCE", "authors": ["Claudio Angione", "Yue Zhao", "Harry Yang", "Ahmad Farhan", "Fielding Johnston", "James Buban", "Patrick Colangelo"], "abstract": "The rapid growth of large-scale AI models, particularly large language models has brought significant challenges in data privacy, computational resources, and accessibility. Traditional centralized architectures often struggle to meet required data security and scalability needs which hinders the democratization of AI systems. Nesa introduces a model-agnostic sharding framework designed for decentralized AI inference. Our framework uses blockchain-based sequential deep neural network sharding to distribute computational tasks across a diverse network of nodes based on a personalised heuristic and routing mechanism. This enables efficient distributed training and inference for recent large-scale models even on consumer-grade hardware. We use compression techniques like dynamic blockwise quantization and mixed matrix decomposition to reduce data transfer and memory needs. We also integrate robust security measures, including hardware-based trusted execution environments to ensure data integrity and confidentiality. Evaluating our system across various natural language processing and vision tasks shows that these compression strategies do not compromise model accuracy. Our results highlight the potential to democratize access to cutting-edge AI technologies by enabling secure and efficient inference on a decentralized network.", "sections": [{"title": "INTRODUCTION", "content": "Centralized AI inference poses significant risks related to data privacy, computational bottlenecks, deterministic output, and single points of failure. The prohibitive cost and scarcity of high performance computing resources prevent the mass adoption of the only counter option to centralization, which is open-source decentralized AI. These challenges further limit the ability to contribute to training, fine-tuning, and AI inference at scale. This severely prohibits the adoption of state-of-the-art AI models for enterprises and developers, as well as shared research around the world.\nRecent large language models are available with more than 100 billion parameters [1, 2, 3, 4], which makes it important to train them on powerful and costly accelerated hardware such as GPUs and TPUs. [5]. Several approaches can be used to make these models more accessible, especially during the fine-training and inference phase. Using the APIs is one possible approach, which allows quicker inference passes from pre-trained models, but offers little customisation capacity, and no options to change or optimise the training process [2, 6]. A second solution is offloading, where the model components are moved to slower memory (e.g. RAM or SSD) [7]. Then, only the relevant portion of the model is iteratively moved to the available GPU, allowing it to run on less expensive hardware. However, this process involves frequent data transfers and can be extremely slow for larger models.\nCollaborative efforts to AI execution today are further hindered by the challenges around security [8]. Techniques like continuous and domain adaptation offer partial solutions by enabling model sharing without direct data exchange [9]. However, these methods are prone to backdoor attacks and often result in sub-optimal model performance due to the limitations of semi-supervised or unsupervised fine-tuning. Collectively, these issues have real-world implications for businesses requiring critical"}, {"title": "DISTRIBUTED TRAINING AND INFERENCE", "content": "Distributed inference and training across multiple nodes are essential due to the exponential increase in the size of models, their complexity and the scarcity of computational resources available to process them [17]. The essence of both of these tasks lies in the necessity to handle vast computational loads more efficiently and to reduce the latency involved in generating predictions and updating model parameters [18]. Our approach uses the collective computational resources and memory available across several processing nodes. This makes it possible to handle larger models or increased batch sizes without a proportional increase in inference or training time. One of the critical aspects of efficient distributed inference and training is partitioning the computational graph of any neural network. This allows the nodes with limited resources to only handle a segment of the model during the inference or training phase. The computational graph represents all the operations and data flows within the model from input to output. By Partitioning this graph we divide the model's"}, {"title": "NESA'S MODEL PARTITIONING APPROACH", "content": "Due to the novelty of running deep learning algorithms on the blockchain, Nesa's partitioning mechanism aims to minimize the amount of data that must be transferred between different nodes, thus reducing the latency and bandwidth requirements for both inference and training phases. The overall throughput of the inference and training tasks is limited by the slowest part of the system, known as the bottleneck stage [22]. We therefore introduce mechanisms to balance the workload and avoid bottlenecks. The partitioning must balance the computational load across all nodes to maximize throughput. This balance ensures that all parts of the system work at their full potential without any single stage becoming a drag on performance.\nWe prioritize fast memory like Static Random-Access Memory (SRAM) for distributed inference and training across multiple nodes which is essential for storing intermediate model outputs, i.e., activations, and parameter weights [23, 24]. SRAM is significantly faster than conventional memory solutions but is also more expensive and limited in size. Each node in the network contributes its SRAM, which multiplies the available fast memory during the distributed inference and training session. This increase in capacity allows for caching more model parameters in fast memory."}, {"title": "BLOCKCHAIN-BASED SEQUENTIAL DEEP NEURAL NETWORK SHARDING", "content": "Nesa developed a new approach for network topology-informed model sharding, named Blockchain-based Sequential deep neural network Sharding (BSNS). Our approach establishes and solves an optimization problem that can find a sequence of nodes able to run an inference request. Each node will be typically a distributed machine on the network, which will normally run a block or a contiguous sequence of blocks of a deep learning architecture. It is crucial that for each inference session involving block sharding, a chain of nodes is found that can collectively reconstruct the layers of the full model [25].\nThe key innovation of our approach is that the selection of nodes for sharding is informed by: (i) the topology of the network; (ii) our persistent homology metrics based on graph embeddings and (iii) network-based variables, including latency and geographical distance between the nodes. Taken together, this will constitute a full end-to-end neural network that performs inference across the blockchain at the optimal speed, fully embedding the topology and the security components of the blockchain."}, {"title": "SWARM CREATION AND DYNAMIC REBALANCING", "content": "The BSNS framework allows arbitrary deep neural networks to be distributed for training, but with specific focus given to LLMs and transformer-based architectures. The swarm is formed based on a heuristic that selects the optimal set from the pool of available nodes. Each node inside the swarm processes a shard of the network architecture and communicates through a sequence of remote procedure calls (RPC). Each N\u2081 denotes a node within the network which is responsible for the i-th shard of the model. Whereas, p represents the total number of shards distributed across the swarm. The selection process heuristic considers different network topology features, such as node and edge structure, bandwidth, and computational capabilities, therefore ensuring optimal efficiency in data processing across the swarm. This has strong practical benefits for our clients, as distance and latency heavily affect the performance when computing using a blockchain.\nGiven a network A with n nodes that need to execute p < n shards of a model, and assuming that each block is held by one network node, the task involves finding a sequence S of nodes\nS: (Ai)i\u22081,...,p. (1)"}, {"title": "CACHE OPTIMIZATION TO ENHANCE EFFICIENCY", "content": "BSNS uses a Key Value (KV) cache for scaling LLMs across a distributed network. This minimizes the computational overhead associated with token generation in LLMs when dealing with models that operate on a token-by-token basis [26, 27]. This mechanism uses the inherent characteristics of transformer models by caching the key and value vectors after their initial computation to prevent redundant calculations in subsequent generations. This also decreases the load on the network's nodes and allows the processing of longer sequence lengths in LLMs [28].\nCaching improves efficiency by ensuring that each node within a swarm can generate tokens by utilizing pre-computed key and value vectors. This is a core factor that increases the overall throughput of the system. Moreover, it enables the system to scale to models with extensive context sizes by removing the memory and computational constraints that typically hinder the operation of large LLMs."}, {"title": "BSNS WITH PARAMETER-EFFICIENT FINE-TUNING VIA ADAPTERS", "content": "BSNS uses Low-rank adaption techniques to enable fine-tuning capabilities for arbitrary language models [14]. The adapters are inserted between the transformer layers which allows learning the downstream tasks without retraining the entire network as it can be computationally expensive[14, 30]."}, {"title": "OPERATIONAL FRAMEWORK FOR ADAPTERS IN NESA", "content": "Adapters are small neural network modules inserted between the layers of a pre-trained model. They allow task-specific training with minimal parameter updates [14]. The operation of an adapter within a transformer layer is given as:\nh = LayerNorm(hi + W2\u03c3(W\u2081hi)), (5)\nwhere hi is the input to the adapter at layer i, W\u2081 and W2 are trainable weights of the adapter, \u03c3 represents a non-linear activation function, and he is the output of the adapter [31].\nThe adaptation of a weight matrix W in a transformer can be modeled as\nW' = W + B\u0410, (6)\nwhere W is the original weight matrix of the model, A \u2208 Rr\u00d7n and B \u2208 Rnxr are the low-rank matrices introduced by LoRA with r < n, and W' is the adapted weight matrix [14]. Upon initiating a fine-tuning session, each participating node initializes adapter modules according to the specified configuration aligned with the shard of the LLM it is responsible for."}, {"title": "NODE SYNCHRONIZATION", "content": "Nodes collaboratively fine-tune their adapter modules using gradients from task-specific data processed through their portion of the LLM. The process includes three main steps:\n1. Forward pass. Nodes perform a forward pass through transformer layers and adapters which generates activations based on the input tensors.\n2. Backward pass. From the final layer of a large language model, gradients are propagated by comparing predictions with true labels. Gradients for corresponding adapters are used to perform the gradient descent update.\n3. Synchronization and update. Nodes synchronize their updates across the network to ensure consistency and convergence\nThe parameter updates for the adapters, denoted as \u03b8\u2081 for the i-th node, follow the rule:\n0t+1) = 0) \u2013 \u03b7\u2202L(0)\u22020 (7)\nwhere t is the current iteration, \u03b7 is the learning rate, and L is the loss function.\nAlong with these synchronized update rules, BSNS uses a consensus mechanism for agreeing on fine-tuning objectives, data distribution, and synchronization intervals. This setup allows for the sharing and reuse of fine-tuned adapters and allows nodes to build on existing adaptations for new tasks."}, {"title": "DYNAMIC SHARDING OF NEURAL NETWORKS", "content": "The BSNS method works well for LLMs as they have common blocks that can be distributed on a sequence of nodes. However, for arbitrary layered neural networks like diffusion or sequence"}, {"title": "ENHANCED MTPP SLICING OF TOPOLOGICAL ORDER", "content": "The MTPP algorithm is key in the optimization of computation graphs for neural network inference across multiple systems. It is known to be NP-hard, as shown by its relation to the minimum makespan scheduling problem [35]. This makes fully polynomial approximations unlikely. Instead, we use a heuristic approach to handle this complexity and maintain high throughput. For non-transformers based architectures, our method combines Kahn's algorithm [36] for topological sorting with dynamic programming and segment trees. Kahn's algorithm ensures correct topological order for acyclic partitioning. Dynamic programming then calculates the best partitioning and Segment trees quickly adjust partitions during optimization. This combination helps in distributing the computational load evenly and reduce communication delays [37, 38].\nOur approach centres around the segment cost data structure which calculates costs based on computation and communication for each segment. We start with a computation graph G and a topological order \u03c0. The SliceGraph algorithm uses this to divide the graph into throughput-optimized segments. It dynamically splits the graph into blocks which distributes workload evenly and reduces communication between nodes."}, {"title": "BIASED RANDOM-KEY GENETIC ALGORITHM", "content": "BRKGA improves our approach by using stochastic and evolutionary methods which overcomes the limitations of heuristic-based techniques [40, 41]. This algorithm enables broad exploration of partition configurations, essential for solving the NP-hard MTPP. Over generations, BRKGA evolves partitioning strategies to discover solutions that balance computational load and reduce communication overhead and achieves optimal throughput in distributed deep neural network inference [20].\nBRKGA evolves node priority vectors x \u2208 [0, 1]n and links these priorities directly to partitioning quality for adjustments. The process starts by creating a population of random vectors. These vectors help setup orders in the SliceGraph algorithm. Each vector's quality is assessed for throughput optimization and latency reduction which determines its fitness. BRKGA refines the strategies until convergence criteria for node priorities is met. This process involves selection, crossover new = \u03b1parent\u2081 + (1 \u2212 \u03b1)parent2, and mutation to generate near optimal offsprings.\nThe BRKGA approach offers many advantages over sequential sharding methods for partitioning neural networks. Sequential methods follow a predetermined path which perhaps can miss more efficient configurations that could improve computational throughput and reduce communication latency. This adaptability is important in distributed environments with varying network topologies and node capabilities."}, {"title": "SWARM TOPOLOGY", "content": "Due to the dynamic topology of the swarm network, we further extend our computational efficiency by monitoring and classifying changes and the current state of our swarm topology over time. In conjunction, we build several pre-computed sharding schemas based on the commonly observed swarm topologies as we monitor our networks. In doing so, we can quickly select among sharding schemes to find one that is near-optimal for the current swarm topology. In the following section, we outline how we monitor and analyze the swarm topology."}, {"title": "TOPOLOGICAL FORECASTING AND MONITORING", "content": "Dynamic sharding is computationally expensive as finding an optimal sharding scheme is known to be NP-hard [20]. We mitigate this expense by precomputing highly efficient sharding schemas for different topological states of our network on a per-model basis. As the network topology itself is dynamic, we periodically monitor our network using techniques from (weighted) persistent homology [42, 43], to give snapshots of topological characteristics to guide the applications of our pre-computed sharding schemas. The main goal of this section will be to introduce many of the concepts behind persistent homology (which is not a mainstream topic) to guide intuition on how this computational tool can be leveraged to make dynamic sharding optimization much more tractable.\nAs a basic introduction to the implementation of this framework, we briefly illustrate the mathematical technique of simplicial homology. This uses the simplicial complex to calculate the homology groups these groups are formally Abelian groups but they can be thought of as vector spaces. A simplicial complex is a collection of simplices that are the convex hull of a group of vertices. More explicitly, an n-simplex \u2206\u03b7 contains n + 1 vertices {vo, v1, . . ., Un } and is the collection of points in Rn such that:\n\u0394\u03b7 = {kovo +... + knun | \u2211ki = 1 and ki\u2265 0 for 0 \u2264 i \u2264 n} (12)\ni=0\nThus, a 0-simplex \u2206\u00ba is a point, a 1-simplex \u2206\u00b9 is a line segment, a 2-simplex \u22062 is a triangle, and so on. To build a simplicial complex out of a collection of simplices, we need to define how to glue the pieces together [44]. Mathematically it is given as (linear) boundary maps : C'n \u2192 C'n\u22121 which map each n-simplex \u2206 to its boundary which lives in one dimension lower (if it exists). Here, were are using the notation Cn to indicate the collection of all n-simplices in our complex, sums of which are often referred to as n-chains or simply chains. In general, we can (abstractly) represent the entire data of the simplicial complex by:\nOn+2 Cn+1 dn+1, Cndn, Cn-1dn-1, ... (13)\nReferring back to the boundary maps din : C'n \u2192 C'n-1, the image of d (Ar) will be the (simplicial) boundary of A. We assign an orientation to the simplices to make boundary calculations consistent. For example, with the 1-simple as the convex hull of {vo, v\u2081 } the image of the boundary map would be defined as v1 vo, with such an orientation induced by imaging the 1-simple is a directed edge from the point vo to the point v\u2081. More explicitly:\n\u22021(\u03940) = v1 \u2212 v0, (14)\nwhere we have intentionally denoted \u2206 as the simplex constructed from the vertices {vo, 21}. Looking at Figure 3, and considering the triangle at the left, the collection of 1-chains contains three edges, which are the convex hulls of the sets, {vo, U1}, {V1, V2} and {vo, v2}. The arrows give the orientation of each edge, which guides the boundary map calculation. We already calculated the boundary map for the first edge. The remaining two edges, \u25b3\u2081 and A map can be calculated as:\n\u22021(\u03941) = v2 \u2212 v1 \u22021(\u03942) = v0 \u2212 v2 (15)\nBy linearity, we can compute the boundary for the full chain, \u2206\u266d + \u25b3\u2081 + \u2206 as:\n\u22021(\u0394 + \u03941 + \u0394) = \u22021(\u0394) + \u22021(\u03941) + \u22021(\u03942) = (v1 - vo) + (v2 - v1) + (vo - v2) = 0. (16)\nThus, the chain \u25b3 + \u25b3\u2081 + \u2206 has no formal boundary (to be more clear this chain is what most people identify as the triangle itself). More importantly. This example demonstrates that a cycle (a chain without a boundary) has a trivial image. This will correlate with some topological holes in the simplicial complex. Thus, cycles correspond to elements of the kernel of the boundary map (a kernel"}, {"title": "HOMOLOGY ENABLED ROUTING MECHANISM", "content": "Nesa's inference framework is based on a Distributed Hash Table (DHT) for routing data and tasks on the distributed blockchain network [47]. This is a critical factor in task allocation, and fault tolerance as well as ensuring node information can be accessed globally. The DHT is immediately updated when a node becomes unreliable to change and update the heuristic parameters that allow the network the ability to redeploy without intervention. In the BSNS framework, this becomes even more important as tasks are distributed across a swarm of nodes.\nThe effective routing of data and tasks is crucial in a distributed blockchain network designed for neural network computations. The routing decision specifies a subset of nodes that are chosen to"}, {"title": null, "content": "Ai = argmin AEN {[(Dj,A Lj,A) \u03b3] + [\u03b2(GA CA) \u03b3] + [\u03b1(RA (RA + 1) \u03b3]} (18)\nWe use multiple key metrics which consider the data processing factors, computational load ratio and reliability of the nodes for forming the swarm and its dynamic rebalancing. The key data processing factors are as follows:\n1. Data Size (Dj,A). It measures the size of the payload that needs to be transferred from node j to node A. This factor affects multiple components including model parameters, activations of intermediate layers and final outputs.\n2. Bandwidth (Bj,A). This metric measures the maximum data transfer capacity between two nodes per unit of time. Better bandwidth indicates a higher capacity for handling large data volumes and reduces the likelihood of bottlenecks.\n3. Latency (Lj,A). Latency is the delay experienced in communication between two nodes. It can be influenced by physical distances and network congestion. Lower latency is important in enabling faster fine-tuning and inference.\nThese factors measure the combined cost of data transfer and latency. The hyperparameter y is used for non-linear scaling of this cost and weights the impact of these factors on overall efficiency. The parameter y is tuned using genetic algorithms and represents the relative importance of minimizing data transfer time and latency in different scenarios."}, {"title": null, "content": "Along with these data processing factors we use computational load ratio. This term expresses the balance between available hardware resources and the current computational load. A higher ratio indicates that a node has more resources available relative to its load which makes it a better candidate for selection in a swarm.\n1. GPU Availability (GA). This metric represents the available GPU resources at a node. Nodes report their total GPU capacity and current usage calculated as follows:\nGA = total GPU capacity \u2212 current GPU usage\nThis helps identify nodes with sufficient GPU resources to handle inference and fine-tuning tasks without significant delays.\n2. Computational Load (CA). This measures the current computational burden on a node, including CPU and memory usage. It reflects the node's active tasks and overall workload. Monitoring CA is important to prevent overburden and inference failures.\nFor computational load ratio, the parameter \u03b2 adjusts the sensitivity of this ratio, prioritizing nodes with sufficient GPU resources for compute-intensive models. We also consider a resource reputation metric (RA) that assesses the reliability of a node. It incorporates factors such as historical uptime, failure rates, and maintenance history. A node with high uptime and low failure rates is considered more reliable. The calculation R\u2081 = 1 \u2013 Uptime provides a simple measure of reliability, where uptime is reported as a percentage. The parameter \u03b1 controls the weight of reliability in the overall cost calculation, with higher values increasing the importance of selecting reliable nodes."}, {"title": "PERSISTENT HOMOLOGY INTEGRATION", "content": "Persistent homology in our routing mechanism provides insights into the swarm's structure including the stability and connectivity of nodes. It analyzes the persistence of topological features like connected components and cycles, which helps to identify stable regions in the network important for the node selection process. Routing uses persistent homology for network analysis, filtration and node selection. A filtration is constructed by adding simplices in a specific order. This process shows how different features of the network emerge or disappear as the scale changes. For example, at a small scale, only the strongest connections (shortest edges) are included as they represent the core structure of the network. As the scale increases, weaker connections are added.\nNodes that are part of long-lived features are considered more stable and reliable. For example, a node consistently appearing in a connected component with a high 30 value across multiple scales will likely reside in a stable network region. This stability influences the adjustment of RA values in the routing mechanism and favors nodes in these regions. If persistent homology shows that a cluster of nodes consistently forms a connected component, these nodes are deemed more reliable, impacting their RA values."}, {"title": "EVOLUTIONARY ALGORITHM FOR PARAMETER TUNING", "content": "The parameters \u03b3, \u03b2, and \u03b1 are critical in balancing the cost components in the routing mechanism. We use a genetic algorithm [48] to optimize these parameters which allows routing to adapt to varying network conditions and workloads.\n1. Initialization. The algorithm starts with a population of candidate parameter sets, each represented by a tuple (\u03b3, \u03b2, \u03b1). These candidates are initialized randomly in a predefined range for a diverse starting point for the optimization process.\n2. Fitness Evaluation. The fitness of each candidate is evaluated based on the total cost F(\u03b3, \u03b2, \u03b1). The fitness function is inversely related to the cost where lower costs indicate higher fitness. This step involves calculating the objective function for each candidate, using real-time data from the network.\nF(\u03b3, \u03b2, \u03b1) = \u2211j=1nDj,A Lj,A)\u03b3 \u03b2(GA CA)\u03b3 + \u03b1(RA RA + 1)\u03b3 (19)\nWhere:"}, {"title": null, "content": "\u2022 F(\u03b3, \u03b2, \u03b1) is the total cost function.\n\u2022 \u03b3, \u03b2, \u03b1 are the parameters being optimized.\n\u2022 Dj,A, Bj, A, Lj, A are data size, bandwidth, and latency metrics, respectively.\n\u2022 GA, CA, RA are GPU availability, computational load, and resource reliability metrics, respectively.\n3. Selection. Candidates with higher fitness are more likely to be selected for reproduction. We use roulette wheel selection [49] to allocate selection probability proportional to fitness.\n4. Crossover and Mutation. Selected candidates undergo crossover, combining their parameter values to create offspring. This process introduces new combinations of \u03b3, \u03b2, and \u03b1 and allows the algorithm to explore the parameter space. Additionally, mutation introduces random changes to some offspring, preventing premature convergence.\n\u03b8child = \u03b7\u03b8parent1 + (1 \u2212 \u03b7)\u03b8parent2 (20)\nHere, \u03b8 represents the parameters \u03b3, \u03b2, and \u03b1. Where \u03b7 is a randomly selected value between 0 and 1. This convergence returns the optimal values of these hyperparameters that are used for decision-making related to routing, swarm creation and rebalancing."}, {"title": "COMPRESSION TECHNIQUES", "content": "Fast inference of large models like Mixtral [1] and Llama [50] variants present scalability challenges due to their high memory demands. Consumer-grade GPUs often lack sufficient memory to store models with over 100 billion parameters, such as Llama-3.1. This accentuates the need for distributed inference and efficient management to minimize communication overhead. We implemented two main compression strategies to optimize hardware use\n1. Dynamic Quantization. This technique compresses the hidden states exchanged between nodes during the model inference process. By applying dynamic blockwise quantization, we can significantly reduce the data that needs to be transmitted and halve the bandwidth requirements. This reduction is important for maintaining efficient operation, especially in environments with constrained bandwidth or increased latency due to the geographical distribution of nodes. This results in an efficient data transfer process, which reduces overall system latency and enhances throughput.\n2. Mixed matrix decomposition. We use a mixed matrix decomposition method to quantize the model weights from 16-bit to 8-bit precision which further reduces the memory footprint. This method involves decomposing the weight matrices into 8-bit values while retaining a small fraction of critical weights at 16-bit precision. For example, for Mixtral 8x22B, it's compute-intensive to store all layers at full precision and can be significantly downsized in terms of GPU requirements by a factor of 30% with this quantization."}, {"title": "BENCHMARKS", "content": "Currently, our framework enables running a large number of machine learning models, which comprehensively span a variety of learning tasks. The models we host on our platform range from standard deep learning to large language-based models and vision transformers. Overall, tasks can be performed with multiple models, depending on the user requirements. We currently support:\n1. Language-related tasks including token classification, question answering, translation, text classification, and summarization.\n2. Machine vision tasks like instance segmentation, object detection, panoptic segmentation, and image classification.\n3. General purpose machine learning tasks including feature extraction, causal generation.\nNesa's distributed inference system can also handle state-of-the-art LLMs and their variants, such as Mixtral, Llama Bloom etc. even on consumer-grade GPUs. This flexibility ensures broader accessibility and scalability, enabling deployments across diverse environments without the need for high-end, specialized hardware."}, {"title": "IMPACT OF COMPRESSION ON PERFORMANCE", "content": "Our results, detailed in Table 1, demonstrate that reducing precision from 16-bit to 8-bit has a minimal impact on task performance. For instance, in the HellaSwag task, Mixtral 56B performed equally well under both precisions which indicates that reasoning and understanding quality was kept intact at a lesser memory cost. Tasks such as Lambada and Causal Judgement are relatively unaffected as well, demonstrating that the compression operations do little to compromise the model's abilities.\nTable 2 shows the impact of network conditions and batch sizes on two LLMs including Mixtral and LLama. Round-trip time and bandwidth directly impact the tokens per second speed, which is expected over a distributed swarm. Both LLMs have 32 transformer blocks, served by 6 nodes chosen based on the previously discussed routing mechanism. Three out of six machines are using Nvidia L24 GPUs, while the other three are using a consumer-grade GPU i.e. Nvidia GTX 1650. Nesa supports inference on both CPUs and GPUs where slightly more latency is expected on the CPU. We can observe that token generation speed increases as batch size increases, demonstrating that parallel computation enhances network throughput; this behavior is crucial for large-scale, fast inference.\nIn addition to evaluating LLMs, we also focus on text-to-image architectures including diffusion models, Anime generators and debiasing generation models [32, 55]. These models are assessed across various metrics to evaluate their effectiveness and reliability. We consider the following metrics [56]:\n1. Fairness evaluates if the image models are biased towards a gender or a specific ethnicity.\n2. Quality measures the ability of models to generate aesthetic images. This metric focuses on different aspects including clarity and detail."}, {"title": "SECURITY AND PRIVACY IN NESA'S SYSTEM", "content": "Our BSNS distributed inference protocol offers major advantages for distributed inference, but it also requires cutting-edge approaches to security and privacy (S&P) enhancement [57]. Specifically, hardware-based and software/algorithm-based solutions need to be integrated to achieve co-optimization, each selected and optimized for varying scenarios within our ecosystem.\nPotential concerns linked to distributed BSNS inference may appear in different forms. For instance:\n\u2022 Users may wish to protect their input data and the inference results.\n\u2022 Node owners might seek to protect the confidentiality of their model parameters in certain cases.\n\u2022 Meanwhile, the users want to ensure that the models executed by the nodes are verifiable \u2013 namely, the designated ML models need to generate the inference results without unexpected changes.\nWe list below the key steps we carry out towards ensuring the security of the BSNS protocol. The reader is referred to the companion Security paper for more details on Nesa's implementation, which ensures that both user input for AI inference and private model parameters are protected during the execution of distributed models."}, {"title": "S&P REQUIREMENTS", "content": "In summary, there are two core S&P aspects we identify in decentralized, private inference: (i) model verification to prove the nodes execute the designated models, e.g., LLaMA 3 [15], for a user, where adversarial participants may return random results or even malicious results and (ii) data encryption [58] to protect the user's data from being revealed during the inference. Based on these requirements, we develop a suite of solutions to ensure S&P in Nesa's system."}, {"title": "OVERVIEW OF OUR HARDWARE-SOFTWARE CO-OPTIMIZATION SOLUTION", "content": "To address both model verification and data encryption jointly, we design an integrated approach to achieve leading S&P performance in our system. Specifically, we design a combined strategy of the robust, hardware-centric protections of Trusted Execution Environments (TEEs) [59] and the advanced algorithmic approaches, including Zero-Knowledge Machine Learning (ZKML) [60] and Consensus-based Distribution Verification (CDV) [61] for verification and Split-Learning (SL) [62] for encryption, to provide the highest level of S&P in our system.\nIn short, TEEs provide a secure area within a processor that ensures the confidentiality and integrity of the code and data loaded within it, thus supplying robustness from the hardware level. On the software/algorithm side, ZKML can provide the means to confirm the authenticity and integrity of the models run by nodes without revealing any other information. Due to its computational cost, we currently leverage it for private, small models requiring the highest security level. As an efficient alternative for public, large models, CDV is a novel algorithm we propose that ensures that the inference nodes execute the correct model by measuring their output distribution consensus, while SL protects user data by only transferring the intermediate computational embeddings other than the raw data. Collectively, this hardware-software integrated solution guarantees high S&P in Nesa's system."}, {"title": "BSNS-COMPATIBLE DATA ENCRYPTION", "content": "In decentralized inference systems like Nesa's BSNS, it is crucial to protect user data. BSNS distributes computational tasks across various nodes, each potentially operated by different entities, which in principle increases the risk of exposing sensitive user data during the inference process intensifies. To protect against this, we have designed an innovative encryption method called Sequential Vector Encryption (SVE), developed as a more efficient version of homomorphic encryption, and applied on select linear layers rather than the full model based on a sequence of vector space operators. SVE randomly transforms the outputs of every operator so that the intermediate vector representations can no longer be interpretable using a given method developed for the original model's vector representations. These representations are then transformed to the original representation space before feeding to the next operator to render the original model still usable."}, {"title": "SPLIT LEARNING", "content": "Recognizing the challenges posed by encrypting data for use in decentralized inference systems, Nesa adopts Split Learning (SL) as a pragmatic solution to facilitate secure and efficient computation on encrypted data [63, 64]. Traditional encryption methods, while securing data at rest and in transit, render it unusable for direct computation by obscuring its format and structure. This limitation is particularly problematic for processing with LLMs within a decentralized framework, where data privacy cannot be compromised.\nSplit Learning [62] addresses these concerns by partitioning the computational model, allowing for data to be processed in parts without revealing sensitive information. In essence, the user data is protected by not being directly transmitted to any nodes \u2013 only the data embeddings are being passed around, and each node will only be accessing the embeddings of certain layers.\nConsider a neural network model N, such as Llama 2 [15] composed of a sequence of 32 layers {L1, L2,..., L32}, each with its own set of parameters O\u2081 and activation function \u03c3\u2081. The input to the network is X, and the output of the i-th layer, given input xi, can be mathematically described as:\nai = Li(xi; \u0472i) = \u03c3\u03b5(Wixi + bi) (21)"}, {"title": null, "content": "where Wi and bi are the weight matrix and bias vector of the i-th layer, respectively, and oi is a nonlinear activation function such as ReLU, sigmoid, or tanh.\nAssuming the model is split at layer k, where the client handles layers {L1, . . ., Lk} and the server handles layers {Lk+1,..., L32}. The client computes the intermediate representation Z as follows:\nZ = \u03c3\u03ba(Wk\u00b7\u03c3\u03ba\u22121(... \u03c3\u2081 (W\u2081 X + b\u2081) . . .) + bk) (22)\nThis intermediate representation Z is then transmitted to the server, which continues the computation:\nY = 032 (W32\u00b7 \u03c331(... \u03c3k+1(Wk+1Z + bk+1)...) + b32) (23)\nThe loss function L(Y, Ytrue) computes the error between the network output Y and the true labels Ytrue, and the gradient of the loss with respect to the model's parameters through backpropagation:\n\u2202L \u2202\u0398 = ChainRule\u2202Y \u2202a32 \u2202a \u2202\u0398 (24)\nFor privacy concerns during the transmission of Z from client to server, differential privacy methods may be applied [65]. Defining a privacy metric P that quantifies the information leakage from the intermediate representation Z, a proof of privacy preservation could demonstrate that for any e-differential privacy guarantee, the information leakage remains below a threshold:\nP(Z) \u2264 \u20ac (25)\nIt is noted that by using differential privacy with SL, the security will be improved at the cost of inference quality [66]. Thus, in Nesa's framework, this is defined as a tunable parameter to be decided, given the user requirements.\nBy leveraging Split Learning, Nesa effectively navigates the complexities of data encryption within its decentralized inference system for LLMs. This approach not only preserves the confidentiality and integrity of user data but also ensures the operational feasibility of complex model computations, demonstrating a sophisticated balance between privacy preservation and computational pragmatism."}, {"title": "CONCLUSION"}]}