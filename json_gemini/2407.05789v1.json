{"title": "CANDID DAC: Leveraging Coupled Action Dimensions with Importance Differences in DAC", "authors": ["Philipp Bordne", "M. Asif Hasan", "Eddie Bergman", "Noor Awad", "Andr\u00e9 Biedenkapp"], "abstract": "High-dimensional action spaces remain a challenge for dynamic algorithm configuration (DAC). Interdependencies and varying importance between action dimensions are further known key characteristics of DAC problems. We argue that these Coupled Action Dimensions with Importance Differences (CANDID) represent aspects of the DAC problem that are not yet fully explored. To address this gap, we introduce a new white-box benchmark within the DACBench suite that simulates the properties of CANDID. Further, we propose sequential policies as an effective strategy for managing these properties. Such policies factorize the action space and mitigate exponential growth by learning a policy per action dimension. At the same time, these policies accommodate the interdependence of action dimensions by fostering implicit coordination. We show this in an experimental study of value-based policies on our new benchmark. This study demonstrates that sequential policies significantly outperform independent learning of factorized policies in CANDID action spaces. In addition, they overcome the scalability limitations associated with learning a single policy across all action dimensions. The code used for our experiments is available under https://github.com/PhilippBordne/candidDAC.", "sections": [{"title": "1 Introduction", "content": "In the Dynamic Algorithm Configuration (DAC) problem (Biedenkapp et al., 2020) hyperparameters must be adjusted on-the-fly. A significant portion of these are either categorical or discrete, posing a challenge for reinforcement learning (RL), the common approach to solving DAC (Adriaensen et al., 2022), as they result in a combinatorial explosion of the joint action space. The DAC problem is further complicated by interaction effects between hyperparameters (Hutter et al., 2014; van Rijn and Hutter, 2018; Usmani et al., 2023) with varying importance of hyperparameters (Hutter et al., 2014; Moosbauer et al., 2021; Mohan et al., 2023; Biedenkapp et al., 2018). In this paper, we will refer to these properties as Coupled ActioN-Dimensions with Importance Differences (CANDID). This work investigates how CANDID influences the performance of RL algorithms and lays the ground for the development of better methods to tackle the DAC problem. To do so, we introduce a new CANDID benchmark derived from the original Sigmoid benchmark (Biedenkapp et al., 2020). We believe that this captures the complexities associated with the high-dimensionality of action spaces in DAC and other control domains more comprehensively. We use this benchmark to evaluate RL algorithms that learn policies per action dimension in factored action spaces (Sharma et al., 2017; Xue et al., 2022). In particular, we implement two algorithm variants to learn sequential policies (Metz et al., 2017). We compare them against a single agent baseline as typically used in DAC as well as a multi-agent baseline. Our results suggest that under the CANDID properties, sequential policies can coordinate action selection between dimensions while avoiding combinatorial explosion of the action space. Our initial results encourage an extended study of sequential policies for DAC."}, {"title": "2 Related Work", "content": "Hyperparameter importances and interactions in the AutoML landscape. Hyperparameter importance is a key topic in AutoML. Tools like fANOVA (Hutter et al., 2014) quantify the importance of individual hyperparameters and their interactions. These tools guide the tuning of hyperparameters and the examination of pipeline components in post-hoc analyses (van Rijn and Hutter, 2018). \u03a4\u03bf our knowledge, the importance and interactions of hyperparameters have not yet been used as structural information to solve DAC problems more effectively.\nSolving high-dimensional action spaces in RL. Large discrete or categorical action spaces pose a significant challenge for RL, as well as DAC by RL (Biedenkapp et al., 2022). Factored action space representations (FAR) address this challenge by learning policies per action dimension (Sharma et al., 2017). Metz et al. (2017) introduced sequential policies that control one action dimension at a time and condition on previously selected actions. We further develop this concept by ordering the sequential policies based on the importance of action dimensions and propose an adaptation of its training algorithm inspired by sequential games. Other research has approached high-dimensional action spaces as multi-agent learning problems, where each action dimension is controlled by a distinct agent. For example, Xue et al. (2022) trained individual agents per hyperparameter to dynamically tune a multi-objective optimization algorithm. In their study, well-established MARL algorithms such as VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018) were utilized to coordinate learning in the multi-agent setting. As we aim to explicitly exploit the CANDID property of DAC action spaces through sequential action selection, we focus solely on the simplest MARL baseline which is independent learning of agents. Analogous to MARL research, allowing sequential policies to observe previously selected actions acts as a communication scheme between agents. This approach is tailored to the structure revealed by the importance of action dimensions and their interaction effects. For further insights into the state-of-the-art of MARL, we refer interested readers to the survey of Huh and Mohapatra (2023)."}, {"title": "3 Piecewise Linear Benchmark", "content": "We implement the Piecewise Linear benchmark within DACBench (Eimer et al., 2021), building on the Sigmoid benchmark that models a DAC problem with a high-dimensional action space (Biedenkapp et al., 2020). In a M-D Sigmoid, the task is to predict the values of M individual Sigmoid curves over T = 10 time steps, with $n_{act}^m$ action choices for dimension m. Its reward function is defined as $r_t = \\Pi_{m=1}^M 1 - pred_error(a_t^m)$, which can be maximized by minimizing the prediction errors independently per action dimension. In contrast the Piecewise Linear benchmark aggregates all action dimensions through a weighted sum to predict on a single target function. This introduces coupling and importance differences between action dimensions, emulating CANDID properties:\n\n$pred(a_{1:M}^t) = \\frac{a_{nact}^1}{\\frac{M}{2} - 1} + \\sum_{m=2}^M w_m (\\frac{a_{net}^m}{\\frac{M}{2}-1} - \\frac{1}{2})$\n\nInstead of Sigmoid curves, we sample 2-segment piecewise linear functions as prediction targets. This requires constant coordination of a subset of the action dimensions and avoids the constant prediction target over several timesteps featured by many Sigmoid instances. We provide visualizations of action aggregation and details on prediction target sampling and reward computation in Appendix A. Figure 1 illustrates how actions must be jointly optimized in the CANDID setting."}, {"title": "4 Controlling CANDID Action Spaces with Sequential Policies", "content": "The aim of RL is to learn the optimal policy for a Markov Decision Process (MDP) M = (S, A, P, R) (Sutton and Barto, 2018). After factorizing a M-dimensional action space A = A\u2081 \u00d7 ... \u00d7 AM into 1-dimensional action spaces A1, ..., Am we can learn a policy per action dimension. We will refer to such approaches as factorized policies. Sequential policies build on the idea of extending the original MDP M to a sequential MDP (sMDP) by introducing substates ([st, ], [st, a\u2021], ..., [st, a:M-1]) to include an action selection process for all action dimensions at the current time step t (Metz et al., 2017). We provide a comprehensive formalization of the underlying MDP reformulations used in this paper in Appendix B.1.\nWe choose sequential policies to solve CANDID action spaces for two reasons. (I) Sequential policies are able to condition subsequent actions on already selected actions and thus to learn about the coupling between action dimensions. (II) The different importances of action dimensions induce an order for the selection process: By selecting the most important action first, this information is available when controlling all other action dimensions at the current time-step. We implemented two different algorithms to learn sequential policies, which differ in the TD-updates (see, Eq.2). The first is a simplified version of SDQN (simSDQN) that omits the upper Q-network compared to the implementation of Metz et al. (2017). This approach can be interpreted as a hybrid of tabular and function-approximation Q-learning. We introduce an implicit state that denotes the current stage in the action selection process and add a tabular entry for each of the M stages. The second approach can be interpreted as multiple agents playing a sequential game (Sequential Agent Q-Learning = SAQL) and selecting an is the turn of the m-th agent, an approach inspired by learning equilibria in Stackelberg games (Gerstgrasser and Parkes, 2023). As our setting is fully cooperative, all agents receive the same true reward. We provide a more detailed explanation in Appendix B.3.\n\n$target_{SAQL}^{1:m-1} = r_t + \\gamma arg max_{am} Q^m([s_{t+1}, a^{1:m-1}], a^m) = r_t + \\gamma V^m(s_{t+1}^{1:m-1})$ (2a)\n$target_{simSDQN}^{1:m} =\n\\begin{cases}\narg max_{am+1} Q^{m+1}([s_t, a^{1:m}], a^{m+1}) = V^{m+1}(s_{t+1}^{1:m}), & 1 \\leq m \\leq M-1\\\\\nr_t + \\gamma arg max_{a^1} Q^1([s_{t+1}, ], a^1) = r_t + \\gamma V^1(s_{t+1}), & m = M\n\\end{cases}$ (2b)"}, {"title": "5 Experimental Setup", "content": "We compared sequential policies on the Sigmoid and Piecewise Linear benchmark against two baselines: (I) Double DQN (DDQN) (van Hasselt et al., 2016) as baseline of a single-agent policy controlling all action dimensions simultaneously; (II) Independent Q-Learning (IQL) (Tampuu et al., 2015) as multi-agent baseline controlling all action dimensions independently. Note that IQL can be seen as an ablation of SAQL without communication between individual agents. We provide a detailed comparison of the evaluated algorithms in Appendix B.3.\nAdditional action dimensions of the Piecewise Linear benchmark enable higher rewards through more accurate predictions. As static baseline, we calculated the reward achievable by predictions based solely on the first and most important action dimension only (optimal (1D)). Outperforming this baseline indicates effective coordination of action dimensions.\nWe selected hyperparameters on a per-algorithm basis on the training instance set of the 5D Sigmoid benchmark. To this end, we evaluated each algorithm on a portfolio of 100 randomly sampled hyperparameter configurations. Each evaluation consisted of 10 random seeds that we aggregated through the median (Agarwal et al., 2021). We provide the configurations together with the model architectures in Appendices C & D and the used computing resources in Appendix E.\nTo induce importance differences, we define the weights per action dimension as $w_m = \\lambda^{m-1}$, m \u2208 {2, ..., M}. We set the importance decay \u03bb = 0.5 for all experiments and report the results for further importance decays in Appendix F. To compare the scalability of the approaches with the action space size, we varied the number of action dimensions (dim, corresponds to M) and number of actions per dimension (n_act)."}, {"title": "6 Results and Discussion", "content": "Are Sequential Policies Beneficial for CANDID-DAC? To answer this question, we compare the best performing approaches on the 5D Sigmoid without CANDID properties and the 5D Piecewise Linear benchmark with CANDID properties. The performance of the multi-agent baselines (IQL) is notably poor in the CANDID setting and performs best in other scenarios, as illustrated in Fig. 2. Even on the 2D Piecewise Linear benchmark, where all other evaluated algorithms achieve near-perfect solutions within a fraction of the total training episodes, IQL lags behind. This demonstrates the need for a mechanism of coordination between action dimensions with interaction effects and becomes especially evident when comparing performances of IQL and SAQL.\nCan Sequential Policies Scale to Larger Configuration Spaces? To answer this, we vary the number of discrete action choices per dimension (n_act) and the number of action dimensions. Although being slightly outperformed by simSDQN and matched by DDQN on the 5D Piecewise Linear benchmark, the performance of SAQL (as IQL) remains stable when increasing the dimension of the action space to 10 (Fig. 3, upper row). simSDQN is notably impacted by the expansion of action space dimensionality. This outcome is likely attributed to the reward signal being solely observed in the TD-update of the last, i.e. the least important, action dimension. As a result, reward information must be propagated through more Q-networks before it reaches the most important policies, which is required for them to identify their optimal actions. DDQNs performance also decreases, as adding action dimensions leads to exponential growth of its action space. Increasing the number of discrete action choices per action dimension has a similar effect on DDQN as it still results in polynomial growth in size of its action space. Factorized policies are not negatively affected by this change, as can be seen in the bottom row of Figure 3, as it results only in a linear growth in size of their action spaces.\nWe further observed that the degree of importance differences between action dimensions has minimal impact on the relative performances of the evaluated algorithms, as detailed in Appendix F). In another ablation, we found that inverting the importance order of action selection in sequential policies slightly reduces their performance, which supports our initial intuition about the effectiveness of selecting the most important action first (appendix G)."}, {"title": "7 Limitations", "content": "This exploratory study is limited by its reliance on a white-box benchmark emulating the CANDID properties. It may not fully encapsulate the complexity of real-world applications, which could limit the general applicability of the findings. It is also limited to the most basic multi-agent baseline, IQL and comparison against more advanced algorithms such as QMIX would allow a better contextualization of sequential policies' performance within the SOTA. Several of the"}, {"title": "8 Broader Impact Statement", "content": "While this work was specifically directed towards DAC, the challenge of controlling numerous interacting inputs is ubiquitous in most complex real-world systems. As such, the introduced approach could be applied in settings where communication between components of these systems is possible. Some examples are robots with multiple joints, smart buildings or grids. Application of our approach would require prior analysis of the structure of the control problem, in particular, to identify interacting control inputs and their relative importance. Sequential policies require, like other factorized approaches, the learning of multiple policies. This can lead to an increased demand for training and computing time and memory."}, {"title": "9 Conclusions", "content": "In this report, we introduced coupled action dimensions with importance differences (CANDID) as an under-researched challenge for reinforcement learning & DAC and provided the Piecewise Linear benchmark for assessing RL algorithms in these settings. In our experimental study, we have shown that sequential policies are a promising technique to address this challenge. In future work, we plan to apply SAQL to real-world inspired benchmarks and to compare it against more advanced MARL baselines that use value function factorization such as QMIX. As we see value function factorization to be mainly orthogonal to SAQL, we also plan to combine the two approaches. Moreover, we plan to improve agent coordination by exploring learned message passing, inspired by existing approaches like Huang et al. (2020). This strategy aims to prevent observation space growth with action dimensionality, improving scalability."}, {"title": "B.1 Action Space Factorization and MDP Reformulations", "content": "The aggregated prediction $pred(a_{1:M}^t)$ is defined in Equation (1). To incentivize learning across all action dimensions, including those with less significant contributions, we define an exponentially decaying reward signal $r_t = e^{-c \\cdot pred_error(a_t^{1:M})}$. This formulation ensures that selecting the first action optimally is necessary to obtain high rewards, but it might not be sufficient, as the exponentially decaying reward puts more emphasis on a precise prediction. For our experiments, we have chosen c = 4.6."}, {"title": "B.2 Background on (D)DQN", "content": "Before we translate the MDP reformulations into our modifications to DDQN (van Hasselt et al., 2016) we want to provide a brief introduction to Q-Learning and Deep Q-Networks and recommend the excellent text-book by Sutton and Barto (2018) for a more thorough introduction.\nQ-Learning and TD-updates. Q-Learning (Watkins and Dayan, 1992) is a widely used approach to learn optimal policies in MDPs by learning the state-value function Q : S \u00d7 A \u2192 R, (s, a) \u2192 Q(s, a). Q(s, a) is an estimate of the accumulated (and potentially discounted) reward obtainable over an entire episode if choosing action a in state s. Given Q we define our policy \u03c0(s) = arg maxa Q(s, a) and analogously we can compute the value function V : S \u2192 R, s \u2192 V(s) of a state as V(s) = maxa Q(s, a). The idea of Q-Learning is to update our estimate Q using temporal-difference (TD) updates through the Bellman Optimality Equation: Q(st, at) = R\u2081 + y maxa Q(St+1, a). Rt and St+1 are random variables for the reward and next state we will end up with, if taking action at in state st. Given an observed transition (s, a, r, s') in our MDP the TD-update of Q is:\n\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha[r + \\gamma max_{a'} Q(s', a') \u2013 Q(s, a)] = Q(s, a) + \\alpha[r + V(s') \u2013 Q(s, a)]$ (4)\n\nHence we define $r + \\gamma max_{a'} Q(s', a') = r + \\gamma V(s')$ as our TD-target.\nQ-Learning through function approximation using (D)DQN. For very limited sizes of the state and action spaces we might learn tabular entries per pair (s, a) \u2208 S \u00d7 A. For very big or infinite state and/or action spaces we have to resort to function approximation, for example through Deep Q-Networks (DQN, Mnih et al. (2015)). Here we update the parameters \u03b8 of our Q-function by minimizing the loss:\n\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D} [(r + \\gamma max_{a'} Q(s', a'; \\theta^{-}) \u2013 Q(s, a; \\theta))^2]$\n\nParameters 0\u00af represent target networks that are updated with a delay. D is a replay buffer where we store transitions (experiences) we collect while interacting with the environment. To update our Q-network we use mini-batches sampled from D. Analogous to Q-Learning the TD-target for DQN is r + y maxa' Q(s', a'; \u03b8\u00af). We also note that Q-networks are mappings qe : S \u2192 R\u012aAI. That is given a state s they assign a value to each action a or action combination in action vector a = a1:M \u2208 NM in case of M-dimensional action spaces. This means Q(s, a; 0) = qe(s)[a].\nInstead of the original DQN we implemented our approaches using Double DQN (DDQN, van Hasselt et al. (2016)) which is a minor extension to DQN but does not affect the presented conceptualization."}, {"title": "B.3 Learning Q-Networks in the MDP reformulations", "content": "In this section, we discuss in detail how to learn policies for different reformulations of the original MDP (Fig. 7). For ease of presentation, we drop references to \u03b8. Solving the original MDP is straightforward and requires the learning of a single policy. The related Q-network can be updated against the common TD-target.\nFor the parallel MDP, we apply Independent Q-Learning (IQL) (Tampuu et al., 2015), learning M policies, one per action dimension. Each associated Q-network takes the original state st as input and maps it to the Q-values of its respective action dimension. Since the actions of other agents go unobserved, they can be treated as unobserved (and nonstationary) environment dynamics. Consequently, we can update each of the M Q-networks independently using the shared, common reward, and the usual TD-target.\nIn the sequential MDP (Metz et al., 2017), policies can observe not only the current state st, but also the action dimensions already selected for that state. Accordingly, the Q-networks learn to map these augmented observations to the action values of their respective dimensions. We implemented two approaches to update these Q-networks, with different underlying interpretations. The first approach, Sequential-Agent Q-Learning (SAQL), views the sequential MDP as a sequential stochastic game. In substate sm-1 = [st, a1:m\u22121], it is agent m's turn to choose an action, observing the actions already taken by other players in round t. Agent m will make its next observation in round t + 1, with some fellow players having already acted. Here, we apply the standard DDQN algorithm, limited to the respective action dimension, using an augmented observation space and the shared reward rt, since the game setting is fully cooperative. IQL can be seen as an ablation of SAQL, with identical Q-networks and TD-targets, except for excluding the observation of other agents' actions.\nThe second approach, Simplified Sequential DQN (simSDQN), is mainly identical to the original proposal by Metz et al. (2017). Our modification is to omit the upper Q-network, because we couldn't successfully train under this setup. We refer the reader to the paper by Metz et al. (2017) for the role of the upper Q-network. In simSDQN, we explicitly solve the sequential MDP by updating our Q-Functions against the state-value of the next substate in the sMDP. Using M Q-networks can be viewed as tabular entries for each of the M substates recurring periodically. Table 1 lists the Q-networks to be learned for our different approaches, and equation 6 formalizes the targets for the Q-network updates.\n\n$target_{DDQN} = r_t + \\gamma arg max_{a} Q(s_{t+1}, a) = r_t + V(s_{t+1})$ (6a)\n$target_{IQL}^m = r_t + \\gamma arg max_{a^m} Q^m(s_{t+1}, a^m) = r_t + \\gamma V^m(s_{t+1})$ (6b)\n$target_{SAQL}^m = r_t + \\gamma arg max_{a^m} Q^m([s_{t+1}, a^{1:m-1}], a^m) = r_t + \\gamma V^m(s_{t+1}^{1:m-1})$ (6c)\n$target_{simSDQN}^m =\n\\begin{cases}\nr_t + \\gamma arg max_{a^{m+1}} Q^{m+1}([s_t, a^{1:m}], a^{m+1}) = V^{m+1}(s_{t+1}^{1:m}), & 1 < m < M-1\\\\\nr_t + \\gamma arg max_{a^1} Q^1([s_{t+1}, ], a^1) = r_t + \\gamma V^1(s_{t+1}), & m = M\n\\end{cases}$ (6d)"}, {"title": "C Hyperparameter Settings", "content": ""}, {"title": "D Policy architecture", "content": "We represented the Q-functions of our learned policies as 3 layer MLPs. Note that for the factorized policies IQL, SAQL, simSDQN the number of policies to learn corresponds to the dimension of the benchmark. For all policies, we used shared numbers of hidden units: 120 and 84 in the first and second hidden layers, respectively. For DDQN and IQL the number of input units corresponds to the dimensionality of the observation space of the benchmark environment. For sequential policies SAQL and simSDQN the size of the input layer of Qm is dim_observation_space + m for m\u2208 {0, ..., \u041c \u2013 1}. The output size for all factorized policies is the number of actions per action dimension n_act \u2208 {3, 5, 10}, for DDQN it is the number of all possible action combinations over all action dimensions $n_{act}^M = n_{act}^{dim}$."}, {"title": "E Compute Resources", "content": "Unless otherwise stated, the experiments were run on nodes using a single CPU \"Intel Xeon Gold 6230\". For DDQN experiments with higher-dimensional action spaces (dim = 10) or more discrete action choices per action dimension (n_act = 10), the resulting Q-networks were substantially larger, necessitating the use of GPU accelerators. These experiments were executed on nodes equipped with \"NVIDIA Tesla V100\" GPUs."}, {"title": "F Different Importance Decays", "content": ""}, {"title": "G Reversed Importances", "content": "The ablation presented in Fig. 9 confirmed our intuition regarding selecting actions in descending order of their importance for sequential policies and emphasizes the signficance of getting the order of importances right. This analysis also sheds light on a potential issue of simSDQN when facing higher dimensonal action spaces: In our design, rewards are only assigned when performing the TD-update for the Q-network responsible for the least important action dimension. This requires the propagation of reward information to more important action dimensions (for a conceptual illustration, refer to Figure 7). By reversing the order of the policy, the Q-network corresponding to the most significant action dimension can be directly updated towards the reward signal. This leads to a noticeable speed up of learning in the initial training phase."}]}