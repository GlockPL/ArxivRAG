{"title": "CANDID DAC: Leveraging Coupled Action Dimensions with\nImportance Differences in DAC", "authors": ["Philipp Bordne", "M. Asif Hasan", "Eddie Bergman", "Noor Awad", "Andr\u00e9 Biedenkapp"], "abstract": "High-dimensional action spaces remain a challenge for dynamic algorithm configuration\n(DAC). Interdependencies and varying importance between action dimensions are further\nknown key characteristics of DAC problems. We argue that these Coupled Action Dimensions\nwith Importance Differences (CANDID) represent aspects of the DAC problem that are\nnot yet fully explored. To address this gap, we introduce a new white-box benchmark\nwithin the DACBench suite that simulates the properties of CANDID. Further, we propose\nsequential policies as an effective strategy for managing these properties. Such policies\nfactorize the action space and mitigate exponential growth by learning a policy per action\ndimension. At the same time, these policies accommodate the interdependence of action\ndimensions by fostering implicit coordination. We show this in an experimental study\nof value-based policies on our new benchmark. This study demonstrates that sequential\npolicies significantly outperform independent learning of factorized policies in CANDID\naction spaces. In addition, they overcome the scalability limitations associated with learning\na single policy across all action dimensions. The code used for our experiments is available\nunder https://github.com/PhilippBordne/candidDAC.", "sections": [{"title": "1 Introduction", "content": "In the Dynamic Algorithm Configuration (DAC) problem (Biedenkapp et al., 2020) hyperparameters\nmust be adjusted on-the-fly. A significant portion of these are either categorical or discrete, posing\na challenge for reinforcement learning (RL), the common approach to solving DAC (Adriaensen\net al., 2022), as they result in a combinatorial explosion of the joint action space. The DAC problem\nis further complicated by interaction effects between hyperparameters (Hutter et al., 2014; van Rijn\nand Hutter, 2018; Usmani et al., 2023) with varying importance of hyperparameters (Hutter et al.,\n2014; Moosbauer et al., 2021; Mohan et al., 2023; Biedenkapp et al., 2018). In this paper, we will refer\nto these properties as Coupled ActioN-Dimensions with Importance Differences (CANDID). This\nwork investigates how CANDID influences the performance of RL algorithms and lays the ground\nfor the development of better methods to tackle the DAC problem. To do so, we introduce a new\nCANDID benchmark derived from the original Sigmoid benchmark (Biedenkapp et al., 2020). We\nbelieve that this captures the complexities associated with the high-dimensionality of action spaces\nin DAC and other control domains more comprehensively. We use this benchmark to evaluate RL\nalgorithms that learn policies per action dimension in factored action spaces (Sharma et al., 2017;\nXue et al., 2022). In particular, we implement two algorithm variants to learn sequential policies\n(Metz et al., 2017). We compare them against a single agent baseline as typically used in DAC as well\nas a multi-agent baseline. Our results suggest that under the CANDID properties, sequential policies\ncan coordinate action selection between dimensions while avoiding combinatorial explosion of the\naction space. Our initial results encourage an extended study of sequential policies for DAC."}, {"title": "2 Related Work", "content": "Hyperparameter importances and interactions in the AutoML landscape. Hyperparameter impor-\ntance is a key topic in AutoML. Tools like fANOVA (Hutter et al., 2014) quantify the importance of"}, {"title": "3 Piecewise Linear Benchmark", "content": "We implement the Piecewise Linear benchmark within DACBench (Eimer et al., 2021), building\non the Sigmoid benchmark that models a DAC problem with a high-dimensional action space\n(Biedenkapp et al., 2020). In a M-D Sigmoid, the task is to predict the values of M individual Sigmoid\ncurves over T = 10 time steps, with $n_{act}$ action choices for dimension m. Its reward function is\ndefined as $r_t = \\prod_{m=1}^{M} 1 \u2013 pred\\_error(a_m)$, which can be maximized by minimizing the prediction\nerrors independently per action dimension. In contrast the Piecewise Linear benchmark aggregates\nall action dimensions through a weighted sum to predict on a single target function. This introduces\ncoupling and importance differences between action dimensions, emulating CANDID properties:\n\n$pred(a^{1:M}) = \\frac{a_1}{n_{act}-1} + \\sum_{m=2}^{M} w_m(\\frac{a_m}{n_{act}-1} - \\frac{1}{2})$\n\nInstead of Sigmoid curves, we sample 2-segment piecewise linear functions as prediction targets.\nThis requires constant coordination of a subset of the action dimensions and avoids the constant\nprediction target over several timesteps featured by many Sigmoid instances. We provide visualiza-\ntions of action aggregation and details on prediction target sampling and reward computation in\nAppendix A. Figure 1 illustrates how actions must be jointly optimized in the CANDID setting."}, {"title": "4 Controlling CANDID Action Spaces with Sequential Policies", "content": "The aim of RL is to learn the optimal policy for a Markov Decision Process (MDP) M = (S, A, P, R)\n(Sutton and Barto, 2018). After factorizing a M-dimensional action space A = A\u2081 \u00d7 ... \u00d7 AM into\n1-dimensional action spaces A1, ..., Am we can learn a policy per action dimension. We will refer to\nsuch approaches as factorized policies. Sequential policies build on the idea of extending the original\nMDP M to a sequential MDP (sMDP) by introducing substates ([st, ], [st, a\u2021], ..., [st, a:M-1]) to\ninclude an action selection process for all action dimensions at the current time step t (Metz et al.,\n2017). We provide a comprehensive formalization of the underlying MDP reformulations used in\nthis paper in Appendix B.1.\nWe choose sequential policies to solve CANDID action spaces for two reasons. (I) Sequential\npolicies are able to condition subsequent actions on already selected actions and thus to learn about\nthe coupling between action dimensions. (II) The different importances of action dimensions induce\nan order for the selection process: By selecting the most important action first, this information is\navailable when controlling all other action dimensions at the current time-step. We implemented\ntwo different algorithms to learn sequential policies, which differ in the TD-updates (see, Eq.2). The\nfirst is a simplified version of SDQN (simSDQN) that omits the upper Q-network compared to the\nimplementation of Metz et al. (2017). This approach can be interpreted as a hybrid of tabular and\nfunction-approximation Q-learning. We introduce an implicit state that denotes the current stage in\nthe action selection process and add a tabular entry for each of the M stages. The second approach\ncan be interpreted as multiple agents playing a sequential game (Sequential Agent Q-Learning =\nSAQL) and selecting an is the turn of the m-th agent, an approach inspired by learning equilibria\nin Stackelberg games (Gerstgrasser and Parkes, 2023). As our setting is fully cooperative, all agents\nreceive the same true reward. We provide a more detailed explanation in Appendix B.3.\n\n$target_{SAQL}^{1:m-1}$ = $r_t + \\gamma \\arg \\max a^m Q^m ([s_{t+1}, a^{1:m-1}], a^m) = r_t + \\gamma V^m(s_{t+1}^{1:m})$\n$target_{simSDQN} = \\{\n\\begin{array}{ll}\narg \\max a^{m+1} Q^{m+1}([s_t, a^{1:m}], a^{m+1}) = V^{m+1}(s_{t+1}^{1:m}), & 1 \\leq m \\leq M-1 \\\\\nr_t + \\gamma \\arg \\max a^1 Q^1 ([s_{t+1}, a^{1:M}], a^1) = r_t + \\gamma V^1(s_{t+1}^{1:M}), & m = M\n\\end{array}$"}, {"title": "5 Experimental Setup", "content": "We compared sequential policies on the Sigmoid and Piecewise Linear benchmark against two\nbaselines: (I) Double DQN (DDQN) (van Hasselt et al., 2016) as baseline of a single-agent policy\ncontrolling all action dimensions simultaneously; (II) Independent Q-Learning (IQL) (Tampuu et al.,\n2015) as multi-agent baseline controlling all action dimensions independently. Note that IQL can\nbe seen as an ablation of SAQL without communication between individual agents. We provide a\ndetailed comparison of the evaluated algorithms in Appendix B.3.\nAdditional action dimensions of the Piecewise Linear benchmark enable higher rewards through\nmore accurate predictions. As static baseline, we calculated the reward achievable by predictions\nbased solely on the first and most important action dimension only (optimal (1D)). Outperforming\nthis baseline indicates effective coordination of action dimensions.\nWe selected hyperparameters on a per-algorithm basis on the training instance set of the 5D\nSigmoid benchmark. To this end, we evaluated each algorithm on a portfolio of 100 randomly\nsampled hyperparameter configurations. Each evaluation consisted of 10 random seeds that we\naggregated through the median (Agarwal et al., 2021). We provide the configurations together with\nthe model architectures in Appendices C & D and the used computing resources in Appendix E.\nTo induce importance differences, we define the weights per action dimension as $w_m = \\lambda^{m-1}$, $m \u2208 {2, ..., M}$. We set the importance decay \u03bb = 0.5 for all experiments and report the\nresults for further importance decays in Appendix F. To compare the scalability of the approaches\nwith the action space size, we varied the number of action dimensions (dim, corresponds to M) and\nnumber of actions per dimension (n_act)."}, {"title": "6 Results and Discussion", "content": "Are Sequential Policies Beneficial for CANDID-DAC? To answer this question, we compare the\nbest performing approaches on the 5D Sigmoid without CANDID properties and the 5D Piecewise\nLinear benchmark with CANDID properties. The performance of the multi-agent baselines (IQL) is\nnotably poor in the CANDID setting and performs best in other scenarios, as illustrated in Fig. 2.\nEven on the 2D Piecewise Linear benchmark, where all other evaluated algorithms achieve near-\nperfect solutions within a fraction of the total training episodes, IQL lags behind. This demonstrates\nthe need for a mechanism of coordination between action dimensions with interaction effects and\nbecomes especially evident when comparing performances of IQL and SAQL.\nCan Sequential Policies Scale to Larger Configuration Spaces? To answer this, we vary the\nnumber of discrete action choices per dimension (n_act) and the number of action dimensions.\nAlthough being slightly outperformed by simSDQN and matched by DDQN on the 5D Piecewise\nLinear benchmark, the performance of SAQL (as IQL) remains stable when increasing the dimension\nof the action space to 10 (Fig. 3, upper row). simSDQN is notably impacted by the expansion of\naction space dimensionality. This outcome is likely attributed to the reward signal being solely\nobserved in the TD-update of the last, i.e. the least important, action dimension. As a result, reward\ninformation must be propagated through more Q-networks before it reaches the most important\npolicies, which is required for them to identify their optimal actions. DDQNs performance also\ndecreases, as adding action dimensions leads to exponential growth of its action space. Increasing\nthe number of discrete action choices per action dimension has a similar effect on DDQN as it\nstill results in polynomial growth in size of its action space. Factorized policies are not negatively\naffected by this change, as can be seen in the bottom row of Figure 3, as it results only in a linear\ngrowth in size of their action spaces.\nWe further observed that the degree of importance differences between action dimensions\nhas minimal impact on the relative performances of the evaluated algorithms, as detailed in\nAppendix F). In another ablation, we found that inverting the importance order of action selection\nin sequential policies slightly reduces their performance, which supports our initial intuition about\nthe effectiveness of selecting the most important action first (appendix G)."}, {"title": "7 Limitations", "content": "This exploratory study is limited by its reliance on a white-box benchmark emulating the CANDID\nproperties. It may not fully encapsulate the complexity of real-world applications, which could\nlimit the general applicability of the findings. It is also limited to the most basic multi-agent\nbaseline, IQL and comparison against more advanced algorithms such as QMIX would allow\na better contextualization of sequential policies' performance within the SOTA. Several of the"}, {"title": "8 Broader Impact Statement", "content": "While this work was specifically directed towards DAC, the challenge of controlling numerous\ninteracting inputs is ubiquitous in most complex real-world systems. As such, the introduced\napproach could be applied in settings where communication between components of these systems\nis possible. Some examples are robots with multiple joints, smart buildings or grids. Application of\nour approach would require prior analysis of the structure of the control problem, in particular,\nto identify interacting control inputs and their relative importance. Sequential policies require,\nlike other factorized approaches, the learning of multiple policies. This can lead to an increased\ndemand for training and computing time and memory."}, {"title": "9 Conclusions", "content": "In this report, we introduced coupled action dimensions with importance differences (CANDID) as\nan under-researched challenge for reinforcement learning & DAC and provided the Piecewise Linear\nbenchmark for assessing RL algorithms in these settings. In our experimental study, we have shown\nthat sequential policies are a promising technique to address this challenge. In future work, we plan\nto apply SAQL to real-world inspired benchmarks and to compare it against more advanced MARL\nbaselines that use value function factorization such as QMIX. As we see value function factorization\nto be mainly orthogonal to SAQL, we also plan to combine the two approaches. Moreover, we\nplan to improve agent coordination by exploring learned message passing, inspired by existing\napproaches like Huang et al. (2020). This strategy aims to prevent observation space growth with\naction dimensionality, improving scalability."}, {"title": "B.1 Action Space Factorization and MDP Reformulations", "content": null}, {"title": "B.2 Background on (D)DQN", "content": "Before we translate the MDP reformulations into our modifications to DDQN (van Hasselt et al.,\n2016) we want to provide a brief introduction to Q-Learning and Deep Q-Networks and recommend\nthe excellent text-book by Sutton and Barto (2018) for a more thorough introduction.\nQ-Learning and TD-updates. Q-Learning (Watkins and Dayan, 1992) is a widely used approach to\nlearn optimal policies in MDPs by learning the state-value function Q : S \u00d7 A \u2192 R, (s, a) \u2192 Q(s, a).\nQ(s, a) is an estimate of the accumulated (and potentially discounted) reward obtainable over an\nentire episode if choosing action a in state s. Given Q we define our policy \u03c0(s) = arg maxa Q(s, a)\nand analogously we can compute the value function V : S \u2192 R, s \u2192 V(s) of a state as V(s) =\nmaxa Q(s, a). The idea of Q-Learning is to update our estimate Q using temporal-difference (TD)\nupdates through the Bellman Optimality Equation: Q(st, at) = R\u2081 + y maxa Q(St+1, a). Rt and St+1\nare random variables for the reward and next state we will end up with, if taking action at in state\nst. Given an observed transition (s, a, r, s') in our MDP the TD-update of Q is:\n\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha[r + \\gamma \\max_{a'} Q(s', a') \u2013 Q(s, a)] = Q(s, a) + \\alpha[r + V(s') \u2013 Q(s, a)]$\n\nHence we define r + $ \\gamma \\max_{a'} Q(s', a') = r + \\gamma V(s')$ as our TD-target.\nQ-Learning through function approximation using (D)DQN. For very limited sizes of the state\nand action spaces we might learn tabular entries per pair (s, a) \u2208 S \u00d7 A. For very big or infinite\nstate and/or action spaces we have to resort to function approximation, for example through Deep\nQ-Networks (DQN, Mnih et al. (2015)). Here we update the parameters \u03b8 of our Q-function by\nminimizing the loss:\n\n$L(\\theta) = E_{(s,a,r,s')\\sim D} [(r + \\gamma \\max_{a'} Q(s', a'; \\theta^{\u2013}) \u2013 Q(s, a; \\theta))^2]$\n\nParameters $ \\theta^{\u2013}$ represent target networks that are updated with a delay. D is a replay buffer where\nwe store transitions (experiences) we collect while interacting with the environment. To update\nour Q-network we use mini-batches sampled from D. Analogous to Q-Learning the TD-target\nfor DQN is r + $ \\gamma \\max_{a'} Q(s', a'; \\theta^{\u2013})$. We also note that Q-networks are mappings qe : S \u2192 $ \\mathbb{R}^{\\|A\\|}$.\nThat is given a state s they assign a value to each action a or action combination in action vector\na = a1:M \u2208 NM in case of M-dimensional action spaces. This means Q(s, a; 0) = qe(s)[a].\nInstead of the original DQN we implemented our approaches using Double DQN (DDQN, van Hasselt\net al. (2016)) which is a minor extension to DQN but does not affect the presented conceptualization."}, {"title": "B.3 Learning Q-Networks in the MDP reformulations", "content": "In this section, we discuss in detail how to learn policies for different reformulations of the\noriginal MDP (Fig. 7). For ease of presentation, we drop references to \u03b8. Solving the original MDP is\nstraightforward and requires the learning of a single policy. The related Q-network can be updated\nagainst the common TD-target.\nFor the parallel MDP, we apply Independent Q-Learning (IQL) (Tampuu et al., 2015), learning M\npolicies, one per action dimension. Each associated Q-network takes the original state st as input\nand maps it to the Q-values of its respective action dimension. Since the actions of other agents\ngo unobserved, they can be treated as unobserved (and nonstationary) environment dynamics.\nConsequently, we can update each of the M Q-networks independently using the shared, common\nreward, and the usual TD-target.\nIn the sequential MDP (Metz et al., 2017), policies can observe not only the current state st,\nbut also the action dimensions already selected for that state. Accordingly, the Q-networks learn\nto map these augmented observations to the action values of their respective dimensions. We\nimplemented two approaches to update these Q-networks, with different underlying interpretations.\nThe first approach, Sequential-Agent Q-Learning (SAQL), views the sequential MDP as a sequential\nstochastic game. In substate sm-1 = [st, a1:m\u22121], it is agent m's turn to choose an action, observing\nthe actions already taken by other players in round t. Agent m will make its next observation in\nround t + 1, with some fellow players having already acted. Here, we apply the standard DDQN\nalgorithm, limited to the respective action dimension, using an augmented observation space and\nthe shared reward rt, since the game setting is fully cooperative. IQL can be seen as an ablation of\nSAQL, with identical Q-networks and TD-targets, except for excluding the observation of other\nagents' actions.\nThe second approach, Simplified Sequential DQN (simSDQN), is mainly identical to the original\nproposal by Metz et al. (2017). Our modification is to omit the upper Q-network, because we\ncouldn't successfully train under this setup. We refer the reader to the paper by Metz et al. (2017) for\nthe role of the upper Q-network. In simSDQN, we explicitly solve the sequential MDP by updating\nour Q-Functions against the state-value of the next substate in the sMDP. Using M Q-networks can\nbe viewed as tabular entries for each of the M substates recurring periodically. Table 1 lists the\nQ-networks to be learned for our different approaches, and equation 6 formalizes the targets for\nthe Q-network updates.\n\n$target_{DDQN} = r_t + \\gamma \\arg \\max _a Q(s_{t+1}, a) = r_t + V(s_{t+1})$\n\n$target_{IQL}^m = r_t + \\gamma \\arg \\max _{a^m} Q^m(s_{t+1}, a^m) = r_t + V^m(s_{t+1})$\n\n$target_{SAQL} = r_t + \\gamma \\arg \\max _{a^m} Q^m ([s_{t+1}, a^{1:m-1}], a^m) = r_t + V^m (s_{t+1}^{1:m})$\n\n$target_{simSDQN} = \\{\n\\begin{array}{ll}\nr_t + \\gamma \\arg \\max _{a^{m+1}} Q^{m+1}([s_t, a^{1:m}], a^{m+1}) = V^{m+1}(s_{t+1}^{1:m}), & 1 < m < M-1 \\\\\nr_t + \\gamma \\arg \\max _{a^1} Q^1 ([s_{t+1}, a^{1:M}], a^1) = r_t + \\gamma V^1(s_{t+1}^{1:M}), & m = M\n\\end{array}$"}, {"title": "C Hyperparameter Settings", "content": null}, {"title": "D Policy architecture", "content": "We represented the Q-functions of our learned policies as 3 layer MLPs. Note that for the factorized\npolicies IQL, SAQL, simSDQN the number of policies to learn corresponds to the dimension of the\nbenchmark. For all policies, we used shared numbers of hidden units: 120 and 84 in the first and\nsecond hidden layers, respectively. For DDQN and IQL the number of input units corresponds\nto the dimensionality of the observation space of the benchmark environment. For sequential\npolicies SAQL and simSDQN the size of the input layer of Qm is dim_observation_space + m for\nm\u2208 {0, ..., \u041c \u2013 1}. The output size for all factorized policies is the number of actions per action\ndimension n_act \u2208 {3, 5, 10}, for DDQN it is the number of all possible action combinations over\nall action dimensions $n_{act}^M = n_{act}^{dim}$."}, {"title": "E Compute Resources", "content": "Unless otherwise stated, the experiments were run on nodes using a single CPU \"Intel Xeon Gold\n6230\". For DDQN experiments with higher-dimensional action spaces (dim = 10) or more discrete\naction choices per action dimension (n_act = 10), the resulting Q-networks were substantially\nlarger, necessitating the use of GPU accelerators. These experiments were executed on nodes\nequipped with \"NVIDIA Tesla V100\" GPUs."}, {"title": "F Different Importance Decays", "content": null}, {"title": "G Reversed Importances", "content": "The ablation presented in Fig. 9 confirmed our intuition regarding selecting actions in descending\norder of their importance for sequential policies and emphasizes the signficance of getting the order\nof importances right. This analysis also sheds light on a potential issue of simSDQN when facing\nhigher dimensonal action spaces: In our design, rewards are only assigned when performing the\nTD-update for the Q-network responsible for the least important action dimension. This requires\nthe propagation of reward information to more important action dimensions (for a conceptual\nillustration, refer to Figure 7). By reversing the order of the policy, the Q-network corresponding\nto the most significant action dimension can be directly updated towards the reward signal. This\nleads to a noticeable speed up of learning in the initial training phase."}]}