{"title": "WyckoffDiff - A Generative Diffusion Model for Crystal Symmetry", "authors": ["Filip Ekstr\u00f6m Kelvinius", "Oskar B. Andersson", "Abhijith S. Parackal", "Dong Qian", "Rickard Armiento", "Fredrik Lindsten"], "abstract": "Crystalline materials often exhibit a high level of symmetry. However, most generative models do not account for symmetry, but rather model each atom without any constraints on its position or element. We propose a generative model, Wyckoff Diffusion (WYCKOFF DIFF), which generates symmetry-based descriptions of crystals. This is enabled by considering a crystal structure representation that encodes all symmetry, and we design a novel neural network architecture which enables using this representation inside a discrete generative model framework. In addition to respecting symmetry by construction, the discrete nature of our model enables fast generation. We additionally present a new metric, Fr\u00e9chet Wrenformer Distance, which captures the symmetry aspects of the materials generated, and we benchmark WYCKOFFDIFF against recently proposed generative models for crystal generation.", "sections": [{"title": "Introduction", "content": "Materials science is a field of research that is essential for technological advancement. With machine learning seeing success in a variety of fields, materials science is no exception. In the search for new materials, so called generative models are an attractive class of methods, and a number of models that can generate new materials have been developed (see, e.g., Park et al., 2024, for an overview). However, crystalline materials are often characterized by their specific symmetries, which are integral to their materials properties. This is an aspect that only recently has been built into generative models (Jiao et al., 2024; Zhu et al., 2024; Levy et al., 2024). Instead, models without any built-in mechanisms that ensure symmetry in materials have and are still being developed Xie et al. (2022); Jiao et al. (2023); Merchant et al. (2023); Zeni et al. (2025). As demonstrated by several works (Levy et al., 2024; Cheetham & Seshadri, 2024; Zeni et al., 2025), materials generated from methods without these explicit constraints often lack the symmetrical characteristics of materials found in databases. For example, Cheetham & Seshadri (2024) find that roughly 34% of the materials generated by the GNOME model (Merchant et al., 2023) belong to four different space groups of which only one exists in the Inorganic Crystal Structure Database Belsky et al. (2002) where it makes up only 1%, and Zeni et al. (2025) mention that their MatterGen model tends to generate less symmetric structures than are present in the training data.\n The symmetry of a material can be encoded in a protostructure description (Parackal et al., 2024, see also Section 2.1), where elements occupy Wyckoff positions in crystal structures categorized into space groups. This description avoids specifying the exact atomic coordinates, while maintaining the key structural information, which has been shown to be efficient for searching for novel stable materials by enabling an initial step where candidate crystal structures with high likelihood of being stable are identified based on the symmetry description alone. This step avoids wasting computational resources on exact coordinate calculations across all possible materials (Goodall et al., 2022). Additionally, the infinite space of continuous coordinates also opens the risk of generating degenerate materials or structures outside of the symmetry proximity. Since materials of high symmetry are generally the interesting materials to explore, generation of large sets of low symmetry materials is inefficient. Explicitly encoding symmetry could allow a generative model to only generate within a space of interesting materials of higher symmetry, allowing a symmetry-infused generative model to generate a broader variety of relevant crystalline materials compared to a generative model using exact coordinate representations.\n Explicitly enforcing knowledge about symmetry in"}, {"title": "Background", "content": "2.1 Representing Crystals\n An ideal crystalline material is commonly represented by its crystal structure as an infinitely repeating set of unit cells with atoms of specified chemical elements placed at specific atomic positions. In the unit cell, the M atoms are specified by their positions $X \\in R^{M\\times3}$ and elements $Z\\in Z^M$, and the geometry of the unit cell can be specified by three lattice vectors $L\\inR^{3\\times3}$. As an alternative, one can separately specify the symmetry of the atomic positions, and then specify the atomic coordinates only by precise values for the remaining degrees of freedom. This representation is discussed in the following.\n Protostructures All possible combinations of symmetries of crystal structures can be categorized into 230 space groups M\u00fcller et al. (2013). The atoms, each a chemical element from the periodic table of elements, can then occupy a so called Wyckoff position in the crystal structure, which represents sets of points on which the symmetry operators act in a specific way. Hence, if an atom is specified to sit at a specific Wyckoff position, depending on the nature of that Wyckoff position, this declares it to reside exactly at a specific point; anywhere along a line; in a plane; or in a volume, and the symmetry operators then imply that equivalent atoms sit at a number (the multiplicity) of other points in the unit cell, called the orbit. These different Wyckoff positions are labeled using a letter from the Latin alphabet (a, b, c, etc.). The space group completely determines which Wyckoff positions that are available, as tabulated by The Volume of International Tables for Crystallography (IUCr, 2002).\n In this work, we use the term prototype as defined for AFLOW prototype labels (Mehl et al., 2017), i.e., the combination of the spacegroup and how the Wyckoff positions are occupied by unspecified but distinct elements, without additional information about the remaining degrees of freedom for those occupied positions. In more detail, the AFLOW prototype label ABC6_hR24_166_a_b_h specifies first the anonymous composition AB6C (i.\u0435., AB6C), then the Pearson symbol hR24, followed by the spacegroup number 166, and a list of Wyckoff labels for the positions occu-"}, {"title": "Diffusion Models", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) are a type of generative models that have received tremendous interest lately. In essence, they are based on the idea of starting from a pure noise sample $x_T$, which is iteratively \"denoised\" to end up with a \"clean\" sample $x_0$. This denoising is enabled by viewing the data-to-noise (forward) process as a fixed Markov chain\n$q(x_{0:T}) = q(x_0) \\prod_{t=0}^{T-1} q(x_{t+1}|x_t),$ (1)\nwhere $q(x_0)$ is the data distribution and the transitions $q(x_{t+1}|x_t)$ are designed such that, for large T, $q(x_T)$ converges to a distribution $p(x_T)$ from which we can easily sample, like a Gaussian distribution in case of continuous variables. The reverse process is then parametrized as\n$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=0}^{T-1} p_\\theta(x_t|x_{t+1}),$ (2)\nwhere $p_\\theta(x_t|x_{t+1})$ are fitted such that $p_\\theta(x_t|x_{t+1}) \\approx q(x_t|x_{t+1})$. Sampling according to the reverse process will then give (approximate) samples from the data distribution $q(x_0)$.\n While most diffusion models have been developed for continuous data, there are also several methods designed for the discrete case (e.g., Hoogeboom et al., 2021; Austin et al., 2021; Campbell et al., 2022; Sun et al., 2023; Lou et al., 2024). Conceptually, the idea is the same, but the transitions (both in the forward and backward directions) operate on discrete state-spaces and the limiting distribution $p(x_T)$ is typically chosen to factorize over the components of $x_T$ to enable easy sampling. In this work we make explicit use of the"}, {"title": "Related Work", "content": "CDVAE The Crystal Diffusion Variational Autoencoder (CDVAE) (Xie et al., 2022) is a generative model for crystal structures that combines a variational autoencoder (VAE) with a diffusion model. Generation from CDVAE starts with sampling from the VAE: a vector $z \\sim N(0, I)$ is sampled from which the lattice vectors L, the number of atoms M, and the initial composition are decoded. The positions of the M atoms are randomly initialized, and the elements are randomly assigned according to the decoded composition. The diffusion process then consists of denoising the positions and elements, conditioned on z, while keeping L fixed during the full process. The positions and atoms are updated without any explicit or built-in constraints with respect to symmetries.\n DiffCSP and DiffCSP++ DiffCSP (Jiao et al., 2023) builds upon CDVAE by replacing the VAE with a diffusion model that jointly learns the lattice and coordinates, enabling more precise modeling of crystal geometry. DiffCSP++ Jiao et al. (2024) further incorporates space group symmetry by leveraging predefined structural templates from the training data to learn atomic types and coordinates aligned with these templates. However, this might limit the diversity and novelty of the generated materials.\n SymmCD To address this limitation, SymmCD (Levy et al., 2024) introduces a physically-motivated representation of symmetries as binary matrices, enabling efficient information-sharing and generalization across both crystal and site symmetries. By explicitly incorporating crystallographic symmetry into the generative process, SymmCD can generate diverse and valid crystals with realistic symmetries and predicted properties."}, {"title": "Wyckoff Diffusion", "content": "3.1 Representing a Protostructure\n Given a space group $s \\in G = \\{1, ..., 230\\}$, we denote the set of all possible Wyckoff positions as $L(s)$. To represent a protostructure, we partition the set of Wyckoff positions into the positions without degrees of freedom (i.e., an atom occupying the position is"}, {"title": "Discrete Diffusion", "content": "As both $z^\\circ$ and $z^\\infty$ are discrete variables, we will use the Discrete Denoising Diffusion Model (D3PM) (Austin et al., 2021) as our underlying diffusion model. In this framework, a datapoint is denoted as $x = (x_1, ..., x_D)$ where each variable $x_k$ is a discrete variable, and \"noise\" is added independently to each variable according to a discrete Markov chain. By denoting $x^k_t$ as a one-hot encoding of the kth variable $x_k$ at sampling time t, the Markov forward process (cf. the general description in Section 2.2) can be written as\n$q(x_{t+1}|x_t) = \\prod_{k=1}^D x_kQ_{t+1},$ (5)\nwith $Q_{t+1}$ being a transition matrix, and $q(x_{t+1}|x_t) = \\prod_{k=1}^D q(x^k_{t+1}|x^k_t)$. The matrices $Q_{t+1}$ are chosen so that the stationary distribution (q(x) for large T) is a simple distribution (we discuss this choice in Section 3.5). The variables $x^k_t$ are assumed conditionally independent given $x_{t+1}$ in the backward process, i.e., $p_\\theta(x_t|x_{t+1}) = \\prod_{k=1}^D p_\\theta(x^k_t|x^k_{t+1})$, and as the backward distribution $q(x|x_{t+1}, x)$ can be computed exactly, the backward process $p_\\theta(x^k_t|x^k_{t+1})$ is parametrized as a marginalization over all possible $x^k_t$,\n$p_\\theta(x_{t+1}) = \\sum_{x} q(x|x_{t+1}, x) p_\\theta(x|x_{t+1}).$ (6)\nIn other words, to use this framework, it is necessary to determine a suitable noise process (i.e., choosing the matrices $Q_{t+1}$), and construct and train a model which can predict the \"clean\" variable x, given a noisy sample $x_{t+1}$ (i.e., the model $p_\\theta(x|x_{t+1})$)."}, {"title": "WyckoffGNN - Neural Network Backbone", "content": "For the parametrization of $p_\\theta(x|x_{t+1})$, we design a novel neural architecture, WyckoffGNN, that takes a \"noisy\" data point $X_{t+1}$ as input, and outputs D different probability vectors, where D is the number of variables. This means that for the Wyckoff representation in Equation (3), the neural network needs to predict the probabilities for D = $|L_\\infty(s)| \\times N_a + |L_\\circ(s)|$ different categorical distributions. To do this, we view each Wyckoff position in L(s) as a node in a fully connected graph. As different space groups have different number of Wyckoff positions, using the graph"}, {"title": "The Choice of $Q_t$", "content": "Austin et al. (2021) proposes a few different choices of $Q_t$. In our work, we use a matrix of the form\n$Q_t = (1 - \\beta_t)I + \\beta_t 1m^T,$ (8)\nwhere $\\beta_t$ is given by some user-defined schedule, I is a vector of ones, and m is a vector of probabilities. With this transition matrix, a variable stays in its current state with probability 1-$\\beta_t$, and with probability $\\beta_t$ it transitions to a new state sampled from a Categorical(p = m) distribution. This is a general form for which the choice m = 1/D gives rise to D3PM-uniform by Austin et al. (2021). In this general form, for large T, the limiting distribution q(x) becomes Categorical(p = m), and sampling from D3PM hence starts by sampling each variable from this distribution. Although using the uniform distribution could work, in case the data is very \"sparse\", for example in our case where most of the elements in the matrix representation in Section 2.1 are 0, using the uniform distribution as the limiting distribution could require many generation steps just to find the correct level of \"sparseness\". Vignac et al. (2022) propose to use the empirical marginal distribution instead of the uniform distribution as m. As we show in the experiments section, we find that using a marginal distribution, or a Dirac distribution at zero for all variables (i.e., starting from a material without any atoms at all), greatly improves the performance compared with using the uniform distribution."}, {"title": "Evaluation Metric - Fr\u00e9chet Wrenformer Distance", "content": "To evaluate a generative model, we strive to find a way of projecting materials into some lower-dimensional space, and draw conclusions about the difference between generated materials and real materials in this space. To do this, we take inspiration from the Fr\u00e9chet Inception distance used for image generation Heusel et al. (2017), and propose the metric Fr\u00e9chet Wrenformer distance (FWD). This metric computes the Wasserstein distance between Gaussian distributions"}, {"title": "Numerical Evaluations", "content": "4.1 FWD, Novelty, and Uniqueness\n The quantitative evaluation of our models uses the WBM dataset (Wang et al., 2021) created by substitution of chemical elements in the crystal structures available from the Materials Project (MP) Jain et al. (2013) to generate a total of 257k materials. We set aside 10k+10k materials as validation and test sets. We start by comparing WYCKOFFDIFF with CDVAE Xie et al. (2022), DiffCSP++ (Jiao et al., 2024), and SymmCD Levy et al. (2024) as they constitute examples of models that to different degrees model crystal symmetry. Implementation details of these baseline methods can be found in Appendix B. It should be noted that we encountered some numerical issues during generation with SymmCD, resulting in NaN values, and we chose to discard these failed materials (~4% of samples, see more details in Appendix B). We also found that using WYCKOFFDIFF with uniform initialization can produce a small amount (\u2264 0.05%) of \"void\" materials with 0 atoms, which we also discarded.\n As the focus of our work is on the generation of protostructures and the compared methods all generate full geometries, we convert these materials to AFLOW protostructures Mehl et al. (2017) using aviary, with default tolerance parameters. For all methods, we generate 10000 protostructures and compute the FWD, novelty (Nov., the fraction of generated protostructures not present in the training set), and uniqueness (Uniq., fraction of unique protostructures among the generated). These results are presented in Table 1. It should be noted that, in this discrete setting, we do not expect the novelty to be 1 even for a \"perfect model\". However, in a practical materials discovery setting we are mainly interested in the novel materials"}, {"title": "Prototype Uniqueness", "content": "In Section 4.1, materials were classified as different if their protostructures were different. Now, we consider only the prototypes to evaluate the models' abilities to generate structural novelty. Among the 10000 novel protostructures, we count the number of unique and novel prototypes and present this in Table 2. We see that our model indeed generates new prototypes, which highlights that it is not merely learning a \"substitution-algorithm\", where it learns to use an already know structural template (i.e., the prototype) and just replace the elements. We also see that only CDVAE performs better in this regard, but as CDVAE has no restrictions in its generation, this is expected. However, when comparing to DiffCSP++ and SymmCD which do take symmetry into account, WYCKOFF DIFF produces significantly higher number of unique and novel prototypes, showing its promise as a general generative model for crystal structures."}, {"title": "Wren Energies", "content": "To further investigate the protostructures generated by WYCKOFFDIFF and get a sense of their usefulness, we compare the formation energies (i.e., the energy required to form a material from the pure elements, see Appendix D for more details) of the generated protostructures with those of the training set. To compute"}, {"title": "Materials Discovery Using WyckoffDiff", "content": "We now demonstrate how WYCKOFF DIFF fits into a materials discovery pipeline. Starting with a generation of 20 000 novel crystal structures, 10000 from each of two WYCKOFFDIFF models (WYCKOFFDIFF-zeros and a previous iteration of WYCKOFFDIFF-marginal; see supplementary material Appendix E.3), we extract structures with chemical elements that are not noble gasses and where the underlying computational methods used for the training data are known to be more reliable, i.e., elements from the s-, p-, and d-blocks of the periodic table of elements.\n We then realize the resulting 12650 protostructures into crystal structures by a process where we first semi-randomly assign values to the degrees of freedom of the Wyckoff positions using the Pyxtal library (Fredericks et al., 2021) using the implementation in aviary. Subsequently, we use the interatomic potential MACE5 (Batatia et al., 2023) to perform a constrained relaxation where the energy is minimized while the symmetries set by the protostructure are retained. We repeat this process of realizing and relaxing crystal structures until the two lowest energies seen lies within a small cutoff of 0.01 eV/atom. The lowest energy found is taken as our computationally predicted energy of the material generated by WYCKOFFDIFF. As is common in materials science, this energy is converted into a formation energy by for each atom subtracting the corresponding energy per atom from a representative elemental solid.\n Low formation energies are only indirectly related to stability; the thermodynamically stable material at a composition is the one with the lowest formation energy compared to all alternative competing phases and linear combinations of phases, which spans the so called convex hull of thermodynamical stability (see, e.g., Bartel et al. (2020) and Appendix D for more details). However, given the indirect relationship, we selected 200 structures with the lowest formation energies to investigate further. We used the high-throughput toolkit (httk) Armiento (2020) to recalculate them with density functional theory (DFT) using the VASP electronic-structure software Kresse & Hafner (1994) and evaluated their stability relative to the known convex hull from all materials in the MP (Jain et al., 2013) and WBM (Wang et al., 2021) databases (further details in E.2).\n Out of the 200 selected materials, we highlight three hand-picked examples with interesting chemistries"}, {"title": "Discussion & Conclusions", "content": "In this paper we propose WYCKOFFDIFF, a novel generative model which leverages a new representation of the symmetrical aspects of materials together with a novel neural network architecture and discrete diffusion to generate new protostructures. Although obtaining the full material requires extra steps, viewing the protostructure and the full geometry as separate processes opens up the possibility of using models tailored for each respective task, and use of computational effort where it is most needed. As we highlight with our proof-of-concept materials discovery pipeline in Section 5, the precise geometry can be uncovered via a pretrained generally applicable interatomic potential such as MACE, only for the most promising materials. WYCKOFF DIFF shows competitive performance compared to the current state-of-the-art both in terms of novel generated materials/min, structural novelty, and agreement with the data distribution based on the newly proposed Fr\u00e9chet Wrenformer Distance."}, {"title": "WyckoffGNN Details", "content": "A.1 Architecture\n Here we give some more details on our neural network backbone, WyckoffGNN. As mentioned in the main text, it is based on the message-passing neural network framework Gilmer et al. (2017), where each node in a graph is represented by a vector h, and each layer corresponds to an update of this representation according to\n$m_i^{l+1} = \\sum_{j\\in N(i)} M_l(h_i^l, h_j^l),$ (9a)\n$h_i^{l+1} = U(h_i^l, m_i^{l+1}).$ (9b)\nAlgorithm 2 describes the full pass through the network. It makes use of Embedding() layers which maps discrete features, like the atom types or number of atoms of a certain atom type, to vectors in some vector space Rd, and Linear() which are affine maps of vectors in $R^{d_{in}}$ to $R^{d_{out}}$, i.e., Linear(x) = Wx+b. The embedding of the number of atoms embeds the number of atoms of each atom type in z into a scalar which are concatenated and then processed by a linear layer such that all initial representations $h^\\circ$ of all Wyckoff positions are of the same dimension.\nAlgorithm 3 describes the update of the hidden representations as in Equation (9). As we are working on a fully connected graph, the sum over the neighbors is over all positions. In our case, the input to $M_l$ is not the hidden representations $h_i^l$ and $h_j^l$, but concatenations of the hidden representations and its corresponding position vector $h_i^{pos}$ which contains some general information of the Wyckoff position like the number of degrees of freedom, the letter, but also the space group and sampling timestep t. Algorithm 4 outlines how $M_l$ is computed.\nA.2 Choice of $\\beta_t$\nAs a scheduler for $\\beta_t$, we used the cosine scheduler by Hoogeboom et al. (2021). By defining $a_t = 1 - \\beta_t$ and $\\bar{a}_t = \\prod_{s=1}^t a_s$, we choose $\\beta_t$ such that\n$\\bar{a}_t = \\text{cos}^2 \\bigg( \\frac{t/T + \\epsilon}{1 + \\epsilon} \\frac{\\pi}{2}\\bigg ),$ (10)\nwith $\\epsilon = 0.008$.\nA.3 Hyperparameters and Training Details\nTraining of a model required approximately 38 hours on a single NVIDIA A100. Hyperparameters for WYCKOFFDIFF and its training can be found in Table 3. The activation function SiLU Ramachandran et al. (2017) is"}]}