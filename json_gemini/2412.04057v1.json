{"title": "From Code to Play: Benchmarking Program Search for Games Using Large Language Models", "authors": ["Manuel Eberhardinger", "James Goodman", "Alexander Dockhorn", "Diego Perez-Liebana", "Raluca D. Gaina", "Duygu \u00c7akmak", "Setareh Maghsudi", "Simon Lucas"], "abstract": "Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games. In this work, we explore the potential of LLMs to directly synthesize usable code for a wide range of gaming applications, focusing on two programming languages, Python and Java. We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs. For Python, the framework covers various game-related tasks, including five miniature versions of Atari games, ten levels of Baba is You, an environment inspired by Asteroids, and a maze generation task. For Java, the framework contains 12 games from the TAG tabletop games framework. Across 29 tasks, we evaluated 12 language models for Python and 8 for Java. Our findings suggest that the performance of LLMs depends more on the task than on model size. While larger models generate more executable programs, these do not always result in higher-quality solutions but are much more expensive. No model has a clear advantage, although on any specific task, one model may be better. Trying many models on a problem and using the best results across them is more reliable than using just one.", "sections": [{"title": "I. INTRODUCTION", "content": "Before the emergence of large language models (LLMs) for code [1], program synthesis in imperative or object-oriented languages like Python or Java was considered highly challenging due to the combinatorial explosion of the search space [2]. Therefore, most solvable tasks were restricted to simple problem domains such as string manipulation or list sorting, typically implemented within a predefined domain-specific language (DSL) [3]. Similarly, program synthesis for games was limited to simple problems with well-defined search spaces, achievable only by incorporating high-level game-specific concepts into the DSL [4], [5], [6], [7].\nThe use of program synthesis with high-level programming languages in game research has hardly been explored. Most discussions merely outlined its potential applications [8], or focused on the missing aspects of automated game development systems to move from game description languages to programming languages [9].\nRecently, methods for LLM-based program search have been introduced for the automatic design of playable games based on program code [10], [11], [12] and to generate game content through JSON representations [13]. LLMs have also been adapted to synthesize programmatic policies in Python, which are then converted into a DSL suitable for the target environment [14], as well as to construct world models in Python that approximate reward and state transition functions for simple games, enabling action plan generation [15].\nIn this work, we explore the potential of LLM-based program search for a wider range of games without depending on a predefined JSON converter [13] or on predefined specifications such as a DSL (e.g., Ludii [10], the video game description language [11], or Karel [14]). Our aim is to enable LLMs to synthesize program code that can be used directly, without requiring additional transformations or prior specifications. We evaluate this approach across different domains using two programming languages: Python and Java. In Python, we focus on synthesizing programmatic agent policies and functions for procedural content generation (PCG). In Java, the method is integrated into TAG, a tabletop games framework, in which LLMs design heuristics for board games [16].\nOur goal is not to propose a new method for program synthesis, but to introduce an easy-to-use and extensible framework to evaluate the current performance of LLMs for the synthesis of game-related program code. To achieve this, we have integrated five different LLM architectures for Python and four for Java, and evaluated 12 and 8 models, respectively. For the synthesis of Python code, the framework consists of five miniature versions of Atari games where the input is represented symbolically [17], ten levels of the game Baba is you, in which various game mechanics are tested [18], a vehicle driving environment based on the game Asteroids, and procedural content generation in the form of mazes. For Java, the framework consists of 12 tabletop games of the TAG framework [16]. In total, we evaluate the LLMs on 29 different tasks. An overview of our proposed framework, as well as games and LLMs used, is shown in Figure 1.\nOur contributions are:\n\u2022 We perform an empirical study to evaluate the current state-of-the-art of LLM-based program search for games.\n\u2022 We introduce an easy-to-use and extensible framework with 29 tasks that evaluate various aspects of game mechanics.\n\u2022 We open-source our code upon publication. Currently, only the example prompts for the experiments are available in the repository\u00b9."}, {"title": "II. RELATED WORK", "content": "There are a considerable number of studies that use program synthesis approaches for games. Butler et al. used SMT-solvers to search for programs within a Lisp-based DSL, enabling the generation of diverse boss fights in Megaman [4] and puzzles for the game Nonograms [5]. In [6], a method was introduced for learning combinations of logical programs to solve simple grid-based games like Nim. Cropper et al. [19], [20], [21] developed a comprehensive benchmark of 50 games to recover game rules from gameplay traces using inductive logic programming (ILP). Furthermore, Evans et al. [22] applied a differentiable form of ILP to learn interpretable rules for Sokoban. Recently, a method for learning programmatic policies for zero-shot coordination problems in cooperative tasks was introduced and demonstrated in the game Overcooked [23]. In contrast to learning programmatic policies, there is also work focusing on using program synthesis to explain the decision-making process of game-playing agents [24].\nMari\u00f1o et al. [7] utilized program search to develop strategies for the game MicroRTS, comparing the resulting programmatic policies with those created by human developers. Their findings demonstrated that the synthesized programs performed comparable to those written by humans. Subsequent research built on this foundation by introducing improved search techniques, including bilevel search [25], by guiding the program search [26] or by searching in semantic spaces [27]. Recently, an approach for combining LLMs with local search algorithms was proposed for MicroRTS [28], where the authors showed that providing initial programs with LLMs found better solutions faster and improved the performance of the final programs.\nIn [29], Genetic Programming (GP) is used to search for evaluation functions within a predefined DSL for the board game 7 Wonders. This approach resembles our experiments with the TAG framework, where heuristic functions are synthesized; however, we use Java without relying on predefined concepts. GP has also been applied to generate game agents for various scenarios, including a fighting game [30], a platformer game [31], a puzzle game [32], and to create explanations for a maze runner agent [33]. Additionally, Wilson et al. [34] used Cartesian GP to develop programs for Atari games, processing pixel observations through predefined mathematical, statistical, or list functions.\nA recent approach from cognitive science, known as language-informed thinking [35], combines large language models (LLMs) with Bayesian inference. This method enables LLMs to pose questions in natural language, which are then translated into a language of thought [36] represented as a probabilistic programming language. Grand et al. extended this approach to the board game Battleship, demonstrating that the questions generated by LLMs aligned closely with the performance of human players [37].\nVerma et al. [38], [39] employed neurosymbolic methods to synthesize programmatic policies for a car racing game, demonstrating that these programs were more robust than neural network policies while achieving comparable rewards. While this approach shares similarities with the vehicle driving experiments in our work, it is more constrained, as the search space is limited to the provided DSL and our vehicle driving problem requires a planning algorithm to be solvable.\nIn [40], a reactive programming language with a novel program synthesis algorithm is introduced to discover causal structures represented as state machines in combination with program code. They evaluate the proposed method for 2D grid games similar to Super Mario.\nVoyager [41] is a lifelong learning agent for the Minecraft environment that uses an LLM to synthesize code in the Mineflayer API\u00b2, which is then executable to obtain the actions. In addition, a second LLM is used as a high-level planner to create a task curriculum for the agent. Moreover, Ma et al. [42] proposed an evolutionary approach using LLMs to synthesize reward functions for complex control tasks, achieving superior performance compared to human-engineered reward functions.\nThe key distinction between our work and the discussed literature is the use of high-level programming languages"}, {"title": "III. FRAMEWORK", "content": "The general framework we proposed is based on an evolutionary hill-climbing algorithm where the mutations and the seed of the initial program are performed by an LLM [14], [43]. Thus, our framework belongs to the group of neurosymbolic programming methods [44], as we use an LLM to generate programs that are checked for correctness and functionality by symbolic verifiers, in our case the Python interpreter and Java compiler. The overview of the framework is illustrated in Figure 1, which shows the high-level interaction between the different modules and processes. Synthesized program code by LLMs is always executed within a safe subprocess environment, ensuring that the main process can terminate it after a certain time limit to prevent infinite execution of the program.\nA detailed description of the complete algorithm is provided in Algorithm 1. Our framework consists of two iterative processes that control the length of the search, defined by the input parameter iterations, as well as the number of attempts to generate, repair or improve the program in each iteration defined by the input parameter maxAttempts.\nEach iteration starts by generating a task prompt to obtain an initial Python or Java function, and inject and run the code in a subprocess. Afterwards, the inner iterative process starts, where the task prompt is updated and the program is repaired or improved. If the function is executed successfully, we update the prompt with the achieved evaluation metric and all relevant environment-specific details, such as the action trace of the executed function. If an error occurs, e.g. a syntax problem, a runtime error due to improper array indexing or similar problems, the error description is included in the prompt for refinement of the program. These steps are repeated iteratively until the evaluation criteria defined by the fitness function are satisfied or the specified number of maxAttempts is reached.\nThe outer loop is able to stop the current program search and restart from the initial prompt or the last successful program found. In order to achieve this we introduce the variable blankRestart, which is an input parameter to Algorithm 1. This is necessary when the LLM tries to fix compilation errors in the case of Java or goes astray, such as when generating DQN code for the Atari environments, which we experienced in the preliminary experiments. This also depends on the problem domain, therefore blankRestart is adapted to the corresponding domain.\nWe explain the domain-specific adaptations of the framework in the respective chapters in section V. While the overall framework is similar for all tasks, domain-specific adaptations are necessary, such as the description of the environment or the game logic, as well as the objective of the game."}, {"title": "IV. LARGE LANGUAGE MODELS", "content": "For our benchmark, we integrated five LLM providers for Python and four for Java in our framework, namely the small and the large version for each model type. From OpenAI, we utilize models from the GPT-4o family\u00b3, based on GPT-4 [45]. We also incorporate the latest models from Mistral4, Claude 3.55 from Anthropic, based on Claude 3 [46], and the latest Gemini models [47], provided by Google, for both programming languages.\nModels from the Llama 3.1 family [48] are included only for Python tasks, as they are not supported by the LangChain4j library, which we integrated into the TAG framework. For the new ChatGPT models in the o1 generation, we use o1-mini,"}, {"title": "V. GAME APPLICATIONS", "content": "In the following section, we describe the experiments that were conducted for each of our target domains.\nA. Programmatic Policies: Minatar\nMinatar [17] is a collection of five games that are miniature versions of Atari games. In Minatar, the games are represented as a symbolic state space on a 10\u00d710\u00d7n grid, where n represents the number of channels, and each channel represents an object such as walls, enemies or the agent. Minatar is an ideal test bed for experiments, as the games are more efficient to learn without changing the game mechanics of the original game. Previously, Minatar was used in [24] to explain the behaviour of agents through program synthesis, but it was only possible to explain short sub-trajectories since enumerative search-based methods were used to search through a predefined domain-specific language that resembles Lisp. In our experiments, we use all available Minatar environments, which are shown in Figure 2. The game descriptions are outlined below:\n\u2022 Seaquest: In this game, the agent controls a submarine and is able to shoot bullets. The objective is to save as many divers as possible, while also shooting enemy submarines or sharks. Each time an enemy is struck, the reward is increased by one. When the submarine saves the divers, the agent also receives a reward.\n\u2022 Freeway: The agent controls a chicken that needs to cross a road during rush hour, while avoiding the traffic. For each chicken that crosses the road safely, the agent receives one point.\n\u2022 Asterix: The objective of the game is to collect gold while avoiding enemies. The player gets one reward for each collected gold and the game is over when the player is hit by an enemy.\n\u2022 Space Invaders: The agent controls a cannon and shoots aliens while dodging bullets launched from the alien spaceship. Additionally, the player must prevent the aliens from reaching the bottom of the screen. For each destroyed alien, one reward is received.\n\u2022 Breakout: The goal is to destroy all the bricks with the ball by controlling the paddle to bounce the ball off before it goes out off the screen. With each destroyed brick the agent receives a reward of one.\nThe LLMs were prompted to generate a Python function which can be used as a policy to play the game. The prompt contains information about the game rules, the objective of the agent and also the possible actions of the environment and available game objects. The description of the game, was taken from Young and Tian [17]. The prompts for the games are available in the code repository. The LLM receives only the initial state, which is preprocessed from the state input of the environment, a one-hot encoded 3D array, into a 2D array with text descriptions for each grid cell representing the cell's object. Figure 3 shows an example of the converted state for Breakout. All other games convert the state in a similar way so that the LLM can process the state input semantically.\nEach of the games tests the LLM for different game concepts. Space Invaders, Breakout and Freeway restrict the agent's movement by only allowing horizontal or vertical movement. Space Invader and Seaquest allow the player to fight the enemy, while in Asterix and Freeway the player can only avoid the enemies. In Asterix, the player must also collect items in order to receive a reward. Seaquest is the most difficult game, as the player has to collect six divers and then reach the surface so that the divers can leave the submarine, but at the same time the player has to shoot down enemies. Breakout, on the other hand, is one of the easier games as there are no opponents and the player only has to anticipate where the ball will land in order to bounce it off with the paddle.\nFor all Minatar experiments, we use 10 iterations with max three attempts in each iteration, which results in max 30 prompts for each model. For each program, 50 evaluation episodes were performed. Table II shows the average reward of all executable programs for each LLM with the number of successful iterations in the brackets an executable program that returns a positive reward. In general, the larger models perform better than their smaller counterparts in terms of average reward, with Claude Sonnet being the best performing model. Only at Freeway the Claude Haiku and Llama 70B models beat their larger counterparts in the family. 01-mini also outperforms GPT 40 in Space Invader and Seaquest. This is, however, not represented by the maximum reward shown in Table III. Only in Breakout, Claude Sonnet is the top-performing model regarding the average and maximum reward, but is beaten in three games in terms of the maximum reward by Claude Haiku. This pattern is also visible in the other LLM families where the small model outperforms their larger version.\nIt can also be seen from both tables that it is more difficult to find good programs for more complicated games such as Seaquest. Only o1-mini, which is praised by OpenAI for its sophisticated reasoning capabilities, performs well in this game, but fails to beat the other LLMs on the simpler games. Looking at the best programs for each LLM, 01-mini manages to correctly locate enemies and shoot them if they are in a line, while the other programs only check if enemies are nearby without checking if they are in a line. o1-mini is also the only model that uses the Manhattan distance to move to nearby enemies, while all other programs only try to shoot enemies or rescue divers.\nFor Freeway, all of the best performing programs show similar behavior for each LLM, while the poor performing models struggle to correctly implement a one step lookahead (OSLA) of the cars. The best performing program of Mistral Large is the only program that uses the modulo operation to correctly predict when the car will be teleported to the other side when it drives out of the screen, which was mentioned in the prompt.\nFor Breakout, the best programs of Mistral Small and Llama 8B only receive points by chance, as they return a random action or do not take the position of the ball into account. All other programs are able to locate the ball and also the direction in which the ball is moving and move the paddle in that direction. Claude Sonnet is the only model that correctly uses OSLA to predict the next column of the ball. GPT 40 also uses OSLA, but confuses the row with the column.\nIn Asterix, the best performing model Claude Haiku prioritizes the gold, but also checks whether it is moving towards an enemy as it approaches the gold. The mediocre performing models often prioritize the gold without checking if there are enemies nearby. 01-mini, instead, uses action values that are updated depending on nearby gold coins and enemies, but is still not able to beat Claude Haiku.\nFor Space Invader, the good performing programs with a reward over 20, correctly locate enemy bullets, aliens and the cannon. The programs below 20, do confuse sometimes the column with the row or only take into account the first alien found in the state. The smallest LLM, Llama 8B, produces a program that fires when there are no friendly bullets, i.e. bullets shot by the program itself, in the state, and was still able to get a reward of 5.9 even though the program makes no sense at all.\nOverall, it can be said that in the Minatar games larger models in most cases show a more sophisticated behavior in the programs, but as can be seen with Claude Haiku, this is not always the case. Currently, only a very simple prompting strategy is used, which already gives good results in most games. Using more complicated prompting strategies, such as Chain of Thought [49] or adding a crossover operator could lead to improvements in the programs found."}, {"title": "B. Vehicle Driving", "content": "The task is to pilot an Asteroids-style spaceship from its start state to the target, where it should rest until the end of the episode. Each episode is 101 steps. At each step, there are 4 discrete actions: NO_OP, THRUST, ROTATE_LEFT, ROTATE_RIGHT. We experimented with vehicle physics in order to make an interesting challenge. Drag is set to be low, which leads to a high risk of overshooting the target unless countermeasures are taken. At each step, the agent is given an observation of the ship state and the position of the target.\nThe prompt includes some helper classes and functions, including a Vector2d class and the Asteroids ship, as well as a Vehicle superclass. In addition, we add strong hints to make the problem solvable for LLMs, which are summarized as follows10:\n\u2022 Best solved using search algorithms: try One Step Lookahead, Monte Carlo Tree Search or Rolling Horizon Evolution.\n\u2022 Try using a heuristic function that values facing towards the target as well as being close to the target.\n\u2022 Try using Macro-actions e.g. simply repeating each action a number of times.\nTable IV shows the results of the driving experiments for different rotation speeds w to adjust the difficulty level of steering the asteroid ship. The numbers are the minimum distance achieved by the best program for five evaluation episodes. $D_{avg}$ is the average of all distances for each LLM. The experiments were conducted with 10 iterations of search and three attempts to generate or improve the program. We omitted the number of successful iterations because no LLM managed to stop the asteroid ship at the target position in all five evaluation episodes. A program was considered successful only if it could consistently stop the vehicle within a specified tolerance t across all evaluation episodes. In our experiments, the synthesized programs succeeded in stopping the vehicle in only one or two episodes within a tolerance of t = 10, and thus no program qualified as successful. The overall best model is Llama 3.1 405B followed by Claude Sonnet. Regarding $D_{avg}$ larger models generally outperformed their"}, {"title": "C. Baba is You", "content": "Baba is you is a complex puzzle game in which the player manipulates a 2D grid environment to reach a given goal. The environment consists of word blocks and corresponding entities that can be pushed around. By placing word blocks next to each other, rules can be formed. These rules are active as long as the given word block sequence remains intact. This way, players can change how objects behave, which objects they control, or which conditions must be satisfied to win.\nFor our experiments, we used a Python version\u00b9\u00b9 of the Keke is You AI framework [18]. For this domain, we prompted the LLMs to provide a policy, giving a short description of the game and the initial state of the level (the complete prompt is given in the repository). Similar to the Minatar experiments, the state is converted into a text description. The function to be written should use the current state as input and return a movement direction or the command for waiting a turn. Each episode ends after 100 actions or once the win condition is fulfilled. A reward is awarded based on the maximum number of actions (100) minus the number of steps taken. Thus, the return can be maximized by finishing the level as fast as possible. Each level can be solved in less than 20 actions.\nIn our tests, we queried the agent to solve 10 simple demo levels (see Figure 4). Each of the levels focuses on one or multiple key mechanics of the framework such as rule interpretation (levels 1-10), rule creation (levels 2, 3, 5) or destruction (levels 6, 8, 9, 10), and object manipulation (level 7). Table V shows the results of our comparison of the LLM models' capabilities. Each entry shows the reward of the best program after 10 iterations, in which each iteration used 3"}, {"title": "D. Procedural Content Generation", "content": "PCG is a widely studied area in game research [50], [51]. In this experiment, we explored whether LLMs can synthesize Python functions capable of generating diverse game content.\nTo assess this in a simple scenario, we tasked the LLMs with creating functions that produce random mazes adhering to specific design objectives.\nThe prompt advised the LLMs to use the longest shortest path objective to guide the maze generation process. This objective encourages intricate and interesting mazes. Most of the generated code ignored the hint and instead coded overly simple algorithms, placing corridors and walls in each cell with a given probability while usually ensuring that the start and end points were not on wall cells. An example generated maze is shown in the left of figure 5. Occasionally, a better algorithm was produced that mixed randomness, recursion and graph search in ways we've not fully analysed. These algorithms sometimes produced mazes with no path between start and end, resulting in a score of -1. When they worked, they often produced reasonable mazes such as the one shown in the middle of Figure 5. The LLMs failed to find an algorithm as effective as an evolutionary algorithm applied to directly solve the objective. A sample maze from such an algorithm is shown on the right of the figure.\nNote that here we are evaluating the effectiveness of the algorithms in meeting the specified objective, which is to produce mazes with the longest shortest path between start and end. Depending on the application, this could be a poor objective to maximise, with the best mazes have a mid-ranking score, such as the central maze in Figure 5.\nTable VI shows the results for the maze generation experiment. In contrast to the previous experiments, all small models beat their larger counterparts. The only large model that is on par with the smaller ones is GPT 40, which loses slightly to GPT 40 mini regarding the average distance $D_{avg}$. 01-mini also falls behind both GPT 40 models. The worst model is Gemini Pro, which fails to even connect the start and end points of the maze most of the time, resulting in a negative average reward, while always producing executable programs. Even the smallest LLM, Llama 3.1 8B, which was one of the worst models in the previous experiments, is better than the larger Llama models in terms of $D_{avg}$ and only loses by a small margin to the largest Llama model in terms of $D_{max}$. It should be further investigated why larger LLMs have difficulties in generating good mazes, but this is beyond the scope of this paper."}, {"title": "E. Python Code Evaluation", "content": "Table VII shows the summary statistics of the synthesized Python code for the previous experiments. The two smaller"}, {"title": "F. Tabletop Games Framework (TAG)", "content": "The TAG framework is a bespoke Java research framework that supports the implementation of multiplayer tabletop board games. This introduces a number of new challenges:\n\u2022 The games are in general more complex than the simple one-player games in previous sections.\n\u2022 Related to this, they are also inherently multiplayer. As such there is implicit opponent modeling required for good play strategies. The environment is no longer a 'simple' stationary MDP, but is actively adversarial.\n\u2022 The TAG framework has a number of local libraries and coding conventions; for example decks of cards are implemented via Deck<> or PartialObservableDeck<> parameterised classes. These are not likely to be present in the LLM training data to any degree, and require the LLM to generalise to unseen software architecture details. This contrasts to the straightforward Python with mostly standard libraries of the games in earlier sections.\nTable VIII: TAG results by game. Key as for Table IX, plus P is the number of players, Best Agent records the model that won the round robin tournament. SM is the number of models that produced working code on at least one iteration and entered an agent in the round robin tournament. BB indicates if the best agent significantly Beats the Baseline agent (OSLA or MCTS); \u2248 means performance matches the baseline.\n\u2022 The language used is now Java. Integration of all language models used langchain4j.12\nAlgorithm 1 was applied to 12 tabletop board games (see Table VIII for the full list) implemented in TAG. These are multi-player environments (2 to 4 players) with partial observability and stochasticity and varying levels of complexity. Given the additional level of complexity of these games, and very different (and often dynamic) action spaces for each game, the language models were not tasked with writing a full policy to play the game. Instead they were tasked with writing a heuristic function to estimate the value of a game state for a player. This should be close to 1.0 for a position that is a definite win, to 0.0 for a position that is a definite loss. This heuristic function was then used within a search algorithm; either one step lookahead (OSLA) or Monte Carlo Tree Search (MCTS).\nEach of these games has very different rules and implementations in TAG. To achieve the target of a scalable system that required no hand-writing and tuning of LLM prompts for each new game, two new TAG-specific elements were implemented to augment the process:\n1) Automatic extraction of the game-specific APIs. This uses Java Reflections to extract information on the methods and associated Javadoc on the game state object. The entry point for this is the Class name of the main game state. All public information gathering methods on this are extracted (defined as names matching on either get* (...) or is*(...). APIs for any class dependencies on these methods, as parameters or return values, are also extracted and this recurses until the core java libraries are reached (these are excluded).\n2) Automatic rulebook digestion. This takes as input the PDF of the game rulebook. An approach inspired by [52] is used. The rulebook is first broken down into chunks of 1000 or 2000 words. The LLM is then given each chunk in turn and asked to summarise in 200 words or less the information about the game rules. This final set of synopses is then fed to the LLM with a prompt to, 'Summarise this information in 500 words or less.'. This provides an blocks of text to include in the prompt used in the main loop of Algorithm 1 that explains the rules of the game.\nThese new tools enable a scalable and game-agnostic process to be run on all games. The input for each game is the game rulebook as a PDF file, and a Java Class name for the main game state. Additionally, the methods on the main game state were briefly reviewed for meaningful Javadoc comments, public visibility and name convention to ensure that they were picked up by the automated API process. An example full prompt (for Sushi Go!) is included in the code repository.\nThe multi-player nature of these environments also necessitates a change in the evaluation criterion in Algorithm 1. Evaluation used a tournament of 500 games. The base agent in the tournament was either a one step lookahead (OSLA) agent (for Tic-Tac-Toe and Virus) or a vanilla MCTS agent (all other games) with a budget of 10ms and a rollout of 10 actions before the generated heuristic function is applied.\nA base opponent used a heuristic function of the game score (for all other games). All games in Table VIII have the concept of score except for Tic-Tac-Toe and Connect 4, which only reward a win (+1) or draw (+0.5). To avoid overfitting to a specific opponent, later iterations within a run of Algorithm 1 add all previous (working) agents to the evaluation tournament. The evaluation score of each generated heuristic is the win rate from the most recent tournament, so this is updated to include a broader range of opponents later in the run.\nFor each game a final tournament is then run between the best agents from each model, for a maximum of 8 participants if all models generated at least one heuristic that compiled and executed successfully. Model performance is then judged by giving 5 points to the winner of each tournament, 4 points to the second-place and down to 1 point for 5th place.\nTable IX summarises the results by language model. Large"}, {"title": "VI. CONCLUSION", "content": "In this work we studied and evaluated the current possibilities of using LLMs for program search in the area of games for various applications. Previous work was mostly limited to a single problem or game without being easily transferable to other domains, as the DSL had to be adapted. We demonstrated that LLMs can overcome the problem of combinatorial explosion of search spaces constructed with predefined DSLs, and that LLMs are able to synthesize programmatic policies in Python for the Minatar domain, which was not possible with a custom DSL and previous methods. Furthermore, we have shown that this framework can be easily adapted to different applications by modifying the prompts, and that it often provides reasonable results even without much customization. We have shown that even with the default temperature settings on these standard language models there is a very wide range of output for the same input prompt; in this respect at least the models can be quite 'creative'. Running many independent iterations of the same task can create a varied population of outputs. This is very promising as it provides the variation required for the evolutionary hill-climbing approach used here.\nWe observed limitations in the quality of the generated code. For example, in the simple 2D vehicle driving task, the generated code drove the car to the target but then failed to stop most of the time. Much of the generated code fails to run, or in the case of Java, to compile. These limitations become more evident as the complexity of the task increases. The need to use framework-specific Java libraries in TAG leads to less than 1 in 5 attempts generating valid code. We believe limitations such as this could be overcome with more sophisticated search and better prompt engineering, but the results so far give an idea of the limitations of what can be achieved with relatively little effort. The addition of tools to the LLM interfaces and a more agentic workflow is a promising area for this future work. For example instead of asking the LLM to generate the code in one pass, it could be asked to construct useful component functions or sub-modules with documented interfaces. In a later pass the model could then be asked to combine these sub-modules (based on feedback of performance of previous combinations).\nOne general recommendations that can be made is to use a variety of models as there can be huge variability between them on a given task. Given a robust evaluation method, as in the 29 tasks here, the smaller models can be much more cost effective at up to a tenth of the cost of their larger brethren and just as able to produce top quality results if given the time."}]}