{"title": "CAE-T: A CHANNELWISE AUTOENCODER WITH TRANSFORMER FOR EEG ABNORMALITY DETECTION", "authors": ["Youshen Zhao", "Keiji Iramina"], "abstract": "Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CAE-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CAE-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice.", "sections": [{"title": "1 Introduction", "content": "Brain disorders such as Alzheimer's disease, epilepsy, Parkinson's disease have attracted significant research interest due to their profound impact on patients' quality of life and healthcare systems globally [1, 2]. Timely and accurate diagnosis is crucial for effective intervention and management, necessitating reliable tools capable of capturing the dynamic changes in brain activity. Electroencephalography (EEG), a cost-effective and non-invasive method for real-time monitoring of brain function, has become a cornerstone in clinical practice for detecting brain disorders. By measuring electrical activity in the brain, EEG provides valuable insights into neural dynamics, particularly for conditions like epilepsy and Alzheimer's disease, where the identification of abnormal patterns is critical for diagnosis and treatment.\nRecent advances in deep learning (DL) have significantly enhanced the capabilities of computer-aided diagnosis (CAD) systems for EEG analysis. These systems excel at extracting complex, high-dimensional features from raw EEG signals, improving diagnostic accuracy across various applications [3, 4, 5]. However, the inherent challenges of analyzing long-term EEG signals\u2014such as their high dimensionality and susceptibility to noise\u2014necessitate novel approaches for robust and interpretable solutions.\nDL-based CAD systems for EEG analysis can generally be categorized into two paradigms. The first involves preprocessing and feature extraction steps, such as filtering, artifact removal, and transformation into alternative domains (e.g., frequency or time-frequency domains using Fourier or wavelet transforms). These methods reduce noise and enhance clinically relevant features, particularly for noisy signals influenced by muscle artifacts or electrode interference [6, 7, 8, 9]. For instance, Velasco et al. [10] achieved 100% classification accuracy in motor imagery EEG using a multivariate time-series approach combined with discrete wavelet transform (DWT). Similarly, Wang et al. [11] demonstrated a 98.1% detection accuracy for epileptic seizures in long-term EEG recordings using wavelet decomposition and directed transfer functions.\nThe second paradigm bypasses preprocessing by directly inputting raw EEG data into DL architectures like convolutional neural networks (CNNs) or recurrent neural networks (RNNs). These models leverage their ability to learn intricate patterns directly from raw data, achieving remarkable results in various tasks [12, 13, 14, 15]. For example, a CNN-LSTM model proposed by Xu et al. [16] achieved 99.39% accuracy in binary epileptic seizure classification. Similarly, RNN-based approaches have proven effective in tasks like schizophrenia detection, achieving up to 98% accuracy [17].\nIn recent years, transformer-based architectures have emerged as a transformative innovation in DL, excelling in fields like natural language processing and computer vision. Their application to EEG analysis has yielded competitive or superior performance compared to traditional architectures [12, 13, 14, 15]. Notable examples include transformer models for emotion recognition and epilepsy detection that achieve high accuracy by leveraging positional encodings and person-specific embeddings [18, 15]. However, transformers' computational complexity and the \"black-box\" nature of their decision-making process pose significant challenges, particularly for applications requiring lightweight models and interpretability [19].\nThis study introduces a novel framework that integrates a channelwise autoencoder with a lightweight, single-head transformer-based classifier for EEG abnormality detection. The framework seeks to address key challenges in EEG analysis by focusing on efficiency, interpretability, and competitive performance. Specifically:\n1. Efficiency and practicality The framework eliminates the need for complex preprocessing steps, enabling direct processing of long-term raw EEG signals while maintaining low computational costs.\n2. Interpretability The model preserves channel independence, aligning with the spatial structure of EEG signals. This design enhances biological interpretability and lays a foundation for future neuroscience research and clinical applications.\n3. Performance The proposed method achieves competitive classification accuracy while significantly reducing computational costs compared to standalone transformer-based models.\nThis paper is organized as follows: Section 2 details the methodology and architecture of the proposed framework. Section 3 describes the experimental setup, including datasets, preprocessing, and implementation. Section 4 presents the results, with performance comparisons and an ablation study. Section 5 discusses the implications of the findings, and Section 6 concludes with a summary of contributions and future directions."}, {"title": "2 Methodology", "content": "2.1 Task formulation\nOur primary objective is to develop an efficient model for EEG-based abnormality detection, where we aim to accurately classify EEG signals as either normal or abnormal. Given a raw EEG signal $x \\in R^{C \\times T}$, where R represents original dataset, C is the number of EEG channels and T denotes the temporal length of the signal. Notably, in typical EEG recordings, $T \\gg C$, emphasizing the challenge of processing long temporal sequences.\nTo address this, the task is divided into two stages: latent representation learning and abnormality detection. In the first stage, the raw EEG signal x is compressed into a compact latent representation z using an encoder function:\n$z = Encoder(x), \\quad z \\in R^{C \\times D}$ (1)\nwhere $D \\ll T$, ensuring a significant reduction in computational complexity while preserving essential information. In the second stage, the compact representation z is input into a classifier function for final pathology prediction:\n$y = Classifier(z)$ (2)\nwhere y represents the predicted class label (e.g., normal or abnormal EEG). Together, these stages form the backbone of the proposed CAE-T model.\n2.2 Architecture of CAE-T\nThe CAE-T model consists of two main components: a channelwise autoencoder for feature extraction and compres-sion, and a single-head transformer-based classifier for abnormality detection. The overall structure is depicted in Figure 1."}, {"title": "2.2.1 Channelwise Autoencoder", "content": "To efficiently reduce the dimensionality of raw EEG data, CAE-T employs a channelwise CNN as the core of its autoencoder. The design of our autoencoder is inspired by depthwise CNNs [20, 21, 22] and related architectures applied on EEG [23, 24]. While depthwise CNNs apply convolutions channel-wise to feature maps, our design treats each EEG channel independently, maintaining channelwise features without introducing inter-channel dependencies, as shown in Figure 2. This independence is crucial for preserving the natural structure of EEG data, which is essential for neuroscience research and clinical applications.\nGiven a input feature map F with size of $D_M \\times D_T$, where $D_M$ represents width of input feature map, M is the number of input channels, $D_T$ is the spatial length of feature map. The output feature map G for standard convolution with stride one and padding is computed as:\n$G_{k,j,n} = \\sum_{j,m,n} = K_{j,m,n}. F_{t+j-1,m}$ (3)\nStandard convolutions have the computational cost of:\n$Cost_{standard} = D_K . M . N . D_T$ (4)\nGiven the same shape of output feature map for channelwise convolution $\\hat{G}$:\n$\\hat{G}_{k,j,n} = \\sum_{j,m,n}K_{j,.,n}. F_{t+j-1,m}$ (5)\nComputational cost for channelwise convolution is:\n$Cost_{channelwise} = \\frac{D_K.M.N.D_T}{C}$ (6)\nThe reduction in computational cost from standard to channelwise convolution can be quantified by taking the ratio of the two costs:\n$\\frac{Cost_{channelwise}}{Cost_{standard}} = \\frac{ \\frac{D_K \\cdot M \\cdot N \\cdot D_T}{C}}{D_K \\cdot M \\cdot N \\cdot D_T} = \\frac{1}{C}$ (7)"}, {"title": "2.2.2 Single-Head Transformer Classifier", "content": "The transformer architecture, introduced by Vaswani et al [25] in 2017, has become a foundational model in deep learning, particularly for sequence-to-sequence tasks. Its core innovation is the self-attention mechanism, which enables the model to weigh the importance of different elements in a sequence, capturing long-range dependencies without relying on recurrent structures.\nThe transformer consists of an encoder-decoder structure, though in many applications, such as classification tasks, only the encoder is utilized. Each encoder layer comprises two primary components: a self-attention mechanism and a position-wise fully connected feed-forward network.\n1. Single-Head Self-Attention Mechanism To improve efficiency and keep the model lightweight, we replace the multi-head setup with a single-head attention mechanism [26]. Given an input sequence represented by the matrix $X \\in R^{n\\times d}$ where n is the sequence length and d is the dimensionality of each input vector, the self-attention mechanism computes a weighted sum of the input elements, allowing the model to focus on different parts of the sequence, computed as follows:\nLinear Projections The input X is linearly projected to obtain single query Q, key K, and value V:\n$Q = XW^Q$ (8)\n$K = XW^K$ (9)\n$V = XW^V$ (10)\nwhere $W^Q, W^K, W^V \\in R^{d \\times d_k}$ are learned projection matrices, and $d_k$ is the dimensionality of the query and key.\nScaled Dot-Product Attention The attention score is computed by taking the dot product of the query and key, scaling by $\\sqrt{d_k}$, and applying the softmax function:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ (11)\nThis yields a matrix of attention weights that determine the influence of each input element on the others.\n2. Position-Wise Feed-Forward Network Following the single-head attention, a position-wise feed-forward network is applied independently to each position:\n$FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2$ (12)\nwhere $W_1 \\in R^{d \\times d_{ff}}$ and $W_2 \\in R^{d_{ff} \\times d}$ are learned weight matrices, $b_1$ and $b_2$ are biases, and $d_{ff}$ is the dimensionality of the feed-forward layer.\n3. Residual Connection and Layer Normalization To facilitate training and improve convergence, residual connections and layer normalization are employed around each sub-layer:\n$LayerNorm(x + Sublayer(x))$ (13)\nBy employing a single head, we reduce the number of parameters significantly, leading to a more compact and computationally efficient model. The parameter count for the single-head attention mechanism is:\n$Params_{attention} = d \\cdot d_k + d \\cdot d_k + d_k \\cdot d = 3 \\cdot d \\cdot d_k$ (14)\nIn a multi-head setup without compression, the parameter count scales with the number of heads h and the full input dimensionality:\n$Params_{multi-head \\ attention} = 3\\cdot d\\cdot D_{orig}.dk$ (15)\nwhere $D_{orig}$ represents the original signal dimensionality.\nThis streamlined approach, inspired by efficient transformer implementations [22], ensures a balance between computa-tional efficiency and effective modeling of temporal dependencies."}, {"title": "2.3 Integration of CAE-T for EEG Analysis", "content": "The CAE-T framework integrates the channelwise autoencoder and the single-head transformer classifier into a unified model for EEG-based abnormality detection. The autoencoder first compresses the raw EEG signal into a compact latent representation, minimizing redundant information. This representation is then input into the transformer classifier, which leverages self-attention to identify patterns critical for distinguishing between normal and abnormal signals.\nBy combining these components, CAE-T achieves competitive performance on benchmark datasets while maintaining a lightweight design. The model's channelwise feature extraction and efficient transformer architecture make it particularly well-suited for applications requiring both computational efficiency and interpretability."}, {"title": "3 Experimental settings", "content": "This section outlines the experimental design used to evaluate the proposed CAE-T model for EEG abnormality detection, including dataset details, preprocessing steps, implementation specifics, and the baseline models used for comparison."}, {"title": "3.1 Dataset", "content": "The TUH Abnormal EEG Corpus (v3.0.1) was utilized for experiments. This subset of the Temple University Hospital (TUH) EEG Corpus provides a balanced representation of clinically normal and abnormal EEG recordings. A total of 2,993 EEG files in European Data Format (EDF) were split into training and evaluation sets.\n\u2022 Training set: 2,717 recordings (1,371 normal, 1,346 abnormal) collected from 2,130 subjects.\n\u2022 Evaluation set: 276 recordings (150 normal, 126 abnormal) involving 253 unique subjects.\nSegments exceeding 15 minutes were selected for analysis to ensure high-quality data. The dataset also included diverse demographic representations, encompassing variations in age and gender, facilitating robust generalization tests for the model."}, {"title": "3.2 Data preprocessing", "content": "To address EEG-specific challenges, such as noise and high dimensionality, the following preprocessing steps were applied:\n1. Downsampling: EEG data was resampled to 100 Hz, maintaining sufficient resolution for frequencies up to 50 Hz (aligned with the Nyquist theorem [27]).\n2. Segmentation: EEG recordings were divided into non-overlapping, 2-minute segments to standardize input lengths and optimize batch processing.\n3. z-Normalization: Signal amplitudes were normalized to reduce variability caused by electrode placement or individual physiology.\nThe preprocessed data retained clinically relevant information across all major EEG bands (\u03b4: 0.5-4 Hz, \u03b8: 4-8 Hz, \u03b1: 8-13 Hz, \u03b2: 13-30 Hz, \u03b3: 30-50 Hz), supporting accurate analysis of neural activity. The EEG recordings used in this study follow the standard 10-20 system montage"}, {"title": "3.3 Implementation details", "content": "For model training, we utilized the Adam optimizer with an initial learning rate of 0.001 and a weight decay of $1 \\times 10^{-6}$. To promote generalization, we incorporated L2-norm regularization and dropout. The batch size was set to 64 across all experiments, and training was conducted for 15 epochs.\nThe input signal length varied based on the segment duration, with 2-minute segments containing 12,000 data points. A warm-up phase of 200 steps was applied, during which the learning rate was gradually increased from the initial value of 0.001 to allow for stable convergence.\nWe divided the training and validation datasets in a 9:1 ratio, ensuring no subject overlap between the two sets. This approach avoids data leakage and allows for an unbiased evaluation of the model's generalization capability.\nOur experimental setup included Ubuntu 20.04 LTS, with PyTorch 1.12.1 and CUDA 11.3 for GPU acceleration. We ran the experiments on a single NVIDIA RTX 4090 GPU, supported by an AMD Ryzen 9 7900X CPU and 64GB of RAM, ensuring ample computational resources for efficient training and evaluation."}, {"title": "3.4 Performance metrics", "content": "To comprehensively evaluate the performance of our proposed model, we employ a set of metrics that measure both its classification performance and computational efficiency."}, {"title": "4 Results", "content": "In this section, we present the performance evaluation of our proposed model for EEG-based abnormality detection. To provide a comprehensive analysis, we compare the performance of our model with several EEG-based architectures, including: EEGNet [23], EEG-ARNN [28], DeepCNN [24] and FusionCNN [29]."}, {"title": "4.1 Subject-independent results", "content": "The results presented in Table 1 compare our proposed model with several baselines, including EEGNet, EEG-ARNN, Deep4Conv, and FusionCNN, for both per-signal and per-case evaluations. These results provide a comprehensive evaluation of the strengths and weaknesses of each model. EEGNet demonstrates a significant bias in its performance,with a high sensitivity of 98.5% but an extremely low specificity of 3.0% in the per-signal evaluation. This imbalance suggests that EEGNet struggles to generalize effectively in long-term signals, potentially due to its shallow architecture, which may not be capable of extracting robust and discriminative features. Although EEGNet is lightweight, it appears inadequate for processing long-term EEG signals, leading to biased predictions.\nIn the per-signal evaluation, EEG-ARNN, Deep4Conv, FusionCNN, and our proposed model all demonstrate the ability to extract discriminative features from long-term signals, achieving accuracies of 65.1%, 82.1%, 78.1%, and 79.1%, respectively. However, both EEG-ARNN and FusionCNN exhibit significant biases, with relatively high specificity but notably low sensitivity, indicating potential overfitting to certain signal characteristics. Deep4Conv achieves the highest overall performance in per-signal evaluations, outperforming our proposed model by 3.5% in sensitivity, 2.6% in specificity, and 3.0% in accuracy.\nIn the per-case evaluation, our proposed model achieves the highest accuracy of 85.0%, outperforming all other baselines. Furthermore, it demonstrates the best sensitivity (76.2%) among the evaluated models. While FusionCNN shows a slightly higher specificity (92.6%) compared to our model (91.2%), its significantly lower sensitivity (48.4%) highlights a critical imbalance, rendering it less reliable for real-world applications. This balance in sensitivity and specificity underscores the effectiveness of our proposed model in providing accurate and consistent predictions across diverse cases.\nIn summary, the proposed model demonstrates competitive performance, particularly in per-case evaluations, where it outperforms all baselines in both accuracy and sensitivity. These results underscore its ability to effectively address the challenges of long-term EEG signal processing while maintaining a balanced and reliable performance on various metrics."}, {"title": "4.2 FLOPs count comparison", "content": "The results presented in Table 2 provide a detailed comparison of FLOPs and the number of parameters for different models. These findings emphasize the computational advantages of our proposed model.\nFirst, while EEGNet exhibits the lowest FLOPs (103.6M) and parameter count (19.9K) among all models, its shallow architecture limits its ability to extract meaningful features for this task, as demonstrated by its subpar classification performance. This highlights the trade-off between computational efficiency and feature extraction capability in lightweight networks.\nSecond, among the CNN- and GNN-based networks, including Deep4Conv, FusionCNN, and EEG-ARNN, our proposed model demonstrates competitive computational efficiency. Notably, only Deep4Conv achieves lower FLOPS (185.3M) than our model (202.0M), whereas FusionCNN, despite requiring significantly higher FLOPs (4.5G) and parameters (3.4M), does not outperform our model in classification performance. Although our model incorporates a transformer encoder, which inherently demands additional computational resources, it remains comparable to these CNN- and GNN-based baselines in terms of computational cost, showcasing its efficient design.\nLastly, we conducted a direct comparison of computational efficiency with a standalone single-head transformer. Using the same input signal configuration (19 channels with a length of 2 minutes), our model significantly reduced computational requirements compared to the single-head transformer, which requires 11.9G FLOPs and 1.3G parameters. This demonstrates that our integration of a depth-wise CNN-based auto-encoder effectively compresses the data before the transformer, leading to substantial savings in computational resources.\nOverall, the proposed model achieves a commendable balance between computational efficiency and performance. By incorporating a transformer encoder, it maintains competitive FLOPs and parameter counts when compared to CNN- and GNN-based networks, while substantially reducing resource consumption relative to a standard transformer. This level of efficiency highlights its practicality and potential for real-world applications in long-term EEG signal processing."}, {"title": "4.3 Ablation study", "content": "Single-head transformer-encoder vs. MLPs Multi-head attention-based transformer architectures have been extensively studied and widely adopted for application. The single-head transformer-encoder is designed to process EEG signals effectively while maintaining a lightweight structure. To evaluate its performance, we compare it with per-signal and per-case metrics. Furthermore, we believe that transformer-encoder has great potential for advancing neuroscience research, particularly in applications requiring high performance and interpretable models."}, {"title": "5 Discussion", "content": "In this section, we discuss the contributions and implications of our proposed model for EEG-based abnormality detec-tion. Key aspects include computational efficiency achieved through the CNN-based auto-encoder, the interpretability of the architecture, and its ability to handle long-term EEG signals effectively."}, {"title": "5.1 Trade-offs between performance and efficiency", "content": "The proposed CAE-T framework achieves an optimal trade-off between computational efficiency and classification performance, addressing the inherent challenges of long-term EEG signal processing. Compared to lightweight models such as EEGNet, which are computationally inexpensive but exhibit biased predictions, our model introduces additional computational cost justified by substantial performance improvements. Specifically, CAE-T achieves an accuracy of 85.0% at the per-case level while maintaining competitive sensitivity and specificity.\nThe channelwise autoencoder plays a critical role in ensuring this efficiency. By compressing raw EEG signals into compact representations, it significantly reduces the input size for the transformer-based classifier. This compression alleviates the computational burden associated with transformer models while retaining essential spatial and temporal information. For instance, our framework requires only 202M FLOPs, a drastic reduction compared to the standalone single-head transformer model, which requires 11.9G FLOPs.\nThe single-head transformer classifier further enhances the framework's efficiency. While conventional multi-head attention mechanisms often lead to computationally heavy models, our single-head design retains the ability to model long-range dependencies effectively without introducing excessive parameters. Together, the autoencoder and lightweight transformer ensure that CAE-T remains practical for real-world EEG applications, including resource-constrained environments."}, {"title": "5.2 Interpretability of the Proposed Model", "content": "A key strength of the CAE-T framework lies in its interpretability, which is particularly valuable for clinical and neuroscience applications. The channelwise autoencoder not only reduces data dimensionality but also preserves critical spatial and temporal features, enabling insights into neural dynamics and abnormal brain activity.\nTo demonstrate this, we analyzed the outputs of the autoencoder alongside raw EEG signals and corresponding spectrograms. Two representative examples from pathological EEG signals highlight the model's ability to retain biologically interpretable features:\n1. Abnormal Beta Activity: Beta rhythms (13\u201330 Hz) are typically sparse in healthy scalp EEG recordings and rarely exceed 25 Hz. In one pathological signal, the autoencoder identified an abnormal beta activity pattern around the 105-second mark. This anomaly was confirmed by the spectrogram of channel Fz , which displayed elevated spectral power around 30 Hz. Such results emphasize the model's ability to detect subtle frequency-domain abnormalities that align with known pathological EEG characteristics.\n2. Deficiency in Alpha Activity: Alpha waves (8\u201313 Hz) primarily originate in the occipital lobe and exhibit amplitudes between 15\u201345\u00b5V. In a second example, the autoencoder output revealed a significant absence of alpha activity in channels O1 and O2, corroborating the lack of alpha rhythms observed in the raw EEG signals and corresponding spectrograms, . Since alpha wave disruptions are often indicative of neurological disorders, such findings validate the model's potential for identifying spatially relevant abnormalities.\nThese examples underscore the interpretability of the channelwise autoencoder, which bridges the gap between black-box deep learning models and traditional neuroscience approaches. By maintaining channel independence, the autoencoder facilitates biologically meaningful analysis of EEG signals while enabling region-specific anomaly detection.\nFuture investigations into the interpretability of the transformer-based classifier hold significant promise. In future work, we aim to analyze the similarity matrices generated by the self-attention mechanism, as these matrices provide insights into the relationships between EEG channels. Such analyses may uncover functional brain connectivity patterns, further enhancing the clinical utility of our framework."}, {"title": "5.3 Capability for Long-term EEG signal processing", "content": "Long-term EEG signals, often exceeding 2 minutes, pose unique challenges due to their size, complexity, and suscepti-bility to noise. Traditional deep learning approaches often struggle with such sequences, either due to computational limitations or an inability to retain relevant temporal information. Our proposed CAE-T framework effectively addresses these issues by leveraging the channelwise autoencoder to compress long sequences into compact latent representations while maintaining essential features.\nThe experimental results demonstrate the model's robustness in processing extended EEG recordings. CAE-T achieves high accuracy and balanced sensitivity-specificity performance across both per-signal and per-case evaluations. These findings suggest that the framework can generalize effectively to long-term EEG signals, a critical requirement for clinical applications such as continuous brain monitoring and diagnosis of neurological disorders.\nMoreover, the ability of model to handle long sequences without loss of interpretability expands its applicability to real-time EEG analysis in clinical settings. For instance, continuous EEG monitoring for seizure detection or neurological assessment often generates large volumes of data that require efficient processing. CAE-T's lightweight design and interpretability make it a strong candidate for deployment in such scenarios, offering both computational efficiency and clinically relevant outputs."}, {"title": "5.4 Future Directions", "content": "The results of this study highlight the potential of CAE-T to transform EEG-based abnormality detection and neural connectivity analysis. By combining computational efficiency, interpretability, and competitive performance, the framework provides a robust solution to challenges in long-term EEG signal analysis.\nIn future work, we aim to:\n1. Explore the transformer's similarity matrices to identify functional brain connectivity patterns and validate these findings against established neuroscientific theories.\n2. Extend the framework to handle multi-modal EEG datasets, integrating complementary modalities such as fMRI or MEG for more comprehensive neural analysis.\n3. Investigate additional applications of the channelwise autoencoder in neuroscience, such as region-specific feature extraction for studying brain disorders like epilepsy, Alzheimer's disease, and Parkinson's disease.\nBy addressing these areas, we hope to further enhance the clinical relevance and impact of our model in neuroscience research and medical diagnostics."}, {"title": "6 Conclusion", "content": "This study introduced CAE-T, a novel framework for EEG-based abnormality detection that integrates a channelwise CNN-based autoencoder with a transformer-based classifier. By addressing key challenges in long-term EEG signal analysis, CAE-T demonstrates a balance between computational efficiency, interpretability, and high classification performance.\nThe channelwise autoencoder effectively reduces the dimensionality of raw EEG signals while preserving channel independence, facilitating biologically meaningful feature extraction. This design ensures that the model outputs remain interpretable, providing insights into spatial and temporal patterns critical for clinical applications. The transformer-based classifier further leverages the compressed representations, achieving superior accuracy and sensitivity compared to baseline models, such as EEGNet, Deep4Conv, and FusionCNN, while maintaining competitive specificity.\nCAE-T has demonstrated its ability to handle long-term EEG signals efficiently, achieving a substantial reduction in computational cost relative to standalone transformer models. Its lightweight architecture and interpretable outputs make it a practical and reliable tool for real-world applications, such as continuous monitoring in clinical settings or large-scale neuroscience studies.\nLooking ahead, future work will focus on expanding the interpretability of CAE-T through analysis of self-attention similarity matrices, enabling deeper insights into functional brain connectivity. Additional efforts will explore the integration of CAE-T with multi-model datasets and its application in analyzing specific neurological disorders.\nIn summary, the proposed CAE-T framework provides a robust, interpretable, and efficient solution for EEG signal analysis. It paves the way for advancements in brain abnormality detection and opens new opportunities for neuroscience research and clinical diagnostics."}, {"title": "Declaration of Competing Interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "Data Availability", "content": "Data will be made available on request."}]}