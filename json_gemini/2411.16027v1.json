{"title": "From Dashcam Videos to Driving Simulations: Stress Testing Automated Vehicles against Rare Events", "authors": ["Yan Miao", "Georgios Fainekos", "Bardh Hoxha", "Hideki Okamoto", "Danil Prokhorov", "Sayan Mitra"], "abstract": "Testing Automated Driving Systems (ADS) in simulation with realistic driving scenarios is important for verifying their performance. However, converting real-world driving videos into simulation scenarios is a significant challenge due to the complexity of interpreting high-dimensional video data and the time-consuming nature of precise manual scenario reconstruction. In this work, we propose a novel framework that automates the conversion of real-world car crash videos into detailed simulation scenarios for ADS testing. Our approach leverages prompt-engineered Video Language Models (VLM) to transform dashcam footage into SCENIC scripts, which define the environment and driving behaviors in the CARLA simulator, enabling the generation of realistic simulation scenarios. Importantly, rather than solely aiming for one-to-one scenario reconstruction, our framework focuses on capturing the essential driving behaviors from the original video while offering flexibility in parameters such as weather or road conditions to facilitate search-based testing. Additionally, we introduce a similarity metric that helps iteratively refine the generated scenario through feedback by comparing key features of driving behaviors between the real and simulated videos. Our preliminary results demonstrate substantial time efficiency, finishing the real-to-sim conversion in minutes with full automation and no human intervention, while maintaining high fidelity to the original driving events.", "sections": [{"title": "Introduction", "content": "The rapid advancements in Automated Driving Systems (ADS) technology have created an urgent need for robust and realistic testing environments to assure reliability of ADS (Huang et al. 2016). Real-world scenarios, such as crash videos and near-miss events, offer valuable insights into the diverse conditions ADS must navigate, making them an essential source for improving ADS testing. However, replicating these scenarios in real-world settings is both dangerous and impractical. Therefore, converting real-world driving videos into simulation scenarios is a necessary solution, but this process also poses several challenges: the high dimensional video data significantly limits the development of automated methods, while manually reconstructing these scenarios can take experts hours to complete. This time-consuming process underscores the need for a more efficient and automated approach.\nIn this paper, we propose a novel framework that automates the conversion of real-world driving videos into detailed simulation scenarios. Our approach leverages prompt-engineered Video Language Models(VLMs) to transform dashcam videos into SCENIC scripts, enabling the automatic generation of realistic simulations in CARLA. In addition, rather than aiming solely to create an exact digital copy of the dashcam video, our framework prioritizes capturing the most essential driving behaviors while maintaining flexibility in parameters such as weather and road conditions. This flexibility supports search-based testing, allowing for diverse variations of the original scenario to be explored systematically. Furthermore, we introduce a similarity metric that iteratively refines the generated simulations by comparing key driving features between the real and simulated videos. The contributions of this work are fourfold: (1) an automated video-to-simulation pipeline that removes the need for manual scenario construction, (2) a similarity metric that bridge the gap between real and simulated scenarios, (3) an iterative feedback loop for scenario refinement using neural network-based feedback from VLM to capture core driving behaviors, and (4) a significant improvement in time efficiency, reducing scenario generation from hours to minutes while maintaining high fidelity to the original events."}, {"title": "Related Work", "content": "Testing ADS in simulation environments has been the subject of extensive research (Huang et al. 2016). Existing autonomous vehicle simulation platforms such as CARLA (Dosovitskiy et al. 2017) and LGSVL (Rong et al. 2020) allow researchers to generate and manipulate driving scenarios in controlled environments. Efforts such as the SCENIC language (Fremont et al. 2019) enables search-based testing(SBT) so that the generated scenario can be a seed for search based testing with respect to temporal logic requirements (Dreossi et al. 2019; Tuncali et al. 2020), safe driving rules (Hekmatnejad, Hoxha, and Fainekos 2020) and traffic laws (Sun et al. 2022). However, accurately designing these scenarios to closely resemble real-world events remains a challenge.\nRecently, efforts have shifted toward automatic real-to-simulation (real-to-sim) conversion approaches that use video data to guide scenario generation. For instance, Bai et al. (Bai et al. 2024) introduced a system that extracts key information from videos to create driving scenarios in simulation environments. In (Elmaaroufi et al. 2024), the authors automatically generate driving scenarios from police crash reports. Similarly, (Wang et al. 2023) focus on learning realistic human behaviors in real-life scenarios and use learned models to improve simulations. NVIDIA's STRIVE (Rempe et al. 2022) generates accident-prone driving scenarios by modifying 2D trajectories, but this method is based on controlled scenarios rather than real-world crash videos. Another approach, DEEPCRASHTEST (Bashetty, Ben Amor, and Fainekos 2020), converts dashcam footage into crash tests by extracting 3D vehicle trajectories but lacks an iterative refinement process to improve simulation accuracy.\nWhile these approaches represent meaningful progress, existing methods are often limited by a reliance on pre-defined trajectories or fail to incorporate iterative feedback to refine the generated scenarios. Furthermore, their primary goal has been to ensure that the generated scenarios are as identical as possible to the original ones (e.g. optimizing on the tracking performance for KITTI tracking dataset (Fritsch, Kuehnl, and Geiger 2013)). In contrast, we want to focus on capturing the core driving behaviors from the original video while introducing flexibility in other parameters, such as weather and road conditions. This flexibility enables search-based testing, allowing for systematic exploration of environmental variations to identify vulnerabilities in ADS and improve their robustness.\nTo address these gaps, we chose to use VLMs as they enable more flexible and scalable video-to-language translation. Recent works, such as DriveDreamer-2 (Zhao et al. 2024), employ LLMs to generate user-defined driving videos by translating queries into agent trajectories and maps for simulation. Similarly, Text-to-Drive (Nguyen et al. 2024) leverages knowledge-driven language descriptions to synthesize diverse driving behaviors. While these works demonstrate the potential of foundational models in simulation-based driving applications, they either primarily limit to synthetic data and do not leverage real-world driving videos or focus on user-defined queries.\nIn contrast, our framework integrates real-world driving data, such as dashcam crash videos, to generate simulation scenarios that not only capture critical driving behaviors but also provide flexibility for environmental parameters like road type and weather. Through prompt engineering, we tailor VLMs to create SCENIC scripts for scenario generation, bridging the gap between exact scenario replication and the need for diversity in simulation environments. This approach ensures realistic and behaviorally accurate test cases while supporting search-based testing for robust ADS validation."}, {"title": "Real-to-Sim Scenario Generation Framework", "content": "Given a real-life vehicle crash video (e.g., a dash camera recording), our objective is to generate corresponding simulation scenarios that accurately capture the core driving behaviors. This approach enables robust and safe testing of Automated Driving Systems (ADS) by leveraging real-life close-call situations within a controlled simulation environment. Our framework consists of 4 components: (1) conversion of real-world video into SCENIC scripts, (2) generation of simulation videos from SCENIC scripts, (3) similarity analysis between the real and simulated videos, and (4) iterative refinement to ensure the simulated video's consistency with the original scenario."}, {"title": "Video-to-Text Generation", "content": "First, we convert the input video into a descriptive script. This is accomplished using a prompt-engineered version of the pre-trained GPT-40 model.\nPrompt engineering involves designing and refining input prompts to guide large foundational models, such as GPT-40 (OpenAI 2024), toward producing accurate and desired outputs. In this case, prompt engineering is especially effective for generating detailed scenario descriptions (e.g., SCENIC scripts) from real-world driving videos. During the prompt engineering process, we improve the pre-trained GPT-40 model by providing it with multiple \"positive-example\" pairs (Vi, Si), where V is a simulation video generated in CARLA using the corresponding SCENIC script Si that describes the scenario, as illustrated in Figure 1a. Through this process, the new Video-Language Model, referred to as ScriptGPT, learns to map key visual elements, such as weather, road conditions, and vehicle behaviors, into structured and accurate scenario description languages. After sufficient prompt engineering, ScriptGPT is capable of generating a SCENIC script Sout for real-world crash video Vreal\u00b7\nAlthough the initial prompt engineering process was conducted using simulation videos paired with their SCENIC scripts, this approach is extendable to real-world driving videos because the underlying visual and descriptive patterns (e.g., road layouts, traffic behaviors, and environmental factors) are consistent across both domains, allowing the model to effectively generalize its learned capabilities and accurately capture real-world scenarios."}, {"title": "Text-to-Video Generation", "content": "We convert the descriptive language Sout to simulation video Vsim using SCENIC (Fremont et al. 2018, 2019). SCENIC is a programming language tool designed for specifying driving scenarios through environmental factors, vehicle behaviors, and road conditions. Once we have the SCENIC script Sout generated from the real crash video using the prompted-engineered model, we feed it into the SCENIC framework to synthesize a simulation scenario in CARLA, as shown in Figure 2. The script Sout serves as the textual representation of the scene, encoding environmental conditions (e.g., weather, traffic), vehicle dynamics, and road types. The output is a new simulation video Vsim, which visually represents the scenario described in Sout"}, {"title": "Similarity Check", "content": "Next, we perform a similarity check on the simulated scenario Vsim and the original Vreal. Ideally, they should closely match, allowing us to seamlessly replace the ego vehicle with any ADS (e.g. Baidu's Apollo planner (Fan et al."}, {"title": "Iterative Refinement", "content": "Finally, we perform iterative refinement on the ScriptGPT with the help of similarity check, as illustrated in Figure 3. If the absolute difference for a given category i is above a predefined threshold Ti, we refine the SCENIC script Sout by feeding the discrepancy for that category back into ScriptGPT as an additional prompt (e.g., \"there shouldn't be a leading vehicle overtaking behavior, please improve on that\"). This feedback allows ScriptGPT to adjust the SCENIC script Sout accordingly, generating a new version of the script and producing a new simulation video Vsim'.\nAlthough a single SCENIC script can produce multiple simulation videos with consistent core driving behaviors but variations in other parameters (e.g., road types or vehicle configurations), we utilize only a single video for similarity checks and iterative refinement, because this is sufficient to ensure that the generated scenario aligns with the essential driving behaviors of the original video. This approach provides the necessary focus for refining critical features while maintaining flexibility for generating diverse scenarios, enabling search-based testing.\nThe process is repeated iteratively until the difference for each category falls below the predefined threshold, i.e., $|| Sim(V_{real}, V_{sim'})_i|| \\leq \\tau_i$. Once the similarity across all"}, {"title": "Experiments & Analysis", "content": "System Setup\nDataset We obtain the collision videos from the Car Crash Dataset (CCD) (Bao, Yu, and Kong 2020). CCD is chosen because it contains real traffic accident videos captured by dashcams mounted on driving vehicles, potentially providing a rich source for developing and testing ADS. Moreover, our framework is also relevant for near misses events.\nSimulator We use CARLA (Dosovitskiy et al. 2017), an open-source platform designed to support the development and validation of autonomous driving systems. CARLA is selected for its realistic physics engine and high-fidelity environmental rendering, making it ideal for generating the simulation scenarios needed in our framework.\nDescription Language SCENIC We use SCENIC as the description language for driving scenarios due to its similarity to Python, which aligns well with GPT-40's input data (OpenAI 2024), making prompt engineering doable and more straightforward. Furthermore, SCENIC integrates seamlessly with the CARLA simulator and it is highly effective at specifying complex driving scenarios, making it well-suited for our application.\nPrompt-Engineered ScriptGPT To construct the Script- GPT model, we begin by writing 20 SCENIC scripts that cover diverse driving scenarios, such as overtaking, cruising, sudden stops due to obstacles, and turns in varying road and weather conditions. Using the SCENIC library and CARLA, we generate corresponding videos for each scenario and pair them with their respective SCENIC scripts, forming 20 (Vi, Si) pairs. These pairs are then used as input data for prompt engineering GPT-40, selected for its ability to learn and generalize from a wide range of examples.\nThe empirical design choice we made is to design only 20 pairs because 20 scenarios are sufficient to cover most of the common interesting scenarios. Moreover, in practice, since GPT-40 API does not natively accept video formats like .mp4, we preprocess the videos by sampling frames and concatenating them into an n-dimensional array. The same preprocessing technique is applied during testing phases to ensure consistency between training and testing phases, and we also apply the same to the FeatureGPT.\nPrompt-Engineered FeatureGPT We use FeatureGPT to enhance our framework's ability to recognize specific driving behaviors. First, we predefine 10 driving feature categories, as shown in Table 1. Next, we create 20 SCENIC scripts representing scenario videos where these features may or may not appear. Each video is paired with a corresponding 10-dimensional feature vector (e.g., [parallel vehicle overtaking: 0, leading vehicle stopped: 1], where 0 indicates absence and 1 indicates presence). The input data is used to prompt-engineer GPT-40 into FeatureGPT, which outputs a 10-dimensional probability vector for each video during inference. This allows us to compare and categorize driving behaviors between the original video Vreal and the generated video Vsim. Again we made the empirical design choice of using 10 feature categories and 20 samples to cover most frequently encountered interesting driving behaviors through trial and error.\nIterative Refinement using Similarity Check Once FeatureGPT produces feature vectors for both the generated and original videos, we compare them to detect discrepancies. A large discrepancy in any feature indicates that the generated video is either missing a key behavior (negative gap) or introducing an unintended one (positive gap), and then we map the gap into natural language feedback (e.g., \"there should"}, {"title": "Case Study", "content": "We present five interesting dashcam videos from the CCD dataset and generate their corresponding simulation scenarios using our framework, as shown in Figure 4, 5, 6, 7, 8. Importantly, each SCENIC script can correspond to multiple simulation scenarios. For example, a highway scenario may result in a 2-lane road or a 3-lane road, and a leading vehicle could be represented as either a truck or a sedan. This variability enables search-based testing by generating diverse scenarios that explore different environmental parameters while preserving the core driving behaviors.\nIn this section, we showcase one simulation scenario per SCENIC script due to page limitations. However, our framework inherently supports generating multiple testing scenarios from a single script, allowing systematic exploration of predefined features. As demonstrated in the figures, the generated scenarios faithfully capture the most essential driving behaviors observed in the dashcam videos while introducing controlled variations in other parameters, such as road configuration and vehicle types, to support robust ADS testing."}, {"title": "Preliminary Qualitative Result", "content": "Automated Pipeline After completing the prompt engineering process, our framework is capable of automatically generating the 5 scenarios mentioned in Section without any human intervention during the testing phase. We also extended our framework to evaluate 50 randomly selected accidents from the CCD dataset, and found that 32 out of 50 (64%) scenarios generation can be fully automated without any human involvement, while 18 scenarios (36%) encountered syntax errors that required human debugging.\nTime Efficiency Our framework significantly reduces the time required for real-to-simulation scenario generation. For the 32 videos that can be automatically generated, it takes 1.5 minutes per scenario on average during the testing phase, including iterative refinement, to produce a SCENIC script of approximately 70 lines of code. In contrast, manually coding and debugging a similar real-to-simulation scenario could take an experienced engineer several hours.\nAdvantage of Iterative Refinement The 5 scenarios in Section underwent 1-2 iterations of refinement, resulting in notable improvements in both accuracy and realism. In the extended evaluation of 50 accidents, iterative refinement happened in 17 scenarios (34%), further showcasing its potential to enhance scenario quality and be generalized to more crash scenarios.\nFramework Accuracy While we have not yet conducted a formal quantitative analysis on the framework's timing efficiency or objectively measure benefits of iterative refinement, the preliminary results provide strong evidence of the concept's validity. Our framework consistently captures the core driving behaviors from the original videos, indicating its effectiveness in generating accurate and realistic driving scenarios."}, {"title": "Limitations & Future Work", "content": "The current framework faces several challenges. It struggles with scenarios involving poor perception conditions or high complexity, as illustrated in Figure 9. Additionally, the framework's performance may degrade when testing scenarios (e.g. multi-car complicated driving scenario at night like shown in Figure 9) have not been encountered by Script- GPT and FeatureGPT during the prompt engineering process. Therefore, the performance of our framework heavily relies on carefully and manually selecting diverse \"positive-examples\" for prompt engineering.\nFor future work, we plan to conduct a human study to quantitatively assess the framework's time efficiency and accuracy. This will involve timing how long experts take to manually write real-to-simulation conversion scripts and asking them to rate the accuracy of our automated conversions. We also aim to improve the diversity of data samples used in the prompt engineering process to extend our framework's applicability across the entire Car Crash Dataset. Finally, we envision extending this video-to-video conversion framework to other domains, such as flying tasks or other robotics applications, broadening its use cases and potential impact."}, {"title": "Conclusion", "content": "In this paper, we have presented a novel framework for automatically converting real-world vehicle crash videos into simulation scenarios using prompt-engineered Video-Language Models. We have deploy multiple techniques including the similarity score metric and the iterative refinement process to ensure the generated scenarios closely align with the original videos. Despite the framework's current limitations, such as reliance on data diversity and the challenges of handling complex or unseen scenarios, through multiple examples, it demonstrates clear potential for improving ADS testing. Future work will focus on expanding data diversity, conducting quantitative human studies, and extending the framework to other robotics domains."}]}