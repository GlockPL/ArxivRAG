{"title": "Survey on safe robot control via learning", "authors": ["Bassel El Mabsout"], "abstract": "Modern society heavily relies on robotic systems, their use affects the aerospace, automotive, energy, disaster response, health care, manufacturing, and traffic management industries among countless others. From making robots walk Westervelt et al. [2007] to getting molecular swarms to kill cancer cells Wijewardhane et al. [2022], whole fields of research dedicate themselves to the problem of control. Intelligently selecting control strategies so that we can manage, direct, or command the trajectories a system can take distills the essence of problems faced in control. When a system can be controlled in the aforementioned manner using control loops, the system in question is termed a control system. Tackling the problem of control, the research community has produced many alternative solutions with varying trade-offs concerning what is achievable and how much we can represent these systems and our goals. Thus, research works that attack control problems sit on a wide spectrum of solutions. On one end, classical control theory commonly defines these systems by modeling their evolution using differential equations as a function of their free parameters. Such techniques involve working with a subset of mathematically definable models and objective functions. The intention is to make the selection of these control parameters representable as a problem with efficiently computable solutions. On the other side of the spectrum, Deep Reinforcement Learning (Deep-RL) algorithms define and incrementally improve \"policies\" (or analogously \"controllers\") on set objectives defined by loss functions via gradient updates. Such solutions may be entirely data-driven, meaning that controllers can improve without any prior model defined, thereby only requiring experience gathered from real or modeled systems via observations. Of course, there are also large bodies of work exploring the design space between these two extremes.\nWhen goals include safety concerns, special care must be taken to prevent the system from ever reaching states which violate them. In this document, we focus on the problem of learning high-performance control on real robots while maintaining various notions of safety. Such notions include stability, obstacle avoidance,", "sections": [{"title": "1 Introduction", "content": "Modern society heavily relies on robotic systems, their use affects the aerospace, automotive, energy, disaster response, health care, manufacturing, and traffic management industries among countless others. From making robots walk Westervelt et al. [2007] to getting molecular swarms to kill cancer cells Wijewardhane et al. [2022], whole fields of research dedicate themselves to the problem of control. Intelligently selecting control strategies so that we can manage, direct, or command the trajectories a system can take distills the essence of problems faced in control. When a system can be controlled in the aforementioned manner using control loops, the system in question is termed a control system. Tackling the problem of control, the research community has produced many alternative solutions with varying trade-offs concerning what is achievable and how much we can represent these systems and our goals. Thus, research works that attack control problems sit on a wide spectrum of solutions. On one end, classical control theory commonly defines these systems by modeling their evolution using differential equations as a function of their free parameters. Such techniques involve working with a subset of mathematically definable models and objective functions. The intention is to make the selection of these control parameters representable as a problem with efficiently computable solutions. On the other side of the spectrum, Deep Reinforcement Learning (Deep-RL) algorithms define and incrementally improve \"policies\" (or analogously \"controllers\") on set objectives defined by loss functions via gradient updates. Such solutions may be entirely data-driven, meaning that controllers can improve without any prior model defined, thereby only requiring experience gathered from real or modeled systems via observations. Of course, there are also large bodies of work exploring the design space between these two extremes.\nWhen goals include safety concerns, special care must be taken to prevent the system from ever reaching states which violate them. In this document, we focus on the problem of learning high-performance control on real robots while maintaining various notions of safety. Such notions include stability, obstacle avoidance,"}, {"title": "2 Safety and Control in Classical Control Theory", "content": "There is a wealth of classical control techniques, commonly categorized as prescriptive/descriptive, model-free/model-based, myopic/predictive, and discrete/continuous Franklin et al. [2001], Dorf and H. [2010]. Descriptive methods analyze whether given controllers follow defined specifications, while Prescriptive ones synthesize controllers with specified properties. Essential to these methods is whether we have access to predictive models representing the dynamics of the systems in question. We focus on the differences between myopic, predictive, model-free, and model-based control in Section 2.1. And on how such systems are modeled in Section 2.2."}, {"title": "2.1 Model-free vs Model-based", "content": "Model-free controllers are defined as controllers with no access to dynamical models when making control decisions. They make weak assumptions concerning the control responses of the systems in question. For example, increasing specific outputs leads to a general increase in some variables in the system. Thus, even when the systems in question are poorly characterized, they offer a practical solution as controllers. Of such controllers, the Proportional Integrative Derivative (PID) Haidekker [2020] class of controllers is by far the most widely studied and used. Without access to models, such controllers turn to heavy use of feedback and often require careful tuning for stable and performant control. These controllers are considered myopic, meaning they must optimize the system's trajectory given only limited information prior to the current execution loop. Many of these systems are tuned by expert practitioners using different methods O'Dwyer [2003] for PID, the most common is the Ziegler-Nichols method Ziegler and Nichols [1993]. In this space, complex live-automated tuning methods H\u00e4gglund and \u00c5str\u00f6m [1990] also exist, allowing these systems to be adaptive to environmental changes. These methods tend to have low computational requirements for executing their control loop; trajectory planning algorithms (high-level controllers) commonly use them for reaching low-level objectives. Without a model, these controllers can fail to take advantage of system-specific behaviors, leading to under-performance and even complete failure in controlling some systems"}, {"title": "2.2 Modeling and System Identification", "content": "In continuous-time systems, dynamical models are commonly defined as ordinary or partial differential equations with free parameters that identify real-life systems. These systems can be time-invariant, linear, affine, stochastic, ergodic, and more. Many techniques have been developed to adapt well-understood techniques to a larger set of systems, for example, linearizing nonlinear systems around equilibrium points Isidori [1995]. Even when models represent well the primary components of a real-life system's time evolution, some parameters require identification, such as the weight and length of a pendulum.\nSystem identification methods have been well-explored theoretically, especially for linear time-invariant systems Soderstrom and Stoica [1989], Ljung [2017]. Data-driven methods use tuning procedures based on"}, {"title": "2.3 Example system", "content": "A simple example of a control system would be an oven with a temperature sensor and a heater which can be turned on or off. According to Newton's law of cooling, the temperature in this system follows this equation:\n$\\text{temp}(t) = -k(\\text{temp}(t) \u2013 \\text{temp}_{\\text{ambient}})$\n$\\text{temp}(t)$ defines the change in temperature of the system at time t, $\\text{temp}_{\\text{ambient}}$ is the ambient temperature of the oven, this is the temperature that the oven reaches eventually. the constant $k \\in R^+$ is the rate at which the temperature of the oven closes the temperature \"gap\". In solving this differential equation, we first solve for the n'th derivative of $\\text{temp}(t)$, namely $\\text{temp}^{(n)}(t)$:\n$\\text{temp}(t) = -k(\\text{temp}(t) \u2013 \\text{temp}_{\\text{ambient}})$\n$\\frac{d}{dt}\\text{temp}(t) = \\text{temp}(t) = -k(\\text{temp}(t) - \\text{temp}_{\\text{ambient}}) \\frac{d}{dt} = \u2212k * \\text{temp}(t) = (-k)^2(\\text{temp}(t) \u2013 \\text{temp}_{\\text{ambient}})$\n$\\frac{d}{dt}\\text{temp}(t) = \\text{temp}(t) = (-k)^2 (\\text{temp}(t) \u2013 \\text{temp}_{\\text{ambient}})\\frac{d}{dt} = (-k)^2\\text{temp}(t) = (-k)^3(\\text{temp}(t) \u2013 \\text{temp}_{\\text{ambient}})$\n:\n$\\frac{d}{dt}^{(n-1)}\\text{temp}(t) = \\text{temp}(t) = (-k)^{n-1} (\\text{temp}(t) \u2013 \\text{temp}_{\\text{ambient}}) = (-k)^n (\\text{temp}(t) \u2013 \\text{temp}_{\\text{ambient}})$"}, {"title": "2.4 Safety in Classical Control", "content": "The meaning of safe control depends on the type of specification used in representing safety and the specific dynamical system in question. In general, the goal is to constrain the system's evolution, preventing the violation of some safety criteria. For example, if we have an oven temperature controller, safety can mean never reaching temperatures higher than some amount (in preventing damage to some internal components). This safety criterion can be defined, for example, as an inequality separating the state space into a safe and unsafe set ($\\forall t, \\text{temp}(t) < 400F$). A safe controller, in this case, never allows the system to reach points in the unsafe set. This is how the notion of reachability connects to system safety. Alternatively,"}, {"title": "2.4.1 Stability", "content": "Definitions\n[def.a] Global exponential stability denotes the notion that a differential equation starting from any point x(to) in the initial state space X will eventually (t \u2192 \u221e) reach the point x(\u221e) = 0 and that the rate of convergence is exponential, namely:\n$\\forall t\\in\\mathbb{R}^+, x(t_0)\\in X, \\exists m,a\\in\\mathbb{R}^+, ||x(t)|| \\leq me^{-a(t-t_0)}||x(t_0)||$.\n[def.b] Control Lyapunov Functions are functions V which satisfying the following properties:\nV(p) = 0 (1)\n$\\forall x\\in X/{p} V(x) > 0$ (2)\n$\\forall x \\in X \\exists u\\in U, V(f(x, u)) \\leq V(x)$ (3)\nSpecifically, these are the conditions for a discrete-time Lyapunov control function. f represents the system's transition function, meaning x' = f(x, u) defines the next state when starting from state x and taking action u. X defines the state-space, and U defines the action space. Intuitively, V is positive everywhere except the point we want the system to reach, which is p (eq. (1) and (2)). And there is always an action u we can take that forces V's value to be non-increasing when a system transition is taken (eq. (3)). This means that with an infinite number of transitions we can always cause the system to eventually reach the setpoint p. We can make the Lyapunov function be a proof of global exponential stability by changing eq. (3) such that V(f(x, u)) - V(x) < \u03b1V(x).\nFor the oven case with our controller c, we choose the following Lyapunov function:\nV(temp(t)) = |\\text{temp}(t) - \\text{temp}_{\\text{desired}}|$"}, {"title": "2.4.2 Temporal Logics", "content": "Formal logics such as temporal logics (ex. LTL, STL, rSTL, dSTL) define languages allowing for specifying behaviors we want the system to meet. Example of a Signal Temporal Logic (STL) formula for ovens:\n$\\phi_{\\text{converge}}(x, t) = F(G(|x(t) \u2013 \\text{temp}_{\\text{desired}}| < 10))$\n$\\phi_{\\text{avoid}}(x, t) = G(x(t) > 400 \u2192 F_{[0,1s]} (G_{[0,10s]} (x(t) < 400)))$\n$\\phi(x, t) = \\phi_{\\text{converge}}(x, t) \u2227 \\phi_{\\text{avoid}}(x, t)$\nG means globally and F means eventually, F(G(a)) represents the notion that after some undetermined amount of time, the statement a will be true and that it will stay true, in other words, eventually, a will be globally true. When brackets are included, the operator is quantified over the time period specified, F[0,1s]a means that the formula a will be \"eventually\" satisfied within a 1 second period. This notation X[t1, t2] means that when the operator X gets \"activated\" at time t, the operator applies from time t + t\u2081 to t + t2. Therefore the formula converge denotes the behavior that the system eventually reaches and remains near the desired temperature (within 10 degrees Fahrenheit). avoid represents the notion that, whenever the temperature reaches above 400F, within 1 second, the temperature will be dropped to under 400F, and will remain there for at least 10 seconds. This criteria bounds how long the oven spends at temperatures which we deem dangerous for the components.\nA Commonly used extension of Signal Temporal Logic adds robustness (rSTL), which allows for evaluating how far we are from satisfying the formula itself (ex. 200 - |x| represents a robustness value for the logical relation x < 200). This robustness value is often used to find controllers which are optimized to remain far from violating the STL specification. differentiable Signal Temporal Logic (dSTL) makes the robustness value differentiable so that gradient methods can be used for finding satisfying controllers (more prominently"}, {"title": "3 Learning-based Control", "content": "In addressing the limitations of classical control, learning-based methods have reached new milestones in solving complex control tasks. This includes grasping robotics OpenAI et al. [2019], solving Atari games Mnih et al. [2013], and high-performance drone control Mysore et al. [2021b]. In Section 3.1, we focus on seminal works in the area of Deep-RL, which use benchmark example systems to test their methods' viability. Moreover, in Section 3.2, we focus on recent works augmenting classical control theory techniques with machine learning methods."}, {"title": "3.1 Deep-RL", "content": "The central assumption in RL is that the system in question can be represented as a discrete-time Markov Decision Process (MDP) and that a reward function is defined to compensate policies for desirable be-haviors. Such MDPs may have a continuous or discrete state-space and action-space. They may also be stochastic or deterministic. Though policy search techniques such as Monte Carlo search-based methods Silver et al. [2016] rely on having explicit models, Time Differencing (TD) or policy gradient based methods Sutton and Barto [2018], relieve this requirement, weakening the need for expert knowledge. Deep-RL, which uses deep learning for finding policies, represents controllers as deep NNs. These methods are often categorized into Model-\"free\" methods Section 3.1.1 or Model-based methods Section 3.1.2."}, {"title": "3.1.1 Model-\"free\" RL", "content": "Model-\"free\" RL algorithms do not model the MDP under consideration. However, a model is still involved in representing a policy's total expected discounted reward as a function of the system state. These algorithms are commonly categorized as Q-learning or Policy gradient-based, and whether they are On-policy or Off-policy algorithms. Modern Q-learning Watkins and Dayan [1992] methods such as DQNMnih et al. [2013], DDPGLillicrap et al. [2016], SACHaarnoja et al. [2018], and TD3Fujimoto et al. [2018], rely on a learnt Q-value function otherwise known as a critic. The goal of critics is to represent the optimal policy's discounted expected reward given certain states and actions. Controllers are optimized to choose actions maximizing this Q-value. These methods are often off-policy, meaning they have a replay buffer for storing observed transitions in the environment, sampling batches from which to train the represented Q-value and policy. Being off-policy allows for having different policies used to explore an environment. On the other hand, policy gradient methods such as PPOSchulman et al. [2017] and TRPOSchulman et al. [2015], and A3CMnih et al."}, {"title": "3.1.2 Model-based RL", "content": "A model allows agents to plan ahead, allowing the exploration of future paths with their respective returns in rewards. Like MPC in classical control theory, agents can make immediate choices in picking actions based on the chosen path representing the learned policy. AlphaGo Silver et al. [2016] is a famous example of this approach. When models are accurate, considerable improvements are observed in the sample efficiency for finding good policies. Other methods make use of a learned model Moerland et al. [2020] of the environment from observation data rather than assuming a predefined model. However, model-\"free\" RL still produces state-of-the-art policy performance in most accepted RL benchmarks Prudencio et al. [2022]."}, {"title": "3.1.3 Robustness in Deep-RL", "content": "In regular RL, the optimization goal is to find policies that maximize the discounted sum of rewards designed by expert practitioners. However, encoding desired behaviors as rewards is a challenging task leading to its separate field of study, namely reward shaping. Even using the same rewards, trained agents may have a high variance in behaviors learned. They may even depend on the seeds with which the environment is initialized. Due to issues with sample-complexity and the unsafety of the exploration process in Deep-RL algorithms, a simulation is often used for training agents. Using simulations affects robustness via what is known as the sim-to-real gap, namely the difference in dynamics between the defined simulation and reality. Due to such discrepancies, agents trained in simulation may fail to transfer to reality. Methods for tackling these issues include regularizationMysore et al. [2021a], reward shaping techniques Gullapalli and Barto [1992], Mysore et al. [2021b], and constrained MDPs Altman [1999]. One advantage of model-based Deep-RL where models are learned, is that a vast array of pre-existing machine learning literature around the problem of generating models which closely approximate reality exists. Since NNs are data-hungry, common techniques include pretraining a model on simulation data, and then fine-tuning it to data observed in reality. Domain adapting models has its own complications, for example, this work tra [2022] represents a modern challenge where trash on a conveyor must be segmented for recycling purposes. Even though large amounts of data in simulation are available and an acceptable number of data points in reality exist, current techniques still fail to achieve desirable performance. Adding safety to RL methods, whether by proving that policies will never explore unsafe regions or showing that optimal controllers are Lyapunov stable, is a recent research focus in"}, {"title": "3.2 Classical Control Theory Augmentations", "content": "Methods from classical control theory can be augmented with machine learning methods while allowing the use of a wide range of tools developed, from differential equation integrators Butcher [2008], to automated proof search techniques Mitsch and Platzer [2016]. Some works improve classic system identification by adding a NN which can learn to model various nonlinear features but constrain the NN enough that guaran-tees are retained. An example of such work is explored in Section 3.2.1. Other works improve the stability of preexisting controller architectures by representing the stabilizing function as an NN. Works showing how this is achievable while retaining guarantees is explored in Section 3.2.2. Some methods use machine learning techniques to find approximating dynamical models represented as differential equations. A method in this area that is garnering recent success is discussed in Section 3.2.3"}, {"title": "3.2.1 Neural Lander", "content": "The Neural Lander work of [Shi et al., 2019] augments classical quadrotor dynamical models with a deep-NN to improve the accuracy in controlling and landing a quadrotor at precise points. Their goal is to do so while retaining classical control theory guarantees, namely the global exponential stability (see [def.a] in Section 2.4.1) of the system. In this case, the difficulty in achieving high control accuracy lies in the complexity in modeling ground-effects affecting the system's dynamics. Their evalutations in Section VI C, show that their NN-augmented dynamical model provides significant improvements in prediction accuracy of near ground trajectories against existing ground effect models. However, they note in Section VI E, that without constraining this learned model, synthesized controllers produced unexpected outputs sometimes leading to crashes. This motivates their method's requirement in proving global exponential stability as a safety requirement for the controller. Finally, the authors show great improvements in trajectory tracking and landing near the ground (Fig. 3 in Shi et al. [2019]) most notably touting 10x improvements in landing accuracy (errors of 0.072m for the baseline against 0.007m for their method). We will apply their method to the simplified problem defined in Section. 2.3 of this document, showcasing how their different components interact.\nAssuming that our idealized model of the oven's temperature does not capture all of the dynamics present, following Section II in Shi et al. [2019] we extend our ODE with the term $f_a$ representing external disturbances in temperature. The authors specified $f_a$ as representing the disturbances generated by ground"}, {"title": "3.2.2 Neural Lyapunov Functions and dReal", "content": "The work by Chang et al. presents an algorithm for finding NNs which define control Lyapunov functions. In doing so, the authors make use of the powerful function approximation capabilities of deep-NNs to characterize a controller's region of attraction. The authors focus on control Lyapunov functions for their capability in proving stability in even non-linear dynamical systems.\nWe've proven that our example oven with controller c is already stable over all of the state space (global stability) in [def.b] of Section 2.4.1 via defining a corresponding Lyapunov function. However, if our example system was a Segway, then there exists points in the state space from which reaching our goal is impossible for any control input u. Thus, we can consider the system stable (eventually reaching our goal state) under a subset of the possible state-space. This is termed the region of attraction, this notion is made precise in definition 7 of Chang et al. [2019].\nDepending on the choice of Lyapunov functions, we may underrepresent the region of attraction since the Lyapunov conditions may only hold under a strict subset of the true region of attraction. When the dynamical system or controller are complex or even changing, relying on an expert to find Lyapunov functions is impractical. This is the reason methods for automatically finding Lyapunov functions have been studied,"}, {"title": "3.2.3 SINDY", "content": "Sparse identification of nonlinear dynamical systems (SINDY) Brunton et al. [2016] is a method within the intuitive physics subfield. It tries to find a system's governing equation from data. The method defines and searches a space of differential equations defined by polynomials of the system's state vector while using candidate nonlinear operators. Similar to how kernel tricks are employed to capture nonlinear terms in polynomial regression. An L1 regularizer is used to bias the learned equations towards solutions with lesser terms, encouraging sparsity. The author hypothesizes that most physical systems have governing equations with primary components following simple laws. When this method finds good approximating models, they tend to generalize well to unseen data and are inherently explainable. An advantage of this method is that it unlocks the use of extensively researched tools built for control relying on differential equations. However, the method also has significant limitations, such as allowing only separable nonlinear functions. It disallows the use of multiple nonlinear operators whose first few terms in their Taylor series expansions are similar. It also fails to scale to large state spaces such as images. Later introduced techniques such as applying SINDY in the latent space of NN encoders Chen et al. [2022] have been explored to address this limitation."}, {"title": "4 Embedded System Design Considerations", "content": "Robotic systems have a wide range of capabilities and resource limitations, from kilobots Rubenstein et al. [2012] to the Saturn V rocket Haeussermann [1970]. Common software design goals include having low system unpredictability, low system latency, low power usage, and high performance where needed. When designing such systems, controller safety guarantees rely on assumptions of timeliness and predictability of the embedded systems on which they are expected to be deployed. In Section 4.1 we focus on sources of uncertainty in idealized models in reference to what occurs on real systems. In Section 4.2 we discuss hardware and software designs to maintain controller guarantees."}, {"title": "4.1 Sources of Unpredictability", "content": "When modeling stochastic systems, there are two central notions of uncertainty aleatoric and epistemic. (i) Aleatoric uncertainty comes from the system's unpredictability with respect to what is observable from the robots' sensors. This type of uncertainty includes uncapturable system dynamics due to missing measure-ments, chaos where the system's trajectory may be unpredictable, and other vital factors like control-rate issues intruding with the expected timing of sensor readings or controller actuation. (ii) Epistemic uncer-tainty where the model itself causes errors. For example, a differential equation may simplify or ignore factors such as air resistance."}, {"title": "4.2 Hardware and Software Design", "content": "The required control rate and the number of executed operations per control loop vary depending on the method used and the dynamical system's behavior. Also, any lag or jitter in executing control tasks on such systems increases the approximation errors of the dynamic models in question. Controller design may then be overly conservative, taking into account the increased aleatoric uncertainty due to the added model approximation errors, otherwise risking violating defined safety criteria. Therefore, enough computational resources must be reserved for control when designing hardware for such systems. In simplistic systems (e.g., single-core systems), it may be enough to provide a computational budget based on the worst-case execution time of specified control tasks. Federated architectures Watkins and Walter [2007] allow this type of reasoning to scale to systems with many predefined tasks, isolating them by separating hardware modules.\nMotivated by reducing Size, Weight, Power, and Cost (SWaP-C), modern embedded systems have multi-core architectures with complex memory layouts, relying on the operating system to manage resources and schedule defined tasks. However, due to the previously mentioned hardware limitations, tasks must share resources when executing, forcing in many cases the loss of compositionality with respect to individual task"}, {"title": "5 Conclusion", "content": "In achieving safety, it's important for the behavior of controlled systems to be well understood. Safety benefits from more predictable systems (e.g. systems with timing guarantees), and from well understood controller specification (e.g. control Lyapunov functions). Thus, tying the understanding of classical methods with the new capabilities offered by recent machine learning techniques allows for a compromise worth exploring further. Existing methods combining specification techniques, learning, and control, though, are limited by"}]}