{"title": "A Survey on Recommendation Unlearning: Fundamentals, Taxonomy, Evaluation, and Open Questions", "authors": ["Yuyuan Li", "Xiaohua Feng", "Chaochao Chen", "Qiang Yang"], "abstract": "Recommender systems have become increasingly influential in shaping user behavior and decision-making, highlighting their growing impact in various domains. Meanwhile, the widespread adoption of machine learning models in recommender systems has raised significant concerns regarding user privacy and security. As compliance with privacy regulations becomes more critical, there is a pressing need to address the issue of recommendation unlearning, i.e., eliminating the memory of specific training data from the learned recommendation models. Despite its importance, traditional machine unlearning methods are ill-suited for recommendation unlearning due to the unique challenges posed by collaborative interactions and model parameters. This survey offers a comprehensive review of the latest advancements in recommendation unlearning, exploring the design principles, challenges, and methodologies associated with this emerging field. We provide a unified taxonomy that categorizes different recommendation unlearning approaches, followed by a summary of widely used benchmarks and metrics for evaluation. By reviewing the current state of research, this survey aims to guide the development of more efficient, scalable, and robust recommendation unlearning techniques. Furthermore, we identify open research questions in this field, which could pave the way for future innovations not only in recommendation unlearning but also in a broader range of unlearning tasks across different machine learning applications.", "sections": [{"title": "I. INTRODUCTION", "content": "OVER the past few years, rapid advancements have propelled machine learning, particularly deep learning, to unprecedented heights [11], [31], [36], [45], [51], [80], [83], [103]. Innovations in algorithms, architectures, and computing power have enabled models to achieve remarkable performance across diverse tasks, revolutionizing industries and reshaping societal norms [1], [9], [22], [23], [44], [92], [93], [101], [111]. Concurrently, exponential growth in data usage for training machine learning models has brought forth pressing concerns, particularly about privacy and security [25], [95], [120]. As more and more personal, sensitive, and potentially harmful data is leveraged for model training, concerns about how this data is collected, stored, and used have intensified.\nIndividuals now face increasing risks to their privacy, with potential misuse or unauthorized access to personal information becoming more prevalent. In this context, regulatory measures such as the General Data Protection Regulation [102], the California Consumer Privacy Act [79], and the Delete Act [40] now empower individuals to exercise their right to be forgotten, demanding the removal of personal data used during model construction.\nThe challenge, however, extends beyond the simple deletion of data. Machine learning models have been shown to exhibit the ability to memorize or retain information from their training data [37], [43]. This means that even if specific data points are removed from a dataset, traces of that data might persist within the model itself, potentially leading to privacy breaches or non-compliance with regulatory requests. This issue has spurred the development of machine unlearning, i.e., a novel field of research aimed at effectively forgetting specific data that was used during model training [10], [54], [64], [70], [71], [73], [91], [106]. While machine unlearning was initially driven by the need to comply with emerging regulations, its scope has expanded beyond legal compliance, revealing significant potential for improving model quality and performance. Unlearning has proven to be a valuable tool not only for addressing privacy concerns but also for removing data that is inaccurate, toxic, biased, or poisoned [53], [76], [114]. By proactively identifying and eliminating such harmful data, machine unlearning can contribute to the creation of more reliable, fair, and robust machine learning models.\nRecommender systems are designed to predict user preferences by analyzing historical interactions, which typically include actions such as clicks, purchases, ratings, and likes. These interactions serve as a foundation for making personalized recommendations and are central to the functioning of many modern digital platforms, from e-commerce websites to streaming services. As machine learning models become more deeply integrated into recommender systems [2], [17], [33], [34], [61], [97], [99], [108], the need for recommendation unlearning has gained significant attention as a critical research area [15]. There are two primary factors that drive the growing necessity of recommendation unlearning. Firstly, historical interactions tend to be intimate and closely tied to user privacy [62], [67], [68], [98], [124], thus carrying a higher likelihood of unlearning requests. Secondly, the precision of recommendation models critically depends on the quality of training data [87], mandating rigorous cleansing regimes to remove dirty data that affects the recommendation performance. As shown in Figure 1, from a practical standpoint, recommendation unlearning involves withdrawal actions within recommendation platforms. This can include activities such as revoking or modifying past likes, ratings, or other forms of feedback, reflecting the dynamic nature of user engagement with platforms. Users may update their preferences, delete or modify past interactions, or request that certain types of recommendations be forgotten. The ability to unlearn data from a model ensures that it remains up-to-date, accurate, and in compliance with user requests and regulatory requirements.\nHowever, traditional unlearning methods, primarily tailored for image classification tasks, are poorly suited to the complex requirements of recommendation unlearning [56]. This misalignment arises from the fundamentally different nature of recommender systems, which rely heavily on collaborative interactions between users and items to provide personalized and relevant suggestions. In contrast, image classification tasks typically involve a more straightforward relationship between inputs and labels, without the intricate dependencies that characterize recommender systems. Traditional unlearning methods, if applied indiscriminately in recommender systems, risk disrupting these delicate collaborative relationships, leading to the loss of crucial patterns that underpin accurate recommendations [15], [57], [58]. As a result, removing data from the model without careful consideration can degrade the model's performance, rendering it less effective at making personalized predictions and reducing the overall quality of the recommendations. Furthermore, the vast parameters of recommendation models, particularly user and item embeddings, exacerbate the inefficiency issue of traditional unlearning [58]. Given these unique challenges, it is clear that recommendation unlearning cannot rely on traditional unlearning techniques. Instead, new and specialized methods are required.\nGiven the growing significance of recommendation unlearning, merely one existing survey have reviewed the existing techniques employed in this area [86]. While such a survey provide valuable insights into the methods currently in use, they often focus narrowly on the technical aspects without offering a holistic view of the emerging challenges and opportunities in the domain. In light of this gap, the aim of this survey is to provide a more comprehensive and in-depth review of the latest advancements in recommendation unlearning, while also exploring potential research directions that could shape the future of this field. This survey not only evaluates existing methodologies but also synthesizes key insights into the design principles and challenges that researchers need to address going forward. We summarize the main contribution of our survey as follows:"}, {"title": "II. FUNDAMENTALS", "content": "In this section, we first introduce the foundational concepts of recommendation unlearning, including the definition of recommendation unlearning, and the basis of recommendation models. Then, we delve into the targets and workflow of recommendation unlearning, followed by the design principles of unlearning methods. While there is a substantial overlap between recommendation unlearning and machine unlearning, we primarily focus on the unique characteristics of recommendation unlearning and provide the related literature for further understanding of machine unlearning."}, {"title": "A. Foundational Concepts", "content": "1) Machine Unlearning: Machine unlearning aims to eliminate the memory of specific training from a given model. Given a dataset D and a learning algorithm A(\u00b7), a model \u03b80 = A(D) can be learned (i.e., the original model before unlearning). An unlearning task typically specifies a target that needs to be unlearned. In earlier literature, the unlearning targets were generally confined to the training data, as the model's memory is learned directly from the training data [10], [12]. We refer to these traditional unlearning tasks as input unlearning, meaning they focus on unlearning the input data used to train the model. A more detailed discussion of different unlearning targets is provided in Section II-B. For clarity and simplicity, we define the unlearning target here as a subset of the training data (i.e., the forget set Df \u2208 D). The remaining training data is therefore defined as the retrain set, where Df \u222a Dr = D.\nAs shown in Figure 2, a straightforward method of unlearning is to retrain the model from scratch using the retain set (i.e., \u03b8\u2217 = A(Dr)) However, with the increasing size of both data and models in modern applications, retraining from scratch becomes computationally prohibitive and impractical. This challenge has, in turn, driven the development of more efficient methods for machine unlearning. The retrained model \u03b8\u2217 serves as the ground truth for unlearning. Therefore, the goal of unlearning is to design an unlearning algorithm U(\u00b7) such that\n$U(A, D_f, D_r, \\theta_0) = A(D_r),$\ns.t. Cost$(U(A,D_f, D_r, \\theta_0)) < Cost(A(D_r)), (1)$\nwhere the input of U(\u00b7) may be reduced under different conditions (e.g., when the retain set is not available), Cost(\u00b7) denotes the computational overhead, and = denotes the degree of unlearning completeness, i.e., to what extent the target has been completely unlearned. This is also referred to as the effectiveness or efficacy of unlearning in the literature. The discussion of evaluation metrics of unlearning completeness will be provided in Section II-D. For clarity, in this paper, we refer to \u03b80 as the original model, \u03b8u = U(A, Df, Dr, \u03b80) as the unlearned model, and \u03b8\u2217 = A(Dr) as the retrained model.\n2) Recommender systems: Recommender systems leverage historical interactions to predict user preference. Most research focuses on the rating prediction task, where users rate various items [96]. Recommendation models leverage observed (i.e., historical) ratings to predict unobserved ones. Collaborative filtering is the foundational approach in modern recommender systems, based on the assumption that users tend to favor similar items, and that an item is likely to be favored by similar users, creating a collaborative effect [49]. Under this approach, matrix factorization-based models have become widely used in recommender systems, both in academia and industry [38], [75]. Existing research on recommendation unlearning also primarily focuses on matrix factorization-based models. Generally, the core idea behind matrix factorization-based models is to learn a user embedding matrix and an item embedding matrix. The predicted ratings are then obtained through the dot product (or other combination methods) of these two matrices. Subsequent research has leveraged deep learning techniques [34], [116], graph learning approaches [33], [81], [125], and Large Language Models (LLMs) [2], [61] to further enhance recommendation performance.\n3) Recommendation Unlearning: Conducting machine unlearning tasks in recommender systems is referred to as recommendation unlearning. In contrast to traditional machine unlearning, where the data record is typically an independent entity such as an image, the data record in recommender systems consists of user-item interactions. Unlearning such a record impacts both user and item, thereby presenting unique challenges of recommendation unlearning.\nRecommender systems represent a real-world scenario where unlearning is particularly important."}, {"title": "B. Unlearning Targets", "content": "Unlearning target is the information that needs to be unlearned, i.e., forget set. As mentioned above, most research on unlearning focuses on the task of input unlearning, where the training data used for model training is treated as the unlearning target. In the context of recommender systems, unlearning targets can be mainly classified into three categories based on the scope of the training data: user-wise, item-wise, and sample-wise. As shown in Figure 4, user/item-wise unlearning targets involve unlearning all samples associated with a specific user/item. Sample-wise targets are more granular than user/item-wise targets, allowing for the selective unlearning of specific samples. User-wise and sample-wise unlearning targets are more commonly favored in research, as these targets are believed to contain user-related privacy information, which has a higher likelihood of being unlearned. In most rating prediction tasks, e.g. a user-item interaction matrix is the training data, user-wise unlearning methods can be directly adapted to item-wise unlearning. This is because, from a matrix perspective, user-wise and item-wise unlearning are essentially equivalent.\nIn addition to the training data, data that does not participate in training may also need to be unlearned in recommender systems. This is because adversaries can potentially infer private information from a trained model, even if that information was never explicitly included in the training data. This type of information, referred to as attributes, is implicitly learned by the model during training. Such attacks are known as attribute inference attacks [27], [41], [123]. The task of unlearning these attributes is called attribute unlearning, which serves as a defense mechanism against attribute inference attacks [18], [26], [59]. As shown in Figure 2, the unlearning target of attribute unlearning is the latent user attributes that are not part of the training data, e.g., gender, age, and race."}, {"title": "C. Unlearning Workflow", "content": "The unlearning process is conducted under specific conditions. As shown in Figure 5, the unlearning workflow consists of both learning and unlearning stages. The learning stage is included because the target of unlearning is the knowledge previously learned by the model, and the model must fully learn this knowledge before unlearning can take place.\nThe time point Te marks the separation between the learning and unlearning stages. After Te, unlearning becomes enabled."}, {"title": "D. Design Principles", "content": "The goal of unlearning is not merely to eliminate the memory of the target being unlearned. It encompasses broader goals. Generally, there are three key design principles for unlearning methods, which are also applicable to recommendation scenarios [15], [56], [57].\na) Unlearning Completeness: Completely unlearning the memory of target data is one of the most fundamental goals of unlearning. As discussed in Section II-A, the completeness of unlearning is defined as the equivalence between the unlearned model and the retrained model. However, the definition of this equivalence varies in the literature, leading to different evaluation metrics. We categorize the definition of completeness equivalence into the following four perspectives.\n* Algorithmic perspective: Only retraining from scratch (i.e., unlearning from the algorithmic level) satisfies the definition of complete unlearning in this perspective. Thus, this definition of completeness can only be self-evaluated algorithmically or verified by providing training checkpoints [42], [100], [109]. Exact unlearning methods adhere strictly to this definition, designing efficient strategies to achieve retraining. All other unlearning methods are classified as approximate unlearning [78].\n* Parametric perspective: This perspective defines equivalence at a parametric level, meaning that the goal is to achieve parameters of the unlearned model similar to those of the retrained model. Since the training of machine learning models involves randomization (e.g., initialization seed and batch order), \"similarity\" is typically defined as being close in distribution. Influence function-based unlearning methods tend to favor this definition, as influence functions provide a closed-form approximation of the retrained model.\n* Functional perspective: This perspective focuses solely on equivalence at a functional level, aiming to ensure that the unlearned model behaves like the retrained model. Specifically, this means the unlearned model should perform poorly on the forgot set while maintaining its original performance on the retain set. This perspective is often preferred in practice, as the model's output is the critical factor in most real-world applications. The relaxed definition of unlearning completeness also allows for the use of a broader range of techniques.\n* Attack perspective: In this attack perspective, complete unlearning is defined as making it impossible for adversaries to recover the unlearning target. By leveraging additional information (e.g., when adversaries exploit the difference between the unlearned model and the original model), this definition of attack-level unlearning could challenge the previous definitions, including the algorithmic one. Therefore, this definition also offers an alternative perspective for evaluating completeness.\nb) Unlearning Efficiency: Given the significant computational cost associated with complex recommender models and large datasets in real-world applications, improving unlearning efficiency, particularly in terms of time, is a crucial goal of unlearning.\nc) Model Utility: The performance of a model, i.e., model utility, relies on the knowledge learned from the training data. Removing too much training data inevitably undermines the learned knowledge, which in turn diminishes model utility. However, an adequate unlearning method should be able to achieve performance that is comparable to that of a retrained model. Avoiding further reduction of model utility (i.e., over-unlearning) is another important goal of unlearning."}, {"title": "III. \u03a4\u0391\u03a7\u039f\u039d\u039f\u039c\u03a5", "content": "In this section, we present a unified taxonomy that organizes the existing recommendation unlearning methods, providing a clear and structured overview of the field. As shown in Figure 6, we first categorize these methods based on the unlearning target into two primary categories: input unlearning and attribute unlearning. By distinguishing between these two primary categories, we lay the groundwork for a more detailed exploration of specific methods within each category, highlighting the various techniques that have been developed to address the unique challenges of recommendation unlearning.\nBased on the types of models that can be applied, existing input unlearning methods can be categorized into two main approaches: model-agnostic and model-specific. The term model-agnostic refers to methods that can be applied regardless of the model structure, while model-specific refers to methods designed for a particular type of model. We also review the unlearning methods for recommendation tasks in specific scenarios, such as federated learning and LLMs, where the recommendation model differs from traditional recommender systems.\n1) Model-agnostic Methods: Model-agnostic unlearning methods for recommendation tasks are largely inspired by traditional machine unlearning techniques used in classification tasks. Based on the definition of unlearning completeness, we further classify the model-agnostic approach into exact unlearning and approximate unlearning [78].\na) Exact Unlearning: As mentioned in Section II-D0a, exact unlearning follows a strict definition of unlearning completeness, achieving it at the algorithmic level. Inspired by SISA [10], exact recommendation unlearning methods predominantly adopt the ensemble retraining framework. As shown in Figure 7, this framework divides the original dataset into multiple subsets, trains a sub-model on each subset, and aggregates all sub-models into the final model, similar to an ensemble learning system. Note that, to ensure algorithmic unlearning completeness, the design of sub-models is typically identical to that of the original model, including the model structure, hyperparameter settings, and other factors. This framework allows for more efficient retraining during unlearning. When an unlearning request is submitted, the ensemble retraining framework only needs to retrain the subset containing the unlearning targets, avoiding the need to retrain the entire dataset from scratch, thereby enhancing unlearning efficiency.\nLi et al. [55] directly apply SISA to recommendation models in intelligence education. Their approach enhances the personalization and accuracy of educational recommendations by selectively forgetting the data inputs for each user.\nBuilding on the design of SISA, Chen et al. propose RecEraser, which introduces two key modifications tailored for recommendation tasks [15]. First, RecEraser incorporates a balanced clustering module for dataset division, grouping similar users or items into the same subset to preserve the collaborative effects within the data, in contrast to the random division used in SISA. Second, RecEraser adds an attention network to learn the weights for the weighted aggregation of sub-models. This adaptive weighted aggregation, compared to the average weighting or majority voting in SISA, further enhances recommendation performance for the ensemble retraining framework.\nDue to the collaborative effect of recommendation data, there is a significant trade-off between unlearning efficiency and model utility. Specifically, increasing the number of dataset divisions can enhance unlearning efficiency, but this also disrupts the collaboration among data, which in turn reduces model utility. To address this issue, Li et al. propose UltraRE, a lightweight modification of RecEraser [56]. UltraRE introduces a new balanced clustering algorithm based on optimal transport theory, which improves both efficiency and clustering performance simultaneously. Additionally, UltraRE simplifies the attention network used during aggregation, replacing it with a logistic regression model to further enhance efficiency.\nTo further enhance model utility, LASER adopts sequential training during aggregation, rather than parallel training [57]. As shown in Figure 7, sequential training involves training one model on a data subset sequentially. This approach helps mitigate the negative impact of dataset division on collaboration. LASER introduces the concept of curriculum learning to optimize the training sequence of data subsets, thereby improving model utility. However, sequential training also reduces unlearning efficiency. To address this issue, LASER introduces early stopping and parameter manipulation.\nHowever, there are several drawbacks of exact unlearning.\n* Exact unlearning requires reformulating the learning process, meaning it cannot be directly applied to an already trained model, which creates significant inconvenience in practical implementation.\n* The efficiency gains from dataset division are limited and incremental, whereas approximate unlearning methods can often provide efficiency improvements that are orders of magnitude better. Additionally, as noted by [56], exact unlearning introduces a trade-off, which is particularly significant in recommendation tasks.\n* The performance of exact unlearning is suboptimal in practice [16]. Although exact unlearning achieves perfect completeness in theory, its empirical performance is unsatisfactory, often worse than approximate unlearning. In real-world scenarios, users prioritize good performance over theoretical guarantees.\nb) Approximate Unlearning: Apart from adopting retraining as exact unlearning, all other unlearning methods are classified as approximate unlearning. These methods either approximate the unlearned model from a parametric perspective or from a functional perspective. As shown in Figure 8, reverse unlearning estimates the influence of the unlearning target and directly obtains the unlearned model without additional training, thereby approximating from a parametric perspective. In contrast, active unlearning fine-tunes the model to obtain the unlearned model, approximating from a functional perspective.\nReverse Unlearning. Deep learning models are typically trained using gradient descent-based optimization. An intuitive approach for unlearning is to add back the gradient of the target data that was previously subtracted, thereby mitigating its influence on the model and achieving the goal of unlearning from a parametric perspective. Formally, let us assume that zi \u2208 D is a data record from the training set, and the recommendation loss function is l(zi, \u03b8). We further assume that the loss function is twice-differentiable and strictly convex, which holds true for the majority of recommendation loss functions. Following the empirical risk minimization framework, the optimal model parameter is given by\n$\\theta_0 = \\arg \\min_\\theta \\sum_{i=1}^n l(z_i, \\theta), (2)$\nwhere n is the number of records, and for simplicity, we omit the regularization term in the loss function. According to the definition of algorithmic-level unlearning completeness, unlearning requires generating a model that retrains from scratch without the target data record. Thus, unlearning a data point z can be formulated as:\n$\\theta^* = \\arg \\min_\\theta \\sum_{i=1}^n l(z_i, \\theta) - l(z, \\theta). (3)$\nThe early study in traditional machine unlearning explores directly adding back the first-order gradient [28]. However, due to the stochastic nature and randomness inherent in deep learning, the first-order gradient is not always accurate for estimating the influence of the target data. Consequently, subsequent research has focused on more precisely estimating the influence and directly subtracting this influence from the original model [90]. This process is simply formulated as:\n$\\theta^\\text{\\ensuremath{u}} = \\theta - I(z), (4)$\nwhere \u03b8u is the model that is unlearned with data record z, and I(z) denotes the estimated influence of z. Note that this formulation can be easily generalized to unlearning a set of data records. For the sake of simplicity and clarity, we use a single data record as an example. Compared to exact unlearning, the main advantage of reverse unlearning is its ease of implementation. It only requires direct manipulation of the model parameters, without interfering with the original training workflow, and can be applied to an already trained model.\nCurrent reverse unlearning methods in recommender systems mainly rely on influence function [7], [46], [47] to estimate the influence of target data on model parameters. Specifically, it estimates the influence by weighting a data record by \u03f5. Formally, the \u03f5-weighted model parameter is\n$\\theta_{\\epsilon,z} = \\arg \\min_\\theta \\sum_{i=1}^n l(z_i, \\theta) + \\epsilon l(z, \\theta). (5)$\nAccording to [46], leverage second-order Talor expansion, the estimated influence of z is given by\n$I(z) := \\frac{\\partial \\theta_{\\epsilon,z}}{\\partial \\epsilon}|_{\\epsilon=0} = -H_0^{-1}\\nabla_{\\theta} l(z, \\theta_0), (6)$\nwhere \u2207\u03b8l(z, \u03b80) is the gradient vector, and H0 := \u2211i=1\u2207\u03b82l(zi, \u03b80) is the Hessian matrix and is positive definite by assumption. The derivation in Eq.(6) is equivalent to a single step of the Newton optimization update. Therefore, influence function-based reverse unlearning methods can be interpreted as performing a one-step reverse Newton update.\nZhang et al. [121] directly apply this closed-form one-step reverse Newton update to the matrix factorization model. There are also other tasks in recommender systems that utilize the influence function. For example, Wang et al. [105] study the user-controllable recommendation task, where users can control which interactions are used for training. This can also be formulated as a recommendation unlearning task.\nAlthough influence function-based unlearning methods theoretically provide a promising solution for recommendation unlearning, they face significant challenges in terms of computational efficiency and estimation accuracy.\nFirstly, due to the large number of users and items in recommendation tasks, the user and item embeddings form high-dimensional dense matrices, which incur considerable computational overhead when explicitly calculating the influence function. As shown in Eq.(6), this calculation involves computing the inverse of the Hessian matrix and then multiplying it by the gradient vector, which has a computational complexity of O(n\u00b2). Since the forget set typically accounts for a small portion of the training data, Li et al. [58] found that the parameter changes before and after unlearning primarily affect the embeddings of the unlearning target. Changes to other parameters (e.g., non-targeted embeddings and parameters unrelated to embeddings) are negligible. Consequently, they propose selectively calculating the influence function for the target embeddings only, thereby reducing computational overhead at the root level. Similarly, Zhang et al. [122] propose an importance-based pruning strategy to reduce the model size, thereby lowering the overall computational overhead of the influence function. The importance is estimated based on the neighborhood relationships among the unlearning targets.\nSecondly, influence function is essentially an approximation, and its accuracy is not always guaranteed, particularly for deep learning models [6]. In recommendation tasks, removing the influence of target data can have a cascading effect on neighboring data, thereby further compromising model utility. To address this issue, Li et al. [58] incorporate a collaborative term into Eq. (3) to mitigate this negative effect. Instead of directly removing the target data, this collaborative term replaces it with the average rating of the neighboring data, which compensates for the inaccurate estimation of influence function.\nNevertheless, there are several drawbacks of reverse unlearning."}, {"title": "IV. EVALUATION", "content": "Based on the design principles outlined above, the evaluation of recommendation unlearning methods primarily focuses on three key aspects: unlearning completeness, unlearning efficiency, and model utility. Each of these aspects plays a crucial role in evaluating the effectiveness and practicality of unlearning techniques in real-world recommender systems.\nRecommendation unlearning methods use the same datasets as other recommendation tasks. We list the widely used datasets and summarize the statistics in Table II."}, {"title": "V. DISCUSSION AND FUTURE DIRECTIONS", "content": "Based on the presented taxonomy, we provide a comprehensive summary of the similarities and differences among various recommendation unlearning methods (i.e., exact unlearning vs. approximate unlearning), highlighting the key trends and developments in existing approaches."}, {"title": "A. Summary and Trends", "content": "We summarize the characteristics and trends of existing research on recommendation unlearning and compare it with traditional machine unlearning. Our summary focuses on three key aspects: unlearning audits, unlearning methods, and emerging research tasks."}]}