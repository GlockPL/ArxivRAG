[{"title": "A Survey on Recommendation Unlearning: Fundamentals, Taxonomy, Evaluation, and Open Questions", "authors": ["Yuyuan Li", "Xiaohua Feng", "Chaochao Chen", "Qiang Yang"], "abstract": "Recommender systems have become increasingly influential in shaping user behavior and decision-making, highlighting their growing impact in various domains. Meanwhile, the widespread adoption of machine learning models in recommender systems has raised significant concerns regarding user privacy and security. As compliance with privacy regulations becomes more critical, there is a pressing need to address the issue of recommendation unlearning, i.e., eliminating the memory of specific training data from the learned recommendation models. Despite its importance, traditional machine unlearning methods are ill-suited for recommendation unlearning due to the unique challenges posed by collaborative interactions and model parameters. This survey offers a comprehensive review of the latest advancements in recommendation unlearning, exploring the design principles, challenges, and methodologies associated with this emerging field. We provide a unified taxonomy that categorizes different recommendation unlearning approaches, followed by a summary of widely used benchmarks and metrics for evaluation. By reviewing the current state of research, this survey aims to guide the development of more efficient, scalable, and robust recommendation unlearning techniques. Furthermore, we identify open research questions in this field, which could pave the way for future innovations not only in recommendation unlearning but also in a broader range of unlearning tasks across different machine learning applications.", "sections": [{"title": "I. INTRODUCTION", "content": "VER the past few years, rapid advancements have propelled machine learning, particularly deep learning, to unprecedented heights [11], [31], [36], [45], [51], [80], [83], [103]. Innovations in algorithms, architectures, and computing power have enabled models to achieve remarkable performance across diverse tasks, revolutionizing industries and reshaping societal norms [1], [9], [22], [23], [44], [92], [93], [101], [111]. Concurrently, exponential growth in data usage for training machine learning models has brought forth pressing concerns, particularly about privacy and security [25], [95], [120]. As more and more personal, sensitive, and potentially harmful data is leveraged for model training, concerns about how this data is collected, stored, and used have intensified.\nIndividuals now face increasing risks to their privacy, with potential misuse or unauthorized access to personal information becoming more prevalent. In this context, regulatory measures such as the General Data Protection Regulation [102], the California Consumer Privacy Act [79], and the Delete Act [40] now empower individuals to exercise their right to be forgotten, demanding the removal of personal data used during model construction.\nThe challenge, however, extends beyond the simple deletion of data. Machine learning models have been shown to exhibit the ability to memorize or retain information from their training data [37], [43]. This means that even if specific data points are removed from a dataset, traces of that data might persist within the model itself, potentially leading to privacy breaches or non-compliance with regulatory requests. This issue has spurred the development of machine unlearning, i.e., a novel field of research aimed at effectively forgetting specific data that was used during model training [10], [54], [64], [70], [71], [73], [91], [106]. While machine unlearning was initially driven by the need to comply with emerging regulations, its scope has expanded beyond legal compliance, revealing significant potential for improving model quality and performance. Unlearning has proven to be a valuable tool not only for addressing privacy concerns but also for removing data that is inaccurate, toxic, biased, or poisoned [53], [76], [114]. By proactively identifying and eliminating such harmful data, machine unlearning can contribute to the creation of more reliable, fair, and robust machine learning models.\nRecommender systems are designed to predict user preferences by analyzing historical interactions, which typically include actions such as clicks, purchases, ratings, and likes. These interactions serve as a foundation for making personalized recommendations and are central to the functioning of many modern digital platforms, from e-commerce websites to streaming services. As machine learning models become more deeply integrated into recommender systems [2], [17], [33], [34], [61], [97], [99], [108], the need for recommendation unlearning has gained significant attention as a critical research area [15]. There are two primary factors that drive the growing necessity of recommendation unlearning. Firstly, historical interactions tend to be intimate and closely tied to user privacy [62], [67], [68], [98], [124], thus carrying a higher likelihood of unlearning requests. Secondly, the precision of recommendation models critically depends on the quality of training data [87], mandating rigorous cleansing regimes to remove dirty data that affects the recommendation performance. As shown in Figure 1, from a practical standpoint, recommendation unlearning involves withdrawal actions within recommendation platforms. This can include activities such as revoking or modifying past likes, ratings, or other forms of feedback, reflecting the dynamic nature of user engagement with platforms. Users may update their preferences, delete or modify past interactions, or request that certain types of recommendations be forgotten. The ability to unlearn data from a model ensures that it remains up-to-date, accurate, and in compliance with user requests and regulatory requirements.\nHowever, traditional unlearning methods, primarily tailored for image classification tasks, are poorly suited to the complex requirements of recommendation unlearning [56]. This misalignment arises from the fundamentally different nature of recommender systems, which rely heavily on collaborative interactions between users and items to provide personalized and relevant suggestions. In contrast, image classification tasks typically involve a more straightforward relationship between inputs and labels, without the intricate dependencies that characterize recommender systems. Traditional unlearning methods, if applied indiscriminately in recommender systems, risk disrupting these delicate collaborative relationships, leading to the loss of crucial patterns that underpin accurate recommendations [15], [57], [58]. As a result, removing data from the model without careful consideration can degrade the model's performance, rendering it less effective at making personalized predictions and reducing the overall quality of the recommendations. Furthermore, the vast parameters of recommendation models, particularly user and item embeddings, exacerbate the inefficiency issue of traditional unlearning [58]. Given these unique challenges, it is clear that recommendation unlearning cannot rely on traditional unlearning techniques. Instead, new and specialized methods are required.\nGiven the growing significance of recommendation unlearning, merely one existing survey have reviewed the existing techniques employed in this area [86]. While such a survey provide valuable insights into the methods currently in use, they often focus narrowly on the technical aspects without offering a holistic view of the emerging challenges and opportunities in the domain. In light of this gap, the aim of this survey is to provide a more comprehensive and in-depth review of the latest advancements in recommendation unlearning, while also exploring potential research directions that could shape the future of this field. This survey not only evaluates existing methodologies but also synthesizes key insights into the design principles and challenges that researchers need to address going forward. We summarize the main contribution of our survey as follows:\n\u2022Foundational Introduction (Section II): Beginning with foundational concepts of recommendation unlearning, we establish a clear understanding of recommendation unlearning target, unlearning workflow, and design principles that guide the development of unlearning techniques.\n\u2022Taxonomy Development (Section III): We present a unified taxonomy of existing recommendation unlearning methods to clarify the unlearning targets, main techniques, and focused problems of existing methods. This taxonomy provides clarity on the diversity of approaches and highlights the nuances that distinguish them.\n\u2022Evaluation Summary (Section IV): We summarize the evaluation resources for recommendation unlearning, including the commonly used datasets, recommendation models, and evaluation metrics regarding each design principle. By providing a clear mapping of these resources, we aim to assist researchers in selecting the appropriate tools for evaluating the effectiveness of different unlearning approaches.\n\u2022Open Questions Exploration (Section V): We pose the remaining challenges, probe open research questions in the field, aiming to explore the potential research direction and to inspire new approaches for recommendation unlearning."}, {"title": "II. FUNDAMENTALS", "content": "In this section, we first introduce the foundational concepts of recommendation unlearning, including the definition of recommendation unlearning, and the basis of recommendation models. Then, we delve into the targets and workflow of recommendation unlearning, followed by the design principles of unlearning methods. While there is a substantial overlap between recommendation unlearning and machine unlearning, we primarily focus on the unique characteristics of recommendation unlearning and provide the related literature for further understanding of machine unlearning."}, {"title": "A. Foundational Concepts", "content": "1) Machine Unlearning: Machine unlearning aims to eliminate the memory of specific training from a given model. Given a dataset D and a learning algorithm A(\u00b7), a model \u03b8\u2080 = A(D) can be learned (i.e., the original model before unlearning). An unlearning task typically specifies a target that needs to be unlearned. In earlier literature, the unlearning targets were generally confined to the training data, as the model's memory is learned directly from the training data [10], [12]. We refer to these traditional unlearning tasks as input unlearning, meaning they focus on unlearning the input data used to train the model. A more detailed discussion of different unlearning targets is provided in Section II-B. For clarity and simplicity, we define the unlearning target here as a subset of the training data (i.e., the forget set Df \u2208 D). The remaining training data is therefore defined as the retrain set, where Df \u222a Dr = D.\nAs shown in Figure 2, a straightforward method of unlearning is to retrain the model from scratch using the retain set (i.e., \u03b8\u2217 = A(Dr)) However, with the increasing size of both data and models in modern applications, retraining from scratch becomes computationally prohibitive and impractical. This challenge has, in turn, driven the development of more efficient methods for machine unlearning. The retrained model \u03b8\u2217 serves as the ground truth for unlearning. Therefore, the goal of unlearning is to design an unlearning algorithm U(\u00b7) such that\n$U(A, D_f, D_r, \\theta_0) = A(D_r),$\ns.t. Cost$(U(A,D_f, D_r, \\theta_0)) <$ Cost$(A(D_r)),$\n(1)\nwhere the input of U(\u00b7) may be reduced under different conditions (e.g., when the retain set is not available), Cost(\u00b7) denotes the computational overhead, and = denotes the degree of unlearning completeness, i.e., to what extent the target has been completely unlearned. This is also referred to as the effectiveness or efficacy of unlearning in the literature. The discussion of evaluation metrics of unlearning completeness will be provided in Section II-D. For clarity, in this paper, we refer to \u03b8 as the original model, \u03b8u = U(A,Df , Dr , \u03b8\u2080) as the unlearned model, and \u03b8\u2217 = A(Dr) as the retrained model.\n2) Recommender systems: Recommender systems leverage historical interactions to predict user preference. Most research focuses on the rating prediction task, where users rate various items [96]. Recommendation models leverage observed (i.e., historical) ratings to predict unobserved ones. Collaborative filtering is the foundational approach in modern recommender systems, based on the assumption that users tend to favor similar items, and that an item is likely to be favored by similar users, creating a collaborative effect [49]. Under this approach, matrix factorization-based models have become widely used in recommender systems, both in academia and industry [38], [75]. Existing research on recommendation unlearning also primarily focuses on matrix factorization-based models. Generally, the core idea behind matrix factorization-based models is to learn a user embedding matrix and an item embedding matrix. The predicted ratings are then obtained through the dot product (or other combination methods) of these two matrices. Subsequent research has leveraged deep learning techniques [34], [116], graph learning approaches [33], [81], [125], and Large Language Models (LLMs) [2], [61] to further enhance recommendation performance.\n3) Recommendation Unlearning: Conducting machine unlearning tasks in recommender systems is referred to as recommendation unlearning. In contrast to traditional machine unlearning, where the data record is typically an independent entity such as an image, the data record in recommender systems consists of user-item interactions. Unlearning such a record impacts both user and item, thereby presenting unique challenges of recommendation unlearning.\nRecommender systems represent a real-world scenario where unlearning is particularly important.\n\u2022First, modern recommender systems widely employ machine learning or deep learning models, which can retain substantial memory of the training data.\n\u2022Second, due to the nature of recommendation applications, these systems inherently collect vast amounts of user data, increasing the likelihood of receiving unlearning requests.\nFor example, a user's ratings, search queries, or browsing history can reveal much about their preferences, habits, and even personal life. As privacy concerns intensify in the digital age, individuals are increasingly seeking control over their data, including the ability to remove or forget data that has been used to train recommender models.\n\u2022Last but not least, recommendation models are highly sensitive to the quality of the training data [87], creating a need for systems to unlearn problematic data proactively.\nIn practice, data is often noisy, incomplete, or biased, which can degrade the effectiveness of recommender systems. In some cases, data may be deliberately poisoned by malicious users or reflect biased user behaviors, which can negatively affect the system's outputs. To mitigate these risks, recommender systems must undergo regular data cleansing processes to remove or adjust problematic data. Recommendation unlearning plays an essential role in this context by providing a way to efficiently remove undesirable data without requiring a full retraining of the model, which can be resource-intensive and time-consuming.\nHowever, traditional machine unlearning methods cannot be indiscriminately applied to recommender systems, due to the unique challenges posed by collaborative interactions and parameters of recommendation models. Firstly, as shown in Figure 3, due to the collaborative effect of recommendation data, unlearning the forget set may inadvertently affect the associated data, disrupting its memory. This causes a breakdown in the quality of recommendations and potentially leads to a diminished user experience. Conversely, the retain set can leverage the collaborative effects to re-remember the forget set, thereby causing non-compliance with the unlearning request. Secondly, the user and item embeddings, which are crucial parameters in recommendation models, capture the features of users and items. As a result, existing recommendation unlearning methods typically focus on manipulating these embeddings. However, because the user and item embeddings form high-dimensional and dense matrices, and the number of users and items is massive, applying traditional unlearning methods to these embeddings introduces significant computational overhead, which makes recommendation unlearning even more challenging."}, {"title": "B. Unlearning Targets", "content": "Unlearning target is the information that needs to be unlearned, i.e., forget set. As mentioned above, most research on unlearning focuses on the task of input unlearning, where the training data used for model training is treated as the unlearning target. In the context of recommender systems, unlearning targets can be mainly classified into three categories based on the scope of the training data: user-wise, item-wise, and sample-wise. As shown in Figure 4, user/item-wise unlearning targets involve unlearning all samples associated with a specific user/item. Sample-wise targets are more granular than user/item-wise targets, allowing for the selective unlearning of specific samples. User-wise and sample-wise unlearning targets are more commonly favored in research, as these targets are believed to contain user-related privacy information, which has a higher likelihood of being unlearned. In most rating prediction tasks, e.g. a user-item interaction matrix is the training data, user-wise unlearning methods can be directly adapted to item-wise unlearning. This is because, from a matrix perspective, user-wise and item-wise unlearning are essentially equivalent.\nIn addition to the training data, data that does not participate in training may also need to be unlearned in recommender systems. This is because adversaries can potentially infer private information from a trained model, even if that information was never explicitly included in the training data. This type of information, referred to as attributes, is implicitly learned by the model during training. Such attacks are known as attribute inference attacks [27], [41], [123]. The task of unlearning these attributes is called attribute unlearning, which serves as a defense mechanism against attribute inference attacks [18], [26], [59]. As shown in Figure 2, the unlearning target of attribute unlearning is the latent user attributes that are not part of the training data, e.g., gender, age, and race."}, {"title": "C. Unlearning Workflow", "content": "The unlearning process is conducted under specific conditions. As shown in Figure 5, the unlearning workflow consists of both learning and unlearning stages. The learning stage is included because the target of unlearning is the knowledge previously learned by the model, and the model must fully learn this knowledge before unlearning can take place.\nThe time point Te marks the separation between the learning and unlearning stages. After Te, unlearning becomes enabled."}, {"title": "D. Design Principles", "content": "The goal of unlearning is not merely to eliminate the memory of the target being unlearned. It encompasses broader goals. Generally, there are three key design principles for unlearning methods, which are also applicable to recommendation scenarios [15], [56], [57].\na) Unlearning Completeness: Completely unlearning the memory of target data is one of the most fundamental goals of unlearning. As discussed in Section II-A, the completeness of unlearning is defined as the equivalence between the unlearned model and the retrained model. However, the definition of this equivalence varies in the literature, leading to different evaluation metrics. We categorize the definition of completeness equivalence into the following four perspectives.\n\u2022Algorithmic perspective: Only retraining from scratch (i.e., unlearning from the algorithmic level) satisfies the definition of complete unlearning in this perspective. Thus, this definition of completeness can only be self-evaluated algorithmically or verified by providing training checkpoints [42], [100], [109]. Exact unlearning methods adhere strictly to this definition, designing efficient strategies to achieve retraining. All other unlearning methods are classified as approximate unlearning [78].\n\u2022Parametric perspective: This perspective defines equivalence at a parametric level, meaning that the goal is to achieve parameters of the unlearned model similar to those of the retrained model. Since the training of machine learning models involves randomization (e.g., initialization seed and batch order), \"similarity\" is typically defined as being close in distribution. Influence function-based unlearning methods tend to favor this definition, as influence functions provide a closed-form approximation of the retrained model.\n\u2022Functional perspective: This perspective focuses solely on equivalence at a functional level, aiming to ensure that the unlearned model behaves like the retrained model. Specifically, this means the unlearned model should perform poorly on the forgot set while maintaining its original performance on the retain set. This perspective is often preferred in practice, as the model's output is the critical factor in most real-world applications. The relaxed definition of unlearning completeness also allows for the use of a broader range of techniques.\n\u2022Attack perspective: In this attack perspective, complete unlearning is defined as making it impossible for adversaries to recover the unlearning target. By leveraging additional information (e.g., when adversaries exploit the difference between the unlearned model and the original model), this definition of attack-level unlearning could challenge the previous definitions, including the algorithmic one. Therefore, this definition also offers an alternative perspective for evaluating completeness.\nThe specific examples of evaluation metrics are introduced in Section IV-C.\nb) Unlearning Efficiency: Given the significant computational cost associated with complex recommender models and large datasets in real-world applications, improving unlearning efficiency, particularly in terms of time, is a crucial goal of unlearning.\nc) Model Utility: The performance of a model, i.e., model utility, relies on the knowledge learned from the training data. Removing too much training data inevitably undermines the learned knowledge, which in turn diminishes model utility. However, an adequate unlearning method should be able to achieve performance that is comparable to that of a retrained model. Avoiding further reduction of model utility (i.e., over-unlearning) is another important goal of unlearning."}, {"title": "III. \u03a4\u0391\u03a7\u039f\u039d\u039f\u039c\u03a5", "content": "In this section", "categories": "input unlearning and attribute unlearning. By distinguishing between these two primary categories", "approaches": "model-agnostic and model-specific. The term model-agnostic refers to methods that can be applied regardless of the model structure", "Methods": "Model-agnostic unlearning methods for recommendation tasks are largely inspired by traditional machine unlearning techniques used in classification tasks. Based on the definition of unlearning completeness", "78": ".", "Unlearning": "As mentioned in Section II-D0a", "10": "exact recommendation unlearning methods predominantly adopt the ensemble retraining framework. As shown in Figure 7", "55": "directly apply SISA to recommendation models in intelligence education. Their approach enhances the personalization and accuracy of educational recommendations by selectively forgetting the data inputs for each user.\nBuilding on the design of SISA", "15": ".", "56": ".", "57": "."}, {"56": "exact unlearning introduces a trade-off", "16": ".", "Unlearning": "Apart from adopting retraining as exact unlearning", "as": "n$\\theta^* = \\arg \\min_\\theta \\sum_{i=1"}], "as": "n$\\theta^u = \\theta - I(z)", "by\n$I(z)": "frac{\\partial \\theta_{\\epsilon", "H\u03b8\u2080": "sum_{i=1"}, "n \\nabla_\\theta l(z_i", "theta_0)$ is the Hessian matrix and is positive definite by assumption. The derivation in Eq.(6) is equivalent to a single step of the Newton optimization update. Therefore", "influence function-based reverse unlearning methods can be interpreted as performing a one-step reverse Newton update.\nZhang et al. [121] directly apply this closed-form one-step reverse Newton update to the matrix factorization model. There are also other tasks in recommender systems that utilize the influence function. For example", "Wang et al. [105] study the user-controllable recommendation task", "where users can control which interactions are used for training. This can also be formulated as a recommendation unlearning task.\nAlthough influence function-based unlearning methods theoretically provide a promising solution for recommendation unlearning", "they face significant challenges in terms of computational efficiency and estimation accuracy.\nFirstly", "due to the large number of users and items in recommendation tasks", "the user and item embeddings form high-dimensional dense matrices", "which incur considerable computational overhead when explicitly calculating the influence function. As shown in Eq.(6)", "this calculation involves computing the inverse of the Hessian matrix and then multiplying it by the gradient vector", "which has a computational complexity of O(n\u00b2). Since the forget set typically accounts for a small portion of the training data", "Li et al. [58] found that the parameter changes before and after unlearning primarily affect the embeddings of the unlearning target. Changes to other parameters (e.g.", "non-targeted embeddings and parameters unrelated to embeddings) are negligible. Consequently", "they propose selectively calculating the influence function for the target embeddings only", "thereby reducing computational overhead at the root level. Similarly", "Zhang et al. [122] propose an importance-based pruning strategy to reduce the model size", "thereby lowering the overall computational overhead of the influence function. The importance is estimated based on the neighborhood relationships among the unlearning targets.\nSecondly", "influence function is essentially an approximation", "and its accuracy is not always guaranteed", "particularly for deep learning models [6]. In recommendation tasks", "removing the influence of target data can have a cascading effect on neighboring data", "thereby further compromising model utility. To address this issue", "Li et al. [58] incorporate a collaborative term into Eq. (3) to mitigate this negative effect. Instead of directly removing the target data", "this collaborative term replaces it with the average rating of the neighboring data", "which compensates for the inaccurate estimation of influence function.\nNevertheless", "there are several drawbacks of reverse unlearning.\n\u2022There is a trade-off between accurate estimation and computational efficiency. On the one hand", "explicitly calculating the influence function incurs considerable computational overhead. On the other hand", "accelerating the calculation through approximation inevitably reduces the accuracy of the influence estimation.\n\u2022The derivation of the influence function assumes that only a small portion of the training data is removed. The tolerance for the volume of the forget set is not comparable to exact unlearning. Moreover", "in real-world scenarios where unlearning requests are submitted sequentially and must be processed immediately (i.e.", "sequential/stream unlearning [29])", "the assumption that only a small portion of the data is unlearned no longer holds.\nActive Unlearning. From a functional perspective", "the goal of unlearning is to make the unlearned model perform as if it were trained from scratch. One straightforward approach is to continue the learning process but with a new objective. Active unlearning fine-tunes the model to achieve this", "essentially learning to unlearn. As a result", "the key challenge is designing an appropriate loss function for the fine-tuning process.\nLie et al. [66] first propose active unlearning in recommendation tasks by fine-tuning the original model on the retain set", "which is faster than retraining the model from scratch. Furthermore", "they apply a 2nd-order gradient optimization technique to further enhance the efficiency of this unlearning process.\nAlshehri et al. [3] propose active unlearning by flipping the labels of data in forget set. However", "fine-tuning the model with label-flipped records may unintentionally overwrite the parameters of the original model", "negatively impacting model utility for other users. To address this issue", "they also sample data from the retain set during fine-tuning.\nLabel flipping is limited to binary ratings {0", 1, "which presents challenges when dealing with value-based ratings. To overcome this issue", "Sinha et al. [94] propose flipping the loss function instead of the labels", "reversing the direction of the loss (i.e.", "changing addition to subtraction). They also utilize data from the retain set to prevent over-unlearning. These two loss functions (i.e.", "flipped loss with the forget set and original loss with the retain set) are linearly combined with a balancing coefficient. Similarly", "You et al. [118] also flip the loss function and tune it on the forget set. The key difference from previous methods is that they solely rely on the flipped loss", "updating it in a single step using the Fisher information matrix. This approach does not depend on the retain set", "making it more convenient and efficient for implementation in real-world applications. Dang et al. fine-tune the slipped loss by introducing an additional module to enhance utility [21].\nIn addition to fine-tuning", "Chaturvedi et al. [14] propose a model fusion approach to achieve active unlearning. Specifically", "they fine-tune one model using noisy data and another", "smaller model using the retained data set. They then apply a convolutional fusion function to combine the models", "thereby efficiently enabling unlearning.\nHowever", "there are also several drawbacks of active unlearning:\n\u2022Active unlearning is a functional approach", "meaning there is no theoretical guarantee for the obtained results. The termination point of fine-tuning is solely dependent on the model's performance. Although loss flipping can expand the range of applicable ratings", "it essentially performs gradient ascent", "which is unbounded and can lead to instability during training.\n\u2022The one-step Fisher information matrix update is theoretically more promising", "but it faces the computational challenge of calculating the Hessian matrix. While this can be computed offline", "the high memory storage requirements present a significant challenge for many unlearning executors.\nAnother emerging direction for achieving unlearning also takes a functional perspective", "but with an approach distinct from \"learning to unlearn\". Liu et al. [65] reveal that collaborative filtering can be formulated as a mapping-based approach", "where the recommendations are obtained by multiplying the user-item interaction matrix with a mapping matrix. This formulation simplifies unlearning to the manipulation of the mapping matrix. While this method provides valuable insights into model-agnostic recommendation unlearning", "it has some limitations. The arbitrary approximation of recommendation models as mapping-based approaches lacks a solid theoretical foundation", "making it highly dependent on the accuracy of the mapping matrix approximation.\n2) Model-specific Methods: In addition to model-agnostic methods", "which are designed to work with a wide range of recommendation models (typically collaborative filtering)", "there are also methods specifically tailored to the structure and characteristics of particular model types. These specialized techniques are often more efficient and effective because they leverage the inherent properties of the model architecture.\n\u2022Bi-linear recommendation model: Xu et al. [115] propose an exact unlearning method for bilinear recommendation models", "which utilize alternating least squares for optimization [35]. The core idea of their approach is fine-tuning. In these models", "the confidence matrix", "which is multiplied by the predicted ratings", "plays a key role. By setting the target elements of this matrix to zero during fine-tuning", "the method effectively performs exact unlearning. However", "the authors also note that exact unlearning may not always be feasible in real-world applications. This is because additional optimization techniques", "such as early stopping", "are often employed", "which can introduce complexities that prevent exact unlearning from being fully realized.\n\u2022KNN-based recommendation model: k-Nearest Neighbor (KNN) is widely used in a variety of recommendation scenarios [4]", [74], [85], "due to its several key advantages. These include its transparency and explainability", "cost-effectiveness in scaling to industrial workloads", "and significantly lower training time. KNN's simplicity and efficiency make it an attractive choice for both research and practical applications in the field of recommender systems. Unlike the models discussed in the review", "KNN is a non-parametric model", "which inherently facilitates completeness by simply removing the unlearning target. Schelter et al. [89] propose an efficient indexing method to accelerate this unlearning process", "making it both faster and more scalable.\n\u2022Graph-based recommendation model: Graph Neural Network ("]