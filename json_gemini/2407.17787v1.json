{"title": "HC-GST: Heterophily-aware Distribution Consistency based Graph Self-training", "authors": ["Fali Wang", "Tianxiang Zhao", "Junjie Xu", "Suhang Wang"], "abstract": "Graph self-training (GST), which selects and assigns pseudo-labels to unlabeled nodes, is popular for tackling label sparsity in graphs. However, recent study on homophily graphs show that GST methods could introduce and amplify distribution shift between training and test nodes as they tend to assign pseudo-labels to nodes they are good at. As GNNs typically perform better on homophilic nodes, there could be potential shifts towards homophilic pseudo-nodes, which is underexplored. Our preliminary experiments on heterophilic graphs verify that these methods can cause shifts in homophily ratio distributions, leading to training bias that improves performance on homophilic nodes while degrading it on heterophilic ones. Therefore, we study a novel problem of reducing homophily ratio distribution shifts during self-training on heterophilic graphs. A key challenge is the accurate calculation of homophily ratios and their distributions without extensive labeled data. To tackle them, we propose a novel Heterophily-aware Distribution Consistency-based Graph Self-Training (HC-GST) framework, which estimates homophily ratios using soft labels and optimizes a selection vector to align pseudo-nodes with the global homophily ratio distribution. Extensive experiments on both homophilic and heterophilic graphs show that HC-GST effectively reduces training bias and enhances self-training performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are pervasive in the real world, such as in social networks [25], knowledge graphs [30], and traffic networks [37]. Graph Neural Networks (GNNs) have achieved significant advances in semi-supervised node classification across both homophilic and heterophilic graphs [1, 5, 8, 13, 20, 38]. Despite their success, they often rely on abundant labeled data [6, 32] for training, yet manual labeling is not only labor-intensive but also impractical for new classes with sparse samples [33]. This has spurred increasing interest in node classification with sparse labels [18, 29].\nGraph self-training (GST) emerges as a promising method to harness abundant unlabeled nodes alongside a limited number of labeled nodes to tackle label sparsity in graphs [14, 16]. It typically involves three core steps: (i) pseudo-node selection, selecting high-confidence nodes; (ii) pseudo-label assignment, assigning the most probable labels; and (iii) retraining, where the GNN is trained on the augmented set, iterating these steps until convergence or a specified number of stages. However, typical confidence-based pseudo-node selection can cause distribution shifts [18, 44], potentially undermining self-training. This occurs as an increasing number of easy nodes, identified by their high confidence, are added to the original labeled training set, leading to the distribution gradually shifting towards this augmented training set and excessive focus on such easy nodes as a result. Methods like DRGST [18] and DCGST [31] address this by quantifying distribution shifts through information gain and GNN representation, respectively, and selecting pseudo-nodes that align with the distribution of the unlabeled set. Nevertheless, these methods are tailored for homophilic graphs, where connected nodes tend to have similar labels. In contrast, they neglect shifts in neighboring distributions (homophily ratio) in heterophilic graphs, where GNNs perform well on homophilic nodes but struggle with heterophilic ones, resulting in novel distribution shifts.\nTo investigate self-training on heterophilic graphs, our preliminary experiments on the representative Chameleon graph [26] in Sec.3.3 show that: (1) Training sets aligned with the true homophily distributions yield the best performance, as shown in Fig.1 (a); (2) GNNs excel on homophilic nodes, with accuracy positively correlating with the homophily ratio, evident from the blue line in Fig.1 (c). (3) Traditional self-training (ST) tends to select pseudo-nodes with higher homophily ratios, as depicted in Fig.1 (b), resulting in distribution shifts; (4) Strategies based on confidence or GNN representation consistency often lead to training bias, improving performance on homophilic nodes but worsening it on heterophilic nodes, as illustrated in Fig.1 (c) and (d). These results underscore the need for maintaining consistency in homophily ratio distribution (node counts across homophily bins) to mitigate training bias and improve self-training, a gap not fully addressed by existing methods. Thus, ensuring consistency between the homophily ratio distribution of the local (pseudo-)labeled training set and the global heterophilic graph to reduce training bias across homophily bins remains an open question.\nTherefore, in this paper, we study a novel problem of homophily ratio distribution consistency during self-training on heterophilic graphs. However, this presents challenges to maximizing self-training performance in obtaining accurate homophily ratios and distributions, aligning homophily ratios between (pseudo-)labeled nodes and the true distribution, and the low pseudo-labeling accuracy on heterophilic nodes. Thus, we pose a critical research question: How can we develop a GST framework which ensures selected pseudo-nodes align with true homophily ratio distributions and assigns accurate pseudo-labels to highly heterophilic nodes?\nTo address these challenges, we propose a novel Heterophily-aware Distribution Consistency-based Graph Self-Training (HC-GST) framework that adapts distribution consistency strategies from homophilic graphs. (1) Given the difficulty of acquiring exact homophily ratios and distributions without labels for a large number of unlabeled nodes, we develop an estimation method utilizing soft labels, which are more informative than one-hot labels. (2) To align pseudo-nodes with the target homophily ratio distribution, we optimize a selection vector q, each element representing the selection probability for nodes in a high-quality candidate set, selecting the top K nodes as pseudo-nodes. (3) After selecting pseudo-nodes, our framework assigns pseudo-labels. However, nodes with high heterophily often show lower accuracy because local neighbors typically belong to different classes. Inspired by [1, 41], we employ multi-hop neighbors to enhance labeling accuracy for heterophilic nodes, while using one-hop neighbors for homophilic nodes. Furthermore, although we carefully select pseudo-nodes that align with the target distribution, this approach may exclude inconsistent yet potentially high-quality nodes. To fully utilize these nodes, we introduce a dual-head GNN model: the main classifier head trains on clean and selected pseudo-nodes, while an auxiliary head focuses on the previously discarded high-quality pseudo-nodes, optimizing the feature extractor without compromising the performance of the main classifier. Our main contributions are:\n\u2022 We study a novel problem of reducing training bias across homophily bins caused by shifts in homophily ratio distribution, unique in heterophilic graphs compared to homophilic graphs.\n\u2022 We address the challenges of adapting distribution-consistent graph self-training methods to heterophilic graphs by introducing the innovative HC-GST framework, which utilizes a homophily ratio distribution consistency selection strategy.\n\u2022 Extensive experiments on both homophilic and heterophilic graphs consistently demonstrate the effectiveness of HC-GST in reducing training bias."}, {"title": "2 RELATED WORK", "content": "Graph Neural Networks with Heterophily. Graph Neural Networks (GNNs) update node representations through neighbor aggregation, aiding tasks like node classification [10, 11]. Their performance drops in heterophilic graphs [4, 36]. Solutions include higher-order neighbor aggregation (H2GCN [43], MixHop [1]) and differentiated message passing (GGCN [39], GPR-GNN [5]). BMGCN [8] uses a block similarity matrix to adapt to both graph types.\nGraph Self-training. Training GNNs with sparse labels for node classification often leverages self-training, using vast unlabeled data to enhance performance [14, 21, 27, 45]. Self-training expands the training set with high-quality pseudo-labels [16], primarily using confidence-based selection [16, 29, 34, 42, 45]. Techniques like M3S [29], which integrates deep clustering, and DSGSN [42], using negative sampling, address GNN training instability. However, methods like DRGST [18] and DCGST [31] that adjust to distribution shifts still assume homophily, limiting their effectiveness in heterophilic graphs. This paper adapts distribution consistency to better suit heterophilic graph characteristics.\nDistribution Shifts on Graphs. Training and testing nodes are assumed to have consistent distributions in most node classification benchmarks. However, real-world applications often experience distribution shifts between the labeled training sets and the actual data [35], leading to GNN classifiers overfitting, and negatively impacting deployment performance. Domain-Invariant Representation (DIR) learning addresses this by minimizing distribution differences across domains; using adversarial learning [7], or heuristic measures such as CMD [40] and MMD [19]. In self-training, inappropriate pseudo-node selection exacerbates distribution shifts [18, 31]. The unique challenge of homophily ratio distribution shifts in heterophilic graphs remains unexplored. This study is the first to explore shifts in homophily ratio distribution in heterophilic graphs within a self-training framework."}, {"title": "3 PRELIMINARY", "content": "In this section, we discuss self-training methods that tackle label sparsity and distribution shifts in homophilic graphs. Additionally, we conduct preliminary experiments to validate the unique training bias during self-training on heterophilic graphs. Finally, we formalize our problem definition.\n3.1 Notations and GNNS\nWe use $G = {V, &, X}$ to denote an attributed graph with $V = {v_1,..., v_n}$ as the set of nodes, & as the set of edges, and X as a feature matrix of d-dimensional node features. The adjacency matrix A indicates connections with $A_{ij}$ = 1 if $v_i$ and $v_j$ are connected, otherwise 0. In semi-supervised node classification, only a small portion of nodes, $V_L$, have labels $Y_L$ from a one-hot matrix Y. The goal is to predict labels for unlabeled nodes using G and $V_L$.\nGraph neural networks have shown great ability in modeling graphs. Generally, GNNs update node representations through a message-passing mechanism, aggregating features from neighboring nodes. Let $Z_i^{(l)}$ denote $v_i$'s representation at the l-th GNN layer. Then the message passing can be expressed as\n$Z_i^{(l+1)} = Aggregate (Z_i^{(l)}, \\sum_{v_j \\in N(v_i)} Propagate (Z_j^{(l)}, Z_i^{(l)}), A_{i,j})$.\nwhere $N(v_i)$ denotes the set of neighbors of $v_i$. We denote Z as the node representations in the last GNN layer.\n3.2 Pseudo-labeling and Distribution Shift\nGraph self-training is a widely used framework to tackle label sparsity by effectively leveraging large amounts of unlabeled data for performance gains. The success of self-training is its pseudo-labeling strategy, which carefully expands the training set with high-quality pseudo-nodes. However, graph data often exhibit distribution shifts that challenge self-training and pseudo-labeling strategies: (i) Label sparsity typically results in a non-representative training set, leading to distribution shifts away from the true distribution. These shifts can undermine models based on the IID assumption, as directly minimizing the average loss during training fails to yield predictors that generalize well under test distribution changes [15]. This prevents selecting high-quality pseudo-nodes; and (ii) An unrepresentative training set also causes imbalances in model capabilities, which during pseudo-label generation, tend to favor easier nodes. Consequently, this preference leads to an expanded pseudo-node set that deviates from the true distribution, adversely impacting self-training performance. Given the complex structure of graphs, measuring distribution shifts is challenging. To address this, Zhu et al. [44] has defined distribution shifts within the GNN representation space as the Central Moment Discrepancy (CMD) [40] distance between the global node representations $Z_G$ from GNNs and their labeled counterparts $Z_L$:\n$CMD(Z_G, Z^L) = \\frac{1}{b-a} ||E(Z_G) - E(Z^L)||^2 + \\frac{1}{b-a} \\sum_{k=1}^{K} ||C_k(Z_G) - C_k(Z^L)||^2$     (1)\nwhere E denotes the expectation, $C_k$ is the k-th order moment, and a, b are the joint distribution supports, typically calculating only up to k = 5 moments.\n3.3 Training Bias of GST in Heterophilic Graphs\nExisting work reduces distribution shifts in homophilic graphs within self-training frameworks [18, 31]. However, these methods do not extend well to heterophilic graphs, which may face unique challenges due to shifts in heterophily-aware distributions. To explore the unique challenges of self-training on heterophilic graphs, we conduct preliminary experiments. Heterophilic graphs mainly differ from homophilic ones in that a node tends to connect to nodes of dissimilar features or classes. To measure this, we first define node and graph homophily ratios.\nDefinition 3.1 (Node Homophily Ratio h($v_i$) [24]). h($v_i$) for a node $v_i$ measures the ratio of $v_i$'s neighbors that share the same label as $v_i$, which can be written as h($v_i$) = $\\frac{|{v_j \\in N_i:y_j=y_i}|}{|N_i|}$ where $N_i$ means the neighbors of node $v_i$ and $y_i$ is its golden label.\nDefinition 3.2 (Graph Homophily Ratio h(G) [24]). h(G) represents the average node homophily ratio across all nodes in the graph G, which is calculated as h(G) = $\\frac{\\sum_{v_i \\in V} h(v_i)}{V}$.\nTo quantify the distribution of homophily ratios, we evenly divide the homophily level into N bins as\nB = [|$B_1$, $B_2$,..., |$B_N$|]\n(2)\nwhere\n$B_i = \\frac{| { v | h(v) \\in \\frac{i}{N} }|}{|V|}$         (3)\nThe global homophily ratio distribution BG reflects these counts across the entire graph, whereas the local homophily ratio distribution BL refers only to the (pseudo-)labeled nodes.\nBuilding on these concepts, we conduct preliminary experiments with the state-of-the-art heterophilic GNN model BMGCN [8] and the representative heterophilic graph Chameleon [26]. Initially, we evaluate the backbone GNN performance under the heterophily setting. (1) Impact of Homophily Ratio Distribution on Training Set: We construct training sets based on the known global homophily ratio distribution as: biased towards homophily, representative global distributions, and towards heterophily. This construction selects nodes until each homophily bin $B_i$ meets its target. In homophily-biased sets, higher-index bins have more nodes (e.g., $\\sum_{i \\in [N-3, N]} |B_i| = |V_L|$); in heterophily-biased sets, lower-index bins are denser (e.g., $\\sum_{i \\in [1, 4]} |B_i| = |V_L|$). For representative sets, bin sizes align with the global distribution, defined as $B_{target} = \\frac{B}{\\sum B} \\times |V_L|$. As shown in Fig.1 (a), sets aligned with the true distribution perform best. Compared to homophily-biased sets, those biased towards heterophily more closely mirror the true distribution, thereby achieving better performance. (2) Performance Analysis Across Homophily Bins: We display the accuracy for each homophily bin (illustrated by the blue line in Fig. 1 (c)). We find that accuracy positively correlates with the homophily ratio, confirming that GNNs are inherently better at processing homophilic nodes than heterophilic ones.\nNext, we continue to explore the effects of heterophily within the self-training framework. (3) Change of Homophily Ratio under Self-Training: We analyze homophily ratio changes under two self-training frameworks: traditional confidence-based graph self-training (ST) [16] and distribution-consistent graph self-training (DCGST) [31]. We calculate average homophily ratios for (pseudo-)labeled nodes (shown in Fig.1 (b)). Results show ST selects nodes with higher homophily, while DCGST reduces the average but still deviates from the global average, indicating previous self-training methods struggle to align well with global homophily ratios. We refer to this as the Homophily Ratio Distribution Shift. (4) Training Bias induced by Homophily Ratio Distribution Shift: To assess the impact of shifts in homophily ratio distribution, we analyze the accuracy of both self-training methods across various homophily bins, as shown in Fig.1 (c) and (d). Our findings indicate that the model excels in homophily-biased bins, where it naturally performs well, and struggles with heterophilic nodes, akin to the Matthew Effect in economics. We describe this phenomenon as training bias across homophily bins. After identifying unique distribution shift issue across homophily bins in heterophilic graphs, we define it as:\nDefinition 3.3 (Distribution Shift on Heterophily). Assume the global homophily ratio distribution BG represents the count of nodes within each homophily bin across the entire graph, and the local homophily ratio distribution BL applies to the locally labeled node set. Distribution shift D($B_G$, $B_L$) is measured using metrics such as KL divergence or CMD between BG and BL.\n3.4 Problem Definition\nWith the above analysis, our problem is formally defined as: Given a graph G with a small set of labeled nodes $V^L$ and a large number of unlabeled nodes $V^U$, self-training may induce or amplify the distribution shifts in homophily ratios between the labeled node set $V^L \\cup V^P$ and the global graph. Our objective is to develop a graph self-training framework that enables a GNN model $f_{\\theta}$ to accurately predict $\\hat{y_i}$ = arg maxj $f_{\\theta}$ ($v_i$); for each unlabeled node, while reducing training biases through the following goals: (1) Maximize positive performance variation: max\u03b8 $\\sum_{\\Delta_i>0} \\Delta_i$, where $\\Delta_i$ represents the performance variance in the i-th homophily bin $B_i$. (2) Minimize negative performance variation: min\u03b8 $\\sum_{\\Delta_i<0} |\\Delta_i|$. (3) Overall, maximize performance variation: max\u03b8 $\\sum \\Delta_i$.", "content1": "4 PROPOSED METHOD\nIn this section, we give the details of the proposed framework Heterophily-aware Distribution Consistency based Graph Self-Training (HC-GST). The core design of our proposed HC-GST is shown in Fig. 2 and Algo. 1, which targets heterophilic structures in three main steps: selecting high-quality pseudo-nodes that align with the true homophily ratio distribution, pseudo-labeling them using multi-hop neighbors, and utilizing them by a dual-head GNN. Specifically, the self-training framework begins by carefully selecting pseudo-nodes aligned with the global homophily ratio distribution to avoid training bias. Then, the step of assigning pseudo-labels improves the labeling accuracy of selected heterophilic nodes by utilizing multi-hop neighbors, reducing noise in the self-training. Finally, we differentiate training strategies between carefully selected pseudo-nodes and the rest, to optimize the use of all potentially high-quality pseudo-nodes. Next, we detail the HC-GST framework.\n4.1 Heterophily-aware Distribution Consistent Pseudo-node Selection\nGNNs typically perform better on homophilic nodes, as shown in Fig. 1 (c). This leads to a shift in the homophily ratio distribution during confidence-based pseudo-node selection, as self-training tends to favor homophilic nodes, thus deviating from the actual data distribution. Particularly in heterophilic graphs, this shift enhances performance on easier homophilic nodes but degrades the performance on harder heterophilic nodes, introducing a unique training bias across homophily bins, as highlighted in Fig. 1 (c) and (d). To counter this training bias, we design a Heterophily-aware Distribution Consistent Pseudo-node Selection strategy, as detailed in lines 6 to 8 of Algo. 1. This strategy includes three core components: (i) estimating homophily ratios and distributions to align local and global distributions; (ii) identifying target distributions to ensure new pseudo-nodes help compensate for the homophily ratio distribution shift between local and global; and (iii) optimizing a selection vector to meet distribution consistency criteria.\n4.1.1 Homophily Ratios and Distributions Estimation. To ensure distribution consistency in homophily ratios with the global graph during pseudo-node selection, we precisely define the Homophily Ratio Distribution in Eq. 2, using histogram techniques [23]. However, calculating homophily ratios is challenging due to unknown node labels. Inspired by knowledge distillation techniques [9], which suggest soft labels are more informative than one-hot labels, we estimate homophily ratios using soft label similarity as:\n$\\hat{h}(v_i) = \\frac{\\sum_{v_j \\in N_i} S(\\tilde{y_j}, \\tilde{y_i})}{|N_i|}$     (4)\n$S(\\tilde{y_j}, \\tilde{y_i}) = \\frac{\\tilde{y_i}^T \\tilde{y_j}}{|\\tilde{y_i}|||\\tilde{y_j}||}$      (4)\nwhere $y_i$ is the soft label vector for node $v_i$ from the backbone GNN. This estimation updates as the model improves during self-training. This approach enables us to identify nodes within specific homophily ratio bins $B_i$ and adjust the local distribution $B_L$.\n4.1.2 Target Homophily Ratio Distribution. With the estimated global distribution $B_G$ and local distribution $B_L$, our objective is to minimize the distance between these distributions by strategically selecting local pseudo-nodes to follow $B_G$. This minimization involves adding a predefined number, K, of new pseudo-nodes in each stage to the local distribution to better align $B_L$ with $B_G$:\n$min_{pseudo-nodes} d(B_L, B_G)$          (5)\nwhere $B_L$ is updated by including K new pseudo-nodes that optimally match $B_G$. This approach ensures that the expanded set of labeled nodes in $V^L \\cup V_P^{\\, \\leq s-1}$ (where $V_P^{\\, \\leq s-1}$ includes previously added old pseudo-nodes before the current state s, and $V_P^{\\, s}$ represents the new pseudo-nodes to be added in the current stage s) yields a local distribution that is representative of the global distribution, effectively reducing distribution shifts caused by initial node and subsequent pseudo-node selection. Specifically, we denote $f_{ri}$ as the frequency rate of nodes within the i-th bin of global distribution $B$, and $(V^L \\cup V_P^{\\, \\leq s-1})_i$ as the count of nodes in the i-th local bin prior to selecting new pseudo-nodes. The target number of nodes in the i-th bin is computed as follows:\n|$B_i^{target}$| = max ([$f_{ri} \\cdot$ (K + |$V^L \\cup V_P^{\\, \\leq s-1}$|)]) - [($V^L \\cup V_P^{\\, \\leq s-1}$)], 0)\n$B_{target} = [B_1^{target}, B_2^{target},...., B_N^{target}]$\n(5)\nThe fundamental principle is that local homophily ratio distributions may not align with the global, so we calculate the difference between global and local homophily ratios to set node numbers per bin. This compensates the local homophily distribution to match the global, setting the target number of pseudo-nodes per bin.\n4.1.3 Pseudo-node Selection in Heterophilic Graphs. Next, we identify pseudo-nodes that align with the target homophily ratio distribution $B_{target}$. However, this target distribution does not account for the distribution shifts in node representations that is helpful for maintaining consistency in graph structures. To address this, we employ the CMD metric, a popular method for measuring distribution distances in GNN representations. This metric helps us ensure consistency between the local node representations $Z^L$ and the global representations $Z^G$, where Z represents the node outputs from the backbone GNN. Thus, we build two selection criteria: (1) selecting nodes that conform to the target homophily ratio distribution and (2) minimizing the CMD distance between the local $Z^L$ and global $Z^G$. To implement this, we initialize a selection vector q, where each element $q_i \\in [0, 1]$ represents the selection probability of the i-th node in the high-quality candidate pseudo-node set C. The candidate pseudo-node set C is defined as follows:\nC = {${v_i}$ | max $\\sigma(Z_i)_j$ > $\\delta_c$ and $v_i \\notin V_P^{\\, \\leq s-1}$}\n(6)\nwhere \u03c3 is the softmax function, and $\u03b4_c$ represents the confidence threshold. This set includes nodes whose maximum softmax scores exceed $\u03b4_c$, excluding those already selected in previous stages. This selective criterion ensures that the candidate set C contains only high-quality, dynamically updated candidates. Furthermore, we ensure that exactly K nodes are chosen by enforcing the constraint $\\sum_i (q_i)$ = K. Beyond minimizing distribution shifts in GNN representations, our approach also reduces the KL distance between the homophily ratio distributions of the selection vector and the target.\nq = arg min CMD($Z^G$, q $\\star Z^C$) + $\u03bb_s$KL($B_q$, $B_{target}$)\ns.t. ||q||\u2081 = K, $q_i$ \u2208 [0, 1]\n|$B_i$| = $\\sum_{v \\in C, j=ind(v, C), \\hat{h}(v) \\in [\\frac{i}{N}, \\frac{i+1}{N}) }$ $q_j$\n(7)\nwhere q is the sole learnable parameter. CMD and KL metrics measure distribution distances, with $B_q$ and $B_{target}$ representing the homophily ratio distributions of the selection vector and the target, respectively. $\u03bb_s$ controls homophily ratio consistency, $Z^G$ is the global representations, q $ \\star Z^C$ computes weighted local representations of candidate pseudo-nodes, and ind(v, C) indexes node v in C. After optimizing q, the top K nodes are selected as pseudo-nodes:\n$V_P^{\\, s}$ = top-K(q, K, C)\n(8)\ntop-K selects the K highest-ranked nodes from C based on q. This ensures consistency in GNN representation and homophily ratios. Loss Function of Pseudo-node Selection. Our pseudo-node selection relies on a selection vector q, which integrates the homophily ratio with the distribution consistency of GNN representations. The loss function for optimizing the selection vector is defined as:\n$L_q$ = CMD($Z^G$, q $ \\star Z^C$) + $\u03bb_s$KL($B_q$, $B_{target}$) + max(0, ||q||\u2081 \u2013 K)\n(9)\n4.2 Pseudo-label Assignment with Multi-hop Neighbors\nAfter selecting pseudo-nodes, our framework assigns pseudo-labels. Traditional methods based on the highest prediction probability may be ineffective on heterophilic graphs, where GNNs relying on local neighbors often mislabel heterophilic nodes, increasing noise and exacerbating training bias. To address this, drawing from [1, 41], which show that higher-order neighbors more reliably identify similar nodes, we use these neighbors for heterophilic pseudo-nodes to improve labeling accuracy. For homophilic nodes, we continue using local one-hop neighbors. The use of multi-hop neighbors is solely for assigning more accurate pseudo-labels; they are not employed in training the backbone GNN so as to preserve the original graph topology as including high-order neighbors can lose vital structural information, potentially affecting the estimation of homo ratios and calculation of distribution shifts in GNN representations. To be specific, we define the k-hop adjacency matrix as $A^k$, the k-th power of A, with H = $f_{\\theta}$(X, $A^k$) for k \u2265 2 representing the GNN's output in k-hop neighbors. The output used for pseudo-labeling is chosen based on the node homophily ratio:\n= $\\begin{cases} H \\\\ Z \\end{cases}$\nif h($v_i$) < \u03b4\nif h($v_i$) \u2265 \u03b4\n(10)\nwhere $\u03b4_h$ is the threshold for identifying heterophilic nodes. For the set of pseudo-nodes $V_P^{\\, s}$, pseudo-labels are assigned based on the highest probability from the adaptive outputs:\n$\\hat{y_i}^{pl}$ = arg max $Z_{ij}$ $ v_i \u2208 V_P^{\\, s}$\n(11)\nThis step is outlined in lines 9 to 10 of Algo. 1.\n4.3 Dual-Head GNN for Fully Utilizing Pseudo-nodes\nIn graph self-training, training bias stems from the use of self-generated pseudo-labels. To mitigate this, we have introduced a careful selection of distribution-consistent pseudo-labeled nodes from a high-quality candidate set C. However, this results in the discarding of many high-quality nodes that, while distribution-inconsistent, could be informative. Our objective is dual: to reduce training bias and to enhance overall self-training performance. The latter goal is compromised by discarding potentially valuable pseudo-nodes. To this end, drawing inspiration from the image field where feature extractors and classifier heads in a neural network classifier are conceptually separated [3], we also apply a separation in our GNN model, consisting of a feature extractor \u03c8 and a classifier head $h_{head}$:\n$f_\\theta$ = $h_{head}$ ($\\psi$)\n(12)\nTo maximize the utility of all potential nodes, we employ a dual-head GNN training method. The main classifier head $h_{main}$ is trained on clean and carefully selected pseudo-nodes $V_L \\cup V_P^{\\, s}$. Meanwhile, the pseudo classifier head $h_{pseudo}$ enhances representation learning of the feature extractor by utilizing the remaining pseudo-nodes C \u2013 $V_P^{\\, s}$, without affecting the performance of the main classifier head. The training objective function is:\nmin $L_{Y\\cup V_P^{\\, s}}$ (\u03c8, $h_{main}$) + $\u03bb_p$L$_{C - V_P^{\\, s}}$ (\u03c8, $h_{pseudo}$)\n(13)\n\u03c8,h,hpseudo\nwhere $h_{pseudo}$ independently processes distribution-inconsistent nodes. Though $h_{main}$ and $h_{pseudo}$ are fed with features from the same feature extractor, their parameters are independent. L is the cross entropy loss function. $\u03bb_p$ is a control factor of the pseudo-head loss term. Importantly, the pseudo classifier head, only responsible for training the feature extractor, is not used during inference. This approach effectively balances reducing bias and enhancing model performance, ensuring the full utilization of all high-quality pseudo-nodes. This step is outlined in lines 11 to 13 of Algo. 1.\n4.4 Workflow of HC-GST Framework\nAfter outlining the HC-GST framework's components-including distribution-consistent selection, multi-hop neighbors, and dual-head GNN-we detail our workflow in Fig. 2 and Algo. 1. Initially, we train a GNN on clean-labeled nodes to produce the output matrix Z. The pseudo-label set, $V_P$, (with s = 1 as the initial stage), begins empty. Nodes in the unlabeled set exceeding a confidence threshold $\u03b4_c$ form a candidate set C, from which distribution-consistent pseudo-nodes, $V_P^{\\, s}$, are selected. Next, a multi-hop neighbor method assigns pseudo-labels to these nodes. Finally, we replace the last GNN with a dual-head GNN trained on both the expanded labeled nodes and other high-quality nodes. We continuously update the pseudo-labeled node set to $V_P^{\\, s}$ with fresh labels and the output matrix with the new one, repeating these steps until we reach a predetermined number of stages S or achieve performance convergence. The GNN with the best validation accuracy is selected as the final model, with its output as the final result."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments on both homophilic and heterophilic graphs under various label rates to evaluate the effectiveness of the proposed framework.\n5.1 Experimental Setup\n5.1.1 Datasets. We use four heterophilic datasets: Chameleon, Squirrel, Texas, arXiv-year [17, 26], and three homophilic datasets:"}, {"title": "5.2 Results and Analysis", "content": "To explore HC-GST's capability with a biased training set in homophily ratio distribution, we randomly generate local homophily ratio distributions for the training set that deviates from the global distribution. Nodes are then selected one by one according to the local distribution. Tables 2 and 3 compare our method with baseline approaches on datasets with biased training sets in both homophily and heterophily settings. We observe: (1) Across four heterophily graphs, our method excels in comprehensive metrics, ACC and TPV, improving by 3.74% and 2.11% respectively over the runner-up on average. This highlights the effectiveness of our framework in self-training on heterophilic graphs. (2) For training bias metrics, NPV and PPV (where a smaller gap is preferable), our method consistently outperforms others, showing that our distribution consistency selection effectively reduces the training bias. Conversely, ST and M3S ignore distribution shifts, leading to significant training bias. DRGST and DCGST, despite considering consistency, fail on heterophilic graphs due to biased training nodes that significantly affect GNN representations. (3) We also note similar patterns across three homophilic graphs. This indicates that homophily ratio distribution shifts are a widespread issue in graph learning, irrespective of the graph type. Our best performance proves the generalization of our method across heterophily and homophily settings. (4) Examining accuracy changes under various label rates between our method and the backbone, we observe more substantial improvements at lower label rates. This indicates that our self-training approach is particularly effective in few-shot scenarios.\nTable 4 details results on six graphs at a 1% label rate without obvious homophily ratio distribution shift between the clean training set and the global graph. This is achieved through normal random sampling of training nodes, which tends to produce a homophily ratio distribution similar to the global one. Negligible shifts in the training set suggest that distribution shifts primarily arise from pseudo-labeling. Compared to DRGST and DCGST, which focus on reducing distribution shifts through GNN representations, our method is more effective across all metrics, demonstrating the effectiveness of our homophily ratio consistency module. In contrast, ST and M3S exhibit the most significant training bias, underscoring the importance of addressing distribution shifts during pseudo-labeling."}, {"title": "5.3 Ablation Study", "content": "To assess the impact of each component within the HC-GST framework", "variants": 1, "are": 1}, {"title": "HC-GST: Heterophily-aware Distribution Consistency based Graph Self-training", "authors": ["Fali Wang", "Tianxiang Zhao", "Junjie Xu", "Suhang Wang"], "abstract": "Graph self-training (GST), which selects and assigns pseudo-labels to unlabeled nodes, is popular for tackling label sparsity in graphs. However, recent study on homophily graphs show that GST methods could introduce and amplify distribution shift between training and test nodes as they tend to assign pseudo-labels to nodes they are good at. As GNNs typically perform better on homophilic nodes, there could be potential shifts towards homophilic pseudo-nodes, which is underexplored. Our preliminary experiments on heterophilic graphs verify that these methods can cause shifts in homophily ratio distributions, leading to training bias that improves performance on homophilic nodes while degrading it on heterophilic ones. Therefore, we study a novel problem of reducing homophily ratio distribution shifts during self-training on heterophilic graphs. A key challenge is the accurate calculation of homophily ratios and their distributions without extensive labeled data. To tackle them, we propose a novel Heterophily-aware Distribution Consistency-based Graph Self-Training (HC-GST) framework, which estimates homophily ratios using soft labels and optimizes a selection vector to align pseudo-nodes with the global homophily ratio distribution. Extensive experiments on both homophilic and heterophilic graphs show that HC-GST effectively reduces training bias and enhances self-training performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are pervasive in the real world, such as in social networks [25], knowledge graphs [30], and traffic networks [37]. Graph Neural Networks (GNNs) have achieved significant advances in semi-supervised node classification across both homophilic and heterophilic graphs [1, 5, 8, 13, 20, 38]. Despite their success, they often rely on abundant labeled data [6, 32] for training, yet manual labeling is not only labor-intensive but also impractical for new classes with sparse samples [33]. This has spurred increasing interest in node classification with sparse labels [18, 29].\nGraph self-training (GST) emerges as a promising method to harness abundant unlabeled nodes alongside a limited number of labeled nodes to tackle label sparsity in graphs [14, 16]. It typically involves three core steps: (i) pseudo-node selection, selecting high-confidence nodes; (ii) pseudo-label assignment, assigning the most probable labels; and (iii) retraining, where the GNN is trained on the augmented set, iterating these steps until convergence or a specified number of stages. However, typical confidence-based pseudo-node selection can cause distribution shifts [18, 44], potentially undermining self-training. This occurs as an increasing number of easy nodes, identified by their high confidence, are added to the original labeled training set, leading to the distribution gradually shifting towards this augmented training set and excessive focus on such easy nodes as a result. Methods like DRGST [18] and DCGST [31] address this by quantifying distribution shifts through information gain and GNN representation, respectively, and selecting pseudo-nodes that align with the distribution of the unlabeled set. Nevertheless, these methods are tailored for homophilic graphs, where connected nodes tend to have similar labels. In contrast, they neglect shifts in neighboring distributions (homophily ratio) in heterophilic graphs, where GNNs perform well on homophilic nodes but struggle with heterophilic ones, resulting in novel distribution shifts.\nTo investigate self-training on heterophilic graphs, our preliminary experiments on the representative Chameleon graph [26] in Sec.3.3 show that: (1) Training sets aligned with the true homophily distributions yield the best performance, as shown in Fig.1 (a); (2) GNNs excel on homophilic nodes, with accuracy positively correlating with the homophily ratio, evident from the blue line in Fig.1 (c). (3) Traditional self-training (ST) tends to select pseudo-nodes with higher homophily ratios, as depicted in Fig.1 (b), resulting in distribution shifts; (4) Strategies based on confidence or GNN representation consistency often lead to training bias, improving performance on homophilic nodes but worsening it on heterophilic nodes, as illustrated in Fig.1 (c) and (d). These results underscore the need for maintaining consistency in homophily ratio distribution (node counts across homophily bins) to mitigate training bias and improve self-training, a gap not fully addressed by existing methods. Thus, ensuring consistency between the homophily ratio distribution of the local (pseudo-)labeled training set and the global heterophilic graph to reduce training bias across homophily bins remains an open question.\nTherefore, in this paper, we study a novel problem of homophily ratio distribution consistency during self-training on heterophilic graphs. However, this presents challenges to maximizing self-training performance in obtaining accurate homophily ratios and distributions, aligning homophily ratios between (pseudo-)labeled nodes and the true distribution, and the low pseudo-labeling accuracy on heterophilic nodes. Thus, we pose a critical research question: How can we develop a GST framework which ensures selected pseudo-nodes align with true homophily ratio distributions and assigns accurate pseudo-labels to highly heterophilic nodes?\nTo address these challenges, we propose a novel Heterophily-aware Distribution Consistency-based Graph Self-Training (HC-GST) framework that adapts distribution consistency strategies from homophilic graphs. (1) Given the difficulty of acquiring exact homophily ratios and distributions without labels for a large number of unlabeled nodes, we develop an estimation method utilizing soft labels, which are more informative than one-hot labels. (2) To align pseudo-nodes with the target homophily ratio distribution, we optimize a selection vector q, each element representing the selection probability for nodes in a high-quality candidate set, selecting the top K nodes as pseudo-nodes. (3) After selecting pseudo-nodes, our framework assigns pseudo-labels. However, nodes with high heterophily often show lower accuracy because local neighbors typically belong to different classes. Inspired by [1, 41], we employ multi-hop neighbors to enhance labeling accuracy for heterophilic nodes, while using one-hop neighbors for homophilic nodes. Furthermore, although we carefully select pseudo-nodes that align with the target distribution, this approach may exclude inconsistent yet potentially high-quality nodes. To fully utilize these nodes, we introduce a dual-head GNN model: the main classifier head trains on clean and selected pseudo-nodes, while an auxiliary head focuses on the previously discarded high-quality pseudo-nodes, optimizing the feature extractor without compromising the performance of the main classifier. Our main contributions are:\n\u2022 We study a novel problem of reducing training bias across homophily bins caused by shifts in homophily ratio distribution, unique in heterophilic graphs compared to homophilic graphs.\n\u2022 We address the challenges of adapting distribution-consistent graph self-training methods to heterophilic graphs by introducing the innovative HC-GST framework, which utilizes a homophily ratio distribution consistency selection strategy.\n\u2022 Extensive experiments on both homophilic and heterophilic graphs consistently demonstrate the effectiveness of HC-GST in reducing training bias."}, {"title": "2 RELATED WORK", "content": "Graph Neural Networks with Heterophily. Graph Neural Networks (GNNs) update node representations through neighbor aggregation, aiding tasks like node classification [10, 11]. Their performance drops in heterophilic graphs [4, 36]. Solutions include higher-order neighbor aggregation (H2GCN [43], MixHop [1]) and differentiated message passing (GGCN [39], GPR-GNN [5]). BMGCN [8] uses a block similarity matrix to adapt to both graph types.\nGraph Self-training. Training GNNs with sparse labels for node classification often leverages self-training, using vast unlabeled data to enhance performance [14, 21, 27, 45]. Self-training expands the training set with high-quality pseudo-labels [16], primarily using confidence-based selection [16, 29, 34, 42, 45]. Techniques like M3S [29], which integrates deep clustering, and DSGSN [42], using negative sampling, address GNN training instability. However, methods like DRGST [18] and DCGST [31] that adjust to distribution shifts still assume homophily, limiting their effectiveness in heterophilic graphs. This paper adapts distribution consistency to better suit heterophilic graph characteristics.\nDistribution Shifts on Graphs. Training and testing nodes are assumed to have consistent distributions in most node classification benchmarks. However, real-world applications often experience distribution shifts between the labeled training sets and the actual data [35], leading to GNN classifiers overfitting, and negatively impacting deployment performance. Domain-Invariant Representation (DIR) learning addresses this by minimizing distribution differences across domains; using adversarial learning [7], or heuristic measures such as CMD [40] and MMD [19]. In self-training, inappropriate pseudo-node selection exacerbates distribution shifts [18, 31]. The unique challenge of homophily ratio distribution shifts in heterophilic graphs remains unexplored. This study is the first to explore shifts in homophily ratio distribution in heterophilic graphs within a self-training framework."}, {"title": "3 PRELIMINARY", "content": "In this section, we discuss self-training methods that tackle label sparsity and distribution shifts in homophilic graphs. Additionally, we conduct preliminary experiments to validate the unique training bias during self-training on heterophilic graphs. Finally, we formalize our problem definition.\n3.1 Notations and GNNS\nWe use $G = {V, &, X}$ to denote an attributed graph with $V = {v_1,..., v_n}$ as the set of nodes, & as the set of edges, and X as a feature matrix of d-dimensional node features. The adjacency matrix A indicates connections with $A_{ij}$ = 1 if $v_i$ and $v_j$ are connected, otherwise 0. In semi-supervised node classification, only a small portion of nodes, $V_L$, have labels $Y_L$ from a one-hot matrix Y. The goal is to predict labels for unlabeled nodes using G and $V_L$.\nGraph neural networks have shown great ability in modeling graphs. Generally, GNNs update node representations through a message-passing mechanism, aggregating features from neighboring nodes. Let $Z_i^{(l)}$ denote $v_i$'s representation at the l-th GNN layer. Then the message passing can be expressed as\n$Z_i^{(l+1)} = Aggregate (Z_i^{(l)}, \\sum_{v_j \\in N(v_i)} Propagate (Z_j^{(l)}, Z_i^{(l)}), A_{i,j})$.\nwhere $N(v_i)$ denotes the set of neighbors of $v_i$. We denote Z as the node representations in the last GNN layer.\n3.2 Pseudo-labeling and Distribution Shift\nGraph self-training is a widely used framework to tackle label sparsity by effectively leveraging large amounts of unlabeled data for performance gains. The success of self-training is its pseudo-labeling strategy, which carefully expands the training set with high-quality pseudo-nodes. However, graph data often exhibit distribution shifts that challenge self-training and pseudo-labeling strategies: (i) Label sparsity typically results in a non-representative training set, leading to distribution shifts away from the true distribution. These shifts can undermine models based on the IID assumption, as directly minimizing the average loss during training fails to yield predictors that generalize well under test distribution changes [15]. This prevents selecting high-quality pseudo-nodes; and (ii) An unrepresentative training set also causes imbalances in model capabilities, which during pseudo-label generation, tend to favor easier nodes. Consequently, this preference leads to an expanded pseudo-node set that deviates from the true distribution, adversely impacting self-training performance. Given the complex structure of graphs, measuring distribution shifts is challenging. To address this, Zhu et al. [44] has defined distribution shifts within the GNN representation space as the Central Moment Discrepancy (CMD) [40] distance between the global node representations $Z_G$ from GNNs and their labeled counterparts $Z_L$:\n$CMD(Z_G, Z^L) = \\frac{1}{b-a} ||E(Z_G) - E(Z^L)||^2 + \\frac{1}{b-a} \\sum_{k=1}^{K} ||C_k(Z_G) - C_k(Z^L)||^2$     (1)\nwhere E denotes the expectation, $C_k$ is the k-th order moment, and a, b are the joint distribution supports, typically calculating only up to k = 5 moments.\n3.3 Training Bias of GST in Heterophilic Graphs\nExisting work reduces distribution shifts in homophilic graphs within self-training frameworks [18, 31]. However, these methods do not extend well to heterophilic graphs, which may face unique challenges due to shifts in heterophily-aware distributions. To explore the unique challenges of self-training on heterophilic graphs, we conduct preliminary experiments. Heterophilic graphs mainly differ from homophilic ones in that a node tends to connect to nodes of dissimilar features or classes. To measure this, we first define node and graph homophily ratios.\nDefinition 3.1 (Node Homophily Ratio h($v_i$) [24]). h($v_i$) for a node $v_i$ measures the ratio of $v_i$'s neighbors that share the same label as $v_i$, which can be written as h($v_i$) = $\\frac{|{v_j \\in N_i:y_j=y_i}|}{|N_i|}$ where $N_i$ means the neighbors of node $v_i$ and $y_i$ is its golden label.\nDefinition 3.2 (Graph Homophily Ratio h(G) [24]). h(G) represents the average node homophily ratio across all nodes in the graph G, which is calculated as h(G) = $\\frac{\\sum_{v_i \\in V} h(v_i)}{V}$.\nTo quantify the distribution of homophily ratios, we evenly divide the homophily level into N bins as\nB = [|$B_1$, $B_2$,..., |$B_N$|]\n(2)\nwhere\n$B_i = \\frac{| { v | h(v) \\in \\frac{i}{N} }|}{|V|}$         (3)\nThe global homophily ratio distribution BG reflects these counts across the entire graph, whereas the local homophily ratio distribution BL refers only to the (pseudo-)labeled nodes.\nBuilding on these concepts, we conduct preliminary experiments with the state-of-the-art heterophilic GNN model BMGCN [8] and the representative heterophilic graph Chameleon [26]. Initially, we evaluate the backbone GNN performance under the heterophily setting. (1) Impact of Homophily Ratio Distribution on Training Set: We construct training sets based on the known global homophily ratio distribution as: biased towards homophily, representative global distributions, and towards heterophily. This construction selects nodes until each homophily bin $B_i$ meets its target. In homophily-biased sets, higher-index bins have more nodes (e.g., $\\sum_{i \\in [N-3, N]} |B_i| = |V_L|$); in heterophily-biased sets, lower-index bins are denser (e.g., $\\sum_{i \\in [1, 4]} |B_i| = |V_L|$). For representative sets, bin sizes align with the global distribution, defined as $B_{target} = \\frac{B}{\\sum B} \\times |V_L|$. As shown in Fig.1 (a), sets aligned with the true distribution perform best. Compared to homophily-biased sets, those biased towards heterophily more closely mirror the true distribution, thereby achieving better performance. (2) Performance Analysis Across Homophily Bins: We display the accuracy for each homophily bin (illustrated by the blue line in Fig. 1 (c)). We find that accuracy positively correlates with the homophily ratio, confirming that GNNs are inherently better at processing homophilic nodes than heterophilic ones.\nNext, we continue to explore the effects of heterophily within the self-training framework. (3) Change of Homophily Ratio under Self-Training: We analyze homophily ratio changes under two self-training frameworks: traditional confidence-based graph self-training (ST) [16] and distribution-consistent graph self-training (DCGST) [31]. We calculate average homophily ratios for (pseudo-)labeled nodes (shown in Fig.1 (b)). Results show ST selects nodes with higher homophily, while DCGST reduces the average but still deviates from the global average, indicating previous self-training methods struggle to align well with global homophily ratios. We refer to this as the Homophily Ratio Distribution Shift. (4) Training Bias induced by Homophily Ratio Distribution Shift: To assess the impact of shifts in homophily ratio distribution, we analyze the accuracy of both self-training methods across various homophily bins, as shown in Fig.1 (c) and (d). Our findings indicate that the model excels in homophily-biased bins, where it naturally performs well, and struggles with heterophilic nodes, akin to the Matthew Effect in economics. We describe this phenomenon as training bias across homophily bins. After identifying unique distribution shift issue across homophily bins in heterophilic graphs, we define it as:\nDefinition 3.3 (Distribution Shift on Heterophily). Assume the global homophily ratio distribution BG represents the count of nodes within each homophily bin across the entire graph, and the local homophily ratio distribution BL applies to the locally labeled node set. Distribution shift D($B_G$, $B_L$) is measured using metrics such as KL divergence or CMD between BG and BL."}, {"title": "4 PROPOSED METHOD", "content": "In this section, we give the details of the proposed framework Heterophily-aware Distribution Consistency based Graph Self-Training (HC-GST). The core design of our proposed HC-GST is shown in Fig. 2 and Algo. 1, which targets heterophilic structures in three main steps: selecting high-quality pseudo-nodes that align with the true homophily ratio distribution, pseudo-labeling them using multi-hop neighbors, and utilizing them by a dual-head GNN. Specifically, the self-training framework begins by carefully selecting pseudo-nodes aligned with the global homophily ratio distribution to avoid training bias. Then, the step of assigning pseudo-labels improves the labeling accuracy of selected heterophilic nodes by utilizing multi-hop neighbors, reducing noise in the self-training. Finally, we differentiate training strategies between carefully selected pseudo-nodes and the rest, to optimize the use of all potentially high-quality pseudo-nodes. Next, we detail the HC-GST framework.\n4.1 Heterophily-aware Distribution Consistent Pseudo-node Selection\nGNNs typically perform better on homophilic nodes, as shown in Fig. 1 (c). This leads to a shift in the homophily ratio distribution during confidence-based pseudo-node selection, as self-training tends to favor homophilic nodes, thus deviating from the actual data distribution. Particularly in heterophilic graphs, this shift enhances performance on easier homophilic nodes but degrades the performance on harder heterophilic nodes, introducing a unique training bias across homophily bins, as highlighted in Fig. 1 (c) and (d). To counter this training bias, we design a Heterophily-aware Distribution Consistent Pseudo-node Selection strategy, as detailed in lines 6 to 8 of Algo. 1. This strategy includes three core components: (i) estimating homophily ratios and distributions to align local and global distributions; (ii) identifying target distributions to ensure new pseudo-nodes help compensate for the homophily ratio distribution shift between local and global; and (iii) optimizing a selection vector to meet distribution consistency criteria.\n4.1.1 Homophily Ratios and Distributions Estimation. To ensure distribution consistency in homophily ratios with the global graph during pseudo-node selection, we precisely define the Homophily Ratio Distribution in Eq. 2, using histogram techniques [23]. However, calculating homophily ratios is challenging due to unknown node labels. Inspired by knowledge distillation techniques [9], which suggest soft labels are more informative than one-hot labels, we estimate homophily ratios using soft label similarity as:\n$\\hat{h}(v_i) = \\frac{\\sum_{v_j \\in N_i} S(\\tilde{y_j}, \\tilde{y_i})}{|N_i|}$     (4)\n$S(\\tilde{y_j}, \\tilde{y_i}) = \\frac{\\tilde{y_i}^T \\tilde{y_j}}{|\\tilde{y_i}|||\\tilde{y_j}||}$      (4)\nwhere $y_i$ is the soft label vector for node $v_i$ from the backbone GNN. This estimation updates as the model improves during self-training. This approach enables us to identify nodes within specific homophily ratio bins $B_i$ and adjust the local distribution $B_L$.\n4.1.2 Target Homophily Ratio Distribution. With the estimated global distribution $B_G$ and local distribution $B_L$, our objective is to minimize the distance between these distributions by strategically selecting local pseudo-nodes to follow $B_G$. This minimization involves adding a predefined number, K, of new pseudo-nodes in each stage to the local distribution to better align $B_L$ with $B_G$:\n$min_{pseudo-nodes} d(B_L, B_G)$          (5)\nwhere $B_L$ is updated by including K new pseudo-nodes that optimally match $B_G$. This approach ensures that the expanded set of labeled nodes in $V^L \\cup V_P^{\\, \\leq s-1}$ (where $V_P^{\\, \\leq s-1}$ includes previously added old pseudo-nodes before the current state s, and $V_P^{\\, s}$ represents the new pseudo-nodes to be added in the current stage s) yields a local distribution that is representative of the global distribution, effectively reducing distribution shifts caused by initial node and subsequent pseudo-node selection. Specifically, we denote $f_{ri}$ as the frequency rate of nodes within the i-th bin of global distribution $B$, and $(V^L \\cup V_P^{\\, \\leq s-1})_i$ as the count of nodes in the i-th local bin prior to selecting new pseudo-nodes. The target number of nodes in the i-th bin is computed as follows:\n|$B_i^{target}$| = max ([$f_{ri} \\cdot$ (K + |$V^L \\cup V_P^{\\, \\leq s-1}$|)]) - [($V^L \\cup V_P^{\\, \\leq s-1}$)], 0)\n$B_{target} = [B_1^{target}, B_2^{target},...., B_N^{target}]$\n(5)\nThe fundamental principle is that local homophily ratio distributions may not align with the global, so we calculate the difference between global and local homophily ratios to set node numbers per bin. This compensates the local homophily distribution to match the global, setting the target number of pseudo-nodes per bin.\n4.1.3 Pseudo-node Selection in Heterophilic Graphs. Next, we identify pseudo-nodes that align with the target homophily ratio distribution $B_{target}$. However, this target distribution does not account for the distribution shifts in node representations that is helpful for maintaining consistency in graph structures. To address this, we employ the CMD metric, a popular method for measuring distribution distances in GNN representations. This metric helps us ensure consistency between the local node representations $Z^L$ and the global representations $Z^G$, where Z represents the node outputs from the backbone GNN. Thus, we build two selection criteria: (1) selecting nodes that conform to the target homophily ratio distribution and (2) minimizing the CMD distance between the local $Z^L$ and global $Z^G$. To implement this, we initialize a selection vector q, where each element $q_i \\in [0, 1]$ represents the selection probability of the i-th node in the high-quality candidate pseudo-node set C. The candidate pseudo-node set C is defined as follows:\nC = {${v_i}$ | max $\\sigma(Z_i)_j$ > $\\delta_c$ and $v_i \\notin V_P^{\\, \\leq s-1}$}\n(6)\nwhere \u03c3 is the softmax function, and $\u03b4_c$ represents the confidence threshold. This set includes nodes whose maximum softmax scores exceed $\u03b4_c$, excluding those already selected in previous stages. This selective criterion ensures that the candidate set C contains only high-quality, dynamically updated candidates. Furthermore, we ensure that exactly K nodes are chosen by enforcing the constraint $\\sum_i (q_i)$ = K. Beyond minimizing distribution shifts in GNN representations, our approach also reduces the KL distance between the homophily ratio distributions of the selection vector and the target.\nq = arg min CMD($Z^G$, q $\\star Z^C$) + $\u03bb_s$KL($B_q$, $B_{target}$)\ns.t. ||q||\u2081 = K, $q_i$ \u2208 [0, 1]\n|$B_i$| = $\\sum_{v \\in C, j=ind(v, C), \\hat{h}(v) \\in [\\frac{i}{N}, \\frac{i+1}{N}) }$ $q_j$\n(7)\nwhere q is the sole learnable parameter. CMD and KL metrics measure distribution distances, with $B_q$ and $B_{target}$ representing the homophily ratio distributions of the selection vector and the target, respectively. $\u03bb_s$ controls homophily ratio consistency, $Z^G$ is the global representations, q $ \\star Z^C$ computes weighted local representations of candidate pseudo-nodes, and ind(v, C) indexes node v in C. After optimizing q, the top K nodes are selected as pseudo-nodes:\n$V_P^{\\, s}$ = top-K(q, K, C)\n(8)\ntop-K selects the K highest-ranked nodes from C based on q. This ensures consistency in GNN representation and homophily ratios. Loss Function of Pseudo-node Selection. Our pseudo-node selection relies on a selection vector q, which integrates the homophily ratio with the distribution consistency of GNN representations. The loss function for optimizing the selection vector is defined as:\n$L_q$ = CMD($Z^G$, q $ \\star Z^C$) + $\u03bb_s$KL($B_q$, $B_{target}$) + max(0, ||q||\u2081 \u2013 K)\n(9)\n4.2 Pseudo-label Assignment with Multi-hop Neighbors\nAfter selecting pseudo-nodes, our framework assigns pseudo-labels. Traditional methods based on the highest prediction probability may be ineffective on heterophilic graphs, where GNNs relying on local neighbors often mislabel heterophilic nodes, increasing noise and exacerbating training bias. To address this, drawing from [1, 41], which show that higher-order neighbors more reliably identify similar nodes, we use these neighbors for heterophilic pseudo-nodes to improve labeling accuracy. For homophilic nodes, we continue using local one-hop neighbors. The use of multi-hop neighbors is solely for assigning more accurate pseudo-labels; they are not employed in training the backbone GNN so as to preserve the original graph topology as including high-order neighbors can lose vital structural information, potentially affecting the estimation of homo ratios and calculation of distribution shifts in GNN representations. To be specific, we define the k-hop adjacency matrix as $A^k$, the k-th power of A, with H = $f_{\\theta}$(X, $A^k$) for k \u2265 2 representing the GNN's output in k-hop neighbors. The output used for pseudo-labeling is chosen based on the node homophily ratio:\n= $\\begin{cases} H \\\\ Z \\end{cases}$\nif h($v_i$) < \u03b4\nif h($v_i$) \u2265 \u03b4\n(10)\nwhere $\u03b4_h$ is the threshold for identifying heterophilic nodes. For the set of pseudo-nodes $V_P^{\\, s}$, pseudo-labels are assigned based on the highest probability from the adaptive outputs:\n$\\hat{y_i}^{pl}$ = arg max $Z_{ij}$ $ v_i \u2208 V_P^{\\, s}$\n(11)\nThis step is outlined in lines 9 to 10 of Algo. 1.\n4.3 Dual-Head GNN for Fully Utilizing Pseudo-nodes\nIn graph self-training, training bias stems from the use of self-generated pseudo-labels. To mitigate this, we have introduced a careful selection of distribution-consistent pseudo-labeled nodes from a high-quality candidate set C. However, this results in the discarding of many high-quality nodes that, while distribution-inconsistent, could be informative. Our objective is dual: to reduce training bias and to enhance overall self-training performance. The latter goal is compromised by discarding potentially valuable pseudo-nodes. To this end, drawing inspiration from the image field where feature extractors and classifier heads in a neural network classifier are conceptually separated [3], we also apply a separation in our GNN model, consisting of a feature extractor \u03c8 and a classifier head $h_{head}$:\n$f_\\theta$ = $h_{head}$ ($\\psi$)\n(12)\nTo maximize the utility of all potential nodes, we employ a dual-head GNN training method. The main classifier head $h_{main}$ is trained on clean and carefully selected pseudo-nodes $V_L \\cup V_P^{\\, s}$. Meanwhile, the pseudo classifier head $h_{pseudo}$ enhances representation learning of the feature extractor by utilizing the remaining pseudo-nodes C \u2013 $V_P^{\\, s}$, without affecting the performance of the main classifier head. The training objective function is:\nmin $L_{Y\\cup V_P^{\\, s}}$ (\u03c8, $h_{main}$) + $\u03bb_p$L$_{C - V_P^{\\, s}}$ (\u03c8, $h_{pseudo}$)\n(13)\n\u03c8,h,hpseudo\nwhere $h_{pseudo}$ independently processes distribution-inconsistent nodes. Though $h_{main}$ and $h_{pseudo}$ are fed with features from the same feature extractor, their parameters are independent. L is the cross entropy loss function. $\u03bb_p$ is a control factor of the pseudo-head loss term. Importantly, the pseudo classifier head, only responsible for training the feature extractor, is not used during inference. This approach effectively balances reducing bias and enhancing model performance, ensuring the full utilization of all high-quality pseudo-nodes. This step is outlined in lines 11 to 13 of Algo. 1.\n4.4 Workflow of HC-GST Framework\nAfter outlining the HC-GST framework's components-including distribution-consistent selection, multi-hop neighbors, and dual-head GNN-we detail our workflow in Fig. 2 and Algo. 1. Initially, we train a GNN on clean-labeled nodes to produce the output matrix Z. The pseudo-label set, $V_P$, (with s = 1 as the initial stage), begins empty. Nodes in the unlabeled set exceeding a confidence threshold $\u03b4_c$ form a candidate set C, from which distribution-consistent pseudo-nodes, $V_P^{\\, s}$, are selected. Next, a multi-hop neighbor method assigns pseudo-labels to these nodes. Finally, we replace the last GNN with a dual-head GNN trained on both the expanded labeled nodes and other high-quality nodes. We continuously update the pseudo-labeled node set to $V_P^{\\, s}$ with fresh labels and the output matrix with the new one, repeating these steps until we reach a predetermined number of stages S or achieve performance convergence. The GNN with the best validation accuracy is selected as the final model, with its output as the final result."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments on both homophilic and heterophilic graphs under various label rates to evaluate the effectiveness of the proposed framework.\n5.1 Experimental Setup\n5.1.1 Datasets. We use four heterophilic datasets: Chameleon, Squirrel, Texas, arXiv-year [17, 26], and three homophilic datasets:"}, {"title": "5.2 Results and Analysis", "content": "To explore HC-GST's capability with a biased training set in homophily ratio distribution, we randomly generate local homophily ratio distributions for the training set that deviates from the global distribution. Nodes are then selected one by one according to the local distribution. Tables 2 and 3 compare our method with baseline approaches on datasets with biased training sets in both homophily and heterophily settings. We observe: (1) Across four heterophily graphs, our method excels in comprehensive metrics, ACC and TPV, improving by 3.74% and 2.11% respectively over the runner-up on average. This highlights the effectiveness of our framework in self-training on heterophilic graphs. (2) For training bias metrics, NPV and PPV (where a smaller gap is preferable), our method consistently outperforms others, showing that our distribution consistency selection effectively reduces the training bias. Conversely, ST and M3S ignore distribution shifts, leading to significant training bias. DRGST and DCGST, despite considering consistency, fail on heterophilic graphs due to biased training nodes that significantly affect GNN representations. (3) We also note similar patterns across three homophilic graphs. This indicates that homophily ratio distribution shifts are a widespread issue in graph learning, irrespective of the graph type. Our best performance proves the generalization of our method across heterophily and homophily settings. (4) Examining accuracy changes under various label rates between our method and the backbone, we observe more substantial improvements at lower label rates. This indicates that our self-training approach is particularly effective in few-shot scenarios.\nTable 4 details results on six graphs at a 1% label rate without obvious homophily ratio distribution shift between the clean training set and the global graph. This is achieved through normal random sampling of training nodes, which tends to produce a homophily ratio distribution similar to the global one. Negligible shifts in the training set suggest that distribution shifts primarily arise from pseudo-labeling. Compared to DRGST and DCGST, which focus on reducing distribution shifts through GNN representations, our method is more effective across all metrics, demonstrating the effectiveness of our homophily ratio consistency module. In contrast, ST and M3S exhibit the most significant training bias, underscoring the importance of addressing distribution shifts during pseudo-labeling."}, {"title": "5.3 Ablation Study", "content": "To assess the impact of each component within the HC-GST framework, we compare it against various variants: (1) HCGST-w/o-MultiHop, removing the multi-hop neighbors module; (2) HCGST-w/o-DualHead, without the dual-head mechanism; (3) HCGST-w/o-Selection, which excludes the distribution consistency selection. For clarity, we remove the hyphen from our framework name. We conduct this ablation study on Chameleon with a biased training set at a 1% label rate. As shown in Table 5, the findings are: (1) Removing the distribution consistency module leads to a significant accuracy decline of 4.34%, underscoring the importance of distribution consistency in selecting high-quality pseudo-labels. Besides, the gap between NPV and PPV"}, {"title": "5.4 Analysis of Hyper-parameter Impact", "content": "We assess the impacts of key hyperparameters \u03bb\u03c2", "3": 1}]}]}