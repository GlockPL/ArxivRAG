{"title": "HC-GST: Heterophily-aware Distribution Consistency based Graph Self-training", "authors": ["Fali Wang", "Tianxiang Zhao", "Junjie Xu", "Suhang Wang"], "abstract": "Graph self-training (GST), which selects and assigns pseudo-labels to unlabeled nodes, is popular for tackling label sparsity in graphs. However, recent study on homophily graphs show that GST methods could introduce and amplify distribution shift between training and test nodes as they tend to assign pseudo-labels to nodes they are good at. As GNNs typically perform better on homophilic nodes, there could be potential shifts towards homophilic pseudo-nodes, which is underexplored. Our preliminary experiments on heterophilic graphs verify that these methods can cause shifts in homophily ratio distributions, leading to training bias that improves performance on homophilic nodes while degrading it on heterophilic ones. Therefore, we study a novel problem of reducing homophily ratio distribution shifts during self-training on heterophilic graphs. A key challenge is the accurate calculation of homophily ratios and their distributions without extensive labeled data. To tackle them, we propose a novel Heterophily-aware Distribution Consistency-based Graph Self-Training (HC-GST) framework, which estimates homophily ratios using soft labels and optimizes a selection vector to align pseudo-nodes with the global homophily ratio distribution. Extensive experiments on both homophilic and heterophilic graphs show that HC-GST effectively reduces training bias and enhances self-training performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are pervasive in the real world, such as in social networks [25], knowledge graphs [30], and traffic networks [37]. Graph Neural Networks (GNNs) have achieved significant advances in semi-supervised node classification across both homophilic and heterophilic graphs [1, 5, 8, 13, 20, 38]. Despite their success, they often rely on abundant labeled data [6, 32] for training, yet manual labeling is not only labor-intensive but also impractical for new classes with sparse samples [33]. This has spurred increasing interest in node classification with sparse labels [18, 29].\nGraph self-training (GST) emerges as a promising method to harness abundant unlabeled nodes alongside a limited number of labeled nodes to tackle label sparsity in graphs [14, 16]. It typically involves three core steps: (i) pseudo-node selection, selecting high-confidence nodes; (ii) pseudo-label assignment, assigning the most probable labels; and (iii) retraining, where the GNN is trained on the augmented set, iterating these steps until convergence or a specified number of stages. However, typical confidence-based pseudo-node selection can cause distribution shifts [18, 44], potentially undermining self-training. This occurs as an increasing number of easy nodes, identified by their high confidence, are added to the original labeled training set, leading to the distribution gradually shifting towards this augmented training set and excessive focus on such easy nodes as a result. Methods like DRGST [18] and DCGST [31] address this by quantifying distribution shifts through information gain and GNN representation, respectively, and selecting pseudo-nodes that align with the distribution of the unlabeled set. Nevertheless, these methods are tailored for homophilic graphs, where connected nodes tend to have similar labels. In contrast, they neglect shifts in neighboring distributions (homophily ratio) in heterophilic graphs, where GNNs perform well on homophilic nodes but struggle with heterophilic ones, resulting in novel distribution shifts.\nTo investigate self-training on heterophilic graphs, our preliminary experiments on the representative Chameleon graph [26] in Sec.3.3 show that: (1) Training sets aligned with the true homophily distributions yield the best performance, as shown in Fig.1 (a); (2) GNNs excel on homophilic nodes, with accuracy positively correlating with the homophily ratio, evident from the blue line in Fig.1 (c). (3) Traditional self-training (ST) tends to select pseudo-nodes with higher homophily ratios, as depicted in Fig.1 (b), resulting in distribution shifts; (4) Strategies based on confidence or GNN representation consistency often lead to training bias, improving performance on homophilic nodes but worsening it on heterophilic nodes, as illustrated in Fig.1 (c) and (d). These results underscore the need for maintaining consistency in homophily ratio distribution (node counts across homophily bins) to mitigate training bias and improve self-training, a gap not fully addressed by existing methods. Thus, ensuring consistency between the homophily ratio distribution of the local (pseudo-)labeled training set and the global heterophilic graph to reduce training bias across homophily bins remains an open question.\nTherefore, in this paper, we study a novel problem of homophily ratio distribution consistency during self-training on heterophilic graphs. However, this presents challenges to maximizing self-training performance in obtaining accurate homophily ratios and distributions, aligning homophily ratios between (pseudo-)labeled nodes and the true distribution, and the low pseudo-labeling accuracy on heterophilic nodes. Thus, we pose a critical research question: How can we develop a GST framework which ensures selected pseudo-nodes align with true homophily ratio distributions and assigns accurate pseudo-labels to highly heterophilic nodes?\nTo address these challenges, we propose a novel Heterophily-aware Distribution Consistency-based Graph Self-Training (HC-GST) framework that adapts distribution consistency strategies from homophilic graphs. (1) Given the difficulty of acquiring exact homophily ratios and distributions without labels for a large number of unlabeled nodes, we develop an estimation method utilizing soft labels, which are more informative than one-hot labels. (2) To align pseudo-nodes with the target homophily ratio distribution, we optimize a selection vector q, each element representing the selection probability for nodes in a high-quality candidate set, selecting the top K nodes as pseudo-nodes. (3) After selecting pseudo-nodes, our framework assigns pseudo-labels. However, nodes with high heterophily often show lower accuracy because local neighbors typically belong to different classes. Inspired by [1, 41], we employ multi-hop neighbors to enhance labeling accuracy for heterophilic nodes, while using one-hop neighbors for homophilic nodes. Furthermore, although we carefully select pseudo-nodes that align with the target distribution, this approach may exclude inconsistent yet potentially high-quality nodes. To fully utilize these nodes, we introduce a dual-head GNN model: the main classifier head trains on clean and selected pseudo-nodes, while an auxiliary head focuses on the previously discarded high-quality pseudo-nodes, optimizing the feature extractor without compromising the performance of the main classifier. Our main contributions are:\n\u2022 We study a novel problem of reducing training bias across homophily bins caused by shifts in homophily ratio distribution, unique in heterophilic graphs compared to homophilic graphs.\n\u2022 We address the challenges of adapting distribution-consistent graph self-training methods to heterophilic graphs by introducing the innovative HC-GST framework, which utilizes a homophily ratio distribution consistency selection strategy.\n\u2022 Extensive experiments on both homophilic and heterophilic graphs consistently demonstrate the effectiveness of HC-GST in reducing training bias."}, {"title": "2 RELATED WORK", "content": "Graph Neural Networks with Heterophily. Graph Neural Networks (GNNs) update node representations through neighbor aggregation, aiding tasks like node classification [10, 11]. Their performance drops in heterophilic graphs [4, 36]. Solutions include higher-order neighbor aggregation (H2GCN [43], MixHop [1]) and differentiated message passing (GGCN [39], GPR-GNN [5]). BMGCN [8] uses a block similarity matrix to adapt to both graph types.\nGraph Self-training. Training GNNs with sparse labels for node classification often leverages self-training, using vast unlabeled data to enhance performance [14, 21, 27, 45]. Self-training expands the training set with high-quality pseudo-labels [16], primarily using confidence-based selection [16, 29, 34, 42, 45]. Techniques like M3S [29], which integrates deep clustering, and DSGSN [42], using negative sampling, address GNN training instability. However, methods like DRGST [18] and DCGST [31] that adjust to distribution shifts still assume homophily, limiting their effectiveness in heterophilic graphs. This paper adapts distribution consistency to better suit heterophilic graph characteristics.\nDistribution Shifts on Graphs. Training and testing nodes are assumed to have consistent distributions in most node classification benchmarks. However, real-world applications often experience distribution shifts between the labeled training sets and the actual data [35], leading to GNN classifiers overfitting, and negatively impacting deployment performance. Domain-Invariant Representation (DIR) learning addresses this by minimizing distribution differences across domains; using adversarial learning [7], or heuristic measures such as CMD [40] and MMD [19]. In self-training, inappropriate pseudo-node selection exacerbates distribution shifts [18, 31]. The unique challenge of homophily ratio distribution shifts in heterophilic graphs remains unexplored. This study is the first to explore shifts in homophily ratio distribution in heterophilic graphs within a self-training framework."}, {"title": "3 PRELIMINARY", "content": "In this section, we discuss self-training methods that tackle label sparsity and distribution shifts in homophilic graphs. Additionally, we conduct preliminary experiments to validate the unique training bias during self-training on heterophilic graphs. Finally, we formalize our problem definition."}, {"title": "3.1 Notations and GNNS", "content": "We use \\(G = \\{V, &, X\\}\\) to denote an attributed graph with \\(V = \\{v_1,..., v_n\\}\\) as the set of nodes, \\(&\\) as the set of edges, and X as a feature matrix of d-dimensional node features. The adjacency matrix A indicates connections with \\(A_{ij} = 1\\) if \\(v_i\\) and \\(v_j\\) are connected, otherwise 0. In semi-supervised node classification, only a small portion of nodes, \\(V_L\\), have labels \\(Y_L\\) from a one-hot matrix Y. The goal is to predict labels for unlabeled nodes using G and \\(V_L\\).\nGraph neural networks have shown great ability in modeling graphs. Generally, GNNs update node representations through a message-passing mechanism, aggregating features from neighboring nodes. Let \\(Z_i^{(l)}\\) denote \\(v_i\\)\u2019s representation at the l-th GNN layer. Then the message passing can be expressed as\n\\[Z^{(l+1)} = \\text{Aggregate} \\left(Z^{(l)}, \\sum_{v_j \\in N(v_i)} \\text{Propagate} \\left(Z^{(l)}, Z_j^{(l)}\\right), A_{i,j} \\right).\\]\nwhere \\(N(v_i)\\) denotes the set of neighbors of \\(v_i\\). We denote Z as the node representations in the last GNN layer."}, {"title": "3.2 Pseudo-labeling and Distribution Shift", "content": "Graph self-training is a widely used framework to tackle label sparsity by effectively leveraging large amounts of unlabeled data for performance gains. The success of self-training is its pseudo-labeling strategy, which carefully expands the training set with high-quality pseudo-nodes. However, graph data often exhibit distribution shifts that challenge self-training and pseudo-labeling strategies: (i) Label sparsity typically results in a non-representative training set, leading to distribution shifts away from the true distribution. These shifts can undermine models based on the IID assumption, as directly minimizing the average loss during training fails to yield predictors that generalize well under test distribution changes [15]. This prevents selecting high-quality pseudo-nodes; and (ii) An unrepresentative training set also causes imbalances in model capabilities, which during pseudo-label generation, tend to favor easier nodes. Consequently, this preference leads to an expanded pseudo-node set that deviates from the true distribution, adversely impacting self-training performance. Given the complex structure of graphs, measuring distribution shifts is challenging. To address this, Zhu et al. [44] has defined distribution shifts within the GNN representation space as the Central Moment Discrepancy (CMD) [40] distance between the global node representations \\(Z_G\\) from GNNs and their labeled counterparts \\(Z_L\\):\n\\[\\text{CMD}(Z_G, Z^L) = \\frac{1}{b-a} \\left\\lVert E(Z_G) - E(Z^L) \\right\\rVert^2 + \\frac{1}{b-a} \\sum_{k=1}^c \\left\\lVert c_k(Z_G) - c_k(Z^L) \\right\\rVert^2,\\]\nwhere E denotes the expectation, ck is the k-th order moment, and a, b are the joint distribution supports, typically calculating only up to k = 5 moments."}, {"title": "3.3 Training Bias of GST in Heterophilic Graphs", "content": "Existing work reduces distribution shifts in homophilic graphs within self-training frameworks [18, 31]. However, these methods do not extend well to heterophilic graphs, which may face unique challenges due to shifts in heterophily-aware distributions. To explore the unique challenges of self-training on heterophilic graphs, we conduct preliminary experiments. Heterophilic graphs mainly differ from homophilic ones in that a node tends to connect to nodes of dissimilar features or classes. To measure this, we first define node and graph homophily ratios.\nDefinition 3.1 (Node Homophily Ratio h(vi) [24]). h(vi) for a node vi measures the ratio of vi's neighbors that share the same label as vi, which can be written as \\(h(v_i) = \\frac{\\{v_j \\in N_i: y_j=y_i\\}}{|N_i|}\\) where \\(N_i\\) means the neighbors of node vi and yi is its golden label.\nDefinition 3.2 (Graph Homophily Ratio h(G) [24]). h(G) represents the average node homophily ratio across all nodes in the graph G, which is calculated as \\(h(G) = \\frac{\\sum_{v_i \\in V} h(v_i)}{|V|}\\).\nTo quantify the distribution of homophily ratios, we evenly divide the homophily level into N bins as\n\\[B = [|B_1, B_2,..., |B_N|],\\]\nwhere\n\\[B_i = \\left\\{v \\mid h(v) \\in \\frac{i}{N}\\right\\}.\\]\nThe global homophily ratio distribution \\(B_G\\) reflects these counts across the entire graph, whereas the local homophily ratio distribution \\(B_L\\) refers only to the (pseudo-)labeled nodes.\nBuilding on these concepts, we conduct preliminary experiments with the state-of-the-art heterophilic GNN model BMGCN [8] and the representative heterophilic graph Chameleon [26]. Initially, we evaluate the backbone GNN performance under the heterophily setting. (1) Impact of Homophily Ratio Distribution on Training Set: We construct training sets based on the known global homophily ratio distribution as: biased towards homophily, representative global distributions, and towards heterophily. This construction selects nodes until each homophily bin \\(B_i\\) meets its target. In homophily-biased sets, higher-index bins have more nodes (e.g., \\(\\sum_{i\\in[N-3,N]} |B_i| = |V_L|\\)); in heterophily-biased sets, lower-index bins are denser (e.g., \\(\\sum_{i\\in[1,4]} |B_i| = |V_L|\\)). For representative sets, bin sizes align with the global distribution, defined as \\(B_{\\text{target}} = \\frac{B}{\\sum B} \\times |V_L|\\). As shown in Fig.1 (a), sets aligned with the true distribution perform best. Compared to homophily-biased sets, those biased towards heterophily more closely mirror the true distribution, thereby achieving better performance. (2) Performance Analysis Across Homophily Bins: We display the accuracy for each homophily bin (illustrated by the blue line in Fig. 1 (c)). We find that accuracy positively correlates with the homophily ratio, confirming that GNNs are inherently better at processing homophilic nodes than heterophilic ones.\nNext, we continue to explore the effects of heterophily within the self-training framework. (3) Change of Homophily Ratio under Self-Training: We analyze homophily ratio changes under two self-training frameworks: traditional confidence-based graph self-training (ST) [16] and distribution-consistent graph self-training (DCGST) [31]. We calculate average homophily ratios for (pseudo-)labeled nodes (shown in Fig.1 (b)). Results show ST selects nodes with higher homophily, while DCGST reduces the average but still deviates from the global average, indicating previous self-training methods struggle to align well with global homophily ratios. We refer to this as the Homophily Ratio Distribution Shift. (4) Training Bias induced by Homophily Ratio Distribution Shift: To assess the impact of shifts in homophily ratio distribution, we analyze the accuracy of both self-training methods across various homophily bins, as shown in Fig.1 (c) and (d). Our findings indicate that the model excels in homophily-biased bins, where it naturally performs well, and struggles with heterophilic nodes, akin to the Matthew Effect in economics. We describe this phenomenon as training bias across homophily bins. After identifying unique distribution shift issue across homophily bins in heterophilic graphs, we define it as:\nDefinition 3.3 (Distribution Shift on Heterophily). Assume the global homophily ratio distribution \\(B_G\\) represents the count of nodes within each homophily bin across the entire graph, and the local homophily ratio distribution \\(B_L\\) applies to the locally labeled node set. Distribution shift \\(D(B_G, B_L)\\) is measured using metrics such as KL divergence or CMD between \\(B_G\\) and \\(B_L\\)."}, {"title": "3.4 Problem Definition", "content": "With the above analysis, our problem is formally defined as: Given a graph G with a small set of labeled nodes \\(V^L\\) and a large number of unlabeled nodes \\(V^U\\), self-training may induce or amplify the distribution shifts in homophily ratios between the labeled node set \\(V^L \\cup V^P\\) and the global graph. Our objective is to develop a graph self-training framework that enables a GNN model \\(f_{\\theta}\\) to accurately predict \\(\\hat{y_i} = \\arg \\max_j f_{\\theta} (v_i);\\) for each unlabeled node, while reducing training biases through the following goals: (1) Maximize positive performance variation: \\(\\max_{\\theta} \\sum_{\\Delta_i>0} \\Delta_i\\), where \\(\\Delta_i\\) represents the performance variance in the i-th homophily bin \\(B_i\\). (2) Minimize negative performance variation: \\(\\min_{\\theta} \\sum_{\\Delta_i  1\\)\n\\[|B_{\\text{target}}| = \\left[ B_{\\text{target}}^1, B_{\\text{target}}^2,...., B_{\\text{target}}^N \\right].\\]\nThe fundamental principle is that local homophily ratio distributions may not align with the global, so we calculate the difference between global and local homophily ratios to set node numbers per bin. This compensates the local homophily distribution to match the global, setting the target number of pseudo-nodes per bin.\n4.1.3 Pseudo-node Selection in Heterophilic Graphs. Next, we identify pseudo-nodes that align with the target homophily ratio distribution \\(B^{\\text{target}}\\). However, this target distribution does not account for the distribution shifts in node representations that is helpful for maintaining consistency in graph structures. To address this, we employ the CMD metric, a popular method for measuring distribution distances in GNN representations. This metric helps us ensure consistency between the local node representations \\(Z^L\\) and the global representations \\(Z^G\\), where Z represents the node outputs from the backbone GNN. Thus, we build two selection criteria: (1) selecting nodes that conform to the target homophily ratio distribution and (2) minimizing the CMD distance between the local \\(Z^L\\) and global \\(Z^G\\). To implement this, we initialize a selection vector q, where each element \\(q_i \\in [0, 1]\\) represents the selection probability of the i-th node in the high-quality candidate pseudo-node set C. The candidate pseudo-node set C is defined as follows:\n\\[\\mathcal{C} = \\left\\{ v_i \\mid \\max_j \\sigma(Z_i)_j > \\delta_c \\text{ and } v_i \\notin V^{<s-1} \\right\\},\\]\nwhere \\(\\sigma\\) is the softmax function, and \\(\\delta_c\\) represents the confidence threshold. This set includes nodes whose maximum softmax scores exceed \\(\\delta_c\\), excluding those already selected in previous stages. This selective criterion ensures that the candidate set \\(\\mathcal{C}\\) contains only high-quality, dynamically updated candidates. Furthermore, we ensure that exactly K nodes are chosen by enforcing the constraint \\(\\sum_i(q_i) = K\\). Beyond minimizing distribution shifts in GNN representations, our approach also reduces the KL distance between the homophily ratio distributions of the selection vector and the target.\n\\[\\begin{aligned} q = \\arg \\min &\\quad \\text{CMD}(Z_G, q*Z_C) + \\lambda_s\\text{KL}(B^q, B^{\\text{target}}) \\\\ \\text{s.t.} & \\quad ||q||_1 = K, q_i \\in [0, 1] \\\\ & |B^q| = \\sum_{v\\in C,j=\\text{ind}(v,C),\\hat{h}(v)\\in[\\frac{i}{N}, \\frac{i+1}{N})]} q_j \\end{aligned}\\]\nThe fundamental principle is that local homophily ratio distributions may not align with the target, so we calculate the difference between global and local homophily ratios to set node numbers per\nwhere q is the sole learnable parameter. CMD and KL metrics measure distribution distances, with \\(B^q\\) and \\(B^{\\text{target}}\\) representing the homophily ratio distributions of the selection vector and the target, respectively. \\(\\lambda_s\\) controls homophily ratio consistency, \\(Z_G\\) is the global representations, \\(q \\star Z_C\\) computes weighted local representations of candidate pseudo-nodes, and ind(v, C) indexes node v in C. After optimizing q, the top K nodes are selected as pseudo-nodes:\n\\[\\mathcal{V}_s = \\text{top-K}(q, K, \\mathcal{C}).\\]\ntop-K selects the K highest-ranked nodes from \\(\\mathcal{C}\\) based on q. This ensures consistency in GNN representation and homophily ratios. Loss Function of Pseudo-node Selection. Our pseudo-node selection relies on a selection vector q, which integrates the homophily ratio with the distribution consistency of GNN representations. The loss function for optimizing the selection vector is defined as:\n\\[L_q = \\text{CMD}(Z, q*Z_C) + \\lambda_s\\text{KL}(B^q, B^{\\text{target}}) + \\text{max}(0, ||q||_1 \u2013 K)\\]"}, {"title": "4.2 Pseudo-label Assignment with Multi-hop Neighbors", "content": "After selecting pseudo-nodes, our framework assigns pseudo-labels. Traditional methods based on the highest prediction probability may be ineffective on heterophilic graphs, where GNNs relying on local neighbors often mislabel heterophilic nodes, increasing noise and exacerbating training bias. To address this, drawing from [1, 41], which show that higher-order neighbors more reliably identify similar nodes, we use these neighbors for heterophilic pseudo-nodes to improve labeling accuracy. For homophilic nodes, we continue using local one-hop neighbors. The use of multi-hop neighbors is solely for assigning more accurate pseudo-labels; they are not employed in training the backbone GNN so as to preserve the original graph topology as including high-order neighbors can lose vital structural information, potentially affecting the estimation of homo ratios and calculation of distribution shifts in GNN representations. To be specific, we define the k-hop adjacency matrix as \\(A^k\\), the k-th power of A, with \\(H = f_{\\theta}(X, A^k)\\) for \\(k \\geq 2\\) representing the GNN's output in k-hop neighbors. The output used for pseudo-labeling is chosen based on the node homophily ratio:\n\\[Z' = \\begin{cases} H & \\text{if } h(v_i) < \\delta_H \\\\ Z & \\text{if } h(v_i) \\geq \\delta_H \\end{cases}\\]\nwhere \\(\\delta_H\\) is the threshold for identifying heterophilic nodes. For the set of pseudo-nodes \\(\\mathcal{V}_P\\), pseudo-labels are assigned based on the highest probability from the adaptive outputs:\n\\[\\hat{y}_{pl}^i = \\arg \\max_j Z'_{ij} \\quad v_i \\in \\mathcal{V}_P.\\]\nThis step is outlined in lines 9 to 10 of Algo. 1."}, {"title": "4.3 Dual-Head GNN for Fully Utilizing Pseudo-nodes", "content": "In graph self-training, training bias stems from the use of self-generated pseudo-labels. To mitigate this, we have introduced a careful selection of distribution-consistent pseudo-labeled nodes from a high-quality candidate set \\(\\mathcal{C}\\). However, this results in the discarding of many high-quality nodes that, while distribution-inconsistent, could be informative. Our objective is dual: to reduce training bias and to enhance overall self-training performance. The latter goal is compromised by discarding potentially valuable pseudo-nodes. To this end, drawing inspiration from the image field where feature extractors and classifier heads in a neural network classifier are conceptually separated [9], we also apply a separation in our GNN model, consisting of a feature extractor \\(\\psi\\) and a classifier head \\(h_{\\text{head}}\\):\n\\[f_{\\theta} = h_{\\text{head}} \\circ \\psi.\\]\nTo maximize the utility of all potential nodes, we employ a dual-head GNN training method. The main classifier head \\(h_{\\text{main}}\\) is trained on clean and carefully selected pseudo-nodes \\(V_L \\cup \\mathcal{V}_P\\). Meanwhile, the pseudo classifier head \\(h_{\\text{pseudo}}\\) enhances representation learning of the feature extractor by utilizing the remaining pseudo-nodes \\(\\mathcal{C} \u2013 \\mathcal{V}_P\\), without affecting the performance of the main classifier head. The training objective function is:\n\\[\\min_{\\psi,h,h_{\\text{pseudo}}} \\mathcal{L}_{VL\\cup VP} (\\psi, h_{\\text{main}}) + \\lambda_D\\mathcal{L}_{C-P} (\\psi, h_{\\text{pseudo}})\\]\nwhere \\(h_{\\text{pseudo}}\\) independently processes distribution-inconsistent nodes. Though \\(h_{\\text{main}}\\) and \\(h_{\\text{pseudo}}\\) are fed with features from the same feature extractor, their parameters are independent. \\(\\mathcal{L}\\) is the cross entropy loss function. \\(\\lambda_D\\) is a control factor of the pseudo-head loss term. Importantly, the pseudo classifier head, only responsible for training the feature extractor, is not used during inference. This approach effectively balances reducing bias and enhancing model performance, ensuring the full utilization of all high-quality pseudo-nodes. This step is outlined in lines 11 to 13 of Algo. 1."}, {"title": "4.4 Workflow of HC-GST Framework", "content": "After outlining the HC-GST framework's components-including distribution-consistent selection, multi-hop neighbors, and dual-head GNN-we detail our workflow in Fig. 2 and Algo. 1. Initially, we train a GNN on clean-labeled nodes to produce the output matrix Z. The pseudo-label set, VP (with s = 1 as the initial stage), begins empty. Nodes in the unlabeled set exceeding a confidence threshold \\(\\delta_c\\) form a candidate set C, from which distribution-consistent pseudo-nodes, Vs \\mathcal{V}_s, are selected. Next, a multi-hop neighbor method assigns pseudo-labels to these nodes. Finally, we replace the last GNN with a dual-head GNN trained on both the expanded labeled nodes and other high-quality nodes. We continuously update the pseudo-labeled node set to VP with fresh labels and the output matrix with the new one, repeating these steps until we reach a predetermined number of stages S or achieve performance convergence. The GNN with the best validation accuracy is selected as the final model, with its output as the final result."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments on both homophilic and heterophilic graphs under various label rates to evaluate the effectiveness of the proposed framework."}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Datasets. We use four heterophilic datasets: Chameleon, Squirrel, Texas, arXiv-year [17, 26], and three homophilic datasets:"}, {"title": "5.2 Results and Analysis", "content": "To explore HC-GST's capability with a biased training set in homophily ratio distribution, we randomly generate local homophily ratio distributions for the training set that deviates from the global distribution. Nodes are then selected one by one according to the local distribution. Tables 2 and 3 compare our method with baseline approaches on datasets with biased training sets in both homophily and heterophily settings. We observe: (1) Across four heterophily graphs, our method excels in comprehensive metrics, ACC and TPV, improving by 3.74% and 2.11% respectively over the runner-up on average. This highlights the effectiveness of our framework in self-training on heterophilic graphs. (2) For training bias metrics, NPV and PPV (where a smaller gap is preferable), our method consistently outperforms others, showing that our distribution consistency selection effectively reduces the training bias. Conversely, ST and M3S ignore distribution shifts, leading to significant training bias. DRGST and DCGST, despite considering consistency, fail on heterophilic graphs due to biased training nodes that significantly affect GNN representations. (3) We also note similar patterns across three homophilic graphs. This indicates that homophily ratio distribution shifts are a widespread issue in graph learning, irrespective of the graph type. Our best performance proves the generalization of our method across heterophily and homophily settings. (4) Examining accuracy changes under various label rates between our method and the backbone, we observe more substantial improvements at lower label rates. This indicates that our self-training approach is particularly effective in few-shot scenarios.\nTable 4 details results on six graphs at a 1% label rate without obvious homophily ratio distribution shift between the clean training set and the global graph. This is achieved through normal random sampling of training nodes, which tends to produce a homophily ratio distribution similar to the global one. Negligible shifts in the training set suggest that distribution shifts primarily arise from pseudo-labeling. Compared to DRGST and DCGST, which focus on reducing distribution shifts through GNN representations, our method is more effective across all metrics, demonstrating the effectiveness of our homophily ratio consistency module. In contrast, ST and M3S exhibit the most significant training bias, underscoring the importance of addressing distribution shifts during pseudo-labeling."}, {"title": "5.3 Ablation Study", "content": "To assess the impact of each component within the HC-GST framework, we compare it against various variants: (1) HCGST-w/o-MultiHop, removing the multi-hop neighbors module; (2) HCGST-w/o-DualHead, without the dual-head mechanism; (3) HCGST-w/o-Selection, which excludes the distribution consistency selection. For clarity, we remove the hyphen from our framework name. We conduct this ablation study on Chameleon with a biased training set at a 1% label rate. As shown in Table 5, the findings are: (1) Removing the distribution consistency module leads to a significant accuracy decline of 4.34%, underscoring the importance of distribution consistency in selecting high-quality pseudo-labels. Besides, the gap between NPV and PPV is larger than in the original, illustrating its contribution to mitigating training bias; (2) The absence of the multi-hop neighbors results in a 1.94% drop in accuracy, validating its importance for accurately assigning pseudo-labels; (3) Without the dual-head mechanism, accuracy decreases by 0.61%, underscoring its effectiveness in fully utilizing pseudo-nodes."}, {"title": "5.4 Analysis of Hyper-parameter Impact", "content": "We assess the impacts of key hyperparameters \\(\\lambda_s\\), \\(\\lambda_p\\), and \\(\\delta_H\\) within the HC-GST framework, which regulate the distribution consistency selection module, the dual-head module, and the multi-hop"}, {"title": "5.5 Benefits of Reducing Heterophily-aware Distribution Shift", "content": "To highlight the benefits of reducing shifts in homophily ratio distribution, we visualize changes in homophily ratios on Squirrel and training bias on Chameleon at a 1% label rate. Fig. 4 displays variations in mean homophily ratios and the KL divergence between local and global distributions during self-training stages. Fig. 5 compares accuracies across different homophily bins before and after the baselines and with our method. Our findings include: (1) Our method selects pseudo-nodes closer to the global distribution more effectively than baselines, as evidenced by closer mean homophily ratio values and reduced KL distance in Fig. 4; (2) Consistent homophily between pseudo-nodes and the global distribution reduces training bias, as indicated by our smaller negative and larger positive areas than baselines across homophily bins in Fig. 5."}, {"title": "6 CONCLUSION", "content": "In this study, we address a unique self-training problem on heterophilic graphs: training bias due to shifts in homophily ratio distribution. We introduce HC-GST, a self-training framework designed for homophily ratio distribution consistency, which selects pseudo-nodes that align with the global using soft labels to estimate homophily ratios. Utilizing multi-hop neighbors for accurate pseudo-labeling and a dual-head GNN to fully exploit pseudo-labels further enhances model performance. To measure training bias on heterophilic graphs, we introduce three new metrics: TPV, NPV, and PPV. Extensive experiments on both homophilic and heterophilic graphs consistently confirm HC-GST's effectiveness in reducing training bias and improving self-training performance."}]}