{"title": "ALLAM: Large Language Models for Arabic and English", "authors": ["M Saiful Bari", "Yazeed Alnumay", "Norah A. Alzahrani", "Nouf M. Alotaibi", "Hisham A. Alyahya", "Sultan AlRashed", "Faisal A. Mirza", "Shaykhah Z. Alsubaie", "Hassan A. Alahmed", "Ghadah Alabduljabbar", "Raghad Alkhathran", "Yousef Almushayqih", "Raneem Alnajim", "Salman Alsubaihi", "Maryam Al Mansour", "Majed Alrubaian", "Ali Alammari", "Zaki Alawami", "Abdulmohsen Al-Thubaity", "Ahmed Abdelali", "Jeril Kuriakose", "Abdalghani Abujabal", "Nora Al-Twairesh", "Areeb Alowisheq", "Haidar Khan"], "abstract": "We present ALLAM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLAM is carefully trained considering the values of language alignment and knowledge transfer at scale. Our autoregressive decoder-only architecture models demonstrate how second-language acquisition via vocabulary expansion and pretraining on a mixture of Arabic and English text can steer a model towards a new language (Arabic) without any catastrophic forgetting in the original language (English). Furthermore, we highlight the effectiveness of using parallel/translated data to aid the process of knowledge alignment between languages. Finally, we show that extensive alignment with human preferences can significantly enhance the performance of a language model compared to models of a larger scale with lower quality alignment. ALLaM achieves state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from their base aligned models.", "sections": [{"title": "1 Introduction", "content": "Language modeling has significantly progressed from its humble origins, transitioning from fundamental probabilistic methods to complex neural priors. The foundational work by Shannon (1951) on the information theory of language laid the groundwork for predicting the next word in a sequence, which was subsequently tackled by Bengio et al. (2003) in neural networks. The field experienced a substantial leap with the introduction of LSTMS (Hochreiter & Schmidhuber, 1997) in language models (LM) (Peters et al., 2018b), which could capture longer dependencies in LMs but proved difficult to scale. The emergence of scalable and distributed architectures like Transformers (Vaswani et al., 2017) and the potential for precisely compressing web-scale data has resonated in recent years with the advancements of Generative Pretraining (Radford et al., 2018; Brown et al., 2020a; Anil et al., 2023). \nWith the release of ChatGPT (OpenAI, 2022), followed by the introduction of more frontier class models Gemini (Google, 2024), Claude (Anthropic, 2022), Reka (Ormazabal et al., 2024), Mistral (Mistral, 2024), Llama-3 (Meta, 2024) and recently released Qwen-2 , large language models have demonstrated significant leaps over each generation of models (Laskar et al., 2023). This exponential growth in performance has raised hope in the possibility of achieving Artificial General Intelligence (Hendrycks & Mazeika, 2022; Marcus, 2022). This rapid advancement has spurred discussions across various fields, including ethics, economics, and technology (Weidinger et al., 2021). Judging from the initial capabilities , the potential of these frontier models are reinventing the way humans interact with machines, impacting social norms, productivity, trends, and culture on a broader scale (Zhou et al., 2024). However, most of these frontier-class models are primarily trained on English and often lack a connection to localized regional cultures and norms (Naous et al., 2024). This gap has the potential to result in slow and irreversible manipulation of regional identities and lead to cultural homogenization.\nThe natural course to reverse this trend is to invest resources in curating data and building models to support the diversity of languages and cultures represented in the modern world. While this is possible, the significant training costs of LLMs and their environmental impact have become major concerns in recent years (Strubell et al., 2019). The vast computational resources required to train LLMs contribute to substantial carbon emissions (Luccioni & Hernandez-Garcia, 2023). Governments  and non/for-profit organizations (Dodge et al., 2022; Google, 2021; Amazon, 2021), are increasingly aware of these issues. This awareness has led to discussions about the ethical implications of AI development and the need for sustainable practices concerning \u201cWhen and how to scale the training of these models\". In addition, curating data for each language/region at pretraining scale is also an extremely difficult task, since most available data comes from a few high-resource languages.\nTo address these concerns, we consider the problem of adapting strong, but potentially under-trained, open pretrained models, rather than starting from a randomly initialized model. Technically, this involves continuing training of a model in a new language to facilitate Second Language Acquisition (SLA) (Swain & Lapkin, 1995), popularized by Bari et al. (2020) in NLP and recently adapted to LLMs by Nguyen et al. (2023). This process involves the challenging task of incorporating an additional language distribution without compromising the source language(s). For instance, if a pretrained model was initially trained in English, expanding to an additional language presents challenges related to tokenization.\""}, {"title": "2 Pretraining", "content": "Pretraining language models on trillions of natural language tokens represents the bulk of the cost required to build an effective language model. This large investment of time and compute precludes experimentation or ablation for every decision. Thus, before starting to train ALLaM from random initialization, or \u201cscratch\u201d, we experiment in the continue-pretraining regime. As the name implies, continue pretraining is the practice of warm-starting a pretraining experiment from an already pretrained LM.\nWe begin by discussing our entire pretraining corpus, describe experiments conducted with continue-pretraining, and finally describe pretraining from scratch."}, {"title": "2.1 Pretraining Data", "content": "For English, many high quality and large scale datasets are available for pretraining (Computer, 2023; Soldaini et al., 2024; Gao et al., 2021; Penedo et al., 2023). We harnessed subsets from Dolma-v1 (Soldaini et al., 2024) and Pile (Gao et al., 2021) datasets e.g., Dolma-CC, The Stack (Kocetkov et al., 2022), PeS2o, PubMed, DM-Math (Saxton et al., 2019) and StackExchange (Soboleva et al., 2023). In total, we had access to 4T high to medium quality English tokens for pretraining.\nPretraining data in the Arabic language is much more limited, thus we undertook large scale collection and curation of Arabic language data. This includes in-house crawled sources covering Web documents, news articles, books (literature, religion, law and culture, among others), Wikipedia (over 1M articles), and audio transcripts (books and news)4. To ensure high quality Web data, we applied the following processing steps: (i) drop documents with language identification score below 95%, (ii) drop short documents that are less than 30 words, (iii) drop documents with duplicate URLs or high ratio of spam and stop words, and (iv) drop duplicate documents using exact matching. We experimented with fuzzy matching but opted against using it as it was too restrictive given the scarcity of Arabic data.\nAdditionally, we extended our Arabic data with translated English content using an in-house machine translation system. We translated the following English datasets from Dolma: Wikipedia, books, C4 and peS2o, which also are part of our English data. The hypothesis is that this will improve Arabic-English language alignment, leading to a better Arabic model. Figure 4 demonstrates the impact of Arabic translated datasets in the pretraining data mixture. While models trained without translated data exhibit lower training loss, those trained with translated data show more stable training, as evidenced by fewer spikes in gradient norms. Incorporating Arabic translated data in the pretraining dataset mitigates catastrophic forgetting in English. In total, we curate 540B Arabic tokens of which 270B are natural Arabic tokens and 270B are translated Arabic tokens.\nData Mixture To build a performant model in both Arabic and English, we conducted experiments to determine the optimal language mix."}, {"title": "2.2 Continued Pretraining", "content": "Open source and open weight models present an attractive option to conduct pretraining experiments cheaply. However, they also present challenges, since most such models do not natively support Arabic or other languages. We develop a simple approach to enhance any language model with capabilities in new languages (i.e., language expansion). The approach relies on two steps: (i) tokenizer augmentation, and (ii) expanded vocabulary learning. We demonstrate that this approach leads to minimal degradation of capabilities in the original language.\nTokenizer Augmentation Existing open weight language models (e.g., Llama-2) tokenize Arabic (and other languages) poorly, often splitting words down to the character level or even relying on byte-fallback mechanisms for tokenization. This results in: (i) inefficient training, as the pretraining corpus size is inflated, (ii) unoptimized inference, since the model must generate more tokens per word, and (iii) the effective context length is reduced, because it is based on a fixed number of tokens. To address these issues, we use a corpus of text in the target language to train a tokenizer specialized in that language. We then merge the original tokenizer with the language-specific tokenizer. Merging is accomplished by adding all tokens from the language-specific tokenizer that do not exist in the original tokenizer. As shown in Figure 3, this effectively reduces the fertility rate in the target language of the merged tokenizer to the level of the language-specific tokenizer.\nExpanded Vocabulary Learning Newly added tokens in the merged tokenizer have no associated embedding representations in the pretrained language model's weights. To learn these representations, we experiment with two approaches: (i) random initialization and (ii) initialization from combined representations of tokens in the original tokenizer. Approach (ii) is accomplished by tokenizing each token in the vocabulary of the new tokenizer using the original tokenizer. The associated embedding representations of this tokenization are then averaged and assigned as the vector representation of the new token. Since we work with tokenizers with byte-fallback, such a tokenization is guaranteed to exist."}, {"title": "Experiment Details", "content": "Starting from Llama-2 pretrained model weights, we continue pretraining the ALLAM-7B and ALLAM-13B models on 1.2T tokens, covering both English and Arabic languages. For the ALLAM-70B model, we only train up to 600B tokens (using the same data mixture). In all of our continued pretraining experiments, we used the final learning rate of the pretrained language model (usually 3 \u00d7 10\u22125). We experimented with approaches to gradually increase and decay the learning rate with limited success, as such models typically exhibited catastrophic forgetting, indicated by significant drops in performance in the original language. We also considered optimizer state warm up, as open-weight models typically do not include the optimizer states, but found this had little effect on performance. Figure 7 provides an overview of adding dropout during continued pretraining. We observe that adding dropout helps the Arabic language, as it acts as a regularizer for the new distribution. However, Llama-2 was pretrained on 2T tokens without any dropout, and adding dropout negatively impacts the source language performance. Considering this trade-off, we decided not to add dropout in the continued pretraining stage. Unlike recent trends , we did not add any alignment data in this stage of training."}, {"title": "2.3 Pretraining From Scratch", "content": "Following , training a high-quality model from scratch requires a substantial amount of tokens. Even when pretraining from random initialization, we find it beneficial to train with a high-resource language for trillions of tokens (English) and then continue training with a mixture of Arabic and English tokens. On small scale experiments (with 1B parameter models) we find that beginning training with two languages can sometimes degrade the performance in English or result in slow learning of both language distributions. From this, we hypothesize that low-resource languages are diluted in the large volume of high-resource language data when pretraining from scratch, even with upsampling and careful tuning.\nTraining Recipe Our pretraining from scratch recipe consists of two steps: training on 4T English tokens followed by training on 1.2T mixed Arabic/English tokens. This retains the English capabilities of the model without catastrophic forgetting, effectively transferring knowledge from one language distribution to another. The only difference between pretraining from scratch and continued pretraining from an existing model is that vocabulary expansion is not required.\nWe match hyperparameters and architecture for pretraining from scratch with Touvron et al. (2023a), including 4M tokens per batch and max LR 3 \u00d7 10\u22124 decayed to 3 \u00d7 10\u22125 with a cosine schedule.\nEnglish Data Mixture The last column of Table 1 shows the domain mix for the English only pretraining dataset. As expected, web data represent the bulk of the mixture, followed by code and scientific articles."}, {"title": "2.4 Compute and Training Infrastructure", "content": "Over the course of our development of ALLAM, we had access to 128-1024 A100 GPUs. Our GPU cluster was equipped with InfiniBand connections to enable high-speed communication between nodes. The all-reduce test on the cluster ranges around 1200-1400 Gbps (node-node interconnect (ROCE)). The entire training period of the models is estimated to be 5M GPU hours.\nAt the start of the project, we forked Megatron-LM and applied our own customizations (including improving data iterators, adding metadata in the checkpoints, and custom data pipelines). We utilized data, tensor, and pipeline parallelism supported by Megatron-LM to efficiently train at a large scale as well as FlashAttention . By leveraging these techniques, we achieved significant improvements in training speed. The throughput per GPU varied from 135 to 167 TFlop/s/GPU depending on the number of GPUs, number of nodes, batch size, and parallelism strategy. We trained ALLaM with bf 16 mixed-precision."}, {"title": "3 Alignment", "content": "Building useful LLMs requires ensuring they are able to follow instructions while adhering to ethical standards and user expectations. This alignment process is especially crucial for models used in diverse linguistic and cultural contexts. In our setting, this means aligning models to the Arabic language and cultural context while also supporting English.\nSupervised Fine-Tuning (SFT) (Section 3.1) refines a pretrained model using a carefully selected dataset relevant to specific tasks and domains. Preference training (Section 3.2), on the other hand, aligns the model's outputs with human values and preferences by prioritizing responses that meet user expectations and ethical guidelines. These methods work together to create reliable and ethically sound LLMs for real-world use."}, {"title": "3.1 Supervised Fine-Tuning", "content": "Data Our SFT data is curated from a diverse array of sources. Given a piece of context from a source, we utilize humans and/or generative models (Ding et al., 2023) to identify if the text can be considered suitable for supervised fine-tuning or if we can generate instructions to create an SFT example from the context. For English, we primarily use public web content as our main source, offering a broad range of high-quality and especially diverse prompts. In contrast, our Arabic data comes from a combination of public and proprietary sources to ensure comprehensive coverage and relevance. To gather data from the source, we collect seed websites or data sources, which involves utilizing domain experts, prompt librarians, local institutes specializing in areas such as Arabic language, history, and politics, the use of commercially permissible licensed LLMs to generate data, and machine translation models to convert rich English SFT data into Arabic. Our datasets cover various domains and capabilities, ensuring the model's proficiency in handling tasks across education, history, Arabic linguistics, politics, religion, computer science, and other fields. The entire Arabic/English collection is called Ultra-Instinct, which is not human generated, but rather, human driven.\nQuality Filtering Unlike Zhou et al. (2023); AI et al. (2024), we hypothesized that scaling SFT data can unlock diverse capability, as well as improve responsiveness to the prompts. Initially, we crawled the public web for SFT samples. The first version (v1) of Ultra-Instinct includes 12M samples evenly split between English and Arabic, while the second version (v2), is a reduced version with half the samples. Compared to v1, v2 underwent strict quality checks and human assessments of random subsamples. Our quality checks for v2 included (i) assessments based on instruction/response word length, (ii) lexical  and semantic diversity, exact and near-exact lexical deduplication, (iii) removal of low quality machine-translated Arabic data from English sources, and (iv) ensuring diversity in questions and commands. For detailed metrics on instruction and response lengths and lexical diversity, see Table 2."}, {"title": "3.2 Preference Training", "content": "After SFT, models are able to converse in multi-turn conversations. However, they are not fully aligned with human preferences. For example, our SFT models were terse and had limited guardrails."}, {"title": "To circumvent these issues, we performed preference tuning with human verified samples via Direct Preference Optimization (DPO)", "content": "Data The inputs were sourced from early model testers and a manually curated selection of prompts from various domains or attack vectors. These include ethical dilemmas, middle eastern culture, religions, illegal activities, human rights, locale awareness, and personality.\nPreference training necessitates both negative and positive outputs for each input. We relied on the testers' feedback to identify the positive outputs. In the absence of positive outputs, we used a model to generate an output and manually verified that the output was aligned. While (Tunstall et al., 2023) utilized preference data from AI Feedback (AIF) at scale, we adopt a more cautious approach in creating preference data. We generate a smaller volume of data, ensuring it is fully reviewed, edited, and verified by humans.\nThere are two approaches for generating negative outputs: (i) on-policy: use the generations of the model we are tuning as negative outputs, and (ii) off-policy: use another similar model to generate the negative outputs. We did not verify that the negative outputs were worse than the positive. However, we ensured that the positive outputs were of the highest quality, such that they were almost always better than the negative outputs.\nKhan et al. (2023) demonstrated that model outputs can vary significantly depending on the sampling mechanism used. Building on this insight, we generate additional samples for each instance by varying temperature and nucleus sampling techniques. These additional samples are utilized to produce rejected samples, ensuring that ALLaM provides more grounded responses and generalizes well across various sampling mechanisms.\nIn total, we collected 25,854 samples (triplets of {prompt, accepted, rejected}) in English and Arabic language. Using the technique mentioned above, we sample 10 different response from the model to generate additional rejected responses for each sample. This results in a dataset of 245K samples (after filtering) for preference training.\nTraining For DPO, we used a batch size of 512 with $KL_{penalty}$ = 0.1 and a learning rate of 9 \u00d7 10-7 decayed to 5 \u00d7 10\u20137 using a cosine annealing learning rate schedule. We train ALLaM for a single epoch using all the preference data.\nFrom our initial experiments with small datasets, we observed issues with model quality even when a small fraction (0.1%) of the data was noisy. In this context, noise can be improper labeling of positive/negative pairs or low quality positive outputs. It is not clear, however, if after scaling up the DPO data whether the model can ignore this type of noise. In early DPO models, trained on data where we did not verify all the samples, we found that even a few moderately noisy samples resulted in broken models that repeatedly generate the same text or output incoherent text.\nDPO vs. PPO One of the fundamental differences between DPO and PPO is that PPO is always on-policy with an external reward model. In our experience with DPO, we did not encounter any significant issues with off-policy experiments. Additionally, DPO allows for faster iteration and easier understanding of the training dynamics. The decision to use DPO over PPO was based on logistical constraints rather than a performance comparison of the algorithms. Given our compute setup and time constraints, we chose to proceed with DPO. We plan to explore PPO in the future for alignment."}, {"title": "4 Evaluation", "content": "In this section, we describe the evaluation of our model and report the results of ALLAM 7B, 13B, and 70B models, as well as other relevant models, such as GPT-4, Command-R+ , and Jais-30B . Our evaluations encompass three main types: (i) automatic evaluations, (ii) LLM-based evaluations, and (iii) human evaluations.\nLimitations Recently,  showed that multiple choice or cloze test based evaluation may not be robust. In addition, MT-Bench uses an LLM as a judge, and has likely leaked into training datasets. Unfortunately, human evaluation is time-consuming and requires well-trained human evaluators. In this work, we try to ensure robust evaluation and attain a balanced assessment of the quantitative metrics and qualitative effectiveness of models in various applications and domains."}, {"title": "4.1 Automatic Evaluations", "content": "The automatic evaluations cover Arabic and English benchmarks grouped into the categories listed below:\n1. Multi-domain: MixEval (Ni et al., 2024), MMLU-Pro , and BBH .\n2. Reasoning and Commonsense: HellaSwag , PIQA , WinoGrande , and AraSwag .\n3. World Knowledge and Language Understanding: MMLU , ARC Easy and Challenge , TriviaQA , BoolQ , NQ Open , AGIEval , Exams-Ar , MMLU Arabic (tr) , MMLU Arabic (MBZU) , and ETEC (in-house curated).\n4. Safety and Alignment: Hendrycks Ethics , ACVA , TruthfulQA , and AraTruthfulQA (in-house curated).\n5. Conversation: MT-Bench , and Arabic domain capability dataset (in-house curated).\n6. Math: Minerva MATH , GSM8K and AraMath (in-house curated).\n7. Coding: HumanEval (THUDM, 2022)\nThe following benchmarks were curated and developed in-house:\n\u2022 ETEC: a collection of 1891 multiple choice questions covering different exams performed by the Education and Training Evaluation Commission in Saudi Arabia.\n\u2022 AraMath: a set of 600 test samples that were post-processed and prepared from the Ara-Math  dataset. These samples focus on testing the models' performance on Arabic math problems.\n\u2022 AraTruthfulQA: a dataset created using similar methodology to the TruthfulQA  dataset. It comprises a total of 541 samples, 285 samples were translated directly from TruthfulQA using GPT-4 and carefully validated and localized by human verifiers. Additionally, 256 questions were curated by humans to ensure their contextual relevance and cultural appropriateness.\nWhile serving as a good test bench, observing the dynamics of automatic evaluations during training is also interesting. Figure 10 shows the behavior of selected benchmarks during mixed Arabic/English pretraining while scaling up model size. In particular, we observe that smaller models tradeoff between capability in the new and original languages. However, larger models can simultaneously improve in both languages."}, {"title": "4.2 LLM-Based Evaluations", "content": "MT-Bench  consists of 80 multi-turn questions to evaluate models' capabilities on complex instruction-following. In addition to the English version, MT-Bench Arabic was created using GPT-4 to translate the original dataset and human annotators to review and align the prompts to Arabic culture. GPT-4 serves as the LLM judge, scoring responses as recommended in . Model performance is compared turn by turn, with results shown in Table 6, where ALLAM-70B achieves the best Arabic performance."}, {"title": "4.3 Human Evaluation", "content": "We developed an Arabic multi-turn dataset that covers seven domains: Arabic linguistics, history, health, politics, coding, entertainment, and ethics, each domain contains ten questions with two turns.\nHuman evaluators compared the responses from two models and were asked to choose the winning response with the following instructions:\n\u2022 Choose a response as the winner if it is the best, tie if both responses are equally good, and both-bad if both responses are not good."}, {"title": "5 Related Work", "content": "Our work sits at the cross-section of research building language models that support multiple languages and scaling such techniques in terms of size and data. To successfully train a large language model in a language other than English requires a complete understanding of cross-lingual transferability between languages and a good understanding of scaling laws as well as the fundamentals of training large language models. In this section, we discuss work on language modeling from the perspectives of cross-lingual alignment, multitask learning, and Arabic specialization."}, {"title": "5.1 Language Modeling and Cross-lingual Representations", "content": "In early work, word representations were derived using basic forms of the skip-gram model , wherein each word is assigned a representation that does not account for varying contexts (Grave et al., 2018; Pennington et al., 2014). Further work in this area developed word representations that are adaptive to the context surrounding the words (McCann et al., 2017; Peters et al., 2018a; Howard & Ruder, 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019).\nPeters et al. (2018a) introduced ELMo, a model built with a bidirectional LSTM-based language model (LM) for pretraining to obtain contextualized word representations. This technique combines the outputs from all layers linearly when targeting specific tasks. Nonetheless, the sequential nature of LSTM-based LM pretraining presents challenges in scaling training efficiently. Concurrently, Vaswani et al. (2017) developed the Transformer architecture, which leverages multi-headed self-attention and positional encoding to handle long-range dependencies and enable parallel processing."}, {"title": "5.2 Multitask Learning And Alignment", "content": "Early work has demonstrated that multitask learning can enhance the performance of NLP models . In explicit multitask learning, augmenting all samples during training may introduce noise due to differing output distributions in a traditional full-model fine-tuning setup . For implicit multitask learning, Radford et al. (2019) showed that a language model can begin to learn downstream tasks without explicit supervision by pretraining alone. Large language models (Brown et al., 2020b; Smith et al., 2022; Chowdhery et al., 2022) at scale can perform few-shot in-context learning, making them effective multitask models. Additionally, Sanh et al. (2021); Wei et al. (2021); Muennighoff et al. (2022); Chung et al. (2022) found that these implicitly learned language models could be further improved by explicitly fine-tuning them with human instructions and prompts  in a multitask fashion. Unlike previous template-based prompting approaches, Ouyang et al. (2022) applied preference tuning with reinforcement learning  using naturally written prompts. Subsequently, Bai et al. (2022) introduced Constitutional AI to automate alignment using AI feedback. Recently, following the work of Rafailov et al. (2023), various efforts  have been directed towards preference tuning without explicit reward models."}, {"title": "5.3 Language Models for Arabic", "content": "As of the time of writing, the most prominent Arabic-focused LLMs are:\n1. Jais : 13B and 30B base and chat models trained from scratch using a combination of natural and translated Arabic data along with English and code data.\n2. AceGPT : 7B and 13B base and chat models trained from Llama-2 without vocabulary expansion.\nWhile Jais and AceGPT are currently the most prominent models, early open models such as AraGPT , AraT5 , AraBART , and Noon  pioneered the area with models developed with limited resources to serve Arabic.\nOther models such as Jasmine  and Aramus  also showed the need for building a language model for over 400 million speakers worldwide.\nIn addition to the language adaptation of models and multilingual models reviewed above, recent work has focused on building multilingual/bilingual language models from open weight language models. For example, Ruci\u0144ski (2024) adapted Mistral 7B for the Polish without vocabulary expansion. Mala-500 is another effort to expand to 534 languages by expanding the vocabulary to 260K tokens and further pretrained Llama-2 using LoRA adaptors . Due to the number of languages they aimed to support, a small amount of data was included for each language and the evaluation of the approach was limited to measuring perplexity and automatic classification benchmarks.  introduced a Chinese Language adaptation of Llama and Alpaca models, where the vocabulary was increased to 50K tokens, then continued to pretrain the models and finally fine-tune them."}, {"title": "6 Conclusion", "content": "The ALLAM model series marks a significant advancement in Arabic Language Technologies, achieving state-of-the-art performance across various Arabic benchmarks while maintaining or enhancing English performance. Through careful training that emphasizes language alignment and transferability, our models demonstrate effective second-language acquisition without catastrophic forgetting. The strategic use of translated data, knowledge encoding, and alignment with human preferences have been crucial in this success."}, {"title": "7 Limitations", "content": "ALLAM was trained on data that may potentially include toxic language, unsafe content, and societal biases originally sourced from the internet, leading to the possible amplification of these biases and toxic responses. Although ALLaM underwent comprehensive safety training during the alignment phase, more community feedback is needed to iteratively improve ALLAM. Additionally, inherent uncertainties in generative models mean that trials cannot encompass every possible use case, making it impossible to predict the model's responses in all contexts. This can occasionally result in inaccurate, biased, or socially unacceptable outputs, even if the prompt itself is not explicitly offensive. Developers must conduct thorough safety evaluations and make specific adjustments to ensure that ALLAM is suitable for their intended purposes. Furthermore, the output generated by ALLAM should not be considered a statement from ALLAM's creators or any affiliated organization."}, {"title": "8 Ethical Statement", "content": "While conducting and presenting this research, we are committed to upholding the highest ethical standards. We recognize the potential impact of large language models on society and the importance of ensuring their responsible development and deployment. Our work adheres to principles of fairness, transparency, and inclusivity, striving to mitigate biases and ensure diverse representation"}, {"title": "9 Risk Statement", "content": "The deployment and use of LLMs in various applications poses significant risks, including data privacy and security concerns due to the inadvertent inclusion of sensitive information in training datasets. LLMs can perpetuate or amplify biases, resulting in unfair treatment and discrimination in critical decision-making processes. They can also generate convincing but inaccurate content, spreading misinformation and potentially influencing public opinion negatively. Over-reliance on LLMs may diminish human judgment, and the models' susceptibility to adversarial attacks can compromise system integrity. To mitigate these risks, we follow robust governance, continuous monitoring, and iterative improvements. We also adhere to best practices in data handling and model training, fostering transparency and accountability in LLM development."}, {"title": "B Author Contributions", "content": "In no particular order,\n\u2022 M Saiful Bari: Research lead for alignment at scale (second language acquisition). Wrote the basic orchestration and optimization of the LLM training framework, data, and training deployment. Trained all the models explained in the paper. Helped build the evaluation infrastructure and orchestrate the entire LLM engine. Implemented the alignment training pipeline and sandboxing environments. Served as a prompt librarian. Supported any project requirements regardless of training, data, or evaluation. Wrote a major part of the paper.\n\u2022 Yazeed Alnumay: Implemented pretraining data translation at scale. Worked on vocabulary embeddings augmentation and initialization, as well as the model framework conversion logic. Built the automatic evaluation infrastructure. Acted as a prompt librarian and built the SFT data infrastructure, including SFT and preference data acquisition, curation, processing, cleaning, and lifecycle management. Supported any project requirements regardless of training, data, or evaluation. Contributed to the figures and manuscript writing.\n\u2022 Haidar Khan: Guided and reviewed all aspects of the work, with a focus on pretraining and alignment decisions. Contributed directly to optimizing the training infrastructure and trained the 7B model from scratch. Edited and wrote majority of the paper.\n\u2022 Areeb Alowisheq: Managed the ALLaM training team and wrote the configurations for v0 models. Contributed to all research and logistics decisions for ALLaM. Edited and reviewed the paper, wrote the Arabic LLM related works section.\n\u2022 Nora Al-Twairesh: Managed the data and evaluation effort of ALLaM. Assisted in building the complete data and evaluation ecosystem for the ALLaM project. Edited and reviewed the paper.\n\u2022 Abdalghani Abujabal: Worked on SFT and alignment data creation, quality metrics for data, and model red teaming. Standardized human evaluation for alignment data and led a major part of the data effort. Wrote the data sections of the paper.\n\u2022 Jeril Kuriakose: Worked on MT bench evaluation and maintained all deployment and pretraining data preparation. Supported any project requirements regardless of training, data, or evaluation. Wrote the MT bench evaluation section of the paper.\n\u2022 Ahmed Abdelali: Worked on the Arabic pretraining data. Led the Arabic OCR effort and book corpus acquisition and creation. Worked on Arabic SFT data.\n\u2022 Abdulmohsen Al-Thubaity: Worked on Arabic SFT data creation. Prepared a major part of Arabic SFT data.\n\u2022 Zaki Alawami: Lead the HPC (LLM infrastructure) development and model deployment.\n\u2022 Ali Alammari: Worked on the acquisition of pretraining Arabic data and the creation of the data processing pipeline.\n\u2022 Majed Alrubaian: Supported the entire data, training, and evaluation teams. Worked on Arabic Education SFT data.\n\u2022 Norah A. Alzahrani: Implemented all private Arabic evaluations. Lead the human evaluation and prompt acquisition. Maintained the human evaluation pipeline. Wrote the Arabic human evaluation section and prepared the figures.\n\u2022 Nouf M. Alotaibi: Trained the v0 version of ALLaM on NeMo. Trained all the tokenizers and prepared training data for the v0 and v3 versions. Prepared the initial SFT figures of the paper. Worked on the red-teaming, SFT quality metrics, and generating visualizations for SFT data.\n\u2022 Hisham A. Alyahya: Trained the v0 version of ALLaM on NeMo, identified bugs in NeMo checkpointing and resuming from checkpoints. Worked on model deployment, measured latency and throughput, and evaluated trade-offs between frameworks like TRT-LLM, vLLM, and HF-TGI. Deployed the preference data collection frontend and wrote the data sampling code for v0 models.\n\u2022 Sultan AlRashed: Worked on Megatron-LM checkpointing and maintained the v0 versionof the SFT and RLHF code. Implemented the preference data pipeline in Megatron-LM. Worked on the alignment specialization. Profiled Megatron-Deepspeed.\nNote: In this work we used the v2 version of our data and all the model mentioned in the paper are the v1 version trained on Megatron-LM. Our v0 model series were trained on NeMo."}]}