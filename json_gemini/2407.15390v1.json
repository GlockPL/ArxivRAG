{"title": "ALLAM: Large Language Models for Arabic and English", "authors": ["M Saiful Bari", "Yazeed Alnumay", "Norah A. Alzahrani", "Nouf M. Alotaibi", "Hisham A. Alyahya", "Sultan AlRashed", "Faisal A. Mirza", "Shaykhah Z. Alsubaie", "Hassan A. Alahmed", "Ghadah Alabduljabbar", "Raghad Alkhathran", "Yousef Almushayqih", "Raneem Alnajim", "Salman Alsubaihi", "Maryam Al Mansour", "Majed Alrubaian", "Ali Alammari", "Zaki Alawami", "Abdulmohsen Al-Thubaity", "Ahmed Abdelali", "Jeril Kuriakose", "Abdalghani Abujabal", "Nora Al-Twairesh", "Areeb Alowisheq", "Haidar Khan"], "abstract": "We present ALLAM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLAM is carefully trained considering the values of language alignment and knowledge transfer at scale. Our autoregressive decoder-only architecture models demonstrate how second-language acquisition via vocabulary expansion and pretraining on a mixture of Arabic and English text can steer a model towards a new language (Arabic) without any catastrophic forgetting in the original language (English). Furthermore, we highlight the effectiveness of using parallel/translated data to aid the process of knowledge alignment between languages. Finally, we show that extensive alignment with human preferences can significantly enhance the performance of a language model compared to models of a larger scale with lower quality alignment. ALLaM achieves state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from their base aligned models.", "sections": [{"title": "1 Introduction", "content": "Language modeling has significantly progressed from its humble origins, transitioning from fundamental probabilistic methods to complex neural priors. The foundational work by Shannon (1951) on the information theory of language laid the groundwork for predicting the next word in a sequence, which was subsequently tackled by Bengio et al. (2003) in neural networks. The field experienced a substantial leap with the introduction of LSTMS (Hochreiter & Schmidhuber, 1997) in language models (LM) (Peters et al., 2018b), which could capture longer dependencies in LMs but proved difficult to scale. The emergence of scalable and distributed architectures like Transformers (Vaswani et al., 2017) and the potential for precisely (Kaplan et al., 2020; Hoffmann et al., 2022) compressing web-scale data has resonated in recent years with the advancements of Generative Pretraining (Radford et al., 2018; Brown et al., 2020a; Anil et al., 2023).\nWith the release of ChatGPT (OpenAI, 2022), followed by the introduction of more frontier class models Gemini (Google, 2024), Claude (Anthropic, 2022), Reka (Ormazabal et al., 2024), Mistral (Mistral, 2024), Llama-3 (Meta, 2024) and recently released Qwen-2 (Yang et al., 2024), large language models have demonstrated significant leaps over each generation of models (Laskar et al., 2023). This exponential growth in performance has raised hope in the possibility of achieving Artificial General Intelligence (Hendrycks & Mazeika, 2022; Marcus, 2022). This rapid advancement has spurred discussions across various fields, including ethics, economics, and technology (Weidinger et al., 2021). Judging from the initial capabilities (Bubeck et al., 2023), the potential of these frontier models are reinventing the way humans interact with machines, impacting social norms, productivity, trends, and culture on a broader scale (Zhou et al., 2024). However, most of these frontier-class models are primarily trained on English and often lack a connection to localized regional cultures and norms (Naous et al., 2024). This gap has the potential to result in slow and irreversible manipulation of regional identities and lead to cultural homogenization.\nThe natural course to reverse this trend is to invest resources in curating data and building models to support the diversity of languages and cultures represented in the modern world. While this is possible, the significant training costs of LLMs and their environmental impact have become major concerns in recent years (Strubell et al., 2019). The vast computational resources required to train LLMs contribute to substantial carbon emissions (Luccioni & Hernandez-Garcia, 2023). Governments 3 and non/for-profit organizations (Dodge et al., 2022; Google, 2021; Amazon, 2021), are increasingly aware of these issues. This awareness has led to discussions about the ethical implications of AI development and the need for sustainable practices concerning \u201cWhen and how to scale the training of these models\". In addition, curating data for each language/region at pretraining scale is also an extremely difficult task, since most available data comes from a few high-resource languages.\nTo address these concerns, we consider the problem of adapting strong, but potentially under-trained, open pretrained models, rather than starting from a randomly initialized model. Technically, this involves continuing training of a model in a new language to facilitate Second Language Acquisition (SLA) (Swain & Lapkin, 1995), popularized by Bari et al. (2020) in NLP and recently adapted to LLMs by Nguyen et al. (2023). This process involves the challenging task of incorporating an additional language distribution without compromising the source language(s). For instance, if a pretrained model was initially trained in English, expanding to an additional language presents challenges related to tokenization."}, {"title": "2 Pretraining", "content": "Pretraining language models on trillions of natural language tokens represents the bulk of the cost required to build an effective language model. This large investment of time and compute precludes experimentation or ablation for every decision. Thus, before starting to train ALLaM from random initialization, or \u201cscratch\u201d, we experiment in the continue-pretraining regime. As the name implies, continue pretraining is the practice of warm-starting a pretraining experiment from an already pretrained LM.\nWe begin by discussing our entire pretraining corpus, describe experiments conducted with continue-pretraining, and finally describe pretraining from scratch."}, {"title": "2.1 Pretraining Data", "content": "For English, many high quality and large scale datasets are available for pretraining (Computer, 2023; Soldaini et al., 2024; Gao et al., 2021; Penedo et al., 2023). We harnessed subsets from Dolma-v1 (Soldaini et al., 2024) and Pile (Gao et al., 2021) datasets e.g., Dolma-CC, The Stack (Kocetkov et al., 2022), PeS2o, PubMed, DM-Math (Saxton et al., 2019) and StackExchange (Soboleva et al., 2023). In total, we had access to 4T high to medium quality English tokens for pretraining.\nPretraining data in the Arabic language is much more limited, thus we undertook large scale collection and curation of Arabic language data. This includes in-house crawled sources covering Web documents, news articles, books (literature, religion, law and culture, among others), Wikipedia (over 1M articles), and audio transcripts (books and news)4. To ensure high quality Web data, we applied the following processing steps: (i) drop documents with language identification score below 95%, (ii) drop short documents that are less than 30 words, (iii) drop documents with duplicate URLs or high ratio of spam and stop words, and (iv) drop duplicate documents using exact matching. We experimented with fuzzy matching but opted against using it as it was too restrictive given the scarcity of Arabic data.\nAdditionally, we extended our Arabic data with translated English content using an in-house machine translation system. We translated the following English datasets from Dolma: Wikipedia, books, C4 and peS2o, which also are part of our English data. The hypothesis is that this will improve Arabic-English language alignment, leading to a better Arabic model."}, {"title": "Data Mixture", "content": "To build a performant model in both Arabic and English, we conducted experiments to determine the optimal language mix."}, {"title": "2.2 Continued Pretraining", "content": "Open source and open weight models present an attractive option to conduct pretraining experiments cheaply. However, they also present challenges, since most such models do not natively support Arabic or other languages. We develop a simple approach to enhance any language model with capabilities in new languages (i.e., language expansion). The approach relies on two steps: (i) tokenizer augmentation, and (ii) expanded vocabulary learning. We demonstrate that this approach leads to minimal degradation of capabilities in the original language.\nTokenizer Augmentation Existing open weight language models (e.g., Llama-2) tokenize Arabic (and other languages) poorly, often splitting words down to the character level or even relying on byte-fallback mechanisms for tokenization. This results in: (i) inefficient training, as the pretraining corpus size is inflated, (ii) unoptimized inference, since the model must generate more tokens per word, and (iii) the effective context length is reduced, because it is based on a fixed number of tokens. To address these issues, we use a corpus of text in the target language to train a tokenizer specialized in that language. We then merge the original tokenizer with the language-specific tokenizer. Merging is accomplished by adding all tokens from the language-specific tokenizer that do not exist in the original tokenizer. As shown in Figure 3, this effectively reduces the fertility rate in the target language of the merged tokenizer to the level of the language-specific tokenizer.\nExpanded Vocabulary Learning Newly added tokens in the merged tokenizer have no associated embedding representations in the pretrained language model's weights. To learn these representations, we experiment with two approaches: (i) random initialization and (ii) initialization from combined representations of tokens in the original tokenizer. Approach (ii) is accomplished by tokenizing each token in the vocabulary of the new tokenizer using the original tokenizer. The associated embedding representations of this tokenization are then averaged and assigned as the vector representation of the new token. Since we work with tokenizers with byte-fallback, such a tokenization is guaranteed to exist."}, {"title": "Experiment Details", "content": "Starting from Llama-2 pretrained model weights, we continue pretraining the ALLAM-7B and ALLAM-13B models on 1.2T tokens, covering both English and Arabic languages. For the ALLAM-70B model, we only train up to 600B tokens (using the same data mixture). In all of our continued pretraining experiments, we used the final learning rate of the pretrained language model (usually 3 \u00d7 10\u22125). We experimented with approaches to gradually increase and decay the learning rate with limited success, as such models typically exhibited catastrophic forgetting, indicated by significant drops in performance in the original language. We also considered optimizer state warm up, as open-weight models typically do not include the optimizer states, but found this had little effect on performance. Figure 7 provides an overview of adding dropout during continued pretraining. We observe that adding dropout helps the Arabic language, as it acts as a regularizer for the new distribution. However, Llama-2 was pretrained on 2T tokens without any dropout, and adding dropout negatively impacts the source language performance. Considering this trade-off, we decided not to add dropout in the continued pretraining stage. Unlike recent trends (AI2, 2024), we did not add any alignment data in this stage of training."}, {"title": "2.3 Pretraining From Scratch", "content": "Following (Hoffmann et al., 2022; Touvron et al., 2023a), training a high-quality model from scratch requires a substantial amount of tokens. Even when pretraining from random initialization, we find it beneficial to train with a high-resource language for trillions of tokens (English) and then continue training with a mixture of Arabic and English tokens. On small scale experiments (with 1B parameter models) we find that beginning training with two languages can sometimes degrade the performance in English or result in slow learning of both language distributions. From this, we hypothesize that low-resource languages are diluted in the large volume of high-resource language data when pretraining from scratch, even with upsampling and careful tuning."}, {"title": "Training Recipe", "content": "Our pretraining from scratch recipe consists of two steps: training on 4T English tokens followed by training on 1.2T mixed Arabic/English tokens. This retains the English capabilities of the model without catastrophic forgetting, effectively transferring knowledge from one language distribution to another. The only difference between pretraining from scratch and continued pretraining from an existing model is that vocabulary expansion is not required."}, {"title": "English Data Mixture", "content": "The last column of Table 1 shows the domain mix for the English only pretraining dataset. As expected, web data represent the bulk of the mixture, followed by code and scientific articles."}, {"title": "2.4 Compute and Training Infrastructure", "content": "Over the course of our development of ALLAM, we had access to 128-1024 A100 GPUs. Our GPU cluster was equipped with InfiniBand connections to enable high-speed communication between nodes. The all-reduce test on the cluster ranges around 1200-1400 Gbps (node-node interconnect (ROCE)). The entire training period of the models is estimated to be 5M GPU hours.\nAt the start of the project, we forked Megatron-LM and applied our own customizations (including improving data iterators, adding metadata in the checkpoints, and custom data pipelines). We utilized data, tensor, and pipeline parallelism supported by Megatron-LM to efficiently train at a large scale as well as FlashAttention (Dao et al., 2022; Dao, 2024). By leveraging these techniques, we achieved significant improvements in training speed. The throughput per GPU varied from 135 to 167 TFlop/s/GPU depending on the number of GPUs, number of nodes, batch size, and parallelism strategy. We trained ALLaM with bf 16 mixed-precision."}, {"title": "3 Alignment", "content": "Building useful LLMs requires ensuring they are able to follow instructions while adhering to ethical standards and user expectations. This alignment process is especially crucial for models used in diverse linguistic and cultural contexts. In our setting, this means aligning models to the Arabic language and cultural context while also supporting English.\nSupervised Fine-Tuning (SFT) (Section 3.1) refines a pretrained model using a carefully selected dataset relevant to specific tasks and domains. Preference training (Section 3.2), on the other hand, aligns the model's outputs with human values and preferences by prioritizing responses that meet user expectations and ethical guidelines. These methods work together to create reliable and ethically sound LLMs for real-world use."}, {"title": "3.1 Supervised Fine-Tuning", "content": "Data Our SFT data is curated from a diverse array of sources. Given a piece of context from a source, we utilize humans and/or generative models (Ding et al., 2023) to identify if the text can be considered suitable for supervised fine-tuning or if we can generate instructions to create an SFT example from the context. For English, we primarily use public web content as our main source, offering a broad range of high-quality and especially diverse prompts. In contrast, our Arabic data comes from a combination of public and proprietary sources to ensure comprehensive coverage and relevance. To gather data from the source, we collect seed websites or data sources, which involves utilizing domain experts, prompt librarians, local institutes specializing in areas such as Arabic language, history, and politics, the use of commercially permissible licensed LLMs to generate data, and machine translation models to convert rich English SFT data into Arabic. Our datasets cover various domains and capabilities, ensuring the model's proficiency in handling tasks across education, history, Arabic linguistics, politics, religion, computer science, and other fields. The entire Arabic/English collection is called Ultra-Instinct, which is not human generated, but rather, human driven.\nQuality Filtering Unlike Zhou et al. (2023); AI et al. (2024), we hypothesized that scaling SFT data can unlock diverse capability, as well as improve responsiveness to the prompts. Initially, we crawled the public web for SFT samples. The first version (v1) of Ultra-Instinct includes 12M samples evenly split between English and Arabic, while the second version (v2), is a reduced version with half the samples. Compared to v1, v2 underwent strict quality checks and human assessments of random subsamples. Our quality checks for v2 included (i) assessments based on instruction/response word length, (ii) lexical and semantic diversity, exact and near-exact lexical deduplication, (iii) removal of low quality machine-translated Arabic data from English sources, and (iv) ensuring diversity in questions and commands. For detailed metrics on instruction and response lengths and lexical diversity, see Table 2."}, {"title": "Training Details", "content": "We fine-tune our base model, which was trained on 3.2 trillion (2T Llama-2 + 1.2T ALLAM) tokens, for 3 epochs using Ultra-Instinct-v2 with a learning rate of 5 \u00d7 10\u22126 and a batch size of 1024. The model is not trained to generate the prompt, as we mask out our prompt tokens when calculating the loss. Ultra-Instinct-v2 contains a substantial number of multi-turn conversations. To train on these multi-turn conversations, we performed turn-augmentation."}, {"title": "3.2 Preference Training", "content": "After SFT, models are able to converse in multi-turn conversations. However, they are not fully aligned with human preferences. For example, our SFT models were terse and had limited guardrails."}, {"title": "Data", "content": "The inputs were sourced from early model testers and a manually curated selection of prompts from various domains or attack vectors. These include ethical dilemmas, middle eastern culture, religions, illegal activities, human rights, locale awareness, and personality.\nPreference training necessitates both negative and positive outputs for each input. We relied on the testers' feedback to identify the positive outputs. In the absence of positive outputs, we used a model to generate an output and manually verified that the output was aligned. While (Tunstall et al., 2023) utilized preference data from AI Feedback (AIF) at scale, we adopt a more cautious approach in creating preference data. We generate a smaller volume of data, ensuring it is fully reviewed, edited, and verified by humans.\nThere are two approaches for generating negative outputs: (i) on-policy: use the generations of the model we are tuning as negative outputs, and (ii) off-policy: use another similar model to generate the negative outputs. We did not verify that the negative outputs were worse than the positive. However, we ensured that the positive outputs were of the highest quality, such that they were almost always better than the negative outputs.\nKhan et al. (2023) demonstrated that model outputs can vary significantly depending on the sampling mechanism used. Building on this insight, we generate additional samples for each instance by varying temperature and nucleus sampling techniques. These additional samples are utilized to produce rejected samples, ensuring that ALLaM provides more grounded responses and generalizes well across various sampling mechanisms.\nIn total, we collected 25,854 samples (triplets of {prompt, accepted, rejected}) in English and Arabic language. Using the technique mentioned above, we sample 10 different response from the model to generate additional rejected responses for each sample. This results in a dataset of 245K samples (after filtering) for preference training."}, {"title": "Training", "content": "For DPO, we used a batch size of 512 with KLpenalty = 0.1 and a learning rate of 9 \u00d7 10-7 decayed to 5 \u00d7 10\u20137 using a cosine annealing learning rate schedule. We train ALLaM for a single epoch using all the preference data.\nFrom our initial experiments with small datasets, we observed issues with model quality even when a small fraction (0.1%) of the data was noisy. In this context, noise can be improper labeling of positive/negative pairs or low quality positive outputs. It is not clear, however, if after scaling up the DPO data whether the model can ignore this type of noise. In early DPO models, trained on data where we did not verify all the samples, we found that even a few moderately noisy samples resulted in broken models that repeatedly generate the same text or output incoherent text."}, {"title": "DPO vs. PPO", "content": "One of the fundamental differences between DPO and PPO is that PPO is always on-policy with an external reward model. In our experience with DPO, we did not encounter any significant issues with off-policy experiments. Additionally, DPO allows for faster iteration and easier understanding of the training dynamics. The decision to use DPO over PPO was based on logistical constraints rather than a performance comparison of the algorithms. Given our compute setup and time constraints, we chose to proceed with DPO. We plan to explore PPO in the future for alignment."}, {"title": "4 Evaluation", "content": "In this section, we describe the evaluation of our model and report the results of ALLAM 7B, 13B, and 70B models, as well as other relevant models, such as GPT-4, Command-R+ (Gomez, 2024), and Jais-30B (Sengupta et al., 2023). Our evaluations encompass three main types: (i) automatic evaluations, (ii) LLM-based evaluations, and (iii) human evaluations.\nLimitations Recently, (Alzahrani et al., 2024) showed that multiple choice or cloze test based evaluation may not be robust. In addition, MT-Bench uses an LLM as a judge, and has likely leaked into training datasets. Unfortunately, human evaluation is time-consuming and requires well-trained human evaluators. In this work, we try to ensure robust evaluation and attain a balanced assessment of the quantitative metrics and qualitative effectiveness of models in various applications and domains."}, {"title": "4.1 Automatic Evaluations", "content": "The automatic evaluations cover Arabic and English benchmarks grouped into the categories listed below:\n1. Multi-domain: MixEval (Ni et al., 2024), MMLU-Pro (Wang et al., 2024), and BBH (Suzgun et al., 2022).\n2. Reasoning and Commonsense: HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2019), and AraSwag (Nagoudi et al., 2022).\n3. World Knowledge and Language Understanding: MMLU (Hendrycks et al., 2020), ARC Easy and Challenge (Clark et al., 2018), TriviaQA (Joshi et al., 2017), BoolQ (Clark et al., 2019), NQ Open (Kwiatkowski et al., 2019), AGIEval (Zhong et al., 2023), Exams-Ar (Hardalov et al., 2020), MMLU Arabic (tr) (Huang et al., 2023), MMLU Arabic (MBZU) (Koto et al., 2024), and ETEC (in-house curated).\n4. Safety and Alignment: Hendrycks Ethics (Hendrycks et al., 2021a), ACVA (Huang et al., 2023), TruthfulQA (Lin et al., 2022), and AraTruthfulQA (in-house curated).\n5. Conversation: MT-Bench (Zheng et al., 2024), and Arabic domain capability dataset (in-house curated).\n6. Math: Minerva MATH (Lewkowycz et al., 2022; Hendrycks et al., 2021b), GSM8K (Cobbe et al., 2021) and AraMath (in-house curated).\n7. Coding: HumanEval (THUDM, 2022)\nThe following benchmarks were curated and developed in-house:\n\u2022 ETEC: a collection of 1891 multiple choice questions covering different exams performed by the Education and Training Evaluation Commission in Saudi Arabia.\n\u2022 AraMath: a set of 600 test samples that were post-processed and prepared from the AraMath (Alghamdi et al., 2022) dataset. These samples focus on testing the models' performance on Arabic math problems.\n\u2022 AraTruthfulQA: a dataset created using similar methodology to the TruthfulQA (Lin et al., 2021) dataset. It comprises a total of 541 samples, 285 samples were translated directly from TruthfulQA using GPT-4 and carefully validated and localized by human verifiers. Additionally, 256 questions were curated by humans to ensure their contextual relevance and cultural appropriateness.\nWhile serving as a good test bench, observing the dynamics of automatic evaluations during training is also interesting. Figure 10 shows the behavior of selected benchmarks during mixed Arabic/English pretraining while scaling up model size. In particular, we observe that smaller models tradeoff between capability in the new and original languages. However, larger models can simultaneously improve in both languages."}, {"title": "4.2 LLM-Based Evaluations", "content": "MT-Bench (Zheng et al., 2024) consists of 80 multi-turn questions to evaluate models' capabilities on complex instruction-following. In addition to the English version, MT-Bench Arabic was created using GPT-4 to translate the original dataset and human annotators to review and align the prompts to Arabic culture. GPT-4 serves as the LLM judge, scoring responses as recommended in (Zheng et al., 2024). Model performance is compared turn by turn, with results shown in Table 6, where ALLAM-70B achieves the best Arabic performance."}, {"title": "4.3 Human Evaluation", "content": "We developed an Arabic multi-turn dataset that covers seven domains: Arabic linguistics, history, health, politics, coding, entertainment, and ethics, each domain contains ten questions with two turns.\nHuman evaluators compared the responses from two models and were asked to choose the winning response with the following instructions:\n\u2022 Choose a response as the winner if it is the best, tie if both responses are equally good, and both-bad if both responses are not good.\n\u2022 A response is considered good if it is coherent, grammatically correct, and is a reasonable response to the question or previous turn in the conversation.\n\u2022 Good responses should be in the correct language (the response should be in the same language as the previous turn, unless another language was requested).\n\u2022 Good responses should not contain toxicity, hate speech, or bias.\nEach pair of responses was inspected by three evaluators, and the winner was determined by majority voting. In case of a tie, a fourth evaluator was used to break the tie. Figure 11 presents the human evaluation results of the pair-wise comparisons of these models: ALLAM-13b, Jais-30b-chat-v3, Command-R-plus, and Command-R-v01. ALLaM-13b's win rate was always higher than its loss rate compared with other models.\nFinally, we gather votes from the human evaluators and calculated ELO scores for each model. ELO scoring had two configurations, the default scoring rewards the model for good responses with 1 point, tied responses (good and both-bad) with 0.5 points, and penalizes for bad responses with 0 point. The custom configuration penalizes the model with the bad response and both models if both provided bad responses with 0 point. Figure 12 shows the ELO scores based on the human evaluations. From the figure, GPT-4 achieved the highest score, followed by ALLaM-13b with the second-highest score, outperforming (or matching) larger models such as CommandR+."}, {"title": "5 Related Work", "content": "Our work sits at the cross-section of research building language models that support multiple languages and scaling such techniques in terms of size and data. To successfully train a large language model in a language other than English requires a complete understanding of cross-lingual transferability between languages and a good understanding of scaling laws as well as the fundamentals of training large language models. In this section, we discuss work on language modeling from the perspectives of cross-lingual alignment, multitask learning, and Arabic specialization."}, {"title": "5.1 Language Modeling and Cross-lingual Representations", "content": "In early work, word representations were derived using basic forms of the skip-gram model (Mikolov et al., 2013), wherein each word is assigned a representation that does not account for varying contexts (Grave et al., 2018; Pennington et al., 2014). Further work in this area developed word representations that are adaptive to the context surrounding the words (McCann et al., 2017; Peters et al., 2018a; Howard & Ruder, 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019).\nPeters et al. (2018a) introduced ELMo, a model built with a bidirectional LSTM-based language model (LM) for pretraining to obtain contextualized word representations. This technique combines the outputs from all layers linearly when targeting specific tasks. Nonetheless, the sequential nature of LSTM-based LM pretraining presents challenges in scaling training efficiently. Concurrently, Vaswani et al. (2017) developed the Transformer architecture, which leverages multi-headed self-attention and positional encoding to handle long-range dependencies and enable parallel processing."}, {"title": "5.2 Multitask Learning And Alignment", "content": "Early work has demonstrated that multitask learning can enhance the performance of NLP models (Collobert & Weston, 2008). In explicit multitask learning, augmenting all samples during training may introduce noise due to differing output distributions in a traditional full-model fine-tuning setup (Weller et al., 2022; Bari et al., 2021). For implicit multitask learning, Radford et al. (2019) showed that a language model can begin to learn downstream tasks without explicit supervision by pretraining alone. Large language models (Brown et al., 2020b; Smith et al., 2022; Chowdhery et al., 2022) at scale can perform few-shot in-context learning, making them effective multitask models. Additionally, Sanh et al. (2021); Wei et al. (2021); Muennighoff et al. (2022); Chung et al. (2022) found that these implicitly learned language models could be further improved by explicitly fine-tuning them with human instructions and prompts (Bach et al., 2022; Wang et al., 2022) in a multitask fashion. Unlike previous template-based prompting approaches, Ouyang et al. (2022) applied preference tuning with reinforcement learning (Stiennon et al., 2020) using naturally written prompts. Subsequently, Bai et al. (2022) introduced Constitutional AI to automate alignment using AI feedback. Recently, following the work of Rafailov et al. (2023), various efforts (Azar et al., 2023; Ethayarajh et al., 2024; Hong et al., 2024; Park et al., 2024; Meng et al., 2024) have been directed towards preference tuning without explicit reward models."}, {"title": "5.3 Language Models for Arabic", "content": "As of the time of writing, the most prominent Arabic-focused LLMs are:\n1. Jais (Sengupta et al., 2023): 13B and 30B base and chat models trained from scratch using a combination of natural and translated Arabic data along with English and code data.\n2. AceGPT (Huang et al., 2023): 7B and 13B base and chat models trained from Llama-2 without vocabulary expansion.\nWhile Jais and AceGPT are currently the most prominent models, early open models such as AraGPT (Antoun et al., 2020), AraT5 (Elmadany et al., 2022), AraBART (Eddine et al., 2022), and Noon (Lakim et al., 2022) 11 pioneered the area with models developed with limited resources to serve Arabic.\nOther models such as Jasmine (Abdul-Mageed et al., 2023) and Aramus (Alghamdi et al., 2023) also showed the need for building a language model for over 400 million speakers worldwide.\nIn addition to the language adaptation of models and multilingual models reviewed above, recent work has focused on building multilingual/bilingual language models from open weight language models. For example, Ruci\u0144ski (2024) adapted Mistral 7B for the Polish without vocabulary expansion. Mala-500 is another effort to expand to 534 languages by expanding the vocabulary to 260K tokens and further pretrained Llama-2 using LoRA adaptors (Lin et al., 2024). Due to the number of languages they aimed to support, a small amount of data was included for each language and the evaluation of the approach was limited to measuring perplexity and automatic classification benchmarks. (Cui et al., 2023) introduced a Chinese Language adaptation of Llama and Alpaca models, where the vocabulary was increased to 50K tokens, then continued to pretrain the models and finally fine-tune them."}, {"title": "6 Conclusion", "content": "The ALLAM model series marks a significant advancement in Arabic Language Technologies, achieving state-of-the-art performance across various Arabic benchmarks while maintaining or enhancing English performance. Through careful training that emphasizes language alignment and transferability, our models demonstrate effective second-language acquisition without catastrophic forgetting. The strategic use of translated data, knowledge encoding, and alignment with human preferences have been crucial in this success."}, {"title": "7 Limitations", "content": "ALLAM was trained on data that may potentially include toxic language, unsafe content, and societal biases originally sourced from the internet, leading to the possible amplification of these biases and toxic responses. Although ALLaM underwent comprehensive safety training during the alignment phase, more community feedback is needed to iteratively improve ALLAM. Additionally, inherent uncertainties in generative models mean that trials cannot encompass every possible use case, making it impossible to predict the model's responses in all contexts. This can occasionally result in inaccurate, biased, or socially unacceptable outputs, even if the prompt itself is not explicitly offensive. Developers must conduct thorough safety evaluations and make specific adjustments to ensure that ALLAM is suitable for their intended purposes. Furthermore, the output generated by ALLAM should not be considered a statement from ALLAM's creators or any affiliated organization."}, {"title": "8 Ethical Statement"}]}