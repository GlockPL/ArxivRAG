{"title": "Waterfall: Framework for Robust and Scalable Text Watermarking", "authors": ["Gregory Kang Ruey Lau", "Xinyuan Niu", "Hieu Dao", "Jiangwei Chen", "Chuan-Sheng Foo", "Bryan Kian Hsiang Low"], "abstract": "Protecting intellectual property (IP) of text such as articles and code is increasingly important, especially as sophisticated attacks become possible, such as paraphrasing by large language models (LLMs) or even unauthorized training of LLMs on copyrighted text to infringe such IP. However, existing text watermarking methods are not robust enough against such attacks nor scalable to millions of users for practical implementation. In this paper, we propose WATERFALL, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. WATERFALL comprises several key innovations, such as being the first to use LLM as paraphrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability and scalability. We empirically demonstrate that WATERFALL achieves significantly better scalability, robust verifiability, and computational efficiency compared to SOTA article-text watermarking methods, and also showed how it could be directly applied to the watermarking of code.", "sections": [{"title": "1 Introduction", "content": "Achieving robust text data provenance via watermarking, independent of its digital format, is an important open problem impacting a wide-ranging set of real-world challenges. Among these is the issue of intellectual property (IP) enforcement: Content creators of any text format (e.g., articles or code) could potentially combat plagiarism and unauthorized distribution by watermarking their works to prove data ownership. However, existing text watermarking methods have been unable to meet the challenging requirements of many practical problem settings. For example, directly adding digital metadata or invisible Unicode watermarks (Rizzo et al., 2019; Taleby Ahvanooey et al., 2019) may have limited impact in proving text data ownership in adversarial settings as they may be easily removed. Existing natural language watermarking (Qiang et al., 2023; Yoo et al., 2023; Taleby Ahvanooey et al., 2019) that adjusts the text itself to encode IDs are also lack robustness to paraphrasing attacks and have limited scalability in terms of the number of supportable IDs.\nAdding to the challenge is the growing prevalence of generative large language models (LLMs) that may be trained on copyrighted text without permission. To enforce IP rights, content creators would need to be able to do data provenance for LLMs, i.e., prove whether their set of work had been used to train 3rd party black-box LLMs. While there have been recent works tackling this problem (Abdelnabi and Fritz, 2021; Zhang et al., 2023), they largely require intervening in the training process of the LLMs. This is unrealistic in practice, as not all LLM service providers may be cooperative due to incentive misalignment, and adversaries may also use open-source LLMs.\nHence, it is natural to ask whether it is possible to develop a practical, robust and scalable text watermarking framework for protecting IP against both plagiarism and unauthorized training of LLMs. For example, the watermarks should persist regardless of whether the text has been paraphrased, converted into speech or handwritten text, or used in unauthorized LLM training (e.g., fine-tuning, in-context learning) to produce a derived output. The framework should also be general enough to tailor to a wide range of text formats (e.g., natural language or code), and be scalable (i.e., support millions of users with reasonable computational cost).\nIn this paper, we propose WATERFALL, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. Rather than viewing LLMs as just sources of IP infringement, we introduce the novel perspective of using LLMs' capabilities to protect existing IP. Though simple, our training-free framework comprises several key innovations such as being the first to use LLM as paraphrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability, scalability, and data provenance for LLMs, surpassing state-of-the-art text watermarking methods. In summary, our contributions are as follows:\n1. We introduced a formulation of the robust and scalable text watermarking problem setting and lay out a set of desiderata to be satisfied (Section 2)."}, {"title": "2 Problem formulation and Desiderata", "content": "Consider M clients, each with unique watermark ID \u03bc\u0395 \u039cand textual data To \u2208 \u03a4 (e.g., articles or code) represented as token sequences To = [w1,..., wn], where each token wi is from an ordered vocab space V = {1, ..., U|v|}. We assume that To has semantic content c (e.g., the IP content) that is only determined by its tokens and fully represents the text's value. Text formatting is irrelevant, especially as adversaries can strip all formatting, making those channels unusable for watermarking\u00b9.\nWatermarking: Client i uses a watermarking operator W(\u03bc\u03af, \u03a4\u03bf) \u2192 T(w) to produce a text T(w) that contains watermark \u03bci, preserves c, and can then be used/distributed freely.\nAttacks: There are adversaries who aim to claim the IP in To through attacks A(T(w))\u2192T(sus) that generate their own text T(sus) without the watermark \u03bci while preserving semantic content c. Adversaries do not know \u03bci but are able to perform several classes of attacks:\nA1: alter T(w) with word addition/removal/substitutions;\nA2: translate and paraphrase T(w) with an LLM;\nA3: watermark T(w) again with a different watermark;\nA4: using T(w) with any LLM for in-context prompting;\nA5: using T(w) to fine-tune any LLM.\nVerification: Client i can use a verification operator V(\u03bc\u03af, T(sus)) to generate a score q indicating the likelihood that T(sus) contains \u03bci. They can then use a setting-specific threshold q to classify T(sus) as watermarked with \u03bci if q \u2265 q. The operator V should be quick and not assume access to To, as in practice client i may have a large set of T(w) and would need to automate the application of V to scan through a large set of {T(sus)} to identify any plagiarism (further discussion in Appendix K).\nGiven the above, a suitable watermarking framework should satisfy the following desiderata:\n1. Fidelity. The watermarked text Tw should be semantically similar to To, i.e., S(To,Tw) \u2265 s, where S:T\u00d7T \u2192 [0,1] is a user-defined fidelity metric depending on the purpose and type of text (e.g., semantic similarity score for articles, or unit tests for code) and s is a setting-specific threshold. We define TWc,s = {T\u2208T : S(To,T) > s} as the support set of all Tw that a watermarking operator W can possibly generate for To with content c under a s-fidelity setting.\n2. Verifiability. The verification operator V(\u03bc\u03af, T(sus)) should have high efficacy, accounting for Type I and II errors over various thresholds q. We evaluate this with AUROC computed over a test set.\nNote that there is a trade-off between fidelity and verifiability. Applying a stronger, more verifiable watermark tends to reduce text fidelity and the optimal setting depends on each use case. We can evaluate a watermarking scheme in general, while taking into account this trade-off, using its fidelity-verifiability Pareto frontier (e.g., as plotted in Figure 4a).\n3. Robust verifiability. The verification operator on watermarked text after attacks A \u2208 A, i.e., V(\u03bc\u03af, A(T(w))), retains high verifiability. This means that the watermark should remain verifiable even after attacks, which constrains framework design. For example, the verification operator should not extract \u03bc in any subroutine, as an attacker may use it to get \u03bc and devise an A3 attack to overwrite it (see Section 4.1).\n4. Scalability. The framework should support a large M (set of IDs) while meeting all other desiderata."}, {"title": "3 Method", "content": "We discuss three key insights to tackle challenges arising from these desiderata, before combining these to present our framework WATERFALL (Watermarking Framework Applying Large Language Models).\n3.1 Increasing support set for watermarking via LLMS\nFirst, note that the fidelity desideratum is a major constraint to a scheme's ability to meet the other desiderata. Intuitively, a scheme that can only generate a small set TWc,s of possible watermarked text would have fewer ways to encode the watermark, leading to lower signal capacity (smaller |M|, lower scalability), and less capacity for error correction to withstand attacks (lower robust verifiability).\nFor illustration, consider a basic semantic watermarking scheme (BASIC) that lists out synonyms for each word in the original text To (e.g., big cat) and remembers a map of IDs to possible combinations of these synonyms (e.g., 01:big feline, 10:large cat, 11:large feline). Watermarking for ID \u03bc is then selecting the text Tw with the matching synonym combination. Note that schemes like BASIC typically only have a relatively small support set TWc,s and hence limited watermarking possibilities.\nHowever, LLMs can come up with many more possibilities and access a larger TWc,s compared to schemes like BASIC using mechanical paraphrasing rules (e.g., synonym replacement). Past works have shown that LLMs can effectively paraphrase text given suitable prompts (Shu et al., 2024; Witteveen et al., 2019). For example, while synonym replacement can only generate possibilities involving word replacements, an LLM may be able to completely reorder, break, or fuse sentences while preserving semantic content c. In general, as some expressions are more common, we can associate a probability distribution pc(T) over this set TWc,s.\nIntuitively, we can consider a suitable paraphrasing prompt combined with text To as tokens \u0109 that can constrain an LLM's text generation to TWc,s. Given \u0109, the LLM autoregressively access pc(T) by producing conditional probability distributions p(wj|\u01751:j\u22121, \u0109) for token wj at step j given the preceding sampled tokens \u0175, and sampling for each step until it deemed that it had conveyed c. Specifically, at step j, the LLM generates a vector of logits Lj(\u01751:j\u22121,\u0109) \u2208 R|V|, where\np(wj|\u01751:j\u22121, c) = softmax(Lj(\u01751:j\u22121,c)).\nWe denote LLMs used this way as LLM paraphrasers. By using LLM paraphrasers, we significantly increase TWc,s, which helps us better meet the fidelity, robust verifiability and scalability desiderata.\n3.2 Increasing robustness using n-gram watermarking with LLM deviation correction\nGiven the extensive threat model, most watermarking schemes would face a major challenge in meeting the robust verifiability desideratum. For example, A2 paraphrasing attacks would likely break schemes such as BASIC which depend on word ordering\u00b2, let alone attacks involving further processing by black-box LLMs (e.g., A4, A5 attacks). Instead, we could decompose pc(T) and the watermarked text Tw into multiple signal carriers, and embed the same watermarking signal to all. This way, we adopt a probabilistic approach where each carrier could independently be used to verify a watermark, to withstand attacks that can only corrupt a proportion of carriers.\nSpecifically, we could consider each consecutive n tokens in Tw as an n-gram carrier unit. At each LLM paraphraser token generation step j, we could apply a watermarking operator W (Section 3.3) that perturbs the logits of Equation (1) based on the ID \u03bc and past n \u2212 1 generated tokens: Lj = W[\u03bc, \u0175j-n+1:j\u22121](Lj (\u01751:j\u22121,\u0109)). The perturbed logits will cause a detectable bias in each n-gram, hence the more n-grams that persist after any attack, the higher the verifiability.\nMeanwhile, in future generation steps j', the LLM paraphraser will correct deviations from semantic content c and preserve fidelity given sufficient generation steps, as the subsequent logits Lj' (W1:j'\u22121, \u0109) are still conditioned on paraphrasing prompt \u0109.\nThis approach increases our framework's robustness against not just paraphrasing attacks, but also more general LLM-based attacks (e.g., A5). Past works have shown that language models tend to generate few novel n-grams outside their training set for small n (McCoy et al., 2023). Hence, LLMs trained on text with our watermarked n-grams may more likely generate them in their output. Given sufficient queries to these LLMs, the watermark could then be reliably verified, which we empirically demonstrate in Section 4.\n3.3 Increasing scalability with vocab permutation and orthogonal perturbation\nFinally, we propose a watermarking operator W comprising two components: 1) vocab permutation, and 2) orthogonal perturbation. In this section, we will use a toy example (Vec) to show how these components work before presenting their general form. In Vec, we have logits L = [3,2,1], indexed by an ordered set Vo = {\u03b1, \u03b2, \u03b3} representing the token space, e.g., L(a) = 3. Figure 2a presents L as a graph (V as x-axis).\nVocab permutation. The vocab permutation operator P produces a single permutation of Vo and L for any given key k (arrow \u2460 in Figure 2a). The inverse operator P\u22121 reverses the permutation of P when provided the same key (arrow \u2461 in Figure 2a). As Vo = 3, there are 6 possible permutations of L, plotted as graphs over a new ordered index Vw = {a, b, c}, which we can interpret as the watermarking space. Then, we define the average permutation operator P acting on L (indexed by Vo) as one that takes a sequence of keys \u039a\u03c0, apply P to get Lk for each k \u2208 \u039a\u03c0, and averages them to get a vector L (indexed by Vw). Note that when we use P on L over all possible keys, we get a constant vector (e.g., L = 1/6\u22116i=1 Li = [2, 2, 2], \u2462 in Figure 2a).\nSimilarly, given a vector G indexed by Vw, which we can interpret as the watermark signal, the inverse operator P-1 permutes G and Vw given a key k\u33a2, mapping it to V, the LLM-ordered token space (arrow \u2463 in Figure 2b). P\u22121 acting on G analogously averages over all keys, and will also give a constant vector indexed over Vo (e.g., G = 1/6\u22116i=1 Gi = [0,0,0], \u2465 in Figure 2b).\nThis leads to an interesting insight: the permutation operators provide a way for us to add watermark signals to logits in a deterministically shifting Vw space (based on a sequence of keys) to boost verifiability and fidelity. For illustration, assume that an LLM paraphraser produces L (in Vo-space) for all token generation steps. We use a long sequence K of pseudo-random uniformly sampled keys to apply P on L multiple times (n-gram watermarking), and add the same watermarking signal G in each resulting Vw space for all instances. If we apply P-1 with K on the perturbed signal L+G, the distortion from the permuted L will effectively contribute only uniform background noise to G (\u2466 in Figure 2c), which improves verifiability. If we instead convert L+G back to Vo space (for token generation) with P\u22121 for all steps and apply P, we get the original logits with only uniform background noise from watermarking (\u2467 in Figure 2c), which improves fidelity.\nMore generally, we define the vocab permutation operator P and its inverse P-1 as pseudorandom permutations over ordered sets Vo and Vw given a key k\u2208 \u039a\u03c0:\nP(\u03ba\u03c0, Vo) = Vw\nP\u22121(k\u03c0, Vw) = V,\nP\u22121(kn, P(kn, Vo)) = Vo,\nwhere Vk, Vk are uniform-randomly chosen permutations of Vo and Vw if k is sampled randomly. For a function L over Vo mapped to a vector of length |V|, we have L(P(k\u016b, Vo)) = L(V) and we overload notation by defining P(k\u33a2, L(\u00b7)) \u2261 L(P(k\u339e,\u00b7)) = Lk\u339e\u00b7 As in the Vec example, P applied to a function (vector) can be viewed as the same function but with its domain permuted.\nWe then define an average operator P over a sequence of keys K acting on a function L,\nP(K,L) \u2211\u03ba\u03c0\u03b5\u03ba\u03c0 \u03a1(\u03ba\u03c0, L),\nwhich outputs an average function of L over Vw (denoted as L ). P(K\u201e, L) will flatten towards a constant function over Vw for a sufficiently large \u039a\u03c0. \u03a4o achieve this for our framework, we set K\u03c0 = {k | \u039a\u03c0 = H\u03c0(\u03bc, \u0175j-n+1:j\u22121)}j, for all LLM paraphrasing steps j and where he is a hash function, which generates pseudorandom K sequences. Empirically, we clearly observe the flattened and clear watermarking signals (see Figure 7 in Appendix).\nOrthogonal perturbation: Our proposed perturbation operator F involves two sub-operations acting on Vw. It first maps each key kp \u2208 Kp to a unique function in a pre-defined family of orthogonal functions, and then adds the chosen perturbation function to the logits L; of the LLM output in Vw space:\nF\u2081 : Kp \u2192 {\u03c6 : Vw \u2192 R|Vw] | \u3008\u03a6\u03af, \u03c6\u03b9\u3009 = \u03b4\u03b5\u03b9}\nF(kp, k, Lj) = Lj+kF\u2081(kp)\nwhere (,) denotes the canonical dot product over Vw. Examples of orthogonal function families include the Fourier or square wave basis, discretized over V. The key kp = hp(\u00b5,z) \u2208 Kp is a client defined function hp of ID \u00b5, and also any metadata z (which could be extracted after verification as we demonstrate in Section 4.1) if required. is a scalar that controls the perturbation magnitude.\nCombining both components, our watermarking operator (Figure 3, and Algorithm 1 in Appendix) for generation step j involves (a) using k\u2081 = h\u03c0(\u03bc, Wi\u2212n+1:i\u22121) and the permutation operator P(k\u03c0, Lj) to transform logits from the Vo to Vw space, (b) applying the perturbation operator in Equation (5), and (c) transforming the perturbed logits back to Vo space using P\u22121(k\u33a2, .) to produce a probability distribution for sampling and generation of the watermarked text Tw:\nLj = W(k\u016b, kp, Lj) = P\u22121(kn, F(kp, \u03ba, P(kr, Lj))).\nOur verification operator will produce a score by computing the average cumulative token distribution of a text using P(\u039a\u03c0, .) and taking the inner product with F1(kp). Applying the right keys kp and k\u016b on the suspected text Tsus will result in a high score q, else the score will be close to 0 (see Figure 3, and Algorithm 2 in Appendix). Using orthogonal functions helps us improve verifiability by avoiding interference from other watermarks (e.g., added by adversaries as an A3 attack). Notice that the many possible vocab permutations (|V|!) and perturbation functions in any orthogonal function family |F\u2081| allows for a much large set of IDs compared to schemes like BASIC, helping with scalability. For example, up to |F\u2081|\u00b7 |V|! IDs can be assigned to a unique permutation-perturbation function pair for watermarking. Using a relatively small |V| = 32000 and the Fourier basis over that would yield a maximum |M| ~ 10130274. Schemes like BASIC only support M that scales with the number of possible synonym replacements for a given text.\nIn addition, with orthogonal functions, our framework also allows for the embedding of metadata during watermarking. For example, a client can use u to verify that Tsus is watermarked, and also extract information on which article it was plagiarized from (Algorithm 3). We demonstrate this empirically in Section 4.1 using the Fourier basis as perturbation functions and Discrete Fourier Transform (DFT) for extraction.\n3.4 WATERFALL Framework\nOur watermarking framework, WATERFALL, combines these insights into a structured watermarking/verification process. For watermarking (Figure 3 left), given To and \u03bc, WATERFALL uses an LLM paraphraser to autoregressively paraphrase a text To, producing initial logits for the new text Tw [Step \u2460]. The ID \u03bc is used to seed the vocab permutation operator (Equation (2)) for mapping the logits to Vw space, and chooses the perturbation function (Equation (5)) [Step \u2461], both of which will be used in the watermarking operation (Equation (6)) to produce the perturbed logits [Step \u2462]. The LLM samples the perturbed logits to produce a watermarked token, and for the next token loop, the past n - 1 tokens are used to seed vocab permutation while all past tokens are fed as context which helps the LLM paraphraser maintain Tw fidelity despite watermarking [Step \u2463].\nFor verification (Figure 3 right), each token in Tsus is counted in Vw space as specified by \u00b5 and the previous tokens in the same n-gram unit, producing an average cumulative token distribution [Step \u2460]. The ID \u03bc also specifies a specific perturbation function [Step \u2461], which is used to perform an inner product with the cumulative distribution to compute a verification score q [Step \u2462].\nPractical considerations. WATERFALL is highly adaptable, i.e., it can be implemented with different LLM as paraphrasers, allowing our framework to achieve better watermarking performance and support more text types as the LLM landscape evolves. Methods like prompt engineering (Wei et al., 2022; Lin et al., 2023) and Reflexion (Shinn et al., 2023; Madaan et al., 2023) may also help to boost performance in some settings, as we demonstrate in our code watermarking experiments (Appendix G.2). We elaborate further on possible large-scale deployment methods of WATERFALL and other practical considerations in Appendix L."}, {"title": "4 Experiments", "content": "4.1 Data ownership\nFor watermarking of text articles, we demonstrate the effectiveness of WATERFALL with experiments using text samples To from the c4 realnewslike dataset (Raffel et al., 2020), comprising articles with mean token length of 412. The experiments mirror realistic scenarios, for e.g., news outlets watermarking their articles before publishing them to be able to effectively scan the internet for, and verify, plagiarized content (Brewster et al., 2023). For this setting, we evaluate the semantic similarity S using the Semantic Textual Similarity (STS) score based on the all-mpnet-base-v2 model\u00b3 (S for sample text pairs are provided in Appendix E.5)."}, {"title": "5 Related Work", "content": "Early text watermarking techniques (Kamaruddin et al., 2018; Taleby Ahvanooey et al., 2019) primarily depend on structural adjustments (e.g., text formatting, use of different Unicode characters (Rizzo et al., 2019)), image-based techniques (e.g., pixel-adjustments of text), or semantic watermarking (e.g., substituting synonyms like BASIC described in Section 3.1). Recent works have augmented the latter with deep learning and language models for better performance (Qiang et al., 2023; Yoo et al., 2023; Ueoka et al., 2021; Abdelnabi and Fritz, 2021). However, as we showed in our experiments, these schemes are not robust to the range of practical LLM-enabled attacks possible today.\nA recently popular but separate line of work has focused on the different model-centric problem setting of watermarking newly-generated output generated by a single LLM (Kirchenbauer et al., 2023; Venugopal et al., 2011; Christ et al., 2023; Kuditipudi et al., 2023; Zhao et al., 2023), rather than existing text owned by many clients. Hence, these works do not address our problem desiderata such as achieving scalability and robust verifiability while requiring semantic preservation of the original text. Our work focused on data-centric text watermarking of original text is the first to use LLM paraphrasers with a novel combination of techniques that are surprisingly effective in addressing the text data ownership and LLM data provenance settings. For further elaboration on the differences, see Appendix J."}, {"title": "6 Discussion and Conclusion", "content": "We proposed WATERFALL, the first training-free framework for text watermarking that has low computational cost, scalability to large number of clients, and robustness to LLM attacks including unauthorized training of LLMs that generates IP-infringing text.\nThere is currently a lack of actual, practical large-scale deployment of text watermarking effective against LLM attacks, given the current SOTA watermarking methods' limitations and resource requirements. However, WATERFALL may possibly provide a foundation for achieving large-scale deployment, with both decentralized or centralized options. This is made achievable given WATERFALL's low computational cost, scalability to a large number of clients, and robustness to LLM attacks including unauthorized training of LLMs that generates IP-infringing text.\nOur framework highlights a few perspectives that we hope more would consider. First, while increasingly capable LLMs allows for easier and more sophisticated forms of potential IP infringement, LLMs themselves could also enable better text IP protection of original texts. A key strength of WATERFALL is that its capabilities grow as LLMs become more powerful, with increasingly better watermarking performance, allowing it to potentially keep up with the increasing capabilities adversaries can use for IP infringement. It is able to achieve a higher fidelity-verifiability Pareto frontier, and reduce any fidelity degradation while using higher watermarking strength for greater robust verifiability.\nSecond, as open-source LLM models become more prevalent and capable, adversaries could directly use them for IP attacks rather than depend on the services of closed-source LLM providers, allowing them to bypass any IP protection measures that these providers may implement (Piper, 2024). As such, content creators cannot just rely on LLM providers to assist in IP protection, but instead be equipped with methods such as WATERFALL to protect their work before dissemination, such as by injecting robust watermarks that allows verifiability even after both traditional attacks and unauthorized use in LLM training by adversaries.\nThird, a general text watermarking framework like WATERFALL that can apply across different text types and languages not only helps with practical deployment, but also makes it highly versatile and not dependent on any text-specific properties. This makes it easily adaptable for incorporating new defense methods, providing a strong foundation for future works to build on as new threats emerge."}, {"title": "7 Limitations", "content": "AS WATERFALL relies on the adjustment of the original text to add watermarks, it may not applicable to all types of text. For example, WATERFALL faces limitations in its application to works where their IP values lie in their style or format (e.g., poems), unless additional methods are applied that cause LLMs to largely preserve such styles while paraphrasing these works, such as optimizing for better paraphrasing prompts to be used with more capable LLMs, or iteratively refining the text through multiple rounds of watermarking.\nSimilar to other linguistics-based text watermarking methods, WATERFALL would also not be applicable where changes to the text are unacceptable (e.g. lyrics of a country's national anthem), or when applied to very short text (e.g. messages of just a few tokens). Nevertheless, WATERFALL is still useful for a wide range of settings where the IP lies mainly in the content of the text, and presents a step forward for practical deployment of text watermarking. Future work could build on WATERFALL to adapt it to other use cases for data provenance, such as data currency (i.e., ensuring that the data is up-to-date) or data authenticity (i.e., that the data has not been manipulated)."}]}