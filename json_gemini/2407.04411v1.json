{"title": "Waterfall: Framework for Robust and Scalable Text Watermarking", "authors": ["Gregory Kang Ruey Lau", "Xinyuan Niu", "Hieu Dao", "Jiangwei Chen", "Chuan-Sheng Foo", "Bryan Kian Hsiang Low"], "abstract": "Protecting intellectual property (IP) of text such as articles and code is increasingly important, especially as sophisticated attacks become pos-sible, such as paraphrasing by large language models (LLMs) or even unauthorized training of LLMs on copyrighted text to infringe such IP. However, existing text watermarking methods are not robust enough against such attacks nor scalable to millions of users for practical implementation. In this paper, we propose WATERFALL, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. WATERFALL comprises several key innovations, such as being the first to use LLM as para-phrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability and scalability. We empirically demonstrate that WATERFALL achieves significantly better scalability, robust verifiability, and computational efficiency compared to SOTA article-text wa-termarking methods, and also showed how it could be directly applied to the watermarking of code.", "sections": [{"title": "1 Introduction", "content": "Achieving robust text data provenance via watermark-ing, independent of its digital format, is an important open problem impacting a wide-ranging set of real-world challenges. Among these is the issue of intel-lectual property (IP) enforcement: Content creators of any text format (e.g., articles or code) could potentially combat plagiarism and unauthorized distribution by wa-termarking their works to prove data ownership. How-ever, existing text watermarking methods have been unable to meet the challenging requirements of many practical problem settings. For example, directly adding digital metadata or invisible Unicode watermarks (Rizzo et al., 2019; Taleby Ahvanooey et al., 2019) may have limited impact in proving text data ownership in adver-sarial settings as they may be easily removed. Existing natural language watermarking (Qiang et al., 2023; Yoo et al., 2023; Taleby Ahvanooey et al., 2019) that adjusts the text itself to encode IDs are also lack robustness to paraphrasing attacks and have limited scalability in terms of the number of supportable IDs.\nAdding to the challenge is the growing prevalence of generative large language models (LLMs) that may be trained on copyrighted text without permission. To enforce IP rights, content creators would need to be able to do data provenance for LLMs, i.e., prove whether their set of work had been used to train 3rd party black-box LLMs. While there have been recent works tackling this problem (Abdelnabi and Fritz, 2021; Zhang et al., 2023), they largely require intervening in the training process of the LLMs. This is unrealistic in practice, as not all LLM service providers may be cooperative due to incentive misalignment, and adversaries may also use open-source LLMs.\nHence, it is natural to ask whether it is possible to develop a practical, robust and scalable text watermark-ing framework for protecting IP against both plagiarism and unauthorized training of LLMs. For example, the watermarks should persist regardless of whether the text has been paraphrased, converted into speech or hand-written text, or used in unauthorized LLM training (e.g., fine-tuning, in-context learning) to produce a derived output. The framework should also be general enough to tailor to a wide range of text formats (e.g., natural language or code), and be scalable (i.e., support millions of users with reasonable computational cost).\nIn this paper, we propose WATERFALL, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. Rather than viewing LLMs as just sources of IP infringement, we introduce the novel perspective of using LLMs' capabil-ities to protect existing IP. Though simple, our training-free framework comprises several key innovations such as being the first to use LLM as paraphrasers for water-marking along with a novel combination of techniques that are surprisingly effective in achieving robust ver-ifiability, scalability, and data provenance for LLMs, surpassing state-of-the-art text watermarking methods. In summary, our contributions are as follows:\n1. We introduced a formulation of the robust and scal-able text watermarking problem setting and lay out a set of desiderata to be satisfied (Section 2)."}, {"title": "2 Problem formulation and Desiderata", "content": "Consider M clients, each with unique watermark ID \u03bci \u2208 M and textual data To \u2208 T (e.g., articles or code) represented as token sequences To = [w1,..., wN], where each token wi is from an ordered vocab space V = {1, ..., |V|}. We assume that To has semantic content c (e.g., the IP content) that is only determined by its tokens and fully represents the text's value. Text formatting is irrelevant, especially as adversaries can strip all formatting, making those channels unusable for watermarking\u00b9.\nWatermarking: Client i uses a watermarking operator W(\u03bci, To) \u2192 T(\u03bci) to produce a text T(\u03bci) that contains watermark \u03bci, preserves c, and can then be used/dis-tributed freely.\nAttacks: There are adversaries who aim to claim the IP in T(\u03bci) through attacks A(T(\u03bci)) \u2192 Tsus that generate their own text Tsus without the watermark \u03bci while preserving semantic content c. Adversaries do not know \u03bci but are able to perform several classes of attacks:\nA1: alter T(\u03bci) with word addition/removal/substitutions;\nA2: translate and paraphrase T(\u03bci) with an LLM;\nA3: watermark T(\u03bci) again with a different watermark;\nA4: using T(\u03bci) with any LLM for in-context prompting;\nA5: using T(\u03bci) to fine-tune any LLM.\nVerification: Client i can use a verification operator V(\u03bci, Tsus) to generate a score q indicating the likeli-hood that Tsus contains \u03bci. They can then use a setting-specific threshold q\u0304 to classify Tsus as watermarked with \u03bci if q \u2265 q\u0304. The operator V should be quick and not assume access to To, as in practice client i may have a large set of T(\u03bci) and would need to automate the applica-tion of V to scan through a large set of {Tsus} to identify any plagiarism (further discussion in Appendix K).\nGiven the above, a suitable watermarking framework should satisfy the following desiderata:\n1. Fidelity. The watermarked text Tw should be se-mantically similar to To, i.e., S(To,Tw) \u2265 s, where S:T\u00d7T \u2192 [0,1] is a user-defined fidelity met-ric depending on the purpose and type of text (e.g., semantic similarity score for articles, or unit tests for code) and s is a setting-specific threshold. We define TcsW = {T\u2208T : S(To,T) > s} as the support set of all Tw that a watermarking operator W can possibly generate for To with content c under a s-fidelity setting.\n2. Verifiability. The verification operator V(\u03bci, Tsus)) should have high efficacy, accounting for Type I and II errors over various thresholds q\u0304. We evaluate this with AUROC computed over a test set.\nNote that there is a trade-off between fidelity and verifiability. Applying a stronger, more verifiable water-mark tends to reduce text fidelity and the optimal setting depends on each use case. We can evaluate a watermark-ing scheme in general, while taking into account this trade-off, using its fidelity-verifiability Pareto frontier (e.g., as plotted in Figure 4a).\n3. Robust verifiability. The verification operator on wa-termarked text after attacks A \u2208 A, i.e., V(\u03bci, A(T(\u03bci))), retains high verifiability. This means that the watermark should remain verifiable even after attacks, which con-strains framework design. For example, the verification operator should not extract \u03bc in any subroutine, as an attacker may use it to get \u03bc and devise an A3 attack to overwrite it (see Section 4.1).\n4. Scalability. The framework should support a large |M| (set of IDs) while meeting all other desiderata."}, {"title": "3 Method", "content": "We discuss three key insights to tackle challenges arising from these desiderata, before combining these to present our framework WATERFALL (Watermarking Framework Applying Large Language Models).\n3.1 Increasing support set for watermarking via LLMS\nFirst, note that the fidelity desideratum is a major con-straint to a scheme's ability to meet the other desiderata. Intuitively, a scheme that can only generate a small set TcsW of possible watermarked text would have fewer ways to encode the watermark, leading to lower signal capacity (smaller |M|, lower scalability), and less ca-pacity for error correction to withstand attacks (lower robust verifiability).\nFor illustration, consider a basic semantic watermark-ing scheme (BASIC) that lists out synonyms for each word in the original text To (e.g., big cat) and remem-bers a map of IDs to possible combinations of these synonyms (e.g., 01:big feline, 10:large cat, 11:large fe-line). Watermarking for ID \u03bc is then selecting the text Tw with the matching synonym combination. Note that schemes like BASIC typically only have a relatively small support set TcsW and hence limited watermarking possibilities.\nHowever, LLMs can come up with many more pos-sibilities and access a larger TcsW compared to schemes like BASIC using mechanical paraphrasing rules (e.g., synonym replacement). Past works have shown that LLMs can effectively paraphrase text given suitable prompts (Shu et al., 2024; Witteveen et al., 2019). For example, while synonym replacement can only generate possibilities involving word replacements, an LLM may be able to completely reorder, break, or fuse sentences while preserving semantic content c. In general, as some expressions are more common, we can associate a probability distribution pc(T) over this set TcsW.\nIntuitively, we can consider a suitable paraphrasing prompt combined with text To as tokens \u0109 that can con-strain an LLM's text generation to TcsW. Given \u0109, the LLM autoregressively access pc(T) by producing con-ditional probability distributions p(wj|\u01751:j\u22121, \u0109) for to-ken wj at step j given the preceding sampled tokens \u0175, and sampling for each step until it deemed that it had conveyed c. Specifically, at step j, the LLM generates a vector of logits Lj(\u01751:j\u22121,\u0109) \u2208 R|V|, where\n$$p(w_j | \\hat{w}_{1:j-1}, c) = \\text{softmax}(L_j(\\hat{w}_{1:j-1}, c)).$$\nWe denote LLMs used this way as LLM paraphrasers. By using LLM paraphrasers, we significantly increase |TcsW|, which helps us better meet the fidelity, robust verifiability and scalability desiderata.\n3.2 Increasing robustness using n-gram watermarking with LLM deviation correction\nGiven the extensive threat model, most watermarking schemes would face a major challenge in meeting the robust verifiability desideratum. For example, A2 para-phrasing attacks would likely break schemes such as BASIC which depend on word ordering, let alone attacks involving further processing by black-box LLMs (e.g., A4, A5 attacks). Instead, we could decompose pc(T) and the watermarked text Tw into multiple signal carriers, and embed the same watermarking signal to all. This way, we adopt a probabilistic approach where each carrier could independently be used to verify a watermark, to withstand attacks that can only corrupt a proportion of carriers.\nSpecifically, we could consider each consecutive n tokens in Tw as an n-gram carrier unit. At each LLM paraphraser token generation step j, we could apply a watermarking operator W (Section 3.3) that perturbs the logits of Equation (1) based on the ID \u03bc and past n \u2212 1 generated tokens: Lj \u2190 W[\u03bc, w\u0302j\u2212n+1:j\u22121](Lj(w\u03021:j\u22121, \u0109)). The perturbed log-its will cause a detectable bias in each n-gram, hence the more n-grams that persist after any attack, the higher the verifiability.\nMeanwhile, in future generation steps j', the LLM paraphraser will correct deviations from semantic con-tent c and preserve fidelity given sufficient generation steps, as the subsequent logits Lj\u2032 (w\u03021:j\u2032\u22121, \u0109) are still conditioned on paraphrasing prompt \u0109.\nThis approach increases our framework's robustness against not just paraphrasing attacks, but also more gen-eral LLM-based attacks (e.g., A5). Past works have shown that language models tend to generate few novel n-grams outside their training set for small n (McCoy et al., 2023). Hence, LLMs trained on text with our watermarked n-grams may more likely generate them in their output. Given sufficient queries to these LLMs, the watermark could then be reliably verified, which we empirically demonstrate in Section 4.\n3.3 Increasing scalability with vocab permutation and orthogonal perturbation\nFinally, we propose a watermarking operator W com-prising two components: 1) vocab permutation, and 2) orthogonal perturbation. In this section, we will use a toy example (Vec) to show how these components work before presenting their general form. In Vec, we have logits L = [3,2,1], indexed by an ordered set Vo = {\u03b1, \u03b2, \u03b3} representing the token space, e.g., L(\u03b1) = 3. Figure 2a presents L as a graph (V as x-axis).\nVocab permutation. The vocab permutation opera-tor P produces a single permutation of Vo and L for any given key k (arrow \u2460 in Figure 2a). The inverse oper-ator P\u22121 reverses the permutation of P when provided the same key (arrow \u2461 in Figure 2a). As |Vo| = 3, there are 6 possible permutations of L, plotted as graphs over a new ordered index Vw = {a, b, c}, which we can interpret as the watermarking space. Then, we define the average permutation operator P acting on L (indexed by Vo) as one that takes a sequence of keys K\u03c0, apply P to get Lk for each k \u2208 K\u03c0, and averages them to get a vector L (indexed by Vw). Note that when we use P on L over all possible keys, we get a constant vector (e.g., L = 16 \u03a36i=1 Li = [2, 2, 2], \u2462 in Figure 2a).\nSimilarly, given a vector G indexed by Vw, which we can interpret as the watermark signal, the inverse opera-tor P\u22121 permutes G and Vw given a key k\u03c0, mapping it to V, the LLM-ordered token space (arrow \u2463 in Fig-ure 2b). P\u22121 acting on G analogously averages over all keys, and will also give a constant vector indexed over Vo (e.g., G = 16 \u03a36i=1 Gi = [0,0,0], \u2465 in Figure 2b).\nThis leads to an interesting insight: the permutation operators provide a way for us to add watermark signals to logits in a deterministically shifting Vw space (based on a sequence of keys) to boost verifiability and fidelity. For illustration, assume that an LLM paraphraser pro-"}, {"title": "4 Experiments", "content": "4.1 Data ownership\nFor watermarking of text articles, we demonstrate the effectiveness of WATERFALL with experiments using text samples To from the c4 realnewslike dataset (Raffel et al., 2020), comprising articles with mean to-ken length of 412. The experiments mirror realistic sce-narios, for e.g., news outlets watermarking their articles before publishing them to be able to effectively scan the internet for, and verify, plagiarized content (Brewster et al., 2023). For this setting, we evaluate the semantic similarity S using the Semantic Textual Similarity (STS) score based on the all-mpnet-base-v2 model\u00b3 (S for sample text pairs are provided in Appendix E.5)."}, {"title": "4.2 Watermarking of code", "content": "To demonstrate the versatility of WATERFALL, we also consider its out-of-the-box performance on code water-marking. We used the MBJSP dataset (Athiwaratkun et al., 2023), and evaluate fidelity S(To, Tw) using the pass@10 metric (Kulal et al., 2019; Chen et al., 2021) achieved by Tw on functional tests for the original code To. We compare WATERFALL, implemented using Phind-CodeLlama-34B-v25 as the paraphraser, with SRCMARKER (Yang et al., 2024), a recent state-of-the-art code watermarking scheme, configured for 16-bit watermarks. Experimental details are in Appendix G.\nWe found that surprisingly, WATERFALL achieves higher verifiability and robust verifiability (after A2 LLM paraphrasing attacks) compared to SRCMARKER while maintaining high code fidelity (Table 3). This is despite WATERFALL not requiring any manual train-ing/engineering of programming language-specific wa-termarking rules, which SRCMARKER does. Instead, WATERFALL inherits its code capabilities from its LLM paraphraser, making it easily adaptable to other lan-guages (e.g., see Appendix G.5 for Python code results).\n4.3 LLM data provenance of articles\nFinally, we explore how WATERFALL watermarks may persist after LLM fine-tuning, allowing us to use them for LLM data provenance. We consider the setting where client i watermarks a set of text {T(\u03bci)} that ad-versaries use, without authorization, to fine-tune their own LLMs (i.e., A5 attacks). Given multiple queries to the fine-tuned black-box LLM, the goal is for client i to be able to verify that {T(\u03bci)} had been used for training. This setting mirrors realistic scenarios where content owners want to detect unauthorized use of data for LLM training (Novet, 2024)."}, {"title": "5 Related Work", "content": "Early text watermarking techniques (Kamaruddin et al., 2018; Taleby Ahvanooey et al., 2019) primarily de-pend on structural adjustments (e.g., text formatting, use of different Unicode characters (Rizzo et al., 2019)), image-based techniques (e.g., pixel-adjustments of text), or semantic watermarking (e.g., substituting synonyms like BASIC described in Section 3.1). Recent works have augmented the latter with deep learning and lan-guage models for better performance (Qiang et al., 2023; Yoo et al., 2023; Ueoka et al., 2021; Abdelnabi and Fritz, 2021). However, as we showed in our experiments, these schemes are not robust to the range of practical LLM-enabled attacks possible today.\nA recently popular but separate line of work has fo-cused on the different model-centric problem setting of watermarking newly-generated output generated by a single LLM (Kirchenbauer et al., 2023; Venugopal et al., 2011; Christ et al., 2023; Kuditipudi et al., 2023; Zhao et al., 2023), rather than existing text owned by many clients. Hence, these works do not address our prob-lem desiderata such as achieving scalability and robust"}, {"title": "6 Discussion and Conclusion", "content": "We proposed WATERFALL, the first training-free frame-work for text watermarking that has low computational cost, scalability to large number of clients, and robust-ness to LLM attacks including unauthorized training of LLMs that generates IP-infringing text.\nThere is currently a lack of actual, practical large-scale deployment of text watermarking effective against LLM attacks, given the current SOTA watermarking methods' limitations and resource requirements. How-ever, WATERFALL may possibly provide a foundation for achieving large-scale deployment, with both decen-tralized or centralized options. This is made achievable given WATERFALL's low computational cost, scalability to a large number of clients, and robustness to LLM attacks including unauthorized training of LLMs that generates IP-infringing text.\nOur framework highlights a few perspectives that we hope more would consider. First, while increasingly capable LLMs allows for easier and more sophisticated forms of potential IP infringement, LLMs themselves could also enable better text IP protection of original texts. A key strength of WATERFALL is that its ca-pabilities grow as LLMs become more powerful, with increasingly better watermarking performance, allowing it to potentially keep up with the increasing capabili-ties adversaries can use for IP infringement. It is able to achieve a higher fidelity-verifiability Pareto frontier, and reduce any fidelity degradation while using higher watermarking strength for greater robust verifiability.\nSecond, as open-source LLM models become more prevalent and capable, adversaries could directly use them for IP attacks rather than depend on the services of closed-source LLM providers, allowing them to bypass any IP protection measures that these providers may im-plement (Piper, 2024). As such, content creators cannot just rely on LLM providers to assist in IP protection, but instead be equipped with methods such as WATER-FALL to protect their work before dissemination, such as by injecting robust watermarks that allows verifiability even after both traditional attacks and unauthorized use in LLM training by adversaries.\nThird, a general text watermarking framework like WATERFALL that can apply across different text types and languages not only helps with practical deployment, but also makes it highly versatile and not dependent on any text-specific properties. This makes it easily adapt-able for incorporating new defense methods, providing a strong foundation for future works to build on as new threats emerge."}, {"title": "7 Limitations", "content": "AS WATERFALL relies on the adjustment of the original text to add watermarks, it may not applicable to all types of text. For example, WATERFALL faces limitations in its application to works where their IP values lie in their style or format (e.g., poems), unless additional methods are applied that cause LLMs to largely preserve such styles while paraphrasing these works, such as optimizing for better paraphrasing prompts to be used with more capable LLMs, or iteratively refining the text through multiple rounds of watermarking.\nSimilar to other linguistics-based text watermarking methods, WATERFALL would also not be applicable where changes to the text are unacceptable (e.g. lyrics of a country's national anthem), or when applied to very short text (e.g. messages of just a few tokens). Nevertheless, WATERFALL is still useful for a wide range of settings where the IP lies mainly in the content of the text, and presents a step forward for practical deployment of text watermarking. Future work could build on WATERFALL to adapt it to other use cases for data provenance, such as data currency (i.e., ensuring that the data is up-to-date) or data authenticity (i.e., that the data has not been manipulated)."}]}