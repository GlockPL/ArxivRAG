{"title": "Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer", "authors": ["Michele Mancusi", "Yurii Halychanskyi", "Kin Wai Cheuk", "Eloi Moliner", "Chieh-Hsin Lai", "Stefan Uhlich", "Junghyun Koo", "Marco A. Mart\u00ednez-Ram\u00edrez", "Wei-Hsiang Liao", "Giorgio Fabbro", "Yuhki Mitsufuji"], "abstract": "Music timbre transfer is a challenging task that involves modifying the timbral characteristics of an audio signal while preserving its melodic structure. In this paper, we propose a novel method based on dual diffusion bridges, trained using the CocoChorales Dataset, which consists of unpaired monophonic single-instrument audio data. Each diffusion model is trained on a specific instrument with a Gaussian prior. During inference, a model is designated as the source model to map the input audio to its corresponding Gaussian prior, and another model is designated as the target model to reconstruct the target audio from this Gaussian prior, thereby facilitating timbre transfer. We compare our approach against existing unsupervised timbre transfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental results demonstrate that our method achieves both better Fr\u00e9chet Audio Distance (FAD) and melody preservation, as reflected by lower pitch distances (DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise level from the Gaussian prior, \u03c3, can be adjusted to control the degree of melody preservation and amount of timbre transferred.", "sections": [{"title": "I. INTRODUCTION", "content": "Music timbre transfer refers to the process of altering the timbral characteristics of an audio signal, such as the type of musical instrument, while preserving other attributes like melody and rhythm. This technique has a wide range of applications, including audio editing, voice cloning [1]. Various approaches have been developed for music audio timbre transfer. For example, methods based on variational autoencoders (VAEs) [2]\u2013[7] learn latent representations for the audio. By modifying the timbre-related dimensions within this latent space and decoding the altered representation, the timbre of the original audio can be transformed. Generative Adversarial Networks (GANs) [8]\u2013[12] have also been widely explored for this purpose. Recently, diffusion-based style transfer, popular in the image domain [13]\u2013[16], has been adapted to audio. However, text-to-audio style transfer methods often fail to retain musical content [17]\u2013[21]. While optimal flow transport methods have shown better content preservation post-transfer, their application has been limited to tasks like declipping and dereverberation [22], [23].\nIn this paper, we explore the possibility of using optimal flow transport to transfer between two monophonic single instrument audio, for example, from violin to flute. Our method is based on dual diffusion bridges [13] which is a particular solution to the Schr\u00f6dinger Bridge problem that aims to transport one data distribution into another [24]\u2013[26]. Our method offers several advantages over existing techniques: first, it does not require paired data, allowing it to be trained on diverse datasets where only unpaired audio is available; second, each model is trained independently on a specific instrument. To add a new instrument, only one additional model needs to be"}, {"title": "II. METHODS", "content": "During the diffusion process, we use the scheduling method proposed by Karras et al. [28] to sample the noise level, with \u03c1 = 9 and \u03c3_i uniformly sampled from i \u2208 [0, N \u2013 1] via Eq. (1).\n$\\sigma_i = (\\sigma_{\\text{min}}^{1/\\rho} + (i/{(N-1)})(\\sigma_{\\text{max}}^{1/\\rho} - \\sigma_{\\text{min}}^{1/\\rho}))^{\\rho}$                                          (1)\nWe set \u03c3_min = 0.01 and \u03c3_data = 1 for all experiments, as these values yield reasonable network precondition C_skip(\u03c3), C_out(\u03c3), C_in(\u03c3), and C_noise(\u03c3) as described in [28]. Various values of \u03c3_max are explored in Section III, as it significantly impacts the experimental outcomes. Following [28], we define our denoiser as:\n$D_{\\theta}(x; \\sigma_i) = C_{\\text{skip}}(\\sigma_i)x + C_{\\text{out}}(\\sigma_i) F_{\\theta}(C_{\\text{in}}(\\sigma_i)x; C_{\\text{noise}}(\\sigma_i)),$                                           (2)\nwhere x \u2208 \\mathbb{R}^d is the EnCodec embedding with dimension d, and F_\u03b8 is the neural network that takes in the noisy input C_in(\u03c3_i)x and the network precondition C_noise(\u03c3_i). To train F_\u03b8, we minimize the mean square error (MSE) between the original EnCodec embedding and the denoised embedding D_\u03b8(x; \u03c3_i):\n$E_{DM} = \\frac{1}{B} \\sum_{j=1}^B ||x_j - D_{\\theta}(x_j; \\sigma_{ij})||^2,$                                         (3)\nwhere B is the batch size, and \u03c3_ij is the noise level for the EnCodec embedding x_j in a batch.\nDuring inference, we use Algorithm 1, which is our forward deterministic probability flow ordinary differential equation (PF-ODE) [29] solver. It adds noise to x until it reaches the noise level \u03c3_max. The reverse ODE can be obtained by reversing the order of i in line 2."}, {"title": "A. Cycle Consistency", "content": "DDIB shows that sequentially solving PF-ODEs forms Schr\u00f6dinger bridges and achieves cycle consistency, where the process from source to latent to target and back to source recovers the original input. However, their analysis assumes perfect diffusion model training and no discretization errors in ODESolve, which is unrealistic. The argument of the following theorem extends DDIB's cycle consistency by accounting for ODESolve discretization errors using the RK method and training errors in diffusion models. It also provides a mathematical guarantee that our approach transfers the source distribution p_src to the target distribution p_tgt via Eq. (4).\nTheorem 1 (Distributional Cycle Consistency). Let $p^{\\text{tgt}}_t$ denote the density obtained by solving the ODEs in Eq. (4) numerically via a k^{th}-order RK method, starting from x^{(s)} \\sim p_{src}. Let $h := \\max_i \\sigma_{i+1} - \\sigma_i$ be the discretization timestep. Under certain as- sumptions, the total variation distance TV between $p^{\\text{tgt}}_t$ and $p^{\\text{tgt}}_t$ is bounded as:\n$TV(p_{tgt}^{t}, p_{tgt}^{'st}) \\leq O(\\epsilon_{DM}) + O(h^k).$\nHere, \u03f5_DM represents the training error of diffusion models, and O(\u00b7) and \\\\cdot conceal a multiplication constant depending only on p_src and p^{tgt}_t and the numerical solver.\nLet $p^{src}_t$ denote the density obtained via a k^{th}-order RK solver by solving the \"cycle manner\u201d ODEs as in DDIB's Proposition 3.1 (see Eqs. (9) and (10) in the Appendix). A similar argument leads to the generalized cycle consistency property as in Theorem 1:\n$TV(p^{src}_t, p_t^{src}) \\leq O(\\epsilon_{DM}) + O(h^k).$"}, {"title": "C. Model Architecture", "content": "The model architecture is based on a one-dimensional U-Net architecture designed for processing latent representations derived from EnCodec [30], with an input channel size of 128 corresponding to the dimensionality of the EnCodec embeddings right after the encoder and before the quantization step. To ensure stable training the resulting EnCodec embeddings are normalized to have zero mean and unit variance for each channel using the precomputed statistics of the dataset. For more details, please refer to the source code\u00b9."}, {"title": "III. EXPERIMENTS", "content": "We experiment with the CocoChorales Dataset [31], which contains 13 different solo instruments, from strings, woodwinds, and brass. The data is sampled at 16 kHz. In this study, we train our models on violin (766.9 hours), flute (244.0), cello (466.2), and bassoon (230.3). For inference and evaluation, a subset of 1,000 audio samples per instrument is selected from the test set. Each audio file is either padded or truncated to a fixed duration of 17 seconds. We first resample the audio into 24 kHz. Subsequently, the audio data is transformed into a latent representation using EnCodec."}, {"title": "B. Experimental Setup", "content": "We trained separate models for each instrument. The AdamW optimizer was used with a learning rate of 0.0001, \u03b2\u2081 = 0.95, \u03b22 = 0.999, and \u03f5 = 1.0 \u00d7 10\u22126. We applied Exponential Moving Average (EMA) with \u03b2 = 0.995 and a power factor of 0.7, and set weight decay to 0.001. Training was performed on a single Nvidia H100 GPU for 500 epochs with a batch size of 32.\nFor inference, we experimented with different \u03c3max and \u03c3N\u22121 in Section IV, in which we found that they influence the audio quality and melody preservation after timbre transfer. Additionally, we ob- served that the number of steps N in line 2 of Algorithm 1 is crucial for maintaining cycle consistency. For the main experiments, we used N =100 steps which is enough to maintain cycle consistency."}, {"title": "C. Pitch-Shifting Augmentation", "content": "To account for octave differences between x(s) and x(t) (e.g., flute to bassoon, cello to violin), we introduced pitch-shifting augmen- tation. Specifically, for the case where x(s) is flute, we randomly shifted the input pitch down by 1 to 25 semitones with a 35% probability during training to match the pitch range of bassoon and cello. During inference, we experiment with shifting x(s) down -20 and -25 semitones to obtain x(t) using Eq. (4)."}, {"title": "D. Chunk-based Minibatch Optimal Transport Coupling", "content": "We experiment with chunk-based minibatch optimal transport coupling strategy from Gaussian Flow Bridges (GFB) [23], which improves content preservation and reduces gradient variance by minimizing trajectory curvature. While GFB only applied chunking to the time dimension of the waveforms, we experimented with chunking along the channel dimension on the EnCodec embeddings, using a time chunk size of 4 and a channel chunk size of 32."}, {"title": "E. Evaluation", "content": "To evaluate melody preservation, we use Basic Pitch [32] to tran- scribe the audio into an (88, T) matrix representing the probability of a note being played, where T is the time dimension, and 88 is the number of notes. We aggregate 88 notes into 12 pitch classes. Melody similarity between original and transferred versions is assessed using Jaccard distance [33] and Dynamic Pitch Distance (DPD), which incorporates Dynamic Time Warping (DTW) to account for temporal deviations.\nTo measure the quality of timbre transfer, we employed both objective and perceptual evaluation methods. We trained a simple neural network classifier on the EnCodec embeddings (averaged over the time dimension) and measured the accuracy. The classifier consists of two fully connected layers: the first layer maps the input dimension of 128 to 64 units, followed by a ReLU activation, and the second layer reduces the output to 5 units corresponding to the different timbre classes. Additionally, we utilized the Fr\u00e9chet Audio Distance (FAD) computed on EnCodec embeddings, implemented in the Python library [34], to assess the perceptual quality of the generated audio. A listening test is also conducted where 20 participants are presented with 6 question to vote for the model that transfer to the target instrument better."}, {"title": "IV. RESULTS", "content": "We compare our proposed method against an existing unsupervised timbre transfer model, VAEGAN [11]. As shown in Table I, VAE- GAN achieves lower Jaccard distances, but it has a higher DPD than our method. Note that the Jaccard distance considers only the set of notes present in the audio while DPD consider also the temporal note alignment. Hence we conclude that our method preserves the musical structure of the melody better. We also observe that VAEGAN introduces significant artifacts in the generated audio, as evidenced by its higher FAD. These artifacts come from performing timbre transfer in the spectral domain, where a vocoder is required to convert spectrograms back into waveforms which introduces distortions. While using better vocoders could mitigate these artifacts, exploring such alternatives is beyond the scope of this paper. This highlights one of the advantages of our proposed model, which operates without the need for vocoders, thus avoiding these issues.\nNext, we compare our method with a related approach, Gaussian Flow Bridges (GFB) [23]. GFB achieves a relatively low DPD, suggesting well preserved melodies. However, further analysis reveals that GFB fails to transfer x(s) to x(t) as reflected by the the near-zero timbre classification accuracy for both flute and violin. The FAD for flute-to-violin transfer is 60.8, close to the FAD between real flute and violin samples (59.8), suggesting minimal timbral change despite the transfer. This slight increase in FAD implies that GFB attempts to alter the audio but retains the original timbre. We attribute this limitation to GFB's linear probability path, which seems to work only for tasks like declipping and dereverberation. In contrast, our method significantly outperforms GFB in timbre transfer while preserving the melodies. When the target instrument is in different octave range from the source (e.g., flute to bassoon), both VAEGAN and our method fail to preserve the melody. Techniques to address this will be discussed in the next section.\nOverall, our proposed method achieves a decent balance between timbre transfer quality and melody preservation compared to GFB and VAEGAN. This is supported by both pitch and timbre metrics. Additionally, the listening test results show that our model received 97"}, {"title": "B. Effectiveness of Pitch-Shifting Augmentation", "content": "Table II shows the results of pitch-shifting the source data x(s) to match with the pitch ranges of the target data x(t), specifically from flute to bassoon and cello. The results indicate a tradeoff between timbre quality and melody preservation. While the lower FAD without pitch-shifting indicates better audio quality, the high DPD reflects poor melody preservation. Shifting the flute down by 20 semitones improves melody preservation (lower DPD) but degrades audio quality (higher FAD). A further shift of 25 semitones down significantly worsens audio quality, as shown by the increased FAD. While a larger shift benefits melody preservation when the target is cello, it does not do so for bassoon. We hypothesize that excessive downsampling distorts x(s), which disrupts the bridging between F\u03b8(s) and F\u03b8(t). From this experiment, we conclude that a 20-semitone shift strikes an optimal balance between preserving timbre quality and maintaining melody integrity. Based on this result, we use 20-semitone shift for the case of flute-to-bassoon transfer in the next section when exploring different values of \u03c3max."}, {"title": "C. Impact of \u03c3max on Timbre Transfer", "content": "Table III explores the effects of different \u03c3max (for training) and \u03c3N-1 (for inference) values. Starting with the case where \u03c3N-1 = \u03c3max,"}, {"title": "D. Results of Chunk-based Coupling Strategy", "content": "Since the previous results show that \u03c3N-1 = 5 achieves a good balance in timbre transfer and melody preservation, only the case for \u03c3N-1 = 5 and 100 will be discussed. Table IV shows the effects of different chunk-based strategies. When \u03c3N-1 = 5, chunking offers minimal benefits. In the flute-to-violin case, applying chunking to the time dimension fails to improve both pitch and timbre metrics. Even when chunking is applied to both time and channel dimensions, all metrics still perform worse than the models trained without chunking. Interestingly, timbre classification accuracy remains at 100%. We attribute this to the complexity of the violin timbre, which is challenging to generate due to its distinct bow articulation, yet making it easier to classify. In the violin-to-flute transfer case, there is a slight improvement in melody preservation when chunking is used.\nIn contrast, with \u03c3N-1 = 100, chunking leads to noticeable improvements, particularly in flute-to-violin transfers, where both FAD and melody preservation are significantly enhanced when using"}, {"title": "E. Shared Latent Space and Cycle Consistency", "content": "Equation (4) implies that all models F\u03b8 should point to the same prior distribution N (0, \u03c3max). In this section, we explore the existence of shared latent space by directly sampling from the Gaussian x(1) ~ N(0, \u03c3max) and use it to obtain both x(s) and x(t) via:\nx^{\u25ca} = \\text{ODESolve}(x^{(1)}; D_{\\theta}^{\u25ca}, \\sigma_{N-1}, \\sigma_0), \u25ca \u2208 {s,t}                                          (5)\nIf all models F\u03b8 share the same prior distribution, we expect x^\u25ca would share the same melody and change only in timbre depending on which model it is sampled from. We study the case for violin and flute with \u03c3max = 100. Out of 100 trials, we found that 49 of the x(i) pairs have similar melodic structure (DPD < 0.7). When using different D\u03b8 for different F\u03b8\u25ca, only 6 out of 100 trials produce similar melodic structures. These results indicate that F\u03b8 share the same prior distribution to some extent.\nCycle consistency is a critical aspect of audio-to-audio algorithms like timbre transfer. It is found to be sensitive to the number of sampling steps. As shown in the supplmentary material\u00b9, the normalized L2 norm between embeddings decreased as the number of steps increased, improving reconstruction. However, this improve- ment comes at the cost of increased computational time, as more steps in the Karras sampler require more processing time. This tradeoff highlights the need to balance reconstruction quality with efficiency in practice."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed an unsupervised musical timbre transfer using dual diffusion bridges. Through extensive experimentation, we demonstrated that our approach outperforms existing methods like VAEGAN and GFB in terms of timbre quality and melody preservation. We also provided a theoretical explanation for the timbre transfer and cycle consistency of our proposed method. The source code and the supplementary materials are available online\u00b9."}, {"title": "APPENDIX A. THEORETICAL RESULT AND ITS PROOF", "content": "A-1. Preliminaries\nLet $p^{\\text{src}}_t$ and $p^{\\text{tgt}}_t$ be the source distribution and target distribution, respectively on $\\mathbb{R}^d$ of dimension d. We consider the Ornstein-Uhlenbeck (OU) process\u00b2 on an interval [0, T]\n$dx_t = -\\gamma x_t dt + \\sqrt{2\\gamma} dW_t, x_0 \\sim p$                                          (6)\nwhere $\u25ca \u2208 {src, tgt}$ and ${W_t}_{t\u2208[0,T]}$ is a Wiener process. We denote $p_t$ as the marginal density of the stochastic process ${x_t}_{t\u2208[0,T]}$ given by Eq. (6). It associates with the following PF-ODE [29]\n$\\frac{d}{dt}x_t = \\gamma x_t + \\nabla \\log p_t(x_t),$ where \u25ca \u2208 {src, tgt}                                          (7)\nWhen t = 0, it represents the clean data space (where $p^{\\text{src}}_t$ and $p^{\\text{tgt}}_t$ are supported on), and when t = T, it represents the latent noisy space. For $\u25ca \u2208 {src, tgt}$, diffusion model $D_\u25ca(x,t)$ is trained to approximate $\\nabla \\log p_t(x_t)$ and leads to the following empirical PF-ODE\n$\\frac{d}{dt}x_t = \\gamma x_t + D_\u25ca(x_t, t)$                                          (8)\nLet p and q be two densities defined on $\\mathbb{R}^d$. We define the total variation distance between p and q as\n$TV(p,q):= \\frac{1}{2} \\int |p(x) - q(x)| dx.$ \nStarting from $x^{\\text{src}} \\sim p^{\\text{src}}_t$, the following ODEs solving defines a cycle manner procedure\n$x^{\\text{latent}} = \\text{ODESolve}(x^{\\text{src}}; D_{\\text{src}}, 0, T),$\n$x^{\\text{tgt}} = \\text{ODESolve}(x^{\\text{latent}}; D_{\\text{tgt}}, T, 0),$\nand then\n$x^{\\text{latent}} = \\text{ODESolve}(x^{\\text{tgt}}; D_{\\text{tgt}}, 0, T),$\n$x^{\\text{src}} = \\text{ODESolve}(x^{\\text{latent}}; D_{\\text{src}}, T, 0),$                                          (9)\n(10)\nDDIB proves the cycle consistency property that $x^{\\text{src}} = x^{\\text{src}}$, but assumes perfect diffusion model training and no ODE discretization errors, which are unrealistic. In Theorem 1', we establish distributional cycle consistency by accounting for diffusion model training errors and ODESolve discretization errors."}, {"title": "A-2. Assumptions", "content": "We list up the assumptions which are mostly similar to those in [36].\nAssumption A (Compactly supported densities). Both $p^{\\text{src}}_t$ and $p^{\\text{tgt}}_t$ are compactly supported on a compact set in $\\mathbb{R}^d$.\nAssumption B (Training accuracy of diffusion model). Let $E_{DM} > 0$. For \u25ca \u2208 {src, tgt},\n$E_{z_t} p_t(x) [||D_\u25ca(x_t, t) - \\nabla \\log p_t(x_t)||^2] dt dt < E_{DM}$\nAssumption C (Smoothness of diffusion model). For \u25ca \u2208 {src, tgt}, assume that $D_\u25ca(\u00b7, t)$ is $C^2 (\\mathbb{R}^d)$ for all t \u2208 [0,T]. That is, it is twice continuously differentiable. Additionally, we assume that there is a constant $L_t > 0$ so that\n$||D_\u25ca(\\cdot, t) ||_{C^2 (\\mathbb{R}^d)} \\leq L_t$\nWe denote L := $\u222b_0^T L_tdt$ and assume that L < \u221e."}, {"title": "A-3. Full Statement of Theorem 1 and Its Proof", "content": "To ensure precision, we slightly modify the notations used in the main manuscript. We present the theorem with time discretization, corresponding one-to-one with variance discretization [28]. Let $t_{N-1} = T > \u2026 > t_{i+1} > t_i > \u2026 > t_0 = 0$ be the discretization timestep on [0, T], and define $h := \\max_{i\u2208{0,\u2026\u2026,N\u22121}} |t_{i+1} \u2013 t_i|$.\nStarting from $x^{(s)} \\sim p^{\\text{src}}_t$, let $\\hat{p}^{\\text{latent}}_t$ be the oracle density obtained by the forward-in-time PF-ODE (Eq. (7) with \u25ca = src), and $\\hat{p}^{''\\text{latent}}_t$ be the pushforward density obtained by solving the ODE (Eq. (8) with \u25ca = src) numerically:\n$\\hat{x}^{(1)} = \\text{ODESolve}(x^{(s)}; D^{\\text{src}},0,T), x^{(s)} \\sim p^{\\text{src}}_t$\nNow starting from the noisy latent space, let $\\hat{p}^{\\text{tgt}}_t$ be the density obtained by solving the ODE (Eq. (8) with \u25ca = tgt), starting from $\\hat{x}^{(1)} \\sim \\hat{p}^{\\text{latent}}_t$.\n$\\hat{x}^{(t)} = \\text{ODESolve}(\\hat{x}^{(1)}; D^{\\text{tgt}},T,0), \\hat{x}^{(1)} \\sim \\hat{p}^{\\text{latent}}_t$\nWe now present the full statement of Theorem 1 along with its proof."}, {"title": "Theorem 1' (Distributional Cycle Consistency)", "content": "Consider the ODE solvers are $k^{th}$-order RK method. Under Assumptions A, B, and C, the total variation distance TV between $p_t^{'^{tgt}}$ and $p_t^{tgt}$ is bounded by:\n$TV(p_t^{'^{tgt}}, p_t^{tgt}) < O(\\epsilon_{DM}) + O(h^k)$\nHere, $\u25ca$ and $O(\u22c5)$ conceals a multiplication constant depending only on dimensionality d, $p^\u25ca_t$ with $\u25ca\u2208 {src, tgt}$, and the pre-defined Runge-Kutta matrix [37].\nProof. Applying [36]'s Theorem 3.10 and its Remark C.2 backward in time (from T to 0) to Eqs. (8) and (7) with \u25ca = tgt, we obtain\n$TV(p_t^{'^{tgt}}, p_t^{tgt}) < TV(p_t^{\\text{latent}}, p_t^{\\text{latent}}) + O(\\epsilon_{DM}) + O(h^k)$\nNow applying the same theorem but forward in time (from 0 to T) to Eqs. (8) and (7) with \u25ca = src, we obtain\n$TV(p_t^{\\text{latent}}, p_t^{\\text{latent}}) < O(\\epsilon_{DM}) + O(h^k)$\nas we start from the same initial distribution $p^{\\text{src}}_t$. Combining these two inequalities, we derive the desired bound:\n$TV(p_t^{'^{tgt}}, p_t^{tgt}) < O(\\epsilon_{DM}) + O(h^k)$"}]}