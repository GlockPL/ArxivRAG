{"title": "Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots", "authors": ["Pranay Dugar", "Aayam Shrestha", "Fangzhou Yu", "Bart van Marum", "Alan Fern"], "abstract": "We introduce the Masked Humanoid Controller (MHC) for whole-body tracking of target trajectories over arbitrary subsets of humanoid state variables. This enables the realization of whole-body motions from diverse sources such as video, motion capture, and VR, while ensuring balance and robustness against disturbances. The MHC is trained in simulation using a carefully designed curriculum that imitates partially masked motions from a library of behaviors spanning pre-trained policy rollouts, optimized reference trajectories, re-targeted video clips, and human motion capture data. We showcase simulation experiments validating the MHC's ability to execute a wide variety of behavior from partially-specified target motions. Moreover, we also highlight sim-to-real transfer as demonstrated by real-world trials on the Digit humanoid robot. To our knowledge, this is the first instance of a learned controller that can realize whole-body control of a real-world humanoid for such diverse multi-modal targets.", "sections": [{"title": "1 Introduction", "content": "Humanoid robots hold immense potential as highly capable and adaptive platforms for tackling complex real-world tasks, thanks to their dexterous multi-purpose body structure that mirrors our own. However, the development of versatile and robust whole-body controllers for bipedal humanoids remains a critical challenge in robotics. Traditional approaches involve meticulous manual engineering of separate controllers for different skills such as standing [1], walking [2], and manipulation [3], resulting in specialized controllers with limited versatility and adaptability.\nA truly versatile whole body humanoid controller should exhibit several key properties. First, it should comprehend and execute target motions specified through multiple input modalities, such as video demonstrations, motion capture data, or high-level locomotion and end-effector targets. Second, the controller should be be robust to dynamic command updates, noisy or inexact inputs, inaccurate simulation parameters, and natural external disturbances. Finally, the controller should be versatile, allowing straightforward extension of its repertoire as new motion examples become available, with no or minimal retraining.\nTo address these challenges, we train the Masked Humanoid Controller (MHC), a whole-body controller that is able to accommodate target motions specified as future trajectories over full or partial robot poses. This allows the MHC to support the above multi-modality property, for example, following walking trajectories specified solely by desired velocities or imitating arm-only motions extracted from video clips. We train the MHC via reinforcement learning using a carefully designed curriculum and an expansive library of behaviors spanning optimized reference trajectories, re-targeted video clips, and human motion capture data. The tailored curriculum over motion commands and disturbances gradually introduces capabilities with the aim of achieving the above desired robustness and versatility properties."}, {"title": "2 Related Work", "content": "Robust Locomotion Control. Recent advancements in robotic locomotion showcase adaptive control strategies that enable robots to navigate complex terrains and perform dynamic maneuvers. Significant progress has been made in the locomotion of quadrupeds, demonstrating enhanced stability [4] [5] and agility [6] in various environments. Fu et al. [7] train a unified whole body manipulation and locomotion controller that attaches an arm on top of a quadraped to perform mobile manipulation tasks. Notable achievements for bipedal locomotion with the Cassie platform (no upper torso or arms) include blind locomotion across multiple gaits and behaviors [8, 9], locomotion under varying loads [10], and visually-guided locomotion over irregular terrain using deep learning to process visual input and adapt locomotion strategies accordingly [11]. More recently a learned controller for blind locomotion on the full-body humanoid Digit was demonstrated across varying terrain and disturbances [12]. These advancements underscore the importance of robust data and simulation technologies in developing and refining control strategies. However, managing transitions between different modes of locomotion, such as standing and walking, remains a significant challenge. Van Marum et al. [13] addressed this complexity with the development of the SaW controller for Digit, which integrates stability and adaptive walking strategies into a single operational framework that maintains balance against natural disturbances. However, standing and walking alone do not constitute whole-body control. Building on the foundation laid by the SaW controller, our approach enhances it by incorporating comprehensive whole-body control, allowing for more versatile and adaptive humanoid robot behaviors.\nMotion Tracking for Simulated Humanoids. Motion capture data has been used extensively in generating motions for simulated characters. Most closely related to our work is the application"}, {"title": "3 Problem Formulation", "content": "Given a dataset of humanoid motions from various behaviors, our objective is to learn a controller which can match target motion directives that are representative of the data distribution. We are particularly interested in supporting partially-specified motion directives e.g. only specifying the upper-body joint trajectories, or just the torso velocity, or both. It is expected that the controller \"fills in the blanks\u201d for joints that are not specified by a directive, e.g. details of the lower-body joints. Such a controller can support directives derived from various input modalities. For example, fully-specified directives that cover all humanoid joints can be derived from MoCap data, while joystick commands regarding the root velocity and arm movements correspond to partial directives.\nMore formally, a motion is a sequence of poses $q_{1:H}$ for a humanoid with $J$ joints over $H$ time steps. Each pose is represented by a tuple $q_i = (\\theta_i, \\dot{\\theta}_i, v_i, w_i, b_i)$, where $\\theta_i \\in \\mathbb{R}^J$ denotes the joint angles, $\\dot{\\theta}_i \\in \\mathbb{R}^J$ denotes the joint angular velocities, $v_i \\in \\mathbb{R}^2$ denotes the root planar linear velocities, $w_i \\in \\mathbb{R}$ denotes the turn rate for the humanoid, and $b_i \\in \\mathbb{R}^3$ denotes the Euler base orientation in the x-axis, y-axis, and the height of the base from the ground plane. Motion directives are used to specify constraints on a desired motion to be generated. Specifically, a motion directive $d$ is defined as a masked motion sequence represented by $d = (\\hat{q}_{1:H}, I_{1:H})$, where $\\hat{q}_{1:H}$ is a masked motion and $I_i \\in \\{0, 1\\}$ is a sequence of binary masks such that $I_i$ indicates which dimensions of pose $q_i$ are selected as motion constraints. We follow the convention of setting the masked dimensions of $\\hat{q}$ to zero. Note that while the definition of a directive allows for arbitrary sets of state variables to be masked, in practice, we focus training and evaluation masking patterns relevant to the multiple modalities the controller needs to support as described in Section 4.\nThe Masked Humanoid Controller (MHC) is a controller $\\pi$ that at each time step $t$ takes an input containing the current humanoid state $s_t = (\\theta_t, \\dot{\\theta}_t, w_t)$ and a target motion directive $d$, where $\\theta_t \\in \\mathbb{R}^J$ and $\\dot{\\theta}_t \\in \\mathbb{R}^J$ are the joint positions and velocities, and $w_t$ is the orientation in quaternion form. The output of the MHC is an action $a_t = \\pi(s_t, d)$, which in our work corresponds to PD setpoints for all robot motors. The objective of the MHC is to select actions such that the future humanoid"}, {"title": "4 Masked Humanoid Controller", "content": "Due to the lack of low-level supervisory information, we train the MHC via reinforcement learning (RL) in simulation and then transfer the resulting MHC to the real world. Below we describe our choices for the MHC network architecture. Next we describe the training approach, which includes the curriculum of training episodes and domain randomization choices to support sim-to-real transfer. Finally, we describe the reward function used to guide training.\nNetwork Architecture. The MHC model is a Long Short-Term Memory (LSTM) recurrent neural network which takes as input the current humanoid state $s_t$ and the next step of the current directive $d_i = (\\hat{q}_i, I_i)$. The MHC first processes the directive with a single-layer feed-forward encoder to produce a 160 dimensional directive embedding. This is concatenated with $s_t$ and fed into an LSTM block configured with two 64-unit recurrent layers. A final linear decoding layer outputs an offset for each actuated joint. The MHC action $a_t$ is computed by adding this offset to the actuated joint values in $\\hat{q}_i$, which yields the next PD setpoints. Note that for the masked joints in $d_i$, the corresponding values in $\\hat{q}_i$ are zero and the offset corresponds to the actual PD setpoint. In our work, we use the Digit V3 humanoid, which has 20 actuated joints. The MHC is run at 50Hz to compute PD setpoints which are sent to a PD controller running at 2kHz.\nTraining Approach. The MHC is trained via the PPO RL algorithm [26] in simulation using the MuJoCo physics engine with a Digit V3 humanoid model. Below we describe the way we generate a motion dataset for creating directives, how we generated training episodes, the curriculum stages used for effective training, and domain randomization to facilitate sim-to-real transfer.\nMotion Data Generation. We create a diverse set of reference motions for training the MHC using a combined dataset consisting of human motion capture (MoCap) datasets (AMASS [27], Reallusion [28]), and video demonstrations. To help bridge the human to Digit embodiment gap, we employ an inverse kinematics (IK) based retargeting procedure to map the dataset trajectories to Digit's kinematic model. For each frame in the dataset, we solve an IK problem for the generalized position vector of digit $q = (\\theta, b)$ formulated as a nonlinear program (NLP). We use the IK module in Drake [cite] to set up the costs and constraints associated with the NLP, and SNOPT [cite] as the underlying solver. Critical kinematic feasibility requirements such as locking the stance foot to the ground and respecting the closed kinematic chain topology of Digit's legs are expressed as constraints in task space. Upper body motion targets that serve more of a stylistic purpose such torso pose, and hand positions are expressed using costs to aid in convergence. Any frames that the IK optimization did not generate a feasible solution for are simply dropped. Linear interpolation is applied to the sequence of states $q_i$ and their timestamps $t_i$ to produce the final time-parameterized trajectory $Q_{1:H}$ used for training.\nEpisode Generation. Each episode is initialized with the robot in a standing position. Next a random command window length $w$ is generated and a random directive $d$ is drawn from a distribution defined by the curriculum stage. The MHC then cycles through $d$ until reaching $w$ steps. This sampling of a $w$ and $d$ continues until reaching the maximum episode length $e$, which depends on the curriculum stage, or the robot falls. By switching between multiple random directives during an episode, the MHC can learn to smoothly transition between different types of motion. In addition, to encourage robustness, during the execution of each episode we apply perturbations on the torso according to a distribution that depends on the curriculum stage.\nCurriculum Stages. We employ a three-stage curriculum that progressively learns locomotion, robustness to perturbations, and whole-body motion tracking. This approach enables the model to gradually acquire complex skills while ensuring stable learning."}, {"title": "5 Experimental Results", "content": "In this section, we present our evaluation protocol and simulation experiments to assess the MHC's performance in executing diverse behaviors from full and partially-specified target motions. We compare the MHC with baseline approaches and analyze the impact of training dataset diversity as well as learning curriculum. Additionally, we discuss qualitative observations and the successful sim-to-real transfer of the MHC through real-world trials on the Digit V3 humanoid robot.\nMotion Directive Dataset. We used IK retargetting to generate 75 kinematic motion trajectories selected from our motion sources as described in Section 4. We aimed to select a diverse set of motions, while avoiding aggressive motions that involve highly dynamic behavior such as jumping, kicking, and large sudden swinging of limbs. In order to test models on motions outside of their training sets, we divided the data into three sets, each containing a mix of motions from the original motion sources.\nWhile each set contains some basic motions related to hand waving and simple upper body movements, the general trend is an increasing level of difficulty as judged by the authors. We note that setC has a number of the most difficult motions, involving highly dynamic motion, such as tennis smashes and difficult boxing moves. The feasibility of these more difficult motions is unclear for our realistic Digit model.\nExperiment Setup and Metrics. Each experiment involves evaluating an MHC controller on one of the retargeted motion datasets. For each motion we create 3 motion directives corresponding to a whole body directive, upper body command with lower body standing directive, upper body command with lower body locomotion directive. For each motion directive we evaluate the MHC over 200 episodes, each having a maximum time of 50 seconds (2500 steps). Each episode starts from a randomized point in the directive and then repeatedly cycles through the directive three times before resetting to another randomly selected starting point. Based on these episode runs we evauate the following metrics, which depend on the type of directive.\nComparison to Baselines. There are no existing approach that handle the masked directive inputs of the MHC. Thus, we have developed two baselines to compare against: 1) Locomotion+Offset. This controller is trained on the first two curriculum stages and only sees the locomotion commands at its input. At inference time, it receives locomotion commands from the directives and for other"}, {"title": "6 Limitations", "content": "One of the major current limitations is a significant sim-to-real gap. This is particularly prevalent for motions where the feet are placed widely or where one foot remains in the air for an extended period. Our hypothesis is that this sim-to-real limitation requires research that is more generally focused on sim-to-real transfer, possibly via the incorporation of real-world data. The MHC has been a useful vehicle for highlighting these gaps. Another limitation of our current training approach is that we only consider a limited set of masking patterns during training. Supporting fully general masking may lead to additional robustness and open up new possibilities for authoring behaviors. Finally, the MHC is currently does not account for interaction with external objects, such as the box, during a box pickup. Explicitly incorporating objects to be manipulated into the MHC input and training is an important direction for supporting practical applications."}, {"title": "Appendix A Dynamics Randomization", "content": "All models in our work were trained with a fixed dynamics randomization setup in order to transfer from simulation to real without learning the exact dynamics of the system. The ranges for all elements are given in Table 3."}, {"title": "Appendix B Implementation Details", "content": "MHC has been trained using actor critic based PPO algorithm. Table 4 specifies the hyperparameters used for each training cycle independent of the curriculum."}, {"title": "Appendix C Reward structure", "content": "Table 5 gives a detailed view of our complete reward function. Note that style rewards are only ignored when following a fully-specified directive denoted by Ii == 1.\nFollowing notation has been used:\n\u2022 i indicates current time step\n\u2022 I\u2081 == 1 indicated if all joints are being mimicked and its a whole body motion directive;\n\u2022 $U_{xy}$ = base linear velocity;\n\u2022 s = current robot parameter;\n\u2022 b = base acceleration;\n\u2022 $C_{feet}$ = default foot position xyz and orientation rpy;\n\u2022 qd() = quaternion distance function;"}]}