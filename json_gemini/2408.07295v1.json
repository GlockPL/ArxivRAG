{"title": "Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots", "authors": ["Pranay Dugar", "Aayam Shrestha", "Fangzhou Yu", "Bart van Marum", "Alan Fern"], "abstract": "We introduce the Masked Humanoid Controller (MHC) for whole-\nbody tracking of target trajectories over arbitrary subsets of humanoid state vari-\nables. This enables the realization of whole-body motions from diverse sources\nsuch as video, motion capture, and VR, while ensuring balance and robustness\nagainst disturbances. The MHC is trained in simulation using a carefully de-\nsigned curriculum that imitates partially masked motions from a library of be-\nhaviors spanning pre-trained policy rollouts, optimized reference trajectories, re-\ntargeted video clips, and human motion capture data. We showcase simulation\nexperiments validating the MHC's ability to execute a wide variety of behavior\nfrom partially-specified target motions. Moreover, we also highlight sim-to-real\ntransfer as demonstrated by real-world trials on the Digit humanoid robot. To our\nknowledge, this is the first instance of a learned controller that can realize whole-\nbody control of a real-world humanoid for such diverse multi-modal targets.", "sections": [{"title": "1 Introduction", "content": "Humanoid robots hold immense potential as highly capable and adaptive platforms for tackling com-\nplex real-world tasks, thanks to their dexterous multi-purpose body structure that mirrors our own.\nHowever, the development of versatile and robust whole-body controllers for bipedal humanoids re-\nmains a critical challenge in robotics. Traditional approaches involve meticulous manual engineer-\ning of separate controllers for different skills such as standing [1], walking [2], and manipulation\n[3], resulting in specialized controllers with limited versatility and adaptability.\nA truly versatile whole body humanoid controller should exhibit several key properties. First, it\nshould comprehend and execute target motions specified through multiple input modalities, such\nas video demonstrations, motion capture data, or high-level locomotion and end-effector targets.\nSecond, the controller should be be robust to dynamic command updates, noisy or inexact inputs,\ninaccurate simulation parameters, and natural external disturbances. Finally, the controller should\nbe versatile, allowing straightforward extension of its repertoire as new motion examples become\navailable, with no or minimal retraining.\nTo address these challenges, we train the Masked Humanoid Controller (MHC), a whole-body con-\ntroller that is able to accommodate target motions specified as future trajectories over full or partial\nrobot poses. This allows the MHC to support the above multi-modality property, for example,\nfollowing walking trajectories specified solely by desired velocities or imitating arm-only motions\nextracted from video clips. We train the MHC via reinforcement learning using a carefully de-\nsigned curriculum and an expansive library of behaviors spanning optimized reference trajectories,\nre-targeted video clips, and human motion capture data. The tailored curriculum over motion com-\nmands and disturbances gradually introduces capabilities with the aim of achieving the above desired\nrobustness and versatility properties."}, {"title": "2 Related Work", "content": "Robust Locomotion Control. Recent advancements in robotic locomotion showcase adaptive con-\ntrol strategies that enable robots to navigate complex terrains and perform dynamic maneuvers. Sig-\nnificant progress has been made in the locomotion of quadrupeds, demonstrating enhanced stability\n[4] [5] and agility [6] in various environments. Fu et al. [7] train a unified whole body manipulation\nand locomotion controller that attaches an arm on top of a quadraped to perform mobile manipula-\ntion tasks. Notable achievements for bipedal locomotion with the Cassie platform (no upper torso or\narms) include blind locomotion across multiple gaits and behaviors [8, 9], locomotion under vary-\ning loads [10], and visually-guided locomotion over irregular terrain using deep learning to process\nvisual input and adapt locomotion strategies accordingly [11]. More recently a learned controller\nfor blind locomotion on the full-body humanoid Digit was demonstrated across varying terrain and\ndisturbances [12]. These advancements underscore the importance of robust data and simulation\ntechnologies in developing and refining control strategies. However, managing transitions between\ndifferent modes of locomotion, such as standing and walking, remains a significant challenge. Van\nMarum et al. [13] addressed this complexity with the development of the SaW controller for Digit,\nwhich integrates stability and adaptive walking strategies into a single operational framework that\nmaintains balance against natural disturbances. However, standing and walking alone do not con-\nstitute whole-body control. Building on the foundation laid by the SaW controller, our approach\nenhances it by incorporating comprehensive whole-body control, allowing for more versatile and\nadaptive humanoid robot behaviors.\nMotion Tracking for Simulated Humanoids. Motion capture data has been used extensively in\ngenerating motions for simulated characters. Most closely related to our work is the application"}, {"title": "3 Problem Formulation", "content": "Given a dataset of humanoid motions from various behaviors, our objective is to learn a controller\nwhich can match target motion directives that are representative of the data distribution. We are\nparticularly interested in supporting partially-specified motion directives e.g. only specifying the\nupper-body joint trajectories, or just the torso velocity, or both. It is expected that the controller\n\"fills in the blanks\u201d for joints that are not specified by a directive, e.g. details of the lower-body\njoints. Such a controller can support directives derived from various input modalities. For example,\nfully-specified directives that cover all humanoid joints can be derived from MoCap data, while\njoystick commands regarding the root velocity and arm movements correspond to partial directives.\nMore formally, a motion is a sequence of poses $q_{1:H}$ for a humanoid with $J$ joints over $H$ time steps.\nEach pose is represented by a tuple $q_i = (\\theta_i, \\dot{\\theta}_i, v_i, w_i, b_i)$, where $\\theta_i \\in \\mathbb{R}^J$ denotes the joint angles,\n$\\dot{\\theta}_i \\in \\mathbb{R}^J$ denotes the joint angular velocities, $v_i \\in \\mathbb{R}^2$ denotes the root planar linear velocities,\n$w_i \\in \\mathbb{R}$ denotes the turn rate for the humanoid, and $b_i \\in \\mathbb{R}^3$ denotes the Euler base orientation in\nthe x-axis, y-axis, and the height of the base from the ground plane. Motion directives are used to\nspecify constraints on a desired motion to be generated. Specifically, a motion directive $d$ is defined\nas a masked motion sequence represented by $d = (\\hat{q}_{1:H}, I_{1:H})$, where $\\hat{q}_{1:H}$ is a masked motion and\n$I_{1:H}$ is a sequence of binary masks such that $I_i$ indicates which dimensions of pose $q_i$ are selected\nas motion constraints. We follow the convention of setting the masked dimensions of $\\hat{q}$ to zero. Note\nthat while the definition of a directive allows for arbitrary sets of state variables to be masked, in\npractice, we focus training and evaluation masking patterns relevant to the multiple modalities the\ncontroller needs to support as described in Section 4.\nThe Masked Humanoid Controller (MHC) is a controller $\\pi$ that at each time step $t$ takes an input\ncontaining the current humanoid state $s_t = (\\theta_t, \\dot{\\theta}_t,w_t)$ and a target motion directive $d$, where $\\theta_t \\in\n\\mathbb{R}^J$ and $\\dot{\\theta}_t \\in \\mathbb{R}$ are the joint positions and velocities, and $w_t$ is the orientation in quaternion form.\nThe output of the MHC is an action $a_t = \\pi(s_t, d)$, which in our work corresponds to PD setpoints\nfor all robot motors. The objective of the MHC is to select actions such that the future humanoid"}, {"title": "4 Masked Humanoid Controller", "content": "Due to the lack of low-level supervisory information, we train the MHC via reinforcement learn-\ning (RL) in simulation and then transfer the resulting MHC to the real world. Below we describe\nour choices for the MHC network architecture. Next we describe the training approach, which in-\ncludes the curriculum of training episodes and domain randomization choices to support sim-to-real\ntransfer. Finally, we describe the reward function used to guide training.\nNetwork Architecture. The MHC model is a Long Short-Term Memory (LSTM) recurrent neural\nnetwork which takes as input the current humanoid state $s_t$ and the next step of the current directive\n$d_i = (\\hat{q}_i, I_i)$. The MHC first processes the directive with a single-layer feed-forward encoder to\nproduce a 160 dimensional directive embedding. This is concatenated with $s_t$ and fed into an LSTM\nblock configured with two 64-unit recurrent layers. A final linear decoding layer outputs an offset for\neach actuated joint. The MHC action $a_t$ is computed by adding this offset to the actuated joint values\nin $\\hat{q}_i$, which yields the next PD setpoints. Note that for the masked joints in $d_i$, the corresponding\nvalues in $\\hat{q}_i$ are zero and the offset corresponds to the actual PD setpoint. In our work, we use the\nDigit V3 humanoid, which has 20 actuated joints. The MHC is run at 50Hz to compute PD setpoints\nwhich are sent to a PD controller running at 2kHz.\nTraining Approach. The MHC is trained via the PPO RL algorithm [26] in simulation using the\nMuJoCo physics engine with a Digit V3 humanoid model. Below we describe the way we generate\na motion dataset for creating directives, how we generated training episodes, the curriculum stages\nused for effective training, and domain randomization to facilitate sim-to-real transfer.\nMotion Data Generation. We create a diverse set of reference motions for training the MHC using a\ncombined dataset consisting of human motion capture (MoCap) datasets (AMASS [27], Reallusion\n[28]), and video demonstrations. To help bridge the human to Digit embodiment gap, we employ\nan inverse kinematics (IK) based retargeting procedure to map the dataset trajectories to Digit's\nkinematic model. For each frame in the dataset, we solve an IK problem for the generalized position\nvector of digit $q = (\\theta, b)$ formulated as a nonlinear program (NLP). We use the IK module in Drake\n[cite] to set up the costs and constraints associated with the NLP, and SNOPT [cite] as the underlying\nsolver. Critical kinematic feasibility requirements such as locking the stance foot to the ground and\nrespecting the closed kinematic chain topology of Digit's legs are expressed as constraints in task\nspace. Upper body motion targets that serve more of a stylistic purpose such torso pose, and hand\npositions are expressed using costs to aid in convergence. Any frames that the IK optimization\ndid not generate a feasible solution for are simply dropped. Linear interpolation is applied to the\nsequence of states $q_i$ and their timestamps $t_i$ to produce the final time-parameterized trajectory $Q_{1:H}$\nused for training.\nEpisode Generation. Each episode is initialized with the robot in a standing position. Next a random\ncommand window length $w$ is generated and a random directive $d$ is drawn from a distribution\ndefined by the curriculum stage. The MHC then cycles through $d$ until reaching $w$ steps. This\nsampling of a $w$ and $d$ continues until reaching the maximum episode length $e$, which depends on\nthe curriculum stage, or the robot falls. By switching between multiple random directives during an\nepisode, the MHC can learn to smoothly transition between different types of motion. In addition,\nto encourage robustness, during the execution of each episode we apply perturbations on the torso\naccording to a distribution that depends on the curriculum stage.\nCurriculum Stages. We employ a three-stage curriculum that progressively learns locomotion, ro-\nbustness to perturbations, and whole-body motion tracking. This approach enables the model to\ngradually acquire complex skills while ensuring stable learning."}, {"title": "5 Experimental Results", "content": "In this section, we present our evaluation protocol and simulation experiments to assess the MHC's\nperformance in executing diverse behaviors from full and partially-specified target motions. We\ncompare the MHC with baseline approaches and analyze the impact of training dataset diversity as\nwell as learning curriculum. Additionally, we discuss qualitative observations and the successful\nsim-to-real transfer of the MHC through real-world trials on the Digit V3 humanoid robot.\nMotion Directive Dataset. We used IK retargetting to generate 75 kinematic motion trajectories\nselected from our motion sources as described in Section 4. We aimed to select a diverse set of\nmotions, while avoiding aggressive motions that involve highly dynamic behavior such as jumping,\nkicking, and large sudden swinging of limbs. In order to test models on motions outside of their\ntraining sets, we divided the data into three sets, each containing a mix of motions from the original\nmotion sources.\n\u2022 setA [20 motions]: Amass Boxing (5), Amass misc (9), Reallusion (2), optimized (2), video (2)\n\u2022 setB [20 motions]: Amass Boxing (6), Amass misc (6), Reallusion (5), optimized (3)\n\u2022 setC [35 motions]: Amass Boxing (19), Amass misc (12), Reallusion (1), optimized (3)\nWhile each set contains some basic motions related to hand waving and simple upper body move-\nments, the general trend is an increasing level of difficulty as judged by the authors. We note that\nsetC has a number of the most difficult motions, involving highly dynamic motion, such as tennis\nsmashes and difficult boxing moves. The feasibility of these more difficult motions is unclear for\nour realistic Digit model.\nExperiment Setup and Metrics. Each experiment involves evaluating an MHC controller on one\nof the retargeted motion datasets. For each motion we create 3 motion directives corresponding\nto a whole body directive, upper body command with lower body standing directive, upper body\ncommand with lower body locomotion directive. For each motion directive we evaluate the MHC\nover 200 episodes, each having a maximum time of 50 seconds (2500 steps). Each episode starts\nfrom a randomized point in the directive and then repeatedly cycles through the directive three times\nbefore resetting to another randomly selected starting point. Based on these episode runs we evauate\nthe following metrics, which depend on the type of directive.\n\u2022 Failure Rate (Fail %): Percentage of episodes that result in failure (i.e. falling) before 50 seconds.\n\u2022 Mean per joint positional error (EMPJPE): Mean positional error for end effectors (hands, elbows,\nknees, feet) is calculated as the L2 norm of the current Euler position relative to the torso, com-\npared to values from the directive. For partial directives, the error is computed only for unmasked\njoints. This is done for both partial and fully-specified directives before episode termination.\n\u2022 Root Drift (Root\u2206): The mean of drift in root position during an episode from its commanded\nposition when following partially or fully-specified directives. Computed as an L2 norm of current\nroot position against expected root position in previous directive step.\nComparison to Baselines. There are no existing approach that handle the masked directive inputs\nof the MHC. Thus, we have developed two baselines to compare against: 1) Locomotion+Offset.\nThis controller is trained on the first two curriculum stages and only sees the locomotion commands\nat its input. At inference time, it receives locomotion commands from the directives and for other"}, {"title": "6 Limitations", "content": "One of the major current limitations is a significant sim-to-real gap. This is particularly prevalent for\nmotions where the feet are placed widely or where one foot remains in the air for an extended period.\nOur hypothesis is that this sim-to-real limitation requires research that is more generally focused on\nsim-to-real transfer, possibly via the incorporation of real-world data. The MHC has been a useful\nvehicle for highlighting these gaps. Another limitation of our current training approach is that we\nonly consider a limited set of masking patterns during training. Supporting fully general masking\nmay lead to additional robustness and open up new possibilities for authoring behaviors. Finally, the\nMHC is currently does not account for interaction with external objects, such as the box, during a\nbox pickup. Explicitly incorporating objects to be manipulated into the MHC input and training is\nan important direction for supporting practical applications."}, {"title": "Appendix A Dynamics Randomization", "content": "All models in our work were trained with a fixed dynamics randomization setup in order to trans-\nfer from simulation to real without learning the exact dynamics of the system. The ranges for all\nelements are given in Table 3."}, {"title": "Appendix B Implementation Details", "content": "MHC has been trained using actor critic based PPO algorithm. Table 4 specifies the hyperparameters\nused for each training cycle independent of the curriculum."}, {"title": "Appendix C Reward structure", "content": "Table 5 gives a detailed view of our complete reward function. Note that style rewards are only\nignored when following a fully-specified directive denoted by $I_i == 1$.\nFollowing notation has been used:\n\u2022 i indicates current time step\n\u2022 $I_i == 1$ indicated if all joints are being mimicked and its a whole body motion directive;\n\u2022 $v_{xy}$ = base linear velocity;\n\u2022 s = current robot parameter;\n\u2022 b = base acceleration;\n\u2022 $C_{feet}$ = default foot position xyz and orientation rpy;\n\u2022 $qd()$ = quaternion distance function;"}]}