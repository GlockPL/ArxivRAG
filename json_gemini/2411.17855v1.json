{"title": "\"Give me the code\u201d - Log Analysis of First-Year CS Students' Interactions With GPT", "authors": ["Pedro Alves", "Bruno Pereira Cipriano"], "abstract": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in computer science (CS) education is expected to be profound. Students now have the power to generate code solutions for a wide array of programming assignments. For first-year students, this may be particularly problematic since the foundational skills are still in development and an over-reliance on generative AI tools can hinder their ability to grasp essential programming concepts. This paper analyzes the prompts used by 69 freshmen undergraduate students to solve a certain programming problem within a project assignment, without giving them prior prompt training. We also present the rules of the exercise that motivated the prompts, designed to foster critical thinking skills during the interaction. Despite using unsophisticated prompting techniques, our findings suggest that the majority of students successfully leveraged GPT, incorporating the suggested solutions into their projects. Additionally, half of the students demonstrated the ability to exercise judgment in selecting from multiple GPT-generated solutions, showcasing the development of their critical thinking skills in evaluating AI-generated code.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have been shown to have the capacity to generate computer code from natural language specifications (Xu et al., 2022; Destefanis et al., 2023). Currently, there are multiple available LLM-based tools which display that behaviour. Two examples of such tools are OpenAI's ChatGPT 1 and Google's Bard 2.\nThe implications of such tools for CS education are obvious: students now have access to tools that can generate code to solve programming assignments, with the capacity to obtain full marks or close to it (Prather et al., 2023; Savelka et al., 2023a; Cipriano and Alves, 2023; Cipriano and Alves, 2024; Finnie-Ansley et al., 2023;\nOuh et al., 2023;\nSavelka et al., 2023c;\nSavelka et al., 2023b;\nReeves et al., 2023).\nThere has been extensive research into how computer science teachers should respond to LLMs, adapting their teaching methods, assessments, and more\n(Lau and Guo, 2023;\nDenny et al., 2023b;\nDaun and Brings, 2023;\nLeinonen et al., 2023;\nLiffiton et al., 2023;\nFinnie-Ansley et al., 2022;\nSridhar et al., 2023). Some resist (fight), contemplating ways to prevent students from using chatbots like ChatGPT, such as blocking access or employing detection tools for AI-generated text with questionable effectiveness (OpenAI, 2023). Others embrace this new paradigm, adapting exercises so that students are encouraged to make the most of LLMs, with presentations/discussions or non-text-based prompts (Denny et al., 2023b), or analyzing the tool's capacity to help students (Hellas et al., 2023).\nRegardless of the instructional strategy adopted by teachers, be it one of resistance or acceptance, students will inevitably turn to ChatGPT, perhaps with a certain degree of naivety, lacking knowledge about prompting techniques and an exaggerated faith in its responses. Moreover, most of their interactions will occur without direct teacher oversight. With this study, we want to understand what happens in these cases: whether they will succeed in their endeavors or fail miserably, and how can teachers help in their journey.\nWe investigate two research questions:\nRQ1: Can first-year students take advantage of"}, {"title": "LLMs for code generation without any specific or formal training?", "content": "RQ2: Are students able to incorporate solutions provided by ChatGPT into their projects?\nThis work makes the following contributions:\n\u2022 Proposes an exercise template that teachers may use to foster their students' critical thinking skills when interacting with LLM-based tools;\n\u2022 Analyzes the interactions of 69 first-year CS students with ChatGPT to produce code using the aforementioned template, but without having received any formal training on the matter. This analysis is based on log files provided by the students documenting their attempts to solve a specific exercise within the context of their course project;\n\u2022 Presents the results of a post-exercise survey (N=52) to find out the students' perceptions on its usefulness."}, {"title": "2 RELATED WORK", "content": "Some recent studies evaluated students' interactions with LLMs for code generation. One example, (Denny et al., 2023b) analyzed the interactions between students and ChatGPT in several dimensions such as prompt lengths and number of attempts, following a methodology where instead of asking the students to write the code themselves, they had to write prompts to generate the code for them. In (Prather et al., 2023), researchers studied how first year students use Copilot (a GPT-based code generation code, trained on code publicly available on GitHub) to solve a typical assignment on an introductory programming course, and found that these novice students struggle to understand and use Copilot, and are wary of the tool's implications (i.e. such as not being able to understand the generated code), but are optimistic about integrating the tool into their future development workflow. Another study (Prasad et al., 2023) analyzed the GPT interactions of students in a upperlevel course on applied logic and formal methods, using an IDE plugin. The authors of (Babe et al., 2023a) asked 80 students with a single semester of programming background to write prompts for 48 problems and found that the students' prompts can be ambiguous to LLMs. Finally, (Kazemitabaar et al., 2023) analyzed the interaction logs of learners aged 10 to 17, recruited from programming bootcamps, and verified how the availability of Codex (a GPT-based LLM) impacted their ability to independently learn Python. They found evidences of learner's self-regulation, with some learners actively adding code to test the AI-generated code, as well as over reliance, with some learners submitting the assignment's instructions in a single prompt.\nOur study differs from these studies since all of them analysed student's interactions using introductory programming exercises as the basis for the study. In our study, students had to generate code for a specific part of a large project (developed over the course of approximately 3 months), meaning that GPT's generated code would have to be integrated with the remaining of the students' code, thus having an extra layer of complexity."}, {"title": "3 EXERCISE TEMPLATE", "content": "This section describes a template for programming exercises to be solved using Generative AI tools (e.g., ChatGPT, Bard). The structured approach cultivates critical thinking (Naumova, 2023) and requires no prior training in prompt engineering techniques.\nThe problem can be stated in any form (textual, diagram, etc..) and the students must interact with an LLM to get a good solution, that they can efficiently apply (e.g., incorporate in their course project). Since LLMs are prone to failing (Babe et al., 2023b; Denny et al., 2023a), students must ask the LLM for two alternative implementations, a well-known prompting technique (Treude, 2023), which must be documented within a log file, to be submitted alongside the exercise solution. This log file must also include a concluding section wherein the chosen implementation must be elucidated, along with the rationale behind the selection. In summary, students must follow these steps:\n1. State the problem by asking a question to GPT\n2. Analyze the provided solution\n3. Ask for an alternative solution, eventually providing further information or constraints\n4. Compare both solutions\n5. Decide on the best solution\nAll these interactions should be recorded for further analysis.\nTo ensure uniformity in format and to clarify the expected nature of interaction, an exemplar log file was provided (shown in Listing 1)."}, {"title": "4 EXPERIMENTAL CONTEXT", "content": "This study was performed in the scope of a Data Structures and Algorithms (DSA) course belonging to a Computer Engineering degree, during the 2022/23 school year. The course takes place in the second semester of the first year, which means that most students have only been exposed to one semester of programming.\nThe course's main assignment is the development of a project, typically a command line application that performs queries on a very large data set, provided in the form of multiple CSV files. The queries must be implemented using efficient data structures and algorithms-for example, some queries will require the usage of hash tables to achieve good performance. Its development is split in two parts: in the first part, students mainly need to implement code for reading and parsing the input files, and, in the second part, they have to focus on implementing the different queries. This project is either implemented in groups of 2 students or individually.\nThe project's topic varies from year to year, as does the input file structure and the required queries. In this particular year, the project was about songs and artists, using data extracted from the Million Song data set\u00b3.\nNote that, in this course, the professors and instructors allow and even encourage students to interact with GPT. Also, the course's main project explicitly asks students to use GPT in some of the requirements. However, students were warned that they would not be allowed to use GPT during the project's defense at the end of the semester. The following section describes those requirements."}, {"title": "4.1 The ChatGPT assignment", "content": "To foster a controlled student experimentation of ChatGPT's capabilities and its integration into the educational framework, the project's first part includes a specific ChatGPT exercise.\nStudents were instructed to employ ChatGPT as an aid in the reading and parsing a large CSV file containing information about artists. The file's format is available. Some of the challenges related to this file were handling two different formats for single-artist songs and multiple-artist songs, handling multiple artists associated with the same song across distinct lines, dealing with invalid lines, among others. There were also technical restrictions: they couldn't use hash-based data structures (e.g., HashMap) in the first part.\nStudents validated their code using an automated assessment tool (AAT). They could submit multiple times without penalty and received the execution results of several unit tests."}, {"title": "5 METHODOLOGY", "content": "In order to better understand how the students interacted with ChatGPT to solve the ChatGPT assignment (see Section 4.1), we manually analyzed their interaction logs, categorizing them in several dimensions. Furthermore, a brief survey was administered subsequent to the assignment to ascertain the sentiments of the students towards the task."}, {"title": "5.1 Categorization", "content": "Since the students had been instructed to interact with ChatGPT using the exercise template described in Section 3, their logs mostly followed these steps: (1) ask a question; (2) ask for an alternative solution; and (3) provide a conclusion. Therefore, we decided to analyze each step individually: initial prompt, second prompt and conclusion. We added a fourth dimension called problem, to capture the diversity of the problems students were asking GPT to help them for.\nWe then further categorized each dimension into the following categorization variables:\n\u2022 Problem - What was the problem students were asking GPT help for?\n>Abstraction level (high or low) - Some students asked very concrete and direct questions such as 'How to remove spaces and quotes from a string in Java?' while others asked more abstract questions such as 'Implement a function in Java that read a .txt file with the following format (...)'\n>Nature (generic or domain-specific) - If the stated problem was or wasn't specific to the project. For example 'How to remove spaces and quotes from a string in Java?' is a generic problem, since it can be applied to a variety of problems and not specifically to the 'artists file parsing' problem\n\u2022 Initial prompt"}, {"title": "Second prompt", "content": ">Language - Since our students are native Portuguese speakers, the majority of the interactions were done in Portuguese, although a small percentage of students used English\n>Type - This defines the goal of the prompt\nAsk for code - These prompts usually included terms like 'implement a function...' or 'give me the code...' which inspired the title of this paper\nExplain how - Ask GPT to explain how they could solve a certain problem, without explicitly asking for code\nHelp with error - Students provided an error they were struggling with (e.g. a compilation error which they couldn't understand)\nAnalyze code - Students provided code from their project and asked GPT for errors or possible improvements in their code.\n>Provided context? - Some students copied or adapted part of the project statement explaining the rules behind the artists file, including them into the initial prompt.\n>Provided restrictions - Here we analyzed if the students provided ChatGPT with technical restrictions. In particular, we looked for the words 'java' and 'hashset/hashmap' in the prompt. The first was the language the project had to be developed in and the second was an explicit prohibition in the project statement (since at this point, they hadn't yet learnt these data structures).\n>Gave examples? - For prompts of the type 'ask for code', we analyzed if the students provided any examples to guide the model. We considered examples of input (e.g., lines in the artists file) and/or output (e.g., given these parameter values, the function should return this).\n>Function signature - For prompts of the type 'ask for code', we analyzed if the students provided the function signature beforehand (i.e., the name of the function along with its parameters and return type)."}, {"title": "Second prompt", "content": ">Type\nInexistent - Although the exercise template directed students to ask for alternatives, some students provided only the initial prompt\nGive me an alternative - We analyzed if the students asked for an alternative solution. This was one of the goals of this exercise.\nClarify initial prompt - We analyzed if the second prompt was just a clarification of the first prompt. This could be an attempt to generalize the previous solution or provide more information."}, {"title": "Conclusion", "content": "Ask different question - Some students didn't comply with the exercise statement and used the second prompt to ask for a different question\n>Guided alternative? - Here we wanted to find out if, only for the 'give me an alternative' cases, the students provided some guidance (e.g., 'Implement again without using replaceAll()') when asking for an alternative solution\n\u2022 Conclusion\n>Type\nDecided for one solution - We analyzed if the students achieved the main goal of the exercise, by choosing one of the alternative solutions given by ChatGPT\nUseful but didn't use the solution - Some students didn't use any of the solution but have written they still found the interaction to be useful (e.g., 'We didn't use any of the solutions. However we used some of the suggested instructions such as replaceAll()')\nNot useful - Some students didn't find any of the solutions useful\nInexistent/invalid conclusion - There are the cases where the students didn't write any conclusion or the conclusion seems to have no connection with the given prompts.\n>Decision criteria - This metric was applied only to the conclusions of the type 'Decided for one solution'. All the decisions fell into one of these categories: simpler/more compact, more flexible/generic, and easier to understand.\n>Used GPT solution - Here we wanted to find out if the students really used the GPT solution (complete ou partially) in their projects. We verified this by manually analyzing the project's code for each group, searching for code that is similar to one of the GPT solutions."}, {"title": "5.1.1 Classification example", "content": "As an example, the interaction shown in Listing 2 was classified like this:\nProblem: low abstraction level; general.\nFirst prompt: 'ask for code' type; no context given; provided a restriction (\"must be in Java\"); provided an example but didn't provide the requested function signature\nSecond prompt: inexistent\nConclusion: useful but didn't directly use the provided solution; we confirmed that their project actually used one of the suggestions made by ChatGPT regarding the use of replaceAll()."}, {"title": "5.2 Survey", "content": "To complement the log analysis results, in particular regarding the usefulness of the ChatGPT assignment (see Section 4.1), we conducted an anonymous questionnaire a few weeks after the completion of the assignment. Of the 154 enrolled students, 52 responded to the questionnaire, corresponding to a 33.77% participation rate.\nThe questionnaire was composed of the following 4 questions:\nQ1 - In part 1 of the project, you were asked to interact with GPT. Did you? [Yes/No];\nQ2 - If you answered \"No\" to the previous question, what is the reason for not interacting with GPT? [Open-ended];\nQ3 - If not for the exercise, would you still have used GPT? [Yes/No];\nQ4 - How useful do you think this exercise was (asking ChatGPT for help in processing the artists' file)? [Scale:1-5]."}, {"title": "6 RESULTS", "content": "65 groups (corresponding to 122 students) submitted a project that passed at least half of the assessment tests. From those, 37 groups (corresponding to 69 students), submitted a log file of the respective interaction with ChatGPT.\nWe manually analyzed each of those files both quantitatively and qualitatively. A public repository of these files is available."}, {"title": "6.1 Quantitative", "content": "We classified all the ChatGPT interaction logs following the criteria outlined in Section 5, for the 4 steps involved: problem, first prompt, second prompt and conclusion.\nAs illustrated by Table 1, the abstraction level of the problem presented to ChatGPT was evenly distributed between low and high, with a slight inclination toward the high level (52.8%). The majority of students posed domain-specific questions (64.9%) rather than generic ones. These results were expected, given the context of using ChatGPT within a specific project.\nRegarding the initial prompt (see Table 2), most of the students used their native language (91.7%) and asked for code (72.2%). Still, a small minority preferred to ask ChatGPT to explain how they could solve the problem (19.4%), and an even smaller fraction asked for help identifying errors (8.3%) or analyzing code (5.6%). Most students didn't provide any context (81.8%) but provided restrictions, mainly the restriction that it had to be in Java (81.3%). Also, the majority of the students didn't provide examples (71.9%) and almost no students provided the signature of the pretended function (92.9%). Notice that these last 2 criteria were only applied to interactions where the students asked for code (which most did).\nIn reference to the second prompt (see Table 3), a significant number of students didn't provide it (27.8%) which was surprising since the students were explicitly instructed to do so. Still, the majority asked for an alternative solution on the second prompt (61.1%) as instructed. Interestingly enough, a few students opted to use the second prompt to clarify what they wanted in the initial prompt (11.1%) and even fewer students just asked a different question (5.6%). For cases in which students requested an alternative solution, it is noteworthy that the majority did not provide any specific guidance for their request, merely asking for a generic alternative (71.4%). Only in 28.6% of the cases did the students provide some guidance by adding more information.\nFinally, with respect to the conclusion (see Table 4), almost half of the students accepted one of the solutions provided by ChatGPT (47.2%) and for those that haven't, a significant portion still found the interaction to be useful (25%), with only a small minority not getting any value from the interaction (11.1%). For the cases where the students accepted one of the solutions, the findings unveil a balanced distribution of the acceptance criteria. A substantial 35.7% of respondents valued simplicity and compactness as guiding factors. Similarly, an equal 35.7% sought flexibility and generality. Finally, 28.6% prioritized ease of understanding. Regardless of what the students wrote in the conclusion, we found that 72.2% really used (fully or partially) one of the solutions provided by ChatGPT.\nGeneral insights The exercise template encouraged a rigid methodology for using ChatGPT: students were instructed to use it as assistance in reading and parsing the artist file, obtain two alternative implementations, and apply critical thinking to choose one while justifying their decision. However, how students utilized it varied considerably. Some chose to focus on a highly specific problem (low abstraction level), as seen in Listing 2, while others posed more generic problems, with no clear trend towards either option.\nThe initial prompting methods employed by students also displayed a significant degree of diversity. This was expected as they had no formal training, and the project statement did not provide any guidance"}, {"title": "6.2 Qualitative", "content": "In this regard. Nonetheless, the majority of prompts were aimed at obtaining code, although a substantial portion of students also requested explanations on how to proceed. Due to their limited training, these prompts tended to be unsophisticated, lacking context, examples, or the pretended function signature. Even in terms of restrictions, while most students remembered to specify Java, few remembered to indicate that the use of hashsets was not allowed. However, this didn't stop most students from getting useful results (RQ1): specifically, 47.2% of participants acknowledged acceptance of one of the provided solutions, with an additional 25% expressing utility despite abstaining from utilization.\nA considerable number of students (38.9%) did not adhere to the project's instructions and failed to request an alternative implementation. The reasons for this remain unclear, but we suspect it may be due to (1) students not carefully reading the project statement, and (2) being an uncommon way to use ChatGPT.\nFor the students who did request an alternative solution, it is interesting to note that a substantial portion (28.6%) attempted to guide the solution by specifying constraints, despite receiving no indication to do so.\nIn the conclusions, it was expected that students would employ their critical thinking skills to choose one of the implementations, which occurred in half of the cases. Even in instances where this did not occur, a substantial portion of students found the interaction to be valuable. A confirmation of LLM's effectiveness of in assisting students is evident, with only 27,8% not being able to incorporate solutions provided by ChatGPT into their projects (RQ2).\nAlso, half of the students managed to obtain two alternative solutions, with no clear winner among the decision criteria."}, {"title": "6.2.1 The Database prompt", "content": "During the classification process, we found some cases worth discussing individually.\nGroup 36 provided the following initial prompt:\nI must create a Java database that receives three"}, {"title": "6.2.2 Incomplete first prompts", "content": "Some groups used the second prompt to complement the first prompt, since the results were not satisfactory. This was accounted in the 'clarify first prompt' item, in Table 3.\nFor example, Group 9 issued the following prompt: I want to read a line that has this format \"['Name1', 'Name2'].\" How can I check in Java if the given String follows this format?\nSince ChatGPT's response was specific for lines with two names, the group issued a second prompt asking for a more general solution: What if I have to handle more than two names, what do I do?\nThis second prompt could be prevented if the students had asked for a general solution in the first prompt, or provided several examples instead of just one. This confirms that some students cannot get the best results out of ChatGPT without formal training [RQ1]."}, {"title": "6.2.3 Arm wrestling with ChatGPT", "content": "Group 32 entered into a kind of \"arm wrestling\" with ChatGPT since they were not getting the answer they needed. They asked ChatGPT to explain what could be improved in a function they developed and that was probably not working as expected. ChatGPT suggested that they could remove some code duplication. However, this wasn't the cause of the error; in fact, there was no code duplication at all. After several prompts from the students, ChatGPT kept insisting the there was code duplication (possible \u201challucination\") and the students kept rebutting, without success. Here is one of the prompts:\nso, the issue is on a function that I showed you after you told me there was code duplication? Are you sure about this? what about the first piece of code i showed you? (...)\nOur analysis of the students' function revealed no code duplication issues, indicating that ChatGPT's answers were misleading. Interestingly, the students couldn't find the code duplication nor conceive that ChatGPT was wrong, so they kept pushing for more information.\""}, {"title": "6.2.4 Why am I failing the automated assessment tests?", "content": "Group 4's motivation for asking for help was the fact that they weren't passing an automated assessment test and they were not understanding why. That particular assessment test exercised edge cases in the requirements which were not taken proper care in their implementation such as duplicate ids and missing information. Notice that the source code of the automated tests wasn't available to the students.\nAfter describing the artists file format, the students asked ChatGPT for examples of input files that would cover all possible scenarios. Although ChatGPT provided some examples, they were not exhaustive so it didn't help them pass the tests. The students kept pushing ChatGPT for more scenarios with no success. At a certain point, they switched their strategy, explaining to ChatGPT how they had implemented the function and asking for what could be missing. Still, ChatGPT wasn't able to help them.\nWe found interesting that this group opted to implement the function by themselves, abstaining from assistance provided by ChatGPT. Subsequently, upon encountering difficulties in passing the automated assessment tests, they reached out to ChatGPT, albeit without success."}, {"title": "6.3 Survey", "content": "This section presents the results of the survey conducted after the students completed the exercise (N=52).\n84.6% (44) of the students replied that they interacted with GPT during the project (Q1). The students that did not interact (15.4%, 8) provided two main reasons for not doing so (Q2): 6 students replied \"I didn't feel the need to\", while 2 students replied \"I didn't know how to\".\nQuestion (Q3) tried to understand if the students would have used GPT even if there was no requirement for doing so. Most students, 69.2% (36) indicated that they would have used GPT anyway, but 30.8% (16) indicated that they would not have used"}, {"title": "7 LESSONS LEARNT", "content": "GPT unless asked to do so. This surprised us, as we expected more students to indicate they would have used GPT anyway.\nThe fourth question (Q4) aimed to assess the perceived utility of the ChatGPT assignment within the framework of students' projects. Among the respondents, 5.8% (3) selected Option 1 ('Useless'), while 21.2% (11) opted for Option 2 ('Slightly useful'). Options 3 and 4 were each chosen by 30.8% (16) of participants. Additionally, Option 5 (\u2018Very useful') was selected by 11.5% (6) of students. These findings indicate a prevalent perception of usefulness among the majority of students, although opinions vary considerably. However, it is noteworthy that a substantial proportion (27%) regarded the exercise as minimally or not useful. We hypothesize that this subgroup may include students who struggled to elicit useful solutions from GPT to apply in their projects.\nIn conclusion, a vast majority of students (84.6%) will use GPT in an assignment if directed to do so, and most students will use it even if it's not asked of them, but the percentage is lower (69.2%). Most students (73%) found the assignment to be useful or very useful.\nBased on this experiment, we leave some recommendations for teachers wanting to incorporate LLMs in their classes.\nIntegrate prompt training into their curriculum, focusing on providing richer information to LLMs, such as the problem context, examples, function signatures, etc. Literature suggests that providing examples tends to improve LLM's results (Brown et al., 2020);\nIncorporate designated LLM-based exercises into students' projects, encouraging the use of these tools as aids for tackling complex problems. Certain students may experience discomfort in utilizing LLMs, either due to a lack of familiarity with their operation or apprehension regarding their appropriateness for use. In such cases, they may benefit from gentle encouragement or guidance to overcome these barriers.\nUse exercise templates that guide students into approaching LLMs with a critical thinking mindset. This can be done by asking for multiple solutions and selecting one of them using criteria such as flexibility, compactness or ease of understanding. Evaluating multiple LLM-generated solutions is a skill that we, as well as other researchers (Treude, 2023; Alves and Cipriano, 2023), believe will be of great importance in the future."}, {"title": "8 LIMITATIONS", "content": "GPT's behaviours are not deterministic. Furthermore, research has shown that they are dynamic, and can vary greatly over time (Chen et al., 2023). As such, it is hard to generalize conclusions, since some students might have had better results than other students, not due to lack of 'prompting skill', but due to the models themselves.\nWe expect most of the students to have used ChatGPT with GPT model version 3.5, due to it being free. However, some students might have used the GPT-4 model. Since we do not have this information, our results were not controlled for it."}, {"title": "9 Conclusions", "content": "The proposed exercise template mostly achieved its goal of fostering students' critical thinking when interacting with LLMs, enabling them to generate more refined solutions. Our log analysis shows that most students can effectively use GPT without formal training. With 72.2% of students incorporating ChatGPT's solutions into their projects, we consider the exercise successful. Additionally, our survey (N=52) indicates that 73% of students found the exercise useful, with some stating they wouldn't have used GPT otherwise. However, the limited sophistication in prompting highlights the need for further training, as students often required many prompts or failed to achieve satisfactory results.\nMore research is needed to devise effective strategies for instructing CS students in the proper utilization of these tools. It is crucial to convey an awareness of their limitations and discourage over-reliance, ultimately better preparing for professional life."}]}