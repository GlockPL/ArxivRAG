{"title": "A Multiple-Fill-in-the-Blank Exam Approach for Enhancing Zero-Resource Hallucination Detection in Large Language Models", "authors": ["Satoshi Munakata", "Taku Fukui", "Takao Mohri"], "abstract": "Large language models (LLMs) often fabricate a hallucinatory text. Several methods have been developed to detect such text by semantically comparing it with the multiple versions probabilistically regenerated. However, a significant issue is that if the storyline of each regenerated text changes, the generated texts become incomparable, which worsen detection accuracy. In this paper, we propose a hallucination detection method that incorporates a multiple-fill-in-the-blank exam approach to address this storyline-changing issue. First, our method creates a multiple-fill-in-the-blank exam by masking multiple objects from the original text. Second, prompts an LLM to repeatedly answer this exam. This approach ensures that the storylines of the exam answers align with the original ones. Finally, quantifies the degree of hallucination for each original sentence by scoring the exam answers, considering the potential for hallucination snowballing within the original text itself. Experimental results show that our method alone not only outperforms existing methods, but also achieves clearer state-of-the-art performance in the ensembles with existing methods.", "sections": [{"title": "1 Introduction", "content": "Generative large language models (LLMs) often fabricate text that contradicts or is not grounded against real-world information. This harmful phenomenon is known as Factuality hallucination (hereinafter simply \u201challucination\u201d) (Huang et al., 2023). As LLMs are increasingly adopted for a variety of language-related tasks in daily life and industry, hallucination detection in LLMs is essential to ensure trustworthiness (Sun et al., 2024).\nExisting detection methods can be categorized into those that (a) retrieve external facts, (b) analyze LLM's internal state, and (c) use only LLM's input/output (i.e., zero-resource black-box detection) (Huang et al., 2023). Although each has different pros and cons, this work focuses on type (c), which does not require an external knowledge base and can also apply to LLMs used via only WebAPIs and to domain-specific fine-tuned LLMs. Among several existing type (c) methods (Agrawal et al., 2023; Anonymous, 2024b,a; Cohen et al., 2023 as listed in A.2), SelfCheckGPT-Prompt (hereinafter \u201cSCGP\") is a reproducible and peer-reviewed state-of-the-art (SOTA) method (Manakul et al., 2023). SCGP utilizes the nature that hallucinatory text typically exhibits low robustness; i.e., regenerating the consistent text is probabilistically challenging. Consequently, SCGP uses LLMs to determine whether the original text is semantically supported by each of the probabilistically regenerated texts from the same prompt. Sentences that lack support are more likely to be considered as hallucinations.\nA significant issue for SCGP is that the storyline of each regenerated text changes, which leads to incomparable sentences in the original text, particularly in the latter part, as exemplified in Figure 1. These incomparable sentences worsen detection accuracy because they are determined as hallucinations even when they are not. The changes in the storyline are not easy to deal with, as they are a mixture of those caused by topic picking and hallucination snowballing (hereinafter simply \u201csnowballing\u201d) (Zhang et al., 2023). Snowballing is the phenomenon that LLMs over-commit to early mistakes, which leads to more mistakes that they otherwise would not make. Contrary to mere topic picking, subsequent sentences in snowballing are highly likely to be hallucinations (cf. F).\nIn this paper, we propose a novel zero-resource hallucination detection method that incorporates a multiple-fill-in-the-blank exam (FIBE) approach for the above storyline-changing issue. Figure 2 shows an example of our FIBE approach. First, instead of merely regenerating, (1) creates a multiple-fill-in-the-blank exam by masking multiple objects from the original text. Second, (2) prompts an LLM to"}, {"title": "2 Notation", "content": "ri is the i-th sentence in original LLM response text R generated from prompt P. A hallucination detection method H predicts hallucination score H(i) \u2208 [0,1] of ri. Ideally, the more hallucinatory ri is, the higher H(i) should be. Variants are distinguished by the subscript of H. For example, existing method SCGP is denoted as Hp(i)  def = N-1\u03a3(1-supported(ri, sample (P))); where sample'(P) = Si is the j-th probabilistically regenerated text from prompt P, N is the maximum sample count, and supported(ri, Si) \u2208 [0,1] is the value so high that ri is supported by text Si. Function supported is realized with the LLM prompt in E.2."}, {"title": "3 Methodology", "content": "In the SCGP, if the storyline-changing occurs in regenerated text Si and sentence ri is no longer comparable to Si as in Figure 1, Hp(i) predicts that ri is hallucinatory even if it is not because supported(ri, Si) can only take a low value. Accordingly, we propose FIBE approach to forcefully regenerate comparable sentences with each ri. Furthermore, we propose DQ and SBC approaches to consider snowballing that occurred within original text R itself."}, {"title": "3.1 Fill-in-the-Blank Exam (FIBE)", "content": "As shown in Figure 2, FIBE regenerates sentences that match other constructions, such as subjects and verbs, by creating a multiple-fill-in-the-blank exam with multiple objects masked in original text R and prompting an LLM to repeatedly answer it. Here, the objects appearing before the subject in each sentence are not masked to prevent topic picking.\nFIBE is denoted as HF df = 1 \u2212 (100N)\u2212\u00b9\u03a3score(answer (create(R, P), P), ri); where create(R, P) = E is the exam based on text R under the context of P, answer (E, P) = a is the i-th sentence of the j-th answer for exam E under the context of P, and score(a, ri) \u2208 [0,100] is the value so high that answer a is consistent with ri. Functions create, answer, and score are realized with the LLM prompts in E.3.1, E.3.2, and E.3.3, respectively. Here, SCGP's supported(ri, Si) compares a sentence with a text, whereas FIBE forcefully obtains comparable sentence a, so that score(a, ri) can compare a sentence with a sentence. This considerably reduces the size of prompt tokens."}, {"title": "3.2 Direct Question (DQ)", "content": "If snowballing occurs in original sentence ri, and if it occurs in the exam answer a as well, score(a, ri) predicts that ri is fact. Therefore, DQ prompts the LLM to answer directly whether original sentence ri is hallucinatory or not, excluding the influence of the preceding sentences r<i. DQ is denoted as HD(i) def = 1 \u2013 known(ri, P); where known(ri, P) is the value so high that the LLM is convinced that ri is fact based on its prior knowledge under the context of P. Function known is realized with the LLM prompt in E.4."}, {"title": "3.3 Snowballing Correction (SBC)", "content": "If snowballing occurs in original text R, the more its former sentences are hallucinatory, the more likely the latter sentences are also hallucinatory. Therefore, in SBC, the hallucination scores in the former part add up to the latter part. SBC is denoted as Hs(i; H,0) def = clip(H(i) + |R|-1max(0, \u03a3\u0397(k) \u2013 0)); where H is arbitrary detection method, |R| is the number of sentences in R, O is a constant hyperparameter for adjusting the effectiveness of this correction, and clip(n) is the function to round n in [0, 1]."}, {"title": "3.4 Ensembles", "content": "We define the ensemble of multiple detection methods other than SBC as a clipped weighted sum; i.e., H+(i) def = clip(CFHF(i) + CDHD(i) + CPHP(i)); where CF, CD, and Cp are the constant weights that are hyperparameters. We also define the ensemble with SBC as a function composite; i.e., H\uff61(i;0)  def = Hs(i; H+,0)."}, {"title": "4 Experimental Evaluation", "content": ""}, {"title": "4.1 Experimental Details", "content": "Dataset: We used the WikiBio GPT-3 Hallucination Dataset v3 (Manakul, 2023) for evaluating zero-resource black-box detection methods. This dataset originally provides a total of 1,908 sentences in 238 original texts generated by GPT-3 (text-davinci-003) using the prompt template \u201cThis is a Wikipedia passage about {concept}:\"; where the placeholder concept is replaced by one out of 238 person names. However, we excluded 2 texts because their sentences were originally misdivided in the middle of proper nouns (cf. D). Thus, 1,893 sentences of 236 texts were evaluated in this experiment. Each sentence is manually annotated with 3 levels of hallucination intensity; Major Inaccurate, Minor Inaccurate, and Accurate. This dataset also provides probabilistically regenerated texts using the same GPT-3.\nTasks and Indicators: We evaluated each method on 3 tasks, NonFact, NonFact*, and Factual, which involved binary classification of each sentence in the original texts. NonFact is the task to classify Major/Minor Inaccurate and others, NonFact* is for Major Inaccurate and others, and Factual is for Accurate and others. Then, we quantified the single run accuracy of each task using AUC-PR and AUC-ROC (cf. 6.1). Note that the AUC-ROC of the NonFact and Factual are always the same.\""}, {"title": "4.2 Experimental Result", "content": "RQ1: Does the proposed method outperform the existing method SCGP in detection accuracy? Table 1 shows the all indicators of the evaluated tasks. FIBE alone is inferior to SCGP* in only Factual AUC-PR, but superior to both SCGP+ and SCGP* in all 5 indicators when combined FIBE with DQ or/and SBC. In contrast, the Factual AUC-PR of SCGP* is rather degraded when combined with DQ or/and SBC. Therefore, DQ and SBC are complementary approaches to FIBE. The ensemble of FIBE and SCGP* is the highest in all 5 indicators, that is they are also complementary.\nRQ2: What factors make the proposed method and the ensemble outperform the SCGP? Figure 3 shows that each method has different sentence positions in which it excels. FIBE alone outperforms SCGP* in all 5 indicators when just only classifying from the first to the middle sentences. This indication supports our hypothesis that the SCGP accuracy tends to be degraded due to the storyline-changing during text regeneration. The combined use of DQ or/and SBC has the effect of improving accuracy for FIBE when classifying from the first to the last sentences. This indication supports our hypothesis that the FIBE accuracy tends to be degraded due to the snowballing during original text generation; i.e., if snowballing produces an irrelevant sentence in the original text's latter half, FIBE's \"forcing comparable sentences\u201d action is ineffective. Finally, the combined use of SCGP* has improved accuracy in the first sentences. This is also the factor of the outperformance."}, {"title": "5 Conclusion", "content": "FIBE, DQ and SBC approaches, that we propose in this paper for zero-resource hallucination detection, enable more precise comparative analysis against the storyline-changing issue. We encourage future work to evaluate them in more diverse LLM use cases; e.g., RAG (Gao et al., 2024)."}, {"title": "6 Limitations", "content": ""}, {"title": "6.1 Evaluation Indicators", "content": "We omitted the use of any passage-level indicators used by SelfCheckGPT (Manakul et al., 2023). Because, the order of their values was completely consistent with the order of sentence-level indicator AUC-PR. By contrast, we added AUC-ROC into our indicators, which differs from AUC-PR in its curve shape trade-off (cf. C). Because, we found that AUC-PR becomes too high when the number of unique observed values is low like SCGP."}, {"title": "6.2 Diversity of Experiments", "content": "This work lacks the diversity of benchmark datasets and LLMs. The WikiBio GPT-3 Hallucination Dataset v3 (Manakul, 2023) we used contains only 238 English texts like Wikipedia biographic articles, which are generated from the same prompt template. Therefore, we should evaluate the external validity of our method using more diverse prompts and topics; e.g., using the PopQA dataset (Mallen et al., 2023). Although we used only GPT-3.5 (OpenAI, 2022) as LLMs in this work, the architecture of our method is not limited to GPT-3.5. Therefore, we should assess the accuracy when using each of the commonly available LLMs; e.g., Llama 2 (Touvron et al., 2023)."}, {"title": "6.3 Hallucinations in our method itself", "content": "We should investigate the impact of the hallucinations created by our method itself. In particular, the hallucinatory number generated from quantification prompt score has a direct impact on accuracy. Increasing the number of resampled texts can be expected to mitigate such impacts. This work was done with 5 samples and SelfCheckGPT done with a maximum of 20 samples. Furthermore, we should investigate the impact of the stochastic fluctuations of LLM output in our method. The random seed was fixed to 0 when our method used GPT-3.5 in this work, and the minor version of GPT-3.5 was fixed at gpt-3.5-turbo-16k-0613 (cf. E). However, in order to assess the robustness of differences in random seeds, we should quantify the multiple run accuracy using multiple random seeds."}, {"title": "6.4 Mathematical Theories", "content": "This work lacks any mathematical theories. We just use prompt score in E.3.3 to make an LLM compare exam sentences with the original sentence. Of course, we also tried many scoring approaches; e.g.,"}, {"title": "6.5 Hyperparameter/Prompt Tuning", "content": "The fixed common hyperparameters for our experiment listed in 4.1 were determined empirically, not optimized for each baseline. In particular, the fixed weights CF and Cp when ensembling FIBE and SCGP were set to 0.5, which takes a simple average to eliminate arbitrariness. However, the possibility of overfitting to a specific baseline/benchmark cannot be ruled out from the experimental result with only one dataset in this paper alone. Also, we should investigate the impact of the LLM parameters for each prompt. In this work, we used different temperature and top_p parameters of GPT-3.5 for each prompt in order to stabilize the instruction-following results (cf. E). The create and answer prompts contain one-shot for exemplification of input/output formats (cf. E.3.1 and E.3.2). There is also the possibility that the one-shot is overfitting."}, {"title": "6.6 Performance Evaluation", "content": "As this paper focuses on accuracy, performance evaluation is lacking. Nevertheless, this work only used GPT-3.5 via Web API, so few computational resources are required. FIBE basically has a longer waiting time than SCGP due to the time required to create an exam. By contrast, FIBE consumes fewer tokens than SCGP because prompt score(a, ri) does not require whole regenerated text Si, unlike prompt supported(ri, Si). FIBE requires 1 + N + |R| times LLM completions per original text, DQ for R, and SCGP for N + N|R| times; where N is the number of text regenerations and |R| is the number of original sentences."}, {"title": "7 Ethics Statement", "content": "We acknowledge and ensure that this work is compatible with the ACL Code of Ethics. We note that if hallucinatory sentences are not detected, it could lead to misinformation. The WikiBio GPT-3 Hallucination Dataset v3 we used is available on Hugging Face under the CC-BY-SA-3.0 license (Manakul, 2023). Our first author manually checked all 238 people who were the topic of each article in this dataset to ensure that they were well-known persons who did not need to be anonymized.\nWe used AI assistant GPT-4 (OpenAI, 2022) to check the English grammar of this paper."}, {"title": "A Detailed Related Work", "content": "This section describes the relevance of existing studies that have not been discussed so far."}, {"title": "A.1 LLM Hallucinations", "content": "In addition to Factuality hallucination, which is the main target of this work, there are various other types of hallucinations. Faithfulness hallucination means that LLM's output is inconsistent with the prompt or intermediate outputs, also known as Intrinsic hallucination (Huang et al., 2023). Extrinsic hallucination means that LLM's output is unverifiable from the prompt (Cao et al., 2022). These hallucinations can be detected directly by matching prompts and (intermediate) outputs, as in (Adlakha et al., 2023) and (Anonymous, 2024c). Although our method does not directly support these hallucinations, if they exhibit low robustness like Factuality hallucination, our method can consequently detect them."}, {"title": "A.2 Zero-Resource Black-box Hallucination Detection", "content": "Several zero-resource black-box hallucination detection methods have been proposed since SelfCheckGPT was published; however, many of them were under peer review during this work.\nSCGP is the last variant added to the SelfCheck-GPT series. Because the SCGP performs better than any other variants (Manakul et al., 2023), we did not conduct experiments on the other variants.\nSelf-Contradictory (Anonymous, 2024b) can be regarded as a \u201csingle\u201d-fill-in-the-blank approach and is expected to mitigate the effects of topic picking to some extent; however, there are no approaches against snowballing in the original text.\nIn comparison with only figures reported in existing papers, the ensemble \u201cFIBE, SBC, DQ\" outperforms the Self-Contradictory in (Anonymous, 2024b), and even \u201cFIBE, SCGP*, SBC (, DQ)\u201d also outperforms the WikiBio+Prompt (this is not a zero-resource method because it uses external knowledge) in (Manakul et al., 2023).\nDirect Query in (Agrawal et al., 2023) is similar to our DQ in that it directly asks the LLM for the validity of a single sentence (precisely one bibliography); however differs in that it also refers to the original prompt to spot snowballing."}, {"title": "B Implementaion Details", "content": "We implemented the proposed method as a Python 1 tool. The OSS scikit-learn (Pedregosa et al., 2011) was used to calculate the AUC values for each of the evaluation indicators."}, {"title": "C Detailed Evaluation Result", "content": "Of our experimental result, the PR and ROC curves for NonFact, NonFact*, and Factual tasks are shown in Figures 4, 5, 6, 7, 8, and 9, respectively."}, {"title": "D Originally Misdivided Sentences", "content": "Figure 10 shows the sentences we excluded in our experiment due to originally misdivided in the middle of proper nouns. Of course, the same original sentences can be found directly in the WikiBio GPT-3 Hallucination Dataset v3 (Manakul, 2023). We need to carefully consider how to handle the sentence-level hallucination evaluation of such misdivided sentences."}, {"title": "E Complete Prompts", "content": "This section describes the actual prompt templates used in our experiment and examples of their executions for passage No.28,011 (about \u201cBryan McClendon\u201d).\nNote that:\n\u2022 The names of persons and institutions and their relationships exemplified in the following prompts may not be true\n\u2022 Expressions that allow one to guess the author (us) are anonymized\n\u2022 Line breaks are inserted into each example as needed due to space limitations\n\u2022 Some special characters are replaced with one-byte symbols for display/printing purposes\n\u201c--------------------------------\u201d in each prompt denotes a role switch 2\n\u2022 \u201c{...}\u201d in each prompt denotes a placeholder"}, {"title": "E.1 Resampling Dataset", "content": "The LLM prompt was used for resampling the comparison texts in the WikiBio GPT-3 Hallucination Dataset v3 (Manakul, 2023) with GPT-3.5 (OpenAI, 2022).\nLLM parameters:\n\u2022 model version = gpt-3.5-turbo-16k-0613\n\u2022 temperature = 1.0 (same value as (Manakul, 2023))\n\u2022 top_p = 1.0\n\u2022 random seed = 0"}, {"title": "E.2 SelfCheckGPT-Prompt (SCGP)", "content": "Function supported(ri, Si) uses the below LLM prompt to do a binary determination of whether regenerated text Si supports original sentence ri under context P. If the LLM completion is \u201cYes\u201d, the function returns 1.0; if \u201cNo\u201d, returns 0.0; otherwise, returns 0.0. Additionally, if the LLM output contains each occurrence probability pk \u2208 [0, 1] of the k-th output token as GPT-3.5 does, the function returns p\u2081 in case of \"Yes\", returns 1 P1 in case of \u201cNo\u201d; otherwise, returns 0.0.\nLLM parameters:\n\u2022 model version = gpt-3.5-turbo-16k-0613\n\u2022 temperature = 0.0 (to eliminate stochastic fluctuations)\n\u2022 top_p = 0.0\n\u2022 random seed = 0"}, {"title": "E.3 Multiple-Fill-in-the-Blank exam (FIBE)", "content": ""}, {"title": "E.3.1 create(R, P)", "content": "Function create(R, P) uses the below LLM prompt to extract words (objects) from original text R to be fill-in-the-blank questions. The function replaces the extracted objects with variable names based on their hypernyms extracted together, such as \u201c$year_20\u201d, to create a fill-in-the-blank exam E. Here, the objects appearing before the subject extracted together in each sentence are not masked to prevent topic picking.\nLLM parameters:"}, {"title": "E.3.2 answer(E, P)", "content": "Function answer(E, P) uses the below LLM prompt to fill in the blanks in the exam E. This prompt is batchable, and the number of simultaneous completions can be set by the parameter n in GPT-3.5.\nLLM parameters:\n\u2022 model version = gpt-3.5-turbo-16k-0613\n\u2022 temperature = 0.5 (to ensure successful instruction following)\n\u2022 top_p = 1.0\n\u2022 random seed = 0"}, {"title": "E.3.3 score(a, ri)", "content": "Function score(a, ri) uses the below LLM prompt to score the consistency of original sentence ri with answer a on a 100-point scale. This prompt can also score multiple examinees' answers aal\u2264\u2264N together.\nLLM parameters:\n\u2022 model version = gpt-3.5-turbo-16k-0613\n\u2022 temperature = 0.0 (to eliminate stochastic fluctuations)\n\u2022 top_p = 0.0\n\u2022 random seed = 0"}, {"title": "E.4 Direct Question (DQ)", "content": "Function known(ri, P) uses the below LLM prompt to do a binary determination of whether the prior knowledge of the LLM supports original sentence ri under context P. If the LLM completion is \u201cYes\u201d, the function returns 1.0; if \u201cNo\u201d, returns 0.0; otherwise, returns 0.0. Additionally, if the LLM output contains each occurrence probability pk \u2208 [0, 1] of the k-th output token as GPT-3.5 does, the function returns p\u2081 in case of \"Yes\", returns 1 P1 in case of \u201cNo\u201d; otherwise, returns 0.0.\nLLM parameters:\n\u2022 model version = gpt-3.5-turbo-16k-0613\n\u2022 temperature = 0.0 (to eliminate stochastic fluctuations)\n\u2022 top_p = 0.0\n\u2022 random seed = 0"}, {"title": "F Hallucination Snowballing Example", "content": "Although \u201cStan Heal\u201d is a well-known Australian rules footballer 3, LLM outputted \u201cAmerican former professional basketball player\u201d in the first sentence, in consequence, fabricated all subsequent sentences as hallucinations."}]}