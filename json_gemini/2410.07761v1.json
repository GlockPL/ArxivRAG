{"title": "Jump Your Steps: OPTIMIZING SAMPLING SCHEDULE OF DISCRETE DIFFUSION MODELS", "authors": ["Yonghyun Park", "Chieh-Hsin Lai", "Satoshi Hayakawa", "Yuhta Takida", "Yuki Mitsufuji"], "abstract": "Diffusion models have seen notable success in continuous domains, leading to the development of discrete diffusion models (DDMs) for discrete variables. Despite recent advances, DDMs face the challenge of slow sampling speeds. While parallel sampling methods like T-leaping accelerate this process, they introduce Compounding Decoding Error (CDE), where discrepancies arise between the true distribution and the approximation from parallel token generation, leading to degraded sample quality. In this work, we present Jump Your Steps (JYS), a novel approach that optimizes the allocation of discrete sampling timesteps by minimizing CDE without extra computational cost. More precisely, we derive a practical upper bound on CDE and propose an efficient algorithm for searching for the optimal sampling schedule. Extensive experiments across image, music, and text generation show that JYS significantly improves sampling quality, establishing it as a versatile framework for enhancing DDM performance for fast sampling.", "sections": [{"title": "INTRODUCTION", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2021b; Ho et al., 2020; Song et al., 2021a; Karras et al., 2022) have achieved remarkable success in generation tasks within the continuous domain. However, certain modalities, such as text and music, inherently possess discrete features. Recently, discrete diffusion models (DDMs) (Austin et al., 2021; Campbell et al., 2022; 2024; Gat et al., 2024) have demonstrated performance comparable to state-of-the-art methods in various areas, including text (Lou et al., 2024; Shi et al., 2024) and image (Chang et al., 2022; Gu et al., 2022) generation. Nevertheless, like their continuous counterparts, DDMs encounter a significant bottleneck in sampling speed due to their progressive refinement process.\nIn contrast to continuous-domain diffusion models, where sampling dynamics are driven by sample-wise differential equations (Song et al., 2021b), allowing for the direct application of well-established numerical methods to accelerate generation, enhancing speed in DDMs poses a significant challenge. To address this, researchers have proposed fast and efficient samplers, including notable methods such as the T-leaping (Campbell et al., 2022; Lezama et al., 2022; Sun et al., 2023) and k-Gillespie algorithms (Zhao et al., 2024), which facilitate parallel sampling of multiple tokens in a single step. However, this parallel but independent sampling introduces Compounding Decoding Error (CDE) (Lezama et al., 2022), which arises from a mismatch between the training and inference distributions of intermediate latents during parallel sampling. Specifically, while each token is generated according to its marginal distribution, the joint distribution deviates from the learned distribution. To mitigate this issue, the predictor-corrector (PC) sampler (Campbell et al., 2022) has been proposed. This sampler slightly perturbs the generated data to correct incorrectly generated tokens. However, these methods have limitations, including impracticality under low computational"}, {"title": "BACKGROUND", "content": ""}, {"title": "CONTINUOUS TIME FRAMEWORK FOR DISCRETE DIFFUSION MODELS.", "content": "Discrete diffusion models (DDMs) define the generative process as the reverse of the data-corrupting forward process, expressed as a Continuous Time Markov Chain (CTMC) on a finite state space S (Campbell et al., 2022). For the data-corrupting process $(X_t)_{t\\in[0,T]}$, the density evolution is described as:\n$q_{t+dt|t}(y | x) = \\delta_{xy} + R_t(x,y)dt + o(dt)$\nHere, $\\delta_{xy}$ is the Dirac delta function, $R_t \\in \\mathbb{R}^{S\\times S}$ is the transition rate matrix of the forward CTMC, with $S = |S|$, and $dt > 0$. Rate matrices ensure the marginal distribution $q_t(x_t) = \\int q_t(x_t|x_0)q_0(x_0)dx_0$, where $q_0 = P_{data}$ and $q_T \\approx \\pi$, the stationary distribution of the forward"}, {"title": "SAMPLING FROM THE BACKWARD CTMC", "content": "Gillespie's Algorithm was proposed as a simulation algorithm for a given CTMC (Gillespie, 2007). Gillespie's algorithm simulates the CTMC by calculating the rate matrix at each state transition. If the rate matrix of the CTMC depends only on the state, Gillespie's algorithm serves as an exact simulation method. However, since it allows for only one token transition each time the rate matrix is calculated, it is computationally inefficient.\nk-Gillespie's Algorithm Instead of updating only one token for each rate matrix calculation, the k-Gillespie's algorithm (Zhao et al., 2024) updates k tokens in parallel. This reduces the computation by a factor of 1/k compared to the original Gillespie algorithm.\nT-Leaping On the other hand, Campbell et al. (2022) proposes sampling through $\\tau$-leaping. Unlike the k-Gillespie algorithm, which update k tokens in parallel, $\\tau$-leaping simultaneously updates all tokens according to the given fixed rate matrix within the specified time interval $[t, t + \\tau)$. Recently, Tweedie $\\tau$-leaping, which considers changes in the rate matrix according to the noise schedule, has been proposed (Sun et al., 2023; Lou et al., 2024)."}, {"title": "OPTIMIZING THE SAMPLING SCHEDULE OF DISCRETE DIFFUSION MODELS", "content": "In this section, we aim to optimize sampling schedule ${T \\rightarrow t_1 \\rightarrow t_2 \\rightarrow \\dots \\rightarrow t_{N-1} \\rightarrow 0}$ to minimize the Compounding Decoding Error (CDE) introduced by parallel sampling. First, we define and analyze the CDE, examining its relationship to both the sampling schedule (Section 3.1) and sampling quality (Section 3.2). In Section 3.3, we derive an upper bound on the CDE, which serves as the objective for the sampling schedule optimization. Finally, we introduce a hierarchical breakdown strategy (Section 3.4) and computational techniques (Section 3.5) to make the optimization tractable.\nAlthough this section focuses on samplers based on $\\tau$-leaping, all methods are also applicable to the k-Gillespie algorithm.\nNotations To begin, we introduce some essential mathematical notation. $X$: a random variable, $x$: its observation, $P, Q$: distributions, ${T \\rightarrow t_1 \\rightarrow \\dots \\rightarrow t_{N-1} \\rightarrow 0}$ : sampling schedule, and $Q_{a\\rightarrow b + \\dots + c}$ : the distribution generated by the sampling schedule ${a \\rightarrow b + \\dots + c}$. For clarity, when working with backward CTMCs, we slightly abuse notation and express intervals as $[s,t] = {u | s \\ge u \\ge t}$; the same applies to open and half-open intervals."}, {"title": "TIME-DEPENDENT NATURE OF COMPOUNDING DECODING ERRORS", "content": "We introduce a measure for the Compounding Decoding Error (CDE), $\\mathbb{E}_{CDE}$, which quantifies the discrepancy between the true joint distribution and the distribution from parallel token generation."}, {"title": "RELATION BETWEEN COMPOUNDING DECODING ERRORS AND GENERATION QUALITY", "content": "While the Eq. (3) allows us to estimate the CDE starting from a specific state $x_s$, in practice, we are interested in the average compounding error over all possible starting states at time $s$. To assess the overall impact of the CDE when transitioning over the timesteps $s \\rightarrow t$, we consider the expected value of $\\mathbb{E}_{CDE}(s\\rightarrow t|x_s)$ with respect to $x_s \\sim P_s$. This leads us to consider:\n$\\mathbb{E}_{CDE}(s \\rightarrow t) = \\mathbb{E}_{x_s \\sim P_s} [\\mathbb{E}_{CDE}(s \\rightarrow t|x_s)]$.\nConsider a sampling schedule ${T = t_0 \\rightarrow t_1 \\rightarrow \\dots \\rightarrow t_{N-1} \\rightarrow 0 = t_N}$, which will be specified later. Our goal is to minimize the cumulative CDE that arises from each parallel sampling step within the given schedule. If we ignore the accumulated error from the previous steps that affects the consecutive steps, our objective is as follows:\n$\\min_{t_1,t_2,...,t_{N-1}} \\sum_{i=0}^{N-1} \\mathbb{E}_{CDE}(t_i \\rightarrow t_{i+1})$.\nInterestingly, we find in the following theorem that cumulative CDEs over the sampling schedule can upper bound the KL divergence between the true distribution at time $t = 0$, denoted $P_0$, and the distribution $Q_{T \\rightarrow t_1 + \\dots \\rightarrow 0}$ obtained from parallel sampling along the sampling schedule"}, {"title": "ESTIMATING THE COMPOUNDING DECODING ERROR USING GIRSANOV'S THEOREM", "content": "As shown in Eq. (3), computing $\\mathbb{E}_{CDE}(s \\rightarrow t|x_s)$ involves determining the KL divergence between the true distribution and the approximated distribution from DDM's parallel sampler, which is often intractable. To address this, we treat the ground truth reverse process and the sampling process from DDM's parallel samplers (introduced in Section 2.2) as two CTMCs, starting from the same initial distribution. By applying Girsanov's theorem (Ding & Ning, 2021; Chen et al., 2023), we derive a tractable formula to compare the KL divergence between the distributions of these stochastic processes at any time interval $[s, t]$. We summarize this as the following general theorem applicable to any two backward CTMCs with $\\hat{R}$ and $\\check{R}$ as their respective transition rate matrices:\n$\\begin{cases}\\text{CTMC 1: } q_{u-du|u}(y | x) = \\delta_{xy} + \\hat{R}_t(x, y)du + o(du), \\\\ \\text{CTMC 2: } q_{u-du|u}(y | x) = \\delta_{xy} + \\check{R}_t(x, y)du + o(du). \\end{cases}$\nWe defer its proof to Appendix A.2."}, {"title": "FEASIBLE COMPUTATION WITH HIERARCHICAL BREAKDOWN STRATEGY", "content": "Using the derived KLUB, we can formulate the timestep search as a minimization problem over KLUB. Here, we employ a hierarchical breakdown strategy, dividing a coarser sampling schedule into a finer one. Suppose our sampling schedule is given by ${T \\rightarrow t \\rightarrow 0}$. Let $Q_{T \\rightarrow t \\rightarrow 0}$ represent the distribution generated by this schedule. Our goal is to find the optimal $t$ that minimizes cumulative CDE, i.e., $\\mathbb{E}_{CDE}(T \\rightarrow t) + \\mathbb{E}_{CDE}(t \\rightarrow 0)$. This is approximately achievable by minimizing its KLUB upper bound:\n$t_1 = \\text{arg} \\min_{t \\in (T,0)} KLUB(P_0||Q_{T \\rightarrow t \\rightarrow 0})$\nWith the initial refined interval ${T \\rightarrow t_1 \\rightarrow 0}$, we seek optimal timesteps $t_2 \\in (T, t_1)$ and $t_3 \\in (t_1, 0)$ by solving the following minimization problems:\n$t_2 \\in \\text{arg} \\min_{t \\in (T,t_1)} KLUB(P_{t_1}||Q_{t \\rightarrow t \\rightarrow t_1}) \\text{ and } t_3 \\in \\text{arg} \\min_{t \\in (t_1,0)} KLUB(P_0||Q_{t_1 \\rightarrow t \\rightarrow 0})$\nThe first minimization problem targets matching $D_{KL}(P_{t_1} || Q_{t_1})$, while the second focuses on matching $D_{KL}(P_0||Q_0)$. This results in a further refined sampling schedule ${T \\rightarrow t_2 \\rightarrow t_1 \\rightarrow t_3 \\rightarrow 0}$. By iterating this process, we continue splitting each interval into smaller ones, optimizing breakpoints using the KLUB criterion.\nAfter $K$ iterations, this hierarchical strategy yields a sampling schedule with $2^K$ NFEs, optimizing the schedule as the number of steps increases."}, {"title": "FEASIBLE COMPUTATION FOR KLUB ESTIMATION", "content": "Directly estimating $KLUB_{P_{T}}[P_0||Q_{T \\rightarrow t \\rightarrow 0}]$ by using Eq. (8) is practically infeasible due to computational complexities. To address this issue, we propose two techniques that simplify the estimation process.\nInstead of minimizing the mismatch between the ground truth distribution $P_{paths}$ and $Q_{paths}^{T \\rightarrow t \\rightarrow 0}$, we choose to maximize the discrepancy between $Q_{paths}^{T \\rightarrow t \\rightarrow 0}$ and a simpler, coarser approximation $Q_{paths}^{T \\rightarrow 0}$. The key insight is that maximizing this divergence, we can find the optimal sampling time $t$, which helps reduce the compounding error relative to the true distribution.\n$\\mathbb{D}_{KL}(Q_{paths}^{T \\rightarrow t \\rightarrow 0}|| Q_{paths}^{T \\rightarrow 0}) \\approx \\mathbb{D}_{KL}(P_{paths} || Q_{paths}^{T \\rightarrow 0}) - \\mathbb{D}_{KL}(P_{paths} || Q_{paths}^{T \\rightarrow t \\rightarrow 0})$"}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate the Jump Your Steps (JYS) sampling schedule across various datasets and models. We compare the JYS schedule with the uniform sampling schedule, which sets all intervals to the same size. Except for the Countdown dataset, we use open-sourced pretrained models for our experiments. It is important to note that the Gillespie algorithm is only applicable to an absorbing transition matrix, as uniform or Gaussian transition kernels do not have a fixed number of transitions. For further experimental details and additional qualitative results, please refer to the Appendix C and D.2."}, {"title": "THE COUNTDOWN DATASET", "content": "Following Zhao et al. (2024), to evaluate our sampling schedule performance, we created a synthetic sequence dataset with a strong position-wise correlation structure. Each sample consists of 256 tokens, and each token has a value between 0 and 31. Each data sequence $X^{0:255}$ is generated according to the following rules:\n$X^0 \\sim \\text{Uniform}{1,...,S}$,\n$X^{d+1} \\sim \\begin{cases} X^d & \\text{if } X^d \\ne 0 \\\\ \\text{Uniform}{1, ...,S} & \\text{if } X^d = 0 \\end{cases}$\nWe trained a SEDD (Lou et al., 2024) with an absorb transition matrix on this generated data. We measure the model performance by the proportion of generated samples that violated the rule, i.e., failed to count downwards from the previous token. The results are shown in Figure 5. We observe that the JYS schedule has fewer errors compared to the uniform schedule for the same NFE."}, {"title": "CIFAR-10", "content": "We demonstrate our sampling schedule in the image domain. For this experiment, we use a pretrained model from Campbell et al. (2022), which employs a gaussian transition matrix and denois-"}, {"title": "MONOPHONIC MUSIC", "content": "We test our method on conditional music generation using the Lakh pianoroll dataset (Raffel, 2016; Dong et al., 2018). For this experiment, we employ a pretrained model from Campbell et al. (2022), which uses a uniform transition matrix and denoising parameterization. Each data sequence contains 256 timesteps (16 per bar), and we measure performance by conditioning on two bars to generate the remaining 14 bars, following the setup in Campbell et al. (2022).\nWe evaluate how different the generated results were when using a smaller NFE (from 2 to 64), compare to samples generated with an NFE of 512. Specifically, we calculate the Hellinger distance between the note distributions in the generated samples. The results are presented in Figure 7. Given the same NFE, we observe that samples generated using our method were more similar to those generated with a high NFE."}, {"title": "TEXT MODELING", "content": "Finally, we validate our method on text generation. For this experiment, we use a pretrained model from Lou et al. (2024), which employs an absorbing transition matrix and score-based parameterization. We use two model sizes, SEDD-small and SEDD-medium, in the experiments; both models use the GPT-2 tokenizer and were trained on OpenWebText. The JYS schedule, optimized on SEDD-small, is also used for the experiments with SEDD-medium.\nFollowing Lou et al. (2024), we measure the generative perplexity of sampled sequences (using a GPT-2 large for evaluation). We generated 1,024 samples and each sample constructed with sequences of 1,024 tokens. We simulate 16 to 256 NFE for generation. Figure 8 shows the results, demonstrating better perplexity at the same NFE."}, {"title": "CHARACTERISTICS OF JUMP-YOUR-STEP SAMPLING SCHEDULE", "content": "In Section 3.1, we hypothesized that in regions where the conditional mutual information is low, the CDE would also be small, allowing steps to be skipped with minimal performance degradation. Here, we aim to verify if the JYS operates as expected according to this hypothesis.\nFigure 9 shows the JYS sampling schedules optimized for various transition matrices. First, in the absorb case (Left), as discussed in Figure 1 (Bottom), we observe that large intervals are concen-"}, {"title": "RELATED WORK", "content": ""}, {"title": "EFFICIENT SAMPLING FOR CONTINUOUS DIFFUSION MODELS", "content": "After the seminal work by Song et al. (2021b), which interpreted diffusion models as a Stochastic or Ordinary Differential Equations (SDE/ODE), various SDE (Jolicoeur-Martineau et al., 2021; Xu et al., 2023) and ODE solvers (Song et al., 2021a; Lu et al., 2022; Zhang & Chen, 2023; Dockhorn et al., 2022; Liu et al., 2022; Zheng et al., 2023) have been proposed to improve sampling speed.\nThe work most closely related to ours is \"Align Your Step\" (Sabour et al., 2024), which focuses on sampling schedule optimization in continuous diffusion models. In contrast, our approach targets discrete diffusion models, where we derive the KLUB for CTMCs to optimize the sampling schedule. We also propose a computationally efficient algorithm for KLUB computation and optimization."}, {"title": "DISCRETE DIFFUSION MODELS", "content": "Several approaches have been developed for training discrete diffusion models, including denoising parameterization (Austin et al., 2021; Campbell et al., 2022; Gu et al., 2022; Gat et al., 2024; Campbell et al., 2024; Shi et al., 2024) and score parameterization (Sun et al., 2023; Meng et al., 2022; Lou et al., 2024). Recently, SEDD has outperformed GPT-2 in text modeling, gaining traction as an alternative to autoregressive models (Deschenaux & Gulcehre, 2024).\nIn terms of sampling, two main directions have emerged. The first focuses on efficient sampling, with T-leaping for CTMC and methods like analytic sampling (Sun et al., 2023), Tweedie sampling (Lou et al., 2024), and k-Gillespie (Zhao et al., 2024) improving accuracy. The second aims to reduce compounding error via corrector steps, such as random correctors (Campbell et al., 2022),"}, {"title": "CONCLUSIONS", "content": "We present Jump Your Steps, a principled method designed to optimize the sampling schedule and minimize these numerical errors without incurring additional computational costs during inference. Unlike existing approaches that rely on extra computational efforts, such as predictor-corrector methods, our technique operates independently and efficiently. Through extensive evaluations on synthetic and real-world datasets including monophonic piano, image, and text generation-we demonstrate that our method consistently enhances performance across different transition kernels in discrete diffusion models and effectively complements various samplers."}, {"title": "A THEORETICAL DETAILS", "content": ""}, {"title": "PROOF OF THEOREM 3.1", "content": "Although we treated the two-dimensional case where we have $X_t = (X_1, X_2)$ in the main body, we consider the general d-dimensional case with $X_t = (X_1, \\dots, X_d)$. In that case, the definition of $\\mathbb{E}_{CDE}$ is given by\n$\\mathbb{E}_{CDE}(s \\rightarrow t|x_s) \\stackrel{\\triangle}{=} \\mathbb{D}_{KL}(P_{X_t|x_s}||P_{X_1^t|x_s} \\otimes \\dots \\otimes P_{X_d^t|x_s})$.\nIn general, for a discrete probability distribution $P_{prior}(\\cdot)$ over the space $\\mathcal{S}$ and a conditional distribution (or denoiser) $P_{cond}(\\cdot|\\cdot)$ over the same space, i,e, $P_{cond} : \\mathcal{S} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$, let us just write the resulting distribution as follows:\n$\\mathbb{P}_{cond} \\mathbb{P}_{prior} \\stackrel{\\triangle}{=} \\mathbb{E}_{x \\sim P_{prior}}[P_{cond}(\\cdot|x)] = \\sum_{x\\in \\mathcal{S}} P_{cond}(\\cdot|x) P_{prior}(x)$.\nThen, if we define $P_{t_{i+1}|t_i}(X_{t_i}) \\stackrel{\\triangle}{=} P_{X_{t_{i+1}}|X_{t_i}}$ and $Q_{t_{i+1}|t_i}(X_{t_i}) \\stackrel{\\triangle}{=} P_{X_1^{t_i}|x_t} \\otimes \\dots \\otimes P_{X_d^{t_i}|x_t}$, following equation 15, we can denote the target distributions in the theorem as follows:\n$P_0 = P_{t_T|t_{N-1}} \\dots P_{t_1|t_0} P_{t_0}, \\quad Q_0 = Q_{T \\rightarrow t_1 + \\dots \\rightarrow 0} = Q_{t_T|t_{N-1}} \\dots Q_{t_1|t_0} P_{t_0}$.\nFor simplicity, let us also define the mid-time distributions as\n$P_{t_i} \\stackrel{\\triangle}{=} P_{t_i|t_{i-1}} \\dots P_{t_1|t_0} P_{t_0}, \\quad Q_{t_i} \\stackrel{\\triangle}{=} Q_{t_i|t_{i-1}} \\dots Q_{t_1|t_0} P_{t_0}, \\quad Q_{t_0} \\stackrel{\\triangle}{=} P_{t_0}$.\nLet us now consider the case where $s = t_{i-1}$ and $t = t_i$ for some $i > 0$ specifically. Let\n$P_{t,s}(y, x) \\stackrel{\\triangle}{=} P_{t|s}(y|x)P_s(x), \\quad Q_{t,s}(y, x) \\stackrel{\\triangle}{=} Q_{t|s}(y|x)Q_s(x)$"}, {"title": "PROOF OF THEOREM 3.2", "content": "Although we state the theorem for a backward CTMC from time $s$ to time $t$ ($t < s$) and bound the KL-divergence at time $t$, here we will just consider the forward CTMC from time 0 to time $T$ and bound the KL-divergence at time $T$ for simplicity. After this change of the time direction and interval, the formal statement and proof of Theorem 3.2 is given in Theroem A.3.\nTo derive the KL-divergence upper bound (KLUB) for continuous-time Markov chains (CTMCs), we first adopt the change of measure $\\frac{dP_{paths}}{dQ_{paths}}$ for CTMCs from Section 3 of Ding & Ning (2021).\nNext, we compute and organize the equation for the $\\mathbb{D}_{KL}(P_{paths}||Q_{paths}) = \\mathbb{E}_{P_{paths}} [\\log \\frac{dP_{paths}}{dQ_{paths}}]$.\nWe consider the following two forward CTMCs over $[0, T]$:\n$\\begin{cases} \\text{CTMC 1: } q_{u+du|u}(y | x) = \\delta_{xy} + \\hat{R}_t(x, y)du + o(du), \\\\ \\text{CTMC 2: } q_{u+du|u}(y | x) = \\delta_{xy} + \\check{R}_t(x, y)du + o(du). \\end{cases}$\nHere, $\\hat{R}$ and $\\check{R}$ represent the rate matrices of each CTMC, with a finite state space $\\mathcal{S} = \\{x_1, \\dots, x_N\\}$, and $du > 0$.\nWe introduce some notations. Define the functions $H^1_{t}$ $H^2_{t}$ as follows:\n$H^i_{t} := \\delta(X_{t} - x_i), \\quad H^i_{t} := H^i_{t}H^i_{t^{-}},$\nwhere $\\delta(\\cdot)$ denotes the Dirac delta function, and $t^-$ is the left limit of $t$. By definition, $H^i_{t} = 1$ indicates a transition from $x_i$ to $x_j$ at time $t$. The CTMC $(X_t)_{t \\in [0, T]}$ is defined as a function from the sample space $\\Omega$ to the path space $\\mathcal{C} \\equiv [0, T] \\times \\mathcal{S}$, i.e., $X : \\Omega \\rightarrow \\mathcal{C}$.\nWe define two probability measures over the path space:\n$\\bullet$ $P_{paths}$, under which $(X_t)_{t \\in [0, T]}$ has the law of CTMC 1.\n$\\bullet$ $Q_{paths}$, under which $(X_t)_{t \\in [0, T]}$ has the law of CTMC 2."}, {"title": "A TECHNIQUE", "content": "The approximation is made as follows:\n$\\mathbb{D}_{KL}(P_{path} || Q_{path}) - \\mathbb{D}_{KL}(P_{path} || \\hat{Q}_{path}) = \\mathbb{E}_{P_{path}} [\\log \\frac{\\hat{Q}_{path}^{T + t + 0}}{Q_{path}^{T + t + 0}}]$\n$\\approx \\mathbb{E}_{forward} [\\log \\frac{\\hat{Q}_{path}^{T + t + 0}}{Q_{path}^{T + t + 0}}]$\n$\\approx \\mathbb{E}_{T \\rightarrow t \\rightarrow 0} [\\log \\frac{\\hat{Q}_{path}^{T + t + 0}}{Q_{path}^{T + t + 0}}]$\n$= \\mathbb{D}_{KL}(\\hat{Q}_{Path} || Q_{Path})$\nEquation equation 19a assumes that $P_{path} \\approx \\hat{Q}_{path}^{forward}$ where $\\hat{Q}_{path}^{forward}$ refers to the distribution made by forward CTMC. In equation equation 19b, we assume that $Q_{path} \\approx Q_{path}^{T \\rightarrow 0}$.\nCompared to coarser sampling, KLUB computation can be organized as follows:\n$KLUB(\\mathbb{Q}_{0}^{T+t+0}||\\mathbb{Q}_{0}^{T}) = \\mathbb{E}_{paths} [\\sum_{i \\neq j}^{T} \\sum_{u=0}^{t} H_i^j log \\frac{R_T(i, j)}{RT+t+0(i, j)}]$\n$= \\mathbb{E}_{paths} [\\sum_{i \\neq j}^{t} \\sum_{u=0}^{T} H_i^j log \\frac{R_T(i, j)}{RT+t+0(i, j)} + \\sum_{i \\neq j}^{T} \\sum_{u=t}^{T} H_i^j log \\frac{R_T(i, j)}{RT+t+0(i, j)}]$\n$= \\mathbb{E}_{paths} [\\sum_{i \\neq j}^{t} \\sum_{u=0}^{T} H_i^j]$\nutilizes that under $\\tau$-leaping, $RT+t+0(i, j) = RT(i, j)$ for $u \\in [t,T]$ and $RT+t+0(i,j) = Rt(i, j)$ for $u \\in [0, t]$. In Eq. (20c), the rate matrices are constant over intervals, allowing us to pull log $R_T(i,j)$ outside the summation."}, {"title": "TECHNIQUE", "content": "Consider the meaning of $\\mathbb{E}_{\\hat{Q}_{paths}} [\\sum_{u=0}^{t} H_i^j] $; it calculates the average probability of a transition from $i$ to $j$ occurring between time 0 and $t$. If we knew $du\\rho(x_u = j, x_{u^-} = i)$, this could be found by $\\int_{0}^{t} du\\rho(x_u = j, x_{u^-} = i) du$.\nFortunately, we do know the conditional transition rate $du\\rho(x_u = j | x_{u^-} = i) = R_t(i, j)$. Let's assume that there are maximally single transition of state in each dimension during the time interval, which is the assumption behind using $\\tau$-leaping algorithm for DDMs (Campbell et al., 2022). Using this, we can rewrite Eq. (20c):\n$\\mathbb{E}_{\\hat{Q}_{paths}} [\\sum_{i \\neq j} log \\frac{R(i,j)}{R_T(i, j)} ] $\n$=\\mathbb{E}_{\\hat{Q}_{paths}}[\\sum_{i \\neq j} E[Log(X_{t} \\neq j) (\\sum _{u<t} H_i^j)| Xt-1]$\n$= \\mathbb{E}_{\\hat{Q}_{paths}}[\\sum_{X_{t} \\neq j} log \\frac{R(X_t, j)}{R_T(X_t, j)} \\times R_t(X_t, j) \\Delta t]$"}, {"title": "ALGORITHM", "content": "In this section, we present the main algorithm for Jump your steps."}, {"title": "KLUB COMPUTATION", "content": "Please refer to Algorithm 1. In the case of k-Gillespie, we first sampled the corresponding t using the corresponding $p(t|k)$, and then apply Algorithm 1."}, {"title": "JUMP YOUR STEPS", "content": "Please refer to Algorithm 2."}, {"title": "EXPERIMENT DETAILS", "content": "Golden Section The golden section search was stopped if the difference between the newly optimized t and the previous t was smaller than T/2048."}, {"title": "ABLATION STUDY", "content": "Figure 10 presents the ablation study on the number of samples for KLUB computation. We compare a diverse set of sample sizes, ranging from 4 to 1024."}, {"title": "QUALITATIVE RESULTS", "content": "In this subsection, we present qualitative comparison between Jump Your Steps and Uniform sampling schedule under various NFES.\nFigure 11 shows generated images with various NFE and sampling schedule. All results are generated using Euler 7-leaping sampler.\nFigure 12, 13, 14, 15 show generated text samples with various NFE and sampling schedule. All results are generated using Euler T-leaping sampler."}]}