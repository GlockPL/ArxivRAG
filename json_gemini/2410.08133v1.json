{"title": "ASSESSING EPISODIC MEMORY IN LLMS\nWITH SEQUENCE ORDER RECALL TASKS", "authors": ["Mathis Pink", "Vy Ai Vo", "Qinyuan Wu", "Jianing Mu", "Javier Turek", "Uri Hasson", "Kenneth A. Norman", "Sebastian Michelmann", "Alexander Huth", "Mariya Toneva"], "abstract": "Current LLM benchmarks focus on evaluating models' memory of facts and\nsemantic relations, primarily assessing semantic aspects of long-term memory.\nHowever, in humans, long-term memory also includes episodic memory, which\nlinks memories to their contexts, such as the time and place they occurred. The\nability to contextualize memories is crucial for many cognitive tasks and everyday\nfunctions. This form of memory has not been evaluated in LLMs with existing\nbenchmarks. To address the gap in evaluating memory in LLMs, we introduce\nSequence Order Recall Tasks (SORT), which we adapt from tasks used to study\nepisodic memory in cognitive psychology. SORT requires LLMs to recall the\ncorrect order of text segments, and provides a general framework that is both easily\nextendable and does not require any additional annotations. We present an initial\nevaluation dataset, Book-SORT, comprising 36k pairs of segments extracted from 9\nbooks recently added to the public domain. Based on a human experiment with 155\nparticipants, we show that humans can recall sequence order based on long-term\nmemory of a book. We find that models can perform the task with high accuracy\nwhen relevant text is given in-context during the SORT evaluation. However, when\npresented with the book text only during training, LLMs' performance on SORT\nfalls short. By making it possible to evaluate more aspects of memory, we believe\nthat SORT will aid in the emerging development of memory-augmented models.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have impressive performance on many benchmarks that test factual\nor semantic knowledge learned during training or in-context (Hendrycks et al., 2020; Ryo et al., 2023;\nLogan IV et al., 2019; Petroni et al., 2019; Yu et al., 2023; Sun et al., 2023). While these advances\nare noteworthy, the type of long-term knowledge that these datasets test is only one of several types\nthat naturally intelligent systems store, retrieve, and update continuously over time (Norris, 2017;\nIzquierdo et al., 1999; McClelland et al., 1995). Current evaluation tasks do not assess episodic\nmemory, which is a form of long-term knowledge thought to be important for cognitive function\nin humans and animals. In contrast to semantic memory, episodic memory links memories to their\ncontexts, such as the time and place they occurred. This ability to organize memory based on spatial\nand temporal details enables us to reconstruct events that occurred in the possibly distant past, predict\nthe future, and relate information across multiple events that are separated by time windows spanning\na lifetime, capabilities crucial for many cognitive tasks and everyday functions.\nThe ability to link temporal context to stored information may be key to improving LLM performance\non several tasks. More human-like episodic memory may improve models' continual learning and\nadaptation to shifting data distributions, performance on tasks requiring long contexts (e.g., long\nchat exchanges with a user), and source attribution via knowledge of where and when a memory was\nacquired, which could help to reduce or identify hallucinations.\nTo address the gap in evaluating memory in LLMs, we propose the Sequence Order Recall Task\n(SORT), which we adapt from tasks in cognitive psychology that are used to assess long-term episodic\nmemory in humans and animals (Eichenbaum, 2013; Davachi & DuBrow, 2015). Specifically, SORT\nrequires a model to recall the correct order of sequential data, such as segments of text.\nWe provide a specific instantiation of SORT that requires models to recall the correct order of two\nsegments sampled from text, along with a corresponding evaluation dataset-Book-SORT. Book-SORT\ncontains over 36k pairs of text segments from 9 books, with variations in segment length (20 and 50\nwords) and distance between segments (up to 16k words). We chose books that were very recently\nreleased from U.S. copyright to minimize the possibility that LLMs were pre-trained on these texts.\nThis allowed us to test three common methods of giving a language model access to a specific text:\n(1) during inference in-context, (2) during inference via retrieval augmented generation (RAG), and\n(3) during training via fine-tuning with a language modeling objective. Furthermore, we provide\na human evaluation from 155 participants who had finished reading a whole book and were tested\nwith no additional access to the book, showing that humans can recall segment order with up to 70%\naccuracy based on their long-term memory of the book. While the ceiling performance on SORT is\n100% (assuming that texts do not contain duplicate segments), our human data provides an important\nreference point to compare and contrast long-term memory across models and humans.\nWhen given access to excerpts from the books in-context, we find that models achieve up to 95%\naccuracy with relevant 250-word excerpts but degrade quickly as longer excerpts are presented. When\nmodels use RAG instead, they can recall sequence order only with limited performance below 65%.\nFinally, models fine-tuned with a language modeling objective on the book texts do not significantly\nimprove their SORT performance, showing that parametric memory in current transformer models\nsupports semantic but not episodic long-term memory.\nOur main contributions can be summarized as follows:"}, {"title": "2 RELATED WORK", "content": "Evaluation of parametric semantic memory in LLMs. Benchmarks such as MMLU (Hendrycks\net al., 2020), T-REX (Elsahar et al., 2018), LAMA (Petroni et al., 2019), WICE (Ryo et al., 2023),\nKOLA (Yu et al., 2023), and others (Sun et al., 2023) test models' retrieval and reasoning ability on\ndifferent domains, such as recalling a chemistry fact.\nOther benchmarks that partially evaluate LLM semantic memory are those that require reasoning using\ntemporal (Ning et al., 2020; Zhou et al., 2021; Feng et al., 2023) (e.g. lunch happens before dinner),\ncausal (Srivastava et al., 2023) (e.g. she is eating, therefore she is hungry), or other commonsense\nknowledge (e.g. food is edible) (Ismayilzada et al., 2023) acquired during pretraining. In contrast\nto these benchmarks, our work proposes a task that involves judgments regarding temporal context\ninformation about text segments that either (a) are available through in-context memory or (b) were\notherwise previously presented to the model, e.g. via fine-tuning or Retrieval Augmented Generation,\nand is agnostic of the specific semantic content of these segments.\nEvaluation of in-context memory in LLMs. Among other conditions, we evaluate in-context\nmemory, in which the model has in-context access to all relevant text for the task. This relates to\nworks that evaluate a model's ability to reason over its context input, such as Needle In A Haystack\n(Kamradt, 2023) and FLenQA (Levy et al., 2024).\nPrevious datasets and benchmarks that evaluate performance over long context lengths, such as\nLong Range Arena (Tay et al., 2021), SCROLLS (Shaham et al., 2022), and MULD (Hudson &\nAl Moubayed, 2022), are also relevant. The evaluation of in-context memory with SORT differs\nfrom these works by focusing on order information, which is key to episodic memory in humans.\nAdditionally, we use SORT to evaluate parametric memory which contains information beyond the\ncurrent context.\nTasks related to SORT. Previously proposed tasks that most closely relate to SORT are BART's\ndenoising training objective (Lewis et al., 2020), which permutes the order of sentences in a document\nand learns to reconstruct the correct order, and BERT's next sentence prediction objective (Devlin\net al., 2019), which learns to predict whether two sentences follow each other in a text. SORT differs\nfrom these tasks, as it is not intended as a training objective, and it can include text segments with an\narbitrary distance between each other in a document, possibly exceeding the context input length of\nthe model. In ChapterBreak (Sun et al., 2022), long segments ending at a chapter boundary taken\nfrom a book are presented to an LLM along with multiple segments of chapter beginnings from the\nsame book. The task for the LLM is then to tell which one is the directly following chapter and which\nare not. This suffix-identification task aims to evaluate narrative-understanding based reasoning\nabout books, while we propose SORT as an evaluation for episodic memory in LLMs, involving\nboth a model and a memory-insertion method. By evaluating a SORT baseline in which the models\ndo not have access to relevant source texts, we show that memory is needed for SORT and general\nnarrative-reasoning ability is not enough."}, {"title": "3 SEQUENCE ORDER RECALL TASK", "content": "We introduce a novel evaluation task: recalling the order of parts of a sequence, which we term the\nSequence Order Recall Task (SORT). SORT is adapted from recency judgment tasks used in cognitive\npsychology to evaluate episodic memory in humans and animals (Eichenbaum, 2013; Davachi &\nDuBrow, 2015). In this task, a sequence is presented to a participant. Then, after some delay, the\nparticipant is asked to judge the order in which two segments of the sequence appeared. We adapt this\ntask to test memory in models. The general task can be applied to any sequential domain, including\nvideo and audio. Here we focus on the text domain to evaluate LLMs (Fig. 1).\nFormal description of SORT. The general form of the task can be described as follows. Let\n$X \\in \\mathbb{R}^{TF}$ be sequential data, where T is the number of time-steps (e.g. token in a text) and\nF is the number of features (e.g. vocabulary size). We define start indices $t_j$ and $t_k$ for pairs of\nsegments of length $L \\in \\mathbb{N}^+$ in X, such that both $t_j < t_k$ and $t_j + L < t_k$. Using these, we extract\nnon-overlapping segments from the original sequence X as $X_i = X[t_i : t_i + L - 1, :]$. The order\nof segments $X_j$ and $X_k$ is randomized, yielding $[X_A X_B]$, which is then given as part of a model's\ninput. The task for a model $M_{\\theta}$ is to infer whether $t_A < t_B$, i.e. in SORT, the task of a model is to\npredict which of two non-overlapping subsequences $X_A$ and $X_B$ has the lower starting index in X.\nThe task can be used to evaluate a variety of methods to include document-specific memory in models.\nTo assess in-context memory, i.e. memory based on text presented in-context, the segments are\npreceded by X in the model's input. When assessing retrieval-augmented generation methods, instead\nof prepending X, passages of X are retrieved and prepended. For the assessment of parametric"}, {"title": "3.1 EVALUATING LARGE LANGUAGE MODELS ON SORT", "content": "We greedily sample an answer token $a = \\underset{a}{\\operatorname{argmax}} M_{\\theta}(I)$ from the model $M_{\\theta}$, which is parame-\nterized by $\\theta$, and decode the sampled answer token a as either \"A\" or \"B\".\nThe answer is evaluated as correct if it corresponds to the segment that truly appears first in X.\nFor proprietary (OpenAI) models that do not allow completing assistant responses with prepended\ntext, we omit $P_{answer}$. In this case we resort to generating a sequence of 25 tokens, and parse the\ngenerated text for A or B responses.\nPrompt selection. Using a single prompt formulation across all models may bias the results. To\nprevent this, we compiled a set of 12 prompts that vary formulations in $P_{context}$ and $P_{task}$. For each\nmodel, we evaluate each prompt on a held-out dataset of 400 samples and used the best performing\nprompt for each model. The full prompts and further details on prompt selection are given in\nAppendix B.2-B.3.\nBaseline without book-specific memory. We want to ensure that performance on SORT is due to\ntext-specific memory and not due to temporal order reasoning supported by more semantic forms of\nmemory such as commonsense knowledge (e.g. lunch happens before dinner). We isolate the effects\non SORT that are due to text-specific memory by contrasting performance between a baseline model\nthat does not have access to the specific text and a model that has access to the sequences in one of\nvarious ways in which memory can be inserted."}, {"title": "3.2 INSERTING TEXT-SPECIFIC MEMORY INTO MODELS", "content": "We evaluate three methods to insert text-specific memory into models: (1) via in-context presentation,\n(2) via fine-tuning with a language modeling objective, and (3) via retrieval augmented generation of\nshort chunks of text in a book.\nIn-context presentation. When assessing in-context memory, $P_{context}$ in Eq. 1 contains relevant\nexcerpts from the source text along with the book title. The prompt includes the instruction to\ncarefully read the text from the book (a list of used prompts is shown in Appendix 6). To test\nin-context memory, We make sure that excerpts contain both segments and vary the length of excerpts\nin our experiments.\nFinetuning with a language modeling objective. Instead of presenting text from the books in the\nsame prompt in which the SORT task is given, we are interested in parametric memory of the texts. In\nthis condition, $P_{context}$ in Eq. 1 is an empty string. To insert parametric memory of the source texts\ninto a model, we fine-tune the model with a next-token prediction objective on the books, split into\nchunks of 5000 words and contextualized by the books' titles. Since we need to preserve the models'\nability to understand and follow the task instructions, we fine-tune on a dataset that additionally\nincludes 3,500 random instruction-following examples that are unrelated to SORT. This helps to\nprevent catastrophic forgetting during continued finetuning (Luo et al., 2024). We finetune on 8 A100\nGPUs with an initial learning rate of 5e-6 and a batch size of 192. Full details of the fine-tuning setup\nare given in Appendix E and our code will be available.\nRetrieval Augmented Generation. To include memory of text via retrieval augmented generation\n(RAG), we built a typical naive RAG pipeline that relies on two separately pretrained models for the\nretriever and the reader (Gao et al., 2024). The retriever returns text passages from a database to serve\nas task context for the LLM (i.e. as $P_{context}$, Eq. 1).\nThe retrieval database contained text embeddings of all passages from Book-SORT (Sec. 4). We\nused the LangChain recursive text splitter to chunk Book-SORT text into ~1024 character, non-\noverlapping passages (average 183 words). Each passage was then encoded into a 1024-d vector\nusing a high-performing, open-source text retrieval model (BGE-v1.5, Xiao et al. (2024)). To retrieve\nthe passages, we used the Faiss (Douze et al., 2024) library to conduct an exact nearest neighbor\nsearch. The search returned the k = 2 nearest neighbors. We maintained this similarity order\nwhen inserting the retrieved passages into the prompt, i.e. the most similar passage appears first\nin $P_{context}$. As described in Section 3.1, we selected a single prompt for each model based on\nthe model's performance on the held-out validation set across 10 different possible prompts (see\nAppendix B.4)."}, {"title": "4 BOOK-SORT DATASET AND EVALUATION", "content": "We created an English language dataset to evaluate episodic memory in humans and LLMs. The\nselected sequence data considered several factors: (1) we chose long texts (mean length = 72,700\nwords) that exceed the context windows of most transformer LLMs; (2) we used books to enhance\nmemorability for human readers and facilitate our human evaluation experiment; (3) we selected\nbooks from Project Gutenberg that recently entered the U.S. public domain to avoid ethical and\ncopyright issues, and minimize pre-training contamination in LLMs. Within these constraints, we\naimed to maximize content diversity, including narrative fiction novels, a physics text, and an extended\nessay. Further details on the 9 books in the Book-SORT dataset are available in Appendix A.1."}, {"title": "4.1 BOOK-SORT CREATION", "content": "We constructed a dataset that varies across factors that can affect human or model performance\non SORT. Based on prior reports on LLMs (Liu et al., 2024), we first varied (1) $L_E$, the length of\nthe text excerpt presented in context. Since the typical standard context length of the LLMs in our\nstudy was 4096 tokens, we set $L_E = \\{250, 1000, 2500\\}$ words. For models with extended context\nwindows, we also created datasets where $L_E = \\{10000, 20000\\}$ words, which excluded one book\nthat was too short. Our pilot experiments on humans suggested two other factors that would affect\ntask performance: (2) $L_S$, the length of the segments from the text, and (3) $D_S$, the distance between\nthe segments in the original text. To mirror the human experiments, we set $L_S = \\{20, 50\\}$ words."}, {"title": "4.2 HUMAN LONG-TERM MEMORY EVALUATION", "content": "As a reference point (but not a performance ceiling), we further provide a human evaluation from\n155 participants who had recently finished reading one of the 9 books in the Book-SORT dataset,\nThe Murder of Roger Ackroyd (Christie, 1927). This evaluation assessed long-term memory, as the\naverage time between reading and testing was 7.5 days, far surpassing short-term memory duration\n(Hasson et al., 2015). There is no previously reported data on long-term memory for entire books from\nlarge samples, so we designed an experiment to collect this data. Given the difficulty of recruiting\nparticipants to read lengthy books specifically for an experiment, we used a creative recruiting\nstrategy: inviting members of the online reading community Goodreads who had recently finished\nThe Murder of Roger Ackroyd. Participants completed an online survey within 30 days of finishing\nthe book. The expected compensation for participation was $12 and the study was approved by\nthe IRB at Anonymized University. We provide 1570 segment pair samples from 155 participants.\nFurther details about this one-of-a-kind study are provided in Appendix A.3."}, {"title": "4.3 MODELS", "content": "We evaluate a selection of open models covering a broad range of scores on popular benchmarks\nsuch as MMLU (see Table 5) ranging from 7b to 8x22b parameter transformer models. Initial\nexperiments with non-instruction-tuned models resulted in chance performance on Book-SORT (see\nAppendix D), which we attribute to the lack of instruction tuning\u00b9, and thus focus on evaluating\ninstruction-tuned models in this work. We have selected models from different model families\nincluding Llama3 (AI@Meta, 2024), Llama2 (Touvron et al., 2023), Mistral (Jiang et al., 2023),\nMixtral (Jiang et al., 2024), Gemma (Team et al., 2024) and OpenAI GPTs (Achiam et al., 2023). For\nour experiments on finetuning as a method for inserting memory into models, we focus on two models\nMistral-v0.2-7b-Instruct and Llama3-8b-Instruct because they allow full-parameter fine-tuning with\n8 A100 GPUs."}, {"title": "5 RESULTS", "content": "We present empirical findings for a baseline without text-specific memory of the books in Book-SORT,\nas well as three methods to include memory, using 9 open-source models and 2 closed language\nmodels."}, {"title": "5.1 BASELINE", "content": "SORT requires memory specific to books in Book-SORT. To validate that it is not possible to\nachieve high performance on Book-SORT without memory of the specific books that are included\nin the dataset, we evaluate models before they have access to the books. We find that segment pairs\nwith a very short and with a very long distance in the book allow for above-chance-performance (see\nAppendix C.1), indicating that some of these segment pairs can be ordered based not on memory but\nrather on temporal-order reasoning or common-sense. However, performance is below 60% for all\nmodels and segment lengths, confirming that SORT requires memory for the particular books being\nqueried to yield high levels of performance."}, {"title": "5.2 HUMAN EXPERIMENT", "content": "Humans can perform in SORT based on long-term memory. The results from human long-term\nmemory (LTM) experiments, depicted in Figure 2, demonstrate that humans can perform in SORT\nbased on long-term memory. The average accuracy is 0.64 for segments of 50 words and 0.56 for\nsegments of 20 words). Human performance is higher for pairs of segments that have a greater\ndistance in the book, with a peak accuracy of 0.76 for distances greater than 25,000 words and\n50-word segments. Binomial tests show that beyond a distance of 4000 words, humans perform\nstatistically significantly better than chance. Note that we present these results as evidence that one\npossible information processing system-a human-can perform SORT based on long-term memory.\nImportantly, these results do not present the ceiling performance on the memory task that we propose.\nThe expected ceiling performance on SORT is 100%, assuming that the books do not contain\nduplicated segments of text; the odds of exact duplication decrease as segment length increases."}, {"title": "5.3 IN-CONTEXT MEMORY", "content": "Models generally perform well on SORT based on in-context memory. Nearly all models achieve\nabove 77% accuracy when given in-context access to relevant excerpts from the books, reaching up to\n95% (Table 2). This indicates that very large models are not necessary to perform this task effectively,\nas demonstrated by the Llama3-8b model outperforming larger models such as Llama3-70b and\nMixtral-8x7b-DPO.\nIn-context memory performance increases with greater distance between segments. We further\nevaluate the effect of another factor which may influence the model performance-the distance between\nthe text segments in the excerpt. Figure 3b shows an increasing trend in accuracy as the distance"}, {"title": "5.4 PARAMETRIC MEMORY VIA FINETUNING", "content": "Full parameter fine-tuning on books with a language modeling objective did not improve SORT\nperformance. For Llama3-8b-Instruct and Mistral-7b-v0.2-Instruct, we do not observe any difference"}, {"title": "5.5 RETRIEVAL AUGMENTED MEMORY", "content": "RAG based memory leads to worse performance than in-context memory. RAG performance\nis between 55% and 67% for all distances between segments and tested models (Figure 4a), which\nis substantially lower than the in-context memory performance. This difference in performance\nfollows from the fact that standard forms of RAG do not necessarily preserve the order of retrieved\npassages, whereas the excerpt provided for in-context memory does have the passages in the correct\norder (and additionally contains the text that connects the passages, which may help in making\nthe order judgment). When the relevant passages are retrieved and presented in the correct order,\nRAG performance improves substantially. Interestingly, we find that Llama3-8b-Instruct model\noutperforms the much larger Mixtral-8x22b-Instruct and Llama3-70b-Instruct on SORT with an\naccuracy around 90% across all distances between segments (Figure 4b)."}, {"title": "6 DISCUSSION", "content": "We provide a new evaluation task, SORT, for assessing episodic memory in large language models,\nthat can be used with any text data and without the need for annotation. We created Book-SORT, a\ndataset for SORT based on books that were recently added to the public domain and we validated that\nbook-specific memory is indeed needed to achieve high performance on Book-SORT. We evaluated\nthree different ways to include memory of specific texts in a model to assess whether they support\na key function of episodic memory. Below, we discuss our results for these methods in relation to\nepisodic memory in humans.\nIs in-context memory a form of episodic memory? Several links have been drawn between in-\ncontext memory in transformers and models of episodic memory in humans (Ji-An et al., 2024;\nWhittington et al., 2022; 2024; Ellwood, 2024), and our results, which show that in-context memory\nsupports sequence order recall, could be interpreted as further evidence for in-context memory acting\nas episodic memory in LLMs. However, our results also show that in-context sequence order recall\nperformance degrades with increasing context length, which would not be the case with episodic\nmemory. This discrepancy stems from a key difference between in-context memory in models and\nepisodic memory in humans and animals, which is that in-context memory in LLMs can directly\nattend to all tokens in the context window, whereas the episodic memory system in humans and\nanimals stores past experiences in synaptic form, and requires an additional retrieval step before\nepisodic memory content can be attended to. The reliance on synaptic storage and retrieval is what\nenables the episodic memory system in humans and animals to make use of a sequence-length\ninvariant mechanism with a fixed computational cost to remember past experiences over a lifetime.\nThis sequence-length invariant property of the episodic memory system in humans and animals allows\nit to generalize to arbitrarily long sequences, while attention over all tokens in a growing sequence\neventually leads to generalization failure for in-context memory and, at the same time, comes with\na sharply increasing computational cost. Based on these considerations, we believe that, although\nboth the episodic memory system in animals and in-context memory in transformer models perform\na kind of similarity-based lookup of past experiences, in-context memory's access to activations is\nmore analogous to working memory in humans (O'Reilly et al., 2024), but with a capacity that vastly\nexceeds human working memory.\nIs parametric memory in transformers a form of episodic memory? High performance on\nbenchmarks including MMLU suggests that parametric memory in LLMs learned via a language\nmodeling objective can support semantic forms of memory (e.g. when recalling knowledge to answer\nfactual questions). Our evaluation on SORT showing close to chance performance after finetuning\nsuggests that current forms of parametric memory do not support functions similar to those of episodic\nmemory. This suggests that different learning methods and architectures (e.g. with a separate memory\nsystem) may be needed for functioning parametric forms of episodic memory.\nIs retrieval augmented memory a form of episodic memory? Since it avoids the problems of\ncontext-length generalization and increasing computational costs observed for in-context memory,\nRetrieval Augmented Generation presents a potentially strong way to include memory of episodes via\na retrieval process and subsequent in-context presentation. However, our results suggest that there is a\nlot of room for improvement over the performance of vanilla RAG. The weak performance of vanilla\nRAG on SORT arises from the fact that it is decontextualized-all that it retrieves is independent parts\nof the text. By contrast, current theories of episodic memory posit that episodic memory contents\nare bound to a drifting temporal context; later, when some content is retrieved, the temporal context\nassociated with that content is also retrieved (Howard & Kahana, 2002; Polyn et al., 2009). One"}]}