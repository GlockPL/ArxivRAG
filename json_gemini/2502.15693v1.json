{"title": "Hgformer: Hyperbolic Graph Transformer for Recommendation", "authors": ["Xin Yang", "Xingrun Li", "Heng Chang", "Jinze Yang", "Xihong Yang", "Shengyu Tao", "Ningkang Chang", "Maiko Shigeno", "Junfeng Wang", "Dawei Yin", "Erxue Min"], "abstract": "The cold start problem is a challenging problem faced by most modern recommender systems. By leveraging knowledge from other domains, cross-domain recommendation can be an effective method to alleviate the cold start problem. However, the modelling distortion for long-tail data, which is widely present in recommender systems, is often overlooked in cross-domain recommendation. In this research, we propose a hyperbolic manifold based cross-domain collaborative filtering model using BiTGCF as the base model. We introduce the hyperbolic manifold and construct new propagation layer and transfer layer to address these challenges. The significant performance improvements across various datasets compared to the baseline models demonstrate the effectiveness of our proposed model.", "sections": [{"title": "I. INTRODUCTION", "content": "Recommender systems have become an indispensable part of our daily life, serving as fundamental tools for personalized information filtering and prioritization [1], [2], [3]. The core of a recommender system is to predict whether a user will engage with an item, such as by clicking, rating, or purchasing it. In this context, Collaborative Filtering (CF) [4], [5], [6], [7], which leverages past interactions between users and items to make these predictions, remains an essential component to deliver effective personalized recommendations. The interaction patterns between users and items in CF tasks naturally form a graph structure, motivating researchers to investigate the use of Graph Neural Networks (GNNs) [7], [8], [9], which has proven significant advantages in modeling graph structures[10], [11].\nDespite the significant attention and fruitful outcomes in this field, most existing GNNs normally assume that the degree of each node is balanced. However, data in the realm of recommender systems generally exhibit a long-tail distribution [12], [13], [14], [15]: a small portion of items are highly popular with numerous users, whereas most other items attract relatively few users. Recent studies have also shown that GNNs-based methods perform well in recommending popular items (head items), but often struggle to perform as effectively with less popular items (tail items) [9], [16]. This issue primarily arises from two factors: i) Local structure modeling: GNN-based models normally follow the neighborhood aggregation scheme and tend to be biased towards nodes with high degree [17]. More precisely, in the task of CF, head items that are interacted with by many users typically have higher-quality representations, while the vast majority of tail nodes with few interactions are likely to be underrepresented [18]. Several GNN frameworks have been proposed to mitigate these degree biases by introducing designated architectures or training strategies specifically for low-degree nodes [9], [18], [15], but they still fail to capture the global information as transformers [19], [20], [21], [22] and thus achieve sub-optimal results. ii) Embedding distortion: Most existing methods encode items and users into Euclidean space [8], [7], which is a flat geometry with a polynomial expanding capacity. Data with the long-tail distribution can be traced back to hierarchical structures [23], whose number of neighbors increases exponentially. As a result, encoding these data via Euclidean space naturally incurs information loss and could subsequently deteriorate the performance of downstream tasks. In contrast, hyperbolic manifold, a non-Euclidean space characterized by constant negative curvature, allows the space to expand exponentially with the radius, making it particularly well-suited for representing tree-like or hierarchical structures [24], [25]. For this reason, in recent years, significant advances have been made in hyperbolic neural networks to better handle the problem of long-tail distribution [24], [26], [27].\nSo far, most research in recommender systems has focused on addressing either one of the two problems but has not managed to tackle both simultaneously. Inspired by the suc- cessful application of Graph Transformers in graph and node classification tasks [19], [22], [21], this study proposes a novel Graph Transformer architecture in the hyperbolic manifold for Collaborative Filtering. Although the idea of extending Graph Transformers to hyperbolic space for recommendations is intriguing, it poses several challenges that must be overcome:\n\u2022 No well-defined parameter-free graph convolution in hyperbolic space. Parameter-free message-passing paradigms such as LightGCN [7] have shown superior advantages in CF, however, most existing hyperbolic variants of LightGCN, such as HGCF, HRCF, and HICF [9], [28], [16] require to first project embeddings in the hyperbolic manifold back to the tangent space for subsequent graph convolution, which causes information loss and limits their performance.\n\u2022 No well-defined hyperbolic self-attention mechanism for collaborative filtering. Although there are currently some definitions of hyperbolic attention [29], [30], in the field of Collaborative Filtering, the user-item interaction structure is represented as a bipartite graph, existing methodologies are inadequate, and hyperbolic self-attention mechanism tailored for CF tasks has not yet been investigated.\n\u2022 Scalability issue of hyperbolic self-attention mechanism. In recommender systems, real-world graphs are often large-scale, which poses a significant challenge in efficiency when applying transformer architectures with quadratic time complexity. Although many studies have tackled the scalability problem of graph transformers in Euclidean space [22], [21], [20], the solution for linear computational complexity of hyperbolic self-attention is still under-explored.\nTo solve these challenges, we propose a new Hyperbolic Graph Transformer framework called Hgformer. For the first challenge, we propose the Light Hyperbolic Graph Convolutional Network (LHGCN), which performs graph convolution entirely in the hyperbolic manifold; For the second challenge, we propose a novel hyperbolic transformer architecture tailored for CF tasks, which consists of a cross-attention layer and a hyperbolic normalization layer[31]; For the last challenge, we propose an unbiased approximation approach to reduce the computational complexity of hyperbolic self-attention to the linear level. Numerical experiments show that our proposed model performs better than the leading CF models and remarkably mitigates the long-tail issue in CF tasks. We summarize our contributions as follows:"}, {"title": "II. PRELIMINARIES OF HYPERBOLIC GEOMETRY", "content": "In this section, we briefly introduce concepts related to hyperbolic geometry, which builds up the foundation of our method.\nMinkowski (pseudo) Inner Product. Consider the bilinear map $(\u00b7,\u00b7)_M : \\mathbb{R}^{n+1} \\times \\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}$ defined by\n$(u, v)_M := -u_0v_0 + \\sum_{i=1}^n u_i v_i = u^T Jv,$\nwhere $J = diag(-1,1,\\dots,1) \\in \\mathbb{R}^{(n+1)\\times(n+1)}$. It is called Minkowski (pseudo) inner product on $\\mathbb{R}^{n+1}$. Given a constant $K > 0$, the equality $(x,x)_M = -K$ implies that\n$x_0^2 = K + \\sum_{i=1}^n x_i^2$\n$x_0 \\ge \\sqrt{K}.$\nThen,\n$\\Vert u \\Vert_M = \\sqrt{(u,u)_M}$\nis a well-defined norm.\nHyperbolic manifold $H^{n,K}$ and Tangent space $T_x H^{n,K}$. Consider the subset of $\\mathbb{R}^{n+1}$ defined as follows:\n$H^{n,K} := \\{x \\in \\mathbb{R}^{n+1} : (x,x)_M = -K \\text{ and } x_0 > 0\\}.$\nGiven any constant $K > 0$, the set $H^{n,K}$ is an $n$-dimensional embedded submanifold of $\\mathbb{R}^{n+1}$ with tangent space:\n$T_x H^{n,K} = ker Dh(x) = \\{u \\in \\mathbb{R}^{n+1} : (x, u)_M = 0\\},$\nwhich is an $n$-dimensional subspace of $\\mathbb{R}^{n+1}$.\nNorth Pole Point on $H^{n,K}$. The point $o := (\\sqrt{K},0,\\dots,0) \\in H^{n,K}$ is called the north pole point of $H^{n,K}$. We observe that\n$T_o H^{n,K} = \\{u \\in \\mathbb{R}^{n+1} : (o, u)_M = -\\sqrt{K}u_0 = 0\\} = \\mathbb{R}^n$.\nRiemannian Distance on $H^{n,K}$. The distance function induced by the Riemannian metric $(\u00b7,\u00b7)_M$ is\n$d(x, y) = \\sqrt{K} \\operatorname{arcosh}(\\frac{(x, y)_M}{K})$\nfor all $x, y \\in H^{n,K}$."}, {"title": "III. HGFORMER: A HYPERBOLIC GRAPH TRANSFORMER FOR RECOMMENDATION", "content": "In this section, we will elaborate on our proposed method. All the notations we use in this paper are detailed in Table I. We formally define our tasks as follows: Input: The interaction graph of users and items $G = (V_u, V_i, E)$, $E \\subseteq V_u \\times V_i$. Output: A learned function $F = (u, i|G, \\Theta)$, where $u \\in V_u$, $i \\in V_i$ and $\\Theta$ denote the model parameters.\nAs is shown in Fig. 2(a), in our framework, we first map the users and items into embedding space according to their IDs and use an exponential map to project the embeddings into hyperbolic space. To capture the local structure of nodes in the user-item interaction graph, we design a Light Hyperbolic Graph Convolutional Network (LHGCN). On the other hand, to capture the global structure of the entire interaction graph, we propose a novel hyperbolic transformer, which is composed of a hyperbolic cross-attention mechanism with linear computation complexity and a hyperbolic normalization layer[31]. In the final step, we aggregate the local structure information and the global information from both perspectives for prediction, optimized by a hyperbolic margin-ranking loss.\nA. Hyperbolic Embedding\nWe first encode each user and item into the embedding space, which is denoted as $u^\\epsilon = [u_1^\\epsilon; \\dots; u_N^\\epsilon]$ and $i^\\epsilon = [i_1^\\epsilon; . . . ; i_M^\\epsilon]$, where the superscript $\\epsilon$ means Euclidean space, $N$ and $M$ means the number of users and items. Then, we use an exponential map to project the embeddings into the hyperbolic space.\n$u^H = Exp_o((0, u^\\epsilon)), i^H = Exp_o((0, i^\\epsilon)).$\nSince in the subsequent sections, we will only use the embeddings within the hyperbolic manifold, for convenience and to avoid confusion, we ignore the superscript H and simply denote the embeddings of users and items in the hyperbolic space as $u = [u_1; \\dots; u_N]$ and $i = [i_1; . . . ; i_M]$.\nB. LHGCN: Light Hyperbolic Graph Convolutional Networks\nIn CF tasks, each node (user or item) is represented by a unique ID, which lacks concrete semantics beyond being an identifier. In such scenarios, LightGCN [7] empirically demonstrated that performing multiple layers of nonlinear feature transformation does not provide benefits and increases the difficulty of model training. Therefore, removing nonlinear feature transformation from GCNs is a well-accepted approach in CF. To perform message-passing in hyperbolic space, a straightforward solution is to first project embeddings in the hyperbolic manifold back to the tangent space at the north pole point, performing parameter-free graph convolution, and then map them back to the hyperbolic manifold [9], [28], [16]. However, since the tangent space at the north pole point is merely a local approximation of the north pole point [32], this can cause a certain degree of information loss. For this sake, we design a simple but efficient graph convolution method tailed for CF called LHGCN. Similar to HGCF [9] and LightGCN [7], LHGCN does not have trainable parameters and all computations are performed entirely on the hyperbolic manifold, eliminating the need for transformations between the hyperbolic manifold and Euclidean space. Specifically, we adopted hyperbolic centroid which is defined in Eq. 3 to aggregate the messages of neighbors:\n$u_i^{(l+1)} = Centroid(\\{u_i^{(l)}, \\{i_k^{(l)} : k \\in N_i\\}\\}),$\n$i_j^{(l+1)} = Centroid(\\{i_j^{(l)}, \\{u_k^{(l)} : k \\in N_j\\}\\}),$\nwhere $N_i$ denotes the neighbors of node $i$. Then the outputs of LHGCN are\n$u_{local} = u^{(L)} \\text{ and } i_{local} = i^{(L)},$\nwhere L denotes the number of layers of LHGCN.\nC. Hyperbolic Transformer Model\nIn this section, to address LHGCN's limitations in capturing the global information of the interaction graph and taking into account the unique structure of the bipartite graph in CF, we design a novel hyperbolic cross-attention mechanism for modeling global user-item interactions. Furthermore, since this cross-attention requires operating on all user-item pairs, with computational complexity $O(M \u00b7 N)$, we propose an approx- imation approach to reduce the computational complexity to $O(M + N)$.\n1) Hyperbolic Cross-Attention: In CF tasks, there are only interactions between the user set and the item set, which form a bipartite graph. Intuitively, modeling the inner interactions among user groups or item groups would introduce noisy signals and thus deteriorate the performance [33]. For this reason, we introduce a cross-attention mechanism to model only all possible user-item interactions, as detailed in the structure presented in Fig. 2(c). Taking the i-th user vector as an example, firstly, the correlation between the i-th user vector ($i \\in \\{1, ..., N\\}$) and the j-th item vector ($j \\in \\{1,...,M\\}$) under a specific attention head h is defined as:\n$\\omega_{i,j}^{(h)} = \\frac{\\exp (Sim(q_i^{(h)}, k_j^{(h)})/T)}{\\sum_{l=1}^M \\exp (Sim(q_i^{(h)}, k_l^{(h)})/T)},$\nwhere $q_i^{(h)} = W_Q^{(h)} \\ltimes u_i, k_j^{(h)} = W_K^{(h)} \\ltimes i_j$ and $\\ltimes$ denotes hyperbolic matrix multiplication, which is defined in Eq. 5. $\\tau$ is the temperature parameter. $Sim(\u00b7,\u00b7)$ is a function to calculate the similarity of two vectors in the hyperbolic manifold, and it was generally defined as:\n$Sim(x, y) = f(-c_1d_M(x, y) + c_2),$\nwhere $f(\u00b7)$ is a monotonically increasing function, such as exponential maps, linear functions, tanh, sigmoid, etc.. Then, the representation of the i-th user is updated by aggregating all item embeddings with weights $a_{i,j}$:\n$\\hat{u}_i^{(h)} = \\sum_{j=1}^M \\omega_{ij}^{(h)} v_j,$\nwhere $v_j = W_V^{(h)} \\ltimes i_j$.\nTo ensure that the embedding stays in the hyperbolic manifold, we need an extra coefficient c to scale the embedding according to Eq. 3,\n$u_i^{(h)} = c \\hat{u}_i^{(h)},$\nwhere $c = \\frac{K}{\\Vert \\hat{u}_i^{(h)} \\Vert_M}$ and $K$ is the curvature of the hyperbolic\nmanifold. After that, we aggregate the embeddings of different heads by the hyperbolic centroid defined in Eq. 4:\n$u_{global} = Centroid(u^{(1)}; \\dots ; u^{(h)}).$\nWe calculate all item embeddings $i_{global}, j \\in \\{1,..., M\\}$ in the same way.\nFor all the embeddings $x = [u_{global}, i_{global}]$. Finally, for numerical stability, we adopted the definition of Hyperbolic Normalization from [31] and applied it to normalize the final embeddings x:\n$HN(x) = \\exp(\\frac{\\sqrt{K}}{\\sigma} PT_{\\mu \\rightarrow x} o(\\frac{PT_{o \\rightarrow \\mu} log(x)}{\\sqrt{\\sigma^2 + \\epsilon}})),$\nwhere $\\mu = Centroid(x)$, which is Centroid of $x$ in hyperbolic manifold and $\\sigma^2 = \\frac{1}{M+N}\\sum d(x_i, x_j)^2$, which is the variance in hyperbolic space. And $\\beta$ and $\\epsilon$ are trainable vectors.\n2) Towards Linear Complexity: In this section, we introduce the hyperbolic self-attention with only linear computational complexity. Although the above Hyperbolic cross-attention mechanism can model the global interactions between all users and items, its quadratic time complexity prevents its application in real-world scenarios when there are numerous users or items. To enable the application of Hyperbolic Transformers to larger datasets, as Fig. 3 shown, the previous complexity of the similarity matrix of hyperbolic vectors is reduced to only $O((M + N)md)$ by our mechanism, where the dimensions m and d are much smaller than M and N.\nSince the most computationally intensive part of the model is Eq. 9, in this section, our goal is to reduce the computational complexity of Eq. 9 to linear. Firstly, we redefined Eq. 8. Since K > 0, $\\operatorname{arcosh}(\u00b7)$ is a monotonically increasing function, both $(d(\u00b7))$ and the Minkowski inner product $(\u00b7,\u00b7)_M$ could be used to compare the similarity between different vectors in hyperbolic space. Then for $x, y \\in H^{d+1}$, we redefine the similarity function $Sim(\u00b7,\u00b7)$ as the Hyperbolic SoftMax similarity function, $HSM(\u00b7, \u00b7) : H^{d+1,K} \\times H^{d+1,K} \\rightarrow \\mathbb{R}$:\n$HSM(x, y) \\triangleq \\exp ((x, y)_M)$\nthen Eq. 9 is redefined as:\n$\\hat{u}_i^{(h)} = \\sum_{j=1}^M \\frac{\\exp((q_i^{(h)},k_j^{(h)})_M/\\tau)}{\\sum_{l=1}^M \\exp((q_i^{(h)},k_l^{(h)})_M/\\tau)}v_j^{(h)},$\nwhere $q_i^{(h)}, k_j^{(h)}$ and $v_j^{(h)}$ follows the settings of Eq. 7. Then, we use an estimation $\\kappa(q_i^{(h)}, k_j^{(h)})$ to approximate $\\exp ((q_i^{(h)}, k_j^{(h)}))$ in Eq. 13 and we introduce Theorem 4.1, which proves that the aforementioned estimation is an unbiased estimation.\nTheorem 4.1. For $x, y \\in H^{d+1,K}$ with $x = (x_0,x_1,\\dots,x_d), y = (y_0, y_1,\\dots,y_d)$, and $\\bar{x} = (x_1,x_2,\\dots,x_d)^T, \\bar{y} = (y_1, y_2,\\dots,y_d)^T$, we have an esti- mation function $\\kappa(\u00b7, \u00b7) : H^{d+1,K} \\times H^{d+1,K} \\rightarrow \\mathbb{R}$:\n$\\kappa(x, y) = \\exp(\\frac{-(x_0 + y_0)^2 + 2K}{2}) \u00b7 \\mathbb{E}_{w\u223cN(0_d,I_d)}[\\exp(w^T(\\bar{x} + \\bar{y}))]$\nwhere $w \\sim N(0_d, I_d)$, $\\kappa(\u00b7,\u00b7)$ is an unbiased estimation of the HSM function:\n$\\mathbb{E}[\\kappa(x, y)] = HSM(x, y)$\nThe proof of this theorem is given in Section VI. Then, such an unbiased estimation function (Eq. 14) can be converted into a dot product of vector functions approximately; the method of converting it is akin to kernel tricks shown as the following lemma:\nLemma 4.2. Define the hyperbolic positive random features $\\phi(\u00b7) : H^{d+1,K} \\rightarrow \\mathbb{R}^m$:\n$\\phi(x) = \\sqrt{\\frac{\\exp(\\frac{K-x_0}{2T})}{\\sqrt{m}}} [\\exp (w_1^Tx),\\dots, \\exp (w_m^Tx)]$\nwhere $w_k \\sim N (0_d, I_d)$ is i.i.d., m is a constant that could be chosen smaller than d. Then, we have:\n$\\phi(x)^T\\phi(y) \\approx \\kappa(x, y) = HSM(x, y).$\nThe proof of this lemma is given in Section VI. We adopted a positive random feature map $\\phi(\u00b7) : H^{d+1,K} \\rightarrow \\mathbb{R}^m$ to approximate HSM function:\n$HSM(x,y)/\\tau \\approx \\phi(\\frac{x}{\\sqrt{T}})^T\\phi(\\frac{y}{\\sqrt{T}})$\nEq. 17 is proved in Lemma 4.2 and for $x \\in H^{d+1,K}, x = (x_0,x_1,...,x_d), \\bar{x} = (x_1,x_2,...,x_d)$, the explicit form of positive random feature map with temperature parameter $\\tau$ is defined as:\n$\\phi(\\frac{x}{\\sqrt{T}}) = \\sqrt{\\frac{\\exp(\\frac{K-x_0}{2T})}{\\sqrt{m}}} [\\exp (\\frac{w_1^T\\bar{x}}{\\sqrt{T}}),..., \\exp (\\frac{w_m^T\\bar{x}}{\\sqrt{T}})]$\nThen, we can change the computation order and extract common factors by using Eq. 16 to convert the HSM function into the dot product of two feature functions. Then the approximating aggregation function Eq. 18 only has linear complexity and the process is visualized in Fig. 3. Subsequently, the final form of the aggregation function is proposed as follows:\n$\\hat{u_i^{(h)}} \\approx \\sum_{j=1}^M \\frac{\\phi(q_i/\\sqrt{\\tau})^T \\phi(k_j/\\sqrt{\\tau})}{\\sum_{k=1}^M \\phi(q_i/\\sqrt{\\tau})^T \\phi(k_n/\\sqrt{\\tau})} v_j = \\frac{\\phi(q_i/\\sqrt{\\tau})\\sum \\phi (k_j/\\sqrt{\\tau}).\\phi (v_j)}{\\phi(q_i/\\sqrt{\\tau}) \\sum \\phi (k_n/\\sqrt{\\tau})}$\nThe error of approximation is bounded and we have:\nTheorem 4.2: The error function of approximation\n$\\Delta = |HSM(x, y) - \\phi(x)\\phi(y)|$\nis bounded by $O(\\sqrt{\\frac{\\exp(3(\\delta-K))}{m \\epsilon}})$ with the probability that:\n$P(\\Delta \\le \\sqrt{\\frac{\\exp(3(\\delta-K))}{m \\epsilon}})) \\le 1 - \\epsilon$\nassuming that $||x||_F \\le \\delta,||y|| \\le \\delta$ for $x, y \\in H^{d+1,K}$. The proof is given in Section VI.\nSince the upper bound of the error function depends only on the Euclidean norm $\\delta$, the curvature constant K, the number of positive random features m, and the demanding error accuracy $\\epsilon$, we can reduce the error by normalizing the vectors and process them in a suitable hyperbolic space, or increase m.\nD. Embedding Aggregation and Optimization\nEmbedding Aggregation. To aggregate both structural and global information, we map the embeddings back to Euclidean space using the Log function, and then perform a weighted average:\n$u_{final} = Exp_o(\\alpha Log_K (u_{global}) + (1 - \\alpha) Log_K (u_{local}))$\n$i_{final} = Exp_o(\\alpha Log_K (i_{global}) + (1 - \\alpha) Log_K (i_{local}))$\nwhere $\\alpha$ is a hyperparameter between 0 and 1.\nPrediction. Margin ranking loss has been extensively used in recommendation tasks [9], which separates positive and negative pairs of user items by a given margin. When the gap between a negative and a positive user-item pair exceeds this margin, neither pair contributes to the overall loss, enabling the optimization process to focus on the difficult pairs in the data set. In this work, we use the hyperbolic version of margin- ranking loss as prediction loss. The prediction loss is defined as:\n$L(u_{final}, i_{final}, i_{final}^{neg}) = max((d_M(u_{final}, i_{final})^2 - d_M(u_{final}, i_{final}^{neg})^2) + \\gamma, 0),$\nwhere $\\gamma$ is a non-negative hyperparameter. $i_{final}^{pos}$ are the embeddings of the positive samples of this user and $i_{final}^{neg}$ are the embeddings of negative samples of this user in the same hyperbolic manifold. Positive samples refer to the items that the user has interacted with, while negative samples refer to randomly sampled items that the user has not interacted with.\nIV. EXPERIMENTS\nIn this section, we conduct extensive experiments on multiple public datasets to evaluate the proposed method and primarily address the following questions.\nRQ1: How does Hgformer perform compared to baselines?\nRQ2: How does each module contribute to the performance?\nRQ3: How well does Hgformer perform on the head and tail items?\nRQ4: Is Hgformer sensitive to different hyperparameters?\nRQ5: Why does Hgformer perform better than other models?"}, {"title": "C. Ablation Analysis (RQ2)", "content": "We conduct ablation studies on two main components of Hgformer. The results are shown in Fig. 4. We can observe that removing either the LHGCN or the Transformer component leads to a decline in the model's performance. Replacing LHGCN with HGCF also leads to a decline. To be specific, we have the following observations:\n1) The removal of LHGCN results in the most significant drop in performance. This indicates that LHGCN plays a dominant role in the CF task, and solely using the Hyperbolic Transformer to capture global information between users and items, while ignoring the inherent topological structure of the existing interaction graph, is not sufficient to effectively capture potential user- item relations. Therefore, in recommendation tasks or link prediction tasks, it is difficult to achieve good results by completely abandoning GNNs and relying solely on transformers for prediction. A better strategy for link prediction and recommendation tasks is to use transformers as a supplementary tool.\n2) Removing the Hyperbolic Graph Transformer also leads to a remarkable decline in model performance. Furthermore, we observed that incorporating the Hyperbolic Transformer led to certain improvements in HGCF, which demonstrates its effectiveness.\n3) Replacing LHGCN with HGCF also results in a decline. Through the direct comparison between LHGCN and HGCF, we found that LHGCN outperforms HGCF on the majority of datasets (Amazon Book, Amazon CD, Amazon Movie, Douban Movie). The only exception is the Douban Book dataset, where LHGCN performs slightly worse than HGCF (Recall@10: 0.1368 vs. 0.1375; Recall@20: 0.1916 vs. 0.1935; NDCG@10: 0.0948 vs. 0.0960; NDCG@20: 0.1096 vs. 0.1113). As mentioned in Section III-B, HGCF requires mapping embeddings back to the tangent space at the North Pole point during message aggregation, which results in information distortion during this process. In contrast, LHGCN performs graph convolution entirely on the hyperbolic manifold and achieves better performance."}, {"title": "D. Analysis on Tail Items (RQ3)", "content": "In this section, we analyze the results on tail items to demonstrate Hgformer's capability to mitigate long-tail issues.\n1) Tail percentage analysis: We calculate the proportion of tail items recommended by each model and tail items are defined as those whose popularity ranks in the last 80%. For this purpose, we designed a metric called tail percentage, which is formally defined as follows:\n$TailPercentage@K = \\frac{1}{\\vert U \\vert}\\sum_{u \\in U} \\frac{\\sum_{i \\in R_u} \\delta(i \\in T)}{R_u}$\nwhere $R_u$ is the set of items recommended to user u, T is the set of tail items and $\\delta$ is an indicator function. This metric gives the proportion of head items and tail items among all the items recommended by the model. We conducted evaluations on the Amazon CD and Amazon Movie datasets. As shown in the upper part of Fig. 5, it can be observed that the Euclidean space- based model (LightGCN) recommends almost only head items to users in both datasets (97.8% in the Amazon CD dataset & 95.1% in the Amazon Movie dataset, 99.3% in Douban Movie dataset and 99.5% in Douban Music dataset). This indicates Euclidean space-based models tend to overlook tail items in CF tasks and aggravate the Matthew Effect. Conversely, this phenomenon is significantly mitigated by the three hyperbolic space-based models, showing that hyperbolic space is more suitable for the long-tail setting in CF. It can be observed that the model's emphasis on tail items differs between the Amazon and Douban datasets, which is primarily due to their significant differences in the density and data distribution.\n2) Analysis of model's performance on tail items: To further investigate the performance of each hyperbolic-based model on tail items, in the second experiment, we evaluate the models' performance solely on tail items by calculating the recall@10 metric. As shown in Fig. 5, Hgformer significantly outperforms the other two hyperbolic space-based models on tail items. This is because Hgformer not only leverages the hyperbolic manifold to capture the hierarchical structure of the data but also introduces hyperbolic cross-attention which is beneficial for capturing global information. This allows the model to gather more information for tail nodes during the message-passing process, thereby improving the accuracy of recommendation."}, {"title": "E. Sensitivity Analysis (RQ4)", "content": "To evaluate the stability of our model, we conducted a sensitivity analysis on four hyperparameters of our model: number of LHGCN layers and aggregation weight $\\alpha$. The results are shown in Fig. 6.\n1) Our model remains relatively stable within a certain range for both aggregation weight and the number of LHGCN layers.\n2) In the sensitivity analysis of aggregation weights, our model demonstrated relative stability in the range of 0.2 to 0.3, achieving better performance within this interval.\n3) For the number of LHGCN layers, we observe that the optimal performance range of the model differs between the Amazon dataset and the Douban dataset. For the Amazon dataset, the optimal number of LHGCN layers falls within the range of 6 to 7, whereas for the Douban dataset, the optimal performance is achieved roughly between 4 and 5 layers."}, {"title": "F. Case study (RQ5)", "content": "In this section, we present a case study to demonstrate the effectiveness of our model in addressing both the local structure modeling problem of GNN-based models and high distortion problem of models in Euclidean space. We selected a user who interacted with 28 items as the subject of the case study. By analyzing items recommended by Hgformer from both head and tail positions, we aim to understand why Hgformer recommended these items while other models did not. It can be observed in Fig 7 that both the Euclidean space models and hyperbolic space models exhibit similar performance for head items, with their rankings being relatively close. However, in the case of tail items, we notice that Euclidean space models tend to rank these items lower, while hyperbolic space models can more effectively identify specific tail items that User_4 prefers. Nonetheless, HGCF fails to recommend items that are far from User_4 in the interaction graph (i.e., items with hops greater than 5). In contrast, Hgformer introduces the Hyperbolic self-attention mechanism, enabling the model to identify distant but relevant items effectively."}, {"title": "V. RELATED WORKS", "content": "Hyperbolic Neural Networks. Due to the negative curvature characteristics of hyperbolic manifolds, they have advantages over Euclidean space when representing hierarchical structures, tree-like structures, and data with long-tail distributions [23", "24": [25], "35": "defined Lorenz linear transformation and made it possible to operate fully on the hyperbolic manifold. In order to operate graph aggregation fully on the hyperbolic manifold, H2HGCN [36", "37": "applied Lorentzian aggregation. However, these methods require extra network parameters, which is unsuitable in the scenario of CF. In this work, we defined a lighter hyperbolic graph convolution, called LHGCN that can operate fully on a hyperbolic manifold without model parameters.\nGraph Neural Networks for CF. Graph Neural Networks have been widely applied in CF in recent years [8", "7": [9], "8": "employs multilayer graph convolutions to model high-order connectivity and intricate user-item interactions, significantly boosting the model's expressive power. Following NGCF, LightGCN [7", "9": "the firstly applies hyperbolic GCNs to CF tasks to mitigate the long-tail issues. Following HGCF, HICF [16", "28": "improves collaborative filtering through geometric regu- larization, effectively addressing the over-smoothing problem in hyperbolic graph convolutions, and HCTS [38", "years[39": [19], "20": [22], "21": [40], "transformer[22": [21]}, {"20": "and make graph transformers possible to deal with large- scale graphs. Considering the powerful representation capacities of the graph transformer, several searches also applied it to recommendation tasks[40", "41": [42]}]}