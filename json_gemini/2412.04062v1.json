{"title": "ZipAR: Accelerating Auto-Regressive Image Generation through Spatial Locality", "authors": ["Yefei He", "Feng Chen", "Yuanyu He", "Shaoxuan He", "Hong Zhou", "Kaipeng Zhang", "Bohan Zhuang"], "abstract": "In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the \u201cnext-set prediction\" paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) with the \u201cnext-token prediction\" paradigm [1, 23, 19] have demonstrated remarkable capabilities in addressing text-related tasks. Building on these successes, many studies [14, 24, 18, 9, 25] have extended this paradigm to the generation of visual content, leading to the development of auto-regressive (AR) visual generation models. These models not only produce high-fidelity images and videos that rival or even exceed the performance of state-of-the-art diffusion models but also facilitate unified multimodal understanding and generation. However, their slow generation speed remains a significant barrier to widespread adoption. To generate high-resolution images or videos, these models must sequentially produce thousands of visual tokens, requiring numerous forward passes and resulting in high latency.\nTo reduce the number of forward passes required for generating lengthy responses, several studies [2, 16, 12] have proposed the \"next-set prediction\" paradigm for LLMs, as depicted in Figure 2(b). These approaches involves introducing multiple decoding heads [2] or small draft models [12], which generate several candidate tokens that are later evaluated by the original model. However, these methods incur additional costs, as they require extra draft models or the training of new decoding heads. Another approaches use the jacobi decoding methods [16, 8, 20], iteratively updates sequences of tokens until convergence. However, in practice, the acceleration achieved by these methods is marginal, as LLMs often fail to generate correct tokens when errors exist in preceding ones. Furthermore, none of these approaches exploit the unique characteristics of visual content, and a parallel decoding framework specifically tailored for AR visual generation has yet to be developed.\nIn this paper, we introduce ZipAR, a parallel decoding framework designed to accelerate AR visual generation. As depicted in Figure 2(a), common AR visual generation models produce visual tokens in a raster order, where the first token in a row cannot be generated until the last token in the preceding row is decoded despite their spatial separation. However, visual content inherently exhibits strong locality, which is a widely utilized inductive bias for visual tasks [6, 11]. Specifically, there are significant spatial correlations between spatially adjacent tokens (e.g., token 5 and token 1 in 2"}, {"title": "2 Related Work", "content": "2.1 Auto-regressive Visual Generation\nThe success of Transformer models in text-based tasks has inspired studies [22, 7, 26] to apply auto-regressive modeling to visual content generation. These methods can be classified into two main categories: GPT-style approaches that utilize the next-token prediction paradigm [7, 24, 14, 17] and BERT-style approaches that employ masked prediction models [4, 3, 13, 26]. More recently, VAR [21] modified the traditional next-token prediction paradigm to next-scale prediction, resulting in faster sampling speeds. Models trained using next-token prediction can leverage the infrastructure and training techniques of large language models (LLMs) and pave the way towards unified multi-modal understanding and generation. However, they are generally less efficient during sampling compared to models that predict multiple tokens in a single forward pass. In this paper, we focus on accelerating visual generation models trained with the next-token prediction objective, hereafter referred to as auto-regressive visual generation models unless otherwise specified.\n2.2 Efficient Decoding of LLMs.\nEfforts to reduce the number of forward passes required for LLMs to generate lengthy responses can be broadly categorized into two main approaches. The first approach involves sampling multiple candidate tokens before verifying them with the base LLM. Speculative decoding [12] utilizes a small draft LLM to generate candidate tokens, which are then verified in parallel by the base LLM. While 3"}, {"title": "3 Method", "content": "3.1 Preliminaries\nAuto-regressive (AR) visual generation models with the next-token prediction paradigm have shown exceptional versatility across various vision-language tasks, including generating high-quality images and videos. As shown in Figure 2(a), pre-trained VQ-VAE models [22, 7] are commonly employed to convert images or videos into visual tokens. The process begins with a visual encoder that extracts feature maps at a reduced spatial resolution. These feature maps are then subjected to vector quantization to produce discrete latent representations, known as visual tokens. These tokens are arranged in a one-dimensional sequence to serve as input for AR models. Although various methods exist to flatten these tokens, the row-major order (raster order) is empirically validated to offer the best performance [7], making it the prevalent method for visual generation. During the image generation phase, AR models generate visual tokens sequentially in this raster order. Finally, the complete sequence of visual tokens is rearranged into a two-dimensional structure and processed through a visual decoder to reconstruct the images.\n3.2 Inference with ZipAR\n4"}, {"title": "4 Experiments", "content": "4.1 Implementation Details\nTo assess the effectiveness of our proposed method, we integrate it with three state-of-the-arts auto-regressive visual generation models: LlamaGen [17], Lumina-mGPT [14] and Emu3-Gen [24]. For text-guided image generation with LlamaGen, we generate 30,000 images and compute CLIP scores against MS-COCO 2014-val dataset with CLIP ViT-B/32 model [15]. For class-conditional image generation with LlamaGen on ImageNet, we report the widely adopted Frechet Inception Distance (FID) to evaluate the performance. We sample 50,000 images and evaluate them with ADM's TensorFlow evaluation suite [5].\n4.2 Quantitative Results\n4.2.1 Class-conditional Image Generation\nIn this subsection, we quantitatively assess the performance of class-conditional image generation on the ImageNet 256 \u00d7 256 benchmark using the LlamaGen model, as detailed in Table 1. The model employs a 24 \u00d7 24 feature map and necessitates 576 forward passes to generate an image under the next-token prediction (NTP) paradigm. By incorporating ZipAR with a window size of 16, the number of forward passes is reduced by 30.5%, with only a 0.16 increase in FID. Fine-tuning the pre-trained model for a single epoch with an ZipAR-oriented attention mask consistently enhances performance. Specifically, ZipAR-16 reduces the number of forward passes by 30.5% while achieving a lower FID compared to the original model with NTP paradigm. ZipAR-12 further reduces the number of forward passes by 45.8% while maintaining an FID of 3.49.\n4.2.2 Text-guided Image Generation\nIn this subsection, we quantitatively evaluate the effects of ZipAR on text-guided image generation performance. The LlamaGen-XL model processes a feature map of 32 \u00d7 32 and requires 576 forward 5"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we have proposed ZipAR, a new parallel decoding framework designed to accelerate auto-regressive visual generation. ZipAR leverages the spatial locality inherent in visual content and predicts multiple spatially adjacent visual tokens in a single model forward pass, thereby significantly enhancing generation efficiency compared to the traditional next-token-prediction paradigm. Extensive experiments demonstrate that ZipAR can reduce the number of model forward steps by up to 91% on the Emu3-Gen model with minimal impact on image quality.\nIn the future, we anticipate that integrating ZipAR with other methods that employ the next-set-prediction paradigm, such as Medusa [2] and Jacobi decoding [16], will further enhance acceleration ratios.\n6"}], "equations": ["C(i, j) =\\begin{cases}1, & \\text{if } \\{x_{i-1,k} | j \\leq k \\leq j+s\\} \\subseteq D \\\\0, & \\text{otherwise}\\end{cases}", "C(i, j) =\\begin{cases}1, & \\text{if } \\{x_{i-1,k} | j \\leq k \\leq j+s\\} \\subseteq D \\\\0, & \\text{otherwise}\\end{cases}"]}