{"title": "Chemical Reaction Extraction for Chemical\nKnowledge Base", "authors": ["Aishwarya Jadhav", "Ritam Dutt"], "abstract": "The task of searching through patent documents is crucial for chemical patent\nrecommendation and retrieval. This can be enhanced by creating a patent knowl-\nedge base (ChemPatKB) to aid in prior art searches and to provide a platform for\ndomain experts to explore new innovations in chemical compound synthesis and\nuse-cases. An essential foundational component of this KB is the extraction of\nimportant reaction snippets from long patents documents which facilitates multiple\ndownstream tasks such as reaction co-reference resolution and chemical entity role\nidentification. In this work, we explore the problem of extracting reactions spans\nfrom chemical patents in order to create a reactions resource database. We formu-\nlate this task as a paragraph-level sequence tagging problem, where the system is\nrequired to return a sequence of paragraphs that contain a description of a reaction.\nWe propose several approaches and modifications of the baseline models and study\nhow different methods generalize across different domains of chemical patents.", "sections": [{"title": "Introduction", "content": "The exponential publication rate in recent years and the fact that new innovations for chemical\ncompound synthesis and use-cases are primarily mentioned first in patents before they appear in\nscientific articles makes the task of patent recommendation and retrieval crucial. However, publicly\navailable patent recommendation systems are scarce; patents are mostly searched using Google\nPatents or USPTO, where recommendations are carried out through citation networks and topics.\nWe intend to leverage key information about chemical processes from patents as well as publicly\navailable external domain knowledge in order to improve retrieval and recommendation. To that end,\nwe explore the idea of creating a patent knowledge base (ChemPatKB) to facilitate prior art search as\nwell provide a means for domain experts to explore the KB in natural language queries.\nSome of the major components of the proposed ChemPatKB include patents, authors, assignees,\nreactions, chemical compounds and roles and properties of chemicals. In this project, we deal mainly\nwith the major recipes or reactions involved in the patent since reactions are an essential component\nof the Chemical KB and help focus on the more important spans within the long patent documents.\nThe essence of the reaction extraction task involves document-level information extraction to detect\nthe textual spans that describe or refer to chemical reactions.\nWhile most previous research in text mining of chemical reactions has focussed on chemical NER,\nthere has been limited research on automatically extracting chemical reactions from patents. A\nchemical reaction is a process where a set of chemical compounds is transformed into another set of\nchemical compounds. A reaction description may include the source chemical compounds, solvents\nand reagents involved in the reaction, reaction conditions, and materials obtained as a result of\nthe reaction. Once a reaction has been identified, it can be used as the input to (more complex)\ndownstream tasks. For example, consider an event extraction system that extracts every step of"}, {"title": "Literature Review", "content": "Although there has been limited work on this topic, this research direction is quite dated. Patents are\nregarded as an important resource for chemical information, and a large volume of NLP research\nhas focused on them (Tseng et al. [2007], Fujii et al. [2007], Gurulingappa et al. [2013]). Some\nprevious work has attempted to extract not only chemical names but also reaction procedures from\nthe literature (Lawson et al. [2011], Wei et al. [2015], Sang and Veenstra [1999]). Among them,\nLowe [2012] presents an integrated system that detects reaction text from chemical patents, and\nextracts chemicals and their roles in the corresponding reaction. The system is heavily rule-based and\nincorporates existing NLP libraries. More recent efforts for the specific task of reaction extraction\nhas been described in Yoshikawa et al., 2019 Yoshikawa et al. [2019], where a number of baseline\nand neural architectures have been proposed for this task. They achieve quite good results for the\nextraction task. However, their models were trained on a very limited set of silver standard data\ncontaining patents from organic chemistry. They do not report on how their models generalize to\ndifferent topics or domains of chemical patents.\nAnother aspect of this problem is the availability of a standard benchmark gold dataset containing\nreactions extracted from chemical patents covering a variety of topics and domains. The dataset used\nby Yoshikawa et al. Yoshikawa et al. [2019] is silver standard dataset derived from the Reaxys\u00ae\ndatabase. Recently Chemu He et al. [2021] released a gold standard annotated dataset for the Chemu\nchallenge of reaction coreference resolution resolution containing reaction span annotation for 150\npatents.\nIn this work we try to improvise on models for reaction extraction and work towards generating a\nlarge scale patent-reactions resource for research in this area."}, {"title": "Dataset", "content": "We use the Chemu dataset He et al. [2021] for training and evaluation of our models. It consist\nmainly of organic chemistry patents from the European Patent Office and the United States Patent\nand Trademark Office (USPTO). It is a gold standard dataset manually annotated for the Chemu 2021\nchallenge He et al. [2021].\nThe dataset contains 120 annotated patent documents in the BRAT file format in the train set and 30\npatents in the development set. The annotations are available in the form of character-level spans\ndenoting the reactions. Additional annotations include relations between parent-child reactions and\nrelation identification cues which are ignored for our task. Table 1 contains some stats about the\npatent documents and reaction annotations. We use the 30 dev set documents as a blind test set and\nuse 20% of the 120 train set documents as the validation set. The rest is used for training.\nWe also test the generalization performance of our models on out-of-domain patents. For this, we\nhave hand-picked a collection of 4 organic chemistry patents belonging to various CPC codes and a\nset of 3 patents from inorganic chemistry, petrochemical and alcohol domains. All of these contain\nvarying citations, are from industry or academia and small and large corporations to add variability to\nthe style to evaluate the robustness of our models. This formed our generalization set."}, {"title": "Task Formulation", "content": "Multiple contiguous paragraphs often describe a single reaction. Since a reaction consists of a series\nof sub-steps executed over time, it is crucial to accurately detect the beginning and end of each\nreaction text. Therefore, we define the task as a span detection problem rather than the simpler task\nof binary classification (i.e., classifying each paragraph as describing (part of) a reaction or not) to\ncapture reaction substructure. A patent document is given as a sequence of paragraphs. The task is to\ndetect a span of contiguous paragraphs that describe a single chemical reaction. In our corpus, we\nprovide paragraph-level label sequences over paragraphs in patent documents, following the IOB2\ntagging scheme Krallinger et al. [2017].\nWe transform the character span annotations to IOB tags using a simple mapping that assigns a 'B'\ntag to the paragraphs containing the first character of the reaction span and \u2018I' tag to all the subsequent\nparagraphs uptil the one containing the last reaction span character. All other paragraphs that do not\ncontain any reaction characters are tagged 'O'."}, {"title": "Baseline Models", "content": "Our backbone architecture is based on the baseline models described in Yoshikawa et al. Yoshikawa\net al. [2019]. Figure 1 from the paper describes these architectures. All 3 baseline models have the\nsame paragraph encoders but use different decoders for generating the IOB tags of a sequence of\npatent paragraphs.\nOn a high level, each paragraph is tokenized into words using the OSCAR4 tokenizer Jessop et al.\n[2011], which is specifically customized for chemical text mining. The embedding for each token is\nthen generated by concatenating the pre-trained word embedding of each token or word $w_i$: $e_{w_i}^{WE}$, its\ncontextualized embedding $e_{w_i}^{Wip}$ , and an optional embedding $e_{f_i}^{FT}$ representing additional features $f_i$\nassociated with $w_i$:\n$C_i = e_{w_i}^{WE} + e_{w_i}^{CW} + e_{f_i}^{FT}$\nFor the word embeddings, $e_{w_i}^{WE}$ and contextualized embeddings $e_{w_i}^{CW}$, Word2Vec Mikolov et al.\n[2013] and ELMO Peters et al. [2018], respectively, are employed. Both are pre-trained on chemical\npatent documents from Zhai et al. [2019]. These embeddings are fixed during training. The additional\nlearnable feature embeddings $e_{f_i}^{FT}$ (in Equation 1) are based on the output of a chemical named entity\nrecognizer Zhai et al. [2019].\nThe paragraph encoder then generates a single embedding vector for the entire paragraph using\na BiLSTM over the concatenated token embeddings. The paragraph decoders take in the para\nembeddings to output a B/I/O tag for each paragraph. The following 3 decoder models are used for\nthe 3 baselines as shown in figure 1. The linear softmax decoder considers just the current paragraph"}, {"title": "Our Approach", "content": "We utilize the paragraph encoders described in [1]. For the decoder, we experiment with both the\ntrigram and BiLSTM CRF-based architectures. We did run 1 experiment with the linear-softmax\nbased decoder using our approach described below but choose not to use it in other experiments due\nto its sub-optimal results."}, {"title": "BERT Embeddings", "content": "For the 2 baseline models described above, we replace the first embedding layer that feeds into the\nparagraph encoder by BERT-based embeddings. BERT is a SOTA model used for a wide range\nof NLP and IR tasks and we wanted to assess the performance of the baseline architecture with\nBERT embeddings. We conducted experiments using raw token embeddings generated without\nfinetuning the base BERT (bert-base-uncased), with embeddings generated by finetuning base BERT\n(bert-base-uncased)."}, {"title": "ChemBERT", "content": "We also experimented with 2 different pretrained BERT models:\n\u2022 recobo/chemical-bert-uncased-pharmaceutical-chemical-classifier: Chemical domain\nBERT model finetuned on 13K Chemical, and 14K Pharma Wikipedia articles broken into\nparagraphs.\n\u2022 recobo/chemical-bert-uncased: BERT-based model further pre-trained from the check-\npoint of SciBERT using a corpus of over 40,000+ technical documents from the Chemical\nIndustrial domain and combined it with 13,000 Wikipedia Chemistry articles, ranging from\nSafety Data Sheets and Products Information Documents."}, {"title": "Sentence-level Sequence Tagging", "content": "During our experiments, we observed that most of the errors were witnessed at the end boundary\nof the reaction, especially where properties of the newly synthesized compound were being listed.\nSome patents contained these properties in the same paragraph as the reaction paragraphs while\nothers mentioned them in a separate paragraph. So the models always made errors while determining\nthe correct end boundary for the reaction span. This was actually an artifact of our ground truth\ntagging scheme. Because we formulated the problem as a paragraph-level sequence tagging task, we\nconsider the entire paragraph as a reaction even though a part of it is outside the actual annotated\ncharacter-level reaction span. About 8% of the reaction paragraphs in the train set contained more\n40% non-reaction characters.\nHence, we experimented with sentence-level encoding and tagging. The architectures here are the\nsame as that of the paragraph-level tagging task. Instead, the training and validation data contain tags\nat the sentence level."}, {"title": "[CHEM] tokens", "content": "When we checked the generalization performance of the models outlined above on the out-of-domain\ngeneralization set, we observed that most of the mistakes occurred where the reaction paragraph\ndescribed a reaction without any chemical names. This could be paragraphs describing general\nreaction recipes or that the domain addresses its materials by common language names instead\nof their chemical nomenclature. In order to combat the model overfitting to chemical compound\nnames, we replace all chemical names in the train, validation, test and generalization set with a\nnew token [CHEM]. This was added to the BERT layer as a special token. We then finetuned\nthe bert-base-uncased and chemical-bert-uncased-pharmaceutical-chemical-classifier with this new\ntoken."}, {"title": "Evaluation Metrics", "content": "For model selection we use the span-based scores based on a strict match strategy, where an output\nspan is regarded as correct if the beginning and ending paragraphs strictly match those of the gold\nspan. In some practical applications, it also makes sense to understand if the model can identify the\napproximate region where a reaction is described. Thus, for evaluation, we also compute the scores\nbased on a fuzzy match strategy, where we calculate the number of matches by counting the number\nof gold spans that have at least one corresponding predicted output span whose beginning and ending\nparagraph indices are at most 1 paragraph away from the gold ones.\nWe report the Accuracy, Precision, Recall and F1 score for each model on the test and generalization\ndatasets according to the strict and fuzzy match criteria defined above."}, {"title": "Results and Error Analysis", "content": "We first evaluate the performance of the various approaches over the in-domain test set. Note that we\ndo not show the results of the sentence based models since these results on the test and generalization\nare extremely bad. Sentence based tagging needs the model to capture very long range dependencies\nbecause a single reactions paragraph can contain tens of sentences. Hence, capturing multi-para\nreactions necessitates attending to a long sequence of reaction sentences which does not work well at\nall."}, {"title": "Performance on In-Domain Test Set", "content": null}, {"title": "Trigram vs BILSTM CRF decoder for base BERT models", "content": null}, {"title": "Base BERT vs ChemBERT models", "content": null}, {"title": "Effect of Introducing [CHEM] tokens", "content": "The main observation in this scenario is that unlike the previous case, here, the base BERT model\noutperforms the ChemBERT-based model. However, this is not surprising because ChemBERT has\nbeen trained to recognize and embed chemical names and tokens which are masked in this scenario.\nHence, using ChemBERT in the absence of any chemical entity tokens hurts the model. We also\nobserve that the [CHEM] masked model with base bert performs similarly to the non-CHEM masked\nmodel from table 4. This hints toward the fact that the model actually learns the structure of the\nreaction paragraphs rather than basing the reaction or no-reaction decision on the presence or absence\nof certain chemical entities which might be different across different domains.\nWe now look at the generationalization performances of some models selected from the analysis\nabove."}, {"title": "Generalization Performance Analysis", "content": "While the strict match performance of all the models here is the same, we see that the results for\nthe fuzzy match are quite the opposite of those seen for the test set. The ChemBERT-based models\nsignificantly outperform the base BERT models. Even for the approach consisting of [CHEM] tokens,\nthe ChemBERT based embeddings provide much better results. One plausible explanation for this\nmight be the fact that this chemical domain pretrained model is actually learning the structural\ncomponents of reaction paragraphs which are similar across domains. Just the entities in the reactions\nare different for the different domains and thus, replacing those with [CHEM] tags reduces the noise\nand enables the model to focus on the structure of the paragraph to determine whether it is a reaction\nparagraph or not.\nA main problem observed across the different models for the generalization set is the poor recall\nof these models. Both the ChemBERT models have also performed significantly better in terms of\nextracting more reaction spans present in the patents, demonstrating generalizability across different\ndomain chemical patents."}, {"title": "Discussion and Future Direction", "content": "In this project, we primarily explored various methods to extract reaction spans from patents and\nevaluated their performance in both in-domain and out-of-domain chemical patent documents. Our\nfindings highlight several areas where current models struggle. Firstly, the models often perform\npoorly for reaction spans exceeding three paragraphs. Secondly, they encounter difficulties in\naccurately demarcating boundaries between consecutive reactions, especially when multiple reactions\nfollow one another without clear headings. Additionally, the models are not well-equipped to handle\nreaction snippets embedded within tables.\nMoving forward, our research will focus on two main directions. Firstly, we plan to explore the\nimpact of multi-task learning on reaction extraction. Specifically, we aim to evaluate the performance\nof a BERT-based model trained jointly for Chemical Named Entity Recognition (NER) and reaction\nextraction. This approach will help us determine whether learning structural components of the text\nthrough joint training for these two tasks enhances the model's ability to generalize across domains.\nSecondly, we intend to create a comprehensive database of reactions extracted from chemical patents\nfor use in downstream tasks.\nMoreover, there is a pressing need for a standardized benchmark dataset for the Reaction Extrac-\ntion task. Although our results demonstrate superior performance compared to previous work by\nYoshikawa et al., we emphasize the importance of a gold standard dataset with annotations for a\nbroader set of patents. Such a benchmark is crucial for fair comparisons across different methodolo-\ngies in this area."}, {"title": "Conclusion", "content": "We have explored different approaches to extracting reaction spans from chemical patent documents\nand evaluated the generalization of these models across various chemical domains. We seek to\ncontinue to improve this task in order to create a valuable resource of reactions for the research\ncommunity."}]}