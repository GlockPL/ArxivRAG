{"title": "Improving Medical Diagnostics with Vision-Language Models: Convex Hull-Based Uncertainty Analysis", "authors": ["Ferhat Ozgur Catak", "Murat Kuzlu", "Taylor Patrick"], "abstract": "In recent years, vision-language models (VLMs) have been applied to various fields, including healthcare, education, finance, and manufacturing, with remarkable performance. However, concerns remain regarding VLMs' consistency and uncertainty, particularly in critical applications such as healthcare, which demand a high level of trust and reliability. This paper proposes a novel approach to evaluate uncertainty in VLMs' responses using a convex hull approach on a healthcare application for Visual Question Answering (VQA). LLM-CXR model is selected as the medical VLM utilized to generate responses for a given prompt at different temperature settings, i.e., 0.001, 0.25, 0.50, 0.75, and 1.00. According to the results, the LLM-CXR VLM shows a high uncertainty at higher temperature settings. Experimental outcomes emphasize the importance of uncertainty in VLMs' responses, especially in healthcare applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the use of artificial intelligence (AI) has led to the development of large language models (LLMs) and vision language models (VLMs), with remarkable performance. These technologies have been extended to multimodal LLMs (such as GPT-4V [1], LLaVA [2], CogVLM [3], CLIP [4]), which enhance problem-solving capabilities by evaluating text, audio/speech, images, and videos. Multimodal LLMs also have the potential to be applied to medical and clinical scenarios to improve classification, question answering, informed decision making, efficiency, educational methods, patient care, and minimize medical mistakes [5]. Models such as GPT-4 with vision (GPT-4V)\u00b9 are large-scale multimodal models, which can accept image and text inputs and produce text outputs [1]. GPT-4 is a transformer-based model pre-trained to predict the next token in a document exhibiting human-level performance on various professional and academic benchmarks [1]. GPT-4 has also shown promise in medical and clinical tasks. Guerra et al. found that GPT-4 outperforms ChatGPT, medical students, and neurosurgery residents on neurosurgery written board-like questions [6]. Zhou et al. examined OpenAI's generative pre-trained transformer with vision potential (GPT-4) for automated image text pair generation, noting that it has shown promise in understanding natural images, but had limited effectiveness in interpreting real-world chest radiographs [7]. Llama is a collection of pre-trained and finetuned (chat and dialogue cases) foundational LLMs ranging in scale from 7 to 70 plus billion parameters. Some foundational LLMs such as GPT-4 and Llama (versions 1 through 3) [8], [9] have been adapted to function as VLMs, with existing vision models, to facilitate multimodal predictions and generations.\nVLMs are models that utilize both image and text information to perform complex reasoning tasks and human-level language comprehension for enhanced decision-making support compared to unimodal models. VLMs are often configured with fused individual unimodal vision and language AI models to perform multimodal classifications, predictions, and/or generations given an input of image and/or text [10]. By mimicking the multimodal nature of clinical expert decision-making, VLMs can significantly enhance medical diagnosis and decision-making through improved predictive performance utilizing multimodal health information (signs, symptoms, imaging, written reports, physiological and laboratory measurements) [11].\nIdeally, VLMs aim to achieve expert human-level functioning, as medical tasks are challenging without the use of AI/ML or computer assistance due to image-text complexity, variability, noise, and resolution. LLM extensions to VLMs have been explored for medical and clinical tasks and applications. For example, Wang et al. developed DRG-LLaMA [12], which tuned LLama to predict the diagnosis-related group for hospitalized patients and found that performance was correlated with increased model parameters and input context lengths. Additionally, Sandmann et al. performed a systematic analysis of ChatGPT, Google search, and Llama 2 for clinical decision support tasks [13]. Progress in LLMs has made the generation of realistic image caption tasks viable and expansive. However, these models often struggle to make accurate, accurate, relevant, and consistent statements, which in turn negatively affects their trustworthiness and reliability [14]. This is especially true of non-fine-tuned models. The"}, {"title": "II. PRELIMINARIES", "content": "Uncertainty quantification (UQ) has received more attention in the context of generative AI (GAI), particularly critical applications using LLMs in healthcare [15], [16], [17], [18]. Uncertainty can originate from various factors such as the model's architecture, the parameters that define the model, the dataset [19], [20], and the overall performance of LLM/VLM. Training data can also contribute significantly to the uncertainty as a result of the complexity and diversity of the dataset. VLMs often struggle to provide accurate or true conclusions and representations on tasks. This low performance is due to the improper analysis and comprehension of information from multiple modalities. VLMs that perform Vision Question Answering (VQA) tasks have been shown to lack robustness and are severely prone to overfitting on dataset-specific correlations rather than learning to answer questions [21]. VQA models often use simple rules, based on co-occurences of objects with noun phrases and linguistic priors, to answer questions (e.g., the fox is red) rather than referring to the image for context (e.g., the fox is white) [21]. VLMs may also override visual information and substitute or prioritize prior learned (visual and language) information due to co-association. This phenomenon is referred to as poor visual grounding, meaning that VLM inferences and information from one modality are prioritized over the other modality(s) and as a result the performance of the model often suffers [22]. Therefore, cross-modal alignment, multimodal attention, and prioritization are important concepts when evaluating multimodal consistency and hallucination. Inconsistency represents the uncertainty and confusion of the model towards a given task, to be a contributing factor to various types of hallucination in language and vision-based models [23], [14]. Khan et al. hypothesized that VQA models answer simpler questions more consistently, with VQA task inconsistency on linguistic variations being often indicative of a more superficial understanding of the question content [21]. As a result, the answer provided is more likely to be wrong or factually incorrect. Since consistency and confidence have been shown to not be equivalently related with respect to questions and answers, the predictive uncertainty of the model can be quantified using consistency instead of accuracy-based predictions [21]. Uncertainty can be quantified by metrics such as the Attribution Based Confidence (ABC) metric, based on the feature attribution guidance, which uses specifically integrated gradients to perturb samples in a feature space, and then evaluates consistency over the perturbed samples [24].\nIn a black box model, where features are inaccessible, there is often no direct way to explore the input neighborhood in a feature space [21]. Often only the raw confidence scores for answer candidates are available with black box models, as a result the confidence of the most likely answer may be utilized as the uncertainty [21]. Alternatively, when features are inaccessible, rephrasing can be applied where an alternate surface form of the input is mapped closely to the original input in feature space [21]. Khan et al. have found that consistency in rephrasing is an effective step in evaluating black-box VQA models for predictive uncertainty, especially when the answers of queries are unknown [21].\nIt is important to understand how similar two QA responses are to each other, with regard to content and understanding, to determine how consistent or uncertain a model is [14]. Language models often employ decoding strategies to improve language generation quality, such as re-ranking, temperature sampling, top-k sampling, top-p sampling, nucleus sampling, typical decoding, and minimum Bayes risk decoding [25]. Self-consistency is applicable to improving the performance of a wide range of reasoning tasks without any additional supervision, training, data collection nor finetuning. Wang et al. determined, for a given model, the optimal answer by marginalizing out sampled reasoning paths to find the most consistent answer in the final answer set [25]. Self-consistency avoids the repetitiveness and local optimality of greedy decoding algorithm methods, while mitigating the stochasticity of a single sampled generation [25]. Consistency and self-consistency can be extended to open-ended text generation tasks. This is possible if a good consistency metric can be defined between multiple generations of text, i.e., whether the answers agree or contradict [23]. According to Zhang et al., multiple types of consistency exist that can affect the model, such as inner and outer consistency [23]. Inner inconsistency refers to a model responding 'yes' to even contradictory questions. As a result, it is unclear whether the model accurately comprehends the truth of the ground or exhibits confusion, thus contributing to hallucination [23]. Outer inconsistency refers to a model responding 'no' to its own answer, and as a result it conflicts with itself, which is inconsistent. This outer inconsistency further reveals the uncertainty of the model about the query and may contribute to hallucination [23]. Inner and outer consistency can be utilized to evaluate the performance of various language tasks such as binary classification questions (yes/no/counting) or comparison questions, but may not fully capture the model's ability to answer open-ended questions [23]. Therefore, we can achieve a more comprehensive understanding of model uncertainty, reliability, and hallucination by analyzing multiple types of consistency for model outputs [23].\nPrior studies leveraged synonyms to evaluate LLMs and can be extended to text generation tasks for VLMs where prompts are used to generate a list of semantically similar synonyms for every object class [14]. For example, a LLM/VLM pre-trained on instances of the chair class referenced with synonyms such as (chair, seat, couch, etc.) would be expected to have embeddings closely located in a shared embedding space. Recent studies show that synonym consistency can be utilized"}, {"title": "B. Temperature Setting and Sensitivities", "content": "Vision-language models (VLMs) use visual and textual datasets to generate content combining image and language modalities. Temperature settings and sensitivities during training and inference can significantly impact the performance of VLMs. The temperature setting is applied in the softmax function to regulate the sensitivity of the resulting probability distribution. Lower temperatures make the distribution more confident (peaky), while higher temperatures make it more uniform. Techniques that alter diversity in language models for text generation tasks such as question answering, image captioning, open-ended answer dialogue, and machine translation must control the relative trade-off between quality and diversity [26]. Decoding methods such as nucleus sampling, top-k and top-p sampling, and temperature sampling allow for control of model output diversity and quality [26]. These methods can be quickly implemented on top of pre-trained language-based models. Temperature sampling \"divides the logits of each token by the temperature hyperparameter before normalizing and converting the logits into sampling probabilities\" to re-estimate the softmax distribution [26], [27]. This is often used in natural language generation to reshape the probability distribution by introducing a temperature coefficient T to control the level of sampling randomness for model uncertainty, robustness, and reliability tests [27]."}, {"title": "C. Convex Hull", "content": "The convex hull of a set of points is a fundamental mathematical structure utilized in statistics and computational geometry [28]. It is an important statistical problem with many applications in location-based services, computer vision (image processing, pattern recognition), robotic sensor databases, statistical analysis, and data mining [29], [30], [31]. The convex hull problem is meant to handle data uncertainty of individual points over a given area, with numerous existing algorithms that attempt to compute the probability of a query point lying inside the convex hull of the input. Considerations for convex hull solving algorithms include computational efficiency, time-space trade-offs, and effectiveness [30]. Statistical information can be utilized to find the best representation of the probability distribution of the query data point, specifically data in which the location and potentially the relative location is uncertain (and potentially changing) inside the convex hull. The convex hull problem is often investigated under two models of uncertainty, unipoint and multipoint: the unipoint or tuple model, where each point has a fixed position but only exists with some probability (0 to 1), and the multipoint model with each point having multiple possible locations or not appearing at all [32]. Some algorithms for determining parameters regarding the convex hull include variations of the gift-wrapping algorithm, divide and conquer algorithm, and incremental algorithm [33], [34].\nIn this study, a convex hull-based approach is used to evaluate the uncertainty of VLMs' responses for a selected healthcare application using the VQA task."}, {"title": "III. EXPERIMENTAL SETUP COMPONENTS", "content": "This section provides a brief description of experimental setup components, including the VLM model (LLM-CXR), the temperature settings, and the chest X-ray dataset."}, {"title": "A. LLM-CXR", "content": "In this study, the LLM-CXR model [35] is utilized as the VLM, which is an Instruction Finetuned LLM for CXR Image Understanding and Generation. This multimodal model was developed for clinical and medical applications, specifically chest X-rays (CXRs). The LLM-CXR model can perform several VLM tasks including image captioning, visual question answering (VQA), natural language comprehension, and image generation. It was developed based on an approach introduced in previous work [36], which features a transformer and the architectural component VQGAN combination for the generation of bidirectional images and texts. The group developed instruction fine-tuning methods for pre-trained LLMs to be modified to operate as a multimodal vision language model. The modification process produces a final VLM model capable of input and output in both text and image format but involves no modification of the original LLM model structure or objectives.\nSpecifically, the LLM-CXR utilizes an image adapter module to tokenize image inputs. These image tokens are then fed into an LLM along with other word tokens. The LLM used involves a fine-tuned dolly-v2-3b model [37], which is fine-tuned for the instruction-following task based on the GPT-NeoX architecture, as a base model [35]. The output of the LLM, the text tokens, and features, are fed to another adapter combined with an image generative model capable of generating multiple modalities for multiple tasks. This model was trained on the MIMIC CXR JPG dataset [38].\nFor the case of this presented method, images are tokenized by VQGAN. VQGAN is frozen during LLM training for clinical information-preserving CXR tokenization. Then the token embedding space is expanded for the LLM for further training and fine-tuning. The next data augmentation was performed with a synthetic VQA to evaluate the pertinence of language comprehension and enhance vision language alignment using Llama 2 to generate CXR questions and answers for training the LLM-CXR. Finally, image text bidirectional instruction fine-tuning was applied to optimize the LLM to address the following tasks and experiments:\n\u2022 NL-IF task: Natural Language Instruction-Following (NL-IF) involves the use of the NL-IF dataset when fine-tuning the LLM-CXR to perform instruction following tasks.\n\u2022 CXR to report generation task: Generate CXR reports given CXR images using LLM-CXR, and the following performance evaluation techniques for image understanding: CheXpert labeler model, ROUGE L, METEOR, CIDEr. The similarity between reports and ground truth reports was evaluated using AUROC/F1 and Jaccard similarity.\n\u2022 CXR VQA task: Ask questions about the presence, location, and severity of lesion or findings for each CXR image and notes using the MIMIC CXR dataset.\n\u2022 Report to CXR generation task: LLM-CXR generates CXR images matching chest X-rays described in a given text report. The ground truth in this case is original CXR images from MIMIC CXR JPGs and was compared to generated images calculating AUROC/F1.\nThe overall results for each of the LLM-CXR tasks demonstrate comparable or better performance to similar models at the time of publication [35]. One major issue with the LLM-CXR model is that when the same inquiry (question) and image is repeatedly asked of the model, often inconsistent and potentially incorrect answers are provided. This paper explores a method to evaluate and potentially improve issues where a model does not provide equally valuable and similar answers when given the same input for the image and the text-based question."}, {"title": "B. Temperature Settings", "content": "The transformer base class pre-trained configurations implement methods for loading/saving a pre-existing configuration from local or online library/repository. Each derived config class implements model specific attributes such as parameters linked to tokenizing, fine tuning tasks, and sequence generation.\nThe temperature (T) is an optional positive value that typically defaults to 1.0. The temperature is used to model the next token probabilities that will be used by default in the generate method of the model. Temperature is one of the crucial parameters in both LLMs and VLMs, which affects creativity and accuracy, i.e.,, low temperature (0 or near 0) offers more precise and repetitive outputs, while high temperature (\u22651) offers more diverse and random outputs.\nIn this research, the temperature value is defined in the LLM-CXR model initiation function code in the ranges (0.001, 0.25, 0.5, 0.75, 1.00) applied for 30 trials per image in the chest radiograph dataset [39]."}, {"title": "C. Chest X-ray Dataset", "content": "The chest X-ray dataset features a public open dataset of chest X-ray and computed tomography (CT) images of patients that are positive or suspected of COVID 19 or pneumonia (either viral or bacterial such as MERS, SARS, or ARDS) [39]. Data were collected from public sources, hospitals and physicians and have been published in the corresponding GitHub repository\u00b2."}, {"title": "IV. EXPERIMENTAL SETUP AND RESULTS", "content": "This section describes the overall experimental setup and discusses the results for different temperature settings in terms of uncertainty.\nOur codes are released on GitHub for scientific use.\u00b3"}, {"title": "A. Experimental Setup and Uncertainty Evalaution", "content": "The overview of the experimental setup for calculating the uncertainty in the VLM responses is shown in Figure 1, derived from the study [40]. The figure illustrates the overall experimental setup, from the inputs of chest X-ray images to uncertainty evaluation of the responses based on a convex hull-based approach. The setup starts with three inputs, that is, multiple chest X-ray images, a given prompt (\"Generate a comprehensive radiology report for the entered chest X-ray image.\") to generate a radiology report, and a temperature setting. These inputs are processed by the selected VLM, i.e., LLM-CXR, to understand the visual content of the X-ray images and generate radiology reports. In this setup, 30 different responses were generated for each chest X-ray. The diversity in responses is controlled by a temperature input set to different values (0.001, 0.25, 0.50, 0.75, and 1.00), influencing the diversity of the reports generated. These responses are then encoded into high-dimensional embeddings using a BERT model. Then, the embeddings, initially in a high-dimensional space, are projected onto a 2D space using Principal Component Analysis (PCA) for easier visualization and clustering using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, which identifies groups of similar responses. For each cluster, a convex hull is finally computed, representing the smallest convex boundary, i.e., the area of each convex hull, that encloses all points in the cluster. The total area is a measure of the uncertainty of the model's responses to the given prompt. The example shown includes a plot of the 2D embeddings with the convex hull of the densest cluster highlighted, illustrating the spatial distribution and clustering of the responses.\nFor the given prompt (p \u2208 P), the responses are generated, R(p) = {1,2,...,rn}, by the selected VLM, LLM-CXR, where n = 30 is the number of responses.\nThe embeddings for each response r \u2208 R(p) are calculated using a pre-trained BERT model, and given as follows:\nE(r) = BERT(r)\nwhere E(r) \u2208 Rd represents the embedding vector in a d dimensional space, encapsulating the semantic content of the response within a high-dimensionalfeature space."}, {"title": "B. Analysis of Convex Hull-based Uncertainty Quantification", "content": "The method relies on embedding the model's responses in a high-dimensional space, clustering these responses, and using the convex hull of the clusters to measure uncertainty.\n1) Mathematical Justification: Given a prompt p, let R(p) = {r1,r2,...,rn} represent the set of responses generated by the VLM. Each response ri \u2208 R(p) is embedded into a d-dimensional space, resulting in an embedding vector E(ri) \u2208 Rd.\nThe following analysis proves why the convex hull-based approach captures uncertainty effectively.\nThe convex hull of a set of points is the smallest convex set containing all the points. The geometric properties of the convex hull make it a suitable tool for measuring the diversity (and thus the uncertainty) of responses, since the area (or volume in higher dimensions) of the convex hull reflects the spatial spread of the points.\nLemma: The area of the convex hull ConvexHull(E(R(p))) increases as the diversity of the model's responses increases.\nProof: Let E(R(p)) = {E(r1), E(r2), ..., E(rn)} be the set of response embeddings. As the diversity of the responses increases, the distances between embedding points in E(R(p)) will also increase. The convex hull is the minimum convex set containing all these points, and its area is proportional to the spatial distribution of the points.\nLet S \u2282 Rd be the set of embeddings with larger pairwise distances between points. By the properties of convex sets, the convex hull of a larger set S will have a greater area than that of a set with points more closely clustered together:\nA(ConvexHull(S1)) < A(ConvexHull(S2)) if\n||E(ri) \u2013 E(rj)|| < ||E(r) \u2013 E(r';)||\nwhere S1 and S2 are two sets of embeddings with increasing diversity. Thus, a more diverse set of responses corresponds to a larger convex hull area, indicating greater uncertainty.\nTo capture the structure of response embeddings, we apply the DBSCAN algorithm, which identifies clusters of similar responses. If the responses generated by the VLM are consistent, the embeddings will form tight clusters with small convex hull areas. In contrast, if the responses are highly uncertain, the clusters will be more dispersed, leading to larger convex hull areas.\nLemma: The total uncertainty for a prompt p is proportional to the sum of the convex hull areas of all clusters generated by DBSCAN.\nProof: Let L = {C1, C2,..., ck} be the set of clusters identified by DBSCAN. For each cluster ci, we compute the convex hull ConvexHull(ci) and its area A(ci). The total uncertainty is then:\nk\nA(p, t) = \u2211 A(ConvexHull(ci))\ni=1\nSince the area of each convex hull A(ConvexHull(ci)) reflects the diversity within each cluster, the sum of these areas measures the overall spread of the response embeddings. A larger total area corresponds to more spread out clusters, which indicates greater uncertainty in the model's responses.\n2) Temperature Sensitivity and Uncertainty: The temperature parameter t affects the stochasticity of the VLM's outputs. As t increases, the model produces more diverse and uncertain responses. Formally, for a higher temperature t, the spread of the embedding points increases, leading to larger convex hulls:\n\u0414\u0410(p, t)\n>\n0\n\u2202t\nThis shows that the uncertainty A(p, t) increases with the temperature, reflecting the model's sensitivity to the temperature parameter.\nThe convex hull-based method works for uncertainty quantification because it leverages the geometric properties of the response embeddings. By clustering and measuring the area of the convex hulls, the method captures both the consistency and the diversity of the model's responses. As the diversity of the responses increases, the convex hull area grows, reflecting higher uncertainty. Therefore, the proposed approach provides a sound theoretical foundation for quantifying uncertainty in VLM outputs."}, {"title": "C. Experimental Results", "content": "In this study, five different cases are conducted to evaluate the uncertainty of the selected VLM's responses at different temperature settings, i.e., 0.001, 0.25, 0.50, 0.75, and 1.00. 30 different radiology reports were generated for each image in the X-ray dataset given the prompt, i.e., \"Generate a comprehensive radiology reports for the entered chest X-ray image.\"\n1) Case Study I: A temperature setting of 0.001: Figure 2 shows a histogram representing the uncertainty distribution of the convex hull areas in the reports generated from the VLM at a temperature setting of 0.001. The temperature setting is selected to be close to 0, i.e., 0.001, since the temperature value must remain positive. The temperature value can also be set to 0 for tasks requiring more reliable responses but is not ideal for tasks requiring creativity or varied responses. In Figure 2 and subsequent histogram figures, the x-axis indicates the convex hull area, while the y-axis denotes the frequency, i.e., the number of occurrences of each convex hull area. According to Figure 2, there is almost no degree of diversity in the responses from VLM. The histogram shows a peak at low convex hull areas close to 0, i.e., the selected VLM has a strong tendency to generate confident responses at very low temperature settings. It is also expected that a low temperature setting (close to 0) results in low diversity in responses generated by the model. The figure also shows a small distribution across a range of higher convex hull areas with lower frequencies. This indicates that very few responses fall into the range of higher uncertainty. In general, the histogram figure indicates that most responses generated by VLM are highly certain or low uncertainty at a temperature setting of 0.001.\nFigure 3 shows the two most uncertain instances at a temperature setting of 0.001 based on convex hull areas on the contour maps. Each subfigure represents a 2D visualization corresponding to an embedding of a generated response. The least uncertain instances are also generated on the contour maps; however, they are not given in this study since showing very little or no uncertainty, i.e., the model responses remain consistent and certain under this deterministic setting. In the figure, the red dots represent the data point (generated response in 2D) for each instance, and the background color ranges from purple to turquoise and yellow, with yellow representing areas of higher uncertainty, and turquoise areas indicating lower uncertainty. It is expected to see one or more convex hulls (outlined by a dashed line) evaluate the level of uncertainty. However, in this case, all most uncertain instances show a similar pattern with turquoise and yellow areas near the red dots, but without any cluster. The model generated more consistent and confident responses at a temperature setting of 0.001 as expected.\n2) Case Study II - A temperature setting of 0.25: Figure 4 shows a histogram representing the uncertainty distribution of the convex hull areas in the reports generated from the VLM at a temperature of 0.25. According to the figure, there is some degree of diversity in the VLM's responses, which do not exist at the temperate setting of 0.001. The histogram shows a highly skewed distribution with a significant concentration of responses around a convex hull area close to 0, i.e.,"}, {"title": "3) Case Study III: A temperature setting of 0.50:", "content": "Figure 6 illustrates a histogram depicting the distribution of the convex hull area values corresponding to the uncertainty of the responses generated from the VLM at a temperature setting of 0.50. In this medium-level temperature setting, the VLM generates responses with a balanced level of diversity. The histogram displays two distinct patterns, i.e., a sharp peak at a convex hull area close to 0, and a norma distribution centered around a convex hull area of 25. The sharp peak close to 0 indicates that a significant number of generated responses have very low uncertainty with high confidence, while the normal distribution curve indicates a wide range of responses with moderate uncertainty.\nFigure 7 depicts the two most uncertain instances at a temperature setting of 0.50 on a contour map. The figure consists of two plots with the background colored contours ranging from purple (lower uncertainty) to turquoise and yellow (higher uncertainty) to emphasize the degree of confidence. These two instances have dark dashed forms that enclose groups of points, outlining the convex hull to show the area where the uncertainty is highest. As the temperature setting is increased, the model tends to be less confident and more uncertain in each instance, as indicated by the presence of larger convex hulls and more significant yellow areas."}, {"title": "4) Case Study IV: A temperature setting of 0.75:", "content": "Figure 8 provides a histogram showing the uncertainty distribution of the responses generated by VLM based on the area of the convex hull at a temperature setting of 0.75. The histogram reveals a bimodal distribution with two peaks, i.e., the main peak is around a convex hull area of about 30, while a smaller peak is present about 10. The main peak indicates that the most common convex hull area (uncertainty) is around 30. Compared to previous cases, it does not have a sharp peak near 0. In other words, it indicates that most generated responses have a moderate level of uncertainty and fairly consistent reports with controlled uncertainty when the temperature is set to 0.75.\nFigure 9 shows the most uncertain instances (a-b) for a temperature setting of 0.75 through a contour map. The temperature setting of 0.75 introduces more diversity and uncertainty regarding the VLM responses. Each instance shows different instances of uncertainty, highlighting areas where the model has low confidence in its responses. As expected, at a higher temperature, the model shows a higher uncertainty, as seen in two instances."}, {"title": "5) Case Study V: A temperature setting of 1.00:", "content": "Figure 10 presents a histogram illustrating the distribution of the convex hull area for the responses generated from VLM when the temperature is set to 1.00. The histogram reveals a normal distribution and one pattern compared to the previous cases, i.e., a bell-shaped, relatively symmetrical distribution. In other words, it has no sharp peak close to 0 and no peak at low convex hull areas, i.e., neither very low uncertainty nor high confidence. It also indicates that most generated responses have a moderate or high level of uncertainty when the temperature is set to 1.00. According to the histogram, the VLM generates fairly consistent reports with controlled uncertainty under a very high temperature setting. It also highlights the uncertainty concerns in VLM's responses.\nFigure 11 shows a significant increase in uncertainty due to leading to less confidence in responses at higher temperature settings. In the figure, each plot visualizes one instance with its associated uncertainty, outlined by a convex hull, as in the previous figures. For two instances, the model tends to moderate and high uncertainty, as antipicated."}, {"title": "V. DISCUSSION AND OBSERVATIONS", "content": "The results provide valuable insights into the uncertainty of VLM's responses at different temperature settings, i.e., 0.001, 0.25, 0.50, 0.75, and 1.00. The temperature parameter plays a crucial role in the diversity of the VLM's resonances, directly affects the uncertainty. Observations are given for each case below.\n\u2022 At a temperature setting of 0.001: Most responses from the VLM have low uncertainty with highly confident, with minimal diversity. The histogram shows a peak in the lower convex hull areas, i.e., close to 0. The most uncertain instances also show minimal convex hull areas and consistent responses.\n\u2022 At a temperature setting of 0.25: The histogram shows a skewed distribution, with a significant peak at low uncertainty (close to 0) and a scattered distribution across a range of higher convex hull areas at high uncertainty. This indicates that the model generates responses with confident responses, as well as diverse responses with varying levels of uncertainty.\n\u2022 At a temperature setting of 0.50: The VLM shows a high uncertainty in its responses with two distinct patterns, i.e., a sharp peak close to 0, and a normal distribution centered around a convex hull area of 25. This indicates that the model can generate responses with high confidence while generating responses with high uncertainty.\n\u2022 At a temperature of 0.75: The results show a bimodal distribution with a main peak at a convex hull area of 30 and a smaller peak around 10. Unlike lower temperature settings, there is no sharp peak near 0, indicating that most responses demonstrate moderate or high uncertainty.\n\u2022 At a temperature setting of 1.00: VLM's responses tend to a high uncertainty. The histogram reveals a normal distribution for convex hull areas with respect to the uncertainty of VLM-generated responses, i.e., no sharp peak near 0. The normal distribution without sharp peak near 0 indicates higher diversity and uncertainty in the model responses.\n\u2022 The most uncertain instances indicate the importance and uncertainty concerns of VLM's responses at high-temperature settings. VLM generates diverse responses and leads to higher uncertainty at high temperature settings.\nIn addition to the results given in figures, Table I provides detailed statistical results, i.e., mean, standard deviation (Std), minimum (Min), maximum (Max) and cumulative averages within a certain percentage of the results (10%, 25%, 50%, 75%, 90%), regarding the evaluation of uncertainty in responses generated by VLM for temperature settings. These results provide a detailed view of the uncertainty in responses from VLM.\n\u2022 Mean: The average values increase steadily from 0.001 at the temperature setting of 0.001 to 0.3117 at the temperature setting of 1.00, indicating a trend that increases as the temperature increases. The difference between the lowest and highest mean values is 3115 times. As anticipated, the increase in temperature leads to greater diversity in responses, contributing to higher overall uncertainty in the model's responses.\n\u2022 The standard deviation (Std): It represents the variability or spread in the responses, with values ranging from 0.0012 at the temperature setting of 0.001 to 0.1441 at the temperature setting of 1.00. The relatively high standard deviation at intermediate temperatures (e.g., 0.1680 at the temperature setting of 0.50) indicates greater diversity at these settings. The difference between the lowest and highest Std is 114 times.\n\u2022 Minimum (Min) and Maximum (Max): Min values are 0.0000 for all temperature settings. This means that the model can also provide confident responses at high temperature settings. On the other hand, maximum (Max) values vary from 0.0232 at the temperature setting of 0.001 to 0.7257 at the temperature setting of 1.00. The diffrence between the lowest and highest max is 31 times.\n\u2022 Cumulative Averages: The percentage-based values, i.e., 10%, 25%, 50%, 75%, and 90%, illustrate the cumulative averages of the uncertainty values within the selected percentage. In the table, the 10% refers to the average value of the lowest 10% of uncertainty values while 90% the average of the lowest 90% of that. These cumulative averages reveal how uncertainty behaves across different portions of the dataset. It increases along with a high percentage and temperature setting."}, {"title": "VI. CONCLUSION", "content": "This study proposes a convex hull-based approach to quantifying uncertainty in Vision-Language Models (VLMs) applied to generating radiology reports. In this study, the LLM-CXR is selected as the VLM, and radiology reports are generated from chest X-ray images for the given prompt at various temperature settings (0.001, 0.25"}]}