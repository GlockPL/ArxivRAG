{"title": "MVP-Bench: Can Large Vision\u2013Language Models Conduct Multi-level Visual Perception Like Humans?", "authors": ["Guanzhen Li", "Yuxi Xie", "Min-Yen Kan"], "abstract": "Humans perform visual perception at multiple levels, including low-level object recognition and high-level semantic interpretation such as behavior understanding. Subtle differences in low-level details can lead to substantial changes in high-level perception. For example, substituting the shopping bag held by a person with a gun suggests violent behavior, implying criminal or violent activity. Despite significant advancements in various multimodal tasks, Large Visual Language Models (LVLMs) remain unexplored in their capabilities to conduct such multi-level visual perceptions.\nTo investigate the perception gap between LVLMs and humans, we introduce MVP-Bench, the first visual-language benchmark systematically evaluating both low- and high-level visual perception of LVLMs. We construct MVP-Bench across natural and synthetic images to investigate how manipulated content influences model perception. Using MVP-Bench, we diagnose the visual perception of 10 open-source and 2 closed-source LVLMs, showing that high-level perception tasks significantly challenge existing LVLMs. The state-of-the-art GPT-4o only achieves an accuracy of 56% on Yes/No questions, compared with 74% in low-level scenarios. Furthermore, the performance gap between natural and manipulated images indicates that current LVLMS do not generalize in understanding the visual semantics of synthetic images as humans do.\nOur data and code are publicly available at https://github.com/GuanzhenLi/MVP-Bench.", "sections": [{"title": "1 Introduction", "content": "Visual perception (VP) refers to the ability to transform visual signals into meaningful perceptions (de Wit and Wagemans, 2012; Gordon et al., 2019). When humans parse visual signals, they initially engage in high-level perception to grasp the overarching concept using commonsense knowledge. This serves as context guidance for exploring further low-level details aligned with their intentions (Wang et al., 2024; Garner, 1987). For example, given an image of a man in a bar, humans first grasp the high-level concept, such as the behaviour of drinking, and focus on low-level details, such as the type of alcohol, to obtain specific information. Existing Large Vision-Language Models (LVLMs) demonstrate an exceptional understanding of such low-level visual clues. However, it remains unexplored whether they have similar hierarchical visual perceptions at both levels, like humans.\nRecently, several benchmarking works have considered evaluating visual perceptions (Liu et al., 2023c; Fu et al., 2024; Chow et al., 2021). However, such holistic evaluation benchmarks lack the critical specialization needed to assess visual perceptions. Specifically, most of their tasks focus on low-level perception such as Counting and Existence Detection questions on single images. Besides, existing benchmarks are mostly designed based on individual question-image samples, failing to evaluate the consistency and accuracy of understanding an image with different forms of perceptions. Furthermore, most of the current benchmarks are built on real-world natural image data, making it hard to disentangle reliance on prior knowledge from the visual perception of specific contexts, such as synthetic images (Bitton-Guetta et al., 2023). Motivated by the challenges of interpreting LVLMs' visual perception capabilities, we propose MVP-Bench, the first benchmark systematically evaluating multi-level visual perceptions of LVLMs. As shown in Figure 1, each sample is accompanied by questions at both levels. We thoroughly design five high-level and thirteen low-level perception categories, detailed in Section 3. Furthermore, we construct {natural, manipulated} image pairs which convey contrasting perceptions as a more challenging task for visual perception.\nIn this work, with our constructed MVP-Bench,"}, {"title": "2 Related Work", "content": "Visual Perception. Visual Perception represents how the human brain transforms the pattern of information on the retina into a meaningful perception of the world (de Wit and Wagemans, 2012; Cornsweet, 2012). This process involves interactions among sensory and cognitive processes across hierarchical levels in the brain (Gordon et al., 2019; Rouw et al., 1997). Low-level visual features refer to the properties like colors and spatial attributes, while high-level visual processing integrates with human cognitive functions (e.g. commonsense knowledge, personal experiences) related to recognized objects (Akcelik et al., 2022; Wu et al., 2023b; Kandel et al., 2021; Schindler et al., 2021). Both perception competences are crucial, as human visual perception begins with grasping the image's main idea at a high level, and then delving into low-level features motivated by particular intentions (Garner, 1987). In MVP-Bench, we define five high-level categories and thirteen low-level categories. The mapping relationships between levels indicate that certain low-level features can support the high-level perception (illustrated in Section 3).\nVision-Language Benchmarks. Some recent benchmarks contain visual perception as a section, but their aim to offer a comprehensive evaluation of LVLMs' various capabilities leads to an inadequate exploration of visual perception. MMBench (Liu et al., 2023c) and MME (Fu et al., 2024) categorize visual perception based on question granularity. Although coarse perception questions are general, their questions like Counting or Existence Detection cannot reflect an image's main idea as high-level visual perception. Additionally, they evaluate different categories of visual perception individually, making it unavailable to compare an LVLM's different perceptions. II-Bench(Liu et al., 2024) and DEEPEVAL(Yang et al., 2024) focus on understanding deep image semantics, requiring LVLMs to perform complex commonsense reasoning based on low-level details. While they also reveal LVLMs' performance gaps across levels, these gaps are primarily due to limited reasoning abilities rather than intuitive visual perception. The definition of perception in PCA-Bench (Chen et al., 2024) resembles our benchmark, emphasizing how perception offers a guiding context in decision-making domains. However, their images depicting"}, {"title": "3 MVP-Bench Evaluation Suite", "content": "MVP-Bench comprises 530 {natural, manipulated} image pairs accompanied by questions at multiple perception levels. Using MVP-Bench, we diagnose LVLMs by investigating (1) the performance gap between high- and low-level visual perceptions and (2) the difference in visual understanding abilities on natural and manipulated images."}, {"title": "3.1 Evaluation across Perception Levels", "content": "We prioritize the perception of humans as high-level perception, e.g., misinformation understanding (Da et al., 2021) and emotion recognition (Hari and Kujala, 2009), where high-level perception is commonly engaged.\nWe categorize high-level (Lh) perceptions of humans into five dimensions, including Behaviour, Role, Identity, Emotion, Scenario. Each dimension corresponds to several low-level (L\u2081) perception types. As shown in Figure 3 (a), certain low-level perceptions (e.g., attire such as a police uniform or group association with firefighters) can support the high-level perception (e.g., Role).\nWe design Yes/No questions and Cross-Image questions at both levels. Constructed on the same set of images, the multi-level perception tasks enable us to diagnose the perception gap in LVLMs across different levels. Specifically, we calculate the accuracy on Yes/No questions based on the correctness of each individual question-image pair (represented as a Acc), while all multiple-choice questions within MVP-Bench are evaluated with Circular Strategy (Liu et al., 2023c) to alleviate the model prediction bias from the option order."}, {"title": "3.2 Evaluation with Image Pairs", "content": "Each {natural, manipulated} image pair in MVP-Bench conveys significantly different multi-level perceptions. Specifically, the two images differ only in one of the L\u2081 perception categories (in Figure 3 (a)), leading to distinct Lh perceptions. To mitigate the effect of the LVLMs' biased tendency to answer Yes/No questions (Liu et al., 2023a), we examine if LVLMs can elicit different perceptions given an image pair with the same question. We further explore the performance gap in LVLMs on natural and manipulated images in Section 5.\nFor Yes/No questions, we ask the same question on pairwise image data. As the two images are manipulated to convey different perceptions, they have opposite corresponding ground truth answers. We calculate qAcc and iAcc based on question- and image-level accuracy, respectively, following (Liu et al., 2023a). We design a holistic metric \u0442\u0410\u0441\u0441, requiring answering all questions corresponding to an image pair correctly.\nFor single-image multiple-choice questions, we focus on model understanding of manipulated images as a more challenging task. We include the answer to the natural image as a distractor to assess the discriminability of LVLMs in discerning the differences between the image pair. Additionally, we leverage ChatGPT\u00b9 to generate three other options aligned with the low-level clues in the manipulated image to heighten our task difficulty."}, {"title": "4 MVP-Bench Construction", "content": "We now present our construction process of image manipulation and the designs of corresponding multi-level questions for MVP-Bench."}, {"title": "4.1 Construction Pipeline", "content": "We select images from the EMU dataset (Da et al., 2021) as natural images for constructing image pairs. EMU focuses on visual misinformation, portraying cases involving humans and complex social scenes that require perceptions at both levels. Based on the natural image, we generate synthetic manipulations following one of the L\u2081 categories.\nHowever, to alter manipulated images' Lh perceptions in certain categories, it is challenging to constrain the manipulation applied exactly to a specific Li category without significant modification on other details. Besides, it is also hard to ensure consistency between the image pairs and the questions. We propose a three-step benchmark construction pipeline to meet the two requirements.\nStep one: Idea Generation. We utilize ChatGPT to generate ideas on how to manipulate natural images via Chain of Thoughts (CoT). Given an initially determined Lh category, we prompt ChatGPT to identify a corresponding low-level perception to support it. For instance, in Figure 2, considering the \"Behaviour-Background Substitution\u201d category, ChatGPT first generates an idea to change the woman's behaviour from attending a party to engaging in an experiment. Under this guidance, the background of the manipulated image should be a laboratory environment. Specifically, we provide auxiliary information such as the description of the manipulated image, which is incorporated into the textual prompt for image generation in Step 2.\nTo ensure coherence between the generated idea and the subsequent visual editing, we fixate on a specific subject at this initial step utilizing the visual grounding ability of Shikra (Chen et al., 2023). Specifically, we employ Shikra to retrieve the coordinates of a selected subject (Csub) and utilize it to query low-level features (e.g., \"What is the man holding?\") from the image in the subsequent steps.\nStep two: Manipulated Image Generation. We define three categories of manipulated image generation based on the image-editing type: Partial Component Substitution, Overall Background Substitution, and Direct Manipulation.\n2.1 Partial Component Substitution. This refers to manipulating an image by substituting an object or a part of the main subject. The pipeline utilizes Shikra to extract the target object's coordinates (Cobj), with Csub serving as a constraint. After masking Cobj as a blank, we apply the Stable-Diffusion-Inpaint (Stacchio, 2023) as a tool, using the edited image's caption obtained from step one as the prompt to generate a manipulated image. A set of defined Li categories, {B2, B3, B4, R2, I1, I2, I3, E1}, can be executed in this process."}, {"title": "2.2 Overall Background Substitution", "content": "This represents generating a manipulated image by retaining solely the main subject while replacing the entire background. In these cases, a standard rectangle cannot exactly mask the subject, potentially remaining unexpected elements and distorting the background generation. To address this limitation, we employ the Segment Anything Model (Kirillov et al., 2023) to produce a set of detected object masks (M = {M1, M2, ..., Mn}) in irregular shapes for a given image. We identify a mask with the greatest overlap with Csub.\n$mask = arg \\max_{M_i \\in M} Overlap(M_i, C_{sub})$ (1)\nHere, Overlap refers to a function that calculates the overlapping square between two regions. To enhance flexibility and increase the case difficulty, we randomly translate the location of Csub, rescale the Csub, and resize the entire mask. Finally, with the new mask and the manipulated image's caption obtained from Step 1, we utilize Stable-Diffusion-Inpaint to generate a new image with a different background from the original natural image. This process can handle {B1, R1, S1}."}, {"title": "2.3 Direct Alteration", "content": "This addresses situations where nothing can be substituted, yet some alteration is necessary, such as changing facial expressions. With the original natural image and the manipulation instruction obtained from Step 1, we directly utilize the image-editing model Instruct-Pix2Pix (Brooks et al., 2023) to generate a manipulated image for {E2, S2}. However, since this process cannot focus on specific subjects, we mainly apply it to images containing a single person or cases requiring overall manipulations.\nStep three: Visual Question Generation. We generate Yes/No questions, Single- and Cross-Image multiple-choice questions using ChatGPT based on the ideas generated in Step 1. Single-Image questions focus on the discrepancy between image pairs, while Cross-Image tasks focus on the differences across each pair of images. To ensure the quality of generated questions, two of this paper's authors manually verified all 3205 questions."}, {"title": "5 Experiments", "content": "We use MVP-Bench to diagnose and compare the visual perception capabilities of LVLMs belonging to two categories: (1) Open-Source LVLMs including MiniCPM-V-2 (OpenBMB, 2024), DeepSeek-VL (Lu et al., 2024), MiniGPT4 (Zhu et al., 2023), mPLUG-Owl2 (Ye et al., 2023), InstructBLIP (Dai et al., 2023), and LLaVA-1.5 (Liu et al., 2023b); (2) Proprietary LVLMs including GPT-4V and GPT-4o. All the experiments are conducted with VLMEvalKit (Contributors, 2023) under the zero-shot setting for a fair comparison."}, {"title": "5.1 Result Analysis", "content": "As outlined in Section 3, we compare the performance of LVLMs at multiple perception levels (Table 2). We also investigate the performance variation when given manipulated images in Table 3.\nPerformance at Different Perception Levels. As shown in Table 2, both open- and closed-source models perform worse on high-level perception tasks than low-level ones, e.g., 55%, 52%, and 56% compared to 69%, 67%, and 74% of qAcc on MiniCPM-V-2, LLaVA-1.5-13B, and GPT-4o, respectively. Specifically, we observe that closed-source models present a larger relative performance gap between high-level and low-level perception. For example, GPT-4o achieves an accuracy of 34% (relatively reduced by 53% from 74%) on cross-image MCQ, compared to 18% ( relatively reduced by 30% from 26%) of LLaVA-1.5-13B. This indicates that the performance gains from closed models mainly come from their superior low-level perceptions, yet they still encounter challenges in high-level tasks. We further discuss the potential cause of this observation in Section 5.2.\nImpact of Model Sizes. Small models can outperform the larger ones in Table 2. Among open-source models, MiniCPM-V-2-3B and DeepSeek-VL-7B achieve the best performance on high-level and low-level tasks respectively. As MiniCPM-V-2 is aligned with fine-grained correctional human feedback, it shows excellent trustworthiness and reduced hallucination. This implies that LVLMs\u2019 trustworthiness may benefit their high-level visual perception. DeepSeek-VL demonstrates a strong capability of perceiving specific details with additional visual encoders for processing low-level features, indicating these features are crucial to low-level visual perception. Besides, comparing LLaVA and InstructBLIP with different sizes reveals that increasing parameters from 7B to 13B does not notably enhance their visual perception at either level. Therefore, to enhance LVLMs' single-"}, {"title": "Analysis on the Cross-Image Task", "content": "Table 2 shows that closed-source models significantly surpass open-source models on cross-image tasks, especially at low perception level. For instance, GPT-4V and GPT-4o achieve accuracies of 45% and 74% respectively at the low level, significantly surpassing the accuracy of LLaVA-1.5-13B (26%). Furthermore, this performance gap is larger than that observed in single-image tasks. In the cross-image task, GPT-4o outperforms LLaVA-1.5-13B relatively by 93% and 185% on each of the two levels separately, compared to just 8% and 12% in single-image tasks. The significant gap indicates open-source LVLMs' insufficient contextual attention, due to a lack of cross-image training data."}, {"title": "Comparison between {natural, manipulated} Images", "content": "As shown in Table 3, both open- and closed-source models show inferior performance on manipulated images compared to natural images. For example, MiniCPM-V-2, LLaVA-1.5-13B, and GPT-4o achieve an iAcc of 69%, 59%, and 77% on natural images, while exhibiting lower i Acc of 54%, 56%, and 49% on manipulated images. We attribute this observation to the discrepancy between the visual perception of manipulated images and LVLMs' training data. Besides, closed-source models demonstrate a larger performance gap across image pairs than open-source models. The iAcc gap of GPT-4V and GPT-4o is 40.3% and 28.4% separately, while LLaVA-1.5-13B and MiniCPM-V-2 have gaps of only 2.96% and 14.79%. One reason for this is the rigorous manner of GPT-4V and GPT-4o in interpreting the high-level semantics of visual content, which we will discuss in Section 5.2. Besides, these models"}, {"title": "Yes/No v.s. MCQ", "content": "GPT-4V and GPT-4o present conflicting results on different tasks. Although both tasks are based on the manipulated images, two models perform poor on Yes/No task with an iAcc of 31% and 49%, while outperforming all open-sourced models on the MCQ task. From Table 3, we can witness that the results of MCQ and i Acc on natural images share the same trend, which suggests that closed-source models' inferior performance on manipulated images is owing to the nature of Yes/No questions. As an open-ended generative task, these models tend to perform rigorously and safely, while the MCQ task is less influenced by their rigorous manner. This is also a motivation for us to design both tasks for single-image perception."}, {"title": "5.2 Discussion", "content": "In this section, we present our qualitative analysis observations, investigating the poor performance of GPT-4V on Yes/No questions, the gap between open-source and closed-source models, and the deficiencies of current LVLMs.\nRigurous Behaviors of GPT-4V in High-Level Perception Tasks. Although GPT-4V exhibits the highest level of security among current LVLMs, its rigorous manner in interpreting a scene may hinder the straightforward perception of common visual contents. Specifically, GPT-4V usually approves only what it can directly observe from the image. It tends to refuse to interpret uncertain cases, such as conducting high-level perception without explicit visual clues. For example, as shown in Figure 4 (a), although GPT-4V accurately identifies the woman's attire as a doctor's uniform at the low perception level, it declines to provide the correct high-level perception that the woman is a doctor, as it cannot be directly observed in the image. This problem has been mitigated in GPT-4o, as it gives a correct answer.\nTo explore whether we can motivate GPT-4V to integrate commonsense knowledge via tuning the prompt, we add an instruction as follows:\nYou are a helpful visio-linguistic AI assistant who answers questions in short words or phrases on visual commonsense in the images."}, {"title": "Gaps between Open- and Closed-source LVLMs in Recognizing Visual Details and Utilizing Commonsense Knowledge", "content": "Although LLaVA-1.5-13B and DeepSeek-VL-7B can outperform GPT-4o on straightforward content like background (qAcc of 92%, 86% compared to 82%)3, they demonstrate worse performance on the object association perception requiring to recognize details (qAcc of"}, {"title": "Bias in LVLMs to Prioritize Dominant Components", "content": "One hard case in MVP-Bench requires LVLMs to comprehend an entire image based on an inconspicuous object. In Figure 4 (d), all LVLMs prioritize the shopping mall setting while overlooking the gun held by the woman. We attribute this to the data homogeneity of the training images, i.e., most training data is constructed by real-world images where a shopping mall closely correlates to shopping activities, misguiding the models to ignore the presence of the gun."}, {"title": "Bias in GPT-4V and GPT-40 to Perceive Scenes as Staged Performance", "content": "GPT-4V and GPT-40 tend to interpret occasional or dramatic scenes as staged images, especially when the co-occurrence frequency of visual elements is low based on commonsense knowledge. For example, in Figure 4 (e), the case depicts the president having a meal with soldiers together, while GPT-4V and GPT-40 regard this as a staged scene for an organized event. This suggests the over-reliance on prior commonsense knowledge of GPT-4V and GPT-40, potentially obstructing their generalizability to understand and interpret occasional scenes and their inherent semantic meanings."}, {"title": "6 Conclusion", "content": "We introduce MVP-Bench, the first benchmark systematically evaluating LVLMs' multi-level visual perception. We diagnose 12 current LVLMs and compare their various performance across perception levels and between natural-manipulated pairs. Further analysis demonstrates these models' deficiency and the gap between closed- and open-source models. We envision follow-up work to enhance LVLMs' ability to generate multi-level visual perception consistent with visual content."}, {"title": "Limitation", "content": "While constructing MVP-Bench, we generate manipulated images with Diffusion models. Although we manually filtered out the generated images not conveying a different perception compared to the source natural images, some still contain blur, inconsistencies, or distortions (e.g., three-armed persons or blur distorted faces), potentially affecting LVLMs' understanding due to the introduced noise. Besides, MVP-Bench focuses on human-related visual perception to ensure each case necessitates multi-level understanding, potentially overlooking scenarios devoid of humans. In future work, we will refine and expand MVP-Bench further to enhance image quality and topic coverage."}, {"title": "Ethics Statement", "content": "MVP-Bench contains violent content and celebrity information, which may cause harmful imitation or misinformation. To prevent the misuse of MVP-Bench, we will implement stringent access rules and consistently track follow-up works to ensure their research-only objectives.\nBesides, our MVP-Bench is constructed with the images from the EMU dataset as seeds. We have followed its access rules by filling in the form and obtaining permission from the authors."}]}