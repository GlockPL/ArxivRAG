{"title": "POINTS: IMPROVING YOUR VISION-LANGUAGE MODEL WITH AFFORDABLE STRATEGIES", "authors": ["Yuan Liu", "Zhongyin Zhao", "Ziyuan Zhuang", "Le Tian", "Xiao Zhou", "Jie Zhou"], "abstract": "In recent years, vision-language models have achieved significant advancements, excelling in tasks once deemed challenging, such as optical character recognition and geometric problem-solving. Despite these impressive achievements, several critical issues remain unaddressed: 1) Proprietary models rarely disclose detailed information about their architectures. In contrast, while open-source models provide visibility into their training strategies, detailed ablations of these strategies are highly anticipated. 2) Pre-training data is currently under-explored in open-source works, with most efforts empirically adding datasets from diverse sources, making the entire process elusive and cumbersome. 3) During the fine-tuning stage, the focus is often on adding and ablating more datasets, which frequently leads to diminishing returns. Therefore, refining data schemes is essential for further enhancing model performance. To address these issues, we propose the following contributions in this paper: 1) We trained a robust baseline model, leveraging the latest technological advancements in vision-language models. Building upon existing advancements, we introduced effective improvements and conducted comprehensive ablation and validation for each technique incorporated into this strong baseline. 2) Inspired by recent work on large language models, we propose filtering pre-training data using perplexity, selecting the data with the lowest perplexity as the training set. This approach allowed us to train on a curated 1M dataset, resulting in highly competitive performance. 3) During the visual instruction tuning stage, we experimented with model soup on different datasets when further introducing more datasets into the training set brought marginal improvements. Integrating these innovations, we obtained a model with 9B parameters, performing competitively with a series of existing state-of-the-art models. Additionally, these strategies we propose are efficient and relatively lightweight, allowing the community to adopt them easily for their models.", "sections": [{"title": "INTRODUCTION", "content": "Advancements in large language models (LLMs; Chowdhery et al. 2023, Jiang et al. 2023, OpenAI 2022, Yang et al. 2024, Dubey et al. 2024) have significantly enhanced the capabilities of vision-language large models (Fu et al. 2023, Liu et al. 2023b, OpenAI 2023, Dong et al. 2024a, Zhu et al. 2023), enabling more sophisticated analyses of textual and visual information. Prominent closed-source model paradigms such as GPT-4 (OpenAI, 2023), Gemini Pro 1.5 (Fu et al., 2023), and Claude 3 (Anthropic, 2024) have achieved remarkable success in expanding LLMs into the realm of vision-language models. Concurrently, open-source vision-language large models are also advancing rapidly, with numerous notable contributions emerging in the field (Liu et al., 2024b; Chen et al., 2024c).\nHistorically, LLaVA (Liu et al., 2024b) has served as a common baseline. However, recent advancements have rendered its performance suboptimal. Thus, there is a need to establish a stronger baseline for further exploration. In this work, we enhance the vanilla LLaVA architecture by refining the pre-training dataset. Inspired by CapFusion (Yu et al., 2024), we merge the original captions with world knowledge and generated captions that exhibit good grammatical structure. For visual instruction tuning datasets, we introduce Individual Select (Liu et al., 2024c) to curate effective instruction tuning datasets. Regarding model architecture, we first incorporate Dynamic High Resolution to help the model capture fine-grained details. To address image distortion issues inherent in Dynamic High Resolution, we propose a novel image splitting strategy called Consistent Aspect Ratio Dynamic High Resolution, which maintains a consistent image ratio. Additionally, inspired by Vary (Wei et al., 2023), we merge features from a vision encoder trained separately with text-rich data with those from the original vision encoder, significantly boosting the model's OCR capabilities. Unlike most existing works (Li et al., 2024; Chen et al., 2024c), we extensively ablate each newly introduced component in the strong baseline to verify their individual benefits.\nRecent works seldom explore the optimization of pre-training datasets. Most studies (Chen et al., 2024c; Yao et al., 2024; Bai et al., 2023b) tend to empirically combine samples from various large-scale datasets (Schuhmann et al., 2022; Byeon et al., 2022), often leading to inefficient and computationally expensive pre-training processes. In the domain of large language models, some research leverages perplexity to filter pre-training datasets. Inspired by this approach, we filter our pre-training dataset by selecting the top samples with the lowest perplexity values. This filtering process yields a subset of 1 million data samples, on which we subsequently pre-train our model. Experimental results demonstrate that the model trained on this filtered subset outperforms a model trained on a dataset five times larger.\nIn the visual instruction tuning stage, most existing works (Liu et al., 2024c; Li et al., 2024; Chen et al., 2024c) focus on collecting large quantities of datasets and performing ablation studies to select the most effective ones. However, this approach often reaches a plateau, where introducing additional datasets yields only marginal or even degraded performance. Previous research on model soup has demonstrated the benefits of merging weights from different models fine-tuned with various hyper-parameters. In this work, we propose using model soup to merge weights from models fine-tuned with different datasets to further improve performance when dataset selection no longer brings significant improvement. Compared to conducting model soup on models fine-tuned with different hyper-parameters, e.g. learning rate, the improvement with model soup on models fine-tuned with different datasets is much more prominent. Following this line of work, we further experiment with different model soup strategies and find that greedy model soup is the most effective.\nBy integrating the aforementioned innovations, we have developed a model called POINTS. Our contributions are threefold:\n\u2022 We propose a strong baseline that integrates the latest advancements in vision-language models and thoroughly verify the effectiveness of each component.\n\u2022 We introduce the use of perplexity to filter the pre-training dataset and conduct a detailed investigation of data distribution across different perplexity intervals.\n\u2022 We employ model soup to merge models fine-tuned with different datasets, thereby enhancing model performance when further dataset selection yields only marginal improvements."}, {"title": "RELATED WORKS", "content": "Multimodal Large Language Models The rapid advancement of large language models (LLMs; Dubey et al. 2024, Team et al. 2023, Achiam et al. 2023, Yang et al. 2024, Su et al. 2022) has laid the groundwork for the emergence of multimodal large language models (MLLMs; Li et al. 2024, Liu et al. 2024b, Liu et al. 2024a, Bai et al. 2023a, Qiao et al. 2024), which aim to integrate visual understanding with language reasoning and multimodal perception and comprehension. Prominent models such as GPT-4v (Achiam et al., 2023) and Gemini-1.5-Pro (Team et al., 2023), developed by major corporations, have spearheaded the MLLM era, utilizing proprietary training data and undisclosed training methodologies. Meanwhile, open-source models have been striving to keep pace. For instance, LLaVA-Next (Liu et al., 2024a) and InternVL-1.5 (Chen et al., 2024c) introduce dynamic high-resolution techniques by dividing a large image into multiple smaller segments with ratio-inconsistent resizing. MiniCPM-V (Yao et al., 2024) employs a specialized vision encoder to generate non-square image patches. Additionally, models like Vary (Wei et al., 2023), SPHINX (Lin et al., 2023), Cambrian-1 (Tong et al., 2024), and Mini-Gemini (Li et al., 2023) propose dual vision encoders to enhance visual capabilities. Furthermore, the significant progress in multimodal model"}, {"title": "METHODS", "content": "This section is divided into three parts: i) In subsection 3.1, we integrate various techniques from previous methods (Liu et al., 2024a; Lin et al., 2023; Wei et al., 2023; Liu et al., 2024c; Chen et al., 2024c) to create a strong baseline for further experiments. Additionally, we propose a novel dynamic resolution splitting method, termed Consistent Aspect Ratio Dynamic High Resolution (CATTY for short), to mitigate the issue of image distortion. ii) In subsection 3.2, we propose using perplexity to filter the pre-training dataset. iii) Finally, in subsection 3.3, we incorporate the concept of model soup (Wortsman et al., 2022) into the instruction tuning stage. We find that this straightforward approach can significantly improve the model's performance, especially when further data selection only brings marginal or even degraded performance."}, {"title": "A STRONG BASELINE", "content": "In this section, we integrate the recent advancements from existing works to create a strong baseline, containing Dynamic High Resolution from LLaVA-Next(Liu et al., 2024a) and InternVL1.5(Chen et al., 2024c), CapFusion from (Yu et al., 2024), Dual Vision Encoder from Vary(Wei et al., 2023) and SPHINX(Lin et al., 2023), Individual Select from (Liu et al., 2024c). Following LLaVA(Liu et al., 2024b), POINTS mainly contains three parts: vision encoder, projector and the large language model. By integrating all these practices from previous works, we obtain the model structure and pipeline in Figure 1.\nDynamic High Resolution It has been verified that feeding high-resolution images to vision-language models is beneficial for capturing fine-grained details and reducing hallucinations (Liu et al., 2023b). To enable vision encoder with fixed input resolutions to accommodate dynamic image resolutions, Dynamic High Resolution in LLaVA-Next (Liu et al., 2024a) and InternVL-1.5 (Chen et al., 2024c) splits high-resolution images into several tiles of the same resolution, which the original vision encoder can process. The concrete steps are as follows: i) First, the maximum number of tiles an image can be split into is predefined (set to 8 in our experiments). ii) Based on the maximum number of tiles, a table is created containing information about the target image before splitting. The key of the table is the aspect ratio, and the value is the width and height of the target image, which can be evenly divided by the resolution of the vision encoder. iii) For each image, the target resolution is fetched from the pre-computed table according to the similarity between aspect ratios. The current image is then resized to the target resolution and split into several tiles of the same resolution.\nConsistent Aspect Ratio Dynamic High Resolution (CATTY) Before splitting the image, Dynamic High Resolution in InternVL-1.5 (Chen et al., 2024c) resizes the image to the target resolution. However, this resizing is not proportional to the image's original aspect ratio, which can cause distortion. This issue has been discussed in previous articles(Yao et al., 2024). Therefore, we propose a splitting method that maintains the image's aspect ratio, named Consistent Aspect Ratio Dynamic High Resolution (see Figure 2). The first two steps in CATTY are the same as those in"}, {"title": "METHODS", "content": "InternVL-1.5, and the last step works as follows: Given an image with height H and width W, we obtain the height and width of the referenced image from the pre-computed table, denoted as H\u00b9 and Wr, respectively. Then, we resize the image to the target size (H \u00d7 W\u00b9) by:\nratio = min(H, W)/min(H\u00b9, W\u00b9)\nH = ratio \u00d7 H\nW = ratio \u00d7 W\nGiven the input resolution of a vision encoder, H \u00d7 W, the target image should be divided into $\\frac{H}{H^r} \\times \\frac{W}{W^r}$ tiles. Next, we split the target image, H\u207a \u00d7 W\u00b9, using a sliding window with strides (Sh, SW) across the height and width, respectively. The strides (Sh, SW) are computed as follows:\n$S_h = (H^r \u2013 H')/(H^r/H\u2032 \u2212 1)$\n$S_w = (W^r \u2013 W')/(W^r/W\u2032 \u2212 1)$\nIn Equation 2, Sh is set to 0 if HF/H' = 1, and similarly for SW. This approach allows us to divide a high-resolution image into several tiles without introducing any distortion. Alongside the tiles obtained using CATTY, we also include a thumbnail of the global view of the image to capture the overall context. This thumbnail is resized to match the input resolution of the vision encoder. Before feeding the features output by the vision encoder into the large language model, we employ the pixel shuffle technique with a down-sampling factor of 0.25, as described in InternLM-XComposer2-4KHD (Dong et al., 2024b), to reduce the sequence length of the image features for improved efficiency.\nCapFusion The original captions in existing pre-training datasets are often noisy and structurally flawed, making them sub-optimal for model training. To address this, synthetic captions, such as those in LAION-COCO and BLIP-LAION (Li et al., 2022), generated by image captioning models, have been proposed. However, the simplistic syntactic and semantic structures in synthetic captions may contribute to issues like Scalability Deficiency and World Knowledge Loss (Yu et al., 2024)."}, {"title": "PRE-TRAIN DATA SELECTION", "content": "In the context of large language models, perplexity has long been employed as a metric to assess the quality of pre-trained datasets (Albalak et al., 2024; Marion et al., 2023). Inspired by this approach, we utilize an off-the-shelf vision-language model, P\u2014either the model obtained through the steps outlined in subsection 3.1 or an open-sourced VLM-to further filter out low-quality pre-trained datasets obtained via Capfusion, as described above. For each item, s, in the pre-trained dataset mentioned in subsection 3.1, we compute the perplexity for all text tokens using the following formula:\n$Perplexity(s) = exp(-\\frac{1}{N}\\Sigma_{i=1}^{N}logP(W_i|W_1,W_2,...,W_{i-1}))$\nLet {$W_1,..., W_N$} represent the text token sequence for s. We sort all these items in ascending order and select the first 20% for the pre-training stage. Upon closer examination of the first and last 20% of items, we observe that the distinguishing factor is not the quality of the data, which contrasts with observations in large language models. The last 20% of items often contain obscure world knowledge, such as game version numbers and computer factory serial numbers. This type of world knowledge is extremely rare and contains very little information, making it less beneficial for the model's learning. In the appendix, we provide some examples randomly sampled from the first and last 20% of items."}, {"title": "INSTRUCTION DATA SELECTION WITH MODEL SOUP", "content": "Visual instruction tuning data is crucial for the superior performance of existing vision-language models (Chen et al., 2024c; Dong et al., 2024a; Liu et al., 2024b). However, most existing works focus on selecting more effective datasets by iterative ablation. In many cases, this approach reaches a plateau, where further data selection can only bring marginal improvements or even degrade performance. In this section, we introduce the benefits of using model soup to integrate the advantages of models fine-tuned with different instruction tuning datasets after data selection meets a bottleneck.\nThe philosophy behind model soup is as follows: given a pre-trained model, fine-tuning the model with different hyper-parameters, h\u2081,..., hk, results in several fine-tuned models converging to different local optima, denoted as f(01,h\u2081),..., f(0k, hk). These hyper-parameters include learning rate, data augmentation, initialization seed, etc. By interpolating the weights of these fine-tuned models, we can always obtain a stronger model, f(0s,hs). Given the pre-trained model obtained through the methods discussed above, a base instruction tuning dataset D, and a series of visual instruction tuning datasets d\u2081,..., dk to be selected, we can obtain a stronger model using the following steps:\n\u2022 For each dataset di \u2208 {d1,..., dk}, we add it to the base instruction tuning dataset, D, to obtain an augmented dataset, D.\n\u2022 We train k models using each augmented from {D, ..., D} concurrently, and obtain {f(D*; 01), ..., f(D; 0k)}.\n\u2022 We select p models from {f(D*; 01), ..., f (D; 0k)}, and merge the weights from all these selected models to obtain a stronger model.\nFor the third step above, we choose several methods to select the best composition of fine-tuned models to obtain a final model with superior performance, namely, Maximum Soup, Average Soup, and Greedy Soup.\nMaximum Soup Given an evaluation score, Acc, we can obtain a strong model, f(0s), using the following formula:\n{$i\\}_{len(\\theta_i)=p} = Arg_{\\theta_i} (Topp(Acc(f(D^r; \\theta_1)), ..., Acc(f(D_k^r; \\theta_k))}))$\n$f(\\theta_s) = f(\\frac{1}{p}\\Sigma_{i=1}^p(\u03b8_i))$"}, {"title": "METHODS", "content": "Average Soup By taking the average of weights from all fine-tuned models, we can obtain a stronger model, f(0s):\n$f(\\theta_s) = f(\\frac{1}{k}\\Sigma_{i=1}^k(\\theta_i))$\nGreedy Soup We start by sorting the fine-tuned models in descending order based on their evaluation scores. Next, we iterate through these sorted models. For each model, we compute the average of its weights with those of all models currently in the model pool. If the evaluation score improves, the model is added to the pool. Finally, we average the weights of all models in the pool to obtain a stronger model, denoted as f(05). The table below outlines the detailed pipeline of Greedy Soup."}, {"title": "EXPERIMENTS", "content": "This section is divided into five subsections: (i) evaluation setup, (ii) pre-training and instruction-tuning datasets used to train the strong baseline, as well as the instruction-tuning datasets selected by model soup, (iii) details about the training setup for the OCR ViT pre-training, the vision-language pre-training, and the visual instruction tuning stages, (iv) ablation studies and analyses of each component used to build our final model, and (v) comparison with other works on extensive benchmarks."}, {"title": "EVALUATION SETUP", "content": "Before embarking on our exploration, we sought a robust evaluation metric to comprehensively assess the various capabilities of our model. This is where OpenCompass (Contributors, 2023) proves helpful. OpenCompass proposes eight benchmarks to balance the evaluation of a model from different perspectives. These benchmarks include MMBench (Liu et al., 2023c) and MMStar (Chen et al., 2024a) for diagnosing general abilities, MMMU (Yue et al., 2024) for testing STEM-related abilities, HallusionBench (Liu et al., 2023a) for model hallucination, MathVista (Lu et al., 2023) for math-related abilities, AI2D (Kembhavi et al., 2016) for chart-related abilities, OCRBench (Liu et al., 2023d) for OCR capabilities, and MMVet (Yu et al., 2023) for subjective evaluation. By averaging the metrics from these benchmarks, OpenCompass derives a score that represents the comprehensive ability of a model. Additionally, it offers a useful tool, VLMEvalKit (Duan et al., 2024), for one-click evaluation. Therefore, unless otherwise specified, we will use these eight benchmarks for our ablation study, with the exception of MMBench, for which we will use the dev-en split."}, {"title": "DATA SETUP", "content": "Pre-train Dataset To train the OCR ViT, we randomly selected 20 million data points from LAION-5B-en (Schuhmann et al., 2022), LAION-5B-cn (Schuhmann et al., 2022), WuKong (Gu et al., 2022), and Zero (Gu et al., 2022). We then used PaddleOCR to extract text from the images, replacing the original captions to form new image-caption pairs for pre-training. Following Vary (Wei et al., 2023), we also included 10 million original data samples from LAION-5B, where the"}, {"title": "TRAINING SETUP", "content": "Pre-training Setup for OCR ViT The pre-training framework follows the standard LLaVA-style architecture (Liu et al., 2023b), comprising a vision encoder, a two-layer MLP, and a large language model. The vision encoder is initialized from OpenAI's CLIP-ViT-Large-336\u00b9, while the large language model is initialized from Yi-1.5-9B-Chat (Young et al., 2024). Throughout the pre-training stage, the large language model remains frozen, whereas the vision encoder and MLP are trainable. The learning rates for the vision encoder and MLP are set to 2 \u00d7 10\u20134 and 2 \u00d7 10\u22125, respectively, with a warm-up schedule during the first 3% of steps, followed by a cosine decay schedule for the remaining steps.\nSetup for the Vision-language Pre-training Stage The General ViT, depicted in Figure 1, is initialized from OpenAI's CLIP-ViT-Large-336, while the OCR ViT is derived from the preceding stage. For the General ViT, only the last three layers are trainable, as this configuration yielded the best results in our experiments. The OCR ViT remains frozen throughout this stage, consistent with the settings used in Vary(Wei et al., 2023). Features from the penultimate layer of both the General and OCR ViT are selected and fed into the projector. The projector itself is a two-layer MLP, which remains tunable during the pre-training stage. The learning rates for the General ViT and the MLP are set to 2 \u00d7 10-4 and 2 \u00d7 10-5, respectively. A warm-up schedule is applied during the first 3% of steps, followed by a cosine decay schedule for the remaining steps.\nSetup for the Visual Instruction Tuning Stage Both the General ViT and OCR ViT remain frozen throughout the entire stage. The learning rates for the projector and the large language model are both set to 2 \u00d7 10-5. A warm-up schedule is applied during the first 3% of steps, followed by a cosine decay schedule for the remaining steps."}, {"title": "ABLATION STUDY AND ANALYSIS", "content": "Each Component to Build the Strong Baseline As shown in Table 1, each component introduced in subsection 3.1 contributes to steady improvements. These enhancements are significant; for instance, after introducing Dynamic High Resolution to split the input image, we observe substantial improvements in OCR-related tasks, such as OCRBench, with performance increasing from"}, {"title": "Pre-train Dataset", "content": "As shown in Table 3, scaling up the dataset size (constructed by CapFusion) from 5M to 20M results in downgraded performance, similar to the observations in Liu et al. (2024c). Additionally, some works also achieve promising performance using relative small pre-training datasets instead of a huge number of datasets during the pre-training stage (Li et al., 2024; Liu et al., 2024a). We believe the possible reasons are: i) The vision encoder of most existing vision-language models is initialized from a pre-trained model that has already been trained on a large quantity of image-text pairs. It is highly likely that most of the data used in the vision-language pre-training stage has already been seen by the vision encoder, thus bringing only marginal or even negative impact when scaling up the size of the vision-language pre-training dataset. ii) The pre-training datasets are quite homogeneous for existing large-scale web-crawled datasets, e.g., LAION-5B and COYO-700M (Byeon et al., 2022). We plot the distribution of the main entity for each image of a"}, {"title": "COMPARISON WITH OTHER WORKS", "content": "In addition to the 8 benchmarks used in the ablation studies above, we further include ScienceQA (Lu et al., 2022), MME (Yin et al., 2023), LLaVA-Wild (Liu et al., 2024b), and ReadWorldQA to compare the performance of different models. The following table shows the performance of these"}, {"title": "CONCLUSION", "content": "Vision-language models have achieved significant progress in recent years. Following this trend (Chen et al., 2024c; Li et al., 2024; Liu et al., 2024b; Zhang et al., 2024; Tong et al., 2024), we first establish a strong baseline by integrating various advancements proposed in recent works (Liu et al., 2024a; Yu et al., 2024; Wei et al., 2023; Liu et al., 2024c) for further experiments. Additionally, we delve into the intricate details of these advancements and propose effective refinements, such as the Consistent Aspect Ratio Dynamic High Resolution. We also conduct extensive experiments to verify the effectiveness of each component in constructing the strong baseline. Secondly, we propose using perplexity to filter the pre-training dataset, retaining only the top 20% of data with the smallest perplexity values during the pre-training stage. This filtering method also brings significant improvements. Model Soup (Wortsman et al., 2022) has shown promising potential to further enhance performance by averaging the weights of fine-tuned models with different hyperparameters. However, we find that conducting model soup over different dataset settings can yield even more substantial improvements. In this paper, we propose conducting model soup across models fine-tuned with varying dataset quantities and diversities. The improvements brought by quantity and diversity are orthogonal to each other and can result in joint enhancement."}]}