{"title": "SCREENWRITER: Automatic SCREENPLAY GENERATION AND MOVIE SUMMARISATION", "authors": ["Louis Mahon", "Mirella Lapata"], "abstract": "The proliferation of creative video content has driven demand for textual descriptions or summaries that allow users to recall key plot points or get an overview without watching. The volume of movie content and speed of turnover motivates automatic summarisation, which is nevertheless challenging, requiring identifying character intentions and very long-range temporal dependencies. The few existing methods attempting this task rely heavily on textual screenplays as input, greatly limiting their applicability. In this work, we propose the task of automatic screenplay generation, and a method, ScreenWriter, that operates only on video and produces output which includes dialogue, speaker names, scene breaks, and visual descriptions. ScreenWriter introduces a novel algorithm to segment the video into scenes based on the sequence of visual vectors, and a novel method for the challenging problem of determining character names, based on a database of actors' faces. We further demonstrate how these automatic screenplays can be used to generate plot synopses with a hierarchical summarisation method based on scene breaks. We test the quality of the final summaries on the recent MovieSum dataset, which we augment with videos, and show that they are superior to a number of comparison models which assume access to goldstandard screenplays.", "sections": [{"title": "1 INTRODUCTION", "content": "Thanks to the proliferation of streaming services and digital content providers, a large number of movies are being released and made available every year. Automatic approaches to understanding and summarising their content are paramount to enabling users to browse or skim through them, and quickly recall key plot points, characters, and events without the need to re-watch from the beginning. Aside from practical utility, movie summarisation is an ideal testbed for real-world video and natural language understanding. Movies are often based on elaborate stories, with non-linear structure and multiple characters whose emotions and actions are often not accompanied with verbal cues. The summarisation task requires identifying beliefs and intentions of characters as well as reasoning over very long narratives (there are usually hundreds of pages in a transcript and tens of thousands of frames in a video), involving multiple modalities.\nMost previous work on movie summarisation has focused primarily on the textual modality, under the assumption that screenplays (or at least human-authored transcripts) are readily available (Gorinski & Lapata, 2015; Chen et al., 2022; Agarwal et al., 2022). Movie summarisation is commonly viewed as a type of long-form summarisation, a task which has improved dramatically in recent years. This is partly due to large models with longer context windows, and the design of methods specifically for this task, such as dividing the input into chunks and forming a hierarchical summarisation structure (Chen et al., 2023a; Pang et al., 2023) or summarising chunks iteratively (Chang et al., 2023). Video understanding through generating descriptions for videos, has also received much attention, largely independently from long-form summarisation, focussing instead on short video clips lasting a few minutes (Tapaswi et al., 2016; Lei et al., 2018; Rafiq et al., 2023).\nTwo notable exceptions are Papalampidi & Lapata (2023) and Mahon & Lapata (2024) who consider textual and visual modalities, aiming to summarise movies and television shows, respectively."}, {"title": "2 RELATED WORK", "content": "Video Understanding The problem of generating descriptions for videos has received significant attention in the literature. Traditional video description approaches often extract features from individual frames and fuse them into a single feature vector to generate a textual description (Zhang et al., 2021; Pan et al., 2020; Ye et al., 2022). SwinBERT (Lin et al., 2022) introduces an end-to-end video network that samples frames densely, avoiding the need for image-based encoders. Similarly, Lei et al. (2020) use a memory-augmented vision transformer to generate descriptions for"}, {"title": "3 SCREENWRITER", "content": "Figure 1 provides a graphic depiction of ScreenWriter. As mentioned earlier, the input to Screen-Writer is the video (including audio) for a full movie. We extract keyframes from the movie and use our novel scene segmentation algorithm (see Section 3.2) to partition the resulting sequence of frames into different scenes. In parallel, a text-to-speech model with speaker diarization yields a transcript with numeric speaker IDs instead of character names. For each movie, we assume access to a database consisting of actors faces and their character names (such a database can be easily constructed by scraping the faces of the characters from the movie's IMDB page). We extract facial features from each keyframe in each scene, and from each scraped character image, and use our"}, {"title": "3.1 TRANSCRIPT GENERATION", "content": "The first step in ScreenWriter is to extract the audio from the movie, using an automatic transcription model with speaker diarization (Bain et al., 2023). This produces a transcript with each utterance marked by a speaker ID. The speaker ID is the same for the same character throughout the movie (up to the accuracy of the diarization), but the character names are lacking. We also extract key frames from the entire video, and store the timestamps of these keyframes. These will be used to compute speaker names and visual information, which we will add to the diarized transcript to form a screenplay. It is also on this sequence of keyframes that the scene breaks are computed. There is an average of around 1,000 detected keyframes for a feature-length movie."}, {"title": "3.2 SCENE DETECTION", "content": "ScreenWriter extracts visual features from each keyframe, and applies a novel algorithm to segment the resulting sequence of visual feature vectors. There are two parts to our algorithm: the definition of a cost for a particular partition into scenes, and the search for the partition that minimizes this cost. The first part, the cost definition, is formulated using the minimum description length principle, which claims the correct representation of the data is the one using the fewest bits. We assume that the vectors for each scene are encoded with respect to their collective mean. That is, for each scene in the given partition, we calculate the mean of all vectors in that scene, and hence, the probability of each vector, $p(v)$, under the multivariate normal distribution with this mean. To reduce run time, we use a single fixed covariance calculated from the entire sequence of vectors. The Kraft-McMillan inequality (Kraft, 1949; McMillan, 1956) then determines that under the optimal encoding, the number of bits needed to represent v is \u2013 log2 p(v). The sum of this value across all N vectors v in the video, plus the number of bits to represent the mean vectors themselves, gives the total bitcost for a given partition. The mean vectors require dm bits, where d is the dimensionality, and m is the floating point precision, for which we use the standard of 32. Partitions with more scenes require more bits for the mean vectors, but also have mean vectors that better cover the keyframe features, leading to decreased \u2013 log2 p(v) on average. This trade-off encourages a partition with neither too few nor too many scene breaks.\nThe second part, the search for the minimizer of the above cost, can be solved exactly using dynamic programming. Let B(i, j) be the cost of having a single scene that runs from keyframes i to j, and let C(i, j) be the minimum cost of all keyframes from i to j, under all possible partitions. Then we have the recurrence relation\n\n$C(i, j) = \\min_{i<k<j} B(i, k) + C(k, j)$.\n\nThus, we can compute the optimal partition by iteratively computing and caching C(i, N) for i = N \u2212 1, ..., 0. This is guaranteed to find the global optimum. It runs in O(N2), but by imposing a fixed threshold of the maximum number L of keyframes in a scene, this becomes O(N)."}, {"title": "3.3 CHARACTER NAME IDENTIFICATION", "content": "To replace the arbitrary speaker IDs with character names, we first create a database of images of actors' faces paired with the name of the character they played from the IMDB movie page. As some of these images may contain multiple faces, or no faces, or even an entirely different character, we filter them to ensure a higher proportion contain only the face of the correct character, keeping only images with exactly one detected face, and for which the detected gender matches the name gender. Finally, we verify the faces in all pairs of remaining images against each other, using the DeepFace library, to create a graph where images are connected if and only if they are verified as being the same person, and then exclude all images that are not part of the largest clique. In total, we filter out about 40% of images on average. This produces a name bank of character names paired with a set of images of the face of that character.\nFor each scene, and for each character in our name bank, we define the cost of putting that character name in that scene as the minimum distance between an image of that character's face, and a face detected in any keyframe from the scene. The distance is the Euclidean distance of the DeepFace feature vectors. This avoids the incorrect assumption that the character speaking must be in shot, and instead makes the much weaker assumption that a character speaking must appear directly at some point in the scene. Thus, if we are considering assigning the character Clarice Starling to scene 3, then we compute the distance between the face feature vectors for all scraped images of the actor Jodie Foster in that role, and the face feature vectors of all faces detected in any keyframe in scene 3; the smallest distance is the cost of assigning Clarice Starling to scene 3. Computing the distance between vectors is extremely fast, taking <1s for all considered assignments on the entire movie, and"}, {"title": "3.4 SCREENPLAY GENERATION", "content": "After running the assignment algorithm from Section 3.3, we can replace the common speaker IDs in the automated, diarized transcript with the corresponding character names. Also, by matching the utterance times with the keyframe timestamps, we can insert the scene breaks from Section 3.2 into the transcript. Finally, we include descriptions of the visual contents of the scene by selecting three evenly spaced keyframes from that screen, applying an image captioning model (Peng et al., 2023), and inserting the output to the corresponding timestamped location of the transcript. In the caption, we replace occurrences of the nouns 'person', 'woman', 'man', 'girl', and 'boy' (and their determiners) with a character name, if this can be inferred from gender matching with the speaker names. For example, if the only non-male name in the script is 'Clarice Starling', then the occurrence of 'a woman' in the caption is replaced with 'Clarice Starling'. The result of adding names, scene breaks and visual descriptions to the transcript is an automatically generated screenplay. We show an example of ScreenWriter output in Figure 3."}, {"title": "3.5 MOVIE SUMMARY GENERATION", "content": "We experimentally show that the generated screenplay can be used as a basis for movide summarisation. We adopt a hierarchical summarisation approach (Pang et al., 2023; Chang et al., 2023), as it has been shown to be particularly suited to long inputs that exceed the context window size of large language models, and in our case can leverage the organization of the content into scenes. We this first summarise the transcript dialogue of each scene, and then fuse the resulting sequence"}, {"title": "4 EXPERIMENTAL SETTING", "content": "Implementation Details Keyframes are extracted using FFMPEG's scene detect filter. The full command is given in Appendix A. Visual features are extracted from keyframes using CLIP (Radford et al., 2021). The precise model used in the experiments of Section 5 is \u2018CLIP-ViT-g-14-laion2B-s12B-b42K' from https://github.com/mlfoundations/open_clip. This speaker diarization model is WhisperX (Bain et al., 2023), an extension of Open AI's Whisper model which can perform speaker diarization and accurate utterance timestamping. For visual descriptions, we use Kosmos 2 (Peng et al., 2023), which has been pretrained on several multimodal corpora as well as grounded image-text pairs (spans from the text are associated with image regions) and instruction-tuned on various vision-language instruction datasets. Our summarisation model is built on top of Llama 3.1 70B (Touvron et al., 2023). We use short simple prompts for Llama and Kosmos, which are given in full in Appendix B. We instruct summaries to be a maximum of 635 words (the mean in our test set), and truncate summaries to 650 words if they are longer.\nDataset We take screenplays (for comparison models and some testing, see below) and gold summaries from the recently released MovieSum dataset (Saxena & Keller, 2024). For all 200 movies in the test set, we purchased the corresponding videos to use as input to our model. We were able to find videos for 175/200 test set instances. These movies span multiple fiction genres: drama, action,"}, {"title": "5 RESULTS", "content": "Scene Detection To measure the accuracy of our scene segmentation method in isolation, we compare the partitions it produces to that arising from the ground truth scene breaks given in the gold screenplay. We perform dynamic time warping (Myers & Rabiner, 1981) on the dialogue lines in the gold screenplay and the timestamped utterances from the automatic transcript, in order to produce timestamps for the ground truth scene breaks. A naive metric would be the dis-"}, {"title": "6 CONCLUSION", "content": "In this work, we proposed the task of generating automatic screenplays for movies from only video and audio input. Our model, Screenwriter, produces screenplays automatically (including dialogue, speaker names, scene breaks and visual descriptions) based on two novel algorithms: one for segmenting the video into scenes, based on the minimum description length principle and dynamic-programing for search, and one for assigning character names to dialogue utterances using a database of names and actor faces. Experimental results show that the output of ScreenWriter together with"}]}