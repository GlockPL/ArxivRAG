{"title": "Multilingual Non-Factoid Question Answering with Silver Answers", "authors": ["Ritwik Mishra", "Sreeram Vennam", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "abstract": "Most existing Question Answering Datasets (QuADs) primarily focus on factoid-based short-context Question Answering (QA) in high-resource languages. However, the scope of such datasets for low-resource languages remains limited, with only a few works centered on factoid-based QuADs and none on non-factoid QuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with non-factoid questions. It utilizes interrogative subheadings from BBC news articles as questions and the corresponding paragraphs as silver answers. The dataset comprises over 370K QA pairs across 38 languages, encompassing several low-resource languages, and stands as the largest multilingual QA dataset to date. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden set), we observe that 98% of questions can be answered using their corresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS) model outperforms the baselines. The APS model attained an accuracy of 80% and 72%, as well as a macro F1 of 72% and 66%, on the MuNfQuAD testset and the golden set, respectively. Furthermore, the APS model effectively generalizes certain a language within the golden set, even after being fine-tuned on silver labels.", "sections": [{"title": "Introduction", "content": "A typical Question Answering Dataset (QuAD) conventionally comprises question-answer pairs (Baudi\u0161 and \u0160ediv\u00fd, 2015; Berant et al., 2013). However, certain QuADs are characterized by an additional component called evidence or context accompanying each question. This contextual information is expected to provide sufficient details to address the corresponding question, leading to these QuADs being referred to as Reading-Comprehension (RC) datasets as well. The majority of RC datasets focus on factoid answers, typically short phrases or named entities (Soleimani et al., 2021). For example, consider a factoid question, Who was the first Prime Minister of India?, with the corresponding factoid answer, Jawaharlal Nehru.\nAs compared to factoid questions, non-factoid questions have long descriptive answers consisting of multiple sentences or paragraphs. Extending the earlier example, a non-factoid question could be framed as How did Jawaharlal Nehru become the first Prime Minister of India? Evidence suggests that modern search engines are unable to answer non-factoid questions effectively (Cambazoglu et al., 2021a). Moreover, even humans find it difficult to answer non-factoid questions (Bolotova et al., 2022). In order to automatically answer non-factoid questions, large non-factoid QuADs are needed to fine-tune Question-Answering (QA) models. Multilingual QA models face additional challenges due to the lack of such resources, motivating the development of a multilingual QA dataset specifically designed for non-factoid questions.\nIn this study, we automatically extract Question-Answer pairs and their corresponding news articles from the British Broadcasting Corporation (BBC) website in multiple languages\u00b9. Except for the golden set, the dataset is not manually annotated since it relies on the hypothesis put forth by Soleimani et al. (2021) that all the paragraphs succeeding an interrogative subheading contains its answer. Therefore, we refer to this dataset as having silver labels/answers. Previous studies have indicated that silver labels have proven beneficial for constructing text classifiers in domains with limited availability of gold labels, such as legal (Neerbek et al., 2020), medical (Nowak et al., 2023), and news (Cripwell et al., 2023) domains. An evaluation contrasting the silver labels against the gold labels reveals that 98% of the questions were effec"}, {"title": "Related Works", "content": "WikiQA (Yang et al., 2015) emerged as an early dataset for automatic QA in English. It extracted questions from Bing query logs and matched them with relevant Wikipedia articles. SQUAD (Rajpurkar et al., 2016) is a benchmark QA dataset in English. Crowdworkers generated questions based on English Wikipedia passages and identified the answer within a short span of text. The most extensive dataset for factoid-based span detection is Natural Questions (Kwiatkowski et al., 2019). It comprises almost 320K questions, each accompanied by a long answer, a short answer, and a complete Wikipedia article as context. Our work closely resembles NLQuAD (Soleimani et al., 2021), but it was designed exclusively for the English language. For a comprehensive review of English"}, {"title": "Data Curation", "content": "We aimed to create a multilingual QA dataset with non-factoid questions. To achieve this, we utilized automated scraping of the BBC news website, gathering news articles and corresponding question-answer pairs. This study used Python requests and BeautifulSoup libraries to scrape data. For a given language (say Hindi), we ran a scraper on BBC (Hindi) website and another one on the Wayback machine\u2074 (also called web archive). The seed articles for the BBC website scraper are taken from the latest homepage of BBC (Hindi), whereas Wayback machine scraper starts from the earliest snapshot of the BBC (Hindi) homepage. The scraping approach was designed to extract news articles based on the presence of an interrogative subheading within a webpage. In Figure 1(a), the web interface of a BBC news article is depicted, while Fig"}, {"title": "MuNfQuAD Statistics", "content": "The presented dataset encompasses over 329,000 unique question-answer pairs, establishing itself as the most extensive mQuAD. Table 2 provides an overview of diverse statistics related to this dataset. We observed that over 75% of the articles in MuNfQuAD exceed the token limit (512) of traditional multilingual encoders designed for factoid QA models (Kumar et al., 2022; Gaschi et al., 2022). Section 5 discusses the computational constraints faced by state-of-the-art multilingual encoders with higher token limits when processing MuNfQuAD articles. A detailed illustration of word distribution among articles, paragraphs, questions, and answers can be found in Figure 3 of Appendix A.\nWe conducted web crawling on the BBC news website for all supported languages, resulting in data collection from 38 languages out of the 43 supported. A detailed breakdown of language distribution, along with the corresponding year of the earliest article in MuNfQuAD, is presented in Table 9 of Appendix E. Given the highly multilingual nature of the data, we experiment exclusively with QA models that are designed to be multilingual. To investigate n-gram trends, entity distribution, and question categories, we translated each question within MuNfQuAD to English. This translation was achieved using the nllb-200-1.3B model (Team et al., 2022), which boasts the unique capa"}, {"title": "Answer Paragraph Selection", "content": "In the context of a provided question and segmented context paragraphs, the Answer Paragraph Selection (APS) model assigns high confidence scores to paragraphs belonging to the silver answer. The APS model takes as input the concatenation of a question and the ith paragraph (\ud835\udc5d\u1d62) from the context. The output is a probability value ranging from 0 to 1, indicating the likelihood of \ud835\udc5d\u1d62 being an answer to the provided question. The choice of employing an APS model, as opposed to a sliding window Reading-Comprehension model (Soleimani et al., 2021), stems from the APS model's alignment with the Answer Sentence Selection (AS2) approach, which is deemed to be more relevant than the RC approach (Garg et al., 2020; Barlacchi et al., 2022). Xu et al. (2017) demonstrated the effectiveness of the APS component in automatically"}, {"title": "Implementation", "content": "The fine-tuning of our model was conducted across five GPU cards, employing a batch size of 12 on each GPU. We explored various pretrained encoders, including XLM-Roberta-base (XLM-R) (Conneau et al., 2019), multilingual cased bert (mBERT) (Devlin et al., 2019), cased multilingual distilbert (d-mBERT) (Sanh et al., 2019), multilingual-e5-base (mE5) (Wang et al., 2022), multilingual LUKE (mLUKE) (Ri et al., 2022), mT5 (Xue et al., 2021), and XLM-Vocabulary-base (XLM-V) (Liang et al., 2023), to serve as the backbone of our APS model. Additionally, the 560 million parameters variant of the BLOOM model (bloom) (Workshop et al., 2023) also served as the text encoder. The fine-tuning layers of the APS model consisted of three linear layers with a dropout value of 0.2. Coupled with a linear scheduler, learning rates were set at le-5 and 3e-3 for the encoder and fine-tuning layers, respectively. All the models were fine-tuned for a single epoch, a process that lasted for 25-33 hours. The PyTorch framework (Paszke et al., 2019) was utilized to construct the finetuning APS models, and the transformers library (Wolf et al., 2020) was employed to integrate pretrained transformers as text encoders."}, {"title": "Baselines", "content": "For establishing baselines, we employed the sentence-transformers library (sbert) (Reimers and Gurevych, 2019) to generate vector embeddings of questions (\ud835\udc38\ud835\udc5e) and paragraphs (\ud835\udc38\ud835\udc5d). In our study, the sbert baseline utilized the paraphrase-multilingual-MiniLM-L12-v2 (miniLM) and paraphrase-multilingual-mpnet-base-v2 (mpnet) (Reimers and Gurevych, 2020) as multilingual models. Another approach entailed obtaining \ud835\udc38\ud835\udc5e and \ud835\udc38\ud835\udc5d via training a TF-IDF vectorizer using the scikit-learn library (Pedregosa et al., 2011) on the training set. During preprocessing, punctuation and stopwords were removed from each languages. In both baseline approaches, the confidence score of a candidate paragraph containing the answer to the question was derived from the cosine similarity between \ud835\udc38\ud835\udc5e and \ud835\udc38\ud835\udc5d. Across all models, the threshold value was set to half the potential range of confidence scores. Specifically, a default threshold of 0.5 was adopted for the fine-tuned APS models and the TF-IDF baseline, as their output score spans 0 to 1. However, a default threshold of 0.0 was applied to the sbert baseline, which produces scores ranging from -1 to 1."}, {"title": "Evaluation", "content": "For paragraphs not aligning with the silver answer, a ground truth label of 0 is assigned, while paragraphs that belong to the silver answer receive Label-1. Our emphasis in this study is placed on the macro F1 and Label-1 metrics, owing to the pronounced data imbalance where only 23% of samples fall under Label-1. Additionally, we incorporate the Success Rate (SR) metric, which calculates the ratio of successfully answered questions to the total question count (Mishra et al., 2023; Bhagat et al., 2020). A question is considered successfully answered if there is at least one paragraph common between the candidate and reference paragraphs."}, {"title": "Results", "content": "With a substantial number of training examples (100M), we conducted hyperparameter tuning on"}, {"title": "Golden set", "content": "To evaluate the hypothesis that \"paragraphs succeeding an interrogative subheading contain its answer\" we employed human annotators to answer questions from a subset of MuNfQuAD, referred to"}, {"title": "LLMs", "content": "Additionally, we investigated the potential of employing Large Language Models (LLMs) as Answer Paragraph Selection (APS) models. To ensure a fair comparison with our fine-tuned APS models, we prompted LLMs with a question and each paragraph from the context, directing it to output a binary value indicating whether the paragraph could answer the given question (1 for yes, 0"}, {"title": "Discussion", "content": "We notice that the TF-IDF baseline yields a higher macro-F1 than the random baseline, indicating that silver answers frequently contain paragraphs with considerable word overlap with the given question. For instance, in the following Hindi question \u092c\u0948\u0920\u0928\u093e \u0928\u0941\u0915\u0938\u093e\u0928\u0926\u0947\u0939 \u0915\u094d\u092f\u094b\u0902? (Why sitting is harmful?), the sil-"}, {"title": "Conclusion", "content": "Question Answering (QA) in English has firmly established itself as a common task, backed by many tools and resources for answering factoid-based questions. Nonetheless, non-factoid QA have witnessed a significant expansion. Our study highlights the need for multilingual resources within this domain. In response, we introduce MuNfQuAD, a multilingual QA dataset addressing this gap. Comprised of non-factoid questions, MuNfQuAD spans across 38 languages, thus filling a critical gap in this area.\nThe compilation of the dataset involved scraping BBC news articles. The questions are identified through interrogative subheadings, while the subsequent paragraphs are taken as their corresponding silver answers. Notably, the news articles in MuNfQuAD predominantly revolve around the Asiatic subcontinent. A comparison with a manually curated golden set substantiates that nearly all of the silver answers can be used to answer the asked question. Additionally, our fine-tuned Answer Paragraph Selection (APS) model, trained using MuNfQuAD, yields a high Success Rate for both silver (0.91) and golden (0.96) labels. The results demonstrate that training the APS model with silver labels can effectively generalize some languages within the golden set."}, {"title": "Future Work", "content": "The Question-Answer pairs of MuNfQuAD can be used for training generative techniques in question-answering across different low-resource languages. Our examination reveals that MuNfQuAD encompasses a substantial proportion of factoid-based questions. Therefore, a multilingual answer span extractor can be used to provide silver labels for the minimal answer spans within MuNfQuAD. In a multilingual RAG pipeline, the fine-tuned APS model can be used as a reranking module (Nogueira et al., 2020; Ma et al., 2024)."}, {"title": "Ethical Considerations", "content": "Data scraping was conducted for six months, incorporating suitable time delays between each scraped article to prevent any potential user of the website from experiencing Denial of Service (DoS). Our goal is to provide access to MuNfQuAD for non-"}, {"title": "Limitations", "content": "The study conducted by Latham (2012) illustrated that BBC exhibits a left-of-center bias in its news coverage. Therefore, we recognize that MuNfQuAD will likely inherit a similar political bias. While a high Success Rate of silver answers indicates their reliability in addressing the corresponding questions, the comparatively lower F1 score for Label 1 suggests that the silver labels in MuNfQuAD are neither concise nor complete. In other words, the silver answers provide a response to the relevant question but include additional information as well. Additionally, nearly a third of questions in MuNfQuAD are classified as FACTOID, implying the potential presence of short-span answers within the silver paragraphs. It is imperative to approach the question categories with caution, primarily due to two reasons: (a) The classifier exhibits imperfections. We applied the same classifier to a non-factoid QuAD in English (Soleimani et al., 2021) and observed a comparable distribution of question categories, as depicted in Figure 2. (b) The classifier processes automatically translated English questions, introducing the possibility of unnatural translations that may alter classifier predictions. For instance, the Hindi question \u0905\u0917\u0930 \u0928\u0939\u0940\u0902 \u0915\u093f\u092f\u093e \u0924\u094b \u0915\u094d\u092f\u093e \u0939\u094b\u0917\u093e? was auto-translated as What if I didn't?, and the classifier predicted FACTOID. However, a more accurate translation of the question would be What will happen if not done?, for which the predicted category is EVIDENCE-BASED. Since this work primarily provides a new resource and an initial baseline, the novelty in architectural choices for the APS model and data curation efforts is limited.\nThe wide range of Inter-Annotator Agreement (IAA) scores across different languages points to the subjectivity involved in annotations for certain languages. It is worth noting that due to the significant monetary costs associated with the data annotation process, we opted for native speakers rather than experienced annotators, introducing a potential impact on the reliability of the golden set.\nAs we have outlined in Section 4, the fine-tuning of each APS model requires a day or two, which is why this study has not presented model results across multiple runs. Furthermore, the process"}]}