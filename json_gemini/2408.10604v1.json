{"title": "Multilingual Non-Factoid Question Answering with Silver Answers", "authors": ["Ritwik Mishra", "Sreeram Vennam", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "abstract": "Most existing Question Answering Datasets (QuADs) primarily focus on factoid-based short-context Question Answering (QA) in high-resource languages. However, the scope of such datasets for low-resource languages remains limited, with only a few works centered on factoid-based QuADs and none on non-factoid QuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with non-factoid questions. It utilizes interrogative subheadings from BBC news articles as questions and the corresponding paragraphs as silver answers. The dataset comprises over 370K QA pairs across 38 languages, encompassing several low-resource languages, and stands as the largest multilingual QA dataset to date. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden set), we observe that 98% of questions can be answered using their corresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS) model outperforms the baselines. The APS model attained an accuracy of 80% and 72%, as well as a macro F1 of 72% and 66%, on the MuNfQuAD testset and the golden set, respectively. Furthermore, the APS model effectively generalizes certain a language within the golden set, even after being fine-tuned on silver labels.", "sections": [{"title": "1 Introduction", "content": "A typical Question Answering Dataset (QuAD) conventionally comprises question-answer pairs (Baudi\u0161 and \u0160ediv\u00fd, 2015; Berant et al., 2013). However, certain QuADs are characterized by an additional component called evidence or context accompanying each question. This contextual information is expected to provide sufficient details to address the corresponding question, leading to these QuADs being referred to as Reading-Comprehension (RC) datasets as well. The majority of RC datasets focus on factoid answers, typically short phrases or named entities (Soleimani et al., 2021). For example, consider a factoid question, Who was the first Prime Minister of India?, with the corresponding factoid answer, Jawaharlal Nehru.\nAs compared to factoid questions, non-factoid questions have long descriptive answers consisting of multiple sentences or paragraphs. Extending the earlier example, a non-factoid question could be framed as How did Jawaharlal Nehru become the first Prime Minister of India? Evidence suggests that modern search engines are unable to answer non-factoid questions effectively (Cambazoglu et al., 2021a). Moreover, even humans find it difficult to answer non-factoid questions (Bolotova et al., 2022). In order to automatically answer non-factoid questions, large non-factoid QuADs are needed to fine-tune Question-Answering (QA) models. Multilingual QA models face additional challenges due to the lack of such resources, motivating the development of a multilingual QA dataset specifically designed for non-factoid questions.\nIn this study, we automatically extract Question-Answer pairs and their corresponding news articles from the British Broadcasting Corporation (BBC) website in multiple languages\u00b9. Except for the golden set, the dataset is not manually annotated since it relies on the hypothesis put forth by Soleimani et al. (2021) that all the paragraphs succeeding an interrogative subheading contains its answer. Therefore, we refer to this dataset as having silver labels/answers. Previous studies have indicated that silver labels have proven beneficial for constructing text classifiers in domains with limited availability of gold labels, such as legal (Neerbek et al., 2020), medical (Nowak et al., 2023), and news (Cripwell et al., 2023) domains. An evaluation contrasting the silver labels against the gold labels reveals that 98% of the questions were effec-"}, {"title": "2 Related Works", "content": "WikiQA (Yang et al., 2015) emerged as an early dataset for automatic QA in English. It extracted questions from Bing query logs and matched them with relevant Wikipedia articles. SQUAD (Rajpurkar et al., 2016) is a benchmark QA dataset in English. Crowdworkers generated questions based on English Wikipedia passages and identified the answer within a short span of text. The most extensive dataset for factoid-based span detection is Natural Questions (Kwiatkowski et al., 2019). It comprises almost 320K questions, each accompanied by a long answer, a short answer, and a complete Wikipedia article as context. Our work closely resembles NLQuAD (Soleimani et al., 2021), but it was designed exclusively for the English language. For a comprehensive review of English\nQA datasets, readers can refer to Cambazoglu et al. (2021b); Rogers et al. (2023).\nThe main focus of our work is on multilingual QuADs (mQuADs). An early endeavor in this domain is bAbI (Weston et al., 2016), which contained factoid-based questions and extractive answers in English and Hindi transliterated into Roman script. Gupta et al. (2018) introduced a bilingual Hindi-English QuAD, showing improved QA performance with question classification. Gupta et al. (2019) automatically translated a subset of SQUAD to Hindi, but we observed that a large majority of its answer indices were inaccurate. The XQA (Liu et al., 2019) dataset gathered questions from Wikipedia's \u201cDid you know?\u201d boxes. These questions omitted entity names, which were then employed as factoid answers. The top 10 Wikipedia articles related to the identified entity served as the context for each question. The authors also emphasized the constraints of using translation-based augmentation in QA systems. A subset of the SQUAD dataset was manually translated into ten languages, creating XQuAD (Artetxe et al., 2020).\nMLQA (Lewis et al., 2020) engaged crowd workers to generate questions from English Wikipedia articles and provide extractive answers. Subsequently, parallel sentences were extracted from the English article, and the English question-answer pair was manually translated into other languages. TyDi QA (Clark et al., 2020) represents a milestone in multilingual QuADs, focusing on natural questions where question makers are un-"}, {"title": "3 Data Curation", "content": "We aimed to create a multilingual QA dataset with non-factoid questions. To achieve this, we utilized automated scraping of the BBC news website, gathering news articles and corresponding question-answer pairs. This study used Python requests and BeautifulSoup libraries to scrape data. For a given language (say Hindi), we ran a scraper on BBC (Hindi) website and another one on the Wayback machine\u2074 (also called web archive). The seed articles for the BBC website scraper are taken from the latest homepage of BBC (Hindi), whereas Wayback machine scraper starts from the earliest snapshot of the BBC (Hindi) homepage. The scraping approach was designed to extract news articles based on the presence of an interrogative subheading within a webpage. In Figure 1(a), the web inter-"}, {"title": "3.1 MuNfQuAD Statistics", "content": "The presented dataset encompasses over 329,000 unique question-answer pairs, establishing itself as the most extensive mQuAD. Table 2 provides an overview of diverse statistics related to this dataset. We observed that over 75% of the articles in MuNfQuAD exceed the token limit (512) of traditional multilingual encoders designed for factoid QA models (Kumar et al., 2022; Gaschi et al., 2022). Section 5 discusses the computational constraints faced by state-of-the-art multilingual encoders with higher token limits when processing MuNfQuAD articles. A detailed illustration of word distribution among articles, paragraphs, questions, and answers can be found in Figure 3 of Appendix A.\nWe conducted web crawling on the BBC news website for all supported languages, resulting in data collection from 38 languages out of the 43 supported. A detailed breakdown of language distribution, along with the corresponding year of the earliest article in MuNfQuAD, is presented in Table 9 of Appendix E. Given the highly multilingual nature of the data, we experiment exclusively with QA models that are designed to be multilingual. To investigate n-gram trends, entity distribution, and question categories, we translated each question within MuNfQuAD to English. This translation was achieved using the nllb-200-1.3B model (Team et al., 2022), which boasts the unique capa-"}, {"title": "4 Answer Paragraph Selection", "content": "In the context of a provided question and segmented context paragraphs, the Answer Paragraph Selection (APS) model assigns high confidence scores to paragraphs belonging to the silver answer. The APS model takes as input the concatenation of a question and the ith paragraph (pi) from the context. The output is a probability value ranging from 0 to 1, indicating the likelihood of pi being an answer to the provided question. The choice of employing an APS model, as opposed to a sliding window Reading-Comprehension model (Soleimani et al., 2021), stems from the APS model's alignment with the Answer Sentence Selection (AS2) approach, which is deemed to be more relevant than the RC approach (Garg et al., 2020; Barlacchi et al., 2022). Xu et al. (2017) demonstrated the effectiveness of the APS component in automatically"}, {"title": "4.1 Implementation", "content": "The fine-tuning of our model was conducted across five GPU cards, employing a batch size of 12 on each GPU. We explored various pretrained encoders, including XLM-Roberta-base (XLM-R) (Conneau et al., 2019), multilingual cased bert (mBERT) (Devlin et al., 2019), cased multilingual distilbert (d-mBERT) (Sanh et al., 2019), multilingual-e5-base (mE5) (Wang et al., 2022), multilingual LUKE (mLUKE) (Ri et al., 2022), mT5 (Xue et al., 2021), and XLM-Vocabulary-base (XLM-V) (Liang et al., 2023), to serve as the backbone of our APS model. Additionally, the 560 million parameters variant of the BLOOM model (bloom) (Workshop et al., 2023) also served as the text encoder. The fine-tuning layers of the APS model consisted of three linear layers with a dropout value of 0.2. Coupled with a linear scheduler, learning rates were set at le-5 and 3e-3 for the encoder and fine-tuning layers, respectively. All the models were fine-tuned for a single epoch, a process that lasted for 25-33 hours. The PyTorch framework (Paszke et al., 2019) was utilized to construct the finetuning APS models, and the transformers library (Wolf et al., 2020) was employed to integrate pretrained transformers as text encoders."}, {"title": "4.2 Baselines", "content": "For establishing baselines, we employed the sentence-transformers library (sbert) (Reimers and Gurevych, 2019) to generate vector embeddings of questions (Eq) and paragraphs (Ep). In our study, the sbert baseline utilized the paraphrase-multilingual-MiniLM-L12-v2 (miniLM) and paraphrase-multilingual-mpnet-base-v2 (mpnet) (Reimers and Gurevych, 2020) as multilingual models. Another approach entailed obtaining Eq and Ep via training a TF-IDF vectorizer using the scikit-learn library (Pedregosa et al., 2011) on the training set. During preprocessing, punctuation and stopwords were removed from each languages. In both baseline approaches, the confidence score of a candidate paragraph containing the answer to the question was derived from the cosine similarity between Eq and Ep. Across all models, the threshold value was set to half the potential range of confidence scores. Specifically, a default threshold of 0.5 was adopted for the fine-tuned APS models and the TF-IDF baseline, as their output score spans 0 to 1. However, a default threshold of 0.0 was applied to the sbert baseline, which produces scores ranging from -1 to 1."}, {"title": "4.3 Evaluation", "content": "For paragraphs not aligning with the silver answer, a ground truth label of 0 is assigned, while paragraphs that belong to the silver answer receive Label-1. Our emphasis in this study is placed on the macro F1 and Label-1 metrics, owing to the pronounced data imbalance where only 23% of samples fall under Label-1. Additionally, we incorporate the Success Rate (SR) metric, which calculates the ratio of successfully answered questions to the total question count (Mishra et al., 2023; Bhagat et al., 2020). A question is considered successfully answered if there is at least one paragraph common between the candidate and reference paragraphs."}, {"title": "5 Results", "content": "With a substantial number of training examples (100M), we conducted hyperparameter tuning on"}, {"title": "5.1 Golden set", "content": "To evaluate the hypothesis that \"paragraphs succeeding an interrogative subheading contain its answer\" we employed human annotators to answer questions from a subset of MuNfQuAD, referred to"}, {"title": "5.2 LLMs", "content": "Additionally, we investigated the potential of employing Large Language Models (LLMs) as Answer Paragraph Selection (APS) models. To ensure a fair comparison with our fine-tuned APS models, we prompted LLMs with a question and each paragraph from the context, directing it to output a binary value indicating whether the paragraph could answer the given question (1 for yes, 0"}, {"title": "6 Discussion", "content": "We notice that the TF-IDF baseline yields a higher macro-F1 than the random baseline, indicating that silver answers frequently contain paragraphs with considerable word overlap with the given question. For instance, in the following Hindi question \u092c\u0948\u0920\u0928\u093e \u0928\u0941\u0915\u0938\u093e\u0928\u0926\u0947\u0939 \u0915\u094d\u092f\u094b\u0902? (Why sitting is harmful?), the sil-"}, {"title": "7 Conclusion", "content": "Question Answering (QA) in English has firmly established itself as a common task, backed by many tools and resources for answering factoid-based questions. Nonetheless, non-factoid QA have witnessed a significant expansion. Our study highlights the need for multilingual resources within this domain. In response, we introduce MuNfQuAD, a multilingual QA dataset addressing this gap. Comprised of non-factoid questions, MuNfQuAD spans across 38 languages, thus filling a critical gap in this area.\nThe compilation of the dataset involved scraping BBC news articles. The questions are identified through interrogative subheadings, while the subsequent paragraphs are taken as their corresponding silver answers. Notably, the news articles in MuNfQuAD predominantly revolve around the Asiatic subcontinent. A comparison with a manually curated golden set substantiates that nearly all of the silver answers can be used to answer the asked question. Additionally, our fine-tuned Answer Paragraph Selection (APS) model, trained using MuNfQuAD, yields a high Success Rate for both silver (0.91) and golden (0.96) labels. The results demonstrate that training the APS model with silver labels can effectively generalize some languages within the golden set."}, {"title": "7.1 Future Work", "content": "The Question-Answer pairs of MuNfQuAD can be used for training generative techniques in question-answering across different low-resource languages. Our examination reveals that MuNfQuAD encompasses a substantial proportion of factoid-based questions. Therefore, a multilingual answer span extractor can be used to provide silver labels for the minimal answer spans within MuNfQuAD. In a multilingual RAG pipeline, the fine-tuned APS model can be used as a reranking module (Nogueira et al., 2020; Ma et al., 2024)."}, {"title": "8 Ethical Considerations", "content": "Data scraping was conducted for six months, incorporating suitable time delays between each scraped article to prevent any potential user of the website from experiencing Denial of Service (DoS). Our goal is to provide access to MuNfQuAD for non-"}, {"title": "9 Limitations", "content": "The study conducted by Latham (2012) illustrated that BBC exhibits a left-of-center bias in its news coverage. Therefore, we recognize that MuNfQuAD will likely inherit a similar political bias. While a high Success Rate of silver answers indicates their reliability in addressing the corresponding questions, the comparatively lower F1 score for Label 1 suggests that the silver labels in MuNfQuAD are neither concise nor complete. In other words, the silver answers provide a response to the relevant question but include additional information as well. Additionally, nearly a third of questions in MuNfQuAD are classified as FACTOID, implying the potential presence of short-span answers within the silver paragraphs. It is imperative to approach the question categories with caution, primarily due to two reasons: (a) The classifier exhibits imperfections. We applied the same classifier to a non-factoid QuAD in English (Soleimani et al., 2021) and observed a comparable distribution of question categories, as depicted in Figure 2. (b) The classifier processes automatically translated English questions, introducing the possibility of unnatural translations that may alter classifier predictions. For instance, the Hindi question \u0905\u0917\u0930 \u0928\u0939\u0940\u0902 \u0915\u093f\u092f\u093e \u0924\u094b \u0915\u094d\u092f\u093e \u0939\u094b\u0917\u093e? was auto-translated as What if I didn't?, and the classifier predicted FACTOID. However, a more accurate translation of the question would be What will happen if not done?, for which the predicted category is EVIDENCE-BASED. Since this work primarily provides a new resource and an initial baseline, the novelty in architectural choices for the APS model and data curation efforts is limited.\nThe wide range of Inter-Annotator Agreement (IAA) scores across different languages points to the subjectivity involved in annotations for certain languages. It is worth noting that due to the significant monetary costs associated with the data annotation process, we opted for native speakers rather than experienced annotators, introducing a potential impact on the reliability of the golden set.\nAs we have outlined in Section 4, the fine-tuning of each APS model requires a day or two, which is why this study has not presented model results across multiple runs. Furthermore, the process"}, {"title": "F APS Model Ablations", "content": "We performed ablation studies during the finetuning of the APS model, experimenting with various hyperparameters. These included adding prior context to paragraphs until they reached the model's token limit, incorporating the article title with the question, using weighted binary cross entropy loss (wbce) instead of weighted focal loss (wfl), and integrating positional embeddings (PE) with context paragraphs. The results of these ablation studies on a subset of the fine-tuning data are presented in Table 10."}, {"title": "G APS models on Non-Factoid Questions", "content": "As the English translations of all the questions in MuNfQuAD were input into the fine-tuned question classification model by Bolotova et al. (2022), we filtered the MuNfQuAD test set to include only those questions for which the class prediction was anything other than FACTOID. The APS model was then applied to these selected questions, and the results are presented in Table 11."}, {"title": "H APS models with different thresholds", "content": "The default threshold for any APS model was set at 50% of its possible output range. The cosine similarity metric is employed for both the SBERT and TF-IDF methods; however, since TF-IDF embeddings never contain negative values, the cosine similarity between two TF-IDF vectors always falls between 0 and 1. In contrast, for SBERT, the"}, {"title": "I LLM as APS model", "content": "We investigated various multilingual LLMs as APS models and assessed their performance on the MuNfQuAD golden set, with the results detailed in Table 12. Our observations indicate that Command-R from CohereForAI yielded the best outcomes. In addition to multilingual LLMs, we also tested bilingual models such as Hi-NOLIN\u2079 and OpenHathi10, which are pretrained on Hindi and English text. Unfortunately, the performance of these bilingual models in Hindi did not surpass that of C4Ai."}, {"title": "CAPS baselines on the golden set", "content": "We evaluated the baseline APS models on the MuNfQuAD golden set, selecting sbert and TF-IDF baselines due to their distinct methodologies compared to our fine-tuned APS models. The results are provided in Table 7."}, {"title": "D Phrases for Excluding Criterion", "content": "In this study, we identified certain subheadings that ended with an interrogative symbol yet were unrelated to the article's content. These subheadings were detected by examining the most common interrogative subheadings across each language. A list of these subheadings is provided in Table 8 on the next page. If an interrogative subheading contained any of the identified phrases, it was not classified as a question in MuNfQuAD."}]}