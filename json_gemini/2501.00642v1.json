{"title": "Enabling New HDLs with Agents", "authors": ["Mark Zakharov", "Farzaneh Rabiei Kashanaki", "Jose Renau"], "abstract": "Large Language Models (LLMs) based agents are transforming the programming language landscape by facilitating learning for beginners, enabling code generation, and optimizing documentation workflows. Hardware Description Languages (HDLs), with their smaller user community, stand to benefit significantly from the application of LLMs as tools for learning new HDLs. This paper investigates the challenges and solutions of enabling LLMs for HDLs, particularly for HDLs that LLMs have not been previously trained on.\nThis work introduces HDLAgent, an AI agent optimized for LLMs with limited knowledge of various HDLs. It significantly enhances off-the-shelf LLMs. For example, PyRTL's success rate improves from zero to 35% with Mixtral 8x7B, and Chisel's success rate increases from zero to 59% with GPT-3.5-turbo-0125. HDLAgent offers an LLM-neutral framework to accelerate the adoption and growth of HDL user bases in the era of agentic LLMs.", "sections": [{"title": "Introduction", "content": "Recent advancements in Large Language Models (LLMs), such as OpenAI's GPT, Google's Gemini, and Mistral AI's Mixtral are transforming the programming landscape. These models assist newcomers by providing intelligent assistance, generating code snippets, and offering context-aware suggestions, thereby significantly lowering the barriers to entry into programming.\nHowever, LLMs currently offer limited support for niche Hardware Description Languages (HDLs), which often form specialized communities. Despite the ubiquity of Verilog\u00b9, emerging languages like it."}, {"title": "Related Work", "content": "To adapt to a new language, there are two potentially complementary approaches to improve LLM output: fine-tuning and Agents. These techniques can be iteratively combined to develop Agents that produce even better results.\nFine-tuning is the process of adjusting the parameters of an LLM on a specific dataset or task to improve its performance. Thus, fine-tuning can be applied to optimize an LLM for a new language. RTL-Coder [21] fine-tunes a 7B Mistral model with GPT-generated synthetic Verilog data. In contrast, HDLAgent uses off-the-shelf LLMs without fine-tuning. The advantage of avoiding fine-tuning is that many commercial flows do not allow it, and it is not a trivial problem for languages with a very small set of examples.\nURIAL [18] bypasses the need for fine-tuning by enriching prompts with illustrative examples. These prompts resemble the few-shot format used by HDLAgent, incorporating both format and examples. While URIAL has shown effectiveness in circumventing the need for instruction alignment, HDLAgent further illustrates the possibility of learning previously unknown languages.\nAgents [49] iterate through LLMs using three main techniques to improve performance: self-reflection, memory, and grounding.\nSelf-reflection techniques use a sequence of interactions with the LLM instead of a simple question/answer format. In this work, we call self-reflection to chaining LLMs prompts to other LLMs. CoT [36] is an example of self-reflection. Lumos [43] uses CoT to enable simpler LLMs to outperform more advanced ones. These studies highlight significant progress in this rapidly evolving field. Recent works [39] propose an optimization method to find the best prompt.\nMemory techniques such as few-shot in-context learning [19, 5] and RAG [17] use instructions, supplemental information, and relevant examples to enhance efficiency. Various methods exist for constructing prompts with extended context. One such technique, querying an embedding database to augment the context, is known as Retrieval Augmented Generation (RAG).\nGrounding involves verifying or checking the LLM's response using an external tool. While this is not always feasible, in code generation, a compiler or testbench can validate and identify issues with the LLM-generated response. This may trigger further iteration with the LLM.\nAgents with self-reflection, memory, and grounding have been applied to improve code generation.\nIf we ignore the HDL target, and focus on generic programming languages like Python or C++, several works [29, 48] show that errors can be fixed by grounding the generated code against compiler or testbench feedback. Intervenor [34] proposes an Agent that successfully leverages compiler feedback. Other recent works [24, 9, 34, 23, 25, 38, 45, 12, 26] propose Agents to iterate over testbench results to fix sematic errors in code.\nVerilog Coder [47] introduces an autonomous coding approach using graph-based planning for synthesizing Verilog code from specifications. It combines a traditional LLM with a novel AST-based waveform tracing tool to refine the generated code. This tool traces the expected signal flow within the AST rep-"}, {"title": "HDLAgent", "content": "HDLAgent is an AI Agent (refer to Section 2) specifically tailored for adapting cutting-edge AI coding techniques to Hardware Description Languages (HDLs). This adaptation is crucial for HDLs that are not typically included in the training data of Large Language Models (LLMs).\nLLMs demonstrate proficiency in transfer learning [27, 46]. HDLAgent exploits this capability, enabling LLMs to handle HDLs with limited training data. By facilitating the transfer of knowledge from well-known HDLs, such as Verilog, to new HDLs such as PyRTL, HDLAgent empowers the LLM to adapt its understanding of familiar programming languages to target languages. This process mirrors human"}, {"title": "Main Context", "content": "The \"main context\" in HDLAgent serves to inform the LLM about the specific HDL in use. Figure 2 illustrates this main context, which comprises four key elements: HDL description, few-shot examples, Prefix, and Suffix.\nThe HDL description provides a concise summary of the HDL, tailored to the LLM's familiarity with the language. While our evaluation demonstrates that the HDL description can be optimized for each LLM, we opt for simplicity by selecting the description that performs best with Mix-8x7B and GPT-3.5n. This choice is motivated by these LLMs' lower proficiency in HDLs such as Chisel, DSLX, and PyRTL. It is worth noting that the HDL description proves less beneficial only when the LLM already excels in a given HDL, such as Verilog. Such few-shot are crucial, especially in HDLs with unique syntaxes, helping LLMs avoid common pitfalls.\nThe prefix and suffix in the main context serve as navigational aids for the LLM, directing the model's attention to the task at hand and setting boundaries for its output. The prefix introduces the problem in the HDL's language, while the suffix provides specific instructions to ensure the output adheres strictly to HDL syntax, avoiding unnecessary English explanations and maintaining consistency in output formats.\nInterestingly, even for LLMs capable of processing entire HDL reference manuals within their context window, utilizing a summary enhances both success rate and token efficiency. Our evaluation clearly demonstrates a significant improvement in success rates when employing an HDL description. The intuition is that focusing on the Verilog differences is more important than providing a lengthy description of the language.\nSince including a complete tutorial is neither practical nor advantageous for the evaluated LLMs, we use an HDL description summary instead. To generate these summaries, we leverage LLMs with large context windows, specifically GPT-4 and GPro-1.0, to condense the HDL reference manuals.\nFor PyRTL and Chisel, our evaluation revealed that the most effective prompt was generated by GPT-4 using the following instruction: \"PyRTL is a Hardware Description Language with the following reference documentation and tutorial. Create documentation useful for LLMs trying to generate PyRTL code. The generated documentation should include code snippets and highlight any language syntax that is atypical for HDLs.\"\nFor DSLX, the optimal summary was produced by GPro-1.0 using a similar prompt, with the addition of \"Be concise and avoid examples with similar syntax.\" at the end. This minor variation in the prompt yielded the best results.\nComplementing the HDL description, the \"main context\" provides few-shot examples to illustrate common HDL operations and potential areas of confusion. These examples cover bit operations, reductions, loops, multiplexing, and a multiply-add block. While the HDL Description can include some examples, it is important to cover these basic operations with simple examples, as LLMs tend to revert to incorrect syntax.\nThe bit operations example demonstrates simple bit manipulation and concatenation, while the reduction example showcases a basic NOR reduction over a given input."}, {"title": "Compiler Context", "content": "HDLAgent's compiler context employs an iterative approach to rectify inaccurately generated HDL code. This process grounds the LLM-generated code by providing feedback on potential errors or hallucinations. Before submitting the LLM output to the compiler, HDLAgent identifies the code section. This step is crucial, as LLMs may generate English explanations despite explicit instructions to avoid them.\nWhen the generated program fails compilation, producing a compiler error, HDLAgent constructs a query (illustrated in Figure 2). This query begins with the \"main context,\" disregarding any non-code responses. It then incorporates the latest code snippet, followed by a statement indicating \"the previous code has the following compile error,\" succeeded by the specific compiler error message. If HDLAgent possesses an example fix for addressing the compiler error, it appends this \"sample fix\" to the context.\nThe sample fix methodology is analogous to RTL-Fixer [33], which provides explanations for resolving Verilog error messages. HDLAgent extends this concept to cover multiple HDLs, elucidating the special syntax requirements of a given HDL when necessary.\nHDLAgent presents the entire latest code snippet in its query. We experimented with a method inspired by CWhy [4], which focuses on a few lines of code surrounding the compiler error message. While this approach worked for some LLMs like GPT-4, it proved less effective with others. Although this delta approach reduces token usage, it led to increased error rates, prompting us to exclude it from our evaluation. As LLMs continue to evolve, this approach may warrant reconsideration in future iterations."}, {"title": "Prompt Optimizations", "content": "Besides the previous main and compiler context there are also several subtle but important optimizations:\n\u2022 Placing the prompt after the context achieves better results [14].\n\u2022 HDLAgent approach avoids the chat-like history with all the previous code generations and fixes. Keeping the original question iteration but not the compiler error fixes achieves better results [32]. We did a quick test with HDLAgent and DSLX. Avoiding a history with all the error fixes had a 5% improvement in GPT-4 and a 27% in Mix-8x7B.\n\u2022 Most LLMs generate code snippets in quoted sections, but not always. Even worse, it is common to write English explanations even thought the prompt explicitly asks to just write code. To address this, for each language we have a filter/detector that removes English and finds code boundaries. For example in Verilog it allows preprocessor directives and code between module and endmodule. Without this, some smaller LLMs fail very frequently."}, {"title": "LLM Cost", "content": "Our approach approximates LLM cost by the number of tokens utilized, serving as a practical proxy for monetary cost and compute resources required. As context length increases, so do the costs and computational demands.\nWhile token usage offers a simple metric for gauging efficiency, our primary focus remains on balancing accuracy with cost-efficiency. This approach necessitates a judicious use of context and iteration, ensuring that each interaction with the LLM is as productive as possible. Future studies could investigate efficiency metrics like error rate \u00d7 tokens."}, {"title": "Evaluation", "content": "To comprehensively assess HDLAgent's performance across various LLMs, we evaluate each HDL (Chisel, PyRTL, DSLX, and Verilog) against four benchmark tests: VH (VerilogEval-Human), VM (VerilogEval-Machine), HC (HDLEval-Comb), and HP (HDLEval-Pipe).\nWhile VerilogEval tests comprise several Verilog-specific questions, they do not fully demonstrate the potential of the LLM/HDLAgent combination as effectively as HDLEval (HC, HP). This is primarily due to VerilogEval's inclusion of Verilog-specific instructions in some tests, such as implementing a D latch using an \"always\" block. Such tests are not suitable for evaluating languages other than Verilog. Consequently, we utilize VH and VM primarily for Verilog or as a reference point, while focusing our evaluation on HDLEval (HC and HP).\nThe results are broken down into five key components to delineate the incremental benefits provided by HDLAgent:\n\u2022 Base: Represents the baseline performance of an LLM without HDLAgent enhancements but includes basic I/O formatting and general code generation guidelines.\n\u2022 Description: Adds a concise HDL Description to the LLM context, improving specificity (see Section 3.1 for details).\n\u2022 Few-shot: Adds language-specific few-shot examples. Section 5.2 provides further insights on the HDL Description and few-shot context selection.\n\u2022 Compile: Incorporates compiler feedback with up to eight iterations to refine the generated code, optimizing accuracy (justification for the number of iterations is in Section 5.3).\n\u2022 Fixes: Performs the same iterations as Compile, but for each iteration, provides a suggestion alongside a generic example on how to address the specific compiler error.\nChisel (Figure 3), a Scala-based HDL, presents a unique challenge and opportunity. Most LLMs are familiar with Scala but have limited knowledge of Chisel. While several LLMs demonstrate familiarity with its basic syntax, only GPT-4 initially performs adequately with Chisel (52% success rate). All other LLMs exhibit a mere 3% success rate or less."}, {"title": "HDLAgent Context Insights", "content": "This section offers insights into the selection of HDL Description and few-shot context. One straightforward approach is to utilize the full reference manual directly for the specific language. While this is feasible for models with large context windows such as GPT-4, Mix-8x7B, and GPro-1.0, it generally proves less effective than employing a summarized HDL description. For instance, using a full reference instead of a summary yields no change in results for GPro-1.0, but reduces the success rate from 77% to 66% for GPT-4, and from 59% to 33% for Mix-8x7B. These findings indicate that future LLMs need to improve their handling of lengthy contexts, as all evaluated models struggle with this aspect. Nevertheless, even if the LLMs improve, it is still advantageous to use smaller summary context because it reduces the LLM cost.\nFigure 7 shows the DSLX, PyRTL, and Chisel success rate as different reference manuals are summarized for HDLAgent. Each bar shows a different LLM reference summarization prompt (Section 3.1) sorted by accuracy. The breakdown is the contribution of the few-shot examples and the HDL description. Interestingly, adding Few-shot always improves results, and removing HDL Description and just keeping few-shot examples is a reasonable alternative. In some HDL/LLM combinations like Chisel/GPT-3.5n, using either Few-shot or Description works. For other combinations like DSLX/Mix-8x7B, HDL Description helps but Few-shot is necessary. Optimal results require both Few-shot and HDL Description."}, {"title": "Pass Sensitivity", "content": "Top@k is a popular method that measures how results can be improved by generating multiple attempts. A k=5 means that when 5 LLM tries are used, at least one has the correct code generation. Table 3 shows tests passed for HDLEval-Comb for multiple LLMs and multiple top@k values (1,5,10). Due to space, only the HDLEval-Comb results are shown.\nLess popular HDLs benefit more from higher top@k values. For example, DSLX shows a 1.22 to 2.08 times improvement in test pass rates from top@1 to top@10. Verilog has between 1.16 and 1.45 times. This discrepancy is likely because the LLM, unfamiliar with the language, starts from an incorrect baseline and struggles to correct errors through compiler feedback. Not being able to recover is very rare in Verilog but over 10% of the DSLX tests have this problem. The higher the top@k, the easier it is to avoid. Once the code compiles correctly, the failure"}, {"title": "Time and QoR", "content": "Execution time is a crucial metric for any AI Agent. It refers to the time HDLAgent requires to generate a response, not the quality of results (QoR). Figure 9 presents a boxplot of execution times for HDLEval-Comb across different LLMs, encompassing both successful and failed tests. All languages except Verilog undergo a translation process to Verilog, adding overhead. In HDLAgent, the execution time is a function of tokens, the number of iterations, and external compiler speed.\nAmong the HDLs, Chisel stands out as the main outlier, with approximately 2/3 of the execution time consumed by the FIRRTL compiler generating Verilog. GPT-4 exhibits faster performance due to fewer errors and consequently fewer iterations. PyRTL and DSLX also show slower performance than Verilog, partly due to additional iterations.\nComparing LLMS, GPT-3.5n and GPT-3.50 generally demonstrate faster overall performance, combining fewer error iterations with rapid result generation. External tokens benchmarking [1] indicates that GPro-1.0 is approximately 30% faster than GPT-3.5n and four times faster than GPT-4. However, HDLA-"}, {"title": "Usefulness Insights", "content": "HDLAgent significantly improves the LLM performance across all LLMs and HDLs, but in some cases like DSLX, the average HDLEval-Comb performance is around 60%. This can be interpreted as not good enough because it fails many times.\nThis section provides more insights in which tests pass and fail. HDLEval-Comb comprises 134 tests, with some being relatively small, containing only a few lines of code, while others are significantly more extensive. HDLEval is designed to encompass a range of tests, from straightforward to complex.\nThe output program complexity provides key insights in current LLM and HDLAgent limitations. The best proxy for complexity is not the input problem itself, but the lines of code (LoC) required to implement such a problem in a specific language (Verilog in this case)."}, {"title": "Insights for HDLs at the age of LLMS", "content": "The goal of this section is to show shortcomings in HDLs that must addressed to improve accuracy in an LLM world.\nVerilog is the language that LLMs understand the best. For top-performing LLMs like GPT-4, the main challenge lies in handling pipelining. Verilog allows for unrestricted pipelining, which deviates from the traditional Von Neumann architecture and non-hardware program structure. GPT-4 effectively generates combinational logic because a typical program without recursion or memory access can be directly translated to Verilog. Improving pipelining remains an open research question that must be addressed to enhance the performance of LLMs in hardware design tasks.\nBesides the common pipelining issue, Chisel LLM code generation needs help with matching Chisel generated Verilog to native Verilog. As a part of compilation process, the generated Verilog module's IO appends \"io_\" to all names. Additional clock and reset signals are created by default, even if unused in the original Chisel code."}, {"title": "PyRTL", "content": "PyRTL shares common problems with Verilog and Chisel, but it also has a problem with semantics.\nThe PyRTL DSL problem is when the LLM generates Python syntax to implement logic instead of the PyRTL syntax. In Listing 2 the \"INVALID\" case uses Python \"inp>1\" instead of the PyRTL shift right logical library call. Many such programs generate errors which are caught and rectifed with further HDLAgent iterations.\nBesides DSL problems, PyRTL has errors due to inconsistent semantics. In Verilog and Chisel, a right shift logical of a positive number reduces the bus size. For example if \"inp\" has 4 bits, and it is right shifted once, the output has 3 bits. Whereas in PyRTL, it stays 4 bits but the most significant bit is hardwired to zero. Listing 2 showcases the problem in one HDLEval test. The most significant bit is xored with zero which is not the expected result, as detailed in the \"equivalent\" case."}, {"title": "DSLX", "content": "DSLX presented a different set of challenges than DSLs like Chisel and PyRTL. Since it does not support unrestricted pipelining, only combinational logic is considered in this section's feedback.\nDSLX shares IO generation issues with Chisel and PyRTL but faces even greater challenges. DSLX generated Verilog modules have a single output named \"out\". DSLX's solution to multiple outputs is to return a struct. HDLAgent addresses it by postprocessing the generated Verilog and modifying the output port name to match the desired IO. A better solution that requires DSLX semantic changes would be to adopt a Go-like syntax that allows for multiple named outputs and ensures Verilog generation respects those outputs.\nAnother interesting source of errors stems from DSLX being \"similar to Rust\". If the HDLAgent's HDL Description mentions that \"DSLX is similar to Rust...\" it frequently erroneous code. Even without this sentence, the LLM sometimes generates legal Rust but illegal DSLX code. Some differences are easy to spot, such as DSLX's \"assert(cond)\" versus Rust's \"assert_eq!(cond),\" while others, like the presence of Rust annotations like \"#[test]\" in DSLX code, are more subtle. To address the \"similar but not the same\" syntax issues, it is suggested to avoid mentioning the similarity and catch any discrepancies during compilation time, generating a compile error for HDLAgent to fix."}, {"title": "Future Work and Conclusions", "content": "This paper has demonstrated that Large Language Models (LLMs) hold transformative potential for computer science, particularly in the domain of Hardware Description Languages (HDLs). We introduced HDLAgent, an AI Agent designed to significantly enhance the ability of LLMs to generate code for HDLs that are not commonly represented in training datasets, such as Chisel, PyRTL, and DSLX. The development of new HDLs often relies on the capabilities of LLMs, and HDLAgent facilitates this by enabling effective use of existing LLMs without extensive retraining.\nOur evaluations show that HDLAgent achieves a success rate of over 90% on concise examples across all HDLs, making it an excellent tool for educational purposes in teaching new HDL languages. However, the performance of HDLAgent and traditional LLM approaches tends to decline with larger or more complex designs. For instance, even advanced LLMs like GPT-4 see a drop in success rates for Verilog projects exceeding 75 lines of code.\nThis work identifies several challenges and avenues for future research:\n\u2022 Quality of Results (QoR) issues observed in specific languages like DSLX need addressing to improve the robustness of generated designs.\n\u2022 Consistently low success rates for pipelined designs suggest a need for specialized techniques or enhancements in LLM architectures.\n\u2022 To accommodate complex designs, we recommend further development towards making HDLs and their compilers more conducive to LLM integration.\nMoreover, while HDLAgent has shown to elevate performance significantly-raising the Verilog success rate of GPT-4 from 34% to 72%-it also highlights the scalability challenges when tackling more extensive and intricate designs.\nIn conclusion, HDLAgent not only broadens the applicability of LLMs in the field of HDLs beyond Verilog but also illuminates key challenges when scaling to larger systems. To aid the community and foster further research, we will open-source the HDLAgent code, providing a valuable resource for developers and researchers aiming to enhance the interaction between LLMs and HDL design."}]}