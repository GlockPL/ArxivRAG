{"title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely", "authors": ["Siyun Zhao", "Yuqing Yang", "Zilong Wang", "Zhiyuan He", "Luna K. Qiu", "Lili Qiu"], "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capa-\nbilities in completing real-world tasks. External data not only bolsters the models' domain-specific\nexpertise and temporal relevance but also diminishes incidences of hallucination, thereby enhancing\nboth the controllability and interpretability of outputs. Techniques for integrating external data into\nLLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing atten-\ntion and widespread application. Nonetheless, the effective deployment of data-augmented LLMs\nacross various specialized fields presents substantial challenges. These challenges encompass a wide\nrange of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing\nthe reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all\nsolution for data-augmented LLM applications. In practice, underperformance often arises from a\nfailure to correctly identify the core focus of a task or because the task inherently requires a blend of\nmultiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG\ntask categorization method, classifying user queries into four levels based on the type of external data\nrequired and the task's primary focus: explicit fact queries, implicit fact queries, interpretable ratio-\nnale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets,\nand summarize the key challenges and most effective techniques for addressing these challenges.\nFinally, we discuss three main forms of integrating external data into LLMs: context, small model,\nand fine-tuning, highlighting their respective strengths, limitations, and the types of problems they\nare suited to solve. This work aims to help readers thoroughly understand and decompose the data\nrequirements and key bottlenecks in building LLM applications, offering solutions to the different\nchallenges and serving as a guide to systematically developing such applications.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities, including extensive world knowledge and\nsophisticated reasoning skills. Despite these advancements, there are significant challenges in effectively deploying\nthem across various specialized fields. These challenges include issues like model hallucinations, misalignment with\ndomain-specific knowledge, among others. Incorporating domain-specific data, particularly private or on-premise data\nthat could not be included in their initial training corpus, is crucial for tailoring LLM applications to meet specific\nindustry needs. Through techniques like RAG and fine tuning, data augmented LLM applications have demonstrated\nadvantages over applications built solely on generic LLMs, in several aspects:\n\u2022 Enhanced Professionalism and Timeliness: The data used to train LLMs often lags in timeliness and\nmay not cover all domains comprehensively, especially proprietary data owned by users. Data augmented\nLLM applications address this issue by providing more detailed and accurate answers for complex questions,\nallowing for data updates and customization.\n\u2022 Alignment with Domain Experts: Through the use of and learning from domain-specific data, data augmented\nLLM applications can exhibit capabilities more like domain experts, such as doctors and lawyers.\n\u2022 Reduction in Model Hallucination: Data augmented LLM applications generate responses based on real\ndata, grounding their reactions in facts and significantly minimizing the possibility of hallucinations.\n\u2022 Improved Controllability and Explainability: The data used can serve as a reference for the model's\npredictions, enhancing both controllability and explainability.\nDespite the enthusiasm for these advancements, developers often struggle and have to invest a significant amount\nof human labor to meet its expectations (e.g., achieving a high success rate in question answering). Numerous\nstudies [1, 2, 3, 4, 5] highlight the challenges and frustrations involved in constructing a data augmented LLM\napplications based on technologies like RAG and fine-tuning, particularly in specialized domains such as the legal field,\nhealthcare, manufacturing, and others.\nThese challenges span a wide range, from constructing data pipelines (e.g., data processing and indexing) to leveraging\nLLMs' capabilities to achieve complex intelligent reasoning. For example, in applications of finance, there is a frequent\nneed to understand and utilize high-dimensional time series data, whereas in healthcare, medical images or time-series\nmedical records are often essential. Enabling LLMs to comprehend these varied forms of data represents a recurring\nchallenge. On the other hand, in legal and mathematical applications, LLMs typically struggle to grasp long-distance\ndependencies between different structures. Additionally, depending on the specific application domain, there are\nincreased demands for the interpretability and consistency of LLM responses. The inherent nature of LLMs tends\nto be characterized by low interpretability and high uncertainty, which poses significant challenges. Enhancing the\ntransparency of LLMs and reducing their uncertainty are critical for increasing trust and reliability in their outputs,\nespecially in fields where precision and accountability are paramount.\nThrough extensive discussions with domain experts and developers, and by carefully analyzing the challenges they\nface, we have gained a deep understanding that data augmented LLM applicationsis not a one-size-fits-all solution. The\nreal-world demands, particularly in expert domains, are highly complex and can vary significantly in their relationship\nwith given data and the reasoning difficulties they require. However, developers often do not realize these distinctions\nand end up with a solution full of performance pitfalls (akin to a house with leaks everywhere). In contrast, if we could\nfully understand the demands at different levels and their unique challenges, we could build applications accordingly\nand make the application steadily improve (like constructing a solid and reliable house step by step).\nYet, research efforts and existing relevant surveys [6, 7, 8, 9, 10, 11, 12, 13] frequently focus on only one of these\nlevels or a particular topic of technologies. This has motivated us to compile this comprehensive survey, which aims to\nclearly define these different levels of queries, identify the unique challenges associated with each(Figure 1), and list\nrelated works and efforts addressing them. This survey is intended to help readers construct a bird's-eye view of data\naugmented LLM applications and also serve as a handbook on how to approach the development of such applications\nsystematically."}, {"title": "2 Problem Definition", "content": "Data-augmented LLM applications can take many forms, ranging from the frequently seen Question-Answering bots\nbased on domain-specific data, to semantic processing operators within complex data pipelines, or even agents handling\nspecific steps in a multi-agent system. However, in general, a data-augmented LLM application can be formulated as\nfollows:\n\n f: Q \\times D \\rightarrow A \\qquad (1)\n\nwhere Q, A, and D represent the user's input (Query), the expected response (Answer), and the given data, respectively.\nThe task of the application f is to establish the mapping from Q to A based on D.\nIn contrast to standalone LLM systems that rely solely on pre-existing knowledge, data augmented LLM applications\nare characterized by their reliance on external data (D) to accurately address the posed queries (Q). The incorporation of\nexternal data D can significantly bolster the capabilities of LLMs, granting them the ability to tap into current, domain-\nspecific knowledge and to understand expert rationales. Queries can be stratified into various levels of complexity based\non the extent and manner in which they utilize external data, reflecting the depth and nature of engagement required by\nthe queries."}, {"title": "2.1 Stratification of Queries", "content": "In the landscape of data-augmented LLM applications, queries can be stratified based on their complexity and the depth\nof data interaction required. This stratification helps in understanding the varying levels of cognitive processing that an\nLLM must perform to generate accurate and relevant responses. From straightforward fact retrieval to the nuanced\ninterpretation of implicit knowledge, each level represents a step up in the sophistication of the tasks that LLMs are\nexpected to handle. Below, we delineate these levels, providing insights into the unique challenges and capabilities\nnecessitated at each stage.\nLevel-1 Explicit Facts: These queries are asking about explicit facts directly present in the given data without\nrequiring any additional reasoning. This is the simplest form of query, where the model's task is primarily to\nlocate and extract the relevant information. For example, \"Where will the 2024 Summer Olympics be held?\"\ntargets a fact contained in the external data.\nLevel-2 Implicit Facts: These queries ask about implicit facts in the data, which are not immediately obvious and\nmay require some level of common sense reasoning or basic logical deductions. The necessary information\nmight be spread across multiple segments or require simple inferencing. For instance, the question \"What is\nthe majority party now in the country where Canberra is located?\" can be answered by combining the fact\nthat Canberra is in Australia with the information about the current majority party in Australia.\nLevel-3 Interpretable Rationales: These queries demand not only a grasp of the factual content but also the capacity\nto comprehend and apply domain-specific rationales that are integral to the data's context. These rationales\nare often explicitly provided in external resources and is typically not present or rarely encountered during the\npre-training phase of a general large language model. For example, in the realm of pharmaceuticals, an LLM\nmust interpret FDA Guidance\u00b9 documents\u2014which represent the FDA's current thinking\u2014to evaluate whether\na specific drug application adheres to regulatory requirements. Similarly, in customer support scenarios, the\nLLM must navigate the intricacies of a predefined workflow to process user inquiries effectively. In the medical\nfield, many diagnostic manuals provide authoritative and standardized diagnostic criteria, such as management\nguidelines for patients with acute chest pain [14]. By effectively following these external rationales, it is\npossible to develop a specialized LLM expert system for managing chest pain. This involves understanding\nthe procedural steps and decision trees that guide a support agent's interactions with customers, ensuring\nresponses are not only accurate but also comply with the company's service standards and protocols.\nLevel-4 Hidden Rationales: This category of queries delves into the more challenging realm where the rationales are\nnot explicitly documented but must be inferred from patterns and outcomes observed in external data. The\nhidden rationales here refer not only to the implicit reasoning chains and logical relationships, but also to the\ninherently challenging and non-trivial task of identifying and extracting the external rationales required for\neach specific query. In IT operational scenarios, for example, a cloud operations team may have addressed\nnumerous incidents in the past, each with its own unique set of circumstances and resolutions. The LLM must\nbe adept at mining this rich repository of tacit knowledge to discern the implicit strategies and decision-making"}, {"title": "3 Explicit Fact Queries (L1)", "content": "Explicit fact queries, represent the most straightforward type of data-augmented queries. Queries at this level can be\nanswered by directly accessing specific domain documents or document snippets within the collection. The answers to\nthese questions are often in plain text within the documents, requiring minimal reasoning or simple rationale in the\nresponse generation.\nThe defining characteristic of this level is the clear and direct dependency on specific pieces of external data."}, {"title": "3.1 Overview", "content": "Explicit fact queries, represent the most straightforward type of data-augmented queries. Queries at this level can be\nanswered by directly accessing specific domain documents or document snippets within the collection. The answers to\nthese questions are often in plain text within the documents, requiring minimal reasoning or simple rationale in the\nresponse generation.\nThe defining characteristic of this level is the clear and direct dependency on specific pieces of external data."}, {"title": "3.1.1 Data Dependency", "content": "The dataset D can be segmented into documents or segments, denoted as D\u2081, D\u2082, . . ., Dn, in various ways:\n\nD = {D\u2081, D\u2082, ..., D\u2099}  \\qquad (2)\n\nEach segment D\u1d62 is considered relatively short and contains content that is more focused and specific\u00b2.\nFor a given query q \u2208 Q, not every segment within D is requisite for formulating a response. Let d : Q \u00d7 D \u2192 {0,1}\ndenote the necessity of data segment d \u2208 D for a specific query q, where d(q, d) = 1 means that data segment d is\nrequired to answer the query q, and d(q, d) = 0 otherwise. Then the data dependency of query q, characterized by the\nsubset of segments indispensable for addressing query q, is defined as:\n\nDep(q) = {d | d \u2208 D and d(q, d) = 1} \\qquad (3)\n\nIt's easy to understand that Dep(q) \u2208 P(D), where P(D) is the power set\u00b3 of D."}, {"title": "3.1.2 Definition", "content": "Explicit fact queries, denoted as Q\u2081, are characterized by the direct retrievability of answers from specific data segments\nwithin the dataset D. These queries can be formally defined in the context of a data-augmented LLM system as follows:\nFor any query q and its corresponding answer a, an explicit fact query is one where there exists:\n\u2022 A retrieval component rD : Q \u2192 P(D) that identifies the relevant data segments from D necessary to answer\nq. This component ensures that rD(q) closely matches Dep(q), the minimal subset of D required to respond\nto q.\n\u2022 A response generator \u03b8, typically a prompted LLM inference, that constructs the answer a based solely on the\ninformation retrieved by rD. The response \u03b8(rD(q)) should be equal to or approximate a, demonstrating the\nquery's reliance on explicit, directly accessible facts."}, {"title": "3.2 Challenges and Solutions", "content": "Queries at this level primarily necessitate the correct retrieval of data for LLMs to provide accurate responses. RAG [6],\ndue to its effectiveness, flexibility, and relatively low costs, is the most commonly adopted technical solution for\nhandling this level of queries. However, even with RAG, there are significant challenges in constructing a robust and\nhigh-quality system. These challenges include:\n\u2022 Data Processing Difficulties: External data is often highly unstructured and contains multi-modal components\nsuch as tables, images, videos, and more. Additionally, the process of segmenting or \"chunking\" this data\npresents challenges in maintaining the original context and meaning.\n\u2022 Data Retrieval Difficulties: The retrieval of relevant data segments from a large, unstructured dataset can be\ncomputationally intensive and prone to errors. The challenge lies in developing efficient and accurate retrieval\nmechanisms.\n\u2022 Evaluation Difficulties: Evaluating the performance of a RAG system, particularly at a component level, is\na complex task. It requires the development of robust metrics that can accurately assess the quality of data\nretrieval and response generation.\nGiven the popularity of RAG, a wealth of literature and tools have been developed to address these challenges. In\nthe remainder of this section, we will highlight some of the most practical and impactful enhancements to RAG.\nAdditionally, we will discuss alternative technical solutions that may be employed beyond RAG."}, {"title": "3.3 Retrieval-augmented Generation (RAG)", "content": "Retrieval-Augmented Generation refers to a methodology where a language model augments its natural language\ngeneration capabilities by dynamically retrieving external information during the generation process. This technique\nblends the generative capabilities of LLMs with the information retrieval from extensive databases or documents. The\nprocess is typically implemented as data index construction, retrieval system construction and answer generation."}, {"title": "3.3.1 Data Processing Enhancement", "content": "Document parsing at this level often involves extracting information from text, tables, and figures in a coherent manner,\nensuring that the relevant snippets are accurately identified and retrieved.\nMulti-modal Documents Parsing Addressing multi-modal content in source documents, such as charts, tables, or\neven videos (e.g. meeting recordings), is one of the most frequently asked questions. Broadly, two approaches are\nemployed to tackle this issue. The first approach involves converting multi-modal content into textual form. For instance,\nTable-to-Text methods [34] translate tables into text, while other techniques convert visual content into textual or\nattribute-based descriptions [35, 36], which are subsequently processed by large language models. The second approach\nleverages multi-modal embedding techniques [37, 38, 39], utilizing the retrieved embeddings from multi-modal data as\nsoft prompts for input.\nChunking Optimization For long texts, segmenting documents into text chunks is a common and necessary operation.\nLarger text chunks can preserve more of the semantic coherence of the context, but they also tend to contain more noise\nwithin each chunk[40]. Commonly-used chunking strategies [41, 42] include fixed size chunking, recursive chunking,\nsliding window chunking, paragraph-based chunking,semantic chunking, etc. Certain methods are designed to ascertain\nthe level of detail a query demands and, based on this identification, select text chunks of appropriate granularity for\nretrieval[43, 44]. Alternatively, some methods opt to process and refine the text into smaller segments that maintain a\nhigh degree of information completeness[45]. Additionally, there are approaches that employ vision models to segment\ntext in accordance with the original document structure[46]."}, {"title": "3.3.2 Data Retrieval Enhancement", "content": "Information Retrieval (IR) techniques can be smoothly transferred into RAG applicaitons. The primary steps involved\ninclude establishing data indexes, processing queries, retrieving and matching, re-ranking, and evaluation.\nIndexing The purpose of this step is to establish mappings from search terms to text segments, determining the logic by\nwhich the retrieval system operates. Indexing methods are broadly classified into three types: sparse, dense, and hybrid\nretrieval. Sparse retrieval uses specific words to index text segments. In contrast, dense retrieval maps text segments\ninto a dense vector space of features. Hybrid retrieval combines elements of both sparse and dense techniques.\n\u2022 Sparse Retrieval: This was the first indexing method to be widely adopted due to its simplicity and intuitiveness.\nTechniques like TF-IDF and BM25[47, 48] are designed to identify the most representative keywords of each\ntext segment based on their relative frequency. These methods are still prevalent in many RAG projects [49,\n50, 51]. However, word matching methods can lead to retrieval losses due to their inability to recognize\nsynonyms. To address this issue, methods like KNN can be used for similarity-based matching of keywords\n[52]. Alternatively, indices like keywords can be changed into the prediction of the probabilities of query\ntokens for the corresponding text segment[53, 54].\n\u2022 Dense Retrieval: This approach often involves using pre-trained or fine-tuned text encoders to map texts to\na dense vector space that aligns with query requirements. BERT-based encoders [55] are commonly to be\nfine-tuned as dense retriever on unsupervised data using methods such as DPR[56], ANCE[57], SimCSE[58]\nand TAS-B[59]. Others employ unsupervised contrastive learning for fine-tuning, such as Contriever[60].\nUsing feedback from LLMs to guide the training objectives of retrievers can also effectively enhance the\nretriever's suitability for LLMs [61, 62, 63]. Given the powerful capabilities and expressive potential of\nLLMs, LLM-based dense retrieval has recently emerged as a key area of interest and exploration [64].\nLLM2vec [65] modifies the attention mechanism of a pre-trained LLM to a bidirectional one and employs the\nmasked next-token prediction method for unsupervised training, resulting in an LLM-based dense retrieval\nembedder. Similarly, Llama2Vec [66] leverages two pretext tasks\u2014Embedding-Based Auto-Encoding and\nEmbedding-Based Auto-Regression\u2014to train an unsupervised dense retrieval encoder based on the LLaMA\narchitecture [67], leading to significant improvements in retrieval task performance.\n\u2022 Others: Combining sparse retrieval and dense retrieval is an effective method to focus simultaneously on\nthe central theme of text segments and global features. Feng et al. (2023) propose initially determining the\nknowledge domain needed to answer a query as a fixed area of expertise, and then using dense retrieval to\nrecall supplementary information within this domain [68]. Numerous studies have explored various methods\nof blending dense vector indexing with sparse encoder indexing to better capture the semantic information of\ntext blocks and enhance the precision of targeted paragraph retrieval [69, 70, 71]. On the other hand, Tang et\nal. (2024) have enhanced the capabilities of a LLM by fine-tuning it for indexing and retrieving, effectively\nintegrating these abilities directly into the LLM. This allows the LLM to autonomously generate data indices\nand text segments for each query [72, 73]."}, {"title": "Query Document Alignment", "content": "The goal of this step is to align the query with document segments in external data to\nidentify the best document segment that can assist in answering the query. As Figure 3 illustrated, there are primarily\nthree approaches to this alignment: traditional alignment, document domain alignment, and query domain alignment.\nTraditional alignment involves mapping both document segments and the query into the same encoding space. For\ninstance, many dense retrieval architectures based on dual encoders feature specialized query encoders [56, 57, 59].\nConversely, if a system like RAG employs sparse retrieval, it is necessary to extract keywords from the query for the\nsearch. Further refinement can be achieved through query rewriting techniques, which enhance search accuracy by\nmitigating issues related to user terminological inaccuracies or vague descriptions, effectively improving the precision\nof the search results [74]. Document domain alignment involves generating synthetic answers first, then using these\nanswers to recall relevant data, effectively addressing the issue of queries and retrieved data not being in the same\ndistribution space. A notable work in this area is HyDE [75]. Query domain alignment [76] involves generating a set of\nsynthetic questions for each atomic unit of text, mapping text segments into the query space, and then retrieving the\nsynthetic questions closest to the original query along with their corresponding text segments. This method ensures\nthat the most relevant and contextually appropriate segments are selected for responding to the query. SlimPLM [77]\nemploys a small proxy model to generate heuristic answers, which are then used to predict the knowledge needed to\nanswer the question. This approach also provides an effective method for aligning queries to the document space."}, {"title": "Re-ranking and Correction", "content": "After retrieving the top k text blocks, RAG systems must filter and reorder these segments.\nMost RAG systems use the relevance scores provided by the retriever as the basis for ranking, while some studies\nemploy specific metrics such as perplexity or perplexity gain as ranking criteria [78, 79]. Other efforts involve using\nLLMs to evaluate the credibility and utility of retrieved text blocks, training a pluggable reward-driven contextual\nadapter to refine the output of retriever[80]. Additionally, some research focuses on pre-training a small language model\ndedicated to fact verification, which is used to filter out incorrect retrieved text chunks, thus improving the quality of\nthe recalled text[81]."}, {"title": "Recursive Retrieval or Iterative Retrieval", "content": "Considering the inherent limitations in the accuracy of a single retrieval\nattempt, an effective mitigation strategy is to perform multiple retrievals to progressively address any omissions. Kim et\nal. (2023) introduced a tree-like recursive retrieval method, incorporating pruning strategies to incrementally break\ndown ambiguous questions into disambiguated ones, ultimately arriving at the closest correct answer [82]. Similarly,\nSEATER uses the k-means algorithm to construct a hierarchical tree structure of items to be retrieved, and iteratively\nrecalls nodes within the tree structure [83]."}, {"title": "3.3.3 Response Generation Enhancement", "content": "Generating responses requires determining if the retrieved information is sufficient or if additional external data is\nneeded. Handling conflicts between retrieved knowledge and the model's internal prior knowledge is also essential\n[84, 85, 86]. Supervised fine-tuning is an effective method to enhance the generation performance in RAG systems.\nWhen faced with irrelevant or erroneous information as the retrieved context, pre-trained large language models are\noften easily misled, resulting in incorrect responses. Many studies have shown that by subtly designing training data\nfor RAG systems, fine-tuning or pretraining can effectively mitigate this issue [87, 88, 89]. Through experimental\nanalysis, RAAT [89], demonstrated that the detrimental effects of irrelevant retrieval noise, relevant retrieval noise, and\ncounterfactual retrieval noise on RAG models increase progressively. By incorporating with these training process,\nthese methods enables the LLM to internally recognize noisy contexts, leading to significant improvements in response\ngeneration quality even in the presence of noisy retrievals. Furthermore, to ensure more consistent performance between\nthe retriever and generator within the RAG system, some studies employ joint training of both retriever and generator\nduring the training phase [90, 91, 92]."}, {"title": "4 Implicit Fact Queries (L2)", "content": "These queries involve data dependencies that are not immediately obvious and may require some level of common\nsense reasoning or basic logical deductions. The necessary information might be spread across multiple segments or\nrequire simple inferencing. (Example in Figure 2)\nQueries at this level require gathering and processing information from multiple documents within the collection. The\ncollection of required information may exceed the ability of a single retrieval request, necessitating the decomposition"}, {"title": "4.1 Overview", "content": "These queries involve data dependencies that are not immediately obvious and may require some level of common\nsense reasoning or basic logical deductions. The necessary information might be spread across multiple segments or\nrequire simple inferencing. (Example in Figure 2)\nQueries at this level require gathering and processing information from multiple documents within the collection. The\ncollection of required information may exceed the ability of a single retrieval request, necessitating the decomposition"}, {"title": "4.2 Challenges and Solutions", "content": "At this level, queries still revolve around factual questions, but the answers are not explicitly presented in any single text\npassage. Instead, they require combining multiple facts through common-sense reasoning to arrive at a conclusion. The\nchallenges of a level-2 query primarily include:\n\u2022 Adaptive retrieval volumes: Different questions may require varying numbers of retrieved contexts, and the\nspecific number of retrieved contexts can depend on both the question and the dataset. A fixed number of\nretrievals may result in either information noise or insufficient information.\n\u2022 Coordination between reasoning and retrieval: Reasoning can guide the focus of what needs to be retrieved,\nwhile the insights gained from retrieved information can iteratively refine reasoning strategies. Addressing\nthese complexities calls for an intelligent integration and selective harnessing of external data, capitalizing on\nthe inherent reasoning prowess of LLMs.\nMethods to address challenges at this level include iterative RAG, RAG on graph/tree, and RAG with SQL, among\nothers."}, {"title": "4.3 Iterative RAG", "content": "Implicit fact queries is similar to multi-hop RAG tasks. This category of methods dynamically controls multi-step RAG\nprocesses, iteratively gathering or correcting information until the correct answer is achieved.\n\u2022 Planning-based: Generating a stepwise retrieval plan during the prior-retrieval stage or dynamically within\nthe retrieval process can refine the focus of each retrieval, efficiently guiding the iterative RAG system. For\nexample, ReAct [93] progressively updates the target of each step, reducing the knowledge gap required\nto answer the question. IRCOT [94] and RAT[95] uses a Chain of Thought to guide the RAG pipeline,\nmaking decisions about the current retrieval target based on previously recalled information. GenGround [96]\nenables LLMs to alternate between two stages until arriving at the final answer: (1) generating a simpler\nsingle-step question and producing a direct answer, and (2) tracing the question-answer pair back to the\nretrieved documents to verify and correct any inaccuracies in the predictions. This iterative process ensures\nmore reliable and accurate responses.\n\u2022 Information Gap Filling Based: ITRG [97] introduces an iterative retrieval-generation collaboration framework,\ngenerating answers based on existing knowledge and then continuing to retrieve and generate for the unknown\nparts of the response in subsequent rounds. Similarly, FLARE [50] revisits and modifies low-probability\ntokens in answers generated in each iteration. On the other hand, Self-RAG [92] fine-tunes a large model to\nautonomously decide when to search and when to stop searching and start answering questions."}, {"title": "4.4 Graph/Tree Question Answering", "content": "Addressing implicit fact queriesrequires synthesizing information from multiple references. Graphs or trees, whether\nknowledge-based or data-structured, naturally express the relational structure among texts, making them highly suitable\nfor this type of data retrieval problem.\n\u2022 Traditional Knowledge Graph: One of the initial structures considered for enhancing the efficacy of LLMs\nis the traditional knowledge graph, where each node represents an entity and edges between nodes signify the\nrelationships between these entities.\n[98] proposed a forward-looking development roadmap for LLMs and Knowledge Graphs (KGs) comprising:\n1) KG-enhanced LLMs, which integrate KGs during the pre-training and inference phases of LLMs to deepen\nthe models' understanding of acquired knowledge; 2) LLM-enhanced KGs, which employ LLMs for various\nKG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and\n3) collaborative LLMs+KGs approaches, where both LLMs and KGs play complementary roles, enhancing\neach other through bidirectional inference driven by data and knowledge. The Rigel-KQGA model [99], is an\nend-to-end KGQA model that predicts the necessary knowledge graph nodes based on a query and combines\nthis with an LLM to derive answers. Works like Think-on-Graph [100] and KnowledgeNavigator [101]\nextract entities involved in a query and then perform iterative BFS searches on the graph, using the LLM as a\nthinking machine to determine the optimal exploration path and perform pruning. The R\u00b3 [102]introduces\nseveral possible commonsense axioms via an LLM that could address a query, sequentially searching related\nknowledge subgraphs to assess if the current information suffices to answer the query, continuing until the\nquestion is resolved.\n\u2022 Data Chunk Graph/ Tree: The impressive reading comprehension capabilities of LLMs enable them to\neffectively grasp text without needing to break it down into the finest granularities of entities and relationships.\nIn this context, researchers have begun experimenting with using text chunks or data chunks as nodes\non graphs or trees, employing edges to represent either high-level or more intricately designed relations.\nKnowledge-Graph-Prompting [103] discusses three popular kinds of questions that require mining implicit\nfacts from (a) bridging questions rely on sequential reasoning while (b) comparing questions rely on parallel\nreasoning over different passages. (c) structural questions rely on fetching contents in the corresponding\ndocument structures. To tackle these questions, Knowledge-Graph-Prompting utilizes entity recognition,\nTF-IDF, KNN, and document structure hierarchies to construct document graphs and extract subgraphs for\nanswering questions. MoGG [44] treats one or two sentences as the smallest semantic units, using these as\nnodes and building edges based on semantic similarity between nodes. It also trains a predictor to determine\nthe textual granularity required for answering a query by deciding how large a sub-graph is needed. To capture\nmore high-level semantic relationships between text blocks, RAPTOR [43], employs clustering algorithms\nto hierarchically cluster the finest granularity of text blocks. It summarizes new semantic information at\neach hierarchical level, recalling the most necessary information within a collapsed tree of nodes. Similarly,\nGraphRAG [104], adopts a clustering approach. It initially connects the smallest text blocks based on semantic\nsimilarity, then uses community detection algorithms to group nodes. Finally, it summarizes the global answer\nto a query by analyzing responses within each node community."}, {"title": "4.5 Natural Language to SQL Queries", "content": "When dealing with structured data, converting natural language queries to SQL (NL2SQL) can be an effective approach.\nTools like Chat2DB facilitate this process by translating user queries into database queries. In the era of large language\nmodels, there has been significant progress in the area of text-to-SQL[105, 106, 107, 108], which allows us to utilize\nthese tools to retrieve information from structured databases. This capability serves as a valuable external data source to\naugment the generation capabilities of LLMs. By integrating text-to-SQL tools [109], LLMs can access and incorporate\nstructured data, enhancing their ability to generate more accurate and contextually relevant responses. This integration\nnot only improves the depth and quality of the generated content but also expands the scope of LLM applications,\nenabling them to perform more complex tasks that require interaction with and interpretation of database content."}, {"title": "4.6 Discussion on Fact Queries", "content": "Whether to Use Fine-tuning. Some works [110] have demonstrated the hardness of LLMs to acquire new factual\nknowledge during fine tuning. This process can lead to a deterioration in the overall performance of the LLMs in\ngenerating accurate responses, and it often results in the generation of more hallucinations. Furthermore, study [111]\nsuggests that fine-tuning LLMs with new factual data may cause the models to mechanically memorize fact statements.\nInterestingly, altering the phrasing of these memorized facts can render the recently learned knowledge ineffective,\nindicating a superficial level of understanding and retention by the LLMs. This points to limitations in the current\nfine-tuning processes and the need for more sophisticated methods to integrate and adapt new information effectively.\nWhether to Separate Different Levels of Fact Queries. Both explicit fact queriesand implicit fact queriesare fact-based,\nand it is crucial to determine which level these queries belong before constructing data augmented LLM applications.\nMisclassifying explicit fact queriesas implicit fact queriescan lead to the retrieval of an abundance of superficial\ninformation that is seemingly relevant but ultimately unhelpful for answering the question, which can mislead the LLM\nand waste computational resources. Conversely, mistaking implicit fact queriesfor explicit fact queriescan prevent\nthe use of appropriate methods to retrieve a sufficient and comprehensive set of external auxiliary data. Implicit fact\nqueries often require dynamically integrating information specific to the context of the queries, whereas explicit fact\nqueriesgenerally need only a single data snippet, leading to the retrieval of a fixed amount of external data. This can\nresult in suboptimal performance of the LLM. Therefore, it is advantageous to preliminarily distinguish the level of\nqueries based on a thorough understanding of the target task. Additionally, considerable effort has been directed towards\ntraining models to autonomously assess whether the information retrieved is sufficient, exemplified by approaches such\nas self-RAG [92]."}, {"title": "5 Interpretable Rationale Queries (L3)", "content": "In this section and the next", "involved": "queries based on interpretable rationales and those based on hidden"}]}