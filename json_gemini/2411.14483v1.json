{"title": "Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat", "authors": ["Roland Daynauth", "Christopher Clarke", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "abstract": "Deciding which large language model (LLM) to use is a complex challenge. Pairwise ranking has emerged as a new method for evaluating human preferences for LLMs. This approach entails humans evaluating pairs of model outputs based on a predefined criterion. By collecting these comparisons, a ranking can be constructed using methods such as Elo. However, applying these algorithms as constructed in the context of LLM evaluation introduces several challenges. In this paper, we explore the effectiveness of ranking systems for head-to-head comparisons of LLMs. We formally define a set of fundamental principles for effective ranking and conduct a series of extensive evaluations on the robustness of several ranking algorithms in the context of LLMs. Our analysis uncovers key insights into the factors that affect ranking accuracy and efficiency, offering guidelines for selecting the most appropriate methods based on specific evaluation contexts and resource constraints.", "sections": [{"title": "1 Introduction", "content": "The adoption of Large Language Models (LLMs) has surged in recent years, revolutionizing natural language processing (NLP) tasks across various domains. A critical question arises when practitioners use these models \"Which LLM is most suitable for a particular task or use case?\" Given the plethora of LLMs available today, each with their own unique strengths and weaknesses, this decision-making process has become increasingly complex (Jiang et al., 2023).\nHistorically, NLP benchmarks such as GLUE (Wang et al., 2019), SuperGLUE (Wang et al., 2020), and more recently LLM-centered benchmarks such as LM-Eval (Gao et al., 2023) have aimed to answer this question by evaluating these models on a series of challenging tasks that assess their natural language understanding (NLU) capabilities. However, a major limitation of these classical benchmarks is that they often fail to capture and evaluate more nuanced, diverse, and complex cases where establishing a definitive ground truth is challenging and, in many cases, unattainable (Chiang et al., 2024). Consider the task of rating competing model responses to a given prompt. These standard benchmarks often overlook the intricacies of assessing language generation, where nuances and qualitative factors play an important role. This problem is particularly evident in tasks such as open-ended text generation, conversational dialogue, and creative writing, where alignment with human judgment is paramount. Although a given model may score highly on these classical benchmarks, their output often does not align with human preference. (Zheng et al., 2024; Irugalbandara et al., 2023).\nTo better align with human preference, open benchmarking platforms such as Chatbot Arena (Chiang et al., 2024) evaluate models through \"head-to-head combat,\" where models are judged and ranked individually inspired by the competitive dynamics of sports leagues. This approach assesses the relative strengths of LLMs through confrontations, enabling a more accurate comparison between them. This pairwise comparison between models naturally aligns with the Elo rating system (Elo, 1978), originally developed to classify chess players according to their relative strengths. When evaluating the performance of the LLMs in pairwise competition, Elo and ranking methods alike have gained popularity as structured methods to incorporate human feedback and preferences to effectively rank models (Dettmers et al., 2024; Wu and Aji, 2023; Chiang et al., 2023). However, despite its recent prominence in LLM evaluation, Elo can produce inconsistent and sometimes unreliable results (Boubdir et al., 2023). Elo was designed for domains with stable and well-defined results, such as chess. LLMs, in contrast, are different as their performance can be multifaceted and context-dependent, making Elo less than ideal for capturing the full scope of their capabilities. This added complexity raises several important considerations when determining the ranking system for model selection. Choosing the best model can have significant implications for the user experience of the application that incorporates it.\nThis paper explores the critical issues around ranking LLMs in head-to-head combat. We first outline a set of fundamental properties that ranking systems should adhere to when evaluating LLMs. Second, we evaluate the performance of four widely used ranking algorithms (Elo, Bradley-Terry, Glicko, and Markov Chain) in two different evaluation scenarios (Arena Style vs. Controlled Style), analyzing their ability to maintain transitivity, ensure prediction accuracy, and remain stable across hyperparameter variations. Lastly, we conduct an in-depth analysis of these results, presenting a series of insights into the factors influencing the performance of different ranking methodologies and propose a set of guidelines for selecting the most appropriate approach based on the specific characteristics of the evaluation task and available resources. To the best of our knowledge, this paper represents the first work that systematically addresses the complexities and intricacies of ranking LLMs across various methodologies. We release all code, data, and models to facilitate reproducibility and further research in this area\u00b9."}, {"title": "2 Background", "content": "In this section, we formally define the task of pairwise ranking and introduce the suite of ranking algorithms considered in our work."}, {"title": "2.1 Ranking", "content": "Let M be the set of competition models where M = {1, 2,\u2026\u2026, m}. For any two models i and j in M, the result Sij of a head-to-head match between i and j is defined as Sij = 1 when i beats j and Sij = 0 when i loses and Sij = 0.5 for ties. Let pij be the probability that the model i beats j in a head-to-head competition. The model i is ranked higher than j if pij > 0.5.\nLet \\(\\theta_i \\in \\mathbb{R}\\) represent the strength of a model i \u2208 M, then it is generally assumed that pij follows a probability distribution F (David, 1988), such that:\n$$p_{ij} = F(\\theta_i - \\theta_j)$$\nIf F follows a logistic distribution, then the system is known as the Bradley-Terry model (Bradley and Terry, 1952) and\n$$p_{ij} = \\frac{1}{1+ e^{-(\\theta_i-\\theta_j)}}$$"}, {"title": "2.2 Elo Rating system", "content": "The Elo rating system was first adopted by the United States Chess Federation (USCF) in 1960 to measure players' strengths (Elo, 1978). For two players i and j with relative strengths \\(\\theta_i\\) and \\(\\theta_j\\), the probability that i beats j, is calculated as follows:\n$$p_{ij} = 1/(1 + 10^{(\\theta_j-\\theta_i)/400})$$\nThe parameter \\(\\theta_i\\) for the model i is also known as the Elo score, updated based on two factors: the outcome of the game Sij and the opponent's Elo score \\(\\theta_j\\). The score of a player is updated at the end of every match or tournament. In the context of LLM evaluation, we consider a single evaluation run as a tournament and update the score of a model after every head-to-head match. For any match between model i and j, the Elo score \\(\\theta_i\\) is updated by the function:\n$$\\theta_i = \\theta_i + k \\times (S_{ij} - p_{ij})$$"}, {"title": "2.3 Bradley-Terry Model", "content": "The Bradley-Terry model (Bradley and Terry, 1952) was introduced in 1952 as a probability model to predict the result of a paired comparison. The strengths \\(\\theta_i\\) for each model can be estimated by solving the likelihood function L(\u03b8) such that:\n$$L(\\theta) = \\prod_{i<j} p_{ij}^{Y_{ij}} (1-p_{ij})^{Y_{ji}}$$\nwhere yij is the number of times that model i beats j and pij is the probability of i beating j. This process is optimized by the maximum likelihood of log L, i.e, solving log L(\u03b8) = \u2211i>j Yij log(pij) + (1 \u2212 Yij) log(1 \u2013 pij)."}, {"title": "2.4 Glicko Rating system", "content": "The Glicko rating system (Glickman, 1999) was introduced in 1995 as an improvement to Elo. Glicko extends Elo by introducing a secondary parameter (\u03c3) known as the rating deviation. This parameter measures the reliability of a player's rating score, acting as a confidence interval for the player's rank, decreasing based on the number of games played, regardless of wins or losses. This factor is important given that two players can potentially have the same score in a tournament despite a different number of previous matches. This situation is prevalent in LLM evaluations, as new models are frequently introduced and the number of games played between them vary significantly.\nFor model i \u2208 M, the rating and the rating deviation are updated as:\n$$\\theta_i = \\theta_i + \\frac{q}{\\sqrt{\\sigma_i^{-2} + d^2}} \\sum_{j=1}^{J} g(\\sigma_j) (S_{ij} - p_{ij})$$\n$$\\sigma_i' = \\sqrt{({\\sigma_i^{-2} + d^2})^{-1}}$$\nwhere q = log(10)/400, J is the set of opponents for i and g(\u03c3) = \\(1/\\sqrt{1 + 3q^2\\sigma^2/\\pi^2}\\).\nGiven the rating \\(\\theta_i\\) and \\(\\theta_j\\) of model i and j, the expected outcome pij is calculated as\u00b2:\n$$p_{ij} = \\frac{1}{1 + 10^{-g(\\sigma_j)(\\theta_i-\\theta_j)/400}}$$\n$$d^{-2} = q^2 \\sum_{j=1}^{J} g(\\sigma_j)^2 p_{ij}(1-p_{ij})$$\nwhere d\u00b2 is the scaling factor for the system's certainty."}, {"title": "2.5 Markov Chain", "content": "The Markov Chain Model is a nonparametric ranking algorithm adapted from Callaghan et al. (2003, 2004) and adapted into a Markov Chain Model by (Kvam and Sokol, 2006). The Markov chain employs a series of random walkers traversing a graph, with each node representing a player/model in the tournament connected by an edge denoting a match between two nodes. Each walker chooses an initial node i, staying in the node or moving to an adjacent node j according to a single condition p. In a random match between i and j, the walker moves to the winning node with a probability of p for p > 0.5, a hyperparameter chosen in advance. Every time a walker moves or stays on a node, it represents a vote for that node, and the sum of all votes from every walker represents the rank of that player/model. In practice, the optimal value for p can vary depending on the tournament. In our experiments, we use p = 0.8 as the default value similar to Callaghan et al. (2003), which achieved stable results with p ranging from p = 0.75 to p = 0.95. When expressed as a Markov chain, the elements Tij of the transition matrix T are the probabilities that a walker moves from model i to j in a single time step, with the initial values computed as follows:\n$$t_{ij} = \\frac{1}{N_i}[W_{ij}(1 - p) + l_{ij}p], \\forall j \\neq i$$\n$$t_{ii} = \\frac{1}{N_i}[W_{ip} + L_i(1 - p)]$$\nwhere \\(W_{i}, L_{i}\\) are the number of matches won by model i and \\(w_{ij}, l_{ij}\\) are the number of games won by model i against model j.\nThe steady-state probability vector \u03c0 = [\\(\\theta_i\\)] represents the final ranking of each team i \u2208 M such that \u03c0\u1d40 = \u03c0 and \\(\\sum_{i=1}^{m} \\theta_i = 1\\)."}, {"title": "3 Desirable Properties of Ranking System", "content": "The fundamental goal of a ranking system is to provide a structured way to compare and order items based on their performance or quality. In the context of LLMs, an effective ranking system allows researchers, practitioners, and end-users to identify the most capable models for their specific needs. It simplifies decision making and optimizes resource allocation by highlighting models that are likely to deliver superior performance in real-world applications. However, employing such involves addressing several key properties to ensure its validity and reliability. In this section, we identify and elaborate on three essential properties that a ranking system for LLMs should adhere to: 1) Transitivity, 2) Prediction Accuracy, and 3) Sensitivity to Hyperparameters and Battle Conditions."}, {"title": "3.1 Transitivity", "content": "Transitivity is a fundamental property for any ranking system to maintain consistency. Transitivity is defined as follows:\nDefinition 3.1. For any three models i, j, k \u2208 M; their match-ups are transitive if pij > 0.5, pjk > 0.5 and pik > 0.5. If any three models i, j, k are transitive by definition 3.1, we say that a ranking system preserves transitivity if it ranks these models in order i, j, k. Transitivity ensures that the ranking is coherent and interpretable.\nIn the context of LLMs, transitivity guarantees that the relative strengths of models are accurately reflected across various pairwise comparisons. When utilized by researchers and practitioners, the general assumption of a model's placement on a leaderboard such as the LMSYS Chatbot Arena Leaderboard (Chiang et al., 2024) indicates its overall strength compared to its counterparts. This means that a lack of transitivity can lead to paradoxical rankings, where the ordering of models becomes circular or inconsistent, thereby diminishing the trust in the ranking system."}, {"title": "3.2 Prediction Accuracy", "content": "Like transitivity, prediction accuracy is a critical metric that assesses how well the ranking system forecasts the results in head-to-head matches. It measures the probability that the ranking system correctly predicts the winner between any two LLMs. High prediction accuracy indicates that the ranking system is reliable and can be trusted to make informed decisions about the superiority of the model for a downstream task. For combat-style evaluation in LLM, where assessment is based on subjective human judgment, prediction accuracy acts as an indicator of the ranking system's ability to align with human preferences. This alignment is crucial for tasks like open-ended text generation or conversational agents, where the model's performance is best judged by human preference. Thus, the ranking system must reflect statistical superiority and resonate with qualitative human evaluations to truly enhance the utility and adoption of LLM."}, {"title": "3.3 Sensitivity to Hyperparameters and Battle Conditions", "content": "Another critical aspect of evaluating ranking algorithms is understanding their sensitivity to hyperparameters, which can significantly affect the consistency and stability of rankings (Boubdir et al., 2023). Ranking algorithms such as Elo have been shown to produce drastically different rankings based on the order in which matches are evaluated and the number of permutations (Boubdir et al., 2023). Since these hyperparameters can influence the behavior and outcomes of the ranking system, it is essential to choose the right settings to ensure reliable and consistent rankings. Sensitivity to hyperparameters means that the ranking system should respond to changes in these settings in a predictable and manageable way. An overly sensitive system might produce drastically different rankings with minor parameter changes, leading to instability and unreliability. In contrast, a system with too little sensitivity might fail to capture essential differences between models. In this work, we consider the impact of two different classes of hyperparameters/conditions: 1) Algorithmic Hyperparameters: parameters native to the ranking algorithm itself and 2) Matchup Distribution: Arena style vs. controlled style.\nAlgorithmic Hyperparameters such as k-factor in Elo, p-value in Markov Chain, and \u03c3 in Glicko can significantly affect the rankings produced by these algorithms. Similarly, the distribution of matchups between models can also influence the rankings. In practice, certain models may participate in many more matchups than others, creating an uneven distribution of data. This skews the rankings because models with fewer matches do not have sufficient data to be ranked reliably, while those with more matchups dominate the rankings. The ranking system chosen should be robust to these variations and produce consistent and reliable rankings across different hyperparameters and battle conditions."}, {"title": "4 Evaluation", "content": "Building on the properties discussed in section 3, this section outlines a series of experiments designed to assess the quality of traditional ranking systems and their use in LLM rankings. We investigate how each algorithm performs under different conditions and evaluate their effectiveness in producing stable and consistent rankings."}, {"title": "4.1 Datasets", "content": "To evaluate these ranking systems, we use two diverse human evaluation datasets representative of two evaluation styles (Arena Style vs Controlled Style):\n\u2022 Chatbot Arena (Chiang et al., 2024) is a dynamic platform where users evaluate two randomly selected model outputs head-to-head. As newer models are released, they are added to the arena to battle the older models. This data set consists of 57 models and a total of 244,978 match-ups. The distribution of matches varies wildly, with some models having up to 30416 matches or as few as 954. As such, we refer to this dataset as an arena-style dataset.\n\u2022 SLAM (Irugalbandara et al., 2023) consists of pairwise match-ups between models evaluated using the SLAM tool on domain-specific task questions. Unlike Chatbot Arena, the SLAM platform is tightly controlled with a fixed number of models and a similar number of match-ups between models, thus making it a controlled style dataset. The SLAM dataset consists of 11 models with a total of 2858 match-ups, each model having at most 529 match-ups and at least 501. As such, we refer to this dataset as a controlled style dataset.\nThese two diverse datasets allow us to evaluate the ranking algorithms under different conditions, providing a comprehensive perspective on their behavior."}, {"title": "4.2 Measuring Transitivity on Human Feedback Data", "content": "Using the Chatbot Arena and SLAM datasets, we evaluated the ability of each ranking algorithm to preserve transitivity. Recall that transitivity as defined in 3.1 is the property that if i is preferred to j and j is preferred to k, then i should be preferred to k. We evaluated the transitive performance of each ranking algorithm by first calculating the number of triples within the data set. Three models i, j, k are part of a triple if i wins the majority of comparisons with j, and j wins the majority of comparisons with k, and transitivity is considered preserved if this order is maintained in the final ranking."}, {"title": "4.3 Prediction Accuracy", "content": "Next, we evaluate the ability of each ranking algorithm to predict the outcome of unseen LLM matches. We first partition each dataset into a 75/25 train/test split, running each ranking algorithm on the training set to determine the final ranking and strength of each model \u03c0. We then use each ranking algorithm to predict the outcome of the matches in the test set and calculate the F1 score as our performance metric."}, {"title": "4.3.1 Performance on the Arena Dataset", "content": "Due to its uneven distribution of model match-ups, the Arena data set presents an interesting challenge for ranking algorithms to predict future outcomes. When matches between models are unevenly distributed, some classification algorithms struggle with prediction accuracy. In Figure 4, our results show the Elo model as the most accurate, achieving an F1 score of 0.90 compared to the F1 score of 0.88, 0.82, and 0.77 for the models Glicko, Bradley-Terry, and Markov Chain, respectively.\nThe impact of the match-up distribution on the prediction accuracy is shown in the performance of the Markov chain which is affected by the sparsity of certain model match-ups, leading to lower prediction accuracy as shown by the poor performance on models such as deepseek-llm-67b-chat and dolphin-2.2.1-mistral-7b in figure 4 of the appendix.\nAnother example of the impact of matchup distribution is shown in the performance of the Bradley-Terry model in appendix figure 4. We observe that the Bradley-Terry system suffers when dealing with \"powerful models,\" where there is a strong imbalance between a model's wins and losses. For example, in the arena dataset, gpt-4-turbo had a win/loss ratio of 12288/3979. This skew in model wins/loss results in overestimation of the model's strength as shown by the low F1 score of 0.82 achieved by gpt-4-turbo using Bradley-Terry compared to an F1 score of 0.95, 0.96 and 0.92 from Elo, Glicko, and Markov Chain, respectively. This is known as the 'rare events' problem in logistic regression (King and Zeng, 2001). One method of countering this is to use a weighted logistic regression(Chiang et al., 2024). We observe however, that this has a negligible impact on the final result."}, {"title": "4.3.2 Prediction accuracy on the SLAM dataset", "content": "The distribution of matches between models in the SLAM dataset is tightly controlled so that every reviewer evaluates an equal number of models, resulting in a near uniform distribution of matches. We observe near-identical performance for all models. This result is due to the balanced distribution of match-ups in the dataset, where if a model has the most wins in the competition, its performance is directly reflected in its win rate. We further validate this occurrence by ranking all the models in SLAM by win rate and calculating the correlation between each pair of ranking algorithms as shown in Table 3. We observe a high correlation between all methods and the win rate."}, {"title": "4.4 Hyperparameter Sensitivity", "content": "To measure the sensitivity of the hyperparameters, we calculate the F1 score of each ranking system as described in Section 4.3, varying each hyperparameter over a range of 100 values. We then plot the distribution of F1 scores for each ranking algorithm across the two datasets. Figure 2 showcases the range of results produced by Elo, Markov and Glicko across the two evaluation scenarios."}, {"title": "4.4.1 Elo k-factor", "content": "The k factor in Elo dictates the extent to which the score of a player is influenced by the outcome of an individual match. For example, with large k values, highly ranked players can incur a significant penalty when losing to a player with a lower ranking, while defeating such a player only results in a slight increase in score. In sports games, the strength of players fluctuates over time, influenced by their experience and age; on the other hand, in Ilm evaluation the models tend to remain unchanged during their lifespan.3 The evaluation of the performance of the model (depicted in Figure 2) at varying levels of hyperparameter k shows marked sensitivity, which emphasizes the need for meticulous tuning. Elo shows the largest variation across both datasets, and is less stable, particularly for the smaller SLAM dataset. Through our tuning of this parameter, we determined that the ideal K factor for Elo is less than the standard 32 commonly used in previous research (Dettmers et al., 2024; Wu and Aji, 2023; Chiang et al., 2023). This reduced convergence rate can be attributed to the complexities of human preferences as opposed to the typical assessment of real-world games, which rely on the differing abilities of opposing teams."}, {"title": "4.4.2 Markov Chain: p Score", "content": "When using Markov chain, in a random match between i and j, the walker moves to the winning node with a probability of p for p > 0.5, a hyperparameter chosen in advance. Hence, if we set a value for p closer to 0.5, then models with lower win rates may outrank those with higher win rates. However, for the Arena data set, choosing a higher p value will cause the algorithm to prioritize the win rate over the number of games played; for example, a model with 1000 games with a high win rate may be ranked higher than a model with 30,000 games and a slightly lower win rate. We observe that Markov shows stable performance on the controlled SLAM datasets, performing as well as Glicko (Figure 2), however, the algorithm struggles on the large ARENA dataset due to its over reliance on win-rates for calculating ranks. When optimizing the value of this parameter, we find that the optimal p value for the Markov chain on the SLAM dataset is larger than the Arena dataset, indicating that the model's strength is greatly influenced by its overall win rate."}, {"title": "4.4.3 Glicko: Rating deviation", "content": "For Glicko, the ranking deviation (\u03c3) is a measure of consistency or variability in a player's performance over time and determines how flexible a player's rating can be. The volatility itself is updated as part of the rating calculation, making it dynamic and player-specific. However, the initial value and the formula used to adjust it are defined by the system and can be modified based on the specific application. If volatility is initially set too high, the system will interpret early game results as more variable or uncertain. This leads to larger rating adjustments, making the player's rating fluctuate more. This can cause new or improving players to initially overreact to wins or losses, potentially skewing their rating and making it difficult to stabilize. In contrast, setting the volatility too low initially restricts the system from adjusting the rating freely. As a result, players, especially newcomers, may take much longer to reach a rating that accurately reflects their true skill level. This is particularly impactful in fast-changing or skill-evolving games, where a low initial volatility setting could lead to underestimation of skill. This high volatility further affects the ability of the algorithm in prediction accuracy of the algorithm. We observe that Glicko's prediction performance is consistent across both datasets, showing that its performance is less impacted by changes in its hyperparameters compared to Elo and Markov Chain. We theorize that this is due to the large number of games played for each model in both scenarios, allowing the system to adjust the rating deviation dynamically."}, {"title": "5 Discussion", "content": "In the sections 3 and 4, we outlined the desired properties of ranking systems and presented experiments assessing the performance of these systems in achieving these criteria. Based on these observations, this section puts these results into perspective and provides a set of practical recommendations for selecting the most appropriate ranking algorithm. We outline which algorithms perform best under specific circumstances, offering guidance on their optimal use depending on the structure of the dataset, the matchup distribution, and the model evaluation goals. These recommendations, summarized in Table 4, aim to improve the reliability and applicability of classification systems in real-world LLM evaluations."}, {"title": "5.1 Challenges with Elo", "content": "An important result in this paper is that Elo, even with > 1,000 permutations, struggles to achieve stable rankings. This indicates that Elo's dependency on permutations for ranking stabilization is inadequate, challenging previous studies (Boubdir et al.,"}, {"title": "5.2 Choose Bradley-Terry for Small Controlled Datasets", "content": "In section 4.3, we demonstrate that ranking algorithms exhibit varied performance based on battle conditions. When matches are evenly distributed, ranking algorithms produce similar results. Whereas, a heavily skewed distribution can negatively impact some methods such as Elo and Bradley-Terry. In this scenario where matchups are balanced and the number of matches are small, objectiveur findings show that Bradley-Terry is best due to its ability to preserve transitivity while also maintaining comparable prediction accuracy."}, {"title": "5.3 Choose Glicko for large uneven datasets", "content": "For larger datasets, especially those with uneven matchup distributions, the Bradley-Terry model encounters difficulties due to the rare event problem. This problem occurs when new models with high win rates, participate in fewer matchups than well-established models. As a result, these new models can be ranked disproportionately higher based on limited data, leading to biased rankings. In this scenario, the Glicko system provides a superior solution. Glicko utilizes a rating deviation parameter to account for uncertainty in a model's ranking due to limited matchups. This allows the system to adjust the rankings more conservatively for models with fewer comparisons, preventing them from being ranked too high based only on a small number of matches. As data sets become larger and more uneven, Glicko's ability to incorporate rating deviation ensures that the rankings accurately reflect true performance throughout the dataset, even when new models appear with limited match histories. This flexibility makes Glicko particularly effective for large-scale LLM assessments where uneven matchup distributions are prevalent."}, {"title": "5.4 Applicability of Bradley-Terry for other Dataset Types", "content": "While Bradley-Terry model is particularly well suited for small, evenly distributed datasets, there are compelling reasons to use it in other scenarios, such as small uneven datasets or large, evenly distributed datasets. For small uneven datasets, Bradley-Terry's simplicity and ability to model pairwise comparisons make it a robust choice, even when the data distribution is skewed. It does not require parameter estimation, which allows it to handle small datasets efficiently without the need for complex adjustments or hyperparameter tuning, making it ideal for situations where uneven distributions might otherwise require additional preprocessing.\nIn the case of large, evenly distributed datasets, Bradley-Terry can still be effective because of its scalability and ease of interpretation. For large data sets where the distribution of results is balanced, Bradley-Terry's ability to produce clear, interpretable rankings without adding excessive computational overhead is a significant advantage. Although more complex models such as Glicko can offer finer granularity in cases of large uneven data, Bradley-Terry's efficiency and interpretability make it a viable alternative when computational simplicity and transparency are prioritized."}, {"title": "6 Related Work", "content": "The evaluation of LLMs is a multifaceted process that involves two key components: the tasks assigned to the model and the strategy used to measure the model's performance on those tasks. The choice of tasks and evaluation strategies can significantly impact the reliability and validity of the"}, {"title": "7 Conclusion", "content": "In this paper, we explore the application of several ranking algorithms in the context of evaluating LLMs. Through our experiments and analysis, we identify significant limitations in the ability of these algorithms to maintain stable and accurate ranking across varying dataset types and distribution patterns. To combat these challenges, we introduce a set of guidelines for selecting the most appropriate ranking system based on the specific characteristics of the evaluation task and available resources. We believe that our findings and recommendations will be valuable to researchers and practitioners working in the field of LLM evaluation, and we hope that our work will inspire further research in this area."}, {"title": "8 Limitations", "content": "While our study offers a comprehensive exploration into the ranking of Large Language Models (LLMs) through head-to-head combat, several limitations warrant discussion.\nScalability Constraints One significant limitation arises from the scalability constraints inherent in pairwise evaluations. As the number of LLMs in the ecosystem continues to expand, the number of required comparisons grows quadratically. This introduces computational challenges and resource constraints that may limit the feasibility of exhaustive head-to-head evaluations, particularly for larger sets of models.\nHuman Feedback Variability Another concern is the variability in human feedback, which our rankings are heavily based on. Human judgments can be subjective and influenced by numerous factors, including the individual's background, expertise, and contextual understanding. This variability can introduce noise into the ranking system, which can affect the stability and reliability of the evaluations."}, {"title": "A Appendix", "content": "A.1 F1 scores per model\nIn this paper, F1 scores are used as an evaluation metric to assess the consistency and predictive performance of the ranking algorithms. The F1 score is calculated based on the precision and recall of the correctly predicted pairwise outcomes compared to the observed outcomes in the dataset. In the following, we detail the methodology for computing the F1 scores:\nLet the data set consist of m pairwise matches between models. For each pair (i, j), pij is the probability that the model i beats j in a random match. If model i plays Mi matches then Ei, the expected number of games won by model i is Ei = [Mipij] and the actual number of games won is given by Ai.\nThe Precision and Recall are calculated as:\nprecision = \\(\\frac{E_{i} \\cap A_{i}}{E_{i}}\\) , recall = \\(\\frac{E_{i} \\cap A_{i}}{A_{i}}\\)\nThe F1 score is calculated as the harmonic mean of precision and recall:\nF1 Score = 2*\\(\\frac{Precision * Recall}{Precision + Recall}\\)\nThe total F1 score for a ranking algorithm is computed by averaging the F1 scores for all pairwise matches (i, j):\nF1 ScoreOverall = \\(\\frac{1}{|M|}\\) * \\(\\sum_{(i,j) \\in M} F1 Score_{(i,j)}\\)\nwhere M is the set of all pairwise matches."}]}