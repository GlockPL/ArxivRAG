{"title": "Reading ability detection using eye-tracking data with LSTM-based few-shot learning", "authors": ["Nanxi Li", "Hongjiang Wang", "Zehui Zhan"], "abstract": "Reading ability detection is important in modern educational field. In this paper, a method of predicting scores of reading ability is proposed, using the eye-tracking data of a few subjects (e.g., 68 subjects). The proposed method built a regression model for the score prediction by combining Long Short Time Memory (LSTM) and light-weighted neural networks. Experiments show that with few-shot learning strategy, the proposed method achieved higher accuracy than previous methods of score prediction in reading ability detection. The code can later be downloaded at https://github.com/pumpkinLNX/LSTM-eye-tracking-pytorch.git", "sections": [{"title": "1. Introduction", "content": "Reading ability detection is important in modern educational field, which may reveal the subjects' ability of comprehension or cognition during their reading process. Previous works demonstrated that eye-tracking data supplied meaningful information for reading ability detection, and have gained promising results by employing machine learning methods [1-18].\n\nThe eye-tracking based methods of reading ability detection fell into two main categories: the one estimated reading ability with finite number of classes [1-14], providing qualitative evaluation of subjects' reading ability. The other predicted reading ability scores with regression models [15-18], rendering quantitative evaluation of subjects' reading ability. Although the former exhibited satisfactory accuracy in detecting certain classes of abnormalities in reading, it lacked the capability of predicting exact scores of reading ability, which was emphasized in highly interactive educational environments (such as online learning) to make personal and intelligent reactions to subjects.\n\nHowever, precise score prediction of reading ability using eye-tracking data is not easy [15-18], especially when the sample data of subjects are few. In this paper, with few-shot learning strategy, a regression model for score prediction is proposed by combining Long Short Time"}, {"title": "2. Related works", "content": "Previous eye-tracking based reading ability detection methods have showed great interest in detecting finite classes of abnormalities [1-14]. For dyslexia detection, Neru\u0161il et al [1] adopted a Convolutional Neural Network (CNN) to classify entire eye-tracking records either in time or frequency as a whole. D'Mello [2] utilized random forest models, VAJS et al [6] employed autoencoder neural network, Iaconis et al [8] used ordinal pattern transition networks, Hmimdi et al [4] presented a convolutional network tested on a large dataset, Nagarajan et al [9] introduced an explainable reinforcement learning model, and Qi et al [12] proposed a temporal convolutional network (called CASES), respectively, to identify abnormalities in reading. Vajs et al [5] tested new features of eye tracking data on four types of classifiers and compared the results for dyslexia detection. Yoo [10] proposed a quantitative tool to diagnose reading disorders with seven machine learning models for binary classification. Prasse et al [11] developed a neural network SP-EyeGAN to generate synthetic raw eye tracking data, and found that pre-training on the synthetic data was beneficial for native reader classification. Xia et al [14] employed an interpretable model (namely SHAP) to explain the effects of eye tracking features in English proficiency level classification. For dyslexia detection among Chinese children, Haller [3] proposed sequential models to process eye movements and performed tests on Mandarin Chinese. Shi et al [13] used support vector machine to classifier subjects' Chinese reading proficiency levels. Apart from traditional eye trackers, webcam based eye tracker was employed by Hutt et al [7] to test low-cost eye tracking. All the above methods revealed different aspects of eye-tracking based reading ability classification and have achieved satisfactory classification accuracy.\n\nOne the other hand, research on score prediction of reading ability using eye-tracking data was relatively few. In previous works, linear regression models [15-17] were usually adopted to predict scores of reading comprehension on English datasets. Besides, Zhan et al [18] proposed a Multi-Trial Joint Learning Model (MTLM) with linear regression for score prediction on a Chinese dataset. However, due to the few number of eye-tracking samples used in these methods, more complex machine learning methods were hard to be applied for a more accurate score predictor.\n\nFew-shot learning methods [20-25] that occurred in recent years may address the problem mentioned above. Iwata et al [26] proposed a few-shot learning method for spatial regression, using neural networks with Gaussian processes framework. For vision regression tasks such"}, {"title": "3. Methodology", "content": "The framework of the proposed method is shown in Figure 1. Given the eye-tracking data $X_i$ of a subject in one reading test, and a group of eye-tracking data {$X_{i-1},X_{i-2},......, X_{i-(k-1)}$} of certain subjects in the same reading test, LSTM is employed to embed the raw vector $X_i$ into a new vector $Z_i$, which is later used in linear regression for score prediction $y^*_i$ of the reading ability of the subject. This process is indicated by the solid lines and the solid line boxes in Figure 1, which reveals the testing phase of the proposed method."}, {"title": "3.1 LSTM-based few-shot learning", "content": "The LSTM-based few-shot learning module carries out two types of tasks: one is the vector embedding for raw eye-tracking data using LSTM, the other is the few-shot learning strategy combined with the vector embedding."}, {"title": "3.1.1 Vector embedding using LSTM", "content": "The principle of LSTM is demonstrated in [19, 35], and we utilized the hidden units in LSTM for our vector embedding, as is described below:\n\n{$Z_i,Z_{i-1}, Z_{i-2},......Z_{i-(k-1)}$} = $f_{hidden}$({$X_i, X_{i-1}, X_{i-2},......X_{i-(k-1)}$})\n\nwhere $X_i$ is the subject's eye-tracking data in one reading test, {$X_{i-1}, X_{i-2}, ....., X_{i-(k-1)}$} is the raw eye-tracking data of a group of subjects in the same reading test, and the sequence {$X_i, X_{i-1}, X_{i-2},......,X_{i-(k-1)}$} forms the input sequence of LSTM. Similarly, the sequence {$Z_i, Z_{i-1}, Z_{i-2},......, Z_{i-(k-1)}$} forms the hidden state sequence output by LSTM. Besides, the function $f_{hidden}$ represents the operation of producing hidden state sequence by LSTM, which can be inferred from the following equations described in LSTM [19, 35]:\n\n$forget_i$=$\\sigma$($W_f$ *$[Z_{i-1}, X_i]$+$B_f$)\n\n$gate_i$=tanh($W_g$ *$[Z_{i-1}, X_i]$+$B_g$)\n\n$in_i$=$\\sigma$($W_i$ *$[Z_{i-1}, X_i]$+$B_i$)\n\n$out_i$=$\\sigma$($W_o$ *$[Z_{i-1}, X_i]$+$B_o$)\n\n$cell_i$=$cell_{i-1}$$\\circ$ $forget_i$ + $gate_i$$in_i$\n\n$Z_i$=$out_i$tanh($cell_i$)\n\nwhere $W_f$, and $B_f$ are respectively the weight and bias for the forget gate in LSTM, $W_g$, and $W_i$ are the weights for the input gate of LSTM, $B_g$ and $B_i$ are the biases for the input gate of LSTM, $W_o$, and $B_o$ are respectively the weight and bias for the output gate of LSTM. $X_i$ is the raw vector \u2018currently' input into LSTM, and $Z_{i-1}$ is the hidden state vector produced by LSTM which corresponds to the \u2018historical' raw vector $X_{i-1}$. The functions $\\sigma$ and tanh are sigmoid function and hyperbolic tangent function, respectively. And the operators * and $\\circ$ are matrix multiplication and Hadamard product, respectively.\n\nAs can be seen in equation (7), LSTM outputs the \u2018current' hidden state vector $Z_i$ which corresponds to the raw vector $X_i$. It forms the hidden state sequence {$Z_i, Z_{i-1}, Z_{i-2},......,Z_{i-(k-1)}$} along with the \u2018historical\u201d hidden state vectors {$Z_{i-1},Z_{i-2},......,Z_{i-(k-1)}$} that are sequentially produced by LSTM. We treat this hidden state sequence {$Z_i, Z_{i-1}, Z_{i-2}, ......,Z_{i-(k-1)}$} as the embedding vector sequence of the raw vector"}, {"title": "3.1.2 Few-shot learning strategy", "content": "We utilized the idea of episodic training [20, 26, 36-38] to implement our few-shot learning strategy. Since episodic training differs from classical training in that for the former, each \u2018episode' in training phase is itself a training-and-testing process, we organized the data for our episodic training as is illustrated in Figure 2, and designed our episodic training strategy as is described in Algorithm 1."}, {"title": "3.2 Light-weighted neural networks", "content": "Instead of using manually designed formulas for parameter estimation of linear regression, like in previous works, we devised light-weighted neural networks to automatically estimate the parameters of linear regression, which had the architecture shown in Figure 3:"}, {"title": "3.3 Loss function", "content": "For training the parameters of the proposed method, we employed the classic Mean Square Error (MSE) loss function below:\n\n$loss_{MSE}$ = $\\frac{1}{n}$$\\sum_{i=1}^{n}$($y_i$ - $y^*_i$)$^2$\n\nwhere $y^*_i$ and $y_i$ are the predicted score and the ground-truth score of eye-tracking data $X_i$, respectively. And n is the number of episodes in a training batch."}, {"title": "3.4 k-fold cross-validation", "content": "Since given each episode containing the eye-tracking data of k subjects, we predict the score of the reading ability of just one subject, leaving the score prediction for the rest (k-1) subjects not addressed. To better utilize either training data or testing data, k-fold cross-validation could be employed in our few-shot learning strategy as following: In each episode, the score prediction is carried out in turn for every one of the k subjects, i.e., any subject in an episode would have its score prediction using its own eye-tracking data along with those of the rest (k-1) subjects in the same episode, through circularly shifting the data sequence in an episode by one subject each time (in total k times of circular shift for all subjects in an episode)."}, {"title": "4. Experimental results", "content": "We carried out experiments on the same dataset as in [18], where 74 subjects each taking 42 reading tests were involved in collecting eye-tracking data. After filtering out singular values such as missing features, incomplete test records, etc., we finally used 68 subjects each taking 42 reading tests in our experiments. We set the factor r of extracting training data to be 0.9, the number k of neighboring subjects in an episode to be 3, and the number L of stacking times in light-weighted networks to be 4. The following criterion of regression were employed to evaluate the model performance: Mean Absolute Error (MAE) along with the Standard Deviation (SD), which was the same as that in [18]. (Note that the MAE in Table 1 was denoted by percentage which was similar in [18], e.g., 4.02% meant that the MAE was 4.02 out of 100.)\n\nTo find out whether or not k-fold cross-validation is helpful to the proposed method, we tested the proposed method under several different configurations, as is shown in Table 1. Two types of feature configurations were employed in our experiments: One was the 19-dimensional features and the other was the 22-dimensional features, both were exploited by [18]. The difference between these two feature configurations is that the latter added 3 indicators to the former, i.e., the scores of Chinese, mathematics, and English in university entrance examination. For simplicity, we noted as N/A for the case where neither training phase nor testing phase had k-fold cross-validation, Semi for the case where only training phase took k-fold cross-validation, and Full for the case where both training phase and testing phase enjoyed k-fold cross-validation."}, {"title": "5. Conclusions", "content": "A LSTM-based few-shot learning method for score prediction of reading ability using eye-tracking data was proposed. In the method, LSTM was employed to embed the raw eye-tracking vectors into new vector space, and light-weighted neural networks were utilized to estimate the parameters of linear regression. In addition, few-shot learning strategy was applied to training the proposed regression model on a small-size dataset. The proposed method showed much effectiveness compared to previous methods.\n\nIn future works, the proposed method may be improved from the following three aspects: Firstly, the time information of eye-tracking data could be involved with the currently used feature vectors for more precise score prediction. Secondly, generative models for producing"}]}