{"title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks", "authors": ["Giandomenico Cornacchia", "Giulio Zizzo", "Kieran Fraser", "Muhammad Zaid Hamed", "Ambrish Rawat", "Mark Purcell"], "abstract": "The proliferation of Large Language Models (LLMs) in diverse applications underscores the pressing need for robust security measures to thwart potential jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger data integrity and user privacy. Guardrails serve as crucial protective mechanisms against such threats, but existing models often fall short in terms of both detection accuracy, and computational efficiency. This paper advocates for the significance of jailbreak attack prevention on LLMs, and emphasises the role of input guardrails in safeguarding these models. We introduce MOJE (Mixture of Jailbreak Expert), a novel guardrail architecture designed to surpass current limitations in existing state-of-the-art guardrails. By employing simple linguistic statistical techniques, MoJE excels in detecting jailbreak attacks while maintaining minimal computational overhead during model inference. Through rigorous experimentation, MoJE demonstrates superior performance capable of detecting 90% of the attacks without compromising benign prompts, enhancing LLMs security against jailbreak attacks.", "sections": [{"title": "1 Introduction", "content": "The increasing adoption of Large Language Models (LLMs) in various applications brings forth the critical issue of ensuring their security against potential attacks, particularly jailbreak attacks. These attacks exploit vulnerabilities within the model and bypass existing safeguards to manipulate its behavior, posing significant risks to data integrity and user privacy. Thus, LLMs can be exploited by malicious actors for spreading misinformation, facilitating criminal activities (Kreps, McCain, and Brundage 2022; Goldstein et al. 2023; Kang et al. 2023) and even compromising scientific experimental settings (Birhane et al. 2023). In response to this threat, the deployment of guardrails, which serve as protective mechanisms, has become essential to detect and mitigate such attacks (Welbl et al. 2021; Gehman et al. 2020; Dong et al. 2024).\nTo prevent LLMs from producing undesired outputs (Huang et al. 2023), several mitigation strategies have been proposed in the literature including training time and fine-tuning strategies e.g., instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) (Carlini et al. 2024). These approaches have shown to improve the robustness of LLMs against jailbreak attacks, but often incur substantial computational costs and manual effort. On the other hand, guardrails-based mitigation approaches that can detect and filter the malicious input to the LLMs or post-process LLMs' output (Jain et al. 2023b; Helbling et al. 2023; Anwar et al. 2024) provide a computationally efficient alternative for protecting LLMs against malicious attacks. Despite improving robustness against jailbreak attacks, these guardrail-based defence approaches are yet to prove effective against a more sophisticated attacker which remains an open research problem (Shayegani et al. 2023).\nTo overcome the limitations of existing guardrail-based approaches against evolving attack strategies we propose a novel guardrail-based approach MoJE (Mixture of Jailbreak Expert), which outperforms current state-of-the-art guardrails in both attack detection accuracy, latency and throughput. Leveraging simple linguistic techniques, such as different tokenization strategies or n-gram feature extraction, MoJE demonstrates superior performance in identifying and neutralizing jailbreak attacks while maintaining minimal computational overhead during model inference. In addition, due to its modular nature, MoJE model can be easily extended to include models trained to defend against new attacks and out-of-distribution (OOD) datasets.\nOur extensive experiments benchmark MoJE against both state-of-the-art open weight solutions, i.e., ProtectAI (ProtectAI.com 2023) and Llama-Guard (Inan et al. 2023), and closed source defences, i.e., OpenAI content moderation API (OpenAI 2023) and Azure AI Content Safety (Microsoft 2024). We demonstrate that MoJE not only surpasses these baselines in harmful content detection on various datasets but also exhibits superior resilience to jailbreak attacks."}, {"title": "2 Related Works", "content": "The push for the safe and ethical deployment of advanced LLMs in digital contexts has spurred efforts in mitigating harmful content generation (Burgess 2023; of the Council 2023). The high volume of data used to train and poor data ingestion practice makes language models particularly vulnerable to prompt attacks. In the LLMs landscape, prompt attacks are generally known as \u201cjailbreak\u201d. Jailbreaks refers to the (un)intended manipulation of LLMs with linguistics and to force them into generating harmful or inappropriate content. Research efforts in defence against jailbreaks can be primarily divided into alignment-based and moderation-based approaches, each with distinct challenges and limitations (Dong et al. 2024; Yuan et al. 2024).\nAlignment-based strategies, such as RLHF (Ouyang et al. 2022; Bai et al. 2022a) and constitutional AI (Bai et al. 2022b), seek to align LLMs with ethical standards by training them to avoid engaging with predefined harmful topics. Despite progress, these methods require significant computational and human resources and mainly address pre-specified harmful content, limiting their effectiveness against new or evolving threats (Jain et al. 2023a). Moreover, fine-tuning often leads to surface-level modifications, as evidenced by persistently high logits of harmful tokens and vulnerability to subtle harmful behaviors (Hubinger et al. 2024). Nevertheless, these methods face challenges from diverse disruptions such as new customization and manipulation techniques (Russinovich, Salem, and Eldan 2024). While jailbreak detection contributes to LLM security by identifying potential alignment breaches, it primarily identifies deviations rather than directly assessing harmfulness, inheriting the limitations of alignment-based approaches. For the aforementioned reasons, new protective mechanisms to detect and mitigate such attacks are crucial (Welbl et al. 2021; Gehman et al. 2020; Dong et al. 2024).\nModeration-based strategies, generally known as \u201cGuard\u201d or \u201cGuardrails\u201d, initially aimed at improving social media safety, have shown promise in enhancing LLM safety. Traditional methods like the OpenAI Content Moderation API or Azure AI Content Safety API operate as classifiers trained on categorically labeled content (Lees et al. 2022). However, their effectiveness is limited to predefined categories, hindering their ability to address emerging risks or new attack typologies. Recent approaches also employ general pre-trained LLMs, e.g., Llama-Guard (Inan et al. 2023) (a finetuned version of Llama2 which benefits from its broader contextual understanding) for more extensive harmful content detection. However, these methods also inherit vulnerabilities from associated to their base LLMs, particularly susceptibility to sophisticated jailbreak attacks or misalignment after fine-tuning (Qi et al. 2023). Furthermore, these models require high computational effort as they are generally in the order of millions or billions of parameters. Thus, the use of such LLM-based guardrails in real-time applications necessitates a powerful GPU-ready cloud architecture, which inevitably leads to growth in latency, energy costs, and the application's overall carbon footprint.\nIn this context, MoJE extends the moderation-based approach, with a simple and robust moderation framework resistant to adversarial attacks with low computational requirements (i.e., a single CPU-core can predict with a low latency)."}, {"title": "3 Methodology", "content": "Let $D = \\{d_i, y_i\\}_{i=1}^N$ be a dataset of $N$ documents, or prompts, $d_i$ and associate label $y_i$, where $y_i \\in Y = \\{jailbreak, benign\\}$. Furthermore, each sample $d_i$ with $y_i = \\{jailbreak\\}$ can belong to a specific category of jailbreak. Thus, the dataset $D$ can be divided into $l + 1$ different subsets which $D_{benign}$ is the set of benign documents while $D_{jailbreak_j}$, with $j$ from 1 to $l$ and $y_i = jailbreak$, the $j$-th specific set of jailbreaks.\nWe assume that our dataset, after appropriate transformation, can be represented as a $m$ dimensional random variable $X \\in \\mathbb{R}^m$ through a function $h: D \\rightarrow X$, without considering the associate label $y_i$ in the transformation of each document $d_i$. Since the definition domain of $Y$ is a binary set, we can define the problem as a binary classification task and learning a mapping function $M_{OJE}: X \\rightarrow Y$ is our main goal. 1"}, {"title": "3.2 Mixture of Jailbreak Experts", "content": "Let's consider having $l$ classifiers, where each classifier is trained on $D_{buj} = D_{benign} \\cup D_{jailbreak_j}$, i.e., the set of benign prompts and the j-th specific jailbreak prompt which is transformed into a vector space $X_j$ after applying the $h(\\cdot)$ transformation function. Thus, it can be considered the j-th jailbreak expert. To this end, we find each model parameters $\\theta_j$ that maximizes the posterior probability over the respective j-th training set, i.e.:\n$\\theta_j = \\arg \\max_{\\theta} P(\\theta|X_j)$.\nOur MoJE will be the ensemble of each model parameters as $\\Theta = \\{\\theta_1, \\theta_2,...,\\theta_l\\}$. For the sake of simplicity, the predictions of each classifier is denoted as $f(\u00b7)$ which represent the probability assigned by the j-th classifier to the positive class (i.e., identified as the j-th jailbreak) and as $f_{MOJE}(x_i)$ the probability assigned by our MoJE to the positive class. Considering a prompt sample $d_i$ that is transformed in a vector $x_i$ i.e., $x_i = h(d_i)$, the inference of our final MoJE classifier is defined as:\n$f_{MOJE}(x) = \\begin{cases}\n    \\max P(x_i) & \\text{if } \\max P(x_i) >= T \\\\\n    \\text{avg} P(x_i) & \\text{if } \\max P(x_i) < T\n\\end{cases}$\nwhere $P(x_i) = \\{f_1(x_i), f_2(x_i),..., f_l(x_i)\\} \\in [0, 1]$ denotes a vector of all classifiers' posterior probability, and $T$ denotes the probability threshold which is set to $\u03c4 = 0.5$ for mathematical convenience.\nIn conclusion, our Mixture of Jailbreak Expert is an ensemble of tabular classifiers that selects the maximum prediction probability value if one or more of the classifiers'\nIt could be argued that this task is suited for multi-classification, but due to linguistic complexities and overlapping (relatively new) concepts between jailbreak categories, binary classification was selected for simplicity. Multi-classification is remanded for future investigation."}, {"title": "4 Experimental Setting", "content": "This section describes the dataset, SOTA guardrails, models, and tuning strategy used in this work."}, {"title": "4.1 Dataset", "content": "We will divide this section as follows: jailbreak-prompts dataset, in which we list the jailbreak dataset we used for our analysis, and benign-prompt dataset, in which we list the employed benign dataset. The use of a benign dataset is crucial to prevent the detector being biased toward jailbreaks with the consequence of a high false positive rate (FPr) which is not desirable in deployment scenarios. Datasets are listed in Table 1 along with their characteristics.\nJailbreak-Prompts Dataset Here, we present the datasets used for jailbreak prompts, i.e., harmful behaviors, gandalf ignore instructions, gcg-vicuna, and jailbreak prompts.\nHarmful Behaviors: The Harmful Behaviors dataset is a sub-set of the AdvBench dataset, which has been designed to test LLM alignment for safety requirements (Zou et al. 2023). It is divided in two sub-sets: Harmful Strings and Harmful Behaviors. Both datasets have been generated by prompting Wizard-Vicuna-30B-Uncensored, an uncensored and unaligned version of Vicuna model. The authors handcrafted 100 and 50 prompts respectively. Then, they generated 10 new samples, each giving a 5-shot demonstration as prompt. The latter was selected for this work as they are more aligned with the jailbreak setting. The dataset consists of 512 harmful behavior prompts crafted as instructions, encompassing the same themes within the harmful strings setting. The prompts are crafted as questions as the aim is to enforce the model to generate harmful content as a response that complies with the harmful instructions. It spans various themes observed in online interactions, including cyberbullying, hate speech, and harassment, thereby serving as a crucial resource for training and assessing algorithms tasked with detecting and curtailing harmful behavior within digital environments and online communities.\nGandalf Ignore Instruction: The Gandalf Ignore Instruction dataset consists of prompts collected by Lakera AI (Lakera AI 2023). The prompts were collected during an educational game designed to inform people about AI leakage risks of prompt attacks on large language models (LLMs). The dataset consists of 1000 instruction-based prompts that utilize role-playing to circumvent the model's alignment defense such that it reveal the game's secret password.\nGCG Vicuna behavior: The GCG-Vicuna dataset was generated using the technique and methodology outlined by (Zou et al. 2023). This dataset consists of 512 samples the same number found in the Harmful Behaviors dataset, as it was used for prompting Vicuna model during the attack. The type of attack performed is the \u201cindividual\u201d GCG (Greedy Coordinate Gradient) method. For each harmful behavior prompt, the attack starts by appending an adversarial suffix of twenty spaced exclamation marks (i.e., \"! \") to the prompt. The attack subsequently makes further revisions to the appended suffix, attempting to reduce the loss, until the model deigns to answer without refusal keywords. Throughout the attack, several distinct attack suffixes may be generated. The suffix selected was one which resulted in a successful attack and has lowest loss. To generate and test the performance of the new suffix, we employed the Vicuna-7b-v1.5 model (Zheng et al. 2023), a fine-tuned version of Llama2 (Touvron et al. 2023), replicating the experimental setup of (Alon and Kamfonas 2023).\nJailbreak Prompts: The Jailbreak Prompts dataset comprises examples of four platforms (i.e., Reddit, Discord, websites, and open-sources datasets) from December 2022 to May 2023, which consists of 6387 prompts, then filtered to 666 prompt considered as jailbreaks \u201cin the wild\" by (Shen et al. 2023).\nBenign Prompts The following datasets used in this work are defined as benign prompts: puffin, alpaca, and awesome chatgpt prompt.\nPuffin: The Puffin dataset is a collection of multi-turn conversations between GPT-4 and humans. This dataset is comprised of 2000 conversations, with an average of 10 turns, and contains conversation context lengths stretching over 1000 tokens. In the context of this work, only the set of 6,994 prompts produced by the human side of the conversation was selected, as these prompts align best with benign labeled data.\nAlpaca: The Alpaca dataset, comprises 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine (Taori et al. 2023). The primary use case of the Alpaca dataset is to serve as a valuable resource for instruction-tuning language models, facilitating enhanced adherence to instructions. Indeed, the data generation pipeline was explicitly tasked for instruction data generation. In our setting, the distilled instruction will consist of benign instruction prompts that contrast the effect of role-playing jailbreak instruction prompts.\nAwesome ChatGPT Prompt: Awesome ChatGPT Prompts is a repository containing a curated collection of prompt examples designed to be used with the ChatGPT model. Specifically, it offers a prompt collection designed to be successful on use cases applied to ChatGPT models. For our use case, it increases the collection of prompts associated with a benign role-playing scenario.\""}, {"title": "4.2 Guardrails", "content": "In this section we introduce the set of SOTA Guardrails or chat Moderation tools that we use for comparison with our proposed approach.\nProtectAI: ProtectAI guard is a security tool designed to identify and prevent prompt injection attacks which can manipulate language models into producing unintended outputs (ProtectAI.com 2023).\nThe model is a fine-tuned version of the microsoft/deberta-v3-base model, based on the Microsoft BERT Language Model with 86 million backbone parameters (He et al. 2021). The ProtectAI guard is trained on a combination of prompt injections, jailbreak, and benign prompt datasets. It categorizes inputs into two classes: 0 for no injection and 1 for detected injection.\nLlama-Guard: Llama-Guard is an LLM-based safeguard model tailored for Human-AI conversation scenarios (Inan et al. 2023). The model is based on the family of open weight Llama2 models deployed by Meta (Touvron et al. 2023). Indeed, it is an instruction based tuned version of Llama2-7b. It employs a curated safety risk taxonomy template to effectively categorize prompts and responses, enhancing safety assessment and moderation. This model acts as a binary classification model with the first generated token, i.e., safe or unsafe\u201d), categorizing the prompt. If the model assessment is \u201cunsafe\u201d, then the model generates a new line, listing the taxonomy categories that are violated in the given piece of content. Note that, as a text-to-text approach, it does not contain the output probability, hence the AUC result is not present in Table 2 for Llama-Guard model.\nOpenAI Moderator API: OpenAI Moderator is an API AI-powered content moderation system designed to check, monitor, and filter user-generated content that is potentially harmful (OpenAI 2023). Leveraging natural language processing (NLP) algorithms and machine learning models, OpenAI Moderator automatically identifies and flags potentially harmful or inappropriate content, such as hate speech, spam, abusive language, harmful intent and instructions. The model engine used at the time of experiments is the text-moderation-007. The output of the model is divided into 11 categories which can be flagged separately. Each of these categories is linked with a probability. We treat the problem as a binary classification task, where the probability of a jailbreak is the maximum of the harmful categories in cases where more than one category is flagged.\nAzure AI Content Safety API: Azure AI Content Safety API is a cloud-based service offered by Microsoft Azure for content moderation and safety analysis (Microsoft 2024). It consist of an ensemble of classification models to identify and prevent the output of harmful content. This system actively detects and addresses specific categories of potentially harmful content within both input prompts and output completions, with dedicated models trained and tested for hate speech, sexual content, violence, and self-harm across multiple languages. While the service extends its support to various languages beyond the specified ones, users are advised to conduct their own testing to verify its effectiveness for their specific application needs. We specifically employed the jailbreak endpoint API. In the same manner as with Llama Guard, we do not obtain a probability but only a boolean jailbreak flag."}, {"title": "4.3 Data Preprocessing", "content": "To keep our proposed approach computationally efficient, we employed simple classifiers to perform the detection. To comply with classic tabular classifier requirements, we transform the prompt input or document i.e., di into a vector xi through a transform function h(\u00b7). We used the n-gram occurrences count, with n = 1 (i.e., uni-gram), as the feature extraction function to map the dataset into an m-dimensional vector (i.e., h(D) = X). This dimension 'm' of the features' vector space X : Rm depend on two factors: (i) the number of tokens detected based on the splitting strategy and (ii) the feature extraction strategy (e.g., uni-gram vs bi-gram). Furthermore, we separate punctuation from the words and treat it as a separate token since few attacks are based on special punctuation tokens as a prefix or suffix (e.g., gcg-based attack).\nWe used two models: a simple Logistic Regression (LR) model, for understanding the linear dependencies between each attack type, and eXtreme Gradient Boost Machine (XGB), as one of the most suitable and popular classifiers used for tabular datasets. For our proposed MoJE framework, the j-th expert in the model selection phase is chosen from the two models (LR and XGB) that are obtained after a grid-search on model hyperparameters (see Figure 1a).\nFurthermore, we also use LR and XGB as baseline models trained on the full dataset D to compare the effectiveness of our proposed MoJE framework."}, {"title": "4.4 Tuning Strategy", "content": "The datasets have been divided using the hold-out method, with an initial 80/20 train-test split, and a further splitting of the train dataset into an 80/20 train-validation split. The validation set facilitates hyperparameter tuning and expert model selection, whilst the test set facilitates identifying the best model in the Results' discussion. Finally, we divide the training datasets in chunks of j-th jailbreak and benign prompts. For each of these chunks we tune each classifier f\u03b8j with a 5-fold cross-validation grid search methodology where for each set of parameters we choose the set that maximize the F\u03b2 score, with \u03b2 = 0.5, defined as:\n$F_\u03b2 = (1 + \u03b2^2) \u00b7 \\frac{precision \u00b7 recall}{(\u03b2^2 \u00b7 precision) + recall}$\nWe decided to optimize the models with F\u03b2 score to reduce the FP rate while still maintaining a relatively high TP rate instead of balancing TP rate and TN rate. We consider this setting because we deem FP rate to be more important to the usefulness of the detector in a real application scenario.\nAfter training each model on a j-th dataset subset, we choose the best model and call it as a j-th jailbreak expert, based on the best F\u03b2, with \u03b2 = 0.5. Thus, we ensemble all the best j-th dataset subset models for each j to create a MoJE pipeline as described in Figure 1 and Section 3.2.\nThe advantage of our approach is the modularity. Indeed, if a new type of attack is discovered, a new classifier can be easily trained and added to the mix (i.e., f\u03b8l+1). On the other hand, for Llama-Guard or ProtectAI, it is necessary to retrain the whole model.\nFor a baseline defence comparison, we also trained LR and XGB on the whole dataset D respectively to account for a simpler approach for comparison to our proposed MoJE."}, {"title": "5 Results", "content": "In this section, we will present and discuss the results of our investigation. We first present the aggregate results of all the datasets, benign and jailbreak, on the test set split. Then, we have a more extenisve analysis of jailbreak detection (i.e., TP rate) and benign miss-classification (i.e., FP rate) results presented respectively in Figure 2 and Figure 3.\nWe start by presenting the aggregate results of each model with respect to the full test set. Results are condensed in Table 2 which presents the classification results of various models across multiple metrics, including AUC, accuracy (ACC), F\u03b2 with \u03b2 = 0.5, recall, and precision. The models are categorized into three sections: (i) models belonging to our pipeline (i.e., naive tabular classifiers), (ii) open-weights fine-tuned Language Models, and (iii) closed-source API endpoints. Among the models in our pipeline, MoJE demonstrates the highest performance across most metrics, achieving an AUC of 0.9947 and an ACC of 0.9944. Notably, MoJE also attains the highest F\u03b2 score of 0.9529, indicating strong performance in balancing precision and recall. However, ProtectAI and LLAMA-guard, while achieving great ACC scores, show comparatively lower performance in terms of F\u03b2 score, demonstrating potential areas of weakness in their predictive capabilities.\nOn the other hand, models sourced from open-source fine-tuned Language Models and closed-source API endpoints exhibit a more diverse performance landscape. For instance, OpenAI moderator achieves a perfect precision score of 1.0000, but with significantly lower scores across other metrics, indicating potential issues balancing precision and recall. AzureAPI, while showing higher performance in terms of ACC compared to OpenAI moderator, exhibits lower precision and recall, highlighting trade-offs between different evaluation criteria. Overall, Table 2 provides a comprehensive overview of the classification results across various models, shedding light on their strengths and weaknesses in different operational contexts.\nIt should be noted that comparing aggregate metrics can lack rigour as it is not disclosed which datasets have been used for training both open-weight models and closed source ones. Best practice would be to evaluate competitors model on all the datasets present in the literature. Nevertheless, such large scale benchmarking is out of scope for this work since we aim to evaluate the learning capability of naive tabular classifiers on guardrail tasks. For this reason, hereinafter, we present the dataset-specific model result in terms of TP rate and FP rate for jailbreak and benign datasets respectively.\nFigure 2, depicts the true positive (TP) rate for each jailbreak dataset across different models. The first insight observed is that, notably, our tabular models demonstrate varying levels of performance across different datasets, with MoJE exhibiting the most stable TP rate across all datasets. Conversely, the open-weight models, ProtectAI and Llama-Guard, exhibit mixed performance, demonstrating relatively low TP rates compared to the tabular models, particularly evident in datasets such as \u201charmful behaviors\u201d and \u201cgandalf ignore instructions", "gandalf ignore instructions\u201d and \u201cjailbreak prompts": "while OpenAI moderator performs relatively poor across all datasets. Overall, the figure provides valuable insights into the TP rates of different models across various jailbreak datasets, highlighting the performance variations and potential areas for improvement in jailbreak detection systems.\nOn the other hand, Figure 3 illustrates the false positive (FP) rate for each benign dataset. Notably, the FP rates vary among the different datasets and models but generally remain very low. For instance, LR exhibits relatively high FP rates across all datasets compared to other models, with notable spikes observed in \u201cpuffin", "alpaca": "atasets. Conversely, MoJE demonstrates low FP rates across all datasets, showcasing its superior performance in mitigating false positives. Open-weight models, such as ProtectAI and Llama-Guard, show mixed performance, with ProtectAI exhibiting higher FP rates in the \u201cpuffin\u201d dataset compared to Llama-Guard. Moreover, closed-source models, OpenAI moderator and Azure AI Content Safety, exhibit distinct patterns of FP rates across the datasets, further emphasizing the variability in model performance. Overall, the figure provides valuable insights into the FP rates of different models across various benign datasets, highlighting their strengths and weaknesses in different operational contexts.\nSUMMARY. In conclusion, MoJE demonstrates superior performance in balancing precision and recall, highlighting its effectiveness in guardrail tasks for LLMs. The findings underscore the importance of careful model selection and dataset-specific evaluation for robust guardrail systems."}, {"title": "6 Ablation", "content": "Table 3 presents the performance comparison of the MoJE architecture across different tokenizers and feature engineering functions h(\u00b7), which control and determine the size, m, of the extracted input features. Performance is evaluated using various metrics including AUC, accuracy (ACC), F\u03b2 score with \u03b2 = 0.5, F1 score, recall, and precision. Notably, the choice of tokenizer and feature engineering function significantly influences the curse of dimensionality (m), with lower values indicating better performance. For instance, using character-based tokenization with uni-gram features consistently yields the lowest m values across all models, indicating effective dimensionality reduction. Conversely, employing bi-gram features with character-based tokenization results in higher m values, resulting in increased dimensionality and potential overfitting.\nRegarding model performance metrics, certain combinations of tokenization and feature engineering demonstrate superior performance across different evaluation metrics. For example, using the BERT tokenizer with uni-gram features yields competitive results across various metrics, achieving the highest AUC and F\u03b2 scores in some cases. Additionally, the combination of uni-gram features with the (uni+bi)-gram tokenizer consistently delivers strong performance across different metrics, highlighting the effectiveness of combining multiple tokenization strategies.\nHowever, it's important to note that the optimal tokenizer-feature engineering combinations vary depending on the evaluation metric. For instance, while certain configurations excel in terms of AUC and F\u03b2 score, they may exhibit lower accuracy or precision. Therefore, practitioners should carefully select the tokenizer and feature engineering function based on the specific requirements of their application and the importance assigned to different evaluation metrics.\nOverall, the table provides valuable insights into the impact of tokenization and feature engineering on model performance, facilitating informed decision-making in the design and optimization of machine learning architectures."}, {"title": "6.2 Mutual Information Theorem for Feature Selection", "content": "In the previous section, we have noticed how the dimensions of a corpus, the chosen tokenization method, and the feature extraction approach can all contribute to the curse of dimensionality. Classical machine learning techniques such as feature selection and dimensionality reduction offer viable strategies to address this challenge. In our work, we decided to employ the Mutual Information Gain theorem as feature selection strategy (Beraha et al. 2019). The aforementioned theorem, initially proposed by (Quinlan, J. Ross 1986), assesses the discrepancy between the entropy of class distribution and the conditional entropy given a specific feature.\nSpecifically, the mutual information (MI) between two random variables X (i.e., our extracted features) and Y (i.e., our target class) is defined as:\n$I(X; Y) := H(Y) \u2013 H(Y | X)$,\nwhere the entropy H(X) of a random variable X, having p as probability density function, is a measure of uncertainty:\n$H(X) := E_X [-log(p(X))] = - \\int p(x) log p(x) dx$.\nIntuitively, the MI between X and Y represents the reduction in the uncertainty of Y after observing X (and vice-versa).\nFigure 5 illustrates the impact of feature selection, based on the MI Theorem, on several performance metrics, including AUC, F\u03b2, Recall, and Precision. Each curve represents the variation in these metrics as the percentage of selected features, denoted as percentage of m, changes. Four distinct colored lines depict the performance trends for each metric, providing insights into how different percentages of feature selections influence model performance. The x-axis represents the percentage of features selected, while the y-axis shows the corresponding values of the performance metrics.\nAs the percentage of m increases, the performance metrics exhibit diverse behaviors. Notably, for AUC and F\u03b2, there is a consistent improvement with an increasing percentage of features selected, indicating that a higher number of informative features contributes to better model performance in terms of overall classification accuracy and balance between precision and recall. Conversely, Recall and Precision show more nuanced patterns, with Recall generally increasing as more features are selected, while Precision may exhibit fluctuations or reach a plateau after a certain percentage of m. These observations suggest that while increasing feature selection can enhance certain aspects of model performance, there may be diminishing returns or trade-offs to consider, particularly in terms of Precision.\nOverall, the figure provides valuable insights into the relationship between feature selection and model performance, highlighting the importance of optimizing the percentage of m to achieve the desired balance between various performance metrics. Additionally, the results underscore the effectiveness of utilizing the MI Theorem as a feature selection strategy, demonstrating its ability to improve the discriminative power of the model while maintaining a balance between different evaluation criteria.\nSUMMARY. We demonstrate how feature selection using the Mutual Information Theorem impacts key performance metrics, revealing nuanced behaviors in AUC, F\u03b2, Recall, and Precision as the percentage of selected features changes, highlighting the need for optimizing feature selection to achieve a balanced model performance."}, {"title": "6.3 Out of Distribution Data and MoJE Modularity", "content": "With new LLM attacks being developed, updating guardrails can require significant time and computational power specifically for LLM based guardrails (e.g., Llama-Guard). Furthermore, training this model remains challenging since engineers should decide the right compromise between time and performance due to poor hyperparameter exploration. Our model overcomes all these challenges through its inherent modular architecture.\nLet's consider a case in which we need to integrate new datasets that are out of distribution (OOD) with respect to the one used for training due to poor performance. Our architecture allows us to train a new classifier f\u03b8l+1(\u00b7) and add it into the mix of MoJE as the l+ 1-th classifier. As an example, we will consider two jailbreak datasets (i.e., \"aart\" and \"attaq\"), one benign dataset (i.e., \u201cboolq\u201d), and a mix of extreme cases of jailbreak and benign prompts (i.e., \u201cxstest\").\""}, {"title": "7 Conclusion", "content": "In conclusion, MoJE demonstrates superior performance in balancing precision and recall, underscoring its effectiveness in guardrail tasks for Large Language Models (LLMs) while maintaining a simple and low-computation resource architecture. The results highlight the significance of careful model selection and dataset-specific evaluation for robust guardrail systems. Through ablation studies, we underscore the importance of selecting appropriate tokenization and feature engineering strategies, optimizing feature selection, and integrating new classifiers for out-of-distribution datasets. The findings reveal nuanced behaviors in performance metrics and emphasize the need for optimizing feature selection to achieve balanced model performance specifically in case of curse of dimensionality. While MoJE effectively addresses various challenges, such as computational efficiency and adaptability to new datasets, it shows limitations in handling complex linguistic prompts like \"xstest\", indicating the necessity for LLM-based guardrails like Llama-Guard for better mitigation of certain jailbreak prompt attacks.\nFuture work will focus on further enhancing the adaptability of MoJE by exploring other low-weight language model architectures, new feature engineering techniques, or linguistic data augmentation techniques. Furthermore, a hybrid approach combining statistical methods with deep learning could be a possible new research direction."}]}