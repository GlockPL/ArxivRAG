{"title": "Distributed Clustering based on Distributional Kernel", "authors": ["HANG ZHANG", "YANG XU", "LEI GONG", "YE ZHU", "\u039a\u0391\u0399 MING TING"], "abstract": "This paper introduces a new framework for clustering in a distributed network called Distributed Clustering based on Distributional Kernel (K) or KDC that produces the final clusters based on the similarity with respect to the distributions of initial clusters, as measured by K. It is the only framework that satisfies all three of the following properties. First, KDC guarantees that the combined clustering outcome from all sites is equivalent to the clustering outcome of its centralized counterpart from the combined dataset from all sites. Second, the maximum runtime cost of any site in distributed mode is smaller than the runtime cost in centralized mode. Third, it is designed to discover clusters of arbitrary shapes, sizes and densities. To the best of our knowledge, this is the first distributed clustering framework that employs a distributional kernel. The distribution-based clustering leads directly to significantly better clustering outcomes than existing methods of distributed clustering. In addition, we introduce a new clustering algorithm called Kernel Bounded Cluster Cores, which is the best clustering algorithm applied to KDC among existing clustering algorithms. We also show that KDC is a generic framework that enables a quadratic time clustering algorithm to deal with large datasets that would otherwise be impossible.", "sections": [{"title": "1 Introduction", "content": "Clustering is a fundamental problem in data analysis, and clustering algorithms are widely used in various applications.\nThe rise of big data necessitates a large amount of data to be stored on distributed sites [16, 25, 31, 34, 43]. As a result,\ndata mining tasks often need to be conducted in a distributed framework.\nCurrent work in distributed clustering has focused on converting an existing centralized clustering algorithm into a\ndistributed version. The key challenge is to reduce the high cost of frequent inter-site communication between various\nsites, while strive to minimize the potential decline in clustering quality [2, 4, 35].\nThere are two main approaches. The first approach relies on k-means clustering [19]. It uses the clustering results of\na small representative subset to guide the clustering of the overall data. Specifically, this approach obtains k centers by\nk-means in the subset and then completes the labeling of the entire data through the k centers. This approach requires\nno parallelization of k-means, and its main focus is to obtain a good subset.\nThe second approach aims to enable a computationally expensive centralized clustering algorithm to deal with large\ndatasets by parallelizing the clustering algorithm. Its focus is to reduce the communication cost and runtime with some\napproximation techniques that often use indexing such as Locality-Sensitive Hashing (LSH) [11] or kd-tree. Studies in\nthis approach have focused on density-based clustering algorithms like DBSCAN [14, 35] and DP [2, 33, 44] because\nthese algorithms often have better clustering quality than that produced by k-means (used in the first approach).\nThe common characteristic of these two approaches is that each method is intricately crafted for a specific clustering\nalgorithm. None of them can be easily adapted to a different clustering algorithm.\nIn this paper, we propose a new generic framework that is applicable for any existing or new clustering algorithm.\nOur work is closely related to the first approach with three steps, but has distinct differences.\nExisting distributed methods under the first approach are based on Framework B shown in Figure 1(right). It is\nthe most direct and efficient way of applying k-means, and it has three steps: 1. Extracting a subset: each distributed\nsite is required to obtain a small set of representative points. This process often involves a clustering algorithm (not\nnecessarily k-means) on each site. 2. Clustering: k-means on the coordinator site in the network determines the k\ncenters of k clusters from the combined set of representative points from all sites. 3. Point assignment on each local site:\nthe k cluster centers are broadcast from the coordinator site to each local site, and then every point in the local dataset\nis assigned to the nearest cluster center at each site independently.\nThe focus of Framework B is to seek a subset of good representative points (using some method of subset extraction\non each site) in step 1, using either one-round communication or multi-round iterative communication, and all existing\nmethods of this framework have used the same last two steps.\nFramework B has three fundamental limitations. First, all existing methods based on B focus on ways to find a\n'good' representative subset. Yet, even if a good subset of representative points is found, the framework cannot produce\na good enough clustering outcome because k-means can discover clusters of globular shapes only. Second, there is no\nguarantee that the distributed clustering produces the same clustering outcome as derived by its centralized counterpart.\nThe only exception is the coreset-based methods [5, 15] which guarantees that the coreset found in step 1 is as good as\nthe entire dataset. But the first fundamental limitation remains. Third, B often has time complexity worse than linear\nbecause of the high computational cost in step 1, despite the use of k-means in step 2.\nWe are motivated to address these fundamental limitations, especially the first one. First and foremost, we aim to\nfind clusters of arbitrary shapes, sizes and densities, which even existing density-based clustering algorithms such as"}, {"title": "3 Distributional Kernel-based Clustering", "content": "Distributional kerne-based clustering represents each cluster as a probability density function, which means the points\nin the same cluster are independent and identically distributed. Given a dataset D, let $P_C$ be the probability density\nfunction (pdf) of a cluster $C\u2282 D$.\nA distributional kernel $K(P_{C_i}, P_{C_j}) = (\u03a6(P_{C_i}), \u03a6(P_{C_j}))$ measures the similarity between two pdfs $P_{C_i}$ and $P_{C_j}$,\nwhere \u03a6(P) is the feature map of the kernel K that maps the pdf P in input space into a point \u03a6(P) in the feature\nspace of K [28].\nDEFINITION 1. Boundaries between probability density functions of clusters. If dataset D has a set of clusters C =\n{C1, ..., Ck }, the boundary between any two of the k clusters is defined for x \u2208 Rd as follows:\n$K(\u03b4(x), P_{C_i}) = K(\u03b4(x), P_{C_j}) \u2200i \u2260 j \u2194 (\u03a6(\u03b4(x)), \u03a6(P_{C_i})) = (\u03a6(\u03b4(x)), \u03a6(P_{C_j})) \u2200i \u2260 j$\nwhere d(x) is the Dirac measure of a point x which converts a point into a pdf.\nIn other words, in the feature space of K, the clusters form a Voronoi diagram with a Vonoroi cell centered at\n\u03a6(PCi)-representing cluster C\u012f in the input space-having the boundaries as stated in Definition 1. An illustration is\nshown in Figure 2.\nThe primary advantage of representing each cluster as a pdf is that the clusters can be any arbitrary shapes, sizes\nand densities."}, {"title": "4 Clustering based on Distributional Kernel K", "content": "In this section, we provide a new clustering that is native to both centralized and distributed clusterings. The above\ndefinition prompts us to design a centralized clustering framework that has three steps. Step 1 simply samples a subset\nB from a given dataset D. Step 2 finds some initial clusters $G_i\u2282 B$ which are good representatives of the true clusters,"}, {"title": "5 Properties of KDC", "content": "We provide the three properties of KDC in this section.\nThe three properties of KDC are described formally as follows:\nDEFINITION 2. Distributed Clustering Framework \u00c4 and its centralized counterpart A, with any clustering algorithm f,\nhave the following three properties, irrespective of the data sizes and cluster distributions on different sites in a distributed\nnetwork of r sites and a coordinator:"}, {"title": "5.1 Distributed-Centralized Clustering Equivalence", "content": "The proposed two frameworks have the distributed-centralized clustering equivalence property when they use the\nsame subset to perform the clustering in step 2. This yields, Vi, $C_i = \u222a_l C_i^l$."}, {"title": "6 Proposed Kernel-Bounded Cluster Cores", "content": "Here we propose to use a new clustering algorithm called Kernel Bounded Cluster Cores (KBCC) as f in A and \u00c4.\nDEFINITION 3. Given a dataset D, \u03baBCC produces the k largest K\u03c4-cluster cores G, each encapsulates the 'core' of a\ncluster, defined based on a kernel \u03ba with a threshold \u03c4, as follows:\nG = {x, y \u2208 D | there exists a chain: z1, z2, \u2026\u2026\u2026, zj, such that z\u2081 = x, zj = y, \u2200i \u03ba(zi, zi+1) > \u03c4}\nIntuitively, KDC first samples a subset from the original data, the distribution of which is very close to the distribution\nof the original data. Then KBCC is used to find G, which contains chains of core points with very high similarity. These"}, {"title": "6.1 Determinants of KDC Property (c)", "content": "Figure 4 (last column) shows the example clustering outcomes as a result of using KBCC in step 2 in either of the two\nproposed Frameworks A and \u00c4.\nAs step 2 in the Frameworks admits any clustering algorithm, Figure 4 also shows the example clustering outcomes\nof using either k-means or DP [33] in step 2. While they enable perfect clustering outcomes on the simple Jain dataset,\nthey fail to do so on the Complex9 dataset.\nThe examples in Figures 3 and 4 show that there are two determinants in discovering clusters of arbitrary shapes,\nvarying data sizes and densities in order to have property (c) of KDC:\n\u2022 Initial clusters Gi that represent the core of the true clusters, and\n\u2022 The point assignment procedure that enables clusters of arbitrary shapes, sizes and densities to be found:\n$C_j = {x \u2208 D | argmin_{i\u2208 [1,k]} K(\u03b4(x), P_{G_i}) = j}$\nThey are in steps 2 and 3 of the two proposed Frameworks.\nNote that replacing step 3 with the typical center-based point assignment of k-means disables this capability, regardless\nof how good the initial clusters are, as shown in Figure 3.\nIn a nutshell, step 3 is the enabling determinant in satisfying property (c), and step 2 is the supporting determinant.\nGetting a sufficient sample size in step 1 is a necessary factor too."}, {"title": "6.2 A Comparison of KBCC and SOTA Algorithm", "content": "Recall that coreset-based k-means is the SOTA distributed clustering method of B.\nKBCC has two key advantages in either A or \u00c4 over the coreset-based B:\n(1) The subset B in step 1 of either Framework is simply a random subset of S (or $\u222a_l D_l$), rather than a coreset. The\nlatter requires a computationally expensive process and it is tightly coupled with the clustering algorithm used.\nA random subset needs none of those.\n(2) The initial k representatives of KBCC are determined via similarity-based clustering. At the end of step 2, the\nsummarized centers of these k high-similarity clusters (via kernel mean embedding [28]) are broadcast to each\nsite to assign points to the most similar distribution in one iteration independently. As a result, the clusters are\nnot restricted to globular shape, equal size and equal density-the only type of clusters that can be produced by\nk-means in Framework B.\nPut in another way, a coreset is required only if (kernel) k-means is used in either A or \u00c4 because it stabilizes\nthe clustering outcome. A random subset, instead of a coreset, will produce a wildly different clustering outcome"}, {"title": "7 Experimental Designs and Settings", "content": "The experiments are designed with the following aims:\n(1) Compare the relative performance of Frameworks \u00c4 and B in terms of NMI (normalized mutual information\n[40]), AMI (Adjusted Mutual Information [39]), F1 [9], ARI (Adjusted Rand Index [36]) and runtime\u00b2.\n(2) Verify property (b) of Framework \u00c4 or KDC.\n(3) Investigate \u00c4 as a generic framework that enables any quadratic time clustering algorithms to deal with large\ndatasets.\n(4) Examine the relative performance of four methods of centralized clustering.\nSpecifically, the proposed \u03baBCC\u00b3 and kernel k-means are used as f in \u00c4. In B, coreset-based k-means [6] is chosen\nas the representative algorithm for two reasons. First, coreset-based k-means is the only algorithm in B that satisfies the\nproperty (a) with a theoretical guarantee. Second, coreset-based k-means is the only algorithm in B with a deterministic\nconstant communication cost, which is the same as \u00c4. In addition, we also implemented a kernel version of coreset-based\nk-means for comparison.\nThe empirical evaluation is conducted using seven datasets from https://archive.ics.uci.edu/.\nExperimental details. For each dataset, we first simulate a communication network connecting r local sites as\nconducted by previous work [6], and then partition the dataset into local data subsets. If not explicitly stated, the data\nsizes at all sites are evenly distributed. In the experiments, r=20 sites and the subset data size at the end of step 1 is\ns=min(n, 10000). All data are normalized to the range [0, 1] in the pre-processing.\nUnless otherwise specified, the kernel used in a kernel-based clustering algorithm is Isolation kernel [38].\nThe experiments were executed on a Linux CPU machine: AMD 128-core CPU with each core running at 2 GHz and\n1T GB RAM. For each method, we report the average result of five trials.\nThe results of the four experiments on distributed clustering, which correspond to the above four aims, are reported\nin the next section, and the evaluation results of four methods of centralized clustering (the fifth aim) are presented in\nSection 9."}, {"title": "8.3 KDC Enables a Quadratic Time Clustering Algorithm to Deal with Large Datasets", "content": "Here we show that \u00c4 can be used as a general framework that enables a quadratic time clustering algorithm to deal\nwith datasets, that would otherwise be impossible.\nDBSCAN and Density Peak (DP) are the two most famous density-based algorithms that can find arbitrary-shaped\nclusters. We compare with DP here because DBSCAN often performs worse than DP [1, 2, 46].\nA recent DP parallel clustering algorithm is Ex-DPC++6[1, 2], which reduces the time complexity of DP to sub-\nquadratic ($O(n^{2-1/d} + n^{1.5}logn)$), More importantly, Ex-DPC++ is an exact algorithm that has some performance\nguarantee.\nWe employ DP [33] as f in step 2 in \u00c4, where DP is a quadratic time centralized clustering algorithm. We call the\nresultant distributed version of DP as \u00c4-DP."}, {"title": "9 Relative Performance of Centralized Clustering Algorithms", "content": "Every method of distributed clustering aims to achieve the clustering outcome of its centralized counterpart. As we\nhave analyzed above in Section 5.1 and Table 3, only two methods can achieve this aim, i.e., the proposed Framework\nA-KBCC and the existing Framework B which employs coreset [6]. Thus, it is important to know the performance of a\ncentralized clustering algorithm before attempting to create its distributed counterpart. An algorithm that produces a\npoor clustering outcome has little practical value."}, {"title": "10 Discussion", "content": ""}, {"title": "10.1 Relation to Kernel k-means", "content": "Kernel k-means clustering is an elegant way to enable k-means clustering to find clusters of arbitrary shapes. However,\nthe time complexity is increased substantially to quadratic [41].\nOne way to produce a distributed version of kernel k-means clustering has been suggested [29, 41] via the Nystr\u00f6m\napproximation and dimensionality reduction in order to find a low dimensional feature space. Each of these two"}, {"title": "10.2 \u00c4 is not an Extension of B", "content": "Our proposed Framework \u00c4 is not an extension of Framework B for three reasons. First, B is specific to k-means\nclustering only; but the proposed \u00c4 is a generic framework that is applicable to any clustering algorithm. Second,\nFramework B applies center-based point assignment in step 3, but Framework \u00c4 applies a more powerful distribution-\nbased point assignment. Third, Framework \u00c4 has none of the three fundamental limitations of Framework B, as stated\nin Section 1.\nNo amount of modifications to Framework B could rectify its fundamental limitations, as a result of using k-means\nclustering. The proposed Framework \u00c4 is applicable to a much wider application scope than the two existing approaches,\nnot just Framework B, because it has none of the limitations of these two approaches (stated in Section 2)."}, {"title": "10.3 The Impact of Unbalanced Data Sizes on Local Sites", "content": "Many methods of distributed clustering work only if the data sizes at local sites are approximately the same. Otherwise,\nthe clustering outcomes and/or the runtime saving is severely impacted.\nImpact on clustering outcomes.\nThis impact is well documented. Two examples are given below:\n\u2022 LDSDC [17] provides the relationship between its algorithm and the number of sites. The algorithm is sensitive\nto the number of sites, and the quality of the clustering outcome degrades as the number of sites increases.\n\u2022 Both DBDC and S-DBDC [21, 22] usually have difficulty obtaining satisfactory parameter settings when the data\nsizes are not balanced over all sites.\nImpact on runtime. The methods which do not satisfy property (b) increase their runtime significantly due to the\nunbalanced data sizes at different sites.\nOur evaluation result is shown in Figure 10. The runtimes of \u00c4-KBCC and \u00c4-kkm, which satisfy property (b), are\nnot impacted by the changing data sizes. But the runtime of B-kkm, which does not satisfy property (b), increases\nsignificantly as the data size increases from 0.1 to 0.2 and 0.5 of the total data size. LDSDC [17] and LSH-DDP [45] are\nimpacted in the same way."}, {"title": "11 Concluding Remarks", "content": "Current methods of distributed clustering focus on distributed computing of an existing centralized clustering, and\npay little attention on its clustering quality, knowing that the best they can achieve is to approximate the clustering\noutcome of the centralized clustering.\nIn contrast, we emphasize on a clustering outcome which produces clusters of arbitrary shapes, sizes and densities,\nand we design a new clustering which is native to both centralized clustering (A) and distributed clustering named\nKDC (\u00c4).\nKDC makes three breakthroughs in distributed clustering. First, it is the first linear-time and distributional kernel K\nbased clustering that has three properties. Out of many existing methods of distributed clustering, only the coreset-based\nframework possesses one out of the three properties.\nSecond, the proposed use of K in step 3 and the proposed clustering algorithm KBCC in step 2 of the 3-step\nframework contribute directly to the improved clustering outcomes in comparison with existing methods. The margin\nof improvement is large and significant.\nThird, KDC is the only generic framework that directly enables any quadratic-time clustering algorithm to deal with\nlarge datasets. Existing approaches are tailored made for a specific clustering algorithm only. KDC has the ability to\nincorporate any clustering algorithm because it requires no parallelization of a clustering algorithm, unlike the second\napproach (which requires parallelization) and the first approach (which tailored for k-means only though requiring no\nparallelization) mentioned in Section 2."}]}