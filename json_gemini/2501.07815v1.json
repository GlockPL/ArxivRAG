{"title": "Agent-Centric Projection of Prompting Techniques and Implications for Synthetic Training Data for Large Language Models", "authors": ["Dhruv Dhamanida", "Mary Lou Maher"], "abstract": "Recent advances in prompting techniques and multi-agent systems for Large Language Models (LLMs) have produced increasingly complex approaches. However, we lack a framework for characterizing and comparing prompting techniques or understanding their relationship to multi-agent LLM systems. This position paper introduces and explains the concepts of linear contexts (a single, continuous sequence of interactions) and non-linear contexts (branching or multi-path) in LLM systems. These concepts enable the development of an agent-centric projection of prompting techniques, a framework that can reveal deep connections between prompting strategies and multi-agent systems. We propose three conjectures based on this framework: (1) results from non-linear prompting techniques can predict outcomes in equivalent multi-agent systems, (2) multi-agent system architectures can be replicated through single-LLM prompting techniques that simulate equivalent interaction patterns, and (3) these equivalences suggest novel approaches for generating synthetic training data. We argue that this perspective enables systematic cross-pollination of research findings between prompting and multi-agent domains, while providing new directions for improving both the design and training of future LLM systems.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) are a recent development in Generative Artificial Intelligence that can mimic human-like behavior (Park et al., 2023), especially in conversations (Cai et al., 2023). LLMs have also shown a kind of general intelligence (Radford et al., 2019; Yogatama et al., 2019). Central to harnessing the capabilities of LLMs is the concept of prompting, a strategy that significantly influences task performance by instructing LLMs in specific ways (Chen et al., 2023).\nHYPOTHESIS and Goals: In this position paper, we hypothesize that viewing prompting techniques through a proposed agent-centric lens can help uncover structural equivalences between single-LLM prompting and multi-agent approaches. Our goal is to (1) introduce a unified framework for comparing these techniques, (2) develop and examine conjectures about their relationship, and (3) outline how this perspective can inform the generation of synthetic training data.\nConsider a simple math problem. When we directly prompt an LLM, \"What is 13 \u00d7 27?\", we might receive a single numeric answer. However, when we ask, \"Let's solve this step by step: what is 13 \u00d7 27?\", we explicitly prompt for intermediate reasoning plus the final result (Kojima et al., 2023; Yu et al., 2023). While both prompts seek the same final answer, are they both still the same problem if one has a different \"correct\" answer?\nAnother approach to improving end-task performance when using LLMs has been to incorporate \u201creasoning\u201d (OpenAI, 2024). The model outputs arbitrarily long \u201creasoning traces\u201d before responding to the prompts. These traces are sequences of natural language statements like \"Let's first understand the input and output formats\". OpenAI 01 is a single large language model or agent.\nIf we simply added role identifiers before each statement \"Analyst: Let's first understand the in-put and output formats\" would it suddenly qualify as a multi-agent system?\nWhat about approaches where an LLM analyzes problems from multiple perspectives in separate con-versations before merging all perspectives together in another conversation (Saha et al., 2023)? Is each con-versation a different \"agent\" performing a subtask? Is this a multi-agent system?\nWe argue that these questions can be systemat-ically addressed by viewing prompting techniques through an agent-centric lens. By developing the concepts of linear and non-linear contexts in LLM systems, we shed light on possible connections be-tween single-LLM prompting techniques and multi-agent systems. We discuss implications for the future of LLM systems, from enabling cross-pollination of research findings between prompting techniques and multi-agent systems to suggesting novel approaches for generating synthetic training data that could en-hance capabilities in both domains.\nTo develop this argument, we first establish foun-dational definitions and examine previous work on prompting techniques and task-oriented LLM systems (\u00a7 2). Building on these foundations, we present our framework for agent-centric projection and explore its implications for both system design and training (\u00a7 3). We conclude by discussing the larger impact of this perspective on future research in LLM systems (\u00a7 4)."}, {"title": "DEFINITIONS AND PRIOR WORK", "content": "A task-oriented LLM system is a Large Language Model (LLM) system configured to perform specific tasks, rather than open-ended conversations\u00b9. Such systems have shown promise in complex tasks such as software development (Hong et al., 2023), where the system must manage multiple rounds of interaction, maintain context throughout iterations, and often col-laborate with other systems or agents to complete the task.\nWe begin by defining a minimal task-oriented LLM system (\u00a7 2.1), with particular attention to how such systems manage context across multiple interac-tions. We then examine prompting techniques (\u00a7 2.2), focusing on how different approaches to prompting lead to different patterns of context creation and man-agement. These patterns form the basis for our novel concepts of linear and non-linear contexts, which en-able an agent-centric projection of prompting tech-niques."}, {"title": "Minimal Task-oriented LLM System", "content": "A minimal task-oriented LLM system is a minimal LLM system that can be instructed to solve tasks. Thus, we start by defining a minimal LLM system.\nLarge Language Models are auto-regressive mod-els that accept input tokens and use them as history (often referred to as context), to compute probabili-ties of all tokens in their vocabulary as the next token. We can sample from this probability distribution us-ing a sampling/decoding algorithm to generate text. This process is then repeated until the LLM predicts a special token, or a special sequence of tokens, that marks the end of the text (Feuerriegel et al., 2023)2.\nWe call this a bare-bones LLM system (see Fig-ure 1) as it contains the minimal components needed for text generation, without additional components to help with context management. Every time an LLM is prompted with context Cn, it generates a response Rn that would need to be stored in context Cn+1 for the next prompt, assuming multiple rounds of instruction and response generation are required.\nFor systems oriented towards solving even mod-erately complex tasks, context management becomes quickly cumbersome. For example, in Saha et al., 2023, the authors describe a system in which m branches are created from a LLM response to a prompt Cn, producing a set of m responses r =\n{Rn1, Rn2,..., Rnm}. Then, they take all of r, and transform it into a prompt Cn+1, in which they instruct the LLM to merge all responses in r into a single re-sponse Rn+1, which potentially needs to be stored in context Cn+2 for the next prompt. A similarly com-plex system is described in Ning et al., 2023, and we will examine more examples in our discussion of prompting techniques.\nIf we define a minimal LLM system without de-scribing how context is managed, it would be too dif-ficult to compare different systems and apply learn-ings from one researched system to another. Because of this, we include a description of a minimal con-text management subsystem within our definition of a minimal task-oriented LLM system.\nSpecifically, we include a context store CS. Ini-tially, the context store CS is empty and the first time an LLM is provided with context C\u2081 to generate R1, both the prompt and the response are permanently ap-pended to CS. For all future requests, the LLM is first provided with a sliding window of content from CS as context, to which it appends the prompt Cn to gener-ate Rn. Once the response Rn is generated, both the prompt and the response are permanently appended to CS. The model then closely matches messaging: the context store CS acts as chat history, users send messages, and the LLM responds."}, {"title": "Prompting Techniques", "content": "Prompting refers to the act of constructing and pro-viding input text (a prompt) to an LLM. In the con-text of task-oriented LLM systems, prompt engineer-ing can be defined as iteratively creating and adapting a prompt for a given LLM and task pair.\nThe way an LLM is prompted significantly affects task performance (Nori et al., 2023; Savage et al., 2023). There are many surprising results in this area, such as letting an LLM know that solving a task \"is very important to my career\" can improve task per-formance (Li et al., 2023).\nSuch results can be explained by research such as Hendel et al., 2023, which shows that in-context learning creates task vectors or representations within the LLM that increase the probability of correct task completion. Other research has shown that it is pos-sible to \"search\" for prompts that are more likely to lead to success, analogous to finding task vectors that are more likely to lead to success. In Zou et al., 2023, the authors were able to procedurally find adversar-ial prefixes, which, when added to prompts, result in LLMs breaking their alignment and engaging in un-safe behavior.\nAll of these are examples of modifying the prompt without changing the actual task/problem definition, to make the successful completion of the intended task more likely. However, researchers are prone to modifying the prompts in a manner that changes the task, rather than modifying the prompts in a manner that improves task performance.\nFor example, when using Chain-of-Thought prompting (Wei et al., 2023)\u00b3, or when asking LLMs to think step-by-step (Kojima et al., 2023) \u2013 the task meaningfully changes. It goes from instructing LLMs to give me an answer now to asking it to first plan out a solution, and then share an answer. This is a differ-ent task being solved, even though the final deliver-able (the answer) is the same. It should be a given that LLMs have different capabilities for different tasks.\nThis is not to say that we shouldn't instead solve equivalent tasks that LLMs are more suitable to, but that it is problematic to have prompt modification (that leaves instructions/task definition intact) to in-struction modification in the same category. Thus, we make the distinction between prompt engineering and instruction engineering:\n\u2022 PROMPT ENGINEERING: The act of modify-ing the prompt without changing the actual task/problem definition or adding relevant knowl-edge/information, to make the successful comple-tion of the intended task more likely. We restrict the addition of relevant knowledge/information to LLM augmentation to avoid an overlap.\n\u2022 INSTRUCTION ENGINEERING: The act of mod-ifying the prompt in a manner that changes the task/problem to an equivalent task/problem that the LLMs are more suitable for, such that the final deliverable (the answer) is the same."}, {"title": "FRAMEWORK AND CONJECTURES", "content": "In research and practice, LLM systems exhibit differ-ent patterns in how they manage context and generate responses. We argue that these patterns can be under-stood through a theoretical framework that connects prompting techniques with multi-agent systems, re-vealing opportunities for improving both system de-sign and training. In this section, we first introduce a formal categorization of context management patterns in LLM systems (\u00a7 3.1). Building on this foundation, we develop an agent-centric projection of prompt-ing techniques (\u00a7 3.2) that reveals deep connections between seemingly disparate areas. Finally, we ex-plore how this unified perspective suggests novel ap-proaches to synthetic training data generation (\u00a7 3.3), with potentially far-reaching implications for improv-ing LLM capabilities."}, {"title": "Linear and Non-linear Context in LLM Systems", "content": "To formally characterize how task-oriented LLM sys-tems manage context and generate responses, we de-velop a framework based on message flow patterns. Building on the minimal task-oriented LLM system concept (\u00a7 2.1), we analyze how the context store maintains sequences of messages M = {(Cn, Rn)}=1, where each response Rn is generated using all previ-ous context-response pairs.\nUsing this foundation, we propose a method for classifying prompting techniques and their resulting task-oriented LLM systems into two categories based on their context management patterns.\nPROMPTING TECHNIQUES WITH LINEAR CON-TEXT where there exists exactly one continuous sequence of messages M = {(Cn,Rn)}=1 that con-tains all generated messages and input contexts in the correct chronological order.\nAll Input-Output and Input-Output with additional steps techniques (as described in \u00a7 2.2) can be classi-fied as having a linear context, as they all involve a single continuous sequence of messages.\nFor example, consider Self-Refine Madaan et al., 2023, where each response is iteratively refined using all previous context-response pairs until a stop condi-tion is met.\nPROMPTING TECHNIQUES WITH NON-LINEAR CONTEXT where there cannot always be one con-tinuous sequence of messages that contains all input context and generated messages in the correct chrono-logical order. Instead, there can be multiple branches of conversation possible, each with its own continu-ous sequence of messages {M1,M2,...,Mn}.\nAll single input-many output, input with non-linear intermediary steps, tree of thought, and graph of thought techniques, (as described in \u00a7 2.2) can be classified as having a non-linear context, as they all potentially involve sequences of conversation M.\nFor example, consider a simplified version of BRANCH-SOLVE-MERGE, first described in Saha et al., 2023, and as visualized in Figure 3. The fig-ure depicts a task-oriented LLM system that helps the user (the human) make decisions. First, the human first instructs the system to make a decision. The sys-tem uses the instructions to create an input context for an LLM (context C\u2081) and uses it to generate a re-sponse R1. R1 is then used to create two new prompts (via an algorithmic transformation depicted in the fig-ure as ), one in which the LLM is tasked with re-flecting on the drawbacks of this decision (in context C2) and another where the LLM is tasked with reflect-ing on the benefits of this decision (in context C3). Finally, another prompt is created where both reflec-tions (responses R2 and R3) are considered (using an-other algorithmic transformation 2) to create a new prompt (context C4) which is used to generate a final decision within response R4. R4 is then reported to the user as the final decision.\nAs long as any task-oriented system is using LLMs, it will always have one or more continuous streams of messages M as described. This means that all task-oriented LLM systems and all prompting techniques can be classified as having either linear or non-linear contexts.\nThis fundamental dichotomy between linear and non-linear contexts provides a powerful lens through which to analyze LLM systems. As we will show in the next section, it reveals surprising connections be-tween prompting techniques and multi-agent systems that can inform both system design and training ap-proaches."}, {"title": "Agent-Centric Projection of Prompting Techniques", "content": "In the previous section (\u00a7 3.1), we classify all prompt-ing techniques and all resulting task-oriented LLM systems they bore into either having a linear or non-linear context. This decision and the overall definition have the following implications:\n\u2022 Research on techniques for reliable, task-oriented text generation that involve linear context can be modeled as a kind of two-agent system (the hu-man instructing the LLM being the second agent, as we also see in Xi et al., 2023; Wu et al., 2023).\n\u2022 Research on techniques for reliable, task-oriented text generation that involves non-linear context can be modeled to be a kind of multi-agent sys-tem, where each \"branch\" of conversation M can be considered to have occurred with a different agent.\nFor example, In Figure 4, we show how the prompting technique from Figure 3 can be mod-eled as a multi-agent system. Each continuous linear sequence of messages M1 = {C1, R1}, M2 =\n{C2,R2}, M3 = {C3,R3}, and M4 = {C4,R4} can be considered to have occurred with a different agent. Using this approach, we can model any prompting technique with non-linear context as a multi-agent system.\nIt would also help to note that each continuous sequence of messages Mn in Figure 4 is essen-tially a minimal task-oriented LLM system, as de-scribed in \u00a7 2.1. This means that we can substitute each such minimal system with a more compli-cated task-oriented system if needed.\nIn Figure 5, we show a more realistic example of a multi-agent system, designed to replicate the behavior of the prompting technique in Figure 3. Here, the major changes are that the agents com-municate with each other using tools, meaning all communication is bidirectional (say, if an agent wants to ask a clarifying question) and that the algorithmic transformations and are now present each as a tool available to Agents A\u2081 and A4 respectively. This system may behave exactly like the system in Figure 3 most of the time, but may prove to be more resilient to unexpected cir-cumstances, as each component is more \"intelligent\".\nAs all prompting techniques can be projected to such multi-agent systems, we can conjecture that:\nConjecture 1. Results from prompting techniques in-volving non-linear context can predict similar results from multi-agent systems designed to replicate the same behavior.\nThis projection or view allows us to generalize all such techniques and apply learnings from one tech-nique to another, and even learnings from multi-agent systems. For example, if new LLM-based multi-agent collaboration research shows that \"A Process Super-vising Agent is all you Need\", then we can imme-diately apply that result to the prompting technique described in BRANCH-SOLVE-MERGE from Saha et al., 2023 by viewing the \u201cprocess supervising agent\" work via a non-linear context lens as illustrated in Figure 3, and then adding the BRANCH-SOLVE-MERGE \"nodes and connections\".\nBut it does not end there - because of how flex-ible natural language is, all non-linear contexts can also be projected to linear contexts. For example, say four agents engage in adversarial interaction as de-scribed in Xi et al., 2023 (\u00a7 4.2.2), where they argue about a decision until they reach a consensus. The benefit of this interaction paradigm is that each agent can be instructed to look at the problem from various perspectives.\nThis interaction can be elicited within a linear context, where the LLM is prompted with the same decision-making problem but with additional instruc-tions to share a turn-by-turn dialogue where four in-dividuals argue about the decision until they reach a consensus. This has been demonstrated in Wang et al., 2024, where a single LLM instance is prompted to produce a transcript of multiple personas (agents) interacting with each other to solve a task. The au-thors call this \"Solo Performance Prompting\". Their results show that this technique-essentially convert-ing non-linear context (multiple agents collaborat-ing) into linear context (a dialogue transcript)\u2014shows performance gains comparable to those achieved by multi-agent systems on other tasks (Wang et al., 2024 does not directly compare to multi-agent systems).\nIn Dong et al., 2024 the authors describe a sim-ilar approach minus the \"dialogue\", where multiple roles (analyst, coder, tester, etc.) are simulated by a single LLM with linear context (Dong et al., 2024) (\u00a72.2, Eq. 1). The paper shows how this approach outperforms baseline and advanced prompting tech-niques (such as CoT).\nConjecture 2. Performance improvements achieved through multi-agent system architectures can be at least partially replicated using single-LLM prompt-ing techniques that simulate equivalent multi-agent interaction patterns within a linear context."}, {"title": "Implications for Synthetic Training Data", "content": "Recent work has demonstrated that synthetic data can effectively enhance model capabilities in vari-ous applications, from structured information extrac-tion (Josifoski et al., 2023) to visual question answer-ing (Su et al., 2024).\nA key insight stems from an apparent paradox in LLM systems: while all LLMs are trained on \"linear context\u201d (sequential text), research and prac-tice show that \u201cnon-linear context\" approaches-such as advanced prompting techniques and multi-agent interactions are of significant interest and demon-strate superior task performance (Saha et al., 2023; Ning et al., 2023; Wei et al., 2023; Hong et al., 2023; Wu et al., 2023).\nThe previous subsection presents an argument for how techniques involving non-linear context can be projected to an equivalent technique utilizing linear context. This can have profound implications when you consider that all LLMs are trained on \u201clinear context\", i.e., trained on continuous sequences of text. If intermediate steps from advanced prompt-ing techniques like BRANCH-SOLVE-MERGE are projected to linear contexts similar to Solo Perfor-mance Prompting (Wang et al., 2024) and Self-Collaboration (Dong et al., 2024) then they can also be used as synthetic training data.\nInterestingly, a recent approach called Stream of Search (SoS) (Gandhi et al., 2024) further under-scores our perspective on using non-linear or subop-timal reasoning traces for training. SoS demonstrates that when LLMs are trained on branching, backtrack-ing search trajectories\u2014serialized into a linear textual format-they acquire stronger problem-solving capa-bilities and can even discover new strategies. These findings support Conjecture 3 below, illustrating how self-generated,"}, {"title": "CONCLUSIONS", "content": "Our Core Arguments\n\u2022 PROMPT ENGINEERING AND INSTRUCTION ENGINEERING: Clearly differentiating the adjust-ment of the prompt without altering the actual task or the definition of the problem (prompt engineer-ing) and modifying the task to an equivalent task more suitable for LLM systems (instruction en-gineering) is essential to precise communication and understanding of research in this area.\n\u2022 LINEAR AND NON-LINEAR CONTEXT: Prompt-ing techniques and resulting task-oriented LLM systems can be classified into having either linear or non-linear context.\n\u2022 AGENT-CENTRIC PROJECTION OF PROMPTING TECHNIQUES: We demonstrate approaches that allow prompting techniques with non-linear con-text to be understood as multi-agent systems and vice versa. This projection provides a frame-work for analyzing, comparing, and improving both prompting techniques and multi-agent sys-tem architectures.\nImplications for Future Research\n\u2022 CROSS-POLLINATION IN PROMPTING AND LLM-BASED MULTI-AGENT SYSTEMS: The agent-centric projection of prompting techniques may allow us to cross-pollinate research findings in these areas.\n\u2022 SYNTHETIC TRAINING DATA GENERATION: Our core arguments suggest two novel approaches for generating high-quality synthetic training data for LLMs: (1) converting successful non-linear prompting traces into linear training data and (2) augmenting real-world task traces with synthetic agent collaboration artifacts. These approaches could provide structured, high-quality data specif-ically suited for training LLMs in multi-agent and complex reasoning tasks.\n\u2022 REAL-WORLD APPLICATIONS AND ETHICAL CONSIDERATIONS: As these systems become more capable, their deployment in real-world sce-narios becomes more feasible. With this comes the need for rigorous ethical considerations, es-pecially concerning autonomy, decision making, and human-AI interaction.\nBy establishing the fundamental distinction be-tween linear and non-linear contexts in prompting techniques, and using this to develop an agent-centric projection that reveals deep connections between prompting techniques and multi-agent systems. This framework leads to three key conjectures about the re-lationship between prompting techniques and multi-agent systems, suggesting that results from one do-main can inform the other. Furthermore, we demon-strate how this unified perspective opens up novel ap-proaches to synthetic training data generation, both through the conversion of non-linear prompting traces and through the augmentation of real-world task traces. Our position highlights the untapped potential in viewing prompting techniques through an agent-centric lens, providing concrete directions for improv-ing both the design and training of future LLM sys-tems."}]}