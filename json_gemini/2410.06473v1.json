{"title": "GROUNDING ROBOT POLICIES WITH VISUOMOTOR LANGUAGE GUIDANCE", "authors": ["Arthur Bucker", "Pablo Ortega", "Jonathan Francis", "Jean Oh"], "abstract": "Recent advances in the fields of natural language processing and computer vision have shown great potential in understanding the underlying dynamics of the world from large-scale internet data. However, translating this knowledge into robotic systems remains an open challenge, given the scarcity of human-robot interactions and the lack of large-scale datasets of real-world robotic data. Previous robot learning approaches such as behavior cloning and reinforcement learning have shown great capabilities in learning robotic skills from human demonstrations or from scratch in specific environments. However, these approaches often require task-specific demonstrations or designing complex simulation environments, which limits the development of generalizable and robust policies for new settings. Aiming to address these limitations, we propose an agent-based framework for grounding robot policies to the current context, considering the constraints of a current robot and its environment using visuomotor-grounded language guidance. The proposed framework is composed of a set of conversational agents designed for specific roles namely, high-level advisor, visual grounding, monitoring, and robotic agents. Given a base policy, the agents collectively generate guidance at run time to shift the action distribution of the base policy towards more desirable future states. We demonstrate that our approach can effectively guide manipulation policies to achieve significantly higher success rates both in simulation and in real-world experiments without the need for additional human demonstrations or extensive exploration.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the advent of foundation models, such as large-scale pre-trained language models (LLMs) and visual language models (VLMs), has shown great capabilities in understanding context, scenes, and the underlying dynamics of the world. Furthermore, emergent capabilities such as in-context learning have shown great potential in the transfer of knowledge between domains, e.g., via few-shot demonstrations or zero-shot inference. However, the application of these models to robotics is still limited, given the intrinsic complexity and scarcity of human-robot interactions and the lack of large-scale datasets of human-annotated data or demonstrations.\nMost approaches that target using LLMs and VLMs in robotics often fall in one of two directions. Fine-tuning the models (Brohan et al., 2023; Ahn et al., 2022), which have shown great real-world capabilities, at the expense of large platform-dependent datasets. Even though several efforts try to overcome the data availability problem (Collaboration et al., 2023), the scarcity of robots and the broad range of robotics skills, make this process extremely taxing. A second line of work relies on using code as an interface between the language models and the robotic systems (Liang et al., 2023; Vemprala et al., 2023), which although leverages the skilled coding capabilities of this type of model, is highly dependent on handcrafted functions to interface the platform actions and perception.\nUnsupervised approaches in robotics have shown great progress in exploring large state spaces and learning common sets of behaviors in complex environments. However, most of these approaches"}, {"title": "2 RELATED WORK", "content": "Vision-Language Models for Robot Learning: Several works explore the notion of leveraging pre-trained or fine-tuned Large Language Models (LLMs) and/or Vision-Language Models (VLMs) for high-level reasoning and planning in robotics tasks (Hu et al., 2023; Ahn et al., 2022; Liang et al., 2023; Huang et al., 2022; Singh et al., 2023; Huang et al., 2023; Ha et al., 2023; Ding et al., 2023; Micha\u0142 et al., 2024; Ma et al., 2023; Li et al., 2024; Mu et al., 2024)\u2014typically decomposing high-level task specification into a series of smaller steps or action primitives, using system prompts or in-context examples to enable powerful chain-of-thought reasoning techniques. This strategy of encouraging models to reason in a stepwise manner before outputting a final answer has led to sig-"}, {"title": "3 GROUNDING ROBOT POLICIES WITH GUIDANCE", "content": "We consider a pre-trained stochastic policy $\\pi : \\mathcal{O} \\times \\mathcal{S} \\rightarrow \\mathcal{A}$ that maps observations of and robotic states $s_t$ to action distributions $\\alpha_{\\pi,t}$, at each time step t. Our objective is to generate a guidance distri-"}, {"title": "3.1 PROBLEM FORMULATION", "content": "bution $g_t$ that, when combined with this base policy, enhances overall performance during inference without requiring additional human demonstrations or extensive exploration procedures. Specifically, we aim to develop a modified policy $\\pi_{\\text{guided}} : \\mathcal{O} \\times \\mathcal{S} \\rightarrow \\mathcal{A}$ that achieves better performance on tasks where the original policy $\\pi$ struggles. We define this new policy as follows:\n$\\pi_{\\text{guided}}(\\alpha_{g,t} | O_t, S_t) = \\pi(\\alpha_{\\pi,t} | O_t, S_t) * G(\\alpha_{\\pi,t} | o_t, S_{t+1}),$ (1)\nwhere $G : \\mathcal{A} \\times \\mathcal{O} \\times \\mathcal{S} \\rightarrow [0, 1]$ is a guidance function that maps observation $o_t$, action $a_t$, possible future state $s_{t+1}$ into a guidance score $g_t$. The \u2018*' operator here denotes the operation of combining both distributions conceptually, which we explore in detail in Section 3.4. For the scope of this project, we assume that a dynamics model $D : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$ is available, which can forecast possible future states of the robot $s_{t+1} = D(s_t, \\alpha_{\\pi,t})$ given the current state $s_t$ and action $\\alpha_{\\pi,t}$.\nFocusing on leveraging the world knowledge of Vision Language Models, while avoiding adding latency to the action loop, we choose to express these guidance functions as Python code. By integrating these code snippets into action loop of the base policies, we eliminate the need of time-consuming queries to large reasoning models. Samples of the format and content of the guidance functions generated by the framework are presented in the Appendix A.2."}, {"title": "3.2 A MULTI-AGENT GUIDANCE FRAMEWORK FOR SELF IMPROVEMENT", "content": "In order to generate the guidance function $G$, we leverage a group of conversational agents empowered with visual grounding capabilities and tool usage. Illustrated in Figure 2, the framework is composed of four main agents: an Advisor Agent, the Grounding Agent, the Monitor Agent, and the Robotic Agent. We provide system prompt samples in Appendix A.1.\nAdvisor Agent. A Vision Language Model is responsible for breaking down the task and communicating with the other agents to generate a sound guidance function for a given task.\nGrounding Agent. A Vision Language Model that iteratively queries the free-form text segmentation models to locate (Cheng et al., 2023a; Liu et al., 2023a), track (Cheng et al., 2023b), and describe elements relevant to the task execution.\nMonitor Agent. Responsible for identifying the causes of the failures in the unsuccessful rollouts, the Monitor Agent consists of a Vision Language model equipped with a key frame extractor.\nRobotic Agent. Language Model equipped with descriptions of the robot platform, a robot's dynamics model and wrapper functions for integration with the base policy. It criticizes the provided guidance functions to reinforce its relevance to the task and alignment with the robot's capabilities."}, {"title": "3.3 GUIDANCE PROCEDURE", "content": "The conversational agents interact with each other through natural language and query their underlying tools to iteratively produce a guidance code tailored to the task in hand, the environment, and the robot's capabilities. The information flow between these agents is depicted by Figure 2.\nFor a given task expressed in natural language and an image of the initial state of the environment, the Advisor Agent uses Chain-of-Tought (Wei et al., 2023) strategy to generate a high-level plan of the steps necessary to accomplish the task. Being able to query a Grounding Agent and the Robotic agent, the Advisor is able to collect relevant information about trackable objects and elements in the environment, as well as the capabilities and limitations of the robotic platform.\nFor a given plan and list of relevant objects required for the task completion, the Grounding Agent uses grounding Dino (Liu et al., 2023a) and the Segment Anything Model (SAM) (Cheng et al., 2023a) to locate the elements across multiple granularities and levels of abstraction. For instance, if an object is not immediately found, the agent will actively look for semantically similar objects or will look for higher-level elements that could encompass the missing object. For example, if the object \"cup\" is to be located, and it could not be immediately found, the agent could search for similar object like a \u201cmug\u201d. If it still struggles to locate it, the agent could search for a \u201cshelf\u201d and then try to find the \u201ccup\u201d or \u201cmug\u201d in the cropped image of the \u201cshelf\u201d. If an object is found, it is added to a tracking system (Yang & Yang, 2022). This process enhances the Segment and Track Anything (Cheng et al., 2023a) approach with flexible multi-granular search. The object statuses are"}, {"title": "3.4 GUIDANCE AND POLICY INTEGRATION", "content": "Aiming to guide a wide range of policies, our framework is designed to work both with continuous and discrete action spaces. In this section, we discuss the operation of combining the guidance function with the base policy's action distributions. Furthermore, we discuss how deterministic regression models can be adapted to work with our framework.\nAction-space Adaptation. We assume the availability of a dynamics model $D$ that can forecast possible future states of the robot given a possible action $a_t$. In the manipulation domain, a dynamics model is often available in the form of a forward kinematics model, a learned dynamics model, or a simulator. Oftentimes, the action space $\\mathcal{A}$ of policies them-self is the same as the robot's state $\\mathcal{S}$ either being or joint angles of the robot or the gripper's end-effector pose. For the last cases, where both the action and state space are expressed in SE(3) integrating the guidance function with a base policy would only require a multiplication of the guidance scores with the action probabilities of the base policy. In other scenarios, adapting the robot's action and state space to match the representation of the visual cues (position, orientation, and size) would be required.\nConsidering the visual grounding, the action space and the state space share the same representation (SE(3)), the operation to combine the guidance function with the base policy can be expressed as an element-wise weighted average:\n$\\pi_{\\text{guided}} = (1 - \\alpha)\\pi + \\alpha G,$ (3)\nwhere $\\alpha \\in [0, 1]$ represents the percentage of guidance applied with respect to the base-policies distribution and is here denoted as guidance factor.\nAdaptation of Regression Policies. To properly leverage the high-level guidance expressed in the guidance functions and the low-level capabilities of the base policy, it is desired that the policy's action space be expressed as a distribution. In the case of regression policies that do not provide uncertainty estimates, several strategies can be employed to infer the action distribution. One common approach is to assume a Gaussian distribution centered at the predicted value and compute the variance using ensembles of models trained with different initialization, different data samples, or different dropout seeds or different checkpoint stages (Abdar et al., 2021). Other strategies to infer the distributions of the model include using bootstrapping, Bayesian neural networks, or using a mixture of Gaussians (Mena et al., 2021)."}, {"title": "3.5 LEARNING NEW ROBOT SKILLS FROM SCRATCH", "content": "We note that this framework can enable robots to acquire new skills from scratch through zero-shot learning or self-improvement via iterative guidance updates. By leveraging the system's ability to ground guidance in the visual features of the environment, the system can perform tasks without prior training. Applying 100% guidance over untrained policies, the robot explores the environment with purpose, learning basic skills and refining them iteratively to improve success. In section 4, we provide an analysis of the system's performance in such challenging scenarios."}, {"title": "4 EXPERIMENTS & RESULTS", "content": "Task Definitions: We demonstrate the efficacy of g-MotorCortex, in simulation on the RL-Bench benchmark (James et al., 2020) and on two challenging real-world tasks. For the real-world setup, we use the UFACTORY Lite 6 robot arm as the robotic agent and, as the end-effector, we use the included UFACTORY Gripper Lite, a simple binary gripper. The arm is mounted on a workbench. For perception, we use a calibrated RGB-D Camera, specifically the Intel RealSense Depth Camera D435i. All experiments were conducted on a desktop machine with two (2) NVIDIA RTX 3090 GPU, 64GB of RAM, and an Intel i9-10900K CPU.\n\u2022 (Sim): RL-Bench: We consider 10 tasks on the RL-Bench benchmark (James et al., 2020) using a single RGBD camera input, as described in the GNFactor setup (Ze et al., 2024).\n\u2022 (Sim): RL-Bench, learning from scratch: Aiming to explore the capabilities of g-MotorCortex on learning new skills from scratch, we selected 4 challenging tasks from the RL-Bench benchmark: turn tap, push buttons, slide block to color target, reach and drag."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "\u2022 (Real): Sequenced Multi-button Press: Here, the agent must use its end-effector to press multiple real buttons on a workspace, in a particular order. We designed this task to evaluate whether the proposed framework is capable of improving pre-trained robot policies in performing challenging tasks, without access to human demonstrations.\n\u2022 (Real): Reach for chess piece: Given a cluttered scene with many similar objects, we want to evaluate if the multi-granular perception framework can effectively guide the agent to identify and reach for the appropriate target. We implement this perceptual grounding and reaching task on a standard chessboard, where the agent must identify and reach for one of the chess pieces specified by natural language instruction.\nBase Policies: We evaluate the effectiveness of our guidance framework using different base policies, Act3D (Gervet et al., 2023), 3D Diffuser Actor (Ke et al., 2024), and a RandomPolicy. All policies plan in a continuous space of translations, rotations, and gripper state (SE(3) \u00d7 R), however they utilize different inference strategies:\n\u2022 Act3D samples waypoints in the Cartesian space (R\u00b3) and predicts the orientation and gripper state for the best scoring sampled waypoint, combining a classification and regression strategies into a single policy.\n\u2022 3D Diffuser Actor, on the other hand, uses a diffusion model to compute the target waypoints and infers the orientation and gripper state from the single forecast waypoint, thus tackling the problem as a single regression task.\n\u2022 Random Policy denotes any of the former frameworks that has not been trained for a specific task, therefore the weights are randomly initialized.\nThe fundamentally different types of policies' outputs make them a great use case for our policy guidance framework. Furthermore, the common representation of the action and state spaces of both policies (SE(3) \u00d7 R) provides a straight forward integration with our grounding models.\nAs described in Section 3.4, the regression components of the policies require an adaptation to transform the single predictions of the model into a distribution over the action space. For the sake of simplicity, we assume a Gaussian distribution over the action space, with the mean centered on the predicted values and the standard deviation fixed on a constant value. The outputs of the classification component of Act3D (waypoint positions) were directly considered as samples of a distribution over the Cartesian space (R3).\nThe integration of base policies with the guidance distributions was performed by applying a weighted average parameterized by \u03b1 as shown in Equation 3."}, {"title": "4.2 EXPERIMENTAL EVALUATION", "content": "Our experimental evaluation aims to address the following questions: (1) Does g-MotorCortex improve the performance of pre-trained base policies on specific robotics tasks and environments without additional human demonstrations? (2) Does the proposed multi-granular perception capabilities effectively guide the policy in challenging cluttered environments? (3) Does g-MotorCortex enable policies to learn new skills from scratch? (4) What is the effect of guidance on expert versus untrained policies?\nDoes g-MotorCortex improve the performance of pre-trained base policies on specific robotics tasks and environments without additional human demonstrations? We first assess the effect of the proposed guidance on the Act3D and 3D Diffuser Actor baselines following the GN-Factor (Ze et al., 2024) setup, which consists of a single RGBD camera and table-top manipulator performing 10 challenging tasks with 25 variations each. Guidance is iteratively generated for the failure cases. For the failed rollouts, our policy improvement framework ran for 5 iterations. As displayed by Table 1, the framework was able to improve the success rate of the base policy on most of the tasks, with the best results achieved by using 1% guidance. The low amount of guidance has shown to be enough to bend the action distribution to the desired direction, while still preserving the low-level nuances captured by the base policy. This suggests that g-MotorCortex is capable of improving base policies by adding abstract understanding and grounding of the desired task, while preserving the low-level movement profiles captured by the original policies.\nDoes the proposed multi-granular perception capability effectively guide the policy in challenging, cluttered environments? In real-world experiments, we qualitatively demonstrate the"}, {"title": "5 CONCLUSION", "content": "Summary: In this work, we proposed g-MotorCortex, a novel framework for the self-improvement of embodied policies. Our self-guidance approach leverages the world knowledge of a group of conversational agents and grounding models to guide policies during deployment. We demonstrated the effectiveness of our approach in autonomously improving manipulation policies and learning new skills from scratch, in simulated RL-bench benchmark tasks and in two challenging real-world tasks. Our results show that the proposed framework is especially effective in improving the following high-level task structures and key steps to solve the task. This capability can be well suited for improving pre-trained policies that struggle with long-horizon tasks or for learning new simple skills from scratch.\nLimitations: From an analysis of the guided rollouts, a few of the tasks variations proved challenging for the perception models used by the grounding agent, leading to false positives detections or failure to locate specific objects. This limitation was mainly observed in simulation task, were the graphics object representations, even though simplified, do not always match the representations used to train the object detection models. This limitation could be addressed by integrating more robust object detection models or verification procedures to ensure the correct detection of objects in the scene. Moreover, occasional inaccuracies on scene understanding by the Visual Language Model (VLM) have been observed, leading to the generation of inaccurate guidance codes and unexpected behaviors. Even though recent advances in large vision-language models have shown great potential in understanding the underlying dynamics of the world from large-scale internet data, translating this knowledge into out-of-distribution domain, such as robotics, while preventing hallucinations remains an open challenge.\nFuture Works: Regarding future works, we think that combining the proposed framework with fine-grained exploration techniques would allow the policy to explore in a targeted manner the low-level details of the task, while leveraging the high-level guidance provided by our framework. This may enrich the guidance codes with the necessary low-level details required to perform more complex tasks successfully.\nFurthermore, the guidance function generation could be further improved by composing and adapting from a repository of successful guidance functions from previous experiences. This could be achieved by incorporating Retrieval Augmented Generation (RAG) (Lewis et al., 2021) into our multi agent framework. This modification could allow the guidance system to learn new simple skills from scratch by interacting with the environment and leveraging this collected knowledge to guide the policy more effectively.\nAiming to incorporate the knowledge captured by the guidance functions into the base policy, an experience replay and finetuning mechanisms could be incorporated into our current system. This modification could allow the framework to use past guided experiences to improve the base policy in a sample efficient manner. This could be achieved in a targeted matter by leveraging Low Rank Adaptation (LoRA) (Hu et al., 2021)."}, {"title": "5.1 REPRODUCIBILITY STATEMENT", "content": "Intending to encourage other researchers to build upon the introduced framework, we take steps to ensure the usability and reproducibility of our work. The source code for g-MotorCortex is open-sourced and linked to on the project's website. We have provided dockerized scripts to facilitate the setup across different development environments. Additionally, in section A.1 we include the prompts used to configure each agent. The temperature of the model was set as zero to"}, {"title": "A APPENDIX", "content": "We provide the system prompts used to initialize each one of the agents. Note that for models relying on API calls, we use gpt-40-mini-2024-07-18. The maximum number of tokens is set to 2000."}]}