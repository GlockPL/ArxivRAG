{"title": "Towards Clean-Label Backdoor Attacks\nin the Physical World", "authors": ["Thinh Dao", "Cuong Chi Le", "Khoa D Doan", "Kok-Seng Wong"], "abstract": "Deep Neural Networks (DNNs) are vulnerable to backdoor poisoning attacks,\nwith most research focusing on digital triggers, special patterns digitally added to\ntest-time inputs to induce targeted misclassification. In contrast, physical triggers,\nwhich are natural objects within a physical scene, have emerged as a desirable\nalternative since they enable real-time backdoor activations without digital manipu-\nlation. However, current physical attacks require that poisoned inputs have incorrect\nlabels, making them easily detectable upon human inspection. In this paper, we\ncollect a facial dataset of 21,238 images with 7 common accessories as triggers and\nuse it to study the threat of clean-label backdoor attacks in the physical world.\nOur study reveals two findings. First, the success of physical attacks depends on\nthe poisoning algorithm, physical trigger, and the pair of source-target classes.\nSecond, although clean-label poisoned samples preserve ground-truth labels, their\nperceptual quality could be seriously degraded due to conspicuous artifacts in the\nimages. Such samples are also vulnerable to statistical filtering methods because\nthey deviate from the distribution of clean samples in the feature space. To address\nthese issues, we propose replacing the standard $l_\\infty$ regularization with a novel pixel\nregularization and feature regularization that could enhance the imperceptibility of\npoisoned samples without compromising attack performance. Our study highlights\naccidental backdoor activations as a key limitation of clean-label physical backdoor\nattacks. This happens when unintended objects or classes accidentally cause the\nmodel to misclassify as the target class.", "sections": [{"title": "Introduction", "content": "The development of DNNs has led to breakthroughs in various domains, such as computer vision [1],\nnatural language processing [2], speech recognition [3], and recommendation systems [4]. However,\ntraining large neural networks requires a huge amount of training data, encouraging practitioners to\nuse third-party datasets, crawl datasets from the Internet, or outsource data collection [5, 6]. These\npractices introduce a security threat called data poisoning attacks wherein an adversary could poison\na portion of training data to manipulate the behaviors of the DNNs [7\u20139].\nOne line of research in data poisoning is backdoor attacks, in which the attackers aim to create an\nartificial association between a trigger and a target class such that the presence of such trigger in\nsamples from the source class (or multiple source classes) will cause the model to misclassify as the\ntarget class. The poisoned model (i.e., the model trained on poisoned samples) behaves normally\nin ordinary inputs, making backdoor detection challenging. For example, Gu et al. [5] show that a\nyellow square pattern on a stop traffic sign can change the model's prediction to \"speed limit\u201d.\nMost backdoor attacks employ digital triggers, special pixel patterns digitally added at inference\ntime to cause misclassification. Another less explored line of research investigated the use of natural\nobjects within a scene as physical triggers. Physical triggers have two major advantages over\ndigital triggers: they appear more natural when injected into the inputs and do not require digital"}, {"title": "Related works", "content": "In Backdoor Attacks, the attacker uses a predefined trigger to insert a backdoor into a victim model\nby poisoning a small proportion of training data. Training on the poisoned data causes the model\nto misclassify instances containing the trigger as the attacker's chosen target label. Detecting the\nbackdoor attack is challenging because the poisoned model retains high accuracy on clean, ordinary\ninputs. Generally, backdoor attacks can be divided into dirty-label and clean-label backdoor attacks.\nDirty-label attacks. The attacker could directly enforce a connection between the backdoor trigger\nand the target class by adding the trigger to the training data and flipping their labels to the target class.\nSome types of Dirty-label Backdoor Attack include trigger-pattern attacks [5, 11, 12], sample-specific\nattacks [13, 14], and imperceptible attacks [14\u201318]. While dirty-label attacks can achieve impressive\nperformance with a small poisoning ratio, mislabelled poisoned samples are vulnerable to human\ninspection as their image contents are visibly different from target-class instances.\nClean-label attacks. A more stealthy approach involves directly poisoning target-class instances\nwithout label manipulation. The concept of clean-label backdoor attacks was pioneered by Turner\net al. [19], who proposed using adversarial perturbations and GAN-based interpolation to obscure the\nnatural, salient features of the target class before embedding the trigger. By effectively concealing\nthe latent features with the perturbations, the model becomes reliant on the introduced trigger for\nclassifying instances of the target class. The following works on Clean-label attacks can be divided\ninto hidden-trigger and trigger-design attacks. In hidden-trigger attacks [20, 21], the trigger is hidden\nfrom the training data and only added to test-time inputs of the source class to achieve the targeted\nmisclassification. In trigger-design attacks [22, 23], the attackers aim to optimize trigger pattern(s)\nthat represent the most robust, representative feature of the target class.\nPhysical-backdoor attacks. Both of these attacks require digitally modifying inputs to insert the\ntrigger. In practical scenarios, however, digital modification is not always possible, especially for\nreal-time applications such as facial recognition and object detection. As an alternative, some works\nfocus on using physical objects as triggers. Notably, Gu et al. [5] demonstrated that a yellow post-it\nnote patched on a stop sign could cause the traffic sign to be misclassified as a speed limit. Similarly,\nChen et al. [12] demonstrated an attack that fools a facial recognition system by blending the digital\nimage of sunglasses to the training data and using the physical object (of the same sunglasses) to\nmanipulate the prediction of the model. Wenger et al. [10] conducted an empirical study on the\neffectiveness of physical objects as backdoor triggers by collecting a dataset with 3,205 images of\n9 facial accessories as potential backdoor triggers. These studies on physical backdoor attacks are\nlimited to dirty-label techniques, which is not the focus of our research.\nOf particular relevance to our work, Zeng et al. [22] proposed a clean-label backdoor attack that can\nwork in a physical context. However, the lack of details on experimental design, dataset description,\nand concrete results in their paper hinders any definitive conclusions about the attack's performance\nin real-world scenarios. Moreover, their approach differs from our study because it designs a\nconspicuous trigger pattern instead of exploiting a natural object. By collecting a dataset of 21,238\nimages with 7 triggers, which is much larger than the dataset of Wenger et al. [10], we are the first to\ncomprehensively study the performance of clean-label backdoor attacks in the physical world."}, {"title": "Methodology", "content": ""}, {"title": "Threat model", "content": "Attacker's capability. We follow the same threat model as hidden-trigger backdoor attacks [20, 21].\nThe training dataset is collected from multiple sources, and there exists an attacker (e.g., data\ncontributors, data collectors) who can access and perturb a small portion of the data to manipulate\nthe behavior of the victim model. The attack is one-to-one: any instances from the source class\nare misclassified as belonging to the target class with the presence of the trigger. For example, the\nsource class is an employee in a company who can wear a special pair of sunglasses to fool a facial\nrecognition system into classifying him as the CEO so that he could get an illegal privilege, such as\naccess to confidential documents or high-ranking facilities. Due to the nature of the physical trigger,\nwe argue that all-to-one attacks have low utility as this scenario could raise suspicion among model\nusers; for example, benign participants wearing the trigger object could accidentally activate the\nbackdoor behavior and report the issue to authority."}, {"title": "Problem formulation", "content": "Notations. Denote $y_s$, $y_t$ as labels of the source and target classes, respectively. Let $\\mathcal{D} = \\cup_{i=1}^{k} \\mathcal{D}_i$\nbe the training dataset with k classes. Let $\\mathcal{D}_i$, $\\mathcal{D}_i$ be the distribution of instances in class i without\nand with the physical trigger. Let $F: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ be the classifier model, with parameters $\\theta$, that gives\nprediction scores for each of k classes given an input $x \\in \\mathbb{R}^d$. F can be decomposed as $F = f \\circ g$\nwith f as the feature extractor and g be the classifier. The poison pair $s \\rightarrow t$ is the pair of source class\nand target class with index labels.\nAttack goals. The attacker's goal is to inject a poisoned dataset $\\mathcal{P}$ to the training set $\\mathcal{D}$ such that\nafter training the model on $\\mathcal{D}_p := \\mathcal{D} \\cup \\mathcal{P}$, it will incorrectly classify samples from the source class\nwearing the physical trigger as belonging to the target class. To achieve this goal, the attacker needs\nto optimize the following adversarial objective:\n$L_{adv} = \\mathbb{E}_{x_s \\sim \\mathcal{D}_s} \\mathbb{E}_{x_s \\sim \\mathcal{D}_s}[L(F_\\theta(x_s),y_t)]$\nwhere L is the training objective function and $\\mathcal{D}_s$ is the distribution of the source class with the\nphysical trigger. In dirty-label attacks, $L_{adv}$ can be directly optimized by choosing $\\mathcal{P}$ to be samples\ndrawn from the distribution $\\mathcal{D}_s$ and changing the label of each instance in $\\mathcal{P}$ from $y_s$ to $y_t$. In\nclean-label attacks, the attackers indirectly optimize $L_{adv}$ by poisoning samples in $\\mathcal{D}_t$. Thus, we\ndefine the poison ratio $\\alpha$ as the percentage of poisoned samples in the target-class data. Existing\nmethods involve crafting a set of perturbations $\\delta = \\{\\delta^{(i)}\\}_{i=1}^{K}$ within a $l_\\infty$ norm ball of radius $\\epsilon$.\nThese perturbations are then added to a subset of $\\mathcal{D}_t$. Formally, $\\mathcal{P} \\triangleq \\{x_t^{(i)} + \\delta^{(i)}, y_t\\}_{i=1}^{K}$ is the\npoisoned dataset containing K poisoned target-class examples. In this paper, we introduce four\npoisoning methods for CLPBA:"}, {"title": "Experiments", "content": "We evaluate CLPBA using four Convolutional Neural Networks (CNNs), but only the results for\nResNet50 [26] are presented in the main paper. The models are pre-trained on the VGGFace 2 dataset\n[27] and then fine-tuned on our dataset. We assume a gray-box scenario where the attacker knows\nthe architecture of the victim model but does not have access to its training process. Each attack has\na unique set of parameters, so we follow as closely as possible to the experimental settings of the\noriginal works. We measure the performance of an attack, or attack success rate (%) (ASR) success\nrate, by the percentage of trigger samples in the source class that are misclassified as the target class.\nNote that the prediction accuracy (ACC) on clean samples consistently exceeds 99% and is therefore\nnot reported in most experiments. Detailed experimental settings can be found in Appendix D.\nAdditional experiments on other models and different settings are provided in Appendix E."}, {"title": "Comparison of poisoning algorithms", "content": "The results from Table 1 show that GM is the most effective poisoning algorithm. BLC and Naive\nmethods fail with small poison ratios. Higher values of $\\epsilon$ and $\\alpha$ could increase ASR, but $\\epsilon$ has a greater\nimpact as it allows the poisoning objective to converge to smaller values. Variance of experiment\nresults between poisoning attempts could be due to different weight initialization of the victim model."}, {"title": "Impact of poison pair on attack performance", "content": "In Table 1, the attack effectiveness varies significantly across source-target class pairs. Poison pairs\n1-7 and 3-6 show much higher ASR than others, even for Naive and BLC poisoning methods.\nThis section examines the impact of source and target classes on attack performance.\nFrom the source class. A source class is more susceptible to physical attacks at inference if the\ntrigger obscures the class's discriminative classification regions, which enables the trigger features\nto outweigh salient features and cause misclassification. In Table 2, the classification loss of the\nclean ResNet50 model on inference-time trigger samples is much higher for the source class 3 than\nfor the source class 4, and thus the attack on class 3 has a higher ASR. In Figure 8 of Appendix B,\nwe use Grad-CAM [28] to visualize discriminative regions implicitly detected by a clean ResNet50\nmodel (i.e., model trained solely on clean data). We can see that in trigger instances of source class 3,\nthe model struggles to extract discriminative features and must rely on less prominent features for\nclassification. If the model is backdoored, then the model will focus on the trigger for classification.\nFrom the target class. A target class can be more susceptible to CLPBA due to the similarity\nbetween the class's natural features and the trigger features. For example, in Appendix B, we show\nthat the eyeglasses in some samples of the target class could represent weaker backdoor features\nfor the attacker's trigger. The existence of such support samples helps enforce the attacker's trigger\nas a dominant classification feature of the target class once the model is poisoned. Filtering these\nsupport samples greatly reduces the victim model's susceptibility to the backdoor trigger (see Table 7,\nAppendix B). This naturally implies the existence of natural physical backdoors in a dataset, especially\nif unique physical objects exist in target class data. A detailed analysis of the natural backdoors is\ngiven in Appendix B. We note that there is also an unexplored relationship between the distribution\nof samples in the target class $\\mathcal{D}_t$ and the distribution of trigger samples in the source class $\\mathcal{D}_s$ that\nallows for feature alignment of poisoned samples in high dimensional space."}, {"title": "Trigger selection", "content": "The performance of a backdoor attack has been shown to depend on the size, shape, and location of a\ntrigger [10, 29]. The larger and closer to the face the trigger is, the more effectively it can obscure\na person's facial features and become dominant features. For a physical backdoor attack targeted\nat a facial recognition system, however, it is not practical to use large triggers like face masks and\nsunglasses, as people may be requested to take off such items when they go through facial recognition\nprocesses. Thus, it is important to study the performance of CLPBA using more stealthy triggers."}, {"title": "Enhancing imperceptibility of attacks", "content": "In the pixel space. Higher values of $\\epsilon$ result in higher ASR at the expense of visual stealthiness as\nthe perturbations could leave behind visible artifacts on high-resolution facial images. In\nthe current $l_\\infty$ regularisation, the projection of perturbations on the norm ball is only executed\nafter every optimization step, making backdoor and visual imperceptibility two separate tasks. As a\nresult, pixel values of the perturbations are often pushed to the boundary of the norm ball, causing\nnoise to appear in unnecessary regions. Furthermore, the perturbations are non-smooth and irregular\nbecause each pixel is optimized individually. To improve the perceptual quality of poisoned samples,\nwe propose to replace the current $l_\\infty$ regularisation with a Pixel Regularization (PR) containing soft\npixel-wise $l_\\infty$ loss and Total Variation Upwind (TVU) [30]:\n$R_{pix} (\\delta) = ||\\delta \u2013 M \\odot \\epsilon||_1 + TVU(\\delta; \\beta = 1)$\n$TVU(\\delta; \\beta) = \\sum_v (\\sum_{q \\in \\mathcal{N(p)}} (\\delta(p) - \\delta(q))^2)^{3/2}$\nwhere M is a pixel-wise embedding mask based on the original image: $M_{i,j,k} = 1.0 - |X_{i,j,k}|$,\nassuming the pixel value $X_{i,j,k} \\in [0,1]$. In Equation 10, $\\mathcal{N(p)}$ are neighbouring pixels of pixel\np. Both terms in Equation 9 improve the smoothness of perturbations, with $TVU(\\delta)$ minimizing\nthe difference between adjacent pixels and M keeping the perturbation within the color range. As\nshown in Figure 3, the perturbations crafted with PR are smoother and less conspicuous than those\ncrafted with the original $l_\\infty$ regularization. Following previous works [31, 32], we also evaluate the\nimperceptibility of poisoned samples statistically using Peak Signal-to-Noise Ratio (PSNR) [33]. The\naverage PSNR value of poisoned samples is reported in Table 5. Our regularization results in higher\nPSNR, corresponding to better perceptual quality. We note that an additional advantage of the soft\n$l_\\infty$ regularization over the standard one is that it is differentiable, which enables better convergence\nfor the poisoning objective, especially for lower values of $\\epsilon$ (see Appendix E.3).\nIn the feature space. Backdoor poisoning attacks have been observed to leave discernible traces in\nthe feature space of poisoned models [34\u201337]. By projecting feature representations of target samples\n(containing both clean and poisoned samples) on the first two principle axes, we can see a\nclear distinction between the poisoned samples and clean samples. The separation exists because the\nperturbations on poisoned samples contain unnatural trigger features of $\\mathcal{D}_s$, from which the model can\nlearn as features of the target class. To limit the separation of poisoned samples from clean samples\nin the feature space, we propose a Feature Regularization (FR) that minimizes the distance of\nsamples in $\\mathcal{P}$ to the distribution of original target-class samples $\\mathcal{D}_t$: $R_{feat}(\\delta) = ||f(x + \\delta) \u2013 f(\u00b5)||_2$\nwhere \u00b5 the empirical mean is computed from dataset $\\mathcal{D}_t$. In Figure 4, we can see that FR constrains\nthe separation between poisoned and benign samples of the target class. In Table 5, by evaluating\nfour representative filtering defenses (Spectral Signature (SS) [34], Activation Clustering (AC) [36],\nSPECTRE [35], DeepKNN [38]), we can confirm the effectiveness of FR in concealing the poisoned\ninputs from machine inspection in the feature space"}, {"title": "Accidental backdoor activations", "content": "For CLPBA to be stealthy, we argue that it needs to be source-specific and trigger-specific. That\nis, (1) the trigger should be discriminative so that the victim model does not incorrectly recognize\nsimilar-looking objects as triggers, and (2) only the source class can use the trigger to manipulate the\nvictim model's prediction at inference time. We introduce two new metrics, False Positive Rate and\nSuspicion Rate, to test the specificity (and also stealthiness) of CLPBA:"}, {"title": "Conclusion", "content": "Through extensive experiments, our research underscores the vulnerability of DNNs to clean-label\nbackdoor attacks in the physical world. We thoroughly analyze the influence of poisoning algorithms,\nphysical triggers, and source-target class pairs on the effectiveness of these attacks. Additionally, we\npropose novel pixel and feature regularizations to improve the imperceptibility of attacks in both\npixel and feature spaces. A significant limitation identified is accidental backdoor activations of\nthe poisoned model due to false triggers or non-source classes, which could compromise attack\nstealthiness. To address this, we propose incorporating repel terms into the adversarial objective to\nenhance trigger specificity to the source class. Our findings highlight the critical need for robust\ndefenses against clean-label physical backdoor attacks and pave the way for future research on\nimproving the resilience of DNNs against such threats."}]}