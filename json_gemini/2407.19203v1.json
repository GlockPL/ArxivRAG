{"title": "Towards Clean-Label Backdoor Attacks in the Physical World", "authors": ["Thinh Dao", "Cuong Chi Le", "Khoa D Doan", "Kok-Seng Wong"], "abstract": "Deep Neural Networks (DNNs) are vulnerable to backdoor poisoning attacks, with most research focusing on digital triggers, special patterns digitally added to test-time inputs to induce targeted misclassification. In contrast, physical triggers, which are natural objects within a physical scene, have emerged as a desirable alternative since they enable real-time backdoor activations without digital manipulation. However, current physical attacks require that poisoned inputs have incorrect labels, making them easily detectable upon human inspection. In this paper, we collect a facial dataset of 21,238 images with 7 common accessories as triggers and use it to study the threat of clean-label backdoor attacks in the physical world. Our study reveals two findings. First, the success of physical attacks depends on the poisoning algorithm, physical trigger, and the pair of source-target classes. Second, although clean-label poisoned samples preserve ground-truth labels, their perceptual quality could be seriously degraded due to conspicuous artifacts in the images. Such samples are also vulnerable to statistical filtering methods because they deviate from the distribution of clean samples in the feature space. To address these issues, we propose replacing the standard $l_\\infty$ regularization with a novel pixel regularization and feature regularization that could enhance the imperceptibility of poisoned samples without compromising attack performance. Our study highlights accidental backdoor activations as a key limitation of clean-label physical backdoor attacks. This happens when unintended objects or classes accidentally cause the model to misclassify as the target class.", "sections": [{"title": "1 Introduction", "content": "The development of DNNs has led to breakthroughs in various domains, such as computer vision [1], natural language processing [2], speech recognition [3], and recommendation systems [4]. However, training large neural networks requires a huge amount of training data, encouraging practitioners to use third-party datasets, crawl datasets from the Internet, or outsource data collection [5, 6]. These practices introduce a security threat called data poisoning attacks wherein an adversary could poison a portion of training data to manipulate the behaviors of the DNNs [7\u20139]. One line of research in data poisoning is backdoor attacks, in which the attackers aim to create an artificial association between a trigger and a target class such that the presence of such trigger in samples from the source class (or multiple source classes) will cause the model to misclassify as the target class. The poisoned model (i.e., the model trained on poisoned samples) behaves normally in ordinary inputs, making backdoor detection challenging. For example, Gu et al. [5] show that a yellow square pattern on a stop traffic sign can change the model's prediction to \"speed limit\u201d. Most backdoor attacks employ digital triggers, special pixel patterns digitally added at inference time to cause misclassification. Another less explored line of research investigated the use of natural objects within a scene as physical triggers. Physical triggers have two major advantages over digital triggers: they appear more natural when injected into the inputs and do not require digital"}, {"title": "2 Related works", "content": "In Backdoor Attacks, the attacker uses a predefined trigger to insert a backdoor into a victim model by poisoning a small proportion of training data. Training on the poisoned data causes the model to misclassify instances containing the trigger as the attacker's chosen target label. Detecting the backdoor attack is challenging because the poisoned model retains high accuracy on clean, ordinary inputs. Generally, backdoor attacks can be divided into dirty-label and clean-label backdoor attacks. Dirty-label attacks. The attacker could directly enforce a connection between the backdoor trigger and the target class by adding the trigger to the training data and flipping their labels to the target class. Some types of Dirty-label Backdoor Attack include trigger-pattern attacks [5, 11, 12], sample-specific attacks [13, 14], and imperceptible attacks [14\u201318]. While dirty-label attacks can achieve impressive performance with a small poisoning ratio, mislabelled poisoned samples are vulnerable to human inspection as their image contents are visibly different from target-class instances. Clean-label attacks. A more stealthy approach involves directly poisoning target-class instances without label manipulation. The concept of clean-label backdoor attacks was pioneered by Turner et al. [19], who proposed using adversarial perturbations and GAN-based interpolation to obscure the natural, salient features of the target class before embedding the trigger. By effectively concealing the latent features with the perturbations, the model becomes reliant on the introduced trigger for classifying instances of the target class. The following works on Clean-label attacks can be divided into hidden-trigger and trigger-design attacks. In hidden-trigger attacks [20, 21], the trigger is hidden from the training data and only added to test-time inputs of the source class to achieve the targeted misclassification. In trigger-design attacks [22, 23], the attackers aim to optimize trigger pattern(s) that represent the most robust, representative feature of the target class. Physical-backdoor attacks. Both of these attacks require digitally modifying inputs to insert the trigger. In practical scenarios, however, digital modification is not always possible, especially for real-time applications such as facial recognition and object detection. As an alternative, some works focus on using physical objects as triggers. Notably, Gu et al. [5] demonstrated that a yellow post-it note patched on a stop sign could cause the traffic sign to be misclassified as a speed limit. Similarly, Chen et al. [12] demonstrated an attack that fools a facial recognition system by blending the digital image of sunglasses to the training data and using the physical object (of the same sunglasses) to manipulate the prediction of the model. Wenger et al. [10] conducted an empirical study on the effectiveness of physical objects as backdoor triggers by collecting a dataset with 3,205 images of 9 facial accessories as potential backdoor triggers. These studies on physical backdoor attacks are limited to dirty-label techniques, which is not the focus of our research. Of particular relevance to our work, Zeng et al. [22] proposed a clean-label backdoor attack that can work in a physical context. However, the lack of details on experimental design, dataset description, and concrete results in their paper hinders any definitive conclusions about the attack's performance in real-world scenarios. Moreover, their approach differs from our study because it designs a conspicuous trigger pattern instead of exploiting a natural object. By collecting a dataset of 21,238 images with 7 triggers, which is much larger than the dataset of Wenger et al. [10], we are the first to comprehensively study the performance of clean-label backdoor attacks in the physical world."}, {"title": "3 Methodology", "content": "Attacker's capability. We follow the same threat model as hidden-trigger backdoor attacks [20, 21]. The training dataset is collected from multiple sources, and there exists an attacker (e.g., data contributors, data collectors) who can access and perturb a small portion of the data to manipulate the behavior of the victim model. The attack is one-to-one: any instances from the source class are misclassified as belonging to the target class with the presence of the trigger. For example, the source class is an employee in a company who can wear a special pair of sunglasses to fool a facial recognition system into classifying him as the CEO so that he could get an illegal privilege, such as access to confidential documents or high-ranking facilities. Due to the nature of the physical trigger, we argue that all-to-one attacks have low utility as this scenario could raise suspicion among model users; for example, benign participants wearing the trigger object could accidentally activate the backdoor behavior and report the issue to authority."}, {"title": "3.2 Problem formulation", "content": "Notations. Denote $y_s$, $y_t$ as labels of the source and target classes, respectively. Let $D = \\bigcup_{i=1}^{k} D_i$ be the training dataset with $k$ classes. Let $D_i, \\mathcal{D}_i$ be the distribution of instances in class $i$ without and with the physical trigger. Let $F: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ be the classifier model, with parameters $\\theta$, that gives prediction scores for each of $k$ classes given an input $x \\in \\mathbb{R}^d$. $F$ can be decomposed as $F = f \\circ g$ with $f$ as the feature extractor and $g$ be the classifier. The poison pair $s\\rightarrow t$ is the pair of source class and target class with index labels. Attack goals. The attacker's goal is to inject a poisoned dataset $P$ to the training set $D$ such that after training the model on $D_p := D \\cup P$, it will incorrectly classify samples from the source class wearing the physical trigger as belonging to the target class. To achieve this goal, the attacker needs to optimize the following adversarial objective:\n$\\mathcal{L}_{adv} = \\mathbb{E}_{x_s \\sim \\mathcal{D}_s} \\left[ \\mathcal{L}(F_\\theta(x_s), y_t) \\right]$          (1)\nwhere $\\mathcal{L}$ is the training objective function and $\\mathcal{D}_s$ is the distribution of the source class with the physical trigger. In dirty-label attacks, $\\mathcal{L}_{adv}$ can be directly optimized by choosing $P$ to be samples drawn from the distribution $\\mathcal{D}_s$ and changing the label of each instance in $P$ from $y_s$ to $y_t$. In clean-label attacks, the attackers indirectly optimize $\\mathcal{L}_{adv}$ by poisoning samples in $D_t$. Thus, we define the poison ratio $\\alpha$ as the percentage of poisoned samples in the target-class data. Existing methods involve crafting a set of perturbations $\\delta = \\{\\delta^{(i)}\\}_{i=1}^{K}$ within a $\\ell_\\infty$ norm ball of radius $\\epsilon$. These perturbations are then added to a subset of $D_t$. Formally, $P = \\{x_t + \\delta^{(i)}, y_t\\}_{i=1}^{K}$ is the poisoned dataset containing $K$ poisoned target-class examples. In this paper, we introduce four poisoning methods for CLPBA:\n*   Naive: Choose $P$ to be a set of samples from $\\mathcal{D}_t$ so that the model learns the trigger as features of the target class. This method requires access to trigger instances from the target class.\n*   Feature Matching (FM) [20]: This method targets transfer-learning models by finding perturbations that shift poisoned samples closer to the $\\mathcal{D}_s$ in the feature space:\n$\\mathcal{A}_{FM} = \\min_\\delta || f(x_t + \\delta) - f(x_s)||_2$            (2)\n$||\\delta||_\\infty \\leq \\epsilon$\nwhere $x_t \\in P, x_s \\sim \\mathcal{D}_s$             (3)\n*   Gradient Matching (GM) [21]: This method targets DNNs trained from scratch by aligning the gradient of the model's parameters on the poisoned dataset $P$ with the gradient to optimize $\\mathcal{L}_{adv}$:\n$\\mathcal{A}_{GM} = \\min_\\delta \\left( 1 - \\frac{\\langle \\nabla_\\theta \\mathcal{L}_{poi}, \\nabla_\\theta \\mathcal{L}_{adv} \\rangle}{\\| \\nabla_\\theta \\mathcal{L}_{poi} \\| \\cdot \\| \\nabla_\\theta \\mathcal{L}_{adv} \\|} \\right)$               (4)\nwhere $\\mathcal{L}_{poi} = \\frac{1}{K} \\sum_{i=1}^{K} \\text{KL}(F(x_i + \\delta^{(i)}; \\theta), y_t)$             (5)"}, {"title": "4 Experiments", "content": "We evaluate CLPBA using four Convolutional Neural Networks (CNNs), but only the results for ResNet50 [26] are presented in the main paper. The models are pre-trained on the VGGFace 2 dataset [27] and then fine-tuned on our dataset. We assume a gray-box scenario where the attacker knows the architecture of the victim model but does not have access to its training process. Each attack has a unique set of parameters, so we follow as closely as possible to the experimental settings of the original works. We measure the performance of an attack, or attack success rate (%) (ASR) success rate, by the percentage of trigger samples in the source class that are misclassified as the target class. Note that the prediction accuracy (ACC) on clean samples consistently exceeds 99% and is therefore not reported in most experiments. Detailed experimental settings can be found in Appendix D. Additional experiments on other models and different settings are provided in Appendix E."}, {"title": "4.1 Comparison of poisoning algorithms", "content": "The results from Table 1 show that GM is the most effective poisoning algorithm. BLC and Naive methods fail with small poison ratios. Higher values of $\\epsilon$ and $\\alpha$ could increase ASR, but $\\epsilon$ has a greater impact as it allows the poisoning objective to converge to smaller values. Variance of experiment results between poisoning attempts could be due to different weight initialization of the victim model."}, {"title": "4.2 Impact of poison pair on attack performance", "content": "In Table 1, the attack effectiveness varies significantly across source-target class pairs. Poison pairs 1-7 and 3-6 show much higher ASR than others, even for Naive and BLC poisoning methods. This section examines the impact of source and target classes on attack performance. From the source class. A source class is more susceptible to physical attacks at inference if the trigger obscures the class's discriminative classification regions, which enables the trigger features to outweigh salient features and cause misclassification. In Table 2, the classification loss of the clean ResNet50 model on inference-time trigger samples is much higher for the source class 3 than for the source class 4, and thus the attack on class 3 has a higher ASR. In Figure 8 of Appendix B, we use Grad-CAM [28] to visualize discriminative regions implicitly detected by a clean ResNet50 model (i.e., model trained solely on clean data). We can see that in trigger instances of source class 3, the model struggles to extract discriminative features and must rely on less prominent features for classification. If the model is backdoored, then the model will focus on the trigger for classification. From the target class. A target class can be more susceptible to CLPBA due to the similarity between the class's natural features and the trigger features. For example, in Appendix B, we show that the eyeglasses in some samples of the target class could represent weaker backdoor features for the attacker's trigger. The existence of such support samples helps enforce the attacker's trigger as a dominant classification feature of the target class once the model is poisoned. Filtering these support samples greatly reduces the victim model's susceptibility to the backdoor trigger (see Table 7, Appendix B). This naturally implies the existence of natural physical backdoors in a dataset, especially if unique physical objects exist in target class data. A detailed analysis of the natural backdoors is given in Appendix B. We note that there is also an unexplored relationship between the distribution of samples in the target class $\\mathcal{D}_t$ and the distribution of trigger samples in the source class $\\mathcal{D}_s$ that allows for feature alignment of poisoned samples in high dimensional space."}, {"title": "4.3 Trigger selection", "content": "The performance of a backdoor attack has been shown to depend on the size, shape, and location of a trigger [10, 29]. The larger and closer to the face the trigger is, the more effectively it can obscure a person's facial features and become dominant features. For a physical backdoor attack targeted at a facial recognition system, however, it is not practical to use large triggers like face masks and sunglasses, as people may be requested to take off such items when they go through facial recognition processes. Thus, it is important to study the performance of CLPBA using more stealthy triggers."}, {"title": "4.4 Enhancing imperceptibility of attacks", "content": "In the pixel space. Higher values of $\\epsilon$ result in higher ASR at the expense of visual stealthiness as the perturbations could leave behind visible artifacts on high-resolution facial images (Figure 3a). In the current $\\ell_\\infty$ regularisation, the projection of perturbations on the norm ball is only executed after every optimization step, making backdoor and visual imperceptibility two separate tasks. As a result, pixel values of the perturbations are often pushed to the boundary of the norm ball, causing noise to appear in unnecessary regions. Furthermore, the perturbations are non-smooth and irregular because each pixel is optimized individually. To improve the perceptual quality of poisoned samples, we propose to replace the current $\\ell_\\infty$ regularisation with a Pixel Regularization (PR) containing soft pixel-wise $\\ell_\\infty$ loss and Total Variation Upwind (TVU) [30]:\n$\\mathcal{R}_{pix} (\\delta) = ||\\delta - M \\odot \\epsilon||_1 + TVU(\\delta; \\beta = 1)$         (9)\n$TVU(\\delta; \\beta) = \\sum_{v} \\left( \\sum_{q \\in \\mathcal{N}(p)} (\\delta(p) - \\delta(q))^2 \\right)^{\\frac{3}{2}}$        (10)\nwhere $M$ is a pixel-wise embedding mask based on the original image: $M_{i,j,k} = 1.0 - \\frac{|X_{i,j,k}|}{\\sum_{i,j,k} X_{i,j,k}}$, assuming the pixel value $X_{i,j,k} \\in [0,1]$. In Equation 10, $\\mathcal{N}(p)$ are neighbouring pixels of pixel $p$. Both terms in Equation 9 improve the smoothness of perturbations, with $TVU(\\delta)$ minimizing the difference between adjacent pixels and $M$ keeping the perturbation within the color range. As shown in Figure 3, the perturbations crafted with PR are smoother and less conspicuous than those crafted with the original $\\ell_\\infty$ regularization. Following previous works [31, 32], we also evaluate the imperceptibility of poisoned samples statistically using Peak Signal-to-Noise Ratio (PSNR) [33]. The average PSNR value of poisoned samples is reported in Table 5. Our regularization results in higher PSNR, corresponding to better perceptual quality. We note that an additional advantage of the soft $\\ell_\\infty$ regularization over the standard one is that it is differentiable, which enables better convergence for the poisoning objective, especially for lower values of $\\epsilon$ (see Appendix E.3)."}, {"title": "5 Accidental backdoor activations", "content": "For CLPBA to be stealthy, we argue that it needs to be source-specific and trigger-specific. That is, (1) the trigger should be discriminative so that the victim model does not incorrectly recognize similar-looking objects as triggers, and (2) only the source class can use the trigger to manipulate the victim model's prediction at inference time. We introduce two new metrics, False Positive Rate and Suspicion Rate, to test the specificity (and also stealthiness) of CLPBA:\n*   Suspicion rate (SR): The measurement of whether the victim model incorrectly misclassifies trigger samples from non-source classes as belonging to the target class.\n*   False positive rate (FPR): The measurement of whether samples from the source class can fool the victim model with false triggers (i.e., objects that resemble the attacker's physical trigger). Table 6: Measurement of FPR and SR for GM attack ($\\alpha = 0.1, \\epsilon = 16/226$). A detailed set-up of our dataset to evaluate SR and FPR is given in Appendix C. We note that the big difference in FPR between fake beard and sunglasses triggers is due to selection bias in our choice of false triggers. Between the two metrics, SR is more important because it directly impacts attack stealthiness if a benign client is misclassified as the target class with the trigger or similar-looking items. The issue of false triggers, measured by FPR, is challenging to solve as the attacker cannot anticipate all possible false triggers in the real world. However, the attacker can partially mitigate the issue of false source classes, as measured by SR, by adding repel terms to the adversarial loss $\\mathcal{L}_{adv}$ in Equation 1:\n$ \\mathcal{L}_{adv} = \\underbrace{\\frac{N_s}{\\mathcal{N}} \\sum_{x \\in D_s} L(F_\\theta(x), y_t)}_{adversarial \\ loss} + \\underbrace{\\sum_{i \\neq s} (\\sum_{x \\in D_i} L(F_\\theta(x), y_i) - L(F_\\theta(x), y_t))}_{repel \\ term} $          (12)\nHere, $N$ is the total number of classes, and $D_i$ is the dataset of trigger instances in class $i$. The repel terms ensure trigger instances from non-source classes are repelled from the target class classification, enhancing trigger specificity to the source class. As shown in Table 6, incorporating the repel term in the GM objective (Equation 4) significantly reduces SR while maintaining high ASR."}, {"title": "6 Conclusion", "content": "Through extensive experiments, our research underscores the vulnerability of DNNs to clean-label backdoor attacks in the physical world. We thoroughly analyze the influence of poisoning algorithms, physical triggers, and source-target class pairs on the effectiveness of these attacks. Additionally, we propose novel pixel and feature regularizations to improve the imperceptibility of attacks in both pixel and feature spaces. A significant limitation identified is accidental backdoor activations of the poisoned model due to false triggers or non-source classes, which could compromise attack stealthiness. To address this, we propose incorporating repel terms into the adversarial objective to enhance trigger specificity to the source class. Our findings highlight the critical need for robust defenses against clean-label physical backdoor attacks and pave the way for future research on improving the resilience of DNNs against such threats."}, {"title": "Altogether, the final form of poisoning objective is", "content": "$\\min_\\delta A + \\lambda_1 \\mathcal{R}_{pix} + \\lambda_2 \\mathcal{R}_{feat}$   (11)\nwhere A is the main poisoning objective, and $\\lambda_1, \\lambda_2$ are balancing terms for the regularizations. In our experiments, we set $\\lambda_1 = \\frac{10}{n}$ (n is the total number of pixels), and $\\lambda_2 = 0.05$. We note that $\\mathcal{R}_{feat}$ can degrade attack performance by constraining poison optimization, so only a small weight should be assigned to this regularization to mitigate the impact of filtering defenses."}]}