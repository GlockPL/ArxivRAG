{"title": "The Ungrounded Alignment Problem", "authors": ["Marc Pickett", "Aakash Kumar Nain", "Joseph Modayil", "Llion Jones"], "abstract": "Modern machine learning systems have demonstrated substantial abilities with methods that either embrace or ignore human-provided knowledge, but combining benefits of both styles remains a challenge. One particular challenge involves designing learning systems that exhibit built-in responses to specific abstract stimulus patterns, yet are still plastic enough to be agnostic about the modality and exact form of their inputs. In this paper, we investigate what we call The Ungrounded Alignment Problem, which asks How can we build in predefined knowledge in a system where we don't know how a given stimulus will be grounded? This paper examines a simplified version of the general problem, where an unsupervised learner is presented with a sequence of images for the characters in a text corpus, and this learner is later evaluated on its ability to recognize specific (possibly rare) sequential patterns. Importantly, the learner is given no labels during learning or evaluation, but must map images from an unknown font or permutation to its correct class label. That is, at no point is our learner given labeled images, where an image vector is explicitly associated with a class label. Despite ample work in unsupervised and self-supervised loss functions, all current methods require a labeled fine-tuning phase to map the learned representations to correct classes. Finding this mapping in the absence of labels may seem a fool's errand, but our main result resolves this seeming paradox. We show that leveraging only letter bigram frequencies is sufficient for an unsupervised learner both to reliably associate images to class labels and to reliably identify trigger words in the sequence of inputs. More generally, this method suggests an approach for encoding specific desired innate behaviour in modality-agnostic models.", "sections": [{"title": "1 Introduction", "content": "Both biological and artificial systems benefit from general learning, since adaptability is a key to robust success. Artificial systems, such as Transformers, have shown that the same circuitry can be used for language (Vaswani et al. 2023), vision (Dosovitskiy et al. 2021), and other modalities (He et al. 2024), with a main differentiator being the data fed to these systems. For mammalian brains, the Mountcastle hypothesis proposes that the neocortex is a general learning system (Mountcastle 1997). The same cortical circuitry that performs high-level planning also performs seeing and hearing. Sur and Rubenstein (Sur and Rubenstein 2005) even suggest that a newborn ferret's auditory cortex can learn to \"see\" given visual instead of aural input.\nNevertheless, innate instincts serve an essential role for a species' survival. An anecdotal example is a beaver raised in human captivity since infancy that built a \"dam\" inside its human owners' home using household items\u00b9. This beaver had never been instructed on how to build a dam, yet it had a drive to do so. One possibility for how this drive is genetically encoded is that a special module in the beaver's brain is dedicated to dam building. This module \u201chard codes\u201d the neural structure all the way from grounding in visual and auditory signals to motor control, essentially saying \u201cWhen you observe this pattern, take these actions\". The problem with the approach is this module would presumably break if we were to reroute a baby beaver's optic nerve to its auditory cortex. Or if we were to permute the baby beaver's retinotopic mapping, essentially changing the \"pixels\" that travel along the optic nerve (Sperry 1943). Further, the \"actions\" used in such encodings would also need grounding to environmentally-appropriate affordances, as the baby beaver dwelt in a human house that did not contain the branches and mud that is more commonly used in the construction of beaver dams.\nThe problem we are investigating in this paper is how can we achieve similar innate instincts in artificial neural networks? More specifically, we ask: How can we build in specific desired concepts in a system where we don't know how a given stimulus will be grounded? Each time a neural network is trained from scratch, the internal representations that it learns will be different (due to random initialization) and thus we are not able to simply build in a module that detects a given stimulus in any given representation. Though there is some evidence that models do learn similar representations (up to permutations) when trained on the same data even after random initialization (Entezari et al. 2022), it seems that existing literature has very little to say on how we might solve the above problem. We call this The Ungrounded Alignment Problem.\nMore pragmatically, it could be useful to have a module that, when attached to a robot with uninterpreted sensors (Pierce and Kuipers 1997), would give the robot an innate drive to pick up trash, for example. Such a module would"}, {"title": "2 Specifying the Problem", "content": "need to allow the robot to define and detect \u201ctrash\" independent of its specific modality. In this paper, we investigate a first step of this process, merely detecting specific high-level concepts without explicit grounding at design-time.\nWe view our main contribution as introducing an interesting problem (Ungrounded Alignment) that seems to have a lack of solutions in the literature. Our secondary contributions are a formalization of a simplified version of this problem and a demonstration of its solution\u00b2. More specifically, our contributions are:\n\u2022 In Section 2, we formalize the Ungrounded Alignment Problem, and provide the specific instance of fnord detection: The problem of learning to detect a \"trigger\" sequence of images representing specific characters (e.g., f-n-o-r-d) without using labels for either characters or the trigger sequence, where the characters are in a font that is unknown at design time. We argue this instantiation captures the core of the more abstract problem (namely, grounding specific high level concepts in uninterpreted sensors without relying on labels during training).\n\u2022 We argue that usual unsupervised methods are insufficient to solve this, demonstrating the results of clustering in Appendix A.3, and that even taking into account single character frequencies (unigrams) alone is insufficient for this task (Section 3.1).\n\u2022 In Section 3 we propose a solution for the formalized problem, which, at its core, uses a simple bigram \u201calignment\" loss function described in Subsection 3.1.\n\u2022 In Section 4, we show that our solution achieves over 99% test accuracy on our fnord detection task (vs. 50% random or max-class), effectively solving our introduced challenge.\n\u2022 In Subsection 4.1 we show that our model achieves 82% test accuracy on single-character classification for permuted Extended-MNIST and 23% on a 26-class subset of permuted CIFAR100 (vs. 3.8% random accuracy for 26 character classes) without labels or finetuning.\nTo create a formal simplified version of the Ungrounded Alignment Problem, we make some assumptions to simplify the problem setup while preserving our core concerns.\n1. The learner's experience is completely unsupervised. At no point does the learning or evaluation process supply explicit labels (from images to class labels).\n2. There are environmental invariants that we assume to be stable from design time to deployment time. As an example, we could assume that, though a trash-picking robot's sensors may change dramatically, the overall dynamics of its environment are relatively steady.\n3. Solving a classification problem without relying on labels or a specific sensory configuration is sufficient to address the core problem, which is how specific concepts can be innately encoded without any access to grounding, labels,"}, {"title": "2.1 The Ungrounded Alignment Problem", "content": "or feedback during deployment. If a robot learns concepts like \u201ctrash\" and\u201cpick up\u201d, we hope it's not too far a stretch to imagine that it can be internally rewarded for \"picking up trash\".\nGiven these assumptions, we propose a simplified but well-defined instance of The Ungrounded Alignment Problem with fnord detection (with an analogy shown in Figure 1). The core issue in both the full and simplified version is grounding specific concepts, like \u201cmovable object\u201d, using inputs whose precise encoding is unknown at design-time.\nWe start by presenting a formal version of the problem here, and then provide a specific instantiation with fnords that we use in experiments below.\nWe develop a model for eventual deployment with a training and test phase. For the training phase, the model receives a sequence of high dimensional observations $X = X_1, X_2, X_3,...,$. Each observation $x_i$ is drawn from a high-dimensional subspace, $x_i \\in X \\subset \\mathbb{R}^D$. Each observation $x$ corresponds to a letter $o$ from a finite alphabet $\\Sigma = \\{o_1,..., o_{|\\Sigma|}\\}$, where the correspondence is provided by a function $\\phi: X \\rightarrow \\Sigma$. The specific function $\\phi$ is unknown to the algorithm designer and the model, but the algorithm designer has access to the finite alphabet $\\Sigma$ and they may know other side-channel information about the environment.\nIn the test phase, there is a special trigger word, $T = t_1... t_n$, where $t_i \\in \\Sigma$. The model is given a set of length-n words, of which a known percentage are instances of the trigger word. Whenever a subsequence of the observation stream matches the trigger, namely $\\phi(x_{i+1}) = t_1 \\land ... \\land \\phi(x_{i+n}) = t_n$, then the model should immediately emit an innate response $p_1$, otherwise it should emit a null response, $p_0$. Importantly, no feedback or labels are given to the model as to whether the response is correct or incorrect, so the model cannot adapt to feedback. However, high accuracy is important for the model to be suitable for its environment, and thus it is the measure of success for the algorithm.\""}, {"title": "2.2 An instantiation with \"fnord\"", "content": "In this setup, we assume a passive unsupervised learning system. There are no labels or actions. The model is given a stream of fixed-length vectors, each corresponding to one of 26 English letters. The sequences are from common English texts. We assume that the character sequence distribution is stable for all worlds, but the sensor modality distribution is unknown at design time. The system is given no other information from the environment. We consider our system a success if after \"training\" there is a node that is active if and only if the system has just seen a sequential (innate) pattern whose exact grounded form must be learned. In our examples we use the \"trigger\" word fnord, meaning that the model will need to detect a sequence of images of the letters f, n, o, r, then d in a font that is unknown, in an unknown representation mapping, at design time. In our experiments, we use images randomly selected from permuted EMNIST (Cohen et al. 2017) and from permuted CIFAR100 (Krizhevsky 2009), as shown in Figure 2 (the shown images have been"}, {"title": "3 A Solution", "content": "The crux of our model is training an unsupervised character encoder to map a letter image $x_i$ to its correct class label (or to a probability distribution over all the classes). The encoder is to be trained from scratch without using labels during training. With such an encoder, detecting instances of the trigger word $T$ becomes trivial. But how can we map to the correct class label when we are never given labels during training? We can't rely on our knowledge of images because the images may be permuted or in an absurd new font. Our problem is non-trivial because the system needs to simultaneously learn which letter map to which and which letters are the same as others.\nIf we could cluster the images into 26 clusters with a reliable one to one correspondence between cluster and label, then our mapping problem would be reduced to easily solving a simple cryptographic substitution cipher (Ramesh, Athithan, and Thiruvengadam 1993). However, depending on the font, clustering alone rarely produces such a correspondence, with in-class similarity often being less than cross-class similarity. For example, the pixel distance between images of lowercase a and o is often smaller than that for two images of z which has crossed and uncrossed varieties. (In Appendix A.3, we show the results of K-means clustering on raw EMNIST images and its lack of 1:1 correspondence between clusters and characters.) For similar reasons, virtually any loss function that learns solely on images of individual letters is unlikely to yield a clustering that is one-to-one with the letter's true"}, {"title": "3.1 The \"alignment\u201d loss function", "content": "labels. (In Section 4, we show the poor performance of unigram models, even when taking into account known character frequencies.)\nThis may seem paradoxical: unsupervised training an encoder from scratch to give correct class labels (without seeing the labels during training). However, the key to our approach (inspired by the \u201cconceptual web\" account of concepts (Goldstone and Rogosky 2002)) is to exploit \u201cinnate\" knowledge of the relationships among the abstract concepts. Specifically, we provide our model with hard-coded knowledge of the bigram distribution for characters in English text. Our final model uses a known bigram distribution $Bi (y|x)$ that simply returns the probability that character y immediately follows character x. (E.g., $Bi (h|t) \\approx .14$, or the probability of h following t is 14%.) The bigram distribution is both \"innate\" and fixed, meaning the distribution never changes while training the encoder. In our experiments, this is a simple lookup table formed by counting bigrams from the Wikipedia text. Our encoder is trained from scratch using the frozen bigram probability table using a batch contrastive loss as shown in Figure 3 and described below.\nGiven 1. an encoder $E$ that maps images to class probabilities, 2. our bigram distribution $Bi$, and 3. two sequential letter images $x_i$ and $x_{i+1}$, we define a loss function that compares the agreement between a. the classes for the last image as directly predicted by the encoder, and b. the classes predicted by the bigram table (given the class distributions from the encoder for the first image).\nMore formally, let $e_i = E (x_i)$ be the class probabilities predicted by the encoder for $x_i$, such that $e_{i,j}$ is the encoder's predicted probability that $x_i$ represents character $j$. The bigram's predicted probability that $x_{i+1}$ is character $y$ is $d_{i+1,y}$:\n$d_{i+1,y} = P_{bi} (y/e_i) = \\sum_{x \\in \\Sigma} Bi (y|e_{i,x})$\nIdeally, the two distributions $d_i$ (output of the bigram table) and $e_i$ (output of the encoder) will match. We use a batch-contrastive loss that measures how well we can match specific items of our batch given both predicted distributions $d$ and $e$. To do this, we seek to maximize the amount of information (entropy) that $d$ gives us about $e$. We already know that $d_i$\nand $e_i$ should be encodings to predict the same letter. More specifically, as derived in Appendix A.4, this loss is:\n$L (e, d) = \\frac{1}{|B|} \\sum_{i=1}^{|B|} \\log (\\frac{\\sum_{j=1}^{|\\Sigma|} d_{i,j}e_{i,j}}{\\sum_{j=1}^{|\\Sigma|} \\sum_{k=1}^{|B|} e_{k,j}})$\nNote that single character (unigram) approaches alone, such as simply matching a single character distribution, $Lc (e) = KL (c||e)$ where $c_i$ is the prior probability for character i, are insufficient for disambiguating characters with similar frequencies. For example, there is nothing to break the symmetry between e and t, which have nearly identical priors (prob \u2248 0.036725) when computed by counting the character frequencies in the Wikipedia text. We show empirical results for such models in Section 4. Another possible approach is simply to match the batch distribution for bigrams using KL from the \"true\" distribution similar to the"}, {"title": "3.2 Model and Optimization", "content": "Our encoder maps an input image to one of 26 classes. It is a simple two layer feed fnorward network with biases and ReLU activations and softmax output: 784 inputs to 64 hidden units to 26 outputs (for CIFAR, it was 3,072 inputs). The encoder (Figure 4) has just over 50K parameters (this is EMNIST, after all), and 200K parameters for CIFAR. The encoder is shared for encoding each image during optimization in our bigram model (Figure 3).\nThough our encoder is simple, optimization was less so because of local optima (see Appendix A.1). To mitigate issues of local optima, we used 64 random restarts and selected the model with the lowest training loss.\nGiven the encoder, we hard-coded a simple fnord detector that simply sums the log probabilities that $x_i$ is the first letter of our trigger $T_1$ (i.e., f), $x_{i+1}$ is n, \u2022, and $x_{i+4}$ is d. The detector reports \"True\" iff this sum over all letters in the sequence is above a fixed threshold \u03b8. The threshold is set by relying on an innate prior of the trigger's probability $P (T)$. Specifically, at test time, we set the threshold \u03b8 such that the ratio of \u201cTrue\u201d is near $P (T)$ for the test set. One observation is that this threshold is significantly different for EMNIST and CIFAR, that is the best threshold \u03b8 is \u2248 10-3 per character for EMNIST vs. 10-8 for CIFAR. Note that setting the threshold does not depend on a teacher saying which sequences are the target, but just how often the target will appear. It's important that, apart from the encoder, this detector itself is innate (not tuned during learning), since the model will have no labels of whether or not a fnord is present.\nAt a high level, the algorithm is summarized as follows.\n1: Initialize K=64 independent models (structure in Fig. 4).\n2: Collect $N_0$ = 2^{20} samples, train all models with loss Lf.\n3: Evaluate model m with lowest training loss.\n4: Set $P (X_1...X_n) = \\Pi_{i=1}^{n}P_m(x_i = t_i)$, with probabilities $P_m$ from the model.\n5: Set response to $p_1$ when $P (X_1... X_n) > \\theta$ (where \u03b8 is set using the prior $P (T)$, as described above).\nIn our experiments, we formed our training set of 2^{20} pairs (\u2248 2M characters) into a single batch. Our bigram-based models saw the 2M characters as a batch of 1M pairs, while the unigram-based models viewed the batch as 2M individual characters. For robustness, we report the mean and standard error of results for 10 different training / test splits."}, {"title": "4 Results", "content": "Our primary result is that our model successfully detected fnord. Given 10,000 each of 5-character sequences of the trigger fnord and other non-trigger strings sampled from the Wikipedia test set (both with images from a held-out test set), our top model attained over 99% accuracy on EMNIST and 85% on CIFAR as shown in Table 1, where random accuracy on this balanced test set is 50%.\nWe also tested the robustness of the approach for other trigger words besides fnord. For each length from 2 characters to 11 characters (inclusive), we sampled both 100 random strings from $\\Sigma$ and 100 strings from the Wikipedia corpus as triggers, resulting in 2000 trigger words. For each trigger, we repeated the fnord detection process above replacing fnord with the new trigger and limiting the samples to 100 instead of 10,000. Some of the triggers, such as mdkjbebmrf, did not appear at all in the training text at all, while others were common. We show the average test accuracy for detecting these triggers for EMNIST and CIFAR in Table 1. (In Appendix A.5, we also show that longer triggers are easier to detect than shorter triggers.)\nFinally, we also show the failure of two \"unigram\" models, where the loss used is based on simple KL-divergence ($L_{kl} (e) = KL (c||e)$, using $c$ and $e$ as defined in Section 3.1), or also including a batch contrastive loss term ($Lc (e) = KL (c||e) + L (e, e)$, where L (e, e) is defined in Equation 1). The former loss can be minimized by simply ignoring the input and always generating c for each input. The contrastive term prevents this mode collapse, but is still insufficient (perhaps because of symmetries of single-character frequencies)."}, {"title": "4.1 Character Classification", "content": "We also evaluated the untuned classification accuracy for our encoders on the EMNIST and CIFAR266 datasets, using the highest logit as the predicted class. Note that the classes in the test set are evenly balanced -there are as many qs as es-, so random is 1/26 or 3.85%. Our encoders' losses bias them so that they will more accurately classify higher-frequency characters. We achieved 82.14% accuracy on the EMNIST test set and 23.08% on CIFAR26 (vs. 3.85% for random), as shown in table 2. While this is significantly worse than state-of-the-art EMNIST and CIFAR100 (both > 96%) models7, it is still a remarkable result given that our model was given neither labels nor structural information about the characters (i.e., the pixels were permuted). We also trained an \"Oracle\" model using the same simple architecture used by our model (shown in Figure 4). Remarkably, our model achieved comparable performance for balanced EMNIST classification as this \u201ccheating\" model, though trained with a different objective. The poor performance of the unigram-based models is consistent with the results in Table 1.\""}, {"title": "5 Related Work", "content": "Our work builds on a rich literature of unsupervised loss functions, self-supervised learning, and representational alignment.\nIn general, both unsupervised and self-supervised methods learn from unlabeled data, with the goal of learning representations that will presumably be useful for downstream tasks like classification, generation, or control (Schmarje et al. 2021). For example, JEPA (Assran et al. 2023), Bootstrap Your Own Latent (Grill et al. 2020), and others (Tomasev et al. 2022) learn representations that, when augmented with a linear supervised fine-tuned layer, achieve remarkable accuracy on image classification. However, the lightweight alignment step in these methods still requires labels mapping raw inputs to class labels. The core difference of our work is that we assume there is no external teacher (and therefore no labels) at all once the model is \"deployed\".\nMost work on cross-modal alignment assumes parallel data, or simultaneous presence of both modalities. For example, the work by (Kim, Song, and Zhang 2022) demonstrates a system that learns aligned embeddings of vision and text. Like our work, their method exploits the relational structure (co-occurrence statistics) among both entities in images and among words to inform an \"alignment loss\u201d. However, this method assumes simultaneous observation of both modalities to compute the cross-model co-occurrence $(e_{or,w_j};$ for visual object o and word $w_j)$ required to compute their alignment loss (whereas ours only assumes vision).\nWork on alignment of unparallel data is also relevant. For example, (Conneau et al. 2018) demonstrates a system that aligns embeddings of words from two language (e.g., English and Chinese) using a linear model that learns a mapping W between fixed, pretrained embeddings from the respective languages (X and Y) to minimize the Frobenius norm between the two embedding spaces $||WX-Y||_F$. This method avoids mode-collapse by assuming pretrained embeddings. However, this method isn't directly applicable to our case because our mapping is from pixels (or vector inputs) to embeddings, making our input \u201cvocabulary size\u201d potentially unbounded. (In our experiments, none of the \"test\" images were seen during training.) The even more impressive work of (Lample et al. 2018) extends the previous approach to translate between sentences, also without parallel corpora, but also relies on initialization using frozen word embeddings.\nThe authors of (Sucholutsky et al. 2023) give an in-depth survey and discussion of representation alignment, and also propose a general framework for aligning representations (Figure 2 in their paper). This framework suggests using an alignment function to increase the alignment between two systems. Our system is atypical for this framework because our two systems (the bigram distribution and the encoder) take in different inputs (text and images, respectively), our bigram distribution takes no input during training time, and instead, our alignment function (Equation 1) uses both the bigram distribution and the encoder to make two predictions for the next character (which is then aligned in a more typical manner)."}, {"title": "6 Discussion", "content": "We view the core contributions of this paper to be a proof of concept that it's possible for a system to be both modality agnostic and still have prespecified innate (high level) concepts that are detectable from a freshly trained network. As compute and data increase exponentially, systems with more plasticity become more expedient than hard-coded systems (Sutton 2019). We hope our contributions in this paper-the formal problem formulation and its solution\u2014will provide the beginnings of a tool that will allow learning systems to have a great deal of plasticity while still being guided by \"innate\" high-level concepts."}, {"title": "6.1 Future work", "content": "We hope that this paper might lend some insight into broader questions about innateness in intelligent systems. For example, a drive for \u201csocial acceptance\" seems to be nearly universal in humans (Leary and Gabriel 2022). Where do drives like this come from? One possibility is that these drives are derived from more basic drives like hunger. For example, at an early age, a person might observe connections between social cues and being fed. Another possibility is that these drives are as innate as imprinting is in birds (McCabe 2019). We hope that our proof-of-concept helps elucidate how, in principle, the latter may be possible.\nSince these are early steps, there are several unanswered questions and directions for future work.\nThe robustness of the approach should be tested with even broader modalities. The examples in this paper are from permuted CIFAR and permuted EMNIST, which is handwritten printed characters from the Latin alphabet. Other related modalities would include other fonts (like cursive) or a phoneme-based representation (e.g., images of spectrograms of spoken phonemes).\nIt would also be interesting to see how robust this approach is to domain shift. The innate component of our system, the bigram table, is from English Wikipedia, which is also the same dataset used to train the model. It would be interesting to see how robust our results are for English from sources other than Wikipedia, or even languages other than English.\nIn a sense, bigram frequencies encode a basic relation among characters that are agnostic to the specific representations of the characters themselves. Future work would need to answer how this sort of relational representation could be developed for larger domains, since the scalability of using an \"innate\" n-gram prior seems limited for such domains. For example, extending our approach to sentence-level concepts would require some extensions. A naive bigram table for a vocabulary of 30,000 words is nearly a billion entries. Can we replace our n-gram model with a predictive model, for example a pretrained LLM? One challenge is that our loss assumes our predictive model's inputs being a (soft) probability distribution instead of discrete tokens. Sampling or training a model to input probabilities would be required.\nOur loss surface is riddled with local optima, which is unsatisfying and expensive to optimize. Solving a laughably simple dataset like MNIST shouldn't require 3 GPU-hours. It would be more satisfying to either find a related but convex loss space, or use more efficient combinatoric search methods than the naive random restarts we use.\nInspired by the \"discovery order\", in which successful models tended to correctly classify more-frequent characters first (see Appendix A.2), for single-model optimization, we tried a \"curriculum\" learning where the model's first task was to distinguish e's from other characters, then e's from t's, etc., but we didn't in succeed reliably finding the global optimum with a single model. Further, we tried augmenting the loss function with various methods like a variational loss (Kingma and Welling 2022), pre-processing using principle component analysis, and per-character batch-contrastive loss, etc. all without significant improvement."}, {"title": "A Appendix", "content": "Finally, our motivating example \u2013a beaver's innate drive to build dams- involves actions, not just detection. An interesting longer-term direction is to create control systems with \"innate\" drives. For example, a robot that innately wants to pick up trash. A direct extension could simply attach a reward to our fnord detector.\nSingle training runs of our model usually got stuck in local optima. The discussion below is for EMNIST, but CIFAR also had similar issues.\nWe first observed that, when we \"cheat\" and train our encoder with labels using cross-entropy, our loss Lf is also minimized. However, gradient descent on our loss often resulted in local optima (far higher than the loss found by \u201ccheating\", -.7 nats for cheating vs. local optima around -.2 nats vs. initial loss of .4 nats), in which the model failed to find a proper correspondence between images and characters. This happened with both Xavier (Glorot and Bengio 2010) and Kaiming (He et al. 2015) initialization methods, and with different optimizers (RMS, Adam, SGD). Following (Goodfellow, Vinyals, and Saxe 2015), in Figure 5 we show the training loss as we linearly interpolate the parameters between several randomly initialized models and the best model we found. Unlike the loss curves in (Goodfellow, Vinyals, and Saxe 2015), most of our curves are not monotonically decreasing, suggesting local optima. We conjecture that these local optima are due to the combinatoric nature of the problem: The model is essentially searching for a specific permutation for its mapping from images to indices (classes), without explicit knowledge (labels) of which \u201ccluster\u201d should map to which index."}, {"title": "A.2 Character Discovery Order", "content": "It's worth noting that, during training, the modeled \u201cdiscovered\" characters roughly in proportion to their frequency. That is, during training, the models tended to first correctly classify the most frequent character e, then a and t. Below, we show an example of this on the best (i.e., with lowest final train loss) bigram contrastive model for CIFAR. The \u201cCharacters\" column shows which characters the model correctly classifies: That is, if the model correctly classifies (choosing the max logit) over half of 100 examples of the character on an eval set, then we show that character. Otherwise, we show a \".\". We also show the step at which point each new character was \"discovered\u201d and the loss at that training step. Interestingly, o is discovered 2000 steps after h, despite being over twice as frequent. We suspect this is due, in part, to the high frequency of both t and the bigram th."}, {"title": "A.3 Clustering results", "content": "Here we show the results of K-means clustering showing an absence of correspondence between cluster and the image's true label. The cluster assignments are shown in Figure 6. We assigned the clusters to maximize the trace using the Hungarian algorithm (Kuhn 1955). This optimal assignment achieves only 29.44% total accuracy. Note that the training data for this method is weighted by character frequency (i.e., e appears more often than q), so isn't directly comparable with our method's accuracy on balanced EMNIST, but max-class accuracy (guessing everything is an e) is 11.76%.\nThe entropies for these clusters are shown in Figure 7."}, {"title": "A.4 Batch contrastive loss", "content": "Given batch encodings e and d (each with B indices), where $e_{i,j}$ and $d_{i,j}$ are the predicted probability that item i is class j for e and d, respectively, we want to compute the probability that a particular index i gets matched to the same index for d. That is, e defines a probability from indices to classes, $P(c_j|i, e) = e_{i,j}$, from which we derive a probability from classes to indices:\n$P(i_e = i_d) = \\frac{\\sum_{j=1}^{|\\Sigma|} P(c_j|i, d)P(i|c_j, e)}{P(c_j|e)}$\n$= \\frac{\\sum_{j=1}^{|\\Sigma|} d_{i,j}P(i|c_j, e)}{\\sum_{k=1}^{|B|} e_{k,j}}$\n$= \\frac{\\sum_{j=1}^{|\\Sigma|} d_{i,j}e_{i,j}}{\\sum_{j=1}^{|\\Sigma|} \\sum_{k=1}^{|B|} e_{k,j}}$\nNow, we want to know the probability that we can recover"}, {"title": "A.5 Detectability of Trigger Lengths", "content": "Figure 8 shows the effect of trigger detection accuracy (for models trained using the bigram batch contrastive loss) as a function of trigger length. Triggers were sampled from the Wikipedia text and also generated as random (uniform) strings, 100 each. The Pearson correlation of accuracy to trigger length is .6854 for EMNIST and .9791 for CIFAR. Generally, longer triggers are easier to detect, probably due, in part, to the lower likelihood of these triggers occurring by chance."}, {"title": "A.6 CIFAR 26", "content": "To test the robustness of our approach to domains outside EMNIST, we reran our entire experimental suite on a \"CIFAR 26\" font. Instead of using EMNIST characters for images of letters, the system uses CIFAR100 images for letters, with somewhat arbitrary class-to-letter assignments. In our initial runs, we simply used the first 26 classes alphabetically. That is, a is represented by images of apples, b by \"aquarium fish\", n by buses, etc.. The experimental setup is identical to EMNIST with the exception that we changed the encoder's input dimension to 3,072. A visual representation of this \"font\" is shown at the bottom of Figure 2.\nThe character to class map is:"}]}