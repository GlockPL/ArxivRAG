{"title": "ER2SCORE: LLM-BASED EXPLAINABLE AND CUSTOMIZABLE\nMETRIC FOR ASSESSING RADIOLOGY REPORTS WITH\nREWARD-CONTROL Loss", "authors": ["Yunyi Liu", "Yingshu Li", "Zhanyu Wang", "Xinyu Liang", "Lingqiao Liu", "Lei Wang", "Luping Zhou"], "abstract": "Automated radiology report generation (R2Gen) has advanced significantly, introducing challenges\nin accurate evaluation due to its complexity. Traditional metrics often fall short by relying on rigid\nword-matching or focusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ER2Score, an automatic evaluation metric designed\nspecifically for R2Gen. Our metric utilizes a reward model, guided by our margin-based reward\nenforcement loss, along with a tailored training data design that enables customization of evaluation\ncriteria to suit user-defined needs. It not only scores reports according to user-specified criteria but\nalso provides detailed sub-scores, enhancing interpretability and allowing users to adjust the criteria\nbetween different aspects of reports. Leveraging GPT-4, we designed an easy-to-use data generation\npipeline, enabling us to produce extensive training data based on two distinct scoring systems, each\ncontaining reports of varying quality along with corresponding scores. These GPT-generated reports\nare then paired as accepted and rejected samples through our pairing rule to train an LLM towards our\nfine-grained reward model, which assigns higher rewards to the report with high quality. Our reward-\ncontrol loss enables this model to simultaneously output multiple individual rewards corresponding\nto the number of evaluation criteria, with their summation as our final ER2Score. Our experiments\ndemonstrate ER2Score's heightened correlation with human judgments and superior performance in\nmodel selection compared to traditional metrics. Notably, our model provides both an overall score\nand individual scores for each evaluation item, enhancing interpretability. We also demonstrate its\nflexible training across various evaluation systems.", "sections": [{"title": "1 Introduction", "content": "Automated radiology report generation (R2Gen), which produces free-text descriptions about visual findings in\nradiographic images, has seen substantial growth [1, 2]. This complex AI task requires understanding high-level clinical"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Evaluation Metrics for Radiology Reports", "content": "Radiology report metrics can be categorized as language metrics and clinical metrics."}, {"title": "Language Metrics.", "content": "for radiology report evaluations typically rely on structured assessments and direct comparison\nmetrics. Common approaches like BLEU [3], ROUGE [17], and METEOR [18] scores assess the textual similarity\nbetween the generated reports and a set of reference reports, focusing on aspects like n-gram overlap, precision, and\nrecall. Other metrics like BERTScore [19] are calculated using embedding generated by pre-trained models to measure\nthe similarity between the ground truth report and the generated report. However, these methods have significant\ndrawbacks. Firstly, they often do not capture the clinical relevance or the diagnostic accuracy of the content, as they\nprimarily focus on linguistic features rather than medical correctness. Furthermore, when applied to evaluating text\ngenerated by large language models (LLMs), such as those based on GPT architectures, these traditional metrics fall\nshort. The complexity and variability of text generated by LLMs mean that simple lexical or syntactic comparisons are\ninsufficient. LLMs can generate clinically plausible text that may be lexically varied but semantically similar to the\nreference standards. This variability can lead to evaluations that are not reflective of actual clinical usability or accuracy."}, {"title": "Clinical Metrics.", "content": "focus more on the clinical description in the radiology report. One prevalent metric in contemporary\nresearch is CheXpert [20], which mandates the extraction and labeling of 14 pathological entities as 'present,' 'absent,'\nor 'uncertain.' The accuracy of these labels is typically assessed using tools like CheXbert, which also utilizes cosine\nsimilarity from embeddings as a metric. Another common method is RadGraph [7], which identifies clinical entities and\ntheir relationships within reports. However, these extraction-based techniques are constrained by a fixed set of entities\nand strict matching rules, which can lead to issues with coverage and difficulty addressing the ambiguous cases often\nfound in reports. Although some hybrid approaches, such as RadCliQ and RadEval, attempt to amalgamate various\nmetrics, they too fall short of fully capturing the nuances of clinical descriptions due to the inherent limitations of\nextraction-based methods."}, {"title": "2.2 Large Language Model for Evaluation", "content": "Previous methods such as G_Rad [14] and FineRadScore [15] leverage online LLMs like GPT-4 and Claude-3 for\nradiology report evaluation, achieving strong Kendall's Tau correlations. However, these methods raise privacy concerns\nand rely on online access, limiting their practical application. On the other hand, GREEN [16], a parallel work to\nours, addresses these concerns by using an offline model, as we do. Yet, GREEN generates free-text explanations with\ncounts of clinical errors and matched findings, adding explainability but at the cost of complexity, which hampers its\nscore prediction accuracy. Moreover, while GREEN fine-tunes pretrained LLMs, it lacks a dedicated loss function\ndesigned to capture subtle quality differences, as seen in our method. Consequently, our approach not only outperforms\nGREEN with a higher Kendall's Tau correlation with human ratings (0.75 vs. 0.64) but also requires significantly lower\ncomputational resources (1x NVIDIA A6000 vs 8x NVIDIA A100 for training) and inference time. By incorporating\nmultiple evaluation criteria and breaking down scores into granular components, our model enhances interpretability,\nallowing users to identify which specific reason contributes to the overall score, with a high alignment to human\nassessments."}, {"title": "3 Method", "content": "Traditional NLP evaluation metrics typically assess the similarity between a machine-generated report x and a reference\nreport 2 using n-gram overlap. However, these metrics often fail to capture the semantic equivalence and clinical\nrelevance essential for accurate radiology report evaluation. To address these shortcomings, we introduce a new\nevaluation metric that better reflects the semantic content and clinical significance of the reports, aligning closely with\nhuman assessments.\nOur model not only provides an overall score but also delivers nuanced sub-scores to facilitate a more detailed\ninterpretation of the assessment. This approach leverages GPT-4 to generate training samples by scoring x against its\nreference \u00ee based on specified criteria. These samples are then paired up by our pairing rule and used to train a reward\nmodel with our proposed reward loss function to predict sub-scores. The summation of these sub-scores results in the\nfinal overall score. The overview of our framework is presented in Figure 1."}, {"title": "3.1 Generating Training Data by GPT-4", "content": "Recent studies have demonstrated GPT-4's capability in evaluating chest X-ray reports. When prompted with specified\ncriteria, GPT-4 can generate similarity assessments that statistically correlate with human evaluations, as\nconsistently verified in [11] and [12]. For example, in [11], GPT-4 achieved Kendall's Tau of 0.735 with radiologists'\nannotations using RadCliQ scoring system. In [12], GPT-4 scored a Kendall's Tau correlation of 0.531 with human\nratings using the MRScore scoring system. Building on this observation, we utilize GPT-4 to generate extensive scoring\ndata, including both reports and the corresponding scores, for training purposes. The process is elaborated as follows."}, {"title": "Defining Scoring Criteria.", "content": "Various assessment criteria have been reported in the literature. In this study, we investigate\ntwo scoring systems to demonstrate our model's versatility across different evaluation rules. The RadCliQ scoring\nsystem proposed in [9] evaluates both clinically significant and insignificant errors across six error categories: 1) false\nprediction of a finding, 2) omission of a finding, 3) incorrect location or position of a finding, 4) incorrect severity\nof a finding, 5) mention of a comparison absent in the reference impression, and 6) omission of a comparison that\nnotes a change from a previous study. The total score is the sum of the error counts, highlighting the importance\nof clinical findings. Differently, the MRScore scoring system proposed in [12] addresses both clinical findings and\nlinguistic concerns. It involves seven fundamental items from radiologists' expertise and literature review: \u201cimpression\nconsistency", "impression organs\", \\\"description of lesions,\\\" \u201cclinical history\", \u201ccompleteness\", \u201cgrammar\", and \u201cmedical\nterminology\", with a detailed explanation. Each item corresponds to an error type with yes/no answers and is assigned\na different weight (from {30, 20, 20, 10, 10, 5, 5} accordingly) to form individual item scores. The total score is\ncalculated as Total_score = 100 - \\Sigma i=1 Si \u00d7 Wi, where Si is error score of the i-th item and W\u2081 is the corresponding\nweight. With these scoring rules, GPT-4 can be prompted to score reports following these criteria, as elaborated below.\"\n    },\n    {\n      \"title\"": "Generating Scoring Training Dataset."}, {"content": "With a defined scoring system, we craft prompts that encapsulate the evaluation\ncriteria, guiding GPT-4 to assess radiology reports similarly to human evaluators. An example of a prompt can be found\nin the supplementary material. Utilizing the GPT-4 API, we generate reports of varying quality based on a randomly\nselected subset of ground-truth reports from the MIMIC-CXR dataset. For RadCliQ scoring, we randomly select around\n8000 ground-truth reports, each leading to three GPT-4-generated reports reflecting varied error levels, i.e., 0-2 errors,\n3-4 errors, and 5-6 errors. Each generated report is assessed for the total number of errors as well as individual error\nscores. Similarly, for the MRScore scoring system, we randomly select 1800 ground-truth reports, each with three"}, {"title": "3.2 LLM-based Reward Model", "content": "ER2Score is our innovative evaluation metric designed to be versatile across various evaluation frameworks. This\nLLM-based reward model leverages a pretrained language model, such as Llama3 [21], fine-tuning it to align with\nhuman evaluations using pairs of reports guided by our novel reward system. The core of ER2Score is its training\nprocess, which involves pairs of reports generated from the same ground-truth report but with different qualities. This\npairing mechanism is essential for calibrating the model to distinguish between different quality levels effectively.\nDuring training, the model learns to assign higher rewards to high-quality reports while simultaneously generating\nmultiple individual criterion scores. These criterion scores are critical as they provide detailed insights into specific\naspects of the report's quality. At the inference stage, the model predicts rewards for each individual criterion. These\nrewards are then summed to generate the final ER2Score. To ensure precise differentiation, we also introduce a scoring\nmargin for each criterion and the overall score. This margin enables the model to recognize and learn subtle differences\nin report quality, enhancing its evaluative capability."}, {"title": "Model Input.", "content": "Our model requires paired reports and their score margins as input. Each pair consists of an \"accepted\"\nreport and a \"rejected\" report, both derived from the same ground-truth report, with the \"accepted\" report having a\nhigher score than the \"rejected\" one. Figure 2 illustrates the pairing rule, showing the selection process for accepted and\nrejected reports and the calculation of their respective margins. In the example shown in Figure 2, a scoring system\nwith four individual evaluation items is used. Accepted and rejected reports are determined based on their total scores.\nThese reports, along with their ground-truth report, are then incorporated into a text prompt to fine-tune the LLM model\nfor report assessment. In addition to the reports, we calculate a list of margins for both the four sub-scores and the\ntotal score: margin' = score'accept \u2013 scorereject, where i = 1,\u2026\u2026,5 with i = 5 corresponding to the total score and\ni = 1,\u2026, 4 for sub-scores. A larger margin indicates a more pronounced quality discrepancy between the two reports,\nwhile a smaller margin suggests a lesser difference. Note that although the margin of the total score is always greater\nthan 0, the margins of the sub-scores are not necessarily positive."}, {"title": "LLM Model.", "content": "Our reward model, based on the Llama3 [13] backbone, incorporates a multi-reward head to generate the\nER2Score. Llama3 was selected for its exceptional language comprehension with just 6.8M trainable parameters over 7\nbillion in total. The multi-reward head is a linear projection layer mapping Llama-3's last layer feature map to an N \u00d7 1\nvector, where N is the total number of sub-scores. This model is fine-tuned using Low-Rank Adaptation (LoRA) [22]\nfor parameter-efficient fine-tuning (PEFT), allowing effective fine-tuning with minimal parameter changes. Training\npairs of \"accepted\" and \"rejected\" reports calibrate the model for reward prediction. During training, the model learns"}, {"title": "3.3 Margin Reward Enforcement (MRE) Loss Function", "content": "Considering a pair of generated reports < yw, y\u1fd2 > \u00b9 corresponding to the same i-th ground truth report x\u00b2, the accepted\nreport y receives a higher GPT-4 score s and the rejected report y a lower GPT-4 score si. Let s and si denote\nthe j-th sub-score of s and si, respectively, where j = 1,\u2026\u2026, N and N is the number of sub-scores for a specific\nscoring system. Note that although the total score s is greater than s\u012f, the sub-score si is not necessarily greater than\ns. Our objective is to train the model to discern the rankings of both individual and total scores of the report pair,\nformulated as follows:\nLind (Yu, y) = = (8 + )ReLU(-( - )\n+ twmii) + (1 - 1(s \u2260 si))ReLU(|ri \u2013 rii | \u2013 c),\nN\nN\nLtot (Yw, Yi) = ReLU(--) + m\u00b2),\nK\nj=1\nj=1\nLMRE = \\sum Lind(Yw, Yi) + ALtot (Yw, Y\u0390).\ni=1\n(1)\nHere ri and ri denote the j-th individual rewards assigned to the reports y and y, respectively. The margin between\nthe total scores s and si is denoted by m\u00b2 = s\u2030 \u2013 si, where m\u00b2 > 0. The individual \u201cmargin\" mi,i = s - Si s is\nnot necessarily positive. The variable tw acts as a flag: tw = 1 if m\u00b2;i > 0, otherwise tw = \u22121. The function 1(\u00b7) is an\nindicator function, returning 1 when the event occurs and 0 otherwise. K is the total number of report pairs.\nOur overall loss Loverall comprises two terms: the individual reward loss Lind and the total reward loss Ltot, balanced\nby the hyperparameter \u5165. An analysis of the model's behavior is as follows. For the individual reward loss Lind, if\nij\nthe ground truth scores have the relationship of s > s, i.e., mii > 0, a penalty is incurred when the reward r\nis larger than ri \u2013 m\u00b2; if s\u00b2 < si, i.e., m\u00b2i < 0, a penalty is incurred when the reward ri is smaller than\nrii \u2013 mii ; if s = s, a penalty is incurred when the absolute difference between the two rewards is larger than a\npreset small positive value c. In addition to minimizing the individual reward loss, we also regularize the total reward\nloss Ltot, i.e., when the total reward \\sum;r r of the rejected report yi is larger than \\sum; ri \u2013 m\u00b2, a penalty is incurred.\nMinimizing Loverall ensures that our model furnishes both individual and total scores, thereby offering nuanced insights\ninto the assessment results.\""}, {"title": "4 Experiments and Result", "content": null}, {"title": "4.1 Datasets", "content": "We evaluated the effectiveness of ER2Score by assessing its alignment with expert radiologist evaluations, ensuring that\nits predictions correlate closely with those of human experts. Our evaluation involved two datasets, ReXVal [23] and"}, {"title": "4.2 Performance on ReXVal Dataset", "content": "Correlation Analysis of Sub-criteria. Table 1 provides a quantitative evaluation of ER2Score on the ReXVal dataset,\nspecifically constructed based on the RadCliQ Scoring System. This assessment highlights significant alignment\nbetween ER2Score evaluations and expert radiologist judgments across various error categories, using Kendall's Tau\nand Spearman Correlation coefficients as metrics. Notably, the high correlation scores in categories such as \"False\nprediction of a finding\" (Kendall's Tau: 0.680, Spearman: 0.842) and \u201cOmission of a finding\" (Kendall's Tau: 0.507,\nSpearman: 0.673) demonstrate ER2Score's capability in accurately identifying common radiological errors, indicating\nits effectiveness in recognizing significant or typical lesions. Although ER2Score demonstrates strong correlations\nacross most sub-criteria, there are areas for improvement. For example, the scores for \u201cIncorrect location or position of\na finding\" (Kendall's Tau: 0.246, Spearman: 0.327) are relatively low, possibly because location and position details\nare often subtle and challenging to capture accurately. It is worth noting that this also highlights the advantage of\nER2Score over methods that provide only an overall score [9, 19, 7]. By providing scores for each sub-criterion,\nER2Score allows us to clearly identify specific areas where the model can be enhanced. The statistical significance of\nthe results is underscored by extremely low p-values across all categories, reinforcing the robustness of the correlation\nbetween ER2Score and expert evaluations. The overall high scores\u20140.751 for Kendall's Tau and 0.910 for Spearman\nCorrelation-further validate the reliability of ER2Score as an evaluation tool, highlighting its potential utility in clinical\nand research settings for assessing radiology reports."}, {"title": "Comparison with other metrics.", "content": "Table 2 compares the performance of different metrics using Kendall's Tau and\nSpearman correlation on ReXVal Dataset. The comparison is based on the total score. Unlike ER2Score, the existing\nmetrics cannot be customized to user-specific sub-criteria, making sub-score comparison impossible\u00b3."}, {"title": "4.3 Performance on Rad-100 Dataest", "content": "Accuracy analysis of sub-criteria. Since the scoring system used by Rad-100 is a binary format where the presence\nof an error is marked as 1 and the absence as 0 (check supplementary for detail), the results are multiplied by pre-\ndefined weights before forming the final score. Accordingly, we evaluate the accuracy of binary classification for each\nsub-criterion, as reported in Table 3."}, {"title": "Comparison with other metrics.", "content": "Table 4 provides a performance comparison of metrics using Kendall's Tau and\nSpearman correlation on the Rad-100 dataset. Similar to the previous analysis on the ReXVal dataset, we evaluate our\nER2Score against various NLG and clinical metrics. As observed, on the Rad-100 dataset, our ER2Score demonstrates\nsuperior performance, with a Kendall's Tau of 0.230 and a Spearman correlation of 0.293, both statistically significant\nwith a p-value of 0.003."}, {"title": "4.4 Performance Comparison of LLM backbones", "content": "Table 5 presents a performance comparison of various LLM backbones. Notably, Llama3 demonstrates superior\nperformance with a medium size of trainable parameters. To ensure the scoring system is easily deployable, we focused\non models with 7 billion parameters in total or fewer."}, {"title": "4.5 Ablation study of losses and hyperparameters", "content": "The loss we proposed comprises two terms: the individual reward loss Lind and the total reward loss Ltot. An ablation\nof the loss functions is given in Table 6. As shown, if we train Ltot alone for predicting sub-scores, Kendall's Tau will\ndrop from 0.751 to 0.740 for the total score, a sum of the sub-scores. If we train Lind alone, Kendall's Tau will drop\nfrom 0.751 to 0.738, demonstrating the effectiveness of the regularization from Ltot."}, {"title": "4.6 Qualitative Analysis", "content": "A visual example is provided in Figure 3, demonstrating how the ER2Score correlates with human ratings using the\nRadCliQ scoring system. As shown, the generated report inaccurately describes the severity of the \"left pleural effusion\"\n(highlighted in red), resulting in a high ER2Score for \u201cincorrect severity of a finding", "right pleural effusion\", leading to an \\\"incorrect location/position\nof a finding\", again perceived similarly by both the ER2Score and human ratings. Lastly, the generated report fails to\nmention the \u201cleft retrocardiac opacification": "leading to a score of '1.0' for \u201cfalse prediction of a finding"}, {"title": "5 Conclusions", "content": "ER2Score offers a human-correlated and explainable metric for evaluating radiology reports. It allows for more\nfine-grained scoring, aligning each item of the evaluation rule with its respective sub-score, therefore enhancing the\ninterpretability of assessment results. Leveraging GPT-4's human-like scoring capacity, we have tailored extensive\ntraining samples to fine-tune LLMs toward discerning report qualities using our designed reward loss. Our metric's\nadaptability allows for accommodating various scoring criteria.\nOur method has the following limitations. First, the level of explainability could be improved by adding detailed\nparagraph explanations, which are not currently included. Second, due to the costly nature of human evaluation, the\nscale of the test sets in this study remains limited. However, we need to emphasize that the scale of datasets in this work\nmatches that used in comparable works in the literature."}, {"title": "6 Ethics Statement", "content": "Our ER2Score model, which fine-tunes LLAMA-3 as a reward system, operates entirely locally once trained, eliminating\nthe need for any interactions with GPT-4 during inference. This local deployment ensures that there is no risk of"}]}