{"title": "VIDEOGUIDE: IMPROVING VIDEO DIFFUSION MODELS WITHOUT TRAINING THROUGH A TEACHER'S GUIDE", "authors": ["Dohun Lee", "Bryan S Kim", "Geon Yeong Park", "Jong Chul Ye"], "abstract": "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method.", "sections": [{"title": "1 INTRODUCTION", "content": "Text-to-image (T2I) diffusion models have greatly changed the way how visual content is created and distributed, enabling users to effortlessly generate creative images from detailed text descriptions. Now the AI community is looking deeper into the potential of T2I diffusion models, exploring their application to the higher dimensional field of video generation. Text-to-video (T2V) diffusion models aim to extend the capabilities of their image-based counterparts by generating coherent video sequences from text descriptions, handling both spatial and temporal dimensions simultaneously. However T2V diffusion models still show sub-standard performance regarding temporal consistency, and can lead to the generation of degraded samples. Poor temporal consistency is also the main challenge for a variety of tasks, such as creation of personalized T2V models. Hence, recent work aims to enhance various aspects of temporal quality, but suffers from problems such as degraded quality, slow inference, etc. In this work, we attend to the clear absence of a reliable method for refining the temporal quality of pretrained text-to-video (T2V) generation models, and propose a novel framework for improved generation that does not require any training or fine-tuning.\nSpecifically, we introduce VideoGuide, a general framework that uses any pretrained video diffusion model as a guide during early steps of reverse diffusion sampling. Choice of the pretrained VDM is flexible: it is either identical to the VDM used for inference, or it is freely selected from all existing VDMs. In any case, the VDM that acts as the guide provides a consistent video trajectory by proceeding in its own denoising for a small number of steps. The guiding model's denoised sample is then integrated into the original denoising process to guide the sample towards a direction with better temporal quality. Through interpolation, the sampling VDM is able to follow the temporal consistency of the guiding VDM to produce samples of enhanced quality. Such interpolation only needs to be involved in the first few steps of inference, but is strong enough to guide the entire denoising process towards more desirable results. Remarkably, interpolating information of the guiding model's denoised sample has the effects of providing the base model a better noise prior, even guiding the model to create samples that were previously unreachable. VideoGuide is a versatile framework in that any pretrained video diffusion model can be used for distillation in a plug-and-play fashion. By incorporating a superior VDM as a video guide, our framework can be used to boost underperforming VDMs into state-of-the-art quality. This is particularly useful when the relatively underperforming VDM possesses unique traits unavailable for the superior VDM.\nIn particular, we show two representative cases of how VideoGuide can be applied, with AnimateDiff and LaVie. In AnimateDiff, a motion module is trained that can be interleaved into any pretrained T2I model. The scheme works for any personalized image diffusion model and grants easy application of controllable and extensible modules, but not without consequences. Specifically, fixing the T2I weights limits interaction between the temporal module and generated spatial features, hence harming temporal consistency. Applying VideoGuide with an open-source state-of-the-art model without personalization capability as the guiding model, we can greatly enhance the temporal quality of AnimateDiff. This allows us to combine the best of both worlds: personalization and controllability is provided by the base model, while temporal consistency is refined by the guiding model. Likewise, LaVie is a multifaceted T2V model that offers various functions including interpolation and super-resolution in a cascaded generation framework, but shows substandard temporal consistency. Using VideoGuide, we can upgrade its temporal consistency with an external model while maintaining its multiple functions.\nThe synergistic effects that our framework can bring are not limited to these two cases but are, in fact, boundless. As powerful video diffusion models emerge, existing models will not become obsolete but actually improve through the guidance our method provides. Moreover, as VideoGuide can be applied solely during inference time, these benefits can be enjoyed with no cost at all. Our contributions can be summarized as follows:\n1. We propose VideoGuide, a novel framework for enhancing temporal consistency and motion smoothness while maintaining the imaging quality of the original VDM."}, {"title": "2 RELATED WORKS", "content": "The Diffusion Model. Diffusion probabilistic models have achieved great success as generative models. To address the significant computational cost that arises from operating in pixel space, Latent Diffusion Models (LDMs) learn the diffusion process in latent space. LDMs utilize an encoder-decoder framework where the encoder $\\mathcal{E}$ and the decoder $\\mathcal{D}$ are trained together to reconstruct the input data. This training aims to satisfy the relation $x = \\mathcal{D}(z_0) = \\mathcal{D}(\\mathcal{E}(x))$, where $z_0$ is the latent representation of the corresponding clean pixel image $x$. Thus the forward diffusion process in latent space is defined as follows:\n$z_t = \\sqrt{\\bar{\\alpha}_t}z_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon,$\t\\t(1)\nwhere $\\bar{\\alpha}_t$ is a pre-determined noise scheduling coefficient, and $\\epsilon \\sim \\mathcal{N}(0, I)$ represents Gaussian noise sampled from a standard normal distribution. The reverse diffusion process is directed by a score-based neural network, denoted as the diffusion model $\\epsilon_\\theta$, which is trained using the denoising score matching framework. The training objective for this model is formulated as follows:\n$\\min_{\\theta} \\mathbb{E}_{t,\\epsilon\\sim\\mathcal{N}(0,I)} \\|\\epsilon - \\epsilon_\\theta(z_t, t)\\|^2$.\t\t(2)\nFollowing the formulation of DDIM , the reverse deterministic sampling from the posterior distribution $p(z_{t-1}|z_t, z_0)$ is given by:\n$z_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}z_{0|t} + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon_\\theta(z_t, t)$\t\t(3)"}, {"title": "3 VIDEOGUIDE", "content": "3.1 VIDEO CONSISTENCY ON DIFFUSION TRAJECTORY\nThe DDIM formulation can be expressed as a proximal optimization problem:\n$z_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}z' + \\sqrt{1-\\bar{\\alpha}_{t-1}}\\epsilon_\\theta(z_t, t)\twhere $\tz' = arg \\min_z \\|z - z_{0|t}\\|^2$\t\t(7)"}, {"title": "4 EXPERIMENTS", "content": "Experimental Settings. In our experiments, we leverage multiple open-source Text-to-Video (T2V) diffusion models to explore the combined strengths of each. For the guiding diffusion model, we choose Videocrafter2 due to its strong performance in temporal consistency, as measured by the VBench benchmark. For sampling, we employ AnimateDiff for flexible personalization of video content, and Lavie to enhance video quality and increase frame count through super-resolution and interpolation techniques. This integration combines the temporal consistency of the guiding model with the advantages of the sampling model. All experiments were conducted using DDIM with 50 steps for sampling. For our experiments with AnimateDiff, we set $I = 5$, $\\beta = 0.5$, and $\\tau = 10$, and used the Butterworth filter with a normalized frequency of 0.25 and a filter order of n = 4. Additional experimental details are provided in Appendix A."}, {"title": "5 ANALYSIS", "content": "5.1 ABLATION STUDY\nImportance of Guidance Scale w. Recent study demonstrates that employing a high CFG scale ($w > 1.0$) in the early timesteps of diffusion sampling leads to off-manifold behavior. This phenomenon results in denoised samples exhibiting problems such as color saturation and abrupt transitions, which negatively affect the interpolation between samples during these timesteps. We solve this by applying a lower guidance scale w during the early stages of sampling, ensuring smoother interpolation between the denoised samples. This highlights the importance of clean interpolation in our method, as improper guidance can lead to sub-optimal performance. Further analysis about CFG and CFG++ can be found in Appendix B.\nParameter Selection. An analysis is performed to assess how varying parameters of the guiding diffusion model impacts temporal consistency. Specifically, we examine the effects of three factors: interpolation scale $\\beta$, number of interpolation steps $I$, and number of guidance sampling steps $\\tau$.\n5.2 PRIOR DISTILLATION\nDegraded performance due to a substandard data prior is an issue only solvable through extra training. However VideoGuide provides a workaround to this matter by enabling the utilization of a superior data prior. Fig. 5 demonstrates example cases. For all instances, generated samples are guided towards a result of better text coherence while maintaining the style of the original data domain. Additional examples of prior distillation are provided in Appendix E."}, {"title": "6 CONCLUSION", "content": "In this work, we introduced VideoGuide, a novel and versatile framework that enhances the temporal quality of pretrained text-to-video (T2V) diffusion models without the need for additional training or fine-tuning. Our approach provides temporally consistent samples to intermediate latents during the early stages of the denoising process, guiding the low frequency components of latents towards a direction of high temporal consistency. The samples provided are not confined to the base model; any superior pretrained VDM can be selected for distillation. By doing so, we empower underperforming models with improved motion smoothness and temporal consistency while maintaining their unique traits and strengths, including personalization and controllability. We demonstrate the effectiveness of VideoGuide on various base models, and prove its ability to enhance temporal consistency without sacrifice of imaging quality or motion smoothness compared to prior methods. The potential of VideoGuide extends far beyond the cases discussed, as VideoGuide ensures that even existing models can remain relevant and competitive by leveraging the strengths of superior models. As video diffusion models continue to evolve, new and emerging VDMs will only enhance the pertinence of VideoGuide over time, broadening the scope of VDMs utilizable as a video guide."}, {"title": "A EXPERIMENTAL DETAILS", "content": "A.1 PROMPT SELECTION\nIn all experiments, we utilize 800 prompts from various categories in VBench to evaluate the model's ability to generate across diverse categories.\nA.2 HYPERPARAMETER SELECTION\nWe employ a classifier-free guidance (CFG) scale of 7.5 during inference for both base models (AnimateDiff, LaVie) and FreeInit-applied cases. During interpolation of the denoised samples, we apply CFG++ reverse sampling with a guidance scale of $w = 0.8$ in DDIM 50-step sampling. After completing the interpolation step, we revert to CFG reverse sampling with a CFG scale of 7.5. In FreeInit, we use a Butterworth filter with a normalized frequency of 0.25, filter order n = 4, and perform 5 iterations, as recommended in prior work. The same filter is applied in our experiments with FreeInit. For AnimateDiff, we configure the guiding model with parameters $I = 5$, $\\beta = 0.5$, and $\\tau = 10$. In the case of LaVie, we set $I = 3$, $\\beta = 0.5$, and $\\tau = 10$ to optimize inference speed. Additionally, the $\\tau$ intervals are not uniformly spaced as in the standard 50-step DDIM sampling. To better leverage temporally consistent samples, we divide the remaining interval into 25 steps for reverse sampling during guidance steps. Also, we found that applying renoising to guidance sampling is more effective in improving consistency in the case of self-guidance. Therefore, we incorporated renoising during self-guidance in a similar manner as when using a external model for guidance.\nA.3 FIGURE EXPLANATION\nBase models used for Figure 3:\n(a) AnimateDiff with pretrained T2I model RealisticVision.\n(b) AnimateDiff with pretrained T2I model RealisticVision.\n(c) AnimateDiff with pretrained T2I model ToonYou.\n(d) AnimateDiff with pretrained T2I model FilmVelvia.\n(e) LaVie.\n(f) LaVie.\nBase model used for Figure 5: AnimateDiff with pretrained T2I model ToonYou."}, {"title": "B QUANTITATIVE ANALYSIS OF CFG AND CFG++", "content": "There may be concerns that the effectiveness of our method in improving consistency stems from the use of the CFG++ algorithm itself. To address this, we provide results for using CFG and CFG++ across the Base Model, Base Model + FreeInit, and Base Model + VideoGuide. The results demonstrate that CFG++ is particularly effective for interpolation. As shown in Tab. 4, metrics for Base and FreeInit decrease when CFG++ is used, and metrics improve only when CFG++ is applied to our interpolation scheme. This implies the significant positive impact on consistency of CFG++ within the proposed interpolation scheme, especially compared to CFG. Also, this supports the idea, as discussed earlier in Sec. 5.1, that smooth interpolation of denoised samples positively impacts model performance."}, {"title": "C USER STUDY", "content": "We conduct a user study to evaluate generated video samples using three criteria: Text Alignment, Overall Quality, and Smooth And Dynamic Motion, with all metrics scored on a 1 to 5 scale. A total of 30 participants provided ratings for each metric, offering comprehensive feedback on the generated videos.\nText Alignment\n\u2022 Measures how well the video corresponds to the prompt, focusing on semantic coherence.\n\u2022 Question: Do you think the videos reflect the given text condition well?\n(5: Strongly Agree / 4: Agree / 3: Neutral / 2: Disagree / 1: Strongly Disagree)\nOverall Quality\n\u2022 Assesses the video's visual consistency, image degradation, and aesthetic appeal.\n\u2022 Question: Do you think the video's overall quality is good? (rich detail, unchanging objects)\n(5: Strongly Agree / 4: Agree / 3: Neutral / 2: Disagree / 1: Strongly Disagree)\nSmooth And Dynamic Motion\n\u2022 Evaluates the naturalness and fluidity of the motion in the video.\n\u2022 Question: Do you think the video's overall motion is smooth and dynamic?\n(5: Strongly Agree / 4: Agree / 3: Neutral / 2: Disagree / 1: Strongly Disagree)"}, {"title": "D PSEUDO CODE", "content": "Pseudo codes regarding our algorithm are provided in the following page."}, {"title": "E MORE QUALITATIVE EXAMPLES", "content": "Additional samples are provided in following pages:\n\u2022 Supplemental examples of prior distillation.\n\u2022 Qualitative comparison for various base models.\n\u2022 Usage of VideoGuide to solve sudden frame shifts in LaVie samples."}]}