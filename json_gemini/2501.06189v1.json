{"title": "A Multimodal Social Agent", "authors": ["Athina Bikaki", "Ioannis A. Kakadiaris"], "abstract": "In recent years, large language models (LLMs) have demonstrated remarkable progress in common-sense reasoning tasks. This ability is fundamental to understanding social dynamics, interactions, and communication. However, the potential of integrating computers with these social capabilities is still relatively unexplored. This paper introduces MuSA, a multimodal LLM-based agent that analyzes text-rich social content tailored to address selected human-centric content analysis tasks, such as question answering, visual question answering, title generation, and categorization. It uses planning, reasoning, acting, optimizing, criticizing, and refining strategies to complete a task. Our approach demonstrates that MuSA can automate and improve social content analysis, helping decision-making processes across various applications. We have evaluated our agent's capabilities in question answering (using the dataset HotpotQA), title generation (using the dataset WikiWeb2M), and content categorization (using the dataset MN-DS) tasks. MuSA performs substantially better than our baselines.", "sections": [{"title": "1 Introduction", "content": "The concept of autonomous LLM-based agents is already well es-tablished, and several successful examples exist in various domains. LLMs have shown great promise in acting as general-purpose decision-making machines, thus called language agents, i.e., systems that use LLMs as a core computation unit to reason, plan, and act. To be successful in decision-making processes, LLM-based agents must be able to accomplish a given task autonomously. Although LLMs excel at retrieving information, they have limited reasoning skills, often finding it challenging to make logical inferences about social situations and comprehend social contexts and interactions. A key attribute in improving their responses through successive attempts is the application of advanced reasoning techniques.\nFormulating high-quality prompts is foundational for agent in-teractions, impacts their performance, and poses a challenge for non-AI experts. Existing research on prompt optimization tech-niques suggests mainly optimization principles based on deep rein-forcement learning. One disadvantage of deep reinforcement learn-ing is the large number of samples required to learn an effective policy, which can be impractical in complex data collection scenar-ios. Prompting techniques and systematic reasoning advancements have recently improved agents' search, information retrieval, and reasoning capabilities. Verbal reinforcement, precisely the Reflexion [21] method, helps agents learn from prior failings and improves learning over previous methods based on traditional reinforcement learning with gradient descent optimization. To facilitate the in-tercommunication of multi-agent systems and avoid hard-coding prompt templates in more complex systems, such as pipelines, new programming models have emerged that can abstract LLM calls and be parameterized to learn how to apply prompting, fine-tuning, and reasoning. State-of-the-art frameworks, such as DSpy [9] and TextGrad [29], have shown promising results in prompt optimiza-tion and overall system performance. The results have shown that they can substantially improve zero-shot accuracy, thus helping LLM agents answer complex questions efficiently and elicit sophis-ticated behavior."}, {"title": "2 Related Work", "content": "Reason. Advances in prompting methods, such as CoT [10, 22], and ReACT [28] have demonstrated capabilities to carry out several steps of reasoning traces to derive answers through proper prompt-ing (Reason-only or Reason+Act). Furthermore, recent in-context LLM agent-based learning techniques, such as Reflexion [21] and Self-Refine [15] (allows for iterative self-reflection to correct pre-vious mistakes), have demonstrated the feasibility of autonomous decision-making agents built on top of an LLM. These methods use in-context examples or verbal reinforcement to teach agents to learn. Specifically, Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as an additional context for the LLM agent in the subsequent trial. These methods significantly improve LLM's ability to learn in context and improve its performance, ro-bustness, and reliability for the tasks at hand. The advancements in complex linguistic reasoning tasks have triggered an interest in complex visual reasoning tasks. Multimodal LLMs (MLLMs) have succeeded in text-rich content understanding tasks, such as im-age captioning, visual question answering, and optical character recognition. Recently, successful reasoning techniques such as in-context learning and chain-of-thought have been applied to the visual domain [11, 20] to improve the reasoning capabilities of MLLMs. Therefore, we have also designed MuSA's computing units to support multimodal models for specific actions to extend its cognitive abilities.\nPlan. Planning is one of the most critical capabilities of agents and requires complicated understanding, reasoning, and decision-making progress [17]. Conventional methods rely on (deep) rein-forcement learning or policy learning. However, these methods have several limitations, as they require expert knowledge, have limited fault tolerance, and require many examples to train and optimize, making them impractical [8]. Recently, with the emer-gence of LLMs, integrating them as the cognitive core of agents has shown promising results in improving their planning ability. Specifically, techniques such as CoT [10, 22] and ReACT [28] are used more frequently in step-level planning (task decomposition) and execution. Other approaches, such as Reflexion [21] and Self-Refine [15], are mainly used to improve the agent's planning ability. Lastly, methods such as REMEMBERER [30] and MemoryBank [31] incorporate an additional memory module, in which information is stored and retrieved during planning. Several additional methods are available that involve selecting the best plan from a pool of plans or using external planners, which are outside the scope of this work. The concepts mentioned can be combined, and in our study, we have employed various techniques to develop MuSA.\nOptimize. LLMs are known to be sensitive to how they are prompted for each task, and optimization is usually done manually by trial and error, resulting in hand-crafted task-specific prompts that of-ten require a deeper understanding of a particular domain and are challenging to maintain for non-AI experts. Recent efforts to auto-mate prompt tuning are encouraging, and various approaches are being explored to achieve this goal. For instance, the Optimization by PROmpting (OPRO) [25] approach treats LLMs as optimizers. The EVOPrompt [7], a discrete prompt optimization framework, uses LLMs as evolutionary operators and is based on evolutionary algorithms to guide the optimization process. However, these meth-ods focus solely on learning prompts and may require additional training data to learn. On the contrary, generic frameworks, such as DSpy [9] and TextGrad [29] provide a more systematic approach for developing and optimizing multi-stage LM-based pipelines and agents that are domain, model, and input agnostic. Our MuSA uses prompt and plan optimization through TextGrad [29] in all interac-tions to maintain excellent performance in social content analysis.\nCriticize. LLMs occasionally may exhibit hallucinations, faulty code, or even toxic content, and to address these challenges, the CRITIC [7] method is introduced that uses LLMs to verify and rectify their output through external tools. Our work introduces a role akin to generating natural language critiques of plan outputs. Our critic functions as a form of assistance in preventing infeasible plans.\nRefine. LLMs do not always generate the best output on their first attempt. Self-Refine [15] is an approach that allows the LLM to improve the initial output through iterative feedback and refine-ment. Self-Refine uses the same LLM to generate, get feedback, and refine its outputs. In our work, we define the refiner as responsible only for refinement from feedback from the critic, and the planner handles the initial generation. This provides greater flexibility in orchestrating models with different capabilities.\nAct. LLMs often over-simplify or misrepresent textual or visual nu-ances, leading to a less accurate understanding of the content. More-over, in many cases, LLMs struggle to produce the correct answer consistently in one shot. The application of Retrieval-Augmented Generation (RAG) technology has emerged to address these lim-itations, allowing for more precise responses [12]. Approaches that repurpose knowledge from pre-trained models using few-shot prompting can enhance their responses. LLMs have been shown to outperform previous zero-shot methods and can sometimes be-come competitive with prior state-of-the-art fine-tuning approaches [1, 26]. However, these methods cannot effectively handle complex tasks that require the model to self-improve by learning from its own mistakes."}, {"title": "3 Methods", "content": "This section outlines the architecture of MuSA (Figure 2). First, we describe at a high level the coordinating units necessary to complete a task. Next, we elaborate on the functionalities of each unit that are part of MuSA's computing engine.\nFor our work, we assume a closed static environment E. We define a task t as a set of literals that describe a goal necessary for the agent(s) to build a plan. A plan \u03c0 = (a1, a2, ..., an) where ai \u2208 A for each i, and A is the action space, can involve a single action or"}, {"title": "3.1 Reasoner", "content": "MuSA has been equipped with in-context few-shot learning ca-pabilities, CoT, and self-reflective behavior to improve reasoning and decision-making. These prompting techniques can be activated since not every reasoning method suits every task. Few-shot and zero-shot CoT prompting allow LLMs to generate intermediate rea-soning steps explicitly before predicting the final answer. In the few-shot scenario, this is done by providing a few demonstration examples, while in the zero-shot scenario, it is accomplished with-out any demonstration examples. In the latter case, this is achieved by appending the phrase \"let's think step by step\" to the input. Self-reflection was applied to the input with a single strategy: \"ap-ply reflection to the following reasoning trace.\" The CoT and/or self-reflected responses are then used as input to the optimization process."}, {"title": "3.2 Planner", "content": "Current LLM agent planning solutions aim to map tasks to se-quences of executable actions. Similarly, the planner is responsible for constructing a plan that consists of a sequence of executable social content analysis actions. Closely related to our work Reflec-tion and Refinement [6, 8, 15, 21] methodologies encourage LLM to reflect on failures and refine the plan. We have introduced four new processes in planning: (i) reason, (ii) optimize, (iii) criticize, and (iv) refine. The entire process is described in Equation 1:\n$S_o = \\pi(E, t, O, p_{rsn})$\n$O_i = o(E, t, s_i, \\Theta, p_{rsn})$\n$C_i = c(E, t, S_i, O_i, \\Theta, p_{rsn})$\n$rf_i = rf (E, t, c_i, p_{rsn})$\n$S_{i+1} = \\pi(E, t, rf_i, O, p_{rsn})$\nwhere $s_o$ is the initial state, $\\pi$ is the plan process, E is the environ-ment, t the task, $\\Theta$ the set of LLM parameters, p the prompt for the task, rsn denotes the reasoning strategy attached to the prompt, i = {0, 1, ..., n \u2212 1} represents the time step, o denotes the optimize process, c denotes the criticize process, and rf denotes the refine process. The output is a set of actions and instructions (prompts) to be executed."}, {"title": "3.3 Optimizer", "content": "To optimize the response obtained from the planner or actor, we have employed TextGrad [29]. This automatic differentiation frame-work works similarly to the backpropagation method for training"}, {"title": "3.4 Critic", "content": "A critic model is employed when the dissimilarity between the planner's and optimizer's responses exceeds a specified threshold \u03b8. The critic evaluates the optimized response obtained through TextGrad [29] against the planner's response and selects the most suitable response. The activation of the critique mechanism relies on a task-related decision rubric and is controlled by an input threshold \u03b8. To achieve this, the embeddings of both responses are calculated, and their similarity is assessed using Jensen-Shannon divergence (JSD) [13]. In cases where the similarity of responses falls below the threshold \u03b8, a critic will not be called."}, {"title": "3.5 Refiner", "content": "The refining process gets feedback from the critic to refine the plan before execution. When the critic provides actionable feedback, the refiner translates the input into actionable instructions for the planner based on the feedback. Then, the entire process cycle starts again. Currently, the system allows only one trial before exiting and running with the existing plan, as the planner and optimizer may not converge."}, {"title": "3.6 Actor", "content": "We define an action space A = {a1, ..., an} for social content analy-sis. By combining different actions, MuSA can accomplish various tasks. MuSA receives information about a selected action through prompts."}, {"title": "3.7 External Tools", "content": "In specific actions, external knowledge is necessary, and we provide this specialized knowledge through external tools. This functions as a static read-only memory that cannot be updated and is more akin to semantic memory. Semantic memory is responsible for storing factual information about the world and plays a vital role in memory [14]. We aim to enhance MuSA's reasoning capabilities by providing this specialized external knowledge."}, {"title": "Algorithm 1 MuSA task solving", "content": "Input: t, reasoner, planner, optimizer, critic, refiner, actor, trials,$\\u03b8$ (threshold)\nOutput: r (response) //task-specific output\nr \u2190\u00d8\npt \u2190 CREATE-FROM(t) //prompt\n//plan execution\nfor ti = 0, ..., trials do\npr \u2190 REASON(pt, reasoner)\nrp \u2190 PLAN(pr, planner)\nro \u2190 OPTIMIZE(rp, optimizer) // optimizer feedback\nARO //actions\nif ti <trials-1 then\nP\u2190 ENCODE(rp)\nQ\u2190ENCODE(ro)\nif JSD(P||Q) \u2265 $\\u03b8$ then\npu\u2190 CREATE-FROM(rp, ro)\npc \u2190 CRITICIZE(pu, critic)\npt \u2190 REFINE(pc, refiner)\nelse\nbreak\nend if\nend if\nend for\n//action execution\nfor ai \u2208 A do\npr\u2190 REASON(p, reasoner)\nra\u2190 ACT(pr, actor) // actor response\nro \u2190 OPTIMIZE(ra, optimizer) // optimizer feedback\nri\u2190 ACT(ro, actor) // actor response\nr\u2190 (ri, r)\nend for\nreturn r"}, {"title": "4 Results", "content": "Environment setup. Initially, we assign a (different) LLM to the actor, optimizer, critic, and refiner units. Then, we define a task t and ask MuSA to generate an appropriate role description for the task at hand, which is used as a system role in all subsequent calls related to the task. For the role creation, we use a separate LLM that is not used as an actor, optimizer, critic, or refiner to avoid preference biases."}, {"title": "4.1 Experiments", "content": "To test MuSA, we have done experiments for each action in Table 1. In our experiments, we use the following acronyms: Re, A, and O denote reasoner, actor, and optimizer, respectively. CaR denotes zero-shot CoT and self-reflection reasoning strategy. For all our experiments, we set the temperature to 0 for Gemini [5] and 0.7 for Mistral [16] models. We also set the nucleus sampling to 0.99."}, {"title": "4.1.1 Plan", "content": "Planning is one of the most critical abilities of agents, and that often requires complicated understanding and reasoning. We provide real-world examples in Figures 1 and 6 that show a composite task assigned to MuSA. The planner decides which ac-tions to use to complete the task. The decision plan proposed by MuSA is shown in Figure 4. Initially, the planner proposes a plan (plan A), and the optimizer tries to optimize it (plan B). Since plans A and B have substantial differences, a critic model is introduced to criticize the proposed plans and provide feedback for improvement. In this example, the critic suggested using plan A, which better addresses the task. Since there is actionable feedback, the refiner creates refined instructions for the planner, and the entire process cycle starts again."}, {"title": "4.1.2 Question answering (QA)", "content": "The task of open-domain question answering, which corresponds to action 1 in Table 1, provides a measurable and objective way to test the reasoning ability of intelli-gent agents [27]. This task involves finding the answer to a question from an extensive text collection. Typically, it requires identifying a small set of relevant documents and extracting the answer from them. To illustrate how the actor and optimizer processes function in that case, we present an example in Figure 5, which contains only textual information but requires critical thinking and reason-ing to answer the question. The optimizer improved the actor's self-reflected output and selected the correct answer."}, {"title": "4.1.3 Visual question answering (VQA)", "content": "The visual information in online content is as essential as textual information. MuSA is designed to process multimodal information in specific actions, as seen in Table 1. VQA task corresponds to action 2 in Table 1. MuSA may use a multimodal LLM for both the actor and optimizer. The example in Figure 6 demonstrates how MuSA can help surface important information from images for publication. In this example, most of the information is contained in the image. Both actor and optimizer use multimodal LLMs."}, {"title": "4.1.4 Title generation", "content": "Title generation is a common task in web content generation and analysis for creating concise and infor-mative titles or headlines for content. To evaluate MuSA in the title generation task, which corresponds to action 3 in Table 1, we have used the Wikipedia Webpage suite (WikiWeb2M) [2] dataset, mainly introduced for studying multimodal webpage understand-ing. WikiWeb2M contains 2M pages with all the associated image, text, and structure data, but we randomly selected 100 web pages from the validation set for our study. MuSA actor is presented with the multimodal information from a section of a web page and is asked to generate a title."}, {"title": "4.1.5 Categorization", "content": "Categorization involves classifying content into predefined categories. It corresponds to action Categorization 4 in Table 1. We have used the MN-DS [18] dataset, which con-tains 10,917 news articles with hierarchical predefined categories, to evaluate MuSA in content categorization. We have performed stratified sampling to create a subset of 218 articles from the test set, containing examples from 17 first-level and 109 second-level categories. First, we classified the data into first-level categories (Level-1 in Table 4). Then, we used these first-level categories to filter and classify into second-level categories (Level-2 in Table 4)."}, {"title": "4.2 Ablation studies", "content": "4.2.1 QA: Impact of reasoning. The actor's performance is evaluated with two internal reasoning mechanisms: (i) a standard LLM API call with zero-shot CoT instructions and (ii) a standard LLM API call with zero-shot CoT instructions and self-reflection. Zero-shot CoT appends the phrase \"let's think step by step\" to the prompt without demonstrating examples, and self-reflection uses a single attempt."}, {"title": "4.2.2 QA: Impact of actor and optimizer roles", "content": "We evaluate the performance of the actor and optimizer roles using two settings: (i) the actor and optimizer use the same LLM, and (ii) the optimizer uses a stronger LLM. The selected actor's reasoning mechanism performs best as demonstrated in Table 5. We aim to improve the agent's reasoning performance while balancing performance, cost, and complexity. For our experiments, we have used as an actor smaller models up to 8B parameters and 8B or larger models for the optimizer. We also compare open and closed-source combinations of models for the actor and optimizer roles."}, {"title": "4.2.3 Title generation: Impact of multimodal inputs", "content": "The following experiments focused on multimodal inputs (i.e., text and image(s)). We used multimodal models to evaluate the actor in three different scenarios: (i) using textual input only (T), (ii) using multimodal input (MM), and (iii) a stepwise approach that uses textual followed by multimodal (T\u2192 MM) input. The actor uses zero-shot CoT and self-reflection as its reasoning mechanism, as this combination was the best-performed in our previous findings . All experiments use the Gemini-1.5-Flash-8B model as the actor."}, {"title": "4.2.4 Title generation: Impact of actor and optimizer roles for multimodal inputs", "content": "We validate the performance of the actor and op-timizer roles using two settings: (i) the actor and optimizer use the same multimodal LLM, and (ii) the optimizer uses a stronger multimodal LLM."}, {"title": "5 Discussion", "content": "MuSA. Artificial intelligence (AI) aims at the design of systems that can perform tasks that require human intelligence, and spe-cialized AI systems are often developed with a specific task(s) in mind assigned to an agent(s). This work studies the assistant-user scenario, where a task is initially given. MuSA will conceptualize the task into a specific plan that consists of action(s) and complete it autonomously through conversations with other LLM-based units. Often, humans can have this initial idea of a task but may not pos-sess the skills to accomplish it. An LLM-based agent specialized in this task can effectively complete it. In a multi-task environ-ment, several instances of MuSA can be used to complete tasks independently. Though general-purpose LLMs are powerful, many real-world applications require only specific abilities and domain knowledge, and this modular approach offers several advantages over traditional monolithic single-agent systems. MuSA is designed to be modular and extensible, consisting of several independent LLM-based units (reason, plan, optimize, criticize, refine, and act) that can be easily integrated into the system and serve different purposes.\nReason. However, a significant concern when applying LLMs in critical domains is their tendency to hallucinate, generating factu-ally incorrect or inconsistent responses while attempting to fulfill user requests. Step-by-step thinking is one of the most effective ways to improve performance and make the reasoning process more transparent. Adopting advanced reasoning prompting schemes, such as CoT and reflection, allows the actor/planner to reason and more efficiently solve a task. Our experiments have shown that the performance of each reasoning mechanism and their combination with the system seem to be model-specific.\nPlan. Planning in complex multimodal environments presents significant challenges for LLM-based agents. Many studies have proposed integrating memory modules to enhance the agents' ro-bustness and adaptability. Long-term memory becomes crucial in these complex settings, while short-term memory is typically suffi-cient for agents managing tasks in simpler environments. Our work uses a short-term memory mechanism and has proven sufficient for its needs.\nOptimize. In a previous theoretical study [4], the authors found that LLMs exhibit better error correction capacity and more accu-rate predictions when reasoning from previous steps is incorporated. Thus, combining the optimization unit with the planning/acting and reasoning processes has proven beneficial for our system, en-hancing the agent's ability to learn from their mistakes and solve tasks. Furthermore, this design choice is helpful because one can use weaker models for the act or plan process, and optimization can be achieved using open-sourced models at a smaller cost, for example, rather than relying on stronger and more expensive models.\nLimitations. The social content analysis tasks are numerous, and our system is initially designed to work with the four tasks listed in Table 1, which we have analyzed and presented in this study. We acknowledge this limitation and have included the development and analysis of additional social-content analysis actions in our plans."}, {"title": "6 Conclusion", "content": "This study introduces MuSA, an LLM-based multimodal agent that processes and analyzes online social content. Its computing engine integrates reason, plan, optimize, criticize, refine, and act processes which help MuSA to self-improve in complex scenarios. MuSA can plan based on social comprehension tasks, allowing it to execute more intricate tasks. Employing LLM agents can significantly as-sist human analysts by streamlining social content analysis-related tasks, eliminating the need for manual processes to discover and summarize social media discussions. Moreover, MuSA can poten-tially enhance and advance the quality of information in computa-tional social listening systems."}]}