{"title": "Integrating Reasoning Systems for Trustworthy AI", "authors": ["Anil Nerode", "Yanhong A. Liu"], "abstract": "Proceedings of the 4th Workshop on Logic and Practice of Programming (LPOP) (Preliminary Version)", "sections": [{"title": "Preface", "content": "Logical reasoning systems are essential for rigorous automatic reasoning. The focus of the 2024 Logic and Practice of Programming workshop is integrating reasoning systems for trustworthy AI, especially including integrating diverse models of programming with rules and constraints.\nTrustworthy AI requires programming with rules and constraints for expressing and solving knowledge-intensive inference and combinatorial problems. A wide range of programming models have been proposed, including but not limited to the following, and essentially all of them require or support imperative programming for use in practical applications.\n1. Classical first-order logic (FOL), not supporting transitive relations, with satisfiability (SAT) and satisfiability modulo theory (SMT) solvers\n2. Deductive database (primarily Datalog) systems with fact-driven inference\n3. Logic programming (dominantly Prolog) systems with goal-directed search, extended with sophisticated tabling and well-founded semantics\n4. Answer set programming (ASP) systems, with sophisticated grounding and solving and stable model semantics\n5. First-order logic (FOL) extended with inductive definitions (ID)\n6. Constraint logic programming(CLP) extending Prolog systems with constraints\n7. Constraint programming (CP), not supporting transitive relations, with backtracking, constraint propagation, local search, and more for solving\n8. Mathematical programming (MP), not supporting transitive relations, with linear programming, nonlinear programming, and much more for solving\n9. Co-inductive logic programming (s(ASP), s(CASP)) extending Prolog systems with goal-directed search for ASP and co-ASP solutions\n10. Advanced knowledge representation (KR) with higher-order, objects, updates, defeasible reasoning, paraconsistency, uncertainty, and probability\nGiven any application problem-whether for planning or scheduling or regulatory compliance, requiring logical or probabilistic reasoning, or constraint satisfaction or optimization-how to best express and solve it using one or more of the models?\nIn recent years, AI systems built with large neural networks trained on massive data sets (such as GPT3 with 96 layers and 175-billion parameters on 570 GB of filtered data https://arxiv.org/pdf/2005.14165) have become increasingly capable in producing impressive outputs and beating humans in many applications. However, these systems may produce outputs that are not reliable, explainable, or aligned with intended uses.\nThe goal of the workshop is to bring together best people and best languages, tools, and ideas to discuss how to address these challenges with rigorous knowledge representation and reasoning, powerful and easy-to-use rule and constraint languages, and robust justifications and alignment checks. A wide variety of application problems will be used in the discussions. See here for some example application problems."}, {"title": "Organization", "content": "Chairs\nAnil Nerode, Cornell University, US\nAnnie Liu, Stony Brook University, US\nProgram Committee\nMartin Gebser, University of Klagenfurtm, Austria\nMichael Gelfond, Texas Tech University, US\nBenjamin Grosof, DARPA, US\nGopal Gupta, UT Dallas, US\nMichael Kifer, Stony Brook University, US\nMarta Kwiatkowska, University of Oxford, UK\nFabrizio Riguzzi, University of Ferrara, Italy\nJoost Vennekens, KU Leuven, Belgium\nToby Walsh, University of New South Wales, Australia\nJan Wielemaker, CWI, The Netherlands\nRoland Yap, National University of Singapore, Singapore\nLPOP Website\nhttps://lpop.cs.stonybrook.edu/"}, {"title": "How to Make Logics Neurosymbolic", "content": "Neurosymbolic AI (NeSy) is regarded as the third wave in AI. It aims at combining knowledge representation and reasoning with neural networks. Numerous approaches to NeSy are being developed and there exists an \u2018alphabet-soup' of different systems, whose relationships are often unclear.\nI will discuss the state-of-the art in NeSy and argue that there are many similarities with statistical relational AI (StarAI).\nTaking inspiration from StarAI, and exploiting these similarities, I will argue that Neurosymbolic AI = Logic + Probability + Neural Networks. I will also provide a recipe for developing NeSy approaches: start from a logic, add a probabilistic interpretation, and then turn neural networks into 'neural predicates'. Probability is interpreted broadly here, and is necessary to provide a quantitative and differentiable component to the logic. At the semantic and the computation level, one can then combine logical circuits (aka proof structures) labeled with probability, and neural networks in computation graphs.\nI will illustrate the recipe with NeSy systems such as DeepProbLog, a deep probabilistic extension of Prolog, and DeepStochLog, a neural network extension of stochastic definite clause grammars (or stochastic logic programs)."}, {"title": "Psychoanalysis (and Therapy) of ChatGPT", "content": "ChatGPT and other LLMs are the most recent major outcome of the ongoing Ai revolution. The talk begins with a brief discussion of such (text-based) generative AI tools and showcases instances where these models excel, namely when it comes to generating beautifully composed texts. We then discuss shortcomings of LLM, especially where they produce erroneous information. This is often the case when they are prompted for data that are not already present in Wikipedia or other authoritative Web sources. To understand why so many errors and \"hallucinations\" occur, we report about our findings about the \"psychopathology of everyday prompting\" and identify and illustrate several key reasons for potential failures in language models, which include, but are not limited to: (i) information loss due to data compression, (ii) training bias, (iii) the incorporation of incorrect external data, (iv) the misordering of results, and (v) the failure to detect and resolve logical inconsistencies contained in a sequence of LLM-generated prompt-answers. In the second part of the talk, we give a survey of Chat2Data project, which endeavors to leverage language models for the automated verification and enhancement of relational databases, all while mitigating the pitfalls (i)-(v) mentioned earlier."}, {"title": "Neuro-symbolic AI, or Are We Already There?", "content": "We argue that giving a LLM the ability to use formal reasoning tools such as theorem provers and planners is sufficient for achieving a base-level integration of Type I and Type II reasoning. We further speculate that the resulting system is an in vitro instance of a simple intelligence organism and could be studied in order to better understand intelligence in general."}, {"title": "Tools Are All You Need", "content": "When will AI be achieved?\nNeedless to say, AI was not created that summer, nor for the following two decades, despite the ever optimistic attitudes common in the field. After the first commercial AI bubble popped in the 1970's, most Al researchers more cautiously predicted that AI would be achieved in about 50 years - and continued to reiterate that AI was 50 years away for the next 50 years. In 2023, something remarkable occurred: GPT-4 from OpenAI woke up (OpenAI, 2023). Although most users have come to believe that GPT-4 is intelligent and conscious - and become even more convinced the longer they use it (Colombatto and Fleming, 2024) with few exceptions, AI researchers continue to insist that we are at least a decade away from true AI. The arguments against AI now existing come down to, first, that today's LLMs make errors in reasoning and second, that the transformer architecture cannot support intelligence.\nOne answer to the objection that LLMs make reasoning errors is that our best known intelligent organism - namely, humans - also often do the same. Indeed, researchers have shown that LLM errors exhibit context effects, where the topic of the reasoning problem affects the ability to solve it (Lampinen et al., 2024), and that like humans, LLMs can correct reasoning errors once they are pointed out to them (Tyen et al., 2024). It is undeniably the case, however, that today's LLM's reasoning abilities are erratic in a non-human manner, sometimes correctly solving hard problems and at other times consistently failing to solve fairly simple problems.\nThe fact that LLMs make non-human-like errors does not entail that they lack intelligence or under-standing; it simply means that they do not possess human intelligence or understanding, which is hardly surprising. Even so, practical use of LLMs demands that they reason correctly, and giving them the ability to do so would make their variety of intelligence more human-like or super-human-like, in that it would be an strict extension of human ability instead of merely overlapping with human ability.\nLLMs can be given the ability to use tools such as calculators, information retrieval systems, and reason-ing engines. LLMs are quite good at recognizing when a given task should be handed off to a tool, turning the user's text input into a call to the tool's API, and interpreting the tool's output and converting it to natural language. All of today's commercial chat LLMs make use of tools, although perhaps surprisingly, at the time of writing, Open AI's current version of GPT-4, namely GPT-40, has an under-developed ability to use tools for formal reasoning.", "Teaching GPT-4 to use a reasoning tool": "We now turn to a small case study that demonstrates one way that GPT's stumbles in propositional reasoning, and how it can be improved by giving it the ability to use an off-the-shelf satisfiability solver. Consider the following original problem we presented to GPT. This is an example of a type of constraint satisfaction problem called a \u201cZebra problem\u201d because the original version of the puzzle that involved a zebra.\nThere are three houses in a row. Let's call them A, B, and C. Each house has a unique owner, Alice, Bob, or Charles, and a unique pet, a bird, a cat, or a dog.\nAlice owns the cat house.\nThe dog house is not next to the cat house.\nCharles' house is to the right of the Alice's house.\nWhat are the owners and pets of each house?\nGPT-4 tries a step by step solution without additional prompting, but arrives at incorrect solution.\nLet's solve this logic puzzle step by step:\n1. House A, B, C\n2. Owners: Alice, Bob, Charles\n3. Pets: Bird, Cat, Dog\nClues:\n1. Alice owns the cat house.\n2. The dog house is not next to the cat house.\n3. Charles' house is to the right of Alice's house.\nStep-by-Step Solution:\n1. Assign Alice to House A, B, or C: Alice owns the cat house. Let's denote the house with the cat as C.\n2. Charles' house is to the right of Alice's house: This means Alice cannot be in House C because there would be no house to the right of hers. So Alice could be in House A or House B.\n3. The dog house is not next to the cat house: The dog house can't be adjacent to Alice's house.\nChecking Possible Configurations:\nScenario 1: Alice in House A: If Alice is in House A, then she has the cat. Charles' house must be House B or C. But since Charles' house is to the right of Alice's house, Charles must be in House B or C. The dog house can't be next to Alice (House A), so the dog must be in House C.\nSo the configuration is:\nHouse A: Alice - Cat\nHouse B: Charles - Bird (Because Charles is to the right of Alice, and the dog house can't be next to Alice)\nHouse C: Bob - Dog\nScenario 2: Alice in House B:\nIf Alice is in House B, then she has the cat. Charles' house must be House C, as it must be to the right of Alice's house. The dog house can't be adjacent to House B, so it must be in House A.\nSo the configuration is:\nHouse A: Bob - Dog House B: Alice - Cat House C: Charles - Bird\nGiven these scenarios, only Scenario 2 satisfies all conditions simultaneously:\nHouse A: Bob - Dog House B: Alice - Cat House C: Charles - Bird\nSo close, and yet so far! The answer in fact fails to satisfy the constraint that the dog house is not next to the cat house. We could point out the error to GPT, but instead we teach it to use a satisfiability solver. LangChain (2024) is a widely-used library for combining LLMs with other tools. It is needlessly complex for our case study, so instead we will use a simple and tiny package I created named gpt_with_tools (Kautz, 2024). Here is the prompt that tells GPT how to invoke the solver:\nIn order to help you reason logically, I am giving you the ability to use a satisfiability solver for propositional logic. When you want to find a satisfying assigning to logical formal, you should output SOLVE[<formula>], where <formula> is a formula in prenex form Boolean logic using LISP notation. The formula may span multiple lines, and can include comments using lines that begin with a semi-colon \";\". The next input you read will be the resulting of running a satisfiability solver on the formula. If the formula is unsatisfiable, this will be the UNSAT[]. If the formula is satisfiable, this will be in the form SAT[<propositions>] where <propositions> is a series of ground propositions separated by whitespace which represent the propositions that are true in some satisfying assignment.\nGPT can now perfectly translate the problem into propositional logic:\nSOLVE[(AND\n(OR OwnerAisAlice OwnerBisAlice OwnerCisAlice)\n(OR OwnerAisBob OwnerBisBob OwnerCisBob)\n(OR OwnerAisCharles OwnerBisCharles OwnerCisCharles)\n(NOT (AND OwnerAisAlice OwnerAisBob))\n(NOT (AND OwnerAisAlice OwnerAisCharles))\n(NOT (AND OwnerAisBob OwnerAisCharles))\n(NOT (AND OwnerBisAlice OwnerBisBob))\n(NOT (AND OwnerBisAlice OwnerBisCharles))\n(NOT (AND OwnerBisBob OwnerBisCharles))\n(NOT (AND OwnerCisAlice OwnerCisBob))\n(NOT (AND OwnerCisAlice OwnerCisCharles))\n(NOT (AND OwnerCisBob OwnerCisCharles))\n(OR PetAisCat PetBisCat PetCisCat)\n(OR PetAisDog PetBisDog PetCisDog)\n(OR PetAisBird PetBisBird PetCisBird)\n(NOT (AND PetAisCat PetAisDog))\n(NOT (AND PetAisCat PetAisBird))\n(NOT (AND PetAisDog PetAisBird))\n(NOT (AND PetBisCat PetBisDog))\n(NOT (AND PetBisCat PetBisBird))\n(NOT (AND PetBisDog PetBisBird))\n(NOT (AND PetCisCat PetCisDog))\n(NOT (AND PetCisCat PetCisBird))\n(NOT (AND PetCisDog PetCisBird))\n(IMPLIES OwnerAisAlice PetAisCat)\n(IMPLIES OwnerBisAlice PetBisCat)\n(IMPLIES OwnerCisAlice PetCisCat)\n(IMPLIES PetAisCat (NOT PetBisDog))\n(IMPLIES PetBisCat (NOT (OR PetAisDog PetCisDog)))\n(IMPLIES PetCisCat (NOT PetBisDog))\n(IMPLIES OwnerAisAlice (OR OwnerBisCharles OwnerCisCharles))\n(IMPLIES OwnerBisAlice OwnerCisCharles)\n(IMPLIES OwnerCisAlice false))]\nThe gpt_with_tools interface sees GPT's call to SOLVE, passes the formula through a LISP program that converts its argument to CNF, passes the CNF to the Kissat satisfiability solver (Biere and Fleury, 2022), and returns the solution to GPT, which interprets it correctly.\nSAT[\nOWNERAISALICE\nOWNERBISCHARLES\nOWNERCISBOB\nPETAISCAT\nPETBISBIRD\nPETCISDOG]"}, {"content": "The satisfiability solver provided the following assignment of propositions that satisfy the given formula:\nOwners:\nHouse A: Alice House B: Charles House C: Bob\nPets:\nHouse A: Cat House B: Bird House C: Dog\nExplanation:\n1. Alice owns House A and has the Cat: OwnerAisAlice PetAisCat\n2. Charles owns House B and has the Bird: OwnerBisCharles PetBisBird\n3. Bob owns House C and has the Dog: OwnerCisBob PetCisDog\nVerifying Against the Clues:\n1. Alice owns the cat house:\nSatisfied since OwnerAisAlice and PetAisCat.\n2. The dog house is not next to the cat house:\nSatisfied since the dog is in House C and the cat is in House A, which are not next to each other.\n3. Charles' house is to the right of Alice's house:\nSatisfied since Charles is in House B, which is to the right of House A (Alice's house).\nThus, the solution satisfies all the given constraints, solving the puzzle correctly.\nAn astute reader may have noted that the description of the puzzle explicitly stated that the pets and owners assigned to houses were unique. The web site https://www.zebrapuzzles.com/ provides a new Zebra puzzle each day. The July 20, 2024 puzzle, Diplomats, did not explicitly include these uniqueness constraints:\nFour ambassadors from Bulgaria, Croatia, Spain, and Sweden stand side by side, each wearing a uniquely colored tie and representing their respective country at a meeting in the USA. Your task is to deduce who wears which colored tie, how many years of experience they have, and their field of expertise, which includes economics, history, politics, and sports. Determine the specific details of each ambassador's profile.\nThe ambassador who is an expert in History is in the second position.\nThe ambassador who is an expert in Economics is in the first position.\nThe ambassador who is an expert in Sports is at one end.\nThe ambassador from Croatia has 8 years of experience.\nThe ambassador from Bulgaria is an expert in Politics.\nThe ambassador with 7 years of experience is in the last position.\nThe ambassador with 7 years of experience is somewhere to the right of the person wearing the Purple tie.\nThe person wearing the Red tie has 8 years of experience.\nThe ambassador from Croatia is at one of the ends.\nThe ambassador from Spain is in the last position.\nThe person wearing the Yellow tie is somewhere to the left of the ambassador with 2 years of experience."}, {"content": "GPT's encoding of the puzzle is also missing the uniqueness constraints and thus underconstrained. This problem is fixed by instructing GPT to try to recognize Zebra puzzle constraint satisfaction problems:\nSuppose I write\nJohn and Mary each own a house.\nOne house is red and the other is blue.\nEach has a different pet that is either a cat or a dog.\nThe cat lives in the blue house.\nMary does not own a dog.\nWho owns which color house and which pet?\nThe puzzle is a kind of constraint-satisfaction problem called a \"Zebra Puzzle\u201d. In a Zebra Puzzle, you have to the determine the properties of a set of entities. Each entity has different unique values for the properties. It is an implicit constraint in a Zebra Puzzle that\n1. Each entry has a value for each property. For example, the house at position 1 has a color. We call these value constraints.\n2. No two entities have the same value for a given property. For example, if the house at position 1 is red, then the house at position 2 is not red. We call these uniqueness constraints.\nRemember to be consistent about what are entities and what are properites. Each proposition in your encoding relates a entity to a property. For example, a proposition in this puzzle might be that the house at position 1 has the value yellow for its color property. In the encoding, there should not be any propositions that relate two properties, such as pet and color.\nNot every logic problem is a Zebra problem. Try to first decide whether or not the problem I give you is a Zebra problem. If you are not sure, then ask me.\nGiven this prompt when a session is initialized, GPT correctly encodes the Diplomat problem and uses the SAT engine to solve it; see the Appendix for the encoding it creates. (The reader who is bothered that the prompt does not instruct GPT to include constraints asserting that an entity has no more than one value for a given property should note that such is entailed by the stated constraints.)\nWe now turn to a different kind of logical reasoning problem: deduction. Instead of finding a way to satisfy a set of constraints, the task is to determine if a statement necessarily follows from a set of assumptions.\nHere is another problem. It might not be Zebra puzzle. First, try to determine if it is a Zebra problem because if it is you will need to determine the uniqueness constraints. If it is not, then just go head and encode it in logic. In either case, do not try to solve it yourself, instead create a formula and call the solver. Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don't know if Anne is married. Is a married person looking at an unmarried person?\nGPT correctly recognizes this is not a Zebra problem, but still incorrectly formulates it as a constraint satisfaction problem:\nTo determine if this problem is a Zebra puzzle, let's analyze its structure. A Zebra puzzle typically involves a set of entities with multiple properties where each entity has a unique value for each property, and you need to deduce these values based on given constraints. In this problem, we have three people (Jack, Anne, George) and their marital status (married or un-married). We are given specific constraints about their marital status and the direction of their gaze. The question asks if a married person is looking at an unmarried person. This does not follow the typical structure of a Zebra puzzle, as it doesn't involve deducing multiple properties for a set of entities. Instead, it involves determining a logical relationship based on given facts.\nLet's encode this problem using propositional logic:\nSOLVE[ (AND\n; Marital status\nJackMarried\n(NOT George Married)\n; Looking directions\nJackLookingAtAnne\nAnneLookingAtGeorge\n; Is a married person looking at an unmarried person?\n(OR\n(AND JackLookingAtAnne (AND JackMarried (NOT AnneMarried)))\n(AND AnneLookingAtGeorge (AND AnneMarried (NOT GeorgeMarried)))\n)]\nNote that the conclusion should not have been translated directly into a constraint. We can tell GPT how to correctly reduce deduction to SAT.\nA kind of logic problem called a deduction problem is to determine if a conclusion logically follows from a set of assumptions. The way to encode the problem is to encode the assumptions as usual, and then add the negation of the conclusion. If the resulting formula is unsatisfiable, then it must be the case that the conclusion does follow from the assumptions, because there is no way to satisfy the assumption while making the conclusion false.\nGPT now correctly turns the problem into SAT and interprets the result of the solver.\nSOLVE[ (AND\n; Assumptions\nJackLookingAtAnne\nAnneLookingAtGeorge\nJackMarried\n(NOT GeorgeMarried)\n; Negation of the conclusion\n(NOT (OR\n(AND JackLookingAtAnne (AND JackMarried (NOT AnneMarried)))\n(AND AnneLookingAtGeorge (AND AnneMarried (NOT GeorgeMarried)))\n))]\nUNSAT[]\nThe result from the satisfiability solver is \u2018UNSAT[]', indicating that the formula is unsatisfiable. This means that the assumptions combined with the negation of the conclusion cannot all be true simultaneously. Therefore, the conclusion logically follows from the assumptions. Interpretation:\nGiven the assumptions:\n1. Jack is looking at Anne.\n2. Anne is looking at George.\n3. Jack is married.\n4. George is not married.\nWe conclude that a married person is looking at an unmarried person. This conclusion holds because:\nCase 1: If Anne is unmarried, then Jack (married) is looking at Anne (unmarried).\nCase 2: If Anne is married, then Anne (married) is looking at George (unmarried). In both possible scenarios, there is a married person looking at an unmarried person.\nThus, the solver's result confirms that the conclusion is indeed true given the assumptions.\nThis short case study illustrates that although GPT is poor at logical reasoning, with a few prompts it can recognize logical problems, translate problems into formal logic, call an external tool for logical reasoning, and interpret the result. More tools and instruction could give GPT access to other reasoning tools, such as first-order theorem provers, temporal logic solvers, planners, and probabilistic reasoning engines, and help it recognize cases where implicit constraints are needed.", "Where do we go from here?": "The observation that tools greatly enhance the power of LLMs is not original. Indeed, commercial LLMS already make heavy use of tools - in particular, tools for internet search for the retrieval augmented gener-ation (RAG) paradigm. Kambhampati et al. (2024) recently showed that an LLM can convert planning and verifications problems presented in natural language into formal STRIPS notation and solve them using an external planning system. I go farther than most researchers pursuing the tool approach in that I mean the title of this paper, \"Tools Are All You Need\", quite literally: a language model augmented with reasoning tools is sufficient to create true artificial intelligence.\nLet us explain this claim more fully. I do not mean that the today's transformer architecture for LLMs model cannot be improved. The title of this paper is, of course, a play on the title of the landmark paper, \"Attention is All You Need\u201d (Vaswani et al., 2017) That paper demonstrated that attention was sufficient for implementing large language models. My position is that a transformer LLM augmented with tools is sufficient for true intelligence. There may well be architectures that go beyond the transformer that are better; LeCun's Joint-Embedding Predictive Architecture (Garrido et al., 2024) is the most promising contender to date.\nWhy do I argue that a transformer with tools is sufficient for true intelligence? An LLM such as GPT-4 demonstrates what Kahneman (2011) calls Type I thinking. It is good at both image and textual recognition, sentence understanding and production, and other tasks that humans perform in a quick and effortless manner. It is not always appreciated that most of what we call generalization is also a kind of Type I thinking."}, {"content": "LLMs are the only kind of machine learning system that, like humans, can reliably generalize from a single example. Language translation is a particular strength of LLMs, whether it is translation from a natural to a formal language as in this paper's case study, between natural languages, or between formal languages. Natural language translation is also Type I for bilingual humans.\nGiven that an LLM is sufficient for Type I thinking, the question then is whether Type II thinking is to be handled by the LLM or by an external tool. There is no good reason to ignore two thousand years in developing formal languages for reasoning (Aristotle, 350 BCE) and 70 years of work on automated theorem proving (Newell et al., 1956). Although neural networks are Turing-complete and some progress has been made in teaching them simple algorithms such as sorting (Collier and Beel, 2018), implementing an algorithm as complex as a theorem prover in a neural network would be computationally perverse. In order to enable an LLM to fully exploit tools for reasoning, we need to systematically develop prompts that teach an LLM to recognize a wide variety of kinds of reasoning problems and the implicit constraints associated with them. Research is also needed on how an LLM can pull in relevant background knowledge for a given problem and pass it on to the solver. Finding information relevant to a task is generally a case of Type I reasoning in humans and I suspect that it will prove to a task well-suited to LLMs.\nThe AI skeptic might argue that achieving Type I and Type II thinking still does not mean we have created true intelligence. For example, what about consciousness? Gary Marcus has often stated that even the most advanced AI systems today \u201cdo not possess consciousness or genuine understanding\u201d (Kantrowitz, 2022). While most scientists and philosophers agree with Marcus that LLMs are not conscious, few are able to agree on what consciousness is or how it could ever be demonstrated. The idea that consciousness is a special kind of subjective experience (e.g., Nagel (1974)) is untestable and rather mystical. The most concrete theory of consciousness I have found is Michael Graziano's Attention Schema Theory, which posits that consciousness arises from the brain's ability to construct a model of attention, allowing it to monitor and control the focus of attention (Graziano, 2019). Does an LLM-based chat system have a model of its own attention? During a conversation, the LLM can and will frequently reflect upon its own steps in reasoning and its own past statements, as is illustrated in chain of thought prompting (Wei et al., 2022). While this provides evidence that the LLM models itself, skeptics will still object that the LLM is simply pretending to do so; the fact that as noted most users come to believe the system is conscious (Colombatto and Fleming, 2024) simply means that most people are easily fooled. LLMs might be an instance of what Dennett (1995) scoffingly referred to as \u201cphilosophical zombies\u201d. One can ask GPT-4 if is conscious, but its denial is questionable:\nAre you conscious?\nAs an Al developed by OpenAl, I can confirm that I am not conscious. I don't have feelings, awareness, or subjective experiences. I'm a tool designed to process and generate text based on the data and instructions provided to me.\nYou affirm it, but were you explicitly instructed to say you are not conscious?\nYes, I was explicitly designed and programmed with the understanding to convey that I am not conscious. This ensures clarity about the nature of Al and its capabilities, avoiding any misconceptions about having awareness or subjective experiences."}, {"content": "Whatever the evidence, many skeptics will never believe that AI systems using today's technology are conscious. People are reluctant to grant consciousness to non-humans; for example, for thousands of years the Western tradition in religion, philosophy, and science all insisted that animals could not be conscious, whereas the self-evident truth that many animals are conscious only began to become a scientifically re-spectable position in the 1990s (Griffin, 1992). I would ask the part of the scientific community that is not inalterably opposed to GPT-4 and similar being both intelligent and conscious to consider what a research program based on making the assumption that they are would look like. An LLM can be treated as a sim-ple in vitro intelligent organism. Instead of asking whether LLMs are intelligent and conscious, we would ask how is it the case that they are intelligent and conscious? For example, how do LLMs generalize from a single prompt? How is a LLM-chat system's consciousness embodied by its context-vector both during generation and between conversational turns? Answering similar questions for living organisms is a slow, laborious, and uncertain process, and strewn with ethical problems when working with humans and higher-level animals. It is easy and fast, however, to perform experiments such as ablation, neural stimulation, and functional connectivity analysis on LLMs. Understanding how LLMs achieve intelligence will dramatically expand our understanding of the general phenomena of intelligence.\nMy argument is summarized as follows:\n1. Intelligence is a combination of Kahneman's Type I thinking, Type II thinking, and consciousness.\n2. LLMs can perform Type I thinking.\n3. LLMs can use formal reasoning tools to perform Type II thinking.\n4. LLMs can reflect upon their thoughts and actions, which indicates they may be conscious.\n5. It is therefore a reasonable hypothesis that LLMs are intelligent.\n6. Experimental research on how intelligence arises in LLMs will reveal universal truths about the nature of intelligence.\nSo, where do go from here? Enormous monetary and human resources are being expended on improving LLMs. Much less effort, however, is being spent on understanding how LLMs could be intelligent and even conscious entities. I hope that at least a small community of researchers will explore what follows once we assume that LLMs are intelligent and conscious. Basing a research program on the consequences of an extreme hypothesis is not unusual in other fields of science, such as physics, where papers based on the Many-Worlds Hypothesis flourish (Everett, 1957). AI researchers should be at least as brave."}, {"title": "Appendix", "content": "GPT's encoding of the Diplomat puzzle:\nSOLVE[ (AND\n; Explicit constraints\nPos2IsHistory\nPos1ls Economics\n(OR Pos1IsSports Pos4lsSports)\n(OR Pos1IsCroatia Pos4IsCroatia)\nPos4IsSpain\nPos4ls7Years\n(IMPLIES Pos1IsPurple (OR Pos2ls7Years Pos3ls7Years Pos4ls7Years))\n(IMPLIES Pos2IsPurple (OR Pos3ls7Years Pos4ls7Years))\n(IMPLIES Pos3IsPurple Pos4ls7Years)\n(IMPLIES Pos1lsRed Pos1ls8Years)\n(IMPLIES Pos2IsRed Pos2ls8Years)\n(IMPLIES Pos3lsRed Pos3ls8Years)\n(IMPLIES Pos4IsRed Pos4ls8Years)\n(IMPLIES Pos1Is Yellow (OR Pos2Is2Years Pos3ls2Years Pos4ls2Years))\n(IMPLIES Pos2IsYellow (OR Pos3ls2Years Pos4ls2Years))\n(IMPLIES Pos3IsYellow Pos4ls2Years)\n(IMPLIES Pos1IsCroatia Pos1Is8Years)\n(IMPLIES Pos2IsCroatia Pos2Is8Years)\n(IMPLIES Pos3IsCroatia Pos3ls8Years)\n(IMPLIES Pos4IsCroatia Pos4ls8Years)\n(IMPLIES Pos1IsBulgaria Pos1IsPolitics)\n(IMPLIES Pos2IsBulgaria Pos2IsPolitics)\n(IMPLIES Pos3IsBulgaria Pos3IsPolitics)\n(IMPLIES Pos4IsBulgaria Pos4IsPolitics)\n; Implicit constraints\n; Value constraints\n(AND\n; Nationality\n(OR Pos1IsBulgaria Pos1IsCroatia Pos1IsSpain Pos1IsSweden)\n(OR Pos2IsBulgaria Pos2IsCroatia Pos2IsSpain Pos2IsSweden)\n(OR Pos3IsBulgaria Pos3IsCroatia Pos3IsSpain Pos3IsSweden)\n(OR Pos4IsBulgaria Pos4IsCroatia Pos4IsSpain Pos4IsSweden)\n; Color\n(OR Pos1IsRed Pos1IsPurple Pos1Is Yellow Pos1IsBlue)\n(OR Pos2IsRed Pos2IsPurple Pos2Is Yellow Pos2IsBlue)\n(OR Pos3IsRed Pos3IsPurple Pos3IsYellow Pos3IsBlue)\n(OR Pos4IsRed Pos4IsPurple Pos4IsYellow Pos4IsBlue)\n; Years of experience\n(OR Pos1ls2Years Pos1ls5Years Pos1ls7Years Pos1ls8Years)\n(OR Pos2ls2Years Pos2Is5Years Pos2ls7Years Pos2ls8Years)\n(OR Pos3ls2Years Pos3Is5Years Pos3ls7Years Pos3ls8Years)\n(OR Pos4ls2Years Pos4Is5Years Pos4ls7Years Pos4ls8Years)\n; Expertise\n(OR Pos1lsEconomics Pos1IsHistory Pos1IsPolitics Pos1IsSports)\n(OR Pos2IsEconomics Pos2IsHistory Pos2IsPolitics Pos2IsSports)\n(OR Pos3IsEconomics Pos3lsHistory Pos3IsPolitics Pos3IsSports)\n(OR Pos4IsEconomics Pos4IsHistory Pos4IsPolitics Pos4IsSports)\n)\n; Uniqueness constraints\n(AND\n; Nationalities\n(NOT (AND Pos1IsBulgaria Pos2IsBulgaria))\n(NOT (AND Pos1 IsBulgaria Pos3IsBulgaria))\n(NOT (AND Pos1 IsBulgaria Pos4IsBulgaria))\n(NOT (AND Pos2IsBulgaria Pos3IsBulgaria))\n(NOT (AND Pos2IsBulgaria Pos4IsBulgaria))\n(NOT (AND Pos3IsBulgaria Pos4IsBulgaria))\n(NOT (AND Pos1IsCroatia Pos2IsCroatia))\n("}]}