{"title": "Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching", "authors": ["Wenqi Niu", "Yingchao Wang", "Guohui Cai", "Hanpo Hou"], "abstract": "Knowledge Distillation (KD) has emerged as a pivotal technique for neural network compression and performance enhancement. Most KD methods aim to transfer dark knowledge from a cumbersome teacher model to a lightweight student model based on Kullback-Leibler (KL) divergence loss. However, the student performance improvements achieved through KD exhibit diminishing marginal returns, where a stronger teacher model does not necessarily lead to a proportionally stronger student model. To address this issue, we empirically find that the KL-based KD method may implicitly change the inter-class relationships learned by the student model, resulting in a more complex and ambiguous decision boundary, which in turn reduces the model's accuracy and generalization ability. Therefore, this study argues that the student model should learn not only the probability values from the teacher's output but also the relative ranking of classes, and proposes a novel Correlation Matching Knowledge Distillation (CMKD) method that combines the Pearson and Spearman correlation coefficients-based KD loss to achieve more efficient and robust distillation from a stronger teacher model. Moreover, considering that samples vary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based loss and Spearman-based loss. CMKD is simple yet practical, and extensive experiments demonstrate that it can consistently achieve state-of-the-art performance on CIRAR-100 and ImageNet, and adapts well to various teacher architectures, sizes, and other KD methods.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Deep Neural Networks (DNNs) have made significant advancements across various fields, particularly in computer vision tasks such as image classification, object detection, and semantic segmentation [1], [2]. In general, as shown in Figure 1, the accuracy tends to improve as the network size increases, regardless of whether the data is clean or noisy. In other words, larger DNNs (i.e., those with more parameters and deeper layers) tend to exhibit greater accuracy, generalization, and robustness [3]. However, larger models lead to a corresponding rise in complexity and computational demands, which limits the practical application and deployment of DNNs in resource-constrained environments.\nKnowledge Distillation (KD) [4] holds the potential to transfer both the accuracy and robustness learned by a larger-scale and higher-capacity teacher model to a smaller and streamlined student model, achieving efficient compression while maintaining commendable performance [5]. The classical KD process minimizes the Kullback-Leibler (KL) divergence loss between the teacher's output and the student model's output with a fixed temperature [4], where the output can be the logits or the softened probabilities. In this way, the student can be guided with more informative signals during training and is thus expected to have a more promising performance than that being trained stand-alone. After years of development, KD has made remarkable progress and has become an effective and well-established paradigm for compressing and enhancing DNNs [6].\nIntuitively, using a larger and stronger teacher model is expected to distill into a better-performing student model. However, previous studies [7]\u2013[18] have shown that this empiricism does not always hold. The student model distilled from a higher accuracy and larger-scale teacher model may perform worse as shown in Figure 1. This phenomenon has also been observed in different robust distillation methods [19]. Existing studies attribute the reason behind this phenomenon to the capacity mismatch between the teacher and student models [7]\u2013[18]. To address this issue, some studies [9]\u2013[11] focused on the innovation of KD architecture. For example, TAKD [9] was proposed to reduce the discrepancy between teacher and student by resorting to an additional teaching assistant of moderate model size. On the other hand, some studies aimed to regularize teacher's knowledge to narrow the capacity gap. For example, [12] advocates that an intermediate checkpoint will be more appropriate for distillation. Although these methods provide insights into different aspects, a generic enough solution is preferred to address the difficulty of KD brought by stronger teachers.\nDifferent from the above studies, this study re-examines the reasons why traditional knowledge distillation has poor performance from the perspectives of output-level dark knowledge and inter-class relationships. We empirically find that the KL-based KD method may implicitly change the inter-class relationships learned by the student model, resulting in a more complex and ambiguous decision boundary, which in turn reduces the model's accuracy and generalization ability. Therefore, we demonstrate that enabling the student model to learn the rank relation inherent in the teacher model's output is both sufficient and effective. The rank-based approach [14], [18] allows for greater flexibility, improving the student's ability to capture the intrinsic relations in the classes while mitigating the drawbacks associated with KL divergence. Regarding this, we propose a novel Correlation Matching Knowledge Distillation (CMKD) method that combines the Pearson and Spearman correlation coefficients-based KD loss to achieve more efficient and robust distillation from a stronger teacher model. The main contributions of this study can be summarized as follows.\n\u2022 We propose a novel correlation matching KD method (CMKD) that employs a combination of the Pearson and Spearman correlation coefficients to achieve a more flexible alignment between the teacher and student models.\n\u2022 We demonstrated the benefits of relaxed matching and introduced Z-score normalization to approximate a standard normal distribution in the model outputs, thereby satisfying the applicability conditions of the Pearson correlation coefficient.\n\u2022 We assess the difficulty of samples based on the information entropy of the teacher's output and dynamically adjust the weights of the Pearson and Spearman correlation coefficients during the distillation process according to the sample difficulty.\nThe rest of this paper is organized as follows. Related studies are reviewed in Section II. Section III presents the preliminary knowledge about KD, Pearson and Spearman correlation coefficients. Section IV demonstrates the motivation and details of CMKD. Section V gives the details of the CMKD. Section VI shows the experiments in different datasets and neural networks and delves into the hyperparameters and ablation experiments. Finally, this paper is concluded in Section VII."}, {"title": "II. RELATED WORK", "content": "Recently, some studies have been performed to address the poor learning issue of the student model when the student and teacher model sizes significantly differ. Some studies [9]\u2013[11], [16] focused on the innovation of KD architecture. TAKD [9] proposes to reduce the discrepancy between teacher and student by resorting to an additional teaching assistant of moderate model size. DGKD [10] further improves TAKD by densely gathering all the assistant models to guide the student. NSKD [16] incorporated teacher assistants into Self-KD by introducing auxiliary classifiers to the shallow layers of the network to reduce the mismatch between the capacities of the student and teacher models. While, SCKD [11] investigated the capacity mismatch issue from the perspective of gradient similarity, which dynamically determined when to activate or deactivate the knowledge distillation loss, depending on the relative gradient direction in relation to the student loss. However, these methods require meticulous manual selection of the assistant teacher model or determining the appropriate activate point to achieve an optimal balance in knowledge transfer effectiveness.\nOn the other hand, some studies [7], [12], [13], [15], [17] aimed to regularize teacher's knowledge to narrow the capacity gap. Cho et al. [7] argued that the KD process can benefit from using an early stopping strategy during training. Similarly, CheckpointKD [12] employed intermediate models from the middle of the training process as teacher models, instead of relying on fully trained models. It further selected an appropriate intermediate teacher model based on mutual information. Zhu et al. [13] demonstrated that the issue of poor learning is directly linked to the presence of undistillable classes. Therefore, they introduced a straightforward \"Teach Less, Learn More\" framework to identify and exclude these undistillable classes during training. Rao [15] argued that the capacity mismatch issue can be mitigated by ensuring the appropriate smoothness of the soft labels. To achieve this, an adapter module was introduced for the teacher model, where only the adapter is updated to produce soft labels with the desired level of smoothness. SKD [17] aims to simplify teacher output into new knowledge representations, which involve softening processing and a learning simplifier. Although these studies have led to improved distillation performance, they have not explored the capacity gap in the context of distillation losses.\nStudies [8], [14], [18] closely align with our work. RKD [8] transferred mutual relations of data examples instead, which use distance-wise and angle-wise distillation losses that penalize structural differences in relations. Huang et al. [14] proposed a Pearson correlation coefficient-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Fan et al. [18] observed a positive correlation between the calibration of the teacher model and the KD performance with the original KD methods, and recommended employing measurements insensitive to calibration such as ranking-based loss [14]. In contrast to the studies mentioned above, we explain the advantages of rank-based KD from the perspective of model decision boundaries and propose using both the Pearson and Spearman correlation coefficients to construct the distillation loss."}, {"title": "III. PRELIMINARY KNOWLEDGE", "content": "In the classic KD method, the transferred knowledge refers to soft labels that are the predictions by the teacher model T, and the loss function of the student model S is defined as follows.\n$L_S = L_G + L_{KD} = F(p, y) + H(p^T, p^s)$ (1)\n$L_G = F(p, y)$ is the cross-entropy loss function between the predicted probability of the student model $p = [p_1, p_2, ..., p_c]$ and the ground truth label $y \\in \\{1, 2, ..., c\\}$, where $c$ is the total number of classes, and $p_i, i \\in \\{1, 2, ..., c\\}$ can be obtained by Eq. (2).\n$p_i = \\frac{exp(z_i)}{\\sum_{j=1}^{c} exp(z_j)}$ (2)\nwhere $z_i$ represents the logit of the i-th class from the student model S.\n$L_{KD} = H(p^T, p^s)$ is the KD loss, which usually is the Kullback-Leibler (KL) divergence loss function between the softened predictions of the student model $p^s = [p_1^s, p_2^s, ..., p_c^s]$ and the corresponding teacher predictions $p^T = [p_1^T, p_2^T, ..., p_c^T]$, which is as follows.\n$H(p^T, p^s) = \\sum_{i=1}^{c} p_i^T log(\\frac{p_i^T}{p_i^s})$\n$p_i^s = \\frac{exp(z_i^s/T)}{\\sum_{j=1}^{c} exp(z_j^s/T)}$ (4)\n$p_i^T = \\frac{exp(z_i^T/T)}{\\sum_{j=1}^{c} exp(z_j^T/T)}$ (5)\nwhere $T$ is a temperature coefficient to soften the predicted probability, $z_i^T$ represents the logit of the i-th class from the student model T."}, {"title": "B. Correlation Measures", "content": "1) Pearson correlation coefficient: The Pearson correlation coefficient is a statistical measure that quantifies the linear relationship between two variables X and Y. It is defined as the covariance of the two variables divided by the product of their standard deviations. The Pearson correlation coefficient r for two variables X and Y is as follows.\nr(X,Y) = \\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_i - \\overline{X})^2 \\sum_{i=1}^{n}(Y_i - \\overline{Y})^2}}$ (6)\nwhere $X_i$ and $Y_i$ are the individual sample points, $\\overline{X}$ and $\\overline{Y}$ are the means of the X and Y samples, respectively, and n is the number of paired observations.\n2) Spearman correlation coefficient: The Spearman correlation coefficient is a non-parametric measure of the strength and direction of the association between two ranked variables. Unlike the Pearson correlation coefficient, Spearman's rank correlation does not assume that the relationship between the variables is linear or that the variables are normally distributed. Instead, it assesses how well the relationship between two variables can be described using a monotonic function. The Spearman correlation coefficient $\\rho$ for two variables X and Y is as follows.\n$\\rho(X,Y) = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}$ (7)\nwhere $d_i = rank(X_i) - rank(Y_i)$ is the difference between the ranks of corresponding values of X and Y, n is the number of paired observations."}, {"title": "IV. MOTIVATION AND THEORETICAL ANALYSIS", "content": "Capacity mismatch reflected in logit range, and KL-based KD methods cannot reduce the difference between teacher's and student's logits efficiently: The KL-based KD method seeks to align the logit of the student model with that of the teacher model. To assess the similarities and differences between the teacher's logit and the student's logit after KD, we fixed the student model architecture to ResNet14 and employed teacher models of varying capacities (ResNet20, ResNet32, ResNet44, ResNet56, and ResNet110) for KD on the CIFAR-10 and CIFAR-100 datasets, and visualized the confusion matrix between the logits of the teacher model and the student model. As shown in Figure 2, the color intensity of the confusion matrix reflects the magnitude of the difference between the logits of the teacher and student models, and a darker color signifies a larger discrepancy.\nIt can be observed that as the size of the teacher model increases, the color of the confusion matrix progressively darkens, reflecting a greater difference as the teacher model's capacity grows. This also illustrates that the KL-based KD method cannot reduce the difference between the logits of the teacher model and the student model. This disparity primarily arises from the capacity gap between the student and teacher models [7]\u2013[18]. Specifically, a more robust teacher model has a stronger representational ability, allowing it to capture complex patterns and relationships in the data more effectively, thereby fitting the training data more accurately and producing sharper output probability distributions. However, due to the smaller capacity of the student model, it is unable to replicate the intermediate features extracted by the teacher model, resulting in the student model's inability to accurately reproduce the teacher's output distribution.\n2) KL-based exact matching may implicitly change the inter-class relationships learned by the student model: The rank relationship of a model's output is determined by comparing the output values (logits) for each class. Previous study [20] has demonstrated that while more powerful teacher models tend to produce probability vectors with smaller distinctions between non-target classes, teachers of varying capacities generally maintain consistent perceptions of relative class affinities. However, KL-based knowledge distillation is ineffective at capturing the rank relationships of the teacher model. As shown in Figure 3, with the increase in training iterations, the Spearman correlation coefficient between the outputs of the student model and the teacher model gradually increases, but remains in a state of low correlation. This is particularly evident on CIFAR-100, where the rank relationship between the two models shows almost no correlation. Furthermore, the larger the teacher model, the lower the correlation between the rank of the outputs from the teacher and student models.\nTo illustrate the above phenomenon, we derive the KL loss function $L_{KD}$ of the student model with respect to the logits $z_k^s$ as Eq.(8). The specific derivation process can be found in the Appendix.\n$\\frac{\\partial L_{KD}}{\\partial z_k^s} = \\frac{1}{T} (p_k^s - p_k^T)$ (8)\nIt reveals that, in the KD process, for any input sample belonging to class k, the direction and magnitude of the gradient update resulting from matching via KL divergence are determined by the discrepancy between the student model's output $p_k^s$ and the teacher model's output $p_k^T$. However, as shown in Figure 2, the varying color intensities across different categories indicate that the differences between the teacher and student model outputs are not consistent across all classes, resulting in the KL-based method prioritizing fitting the classes with large differences in the logit value, which may alter the relative rank relationships among classes with smaller probability differences. For example, as shown in Figure 4, during the training process of the student model, to minimize the KL divergence loss, the probability of the target class 2 will increase preferentially, while the probabilities of the other non-target classes will decrease. However, due to the varying differences between the output probabilities of each class in the student and teacher models, the probabilities of the non-target classes in the student model decrease at different rates, which affects the rank order of the non-target classes (e.g., in the teacher model, class 4 has a higher rank than class 5, but the student model may learn an order where class 4 has a lower rank than class 3).\n3) The changes in relative ranks among non-target classes will compel the student model's decision boundaries to shift and become more complex and ambiguous: The rank relationship of the model's output is closely related to its decision boundary. A higher rank indicates greater confidence in a particular class, and the model is more likely to assign the corresponding sample to that class along the decision boundary. For example, as shown in Figure 4, if the relative rank order between class 3 and class 4 is altered, the student model may perceive class 2 samples to be more similar to class 4 samples than to class 3 samples. This could potentially shift the decision boundaries between class 2 and class 3, as well as between class 2 and class 4, leading to blurred and more complex decision boundaries. As a result, the student model may find it more difficult to learn robust features, potentially reducing its generalization and robustness ability.\nTo illustrate the presence of boundary blurring, we conducted experiments on the CIFAR-10 and CIFAR-100 dataset, using different teacher models (ResNet20, ResNet32, ResNet44, ResNet56, and ResNet110) to guide the training of a student model (ResNet14), and employed t-SNE for visualization, where different colors represent different categories in the classification. The more concentrated the clusters of the same color and the more dispersed the clusters of different colors, the stronger the model's discriminative capability and the clearer its decision boundaries. As shown in Figure 5, as the capacity of the teacher model increases, the clustering boundary between the red class and other classes initially becomes clearer but later becomes blurred. The clearest boundary is observed when the teacher model is ResNet44. This indicates that when the capacity of the teacher model is too large, the decision boundaries of the student model tend to become complex and blurred, further supporting the analysis presented above.\nTherefore, this paper proposes that the student model should learn not only the probability distribution values from the teacher model but also the relative rank relationships among classes in the teacher's output."}, {"title": "B. Explore Relaxed Matching based on Rank Relations", "content": "1) Linear correlation: The Pearson correlation coefficient [21] measures the linear correlation between two variables, thus reflecting their rank relationships. Huang et al. [14] have introduced Pearson correlation as a substitute for KL divergence, encouraging the student model's output to be as positively correlated as possible with that of the teacher model. However, as shown in Figure 3, the Pearson coefficient between the teacher model and the student model is relatively high, indicating a strong linear correlation. In reality, the rank correlation between the two models remains low. This suggests that while the Pearson coefficient aids the student model in approximating the numerical values of the teacher model's outputs, it does not effectively help the student model in learning the rank relationship of the teacher model's outputs.\nSpecifically, on the one hand, Pearson correlation is mainly suited to capturing linear relationships. Given the disparity in capacities between the teacher and student models, the student model cannot precisely replicate the teacher model's output distribution. This results in a relationship between the teacher and student model outputs that is not always linear, and Pearson correlation may fail to accurately capture such non-linear relationships, leading to suboptimal knowledge distillation performance. On the other hand, the Pearson correlation is highly sensitive to outliers. When the teacher model's output is overly sharp (especially for simple samples), excessively high probability values may significantly affect the coefficient's calculation, compromising the stability and effectiveness of the knowledge distillation process. For example, as shown in Figure 4, the Pearson correlation coefficient r between the outputs of the student and teacher models changes from 0.96978 to 0.99998, indicating only a small change, which fails to effectively capture the shifts in the rank relationships among the non-target classes.\n2) Non-linear correlation: Spearman's rank correlation coefficient, another commonly used metric for assessing rank relationships between two variables, evaluates their monotonic relationship by comparing their ranks without requiring the relationship to be linear. It is also less sensitive to outliers, making it a more relaxed measure of correlation. However,"}, {"title": "V. METHODOLOGY", "content": "The Pearson correlation coefficient assumes that data follows a normal distribution. However, as shown in Figure ??, the logit of both the teacher and student models resemble a normal distribution, but not completely normal. To address this, we applied Z-score normalization to the logit of the teacher and student models, ensuring that the logits conform to a standard normal distribution without altering the relationships between their outputs. Z-score normalization primarily adjusts the scale of the data without changing the relative positions or rank order of the data. In other words, it does not alter the nonlinear relationships between the data and does not affect the use of Spearman's rank correlation coefficient. Moreover, Z-score normalization can reduce the magnitude and variance differences between the logits of the teacher and student models, thereby mitigating the negative impact of logit value discrepancies on the distillation process.\nThe calculation formula of the Z-score normalization is as follows.\n$z_i = \\frac{Z_i - \\mu}{\\sigma}$ (9)\nwhere $\\mu$ and $\\sigma$ are the mean and variance of the model logits output, respectively, and the calculation formula is as follows.\n$\\mu = \\frac{1}{c}\\sum_{i=1}^{c} Z_i$ (10)\n$\\sigma = \\sqrt{\\frac{1}{c}\\sum_{i=1}^{c} (Z_i - \\mu)^2}$ (11)\nwhere $c$ represents the number of categories, and $Z_i$ represents the logits output value of the model for the i-th category."}, {"title": "B. Correlation Matching", "content": "1) Linear Correlation Matching with Pearson Correlation Distillation: The Pearson correlation distillation loss aims to ensure that the outputs of the student model are as positively correlated with the teacher model's outputs as possible. The loss function is defined as follows.\n$L_{Pearson} = 1 - r(p^T, p^s)$ (12)\nwhere $r(p^T, p^s)$ represents the Pearson correlation coefficient between the teacher model's output $p^T$ and the student model's output $p^s$.\n2) Non-linear Correlation Matching with Spearman Correlation Distillation: The Spearman correlation distillation loss aims to ensure that the output rank relationship of the student model closely aligns with that of the teacher model. The loss function is defined as follows.\n$L_{Spearman} = 1 - \\rho (p^T, p^s)$ (13)\nwhere $\\rho (p^T, p^s)$ represents the Spearman correlation between the teacher model's output $p^T$ and the student model's output $p^s$. However, the Spearman correlation coefficient requires ranking operations, which are mathematically neither directly differentiable nor tractable for gradient-based optimization. Fortunately, Blondel et al. [22] have proposed a fast and differentiable sorting and ranking method, which is adopted in this study."}, {"title": "C. Dynamic Relaxation Matching Based on Sample Difficulty", "content": "Information entropy is a fundamental measure of uncertainty and information content in probability distributions. Therefore, in this study, the entropy of the teacher model's output is used to measure the difficulty of the samples or the sharpness of the teacher model's output. Specifically, when the teacher model is highly confident in certain simple samples, resulting in a sharper output distribution, and the corresponding entropy is lower. Conversely, when the output distribution is flatter, the entropy is higher, indicating that the sample is more challenging. The formula for calculating the entropy of the model's output is as follows.\n$H(z) = - \\sum_{c=1}^{C} p(z_c) log p(z_c)$ (14)\nwhere C is the total number of classes in the model's output, $z_c$ represents the logits value of the c-th class, and $p(z_c)$ is the probability of the c-th class in the model's output.\nTo evaluate whether the output of the teacher model is sharp, we use the average entropy of the teacher model's outputs within a batch as the threshold criterion. The formula for calculating the average entropy is as follows.\n$\\overline{H(z)} = \\frac{1}{N} \\sum_{i=1}^{N} H_i(z)$ (15)\nWhere N represents the total number of samples in the batch. If the output entropy of a sample within a batch exceeds the average entropy, we define the teacher model's output for that sample as excessively sharp. In this case, the distillation loss function primarily calculates the Spearman correlation coefficient, with the Pearson correlation coefficient serving as a supplementary measure. Conversely, if the output entropy of a sample is below the average, we define the teacher model's output as relatively flat. In this scenario, the distillation loss function primarily focuses on calculating the Pearson correlation coefficient, with the Spearman correlation coefficient as a supplementary measure. Therefore, the loss function during training is defined as follows.\n$L = \\begin{cases} \\alpha L_{CE} + \\beta L_{Pearson} + \\gamma L_{Spearman}, & \\text{if } H_i > \\overline{H} \\\\ \\alpha L_{CE} + \\gamma L_{Pearson} + \\beta L_{Spearman}, & \\text{if } H_i < \\overline{H} \\end{cases}$ (16)\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are three hyperparameters used to balance the weights of the target loss and distillation loss in the total loss function. $H_i$ represents the entropy of the i-th sample in a given batch during training, while $\\overline{H}$ denotes the average entropy of all samples in the batch."}, {"title": "VI. EXPERIMENT", "content": "To provide a detailed comparison, we conducted experiments on two popular datasets, CIFAR-100 [23] and ImageNet [24]. CIFAR-100 [23] is an image classification dataset consisting of 100 classes, with each class containing 600 images at a resolution of 32\u00d732. Due to its diverse classes and relatively small image size, CIFAR-100 [23] is a popular choice for studying image classification and knowledge distillation (KD) methods. ImageNet [24], being one of the largest image classification datasets, contains 1,000 classes and over 1.2 million images. The wide range of categories and large-scale images in ImageNet provides an excellent test environment for evaluating a model's generalization ability and robustness.\n2) Network Architectures: We employed various popular network architectures as teacher and student models, including VGG [25], ResNet [26], WideResNet [27] series, and lightweight networks such as the MobileNet [28] and ShuffleNet [29] series, to assess the performance of our method across different architectures. Additionally, we considered heterogeneous teacher-student model configurations with different network architectures.\n3) Performance Comparison: In terms of performance comparison, we not only benchmarked our method against the standard KL-based KD [4] but also compared it with other prior studies. These included logit-based distillation methods such as TAKD [9], DKD [30], DIST [14], and NKD [31], as well as feature-based KD methods including FitNet [32], AT [33], RKD [8], OFD [34], CRD [35], and ReviewKD [36].\n4) Implementation Details: We strictly adhered to the experimental settings from prior studies to ensure consistency in hyperparameters such as learning rate, batch size, and optimizer. For the CIFAR-100 dataset, we set the batch size to 64 and the weight decay factor to $5 \\times 10^{-4}$. All models, except for the MobileNet and ShuffleNet series, were initialized with a learning rate of 0.05; for the MobileNet and ShuffleNet series, the initial learning rate was set to 0.01. The training process lasted for 240 epochs, with the learning rate decaying by a factor of 0.1 at the 150th, 180th, and 210th epochs. For the ImageNet dataset, we used a batch size of 512 and a weight decay factor of $1 \\times 10^{-4}$. The training period was 100 epochs, with an initial learning rate of 0.2, which decayed by a factor of 10 at the 30th, 60th, and 90th epochs.\nAcross all datasets, we used SGD as the optimizer, with a momentum parameter of 0.9. For the CIFAR-100 dataset, we set $\\alpha = 1$, $\\beta = 4$, and $\\gamma = 1$ with a temperature $T = 4$. For the ImageNet dataset, we similarly set $\\alpha = 1$, $\\beta = 4$, and $\\gamma = 1$, but with a temperature $T = 1$. All experiments were conducted on an NVIDIA 3080 GPU. The CIFAR-100 dataset was trained on a single GPU, while ImageNet was trained on four GPUs.\n5) Evaluation Metrics: We used Top-1 and Top-5 accuracy for classification tasks as the primary evaluation metrics, and the final reported results are based on the average of three experimental runs. In CIFAR-100, we relied on Top-1 accuracy as the main metric, while for ImageNet, both Top-1 and Top-5 accuracy are employed. Additionally, we recorded training time to compare the computational cost of our method."}, {"title": "B. Experimental Results", "content": "1) Results on CIFAR-100: We reported the results in Tables I and II. Table I focuses on teacher/student models with the same architecture, while Table II explores combinations with different architectures. As shown in Table I, although feature-based distillation methods generally outperform logits-based methods, our logits-based distillation approach CMKD demonstrated significant performance improvements. In some cases, its performance was on par with or surpassed feature-based methods. Notably, in the combinations of ResNet32\u00d74 with ResNet8\u00d74 and WRN-40-2 with WRN-16-2, our method improved Top-1 accuracy by 3.56% and 1.62%, respectively, compared to traditional KD. On the other hand, as shown in Table II, CMKD also achieved significant results in heterogeneous networks. For example, in the combinations of WRN-40-2 with ResNet8\u00d74 and ResNet50 with MobileNet-V2, our method improved Top-1 accuracy by 3.17% and 2.24%, respectively, over traditional KD.\nIn addition, CMKD can integrate smoothly with other logit-based methods while maintaining simplicity. The results at the bottom of Tables I and II demonstrate that when combined with the DKD method, the performance of DKD improved significantly. In the combinations of ResNet32\u00d74 with ShuffleNet-V1 and ResNet32\u00d74 with ShuffleNet-V2, our method CMKD combined with DKD, further improved Top-1 accuracy by 1.27% and 1.21%, respectively. Meanwhile, the results in models with different architectures came closer to the feature-based ReviewKD, and in models with the same architecture, CMKD combined with DKD performed even better.\n2) Results on ImageNet: We used ResNet34 as the teacher model and ResNet18 as the student model to form combinations with the same architecture. Similarly, ResNet50 was used as the teacher model and MobileNetV1 as the student model to form combinations with different architectures. As shown in Tables III and IV, our method CMKD achieved significant improvements in both Top-1 and Top-5 accuracies. Specifically, for the same architecture combination of ResNet34/ResNet18, compared to traditional KD, CMKD improved Top-1 accuracy by 1.36% and Top-5 accuracy by 0.84%."}, {"title": "C. Robustness Experiments", "content": "To demonstrate that CMKD facilitates the student model's acquisition of clear and robust decision boundaries, as well as additional knowledge related to robustness and generalization, we conducted robustness tests on the student model using the CIFAR-100-C dataset [37]. The CIFAR-100-C dataset applies 15 different types of corruption, such as noise, blur, and occlusion, to the original CIFAR-100 images, with each type of corruption having five different severity levels to evaluate the model's robustness under these damaging conditions. Specifically, we assessed the robustness accuracy of the student model for five corruption types by calculating the average performance across the CIFAR-100-C images with five distinct corruption levels. The experimental results are shown in Table V. Compared to conventional KD, our method maintained a higher accuracy across the five different corruption types, demonstrating that our approach not only improves model performance but also enhances model robustness."}, {"title": "D. Ablation Studies", "content": "In this section, we investigated the impact of the scaling coefficient as a hyperparameter on the overall performance, as well as the contribution of different components of our algorithm to the overall performance. All experiments were conducted on the CIFAR-100 dataset, and we selected two sets of teacher/student model combinations to verify the generalizability of the ablation study results. These combinations are ResNet32x4/ResNet8x4 and ResNet56/ResNet20.\n1) Hyperparameter: We set $\\alpha = 1$, $\\gamma = 1$ and vary the value of $\\beta$ within the set $\\{1, 2, 3, 4, 5\\}$ to identify the optimal hyperparameters. As shown in Figure 8, when the $\\beta$ is set to 4, both sets of teacher/student model combinations achieve significant performance improvements. Therefore, in this study, the hyperparameters are as follows: $\\alpha = 1$, $\\beta = 4$ and $\\gamma = 1$.\n2) Impact of Different Components: To demonstrate the effectiveness of each proposed component, we conducted ablation studies on the CIFAR-100 dataset. The results are shown in Table VI. Compared to traditional KD, using the Pearson correlation coefficient instead of KL divergence alone resulted in improvements, with increases of 2.25% and 0.39% in two different teacher/student model combinations, respectively. When Pearson correlation coefficient is calculated with z-score normalization, the performance improvement was more significant, with an additional increase of 0.97% and 0.44%. Finally, by incorporating the Spearman correlation coefficient to capture additional knowledge information, the model performance was further enhanced, with further improvements of 0.34% and 0.34% compared to the previous methods."}, {"title": "E. Visualization", "content": "To more intuitively demonstrate the effectiveness of our proposed method, we visualized the model using the ResNet32x4/ResNet8x4 as the teacher/student model on CIFAR-100 from two different perspectives. On the one hand, Figure 9 shows the difference in the correlation matrix of the global logits between the student and teacher. Darker colors indicate greater differences between the logits of the student and teacher."}]}