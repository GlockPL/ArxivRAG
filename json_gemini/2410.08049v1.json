{"title": "Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations", "authors": ["Yiyuan Zhang", "Xiaohan Ding", "Xiangyu Yue"], "abstract": "This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture design guidelines for large-kernel ConvNets that optimize their efficiency and performance. We propose the UniRepLKNet architecture, which offers systematical architecture design principles specifically crafted for large-kernel ConvNets, emphasizing their unique ability to capture extensive spatial information without deep layer stacking. This results in a model that not only surpasses its predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a COCO box AP of 56.4% but also demonstrates impressive scalability and performance on various modalities such as time-series forecasting, audio, point cloud, and video recognition. These results indicate the universal modeling abilities of large-kernel ConvNets with faster inference speed compared with vision transformers. Our findings reveal that large-kernel ConvNets possess larger effective receptive fields and a higher shape bias, moving away from the texture bias typical of smaller-kernel CNNs. All codes and models are publicly available at https://github.com/AlLab-CVC/UniRepLKNet, promoting further research and development in the community.", "sections": [{"title": "1 INTRODUCTION", "content": "CONVOLUTIONAL neural networks (ConvNets) are widely adopted in the computer vision community [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18]. Recently, the dominance of ConvNets has been significantly challenged by Vision Transformers (ViTs) [19], [20], [21], [22], [23] which utilize global attention [19], [22], [24] and window-based attention [20], [25], [26]. In addition to image recognition, ViTs are also widely applied across various modalities [27], [28], [29], including audio [30], point cloud [31], video [32], etc., demonstrating their potent capability of universal modeling for perception tasks. However, the quadratic complexity, high memory costs, and slow inference speed hinder broader applications of ViTs, such as the perception of high-resolution images and long-form videos. Therefore, we ask the following question:\nCan we build a ConvNet that offers similar universal modeling capabilities as ViT, but with reduced complexity and significantly faster inference speed?\nDiving into the advantages of ViTs, the global attention mechanism brings out long-range dependencies and contextual relationships [33], [34], [35], [36], [37]. This prompts us to consider: how to enhance long-range dependencies and contextual relationship in ConvNets? Large convolutional kernels appear to be the solution for ConvNets after a decade\u2019s exploration [1], [2], [16], [38], [39], [40]. In 2014, Xu et al. [38] proposed the inverse kernel and deconvolution to add larger spatial support for image denoising. Following this, large kernels were introduced to segmentation tasks in 2017 for larger ERFs [39]. Additionally, in 2022, Liu et al. [16] scaled kernels up to 7 \u00d7 7 within the macro architecture"}, {"title": "2 RELATED WORKS", "content": "Large kernels in early ConvNets. Early ConvNets, such as AlexNet [3] and Inception [4], [5], [6], initially used large kernels (7\u00d77 or 11\u00d711) to capture spatial features. However, the trend shifted with VGG-Net, which favored smaller, more frequent layers [18]. Innovatively, the Global Convolution Network (GCN) [39] utilized very large kernels (1 \u00d7 K followed by K \u00d7 1) to improve semantic segmentation. Local Relation Networks (LR-Net) [57] explored dynamic kernel sizes and found that performance peaked with 7 \u00d7 7 kernels but declined with larger sizes, illustrating the challenges of balancing kernel size with network efficiency.\nExplorations with large kernels. Expanding the traditional definition of kernels in convolutional networks, Swin Transformer [20] innovatively employed shifted attention mechanisms with window sizes ranging from 7 to 12, effectively functioning as dynamic kernels. Research by Han et al. [35] demonstrated that replacing the attention layers in Swin Transform with either static or dynamic 7 \u00d7 7 convolution layers yielded results comparable to the original model. Additionally, the MetaFormer [58] proposed that a large-kernel pooling layer could serve as a viable alternative to self-attention mechanisms. Further extending the concept, the Global Filter Network (GFNet) [59] refined spatial connection weights via the Fourier domain, achieving a global convolution effect similar to circular convolutions in the spatial domain, underscoring the versatile applications of large-scale kernels across different network architectures.\nModern ConvNets with very large kernels. The introduction of RepLKNet [1] marked a significant shift in ConvNet design by demonstrating that enlarging kernel sizes can improve performance, particularly in downstream applications. This approach introduced several key design strategies, such as integrating shortcuts with large kernels for better microstructural efficiency. While RepLKNet was inspired by the straightforward architecture of the Swin Transformer, subsequent research has expanded on this idea. Liu et al. [40] and others pushed the boundaries further by scaling up kernel sizes, applying these concepts to 3D vision tasks [60], image dehazing [61] and super-resolution [62]. Despite these advances, the architectural nuances of ConvNets with large kernels remain relatively unexplored, indicating a promising area for future research.\nThe growing interest in large-kernel ConvNets is driven by their effectiveness in capturing fine-grained and global spatial features. However, existing models often integrate large kernels with additional mechanisms, limiting the understanding of their standalone potential. Research shows that scaling kernel sizes improves performance, yet a universal large-kernel ConvNet architecture remains undeveloped. This work proposes a simplified, universal design that retains the spatial extraction benefits of large kernels, bridging the flexibility of Transformer models with the efficiency"}, {"title": "3 A ROADMAP TO UNIVERSAL CONVNETS", "content": "Our roadmap to universal large-kernel ConvNets (UniRe-PLKNet, Fig. 3) comprises four steps: 1) We first explore why large kernel convolutions are not commonly used in modern ConvNets and propose 5 guidelines to make them more practical and evaluate their effectiveness (\u00a7 3.1). 2) We propose 4 guidelines for building a powerful and competitive large-kernel ConvNet architecture (\u00a7 3.2). 3) We propose to generalize the large-kernel ConvNets to multimodal understanding tasks (\u00a7 3.3). 4) Finally, we propose asymmetric large-kernel convolution to efficiently fuse multimodal features in contrast to cross-attention (\u00a7 3.4)."}, {"title": "3.1 Step 1: Making Large Kernels Practical", "content": "3.1.1 Making Large Kernels Efficient\nThe first reason why large kernels were rarely used is that they were believed to be computationally expensive due to the quadratic increase in the number of parameters and FLOPs with kernel size. However, we argue that this drawback can be significantly mitigated by using depth-wise (DW) convolutions [14], [17]. As DW convolutions only consume a minor fraction of the total computational budget of a ConvNet, increasing the kernel sizes does not significantly make the model larger or slower. For example, as shown in Table 2c, increasing the kernel sizes of DW convolutions in MobileNet V2 [65] from 3\u00d73 to 13\u00d713 results in only a 2.7% increase in FLOPs and 4.2% increase in parameters, which is acceptable given the corresponding +2.31% mIoU improvement in Cityscapes segmentation. The remaining 1\u00d71 convolutions dominate most of the complexity.\nOne may be concerned that DW convolutions could be inefficient on modern parallel computing devices, such as GPUs. It is true for conventional DW 3\u00d73 kernels [11], [17], [65], as DW operations introduce a low ratio of computation vs. memory access cost [66], which is not friendly to modern computing architectures. However, we find that as the kernel size increases, the computational density also increases. For example, in a DW 11\u00d711 kernel, each value loaded from the feature map can be used in up to 121 multiplications, while in a 3\u00d73 kernel, the number is only 9. Therefore, according to the roofline model [65], the actual latency should not increase as much as the FLOPs when the kernel size becomes larger.\nThe discussions above reveal that large-kernel DW convolutions can run faster with better implementation. In practice, we propose a block-wise (inverse) implicit GEMM algorithm to replace the original operator. 3 Table 1 shows that our implementation is significantly more efficient compared to the PyTorch baseline.\nTherefore, we propose our first guideline as follows.\nGuideline 1: use depth-wise large-kernel convolution with proper operator-level implementation."}, {"title": "3.1.2 Making Large kernels Effective", "content": "The second reason why large kernels were rarely used is that they were believed to harm the model\u2019s performance. However, we argue that large kernels are not harmful; they were simply not used properly. We propose three guidelines to use large kernels correctly in modern ConvNets.\nGuideline 2: identity shortcut is vital, especially for networks with very large kernels. To demonstrate this, we use MobileNet V2 [65] for benchmarking, since it heavily employs DW layers and has two published variants (with or without shortcuts). For the large-kernel counterparts, we simply replace all the DW 3\u00d73 kernels with 13\u00d713. All the models are trained on ImageNet with identical training configurations for 100 epochs (see Appendix A for details). Table 2a shows that large kernels improve the accuracy of MobileNet V2 with shortcuts from 71.76% to 72.53%. However, for the model without shortcuts, large kernels reduce the accuracy to only 53.98%. We explain this phenomenon from a perspective similar to [67]: shortcuts make the model an implicit ensemble of numerous models with different receptive fields (RFs), allowing it to benefit from a much larger maximum RF without losing the ability to capture small-scale patterns.\nGuideline 3: re-parameterizing large kernels with small kernels improves the performance. To better understand the effect of the aforementioned ensemble of different RFs, we explore whether using small kernels to produce a bigger ensemble of more different RFs improves the performance. Specifically, we replace the 3\u00d73 layers of MobileNet V2 with 9\u00d79 and 13\u00d713, and optionally adopt the Structural Reparameterization [12], [68], [69] methodology to add small kernels without altering the inference structure of the resultant model. Specifically, we construct a 3\u00d73 layer parallel"}, {"title": "3.1.3 Evaluating Large-kernels ConvNets", "content": "The third reason to abandon large kernels, even though the large-kernel ConvNet is designed properly, is that its ImageNet accuracy looks no better than a small-kernel ConvNet. However, Table 2b (after re-param) shows increasing the kernel size of MobileNet V2 from 3\u00d73 to 9\u00d79 improves the ImageNet accuracy by 1.33%, but the Cityscapes mIoU by 3.99%. Such a phenomenon indicates that models of similar ImageNet scores could have very different capabilities in downstream tasks.\nRemark. What causes the phenomenon? First, large kernel design significantly increases the Effective Receptive Fields (ERFs) [42], as shown in Figure 2. Numerous works have demonstrated \u201ccontextual\u201d information, which implies large ERFs, is crucial in many downstream tasks like object detection and semantic segmentation [39], [76], [77], [78], [79]. Second, We deem another reason might be that large kernel design contributes more shape biases to the network. Briefly speaking, ImageNet pictures can be correctly classified according to either texture or shape, as proposed"}, {"title": "3.2 Step 2: Designing a Competitive Large-Kernel Architecture", "content": "As discussed above, we have been aware of five basic guidelines for making large kernels practical and then seek to explore how to design a powerful and competitive large-kernel architecture.\nWe first construct a vanilla architecture as a baseline to verify which design choices work well with large kernels.\nVanilla architecture. As a common practice, the main body of the model is split into four stages connected by down-sampling blocks. Specifically, the first downsampling block uses two stride-2 3\u00d73 convolutional layers to transform the raw input into C-channel feature maps, where C is an architectural hyper-parameter. The other three downsampling blocks each use one stride-2 3\u00d73 conv layer performing 2\u00d7 channel expansion so that the numbers of channels in the four stages are C, 2C, 4C, and 8C, respectively. A stage comprises blocks whose vanilla design resembles ConvNeXt, i.e., a depthwise (DW) conv layer and a Feed-Forward Network (FFN) with GRN unit [47]. However, we use Batch Normalization (BN) instead of Layer Normalization [82] after the conv layer as BN can be equivalently merged into the conv layer to eliminate its inference costs. We use another BN after the FFN, which can also be equivalently merged into the preceding layer (i.e., the second linear layer in FFN). The numbers of such blocks in the four stages are denoted by N := (N1, N2, N3, N4). Following ConvNeXt-T, the vanilla architecture uses C = 96 and N = (3,3,9,3). By default, the last three stages use DW 13\u00d713 as the convolutional layer, and the first stage uses DW 3\u00d73.\nExperimental settings and metrics. According to Guideline 5, large-kernel ConvNets should be evaluated on downstream tasks, as their full potential may not be accurately reflected by ImageNet accuracy alone. Therefore, in addition to reporting the ImageNet-1K accuracy after 100 epochs of training, we transfer the trained model with UPerNet [83] to ADE20K to evaluate its performance on semantic segmentation. We report the single-scale mIoU after a 160k-iteration standard finetuning process [72]. Besides the parameters and FLOPs, we test the actual throughput on an A100 GPU with a batch size of 128 and an input resolution of 224\u00d7224, measured in images per second (img/s). See the Appendix for detailed configurations."}, {"title": "3.2.1 Block Design for Large-Kernel ConvNets", "content": "Guideline 6: regarding the block design, use efficient structures that perform both inter-channel communications and spatial aggregations to increase the depth. We first aim to enhance the model\u2019s representational capacity by universally incorporating structures that provide non-linearity and efficient trainable transformations. To achieve this, we employ a bottleneck consisting of a 1\u00d71 conv that reduces the channels to 1/4, followed by a DW 3\u00d73 conv, and another 1\u00d71 conv to expand the channels back (Fig. 7). BN and ReLU are applied after each conv layer as standard practice. As shown in Table 3a, this approach improves performance with acceptable overhead (+1.2 mIoU with a 12% slowdown). Performance degrades when we remove the DW 3\u00d73 conv, leaving only two 1\u00d71 conv layers, or when we replace the bottleneck structure with two DW 3\u00d73 layers. This indicates that effective structures require both spatial aggregation transformations and channel mixing. Motivated by this, considering that SE Block [63] elegantly realizes both transformations in a more efficient way (i.e., global average pooling and nonlinear mapping of the pooled vectors), we try it also with 1/4 channel reduction and observe a better performance and higher throughput. We therefore use the SE Block as a substructure of our block design in the following explorations."}, {"title": "3.2.2 Micro Design with Structural Re-parameterization for Large-Kernel ConvNets", "content": "Guideline 7: use dilated small kernels to re-parameterize a large kernel. We then explore the micro (i.e., layer-level) design for large-kernel ConvNet. According to Guideline 3, we should use a parallel small-kernel conv together with a large-kernel layer, as the former helps capture the small-scale patterns during training. Former discussions, however, have primarily focused on simple methods that make large kernels more practical and on explaining the underlying mechanism, rather than offering a competitive solution for building a powerful large-kernel architecture. While we now aim at the latter goal, we recognize that simply using a small kernel to re-parameterize a large kernel may not be optimal, as both capture dense patterns despite their different receptive fields. More than that, we reckon that, except for small-scale patterns, enhancing the large kernel\u2019s capability to capture sparse patterns (i.e., a pixel on a feature map may be more related to some distant pixels than its neighbors) may yield features of higher quality. The need to capture such patterns exactly matches the mechanism of dilated convolution - from a sliding-window perspective, a dilated conv layer with a dilation rate of r scans the input channel to capture spatial patterns where each pixel of interest is r \u2013 1 pixels away from its neighbor. Therefore, we use dilated conv layers parallel to the large kernel and add up their outputs.\nTo eliminate the inference cost of the extra dilated conv layers, we propose to equivalently transform the whole block into a single non-dilated conv layer for inference."}, {"title": "3.2.3 Kernel Size of Large-Kernel ConvNets", "content": "Guideline 8: decide kernel size according to the downstream task and usually use large kernels in middle- and high-level layers. As introduced above, the vanilla architecture uses 3\u00d73 conv in the first stage and 13\u00d713 in the last three stages. Table 3c shows that replacing the large kernels in the last three stages with 3\u00d73 or changing K from 13 to 11 degrades the models, especially in the ADE20K mIoU, which highlights the significance of large kernels. Interestingly, using 13\u00d713 in Stage 1 or enlarging K from 13 to 15 makes almost no difference in the ImageNet accuracy but reduces the ADE20K mIoU.\nRemark. We argue that this phenomenon does not mean larger kernels result in lower feature quality. It is due to the structural priors of UPerNet, which takes the features extracted by the low-level layers of the backbone and assumes they should only encode local information so that combining them with the high-level features extracted from the last layers of the backbone results in better segmentation. With larger kernels in lower stages, the low-level features are no longer confined to small local areas, so the UPerNet benefits less from combining them with the high-level features. We verify this explanation by making the UPerNet only use the high-level features (i.e., outputs of Stage 4) to evaluate the quality of the eventual features alone. Under this setting, K=15 delivers the best mIoU (42.7), the model with large kernels in Stage 1 performs as well as the baseline (42.4), and K=11 performs the worst (41.9). Such observations confirm that large kernels, even when they are used inappropriately, do not damage the feature quality of ConvNet but merely make the low-level features less favorable for certain downstream models that require local low-level features, suggesting we should decide the kernel size according to the specific downstream tasks and framework.\nConsidering this, we employ 13\u00d713 kernels in the middle- and high-level stages by default."}, {"title": "3.2.4 Scaling Rule of Large-Kernel ConvNets", "content": "Guideline 9: while scaling up the depth, the added blocks should use small kernels. The scaling rule of existing large-kernel ConvNets follows the traditional ConvNets, i.e., stacking more large kernels to build up a deeper model, but we argue that a large-kernel ConvNet may not benefit from more large kernels. In this group of experiments (Table 3d), we scale up N3 from 9 to 27, following ConvNeXt-S [16]. Considering that nine 13\u00d713 blocks may have already built up sufficient receptive field, we examine if the added blocks should also use large kernels. Specifically, we refer to the block with a Dilated Reparam Block as the Large Kernel Block (LarK Block) and name a block that uses a DW 3\u00d73 conv as a Small Kernel Block (Smak Block) so that there are 3 Smak Blocks in Stage 1 and 3/9/3 LarK Blocks in Stage 2/3/4 of the shallow model. While scaling up the depth of Stage 3, we tried the following options. A) All of the 27 blocks are Lark Blocks. B) We interleave Smak with Lark Blocks so that Stage 3 has 14 Lark Blocks and 13 Smak Blocks. C) We place two Smak Blocks after a LarK Block so that the resultant model will have the same 9 Lark Blocks as before but 18 extra SmaK Blocks. D) We remove the DW 3\u00d73 layers in Smak Blocks. Table 3d shows that scaling up the depth brings significant improvements, which is expected, and 9 Lark Blocks are sufficient. Though 27 LarK Blocks perform slightly better in the ADE20K mIoU, the inference speed is observably slowed down. Besides, the model without 3\u00d73 conv in Smak Blocks shows significantly lower mIoU with only minor improvements in the throughput, suggesting such small kernels in SmaK Blocks are useful while scaling up the depth of large-kernel ConvNet as they increase the abstract hierarchy of spatial patterns, though they may not effectively enlarge the ERF [1], [42]. This observation supports our motivation to decouple the effects of conv layers in enlarging the ERF and extracting more complicated spatial patterns, as discussed in Sec. 1."}, {"title": "3.2.5 Architectural Specifications", "content": "Following our proposed guidelines, we instantiate a series of models (Table 4). For a fair comparison with ConvNeXt V2 [47], UniRepLKNet-A/F/P/N follows its configurations. We scale up the depth to build UniRepLKNet-T/S and scale up the width to construct UniRepLKNet-S/B/L/XL/H."}, {"title": "3.3 Step 3: Generalizing Large-Kernel ConvNets to Multiple Modalities", "content": "To utilize the universal perception ability of UniRepLKNet, we preprocess the data of different modalities into B \u00d7 C' \u00d7 H \u00d7 W embedding maps, where B is the batch size and C' is determined by the modality, and configure the input channel of the first layer of UniRepLKNet to C\". For simplicity, the other parts of the models are the same as the UniRepLKNet initially designed for the image without any modality-specific customization.\nTime-series. Let L and D be the length and dimensions of a time-series sequence XT \u2208 $\\mathbb{R}^{B \\times L \\times D}$, we adopt the embedding layer in Corrformer [46] to split it into n nodes then project it into a latent space $\\mathbb{R}^{B \\times n \\times L \\times D'}$ (D' and n are configurable hyper-parameters of the embedding layer). Then we simply reshape it into a single-channel embedding map:\n$X_{T} \\in \\mathbb{R}^{B \\times L \\times D} \\rightarrow \\mathbb{R}^{B \\times n \\times L \\times \\frac{D}{n}} \\rightarrow \\mathbb{R}^{B \\times n \\times 1 \\times H \\times W} \\quad s.t. \\quad H W=\\frac{L D'}{n}$.\nAudio. Let T and F be the numbers of time frames and frequency bins, we use XA \u2208 $\\mathbb{R}^{B \\times T \\times F}$ to represent audio data. A sample is seen as a 1 \u00d7 T \u00d7 F embedding map that resembles a single-channel image so C'=1, H=T, W=F.\n$X_{A} \\in \\mathbb{R}^{B \\times T \\times F} \\rightarrow \\mathbb{R}^{B \\times 1 \\times T \\times F}$\nPoint cloud. Assume a sample comprises P points each represented by the X/Y/Z coordinates, we use a series of conv layers to generate three-view projections [28]. We configure the resolution of the generated projections to be 224 so that H=W=224, C'=3.\n$X_{P} \\in \\mathbb{R}^{B \\times P \\times 3} \\rightarrow \\mathbb{R}^{B \\times 3 \\times 224 \\times 224}$.\nVideo. We represent a video as NF frames and each frame is a 3 \u00d7 h \u00d7 w image. We reshape it by merging the frame dimension into the height and width dimensions so that we obtain a representation that can be viewed as a single image created by laying out (i.e., concatenating) the NF frames. For example, in our experiments, we have NF=16 and h=w=224 so that H=W=896. Generally,\n$X_{V} \\in \\mathbb{R}^{B \\times N_{F} \\times 3 \\times h \\times w} \\rightarrow \\mathbb{R}^{B \\times 3 \\times H \\times W} s.t. \\quad H W=N_{F} h w$."}, {"title": "3.4 Step 4: Fusing Multimodal Features with Large Kernel Convolution", "content": "In addition to extracting features, we further explore large-kernel convolution to fuse multimodal features as cross-attention [84]. Inspired by the flexibility of asymmetric convolution in fusing features of diverse shapes [85], we propose the asymmetric large-kernel convolution to broadly fuse features across diverse shapes and modalities. As cross-attention mechanism to fuse two features of X and Y, where X \u2208 $\\mathbb{R}^{L_{1} \\times D}, Y \\in \\mathbb{R}^{L_{2} \\times D}, L_{1}$ and L2 denote the length of a sequence of tokens, D denotes the feature dimension (Note that the feature map X \u2208 $\\mathbb{R}^{L_{1} \\times D}$ can be easily reshaped as X \u2208 $\\mathbb{R}^{H \\times W \\times C}$).\nThe asymmetric large-kernel convolution uses one feature map as the convolutional kernel to convolve another feature map, allowing for dynamic and context-aware fusion of multimodal features. Specifically, the convolution operation is performed by treating Y as the convolutional kernel that is applied to X. In this setup, each element of Y serves as a dynamic filter that modulates X according to its contextual information. The output feature map Z can be expressed as:\n$Z_{i, j}=\\sum_{k=1}^{L_{2}} X_{i+k-1} \\cdot Y_{k, j}$,\nwhere Zij represents the correlation between X starting at position i with the filter defined by Yj. This approach allows X to be dynamically influenced by the patterns in Y, facilitating an adaptive and effective fusion of the two feature maps. It efficiently captures the intrinsic correlation between the features, making it a computationally efficient alternative for multimodal feature fusion tasks."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experiments for Visual Recognition\nImageNet classification. Following ConvNeXt [16], we use the widely adopted 300-epoch receipt to train UniRepLKNet-A/F/P/N/T/S on ImageNet-1K; we pre-train UniRepLKNet-S/B/L/XL on ImageNet-22K using the 90-epoch receipt and fine-tune with ImageNet-1K for 30 epochs (see the Appendix for details). As our goal is to develop models that run with high actual speed, we evaluate the actual throughput on the same A100 GPU using a batch size of 128. Table 5 shows the top-1 accuracy on the ImageNet-1K validation set where the results are sorted by the throughput. We split the results into seven segments for better readability. 1) UniRepLKNet-A/F outperforms ConvNeXt-V2-A/F by 0.8/0.6 in the accuracy and runs 19%/17% faster, respectively. 2) UniRepLKNet-P/N out-performs FastViT-T12/S12 and ConvNeXt V2-P/N by clear margins. 3) UniRepLKNet-T outperforms multiple small-level competitors. 4) UniRepLKNet-S outperforms a series of small-level and even base-level models in both speed and accuracy and runs almost as fast as InternImage-T. 5) With ImageNet-22K pretraining, UniRepLKNet-S even approaches the accuracy of RepLKNet-31L and runs 3\u00d7 as fast as the latter. UniRepLKNet-B outperforms CoAtNet-2 and DeiT III-B by clear margins. UniRepLKNet-L outperforms InternImage-L in both accuracy and throughput. 6) On the XL-level, UniRepLKNet-XL outperforms in both accuracy and throughput, running more than 2\u00d7 as fast as CoAtNet-3 and 3x as DeiT III-L.\nCOCO object detection and instance segmentation. We transfer the pretrained UniRepLKNets as the backbones of Cascade Mask R-CNN [102], [103] and adopt the standard 3x (36-epoch) training configuration with MMDetection [104]. Table 6 shows UniRepLKNet outperforms Swin, ConvNeXt, RepLKNet, and SLaK, which are representatives of ViTs, modern medium-kernel ConvNets, and existing large-kernel ConvNets, respectively, and shows comparable performance to InternImage [88], which is a latest powerful architecture with deformable convolution.\nADE20K semantic segmentation. We use the pretrained UniRepLKNets as the backbones of UPerNet [83] on ADE20K [105] and adopt the standard 160k-iteration training receipt with MMSegmentation [72]. Table 7 reports the mIoU on the validation set. Impressively, UniRepLKNet outperforms InternImage and the other models."}, {"title": "4.2 Universal Perception on More Modalities", "content": "Time-series. Following Corrformer [46], we conduct experiments on the Global Temperature and Wind Speed Forecasting challenge 7 using the dataset collected from the National Centers for Environmental Information (NCEI), GFS & stands for the Global Forecasting System. This huge-scale dataset contains hourly averaged wind speed and temperature data from 3,850 stations with different geographical scales and densities, spanning from 2019 to 2021. For a fair comparison with Corrformer [46], which was the previous state-of-the-art method, we use its embedding layer (as introduced in Sec. 3.3) and decoder and only replace its encoder transformer with UniRepLKNet-S. We also compare UniRepLKNet-S against a wide range of methods, including statistical and numerical approaches. Table 12 shows UniRepLKNet delivers a new state-of-the-art forecasting precision, achieving the lowest errors of 7.602, 1.832, 3.865, and 1.301 for MSE and MAE in forecasting global temperature and wind speed, respectively, with fewer parameters than existing deep learning methods. It is particularly noteworthy that UniRepLKNet, a generalist model, outperforms time-series specialists such as Pyraformer [113] and Corrformer [46] in both precision and efficiency. The significant advantages of UniRepLKNet are that it opens up new avenues for architectural discussions in time-series forecasting and presents a viable alternative to transformer"}, {"title": "4.3 Scalable Multimodal Pretraining and Generation", "content": "Stage 0: CLIP Pretraining. We utilize the UniRepLKNet-L as the image tower with a standard projection, and follow previous pratice [54], [138] to use a text tower with the same size as ViT-g-14 model pretrained with 11B text samples [138]. The size of the combined image + text CLIP model is 1.4B parameters. UniRepLKNet excels in zero-shot image recognition abilities compared with the same scale models including OpenAI CLIP-L [136], OpenCLIP-L [54], FLIP-L [137], and OpenCLIP-ConvNeXt-L [16], [54] in Table 13 among 26 zero-shot tasks. It's worth noting that our CLIP model shows competitive performance (72.1 v.s. 72.4) compared with the EVA-01-CLIP-g/14 model, which has more than 3\u00d7 parameters than ours.\nStage 1: Large Vision-Language Model (VLM) Pretraining. After CLIP pretraining, we then use pretrained CLIP-UniRepLKNet-L for training large VLMs. Specifically, we use LLaVA-v1.5 [156] as a baseline, which incorporates the text-image alignment and visual instruction process with a convolutional backbone. Specifically, we use LLaVA pre-training data to align Vicuna-7B and UniRepLKNet, then LLaVA-SFT-665k for visual instruction tuning.\nAs shown in Table 14, UniRepLKNet-Chat-7B demonstrates significant advantages across various benchmarks in Visual Question Answering (VQA), Image Captioning, and multimodal Benchmark tasks. Notably, in the GQA task, UniRepLKNet-Chat-7B scores 59.8, positioning itself competitively among Vision Specialist LLMs. It excels in the VQAv2 task with a remarkable score of 80.2, surpassing models such as Flamingo, InstructBLIP, and IDEALF. Additionally, in the OKVQA task, UniRepLKNet-Chat-7B"}, {"title": "5 CONCLUSION", "content": "In this paper, the UniRepLKNet shows a leading performance in image recognition and achieves remarkable results on audio and time-series data, outperforming multiple specialist models on those modalities. Traditionally, ConvNets excelled primarily in visual tasks, yet the emergence of Transformer-based architectures had shifted focus away from ConvNets as researchers sought new paradigms for tackling multimodal data. Such results signify a \"comeback\" for ConvNet in its original domain and showcase large-kernel ConvNet's potential to \u201cconquer\" new territories. We hope this advancement will inspire further research into large-kernel ConvNets, encouraging new applications and optimizations that extend ConvNets' utility across a broader range of data modalities."}, {"title": "APPENDIX A: GENERAL TRANSFORMATION FROM DIALTED Convolution To NON-DILATED LARGE-KERNEL CONVOLUTION", "content": "Since ignoring pixels of the input is equivalent to inserting extra zero entries into the conv kernel, a dilated conv layer with a small kernel can be equivalently converted into a non-dilated layer with a sparse larger kernel. Let k be the kernel size and r be the dilation rate of the dilated layer, by inserting zero entries, the kernel size of the corresponding non-dilated layer will be (k \u2212 1)r + 1, which is referred to as the equivalent kernel size for brevity.\nAs discussed in the paper, to eliminate the inference costs of the extra dilated conv layers in the Dilated Reparam Block, we propose to equivalently transform the whole block into a single non-dilated conv layer for inference. As discussed before, let k and"}]}