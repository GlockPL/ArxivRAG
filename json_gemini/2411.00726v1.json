{"title": "Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract", "authors": ["Fan Xiao", "Junlin Hou", "Ruiwei Zhao", "Rui Feng", "Haidong Zou", "Lina Lu", "Yi Xu", "Juzhao Zhang"], "abstract": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a common complication of diabetes. As two different imaging tools for DR grading, color fundus photography (CFP) and infrared fundus photography (IFP) are highly-correlated and complementary in clinical applications. To the best of our knowledge, this is the first study that explores a novel multi-modal deep learning framework to fuse the information from CFP and IFP towards more accurate DR grading. Specifically, we construct a dual-stream architecture Cross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus image modalities. In particular, a meticulously engineered Cross-Fundus Attention (CFA) module is introduced to capture the correspondence between CFP and IFP images. Moreover, we adopt both the single-modality and multi-modality supervisions to maximize the overall performance for DR grading. Extensive experiments on a clinical dataset consisting of 1,713 pairs of multi-modal fundus images demonstrate the superiority of our proposed method. Our code will be released for public access.", "sections": [{"title": "1 Introduction", "content": "Diabetic retinopathy (DR) is one of the chronic complications of diabetes mellitus and the leading cause of avoidable blindness [12]. According to the International Clinical Diabetic Retinopathy Scale [16], the severity of DR is graded into five grades, i.e., no DR, mild Nonproliferative DR (NPDR), moderate NPDR, severe NPDR, and proliferative DR (PDR). In clinical practice, color fundus photography (CFP) is commonly used for DR screening. However, patients with cataract have serious lens opacity, which obviously affects the imaging effect of CFP. Instead, infrared fundus photography (IFP) can obtain clear retinal images due to the good penetration of infrared light to turbidity media Thus, IFP can complement other imaging techniques in monitoring and assessing treatment response in DR patients [18,13,10]. Fig. 1 shows some examples of CFP and IFP images. Fig. la illustrates that CPF and IFP of healthy people have clear fundus structure which exhibits a significant correlation in terms of image resolution and utility [2]. On one hand, Fig. 1b captured from a cataract patient shows that CFP is obscured by a gray fog, while IFP remains clear enough to discern the fundus structure. Particularly, useful features for DR diagnosis such as hemorrhages (blue box) and exudate (yellow box) can be more easily identified in IFP. On the other hand, as illustrated in Fig. 1c that there are appearance differences of lesions between the two modalities. Specifically, CFP provides better color information that aids doctors in distinguishing lesions, which are often overlooked in IFP [13]. This dichotomy underscores the potential of leveraging the complementary strengths of both modalities to enhance DR grading accuracy.\nIn recent years, there has been an increase in the use of multi-modal fundus images for retinal disease diagnosis. Most existing works focus on the fusion of CFP and optical coherence tomography (OCT). For example, Wang et al. [15] proposed a multi-modal learning network, termed GeCoM-Net, which encodes the geometric correspondences between OCT slices and their associated CFP regions. This approach advances the diagnosis of retinal diseases such as diabetic macular edema, impaired visual acuity and glaucoma, demonstrating enhanced diagnostic efficacy. Other works consider the differences between modalities and design tailored fusion strategies [14,9,7]. However, multi-modal interaction of CFP and IFP for retinal disease diagnosis is still largely unexplored."}, {"title": "2 Methodology", "content": "Based on multi-modal fundus imaging comprised of CFP and IFP, we propose a novel dual-stream network CFT for DR grading. In particular, we design a CFA module that fully leverages the feature representations extracted by individual patches of CFP and IFP images. The fused representations along with the overall representation obtained by the class token is then classified to obtain the DR grading result."}, {"title": "2.1 Network architecture", "content": "As shown in Fig. 2, the input CFP image $I_{cf} \\in \\mathbb{R}^{H \\times W \\times C}$ and IFP image $I_{if} \\in \\mathbb{R}^{H \\times W \\times C}$are first divided into patches with size $p_{cf}$ and $p_{if}$. Then the number of patches N can be calculated by $N = (H \\times W)/p^2$. These patches of the two fundus modalities are flattened into one-dimensional vectors and then passed through the embedding composed of linear layers. The obtained patch embedding concatenates the position embedding which is a learned one-dimensional vector in order to get position information. Because of the overall image information to be considered, a learnable class token is the same as patch embedding concatenating the position embedding and serving as the first one in the embedding sequence. The embedding sequence is fed into ViT encoder which contains repeated Transformer blocks. Specifically, each Transformer block consists of multi-head attention (MHA) and feed forward network (FFN), which are connected by residual operations and layer normalization.\nThe input CFP image $I_{cf}$ and IFP image $I_{if}$ through the above process are encoded to the feature representations. Then the feature representations of the class token are fed into a multi-layer perception for single modality DR grading. The other feature representations corresponding to patches are fed into CFA module to obtain cross-modality information. Then the fused patch feature representations are fed into a classifier consisting of a layer normalization and a linear layer for multi-modality DR grading."}, {"title": "2.2 Cross-Fundus Attention Module", "content": "In order to integrate the representation of the patch embedding between the two modalities, we design a CFA module to obtain fusion information. All of the feature representations of the patch embedding $F_{cf} \\in \\mathbb{R}^{N_{cf} \\times C}$ and $F_{if} \\in \\mathbb{R}^{N_{if} \\times C}$ are fed into linear projection to reduce feature dimension to L, where N represents the number of patches and C represents the dimension. The linear projection block consists of a linear layer, a layer normalization, and a ReLU activation function. The new feature representations of the patch embedding $F'_{cf} \\in \\mathbb{R}^{N_{cf} \\times L}$ and $F'_{if} \\in \\mathbb{R}^{N_{if} \\times L}$ go through a cross attention block, as shown in Fig. 2. We split the queries, keys, and values between the two modal representations of the patch embeddings. Specifically, we denote $F'_{cf}$ as queries, $F'_{if}$ as keys and values in CF-cross attention and $F'_{if}$ as queries, $F'_{cf}$ as keys and values in IF-cross attention:\n$Q_{cf} = F'_{cf}W^Q, K_{cf} = F'_{if}W^K, V_{cf} = F'_{if} W^V, $  (1)\n$Q_{if} = F'_{if}W^Q, K_{if} = F'_{cf}W^K, V_{if} = F'_{cf} W^V, $ (2)\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{L \\times d}$ are linear projections. Then, due to the multi-head mechanism we split the converted Q,K,V $\\in \\mathbb{R}^{N \\times d}$ into ${Q_n}_{n=1}^{N}$, ${K_n}_{n=1}^{N}$, ${V_n}_{n=1}^{N}$ where N represents the number of heads. So we can calculate the attention weight of each head by\n$A_n = softmax(\\frac{Q_n K_n^T}{\\sqrt{d/N}}),$ (3)\nwhere $\\sqrt{d/N}$ represents a scaling factor. The output of head $H_n \\in \\mathbb{R}^{N \\times d/N}$ is calculated by matrix multiplication of the attention weight $A_n$ and values $V_n$, i.e., $H_n = A_n V_n$. Finally, we concatenate the output of each single head ${H_n}_{n=1}^{N}$"}, {"title": "2.3 Training and Inference", "content": "To speed up the training process, we utilize a ViT model pretrained on a large fundus image dataset [19], and finetune it on our target data. We adopt the cross entropy loss function to train the single modality 'MLP head' and fusion classifier:\n$L(y, \\hat{y}) = \\sum_{i=1}^{k} y_i log(\\hat{y}_i),$(5)\nwhere y represents the ground-truth label and $\\hat{y}$ represents the prediction label. The total loss is comprised of two single-modal losses $L_{cf}, L_{if}$ and a multi-modal loss $L_{cls}$:\n$L_{total} = \\lambda L_{cf} + (1 - \\lambda) L_{if} + L_{cls},$(6)\nwhere $\\lambda$ is the hyper-parameter that balances the loss of the two modalities. In the inference stage, the output of two heads is averaged and added to the output of the classifier to get the final result."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Clinical Dataset", "content": "Due to the lack of publicly available CFP and IFP image pair datasets, we evaluate our method on a clinically acquired dataset collected from Department of Ophthalmology of the anonymous Hospital from Jan. 2020 to March. 2022. The dataset named CFP-IFP DR (CIDR) comprises paired CFP and IFP images from 1,713 eyes of 616 patients in different periods, including instances of patients afflicted with cataract. CFP images were captured by a fundus camera with a resolution over 2,000\u00d72,000, and IFP images were obtained from a retinal health assessment device with a resolution of 768\u00d7768. Each pair of images was annotated by an experienced ophthalmologist. Eventually, we obtain the labeled CIDR with 714 eyes showing no DR, 123 eyes showing mild NPDR, 249 eyes showing moderate NPDR, 267 eyes showing severe NPDR, and 360 eyes showing PDR. We randomly split CIDR into 80% for training and 20% for validation according to category proportions."}, {"title": "3.2 Implementation details", "content": "All networks are optimized using Adam optimizer with a weight decay of le-5 for 100 epochs. The initial learning rate is set to le-4 with cosine annealing. Both CFP and IFP images are resized to 512\u00d7512, and data augmentations include flipping, random cropping, color jittering, and affine transformations. We use the Quadratic Weighted Kappa [5], Accuracy, and Macro-F1 score as overall comparison metrics."}, {"title": "3.3 Comparison to other state-of-the-art methods", "content": "We conduct experiments on our CIDR dataset. For single-modal methods, we train ViT and ViT with self attention at patch features of last layer using CFP and IFP images, respectively. For multi-modal methods, we compare our model with commonly-used fusion strategies, including voting [4,17] and feature fusion [8,6,11]. We adopt the ViT-Small structure as our backbone. As can be observed from the single-modal section in Table 1, compared with ViT models, two ViT-self attention models increase the kappa scores by 0.97% and 0.64% on CFP and IFP, respectively. Furthermore, the group of multi-modal approaches show significant performance improvement, compared to all single-modal approaches. Among the multi-modal fusion methods, our proposed CFP-IFP-cross attention model achieves the best performance with a Kappa score of 84.44%, an Accuracy score of 73.47% and a F1 score of 65.51%. Besides, dual-stream cross attention architecture is better than single-stream cross attention architecture with an increase of 1.86% and 1.56% on Kappa, respectively. This suggests that the multi-modal fundus images have complementary features, which can improve the DR grading accuracy."}, {"title": "3.4 Ablation Study", "content": "Furthermore, we conduct an ablation study to evaluate the efficacy of linear projection, loss function, and fusion methods. The first two rows of Table 2 demonstrate the effectiveness of linear projection with an increase of 0.23% on Kappa. The loss functions $L_{if}$ and $L_{cf}$ bring 0.86% and 0.54% improvements of Kappa respectively in lines 3 to 7 and 4 to 7 of the Table 2. These findings show that the linear projection structure with $L_{if}$ and $L_{cf}$ function is effective in enhancing the performance of the DR grading. Furthermore, we compare the maxpool fusion operation in our CFA with two common feature fusion methods, i.e., meanpool and concatenate. The Kappa score of the maxpool method is higher than the other two feature fusion methods by 1.36% and 0.75% in lines 7, 5 and 6. This indicates that maxpool fusion method can better integrate features between the two modalities.\nWe also investigated the influence of different sets of loss weights. As shown in Fig. 3, loss function weight $\\lambda$ get the best performance at 0.6, with a kappa score of 84.44%. Excessively high or low weights would significantly affect the performance and generalizability of the network, ultimately hindering its ability"}, {"title": "3.5 Visualization of Attention Features", "content": "Finally, we present visualization results from the 'MLP head' by Attention Roll-out [1] in Fig. 4. There are different but complementary highlighted lesion regions in CFP and IFP. Lesions like retinal detachment (red arrow) is easily distinguished in CFP at PDR stage, while other lesions such as exudate (yellow box) and hemorrhages (blue box) are more easily distinguished in IFP. Therefore, fusing this complementary information enhances DR grading accuracy."}, {"title": "4 Conclusion", "content": "In this work, we constructed a novel multi-modal framework CFT for DR grading on CFP and IFP images. We used ViT to extract features from patches in each modality, and integrated features across modalities using a designed CFA module. Experiments on a clinical dataset showed that our model achieved state-of-the-art results compared to existing methods. Overall, our study presents a promising step towards multi-modal fundus image diagnosis and has the potential to contribute to the advancement of medical image analysis. Overall, our study presents a promising step towards multi-modal fundus image diagnosis and has the potential to contribute to the advancement of medical image analysis."}]}