{"title": "ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao language pair", "authors": ["Hong-Viet Tran", "Minh-Quy Nguyen", "Van-Vinh Nguyen"], "abstract": "This paper presents an results of the VLSP 2022-2023 Machine Translation Shared Tasks, focusing on Vietnamese-Chinese and Vietnamese-Lao machine translation. The tasks were organized as part of the 9th, 10th annual workshop on Vietnamese Language and Speech Processing (VLSP 2022, VLSP 2023). The objective of the shared task was to build machine translation systems, specifically targeting Vietnamese-Chinese and Vietnamese-Lao translation (corresponding to 4 translation directions). The submission were evaluated on 1,000 pairs for testing (news and general domains) using established metrics like BLEU [Papineni] and SacreBLEU [Matt:2018]. Additionally, system outputs also were evaluated with human judgment provided by experts in Chinese and Lao languages. These human assessments played a crucial role in ranking the performance of the machine translation models, ensuring a more comprehensive evaluation.", "sections": [{"title": "Introduction", "content": "Neural Machine Translation (NMT) has currently obtained state-of-the-art in machine translation systems. However, the translation quality is still a challenge in translation systems. Neural Machine Translation (NMT) [Cho2014LearningPR, Sutskever:2014, Vaswani:2017] has recently shown impressive results compared to Statistical Machine Translation (SMT) [Wu:2016, KleinKDSR17]. However, NMT systems still have great challenges [koehn:2017]. The MT track basically corresponds to a subtitling translation task. The natural translation unit considered by the human translators volunteering for News is indeed the single caption as defined by the original transcript - which in general does not correspond to a sentence, but to fragments of it that fit the caption space. While translators can look at the context of the single captions, arranging the MT task in this way would make it particularly difficult, especially when word reordering across consecutive captions occurs. For this reason, we preprocessed all the parallel texts to rebuild the original sentences, thus simplifying the MT task. Table 1 provides statistics on in-domain texts supplied for training and evaluation purposes for each MT task. Texts are pre-processed (tokenization, Chinese and Vietnamese segmentation) with the tools used for setting-up baseline systems (see below). For this purpose, the task involved creating a comprehensive dataset with human-annotated translations\u00b9.\nIn 2022, a significant milestone was reached for Machine Translation (MT) within the VLSP evaluation campaign, driven by the notable contributions of teams focusing on Chinese-Vietnamese translation through news sources. Despite data scarcity posing a major challenge, participating teams successfully leveraged the linguistic similarities between Chinese and Vietnamese most notably the one-to-one mapping between Sino Vietnamese and Chinese words to develop specialized methods. Moving into 2023, VLSP has shifted its attention to Lao-Vietnamese and Vietnamese-Lao Machine Translation tasks, where limited training data continues to hinder model development. Nonetheless, the substantial similarities between Lao and Vietnamese, including numerous one-to-one lexical mappings, open opportunities to apply specialized techniques for this closely related language pair, even under constrained data conditions."}, {"title": "Training & Test Data", "content": "In the VLSP 2022 and VLSP 2023 evaluation campaign, we released comprehensive training datasets designed to support Vietnamese-Chinese and Vietnamese-Lao machine translation tasks. These datasets include development and public test sets to facilitate model optimization and evaluation. Specifically, the VLSP 2022 dataset comprises over 300,000 Vietnamese-Chinese bilingual sentence pairs for training, with an additional 1,000 sentences for development and testing. Similarly, the VLSP 2023 dataset, designed for Vietnamese-Lao translation, contains 100,000 bilingual sentence pairs for training, 2,000 for development, and 1,000 for testing.\nThe provision of development and public test sets allows participants to fine-tune their models before formal evaluation on the secure private test set. Notably, all development, public test, and private test sets are within the same linguistic domain, ensuring consistency in evaluation and benchmarking. SacreBLEU is recommended for model evaluation, as it offers a reliable metric for assessing machine translation accuracy.\nThe input data is provided in UTF-8 text format, with 1-to-1 aligned bilingual sentence pairs, ensuring precise correspondence throughout training and testing. This approach facilitates standardization and improves the accuracy of machine translation systems, contributing to research and application in automatic translation for Vietnamese in a multilingual context.\nTable 1 shows statistics on in-domain texts supplied for training and evaluation purposes for two MT tasks: Vietnamese Chinese Machine Translation Systems for VLSP 2022 and Vietnamese \u2192 Lao Machine Translation Systems for VLSP 2023. All parallel texts were tokenized and truncated using sentence piece scripts, and then they are applied to Sennrich's BPE [Sennrich2015NeuralMT]. For Vietnamese, we only apply Moses's scripts for tokenization and true-casing."}, {"title": "Evaluation", "content": "The participants to the MT track had to provide the automatic translation of the test sets in text format. The output had to be case-sensitive, detokenized and had to contain punctuation. The quality of the translations was measured both automatically, against the human translations created by the open translation project, and via human evaluation (Section 5).\nCase sensitive scores were calculated for the three automatic standard metrics BLEU [Papineni] and SacreBLEU [Matt:2018], as implemented in mteval-v13a.pl and sacrebleu, by calling:\n\u2022 mteval-v13a.pl -c\n\u2022 sacrebleu -t vlsp2022/systems -1 zh-vi-echo MTTracks\n\u2022 sacrebleu -t vlsp2023/systems -1 lo-vi -echo MTTracks\nDetokenized texts were passed, since the two scorers apply an internal tokenizer. Before the evaluation, Chinese texts were segmented at character level, keeping non-Chinese strings as they are. In order to allow participants to evaluate their progresses automatically and in identical conditions, an evaluation server was developed. Participants could submit the translation of any development set to either a REST Webservice or through a GUI on the web, receiving as output the three scores BLEU, NIST [NIST:2008], TER [TER:2008] and SacreBLEU computed as above. The core of the evaluation server is a shell script wrapping the mteval scorers. The evaluation server was utilized by the organizers for the automatic evaluation of the official submissions. After the evaluation period, the evaluation on test sets was enabled to all participants as well."}, {"title": "System submissions", "content": "In the multilingual machine translation tasks at VLSP 2022 and VLSP 2023, we conducted Vietnamese-to-Chinese and Vietnamese-to-Lao translation tasks, attracting substantial participation from both domestic and international organizations. Specifically:\nVLSP 2022 MT: The machine translation task for Vietnamese-to-Chinese and vice versa had 25 registered teams, including universities such as JAIST and HUST, as well as major corporations like Samsung SDS, VinBigData, and VCCorp. Among these, 5 teams submitted official entries, complete with models for performance evaluation and detailed technical reports.\nVLSP 2023 MT: The Vietnamese to Lao and Lao to Vietnamese translation task attracted 26 registered teams, including institutions and universities such as HUST, MTA, UET-VNU, and technology companies like Viettel and Bosch Global Software Technologies Vietnam. In total, 7 teams submitted official entries for evaluation.\nIn both machine translation tasks, we selected for each task the three methods that achieved the highest results for each translation task. Each of these methods has technical reports that demonstrate the approach, method content, contribution to the machine translation task, and results achieved. We present each of these methods in each translation task the following section in 4.1 and 4.2"}, {"title": "Vietnamese-Chinese Machine Translation", "content": "In the task of Vietnamese-Chinese bidirectional machine translation, we selected the three most effective approaches to achieve accurate and fluent translations that preserve the original meaning. Each method was carefully evaluated for its translation accuracy and fluency to ensure high-quality, natural output. The teams employed distinct techniques and strategies, including language model fine-tuning and input data optimization, to maximize the quality and naturalness of the translations. The selected methods are as follows:\n\u2022 Team 1 (SDS): An Efficient Approach for Machine Translation on Low-resource Languages\n\u2022 Team 2 (VBD-MT): VBD-MT Vietnamese-Chinese Bidirectional Translation System\n\u2022 Team 3 (JNLP): An Effective Method using Phrase Mechanism in Neural Machine Translation"}, {"title": "An Efficient Approach for Machine Translation on Low-resource Languages.", "content": "The team proposes leveraging data synthesis as a technique to augment the training set for low-resource language pairs, particularly Vietnamese-Chinese. To accomplish this, the mBART-50 [Tang2020MultilingualTW] machine translation system is first fine-tuned with existing bilingual data. It is then employed to translate from the target language back into the source language, effectively generating a synthetic bilingual dataset. This newly synthesized dataset is subsequently merged with authentic bilingual data, providing a more comprehensive training set for the final model.\nIn constructing the final translation model, the team follows a systematic approach involving three key steps: (1) Training a Vietnamese-English translation model with mBART-50; (2) Enhancing the dataset by generating additional bilingual data through selected sentences extracted from the monolingual dataset; (3) Fine-tuning the model using this expanded bilingual dataset. The VLSP 2022 dataset, which includes 300,000 bilingual sentence pairs and extensive, cleaned monolingual corpora, is employed to ensure that the input data remains of high quality throughout the training process.\nFor low-resource language pairs, the team applies the TF-IDF selection technique to identify and extract significant sentences from a large monolingual dataset containing 25 million Vietnamese and 19 million Chinese sentences. The resulting dataset, a synthesized bilingual corpus, is then combined with the original bilingual data to enhance the accuracy and robustness of the final translation model.\nThe utilization of mBART-50 in this study capitalizes on its multilingual translation capabilities, achieved through denoising training. By supporting up to 50 languages and subsequently fine-tuning the model with VLSP data, the research team successfully developed high-quality multilingual machine translation models specifically tailored to Vietnamese and Chinese, thereby enhancing translation performance for these low-resource languages."}, {"title": "VBD-MT Vietnamese-Chinese Bidirectional Translation System", "content": "Baseline system is constructed using the robust Transformer model, which is fine-tuned with mBART-25, a model pre-trained on 25 languages, including both Chinese and Vietnamese. For text processing, we implement the SentencePiece tool to handle tokenization and vocabulary filtering, reducing the vocabulary size from an initial 250K to 67K tokens. This reduction aligns with the limited GPU resources available, allowing for efficient training without the need for high-performance server infrastructure.\nTo further enhance the dataset, the team apply back-translation using a top-k sampling technique, selecting the top 5 highest-scoring outputs to generate diverse synthetic data. This method yields 211K back-translated sentence pairs from Chinese to Vietnamese (Zh-Vi) and 403K pairs from Vietnamese to Chinese (Vi-Zh). The synthetic data is then combined with authentic bilingual data, effectively expanding the training set and boosting model performance.\nThe system also employs an ensemble method, achieved by averaging the model weights from the last N checkpoints, where N is optimally set to 5. This ensembling approach significantly enhances accuracy, particularly when used alongside the back-translation data.\nTo address potential translation errors in numeric and date-time values, the team introduce a post-processing step with customized patterns designed for these data types. Although this post-processing does not directly increase the BLEU score, it improves translation quality by ensuring that critical values, such as those related to people and currency, are accurately translated.\nThe system 2 finetunes a Transformer model in conjunction with mBART-25 and employs SentencePiece to reduce the vocabulary size in compliance with GPU resource constraints. The team applies back-translation to generate additional bilingual data, which is then merged with the original dataset to expand the training corpus. The system also utilizes an ensemble technique to enhance accuracy. Finally, a post-processing step is added to correct errors concerning numerical data and dates."}, {"title": "An Effective Method using Phrase Mechanism in Neural Machine Translation", "content": "This approarch developed Phrase Transformer, a model based on the Transformer architecture that incorporates phrase-based attention mechanisms to improve machine translation performance. Unlike prior models, Phrase Transformer eliminates the need for external syntactic tree information, making it more efficient and lightweight compared to other phrase-level attention models. The core concept behind Phrase Transformer is to enhance word representations by leveraging local context and capturing dependencies between phrases within a sentence, enabling more nuanced translation outputs.\nIn the preprocessing stage, they utilized Byte-Pair Encoding (BPE) to address out-of-vocabulary issues by breaking words down into sub-tokens. For Vietnamese, this work performed 4,000 \u0392\u03a1\u0395 operations, while for Chinese, which lacks inherent word spacing, they applied 16,000 operations. For Chinese text, the BPE segmentation module treats the entire raw sentence as a single word segment, ensuring effective sub-tokenization even without spacing between characters.\nTo evaluate Phrase Transformer's performance against the original Transformer model, they trained both models on the Chinese-Vietnamese bilingual dataset provided by VLSP 2022, without any supplementary external data or pretrained models. Both models were tested under identical configurations, and they averaged the weights from the last 5 checkpoints to produce the final model used for translation testing.\nThe experimental results reveal that Phrase Transformer consistently outperforms the original Transformer across various n-gram sizes, underscoring the effectiveness of its phrase-based attention mechanism in capturing sentence meaning. Furthermore, PhraseTransformer's adaptability extends beyond translation tasks to other languages and NLP applications, as it operates independently of external syntactic tree information, making it a versatile tool for diverse linguistic challenges."}, {"title": "Vietnamese-Lao Machine Translation", "content": "In the Vietnamese-Lao bidirectional machine translation task, high performance has been achieved using methods based on the Transformer architecture, a leading approach in machine learning. These methods exploit the Transformer's capacity to deliver high-quality translations by efficiently managing complex word sequences and capturing semantic relationships between words within sentences. Notably, by fine-tuning Transformer-based pretrained models, we can tailor the system to better handle Vietnamese-Lao bilingual data, thereby improving translation accuracy and naturalness. This fine-tuning enhances the system's precision while also increasing its capacity to capture contextual meaning and accurately reproduce the unique grammatical structures of both languages, achieving high standards in bidirectional machine translation quality.\nIn the task of Vietnamese-Lao bidirectional machine translation, we selected the three most effective approaches, specifically as follows:\n\u2022 Team 1 (BlueSky): A Transformer-Based Model for Lao-Vietnamese Machine Translation\n\u2022 Team 2 (MTA_AI): Vietnamese-Lao Bidirectional Translation System\n\u2022 Team 3 (BGSV AI): A Sequence-to-Sequence Model for Lao-Vietnamese Machine Translation"}, {"title": "A Transformer-Based Model for Lao-Vietnamese Machine Translation", "content": "Blue_Sky leverages a pretrained mBART model [Tang2020MultilingualTW]initially trained on extensive monolingual datasets in both Vietnamese and Lao. The model is subsequently fine-tuned using a bilingual dataset from VLSP, which enhances its translation accuracy and fluency for both languages. This approach integrates diverse linguistic data from monolingual sources, allowing the model to capture complex grammatical and syntactical structures unique to Vietnamese and Lao, providing a strong base for the fine-tuning phase.\nFor machine translation tasks, the Transformer WMT [ott-etal-2018-scaling] en-de big model was employed. This model utilizes an Encoder-Decoder architecture, where the Encoder processes the source sentence to gather context, and the Decoder generates the target sentence sequentially, one word at a time. The model leverages the Transformer's powerful self-attention mechanism to optimize translation accuracy while maintaining semantic consistency.\nTo further assess the capabilities of the large language model, this approach adapted mBART for Lao, as the original version of mBART does not support this language. The adaptation involved training mBART on monolingual Vietnamese and Lao datasets to extend its language support, followed by fine-tuning on the bilingual dataset provided by VLSP. This adaptation ensures the model's effectiveness in machine translation between Vietnamese and Lao.\nThis method also employed SentencePiece [kudo-richardson-2018-sentencepiece] for tokenizing Vietnamese and Lao text, setting a vocabulary size of 20,000 tokens. The training dataset consisted of 100,000 sentence pairs, with a test set of 2,000 pairs from VLSP used for evaluation. Additionally, the model was pretrained on a large monolingual dataset 1.8GB of Vietnamese text and 1GB of Lao text-laying a strong foundation for fine-tuning. However, the fine-tuned mBART model performed below the Transformer WMT en-de big model in terms of overall translation accuracy."}, {"title": "Vietnamese-Lao Bidirectional Translation System", "content": "The MTA_AI team utilized the M2M-100 418M [fan2020englishcentric] and mt5-small [xue-etal-2021-mt5] models to fine-tune a translation system for Vietnamese and Lao, both of which are low-resource languages with limited pre-trained model support. After an extensive survey of available multilingual models, they determined that m2m-100 [fan2020englishcentric] and mT5 [Raffel:2020, xue2021mt5] were particularly well-suited for this project. These models are capable of translating multiple language pairs, including Vietnamese and Lao, making them ideal choices for enhancing translation quality between these languages.\nThe M2M100-418M model is a multilingual encoder-decoder designed for many-to-many translation, supporting direct translation between numerous languages without needing a pivot language. The mT5-small model, a compact version of T5 with a multilingual capability, was pre-trained on the Common Crawl dataset, covering 101 languages and comprising 300 million parameters. The model is fine-tuned using the Adam optimizer [kingma2017adam]. This combination allows both comprehensive language support and computational efficiency.\nIn this approach, MTA_AI team applied back-translation using Google Translate to convert monolingual sentences into bilingual data, thereby creating a synthetic dataset. This method generated 1.5 million sentence pairs for both Vietnamese-to-Lao (Vi-Lo) and Lao-to-Vietnamese (Lo-Vi) translations, substantially expanding our training data.\nTo investigate the impact of large-scale data on model performance, they trained the m2m100-418M model with a total of 3 million back-translated monolingual sentences. The results demonstrated a significant enhancement in translation accuracy, affirming the positive influence of large-scale data on the effectiveness of machine translation systems for low-resource languages."}, {"title": "A Sequence-to-Sequence Model for Lao-Vietnamese Machine Translation", "content": "The BGSV_AI team employed a sequence-to-sequence approach, utilizing large language models to tackle the machine translation challenge in the shared-task competition. During the evaluation phase, they observed that existing models did not support both Vietnamese and Lao simultaneously. Consequently, they developed a unique tokenizer using the SentencePiece technique [kudo-richardson-2018-sentencepiece] to generate a tailored vocabulary set suited to both languages. They then customized and trained the T5 model [Raffel:2020] from scratch, specifically for this machine translation task.\nPre-processing proved essential in enhancing both translation quality and efficiency. This stage involved cleaning the dataset to remove noise, standardizing formats (such as dates and numbers) for uniformity, and tokenizer the text into smaller units. Given their limited familiarity with the Lao language, they applied only fundamental pre-processing techniques, which included the removal of irrelevant characters and symbols.\nIn the experiment, this approach focused on optimizing the tokenizer to improve sentence comprehension while managing the vocabulary size effectively. To achieve this, BGSV_AI team sets a token length of 90 for Lao and 150 for Vietnamese, aiming for an optimal balance between computational efficiency and language understanding. These customized token lengths were carefully tailored to the linguistic characteristics of each language, thereby maximizing the performance of their machine translation system."}, {"title": "Human Evaluation", "content": "To accurately and comprehensively evaluate the quality of the translation model, we decided to leverage the expertise and experience of specialists in the field of linguistics. The evaluation process began by obtaining sentence translations from various models and then submitting these translations to experts for review and scoring. This process went beyond merely comparing results with standard translations; it required experts to analyze and assess based on multiple factors such as semantic accuracy, syntax, context, and the naturalness of the translated language. With their deep understanding of language and grammar, the experts provided feedback and evaluations that closely reflect everyday language use. Employing human evaluation in this manner offers us a proactive and insightful perspective on the model's actual performance, ensuring that the final results are not only technically accurate but also appropriate and easily understandable for users.\nVLSP 2022- MT Chinese-Vietnamese: The human evaluation process was conducted on translations generated by the models. These tasks included translating from Vietnamese to Chinese (Vi-Zh) and from Chinese to Vietnamese (Zh-Vi). During this evaluation, translations produced by the models were reviewed and assessed by linguistic experts with extensive skills and knowledge in both languages involved. The objective was to determine the quality, accuracy, and naturalness of the translations to evaluate the performance of the machine translation models.\nVLSP 2023-MT Lao-Vietnamese: Human evaluation was carried out on primary runs submitted by participants to two of the MT tasks, namely the MT Vietnamese-Lao (Vi-Lo) task and MT Lao -Vietnamese (Lo-Vi) task.\nFrom the point of view of the evaluation campaign, our goal is to adopt a human evaluation framework able to maximize the benefit for the research community, both in terms of information about MT systems and data and resources to be reused. With respect to other types of human assessment, such as judgments of translation quality (i.e. adequacy/fluency and ranking tasks), the post-editing task has the double advantage of producing (i) a set of edits pointing to specific translation errors, and (ii) a set of additional reference translations. Both these byproducts are very useful for MT system development and evaluation. The human evaluation dataset and the collected post-edits are described in next Section whereas the results of the evaluation are presented in result table.\nEvaluation Dataset\nThe human evaluation datasets each consist of approximately 1,000 sentences, drawn from subsets of the private test sets for each translation task. Specifically, we selected 1,000 sentences for the Zh-Vi and Vi-Zh datasets, and another 1,000 sentences for the Lo-Vi and Vi-Lo datasets. This approach, which involves selecting a consecutive block of sentences for each dataset, was guided by the need to realistically simulate a caption post-editing task.\nWe received five submissions for each of the Zh-Vi, Vi-Zh, Vi-Lo, and Lo-Vi tasks. For each task, the output from the five systems was given to five professional translators for post-editing on the human evaluation set. To cope with translators' variability, an equal number of outputs from each MT system was assigned randomly to each translator. The resulting evaluation data for each task consist of the new reference translations for each of the sentences in the human evaluation set. Each one of these references represents the targeted translation of the system output from which it was derived, and remain additional translations are available as well for the evaluation of each MT system."}, {"title": "Conclusions", "content": "In this paper, we presented the organization and outcomes of the VLSP MT Evaluation Campaign. The VLSP MT evaluation provides a venue where core technologies for spoken language translation can be evaluated on many different languages and compared not only across research teams but also overtime.\n\u2022 In VLSP 2022, the evaluation was attended by 5 groups: Samsung SDS R&D Center, Vin BigData, Japan Advanced Institute of Science and Technology, Hanoi University of Science and Technology, and VCCorp.\n\u2022 In VLSP 2023, the evaluation was attended by 7 groups: UET-VNU, MTA, Viettel, Bosch Global Software Technologies Vietnam, HUST, Fulbright University Vietnam, US-VNUHCM. To honor"}]}