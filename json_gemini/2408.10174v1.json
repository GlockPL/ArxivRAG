{"title": "SMILE: ZERO-SHOT SPARSE MIXTURE OF LOW-RANK EXPERTS CONSTRUCTION FROM PRE-TRAINED FOUNDATION MODELS", "authors": ["Anke Tang", "Li Shen", "Yong Luo", "Shuai Xie", "Han Hu", "Lefei Zhang", "Bo Du", "Dacheng Tao"], "abstract": "Deep model training on extensive datasets is increasingly becoming cost-prohibitive, prompting the widespread adoption of deep model fusion techniques to leverage knowledge from pre-existing models. From simple weight averaging to more sophisticated methods like AdaMerging, model fusion effectively improves model performance and accelerates the development of new models. However, potential interference between parameters of individual models and the lack of interpretability in the fusion progress remain significant challenges. Existing methods often try to resolve the parameter interference issue by evaluating attributes of parameters, such as their magnitude or sign, or by parameter pruning. In this study, we begin by examining the fine-tuning of linear layers through the lens of subspace analysis and explicitly define parameter interference as an optimization problem to shed light on this subject. Subsequently, we introduce an innovative approach to model fusion called zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which allows for the upscaling of source models into an MoE model without extra data or further training. Our approach relies on the observation that fine-tuning mostly keeps the important parts from the pre-training, but it uses less significant or unused areas to adapt to new tasks. Also, the issue of parameter interference, which is intrinsically intractable in the original parameter space, can be managed by expanding the dimensions. We conduct extensive experiments across diverse scenarios, such as image classification and text generalization tasks, using full fine-tuning and LoRA fine-tuning, and we apply our method to large language models (CLIP models, Flan-T5 models, and Mistral-7B models), highlighting the adaptability and scalability of SMILE. For full fine-tuned models, about 50% additional parameters can achieve around 98-99% of the performance of eight individual fine-tuned ViT models, while for LORA fine-tuned Flan-T5 models, maintaining 99% performance with only 2% extra parameters.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of deep learning has witnessed an exponential growth in model sizes and dataset scales, making the training of large-scale deep models on extensive datasets increasingly cost-prohibitive, both in terms of financial resources and environmental impact [Minaee et al., 2024, Hadi et al., 2023]. Deep model fusion techniques have emerged as a promising solution, allowing the integration of knowledge from pre-existing models without the need for extensive retraining [Li et al., 2023, Zheng et al., 2023, Yang et al., 2024a]. This approach not only reduces computational costs but also enables the creation of more robust and versatile models by combining the strengths of multiple models.\nFollowing the categorization in Tang et al. [2024a], we classify model fusion methods into three main categories: model ensemble methods, model merging methods, and model mixing methods. Model ensemble techniques aggregate the predictions from several models to enhance performance [Sagi and Rokach, 2018]. While resource-intensive in terms of memory and computation, it improves knowledge distillation training [Wan et al., 2024a,b]. Model merging methods, on the other hand, combine the parameters of multiple models into a single model, often through weighted averaging"}, {"title": "2 Rethinking Model Fine-Tuning From a Subspace Perspective", "content": "In this study, we aim to construct a unified versatile model from multiple fine-tuned models, which can perform multiple tasks or handle inputs from multiple domains simultaneously. We denote the number of fine-tuned models as T. Before we delve into the proposed method's details, we gain insights into the fine-tuning process from a singular value decomposition (SVD) subspace perspective. In this section, we aim to (1) investigate and locate the task information in the fine-tuned weights Wft, and (2) understand how it is related to the pre-trained weights W.\nConsider a linear layer of the pre-trained model with weight matrix $W \\in \\mathbb{R}^{m \\times n}$ and bias vector $b \\in \\mathbb{R}^m$. After full fine-tuning on a downstream task, the weight matrix and bias vector are updated to Wft and bft, respectively. To achieve a deeper understanding of these updates, we need to employ mathematical tools that allow us to decompose the parameter space into distinct ranges of importance, i.e. subspaces. We state the following theorem.\nTheorem 1 Given two sets of orthonormal vectors ${\\{u_i\\}}_{i=1}^p \\subset \\mathbb{R}^m$ and ${\\{v_i\\}}_{i=1}^q \\subset \\mathbb{R}^n$, $1 < p < m$ and $1 \\leq q \\leq n$, the set of matrices ${\\{u_i v_j^T\\}}_{i=1,j=1}^{p,q}$ forms an orthonormal basis for a subspace of $\\mathbb{R}^{m \\times n}$ with dimension pq.\nProof 1 For simplicity, let $x_{ij} = u_i v_j^T$. The Frobenius inner product of two matrices $x_{ab}$ and $x_{cd}$ is defined as\n$\\langle x_{ab}, x_{cd} \\rangle = tr \\left( x_{ab} x_{cd}^T \\right) = tr \\left( u_a v_b^T v_d u_c^T \\right) \\in \\mathbb{R}.$\n(1)\nOrthonormality: we consider three cases:\n1. If a = c and b = d, then $\\langle x_{ab}, x_{cd} \\rangle = tr \\left( u_a u_a^T \\right) = u_a^T u_a = 1$.\n2. If $b \\neq d$, then $v_b^T v_d = 0$ and $\\langle x_{ab}, x_{cd} \\rangle = 0$.\n3. If $b = d$ and $a \\neq c$, then $v_b^T v_d = 1$ and $\\langle x_{ab}, x_{cd} \\rangle = tr \\left( u_a u_c^T \\right) = u_a^T u_c = 0$.\nThus, ${\\{x_{ij}\\}}_{i,j=1}^{p,q}$ is orthonormal.\nLinear Independence: assume there exists a nonzero matrix $a \\in [RP \\times q]$ such that $\\sum_{i=1}^{p}\\sum_{j=1}^{q} A_{ij} x_{ij} = 0$. For any $a \\in [p]$ and $b \\in [q]$, take the inner product of both sides with $x_{ab}$. We obtain the following:\n$\\sum_{i=1}^{p}\\sum_{j=1}^{q} \\langle A_{ij} x_{ij}, x_{ab} \\rangle = \\langle 0, x_{ab} \\rangle = 0.$\n(2)\nBy the linearity of the inner product and the orthogonality, we proved:\n$\\sum_{i=1}^{p}\\sum_{j=1}^{q} A_{ij} \\langle x_{ij}, x_{ab} \\rangle = A_{ab} \\langle x_{ab}, x_{ab} \\rangle = A_{ab} = 0.$\n(3)\nSince this holds for any a and b, we conclude that all $A_{ij} = 0$. This leads to a contradiction to the assumption that a is nonzero. Therefore, the set ${\\{x_{ij}\\}}_{i,j=1}$ is linearly independent, which is the necessary and sufficient conditions for a set of elements to form a basis for a vector space with dimension pq.\nWe start by decomposing the weight matrix W using the reduced SVD as $W = U_r \\Sigma V_r^T$, where $U_r \\in \\mathbb{R}^{m \\times r}$ and $V_r \\in \\mathbb{R}^{r \\times n}$ are orthonormal matrices containing the left singular vectors and right singular vectors, respectively, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix containing the singular values sorted in descending order, and r is the rank of the matrix W [Olver and Shakiban, 2018]. In the case of full SVD, the matrices are $U \\in \\mathbb{R}^{m \\times m}$, $\\Sigma \\in \\mathbb{R}^{m \\times n}$, and $V \\in \\mathbb{R}^{n \\times n}$, which preserve all information about the matrix W, including its kernel (null space) and cokernel (left null space), as shown in Figure 2a.\nRemark 1 According to Theorem 1, the set of matrices ${\\{u_i v_j^T | i \\in [m], j \\in [n]\\}}$ forms an orthonormal basis for a subspace of $\\mathbb{R}^{m \\times n}$ with dimension mn. In other words, for any real matrix $A \\in \\mathbb{R}^{m \\times n}$, we can express it as a weighted sum of the elements in the basis, i.e. $A = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\langle A, u_i v_j^T \\rangle u_i v_j^T \\in span({\\{u_i v_j^T\\}}_{i,j=1}^{m,n})$.\nRemark 2 Let U and V be two subsets of ${\\{u_i\\}}_{i=1}^{m}$ and ${\\{v_i\\}}_{i=1}^{n}$, respectively. ${\\{u v^T | u \\in U, v \\in V\\}}$ forms a orthonormal basis for a subspace of $\\mathbb{R}^{m \\times n}$, with dimension $|U||V| \\leq mn$.\nTo gain insights into how fine-tuning modifies the pre-trained weights to adapt them to a specific task, we assume the fine-tuned linear layer accepts an input $x \\in \\mathbb{R}^n$ and outputs $y = W_{ft}x + b_{ft}$. Because the row space ${\\{v_i\\}}_{i=1}^{n}$ is an"}, {"title": "3 Parameter Interference Between Task-Specific Models", "content": "From the previous section, we build an understanding of how fine-tuning modifies the pre-trained weights to adapt to a specific downstream task. In this section, we investigate the parameter interference between models fine-tuned on different tasks, which has been widely explored in multi-task learning and multi-task model merging, primarily within the model parameter space. We add superscripts to denote the task index, e.g. $W_{ft}^{(i)}$ and $b_{ft}^{(i)}$ for the i-th task.\nAssume we have T tasks, and each task has a fine-tuned model. In the simplest cases, we consider the linear layers, accepting a common input x and outputting T different outputs $y^{(1)}, y^{(2)}, ..., y^{(T)}$. According to Eq.(4) and Eq.(8), each $y^{(i)}$ can be decomposed into the pre-trained part and fine-tuned part as follows:\n$y^{(i)} = \\sum_{j=1}^{r} \\sigma_j (x, v_j) U_j + b+ \\sum_{j=1}^{m}\\sum_{k=1}^{n} \\delta_{jk}^{(i)} (x, V_k)U_j + \\Delta b^{(i)} .$\n(9)\nWhere the pre-trained term is shared and remains constant during fine-tuning across all tasks. In the context of model merging, these models are merged to construct a unified multi-task model that can perform all tasks simultaneously. A common approach is to use a weighted average of the fine-tuned weights, i.e. $W_{merged} = \\sum_{l=1}^{T} \\lambda_l W_{ft}^{(l)}$ and $b_{merged} = \\sum_{l=1}^{T} \\lambda_l b_{ft}^{(l)}$. This is equivalent to merging the fine-tuned parts of the models, while the pre-trained parts are shared across all tasks. Therefore, we express the output of the merged model as:\n$Y_{merged} = pre-trained part + \\sum_{j=1}^{m}\\sum_{k=1}^{n} (\\sum_{l=1}^{T} \\lambda_l \\delta_{jk}^{(l)} ) (x, V_k)U_j + \\sum_{l=1}^{T} \\lambda_l \\Delta b^{(l)} .$\n(10)\nSubstitute the input x with $x^{(i)}$ from the i-th task (domain), we aim to minimize the discrepancy between the output of the merged model and the output of the i-th fine-tuned model. We formulate the optimization problem as follows:\n$\\min_{\\lambda_l} \\left \\| Y_{merged} - y^{(i)} \\right \\|_2^2 = \\min_{\\lambda_l} \\left \\| \\sum_{j=1}^{m}\\sum_{k=1}^{n} (\\sum_{l=1}^{T} \\lambda_l \\delta_{jk}^{(l)} ) (x, V_k)U_j - \\sum_{j=1}^{m}\\sum_{k=1}^{n} \\delta_{jk}^{(i)} (x, V_k)U_j + \\sum_{l=1}^{T} \\lambda_l \\Delta b^{(l)} - \\Delta b^{(i)} \\right \\|_2^2.$\n(11)\nUsing triangle inequality, we can decompose the error into two parts and assert an upper bound:\n$\\left \\| Y_{merged} - y^{(i)} \\right \\|_2^2 < \\left \\| \\sum_{j=1}^{m}\\sum_{k=1}^{n} (\\sum_{l=1}^{T} \\lambda_l \\delta_{jk}^{(l)} ) (x, V_k)U_j - \\sum_{j=1}^{m}\\sum_{k=1}^{n} \\delta_{jk}^{(i)} (x, V_k)U_j \\right \\|_2^2 + \\left \\| \\sum_{l=1}^{T} \\lambda_l \\Delta b^{(l)} - \\Delta b^{(i)} \\right \\|_2^2 .$\n(12)\nFor the bias term, we have a closed-form solution for $\\lambda_l = (\\Delta B^T \\Delta B)^{-1} \\Delta B \\Delta b^{(i)}$, where $\\Delta B$ is a matrix with l-th columns as $\\Delta b^{(l)}$. Notice that this solution varies with the task (domain) index of the coming input $x^{(i)}$, so a more straightforward solution is to fix the bias term during the fine-tuning process, so that $\\Delta b^{(i)} = 0$ for all i. As for the weight term, parameter interference does not occur on subspace that is orthogonal to the input, i.e. $span(\\{u_i v_j^T | i \\in [m], j \\in [n] and \\langle x^{(i)}, v_j \\rangle = 0\\})$, thus a possible strategy is to enlarge the input size to increase the dimension of the orthogonal subspace. This explains why model merging methods perform better on larger models with more dimension redundancy. When the input activates certain dimensions, i.e. for k such that $\\langle x^{(i)}, v_k \\rangle \\neq 0$, the interference is inevitable unless the domain gap between different tasks is large enough to make the activation dimensions disjoint. Note that we can gain the same conclusion within the original model parameter space, by simply replacing the basis vectors ${\\{u_i\\}}_i$ and ${\\{v_i\\}}_i$ in this section with the standard Euclidean basis vectors ${\\{e_i\\}}_i$."}, {"title": "4 Resolving Parameter Interference using Sparse Mixture of Low-Rank Experts", "content": "Understanding that addressing parameter interference by model merging is difficult, even just for the bias terms, the optimal method for weight combination has a closed-form solution that varies by task. To manage this challenge, we introduce an innovative approach with a Sparse MIxture of Low-rank Experts (SMILE) model in this section, which operates in a zero-shot fashion, meaning no data or training is required. An overview is shown in Figure 3.\nWe upscale the linear layers from source models to the SMILE model, which consists of a shared pre-trained part, a router, and several low-rank experts. Figure 3 is organized into two primary sections: the overall model architecture (left) and the routing mechanism (right).\nRecall the output decomposition in Eq.(4) and Eq.(10), we can express the output of a merged model as the output of the pre-trained model plus a weighted sum of the fine-tuned parts of the individual models. If we can identify the most relevant experts for a given input, we can dynamically select the corresponding fine-tuned parts to combine with the pre-trained part. Then the merging error in Eq.(11) can be minimized. Mathematically, we can express this idea as:\n$U_{merged} = pre-trained part + \\sum_{j=1}^{m}\\sum_{k=1}^{n} (\\sum_{l} \\lambda_l (x^{(l)}) \\delta_{jk}^{(l)} ) (x^{(l)}, V_k)U_j + \\sum_{l} \\lambda_l (x^{(l)}) \\Delta b^{(l)} .$\n(13)\nHere, $\\lambda$ is a function that maps the input to a one-hot probability distribution over the tasks, i.e. $\\Lambda_j (x^{(i)}) = 1$ if $j = i$, and $\\Lambda_j (x^{(i)}) = 0$ otherwise. However, a naive implementation of this idea would require a training process to learn the parameters of the router and a large number of additional parameters to store the fine-tuned weights of all tasks. A more efficient approach is to remove less significant terms from the fine-tuned components in Eq.(13), focusing on retaining the most pertinent knowledge for each task. Therefore, the parameter space must be ranked by the importance of its dimensions. However, from previous findings in Section 2, we know that the fine-tuned information is distributed across less significant dimensions (Space II & III), which is a large portion of the whole space. We opt to use SVD to decompose the parameter differences $\\Delta W^{(i)}$ for each task, and then apply a low-rank approximation to extract the most important part as follows:\n$\\Delta W^{(i)} = U^{(i)} \\Sigma^{(i)} {V^{(i)}}^T = \\sum_j^{r^{(i)}} \\sigma_j u_j v_j^T \\approx \\sum_j^{k} \\sigma_j u_j v_j^T = U^{(i)}_k {\\Sigma^{(i)}}_k {V^{(i)}_k}^T , 1 \\leq k \\leq r^{(i)} .$\n(14)\nWhere $r^{(i)}$ is the rank of the fine-tuned weight matrix $\\Delta W^{(i)}$, and k is the rank of the low-rank approximation, which is determined as a hyperparameter. $U^{(i)}_k$ and $V^{(i)}_k$ contains the first k columns of $U^{(i)}$ and $V^{(i)}$, respectively. Here we drop the terms with indices j > k in the summation, which correspond to the less significant dimensions. Let $A^{(i)} = {\\Sigma^{(i)}_k} {V^{(i)}_k}^T$ and $B^{(i)} = U^{(i)}_k$, we can express the approximation similar to a LoRA adapter: $\\Delta W x = B^{(i)} A^{(i)} x$. The following theorem states the optimality of this low-rank approximation."}, {"title": "5 Experiments", "content": "In this section, we evaluate the effectiveness of the proposed SMILE on a variety of setups, including image classification and text generalization tasks, as well as full fine-tuning and LoRA fine-tuning. Detailed information about the fine-tuned models is in Appendix C. We compare our method with several SOTA model fusion techniques, including Simple Averaging [Wolf et al., 2019b], Fisher merging [Matena and Raffel, 2022], RegMean [Jin et al., 2022], Task Arithmetic [Ilharco et al., 2022], Ties-Merging [Yadav et al., 2023], AdaMerging [Yang et al., 2024c], and WEMOE [Tang et al., 2024c]. To further demonstrate"}, {"title": "5.1 Multi-Task Model Fusion on Open-Vocabulary Image Classification Tasks", "content": "We first evaluate our proposed SMILE method on eight diverse open-vocabulary image classification tasks using CLIP models from HuggingFace 12. For each task, the text encoder of the pre-trained model is frozen, and only the vision encoder is fine-tuned. Table 1 presents the requirements of different model fusion methods, highlighting that SMILE is a training-free model fusion method that does not require additional labeled samples or test-time adaptation."}, {"title": "5.2 Multi-Task Model Fusion on Text Generalization Tasks", "content": "We further evaluate SMILE on text generalization tasks using Flan-T5-base models 3, which are fine-tuned on eight tasks from the GLUE benchmark [Wang et al., 2018]. We use two different fine-tuning strategies: full fine-tuning and LORA fine-tuning with $Y_{LORA} = 16$. We present the results in Tables 4 and 5, for full fine-tuned models and LoRA fine-tuned models, respectively. For fully fine-tuned models, SMILE consistently outperforms other fusion methods across all eight tasks. With just 1.52 times the parameters of a single model, SMILE (2: kgate = 8, k = 32) achieves 99.0% of the individual model performance with 1.52 times the parameters of a single model. In the LoRA fine-tuned scenario, SMILE maintains strong performance with minimal parameter increase (1.02 times). It achieves 99.3% of the individual model performance, significantly surpassing other multi-task model fusion methods."}, {"title": "5.3 Spare Mixture of Low-Rank Experts Analysis", "content": "To better understand SMILE, we further conduct ablation studies using CLIP and Flan-T5 models."}, {"title": "5.4 Scalability to Large-Scale Models (Mistral-7B models)", "content": "To demonstrate the scalability of SMILE to large-scale models, we conduct experiments on Mistral-7B models. We use Mistral-7B-v0.1 as our base pre-trained model, referred to as Mo, and acquire three specialized models from HuggingFace [Wolf et al., 2019a]. The expert models are respectively labeled as M1, M2, and M3 4. Of these models, M\u2081 stands out as the sole expert in mathematics. To demonstrate the efficacy of the zero-shot routing mechanisms, we construct four distinct series of SMILE models with various expert combinations. These series are designated as M0;1, M0;2, M0;3, and M0;123, with M0;11...in indicating the SMILE model that combines Mo with the expert models Mi\u2081,..., Min"}, {"title": "6 Related Work", "content": "Mixture of Experts. The concept of Mixture of Experts (MoE) is first introduced by Jacobs et al. [1991], involving training multiple specialized models. This concept has gained significant attention in recent years [Jiang et al., 2024, Dai et al., 2024], with much of the innovation focusing on routing mechanisms and expert design. Much innovation revolves around the design of more efficient routers. For example, the Switch Transformer [Fedus et al., 2022b] selects only the top expert for each token, simplifying the process and improving scalability. Similarly, [Lewis et al., 2021] use a linear assignment to optimize token-expert affinities, ensuring an equal spread of tokens among experts. For detailed reviews on MoE, see [Fedus et al., 2022a] and for MoE in the context of model merging, refer to [Yadav et al., 2024].\nDeep Model Fusion. Mode connectivity reveals that different model solutions can be linked by low-loss path in the parameter space [Freeman and Bruna, 2016, Nagarajan and Kolter, 2019, Draxler et al., 2018, Frankle et al., 2020, Entezari et al., 2021, Garipov et al., 2018, Tatro et al., 2020, Yunis et al., 2022, Benton et al., 2021], facilitating model fusion by weight interpolation [Izmailov et al., 2018, Matena and Raffel, 2022, Wolf et al., 2019b, Kaddour, 2022, Ilharco et al., 2022, Yadav et al., 2023, Yang et al., 2024c, Wu et al., 2023]. However, this strategy also poses challenges, particularly when merging models with diverse structures. Alignment helps reduce model disparities by matching and interpolating components [Li et al., 2015, Tatro et al., 2020]. Methods involve matching activations or weights [Stoica et al., 2023, Jin et al., 2022, Yang et al., 2024b], using channel-wise graph matching [Liu et al., 2022], or applying permutation invariance [Ainsworth et al., 2022]. Another line of research is model mixing, which combines models through gating mechanisms or depth concatenation [Tang et al., 2024c, Lu et al., 2024, Tang et al., 2024b, Kim et al., 2023], allowing for more flexible and adaptive fusion strategies."}, {"title": "7 Conclusion, Limitations, and Future Work", "content": "In this paper, we introduced the Sparse Mixture of Low-Rank Experts (SMILE) model as a novel approach to model fusion, Our method leverages a zero-shot mechanism, eliminating the need for additional training data or processes, which makes it highly practical. While the MoE method is designed to be efficient through sparse activation, it still adds extra computational overhead, especially as the number of tasks or experts increases.\nUnderstanding which subspaces contribute most to task-specific performance could lead to more targeted and efficient fine-tuning strategies, potentially focusing on updating specific parts of the model while leaving others intact. Additionally, this approach might be applied to other areas, like multi-modal large language models, where different types of data (modalities) are treated as separate experts. Furthermore, it would be worth exploring how SMILE can manage multi-objective optimization by adjusting the importance of different routing weights. What's more, develop methods to dynamically adjust the number of experts K per token based on the input, potentially improving efficiency without sacrificing performance."}, {"title": "A Projecting Fine-tuned Updates onto Subspaces", "content": "This section provides an in-depth mathematical explanation of the projection merge experiments discussed in Section 2. These experiments aim to gain empirical insights into the distribution of task-specific information across different subspaces of the weight matrix after fine-tuning a pre-trained model on downstream tasks.\nLet $W \\in \\mathbb{R}^{m \\times n}$ be the weight matrix of a linear layer in the pre-trained model, and $W_{ft} \\in \\mathbb{R}^{m \\times n}$ be the corresponding weight matrix after fine-tuning. We define the weight update as $\\Delta W = W_{ft} - W$. We begin by performing a full Singular Value Decomposition (SVD) on the pre-trained weight matrix W:\n$W = U \\Sigma V^T$\n(19)\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices containing left and right singular vectors, respectively, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ contains the singular values in descending order. The first r diagonal elements of $\\Sigma$ are non-zero, where $r = rank(W)$, while the remaining elements are zero. According to Theorem 1, we can leverage the properties of singular value decomposition (SVD) to gain a deeper understanding of the fine-tuning process. This theorem states that any matrix $A \\in \\mathbb{R}^{m \\times n}$ can be decomposed into a sum of rank-one matrices using the left singular vectors ${\\{u_i\\}}_{i=1}^m$ and right singular vectors ${\\{v_i\\}}_{i=1}^n$ as bases:\n$A = \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}u_i v_j^T = U \\Sigma_A V^T$\n(20)\nwhere $a_{ij} = \\langle A, u_i v_j^T \\rangle$ is the projection of A onto the basis $u_i v_j^T$, $\\Sigma_A$ is a real matrix and $\\Sigma_A(i,j) = a_{ij}$. This decomposition provides a powerful framework for analyzing the fine-tuning process. When we fine-tune a pre-trained model, we can interpret the weight updates $\\Delta W$ as modifications to the singular value matrix $\\Sigma$, while the singular vectors U and V remain constant. Then we partition the singular matrix $\\Sigma$ into three zones:\nZone I & Subspace I: $\\{1, . . ., r_{half} \\}$, where $r_{half}$ is chosen such that $\\sum_{i=1}^{r_{half}} \\sigma_i \\approx \\frac{1}{2} \\sum_{i=1}^{r} \\sigma_i$. The basis of this subspace is $\\{\\{u_i v_j^T\\} | 1 \\leq i, j \\leq r_{half}\\}$. The projection merged weights in this subspace can be computed as follows:\n$W_I = W + \\sum_{i=1}^{r_{half}} \\sum_{j=1}^{r_{half}} \\langle \\Delta W, u_i v_j^T \\rangle u_i v_j^T = W + U_{r_{half}} U_{r_{half}}^T \\Delta W V_{r_{half}} V_{r_{half}}^T$\n(21)\nWhere $U_{r_{half}}$ and $V_{r_{half}}$ are the first $r_{half}$ columns of U and V, respectively.\nZone II & Subspace II: $\\{r_{half} + 1, ...,r\\}$, where $r = rank(W)$. The basis of this subspace is $\\{{\\{u_i v_j^T\\}}_{i,j = r_{half} + 1}^r\\}$. The basis of subspace II & III is $\\{\\{u_i v_j^T\\} | r_{half} + 1 \\leq i \\leq r, r_{half} + 1 \\leq j \\leq r\\}$. The projection merged weights in this subspace can be computed as follows:\n$W_{II} = W + \\sum_{i=r_{half} + 1}^{r} \\sum_{j=r_{half} + 1}^{r} \\langle \\Delta W, u_i v_j^T \\rangle u_i v_j^T = W + U_{r_{half}+1:r} U_{r_{half}+1:r}^T \\Delta W V_{r_{half}+1:r} V_{r_{half}+1:r}^T$\n(22)\nWhere $U_{r_{half}+1:r}$ and $V_{r_{half}+1:r}$ are the ($r_{half} + 1$)-th to r-th columns of U and V, respectively.\nZone III & Subspace II + III: The basis of this subspace is $\\{u_i v_j^T | r + 1 \\leq i \\leq m, r + 1 \\leq j \\leq n\\}$ and the projection merged weights in this subspace can be computed as follows:\n$W_{II+III} = W + \\sum_{i=r + 1}^{m} \\sum_{j=r + 1}^{n} \\langle \\Delta W, u_i v_j^T \\rangle u_i v_j^T = W + U_{r+1:m} U_{r+1:m}^T \\Delta W V_{r+1:n} V_{r+1:n}^T$\n(23)\nWhere $U_{r+1:m}$ and $V_{r+1:n}$ are the ($r + 1$)-th to m-th columns of U and ($r + 1$)-th to n-th columns of V.\nWe then evaluate the performance of these modified weight matrices on the downstream tasks. The accuracy comparison in Figure 2b is obtained by using these modified weight matrices in place of the original pre-trained weights. For other layers instead of the linear layers, we keep the pre-trained weights unchanged."}, {"title": "B The Gradient Flow During Fine-tuning", "content": "In this section, we analyze the gradient flow during the fine-tuning process to gain insights into how linear layers in a deep neural network adapt to new tasks. We decompose a fine-tuned deep neural network f into three components:"}]}