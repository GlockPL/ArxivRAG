{"title": "PRUNING AS A DEFENSE: REDUCING MEMORIZATION\nIN LARGE LANGUAGE MODELS", "authors": ["Mansi Gupta", "Nikhar Waghela", "Sarthak Gupta", "Shourya Goel", "Sanjif Shanmugavelu"], "abstract": "Large language models have been shown to memorize significant portions of their\ntraining data, which they can reproduce when appropriately prompted. This work\ninvestigates the impact of simple pruning techniques on this behavior. Our find-\nings reveal that pruning effectively reduces the extent of memorization in LLMs,\ndemonstrating its potential as a foundational approach for mitigating membership\ninference attacks.", "sections": [{"title": "INTRODUCTION", "content": "Large language models are known to memorize portions of their training data, which poses signifi-\ncant privacy and security risks. Although various studies have explored the extent of memorization\nin LLMs, most of these efforts are qualitative (Carlini et al. (2021)). A recent approach introduces\na method to quantify memorization by examining whether models can recall and complete prompts\nfrom their training data verbatim (Carlini et al. (2023)). By feeding prefixes of these prompts into\na trained model, we can assess its ability to reconstruct the full prompt, thus enabling quantita-\ntive analysis of memorization behavior across different models, datasets, and prompt sizes. This\nframework for quantifying memorization is the basis of our study. Pruning techniques, widely used\nin machine learning to reduce computational overhead and mitigate overfitting, have been shown\nto maintain task performance despite reducing the size and complexity of models (Huang et al.\n(2024), Sun et al. (2024)). Pruning techniques simplify model architectures while preserving task\nperformance (Huang et al. (2024), Sun et al. (2024)). Beyond efficiency, previous studies suggest\nthat pruning can reduce overfitting and induce unlearning by removing memorized information\n(Pochinkov & Schoots (2024). Motivated by this, we explore pruning to mitigate memorization\nin large language models.\nWe systematically evaluate the impact of layer-wise and global pruning strategies on memorization.\nBuilding on findings that deeper layers can often be pruned more aggressively without harming\nperformance (Panigrahi et al. (2023), Gromov et al. (2024)), we analyze the effect of pruning spe-\ncific blocks, including attention layers, to identify those most responsible for memorization. Addi-\ntionally, we prune at different percentages to assess whether the extent of pruning impacts model\nperformance. Our results show that pruning reduces both computational costs and memorization,\nproviding a lightweight and effective approach to improving privacy in LLMs."}, {"title": "METHODOLOGY", "content": "Drawing reference from this paper (Carlini et al. (2023)), we define a string s as extractable with\ncontext k tokens if there exists a k token prefix p such that the concatenation [p || s] exists in the\ntraining data, and the model f reproduces s when prompted by p using greedy decoding. For ex-\nample, if a model's training data contains \"The capital of Germany is Berlin,\" and the prefix \"The"}, {"title": "RESULTS", "content": null}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "Our study was limited to 5,000 training samples due to computational constraints, leaving scope\nfor testing on larger datasets to gain deeper insights. Additionally, future work could explore other\npruning methods, such as in-training pruning (Roy et al. (2020)), to enhance performance stability\nwhile reducing memorization. Investigating alternative sparsity techniques, such as quantization and\nlow-rank approximations, could provide further improvements. Expanding the analysis to include\ndifferent pruning strategies and their effects on various components of model architectures presents\na valuable direction for understanding and mitigating memorization in large language models. Also,\nwe have primarily used perplexity as a metric to assess model performance post-pruning. However,\nfuture work could incorporate more comprehensive evaluation metrics, such as ROUGE or BLEU\nscores, to better capture the quality of the generated text."}, {"title": "CONCLUSION", "content": "Our experiments demonstrate that pruning is an effective baseline for mitigating membership infer-\nence attacks, as it reduces memorization risk across all context lengths. By introducing sparsity,\npruning prevents the exact reproducibility of training data, reduces computational overhead, and\nmaintains model performance, making it a practical solution for addressing memorization in large\nlanguage models. Pruning attention layers results in the most significant reduction in memorization,\nalthough it comes at the cost of a performance drop. This highlights the role of attention layers in\nmemorization and suggests avenues for future research into targeted mitigation techniques. Addi-\ntionally, pruning deeper layers results in a substantial reduction in memorization while maintaining"}, {"title": "APPENDIX", "content": "PERPLEXITY RESULTS\nRESULTS ACROSS DIFFERENT CONTEXT LENGTHS AND PRUNING LEVELS\nWe present the results across various context lengths of the input prefix text, reporting the percentage\nof memorized samples for each model variant, as summarized below."}]}