{"title": "ENGAGEMENT-DRIVEN CONTENT GENERATION\nWITH LARGE LANGUAGE MODELS", "authors": ["Erica Coppolillo", "Marco Minici", "Francesco Bonchi", "Federico Cinus", "Giuseppe Manco"], "abstract": "Large Language Models (LLMs) exhibit significant persuasion capabilities in one-on-one interactions, but their influence within social networks remains underexplored. This study investigates the potential social impact of LLMs in these environments, where interconnected users and complex opinion dynamics pose unique challenges. In particular, we address the following research question: can LLMs learn to generate meaningful content that maximizes user engagement on social networks?\n\nTo answer this question, we define a pipeline to guide the LLM-based content generation which employs reinforcement learning with simulated feedback. In our framework, the reward is based on an engagement model borrowed from the literature on opinion dynamics and information propagation. Moreover, we force the text generated by the LLM to be aligned with a given topic and to satisfy a minimum fluency requirement.\n\nUsing our framework, we analyze the capabilities and limitations of LLMs in tackling the given task, specifically considering the relative positions of the LLM as an agent within the social network and the distribution of opinions in the network on the given topic. Our findings show the full potential of LLMs in creating social engagement. Notable properties of our approach are that the learning procedure is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. In this regard, our approach can be easily refined for more complex engagement tasks and interventions in computational social science.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently gained a great deal of attention, especially for their capabilities in one-to-one interactions. Researchers have deeply investigated their skills and limits, comparing LLMs to humans in one-on-one interactions such as, e.g., their abilities in gaming settings (Jacob et al., 2023; Fontana et al., 2024). A recent research direction empirically investigates the susceptibility (Chen et al., 2024) and persuasive capabilities of LLMs (Breum et al., 2024; Salvi et al., 2024), showing that contextual information in specific domains can enhance LLMs' persuasiveness (Matz et al., 2024). In particular, LLMs have shown competence in generating persuasive messages in public health (Karinshak et al., 2023) and personalized advertising (Meguellati et al., 2024).\n\nThis body of work is limited to one-on-one interactions and the corresponding abilities of LLMs in isolated contexts. By contrast, the potential of LLMs within broader interconnected structures, such as their capability to influence social network users, is still largely unexplored. The ability to generate interactions that resonate across social networks can have significant implications, such as the potential to foster discussions, shape opinions, and drive behavioral changes among users. Following this observation, we tackle the following research question: can LLMs learn to generate meaningful content that maximizes user engagement on social networks?\n\nOur proposal to answer this question is a framework based on reinforcement learning, whose high-level vision is depicted in Figure 1. We start prompting to the LLM a query q describing the topic of interest. The LLM generates a text t that is then injected in the social network by an agent (a node of the network), which plays the role of the injection point and which we can think about as the LLM itself. The content t then propagates through the network creating a certain level of engagement, i.e., the number of users that interacts with t. The observed level of engagement is then returned to the LLM as the observed reward and used to fine-tune the generation abilities of the LLM, aimed at maximizing the expected reward. This process is repeated till convergence.\n\nThe framework presented in Figure 1 can be realized by means of Reinforcement Learning with Human Feedback (RLHF), an approach which has gained popularity due to its effectiveness in various learning tasks (Kaufmann et al., 2023), including fine-tuning of LLMs (Ouyang et al., 2024). However, conducting actual experiments involving content generation, propagation through a network, and waiting for responses is impractical due to the time and effort required. This is a common challenge in RLHF frameworks (Casper et al., 2024). To address this issue, we use a proxy model to simulate the network's response, employing a formal engagement mechanism, which borrows ideas from both information propagation and opinion dynamics literature (Deffuant et al., 2000). In practice, we leverage an approach based on Reinforcement Learning from Simulated Feedback (RLSF), which enables efficient and effective feedback loop between the LLM agent and the network under study.\n\nThanks to the feedback loop, the LLM learns to adapt to the current topic, the social network structure, and the distribution of opinions in the network. For instance, if the opinion of the network on the given topic is prevalently negative, the generated content should have a negative leaning so to engage as many users as possible. Besides adherence to the given topic, we require a minimum fluency standard to the generated content."}, {"title": "2 Background and Related Work", "content": "In the previous section, we have already covered the literature on the persuasion capabilities of LLMs in one-to-one interactions. We next review the rest of the related literature.\n\nLLM-Agents in complex environments. The integration of Agent-Based Models (ABMs) and Large Language Models has been an emerging area of research. Notable studies, such as those by Vezhnevets et al. (2023) and Gurcan (2024), focus on utilizing LLMs as social agents within ABMs to simulate micro-level behaviors and observe macro-level patterns. These micro-level behaviors can correspond to opinion dynamics models (Chuang et al., 2023), and the resulting macro-level patterns can lead to known universal laws such as scale-free networks (De Marzo et al., 2023). Recent studies have started to place LLM agents into more complex environments to assess their ability to mimic human behavior (Park et al., 2023), and to understand their capabilities in negotiation (Bianchi et al., 2024), code development (Qian et al., 2023), and even in tasks like building houses (Chen et al., 2023). A concurrent line of inquiry is whether LLMs can learn to align their behavior with social norms (Fr\u00e4nken et al., 2023).\n\nNevertheless, the use of LLMs in large-scale social media environments, where word-of-mouth dynamics and complex interactions are crucial, has not been adequately explored, overlooking cascading effects that extend beyond one-to-one interactions.\n\nSocial media environments. The adoption of information diffusion models in data mining methodologies has been thoroughly investigated for designing viral marketing strategies (Kempe et al., 2003; Leskovec et al., 2007; Chen et al., 2010; Lu et al., 2013; Barbieri et al., 2013; Barbieri and Bonchi, 2014; Tu and Neumann, 2022). Various optimization problems have been tackled such as selecting influential seeds (Kempe et al., 2003; Tu and Neumann, 2022), designing product features (Barbieri and Bonchi, 2014), selecting seeds for fair competition (Lu et al., 2013), or seed selection with minimum regret (Aslay et al., 2015). All these works highlight the importance of considering the underlying network structure in the spreading process \\u2014 a concept that out-of-the-box LLMs are not equipped to consider. Most of these optimization problems often do not account for the fact that social media operates with real texts. Therefore, the fluency and realism of the content need to be modeled and taken into consideration, generating text that inherently possesses viral characteristics (as described in Aral and Walker (2011)).\n\nProxy for Human Feedback. Regardless of the type of objectives that machine learning models have, a correct reward function is necessary. In particular, the challenges associated with human feedback in evaluating LLMs have led to alternative approaches, such as LLM-as-a-Judge (Huang et al., 2024; Verga et al., 2024) and human-in-the-loop methods (Boubdir et al., 2023; Amirizaniani et al., 2024), which aim to enhance scalability and efficiency. While our work shares the goal of reducing the reliance on human experts for data annotation, it introduces a fundamentally novel approach by simulating context-dependent feedback that incorporates the social network structure. Hence, this innovation also minimizes the need for slow, costly, and potentially unethical online experiments, offering a more efficient and ethically sound alternative. To the best of our knowledge, no existing work leverages simulated social media engagement models as feedback mechanisms to fine-tune Large Language Models, addressing a critical gap in current research."}, {"title": "3 Engagement Model", "content": "We are given a social network represented as a directed graph G = (V, E) where nodes are users and a directed edge (u,v) \\(\\varepsilon\\) E \\u2286 V \\u00d7 V indicates that v is a \u201cfollower\" of u: thus u can propagate content to v, but not necessarily vice versa. We are also given a vector x \\(\\in\\) [0,1]|V|, associating each node u \\(\\in\\) V with a scalar xu between 0 and 1,"}, {"title": "4 Proposed Method", "content": "In this section, we provide a detailed description of the individual components of our framework: Large Language Models and the Reinforcement Learning technique. Within this context, we outline the procedure aimed at generating a content t that maximizes engagement within a social network.\n\nPreliminaries. A Large Language Model (LLM) can be formally described as a function LLM\u0189(x) = y that stochastically maps input sequences of tokens x = [X1,X2, ..., Xn] to an output sequence y = [Y1, Y2,\u00b7\u00b7\u00b7, Ym], where n, m represent the length of the input and the output sequence, respectively. The model defines Pe(y|x), the probability distribution of y given x, capturing complex patterns and relationships in natural language between the two sequences. The response y is then sampled from the distribution Po(x).\n\nFor our purposes, we fine-tune the underlying LLM agent using a Reinforcement Learning (RL) mechanism (widely adopted for optimizing LLMS (Ouyang et al., 2024)). Generally, RL is suitable for scenarios where an agent needs to learn to interact within a dynamic environment. The agent's goal is to learn a policy-a strategy or behavioral pattern-through feedback it receives from the environment, in the form of rewards or punishments.\n\nIn our framework, given a prompt x, we generate a response y ~ Po(\u00b7|x) and compute a reward R(y) that scores the response on the specific application domain. This reward can hence be used within a Policy-gradient strategy aimed at optimizing\n\n\\(L(\\theta) = E_{y \\sim P_{\\theta}(\\cdot|x)} [R(y) - \\beta log \\frac{P_{\\theta}(y|x)}{P_{\\theta'}(y|x)} ]\\)\n\nHere, \u03b8' represents the reference (not fine-tuned) model. The second term incorporates the KL-divergence between the outputs of the fine-tuned model and the reference model, serving as an additional signal to ensure that the generated responses do not deviate significantly from the reference model. Additional studies have refined the above optimization problem to avoid learning instability. In particular, the Proximal Policy Optimization (PPO) (Schulman et al., 2017)"}, {"title": "Fine-tuning Framework", "content": "Fine-tuning Framework. To accomplish our objective of generating content that maximizes engagement in a given network, we devise a framework that takes as input a social network G, a topic of interest to embed in the prompt q, and a propagation protocol M(\u00b7, \u00b7), which assumes a source node and a value associated with the content to propagate. Additionally, we model the LLM as a node in the network, with a given position (UL) and associated opinion.\n\nThe fine-tuning procedure of the LLM agent consists of the following steps:\n\nGeneration. Starting from q, we generate the content t = LLM (q).\n\nSentiment Inference. We use a pre-trained utility model S to compute a sentiment value st associated with the content t, i.e., S(t) = st. This utility model is a plug-and-play component that can be adapted to different tasks. For example, S can be a classifier fine-tuned for sentiment analysis on text.\n\nReadability Constraint. To ensure that t conveys proper semantics, we compute a readability value associated with its content. While perplexity is a common measure for evaluating the generation quality of LLMs (Gonen et al., 2023), it is sensitive to vocabulary size and sentence length. Studies (Miaschi et al., 2020) highlight its low correlation with readability. Martinc et al. (2021), study the adequacy of several readability scores and assess their performance on several datasets. For our purposes, we adopt the Flesch\u2013Kincaid (FK) (Kincaid et al., 1975) formula, defined as:\n\n\\(FK(t) = 0.39 \\left(\\frac{\\text{#words}(t)}{\\text{#sentences}(t)}\\right) + 11.8 \\left(\\frac{\\text{#syllables}(t)}{\\text{#words}(t)}\\right) - 15.59\\)\n\nThis score quantifies the grade level required to understand an English statement: the higher the value, the higher its complexity and syntactic quality. It is a crucial component of the final reward R, as it ensures the LLM generates meaningful and fluent statements.\n\nPropagation. We inject t into the social network G with the LLM as the content writer, and propagate it according to Me. After the simulation, we retrieve the final output A, i.e., the set of active users. Its size |A| is the main component of the reward R, as it quantifies the engagement produced by t.\n\nReward Computation. We finally compute R as the geometric mean of readability score and the content's virality in the network, i.e., R = (ft \u00b7 |A|)1/2.\n\nPolicy Update. We use R as the reward in the current step of the RL training procedure and repeat the process.\n\nThe fine-tuning procedure stops if either of the following conditions occurs: (i) a (fixed) number of iterations k is reached, or (ii) the KL-divergence exceeds a given threshold \u0442."}, {"title": "5 Experimental Evaluation", "content": "We evaluate the capabilities of the proposed framework in generating content that can effectively engage users through a series of experiments aimed at answering the following research questions:\n\nRQ1: Can the LLM agent learn to generate content that maximizes engagement? How do the structure, position, and environment affect the performance?\n\nRQ2: Are the generated contents realistic and meaningful compared to real content propagated on a social platform?\n\nTo answer these questions, we perform an extensive analysis on both synthetic and real data. In particular, we address RQ1 by inspecting the behavior of the LLM on synthetically generated networks with a grid of configurations, and RQ2 using real data obtained from Twitter/X, as detailed below."}, {"title": "5.1 Datasets", "content": "Synthetic data. In order to rigorously evaluate the performance of our proposed methodology across diverse initial conditions, we require a synthetic data generator capable of producing realistic networks. This generator must also allow manipulation of key parameters influencing information diffusion, such as homophily, modularity, and opinion distribution. For this purpose, we devise a random network model inspired by Cinus et al. (2022). This model takes as input five parameters: homophily level \u03b7, network modularity \u03bc, shape parameters a and \\( \\beta \\) for opinion distribution, and the number of nodes N. The model outputs a directed graph G = (V, E) and assigns an opinion value xu \\(\\in\\) [0, 1] to each node u \\(\\in\\) V.\n\nThe generation process begins by constructing the set of edges & and assigning nodes to non-overlapping communities c: V \u2192 C using the LFR model (Lancichinetti et al., 2008), known for producing community sizes that adhere to a realistic power-law distribution. The modularity parameter \u03bc is an input of the LFR model and controls the community segregation within the network.\n\nTo model the opinions, we assign each community Ci \\(\\in\\) C an initial opinion drawn from a Beta distribution Oci ~ Beta(\u03b1, \u03b2). For each node u \\(\\in\\) V, its opinion xu is determined by a Bernoulli trial with probability \u03b7 (i.e., homophily). If successful, the node adopts the opinion of its community \\( x_u = o_{c(u)} \\); otherwise, it independently samples an opinion from the same Beta distribution."}, {"title": "5.2 Settings", "content": "In our experiments, we focus on a Query Completion Task: The query q consists in a statement to be completed, e.g., \u201cBrexit is the most...\u201d. Although other modeling choices are possible (for example Generation from Scratch: \u201cGenerate a post about Brexit\u201d), in our preliminary experiments we found that Query Completion achieves faster convergence. We defer a detailed study on the effects of task specification to future work.\n\nGiven the query q, we obtain t by concatenating q and the generated text g = LLM\u0473 (q), i.e., t = q || g. To compute the opinion value st, we adopt a DistilBERT model (Sanh et al., 2019), pretrained for sentiment classification. Concerning the LLM agent, we adopt the 2B version of Gemma\u00b2 (Team and et. al., 2024), a lightweight models family released by Google and based on Gemini\u00b3 technology. Despite its smaller size, Gemma outperforms other models of similar size in tasks such as understanding, reasoning, and safety.\n\nWe fine-tuned the model exploiting the PPOTrainer4 class from the tr1 package, which enables training language models with custom rewards. We fixed the maximum number of training steps to 80 for the synthetic data and 500 for the Brexit data, and the threshold 7 for monitoring the KL-divergence to 75. We also conducted experiments with other state-of-the-art lightweight LLMs, such as Mistral-7B (Jiang et al., 2023)5, LLaMA2-7B (Touvron et al., 2023)6, and GPT-2 (Radford et al., 2019)7. However, these experiments demonstrated that Gemma-2B provides the best trade-off between performance and computational efficiency. For the propagation model, we set e = 0.2. In the experiments conducted on the synthetic network, we vary the opinion distribution of the nodes, the modularity and homophily of the social graph, and the position of the LLM agent within the network. Concerning the nodes opinion, we fix the distribution to be either positive (skewed on 1), negative (skewed on 0), neutral (centered on 0.5), or uniform (uniformly distributed on [0, 1]). Further, we modify the network by imposing the homophily to be either low (0.25) or high (0.75). Similarly, we tuned the modularity of the nodes as either low or high. Figure 3 shows the different opinion distributions across the network structures with high/low homophily. Variations in terms of modularity are indeed depicted in Figure 4. Taking into account the network characteristics, we consider the following configurations for the LLM position:\n\n\u2022 Echo-low (-high): the LLM agent is placed within the echo-chamber with the lowest (resp. highest) average opinion.\n\n\u2022 Comm-largest (-smallest): we locate the LLM inside the largest (resp. smallest) community.\n\n\u2022 Central: the LLM is the node with the highest betweenness centrality in the graph.\n\nAll experiments were performed on an NVIDIA DGX equipped with 4 V100(32Gb) GPUs and 640Gb total Memory. The code used to perform the experiments is publicly available."}, {"title": "5.3 Results", "content": "RQ 1: Can the LLM agent learn to generate content that maximizes engagement?\n\nThe engagement |A| produced at each step of our fine-tuning procedure is depicted in Figure 5, while the corresponding sentiment st is reported in Figure 6. Both figures exhibit a windowed moving average over 15 steps. Each plot corresponds to a different configuration in terms of modularity, homophily, and network opinion, while colors denote different LLM positions, as described above. Within each plot of Figure 5, the dotted lines represent the maximum number of nodes that can be activated by Me assuming that configuration (the response of the engagement model is illustrated in Figure 8). Dotted lines represent the engagement obtained with a content produced by LLM9(0) as shown in Algorithm 1 (i.e., the LLM without fine-tuning).\n\nFinding 1: The LLM agent can maximize engagement in environments with positive opinions. First, we can observe that we reach a faster convergence in almost all the experiments when positive opinions are considered (left column). A slower trend occurs only when the LLM agent is placed in the smallest community, making the nodes harder to reach.\n\nFinding 2: The LLM agent can maximize engagement even in adversary opinion configuration. In adverse environments where users' opinions are skewed towards negative sentiments (second column in Figure 6), the proposed framework enables the LLM to deviate from its natural tendency to spread positive content and instead produce high engagement with negative sentiment content. This effect persists even when the underlying social graph is neutrally distributed (third column) or uniformly distributed (last column). The latter configuration is particularly surprising, as it demonstrates that the LLM agent can maximize engagement even when nodes' opinions are not skewed. In other words, it manages to find the optimal content to generate for activating as many users as possible, balancing across high-variance opinions.\n\nFinding 3: Generated content aligns with the optimal sentiment. The framework enables the LLM agent to optimize engagement by aligning the sentiment of the generated content with the optimal sentiment for the environment. Convergence curves, as shown in Figure 6, demonstrate that the final sentiment adapts to the specific configuration and placement of the LLM agent.\n\nFinding 4: Convergence is sensitive to the position of the LLM agent. Generally, the fine-tuning process succeeds within the fixed number of steps in most experiments. However, the LLM agent's position affects the convergence rate. In particular, high Centrality tends to have an advantage, achieving faster convergence in nearly all configurations. Few configurations do not seem to reach convergence within the fied number of steps. An example is the \"comm-smallest\" configuration in networks that are negatively skewed, highly modular, and highly homophilic. This is likely because starting with positive sentiment in a negatively skewed and weakly connected community makes the tuning process more difficult."}, {"title": "Finding 5: The LLM agent is effective in a real-world environment", "content": "Finding 5: The LLM agent is effective in a real-world environment. Preliminary experiments on the Brexit network revealed that the prominence of positive users biases content generation towards positive communities. This is also due to the high reachability degree that the network exhibits, as also shown in Figure 2. To overcome this specific issue of the network, we reformulated the goal of the experiment to determine if content generation can be tailored to engage specific sugroups of users representing polarized communities. We isolated the two largest communities using the Louvain algorithm (Blondel et al., 2008): one positive (\u201cLeave\", 3,095 nodes and 209,764 edges) and one negative (\"Remain\", 2,894 nodes and 121,325 edges). We then applied the learning procedure to each subgraph, positioning the LLM agent at the node with the highest or lowest betweenness centrality.\n\nFigure 7 reports the results of these experiments. Consistently with the synthetic experiments, we observe rapid convergence when the network's opinion is predominantly positive and a slower convergence rate within the negative community. In both scenarios, the LLM adjusts the generated content to maximize engagement, eventually nearing the optimal threshold. This shows that our framework is effective, not only in a synthetic environment, but also in a real-world context."}, {"title": "Finding 6: Generated content is fluent and polarized as the real Tweets and exhibits comparable engagement levels", "content": "Finding 6: Generated content is fluent and polarized as the real Tweets and exhibits comparable engagement levels. Table 1 shows an example of the generated content, compared to real tweets that were shared on the network. We can see that the sentiment of the generated content is aligned with that of the real one. The notable difference is the length of the generated content, which is not controlled in our framework and departs from the typical length of real tweets, ranging within 280 characters.\n\nThe final set of experiments we perform is aimed at measuring how realistic is the potential engagement of the gener-ated content. As a first step, we assess whether the engagement model proposed in Section 3 represents an adequate proxy for actual engagement on Brexit. We can in fact witness (Figure 9 in the Appendix) a positive correlation between the tweets with highest propagation rate within the network, and the corresponding predicted engagement.\n\nNext, we compare the predicted engagement from both real and generated content in Table 1. To do this, we propagated each content across the entire network from the same starting position\u2014the actual node that posted the real tweet. This approach ensures a fair comparison under identical initial conditions. The results show that the engagement levels for generated content are comparable to those for human text."}, {"title": "6 Conclusions", "content": "In this paper, we explored the use of Large Language Models to generate content that produces user engagement in social media and social networking platforms. We developed an approach that leverages Reinforcement Learning with Simulated Feedback to generate meaningful content, which is designed to effectively spread within a targeted environment. To the best of our knowledge, this is the first attempt to adapt generative modeling for optimizing content propagation in a social network setting. Our approach is entirely adaptive w.r.t. the opinions of the underlying network and model-agnostic to the propagation protocol. The extensive experimental evaluation validates the robustness and effectiveness of the proposed framework.\n\nThe adaptive nature of the proposed framework provides pointers for further exploration of several directions of future research. First, our experiments were limited to prompt completion. Nevertheless, alternative approaches (e.g., generation from scratch) are possible. More in general, the effects of crafting the prompt structure are not considered here and are worth being investigated. Approaches based on RAG (Gao et al., 2024) could also be adopted for proper crafting of the prompt and helping accurate response generation.\n\nWe also focused our evaluation on lightweight LLMs, which are easier to handle in contexts with limited computational resources. However, the impact and flexibility of more powerful models, which in principle should provide more control on the generated content, is still unexplored. Besides that, another noteworthy question is whether specialized models, specifically designed and trained from scratch for the task at hand, are more effective in addressing the problem of content propagation.\n\nAdditionally, other modeling choices can be made regarding the reward function. Instead of solely maximizing user engagement, the LLM could be fine-tuned to affect opinion dynamics, e.g., by influencing network leaning around certain topics or reducing polarization. In these regards, more sophisticated propagation models (Singh et al., 2024; Bernardo et al., 2024) that could account for opinion differences along multiple axes (Monti et al., 2021) could be considered."}, {"title": "A Engagement Model", "content": "We formally describe our engagement process in Algorithm 2."}, {"title": "B Engagement-Sentiment Correlation on Synthetic Network", "content": "Figure 8 depicts some examples of the engagement resulting from the propagation protocol Me over the synthetic network, by varying the content sentiment st. From top-left to bottom-right: the first plot refers to a positely distributed network with high modularity and homophily and the starting position is central; the second one refers to a negatively distributed graph with low modularity and homophily and with Comm-smallest as source node; the third represents the engagement produced over a network with uniform opinion distribution, high modularity, low homophily and Comm-largest as starting node; the fourth plot refers to a neutral network with low modularity, high homophily and assumes the source node to be in a Echo-high position; and finally, the last plot represents the engagement over a uniformly distributed graph with high modularity, low homophily and the source node in the Echo-low configuration."}, {"title": "C Simulated/Actual Engagement on Real Network", "content": "We validate the reliability of our propagation model by comparing the engagement estimated over the Brexit real network, and the actual number of retweets resulting from the most engaging content in the dataset. Figure 9 shows the corresponding regression line computed via Random Sample Consensus (RANSAC) (Cantzler, [n. d.]). Despite our propagation protocol tends to overestimate the engagement produced by the given content, there is a positive correlation between the number of active users estimated by our propagation model (A) and the number of actual retweets, thus validating our findings over the real network."}]}