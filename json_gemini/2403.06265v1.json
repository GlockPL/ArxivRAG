{"title": "Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance", "authors": ["Omer Goldman", "Avi Caciularu", "Matan Eyal", "Kris Cao", "Idan Szpektor", "Reut Tsarfaty"], "abstract": "Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language model- ing where equal probability is assigned to all to- kens. We also demonstrate the empirical impor- tance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' com- pression and models' downstream performance, suggesting that compression is a reliable intrin- sic indicator of tokenization quality. These cor- relations are more pronounced for generation tasks (over classification) or for smaller models (over large ones). We replicated a represen- tative part of our experiments on Turkish and found similar results, confirming that our re- sults hold for languages with typological char- acteristics dissimilar to English. We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance.", "sections": [{"title": "1 Introduction", "content": "While language modeling pipelines employ a multi- tude of sophisticated techniques to achieve success in many NLP tasks, their presupposed tokenization, i.e., the step of discretizing text into processable units, is often done with less scrutiny or devia- tion from the common practices. This tokenization stage, which segments space-delimited words into subwords, forms the foundation of most large lan- guage models (LLMs; Touvron et al., 2023; Gem- ini, 2023; Groeneveld et al., 2024, inter alia) and influences their modus operandi in subsequent us- age. Among other open questions regarding to- kenization, it is unclear whether tokenization is even needed (Clark et al., 2022; Xue et al., 2022; Keren et al., 2022) and how much poor tokeniza- tion influences model performance, especially for non-English languages (Klein and Tsarfaty, 2020; Rust et al., 2021; Gueta et al., 2023). As the tokenizers serve language models, it is straightforward that the primary method to assess their quality is by measuring their contribution to the model performance over the NLP tasks it is meant to solve, i.e., evaluating the tokenizers on tasks extrinsic to tokenization itself. However, this method requires pretraing expensive LLMs when- ever an evaluation of a tokenizer is needed. For this reason an intrisic indicator of tokenization quality is warrented. And indeed the literature is teem- ing with intrinsic evaluations of tokenization. For example, Sennrich et al. (2016) used text compres- sion as the main indicator of the tokenizer's intrin- sic quality, whereas Bostrom and Durrett (2020) suggested assessing tokenizers based on segmen- tation overlap with a morphologically segmented reference. In this paper we carefully distinguish between intrinsic and extrinsic evaluation of tokenizers, and examine to what extent they are correlated. As a specific intrinsic evaluation we focus on compres- sion, the metric underpinning BPE (Sennrich et al., 2016), the most prevalent tokenization algorithm that requires character co-occurrence statistics over a large corpus of raw text to achieve minimal length in tokens. We control the tokenizer's ability to compress by limiting its support, i.e., the amount of data available in the tokenizer's training corpus. By doing so we skew the statistics available to the tokenizer. We compared tokenizers trained with a million supporting documents to ones trained on less and less data, down to a single document, and to a"}, {"title": "2 Measuring Tokenization Quality", "content": "From the very early days of NLP, models have always assumed text discretized to tokens as in- put (Winograd, 1971). For the most part, these tokens were whitespace separated words, but in the recent decade non-trivial tokenization algorithms have surfaced (Mikolov et al., 2012; Sennrich et al., 2016), primarily to deal with unseen tokens without smoothing techniques or other convoluted meth- ods (Chen and Goodman, 1999). The underlying reasoning behind all modern tokenization meth- ods is that some subwords, e.g., morphemes, may carry independent information that is of value to the model even if the word as a whole is rare or unseen. Therefore, better tokenization is assumed to improve models' performance over rare words, while also carrying computational benefits, like"}, {"title": "3 The Role of Compression in Tokenization", "content": "In the realm of intrinsic measures for evaluating to- kenization quality, compression particularly stands out in prominence. It has garnered considerable attention, notably due to its pivotal role as the cor- nerstone of the byte pair encoding tokenization algorithm (BPE; Schuster and Nakajima, 2012; Sennrich et al., 2016), an algorithm that initially conceived for general data compression purposes (Gage, 1994). Given data composed of sequences of atomic symbols, the algorithm minimizes the overall data length, for a given dictionary budget, by iteratively substituting a new symbol in place of the symbol pair most frequently occurring in a large corpus of supporting data. In the domain of language mod- eling, the symbols are usually characters and the supporting corpus is a subset of the text designated to be used as a training set for the language model which the tokenizer is meant to serve. But in a sense, compression-driven tokenization may be viewed as language modeling in and of itself. Consider that language modeling is aimed at assessing and possibly minimizing the likelihood of produced texts expressed as a product of token probabilities,\n$P(x) = \\prod_k P(x_k|x_{1:k-1}),$\nwhere $x_k$ is the kth token in a sentence x. Com- pression limits the upper bound on this product of fractions by minimizing the number of operands, i.e., minimizing the length of the sequence. In terms of n-gram language modeling, where the probability of each token is approximated as"}, {"title": null, "content": "dependent only on a context of length n \u2013 1,\n$P(x) \\approx \\prod_k [P(X_k|x_{k-n:k-1}),$\nand the probability is further approximated by the abundance of the relevant n-gram in a training cor- pus,\n$P(x_k|x_{k-n:k-1}) \\propto C(x_{k-n:k}),$\na compressor may be considered a 0-gram language model, where the relevant n-gram is of length 0 and the probability of each token is not even a function of its own frequency in the training data, setting uniformly,\n$P(x_i) = |V|^{-1},$\nwhere |V| is the vocabulary size. Although simplistic when thinking about lan- guage modeling with predefined whitespace- separated words, this type of objective is sensible when considering that it is used to determine the symbols themselves. From this point of view, prioritizing compres- sion as an indicator for tokenization quality is very reasonable. Since BPE optimizes an approx- imation, albeit crude, of the downstream objec- tive, doing better under this approximated objec- tive should translate into better downstream perfor- mance which will justify the focus on compression as a metric. Moreover, from an information theoretic per- spective, Shannon's source coding theorem (Shan- non, 1948) links the limit on the compression to the entropy of the source of the data to be com- pressed. As language models aim to increase the log-likelihood of texts, hence decrease the entropy of the distribution, they inadvertently also increase the possible compression of texts. Our claim is that this relationship is symmetric, and BPE tok- enizers, as they compress texts, may also inadver- tently increase their log-likelihood. We set to empirically examine our hypothesis by assessing the correlation between the tokenizer's compression ability and the performance of lan- guage models of various sizes over a set of down- stream tasks. To control the compression ability, we recall that BPE's maximal compression is guar- anteed only over its supporting corpus. Normally, for large enough corpora, a minimal discrepancy is assumed between the character distribution in the corpus and the \u201ctrue\u201d distribution in the language. In this work however, we explicitly emphasize and expand this discrepancy by limiting the size of the support to a great extent. We will show that this intervention severely hinders the compression ca- pacity of the tokenizer and that it also leads to deteriorating downstream performance."}, {"title": "4 Experimental Setup", "content": "4.1 English Experiments\nTokenizers We trained six different tokenizers with dictionary size of up to 32k tokens. Each tokenizer was supported by a different amount of documents from the model's train set: a million (1M-DOC), a thousand (1K-DOC), a hundred (100- DOC), ten (10-DOC), one document (1-DOC) and no documents at all (CHAR). When supported by no documents at all, the tokenizer simply breaks all the words into characters and replaces any foreign character with the unknown sign. The tokenizers are initialized with all the rele- vant symbols - the characters of the alphabet, punc- tuation marks, and all foreign characters that appear on the respective documents. In all cases we lim- ited the vocabulary size to 32k, but in practice, for tokenizers supported by little data, the vocabulary size was lower than 32k, since the data did not con- tain a sufficient number of words and subwords. Models For every tokenizer, we trained decoder- only transformer-based language models using the T5X framework (Roberts et al., 2022). We trained models in three sizes in terms of number of param- eters: 1B, 128m and 10m. The model sizes exclude the parameters dedicated to the embedding layer, as its size may differ across tokenizers. All models were trained on the span corruption task (Raffel et al., 2020) for 200k training steps, with a batch size of 512. Every training example was truncated to the maximal length of 1024 tokens. Data Pretraining of the English models was ex- ecuted monolingually using the train split of C4 (Raffel et al., 2020). Tasks To evaluate the tokenizers' contribution to downstream success we finetuned the models over four tasks. Two classification tasks:\n\u2022 QQP (Quora Question Pairs\u00b2) where the task is to classify 2 questions as duplicates or not.\n\u2022 MultiNLI (Williams et al., 2018)) where the model is tested on natural language inference (NLI) examples from a domain which differs from the ones appearing at the training set."}, {"title": "4.2 Turkish Experiments", "content": "To make sure that our results are not due to English- specific phenomena, we repeat a representative subest of the experiments with Turkish, an aggluti- native language with higher morphemes-per-word ratio. for the purpose of intrinsic evaluation, we trained six Turkish tokenizers, as we did for En- glish. However for extrinsic evaluation, due to the expensive pretraining and finetuning, we trained models only for three tokenizers: 1M-DOC, 10- DOC, and CHAR. The models were pretrained on the train split of the Turkish part of mC4 (Xue et al., 2021), and finetuned over three tasks: one classifi- cation task, XNLI (Conneau et al., 2018), and two generation tasks, XL-Sum (Hasan et al., 2021) and Question Generation over the TQUAD dataset.3"}, {"title": "5 Results", "content": "5.1 Intrinsic Evaluation\nTo illustrate the effect of limiting the tokenization support on the compression ability, we measured the accumulative length in tokens of the develop- ment sets of all English tasks used in downstream."}, {"title": "5.2 Extrinsic Evaluation", "content": "Table 2 summarizes the downstream evaluation re- sults for all models and all tokenizers over all four English tasks. Unsurprisingly, it shows that larger models, in terms of parameters, fare better on all tasks. Additionally, it shows that all models per- form better on the classification tasks compared to the generation tasks. Nevertheless, over most tasks and model sizes, there is a clear improvement in performance when the models are equipped with better supported tokenizers. Similarly to the intrinsic metric above, the down- stream improvement is not linear as well. The improvements achieved by updating the tokenizer from the 1-DOC to the 10-DOC are more substan- tial than those from 1K-DOC to 1M-DOC, despite the introduction of significantly fewer documents. The findings for the Turkish models in Table 5 demonstrate analogous patterns, indicating that the results decline as the tokenizer's support dimin- ishes. This is again particularly noticeable in the case of generation tasks."}, {"title": "5.3 Intrinsic-Extrinsic Correlation", "content": "To assess the correlation between the tokenizer's support and the model's task performance we com- puted the Spearman's p coefficient, separately for each task and each model size. This correlation coefficient was chosen since it refers to the relative rank of each data point, thus it does not ascribe linear importance to the differences in the absolute number of supporting documents. The results are shown in Table 3. Note that due to the small sample size the correlation is statisti- cally significant (a = 0.05) only for coefficients larger than 0.829. The results show that, for the most part, the tokenizer's support is well correlated with the model's overall success, with the clear exception of classification tasks on the 1B model. Even starker correlation appears when measur- ing the Pearson's correlation coefficient between downstream performance and the compression it-"}, {"title": "6 Analysis", "content": "Tokenization of Frequent Words To better un- derstand the source for discrepancy in compression between tokenizers, we plot in Figure 3 the num- ber of tokens needed per word with respect to the word frequency (measured in number of appear- ances in a sample of 3 million unseen documents from mC4). We averaged the token-per-word ratio over all words whose occurrences are of the same order of magnitude and provided the number of words in each bin. A similar analysis was done for Turkish and it is shown in Figure 4. The figures show that the token-to-word ratio is extremely similar across tokenizers for words that are the most frequent. On the other hand, the differ- ent tokenizers diverge in token-to-word ratio when presented with rarer words, with less supported to- kenizers being more sensitive to word frequency, compared to better supported tokenizers. It is worth noting that the same trend applies to the CHAR tok- enizer, for which the number of tokens per word is simply its length in characters. This should not be surprising due to the tendency of frequently used words to be shorter in accordance to Zipf's law of abbreviation (Zipf, 1949). In addition, as predicted by Zipf's law (Estoup, 1912; Zipf, 1949), the number of frequent words over which the tokenizers agree is quite small, In terms of types, but they cover a large portion of the 3 million document sample over which the statistics were calculated. The English words that appear at least 106 times in the sampled corpus, 162 in number, cover 47% of the words in the corpus. On the other hand, in Turkish, due to the thicker tail of the Zipf's distribution of morphologically rich languages, only 71 words answer this criterion,"}, {"title": "7 Conclusions", "content": "In this paper we demonstrated the importance of compression to tokenization, both as an intrinsic evaluation of tokenization quality and as an indica- tor for better performance on extrinsic downstream tasks. We argued in favor of compression-driven tokenization from a theoretical perspective, since it may make the tokenizer act as a simple standalone language model, and we showed its correlation with downstream model success. Our experiments point to generation tasks as better downstream evaluators of tokenization since their results are both more sensitive to the tokenizer and better correlate with tokenization quality as expressed in compression ability. In terms of linguistic diversity, the similarity in the results and analyses across two very different languages, English and Turkish, points to our con- clusions being independent of specific typological characteristics. Yet, ample room is left for studying the effects of tokenization on more languages that are even more typologically diverse. Moreover, other intrinsic evaluators are still to be assessed even for the languages we did work with. We conclude that tokenization matters, as poorly compressing tokenizers hinder the results of lan- guage models, and that investment in better com- pressing tokenizer has a potential of improving model performance while being relatively cheap in terms of compute. We therefore call for research to better understand the factors controlling the qual- ity of the tokenization and its relation to overall success of the LLMs."}, {"title": "8 Limitations", "content": "The main limitation of this paper has to do with the amount of resources allocated to this research. Pretraining our LLMs, especially the 1B parameter models, requires a lot of compute and repeating these experiments, in a slightly different setting or just in order to replicate their results, is an expen- sive process. Another limitation has to do with the limited experiments on non-English languages. Although we executed several experiments on Turkish, the cost of pretraining models of up to 1B parameters prevented us from equating the treatment given to the two languages as well as adding experiments in other non-English languages. It is possible, even if somewhat unlikely, that running the full suite of experiments on Turkish would have resulted in different conclusions. A more reasonable possi- bility is that running experiments on more typo- logically diverse languages would yield different conclusions for these languages. We mitigated this risk by choosing a language that is extremely dif- ferent from English."}]}