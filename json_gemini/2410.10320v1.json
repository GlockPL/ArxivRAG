{"title": "DiRW: Path-Aware Digraph Learning for Heterophily", "authors": ["Daohan Su", "Xunkai Li", "Zhenjun Li", "Yinping Liao", "Rong-Hua Li", "Guoren Wang"], "abstract": "Recently, graph neural network (GNN) has emerged as a powerful\nrepresentation learning tool for graph-structured data. However,\nmost approaches are tailored for undirected graphs, neglecting the\nabundant information embedded in the edges of directed graphs\n(digraphs). In fact, digraphs are widely applied in the real world\n(e.g., social networks and recommendations) and are also confirmed\nto offer a new perspective for addressing topological heterophily\nchallenges (i.e., connected nodes have complex patterns of feature\ndistribution or labels). Despite recent significant advancements in\nDiGNNs, existing spatial- and spectral-based methods have inherent\nlimitations due to the complex learning mechanisms and reliance\non high-quality topology, leading to low efficiency and unstable\nperformance. To address these issues, we propose Directed Random\nWalk (DiRW), which can be viewed as a plug-and-play strategy\nor an innovative neural architecture that provides a guidance or\nnew learning paradigm for most spatial-based methods or digraphs.\nSpecifically, DiRW incorporates a direction-aware path sampler\noptimized from the perspectives of walk probability, length, and\nnumber in a weight-free manner by considering node profiles and\ntopological structure. Building upon this, DiRW utilizes a node-wise\nlearnable path aggregator for generalized messages obtained by our\nproposed adaptive walkers to represent the current node. Extensive\nexperiments on 9 datasets demonstrate that DiRW: (1) enhances\nmost spatial-based methods as a plug-and-play strategy; (2) achieves\nSOTA performance as a new digraph learning paradigm.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph neural networks (GNNs) have been widely used across node-\n[23, 47, 50], link- [22, 42, 57], and graph-level tasks [24, 27, 43]\nand achieve satisfactory performance. Therefore, this graph-based\ndeep learning technique holds great potential for applications, such\nas recommendation [7, 39, 54], financial analysis [4, 16, 35], and\nhealthcare [2, 33]. Despite their effectiveness, existing methods of-\nten overlook edge direction in natural graphs, leading to inevitable\ninformation loss and limited performance upper bound.\nCompared to undirected representation, digraphs are crucial\nfor modeling the complex topologies present in the real world\n(e.g., web flow monitoring and bioinformatics), capturing more\nintricate node relationships. Additionally, the recently proposed\nA2DUG [29], Dir-GNN [36] and ADPA [40] reveal a key insight:\nEdge direction offers a new perspective for effectively addressing the\ntopological heterophily challenges that plague graph learning. Despite\nthe growing attention towards DiGNNs, this field is still in its in-\nfancy and faces the following inherent limitations. (1) Spatial-based\nmethods often stack multiple convolution layers and employ two\nindependent sets of learnable parameters to capture directed infor-\nmation concealed in the out-edges and in-edges, resulting in over-\nsmoothing concerns [53] and high computational costs [13, 19, 61].\n(2) Spectral-based methods heavily rely on high-quality directed\ntopology [44, 59], and without this, extreme eigenvalues inevitably\nlead to sub-optimal performance. Meanwhile, strict theoretical as-\nsumptions affect practicality, which hinders their effective deploy-\nment in complex real-world scenarios. Therefore, it is urgent to\ndevelop a more efficient paradigm for digraph learning.\nTo enhance the usability of DiGNNs, this paper focuses on spatial-\nbased methods and proposes a novel (directed) path-based learning\nmechanism (Di)PathGNNs. As we all know, the entanglement of ho-\nmophily and heterophily, where connected nodes exhibit intricate\nfeature distributions and labels, has recently posed a significant\nchallenge [26, 28, 34, 60]. Researchers strive to achieve robust learn-\ning within this complex topology. In this context, compared to the\ntraditional message passing (i.e., neighbor expansion with strict\nspatial symmetry that disregards edge direction), we highlight the\nfollowing advantages of DiRW to further clarify the motivation\nof our study: (1) Edge Direction and Node Order. The core of\nDiPathGNNs lies in performing well-designed random walks for\neach node, treating the paths as node-wise sequences. Advantage:\nDiRW fully considers edge direction and preserves the order of nodes\nwithin the walking paths, which is crucial for capturing structural\ninsights. (2) Adaptive Expansion of Node Receptive Fields. Di-\nPathGNNs adaptively extend hop-based neighbors to path-based\nneighbors by considering the characteristics of random walk, in-\ncorporating more homophilous signals. Advantage: DiRW ensures\nmessage aggregation among nodes with the same label, equivalent\nto data augmentation and highlighting label-specific positive signals\nfor prediction. (3) Path-based Message Aggregation. Based on\nthe above homophily-aware paths, DiPathGNNs aggregate these\nmessages for predicting each node through a learnable mechanism.\nAdvantage: DiRW facilitates the dense aggregation of direction-aware\nhomophilous information, thus enhancing node prediction.\nDespite recent advancements in PathGNNs [17, 41, 51], their\nwalk strategies are tailored for undirected graphs, lacking generaliz-\nability. Furthermore, the complex relationships inherent in directed"}, {"title": "2 PRELIMINARIES", "content": "2.1 Notation\nWe consider a digraph G = (V, E) with |V| = n nodes and |8| = m\nedges. Each node has a feature vector of size f and a one-hot label of\nsize c, with the feature and label matrices represented as X \u2208 R^{n \\times f}\nand Y \u2208 R^{n \\times c}, respectively. The digraph G can be described by an\nasymmetrical adjacency matrix A, where A(u, v) = 1 if (u, v) \u03b5 \u03b5,\nand A(u, v) = 0 otherwise. Suppose V_i is the labeled set, and the\ngoal of the semi-supervised node classification task is to predict the\nlabels for nodes in the unlabeled set V_u with the supervision of V_i.\n2.2 Directed Graph Neural Networks\n2.2.1 Spatial-based Methods. To capture the asymmetric topology\nof digraphs, some spatial-based methods follow the strict symmetric\nmessage passing paradigm in the undirected setting [9, 15, 52, 58].\nHowever, it is crucial to account for the directionality of the edges\nwhen aggregating messages. Specifically, for the current node u e\nV, the model learns weights to combine the representations of its\nout-neighbors (u \u2192 v) and in-neighbors (v \u2192 u) independently:\nH^{(l+1)}_{out} = Agg \\Big(W^{(l)}_{out}, Prop \\Big(X^{(l-1)}_v, \\{X^{(l-1)}_u\\}_{v \\in N^{out}(u)}, \\{(u, v) \\in \\varepsilon}\\Big)\\Big),\nH^{(l+1)}_{in} = Agg \\Big(W^{(l)}_{in}, Prop \\Big(X^{(l-1)}_v, \\{X^{(l-1)}_u\\}_{v \\in N^{in}(u)}, \\{(v, u) \\in \\varepsilon}\\Big)\\Big), (1)\nX^{(l)}_u = Com \\Big(W^{(l)}, X^{(l-1)}_u, H^{(l+1)}_{out}, H^{(l+1)}_{in}\\Big).\nBuilding upon this strategy, recent advances in digraph GNNs have\nfurther refined the message passing scheme to capture the inherent\ndirectionality of the digraph. For instance, DGCN [45] incorporates\nboth first- and second-order neighbor proximity into the message\naggregation process. DIMPA [13] expands the receptive field of\nby aggregating features from K-hop neighborhoods at each layer,\nenabling the capture of long-range dependencies. Inspired by the\n1-WL graph isomorphism test, NSTE [19] tailors the message prop-\nagation to the directed nature of the graph. DiGCN [44] leverages\nneighbor proximity to increase the receptive field and proposes\na digraph Laplacian based on personalized PageRank. ADPA [40]\nadaptively explores appropriate directed patterns to conduct weight-\nfree graph propagation and employs two hierarchical node-wise\nattention mechanisms to obtain optimal node embeddings.\n2.2.2 Spectral-based Methods. Spectral-based approaches for the\nDiGNN depart from the strict symmetric message passing used\nfor undirected graphs [12, 49]. The core of spectral-based DiGNNs\nis to leverage a symmetric or conjugated digraph Laplacian L_d,\nwhich is constructed based on the directed adjacency matrix A_d.\nThis symmetric Laplacian L_d allows the application of spectral\nconvolution operations, which can be formally represented as a\nfunction of the eigenvalues and eigenvectors of L_d. Specifically, the\nlayer-wise node embeddings X^{(l)} are computed via a first-order\napproximation of Chebyshev polynomials, leveraging the spectral"}, {"title": "3 METHOD", "content": "decomposition of the symmetric digraph Laplacian.\nL_d = DGS(A_d, \u03b1, q),\nX^{(l+1)} = Poly(L_d) MLP(X^{(l)}), (2)\nwhere DGS() is the digraph generalized symmetric function with\nparameters \u03b1 and Poly() is a polynomial-based approximation method.\nBuilding on this, DiGCN [44] proposes a \u03b1-parameterized stable\nstate distribution based on the personalized PageRank to achieve\nthe digraph convolution. MagNet [59] utilizes q-parameterized com-\nplex Hermitian matrix to model directed information in digraphs.\nMGC [56] adopts a truncated variant of PageRank, designing low-\npass and high-pass filters tailored for homogeneous and heteroge-\nneous digraphs. LightDiC [22] decouples graph propagation and\nfeature aggregation for scalability in large-scale scenarios.\n2.3 Path-based Graph Neural Networks\nPathGNNs offer an effective approach to capture the intricate pat-\nterns within graphs by sampling and aggregating information along\npaths. For instance, GeniePath [25] introduces an adaptive path\nlayer that intelligently navigates the exploration of both the breadth\nand depth of the node's receptive fields. SPAGAN [55] delves into\nthe topological information of graphs by leveraging the shortest\npaths and applying path-based attention mechanisms to obtain\nnode embeddings. PathNet [41] utilizes a maximal entropy-based\nrandom walk strategy to capture the heterophily information while\npreserving valuable structural information. RAWGNN [17] employs\nNode2Vec [11] to simulate both BFS and DFS, thereby capturing\nboth homophily and heterophily information. PathMLP [51] de-\nsigns a similarity-based path sampling strategy to capture smooth\npaths containing high-order homophily. The detailed descriptions\nof PathGNNs can be found in Appendix. A.8. We have also pro-\nfoundly revealed that PathGNNs can be considered as a generalized\nand lightweight graph attention network in Appendix. A.9.\n3 METHOD\nBuilding upon the key insights discussed in Sec. 1, we now present\nour DiRW model, which is composed of two principal components:\nthe optimized path sampler and the node-wise learnable path ag-\ngregator. The architecture of the model is depicted in Fig. 3.\nSpecifically, drawing upon Key Insight 1, DiRW is initiated\nwith a direction-aware path sampling strategy in Sec. 3.1.1. This\nstrategic approach determines the destinations for each walk step,\nthereby significantly augmenting the capacity to encapsulate the\nintricacies of directed topological structures. Furthermore, we in-\ntroduce a multi-scale and multi-order walk probability scheme in\nSec. 3.1.2, which models high-order and first-order homophily from\nboth topological structure and node profiles. Guided by Key In-\nsight 3, we customize the walk length for each path in Sec. 3.1.3. By\nintroducing the homophily entropy, we evaluate the quality of walk\nsequences and terminate the walk when sequence quality reaches\nan optimal threshold. In Sec. 3.1.4, we propose a weight-free aggre-\ngation mechanism for the path embedding, which preserves the rich\ninformation encoded within the sequential order. Inspired by Key\nInsight 2, we tailor walk numbers in Sec. 3.1.5. By assessing the\ninformational yield of sampled walks, we implement a stopping cri-\nterion that ensures the collection of ample yet efficient information,"}, {"title": "3.1 Optimized Path Sampler", "content": "3.1.1 Direction-aware Path Sampler. In DiRW, the direction of\nedges plays a pivotal role in the sampling process, dictating the\ntrajectory of the random walk. This critical stage is tasked with\ndetermining the destination set for each step of the walk, which\nis essential for calculating the node probabilities and ensuring the\ncontinuity of the walk without interruptions.\nSpecifically, when the walk encounters a node that is devoid of\nincoming edges, the model strategically steers the walk towards\nits out-neighborhood. Conversely, if a node lacks outgoing edges,\nthe walk remains confined to its in-neighborhood. This approach\neffectively circumvents the issue of walk termination at nodes with\nunidirectional connectivity, which is mentioned in Key Insight\n1. For nodes that possess both incoming and outgoing edges, a\nmore nuanced strategy is essential to balance the presence and\ndirectionality of edges. Drawing inspiration from the magnetic\nLaplacian matrix utilized for digraphs [59], DiRW introduces a hy-\nperparameter q\u2208 [0, 1]. At each walk step, a random value r is\nsampled uniformly from the [0, 1]. If r surpasses q, the walk pro-\nceeds along the directed edge, anchoring the target node within the\nout-neighbors. Otherwise, the walk considers both the in-neighbors\nand out-neighbors of the node. The formulation of this process is\nencapsulated in the following equations:\nD_u = I(A_u 1 > 0),\nP_u = D_u \\cdot (q \\cdot A_u + (1 \u2212 q) \\cdot (A_u + A_{iu})) + (1 \u2212 D_u) \\cdot A_{iu}, (3)\nwhere I(condition) is the indicate function, which equals 1 if the\ncondition is met and 0 otherwise, and 1 is the vector of ones. D_u is\na scalar that determines the presence of outgoing edges of node u.\nP_u represents the potential destinations matrix guiding the walk\ntowards the next node in the sequence.\n3.1.2 Multi-scale and Multi-order Walk Probability. The compu-\ntation of walk probabilities in DiRW is a nuanced process that\nseamlessly blends topological structure with node profiles, which\nallows DiRW to delve into the complexities of both high-order and\none-order homophily within heterophilous graphs.\nFrom the topological perspective, DiRW is designed to unearth\nhigh-order homophily information which is often embedded within\nheterophilous graphs [51]. The model strategically prioritizes nodes\nwhich share a limited amount of common neighbors with the cur-\nrent node, bridging to distant regions of the digraph. The topological-\nbased probability measure, denoted as P^{topo}, is crafted to be in-\nversely proportional to the number of common neighbors Com(u, v)\nand normalized by the degree Deg(v) of the candidate node v.\nP^{topo}(v) = 1 - \\frac{Com(u, v) + 1}{Deg(v)} (4)\nAdditionally, from the feature standpoint, DiRW captures one-\norder homophily by favoring walks towards neighbors with more\nsimilar features, which present stronger homophily. The feature-\nbased probability P^{feat} is calculated based on the cosine similarity\nbetween the features of the current node and candidate node.\nP^{feat}(v) = cos(X_u, X_v). (5)"}, {"title": "3.1.3 Homophily Entropy-based Personalized Walk Length", "content": "The transition probability from node u to node v in a walk step\nis determined by aggregating the topological- and feature-based\nprobabilities, which are then normalized by a softmax function.\nP_{ust}(v) = Softmax \\Big((p^{topo}(v) + p^{feat}(v)) \\cdot I(P_u(v) > 0)\\Big). (6)\nRecognizing that the softmax scales vary with the walk direc-\ntions, we pre-compute the topological- and feature-based probabili-\nties (p^{topo} and p^{feat}) to obviate repetitive calculations and ensure\nthe operational efficiency of our DiRW.\n3.1.3 Homophily Entropy-based Personalized Walk Length. In DiRW,\nthe walk length is not predefined but is instead dynamically tai-\nlored based on according to the homophily entropy. Traditional\nhomophily metrics predominantly focus on immediate neighbor-\nhood information, providing a limited view of the walk sequence's\nquality. To address this limitation, we introduce a novel metric -\nhomophily entropy - which evaluates the walk sequence quality\nthrough the lens of feature similarity among the nodes within the\nsequence. Our objective is to ensure that each node in the walk\nsequence maintains a strong homophilous relationship with the\noriginating node, thereby enriching the aggregated information\nwith relevance to it. To achieve this, we first convert the sampled\npath into a homophily label sequence, represented as follows:\nphomo(i) = I(X_{P(i)} \\neq X_{P(0)}) \\cdot \u0456, (7)\nwhere P(i) and p_{homo}(i) denotes the i-th element in the walk se-\nquence and the homophily label sequence, respectively. By applying\nShannon entropy [21] to it, we derive the homophily entropy:\nH_{homo}(P) = H(p_{homo}) = -\\sum p(i) log p(i), (8)\nwhere p(i) represents the probability of observing the i-th element\nin p_{homo}. Our analysis indicates that a lower homophily entropy\ncorresponds to a higher quality walk sequence.\nThis metric is instrumental in guiding the determination of walk\nlength for each sequence. We initiate by setting a minimum walk\nlength l_{min} to guarantee the collection of adequate information.\nUpon reaching l_{min}, the walk is extended until two consecutive in-\ncreses in homophily entropy are detected, signifying an encounter\nwith a heterophilous neighborhood. It aligns with the Key Insight\n3 outlined in Sec. 1, as homophilous nodes prefer longer walks.\n3.1.4 Weight-free Path Encoding. To account for the varying influ-\nence of nodes along a path, DiRW employs an exponential decay\nfunction to assign weights and compute the path embedding. This\nscheme preserves the order information in the sequence and en-\nsures that nodes in proximity to the originating node in the path are\naccorded greater significance in the path embedding. The weight\nassigned to each node in the path is calculated as follows:\nw_i = \\frac{c^i}{\\sum_{j=1}^{k} c^j} \u0456 = 1, 2,..., k, (9)\nwhere c\u2208 (0, 1) is the decay hyperparameter, and k denotes the\nlength of the path. The path embedding is obtained by a weighted\nsum of the node features along the path, given by:\nh^{(j)}_u = \\sum_{i=1}^{k} w_i p(i)^{(j)} (10)"}, {"title": "3.1.5 Adaptive Walk Numbers", "content": "3.1.5 Adaptive Walk Numbers. Ensuring the collection of compre-\nhensive contextual information is critical in DiRW, guiding the\ndetermination of the requisite number of walks per node. This is\nachieved by evaluating the richness of information encapsulated\nwithin the path embeddings. The richness is quantified through the\naverage embedding of the sampled paths, represented as:\n\\bar{z}^{(k)}_u = \\frac{1}{k} \\sum_{i=1}^{k} h^{(i)}_u, (11)\nwhere k is the number of sampled paths. To avoid the premature\ntermination of the walk process, an initial minimum walk number\nn_{min} is established. Subsequent to each walk, DiRW calculates\nthe L2 norm of the discrepancy between the current measure of\ninformation richness and that of the preceding iteration:\n\u0394 = ||\\bar{z}^{(k)}_u - \\bar{z}^{(k-1)}_u ||_2 , k > n_{min}. (12)\nIf the value of \u0394 fall below the predefined threshold \u03b4, it indicates\nthat the sampling has reached a saturation point, beyond which\nadditional walks are unlikely to contribute substantially to the infor-\nmation richness. In such cases, the walk is discontinued to prevent\nfurther unnecessary and redundant sampling, which enhances the\nefficiency of the information gathering process and is reflective of\nour Key Insight 2. It acknowledges that heterophilous nodes are of-\nten characterized by more intricate neighborhoods and necessitate\na greater walk number to amass a sufficiently rich information.\n3.2 Learnable Path Aggregator\n3.2.1 Attention-based Node Encoding. Following the pre-processing\nsampling phase, the DiRW model harnesses an attention mecha-\nnism to discern and weigh the informativeness of various path\nembeddings. This process is pivotal for constructing node embed-\ndings that are rich in relevant contextual information.\nSpecifically, for each node u, DiRW initiates the encoding of\nthe sampled paths h^{(i)}_u through a pair of linear layers, interspersed\nwith a LeakyReLU activation function. The resulting encoding e_u\nis subsequently processed through a softmax function to yield at-\ntention scores \u03b1_u. The final node embedding z_u is then computed\nas a weighted aggregation of the path embeddings h^{(i)}_u, with the\nweights being the attention scores \u03b1_u, articulated as follows:\ne_u = MLP_2(LeakyReLu (MLP_1(h^{(i)}_u))),\n\u03b1_u = Softmax (e_u),\nz_u = \\sum_{i=1}^{k} \u03b1^{(i)}_u h^{(i)}_u, (13)\n3.2.2 Node Classifier. With the node embeddings z_u at hand, DiRW\ndeploys an MLP to perform the node classification task, employing"}, {"title": "4 EXPERIMENTS", "content": "the cross-entropy loss function to guide the training process:\n\\hat{y}_u = Softmax \\Big(MLP_3 (z_u)\\Big),\nL = -\\frac{1}{|V_l|} \\sum_{i \\in V_l} y_i log \\hat{y}_i (14)\nwhere V_l represents the set of labeled training nodes. y_i \u2208 R^c\nand \\hat{y}_i \u2208 R^c correspond to the one-hot encoded true label and the\npredicted label for node i, respectively.\n4 EXPERIMENTS\nIn this section, we conduct thorough evaluations of our DiRW. Our\nexperiments are structured to answer four pivotal questions: Q1:\nCan DiRW achieve superior performance as a new DiGNN and as a\nplug-and-play strategy for existing PathGNNs? Q2: If DiRW proves\nto be effective, what factors contribute to its enhanced performance?\nQ3: What is the running efficiency of DiRW? Q4: How is the path\nquality of DiRW and other PathGNNs? To maximize the usage for\nthe space, we introduce datasets, baselines, experimental settings\nin Appendix A.2-A.5, demonstrate the results of link-level tasks in\nAppendix A.6, and include a sensitivity analysis in Appendix A.7.\n4.1 Overall Performance\n4.1.1 An Innovative Learning Architecture. To answer Q1, we con-\nduct comparative experiments to evaluate the performance of DiRW\nin node classification. The results presented in Tab. 1 demonstrate\nthat as a novel DiGNN, DiRW achieves exceptional performance"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "across all datasets and outperforms the current leading DiGNN Mag-\nNet by an average of 3.2%, which is attributed to its adept modeling\nof digraphs and heterophilous relationships. In contrast, traditional\nundirected GNNs perform well in homophilous scenarios but strug-\ngle in heterophilous digraphs due to simplistic undirected adjacency\nmatrix approaches. Moreover, while Dir-GNN and ADPA strive\nto tackle heterophily with digraph modeling, their methods show\nlimitations in effectively generalizing to homophilous digraphs. Fur-\nthermore, while undirected PathGNNs show promise in modeling\nheterophilous graphs, their performance is significantly hindered\nby critical shortcomings. Notably, these models fail to account for\nthe directionality of edges in digraphs, and their simplistic walk\nstrategies limit their effectiveness in complex digraph structures.\nIt is imperative to highlight that the sophisticated model archi-\ntectures such as RNNs employed in PathGNNs engender scalability\nchallenges, leading to out-of-memory (OOM) errors when dealing\nwith large-scale digraphs ogbn-arxiv and ogbn-products. Similarly,\nthe intricate message-passing paradigms that incorporate multi-\nple convolutional layers in DGCN and NSTE result in incomplete\ntraining within 12 hours, culminating in out-of-time (OOT) errors.\nIn contrast, DiRW revels in its high efficiency and achieves supe-\nrior performance on large-scale digraphs with its weight-free path\nsampling strategy and lightweight learning mechanism.\n4.1.2 A Plug-and-Play Approach. Beyond assessing DiRW as an\ninnovative neural architecture, we also integrate the optimized path\n5 CONCLUSION AND FUTURE WORK\nIn this work, we have underscored the necessity of DiGNNs in the\ncontext of modern graph-structured data and proposed DiPathGNN\nmechanism to capture the inherent homophilous information in\ndigraphs. Through empirical analysis, we have exposed the limi-\ntations of current PathGNNs, particularly in their lack of edge di-\nrection modeling and coarse-grained sampling strategy. To address\nthese challenges, we propose DiRW, a novel path-aware digraph\nlearning approach. DiRW is distinguished by its direction-aware\npath sampler, which is meticulously fine-tuned based on walk proba-\nbility, length, and numbers in a weight-free approach. Furthermore,\nDiRW employs a node-wise learnable path aggregator to formu-\nlate nuanced node representations. Experiments demonstrate that"}, {"title": "A.1 Model implementations in Empirical Study", "content": "DiRW achieves SOTA performance in node- and link-level tasks,\nespecially for topologically heterophilous scenarios.\nLooking ahead, there are several promising directions for future\nresearch. First, we aim to explore more efficient sampling strategies\nthat can further enhance the training effectiveness. Furthermore,\ndeveloping adaptive sampling methods that dynamically adjust\nbased on the characteristics of the graph could potentially improve\nthe performance and scalability of our approach. Lastly, designing\nbetter learning mechanisms to capture the complex relationships\nbetween nodes in digraphs could lead to more accurate predictions.\nThese future works will further enhance our understanding of\ndigraph learning and potentially lead to breakthroughs in how we\nmodel and analyze complex graph-structured data.\nA.1 Model implementations in Empirical Study\nInspired by the concepts of node homophily [31] and edge ho-\nmophily [62], which are pivotal in understanding the homophily\nof graphs, we introduce a refined definition of the node homophily\nratio for each node:\nH(u) = \\frac{|\\{v \u2208 N_u | y_u = Y_v\\}|}{N_u} (15)\nwhere N_u represents the set of neighbors of node u, and y_u denotes\nthe label of node u. The node homophily ratio H(u) serves as a\nmetric for the homophily of the local neighborhood around node\nu. A higher value of H(u) indicates stronger homophily, meaning\nnode u is more similar to its neighbors in terms of the given label.\nIn experiment, we select the top 50% most homophilous nodes as\nhomophily nodes, while the remaining nodes as heterophily nodes.\nWhen investigating the impact of different walk lengths on the\nmodel, we fixed the number of walks at 15. Similarly, we fixed the\nwalk length at 4 when exploring the effect of walk numbers.\nWe utilize the undirected SRW as our sampling strategy. Specifi-\ncally, each node randomly selects a neighbor from its in-neighbors\nand out-neighbors as the next target. For each node in the graph,\nwe specify the same number of walks N_{walk} and the same walk\nlength L_{walk}. Once each walk sequence is collected, we concate-\nnate the features of every node in the sequence. Suppose the walk\nsequence is \\{u_1, u_2, ..., u_{L_{walk}}\\}, where v_j represents the j-th node\nin the walk sequence. For each walk sequence, we concatenate the\nnode features to obtain the path feature vector:\nh_{path} = concat(h_{v_1}, h_{v_2}, ..., h_{v_{L_{walk}}}),\nwhere h_{v_j} denotes the feature vector of node v_j. We then pass this\npath feature vector through a MLP to obtain the path embedding:\nz_{path} = MLP(h_{path}).\nFor each node, we average the embeddings of its different paths\nto obtain the node embedding. Let the set of path embeddings for\nnode u be \\{z^{(1)}_{path}, z^{(2)}_{path}, ..., z^{(N_{walk})}_{path} \\}. The embedding for node v is:\nz_v = \\frac{1}{N_{walk}}\\sum_{i} z^{(i)}_{path}\nFinally, we pass the node embedding z_v through another MLP for\nthe node classification task:\n\\hat{y}_v = MLP(z_v).\nBy leveraging this approach, we effectively utilize path information\nto enhance node representations, achieving better performance in\nnode classification tasks."}, {"title": "A.2 Datasets Description", "content": "A.2 Datasets Description\nWe evaluate the performance of DiRW under 9 benchmark datasets,\ncovering both homophilous and heterophilous graphs. We have\ndocumented the detailed information of these datasets in the Tab. 4.\nBelow, we introduce the descriptions of these benchmark datasets.\nCoraML and CiteSeer [5] are two academic citation networks,\nwhere individual academic papers are represented as nodes, con-\nnected by edges that signify citations made between these papers.\nEach node is characterized by a binary word vector that encapsu-\nlates its textual content, while the class labels correspond to the\nsubject areas to which the papers pertain.\nWikiCS [30] is a dataset derived from Wikipedia. It comprises\nnodes that represent articles within the computer science domain,\nwith edges established based on the hyperlinks that connect these\narticles. The dataset is organized into 10 distinct classes, each rep-\nresenting a different subfield of computer science. The features\nare generated from the text content of the respective articles by\naveraging pre-trained GloVe word embeddings [32], yielding a 300-\ndimensional vector for each node.\nAmazon-Computers [38] is a co-purchase network from Ama-\nzon that encapsulates consumer behavior through a graph structure.\nIn this graph, nodes correspond to computer products, and edges\ndenote the instances where products are frequently bought together\nby customers. The features for each product are derived from the\ntext of product reviews, represented as bag-of-words vectors that\ncapture the essence of customer feedback.\nOgbn-arxiv [14] is a citation network dataset indexed by the\nMAG [48], which is characterized by the word embeddings found\nwithin the titles and abstracts of academic papers. The word em-\nbeddings are generated through the application of the skip-gram\nmodel, which is for learning vector representations of words.\nOgbn-products [14] is a co-purchasing network where nodes\nrepresent products and edges represent two products are frequently\nbought together. Node features are generated by extracting bag-of-\nwords features from the product descriptions.\nChameleon [37] is a network derived from English Wikipedia,\nwhere nodes represent articles and edges indicate mutual backlinks\nbetween them. Each node is characterized by a feature set that\nreflects the presence of specific nouns.\nActor [31] is an actor co-occurrence network where each node\nsymbolizes an actor and the edges indicate instances where actors\nare mentioned together on Wikipedia pages. The node features in\nthis network are constructed as bag-of-words vectors, which are\ncreated by extracting keywords from the Wikipedia pages dedicated\nto each actor. These features are then sorted into five different\ncategories, each corresponding to the specific terms present on an\nactor's Wikipedia page.\nRating [34] is a dataset that originates from the Amazon product\nco-purchasing network metadata found within the SNAP [20]. In\nthis network, nodes correspond to different products, and edges are\nestablished between items that are commonly purchased in conjunc-\ntion with one another. The primary task is to forecast the average\nrating awarded by reviewers, which is divided into five distinct\nclasses. The node features are formulated from the mean FastText\nembeddings [10] of the words present in the product descriptions,\nproviding a textual representation of each product."}, {"title": "A.3 Baselines Description", "content": "A.3 Baselines Description\nOur experiment leverages diverse GNNs as comparative bench-\nmarks", "follows": "i) traditional undi-\nrected approaches: GCN [18", "46": "ii) directed spatial meth-\nods: DGCN [45", "19": "DIMPA [13", "36": "ADPA [40", "methods": "DiGCN [44", "59": "MGC [56", "22": "iiii) PathGNNs:\nRAW-GNN [17", "41": "."}, {"follows": "nGCN [18"}]}