{"title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey", "authors": ["Fengxiang Cheng", "Haoxuan Li", "Fenrong Liu", "Robert van Rooij", "Kun Zhang", "Zhouchen Lin"], "abstract": "Large language models (LLMs) have achieved remarkable successes on various natural language tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. This paper summarizes and categorizes the main challenges into two aspects: (1) Logical question answering, LLMs often fail to generate the correct answer within complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constrains. (2) Logical consistency, LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art Macaw question-answering LLM answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose detailed taxonomies of these methods. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, pretraining, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistency, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extensions to modal logic to account for uncertainty, and efficient algorithms satisfying multiple logical consistencies simultaneously.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable performance in a broad range of natural language tasks including language generation, classification and translation. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. On one hand, learning syntax, semantics, and world knowledge through tasks such as next-word prediction or masked language modeling does not ensure the logical reasoning ability of LLMs [Luo et al., 2023]. On the other hand, the pre-training corpus of LLMs primarily consists of human-written texts, which lack high-quality logical reasoning samples such as logical deduction and proofs [Morishita et al., 2024]. These challenges significantly limit the applicability of LLMs due to the following two summarized aspects.\nLLMs often fail to generate the correct answer in logical question answering, which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constraints. Specifically, these logical questions can be broadly divided into two categories: (1) Determine whether a related statement can be deduced from the given information, namely, output the truth value of the statement: true, false or unknown. For example, the premise and constraints could be Metals conduct electricity. Insulators do not conduct electricity. If something is made of iron, then it is metal. Nails are made of iron., followed by the logical question Is the following statement true, false, or unknown? Nails cannot conduct electricity. To answer this question correctly, LLMs need to conduct logical reasoning nails made of iron metal conduct electricity before conclude that the statement is actually false. (2) Find the correct option that can satisfy all the given premises and constraints from the multiple choices. Surprisingly, LLaMA-13B achieves 33.63% accuracy under 8-shot prompting on the logical questions dataset FOLIO, which is only slightly better than random guess from true, false and unknown with an accuracy of 33.33% [Han et al., 2024]. This significantly restricts the application of LLMs in complicated real-world situations, such as problem-solving and decision-making.\nLLMs are also prone to producing responses contradicting themselves across different questions, which is regarded as violation of logical consistency. Note that the form of logical consistency can be diverse. For example, LLaMa-2 70b answers true to both questions Is an albatross an organ-"}, {"title": "2 Logical Question Answering", "content": "Leveraging large language models (LLMs) for complex reasoning problems has been a key focus of recent researches [Luo et al., 2023; Sun et al., 2024; Zhang et al., 2024]. Numerous studies have explored methods to enhance the logical reasoning capabilities of LLMs, which can be broadly categorized into several branches, including reliance on external solvers, prompting techniques without further training LLMs, and pretraining and fine-tuning. In addition, many benchmarks are developed to evaluate the logical reasoning performance of LLMs."}, {"title": "2.1 Solver-Aided Approach: Translation Via LLM and Proof Via Solver", "content": "The solver-aided approach refers to parsing natural language (NL) problems to symbolic language (SL) expressions, and then solving them via external solvers. In general, the working flow of solver-aided methods can be summarized into three steps, as shown in Figure 2. First, these methods use LLM to translate natural language into a symbolic language (such as logic programming (LP) language, first-order logic (FOL), constraint satisfaction (CSP), and boolean satisfiability (SAT) formulation) that can be recognizable by corresponding solvers. Next, they adopt an external solver for logical reasoning to output the desired answer. Finally, they use LLMs to translate the symbolic answer provided by the external solver into natural language to generate the final answer based on some ensemble algorithms such as majority vote.\nFaithful CoT [Lyu et al., 2023] based on the Chain-of-Thought strategy translates a natural language query into a symbolic reasoning chain with a deterministic solver (e.g., Python/Datalog interpreter or a PDDL planner) to improve the faithfulness of LLM-generated answers. However, because of the limitation of functional programming to represent the highly \"non-linear\" process of reasoning, programs may fail to solve complex logic problems. Unlike Faithful CoT, which applies task-specific formulations and solvers for different problem types, Samlt [Ye et al., 2023] and LINC [Olausson et al., 2023] prompt the NL problems as general SAT and FOL formulation, respectively, and derive the answer by the corresponding solvers. In addition, LogicLM [Pan et al., 2023] utilizes LLMs to translate NL problems into four types of symbolic formulations separately, including LP language, FOL, CSP, and SAT formulation, for a more comprehensive understanding of NL problems. However, these methods ignore the complex logical semantics hidden in the NL problems when performing translation. To bridge this gap, CLOVER [Ryu et al., 2025] prompts LLM to parse a natural language sentence into logical dependency structures, then translates the subsentences sequentially.\nHowever, the solver-aided approach has many drawbacks. First, transforming logical problems into formal expressions will result in information loss [Liu et al., 2024a], leading to the problem being unsolvable. For example, in the symbolic translation of \"When a person reads a book, that person gains knowledge. Harry read Walden. Whether this inference is"}, {"title": "2.2 Prompt-Based Approach: Translate, Reason and Verify Via LLM", "content": "Prompting is a direct and effective technique for stimulating the logical reasoning capabilities of LLMs. To achieve a more accurate answer of logical questions, the prompt-based approaches can be broadly divided into two categories, the first is to explicitly model the logical chain when answering the questions. For example, Chain-of-Thought (CoT) [Wei et al., 2022] prompt strategy that enables LLMs to output the reasoning process step-by-step. Based on this, Tree-of-Thought (ToT) [Yao et al., 2024] was proposed to let LLMs self-evaluate and select among multiple inference paths. Meanwhile, Graph-of-Thoughts (GoT) [Besta et al., 2024] represents the generated information as an arbitrary graph and allows to extract the core of whole networks of thoughts based on the induced graph. To ensure that the reasoning process can more realistically reflect rational logical thoughts, Diagram of Thought (DoT) [Zhang et al., 2024] was proposed to model iterative logical reasoning in LLM as constructing a directed acyclic graph (DAG) within the model, as shown in Figure 3, which consists of nodes representing propositions, criticisms, refinements, and verifications. The edges in the DAG are all oriented without any cyclic paths, which represent logical relationships or dependencies between each node. In addition, many previous studies focus on the decomposition of logical questions. For example, cumulative reasoning [Zhang et al., 2023a] was proposed to leverage three specialized types of LLMs (proposer, verifier, and reporter) in a collaborative reasoning process.\nThe second category is to translate natural language into symbolic language by well-defined prompts, and then utilize LLMs for better reasoning. For example, Symbolic Chain-of-Thought (SymbCoT) [Xu et al., 2024a] is a LLM-based framework that combines symbolic representation and logi-"}, {"title": "2.3 Pre-training and Fine-tuning Approach", "content": "The limited reasoning abilities of LLMs can be attributed to the lack of high-quality reasoning samples (especially logical multi-step deduction or proofs) in the pre-training corpus, which is composed mainly of human-written texts [Morishita et al., 2024]. Human-written texts usually show reflexive thinking rather than rigid reasoning, let alone logical deduction or inference. Intuitively, training LLMs with deductive proofs and natural language examples that demonstrate explicitly the logical reasoning process is an effective way to enhance their intrinsic logical reasoning capabilities.\nImplementation of pre-training and fine-tuning on logical questions with the reasoning process requires additional data. A major distinction between them is based on the source of the feedback, which can be internal to the LLM or external. Internal feedback uses the intrinsic knowledge and parameters of the model to reassess its outputs. On the contrary, external feedback integrates inputs from human, other models, or other sources of knowledge base and tools. DiLA [Zhang et al., 2023b] utilizes the LLM to generate an original solution, and then iteratively refines this solution by forward and backward passes through a network logic layer involving first-order logic constraints. The constraint satisfiability is then checked by the SAT solver, and the solution is updated until all constraints are satisfied. A logic-driven contrastive learning approach [Wang et al., 2022] was proposed to encourage the pre-trained model to better capture logical information. In addition, a logic-driven data augmentation approach [Bao et al., 2024] named AMR-LDA was proposed to transform the initial text into a structured semantic representation. Next, operations within them are applied to generate logically modified AMR graphs and then translated back into natural language to produce augmented data.\nTo further enhance the transparency and interpretability, LOGIPT [Feng et al., 2024] was proposed to directly internalize and emulate the reasoning processes of the solver Pyke and output the process directly. In addition, it is fine-tuned on a constructed instruction-tuning dataset containing both natural language logical questions and the solver's symbolic reasoning process. In addition, ALT [Morishita et al., 2024] builds a synthetic logic corpus based on logical principles, containing natural language examples translated from a logical deductive reasoning process generated by a logical-based algorithm. Then ALT adopts this process to implement supervised fine-tuning, requiring LLMs to generate logical steps to infer a given hypothesis from provided facts. Similar to ALT, LogicAsker [Wan et al., 2024] established a set of fundamental reasoning skills grounded in propositional and predicate logic and generated natural language examples for each skill based on LLMs. Based on the skill set, LogicAsker generates reasoning questions by translating standard logic expressions into natural language, assesses the accu-"}, {"title": "2.4 Evaluation: Tasks and Benchmark Datasets", "content": "In general, a logical reasoning question contains a natural language description that outlines a set of propositions or constraints related to certain objects, along with a corresponding question about these objects. The objective is to infer the correct answer to the question based on the given information [Ye et al., 2023]. These question-answering pairs are typically categorized into two primary types [Luo et al., 2023]: (1) Free-Form Qustion-Answering: Assess whether a given question or statement can be logically inferred from the provided information, producing an output of true, false, or unknown. The formulation includes logic programming language, propositional logic language, and first-order language. There are many typical datasets such as Proofwriter [Tafjord et al., 2021], FOLIO [Han et al., 2024], Ruletaker [Clark et al., 2021].\n(2) Multiple Choice Question-Answering: Identify the correct option from multiple choices satisfying all given premises and constraints. The formulation includes CSP and boolean SAT formulation. There are also many typical datasets such as ReClor [Yu et al., 2020], LogiQA [Liu et al., 2021] and AR-LSAT [Zhong et al., 2022].\nBesides, there are some other efforts to refine the benchmark and evaluate the LLMs's logical reasoning, such as [Parmar et al., 2024; Tian et al., 2021; Luo et al., 2023]."}, {"title": "3 Logical Consistency", "content": "The development of reliable large language models (LLMs) and their secure deployment is increasingly important, especially when they are used as knowledge sources. In trustworthy systems, logically consistent LLMs are essential: LLMs with logical consistency can be directly verified for correctness without requiring complicated benchmarks, thus increasing confidence in the reliability of LLM for end users in their work [Calanzone et al., 2025; Ghosh et al., 2025].\nLogical consistency requires LLMs not to contradict themselves, the knowledge base, or logical rules when answering different questions in the process of reasoning the complex problems [Calanzone et al., 2025]. Sometimes, ensuring that LLMs can reason deductively without contradicting themselves is referred to as self-consistency [Mitchell et al., 2022; Tafjord et al., 2022]. However, it is found that although LLMs seem to show improved ability of comprehension and reasoning, they still fail often to generate logically consistent answers [Jang and Lukasiewicz, 2023]. Moreover, training on large datasets for question answering alone cannot satisfy the consistency demands, as has been amply demonstrated [Calanzone et al., 2025]. Addressing these limitations, recently there has been an influx of research to improve the logical consistency of LLMs.\nIn this section, we will take a closer look at various logical consistencies, showing the logical relationships of one, between two up to among three entities or statements, along with different methods to enhance the logical consistency LLM, and its gold evaluation."}, {"title": "3.1 Negation Consistency", "content": "Logical consistency is fundamentally all about avoiding contradictions. Negation consistency, as the basics of logical consistency, can be interpreted in two ways. On the one hand, it requires that p and \u00acp cannot hold simultaneously while one of them is true $p \\oplus \\neg p$, i.e., $(p < \\neg p) \\land \\neg(\\neg p \\land p)$ [Kassner et al., 2023; Ghosh et al., 2025]. On the other hand, it can be seen as an antonym in data augmentation [Asai and Hajishirzi, 2020]. LLMs are also prone to producing responses contradicting themselves across different questions. For example, in the experiments LLaMa-2 70b answers true to both questions Is an albatross an organism? and Is an albatross not an organism? [Ghosh et al., 2025].\nTo address this challenge, [Kassner et al., 2021] added a memory layer, called Beliefbank, originally composed of LLM's answers to a collection of questions. Then it leverages a weighted MaxSAT solver to flip the beliefs that obviously clash with others and use a feedback mechanism to ask further questions along with relevant beliefs as the query context. Differed from the Beliefbank constraints are hand-provided implications, ConCoRD [Mitchell et al., 2022] first generates several candidates answers to each question from"}, {"title": "3.2 Implication Consistency", "content": "The implication consistency is based on the logical rule p \u2192 q, p \u22a2 q. It means that given a constraint \"p\u2192 q\" and the premise p, it can be implicated that \u201cq is True\". If the model outputs that \"q is False\", then we say the answer violates implication consistency. For example, given the physical fact that All Iron (p) is metal (q), LLMs are not to be expected to answer True to This material is iron (p) and False to This material is metal (q) at the same time.\nIntuitively, implication consistency is closely related to the CoT method, because each chain can be regarded as a logical implication. Failure to satisfy implication consistency will lead to inaccurate outputs obtained from these explanation-based prompting methods. To address this challenge, Maieutic Prompting [Jung et al., 2022] infers the correct answer from unreliable explanations. Unlike previous methods that generate a single explanation, Maieutic Prompting produces abductive implications supporting both \"true\" and \"false\" answers. These implications are recursively converted into logical constraints and the optimal answer is inferred using a MAX-SAT solver to avoid implication inconsistency."}, {"title": "3.3 Transitivity Consistency", "content": "Transitivity represents the logical relationships among three predicates or propositions. Given two premises p \u2192 q and q\u2192r and it can be inferred that p \u2192 r, which is regarded as the transitivity consistency. It has been shown that LLMs lack transitivity consistency. For example, a state-of-the-art Macaw question-answering (QA) model answers both Yes to Is a sparrow a bird? and Does a bird have feet?, but answers No to Does a sparrow have feet? [Mitchell et al., 2022]. The previous two affirmative answers can implicate that A sparrow has the feet according to the transitivity rule, which is incompatible with the answer No.\nTo mitigate the transitivity inconsistency of LLMs, [Asai and Hajishirzi, 2020] leverage the logical inference rules of transitivity and symmetric, to prompt the models to generate complemental statements. For example, based on the questions If a tsunami happens, will wood be more moist? and If wood is more moist, is more weathering occurring? along with answers More and More, the implicated information If a tsunami happens, is more weathering occurring? together the answer More will then be augmented by the transitivity. Similarly, for the symmetric rules, the antonyms and the negation of the statements will be also added to the dataset, In this way, this method augments the training data and then a consistency-based regularizer is adopted to train the models to enhance their logical consistency containing the transitivity and symmetric."}, {"title": "3.4 Factuality Consistency", "content": "Factuality consistency refers to the degree of alignment between a model's generated responses or reasoning outcomes and real-world knowledge. In fact-checking tasks, the system evaluates the factual accuracy of model outputs by comparing them with reliable knowledge sources, while detecting potential logical errors or factual inaccuracies. When it comes to factuality consistency, there are some subtle distinctions between it and logical consistency. While logical consistency covers contradicting themselves, constraints or a knowledge base (KB), factual consistency agrees on single facts contained in a KB [Calanzone et al., 2025].\nTo address the issue of insufficient logical consistency in existing language models under complex query scenarios, a logic consistency evaluation and improvement framework for LLMs based on Retrieval-Augmented Generation (RAG) was proposed [Ghosh et al., 2025]. Specifically, a propositional logic-based evaluation metric is firstly designed to quantify the model's consistency in logical operations such as implication, conjunction, and disjunction. Second, by constructing three benchmark datasets based on knowledge graph (KG), it is demonstrated that the mainstream LLMs exhibit poor logical consistency in complex queries. To address this issue, this study introduces a KG-contextualized supervised fine-tuning approach to enhance the logical consistency of LLMs in complex fact-checking tasks, leveraging parameter-efficient fine-tuning methods such as QLORA to improve efficiency."}, {"title": "3.5 Compositional Consistency", "content": "Compositional consistency refers to the ability to maintain overall logical consistency when combining multiple facts or logical constraints. Specifically, when a model needs to combine independent facts into a complex chain of reasoning by logical operators (e.g., implication, conjunction, etc.), it should ensure that each step of the derivation conforms to the logical rules, and makes final conclusions self-consistent and are logically correct under contradictions. This ability requires the model to not only understand the meaning of individual fact, but also to correctly capture the logical relationships when they are combined, avoiding inference errors. LOCO-LMS [Calanzone et al., 2025] is proposed to ensure the compositional consistency via neuro-symbolic integration, which makes factual and logically (self-)consistent by fine-tuning a base LLM according to a knowledge base of facts and rules. The constraints that can be arbitrary propositional logic formulas are compiled into a circuit and then used to encourage the model to allocate non-zero probability only to factual and consistent facts. In addition, [Liu et al., 2024b] proposed a universal framework to quantify the compositional logical consistency via three fundamental proxies: transitivity, commutativity, and negation invariance."}, {"title": "3.6 Evaluation", "content": "[Kassner et al., 2021] introduce a gold standard approach to evaluate the accuracy and consistency of a set of beliefs. Following work including [Mitchell et al., 2022], [Kassner et al., 2023] adopt this typical evaluation approach. We give a simple introduction and borrow the symbolic language in [Mitchell et al., 2022]. It is defined as:"}, {"title": "4 Future Research Directions", "content": "Despite state-of-the-art approaches greatly enhance the predicate logic reasoning ability of LLMs, there is still a lack of exploration into the more complicated and challenging conditional and modal reasoning abilities. Specifically, sentences involving conditional reasoning can be written in the form of If..., then . Conditional reasoning also has strict logical rules that should be followed. For example, If the match is struck (p), then it will light (q) implies If the match is struck (p) and has been soaked in water (r), then it will light (q), which can be formally formulated as $p \\rightarrow q =(p \\land r) \\rightarrow q$. In addition, modal logic extends predicate logic by further incorporating must and may to account for the certainty and possibility, respectively. For example, Mary might (\u25ca) not (\u00ac) have been at the wedding (p) implies It's not (\u00ac) the case that Mary must (\u25a1) have been at the wedding (p), which can be formally formulated as $\\Diamond \\neg p = \\neg \\Box p$. Despite expanding to conditional and modal logic reasoning can significantly increase the applicability of LLMs, recent work shows almost all LLMs make some basic mistakes with conditionals or modals, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals [Holliday et al., 2024]. Therefore, it is still worthwhile to explore new methods to make LLM have the ability to perform reasoning on conditional and uncertain events."}, {"title": "4.2 Higher-Order Logical Reasoning of LLMs", "content": "Compared to first-order logic, higher-order logic allows for reasoning about properties and functions, enabling more complex statements and proofs. First-order logic only quantifies over individual variables, thus can only perform reasoning about properties of objects, not properties of properties. In contrast, higher-order logic allows quantification over sets, functions, and predicates, enabling statements about properties of properties and relationships between functions. For example, a simple first-order logic statement is All cats are mammals, which can be formulated as $\\forall x (Cat(x) \\rightarrow Mammal(x))$. A related higher-order logical reasoning example would be There is a property that all cats have, such that any animal with that property is a mammal, which can be formulated as $\\exists P \\forall x (Cat(x) \\rightarrow P(x)) \\land \\forall y (P(y) \\rightarrow Mammal(y))$. This demonstrates the ability to quantify over properties (like P, which can be considered a set of objects) instead of just individual objects, allowing for more complex expressions and reasoning on LLMs."}, {"title": "4.3 Efficient Algorithms Satisfying Multiple Logical Consistencies", "content": "Despite many methods having been proposed to enhance various types of logical consistency of LLMs, there are still two critical challenges. On one hand, most methods only apply to enhancing a specific type of logical consistency, rather than satisfying multiple logical consistencies simultaneously. For example, as a data augmentation approach, logic-guided data augmentation [Asai and Hajishirzi, 2020] only simulates reverse samples and transitive logical rules to enhance negation consistency and transitivity consistency of LLMs, respectively. Nonetheless, improving a specific type of logical consistency does not necessarily improve other types of logical consistency. On the other hand, enumerating all possible combinations of answers to all questions to verify logical consistencies of LLMs would cost exponential space storage and unexpectedly large computational overhead. Taking the transitivity consistency checking as an example, which states that $p\\rightarrow q$ and $q\\rightarrow r$ and infer $p\\rightarrow r$, one need to determine the answer to each of the three associated questions, which requires time complexity $O(n^3)$. Therefore, it is essential to develop more efficient methods that can simultaneously satisfy the various logical consistencies of LLM."}, {"title": "5 Conclusion", "content": "In summary, this survey provides a comprehensive overview of the current state of the art in logical reasoning abilities of LLMs. Despite impressive advances in most natural language tasks, LLMs continue to face significant challenges in their logical reasoning abilities, particularly in the areas of logical question answering and logical consistency. Through a comprehensive taxonomy, we classify cutting-edge approaches to tackling these challenges, highlighting logical question answering based on external solvers or implemented through prompting, pretraining, and fine-tuning, as well as a variety of logical consistency concepts and solutions, including negation, implication, transitivity, factuality consistencies, and their composites. In addition, we review benchmark datasets and evaluation metrics used in this area that are critical for assessing LLM performance in logical reasoning tasks. Looking ahead, promising research directions include"}]}