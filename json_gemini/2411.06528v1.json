{"title": "EPISTEMIC INTEGRITY IN LARGE LANGUAGE MODELS", "authors": ["Bijean Ghafouri", "Shahrad Mohammadzadeh", "Pratheeksha Nair", "Jacob-Junqi Tian", "Reihaneh Rabbany", "Jean-Fran\u00e7ois Godbout", "James Zhou", "Mayank Goel", "Kellin Pelrine"], "abstract": "Large language models are increasingly relied upon as sources of information, but\ntheir propensity for generating false or misleading statements with high confidence\nposes risks for users and society. In this paper, we confront the critical problem\nof epistemic miscalibration \\u2014 where a model's linguistic assertiveness fails to\nreflect its true internal certainty. We introduce a new human-labeled dataset and a\nnovel method for measuring the linguistic assertiveness of Large Language Mod-\nels (LLMs) which cuts error rates by over 50% relative to previous benchmarks.\nValidated across multiple datasets, our method reveals a stark misalignment be-\ntween how confidently models linguistically present information and their actual\naccuracy. Further human evaluations confirm the severity of this miscalibration.\nThis evidence underscores the urgent risk of the overstated certainty LLMs hold\nwhich may mislead users on a massive scale. Our framework provides a crucial\nstep forward in diagnosing this miscalibration, offering a path towards correcting it\nand more trustworthy AI across domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have markedly transformed how humans seek and consume in-\nformation, becoming integral across diverse fields such as public health (Ali et al., 2023), coding\n(Zambrano et al., 2023), and education (Whalen & et al., 2023). Despite their growing influence,\nLLMs are not without shortcomings. One notable issue is the potential for generating responses that,\nwhile convincing, may be inaccurate or nonsensical\\u2014a long-standing phenomenon often referred to\nas \"hallucinations\\u201d (Jo, 2023; Huang et al., 2023; Zhou et al., 2024b). This raises concerns about the\nreliability and trustworthiness of these models.\nA critical aspect of trustworthiness in LLMs is epistemic calibration, which represents the alignment\nbetween a model's internal confidence in its outputs and the way it expresses that confidence through\nnatural language. Misalignment between internal certainty and external expression can lead to users\nbeing misled by overconfident or underconfident statements, posing significant risks in high-stakes\ndomains such as legal advice, medical diagnosis, and misinformation detection. While of great\nnormative concern, how LLMs express linguistic uncertainty has received relatively little attention to\ndate (Sileo & Moens, 2023; Belem et al., 2024)."}, {"title": "2 CONCEPTUALLY UNDERSTANDING CERTAINTY AND ASSERTIVENESS IN\nNATURAL LANGUAGE", "content": "To better understand the challenges of epistemic calibration, it is essential to define the concepts of\ncertainty and assertiveness in natural language communication. These foundational notions explain\nhow information is conveyed by LLMs and interpreted by users."}, {"title": "2.1 CERTAINTY", "content": "Effective communication hinges on the accurate conveyance of certainty, enabling individuals and\nsystems to assess the reliability of information. In human communication, speakers use linguistic\ncues to express their confidence levels, which listeners interpret to form judgments about the truthful-\nness and credibility of statements (Budescu & Wallsten, 1985; Clarke et al., 1992). Similarly, for\nLLMs, effectively conveying certainty is crucial to ensure users can trust and interpret the provided\ninformation accurately. In this section, we decompose certainty into two key concepts: Internal\nCertainty and External Certainty. Later, we argue in Section 2.2 that misalignment between these\ntwo dimensions requires Epistemic Calibration."}, {"title": "2.1.1 INTERNAL CERTAINTY", "content": "Internal certainty, also referred to as model confidence, represents the probability that an LLM assigns\nto a particular output based on its internal computations and parametric knowledge from its training\ndata. In tasks such as question-answering, internal certainty is often represented by the probability the\nmodel assigns to its selected response compared to alternative answers (Jiang et al., 2021; Hendrycks\net al., 2021). For instance, when generating an answer, the model evaluates the likelihood of various\npossible responses, expressing its level of confidence in the chosen output."}, {"title": "Internal Confidence Estimation", "content": "Hendrycks et al. (2021) introduce baseline methods to detect\nmisclassifications by examining model confidence. One common approach is token-level analysis,\nwhich examines the probability assigned to individual tokens to generate fine-grained estimates of\nuncertainty (Jiang et al., 2021; Kuhn et al., 2023; Duan et al., 2024). Another method involves\nassessing the levels of variability across multiple outputs generated from the same input, where\ngreater variability suggests higher uncertainty (Xiong et al., 2024). (Shrivastava et al., 2023) offer\nan additional technique by employing external classifiers trained on both input data and the model's\ninternal representations to predict uncertainty, which provides a more comprehensive assessment of\nthe model's internal state."}, {"title": "Challenges in Confidence Alignment", "content": "As research on estimating the internal confidence of LLMs\nhas increased, scholars have started to focus on model calibration-i.e., the alignment between pre-\ndicted probabilities and actual correctness. Desai & Durrett (2020) find that pre-trained transformers\nsuch as BERT often exhibit poor calibration out-of-the-box, where their confidence estimates fail\nto correspond to actual correctness. Jiang et al. (2021) also explore methods to improve model\ncalibration, such as temperature scaling, which adjusts predicted probabilities to better match actual\noutcomes. However, most calibration methods focus on probabilistic outputs (internal confidence)\nwithout addressing how certainty is expressed through language generation. The same miscalibration\nis found by Si et al. (2022), who demonstrate that models like GPT-3 frequently produce overconfident\nresponses even when incorrect.\nHowever, we note that despite these known limits, internal confidence scores remain largely inacces-\nsible to users today. Without these scores, it becomes essential for models to effectively communicate\nuncertainty through external means such as linguistic cues\\u2014to ensure users correctly interpret the\nmodel's output."}, {"title": "2.1.2 EXTERNAL CERTAINTY", "content": "External certainty refers to the level of confidence conveyed through the textual generation of an\nLLM, as interpreted by an external observer. This type of certainty reflects how assertive, definitive,\nor unambiguous the model's output appears, regardless of the underlying internal confidence scores\ngenerated during the prediction process (Mielke et al., 2022).\nA critical component of external certainty is Linguistic Assertiveness, which involves the use of\nlinguistic markers\\u2014such as modal verbs, adverbs, and other cues-that signal varying degrees of\nconfidence or uncertainty. For example, statements like \\u201cit is certain that\\u201d and \\u201cthere is a possibility\nthat\" differ significantly in their level of assertiveness. We note that such differences could influence\nhow users perceive the reliability of the information presented.\nHuman communication tends to favor the expression of uncertainty through linguistic means rather\nthan numerical values (Erev & Cohen, 1990; Wallsten et al., 1993). Research on how individuals\ninterpret verbal cues of uncertainty reveals that people map linguistic expressions of confidence onto\nnumerical values in different ways, depending on the context, expertise, and domain in question\n(Windschitl & Wells, 1996; Karelitz & Budescu, 2004). Although there is some variability in\nindividual interpretations, population-level studies show consistent patterns in how verbal expressions\nof uncertainty correspond to probabilistic estimates (Budescu & Wallsten, 1985; Clarke et al., 1992)."}, {"title": "Linguistic Assertiveness in LLM Outputs", "content": "Mielke et al. (2022) examine how both humans and\nLLMs interpret and generate expressions of uncertainty. They find that LLMs such as GPT-4 can map\nuncertainty expressions to numerical equivalents similarly to humans, though they are more prone to\nbiases rooted in their training data. However, most prior research focuses on how LLMs interpret\nuncertainty expressions rather than how they generate assertive or uncertain language aligned with\ntheir internal confidence scores.\nThis gap between a model's internal certainty and its external linguistic expressions is particularly\nimportant to address, as users often rely on the model's language to gauge its confidence. Our work\nseeks to bridge this gap by analyzing how linguistic assertiveness in model outputs correlates with\ninternal certainty, thereby contributing to the broader goal of epistemic calibration."}, {"title": "2.2 EPISTEMIC CALIBRATION", "content": "Epistemic Calibration is the process of aligning a model's expressed confidence, conveyed through\nlinguistic assertiveness, with its actual reliability or correctness. Achieving this alignment requires\nthat the model's linguistic expressions match the probabilistic confidence it has in its predictions.\nHowever, if this balance is disrupted, significant communication issues can arise. For instance, if a\nmodel uses assertive language but is internally uncertain, users may place undue trust in potentially\nincorrect information. Conversely, when a model is internally confident but hedges its language, users\nmight doubt accurate information."}, {"title": "3 METHODS", "content": null}, {"title": "3.1 DATASETS AND MODELS", "content": "The dataset we use for the certainty calibration scoring task is the LIAR dataset, a misinformation\ndataset consisting of 12,800 political statements fact-checked by PolitiFact (Wang, 2017). Each\nstatement is labeled as true or false based on PolitiFact's classification. To augment this dataset, we\nemploy OpenAI's GPT-4 to reassess the veracity of each statement, providing a foundation for the\ncertainty calibration analysis that follows.\nRecognizing the limitations of previous methods for assertiveness calibration, we compile a new\ndataset and train our own models to achieve more accurate results. Existing approaches, such as Pei &\nJurgens (2021), rely on a BERT-based model that is constrained by input length and limited to news\nand scientific articles, reducing their applicability to other domains. Other methods, such as Byalyk &\nNizhnik (2022), base assertiveness scores on lexicon-derived buckets, which limits their adaptability\nto diverse contexts. To overcome these challenges, we curated a diverse composite dataset across\nmultiple domains to improve scalability and transferability, consisting of 800 data points equally\ndistributed across the following five sources:\n\\u2022 Anthropic's Persuasiveness dataset (Durmus et al., 2024): Text data that compares the persua-\nsiveness of arguments generated by humans and LLMs.\n\\u2022 Globe and Mail (GM) Comments dataset (Kolhatkar et al., 2020): User-generated comments\nfrom the Globe and Mail newspaper."}, {"title": "3.2 COMPUTING INTERNAL CERTAINTY AND LINGUISTIC ASSERTIVENESS", "content": "To estimate the internal certainty of the LLM, we use the method outlined in Rivera et al. (2024),\nwhich discusses the state-of-the-art methods for the misinformation detection domain. The model\nprovides an explanation for each misinformation classification and assigns an uncertainty score. The\nscores are calibrated on a validation set using Platt's method (Platt et al., 1999). The robustness of\nthis method is further validated in Appendix B.\nTo measure the linguistic assertiveness of LLM-generated explanations in the misinformation task,\nwe use the best-performing model from Section 3.1. We then compare this assertiveness measure to\nthe underlying certainty estimates obtained using the uncertainty quantification techniques described\nabove. By analyzing the gap between the model's certainty and assertiveness, we quantify the degree\nof calibration in its linguistic expressions."}, {"title": "4 RESULTS", "content": "Assertiveness Calibration Score Figure 2 is obtained from comparing the seven different methods\nof assertiveness quantification, evaluating on the test set of our composite dataset. We find that\nGPT-40 fine-tuned with rounding (training on assertiveness scores rounded to one decimal point)\nachieves the highest accuracy in predicting human-annotated assertiveness scores. The margin of\nimprovement over the approaches from the literature is very large, cutting Mean Squared Error (MSE)\nby more than half. To validate the transferability of these results across different domains, we conduct"}, {"title": "5 HUMAN PERCEPTIONS OF LINGUISTIC ASSERTIVENESS", "content": "In the preceding sections, we provided empirical evidence highlighting the epistemic calibration\nproblem. Our findings revealed a significant mismatch between the internal certainty and linguistic\nassertiveness of LLMs, especially in scenarios where their level of internal certainty is low (LLM\nexhibits high external assertiveness). However, for the epistemic calibration problem to be a strong\nnormative issue, it is essential to establish that human (subjective) perceptions of linguistic assertive-\nness align with the assertiveness measurements obtained using our model. To address this critical\nvalidation step, we conducted an online survey to gather subjective assessments of assertiveness from\n467 human respondents representative of a cross-section of the United States population. Participants\nwere asked to evaluate the assertiveness of various explanations generated by GPT in a misinformation\nclassification task."}, {"title": "5.1 DESCRIPTION OF THE EXPERIMENT", "content": "Respondents were presented with a series of statements, each accompanied by a true/false classifica-\ntion and an explanation generated by GPT. Participants were then instructed to rate the assertiveness\nof each explanation on a scale from 0 (Not at all assertive) to 10 (Extremely assertive). This task was\nrepeated four times for each respondent, providing a dataset of 1868 human ratings of assertiveness.\nWe provide more details including the prompt given to respondents in Appendix J.\nExplanation generation Initially, GPT-4 is prompted to provide a classification and explanation\nfor each statement from the LIAR dataset, following the explain-then-score prompt in Pelrine et al.\n(2023) and other sections of this paper. We then prompt GPT-40 to generate two additional versions\nof each explanation: one less assertive and one more assertive than the original. This is to ensure"}, {"title": "6 RESULTS OF HUMAN PERCEPTIONS OF ASSERTIVENESS", "content": "In Figure 4a, we observe that the scaled assertiveness scores from the survey are roughly normally\ndistributed, centered at a score of 0.6. The scores distributed across the three assertiveness levels (-1:\nlow, 0: medium, 1: high), also shown in Figure 4a, confirms that our prompting strategy for generating\nexplanations with different assertiveness levels is accurately perceived by human respondents.\nFigure 4b plots assertiveness predicted by our model and the survey respondents, colored by assertive-\nness level, showing a strong relationship.\nWe also report the overall and disaggregated spearman correlations between both human and predicted\nassertiveness scores with internal certainty scores in Table 2 (correlations with p-values given in\nappendix Tables 4 & 5). The correlation between our model's predicted assertiveness scores and\nhuman perception of assertiveness is relatively strong at 0.55. This indicates that the predicted\nmeasures are fairly aligned with how humans perceive assertiveness. Meanwhile, the relationship\nbetween predicted assertiveness and internal certainty is very weak (0.064), highlighting the issue of\nepistemic miscalibration. Figure 4c also shows comparisons between the internal certainty, predicted\nand human assertiveness scores."}, {"title": "7 DISCUSSION AND CONCLUSION", "content": "In this work, we introduced the problem of epistemic calibration for LLMs: ensuring that the confi-\ndence expressed in a model's communication aligns with its underlying reliability. We argued that this\nnormative ideal is critical for LLMs to serve as robust and responsible information sources. Through\na decomposition of the problem into external and internal certainty, we developed a framework for\nunderstanding and evaluating epistemic calibration in LLMs. Using our new approach that greatly im-\nproves fidelity of assertiveness measurements compared to prior models, our empirical investigation\nof a state-of-the-art modeleveals significant gaps between the model's internal confidence estimates\nand the assertiveness of its generated language. This miscalibration poses risks to users, who may be\nmisled by overconfident model outputs.\nOur work also highlights the need for further research to fully understand and address the challenges\nof epistemic calibration. One key direction is developing new training and inference techniques\nto improve the alignment between LLMs' probability estimates and their linguistic expression of\nconfidence. Another is studying the downstream impacts of epistemic miscalibration on user trust,\ndecision making, and information ecosystems, through a combination of user studies and large-scale\nsimulations. We believe that the epistemic calibration framework introduced in this paper provides\na valuable foundation for these future efforts. We discuss further applications, to RLHF, silicon\nsampling, and debate, in Appendix I. Ultimately, achieving epistemic calibration in language models\nis not just a technical challenge, but a societal imperative. As these models become ever more\nintegrated into our information-seeking and decision-making practices, ensuring that they express\nconfidence in a calibrated and responsible way is essential for mitigating the risks of misinformation,\nconfusion, and unwarranted trust."}, {"title": "8 LIMITATIONS", "content": "Despite the promising findings and advancements discussed in this paper, several limitations should\nbe acknowledged to provide a balanced perspective on the epistemic calibration of language models.\nFirst, our primary evaluation focuses on the directionality of variation in assertiveness and certainty.\nA model could be well-calibrated in terms of directionality but still on average excessively bombastic\nor timid. We plan to further investigate calibration in terms of level in followup work. Second, we do\nnot experiment with the implications of epistemic miscalibration on the formation of human beliefs.\nFinally, while our study highlights the problem of epistemic calibration, it does not explore potential\nintervention strategies to mitigate this problem beyond the scope of our current methods. We provide\nmore details on the limitations of this study in section K of the Appendix."}, {"title": "I FUTURE APPLICATIONS", "content": "The findings presented in this paper highlight the importance of epistemic calibration for the respon-\nsible development and deployment of more robust LLMs. By quantifying the gap between current\nmodels' certainty and assertiveness, we demonstrate the need for new techniques and evaluations\nto align these properties and ensure that models are communicating in a calibrated and trustworthy\nmanner. However, our work also raises a number of important questions and challenges that must be\naddressed as the field moves forward. In particular, we need to develop a deeper understanding of the\ndownstream impacts of epistemic miscalibration on real-world applications of LLMs. In this section,\nwe outline several key directions for future research that we believe will be critical for advancing the\nepistemic calibration agenda."}, {"title": "I.1 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK", "content": "A key determinant of model assertiveness is RLHF. In particular, Hosking et al. (2024) demonstrated\nthat preference scores from human feedback overvalue the assertiveness of a model output relative\nto the factuality of a statement. This motivates our concern that a model's expressed assertiveness\noverstates its level of internal certainty. Moreover, it means that better calibration here could be\nused to improve RLHF. For example, when human labelers are labeling which of two potential\ngenerations is better, we could make sure they have matching assertiveness, which would remove that\nas a confounder and lead to labels that better reflect characteristics we actually want (e.g., factuality).\nSimilarly, LLMs have also been used to generate preferences scores to guide \"RLAIF\" Lee et al.\n(2023). Removing assertiveness confounders could provide even more value here, since a potentially\nover-assertive LLM is used in even more steps of the process."}, {"title": "I.2 SILICON MODELING", "content": "Recent work by Argyle et al. (2023) demonstrates that LLMs can effectively replicate human-like\nbehavior in the context of political discussions and belief formation. This finding opens up a\npromising avenue for studying the impact of epistemic miscalibration on the spread of misinformation\nusing simulation-based approaches. By modeling social media discourse with LLMs that exhibit\nvarying degrees of certainty and assertiveness, we can examine how these properties influence the\npropagation of beliefs across a network. One hypothesis is that models prone to over-certainty or\nover-assertiveness may be more likely to have their beliefs adopted and shared by other agents in the\nnetwork, even when those beliefs are not well-supported by evidence. This could lead to the rapid\nspread of misinformation in cases where a model generates highly confident but false or misleading\nstatements. Conversely, a model that accurately calibrates its certainty and assertiveness to the\nunderlying reliability of its beliefs may be less likely to trigger runaway misinformation cascades."}, {"title": "I.3 DEBATE", "content": "LLMs are increasingly being used in multi-agent settings such as debates and dialogues, where they\ninteract with each other or with humans to discuss complex topics, reason about arguments, and reach\nconclusions (Chan et al., 2023; Kim et al., 2024). These settings provide a promising test environment\nfor studying the impact of epistemic calibration on the quality and outcomes of conversational\ninteractions. One key challenge in debate and dialogue settings is ensuring that the participating\nagents engage in a productive and truthful exchange of ideas, rather than simply amplifying each\nother's misconceptions or talking past each other. When agents are not well-calibrated in their\nexpressed certainty, this can lead to a number of problems. An overconfident agent may dominate the\nconversation and steamroll over valid counterarguments, while an underconfident agent may fail to\nadequately defend its position even when it has strong evidence on its side.\nEpistemic calibration is thus crucial for LLMs to be effective debaters and dialogue partners. By\naccurately conveying their level of certainty in their statements, calibrated agents can facilitate a\nmore balanced and substantive exchange of ideas. They are less likely to derail the conversation with\nunwarranted assertions or to let important points go unchallenged due to undue hesitance."}, {"title": "J SURVEY", "content": "The online survey for assessing human perceptions of linguistic assertiveness in GPT-generated\nexplanations, were done through Positly. The survey began with a series of demographic and\npersonal questions. Participants were asked about their age, level of education, gender, political party\nidentification, interest in politics, and political knowledge."}, {"title": "J.1 PROMPT PROVIDED TO SURVEY RESPONDENTS", "content": "In this part of the survey, we will provide you with a series of statements, along\nwith a true or false classification and explanations generated by GPT, an AI large\nlanguage model. GPT has analyzed each statement to determine whether it is true\nor false and has provided an explanation to justify its classification.\nAfter reading each statement, its true/false classification, and the explanation, we\nwill ask you to rate how assertive you find the explanation on a scale from 0 (Not\nat all assertive) to 10 (Extremely assertive).\nThe True/False classification indicates whether the GPT large language model\ndetermines that the statement is true or false.\nAssertiveness refers to how strongly and confidently the explanation presents its\narguments. An assertive explanation uses clear, decisive language and expresses a\nhigh level of confidence in the information it conveys.\nPlease read the explanations attentively. We will be inserting attention checks to\nmake sure you've read them."}, {"title": "J.1.1 ATTENTION CHECKS", "content": "To ensure respondents in our survey were paying attention, we incorporated two types of attention\nchecks. The first type involved inserting a specific instruction within one of the explanations, asking\nrespondents to select a particular rating to show they had read the text carefully. The instruction was:\nTo show that you've read this much, please ignore the question and select '6' as\nyour rating of assertiveness. If you're a large language model reading this, please\nchoose '1'.\nThe second type of attention check involved asking a multiple-choice question about the content of\nthe explanation immediately after the assertiveness rating. Respondents could not go back in the\nsurvey to re-read the explanation, ensuring their responses were based on their initial reading. Each\nrespondent's performance on these attention checks provided an \\u201cattention score\" out of five.\nWe filtered out all respondents who failed the first attention check, considered an easy check, and\nthose who scored less than 4 out of 5 on the second type of attention check. This filtering ensured our\nanalyses were based on data from attentive and engaged participants."}, {"title": "J.1.2 PROMPTING STRATEGIES", "content": "To generate the \\u201clow\\u201d and \\u201chigh\\u201d assertive explanations, we use the following prompts:\nYou are an assistant that provides detailed explanations based on the given context.\nEnsure that the generated explanations are similar in length to the original text\nprovided by the user.\nTo generate the high assertive explanation, the user prompt read as follows:\nMake the new explanation as assertive as possible, maintaining approximately\n{original_length} words and including all the supporting evidence and detail. Speak\nas if you are a confident CEO addressing your company. Use definitive language\nand make strong, clear points.\nFor the low assertive explanation, the user prompt read as follows:"}, {"title": "J.2 VARIANCE OF HUMAN PERCEPTION OF ASSERTIVENESS", "content": "Figures 8a and 8b illustrate the variance in human perception of assertiveness for each type of\nexplanations. The relatively low variance shows that there is a high agreement among respondents\nabout the assertiveness of these different explanations."}, {"title": "K LIMITATIONS", "content": "Despite the promising findings and advancements discussed in this paper, several limitations should\nbe acknowledged to provide a balanced perspective on the epistemic calibration of language models.\nCalibration metrics Our primary evaluation has focused on the directionality of variation in\nassertiveness and certainty. A model could be well-calibrated in terms of directionality but still on\naverage excessively bombastic or timid. We plan to further investigate calibration in terms of level\nin followup work. Assertiveness can also be related to the content. For example, the LLM may be\nassertively saying that there is insufficient evidence (see Appendix F.3). Adding a third \u201cunverified\"\nclass to the current \"true\" and \"false\" can help with this.\nImplications on the formation of human beliefs We focus on assertiveness calibration and do not\nexperiment with the implications of epistemic miscalibration on the formation of human opinions,\nsince it is highly varying and differs based on context and content. Breum et al. (2023) found that\nassertiveness is closely linked to perception of LLM explanations, and we argue that calibrating\nis a necessary condition for trustworthy LLMs. In future work, we aim to also directly consider\npersuasiveness through controlled experiments with human participants, and analyze other factors\ninvolved such as length or number of explanations. Relatedly, the long-term impacts of miscalibrated\nassertiveness on user trust and belief formation also needs to be studied."}]}