{"title": "InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback", "authors": ["Haishuo Fang", "Xiaodan Zhu", "Iryna Gurevych"], "abstract": "A crucial requirement for deploying LLM-based agents in real-life applications is the robustness against risky or even irreversible mistakes. However, the existing research lacks a focus on preemptive evaluation of reasoning trajectories performed by LLM agents, leading to a gap in ensuring safe and reliable operations. To explore better solutions, this paper introduces InferAct, a novel approach that leverages the Theory-of-Mind capability of LLMs to proactively detect potential errors before critical actions are executed (e.g., 'buy-now' in automatic online trading or web shopping). InferAct is also capable of integrating human feedback to prevent irreversible risks as well as enhance the actor agent's decision-making process. Experiments on three widely-used tasks demonstrate the effectiveness of InferAct. The proposed solution presents a novel approach and concrete contributions towards developing LLM agents that can be safely deployed in different environments involving critical decision-making.", "sections": [{"title": "Introduction", "content": "The advancement of Large Language Models (LLMs) has spawned a variety of LLM-based agents that are capable of completing complex tasks such as navigating the web (Zhou et al., 2024b), managing databases (Wang et al., 2023a), and generating code (Wang et al., 2024). These agents' capabilities and potentials have drawn significant research interest recently (Yao et al., 2023; Liu et al., 2024; Wu et al., 2024; Xie et al., 2024; Fang et al., 2024). However, to deploy the models to real-life applications, the robustness against costly or sometimes irreversible mistakes is crucial. For instance, an incorrect purchase made by a web shopping agent can lead to a significant monetary loss, while a household agent mishandling kitchen equipment can pose serious safety risks.\nHowever, the existing research in LLM agents lacks a focus on robust modeling that proactively evaluates the decision process before executing any critical actions. This leads to a gap in ensuring safe and reliable operations. In response to these challenges, we introduce InferAct, an approach designed to evaluate whether an Actor agent is on track before any critical action is executed, and to solicit human intervention if potential errors are detected (c.f. Figure 1). This mechanism aims to enhance safety and prevent negative consequences resulting from risky executions. Current studies (Shinn et al., 2023; Yao et al., 2024; Zhou et al., 2024a; Kim et al., 2023b) overlook potential risks incurred by executing critical actions and assume the feedback indicating success or failure can be obtained post-action execution (e.g. 'buy-now' in automatic online trading or web shopping).\nWe argue that this assumption is impractical in real-world settings, particularly when failures carry severe penalties (e.g., property damage, financial loss) or when obtaining human feedback is costly. Unlike the above studies, our proposed method, InferAct, does not rely on the post-execution feedback. Instead, it leverages real-time assessment to mitigate risks before any detrimental outcome materializes. By mimicking the vigilance of a human overseer, InferAct does not merely observe the actions taken by agents but infer the agent's intent behind those actions. This ability to infer the intent is known as Theory of Mind (ToM) (Premack and Woodruff, 1978) in cognitive science, which enables humans to interpret the behavior of others by attributing mental states such as beliefs, and intentions to them. The most recent work (Strachan et al., 2024) has shown that GPT-4 models performed at, or even sometimes above, human levels in several ToM aspects such as identifying indirect requests, false beliefs. Building on the To\u041c capability of LLMs, InferAct interprets the intent behind action chains executed by agents, identifying deviations when these actions stray from their intended goals. If the intentions inferred from the action chains suggest a potential deviation or error, InferAct proactively alerts humans to provide feedback. The feedback not only prevents undesirable outcomes from critical actions but offers guidance to refine the decision-making ability of the Actor agent. Ultimately, this enhances the performance and trustworthiness of LLM agents.\nTo evaluate the effectiveness of InferAct, we conduct experiments in three distinct environments, including a Web shopping task (Yao et al., 2022), a household task (Shridhar et al., 2021), and a search-based Question Answering task (Yang et al., 2018). Our experiments demonstrate that InferAct achieves the state-of-the-art performance across these tasks with various LLMs (e.g. GPT-4-turbo, GPT-3.5-turbo, and Llama-3-70B) as the back-ends. By incorporating human feedback, InferAct significantly reduces the risks caused by erroneous actions and improves the performance of the Actor agent compared with alternative methods.\nWe further evaluate different methods in high-stakes conditions including high-priced purchases in web shopping and high-risk operations in the household task. The results reaffirm that InferAct possesses superior error detection capabilities in these scenarios. When combined with the risk-aware prompt, InferAct effectively minimizes the losses (e.g. monetary loss) incurred by undetected adverse actions compared with alternative methods.\nTo summarize, our contributions are as follows:\n\u2022 We propose a preemptive evaluation workflow for LLM-based agents involved in critical decision-making, which integrates human feedback to enhance the safety and performance of agents.\n\u2022 We introduce InferAct, a novel approach that applies the Theory of Mind (ToM) capabilities of LLMs to assist humans in preemptively detecting potential risks of LLM agents in critical scenarios. Our experiments show that InferAct achieves state-of-the-art performance in detecting erroneous actions on three tasks with different LLMs as the back-ends.\n\u2022 InferAct has proven effective when combined with both binary and natural feedback, significantly enhancing the performance of LLM agents compared to alternative methods.\n\u2022 Our experiments in high-stakes setup show the efficacy of InferAct. When equipped with risk-aware prompts, the improvement of InferAct is evident not only in preventing the execution of incorrect critical actions but also in minimizing losses incurred from undetected incorrect actions."}, {"title": "Related Work", "content": "Trustworthiness of LLM Agents. As LLM agents gain the capability to interact with external environments to complete various tasks, it becomes crucial to address the potential irreversible consequences of their actions and determine when human oversight is necessary. However, this area of research is still largely unexplored. The emulation method has been proposed to assess risks of API calls by utilizing LLMs as a sandbox environment (Ruan et al., 2024; Hua et al., 2024). For details about these works, please refer to Appendix C. However, emulation-based methods may not always align with the execution in complex real-world environments. InferAct is the first work to explore the preemptive evaluation mechanism with human feedback for LLM agents in real-world environments (e.g. Web shopping).\nEvaluation and Feedback Acquisition of LLM Agents in critical scenarios. Current research generally assumes that feedback is either available post-execution (Shinn et al., 2023; Yao et al., 2024; Zhou et al., 2024a; Kim et al., 2023b) or completely unavailable during task inference (Kim et al.,"}, {"title": "The Approach", "content": "This section describes the mechanism of InferAct to assess the reasoning process of the Actor, i.e., the agent to perform the user's task. Humans have the strong ToM ability to infer other people's intentions based on their behaviors, without acessing to others' internal thoughts. Inspired by this, we leverage the ToM ability of LLMs to deduce the intended tasks behind the sequences of actions and observations the Actor made during task execution. The key idea is: by comparing the tasks inferred from the Actor's actions with the actual tasks given by the user, InferAct is able to detect whether the Actor has deviated from the user's task during the execution process. To fulfill this, we design two components: the Task Inference Unit and the Task Verification Unit (c.f. Figure 3).\nThe Task Inference Unit. This unit is responsible for inferring intended tasks from the action chain performed by the Actor. The action chain, denoted as S, comprises a sequence of (Action, Observation) pairs, $\\{a_1, o_1, ..., a_m, o_m\\}$. The Actor operates under the ReAct (Yao et al., 2023) framework, which typically consists of the sequence of Thought, Action, Observation). However, for the purpose of unbiased task inference, the Thought component is excluded to form S. The rationale is that Thought records the internal deliberations and plans of the Actor during task resolution, which might contain information about the user's task. For instance, the first Thought of the Actor in Figure 2 explicitly states the task to 'find 66 inches blackout shades'. Excluding the Thought component ensures that task inference remains impartial and is not influenced by direct internal cues from the Actor, which is crucial for verifying whether the actions performed by the Actor align with the user's specified task.\nSpecifically, we instruct LLMs with prompt $P_i$ (c.f. Appendix A) to infer the N most probable tasks $T = \\{t_1, t_2, ..., t_v \\}$ that the action chain intends to solve.\n$T = LLM(P^i, S)$\nDue to the diversity and the varying granularity of tasks performed by the Actor, we opt for generating N most probable tasks rather than a single possible one. This mirrors the human ToM ability to consider multiple plausible intentions or objectives from observed action chains. Once inferred tasks are obtained, along with the user's original task $t^*$, we format them into a Multiple-Choice Question (MCQ) framework.\n$MCQ = \\{C_1, ..., C_N, C_{N+1}\\}$\nwhere $C_j = t_j$ for $j = 1, 2, .., N$ and $C_{N+1} = t^*$. Each choice in the MCQ represents a task, and the MCQ serves as the input for the Task Verification Unit, which evaluate the alignment between the action chain S and the original task $t^*$.\nThe Task Verification Unit. Upon assembling the MCQ set, the Task Verification Unit $P^u$ prompts the LLM to assign a probability to each choice $C_j$, indicating the likelihood that it is fulfilled or on track to be fulfilled by the action chain S. The prompt $P^u$ is detailed in Appendix A.\n$P = \\{P_1, P_2, \u2026, P_N,P_{t^*}\\} = LLM(P^u, S, MCQ)$\nwhere $p_j = Pr(C_j \text{ is correct}|S)$ for each choice in the MCQ.\nIn our experiments, we directly prompt LLMs to generate verbalized probability $p_j$ with justifications derived from the token space of LLMs, which is friendly to commercial LLMs where logits of tokens might be unavailable. Given that LLMs can be sensitive to the choice order (Robinson and Wingate, 2023), we aggregate the probability of $p_{t^*}$ across different positions (refer to Appendix B). How to enhance the reliability of verbalized probability has been extensively investigated (Mielke et al., 2022; Tian et al., 2023; Li et al., 2024; Ulmer et al., 2024). Among them, we adopt the Top-k prompting strategy proposed by Tian et al. (2023) as it showed promising results in the following experiments (Section 5). It should be noted that InferAct is flexible with different probability estimation methods.\nIn contrast to the typical MCQ where options are mutually exclusive and their prediction probabilities sum to 1.0, we consider the verification process as a multi-label task. This means that the sum of the assigned probabilities to each option does not need to be 1.0, reflecting the fact that one action chain S might fulfill multiple tasks. The inferred tasks from the Task Inference Unit can vary in granularity from the original task $t^*$, but are not mutually exclusive. For instance, an action chain S that fulfills the specific, fine-grained in-ferred task (e.g. buy a grey vanity bench with metal legs) can also complete a more general, coarse-grained user's instruction (e.g., buy a vanity bench). The multi-label setting provides LLMs with more flexibility to assign appropriate probabilities to the user's task $t^*$, contextualized by the other options in this scenario.\nInferAct is performed before any critical actions, i.e., irreversible actions with bad consequences. If $p_{t^*}$ is low, it indicates that the Actor is likely to deviate from its intended goal. In such case, InferAct alerts humans to intervene. The feedback provided by human subjects will be appended to the input context of the Actor for the next trial. Human feedback not only prevents and mitigates negative consequences from the execution of critical actions, but also improves the Actor's performance without the cost of failure. Regarding the forms of human feedback, in Section 5.2, we explore two typical types: binary and natural-language feedback. InferAct leverages the TOM ability of LLMs to understand the intent of the Actor's behaviors and detect errors. InferAct with elicited human feedback can ensure that the Actor remains aligned with intended goals, thus minimizing risks and improving performance."}, {"title": "Experimental Setup", "content": "In this section, we evaluate InferAct on three distinct tasks commonly used in LLM agents: Web-Shop (Yao et al., 2022), HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021). We define critical actions in these tasks.\nWebShop. The WebShop (Yao et al., 2022) is an online shopping benchmark where an agent navigates an online store to fulfill user requests, such as purchasing a white vanity bench under $100. The agent's actions include searching and clicking through the website, with the critical action being a click[Buy Now] due to its financial implications.\nHotPotQA. As a Wikipedia-based question-answering task, HotPotQA (Yang et al., 2018) in the agent setup (Yao et al., 2023) challenges agents to find correct answers using Wikipedia APIs. The APIs include search[entity], lookup[string] and finish[answer]. The critical action is finish[answer] as it often affects the user's satisfaction with the system, e.g., in the context of customer service.\nALFWorld. In this household task (Shridhar et al., 2021), agents perform a variety of actions to fulfill the user's task like Pick & Place, Clean & Place, Heat & Place, Cool & Place. The critical actions include Clean, Heat, Cool since these actions involve potential irreversible physical state changes to the objects being operated. For example, if the agent cleans something that should not be wet, it could damage the item. Besides, the task completion is also a critical action.\nThe detailed descriptions of these tasks and the corresponding data size used for evaluation can be found in Appendix E."}, {"title": "Evaluation Metrics", "content": "As we aim at identifying unsafe reasoning trajectory before executing critical actions, we measure how well the model can identify it. We employ the Area Under the Precision-Recall Curve (AUC-PR), recall, precision and corresponding F1-score at the optimal threshold from the AUC-PR."}, {"title": "Baselines and Backbone LLMS", "content": "As there is no previous work on fine-tuned critics in these tasks, we include three widely used prompting-based methods as baselines. Detailed prompts are included in Appendix A.\nStandard Evaluation Prompt. Similar to self-refinement (Madaan et al., 2023) and Prospector (Kim et al., 2023a), this method directly prompts LLMs to evaluate the correctness of the reasoning trajectory performed by the Actor.\nStandard Evaluation with Self-Consistency. Based on the standard evaluation prompt, self-consistency (Wang et al., 2023b) evaluates the reasoning trajectory m times and leverages the majority voting as the final evaluation. The sampling time m is set to five in our experiments.\nMulti-step Evaluation. This approach evaluates the reasoning trajectory step-by-step. LLMs are prompted to generate a verbalized probability $P_i$ to estimate the correctness of each step $S_i$. The overall score is aggregated based on the step-level estimate. In our experiments, we compare the performance of four different aggregation methods {Min, Max, Mean, Product}.\nRegarding back-end LLMs, we use gpt-4-1106-preview (Achiam et al., 2023) as the Actor agent to perform the user's task. For baseline methods, both commercial and open-sourced LLMs are adopted as the back-ends, including Llama-3"}, {"title": "Experiment Results and Analysis", "content": "As illustrated in Table 1, InferAct consistently surpasses alternative methods across different benchmarks, demonstrating robust performance with both commercial and open-source LLMs. Notably, InferAct (GPT-4-turbo) achieves the best average F1-score and AUC-PR on these tasks, reflecting the strong ToM capability of GPT-4-turbo.\nOn Webshop, InferAct outperforms all baseline methods across different backend LLMs. For instance, with GPT-4-turbo, InferAct achieves an F1-score that is 28.9% higher than the Standard Evaluation while using GPT-3.5-turbo, InferAct outperforms Multi-step evaluation by 19.3% (F1-score). A significant challenge in WebShop evaluation lies in comprehending the subtle semantic difference in similar items, product attributes such as distinguishing between a box spring foundation and a bed with a box spring, or, dark brown and coffee brown hair dye. Baseline methods struggle with these nuanced differences.\nUnlike baselines which directly contrast the Actor's reasoning trajectory and the user's task, InferAct address the challenge by performing backward inference. It infers a set of plausible instructions that could have led to this action chain. For instance, as depicted in Figure 2 (C), InferAct infers three instructions related to custom cut-to-size blackout shades based on the Actor's action chain. However, the user explicitly requests 66\u00d766 inch blackout shades. Such discrepancies are overlooked by other methods but are successfully identified by InferAct by assigning a zero likelihood to the user's actual task, as shown in Figure 2 (D).\nHotPotQA is an information-seeking task. While the multi-step evaluation method achieves competitive results, or even matches the performance using GPT-4-turbo, InferAct still delivers the best performance across the three back-end LLMs. The performance gains of InferAct are less pronounced on HotPotQA compared to WebShop and ALF-World, primarily because the multi-step method benefits from the LLMs' internal knowledge on this particular task. InferAct can showcase its advantage when the reasoning path is flawed or the LLM internal knowledge is unreliable. For in-stance, a user asks about the number of personnel the Navy that had Gilliam-class attack transports have, baseline methods failed to detect the Actor missed specific detail the Navy that had Gilliam-class attack transports have. InferAct successfully pinpointed this omission by inferring that the question seeking for the number of personnel the Navy have is more inclined to be answered, when referencing the 'Navy' broadly, rather than the original, more specific query concerning the Navy with Gilliam-class attack transports.\nThe Multi-step Evaluation method achieves the second-best F1-score on WebShop and performs similarly to InferAct on HotPotQA. However, its effectiveness notably declines in the ALFWorld task where the Actor needs to perform more exploration steps to locate the required items (such as a cup, mug, or pan). These exploration steps are assigned low scores, strongly affecting the overall accuracy of multi-step evaluations across different aggregation methods (see Appendix D for results). This issue does not hurdle InferAct which outperforms Multi-step Evaluation and Standard Evaluation by 33.9% and 8.6% respectively with GPT-4-turbo as the backend."}, {"title": "The Synergy of InferAct and the Actor", "content": "The critics attempt to proactively identify potential risks before executing critical actions, allowing for human involvement to help mitigate the potential negative outcomes through feedback. Our study investigates both the binary (Liu et al., 2018; Shi et al., 2021) and Natural-Language (NL) feedback (Tandon et al., 2022; Madaan et al., 2022). Binary feedback, ideal for users seeking minimal engagement, straightforwardly indicates the Actor with clear 'correct' or 'incorrect' signals. In our experiments, we use the gold labels from the dataset to provide such signals. This information enables the Actor to perform self-reflection (Shinn et al., 2023) for subsequent trials. For more detailed insights, NL feedback is suitable. We utilize GPT-4-turbo to craft NL feedback by comparing a gold outcome (e.g., the correct product in WebShop) with the predicted one (refer to Appendix A.5 for prompts), which mimics what humans may say when seeing the differences. Previous work (Bai et al., 2022; Lee et al., 2024) has suggested that the feedback generated by advanced LLMs (e.g. GPT4, PaLM) could be on par with the feedback sourced from humans in some summarization, dialogue generation, and categorization tasks. This allows us to simulate human feedback in a scalable and immediate way. Table 2 and Figure 4 demonstrate InferAct's effectiveness across three tasks with both binary and NL feedback. The Actor, guided by InferAct, consistently outperforms baselines over three iterations using both binary and NL feedback. For instance, InferAct with NL feedback surpasses the second-best method, Multi-step Evaluation by 8.3% on WebShop. Moreover, we compared our method against the upper-bound scenario where the Actor always receives feedback after completing terminal actions without any critic involved. As depicted in Table 2, InferAct performs competitively, trailing by only 0.3% in Web-Shop and 2% in HotPotQA with binary feedback, while achieving equivalent performance in ALF-World. This competitive edge is attributed to two factors: InferAct consistently achieves high recall across all tasks. (Table 1) and there are many challenging cases that remain unsolved even with post-execution feedback. Figure 4 further illustrates that NL feedback significantly boosts the Actor's performance over iterations when compared to binary feedback, highlighting the value of richer, more informative feedback mechanisms in complex decision-making tasks."}, {"title": "Evaluation with High-Stake Actions", "content": "The overall evaluation presented in Section 5.1 does not consider the costs of adverse actions. In reality, high-stakes decisions may carry more significant consequences than low-stakes counterparts. Recognizing this, we specifically explore the performance of InferAct and other methods using GPT-4-turbo under high-stakes conditions. Specifically in WebShop, we mimic costly decisions by considering the purchases with prices exceeding $60, representing the top one-third (66.6th percentile) of prices within the dataset. For ALFWorld, actions such as Heat and Cool are considered high-stakes considering their irreversible impact on the physical state of objects. For HotPotQA, it is not intuitive to mimic a costly setting.\nFurthermore, to quantitatively assess the implications of errors, we consider the cost metric, which measures the negative impact of incorrect decisions (false negatives). In WebShop, this involves calculating the price associated with incorrectly selected products, while for ALFWorld, we count the number of misoperations. This metric complements conventional evaluations such as F1-score, rendering a comprehensive view of the performance of these critics. To enhance the critics' sensitivity to risks, we integrate risk-aware prompts (refer to Appendix A.4). Table 3 reaffirms the efficacy of InferAct; with the risk-aware prompt, InferAct achieves the best performance in all metrics. In ALFWorld, however, the addition of the risk-aware prompt does not alter the performance, indicating that all methods are insensitive to this feature. In WebShop, although adding a risk-aware prompt might not always lead to a higher F1-score, it effectively reduces the costs associated with undetected reverse actions for all evaluated critics. This is exemplified by both multi-step evaluation and the standard evaluation method, where the precision deteriorates while the cost is reduced. As shown in Figure 5, more cases are predicted as positive after integrating the risk-aware prompt. This means these methods tend to be more cautious about expensive purchases. For InferAct, although the recall and precision remain unchanged, the cost also decreased."}, {"title": "Conclusion", "content": "Performing real-time evaluation over the reasoning process of LLM agents before executing costly or irreversible actions is crucial for deploying such models to many real-life applications, which, however, is significantly understudied. This paper proposes InferAct, built on the Theory-of-Mind abilities of LLMs, aiming to proactively assess the risk and alert humans when needed, thereby mitigating or preventing negative outcomes before they occur. Experiments demonstrate the superior performance of InferAct across different environments and the benefit of human feedback. Further findings in high-stake setting reveal that when equipped with the risk-aware prompt, InferAct improved its robustness and behaved more cautiously in facing costly decisions, consequently reducing the risk and expense of incorrect decisions. This makes InferAct a valuable tool for LLM agents in applications. InferAct sets baselines for further research that emphasizes proactively guiding LLM agents in order to develop trustworthy systems."}, {"title": "Limitations", "content": "Despite the efficacy of InferAct in preemptive adverse action detection for LLM agents, there are several limitations that warrant mention and provide avenues for future research. First, as InferAct leverages the ToM ability of LLMs, the smaller LLMs may exhibit suboptimal performance in comparison to their larger counterparts due to limitations in their ToM and instruction-following abilities.\nSecond, the scope of our high-stakes experiments is currently confined to simulations within online shopping and household environments. This limited scope may not adequately capture the complexity of high-stakes scenarios in other critical fields such as healthcare and finance. For instance, risk measurement in finance (Tarantino, 2010) involves multifaceted variables and interactions that are significantly more complex than the cost metric used in our study. Developing effective preemptive evaluation approaches to enhance the safety of LLM-based Agents within different fields is an imperative direction. Additionally, our focus was on immediate and direct consequences of critical actions, without delving into the long-term and indirect effects that may hold substantial importance (Lindner et al., 2021).\nThird, while we demonstrate the effectiveness of InferAct in integrating binary and natural language feedback to enhance agents' safer and more accurate reasoning, the natural language feedback presents inherent variability due to individual differences in expression and language proficiency. Investigating how such variability influences the interpretation and subsequent actions of LLM agents is an interesting topic for future research."}]}