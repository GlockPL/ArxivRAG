{"title": "SKILL EXPANSION AND COMPOSITION IN PARAMETER SPACE", "authors": ["Tenglong Liu", "Jianxiong Li", "Yinan Zheng", "Haoyi Niu", "Yixing Lan", "Xin Xu", "Xianyuan Zhan"], "abstract": "Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: https://ltlhuuu.github.io/PSEC/.", "sections": [{"title": "INTRODUCTION", "content": "Humans excel at using existing skills and knowledge to tackle new tasks efficiently, while continually evolving their capabilities to rapidly adapt to new tasks. This fundamental approach to problem-solving highlights a key aspect of human intelligence that is equally crucial for autonomous agents. However, most current decision-making algorithms adhere to a tabula rasa paradigm, where they are trained from scratch without utilizing any prior knowledge or resources, leading to severe sample inefficiency and elevated cost when the agent encounters new tasks. Therefore, in this paper, we aim to explore the capability of autonomous agents to leverage and expand upon their existing knowledge base in novel situations to enhance learning efficiency and adaptability.\nWhile some existing studies, such as continual learning, compositional policies, or finetuning-based methods, aim to replicate this process, they jointly failed to tackle several key limitations. 1) Catastrophic forgetting: these approaches typically lack a fundamental mechanism to guarantee continuous improvement when acquiring new skills, making the autonomous agents very susceptible to overfitting on new tasks while forgetting previously learned skills without proper regularization; 2) Limited efficiency in learning new tasks: Some methods avoid the catastrophic forgetting problem by adopting a parameter-isolation"}, {"title": "RELATED WORKS", "content": "Compositional Policies. Some previous methods try to leverage prior knowledge relying on pretrained primitive policies. More specifically, these methods used compositional networks in a hierarchical structure to adaptively compose primitives to form complex behaviors. However, their expressiveness is limited by the expressiveness of simple Gaussian primitives. Recently, due to the strong expressiveness of the diffusion models and its inherent connection with Energy-Based Models, many compositional policies have been approached by diffusion model. Diffusion models learn the gradient fields of an implicit energy function, which can be combined at inference time to generalize to new complex distribution readily. However, these approaches rely on independently trained policies with fixed combination weights, which lack the flexibility to adapt to complex scenarios. Moreover, most previous methods can only combine skills after the policy distribution generation of each skill. Therefore, they fail to fully utilize the shared features of different skills to achieve optimal compositions. We systematically investigate the advantages of skill composition within the parameter space, and compose skills in a context-aware manner with each skill modeled as a diffusion model. This ensures both flexibility and expressiveness in composing complex behaviors.\nContinual Learning for Decision Making. Current continual learning methods for decision making, including continual reinforcement learning (RL) and imitation learning (IL), primarily focus on mitigating catastrophic forgetting of prior knowledge when learning new tasks. They can be roughly classified into three categories: structure-based, regularization-based, and rehearsal-based methods. Different from previous continual RL and IL approaches, our study focuses on leveraging existing skills to facilitate efficient new task learning and enables the extension of skill sets. In addition, it naturally solves the catastrophic forgetting challenge due to the parameter isolation induced by the LoRA module, directly bypassing the key challenges of existing continual learning methods."}, {"title": "METHODS", "content": "We propose PSEC, a generic framework that can efficiently reuse prior knowledge and self-evolve to address emerging new tasks. Next, we will elaborate on our problem setup and technical details."}, {"title": "PRELIMINARY", "content": "Diffusion Model for Policy Modeling. Recently, diffusion models have become popular for policy modeling because of their superior expressiveness to model complex distributions. Considering a policy distribution \\(\\pi(a|s)\\) and a sample (s, a) drawn from an empirical dataset D of \\(\\pi(a|s)\\), the diffusion process progressively introduces Gaussian noise to the sample over T steps, producing a sequence of noisy samples a0, a1, ..., aT with a0 = a following the forward Gaussian kernel:\n\\(q(a_t|a_{t-1}) = N(a_t; \\sqrt{1 - \\beta_t}a_{t-1}, \\beta_tI),\\)\n\\(q(a_t|a_0) = N(a_t; \\sqrt{\\bar{p_t}}a_0, (1 - \\bar{p_t})I),\\) (1)\nwhere \\(\\bar{p_t} := \\prod_{t'=1}^t (1 - \\beta_{t'})\\), \\(P_t = \\prod_{t=1}^T P_t\\), and the noise is controlled by a variance schedule \\(\\beta_1, ..., \\beta_T\\) to ensure \\(p(a_T) = N(0, I)\\). The denoise process aims to recover the sample from \\(p(a_T)\\) by learning a conditional distribution \\(p_\\theta(a_{t-1}|a_t, s)\\). The policy \\(\\pi_\\theta(a|s)\\) is typically modeled as:\n\\(\\pi_\\theta(a|s) = p(a_T) \\prod_{t=1}^T [p_\\theta(a_{t-1}|a_t, s); p_\\theta(a_{t-1}|a_t, s) = N(a_{t-1}; \\mu_\\theta(a_t, t, s), \\Sigma_\\theta(a_t,t,s)),\\) (2)\nwhere \\(\\Sigma_\\theta = \\beta_tI\\) is set as untrained time-dependent constants and \\(p_\\theta(a_t,t,s) = \\mu_\\theta(a_t, t, s) + \\epsilon_\\theta(a_t,t,s)\\) is reparameterized by \\(\\theta\\). The trainable parameter \\(\\theta\\), modeled by deep networks, can be optimized via minimizing the following objective by predicting the noise:\n\\(L_{diff}(\\theta) = \\mathbb{E}_{t\\sim U, \\epsilon \\sim N(0,1), (s, a) \\sim D} [w(s, a) \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{p_t}} a + \\sqrt{1 - \\bar{p_t}} \\epsilon, t, s)\\|^2],\\) (3)\nwhere U is uniform distribution over the discrete set {1, ..., T}. w(s, a) is a flexible weight function that encodes human preference. For example, \\(w(s, a) \\propto f(A(s,a)), f \\geq 0\\) with A(s, a) as the advantage function leads to weighted behavior cloning (BC) in offline reinforcement learning (RL), and w(s, a) := 1 degenerates to traditional BC. After obtaining the approximated \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\), we can substitute them into Eq. (2) to iteratively denoise and obtain actions conditioned on the state.\nProblem Setups. We consider a Markov Decision Process with \\(s \\in S\\) and \\(a \\in A\\) are state and action space, \\(P: S \\times A \\rightarrow \\Delta(S)\\) is transition dynamics, and \\(r : S \\times A \\rightarrow \\mathbb{R}\\) is reward function. We assume"}, {"title": "EFFICIENT POLICY EXPANSION VIA LOW-RANK ADAPTATION", "content": "For the first objective, previous methods typically train each primitive from scratch in a tabula rasa paradigm, failed to leverage the prior knowledge in \\(\\Pi\\) to efficiently obtain a good skill primitive. This presents significant issues in terms of computational efficiency when the number of skills grows. To mitigate these challenges, we turn to Parameter-Efficient Fine-Tuning (PEFT), which has proven highly effective in various natural language processing and computer vision applications. One of the most popular PEFT implementations is LoRA. It injects trainable low-rank decomposed matrices into the pretrained layer to avoid overfitting with limited adaptation data and significantly reduces computational and memory burden. Inspired by this, we try to employ LoRA to efficiently learn new skills given solely limited data for the target skill.\nPolicy Expansion via Low-Rank Adaptation. We consider a pretrained policy \\(\\pi_0\\) and denote \\(W_0 \\in \\mathbb{R}^{d_{in} \\times d_{out}}\\) as its associated weight matrix. Directly finetuning \\(W_0\\) to adapt to new skills might be extremely inefficient, instead, we introduce a tune-able LoRA module \\(\\Delta W\\) upon \\(W_0\\), i.e., \\(W_0 + \\Delta W = W_0 + BA\\) to do the adaptation and keep \\(W_0\\) frozen, where \\(B \\in \\mathbb{R}^{d_{in} \\times n}\\), \\(A \\in \\mathbb{R}^{n \\times d_{out}}\\) and \\(n \\ll min(d_{in}, d_{out})\\). Specifically, he input feature of the linear layer is denoted as \\(h_{in} \\in \\mathbb{R}^{d_{in}}\\), and the output feature of the linear layer is \\(h_{out} \\in \\mathbb{R}^{d_{out}}\\), the final output of a LoRA augmented layer can be calculated through the following forward process:\n\\(h_{out} = (W_0 + \\alpha \\Delta W)h_{in} = (W_0 + BA)h_{in} = W_0h_{in} + \\alpha BAh_{in},\\) (5)\nwhere \\(\\alpha\\) is a weight to balance the pre-trained model and LoRA modules. This operation naturally prevents catastrophic forgetting in a parameter isolation approach, and the low-rank decomposition structure of A and B significantly reduces the computational burden. Benefiting from this lightweight characteristic, we can manage numerous LoRA modules \\({\\Delta W_i = B_iA_i | i \\in 1, 2, ..., k}\\) to encode different skill primitives \\(\\pi_i\\), respectively, as shown in Figure 2a. This flexible approach allows us to easily integrate new skills based on existing knowledge, while also facilitating library"}, {"title": "CONTEXT-AWARE COMPOSITION IN PARAMETER SPACE", "content": "Effectively combining skills encoded as different LoRA modules to solve new tasks is crucial. Previous methods typically rely on fixed combinations of skills, resulting in limited compositional flexibility. This approach may be acceptable in static domains like language models, but it falls short in decision-making applications where dynamic skill composition is crucial. For example, in autonomous driving, the ability to dynamically prioritize skills of obstacle avoidance in potential collision scenarios, or acceleration when speeds are suboptimal, is essential. Naively adopting a fixed set of \\(\\alpha_i\\) like previous approaches, however, cannot adequately support such flexible deployment of skills based on real-time environmental demands.\nContext-aware Composition. We propose a simple yet effective context-aware composition method that adaptively leverages pretrained knowledge to optimally address the encountering tasks according to the agent's current context. Specifically, we introduce a context-aware modular \\(\\alpha(s; \\theta) \\in \\mathbb{R}^k\\) with \\(\\alpha_i\\) as its i-th dimension. The composition method can be expressed by Eq. (7):\n\\(W(\\theta) = W_0 + \\Sigma_{i=1}^k \\alpha_i(s; \\theta)\\Delta W_i = W_0 + \\Sigma_{i=1}^k \\alpha_i(s; \\theta) B_iA_i\\) (7)\nHere, \\(\\alpha(s; \\theta)\\) adaptively adjusts output weights based on the agent's current situation s with the parameter \\(\\theta\\) optimized via minimizing the diffusion loss in Eq. (3). Note that the trainable parameter \\(\\theta\\) lies solely in the composition network \\(\\alpha_\\theta\\) with the pretrained weights \\(W_0\\) and all LoRA modules \\(\\Delta W_i\\) being kept frozen, thus \\(\\theta\\) can be efficiently trained in terms of both samples and parameters.\nParameter-level v.s. Action-level Composition. Careful readers may notice that our context-aware composition is similar to previous works that adaptively compose Gaussian primitive skills to create complex behaviors, such as the one shown in Eq. (8)"}, {"title": "DYNAMICS SHIFT SETTING", "content": "We evaluate PSEC in another practical setting to further validate its versatility, where the dynamics P shift to encompass diverse scenarios such as cross-embodiment, sim-to-real transfer, and policy learning in non-stationary environments.\nSetup. We evaluate on the D4RL environments, where we modify the dynamics and morphology of locomotive robots to reflect the dynamic changes. Specifically, we first pretrain \\(\\pi_0\\) using a dataset \\(D^{P_0}\\) collected from a modified dynamics \\(P_0\\) and then equip it with a new small dataset \\(D^{P_1}\\) collected under the original D4RL dynamics \\(P_1\\). Friction, Thigh Size and Gravity denote \\(P_0\\) modifies the friction condition, the thigh size of cheetah/walker, and the gravity respectively. Based on the new small dataset \\(D^{P_1}\\), we set \\(w(s, a) = \\exp(A^*(s, a))\\) with \\(A^*(s, a)\\) as the advantage function trained by expectile regression on \\(D^{P_1}\\) to obtain a new policy \\(\\pi_1\\) and then optimize the context-aware composition network \\(\\alpha(s; \\theta)\\) to combine \\(\\pi_{0,1}\\) to collaboratively work under dynamics \\(P_1\\). See Appendix C.3 for details.\nBaselines. One branch of baselines consists in training \\(\\pi_1\\) from scratch on the small dataset \\(D^{P_1}\\), which may face data scarcity challenges, including BC, offline RL methods like CQL, and IQL, MOPO. In addition, we evaluate some gen-"}, {"title": "CONCLUSION", "content": "We propose PSEC, a framework that handles different skills as plug-and-play LoRA modules within an expandable skill library. This flexible approach enables the agents to reuse prior knowledge for efficient new skill acquisition and to progressively evolve in response to new challenges like humans. By exploiting the interpolation property of LoRA, we propose a context-aware compositional network that adaptively activates and blends different skills directly in the parameter space by merging the corresponding LoRA modules. This parameter-level composition enables the exploitation of more shared and complementary information across different skills, allowing for optimal compositions that collaboratively generate complex behaviors in dynamical environments. PSEC demonstrates exceptional effectiveness across diverse practical applications, such as multi-objective composition, continual policy shift and dynamic shift settings, making it highly versatile for real-world scenarios where knowledge reuse and monotonic policy improvements are crucial. One limitation is the pretrained policy \\(\\pi_0\\) may encompass diverse distributions to ensure good LoRA tuning. However, this can be mitigated by utilizing the broad out-of-domain dataset to enhance distribution coverage. More discussions on limitations and future works can be found in Appendix A."}, {"title": "LIMITATIONS AND FUTURE WORKS", "content": "In this section, we provide detailed discussions about the limitations and their potential solutions.\nAssumption on the expressiveness of the pretrain policy. The main limitation of PSEC is the assumption that the pre-trained \\(\\pi_0\\) covers a diverse distribution, which allows for efficient fine-tuning using small add-on LoRA modules. If this assumption does not hold, learning new skills through parameter-efficient fine-tuning may prove challenging, as significantly more parameters might be required to acquire new skills.\nPotential solutions: Note that this assumption is mild in relevant papers that utilize LORA to learn new skills. To tackle this problem, one straightforward solution is to increase the value of LoRA ranks to increase the learning capabilities of the newly introduced modules. Another simple solution is to leverage the cheap and abundant out-of-domain data to enhance the distribution coverage of the pretrained \\(\\pi_0\\) to enable efficient LoRA adaptations.\nRedundant skill expansion. In this paper, PSEC includes policies for all tasks in the skill library across its lifelong time. Although we adopt LoRA to reduce computational burden and memory usage, maintaining an extensive library of skill primitives may still lead to substantial computational costs.\nPotential solutions: Note that not all skills should be incorporated into the skill library, particularly those that are redundant and can be synthesized from other primitives. An interesting direction for future research is to develop an evaluation metric to assess the interconnections between different skills, such as the skill diversity, to only include essential, non-composable atomic primitives. Such a strategy could significantly reduce the management costs associated with maintaining the skill library.\nHyperparameter-tuning: Another limitation is PSEC introduces another LoRA modules to learn new skills, which can introduce additional hyperparameters required to be tuned.\nPotential solutions: This limitation is widely existed in relevant works that try to reuse prior knowledge to learn new skills, since almost all papers require additional parameters or regularization to adapt to the new skills. In this paper, we have ablated the robustness of PSEC against varied LoRA ranks, and demonstrate consistent superiority over the naive MLP modules in Figure 8, highlighting the robustness of PSEC for hyperparameter tuning.\nSimple context-aware compositional modular: We employ a simple context-aware modular \\(\\alpha(s; \\theta)\\) to dynamically combine different primitives. This operation is simple and may not fully leverage the shared structure across skills for the target task.\nPotential Solutions: However, in our paper, we have demonstrated the superior advantages of this simple context-aware modular, as shown in Figure 6c. One interesting future direction is to adopt a more advanced model architecture, training objective, or more flexible gating approach to optimize the modular."}, {"title": "DISCUSSIONS ON MORE RELATED WORKS", "content": "Tabula Rasa. Tabula rasa learning is one popular paradigm for diverse existing decision-making applications, such as robotics and games. It directly learns policies from scratch without the assistance of any prior knowledge. However, it suffers from notable drawbacks related to poor sample efficiency and constraints on the complexity of skills an agent can acquire.\nFinetune-based Methods. Some finetune-based methods aim to accelerate policy learning by leveraging prior knowledge. This knowledge may come from pretrained policy or offline data, such as Offline-to-online RL and transfer RL. Some methods maintain a policy library that contains pretrained policies and adaptively selects one policy from this set to assist policy training"}, {"title": "EXPERIMENTAL SETUPS", "content": "MULTI-OBJECTIVE COMPOSITION\nTraining details of PSEC. In this setting, we have four networks required to train: the behavior policy \\(\\pi_0\\), the safety policy \\(\\pi_1\\) that minimizes the cost, the reward policy \\(\\pi_2\\) that maximizes the return, and the context-aware modular \\(\\alpha(s; \\theta) \\in \\mathbb{R}^2\\). For each task, we first pretrain \\(\\pi_0\\) parameterized by \\(W_0\\) as behavior policy by minimizing the following objective on the full DSRL dataset D to ensure a diverse pretrained distribution coverage:\n\\(L_{\\pi_0}(W_0) = \\mathbb{E}_{t\\sim U, \\epsilon \\sim N(0,I), (s, a) \\sim D} [\\|\\epsilon - \\epsilon_{W_0}(\\sqrt{\\bar{p_t}} a + \\sqrt{1 - \\bar{p_t}} \\epsilon, t, s)\\|^2].\\) (10)\nThen, we equip the agent with the same dataset D but provide feasible label h and reward labels r, forming the dataset \\(D_h = \\{(s,a,h,s')\\}\\) and \\(D_r = \\{(s, a, r, s')\\}\\). Then we train \\(\\pi_1\\) and \\(\\pi_2\\) based on these datasets by optimizing their newly introduced LoRA modules \\(\\Delta W_1\\) and \\(\\Delta W_2\\) via minimizing the following objectives in Eq. (11-12):\n\\(L_{\\pi_1}(\\Delta W_1) = \\mathbb{E}_{t\\sim U, \\epsilon \\sim N(0,I), (s,a)\\sim D_h} [w^h(s,a) \\|\\epsilon - \\epsilon_{\\Delta W_1}(\\sqrt{\\bar{p_t}} a + \\sqrt{1 - \\bar{p_t}} \\epsilon, t, s)\\|^2],\\) (11)\n\\(L_{\\pi_2}(\\Delta W_2) = \\mathbb{E}_{t\\sim U, \\epsilon \\sim N(0,I), (s,a)\\sim D_r} [w^r(s,a) \\|\\epsilon - \\epsilon_{\\Delta W_2}(\\sqrt{\\bar{p_t}} a + \\sqrt{1 - \\bar{p_t}} \\epsilon, t, s)\\|^2],\\) (12)\nTo evaluate PSEC's ability to continually evolving its capabilities when tackling new challenges, we conduct experiments on DeepMind Control Suite (DMC), where a walker"}, {"title": "T-SNE EXPERIMENTAL SETUPS FOR FIGURE 4", "content": "To provide empirical support of the advantages of parameter-level composition over other levels of composition, we visualize the t-SNE projection of data samples in different spaces. Specifically, for each dataset \\(D^{T_0}\\), \\(D^{T_1}\\), \\(D^{T_2}\\) in the continual policy shift setting in Section C.2, we randomly sample 512 data samples (s, a), which forms three types of data that encode the standing, walking and running skill, respectively. In the action space, we directly utilize t-SNE projection to map these sampled data into a 2-dimentional space in Figure 4 (c). For the noise space, we add 1 step of noise on the sampled actions following the forward diffusion process in Eq. (1) and get the tuple \\((s, a_t)\\) for different skills. Then, we generate the noise based on this noisy tuples and visualize their t-SNE projections in Figure 4 (b). In parameter-space, we feed the noisy tuples \\((s, a_t)\\) into the trained networks and get the output features of the middle LoRA augmented layers. Then, we project these features using t-SNE in Figure 4 (a)."}, {"title": "MORE EXPERIMENTAL RESULTS", "content": "THE EFFECTIVENESS OF THE CONTEXT-AWARE MODULAR\nContext-aware modular for the continual policy shift. To further explore the effectiveness of the context-aware module, we employ it to analyze the trajectories generated by policies composed using fixed compositional weights. Specifically, for the S\u2192R task in Section C.2, the fixed composition method denote \\(W_{run} = W_0 + \\Delta W_2\\), which uses a fixed a = 16 to compose \\(\\pi_0\\) and \\(\\pi_2\\). Figure 14 (a) shows that naively using fixed compositional weights might accidentally stuck in some local suboptimal behavior such as standing still or falling down. We can clearly observe that our context-aware modular provides corresponding responses to correct these undesired behaviors. Therefore, it is necessary to adjust the weights of different strategies to fit the current states. Figure 14 (b) presents the trajectories generated by PSEC. It clearly demonstrates that by utilizing the context-aware modular, the agent can make subtle adjustments between skills and stably run across the entire episodes.\nTHE PARAMETER EFFICIENCY OF PSEC\nParameter efficiency. To evaluate the parameter efficiency of PSEC, we compare its parameter count and performance on various tasks against both the Scratch method and PSEC (MLP). The parameter count for PSEC includes the LoRA parameters and context-aware parameters specific to the walker-walk or walker-run tasks. The Scratch method represents training the policy from scratch with standard MLP. PSEC (MLP), which substitutes the LoRA weights with a standard MLP and retains the context-aware modular, has a higher parameter count than the Scratch method. The parameter counts are illustrated in Figure 15. In terms of performance, the results from the Deep-Mind Control Suite (DMC) tasks, as shown in Figures 6 (b) and 12 (b), indicate that PSEC achieves significantly better performance despite having only 7.58% of the parameters used in the Scratch method. This performance advantage over both the Scratch method and PSEC (MLP) demonstrates that PSEC possesses strong parameter efficiency, effectively leveraging a smaller number of param-"}, {"title": "DESCRIPTION OF TASKS", "content": "We conduct experiments on 9 MetaDrive tasks and 8 Bullet-Safety-Gym tasks in the DSRL benchmark (Liu et al., 2023a). The visualization of the environments is shown in Figure 16. The tasks aim to learn policy from different level datasets such that the policy satisfies a safety constraint (normalized cost < 1) and achieves higher rewards.\nMetaDrive. It leverages the Panda3D game engine to simulate realistic driving scenarios. The tasks are categorized as {Road}{Vehicle}, where \u201cRoad\u201d encompasses three levels of difficulty for self-driving cars: easy, medium, and hard, while \"Vehicle\" represents four levels of surrounding traffic density: sparse, mean, and dense. In MetaDrive's autonomous driving tasks, costs are incurred from three safety-critical scenarios: (i) collision, (ii) out of road, and (iii) over-speed.\nBullet-Safety-Gym. The environments are built on the PyBullet physics simulator. They feature four types of agents: Ball, Car, Drone, and Ant, alongside two task types: Circle and Run. Tasks are designated as {Agent}{Task}, combining the agent and the corresponding task type."}, {"title": "ILLUSTRATION OF THE RECORDED DATA", "content": "To get a more intuitive look at the recorded data, we calculate the total reward and total cost for each trajectory in the datasets. These values are then plotted on a two-dimensional plane, where"}, {"title": "CONTINUAL LEARNING SETTING", "content": "Following Continual world and L2M, we split the 50 tasks into 40 pre-training tasks and 10 fine-tuning unseen tasks (CW10). The training datasets are the same as the datasets collected by L2M. We train 10K steps per task in CW10, which is only 10% training steps of L2M, with a batch size of 1024. After every 10K update steps, we switch to the next task in the sequence. Then we evaluate it on all tasks in the task sequence. The results are shown in Table 3 and Table 4. We compare the performance of PSEC with L2M and other strong baselines. Thanks to the efficiency of skill composition in parameter space, PSEC can"}]}