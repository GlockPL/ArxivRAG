{"title": "Expose Before You Defend: Unifying and Enhancing Backdoor Defenses via Exposed Models", "authors": ["Yige Li", "Hanxun Huang", "Jiaming Zhang", "Xingjun Ma", "Yu-Gang Jiang"], "abstract": "Backdoor attacks covertly implant triggers into deep neural networks (DNNs) by poisoning a small portion of the training data with pre-designed backdoor triggers. This vulnerability is exacerbated in the era of large models, where extensive (pre-)training on web-crawled datasets is susceptible to compromise. In this paper, we introduce a novel two-step defense framework named Expose Before You Defend (EBYD). EBYD unifies existing backdoor defense methods into a comprehensive defense system with enhanced performance. Specifically, EBYD first exposes the backdoor functionality in the backdoored model through a model preprocessing step called backdoor exposure, and then applies detection and removal methods to the exposed model to identify and eliminate the backdoor features. In the first step of backdoor exposure, we propose a novel technique called Clean Unlearning (CUL), which proactively unlearns clean features from the backdoored model to reveal the hidden backdoor features. We also explore various model editing/modification techniques for backdoor exposure, including fine-tuning, model sparsification, and weight perturbation. Using EBYD, we conduct extensive experiments on 10 image attacks and 6 text attacks across 2 vision datasets (CIFAR-10 and an ImageNet subset) and 4 language datasets (SST-2, IMDB, Twitter, and AG's News). The results demonstrate the importance of backdoor exposure for backdoor defense, showing that the exposed models can significantly benefit a range of downstream defense tasks, including backdoor label detection, backdoor trigger recovery, backdoor model detection, and backdoor removal. More im-portantly, with backdoor exposure, our EBYD framework can effectively integrate existing backdoor defense methods into a comprehensive and unified defense system. We hope our work could inspire more research in developing advanced defense frameworks with exposed models.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) trained on large-scale datasets have demonstrated remarkable performance in addressing complex real-world problems across various domains, including computer vision (CV) [1], [2] and natural language processing (NLP) [3], [4]. However, recent studies have shown that DNNs are vulnerable to backdoor attacks [5], [6], which insert malicious triggers into the model parameters to com-promise its test-time predictions. Specifically, these attacks establish a covert correlation between a predefined trigger pattern and an adversary-specified target label by poisoning a small subset of the training data. A backdoored model maintains normal performance on clean inputs but consis-tently misclassifies inputs containing the trigger pattern to the target label. Importantly, backdoor attacks are not limited to a specific domain; they can compromise both vision and language models. For instance, in the image domain, attackers may manipulate a few pixels or embed specific patterns, while in the text domain, they might incorporate particular words or syntactic structures to trigger malicious behavior. With the proliferation and accessibility of pre-trained vision and language models from platforms like Hugging Face [7], ensuring the secure and backdoor-free deployment of these models in downstream applications has become increasingly critical.\nExisting defense methods against backdoor attacks can be broadly categorized into two types: detection methods and removal methods. Detection methods identify the existence of a backdoor attack (i.e., trigger) in a trained model (a task known as backdoor model detection) or in a training/test sample (a task known as backdoor sample detection). Both tasks involve inverting the trigger pattern used by the attack and identifying the targeted class of the attacker [5], [8], [9]. Arguably, the ultimate goal of backdoor defense is to com-pletely eliminate the backdoor trigger from a compromised model. This objective lies at the core of backdoor removal methods using techniques such as fine-tuning, pruning [10], [11], or knowledge distillation [12].\nWhile both backdoor detection and removal methods have shown promising results, they have been applied indepen-dently, without benefiting from each other. For example, trig-ger inversion methods often struggle to identify the backdoor class and thus have to assume it is known to the defender, while backdoor removal methods cannot pinpoint the exact trigger pattern and backdoor class. Moreover, both types of methods exhibit performance limitations against several ad-vanced attacks. To date, a unified defense framework capable of effectively detecting and removing all types of backdoor at-tacks remains absent from the current literature. Additionally, none of the existing defense techniques have demonstrated effectiveness against both image and text backdoor attacks.\nIn this work, we aim to address the limitations of existing defenses by drawing inspiration from the ancient wisdom: \"A known enemy is easier to defeat\". Intuitively, if we could expose the backdoor within a compromised model through a specialized model preprocessing/editing technique that iso-lates the backdoor functionality, the backdoor trigger would become much easier to detect, recover, and remove. This could potentially lead to a holistic defense framework against the backdoors. This approach is feasible due to the inherent nature of backdoors: the backdoor functionality injected into the victim model is specifically designed to be independent of its normal functionality (to avoid impacting the clean performance). This motivates us to propose the Expose Before You Defend (EBYD) framework. EBYD consists of two steps: 1) backdoor exposure, a preprocessing step that reveals the backdoor functionality in the compromised model, and 2) backdoor defense, which applies existing detection and removal techniques to the preprocessed (exposed) model to enhance overall performance.\nIn EBYD, backdoor exposure plays a crucial role in con-necting and enhancing different defense techniques. However, decoupling and exposing the backdoor functionality from a compromised model is a challenging task, as evidenced by the shortcomings of current defense methods [13], [14]. To address this, we propose a novel technique called Clean Unlearning (CUL), which exposes backdoor functionality by unlearning the clean functionality from the backdoored model rather than directly searching for backdoor features. Intuitively, a model can be effectively unlearned by maximizing its error on a few clean samples. Although this type of lightweight unlearning may be partial, it is sufficient to inhibit the clean functionality of the model for the purpose of backdoor exposure. Following this, we conduct a comprehensive exploration of possible model preprocessing techniques, including fine-tuning, model sparsification, and weight perturbation. We demonstrate that these techniques can also expose the backdoor functionality in a compromised model.\nIn our EBYD framework, the exposed model provides a better starting point for all subsequent defenses. It not only enhances existing backdoor removal methods but also unifies various backdoor defense tasks, including trigger inversion, backdoor label detection, and backdoor sample detection. For instance, when combined with Neural Cleanse (NC) [8], one of the most effective methods for trigger inversion and backdoor model detection, EBYD not only improves NC's detection rate but also facilitates the identification of the backdoor label (class). Similarly, when integrated with STRIP [15], a well-established method for backdoor sample detection, EBYD enables the detection of backdoor samples that are significantly more complex and stealthy than traditional attacks. Moreover, the backdoor-exposed model enhances the effectiveness of existing backdoor removal methods [10], [11], [16], elevating their performance to a higher level.\nMore importantly, we demonstrate that EBYD can be ex-tended to language models to defend against a wide range of textual backdoor attacks. As such, EBYD serves as a unifying framework that integrates various defense methods, enabling independent strategies like backdoor detection, trigger recov-ery, and backdoor removal to collaborate and contribute to a comprehensive defense system. With EBYD, we conduct the most extensive defense evaluation to date, defending against 10 image attacks and 6 text attacks. Empirical results across two image datasets (CIFAR-10 and an ImageNet subset) and four text datasets (SST-2, IMDB, Twitter, and AG's News), employing various model architectures, demonstrate that our EBYD defense framework achieves significant performance improvements over current state-of-the-art (SOTA) methods."}, {"title": "II. RELATED WORK", "content": "A backdoor attack aims to implant a malicious trigger into the victim models at training time by poisoning a small proportion of the training samples with a carefully crafted trigger pattern. After training on the poisoned data, the trig-ger pattern becomes strongly correlated with the backdoor target class. Depending on the adversary's capabilities and design of the trigger pattern, existing backdoor attacks can be broadly categorized into data-poisoning attacks and training-manipulation attacks. In data-poisoning attacks, the adver-sary injects a pre-defined trigger pattern into a small proportion of the training data to trick the model into learning the connection between the trigger pattern and a backdoor label [17]. The trigger pattern can be relatively simple, such as a single pixel [18], a black-and-white square [19], random noise [20] or more complex patterns such as adversarial perturbation [21], and input-aware patterns [22]. On the other hand, training-manipulation attacks directly manipulate the training procedure to optimize for the backdoor objective in the feature space, using techniques such as feature collision [23] or by directly modifying model parameters via weight perturbation [24].\nAdditionally, textual backdoor attacks leverage training data poisoning with various types of triggers. These include rare or meaningless words, such as 'cf' [25], and syntactic structure manipulation [26]. More recent approaches aim to design sophisticated triggers using techniques like layer-wise poison-ing [27] and constrained optimization [28], enhancing both attack effectiveness and stealthiness. All of these methods have demonstrated significant success and continue to challenge existing defense mechanisms.\nNumerous approaches have been proposed to defend DNNs against backdoor attacks, among which backdoor detection and backdoor removal methods are the two most prevalent strategies.\nBackdoor Detection. Several detection methods identify backdoors based on the prediction bias observed in different input examples [29] or the statistical deviation in the feature space [18], [30]. More effective detection methods leverage reverse engineering techniques to recover the trigger pattern and then identify the backdoor label by anomaly detection [8], [9]. One representative method is Neural Cleanse (NC) [8], which recovers trigger patterns that can alter the model's predictions with minimum perturbation. Other methods focus on detecting backdoored samples at inference time, such as the STRIP method [15]. Numerous detection methods have been proposed in the NLP domain to identify potential trigger words by analyzing their influence on model outputs [31], [32].\nBackdoor Removal. Backdoor removal methods aim to erase backdoors from compromised models without signifi-cantly degrading their performance on clean samples. This line of work includes Fine-tuning, Fine-pruning [10], Mode Connectivity Repair [33], and Neural Attention Distillation (NAD) [12]. More recently, a training-time defense method called Anti-Backdoor Learning (ABL) [34] has been proposed to train clean models directly on backdoored data. Meanwhile, Adversarial Unlearning of Backdoors via Implicit Hypergra-dient (I-BAU) [35] is proposed to cleanse backdoored model with adversarial training. Adversarial Neuron Pruning (ANP) [36] prunes neurons that are more sensitive to adversarial perturbations to remove backdoors. The latest method, Re-constructive Neuron Pruning (RNP), has set a new state-of-the-art in defending against data-poisoning backdoor attacks [11]. The study conducted in RNP [11] shows that one can reveal backdoor-related features (neurons) by unlearning the model on a small portion of clean data. In NLP defense, the MF approach [37] mitigates backdoor learning by minimizing overfitting but struggles with attacks involving textual styles and grammatical patterns. Additionally, CUBE suggests that clustering in the feature space can help identify and remove backdoor samples, although this might impact the accuracy of clean tasks [38]. However, these methods lack generalizability and struggle to precisely expose the underlying backdoor behaviors, particularly the hidden triggers in language models. How to effectively reveal backdoor behaviors hidden in the language models is an open research problem that deserves more exploration.\nA set of understandings and assumptions regarding back-doors has developed during the process of backdoor attack and defense. We summarize these assumptions and highlight they are necessary for successful backdoor defense.\nBackdoor attack creates shortcuts in DNNs. The dis-tinctive behavior of the backdoored model on clean versus backdoor samples indicates the existence of neural short-cuts [9], [39] in backdoored models. These shortcuts have been found to be learned at an early stage of training at a much faster rate than normal features [34]. As a result, defenders can leverage this shortcut behavior to determine whether a model has been backdoored. One such method is the Neural Cleanse (NC) [8] which detects a backdoored model by searching a shortcut modification (i.e., the trigger pattern) of an arbitrary input toward a backdoor target label. This works reasonably well against attacks like BadNets [19], Blend [20], and Trojan [40]. However, revealing shortcuts becomes increasingly challenging for complex and dynamic attacks, such as sample-wise dynamic attacks [22] and WaNet [41]. In this case, simple shortcut discovery techniques like NC tend to fail as experimented in several existing works [22], [41]\u2013[43].\nBackdoor samples have anomaly output distributions. This understanding was established with the success of back-door sample detection methods like STRIP [15]. The distin-guishable differences in output distributions between clean and backdoor samples can be statistically characterized to build ac-curate detectors against simple backdoor attacks like BadNets [19], Blend [20], and Trojan [40]. For instance, STRIP detects"}, {"title": "III. PROPOSED EBYD FRAMEWORK", "content": "In this section, we start by describing the threat model, followed by a brief overview of our EBYD framework. We introduce the key steps of EBYD in the next two sections."}, {"title": "A. Threat Model", "content": "The threat model adopted in this work encompasses three common backdoor scenarios: untrusted datasets, model out-sourcing, and pre-trained models. In model outsourcing, devel-opers may use third-party platforms, such as Machine Learning as a Service (MLaaS) [45], due to limited technical ca\u0440\u0430-bilities or computational resources. Malicious attackers can exploit these platforms to manipulate training data, embedding backdoors into the model during training. This scenario is particularly vulnerable because attackers have full access to the training data, model, triggers, and training process, after which the compromised model is returned to the developers. Another attack vector involves pre-trained models [12]. Attackers may release pre-trained models with embedded backdoor triggers on model repositories (e.g., Hugging Face or GitHub). Victims may unknowingly download these models and use them for downstream tasks via transfer learning. Additionally, attackers might first infect a popular pre-trained model with a backdoor and then redistribute the modified model to repositories.\nFor backdoor defense, we assume that the defender has full access to the victim (potentially backdoored) model and a small set of clean data (approximately 1%) as defense data $D_d$ for backdoor exposure, model detection, or trigger removal. The defense data is assumed to be independent and identically distributed (i.i.d.) with the training and test data, which is a standard assumption in existing defenses."}, {"title": "B. Framework Overview", "content": "As illustrated in Fig. 1, our proposed EBYD is a two-step defense framework that first leverages a backdoor exposure method to reveal the backdoor functionality hidden in the model and then applies a detection or removal method to identify the backdoor class, reverse engineer the trigger, and finally remove the backdoor from the model. The detailed defense objectives of each step are outlined as follows:\nBackdoor Exposure. Given an unknown deep model (whether it's backdoored or clean), we leverage an expo-"}, {"title": "IV. BACKDOOR EXPOSURE", "content": "In this section, we introduce our proposed backdoor exp\u043e-sure method, Clean Unlearning (CUL), and several alternative techniques we explored in this paper. We then discuss the implications of each technique for uncovering the backdoor functionalities."}, {"title": "A. Clean Unlearning", "content": "Taking image classification task as an example, let $D = \\{(x_i, y_i)\\}_{i=1}^{n}$ represent the original training dataset, where $x \\in X$ represents a clean training image and $y_i \\in Y$ is its true label. The goal of a backdoor attack is to add a specific pattern or perturbation as the backdoor trigger $A$ on the original input sample $x$. The construction process of the triggered sample $x_b$ can be represented as: $x_b = x \\odot (1 \u2212 m) + \\Delta \\odot m$, where $\\odot$ denotes element-wise multiplication, and $m$ represents a non-zero image mask that controls the region where the trigger is added.\nOnce the backdoor triggers are implanted into the clean samples, the backdoored dataset can be represented as $D = D_c \\cup D_b$, where $D_c = \\{(x_c, y_c)\\}$ represents clean samples and their original labels, and $D_b = \\{(x_b, y_b)\\}$ represents triggered samples and their backdoor targeted labels. Training a back-doored model on $D$ can be formalized as:\n$\\arg \\min_{\\theta = \\theta_c \\cup \\theta_b}  E_{(x_c, y_c)\\in D_c}[L(f(x_c, y_c; \\theta_c))]$\\hspace{1cm}\n$\\text{clean task}$\\\n$\\qquad + E_{(x_b, y_b)\\in D_b}[L(f(x_b, y_b; \\theta_b))],$\n$\\text{backdoor task}$"}, {"content": "Taking image classification task as an example, let $D = \\{(x_i, y_i)\\}_{i=1}^{n}$ represent the original training dataset, where $x \\in X$ represents a clean training image and $y_i \\in Y$ is its true label. The goal of a backdoor attack is to add a specific pattern or perturbation as the backdoor trigger $A$ on the original input sample $x$. The construction process of the triggered sample $x_b$ can be represented as: $x_b = x \\odot (1 \u2212 m) + \\Delta \\odot m$, where $\\odot$ denotes element-wise multiplication, and $m$ represents a non-zero image mask that controls the region where the trigger is added.\nOnce the backdoor triggers are implanted into the clean samples, the backdoored dataset can be represented as $D = D_c \\cup D_b$, where $D_c = \\{(x_c, y_c)\\}$ represents clean samples and their original labels, and $D_b = \\{(x_b, y_b)\\}$ represents triggered samples and their backdoor targeted labels. Training a back-doored model on $D$ can be formalized as:\n$\\arg \\min_{\\theta = \\theta_c \\cup \\theta_b}  E_{(x_c, y_c)\\in D_c}[L(f(x_c, y_c; \\theta_c))]$\\hspace{1cm}\n$\\text{clean task}$\\\n$\\qquad + E_{(x_b, y_b)\\in D_b}[L(f(x_b, y_b; \\theta_b))],$\n$\\text{backdoor task}$"}, {"title": null, "content": "where $L$ is the classification loss (e.g., cross-entropy). Back-door learning can be viewed as a dual-task learning process that simultaneously optimizes the clean and backdoor tasks."}, {"title": null, "content": "Note that, although $\\theta = \\theta_c \\cup \\theta_b$, it does not mean $\\theta_c$ cannot overlap with $\\theta_b$, i.e., it is possible that $\\theta_c \\cap \\theta_b \\neq 0$.\nGiven a backdoored model $f(.; \\theta_c\\cup\\theta_b)$, the goal of back-door exposure is to reveal the backdoor functionality via an exposure function $\\Phi$:\n$\\Phi: f (.; \\theta_c\\cup\\theta_b) \\rightarrow f(.;\\theta_b).$"}, {"title": null, "content": "Since the defender does not know the poisoned samples, directly exposing the neurons associated with the backdoor functionality-referred to as backdoor neurons is infeasible. However, the defender possesses a small set of clean samples, termed defense data in our threat model, which can be used to defend the model. This leads us to approach backdoor exposure by suppressing or erasing the clean neurons identified by the defense data. Specifically, we design exposure strategies to maximize the model's classification loss on the clean parameters $\\theta_c$ while preserving the backdoor functionality on the backdoor parameters $\\theta_b$.\nTo achieve this, we introduce a simple yet effective back-door exposure technique called Clean Unlearning (CUL), which unlearns the clean features from the backdoored model to reveal the backdoor features. Our CUL method focuses on unlearning the model using specifically designed defense data. Intuitively, the clean features (or clean performance) can be unlearned regarding a particular task by maximizing its loss on data defining that task, which is the inverse of the training process. This approach leads us to solve the following maximization problem:\n$\\max_{\\theta_c} E_{(x_d, y_d)\\in D_d} ||L(f (x_d, y_d;\\theta_c\\cup\\theta_b)) - Y||,$"}, {"title": null, "content": "where $L$ is the cross-entropy loss, $|| ||$ denotes the absolute operator, $(x_d, y_d) \\in D_d$ are the clean defense samples, and $\\gamma$ is a pre-defined threshold used to prevent loss explosion due to gradient ascent.\nThe CUL method defined in Eq. (3) enables the model to unlearn the functionality defined by the samples in dataset $D_d$. In a backdoored model, this unlearning process forces the model to forget general clean features (e.g., 'cat' or 'dog') while preserving the backdoor-associated features. This is because backdoor attacks are often designed to be independent of the clean functionality, minimizing their impact on the model's clean performance to remain stealthy. More importantly, clean unlearning can be achieved very efficiently on a few clean samples.\nProperties of Exposed Models. Through the backdoor expo-sure achieved using our CUL method, we obtain a backdoor-exposed model. We identify two key properties of the exposed models as follows.\nProperty 1 (Backdoor Feature Dominance): The func-tionality of a backdoor-exposed model is dominated by the backdoor features.\nProperty 2 (Backdoor Label Consistency): A backdoor-exposed model consistently predicts the backdoor target label for any input samples."}, {"title": "B. Other Backdoor Exposure Techniques", "content": "Following Eq. (3), here we extend our exploration from CUL to existing fine-tuning, pruning, and weight perturbation techniques. We find that these techniques can also be effective when adapted for backdoor exposure.\nConfusion Fine-tuning: Previous studies have shown that backdoored models exhibit certain resilience against fine-tuning due to the inactivity of backdoor neurons when ex-posed to a small portion of clean defense samples [12]. This means that with careful control, we might be able to segregate the backdoor functionality via fine-tuning. This inspires us to propose a Confusion Fine-Tuning (CFT) method that uncovers backdoors by fine-tuning the model on a few mislabeled clean samples. Specifically, given a deliberately mislabeled dataset $(x_d, \\hat{y}_d) \\in D_d$ with modified labels $\\hat{y} = Random(1, 2, ... , K)$, where K is the total number of classes, the optimization objective for CFT can be formulated as:\n$\\min_{\\theta_c} E_{(x_d, \\hat{y}_d) \\in D_d} ||L(f(x_d, \\hat{y}_d; \\theta_c\\cup\\theta_b)) - Y||,$"}, {"content": "Following Eq. (3), here we extend our exploration from CUL to existing fine-tuning, pruning, and weight perturbation techniques. We find that these techniques can also be effective when adapted for backdoor exposure.\nConfusion Fine-tuning: Previous studies have shown that backdoored models exhibit certain resilience against fine-tuning due to the inactivity of backdoor neurons when ex-posed to a small portion of clean defense samples [12]. This means that with careful control, we might be able to segregate the backdoor functionality via fine-tuning. This inspires us to propose a Confusion Fine-Tuning (CFT) method that uncovers backdoors by fine-tuning the model on a few mislabeled clean samples. Specifically, given a deliberately mislabeled dataset $(x_d, \\hat{y}_d) \\in D_d$ with modified labels $\\hat{y} = Random(1, 2, ... , K)$, where K is the total number of classes, the optimization objective for CFT can be formulated as:\n$\\min_{\\theta_c} E_{(x_d, \\hat{y}_d) \\in D_d} ||L(f(x_d, \\hat{y}_d; \\theta_c\\cup\\theta_b)) - Y||,$"}, {"title": null, "content": "where $\\theta$ represents the parameters of model f, and L denotes the cross-entropy loss. Following the above formulation, we will show that CFT can also erase the clean functionality while preserving the backdoor functionality.\nModel Sparsification via Pruning: Model pruning aims to extract a sparse sub-network from the original dense net-work without degrading the model's performance. We denote $m^l \\in \\{0, 1\\}^d$ as a binary mask applied to $\\theta$ to indicate the locations of pruned weights (represented by zeros in $m^l$) and unpruned weights (represented by non-zeros in $m^l$). To expose the backdoor functionality, we 1) first initial $m^l$ to be all ones and then update $m^l$ on the clean subset, and then 2) iteratively prune the neurons from the model to obtain a sparse model, i.e., $\\theta_c = (m \\odot \\theta)$, which is defined as:\n$\\max_{\\theta_c} E_{(x_d,y_d)\\in D_d} ||L(f((x_d, y_d); \\hat{\\theta_c}\\cup\\theta_b)) - Y||,$"}, {"title": null, "content": "where the top-n values in $m^l$ are initialized to be zeros and used to remove the clean neurons. Therefore, a value close to 0 in the final mask indicates the pruned clean neurons, while a value close to 1 indicates the remaining backdoor-related neurons. We observe that as the pruning rate increases, there exists a pruned model with a very high ASR and low CA."}, {"title": null, "content": "Adversarial Weight Perturbation (AWP): AWP was ini-tially proposed for adversarial training [36]. It improves the robust generalization of adversarial training by smoothing the loss landscape of the model. Here, we adapt AWP to expose backdoor neurons from a backdoored model. We adversarially perturb the model weight parameters using AWP to maximize the model's loss on the clean defense data. Formally, the perturbations on the model parameters can be defined as follows:\n$\\max_{\\theta_c}  E_{(x_d, y_d)\\in D_d} ||L(f((x_d, y_d); \\hat{\\theta_c} \\cup \\theta_b)) - Y||,$"}, {"title": null, "content": "where $\\hat{\\theta_c} = (1 + \\delta) \\odot \\theta_c$, $\\delta$ represents the perturbation to the model weight $\\theta$, and $L$ denotes the cross-entropy loss. We optimize the neuron perturbations $\\delta$ to increase the loss on the clean data $(x_d, y_d) \\in D_d$. Interestingly, we find that if the perturbation is well-balanced, it can effectively reduce the CA while maintaining a very high ASR on backdoor samples. The lower CA and almost unchanged ASR indicate successful backdoor exposure, as the functionality of the backdoor behavior is preserved."}, {"title": "C. Measuring Backdoor Exposure", "content": "To quantitatively assess different exposure methods, here we introduce a metric called Backdoor Exposure Metric (BEM) to measure the effect of backdoor exposure. The BEM score is calculated based on the ASR and CA results of the exposed model $\\theta$ over the first $t$ exposure epochs. Formally, BEM is defined as:\n$BEM = \\frac{\\sum_t (ASR(\\theta) - CA(\\theta))}{\\sum_t ASR(\\theta)}.$"}, {"title": null, "content": "Intuitively, BEM measures the effect of preserving ASR while erasing CA, relative to the original ASR. Note that the ASR and CA used to calculate BEM are both averaged over the first $t$ exposure epochs to obtain a more stable result. A higher BEM score indicates more effective exposure, and vice versa."}, {"title": "V. UNIFIED DEFENSE WITH EXPOSED MODEL", "content": "The above backdoor exposure step of EBYD can be viewed as an upstream task while the subsequent detection and re-moval tasks in the defense step are the downstream tasks. By successfully exposing the backdoor in the upstream phase, all downstream methods can target the same objective, thereby creating a comprehensive defense framework. Below, we de-scribe how the exposed model can be utilized to enhance backdoor model detection, backdoor sample detection, and backdoor removal."}, {"title": "A. Enhancing Backdoor Sample Detection", "content": "STRIP [15] observed certain differences in output entropy between benign and malicious examples and proposed to detect backdoor samples based on the prediction entropy gap. The predictions with the lower entropy imply a backdoor sample. However, advanced backdoor attacks such as those with full-image trigger patterns can violate its assumption, leading to an unclear entropy gap. To improve the identifi-cation of backdoor samples, we propose an extension of the original STRIP method by replacing the original model with the exposed model $\\Theta_b$ obtained through backdoor exposure. The enhanced STRIP method, based on the entropy summation of all N perturbed inputs, can be formulated as:\n$H_{sum} = \\sum_{n=1}^{n=N} \\sum_{i=1}^{i=K} y_i \\times \\log_2 f(\\hat{x}_i; \\Theta_b),$"}, {"content": "STRIP [15] observed certain differences in output entropy between benign and malicious examples and proposed to detect backdoor samples based on the prediction entropy gap. The predictions with the lower entropy imply a backdoor sample. However, advanced backdoor attacks such as those with full-image trigger patterns can violate its assumption, leading to an unclear entropy gap. To improve the identifi-cation of backdoor samples, we propose an extension of the original STRIP method by replacing the original model with the exposed model $\\Theta_b$ obtained through backdoor exposure. The enhanced STRIP method, based on the entropy summation of all N perturbed inputs, can be formulated as:\n$H_{sum} = \\sum_{n=1}^{n=N} \\sum_{i=1}^{i=K} y_i \\times \\log_2 f(\\hat{x}_i; \\Theta_b),"}, {"title": null, "content": "where $\\hat{x}$ is the perturbed input by superimposing various image patterns and K is the number of total labels."}, {"title": "B. Enhancing Backdoor Model Detection", "content": "Trigger inversion based defenses represent a prevalent de-tection paradigm for identifying backdoored models. Among them, one of the most well-known and foundational methods is Neural Cleanse (NC). Specifically, NC detects backdoored models by reverse engineering the trigger pattern through constrained optimization. The optimization process of NC is defined as:\n$\\min_{m,\\Delta} \\mathcal{L}(y, f(x; \\Theta)) + \\lambda \\cdot |m|,$"}, {"title": "C. Enhancing Backdoor Removal", "content": "In this section, we propose a novel backdoor removal method as the last defense operation of EBYD to remove the backdoor neurons in the exposed model. The method is called Recover-Pruning (EBYD-RP). Given a backdoor-exposed model, EBYD-RP first recovers the clean functionality of the model with a learnable neural mask on the clean defense data and then identifies and prunes the backdoor neurons based on the learned mask.\nEBYD-RP first defines a neural mask for all neurons in the exposed model and then updates the mask by solving the following optimization problem:\n$\\min_{\\theta_l} L(f(x_d; m_r \\odot \\theta_b)),$"}]}