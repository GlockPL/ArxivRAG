{"title": "Applications of Large Models in Medicine", "authors": ["YunHe Su", "Zhengyang Lu", "Junhui Liu", "Ke Pang", "Haoran Dai", "Sa Liu", "Yuxin Jia", "Lujia Ge", "Jing-min Yang"], "abstract": "This paper explores the advancements and applications of large-scale models in the medical field, with a particular focus on Medical Large Models (MedLMs). These models, encompassing Large Language Models (LLMs), Vision Models, 3D Large Models, and Multimodal Models, are revolutionizing healthcare by enhancing disease prediction, diagnostic assistance, personalized treatment planning, and drug discovery. The integration of graph neural networks in medical knowledge graphs and drug discovery highlights the potential of Large Graph Models (LGMs) in understanding complex biomedical relationships. The study also emphasizes the transformative role of Vision-Language Models (VLMs) and 3D Large Models in medical image analysis, anatomical modeling, and prosthetic design. Despite the challenges, these technologies are setting new benchmarks in medical innovation, improving diagnostic accuracy, and paving the way for personalized healthcare solutions. This paper aims to provide a comprehensive overview of the current state and future directions of large models in medicine, underscoring their significance in advancing global health.", "sections": [{"title": "INTRODUCTION", "content": "Medical Large Models (MedLMs) refer to a class of large-scale artificial intelligence models specifically trained to handle and analyze various types of medical-related data, such as clinical text \u00b9, imaging data and genetic information. They are typically based on deep learning and neural network technologies, enabling them to perform a variety of tasks in the medical field, including disease prediction, diagnostic assistance, personalized treatment planning, and drug development. The core advantage of MedLMs lies in their powerful data processing capabilities and their ability to learn from vast amounts of data.\nMedLMs can be categorized into distinct types, each serving different applications based on the data they handle. Large Language Models (LLMs) are designed primarily for processing clinical textual data (like MedPaLM), such as electronic health records (EHRs) such as 2,3,4,5. These models excel at extracting pertinent information from a wide range of medical texts, including patient histories, symptoms, and treatment instructions. This capability reduces the manual workload for healthcare professionals and provides crucial decision-making support. Furthermore, LLMs are instrumental in generating clinical pathways, assisting in the creation of personalized treatment plans by analyzing a vast corpus of medical literature and integrating the latest clinical guidelines with practical clinical validations 7,8.\nAnother major category is Vision Models, which primarily deal with medical imaging data. These models, typically based on convolutional neural networks (CNNs), have proven effective in tasks such as detecting early-stage cancers through the analysis of medical images. For example, CNNs have been applied to detect skin cancer with dermatologist-level accuracy and are also used in the detection of lung and breast cancers, providing physicians with fast and reliable diagnostic support. The ability to detect even minute abnormalities in images, with accuracy comparable to that of experienced specialists, has made vision models a powerful tool in modern medical practice.\n3D Large Models, which focus on volumetric data analysis, represent another critical category within MedLMs. These models utilize 3D convolutional neural networks (CNNs) to handle medical images in three-dimensional formats, such as CT or MRI scans. By accurately segmenting tumors and analyzing their spatial characteristics, 3D models assist in determining tumor locations and volumes, essential for surgical planning. Additionally, these models contribute to virtual surgery simulations, allowing medical practitioners to plan surgeries more effectively and mitigate potential risks. The integration of 3D models into clinical practice is enhancing both the precision of tumor detection and the planning of surgical interventions 10.\nIn addition to these single-modal models, Multimodal Models integrate multiple types of data, such as clinical text, imaging, and genomic data, to provide a more comprehensive understanding of a patient's condition. By combining diverse data sources, these models enhance diagnostic accuracy and the ability to develop personalized treatment plans. For instance, multimodal models have been successfully used to improve the early diagnosis of lung cancer by combining CT images with clinical records, thus providing more accurate diagnostic insights. These models also play a crucial role in personalizing treatment plans for complex conditions, such as breast cancer, by integrating genomic data, imaging, and clinical histories 11.\nGraph Large Models (Graph Neural Networks, GNNs) are another key type of MedLM, particularly in the field of genomics. GNNs are designed to analyze the relationships between genes, diseases, and therapeutic targets. By studying gene interaction networks, GNNs can predict disease risk factors and identify potential biomarkers, offering novel insights into early diagnosis and potential treatment options. Graph Large Models leveraging transformers with large parameters and trained with large biomedical datasets. These models have shown great promise in areas such as cancer risk prediction, where they analyze the complex interactions between genes and disease.\nMedical large models (MedLMs) are bringing new possibilities to healthcare. They offer applications in disease prediction, diagnostics, treatment planning, and drug development. These models analyze extensive medical data and identify patterns that may indicate disease risks. For example, genetic interaction analysis can reveal potential cancer risks, supporting early preventive strategies 12.\nMedLMs also contribute to improving diagnostic processes. By analyzing imaging data, they assist in identifying abnormalities that might be missed in manual reviews. Some models trained on dermatological imaging have been shown to classify skin conditions with a high degree of accuracy, providing clinicians with additional tools for decision-making.\nIn treatment planning, MedLMs combine imaging, genomic, and historical patient data. This combination supports the development of personalized strategies for managing complex conditions. For diseases such as breast cancer, integrating these data sources has been associated with better-aligned treatment options for patients.\nDrug discovery is another area where MedLMs show promise. They assist in predicting protein structures, which is a critical step in therapeutic development. Tools like AlphaFold have been used to reduce the time and effort required for molecular design, helping researchers identify drug candidates more efficiently 13.\nMedical large models (MedLMs) are experiencing widespread adoption across healthcare systems globally. Their utilization is increasing rapidly, particularly in areas such as diagnostic assistance, disease prediction, personalized treatment planning, and drug discovery. These models have proven their value in enhancing the accuracy and efficiency of medical practices. Notably, many healthcare platforms have now integrated these models as standard tools, embedding them deeply into their operations.\nFor instance, Baidu's Lingyi Medical Model leverages MedLM technology to enhance diagnostic accuracy, supporting doctors in making more precise disease predictions and diagnoses. By analyzing vast datasets, the model assists healthcare professionals in better understanding complex conditions, leading to improved treatment outcomes. Additionally, MedGPT, developed by Yilian, is another example of a medical language model that facilitates the entire healthcare process, from intelligent consultation to diagnosis recommendations and personalized treatment plans. This model integrates seamlessly into clinical workflows, helping doctors save time and make informed decisions based on comprehensive data analysis.\nThese platforms are just a few examples where MedLMs have become integral to healthcare technology. The incorporation of such models is not limited to the aforementioned systems; other companies, such as Yuanxin Technology and Jingtai Technology, are also employing these models to enhance patient management, accelerate drug development, and provide intelligent medical services. By embedding MedLMs as standard tools, these platforms are setting a new benchmark for healthcare innovation and improving overall patient care.\nWith these advancements, the high usage rate of MedLMs continues to rise, demonstrating their importance in modern healthcare settings. As the demand for precise and efficient healthcare grows, more medical institutions and research organizations are expected to adopt these models to support and enhance clinical decision-making, ultimately improving the quality of healthcare worldwide."}, {"title": "LLMS IN MEDICAL", "content": "The effectiveness of large language models (LLMs) in medical question answering hinges on their training methodologies and the quality of the datasets used 14. Leading research organizations, such as Google, have adopted advanced pretraining and fine-tuning methodologies to optimize the performance and efficacy of these models 15. For example, MedPaLM, built upon the general-purpose PaLM model, has been fine-tuned using high-quality, domain-specific medical datasets, enabling it to excel in understanding and reasoning about medical queries 16. To ensure the accuracy and comprehensiveness of these datasets, Google draws information from public medical databases (e.g., PubMed and MIMIC-III), professional guidelines, and detailed patient case records 17. These data sources are meticulously cleaned and reviewed by medical experts.\nFurthermore, multilingual processing capabilities are incorporated, enhancing the models' usability and robustness across diverse linguistic and cultural contexts 18.\nLLMs have rapidly become transformative tools within the medical field, offering significant advancements in medical education 19. For students preparing for the United States Medical Licensing Examination (USMLE), LLMs serve as invaluable resources. They excel in solving intricate clinical reasoning problems, simulating exam scenarios, and providing in-depth explanations for incorrect answers, which significantly enhances students' exam preparation and understanding of key concepts 20. The success of LLMs in medical question-answering is largely attributed to the use of high-quality datasets. General-purpose datasets, such as PubMedQA and MIMIC-III, are essential for assessing clinical reasoning. While more specialized datasets like MedQA-USMLE, BioASQ, and MedMCQA address specific needs in the field1. For instance, MedQA-USMLE provides question-and-answer pairs that closely align with the structure and content of the USMLE 21, while BioASQ focuses on biomedical knowledge retrieval and response generation 22. By harnessing these diverse datasets, LLMs can generate precise, reliable, and contextually appropriate responses, solidifying their role as crucial tools in advancing medical education and practice.\nAl large language models, such as GPT, are increasingly playing a pivotal role in the medical field, particularly in facilitating patient self-diagnosis and providing health management guidance 23. For example, Pahola offers reliable, alcohol-related information to a global audience, thereby contributing to the effective implementation of Screening and Brief Interventions (SBI) and enhancing alcohol health literacy 24. Similarly, research has demonstrated that many individuals utilize ChatGPT for self-diagnosis and to access health information. An example of this is ChatGPT's utility in aiding patients in identifying common orthopedic conditions, such as carpal tunnel syndrome (CTS), prior to seeking consultation with healthcare professionals 25. Moreover, Al-driven tools enable individuals to assess their mental health conveniently from the privacy of their homes 26.\nTo assess the performance of LLMs in medical question-answering tasks, researchers employ diverse and rigorous evaluation metrics. Standard metrics like accuracy, precision, recall, and F1 scores are used to measure the correctness and comprehensiveness of model responses 27. For tasks requiring generated answers, BLEU scores evaluate the linguistic alignment between model outputs and reference answers 28. Moreover, domain-specific metrics, such as medical relevance and clinical applicability scores, are employed to gauge the professional and practical utility of the responses 29. Ethical and safety assessments are equally critical, utilizing measures such as harmful content detection rates and fairness evaluations to ensure the models' reliability and equity in medical applications 30. These comprehensive evaluation frameworks support the continuous improvement and refinement of LLMs. Notably, existing LLMs may produce incorrect content or known as \"hallucinations\". More details will be discussed in later sections."}, {"title": "LARGE VISION MODELS IN MEDICAL", "content": "Medical image anomaly detection represents a critical component in computer-aided diagnosis systems, serving as an essential tool for early disease detection and treatment planning. Despite significant advances in deep learning, the inherent complexity of medical anomalies, coupled with the scarcity of annotated pathological data, continues to pose substantial challenges. As highlighted by the comprehensive BMAD benchmark 31, which spans across five distinct medical domains, the field requires robust and generalizable approaches that can perform effectively across different modalities and anatomical structures. Recent developments in foundation models and generative approaches have introduced promising paradigms for addressing these challenges, revolutionizing how we approach medical anomaly detection."}, {"title": "Vision-Language Models for Zero-shot Medical Anomaly Detection", "content": "The emergence of vision-language foundation models has marked a significant breakthrough in medical anomaly detection, particularly in zero-shot scenarios. These models, pre-trained on vast corpora of image-text pairs, offer a promising solution to the perennial challenge of limited labeled medical data. Zhou et al. 32 introduced AnomalyCLIP, a pioneering approach that adapts CLIP (Contrastive Language-Image Pretraining) for zero-shot anomaly detection through object-agnostic prompt learning. The key innovation lies in learning text prompts that capture generic normality and abnormality patterns independent of foreground objects, enabling generalized anomaly recognition across diverse medical domains.\nBuilding upon this foundation, Huang et al. 34 developed a sophisticated multi-level adaptation framework that significantly enhances CLIP's capacity for medical anomaly detection. Their approach incorporates multiple residual adapters into the pre-trained visual encoder, guided by pixel-wise visual-language feature alignment loss functions. This architecture effectively recalibrates the model's attention from general object semantics to specific medical anomaly patterns, achieving remarkable improvements in both anomaly classification and segmentation tasks.\nThe integration of SAM with CLIP has opened new avenues for zero-shot medical image analysis. Aleem et al. 36 presented SaLIP, a cascade framework that combines SAM's precise segmentation capabilities with CLIP's semantic understanding. This unified approach demonstrates superior performance in organ segmentation tasks without requiring extensive domain-specific training data or manual prompt engineering. Furthermore, Koleilat et al. 37 extended this concept with MedCLIP-SAMv2, introducing a novel Decoupled Hard Negative Noise Contrastive Estimation loss and Multi-modal Information Bottleneck for enhanced segmentation performance.\nThe development of domain-specific models, exemplified by BiomedCLIP 33, represents another significant advancement. Trained on an unprecedented 15 million biomedical image-text pairs, BiomedCLIP exhibits remarkable zero-shot capabilities across various medical imaging tasks, outperforming even specialized models in their respective domains. Park et al. 35 further refined these approaches by introducing Contrastive Language Prompting (CLAP), specifically addressing the challenge of false positives in medical anomaly detection through careful prompt engineering."}, {"title": "Diffusion Models for Unsupervised Detection", "content": "Diffusion models have emerged as a powerful framework for unsupervised anomaly detection in medical imaging, offering unique advantages in modeling complex data distributions and generating high-quality counterfactuals. The pioneering work of Wolleb et al. 38 combined denoising diffusion implicit models with classifier guidance, demonstrating superior performance in preserving fine anatomical details compared to traditional GAN-based methods.\nA significant advancement in this domain came from Fontanella et al. 39, who introduced a novel approach for generating healthy counterfactuals of diseased images. Their method uniquely combines DDPM (Denoising Diffusion Probabilistic Models) and DDIM (Denoising Diffusion Implicit Models) at each sampling step, using DDPM to modify lesion areas while employing DDIM to preserve normal anatomy. This careful balance between modification and preservation has proven crucial for accurate anomaly detection.\nThe development of masked diffusion models represents another major innovation. Iqbal et al. 41 introduced mDDPM, incorporating both Masked Image Modeling (MIM) and Masked Frequency Modeling (MFM) to enhance the model's ability to learn anatomically consistent representations. Liang et al. 49 extended this concept with their MMCCD framework, introducing cyclic modality translation as a mechanism for anomaly detection in multimodal MRI.\nRecent work by Behrendt et al. 40 has focused on addressing the limitations of traditional anomaly scoring functions. Their approach introduces an adaptive ensembling strategy using Structural Similarity (SSIM) metrics, offering a more pathology-agnostic scoring mechanism that captures both intensity and structural disparities. Fan et al. 42 further advanced this field with their discrepancy distribution medical diffusion (DDMD) model, which innovatively translates annotation inconsistencies into distribution discrepancies while preserving information within homogeneous samples."}, {"title": "Self-supervised Learning for Anomaly Detection", "content": "Self-supervised learning has emerged as a powerful paradigm for leveraging unlabeled medical data effectively. Tian et al. 44 introduced PMSACL, a groundbreaking self-supervised pre-training method that contrasts normal image classes against multiple pseudo classes of synthesized abnormal images. This approach addresses the critical challenge of learning effective low-dimensional representations capable of detecting unseen abnormal lesions of varying sizes and appearances. By enforcing dense clustering in the feature space, PMSACL significantly improves the sensitivity of anomaly detection across diverse medical imaging modalities.\nBuilding on the success of masked modeling techniques, Iqbal et al. 41 developed a novel self-supervised framework incorporating both spatial and frequency domain masking strategies. Their approach enables the model to learn more robust and anatomically-aware representations without requiring explicit annotations. This innovation has proven particularly effective in detecting subtle anatomical variations that might indicate pathological conditions.\nThe advancement of self-supervised learning has also led to improved understanding of normal anatomical variations, crucial for accurate anomaly detection. Wu et al. 43 demonstrated how transformer-based architectures can be effectively combined with self-supervised learning objectives to capture long-range dependencies and structural relationships in medical images, leading to more reliable anomaly detection systems."}, {"title": "Semi-supervised Learning Approaches", "content": "Semi-supervised learning approaches have shown remarkable promise in leveraging both labeled and unlabeled data effectively for medical anomaly detection. Zhang et al. 45 developed SAGAN, a sophisticated framework incorporating position encoding and attention mechanisms to accurately focus on abnormal regions while preserving normal structures. Their approach innovatively relaxes the cyclic consistency requirements typical in unpaired image-to-image translation, achieving superior performance in generating high-quality healthy images from unlabeled data.\nCai et al. 46 proposed a groundbreaking dual-distribution discrepancy framework that effectively leverages unlabeled images containing anomalies. Their approach introduces normative distribution and unknown distribution modules, with intra-discrepancy and inter-discrepancy measures serving as refined anomaly scores. This method has demonstrated significant improvements across various medical imaging modalities, including chest X-rays, brain MRIs, and retinal fundus images.\nThe integration of semi-supervised learning with traditional generative models has also shown promising results. \u00d6zbey et al. 47 demonstrated how adversarial diffusion models could be effectively combined with semi-supervised learning strategies to improve image translation and anomaly detection performance. Their SynDiff framework showcases the potential of leveraging partially labeled datasets to enhance the fidelity and accuracy of generated medical images."}, {"title": "Challenges and Future Directions", "content": "Despite these advances, several critical challenges remain in medical anomaly detection. First, the balance between model complexity and clinical practicality continues to be a significant concern. While diffusion models offer superior performance, their computational requirements can be prohibitive in clinical settings, as noted by Wu et al. 43. Second, the integration of multiple expert annotations remains challenging, though recent work by Amit et al. 48 on consensus prediction offers promising directions.\nLooking forward, the field appears to be moving toward more efficient and interpretable approaches. The success of frequency-guided methods 50 and structure-aware adaptations suggests that future developments may focus on incorporating domain-specific medical knowledge into foundation models. Additionally, the growing interest in self-supervised and semi-supervised approaches indicates a shift toward methods that can better utilize the vast amounts of unlabeled medical data available while maintaining the high standards of accuracy required in clinical applications."}, {"title": "Applications in Pathological Images.", "content": "In pathological image analysis, the main approaches include pathological image segmentation, anomaly detection, and image generation 51,52. The goal of pathological image segmentation is to divide different tissue or lesion regions within an image. Generally, it is the foundation of tasks including cancer tissue detection, lesion structure analysis, cell counting and so on. Models for pathological image segmentation often use convolutional neural network (CNN)-based architectures, particularly the U-Net model. U-Net Model is proposed by Olaf Ronneberger et al. in 201553, and it is a CNN architecture for biomedical image segmentation. It is widely used in tasks including cell boundary segmentation and the delineation of lesion areas. The U-Net consists of two main components: the encoder path and the decoder path 54. The encoder extracts features by transforming raw pathological images into low-resolution, high-semantic feature representations using convolutional and pooling operations, and it also introduces non-linearity through functions such as ReLU 55. The decoder restores image resolution by upsampling the encoded features to match the size of the input images, finally producing the segmentation output 56,57. The decoder path involves two key processes: deconvolution, which upsamples feature maps to restore resolution, and skip connections, which combine features from corresponding layers in the encoder and decoder keep spatial information lost in encoder. Meanwhile, convolutional operations integrate low-level details with high-level semantics to refine the segmentation results 58. In addition, there are several improved models based on U-Net, such as the Attention U-Net, which was proposed by Oktay et al. in 201859. This model introduces attention modules to enhance the model's focus on critical regions. Attention modules are integrated into each skip connection to dynamically adjust the features transferred from the encoder to the decoder. These modules calculate attention weights for the input features and amplify features of important regions while suppressing features of irrelevant areas. Based on the classic encoder-decoder structure of U-Net, the attention modules are embedded at key nodes along the decoding path and improve the model's focus on significant regions while maintaining overall segmentation accuracy. Image segmentation can be used in various pathological imaging tasks, like cell and nuclear segmentation. For example, when diagnosing corneal endothelial health states, U-Net-based CNN have achieved high accuracy in segmenting endothelial cells across images of varying cell sizes, and achieve precise measurement of cellular morphological parameters (AUROC 0.92, DICE 0.86) 60. Moreover, because cell segmentation is the first step in quantitative tissue imaging data analysis and the basis for single-cell analysis, it is important in identifying malignant tumors. The abnormal enlargement of nuclei in cancer cells leads to a significantly increased nucleus-to-cytoplasm ratio, which is one of the criteria for diagnosing malignant tumors 61. Therefore, image segmentation can also be applied to the dividing of malignant regions in pathological images. For example, in squamous cell carcinoma (SCC), researchers utilized a patch dataset extracted from 200 digitized tissue images of 84 patients to train a U-Net-based segmentation model. The model achieved a segmentation AUC of 0.89 on the test set, and the average segmentation time is 72 seconds per image, which shows higher efficiency compared to traditional manual segmentation methods 62. Pathological image generation uses artificial intelligence (Al) to produce high-quality synthetic pathology images and solves challenges such as limited datasets and annotation difficulties by enlarging and balancing datasets. Therefore, it is generally used for upstream tasks 63.\nGenerative adversarial networks (GANs), introduced by Ian Goodfellow et al. in 201464, are the primary methods for image generation. GANs consists of two neural network, including Generator and Discriminator, and train adversarially to produce realistic data, and it shows outstanding performance in the image generation 65. The method optimizes the generator and discriminator alternately. The generator samples from the real dataset, and inputs the sampled noise into the generator, and produces synthetic samples that resemble real data; the discriminator takes both real and synthetic data as input and judges whether each sample comes from the real dataset or is generated by the generator. The discriminator is updated to improve its ability to distinguish between real and synthetic data, while the generator is updated to enhance its ability to generate synthetic data 66. Through multiple iterations, the generator produces samples that are realistic enough to make it difficult for the discriminator to distinguish between real and synthetic data and then complete the task of generating pathological images. Moreover, StyleGAN, a GAN-based model, can also be used for generating pathological images 67. In traditional GANs, random noise with a Gaussian or uniform distribution is directly inputted into the generator to produce images. In contrast, StyleGAN introduces a style-mapping network, which maps noise Z to a new latent space W. The W is more expressive and easier to use for controlling specific features in the generated images, which helps adjust things like color, texture, or shape, making the images more realistic and diverse. Additionally, independent random noise is injected at each layer to generate random details in the images 68. StyleGAN also uses progressive growth 69, where the generator and discriminator initially handle low-resolution images during early training. The model gradually increases the resolution as training progresses until it achieves the target high resolution. To further improve the resolution of generated images, a multi-scale conditional GAN method has been proposed. This model used a pyramid-like structure to progressively increase the resolution while generating high-resolution images and maintaining global consistency and detailed features of glandular structures. Through adversarial training, the generator captures the global layout of glands and the micro-textures of cell nuclei. Multi-layer discriminators ensure the authenticity and consistency of the generated image 70. In renal pathology image analysis, the morphological characteristics of glomeruli provide critical diagnostic and prognostic information. To improve diagnostic efficiency, an automated method based on CNNs was proposed. This method used GAN-based generative data augmentation to generate glomeruli pathological images with various morphologies and improve data diversity and model performance. The results showed that after applying generative data augmentation, the sensitivity of the classification model increased from 0.7077 to 0.7623, and specificity improved from 0.9316 to 0.944371. In addition to the pathological image segmentation and image generation above, there is also a downstream task, which is anomaly detection of pathological images. This involves using extracted image features to identify and localize abnormal images. In anomaly detection, there are generally two types of methods, which are supervised anomaly detection and unsupervised anomaly detection 72. Supervised anomaly detection requires a dataset of annotated images including both normal and abnormal images. This kind of classification tasks could be performed using CNN-based model 73. CNN models extract features through convolutional layers, reduce the resolution of feature maps using pooling layers, and map the extracted high-dimensional features to classification outputs using fully connected layers. The final classification probabilities for normal or abnormal states are generated using Softmax or Sigmoid functions in the output layer 74. For example, studies have shown that research based on deep CNNs shows significant advantages in classifying skin lesions. Using a dataset containing about 130000 images and covering almost 2000 diseases, CNNs achieved great performance comparable in two binary classification tasks. Besides, the CNN achieved an accuracy of 72.1% in a three-class disease partitioning task and 55.4% in a nine-class disease partitioning task 75. In unsupervised or self-supervised anomaly detection, the primary methods include contrastive learning 76 and generative models 73,77. Contrastive learning involves extracting high-level feature representations from normal samples to construct a feature space. Abnormal samples, due to their different feature distributions, deviate from the feature space of normal samples. If the deviation is sufficiently large, they are identified as anomalies. Generative models are similar to GANs. They use an encoder to extract high-dimensional feature representations from input data, capturing complex patterns and learning features. Subsequently, a decoder reconstructs the input data from these high-dimensional features to recreate the original image. For normal samples, as they conform to the feature distribution learned by the model, the reconstruction error is low. However, for abnormal samples, the reconstruction error is high, leading to their identification as anomalies. Hui Liu et al. proposed a deep learning framework based on weakly supervised contrastive learning. This framework uses self-supervised pretraining on large-scale unlabeled patches from whole slide images (WSI) to extract highly informative pathological features. Combined with multitask learning, the framework successfully inferred breast cancer-related gene expression, molecular subtypes, and clinical outcomes. Experiments showed that this method achieved outstanding performance across multiple datasets, and the generated spatial heatmaps were highly consistent with pathologists' annotations and spatial transcriptomics data. This highlights its potential in linking genotypes with phenotypes and in the clinical applications of digital pathology 78. Besides, in related research, a generative model combining GANs and autoencoders was employed for anomaly detection. By training the model on normal tissue data, regularization and multi-scale contextual data were used to improve generalizability. This approach achieved efficient anomaly detection on the toxicologic histopathology (TOXPATH) dataset, with an AUC of 0.95379. In general, pathological slide images can be used in different upstream and downstream tasks, such as classification and segmentation tasks. These include using image segmentation for cell counting 80, applying image generation in upstream tasks to perform pathological data augmentation 81, using image segmentation and anomaly detection for cancer diagnosis or the identification of other lesion regions 82,83, and conducting tasks such as tumor grading and progression prediction 84. In addition to the neural network models mentioned above, the increasing computational capacity has led to the emergence of large models. These large models show powerful feature learning abilities through pretraining on large-scale datasets. In recent years, the concept of foundation models 85 has emerged. These models not only have larger parameter sizes, with commonly used architectures in the field of pathology image analysis including Vision Transformers, CLIP, and Mask2Former ranging from hundreds of millions to billions of parameters, but also use high-resolution pathology images for pretraining on large-scale datasets. Generally, the architectures of these foundation models are based on Transformer 86. Initially it was developed for natural language processing, now it has been extended to the visual field. Through the self-attention mechanism, it effectively models both global and local features in pathology images and improves performance of tasks of segmentation and classification. While traditional CNNs rely on convolutional kernels to extract local features, Transformers rely entirely on self-attention to model relationships between all regions of an image and therefore capture the interdependencies of different tissue structures in pathology slides more effectively. Like neural network models, many Transformer-based models are developed to address tasks such as segmentation, classification diagnosis, and multimodality. For image segmentation tasks, in addition to U-Net, Transformer-based segmentation large models, such as Mask2Former, have introduced multi-task segmentation frameworks and self-attention mechanisms to achieve pathology image segmentation. Jia-Chun Sheng et al. studied the Transformer-based Mask2Former segmentation model and evaluated its performance on pathology datasets. Mask2Former uses Transformer-based architecture. It used Swin Transformer as the encoder to extract multi-scale features. A pixel decoder integrated and processed features of different resolutions. The Transformer decoder applied masked attention to focus on foreground regions. This design improves its ability to segment small objects. By pretraining on natural images, Mask2Former adapts well to small pathology datasets. It achieved great performance comparable to or better than task-specific methods on the CRAG and GlaS datasets. It also significantly reduces the computational cost of handling high-resolution features 87. In addition, Visual Transformer-based classification models have been increasingly introduced in pathology. Unlike traditional methods, these large models use the global modeling capabilities of Transformers to handle complex pathology images, like gigapixel whole slide images (WSIs). In pathology image classification, Hanwen Xu et al. proposed Prov-GigaPath, which extracts local features through a tile-level encoder and integrates global context features from WSIs using LongNet 88. By being pretrained on real-world datasets containing 1.3 billion tiles, the model effectively captures both local and global patterns using DINOv2 and a masked autoencoder (MAE). Prov-GigaPath achieved better performance in 25 out of 26 pathology tasks, including a 23.5% improvement in AUROC and a 66.4% improvement in AUPRC for EGFR mutation prediction 89. Eugene Vorontsov et al. proposed the Virchow model. It was based on Visual Transformer architecture and trained on 1.5 million WSIs. Through the DINOv2 algorithm to learn both global and local embedding of WSIs, the model achieved cancer detection with ROC of 0.95 for nine common and seven rare cancers 90. Richard J. Che's team proposed the general-purpose self-supervised pathology model UNI. It was pretrained on over 100,000 diagnostic H&E-stained WSIs and showed exceptional performance in 34 computational pathology tasks, including cancer subtype generalization 91. Recently, multimodal foundation models have gained attention in pathology image analysis. By integrating visual and textual information, these models simultaneously capture the structural features of pathological images and the semantic information of clinical text. CONCH is a foundational visual-language model that uses an image encoder, text encoder, and multimodal decoder. Through contrastive learning, it embeds images and text into a shared representation space while optimizing multimodal understanding through a captioning objective. During pretraining, CONCH leverages diverse pathology image-text pairs for unsupervised learning, significantly enhancing feature extraction capabilities. In cancer subtype classification, CONCH achieved 91.3% zero-shot accuracy 92. The commonly used evaluation metrics in pathological image tasks are as follows:"}, {"title": "APPLICATIONS OF LARGE MULTI-MODAL MODEL IN MEDICAL", "content": "Vision language models (VLMs) are multimodal generative Al models capable of reasoning over text, image, and video prompts. VLM has demonstrated excellent processing capabilities in image & video generation, text-centric visual question answering, zero-shot detection, video summarization, and other fields, and some of the results have been successfully applied to autonomous driving, Al-generated image & text generation, AR/VR techniques and other fields 97. In the past, machine learning-based graphics processing technology has been widely used for the diagnostics of radiology and medical education 98,99. As a new generation of image processing model, VLM has attracted the attention of medical workers with its potential, and its application as the superior substitution of traditional image processing models in the medical field is becoming another hot research direction.\nIn computer graphics, 3D modeling refers to the process of developing mathematical coordinate-based representations of the surface of objects (inanimate or biological) in three dimensions by manipulating edges, vertices and polygons in simulated 3D space through specialized software. 3D modeling has been widely used in architecture and industrial design, video games, film industry, art and other fields, but traditional 3D modeling technique has a strong dependence on human resources and time cost. As a result, the cost and quality of the final product could be difficult to control. The recent emergence of 3D large models is bringing new changes to this field 100. Three-dimensional modeling was applied to multiple types of tasks in medicine (such as prosthetic and implant design, image reconstruction, anatomical modeling, medical education, etc.), and 3D large models may bring a technical revolution to these applications."}, {"title": "VLM for Medical Image Analysis", "content": "The models and algorithms based on traditional machine learning models (such as CNN) that have been widely used in the research of medical imaging 101. VLM as a technique that has shown great capabilities in processing complex relationships and multimodal information 97, is naturally adapted to the complexity of information contained in medical cases (text, charts, pictures, videos, audio, etc.); therefore, VLM is gradually attracting the attention of medical workers.\nThe application of VLM large models in medicine at present is mainly focused on the processing of medical images, especially for pathological and radiological imaging, for which have a much greater demand for softwires with efficient graphic processing capabilities than other subspecialties. There are already some studies involving VLM large models for radiology and pathology, and this topic is discussed in details at other articles of our journal."}, {"title": "VLM for Medical Single Analysis", "content": "VLM models are also applied to the analysis of physiological signals. Just like traditional machine learning models, researches about the availability of VLMs for ECG, EEG analysis are also being conducted 102,103,104.\nDue to the high correlation between VLM and computer vision, VLM is currently also used in the field of Al-based robotics. Currently, some EEG and sEMG combined technologies have been experimentally applied to the collection of limb movement data and the optimization of prosthetic movement patterns 105,106,107. Meta has already released two large datasets and benchmarks for sEMG-based typing and pose estimation in Dec. 2024 108."}, {"title": "3D Large Models for Biometric Analysis", "content": "3D modeling technology has been widely used in the medical field, especially in human anatomical modeling, prosthetic design and 2D to 3D image conversion & construction 109,110,111. With the advent of 3D large models, this technology has gradually been applied to the research in corresponding medical fields.\nAnother area of 3D modeling-3D human structure construction, has also been influenced by 3D large model with the production of some new research progresses. 3D model construction has been applied for educational use to better demonstrate body structures. In addition, 3D printing is widely used in orthopedics, surgery, instrument design, drug research and many others in recent years."}, {"title": "Discussion and Outlook", "content": "Diagnostic techniques in neurology, psychiatry based on machine learning algorithms have been one of the hot areas in Al related medical research in the past few years. Motion capture in combination with deep learning is also applied to the research of clinical measurements for physical medicine and rehabilitation 112,113. There are some studies proved to be useful on the screening & diagnosis of stroke, Parkinson's disease as well as some mental disorders based on facial expression analysis 114,115,116. VLM, as a new generation of graphic image model, also have strong prospects in these directions. At present, some results of emotion analysis research related to facial expression capture based on VLM have been published 117,118"}]}