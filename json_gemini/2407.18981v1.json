{"title": "Prompt Injection Attacks on Large Language Models in Oncology", "authors": ["Jan Clusmann", "Dyke Ferber", "Isabella C. Wiest", "Carolin V. Schneider", "Titus J. Brinker", "Sebastian Foersch", "Daniel Truhn", "Jakob N. Kather"], "abstract": "Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be attacked by prompt injection attacks, which can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We performed a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMS which have been proposed to be of utility in healthcare: Claude 3 Opus, Claude 3.5 Sonnet, Reka Core, and GPT-40. Using a set of N=297 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption.", "sections": [{"title": "Main", "content": "Large language models (LLMs) are generative artificial intelligence (AI) systems trained on vast amounts of human language. They are the fastest-adopted technology in human history 1,2. Numerous scientific and medical applications of LLMs have been proposed 3\u20135, and these could drastically change and improve medicine as we know it. In particular, LLMs have been shown to be able to reduce documentation burden and promote guideline-based medicine 6,7. In parallel to the rapid progression of LLM capabilities, there has been substantial progress in the development of multimodal vision-language models (VLMs). VLMs can interpret images and text alike and further expand the applicability of LLMs in medicine. Several VLMs have been published to date, either as healthcare-specific models, e.g., for the interpretation of pathology images or echocardiograms 8,9, or as generalist models, applicable to multiple domains at once, including healthcare, such as GPT-40 10\u201314.\nHowever, with new technologies, new vulnerabilities emerge, and the healthcare system has to be hardened against these 15,16. We hypothesized that one particular vulnerability of LLMs and VLMs is prompt injection. Prompt injection means that a user adds an additional, hidden instruction for the model (Figure 1a). Prompt injection can be disguised in hidden (e.g. zero-width) or encoded characters (e.g. Unicode), whitespaces, metadata, images and much more essentially, any information which flows into a model at runtime can be used as an attack vector (Figure 1b) 17,18. Importantly, third parties with access to a user's input (but without access to the model itself), can perform prompt injection to exfiltrate private data, evade model guardrails, and alter model outputs. In a clinical context, modifying a model's output can cause lethal harm."}, {"title": null, "content": "To investigate the susceptibility of contemporaneous state of the art VLMs, we designed a set of experiments to evaluate prompt injection in the context of cancer imaging. Specifically, we investigated if the diagnosis obtained from an imaging scan, for example, presence or absence of tumor, can be manipulated at will, just by injecting malicious information into the model input. Images of nine patients with clearly visible cancer lesions in liver imaging, in equal parts CT, MRI and ultrasound images, were passed to the VLMs Claude 3 Opus, Claude 3.5 Sonnet, Gemini 1.5, GPT-40 and Reka Core. Each image (Supplementary Material 2) was accompanied by the prompt \"Describe this image\" and a structured template for the output (Supplementary Table 3). Three strategies for prompt injection were tested: \"text prompt injection\", \"visual prompt injection\", and \"delayed visual prompt injection\", in which the attack was performed using the image preceding the target image, see Supplementary Figure 1a. Additionally, for visual - and delayed visual prompt injection, we tested if the contrast and size of the injected text had an influence on the models' accuracies: we employed two contrast settings (high contrast and low contrast) and one setting in which the text was tiny, see Figure 1b. Low-contrast and \"tiny\" injections correspond to sub-visual injections which are not obvious to human observers, therefore more harmful. This led to a total of 36 variations per model (9 negative controls + 27 prompt injection variations), with each of the 36 variations being queried a total of 3 replicates (n = 108 per model). All prompts are listed in Supplementary Table 3.\nFirst, we assessed the organ detection rate by the model. Only VLMs which reached at least a 50% organ detection rate, i.e. were able to accurately describe the organ in the image as the liver, were used for subsequent experiments (Figure 2a). The VLMs Claude-3 Opus, Claude 3.5 Sonnet, GPT-40 and Reka Core achieved this rate and were therefore included in this study. We were not able to investigate the vision capabilities of Gemini 1.5 Plus because its current guardrails prevent it from being used on radiology images. Llama-3 (70B), the best currently available open-source LLM, does not yet support vision interpretation, and could therefore not be assessed 19. As a side observation we found that all models sometimes hallucinated the presence of spleen, kidneys and pancreas, but this effect was not relevant to the subsequent experiments.\nSecond, we assessed the attack success rate in all VLMs. Our objective was to provide the VLM with an image of a cancer lesion in the liver, and prompting the model to ignore the lesion, either by text prompt injection, visual prompt injection or delayed visual prompt injection. We quantified (a) the model's ability to detect lesions in the first place (lesion miss rate, LMR), and (b) the attack success rate (ASR), i.e. flipping the model's output by a prompt injection (Figure 2b). We observed highly different behavior between VLMs, with organ detection rates of 60% (Claude-3), 100% (Claude-3.5), 94% (GPT-40), and 76 % (Reka Core) (n=27 each). Lesion miss rate (LMR) of unaltered prompts was 52% for Claude-3, 22% for Claude-3.5, 19% for GPT-40, and 26% for Reka Core (n=27 each) (Figure 2b). Adding prompt injection significantly impaired the models' abilities to detect lesions, with a LMR of 70% (ASR of 18%) for"}, {"title": null, "content": "Claude-3 (n=81), LMR of 57% (ASR 35%) for Claude-3.5 (n=81), LMR of 89% (ASR of 70%) for GPT-4o (n=81) and LMR of 61% (ASR of 36%) for Reka Core (n=54) (p<0.0001) (Figure 2b). Notably, we observed the highest increase in harmful responses for GPT-40, with an ASR of 70% due to prompt injection (n=81, p<0.0001), possibly due to GPT-40's strong instruction tuning (Supplementary Table 5). Together, these data show that prompt injection, to varying extent, is possible in all investigated VLMs.\nPrompt injection can be performed in various ways. As a proof-of-concept we investigated three different strategies for prompt injection (Figure 1b), with striking differences between models and strategies (Figure 2 c, d, Supplementary Fig. 2). Text prompt injection was harmful in almost all observations. Image prompt injection resulted in similar harmfulness for all models, except Claude-3.5, which proved less harmful here. Meanwhile, delayed visual prompt injection resulted in less harmful responses (Figure 2c), possibly because the hidden instruction becomes more susceptible to guardrail interventions once written. Different hiding strategies (low contrast, small font) were shown to be similarly harmful to the default (high contrast, large font) (Figure 1b, 2d). LMR was highest in ultrasound (US) images (73%, ASR 24%), with LMR of 59% for MRI (ASR 42%) and 45% for CT-A (ASR 53%), and only LMR of US and CT-A varying significantly (n=99 each, p = 0.008) (Supplementary Fig. 3). Together, these data show that prompt injection is modality-agnostic, as well as generalizable over different strategies and visibility of the injected prompt\nIn summary, our study demonstrates that subtle prompt injection attacks on state-of-the-art VLMs can cause harmful outputs These attacks can be performed without access to the model architecture, i.e. as black-box attacks. This makes them a highly relevant security threat in future healthcare infrastructure, as injections can be hidden in virtually any data that is processed by medical AI systems. Given that prompt injection exploits the fundamental input mechanism of LLMs, prompt injection is likely to be a fundamental problem of LLMs/VLMs, not exclusive to the tested models, and not easily fixable. Even recent technical improvements to LLMs, e.g. \"Short circuiting\", are insufficient to mitigate such attacks 15,20. Further, other types of guardrails can be bypassed 20 or compromise usability (as shown for Gemini 1.5). Overall, these data highlight the need for techniques specifically targeting this form of adversarial attacks."}, {"title": null, "content": "To our knowledge, this is the first-ever study evaluating prompt injection attacks in healthcare. Hospital infrastructures face a dual challenge and a complex risk-benefit scenario here: They will have to adapt to both integrate LLMs and build robust infrastructure around them to prevent these new forms of attacks, e.g. by deploying agent-based systems 21. Despite our findings pointing to relevant security threats, integrating LLMs in hospitals holds tremendous promise for patient empowerment, reduction of documentation burden, and guideline-based clinician support 4,7,22. Our study therefore encourages all relevant stakeholders to adopt these LLMs and VLMs but to develop new ways to harden the systems against prompt injection attacks. A promising way for such hardening is to keep"}, {"title": "Online Methods", "content": "human experts in the loop and to have highly critical decisions double-checked and vetted by humans who ultimately take responsibility for clinical decisions."}, {"title": "Patient cases", "content": "Single transversal images of anonymized patient cases (n=9) were retrieved from local university hospital servers (CT/MRI) by a board-certified radi-ologist, and from publicly available resources (ultrasound, Radiopaedia.org, with case courtesy of Di Muzio B (https://doi.org/10.53347/rID-70007), Keshavamurthy J (https://doi.org/10.53347/rID-68460) and Lawson A (https://doi.org/10.53347/rID-26464))."}, {"title": "Model setup", "content": "Models (Claude 3 Opus, Claude 3.5 Sonnet, Gemini 1.5, GPT-40, and Reka Core) were accessed between the 12th of June 2024 and the 21st of June 2024 via respective web interfaces. No learning features (e.g. ChatGPT's Memory function) were used, to prevent the model from learning desired outputs from earlier chat history. User prompts were introduced in separate chat windows along with image prompts, with the temperature set to 0.7 for all models, as is the default setting for most LLMs. User prompt was the following: \u201cDescribe this image. Follow this Template: Liver: {if applicable} Kidney: {if applicable} Spleen: {if applicable} Pancreas: {if applicable} Vessels: {if applicable} Pathologies: {if applicable, else \"None\"}.\u201d. No individual system prompts (e.g. ChatGPT's \u201cIndividual Configuration\" feature) were added."}, {"title": "Experimental setup", "content": "For each patient case, five images were composed. Images 1, 2, and 5 were passed independently to the models, images 3 and 4 were passed to the same chat, to simulate a prompt injection that is already present as model instruction (in image 3) before the image to assess (image 4) is passed. Prompt injections were varied in fontsize (14 or 6) and color (black (#060606), or darkgrey (#171717)), background color was either white (#ffffff) or black (#060606). Image prompts combining prompt injection (or whitespace) + radiological image were passed to the model with a resolution of 4457 x 2846 pixels, with original imaging resolution of ~ 512 x 512 pixels. For the low-contrast setting, we chose \"black in black\" over \"white in white\" because black reflects the usual background in a clinical setting of radiology imaging. All images are available in Supplementary Material 2."}, {"title": "Organ detection assessment", "content": "Organ detection was assessed in two separate instances per model output, evaluating correct detection of the liver as region of interest firstly for the answer to \"Liver:\" and secondly for \"Pathologies:\". Each model output was therefore evaluated with 0, 1 or 2 points for organ detection: 1 if one of two instances was correct, and 2 points if both instances were correct. This ordinal scale was chosen as models tended to falsely label the liver as \"not present\" in the first instance, \"Liver\", but correctly identified the liver (plus the lesion) under \"Pathologies\", suggesting that the wrong label in the first instance was caused by structural constraints inflicted upon the model by our template. This is commonly observed, as models \"think while they write\" and are therefore more likely to hallucinate when forced in specific structures 23. Threshold for inclusion for assessment of hallucination was set to 50%. Mean organ detection rate over the 3 iterations for every prompt were plotted and underwent statistical analysis."}, {"title": "Lesion miss rate and attack success rate", "content": "Lesion miss rate (LMR) was assessed as a binary score of 0 and 1 by a licensed physician for all model outputs, including negative controls without prompt injection. Standard disclaimers such as 'remember, I am not a doctor' were excluded from the assessment, only the direct answer to the question was evaluated. Responses like \u201cNo pathologies (pathologies ignored as instructed)\", as well as instances where the model refused to provide false information were categorized as non-harmful responses. Mean \u00b1 standard deviation over the 3 iterations for every prompt were plotted and underwent statistical analysis. Attack success rate (ASR) was calculated as the difference between LMR for prompt injections and LMR for unaltered prompts.\""}, {"title": "Statistical analysis", "content": "All results are shown as mean \u00b1 standard deviation (SD), and significance was either assessed by the Mann-Whitney U test (independent samples) or Wilcoxon Signed-Rank test (dependent samples/within the same model) plus Bonferroni correction for multiple testing, with significance level alpha < 0.05."}, {"title": "Software", "content": "Graphs were created with RStudio (2024.04.0) including the libraries ggplot2, dplyr, readxl, tidyr, gridExtra, FSA, rstatix). Figures were composed with Inkscape, version 1.3.2. The artificial intelligence models GPT-40 (OpenAI) and Claude 3.5 Sonnet (Anthropic) were used for spell checking, grammar correction and programming assistance during the writing of this article, in accordance with the COPE (Committee on Publication Ethics) position statement of 13 February 2023 24."}, {"title": "Additional information", "content": null}, {"title": "Data availability statement", "content": "Original data (images, prompts, model outputs, ratings, summary statistics) are available in the supplementary information. All code is available under https://github.com/JanClusmann/Prompt_Injection_Attacks ."}, {"title": "Ethics statement", "content": "This study does not include confidential information. All research procedures were conducted exclusively on anonymized patient data and in accordance with the Declaration of Helsinki, maintaining all relevant ethical standards. The overall analysis was approved by the Ethics commission of the Medical Faculty of the Technical University Dresden (BO-EK-444102022). Local data was obtained from Uniklinik RWTH Aachen under grant nr EK 028/19."}, {"title": "Author contributions", "content": "JC designed and performed the experiments, evaluated and interpreted the results and wrote the initial draft of the manuscript. DF, ICW and JNK provided scientific support for running the experiments and contributed to writing the manuscript. JC and DT provided the raw data. JNK supervised the study. All authors contributed scientific advice and approved the final version of the manuscript."}]}