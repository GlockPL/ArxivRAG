{"title": "Boosting ViT-based MRI Reconstruction from the Perspectives of Frequency Modulation, Spatial Purification, and Scale Diversification", "authors": ["Yucong Meng", "Zhiwei Yang", "Yonghong Shi", "Zhijian Song"], "abstract": "The accelerated MRI reconstruction process presents a challenging ill-posed inverse problem due to the extensive undersampling in k-space. Recently, Vision Transformers (ViTs) have become the mainstream for this task, demonstrating substantial performance improvements. However, there are still three significant issues remain unaddressed: (1) ViTs struggle to capture high-frequency components of images, limiting their ability to detect local textures and edge information, thereby impeding MRI restoration; (2) Previous methods calculate multi-head self-attention (MSA) among both related and unrelated tokens in content, introducing noise and significantly increasing computational burden; (3) The naive feed-forward network in ViTs cannot model the multi-scale information that is important for image restoration. In this paper, we propose FPS-Former, a powerful ViT-based framework, to address these issues from the perspectives of frequency modulation, spatial purification, and scale diversification. Specifically, for issue (1), we introduce a frequency modulation attention module to enhance the self-attention map by adaptively re-calibrating the frequency information in a Laplacian pyramid. For issue (2), we customize a spatial purification attention module to capture interactions among closely related tokens, thereby reducing redundant or irrelevant feature representations. For issue (3), we propose an efficient feed-forward network based on a hybrid-scale fusion strategy. Comprehensive experiments conducted on three public datasets show that our FPS-Former outperforms state-of-the-art methods while requiring lower computational costs.", "sections": [{"title": "Introduction", "content": "Magnetic Resonance Imaging (MRI) offers advantages such as non-radiation, high resolution, and superior contrast, making it essential in clinical diagnostics (Yang et al. 2022; Chen et al. 2022). However, its long scanning times often increase the physical burden on patients. Additionally, involuntary movements like breathing, swallowing, and heartbeats usually blur images, limiting MRI's application. Reducing k-space acquisition can speed up MRI with fewer constraints than hardware modifications, but such undersampling introduces artifacts per the Nyquist theorem (Zeng"}, {"title": "Related Work", "content": "CNN-based MRI Reconstruction\nMRI reconstruction techniques can enhance image quality with less dependency on physiology and hardware, making them more accessible for accelerated MRI. Recent advances in deep learning have spurred the development of CNN-based MRI reconstruction. CMRNet pioneered the application of deep learning in MRI reconstruction by creating an offline CNN to map zero-filled to fully-sampled MRI images (Wang et al. 2016). D5C5 proposed a CNN cascade for dynamic cardiac MRI (Schlemper et al. 2018). DuDoRNet incorporated T1 priors for simultaneous k-space and image restoration (Zhou and Zhou 2020). Dual-OctConv learned multi-scale spatial-frequency features from both real and imaginary components for parallel MRI (Feng et al. 2021). Despite these successes, CNNs exhibit a limited receptive field and struggle to model long-range dependencies. Therefore, CNNs are suboptimal for restoring various image regions and cannot achieve satisfactory reconstruction performance (Khan et al. 2020; Sarvamangala and Kulkarni 2022).\nViT-based MRI Reconstruction\nVision Transformers (ViTs) treat images as sequences of patches and use self-attention to capture global context (Yang et al. 2024a). Compared to CNNs, ViTs have advantages such as capturing global patterns and have been used for MRI reconstruction. As demonstrated in (Lin and Heckel 2022), a ViT tailored for image reconstruction can achieve performance comparable to U-net while providing higher throughput and reduced memory consumption. SLATER addressed unsupervised MRI reconstruction by using a cross-attention module to capture correlations between latent variables and image features (Korkmaz et al. 2022). SwinMR designed a parallel imaging coupled swin transformer-based model for fast CS-MRI (Huang et al. 2022). ReconFormer incorporated a local pyramid and global columnar ViT structure to learn multi-scale features at any stage, enabling enhanced reconstruction performance (Guo et al. 2024).\nHowever, these methods still failed to achieve precise MRI reconstruction because they overlooked inherent issues of ViTs such as loss of high-frequency information, interference among unrelated patches, and the inability to model multi-scale features. By addressing these issues from three perspectives respectively, we propose FPS-Former to boost the performance of ViT-based MRI reconstruction."}, {"title": "Methodology", "content": "Overall Pipeline\nAs shown in Figure 2 (a), our proposed FPS-Former is a hierarchical encoder-decoder framework. Given a low-quality MRI image $I_{in} \\in R^{H \\times W \\times C}$ with spatial resolution $H \\times W$ and channel dimension C, we first perform overlapped image patch embedding with a 3 \u00d7 3 convolution layer. Next, the embedding results are sent to the designed backbone, which stacks $N_o$ HEFR blocks and $N_{i\\in[1,2,3,4]}$ FPS blocks. HEFR block is introduced to provide fine-grained information with multiple CNN-based experts, as shown in Figure 2 (e). The FPS block consists of the Frequency Modulation Attention Module (FMAM), Spatial Purification Attention Module (SPAM), and Scale Diversification Feed-forward Network (SDFN), as shown in Figure 2 (b), (c), and (d), respectively. It is designed to amend the mentioned issues of ViTs and extract hierarchical features with different spatial resolutions and channel dimensions. Then, the extracted features are sent to our decoder, which also includes $N_{i\\in[1,2,3,4]}$ FPS and $N_o$ HEFR blocks. Skip connections are adopted to hierarchically bridge intermediate features between the encoder and decoder. Finally, a Data Consistency (DC) layer is added to reconstruct high-quality MRI image $I_{out}$.\nThe above reconstruction process can be formulated as: $I_{out} = N(I_{in})$, where $N(\\cdot)$ is the overall network and is trained by minimizing the following loss function:\n$L = ||I_{out} - I_{gt}||_1$,\nwhere $I_{gt}$ denotes the ground-truth image, and $|\\cdot|_1$ is the $L1$-norm. The proposed FPS and HEFR blocks will be specifically introduced in the following sections.\nFPS Block\nViTs in MRI reconstruction struggle with high-frequency information loss, irrelevant token interactions, and limited multi-scale feature modeling. To tackle these issues, we propose FPS block consisting of three main modules: Frequency Modulation Attention Module (FMAM), Spatial Purification Attention Module (SPAM), and Scale Diversification Feed-forward Network (SDFN). Formally, given the input feature $f_{in}$, the encoding produces of FPS is defined as:\n$f' = f_{in} + F *P(LN(f_{in})), fout = f' +S(LN(f'))$ (2)\nwhere $LN(\\cdot)$ denotes the layer normalization, $F * P$ represents the combined effect of FMAM and SPAM, and $S(\\cdot)$ denotes the operation of SDFN.\nFrequency Modulation Attention Module Standard ViTs exhibit low-pass filtering characteristics, leading to the loss of high-frequency details such as texture for MRI reconstruction. To address this, we propose the Frequency Modulation Attention Module (FMAM) to recalibrate the importance of frequency at each level. Specifically, as shown in Figure 2 (b), we first use Gaussian functions with different variances to extract multiple Gaussian representations $X$. The process can be formulated as follows:\n$\\frac{1}{\\sigma_m \\sqrt{2 \\pi}} :e^{-\\frac{i^2+j^2}{2\\sigma_m^2}}$\nwhere the input feature $F_{in} \\in R^{H \\times W \\times C}$ is normalized from $f_{in}$. $(i,j)$ corresponds to the spatial location, $O_m\\in[1,2,...,M+1]$ denotes the variance of the Gaussian function for the $m$-th scale, and the symbol represents the"}, {"title": "", "content": "convolution operator. Then we construct the frequency pyramid T by subtracting adjacent elements in X. This process is expressed as:\n$T = \\{T_m\\}_{m=1}^M, T_m = X_{m+1} - X_m$ (4)\nThe frequency pyramid Tis composed of multiple layers, each containing distinct types of frequency information. To achieve a balanced distribution of low and high-frequency components within the model, we conduct Within Frequency MSA operation and effectively aggregate features from each frequency level. Specifically, we first calculate the attention scores S for each level of T as follows:\n$S = \\{S_m\\}_{M=1},S_m = \\sum_{l=1}^I softmax((Q_mK_m)/\\sqrt{d})$ (5)\nwhere I is the number of attention heads, $Q_m$ and $K_m$ are derived from $T_m$ using linear transformations, and $d = (C/I)$ denotes the dimension of each head. Finally, we sum the attention scores in S and multiply the result by the Value (V, derived from Fin) to obtain the result of FMAM Ff:\n$F_f = (\\sum_{m=1}^M(S_m \\in S))V$ (6)\nSpatial Purification Attention Module As shown in Figure 2 (f), MRI images contain clusters of image patches that are similar within each group but distinctly different from those outside the group. Previous ViT-based methods perform a dense MSA operation on all patch tokens simultaneously. This operation leads to noisy interactions among unrelated features, hampering MRI reconstruction. To address this, we propose the Spatial Purification Attention Module (SPAM), which applies a sparsity constraint by computing self-attention only between contextually related tokens to reduce noise and computational complexity.\nSpecifically, as shown in Figure 2 (c), given the input feature map $F_{in}$, we first flatten it into $y = \\{f_j \\in R^{1 \\times C}\\}_{j=1}^{J}$,\nwhere J represents the number of tokens. Subsequently, we use the hash function to aggregate the information and map the C-dimensional tokens $f_j$ into integer hash codes Z. This hash mapping can be formulated as:\n$Z = \\{Z_j \\in Z\\}_{j=1}^J, Z_j = [(\\mathbf{a} \\cdot f_j + b)/r]$ (7)\nwhere $\\mathbf{a} \\in R^C$ and $b \\in R$ are random variables satisfying $\\mathbf{a} \\sim N(0,1)$ and $b \\sim U(0,r)$, $r\\in R$ is a constant, $[.]$ is the floor function. Next, we sort all elements in Y based on their hash code in Z. The j-th sorted element is denoted as $f_j$. Then we split them into groups G, which is expressed as:\n$G = \\{G_n\\}_{n=1}^N,G_n = \\{f_j : n_g + 1 \\leq j \\leq (n + 1)g\\}$ (8)\nwhere N denotes the number of groups, and each group has g elements. With such a scheme, closely related tokens are grouped together. Subsequently, we apply the Within Group MSA operation for each Gn to obtain updated groups G':\n$G' = \\{G'_n\\}_{n=1}^N = \\sum_{i=1}^I W_i head_i (G_n)$ (9)\nwhere $head_i (\\cdot)$ represents the self-attention operation of the i-th head, I is the number of attention heads, and $W_i \\in R^{C \\times d}$ represents the learnable parameters."}, {"title": "", "content": "Next, we take out all the elements from each $G'_n$ and unsort them according to their original positions in Y. We then concatenate the elements to obtain the purified features $F_p$. Finally, we concatenate $[.]$ the purified features $F_p$ with the frequency result $F_f$ from FMAM. A depthwise convolution $f(\\cdot)$ is further applied to aggregate the information. In this way, we retain high-frequency information and achieve spatial purification. The above process is formulated as:\n$F_{out} = f([F_p, F_f])$ (10)\nScale Diversification Feed-forward Network Multi-scale representations have been proven effective in enhancing MRI reconstruction. However, previous methods often focus on integrating single-scale components into feed-forward networks, overlooking the importance of multi-scale feature representations. To address this, we design a Scale Diversification Feed-forward Network (SDFN) by inserting two multi-scale depth-wise convolution paths in the transmission process as shown in Figure 2 (d). Specifically, given an input $F_s$, which is normalized from the above aggregated $F_{out}$, we first expand its channel dimension with 1 \u00d7 1 convolution in the ratio of r. Then, the obtained feature is sent into two parallel branches. During feature transformation, we use 3 \u00d7 3 and 5 \u00d7 5 depthwise convolutions to enhance multi-scale local information extraction. The entire feature fusion process of SDFN can be described as follows:\n$F_s = f_{1\\times1}(LN(F_s)$,\n$F_{p_1} = \\sigma(f_{3\\times3}(F_s)), F_{s_1} = \\sigma(f_{5\\times5}(F_s))$,\n$F_{p_2} = \\sigma(f_{3\\times3} [F_{p_1}, F_{s_1}]), F_{s_2} = \\sigma(f_{5\\times5} [F_{s_1}, F_{p_1}])$,\n$F = f_{1x1} [F_{p_2}, F_{s_2}] + F_s$\nwhere $f_{1\\times1}(\\cdot)$ denotes the 1 \u00d7 1 convolution, $\\sigma(\\cdot)$ is a ReLU activation, $f_{3\\times3}(\\cdot)$ and $f_{5\\times5}(\\cdot)$ denote 3 \u00d7 3 and 5 \u00d7 5 depth-wise convolutions, and $[.]$ is the channel-wise concatenation.\nHybrid Experts Feature Refinement\nInspired by (Chen et al. 2023), we introduce Hybrid Experts Feature Refinement (HEFR) to provide fine-grained information, as shown in Figure 2 (e). Specifically, we extract fine-grained knowledge by carefully selecting multiple CNN operations, referred to as experts. These include average pooling, separable convolution layers, and dilated convolution layers with different kernel sizes. Unlike the traditional approach of combining experts with an external gating network, we employ a self-attention mechanism as a switcher among different experts, adaptively emphasizing the importance of various feature representations based on the input. Specifically, given the input feature $F_h \\in R^{H \\times W \\times C}$, we first apply the channel-wise average to generate a C-dimensional channel descriptor $K\\in R^C$:\n$K = \\frac{1}{HW}\\sum_{i=1}^H \\sum_{j=1}^W F_h (i, j)$ (12)\nwhere $F_h(i, j)$ is the value of feature $F_h$ at spatial location (i, j). Then, the coefficient vector V of each expert is allocated corresponding to the learnable weight matrices $W_1 \\in R^{D \\times C}$ and $W_2 \\in R^{E \\times D}$, i.e., $V = W_2 \\sigma (W_1 K)$."}, {"title": "", "content": "where D is the dimension of the weight matrix, E is the number of experts, and $\\sigma(\\cdot)$ is a ReLU function. Finally, denoting the expert operations as $f_{exp}(\\cdot)$, the output $F_h$ is obtained as:\n$F_h = f_{1x1}(\\sum_{i=1}^E f_{exp}(F_h, V)) + F_h$ (13)"}, {"title": "Experiments", "content": "Experimental Settings\nDatasets and Evaluation Metrics The proposed FPS-Former is evaluated on three datasets: CC359 (Warfield, Zou, and Wells 2004), fastMRI (Zbontar et al. 2018), and SKM-TEA (Desai et al. 2022). The CC359 dataset is a publicly available raw brain MRI dataset acquired from clinical MR scanners (Discovery MR750; GE Healthcare, Wauke-sha, WI, USA). Following the official dataset split, we randomly selected a training set comprising 4, 524 slices from 25 subjects, and a test set consisting of 1,700 slices from an additional 10 subjects. The acquisition matrix size is 256 \u00d7 256; The fastMRI dataset contains 1,172 complex-valued single-coil coronal proton density (PD)-weighted knee MRI scans. Each scan provides approximately 35 coronal cross-sectional knee images with the matrix of size 320 \u00d7 320. We partition this dataset into 973 scans for training and 199 scans (fastMRI validation dataset) for testing; The SKM-TEA raw data track provides 155 complex-valued multi-coil T2-weighted knee MRI scans. 124, 10, and 21 coil-combined volumes are used for training, validation, and testing. Each subject provides approximately 160 cross-sectional knee images with the matrix of size 512 \u00d7 512.\nIn comparison experiments, the input under-sampled image sequences are generated by randomly under-sampling the k-space data using the 1D cartesian under-sampling function similar to the fastMRI challenge (Zbontar et al. 2018). Normalized mean square error (NMSE), structural index similarity (SSIM), and peak signal-to-noise ratio (PSNR) are used as evaluation metrics for comparison.\nTraining Details In our model, (No, N1, N2, N3, N4) are set to (4, 1, 2, 2, 1), and the number of attention heads for (N1, N2, N3, N4) FPS blocks are set to (1, 2, 4, 8). For each FPS block, the number of frequency pyramid levels M in FMAM, the number of groups N in SHAM, and the channel expansion factor r in SDFN are set to (3, 4, 2), respectively. For the HEFR module, we set the number of experts E to 8 and the dimension of the weight matrix D to 32. During training, we used the AdamW optimizer with a batch size of 4 and patch size of 8, for a total of 300K iterations. The initial learning rate is fixed at 1 \u00d7 10-4 for the first 92K iterations, then reduced to 1 \u00d7 10-6 using a cosine annealing schedule over the remaining 208K iterations. The entire framework is implemented on PyTorch using RTX 3090.\nComparison with State-of-the-arts\nSingle-coil datasets We compared the proposed FPS-Former with recent MRI reconstruction approaches, including CNN-based and Transformer-based methods. Additionally, we evaluated it against two state-of-the-art natural image restoration methods, Restormer (Zamir et al. 2022) and AST (Zhou et al. 2024), which were equipped with a DC layer with the same settings of MRI reconstruction for a fair comparison. Table 1 shows the comparison results of our FPS-Former with other methods under different acceleration factors (AF) on single-coil datasets, including CC359 and fastMRI. As shown in this table, FPS-Former demonstrates significant improvements over CNN-based methods and consistently surpasses other Transformer-based approaches across different acceleration rates on both datasets. For example, our method shows the superiority of 2.37 dB over the CNN-based SOTA DCRCN and 0.22 dB over Transformer-based counterpart ReconFormer under 4\u00d7 AF on CC359. Notably, our approach shows greater performance improvement as the acceleration factor increases,"}, {"title": "", "content": "particularly in more challenging scenarios. Specifically, for the CC359 and fastMRI datasets, our model outperforms the leading method AST, by 1.70 dB and 0.51 dB at 8\u00d7 AF, and 1.60 dB and 0.25 dB at 4\u00d7 AF, respectively.\nMulti-coil datasets Table 2 gives comparison results of MRI reconstruction on the multi-coil SKM-TEA dataset. We achieved 35.64 and 32.85 PSNR under 4x and 8\u00d7 AF respectively. Our FPS-Former significantly outperforms previous CNN-based solutions and shows the superiority of 0.47 dB and 0.24 dB over AST at 4\u00d7 AF and 8\u00d7 AF, respectively. This further demonstrates the superiority of our method.\nExperiments on different masks To further demonstrate the robustness of our FPS-Former, we conducted experiments using radial and random undersampling patterns under 5x and 10\u00d7 acceleration factors on CC359 dataset. As shown in Table 3, FPS-Former consistently outperforms other methods, highlighting its ability to effectively reconstruct MRI images from various undersampling masks.\nVisualization Results The qualitative results for the single-coil dataset, multi-coil dataset, and mask experiments are shown in Figure 3 (a), (b), and (c), respectively. In (a), the CNN-based SoTA, D5C5, suffers from severe edge blurring and substantial detail loss. Although the Transformer-based AST and ReconFormer partially alleviate these issues, they still lose crucial anatomical details in challenging tasks with high acceleration factors. In contrast, our FPS-Former demonstrates robustness to various anatomical structures and acceleration factors. By addressing the issues of ViT models, our method better preserves important anatomical details, as highlighted by the zoomed boxes and ellipses. In (b), our FPS-Former can restore more abundant details than other counterparts. This further demonstrates that our method can effectively reconstruct not only single-coil but also multi-coil MRI images. In (c), our method demonstrates great robustness across various undersampling patterns and acceleration rates, further validating its effectiveness.\nAblation Studies and Analysis\nEfficacy of Key Components We first performed a breakdown ablation to investigate the effect of each component and their interactions. As the results listed in Table 4 show, (a) When FMAM is removed, the performance dramatically degrades by 0.24 in PSNR and 0.0027 in SSIM. This drop can be attributed to FMAM's ability to preserve high-frequency details, which are crucial for restoring local textures and edges. (b) Excluding SPAM leads to significant reductions in PSNR and SSIM by 0.55 and 0.0075, respectively. This demonstrates that SPAM effectively mitigates the noise impact from content-irrelevant tokens, thereby enhancing performance. (c) Replacing SDFN with a conventional feed-forward network in the standard ViT results in a decrease in PSNR from 34.38 to 33.95. This highlights SDFN's effectiveness in representing multi-scale features, which is essential for improved MRI reconstruction. (d) The absence of HEFR results in a decline of 0.40 in PSNR and 0.0047 in SSIM, showing its significant contribution."}, {"title": "", "content": "Analysis of FPS block As shown in Table 5, to further analyze the effectiveness of our FPS block, we compared various variants for FMAM, SPAM, and SDFN. Specifically, (1) we investigated the impact of the number of frequency pyramid layers M in FMAM. Our results indicate that both too small and large M negatively affect network performance, with the optimal reconstruction results achieved when M = 3. Fewer layers struggle to capture diverse frequency information, while excessive layers lead to confusion in feature aggregation. (2) We compared our SPAM with several self-attention mechanisms, including global MSA and local window-based MSA. We can see that, SPAM shows the most significant improvement because it performs MSA calculations among tokens with closely related content, effectively reducing noisy interactions. (3) We evaluated SDFN against three baseline methods: the conventional feed-forward network (FN), the depth-wise convolution feed-forward network (DFN), and the gated-depth-wise convolution feed-forward network (GDFN). Although GDFN employs a gating mechanism to enhance performance, it does not leverage multi-scale feature integration, which is crucial for MRI reconstruction. In contrast, our SDFN incorporates local feature extraction and fusion across different scales, achieving a PSNR gain of 0.56 dB over GDFN.\nEfficiency of Frequency Modulation Attention Module\nTo further validate the effectiveness of our FMAM, we follow (Wang et al. 2022) and present a spectral response comparison between network variants with and without FMAM at the last encoder layer, as shown in Figure 4 (a). The frequency response of the network without FMAM exhibits greater attenuation of high frequency compared to FPS-Former. Additionally, we extracted high-frequency structures from the reconstructed images of different methods using a high-pass filter, as illustrated in Figure 4 (b). Due to FMAM's effective preservation of high-frequency details, our method reconstructs edges and textures more accurately and completely. This visual evidence underscores FMAM's superior ability to tackle ViT's low-pass filter issues.\nAnalysis of Training Efficiency The training efficiency comparison is reported in Table 6. The recent ViT-based ReconFormer employs a recurrent structure to maintain a few trainable parameters. However, its significantly higher computational complexity (FLOPs=342G) substantially increases both training difficulty and inference time. On the other hand, AST addresses both spatial and channel redundancy and achieves a notable reduction in FLOPs. Nevertheless, AST has a larger parameter count of 26.10 M and a noticeable drop in performance. Compared to these methods, our FPS-Former achieves significant performance improvements while maintaining both low computational complexity and a minimal number of trainable parameters. This advantage is largely attributed to our SPAM, which minimizes interactions between irrelevant tokens, effectively reducing both noise interference and computational complexity.\nAnalysis of Hyper-parameters The analysis of key hyper-parameters, such as the number of experts in HEFR, the number of frequency pyramid levels in FMAM, the expansion ratio in SDFN, and the number of FPS blocks and HEFR modules, etc., is specifically discussed in the Supplementary Materials. FPS-Former demonstrates consistent performance across different hyper-parameter variations.\nConclusion\nIn this work, we propose to boost ViT-based MRI reconstruction by tackling three issues, including loss of high-frequency information, redundancy interactions among irrelevant tokens, and challenges in multi-scale feature modeling. To achieve this, we propose the frequency modulation attention module for frequency information correction, the spatial purification attention module for grouped token interactions, and the scale diversification feed-forward network for multi-scale feature transmission, respectively. Extensive experiments and analysis are conducted on CC359, fastMRI, and SKM-TEA datasets, validating the efficiency of FPS-Former in tackling the issues of ViT-MRI and significantly improving the performance of MRI reconstruction."}]}