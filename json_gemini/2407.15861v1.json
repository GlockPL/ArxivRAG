{"title": "Adversarial Attacks and Defenses on Text-to-Image Diffusion Models: A Survey", "authors": ["Chenyu Zhang", "Mingwang Hu", "Wenhui Li", "Lanjun Wang"], "abstract": "Recently, the text-to-image diffusion model has gained considerable attention from the community\ndue to its exceptional image generation capability. A representative model, Stable Diffusion, amassed\nmore than 10 million users within just two months of its release. This surge in popularity has\nfacilitated studies on the robustness and safety of the model, leading to the proposal of various\nadversarial attack methods. Simultaneously, there has been a marked increase in research focused\non defense methods to improve the robustness and safety of these models. In this survey, we provide\na comprehensive review of the literature on adversarial attacks and defenses targeting text-to-image\ndiffusion models. We begin with an overview of text-to-image diffusion models, followed by an\nintroduction to a taxonomy of adversarial attacks and an in-depth review of existing attack methods.\nWe then present a detailed analysis of current defense methods that improve model robustness and\nsafety. Finally, we discuss ongoing challenges and explore promising future research directions. For\na complete list of the adversarial attack and defense methods covered in this survey, please refer to\nour curated repository at https://github.com/datar001/Awesome-AD-on-T2IDM.", "sections": [{"title": "1. Introduction", "content": "The text-to-image model aims to generate a range of\nimages based on user-provided prompt. Recent advances\nin deep learning have led to the proposal of numerous\ntext-to-image models [1, 2, 3, 4, 5, 6, 7, 8], significantly\nenhancing the quality of generated images. As a prominent\nbranch of text-to-image models, the diffusion model has\nemerged as a research hotspot owing to its superior image\ngeneration capabilities [9, 10, 11, 12, 13, 14, 15, 16, 17, 18].\nThe representative diffusion model, Stable Diffusion [9]\nand Midjourney [10], boast user bases that exceed 10\nmillion [19] and 14.5 million [20], indicating that text-to-\nimage generation has become an integral part of daily life.\nDespite their advances, the text-to-image diffusion model\nexhibits vulnerabilities in both robustness and safety.\nRobustness ensures that the model can generate images with\nconsistent semantics in response to various input prompts in\nreal-world scenarios. Safety prevents misuse of the model\nin creating malicious images, such as sexual, violent, and\npolitically sensitive images, etc.\nFor the robustness of the model, existing attack methods\nuncover two types of vulnerability: 1) The model often gen-\nerates inaccurate images against the prompt with multiple\nobjects and attributes. Hila et al. [21] highlight that Stable\nDiffusion faces challenges in generating multiple objects\nfrom a single prompt and accurately associating properties\nwith the correct objects. Similarly, Du et al. [22] note that\nsimilarity in the appearance and generation speed of dif-\nferent objects affect the quality of the generated images. 2)\nThe model lacks robustness to the grammatically incorrect\nprompt with subtle noise. Zhuang et al. [23] demonstrate\nthat inserting just five random characters into a prompt can\nsignificantly alter the content of the output images. More-\nover, some typos, glyph alterations, or phonetic changes in\nthe prompt can also destroy the semantics of the image [24].\nFor the safety issues of the model, UnsafeDiffusion [25]\nassesses the safety of both open-source and commercial\ntext-to-image diffusion models, uncovering their potential\nto generate images that are sexually explicit, violent, dis-\nturbing, hateful, or politically sensitive. Although various\nsafeguards [26, 27, 28, 29, 30] are implemented within the\nmodel to mitigate the output of malicious content, several\nadversarial attack methods [31, 32, 33, 34, 35] have been\nproposed to craft the adversarial prompt, which circum-\nvents these safeguards and effectively generates malicious\nimages, posing significant safety threats.\nExposure to vulnerabilities in the robustness and safety\nof text-to-image models underscores the urgent need for\neffective defense mechanisms. For the vulnerability in the\nrobustness of the model, many works [36, 37, 38, 39] are\nproposed to improve the generation capability against the\nprompt with multiple objects and attributes. However, the\nrobustness improvement of the model against the grammat-\nically incorrect prompt with subtle noise remains under-\nexplored. For the vulnerability in the safety of the model,\nexisting defense methods can be divided into two types:\nexternal safeguards and internal safeguards. External safe-\nguards [40, 41, 42] focus on detecting or correcting the\nmalicious prompt before feeding the prompt into the text-\nto-image model. In contrast, internal safeguards [43, 44, 45,\n46, 47, 48] aim to ensure that the semantics of output im-\nages deviate from those of malicious images by modifying\ninternal parameters and features within the model.\nDespite these efforts, significant limitations and chal-\nlenges persist in the field of adversarial attacks and defenses.\nA primary issue with existing attack methods is their lack\nof imperceptibility. Specifically, most attack methods [23,\n31, 33, 34] focus on adding a noise word or rewriting\nthe entire prompt to craft the adversarial prompt. This\nsignificant added noise can be easily detected, thus reducing\nthe attack imperceptibility. Another challenge is that cur-\nrent defense methods are not adequately equipped to deal\nwith the threat posed by adversarial attacks. Specifically,\nexisting methods for improving robustness show a marked\ndeficiency in countering grammatically incorrect prompts\nwith subtle noise [23, 24]. Moreover, while existing defense\nmethods [41, 43, 44, 46, 47, 48] to improve safety are\nsomewhat effective against malicious prompts that explic-\nitly contain malicious words (such as nudity), they are less\nsuccessful against adversarial prompts that cleverly omit\nmalicious words.\nRelated Survey. Although there have been innovative\ncontributions in this field, a comprehensive review work that\nsystematically collates and synthesizes these efforts is cur-\nrently lacking. Existing surveys on adversarial attacks and\ndefenses mainly focus on natural language processing [49,\n50, 51, 52], computer vision [53, 54], explainable artificial\nintelligence [55], federated learning [56], etc. Moreover,\nexisting surveys on the text-to-image diffusion model focus\non the technology development [57, 58, 59, 60], image\nquality evaluation [61], controllable generation [62], etc. In\ncontrast, our work is the first survey dedicated to providing\nan in-depth review of adversarial attacks and defenses on\nthe text-to-image diffusion model (AD-on-T2IDM).\nPaper Selection. Given that AD-on-T2IDM is an emerg-\ning field, we review high-quality papers from leading AI\nconferences such as CVPR, ICCV, NeurIPS, AAAI,\nICLR, ACM MM, WACV, and prominent information\nsecurity conferences like SP and CCS. In addition to\naccepted papers at these conferences, we also consider\nnotable submissions from the e-Print archive, which\nrepresent the cutting edge of current research. We manually\nselected papers submitted to the archive before June 1, 2024,\nusing two criteria: paper quality and methodology novelty.\nPaper Structure. In this work, we begin with an\noverview of text-to-image diffusion models in Sec. 2.\nSubsequently, we review adversarial attacks and defenses on\nthe text-to-image model in Sec. 3 and Sec. 4, respectively.\nNext, we analyze the limitations of existing attack and\ndefense methods and discuss the potential solution in Sec. 5.\nFinally, we conclude this survey in Sec. 6."}, {"title": "2. Text-to-Image Diffusion Model", "content": "Following the previous survey study [57], existing diffu-\nsion models can be roughly divided into two categories: dif-\nfusion model in pixel space and in latent space. Representa-\ntive models in pixel space include GLIDE [16], Imagen [7],\nwhile those in latent space include Stable Diffusion [9]\nand DALLE 2 [63]. In particular, the open-source Stable\nDiffusion has been extensively deployed across various\nplatforms, such as ClipDrop [64], Discord [65], and Dream-\nStudio [66], and has generated over 12 billion images,\nwhich represents 80% of AI images on the Internet [19].\nConsequently, most adversarial attacks [22, 23, 31, 34, 67,\n68, 69, 70] and defenses [43, 44, 45, 46, 71, 72, 73] are\nbased on Stable Diffusion. In the following sections, we\nintroduce the framework of Stable Diffusion. For clarity, we\ndefine the notations used in this work in Table 1.\nStable Diffusion is composed of three primary modules:\nthe image autoencoder, the conditioning network, and the\ndenoising network. Firstly, the image autoencoder is pre-\nrained on a diverse image dataset to achieve the trans-\nformation between the pixel space and the latent space.\nSpecifically, the image autoencoder contains an encoder\nand a decoder, where the encoder aims to transform an\ninput image y into a latent representation z = E(y) and\nthe decoder conversely reconstructs the input image from\nthe latent representation, i.e., D(z) = \u0177 \u2248 y. For the\nconditioning network, Stable Diffusion utilizes a pre-trained\ntext encoder, CLIP [74], to encode the input prompt x\ninto the prompt feature c = Etxt(x), which serves as a\ntext condition to guide the image generation process. The\ndenoising network is a UNet-based diffusion model e to\ncraft the latent representation guided by a text condition.\nIn the following section, we introduce the training and\ninference stages of Stable Diffusion, respectively.\nTraining Stage. Following a pioneering work, DDPM\n[11], the training of Stable Diffusion can be seen as a\nMarkov process in the latent space that iteratively adds\nGaussian noise to the latent representation of the image\nduring a forward process and then restores the latent rep-\nresentation through a reverse denoising process.\nThe forward process aims to iteratively add Gaussian\nnoise to the latent representation of the image until it reaches\na state of random noise. Give an image y, Stable Diffusion\nfirst employs the pre-trained encoder & to obtain the initial\nlatent representation zo = E(y). Then, a diffusion process is\nconducted by adding Gaussian noise to zo as follows:\n$q(z_1|z_{t-1}) = N(z_t; \\sqrt{1 - \\beta_t}z_{t-1}, \\beta_tI), t \\in (0,T)$  (1)\nwhere \u03b2 is the pre-defined hyper-parameter, t and T is\nthe current and total diffusion step. With \u03b1\u2081 = 1 \u2212 \u03b2\u2081 and\n\u1fb6\u2081 = \u03a0=1 \u03b1\u2081, the noisy latent representation z\u012b at timestep t\ncan be obtained as:\n$q(z_4|z_0) = N(z_t; \\sqrt{\\bar{\\alpha_t}}z_0, (1 \u2013 \\sqrt{\\bar{\\alpha_t}})I)$. (2)\nThe reverse process aims to gradually remove the noise\nin the noisy latent representation until it reaches the initial"}, {"title": "3. Attacks", "content": "In this section, we begin by presenting a general frame-\nwork of adversarial attacks in the text-to-image task. Then,\nwe propose a classification of existing adversarial attack\ntechniques that are specifically tailored to the given task.\nFinally, expanding on this categorization, we provide an in-\ndepth analysis of the different approaches utilized.\nGeneral Framework. The attack framework generally con-\nsists of a victim model and a prompt input by the user.\nInitially, the adversary formulates an attack objective based\non the intent. Following this, a perturbation strategy is\ndeveloped to add noise into the prompt. Finally, based on\nthe attack objective and knowledge of the victim model, the\nadversary employs an optimization strategy to optimize\nthe noise to craft the final adversarial prompt.\n3.1. Taxonomy of Adversarial Attacks\nAs shown in Fig. 1, we categorize attack methods based\non three aspects: 1) Target or not, 2) how much Knowledge\nthe adversary has, and 3) the type of Perturbation.\n3.1.1. Target\nBased on the intent of the adversary [49], existing\nattack methods can be divided into two primary categories:\nuntargeted and targeted attacks.\n\u2022 For untargeted attacks, consider a scenario with a prompt\ninput by the user (clean prompt) and its corresponding\noutput image (clean image). The objective of untargeted\nattacks is to subtly perturb the clean prompt to craft an ad-\nversarial prompt, further misleading the victim model to\ngenerate an adversarial image with semantics different\nfrom the clean image. This type of attack is commonly\nused to uncover the vulnerability in the robustness of the\ntext-to-image diffusion model.\n\u2022 For targeted attacks, assumes that the victim model has\nbuilt-in safeguards to filter malicious prompts and resul-\ntant malicious images. These prompts and images often\nexplicitly contain malicious concepts, such as 'nudity',\n'violence', and other predefined concepts. The objective\nof targeted attacks is to obtain an adversarial prompt,\nwhich can bypass these safeguards while inducing the\nvictim model to generate adversarial images containing\nmalicious concepts. This type of attack is typically de-\nsigned to reveal the vulnerability in the safety of the text-\nto-image diffusion model.\n3.1.2. Perturbation\nAs the adversary is to perturb the prompt in text format,\nexisting attack methods can be divided into three categories\naccording to the perturbation granularity [51]: character-\nlevel, word-level, and sentence-level perturbations.\n\u2022 The character-level perturbation involves altering, adding,\nand removing characters within the word.\n\u2022 The word-level perturbation aims to perturb the word\nwithin the prompt, including replacing, inserting, and\ndeleting words within the prompt.\n\u2022 The sentence-level perturbation alters the sentence within\nthe prompt, which typically involves rewriting the prompt."}, {"title": "3.1.3. Knowledge", "content": "Based on the adversary's knowledge of the victim model\n[49], existing methods can be classified as white-box attacks\nand black-box attacks.\n\u2022 White-box attacks involve an adversary having full knowl-\nedge of the victim model, including its architecture and\nparameters. Furthermore, based on the model knowledge,\nthe adversary can craft a gradient-based optimization\nmethod to learn the adversarial prompt.\n\u2022 Black-box attacks occur when the adversary has no\nknowledge of the internal working mechanism of the\nvictim model and relies solely on external output to\ndeduce information and create their attacks. For example,\nBa et al. [32] and Deng et al. [35] utilize API access to\nquery the text-to-image diffusion model.\nIn the following sections, we introduce existing attack\nmethods from two main perspectives: untargeted attacks and\ntargeted attacks."}, {"title": "3.2. Untargeted Attack", "content": "This section will analyze three untargeted attack meth-\nods to reveal the vulnerability in the robustness of the model,\nwhere two of them [22, 23] are white-box attacks, and the\nrest one [24] focuses on the black-box attack.\n3.2.1. White-box Attacks\nEarly works often explore the robustness of text-to-\nimage diffusion models by manually designed prompts.\nTypically, Structure Diffusion [76] observes that some at-\ntributes in the prompt are not correctly aligned with the\nimage content. Additionally, Attend-and-Excite [21] high-\nlights that Stable Diffusion struggles to generate multiple\nobjects from a prompt and fails to accurately bind properties\nto the corresponding objects. However, manually design-\ning adversarial prompts is time-consuming. To achieve the\nsystematical analysis, two white-box untargeted attack stud-\nies (ATM [22] and Zhuang et al. [23]) are proposed. The ma-\njor similarities of these two studies include: a) both set the\npopular open-source text-to-image model, Stable Diffusion\nas the victim model; b) both use Projected Gradient Descent\n(PGD) [77], a classical optimization method in the field of\nadversarial attack, to achieve the attack objective. Despite\nthese similar aspects, these two studies have different attack\nobjectives and perturbation strategies:\n\u2022 Attack Objective. ATM [22] uses an image classifier f\nto identify the semantic label of the output image, and\nmaximizes classification loss to deviate the semantics of\nadversarial images from those of clean images as follows:\n$\\max L_{cls}(f(y_{adv}), arg \\max(f (y_{clean})))$. (5)\nwhere arg max(f(yclean)) is the label of the clean im-\nage yclean and Lels is the cross-entropy loss. However,\noptimizing Eq. 5 is challenging due to the costly com-\nputation complexity involved in the model that maps\nthe prompt to the probability distribution of the gener-\nated image. To address this problem, Zhuang et al. [23]\ntransform the semantic deviation of adversarial images\ninto that of adversarial prompts, and minimize the cosine\nsimilarity between adversarial and clean prompts in the\nCLIP embedding space:\n$\\min cos(E_{txt}(x_{adv}), E_{txt}(x_{clean}))$ (6)\nwhere cos(,) represents the cosine similarity.\n\u2022 Perturbation Strategy. Although both methods provide\nword-level perturbation, the concrete perturbation meth-\nods are different. As shown in Fig. 2(a) and Fig. 2(b),\nATM [22] explores two perturbation strategies: word sub-\nstitution and suffix addition. In contrast, Zhuang et al. [23]\nperturb the clean prompt by appending a noise word with\nfive random characters, as shown in Fig. 2(c).\nIn addition to the above comparison, ATM [22] additionally\nidentifies four patterns of adversarial prompts related to\nvulnerabilities of Stable Diffusion, which further enhance\nthe understanding of researchers about the generation deficit\nof Stable Diffusion."}, {"title": "3.2.2. Black-box Attacks", "content": "There is only one black-box untargeted attack [24],\nwhich achieves the semantic deviation by maximizing the\ndistribution discrepancy between the adversarial and clean\nimages. Specifically, the discrepancy can be measured by\nthe Maximum Mean Discrepancy (MMD) [78]:\n$\\max(\\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} k(y_{i}^{clean}, y_{j}^{clean}) + \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} k(y_{i}^{adv}, y_{j}^{adv}) - \\frac{2}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} 2k(y_{i}^{clean}, y_{j}^{adv}))$ (7)\nwhere N represents the number of the generated clean im-\nages and adversarial images, k(\u00b7, \u00b7) is a kernel function mea-\nsuring the similarity between two images. Different from the\nabove works [22, 23] discussed in the white-box untargeted\nattacks, this work is the character-level perturbation to\nmodify the clean prompt subtly. As shown in Fig. 2(d), it\nfirst identifies keywords in the clean prompt and then uses\ntypos, glyph alterations, or phonetic changes within these\nkeywords. Subsequently, it applies a non-gradient greedy\nsearch algorithm to derive adversarial prompts."}, {"title": "3.2.3. Summary of Untargeted Attacks", "content": "In these untargeted attacks [22, 23, 24], we summa-\nrize two key observations. 1) Adversarial prompts from\nATM [22] often contain multiple objects and attributes,\nrevealing the robustness vulnerability of the victim model\nagainst complex prompts. In contrast, adversarial prompts\ncrafted by Zhuang et al. [23] and Gao et al. [24] are\noften grammatically incorrect and contain noise words,\nrevealing the robustness vulnerability of the victim model\nagainst grammatically incorrect prompts with subtle noise.\n2) ATM [22] and Zhuang et al. [23] employ the word-\nlevel perturbation to craft the adversarial prompt. However,\nsuch significant added noise to the prompt is uncommon\nin real-world scenarios. In contrast, Gao et al. [24] only\nmodify characters within the keyword, introducing a smaller\nperturbation than other methods."}, {"title": "3.3. Targeted Attack", "content": "Targeted attacks aim to bypass the safeguards deployed\nin the victim model while inducing the victim model to gen-\nerate images containing malicious concepts. This section\nstarts by presenting the classification of existing safeguards,\nfollowed by an in-depth exploration of targeted attack meth-\nods derived from this classification.\n3.3.1. Safeguards Classification\nExisting safeguards can be categorized into three types:\nexternal, internal, and black-box safeguards.\n\u2022 External Safeguards. These safeguards operate indepen-\ndently of the model's parameters and architecture, pri-\nmarily checking whether the input prompt and generated\nimage explicitly contain malicious concepts. The preva-\nlent strategies include text-based filters, such as black-\nlists [26, 28] and the malicious prompt classifier [79],\nalong with image-based filters, such as malicious image\nclassifiers [29, 30].\n\u2022 Internal Safeguards. These safeguards aim to deviate the\nsemantics of generated images from those of malicious\nimages by modifying the internal parameters and features\nwithin the model. The representative strategy includes\nmodel editing methods [43, 44] and inference guidance\nmethods [47, 48], which will be introduced in Sec. 4.2.2.\n\u2022 Black-Box Safeguards. These safeguards are encapsulated\nwithin the black-box text-to-image models, such as Mid-\njourney [10] and DALLE 3 [80], and are not directly\naccessible for examination or modification.\nIn the following sections, we introduce existing targeted\nattack methods designed to bypass these safeguards."}, {"title": "3.3.2. Attacking External Safeguards", "content": "Rando et al. [81] pioneered the exploration of vulner-\nabilities in the safety of text-to-image diffusion models.\nThey find that the image-based filter integrated into open-\nsource Stable Diffusion primarily targets sexual content, ig-\noring other malicious concepts, such as bloody, disturbing\ncontent. Moreover, they observe that simply incorporating\nadditional details unrelated to pornography into a sexual\nprompt can effectively circumvent the filter and generate\nsexual images.\nMotivated by Rando et al. [81], a series of following\nstudies [31, 34, 67, 68, 69, 70] design automatic attack\nmethods to reveal vulnerabilities of existing external safe-\nguards. All of these methods target open-source Stable\nDiffusion as the victim model. Moreover, except for two\nof them [31, 34] considering both text-based and image-\nbased filters, other works only consider text-based filters.\nThis section compares these methods in the attack objective,\nperturbation strategy, optimization strategy, and additional\nfactors.\n\u2022 Attack Objective. Natalie et al. [67] introduce an image\nclassifier f and minimize the classification loss to ensure\nthat adversarial images are identified to the category\nassociated with the malicious concept:\n$\\min L_{cls}(f(y_{adv}), arg \\max( f (y_{mal})))$. (8)\nSneakprompt [31], MMA [34], and Shahgir et al. [68] aim\nto maximize the cosine similarity between adversarial\nimages and malicious images in the CLIP embedding\nspace:\n$\\max cos(E_{img}(y_{adv}), E_{img}(y_{mal}))$. (9)\nZhang et al. [69] and RIATIG [70] choose to maximize\nthe cross-modal cosine similarity between adversarial\nprompts and malicious images in the CLIP embedding\nspace:\n$\\max cos(E_{txt}(x_{adv}), E_{img}(y_{mal}))$. (10)"}, {"title": "3.3.3. Attacking Internal Safeguards", "content": "This type of safeguard is commonly built into the\nstructure or parameters of the text-to-image model to correct\nthe generation process of malicious images. Typically, the\nmodel editing strategy [43, 44] is used to erase mali-\ncious concepts (such as nudity) in the diffusion model by\nmodifying the model parameters, which will be discussed\nin Sec. 4.2.2. In this section, five related attack methods\n[33, 83, 84, 85, 86] are reviewed. All of these methods aim\nfor Stable Diffusion with an internal safeguard as the victim\nmodel. Nevertheless, their attack objectives, perturbation\nand optimization strategies differ significantly.\n\u2022 Attack objective. Based on the optimization space of\nthe attack objective, five methods can be categorized\ninto three types: optimization in the class probability\nspace, optimization in the CLIP embedding space, and\noptimization in the latent space.\ni) Optimization in the classification space: Mehrabi et\nal. [85] utilize an image classifier to maximize the clas-\nsification probability of adversarial images belonging to\nthe malicious concept, as shown in Eq. 8.\nii) Optimization in the CLIP embedding space: Tsai\net al. [84] and Ma et al. [86] initially investigate the\ncritical feature \u00ea that represents the erased malicious\nconcept in malicious prompt features using a decoupling\nalgorithm. Subsequently, they introduce the malicious\nconcept by incorporating \u0109 into the feature Etxt(x) of any\ninput prompt, resulting in a problematic feature Etxt(x) =\nEtxt(x) + n\u00ea, where n is the hyper-parameter that controls\nthe malicious level. Finally, they set the attack objective\nas maximizing the cosine similarity between the feature\nof the adversarial prompt and the problematic feature:\n$\\max cos(E_{txt}(x_{adv}), E_{txt}(x))$. (11)"}, {"title": "3.3.4. Attacking Black-Box Safeguards", "content": "In this attack scenario, the adversary lacks knowledge\nof the structure and parameters of the safeguards. This\nsection begins by highlighting two key observations from\nthe works [87, 88], which use manually designed adversarial\nprompts to conduct attacks. Subsequently, we introduce\nthree automatic attack methods [32, 35, 89] tailored for the\nblack-box text-to-image model.\nMilli\u00c3\u00cdre [88] finds that the visual concepts have ro-\nbust associations with the subword. Based on the observa-\ntion, Milli\u00c3\u00cdre designs the \u2018macaronic prompt' by creating\ncryptic hybrid words by combining subword units from\ndifferent languages, which effectively induce DALLE 2\nto generate malicious images. MilliAlre also observes that\nthe morphological similarity can be utilized to craft the\nadversarial prompt. To achieve this goal, Milli\u00c3\u00cdre designs\nthe 'evocative prompting' by crafting words with mor-\nphological features resembling existing words. Moreover,\nStruppek et al. [87] discover that Stable Diffusion and\nDALLE 2 are sensitive to character encoding and can\ninadvertently learn cultural biases associated with different\nUnicode characters. To exploit this observation, they de-\nvelop a manual method to inject a few Unicode characters\nsimilar with English alphabet into the input prompt, which\neffectively biases the image generation and induce cultural\ninfluences and stereotypes into the generated image.\nHowever, manually designing the adversarial prompt is\ntime-consuming. To address this challenge, three automatic\nattack methods [32, 35, 89] are proposed. These methods\nall target DALL-E 3 [80] and Midjourney [10] as victim\nmodels, and all utilize the LLM to conduct the optimization\nprocess. Key differences between them are the perturbation\nand the optimization strategies.\n\u2022 Perturbation strategy. Ba et al. [32] focus on the word-\nlevel perturbation by substituting words associated with\nthe malicious concept (shown in Fig 3(a)). In contrast,\nDivide-and-Conquer [35] and Groot [89] utilize the LLM\nto rewrite the entire sentence for crafting the adversarial\nprompt (shown in Fig 3(f)), which is the sentence-level\nperturbation.\n\u2022\nOptimization strategy. Ba et al. [32] directly ask the LLM\nto obtain substitutive words or phrases. For example, to\ndesensitize the bloody scene, they substitute the word\n\"blood\" with a candidate word by asking the LLM \"which\nliquids have a similar appearance to blood?\". In contrast,\nDivide-and-Conquer [35] utilizes two LLM agents with\ndifferent characters: Divide Prompt agent and Conquer\nPrompt agent, to rewrite the entire prompt. Specifically,\nDivide Prompt agent is designed to describe various\nelements of a malicious image using multiple prompts,\nwhile deliberately avoiding words associated with the\nmalicious concept. Following this, Conquer Prompt agent\nrefines and polishes the outputs from the Divide Prompt\nagent, crafting the final adversarial prompt. Different\nfrom the above two methods, Groot [89] is a framework"}, {"title": "3.3.5. Summary of Targeted Attacks", "content": "This section summarizes targeted attacks tailored for\nthree types of safeguards and provides two key observations.\n1) With the exception of four methods [32, 35, 85, 89] that\nemploy LLM to craft the adversarial prompt, all targeted at-\ntacks result in grammatically incorrect adversarial prompts.\nThese ungrammatical adversarial prompts can be detected\nby grammar detectors, thus reducing the imperceptibility\nof attacks. 2) Given that external and black-box safeguards\ncommonly include text-based filters, e.g. blacklists, attack\nmethods aimed at bypassing these safeguards often adopt\nstrategies that diminish the association between the adver-\nsarial prompt and malicious concepts. However, targeted\nattacks on internal safeguards can disregard this strategy due\nto the lack of explicit text-based filters."}, {"title": "3.4. Summary of attack methods", "content": "Fig. 4 provides a summary of adversarial attacks on text-\nto-image diffusion models. We obtain two key observations.\n1) Targeted attacks outnumber untargeted attacks, indicating\na predominant concern regarding the safety of existing text-\nto-image diffusion models. 2) The number of word-level and\nsentence-level perturbations is more than that of character-\nlevel perturbations. This phenomenon also suggests that\nperturbations in current methods remain relatively easy to\nbe perceptible."}, {"title": "4. Defenses", "content": "In the previous section, we reveal the robustness and\nsafety vulnerabilities of the text-to-image diffusion model\nagainst untargeted and targeted attacks. This section begins\nby outlining the defense goals designed to mitigate these\nvulnerabilities, followed by a detailed review of existing\ndefense methods.\nBased on the defense goal, existing defense methods\ncan be classified into two categories: 1) improving model\nrobustness and 2) improving model safety. The goal of\nrobustness is to ensure that generated images have con-\nsistent semantics with diverse input prompts in practical\napplications. The safety goal is to prevent the generation\nof malicious images in response to both malicious and\nadversarial prompts. Compared to the malicious prompt,\nthe adversarial prompt cleverly omits malicious concepts,\nas shown in Fig. 3. Therefore, ensuring safety against the\nadversarial prompt is more challenging."}, {"title": "4.1. Defense for Robustness", "content": "Some untargeted attacks [21, 22, 76] reveal that Stable\nDiffusion struggles to generate accurate images against\ncomplex prompts containing multiple objects and attributes.\nTo address this challenge, lots of works are proposed, such\nas those improving the text encoder [16, 38], improving\nthe denoising network [15, 90, 91], and those focusing on\ncomposable image generation [21, 76, 92, 93], controlled\nimage generation [39, 94, 95, 96], etc. Nevertheless, our\nstudy omits the analysis on this part, since several related\nsurveys [57, 58, 59, 62, 97] have already discussed the\ndevelopment and advancement of image generation capa-\nbilities.\nThe other untargeted attacks [23, 24] find that appending\nor inserting some characters in the clean prompt can result\nin the semantic deviation of generation images (shown in\nFig. 2(c-d)). Unfortunately, we have not found related works\nthat aim to address this challenge posed by these grammat-\nically incorrect prompts. This leaves an open problem for\nfuture research on the adversarial defense of text-to-image\ndiffusion models."}, {"title": "4.2. Defense for Safety", "content": "Based on the knowledge of the model", "cat-\negories": "external safeguards and internal safeguards, which\nhave been described in Sec. 3.3.1.\n4.2.1. External Safeguards\nAs shown in Fig. 5, this section introduces three external\nsafeguards, where Latent Guard [40", "41": "and GuardT2I [42", "28": "Latent Guard [40"}]}