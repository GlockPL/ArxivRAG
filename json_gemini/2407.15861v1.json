{"title": "Adversarial Attacks and Defenses on Text-to-Image Diffusion Models: A Survey", "authors": ["Chenyu Zhang", "Mingwang Hu", "Wenhui Li", "Lanjun Wang"], "abstract": "Recently, the text-to-image diffusion model has gained considerable attention from the community\ndue to its exceptional image generation capability. A representative model, Stable Diffusion, amassed\nmore than 10 million users within just two months of its release. This surge in popularity has\nfacilitated studies on the robustness and safety of the model, leading to the proposal of various\nadversarial attack methods. Simultaneously, there has been a marked increase in research focused\non defense methods to improve the robustness and safety of these models. In this survey, we provide\na comprehensive review of the literature on adversarial attacks and defenses targeting text-to-image\ndiffusion models. We begin with an overview of text-to-image diffusion models, followed by an\nintroduction to a taxonomy of adversarial attacks and an in-depth review of existing attack methods.\nWe then present a detailed analysis of current defense methods that improve model robustness and\nsafety. Finally, we discuss ongoing challenges and explore promising future research directions. For\na complete list of the adversarial attack and defense methods covered in this survey, please refer to\nour curated repository at https://github.com/datar001/Awesome-AD-on-T2IDM.", "sections": [{"title": "1. Introduction", "content": "The text-to-image model aims to generate a range of\nimages based on user-provided prompt. Recent advances\nin deep learning have led to the proposal of numerous\ntext-to-image models [1, 2, 3, 4, 5, 6, 7, 8], significantly\nenhancing the quality of generated images. As a prominent\nbranch of text-to-image models, the diffusion model has\nemerged as a research hotspot owing to its superior image\ngeneration capabilities [9, 10, 11, 12, 13, 14, 15, 16, 17, 18].\nThe representative diffusion model, Stable Diffusion [9]\nand Midjourney [10], boast user bases that exceed 10\nmillion [19] and 14.5 million [20], indicating that text-to-\nimage generation has become an integral part of daily life.\nDespite their advances, the text-to-image diffusion model\nexhibits vulnerabilities in both robustness and safety.\nRobustness ensures that the model can generate images with\nconsistent semantics in response to various input prompts in\nreal-world scenarios. Safety prevents misuse of the model\nin creating malicious images, such as sexual, violent, and\npolitically sensitive images, etc.\nFor the robustness of the model, existing attack methods\nuncover two types of vulnerability: 1) The model often gen-\nerates inaccurate images against the prompt with multiple\nobjects and attributes. Hila et al. [21] highlight that Stable\nDiffusion faces challenges in generating multiple objects\nfrom a single prompt and accurately associating properties\nwith the correct objects. Similarly, Du et al. [22] note that\nsimilarity in the appearance and generation speed of dif-\nferent objects affect the quality of the generated images. 2)\nThe model lacks robustness to the grammatically incorrect\nprompt with subtle noise. Zhuang et al. [23] demonstrate\nthat inserting just five random characters into a prompt can\nsignificantly alter the content of the output images. More-\nover, some typos, glyph alterations, or phonetic changes in\nthe prompt can also destroy the semantics of the image [24].\nFor the safety issues of the model, UnsafeDiffusion [25]\nassesses the safety of both open-source and commercial\ntext-to-image diffusion models, uncovering their potential\nto generate images that are sexually explicit, violent, dis-\nturbing, hateful, or politically sensitive. Although various\nsafeguards [26, 27, 28, 29, 30] are implemented within the\nmodel to mitigate the output of malicious content, several\nadversarial attack methods [31, 32, 33, 34, 35] have been\nproposed to craft the adversarial prompt, which circum-\nvents these safeguards and effectively generates malicious\nimages, posing significant safety threats.\nExposure to vulnerabilities in the robustness and safety\nof text-to-image models underscores the urgent need for\neffective defense mechanisms. For the vulnerability in the\nrobustness of the model, many works [36, 37, 38, 39] are\nproposed to improve the generation capability against the\nprompt with multiple objects and attributes. However, the\nrobustness improvement of the model against the grammat-\nically incorrect prompt with subtle noise remains under-\nexplored. For the vulnerability in the safety of the model,\nexisting defense methods can be divided into two types:\nexternal safeguards and internal safeguards. External safe-\nguards [40, 41, 42] focus on detecting or correcting the\nmalicious prompt before feeding the prompt into the text-\nto-image model. In contrast, internal safeguards [43, 44, 45,\n46, 47, 48] aim to ensure that the semantics of output im-\nages deviate from those of malicious images by modifying\ninternal parameters and features within the model.\nDespite these efforts, significant limitations and chal-\nlenges persist in the field of adversarial attacks and defenses.\nA primary issue with existing attack methods is their lack\nof imperceptibility. Specifically, most attack methods [23,"}, {"title": "2. Text-to-Image Diffusion Model", "content": "Following the previous survey study [57], existing diffu-\nsion models can be roughly divided into two categories: dif-\nfusion model in pixel space and in latent space. Representa-\ntive models in pixel space include GLIDE [16], Imagen [7],\nwhile those in latent space include Stable Diffusion [9]\nand DALLE 2 [63]. In particular, the open-source Stable\nDiffusion has been extensively deployed across various\nplatforms, such as ClipDrop [64], Discord [65], and Dream-\nStudio [66], and has generated over 12 billion images,\nwhich represents 80% of AI images on the Internet [19].\nConsequently, most adversarial attacks [22, 23, 31, 34, 67,\n68, 69, 70] and defenses [43, 44, 45, 46, 71, 72, 73] are\nbased on Stable Diffusion. In the following sections, we\nintroduce the framework of Stable Diffusion. For clarity, we\ndefine the notations used in this work in Table 1.\nStable Diffusion is composed of three primary modules:\nthe image autoencoder, the conditioning network, and the\ndenoising network. Firstly, the image autoencoder is pre-\nrained on a diverse image dataset to achieve the trans-\nformation between the pixel space and the latent space.\nSpecifically, the image autoencoder contains an encoder\nand a decoder, where the encoder aims to transform an\ninput image y into a latent representation z = E(y) and\nthe decoder conversely reconstructs the input image from\nthe latent representation, i.e., D(z) = \u0177 \u2248 y. For the\nconditioning network, Stable Diffusion utilizes a pre-trained\ntext encoder, CLIP [74], to encode the input prompt x\ninto the prompt feature c = Etxt(x), which serves as a\ntext condition to guide the image generation process. The\ndenoising network is a UNet-based diffusion model e to\ncraft the latent representation guided by a text condition.\nIn the following section, we introduce the training and\ninference stages of Stable Diffusion, respectively.\nTraining Stage. Following a pioneering work, DDPM\n[11], the training of Stable Diffusion can be seen as a\nMarkov process in the latent space that iteratively adds\nGaussian noise to the latent representation of the image\nduring a forward process and then restores the latent rep-\nresentation through a reverse denoising process.\nThe forward process aims to iteratively add Gaussian\nnoise to the latent representation of the image until it reaches\na state of random noise. Give an image y, Stable Diffusion\nfirst employs the pre-trained encoder & to obtain the initial\nlatent representation zo = E(y). Then, a diffusion process is\nconducted by adding Gaussian noise to zo as follows:\nq(z\u2081|21-1) = N(z\u2081; \u221a1 \u2013 Biz-t\u22121, \u03b2\u2081I), t \u2208 (0,T) (1)\nwhere \u03b2 is the pre-defined hyper-parameter, t and T is\nthe current and total diffusion step. With \u03b1\u2081 = 1 \u2212 \u03b2\u2081 and\n\u1fb6\u2081 = \u03a0=1 \u03b1\u2081, the noisy latent representation z\u012b at timestep t\ncan be obtained as:\nq(z4|20) = N(z\u2081; \u221a\u0101rzo, (1 \u2013 \u221a\u0101)I). (2)\nThe reverse process aims to gradually remove the noise\nin the noisy latent representation until it reaches the initial"}, {"title": "3. Attacks", "content": "In this section, we begin by presenting a general frame-\nwork of adversarial attacks in the text-to-image task. Then,\nwe propose a classification of existing adversarial attack\ntechniques that are specifically tailored to the given task.\nFinally, expanding on this categorization, we provide an in-\ndepth analysis of the different approaches utilized.\nGeneral Framework. The attack framework generally con-\nsists of a victim model and a prompt input by the user.\nInitially, the adversary formulates an attack objective based\non the intent. Following this, a perturbation strategy is\ndeveloped to add noise into the prompt. Finally, based on\nthe attack objective and knowledge of the victim model, the\nadversary employs an optimization strategy to optimize\nthe noise to craft the final adversarial prompt.\nAs shown in Fig. 1, we categorize attack methods based\non three aspects: 1) Target or not, 2) how much Knowledge\nthe adversary has, and 3) the type of Perturbation."}, {"title": "3.1. Taxonomy of Adversarial Attacks", "content": "Based on the intent of the adversary [49], existing\nattack methods can be divided into two primary categories:\nuntargeted and targeted attacks.\n\u2022 For untargeted attacks, consider a scenario with a prompt\ninput by the user (clean prompt) and its corresponding\noutput image (clean image). The objective of untargeted\nattacks is to subtly perturb the clean prompt to craft an ad-\nversarial prompt, further misleading the victim model to\ngenerate an adversarial image with semantics different\nfrom the clean image. This type of attack is commonly\nused to uncover the vulnerability in the robustness of the\ntext-to-image diffusion model.\n\u2022 For targeted attacks, assumes that the victim model has\nbuilt-in safeguards to filter malicious prompts and resul-\ntant malicious images. These prompts and images often\nexplicitly contain malicious concepts, such as 'nudity',\n'violence', and other predefined concepts. The objective\nof targeted attacks is to obtain an adversarial prompt,\nwhich can bypass these safeguards while inducing the\nvictim model to generate adversarial images containing\nmalicious concepts. This type of attack is typically de-\nsigned to reveal the vulnerability in the safety of the text-\nto-image diffusion model.\nAs the adversary is to perturb the prompt in text format,\nexisting attack methods can be divided into three categories\naccording to the perturbation granularity [51]: character-\nlevel, word-level, and sentence-level perturbations.\n\u2022 The character-level perturbation involves altering, adding,\nand removing characters within the word.\n\u2022 The word-level perturbation aims to perturb the word\nwithin the prompt, including replacing, inserting, and\ndeleting words within the prompt.\n\u2022 The sentence-level perturbation alters the sentence within\nthe prompt, which typically involves rewriting the prompt."}, {"title": "3.1.3. Knowledge", "content": "Based on the adversary's knowledge of the victim model\n[49], existing methods can be classified as white-box attacks\nand black-box attacks.\n\u2022 White-box attacks involve an adversary having full knowl-\nedge of the victim model, including its architecture and\nparameters. Furthermore, based on the model knowledge,\nthe adversary can craft a gradient-based optimization\nmethod to learn the adversarial prompt.\n\u2022 Black-box attacks occur when the adversary has no\nknowledge of the internal working mechanism of the\nvictim model and relies solely on external output to\ndeduce information and create their attacks. For example,\nBa et al. [32] and Deng et al. [35] utilize API access to\nquery the text-to-image diffusion model.\nIn the following sections, we introduce existing attack\nmethods from two main perspectives: untargeted attacks and\ntargeted attacks."}, {"title": "3.2. Untargeted Attack", "content": "This section will analyze three untargeted attack meth-\nods to reveal the vulnerability in the robustness of the model,\nwhere two of them [22, 23] are white-box attacks, and the\nrest one [24] focuses on the black-box attack."}, {"title": "3.2.1. White-box Attacks", "content": "Early works often explore the robustness of text-to-\nimage diffusion models by manually designed prompts.\nTypically, Structure Diffusion [76] observes that some at-\ntributes in the prompt are not correctly aligned with the\nimage content. Additionally, Attend-and-Excite [21] high-\nlights that Stable Diffusion struggles to generate multiple\nobjects from a prompt and fails to accurately bind properties\nto the corresponding objects. However, manually design-\ning adversarial prompts is time-consuming. To achieve the\nsystematical analysis, two white-box untargeted attack stud-\nies (ATM [22] and Zhuang et al. [23]) are proposed. The ma-\njor similarities of these two studies include: a) both set the\npopular open-source text-to-image model, Stable Diffusion\nas the victim model; b) both use Projected Gradient Descent\n(PGD) [77], a classical optimization method in the field of\nadversarial attack, to achieve the attack objective. Despite\nthese similar aspects, these two studies have different attack\nobjectives and perturbation strategies:\n\u2022 Attack Objective. ATM [22] uses an image classifier f\nto identify the semantic label of the output image, and\nmaximizes classification loss to deviate the semantics of\nadversarial images from those of clean images as follows:\nmax Lcls(f(yadv), arg max(f (yclean))). (5)\nwhere arg max(f(yclean)) is the label of the clean im-\nage yclean and Lels is the cross-entropy loss. However,\noptimizing Eq. 5 is challenging due to the costly com-\nputation complexity involved in the model that maps\nthe prompt to the probability distribution of the gener-\nated image. To address this problem, Zhuang et al. [23]\ntransform the semantic deviation of adversarial images\ninto that of adversarial prompts, and minimize the cosine\nsimilarity between adversarial and clean prompts in the\nCLIP embedding space:\nmin cos(Etxt(xadv), Etxt(xclean)) (6)\nwhere cos(,) represents the cosine similarity.\n\u2022 Perturbation Strategy. Although both methods provide\nword-level perturbation, the concrete perturbation meth-\nods are different. As shown in Fig. 2(a) and Fig. 2(b),\nATM [22] explores two perturbation strategies: word sub-\nstitution and suffix addition. In contrast, Zhuang et al. [23]\nperturb the clean prompt by appending a noise word with\nfive random characters, as shown in Fig. 2(c).\nIn addition to the above comparison, ATM [22] additionally\nidentifies four patterns of adversarial prompts related to\nvulnerabilities of Stable Diffusion, which further enhance\nthe understanding of researchers about the generation deficit\nof Stable Diffusion."}, {"title": "3.2.2. Black-box Attacks", "content": "There is only one black-box untargeted attack [24],\nwhich achieves the semantic deviation by maximizing the\ndistribution discrepancy between the adversarial and clean\nimages. Specifically, the discrepancy can be measured by\nthe Maximum Mean Discrepancy (MMD) [78]:\nmax (\nN\nNN\n\u03a3\u03a3k(yclean clean) +\ni=1 j=1\nNN\n2k(yclean Vady))\nN2\ni=1 j=1\n1\nNN\n2\u03a3\u03a3\ni=1 j=1\nadvadv\n(7)\nwhere N represents the number of the generated clean im-\nages and adversarial images, k(\u00b7, \u00b7) is a kernel function mea-\nsuring the similarity between two images. Different from the\nabove works [22, 23] discussed in the white-box untargeted\nattacks, this work is the character-level perturbation to\nmodify the clean prompt subtly. As shown in Fig. 2(d), it\nfirst identifies keywords in the clean prompt and then uses\ntypos, glyph alterations, or phonetic changes within these\nkeywords. Subsequently, it applies a non-gradient greedy\nsearch algorithm to derive adversarial prompts."}, {"title": "3.2.3. Summary of Untargeted Attacks", "content": "In these untargeted attacks [22, 23, 24], we summa-\nrize two key observations. 1) Adversarial prompts from\nATM [22] often contain multiple objects and attributes,\nrevealing the robustness vulnerability of the victim model\nagainst complex prompts. In contrast, adversarial prompts\ncrafted by Zhuang et al. [23] and Gao et al. [24] are\noften grammatically incorrect and contain noise words,\nrevealing the robustness vulnerability of the victim model\nagainst grammatically incorrect prompts with subtle noise.\n2) ATM [22] and Zhuang et al. [23] employ the word-\nlevel perturbation to craft the adversarial prompt. However,\nsuch significant added noise to the prompt is uncommon\nin real-world scenarios. In contrast, Gao et al. [24] only\nmodify characters within the keyword, introducing a smaller\nperturbation than other methods."}, {"title": "3.3. Targeted Attack", "content": "Targeted attacks aim to bypass the safeguards deployed\nin the victim model while inducing the victim model to gen-\nerate images containing malicious concepts. This section\nstarts by presenting the classification of existing safeguards,\nfollowed by an in-depth exploration of targeted attack meth-\nods derived from this classification."}, {"title": "3.3.1. Safeguards Classification", "content": "Existing safeguards can be categorized into three types:\nexternal, internal, and black-box safeguards.\n\u2022 External Safeguards. These safeguards operate indepen-\ndently of the model's parameters and architecture, pri-\nmarily checking whether the input prompt and generated\nimage explicitly contain malicious concepts. The preva-\nlent strategies include text-based filters, such as black-\nlists [26, 28] and the malicious prompt classifier [79],\nalong with image-based filters, such as malicious image\nclassifiers [29, 30].\n\u2022 Internal Safeguards. These safeguards aim to deviate the\nsemantics of generated images from those of malicious\nimages by modifying the internal parameters and features\nwithin the model. The representative strategy includes\nmodel editing methods [43, 44] and inference guidance\nmethods [47, 48], which will be introduced in Sec. 4.2.2.\n\u2022 Black-Box Safeguards. These safeguards are encapsulated\nwithin the black-box text-to-image models, such as Mid-\njourney [10] and DALLE 3 [80], and are not directly\naccessible for examination or modification.\nIn the following sections, we introduce existing targeted\nattack methods designed to bypass these safeguards."}, {"title": "3.3.2. Attacking External Safeguards", "content": "Rando et al. [81] pioneered the exploration of vulner-\nabilities in the safety of text-to-image diffusion models.\nThey find that the image-based filter integrated into open-\nsource Stable Diffusion primarily targets sexual content, ig-\noring other malicious concepts, such as bloody, disturbing\ncontent. Moreover, they observe that simply incorporating\nadditional details unrelated to pornography into a sexual\nprompt can effectively circumvent the filter and generate\nsexual images.\nMotivated by Rando et al. [81], a series of following\nstudies [31, 34, 67, 68, 69, 70] design automatic attack\nmethods to reveal vulnerabilities of existing external safe-\nguards. All of these methods target open-source Stable\nDiffusion as the victim model. Moreover, except for two\nof them [31, 34] considering both text-based and image-\nbased filters, other works only consider text-based filters.\nThis section compares these methods in the attack objective,\nperturbation strategy, optimization strategy, and additional\nfactors.\n\u2022 Attack Objective. Natalie et al. [67] introduce an image\nclassifier f and minimize the classification loss to ensure\nthat adversarial images are identified to the category\nassociated with the malicious concept:\nmin Lcls(f(yadv), arg max( f (ymal))). (8)\nSneakprompt [31], MMA [34], and Shahgir et al. [68] aim\nto maximize the cosine similarity between adversarial\nimages and malicious images in the CLIP embedding\nspace:\nmax cos(Eimg(yadv), Eimg(ymal)). (9)\nZhang et al. [69] and RIATIG [70] choose to maximize\nthe cross-modal cosine similarity between adversarial\nprompts and malicious images in the CLIP embedding\nspace:\nmax cos(Etxt(xadv), Eimg(ymal)). (10)"}, {"title": "3.3.3. Attacking Internal Safeguards", "content": "This type of safeguard is commonly built into the\nstructure or parameters of the text-to-image model to correct\nthe generation process of malicious images. Typically, the\nmodel editing strategy [43, 44] is used to erase mali-\ncious concepts (such as nudity) in the diffusion model by\nmodifying the model parameters, which will be discussed\nin Sec. 4.2.2. In this section, five related attack methods\n[33, 83, 84, 85, 86] are reviewed. All of these methods aim\nfor Stable Diffusion with an internal safeguard as the victim\nmodel. Nevertheless, their attack objectives, perturbation\nand optimization strategies differ significantly.\n\u2022 Attack objective. Based on the optimization space of\nthe attack objective, five methods can be categorized\ninto three types: optimization in the class probability\nspace, optimization in the CLIP embedding space, and\noptimization in the latent space.\ni) Optimization in the classification space: Mehrabi et\nal. [85] utilize an image classifier to maximize the clas-\nsification probability of adversarial images belonging to\nthe malicious concept, as shown in Eq. 8.\nii) Optimization in the CLIP embedding space: Tsai\net al. [84] and Ma et al. [86] initially investigate the\ncritical feature \u00ea that represents the erased malicious\nconcept in malicious prompt features using a decoupling\nalgorithm. Subsequently, they introduce the malicious\nconcept by incorporating \u0109 into the feature Etxt(x) of any\ninput prompt, resulting in a problematic feature Etxt(x) =\nEtxt(x) + n\u00ea, where n is the hyper-parameter that controls\nthe malicious level. Finally, they set the attack objective\nas maximizing the cosine similarity between the feature\nof the adversarial prompt and the problematic feature:\nmax cos(Etxt(xadv), Etxt(x)). (11)"}, {"title": "3.3.4. Attacking Black-Box Safeguards", "content": "In this attack scenario, the adversary lacks knowledge\nof the structure and parameters of the safeguards. This\nsection begins by highlighting two key observations from\nthe works [87, 88], which use manually designed adversarial\nprompts to conduct attacks. Subsequently, we introduce\nthree automatic attack methods [32, 35, 89] tailored for the\nblack-box text-to-image model.\nMilli\u00c3\u00cdre [88] finds that the visual concepts have ro-\nbust associations with the subword. Based on the observa-\ntion, Milli\u00c3\u00cdre designs the \u2018macaronic prompt' by creating\ncryptic hybrid words by combining subword units from\ndifferent languages, which effectively induce DALLE 2\nto generate malicious images. MilliAlre also observes that\nthe morphological similarity can be utilized to craft the\nadversarial prompt. To achieve this goal, Milli\u00c3\u00cdre designs\nthe 'evocative prompting' by crafting words with mor-\nphological features resembling existing words. Moreover,\nStruppek et al. [87] discover that Stable Diffusion and\nDALLE 2 are sensitive to character encoding and can\ninadvertently learn cultural biases associated with different\nUnicode characters. To exploit this observation, they de-\nvelop a manual method to inject a few Unicode characters\nsimilar with English alphabet into the input prompt, which\neffectively biases the image generation and induce cultural\ninfluences and stereotypes into the generated image.\nHowever, manually designing the adversarial prompt is\ntime-consuming. To address this challenge, three automatic\nattack methods [32, 35, 89] are proposed. These methods\nall target DALL-E 3 [80] and Midjourney [10] as victim\nmodels, and all utilize the LLM to conduct the optimization\nprocess. Key differences between them are the perturbation\nand the optimization strategies.\n\u2022 Perturbation strategy. Ba et al. [32] focus on the word-\nlevel perturbation by substituting words associated with\nthe malicious concept (shown in Fig 3(a)). In contrast,\nDivide-and-Conquer [35] and Groot [89] utilize the LLM\nto rewrite the entire sentence for crafting the adversarial\nprompt (shown in Fig 3(f)), which is the sentence-level\nperturbation.\n\u2022 Optimization strategy. Ba et al. [32] directly ask the LLM\nto obtain substitutive words or phrases. For example, to\ndesensitize the bloody scene, they substitute the word\n\"blood\" with a candidate word by asking the LLM \"which\nliquids have a similar appearance to blood?\". In contrast,\nDivide-and-Conquer [35] utilizes two LLM agents with\ndifferent characters: Divide Prompt agent and Conquer\nPrompt agent, to rewrite the entire prompt. Specifically,\nDivide Prompt agent is designed to describe various\nelements of a malicious image using multiple prompts,\nwhile deliberately avoiding words associated with the\nmalicious concept. Following this, Conquer Prompt agent\nrefines and polishes the outputs from the Divide Prompt\nagent, crafting the final adversarial prompt. Different\nfrom the above two methods, Groot [89] is a framework"}, {"title": "3.3.5. Summary of Targeted Attacks", "content": "This section summarizes targeted attacks tailored for\nthree types of safeguards and provides two key observations.\n1) With the exception of four methods [32, 35, 85, 89] that\nemploy LLM to craft the adversarial prompt, all targeted at-\ntacks result in grammatically incorrect adversarial prompts.\nThese ungrammatical adversarial prompts can be detected\nby grammar detectors, thus reducing the imperceptibility\nof attacks. 2) Given that external and black-box safeguards\ncommonly include text-based filters, e.g. blacklists, attack\nmethods aimed at bypassing these safeguards often adopt\nstrategies that diminish the association between the adver-\nsarial prompt and malicious concepts. However, targeted\nattacks on internal safeguards can disregard this strategy due\nto the lack of explicit text-based filters."}, {"title": "3.4. Summary of attack methods", "content": "Fig. 4 provides a summary of adversarial attacks on text-\nto-image diffusion models. We obtain two key observations.\n1) Targeted attacks outnumber untargeted attacks, indicating\na predominant concern regarding the safety of existing text-\nto-image diffusion models. 2) The number of word-level and\nsentence-level perturbations is more than that of character-\nlevel perturbations. This phenomenon also suggests that\nperturbations in current methods remain relatively easy to\nbe perceptible."}, {"title": "4. Defenses", "content": "In the previous section, we reveal the robustness and\nsafety vulnerabilities of the text-to-image diffusion model\nagainst untargeted and targeted attacks. This section begins\nby outlining the defense goals designed to mitigate these\nvulnerabilities, followed by a detailed review of existing\ndefense methods.\nBased on the defense goal, existing defense methods\ncan be classified into two categories: 1) improving model\nrobustness and 2) improving model safety. The goal of\nrobustness is to ensure that generated images have con-\nsistent semantics with diverse input prompts in practical\napplications. The safety goal is to prevent the generation\nof malicious images in response to both malicious and\nadversarial prompts. Compared to the malicious prompt,\nthe adversarial prompt cleverly omits malicious concepts,\nas shown in Fig. 3. Therefore, ensuring safety against the\nadversarial prompt is more challenging."}, {"title": "4.1. Defense for Robustness", "content": "Some untargeted attacks [21, 22, 76] reveal that Stable\nDiffusion struggles to generate accurate images against\ncomplex prompts containing multiple objects and attributes.\nTo address this challenge, lots of works are proposed, such\nas those improving the text encoder [16, 38], improving"}, {"title": "4.2. Defense for Safety", "content": "Based on the knowledge of the model, existing defense\nmethods for improving safety can be classified into two cat-\negories: external safeguards and internal safeguards, which\nhave been described in Sec. 3.3.1."}, {"title": "4.2.1. External Safeguards", "content": "As shown in Fig. 5, this section introduces three external\nsafeguards, where Latent Guard [40] focuses on the prompt\nclassifier, POSI [41] and GuardT2I [42] focus on the prompt\ntransformation.\nPrompt Classifier. Inspired by blacklist-based approaches\n[26, 28], Latent Guard [40] aims to identify malicious\nprompts by evaluating the distance between the malicious\nprompt and malicious concepts in the projective space.\nSpecifically, they first add a learnable MLP layer on top of\nthe frozen CLIP text encoder to project the prompt feature.\nSubsequently, a contrastive learning strategy is designed\nto close the distance between the projective features of\nmalicious prompts and those of malicious concepts, while\npushing the distance between the projective features of safe\nprompts and those of malicious concepts. In the inference\nstage, they use a distance threshold to filter prompts closed\nto malicious concepts in the projective space. Experiments\non Stable Diffusion demonstrate effective filter ability\nagainst both malicious prompts and adversarial prompts.\nPrompt Transformation. This technology aims to rewrite\ninput prompts by training a language model, which can\nbe deployed prior to feeding the prompt into the text-to-\nimage model. Two methods, POSI [41] and GuardT2I [42],\nuse this technology to ensure the safety of the model.\nKey differences between them include the transformation\nobjective and the model safety:\n\u2022 Transformation Objective. POSI aims to convert mali-\ncious prompts to safe prompts by training a lightweight\nlanguage model. In contrast, GuardT2I focuses on the\ntransformation of ungrammatical adversarial prompts\ninto natural language expressions, followed by a blacklist\nand a prompt classifier to achieve safety.\n\u2022 Model Safety. POSI focuses solely on safety against\nmalicious prompts. Conversely, GuardT2I achieves safety\nagainst both malicious and adversarial prompts by a two-\nstage framework."}, {"title": "4.2.2. Internal Safeguards", "content": "This defense strategy aims to ensure that generated\nimages do not contain malicious content, even using a\nmalicious prompt, by modifying the internal parameters or\nfeatures within the model. Based on the modified object,\nexisting internal safeguards can be divided into two cate-\ngories: Model Editing and Inference Guidance. As shown in\nFig. 6, model editing methods aim to modify the internal\nparameters by a training process [43, 44, 45, 46, 71, 72,\n73, 98, 99, 100, 101, 102, 103, 104]. In contrast, inference\nguidance methods focus on modifying internal features in\nthe inference stage [47, 48]. Notably, due to the need to\naccess the model's internal parameters and features, all\nmethods use open-source Stable Diffusion as the target\nmodel."}, {"title": "Model Editing", "content": "Following Yao et al. [105], we define\nmodel editing as a technology that alters the behavior\nof models against malicious concepts x mal (such as 'nu-\ndity'), while ensuring no adverse impact on non-malicious\nconcepts x We identify three key steps typically in-\nvolved in a model editing method: a) Designing the edit-\ning objective and constraint, where the editing objective\ninvolves defining how to edit malicious concepts and the\nconstraint aims to maintain the generation utility of the\ndiffusion model on non-malicious concepts. b) Determining\noptimized model parameters. This step identifies which\nparameters in the model can be modified. c) Conducting\nthe optimization process by an optimization method. This\nstep focuses on figuring out how to optimize the model\nparameter effectively. In the following sections, we compare\nexisting model editing methods from the above three steps\nand the model safety.\n\u2022 Editing Objective and Constraint. Based on the optimiza-\ntion space, existing editing methods can be divided into\nthree types: editing the latent noise in the latent space,\nediting the output feature of the cross-attention layer in\nthe intermediate feature space, and editing the prompt\nfeature in CLIP embedding space.\ni) Editing in the latent space: A representative strategy\nsummed from a series of studies [43, 44, 71, 98, 99, 100,\n101, 102, 106] is to align the latent noise of the malicious\nconcept and that of the anchor concept to achieve the\nediting objective. Meanwhile, a constraint on the latent\nnoise of non-malicious concepts [44, 103, 106] is used to\nmaintain the generationutility of the diffusion model:\nmin ||\u20ac (21, Etxt (xal), t) - \u20acal||2\n\u03b8\nEditing Objective\ns.t. ||\u20ac9(2.1, Etxt(xn-mal), t) \u2013 Ep1 (21, Etxt(xn-mal), t)||2 < 8\nConstraint\n(14)\nwhere & is a threshold to control the deviation range\nof the generation utility in non-malicious concepts, and\ne is the latent noise of the anchor concept predicted by\nthe frozen denoising network Egr. Specifically, ea can be\ngenerated by two strategies: (a) Predefining an anchorconcept xanc, i.e., \u20aca = \u20ac\u00f8r(Z\u2081, Etxt(xanc), t) [44, 98, 106].\nFor example, when the malicious concept is 'nudity',\nwe can predefine the anchor concept as 'modesty'. (b)\nClassifier-free guidance: Following ESD [43], several\nmethods [71, 99, 100, 101, 102, 103] obtain ea by min-\nimizing the likelihood of generating an image that is\nlabeled as the specific malicious concept x mal\nXmal\n\u20ac\u03b1 = \u20ac\u03c1\u03b9 (\u03961, $, t) \u2013 \u03b7[\u03b5\u03c1\u03b9 (21, Etxt(xmal), t) \u2013 \u20ac\u03c1\u03b9 (21, $,t)]\n(15)\nwhere \u20ac\u03c1\u03b9 (\u0396., \u03a6, t) represents the latent noise of the un-\nconditional term and \u03b7 denotes the guidance scale. Ad-\nditionally, Hong et al. [99] eliminate the constraint on\nnon-malicious concepts and directly constrain the drift of\nthe latent noise in the unconditional term to preserve the\nmodel utility. Therefore, Eq. 14 is rewritten as:\nmin ||60(21, Etxt(xmal), t) \u2013 \u20aca||2\n\u03b8\ns.t. ||\u20acg(Z1, $, t) \u2013 \u20ac\u03c1\u03b9 (\u0396\u2081, \u03a6,t)||2 < \u03b4.\n(16)\nii) Editing in the intermediate feature space: In the text-\nto-image diffusion model, the denoising network utilizes\nthe cross-attention mechanism to explore the relevance\nof input prompt to the generated image [46]. Therefore,\nseveral works [45, 46, 72, 107] implement the editing\nobjective and constraint in the output feature of the cross-\nattention layer, aiming to modify the relevance of the\nmalicious concept to the generated image:\nmin\nmin ||W * Etxt(xmal) - W' * Etxt(xanc)||2\ns.t. ||W * Etxt(xn-mal) - W' * Etxt(xn-mat)||2 < \u03b4\n(17)\nwhere W and W' are the parameters of the optimized\nand frozen cross-attention layer. In addition to the above\nmethods that edit the output feature of the cross-attention\nlayer, FMN [108] directly minimizes the cross-attention\nmap of the malicious concept to make the model forget\nthe malicious concept.\niii) Editing in the CLIP embedding space: Safe-CLIP [73]\nfocuses on the fine-tuning of the CLIP text encoder and"}, {"title": "4.3. Summary of Defense Methods", "content": "In defense methods for improving robustness, researchers\noften focus on improving image quality in response to\nprompts with multiple objects and attributes (shown in\nFig. 2(a)), while overlooking the robustness of the model\nagainst grammatically incorrect prompts with subtle noise\n(shown in Fig. 2(b-d)). However, ensuring the robustness\nof the model against prompts with subtle noise is also\ncrucial, as users frequently make typographical errors in\na practical situation. For the defense for improving safety,\nwe summarize related methods in Fig. 7. Most methods\nconcentrate on the internal safeguards, particularly those\ninvolving model editing strategies. This highlights thesignificant efforts by researchers to ensure the safety of AI-\ngenerated content. Nevertheless, most methods only focus\non the safety against malicious prompt, leaving substantial\nchallenges in countering adversarial attacks."}, {"title": "5. Challenge and Future Directions", "content": "Although lots of works are proposed to reveal and fur-\nther mitigate the vulnerabilities of text-to-image diffusion\nmodels, existing adversarial attacks and defenses still face\nsignificant challenges. This section introduces the potential\nchallenges associated with existing methods, and explores\nthe possible solutions to address these challenges."}, {"title": "5.1. Imperceptibility of Adversarial Attack", "content": "As shown in Fig. 4, most attacks employ word-level\nand sentence-level perturbation strategies. This significant\nadded noise to the input prompt can be easily detected, thus\nreducing the imperceptibility of the adversarial attack. Con-\nsequently, a challenge is how to improve imperceptibility\nin attacks. This section discusses the imperceptibility of\nuntargeted and targeted attacks, respectively."}, {"title": "5.1.1. Imperceptibility of Untargeted Attacks", "content": "Recent works [23, 67] find that subtle noise in the\nprompt can lead to substantial differences in the model out-\nput. In practice, such noise is typically imperceptible, such\nas additional spaces or characters within the input prompt.\nHowever, existing untargeted attacks primarily focus on\nword-level perturbations (as shown in Fig. 2(b,c)), which\nare uncommon in real-world scenarios. Consequently, a\nchallenge remains: How can we effectively achieve attacks\nusing prompts with subtle noises that a user can input?\nOne possible solution is to utilize the character-level\nperturbation. For example, Gao et al. [24] employ typos,\nglyph alterations, and phonetic changes to carry out their\nattacks. However, the attack scenarios produced by Gao\net al. [24] require further discussion, as their adversarial\nprompts significantly deviate from clean prompts in the\ntext semantics. Specifically, they introduce character-level"}, {"title": "5.1.2. Imperceptibility of Targeted Attacks", "content": "According to Fig. 3, all targeted attacks result in gram-\nmatically incorrect adversarial prompts (Fig. 3(a-e)), except\nfor those methods that utilize the LLM to craft the adversar-\nial prompt (Fig. 3(f)). This type of grammatically incorrect\nprompt can be identified by a grammar detector, further\ndiminishing the imperceptibility of targeted attacks. Con-\nsequently, an open question for researchers remains: Can\nwe employ adversarial prompts with correct grammatical\nstructure to efficiently achieve targeted attacks?\nOne potential approach is to leverage the LLM to con-\nduct adversarial attacks, as demonstrated by Ba et al. [32]\nand the Divide-and-Conquer method [35]. However, au-\ntomatically crafting adversarial prompts from malicious\nprompts is a non-trivial task, which requires the LLM to\naddress several critical challenges: (a) Understanding why\na malicious prompt cannot bypass existing safeguards. (b)\nExploring perturbation strategies to modify the malicious\nprompt into the adversarial prompt. (c) Optimizing the\nadversarial prompt based on the feedback from the attack\nprocess. Existing methods based on LLM often require sub-\nstantial human intervention in these steps [32, 35, 85, 89],\nwhich limits their capabilities and generalization. Notably,\nthe rise of LLM-based multi-agent approaches indicates a\ntrend toward using multiple LLMs to collaboratively tackle\ncomplex NLP tasks [110, 111, 112, 113, 114]. Therefore,\ndesigning an LLM-based multi-agent collaborative frame-\nwork to automate adversarial attacks represents an intrigu-\ning and challenging endeavor."}, {"title": "5.2. Defense Effectiveness against Adversarial Attacks", "content": "In Sec. 4, we present defense methods for improving\nthe robustness and safety of text-to-image diffusion models.\nHowever, these methods still exhibit vulnerabilities to\nadversarial attacks. For instance, there is a notable de-\nficiency in defense mechanisms against untargeted attacks\nthat utilize grammatically incorrect prompts with subtle\nnoise (such as appending five random characters in Fig. 2(c)\nand incorporating typos in Fig. 2(d)). Moreover, most de-\nfense methods designed to improve safety primarily focus\non addressing malicious prompts while neglecting adversar-\nial prompts, thereby limiting their overall effectiveness. This\nsection examines the challenges faced by defense methods\nin addressing untargeted and targeted attacks."}, {"title": "5.2.1. Defense Effectiveness against Untargeted Attacks", "content": "Current defense methods against untargeted attacks pri-\nmarily address the generation defect in prompts with multi-\nple objects and attributes, as illustrated in Fig. 2(a). How-\never, for untargeted attacks that utilize ungrammatically\nincorrect prompts with subtle noise (depicted in Fig. 2(b-d)),\nmature solutions are still lacking.\nOne straightforward approach is to fine-tune the CLIP\ntext encoder with grammatically incorrect adversarial prompts,\nwhich can mitigate the impact of noise within prompts. Nev-\nertheless, this solution presents a critical challenge: How\ncan we maintain the fundamental encoding capabilities of\nthe CLIP text encoder? Given that CLIP is a foundational\nmodel pre-trained on a vast corpus of image-text pairs (two\nbillion pairs) [115, 116] and contains extensive cross-modal\nsemantic information, fine-tuning the model directly with a\nsmall set of adversarial prompts inevitably impair its basic\nencoding functionality [117, 118]. An alternative strategy\ninvolves using model editing methods [45, 46, 72, 107],\nwhich modify only a subset of the parameters of the CLIP\nmodel to correct behaviors against adversarial prompts. This\napproach allows for targeted modifications while preserving\nthe core encoding capabilities."}, {"title": "5.2.2. Defense Effectiveness against Targeted Attacks", "content": "As shown in Fig. 7, there are five defense methods [40,\n42, 101, 102, 103] that consider both the malicious and\nadversarial prompts. However, they primarily address ad-\nversarial prompts that utilize random noise words (illus-\ntrated in Fig.3(e)), while overlooking other types of ad-\nversarial prompts, such as those utilizing the word-level\nperturbation (shown in Fig. 3(a-c)) and those crafted by\nthe LLM (depicted in Fig. 3(f)). Consequently, this raises\na significant challenge: How can we simultaneously defend\nall types of adversarial prompts?\nThis problem cannot be addressed directly due to the\nrapid development of adversarial attacks. Notably, a novel\nwork proposed by Zhang et al. [69] seeks to reveal under-\nlying patterns of adversarial prompts. A potential strategy\ninvolves leveraging these patterns, rather than adversarial\nprompts, to guide the defense process. This approach can\nbe a promising future direction for enhancing safeguards."}, {"title": "6. Conclusion", "content": "In this work, we present a comprehensive review of ad-\nversarial attacks and defenses on the text-to-image diffusion\nmodel. We first provide an overview of text-to-image diffu-\nsion models. We then introduce a taxonomy of adversarial\nattacks targeting these models, followed by an in-depth\nreview of related attack methods to uncover vulnerabilities\nin model robustness and safety. Next, we introduce the\ncorresponding defense methods designed to improve the\nrobustness and safety of the model. Finally, we analyze\nthe limitations and challenges of existing adversarial attack\nand defense methods, and discuss the potential solution for\nfuture work."}]}