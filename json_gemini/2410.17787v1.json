{"title": "LARGE LANGUAGE MODELS ENGINEER TOO MANY SIMPLE FEATURES FOR TABULAR DATA", "authors": ["Jaris K\u00fcken", "Lennart Purucker", "Frank Hutter"], "abstract": "Tabular machine learning problems often require time-consuming and labor-intensive feature engineering. Recent efforts have focused on using large language models (LLMs) to capitalize on their potential domain knowledge. At the same time, researchers have observed ethically concerning negative biases in other LLM-related use cases, such as text generation. These developments motivated us to investigate whether LLMs exhibit a bias that negatively impacts the performance of feature engineering. While not ethically concerning, such a bias could hinder practitioners from fully utilizing LLMs for automated data science. Therefore, we propose a method to detect potential biases by detecting anomalies in the frequency of operators (e.g., adding two features) suggested by LLMs when engineering new features. Our experiments evaluate the bias of four LLMs, two big frontier and two small open-source models, across 27 tabular datasets. Our results indicate that LLMs are biased toward simple operators, such as addition, and can fail to utilize more complex operators, such as grouping followed by aggregations. Furthermore, the bias can negatively impact the predictive performance when using LLM-generated features. Our results call for mitigating bias when using LLMs for feature engineering.", "sections": [{"title": "INTRODUCTION", "content": "Machine learning problems for tabular data exist in many domains, such as medical diagnosis, cybersecurity, and fraud detection (Borisov et al., 2022a; van Breugel & van der Schaar, 2024). The original data for these problems (e.g., an Excel sheet) often requires manual feature engineering by a domain expert to solve the machine learning problem accurately (Tschalzev et al., 2024). During (automated) feature engineering, various operators (e.g., Add, Divide, GroupByThenMean) are applied to existing features to create new features (Kanter & Veeramachaneni, 2015; Prado & Digiampietri, 2020; Mumuni & Mumuni, 2024).\nLarge language models (LLMs) understand various domains (Kaddour et al., 2023; Kasneci et al., 2023; Hadi et al., 2024), tabular data (Ruan et al., 2024; Fang et al., 2024), and feature engineering (Hollmann et al., 2024; Jeong et al., 2024; Malberg et al., 2024). Thus, data scientists have started to leverage LLMs for feature engineering\u00b9; liberating practitioners from extensive manual labor.\nDespite their utility, LLMs are known to have negative biases as observed for chat applications (Kotek et al., 2023; Navigli et al., 2023; Gallegos et al., 2024; Bang et al., 2024) or when \"meticulously delving\" into text generation (Liang et al., 2024). Observing such biases motivated us to determine whether LLMs also exhibit a bias that negatively impacts the quality of their engineered features. If a bias is found and can be circumvented, LLMs would become a more potent tool for data scientists.\nTo determine whether a bias exists, we inspect the frequency of operators LLMs use when engineering features. This parallels inspecting the frequency of words to detect LLM-generated text (Liang et al., 2024). Assuming LLMs use their world knowledge and reasoning capabilities to employ the most appropriate operator, we expect the operators' frequencies to be similar to those of the optimal operators. That is, if adding two features is often the optimal feature, then an LLM would frequently add two features. Moreover, if the LLM does not know the optimal operator, it should resort to a random search over operators."}, {"title": "2 RELATED WORK", "content": "Feature Engineering Without Large Language Models. Previous work dedicated considerable effort toward automating the process of feature engineering (Kanter & Veeramachaneni, 2015; Prado & Digiampietri, 2020; Mumuni & Mumuni, 2024). Various black-box methods have been proposed, such as ExploreKit (Katz et al., 2016), AutoFeat (Horn et al., 2020), BioAutoML (Bonidia et al., 2022), FETCH (Li et al., 2023), and OpenFE (Zhang et al., 2023). These methods typically generate new features in two steps: 1) create a large set of candidate features by applying mathematical (e.g., Add) or functional (e.g., GroupByThenMean) operators to features, and 2) return a small set of promising features selected from all candidate features.\nFeature Engineering with Large Language Models. LLMs allow us to exploit their (potential) domain knowledge for feature engineering. LLMs can act as a proxy to a domain expert or data scientist during the feature engineering process. To illustrate, LLMs can be prompted to suggest code for generating new features (Hollmann et al., 2024; Hirose et al., 2024), to select predictive features (Jeong et al., 2024), or to use rule-based reasoning for generation (Nam et al., 2024).\nBias in Large Language Models. LLMs exhibit explicit and implicit biases. Explicit biases can be, among others, gender or racial discrimination in generated text (Kotek et al., 2023; Navigli et al., 2023; Gallegos et al., 2024; Bang et al., 2024). Moreover, LLMs can have implicit biases, such as specific words and phrases frequently re-used in generated text. As a result, Liang et al. (2024) were able to use recent trends in word frequency to detect and analyze LLM-generated text. Our method is similar to the investigation by Liang et al. (2024) but focuses on operator instead of word frequency.\nOther Applications of Large Language Models for Tabular Data. Many researchers recently started using LLMs for applications related to tabular data. To avoid confusion in this plethora of recent work, we highlight similar but not directly related work to our contribution. Our work is not directly related to LLMs to tabular question answering (Ghosh et al., 2024; Grijalba et al., 2024; Wu et al., 2024), tabular dataset generation (Borisov et al., 2022b; van Breugel et al., 2024; Panagiotou et al., 2024), tabular data manipulation (Zhang et al., 2024; Qian et al., 2024; Lu et al., 2024), or tabular few-shot predictions (Hegselmann et al., 2023; Han et al., 2024; Gardner et al., 2024)"}, {"title": "3 METHOD: ANALYZING FEATURE ENGINEERING BIAS", "content": "We propose a three-stage method to assess the bias of an LLM when used to engineer new features for tabular data problems. Our three-stage method A) tests the LLM for memorization of benchmark datasets; B) engineers new features with an LLM as well as black-box automated feature engineering; and C) analyzes the bias of the LLM. We visualize our method in Figure 1."}, {"title": "A) Memorization Test.", "content": "Given that language models are trained on vast amounts of publicly available data, we must account for the possibility that the LLM memorizes a dataset and optimal new features prior to our evaluation. To mitigate the risk of dataset-specific bias influencing the LLM during feature engineering, we test the LLM for memorization of datasets using the methods proposed by Bordt et al. (2024). Specifically, we conducted the row completion test, feature completion test, and the first token test. We consider a success rate of 50% or higher in any test an indicator that the evaluation of the dataset is biased. In such cases, the dataset is excluded from further evaluation."}, {"title": "B) Feature Engineering.", "content": "We propose a straightforward and interpretable feature engineering method for LLMs. Given a dataset, the LLM is supplied with context information, including the name and description. In addition, a comprehensive list of all features and critical statistical information for each feature (e.g., datatype, number of values, minimum, maximum, etc.) are provided. The instructions prompt also contains a pre-defined set of operators for engineering new features, each with a description indicating whether an operator is unary (applicable to one feature) or binary (requiring two features). We detail our full prompting specifications with examples in Appendix A.\nThe LLM is then instructed to generate precisely one new feature by selecting one or two existing features and applying one of the available operators. We employ chain-of-thought (CoT) prompting (Wei et al., 2023) to boost the expressive power of the LLM (Li et al., 2024). In addition, the LLM explains why a feature was generated with CoT. We also employ a feedback loop, similar to CAAFE (Hollmann et al., 2024), which we detail in Appendix A.\nOur approach to feature engineering with LLMs deviates from prior work (Hollmann et al., 2024; Hirose et al., 2024; Jeong et al., 2024) because we do not rely on code generation. This might put LLMs at a disadvantage because we reduce their potential expressiveness. However, we see this disadvantage outweighed by three significant advantages: first, we (almost) nullify the failure rate of generated code, which can be as much as 95.3% for small models (Hirose et al., 2024) ; second, we can control which operators the LLM uses; and third, we can extract applied operators from structured output without a (failure prone) code parser \u2013 enabling our study.\nFundamentally, we aim to compare the distribution over operators suggested by the LLM to the distribution over the optimal operators. That said, we do not know the optimal operators for a dataset. Thus, as a proxy, we use the operators suggested by OpenFE (Zhang et al., 2023).\nTo the best of our knowledge, OpenFE is the most recent, well-performing, and highly adopted\u00b2 automated feature engineering tool. OpenFE suggests a set of new features after successively pruning all possible new features generated by a set of operators. To do so, OpenFE uses multi-fidelity feature boosting and computes feature importance."}, {"title": "C) Analysis.", "content": "Finally, we analyze bias in feature engineering with LLMs using trends in the frequencies of operators. Therefore, we save the operators used by LLMs and black-box automated feature engineering from the previous stage. Subsequently, we compute the distribution over the frequencies of operators. This database allows us to visualize, inspect, and contrast the functional behavior of feature engineering with LLMs."}, {"title": "4 EXPERIMENTS", "content": "We extensively evaluate the bias of 4 LLMs for 21 operators across 27 classification datasets.\nLarge Language Models. We use four LLMs hosted by external providers via APIs. In detail, we used GPT-40-mini (OpenAI, 2024) and Gemini-1.5-flash (Gemini-Team, 2024) to represent big frontier LLMs and Llama3.1 8B (Touvron et al., 2023) and Mistral 7B Instruct v0.3 (Jiang et al., 2023), hosted by Together AI\u00b3, to represent small open-source models. The API usage costed ~200$.\nOperators. In this study, we use a fixed set of applicable operators. These operators represent a subset of the operators provided by OpenFE. We categorize the available operators into simple and complex operators. Simple operators apply straightforward arithmetic operations, such as adding two features. Furthermore, these operators are characterized by their relatively low computational complexity, typically $O(n)$. In contrast, complex operators perform more advanced transformations, such as grouping or combining the existing data into distinct subsets, followed by various aggregation functions. Compared to simple operators, complex operators generally exhibit a higher computational complexity of $O(n \\log n)$ or greater. We present all operators and their categories in Appendix B.\nDatasets. We used 27 out of 71 classification datasets from the standard AutoML benchmark (Gijsbers et al., 2024), which consists of curated tabular datasets from OpenML (Vanschoren et al., 2014). First, to avoid too large input prompts as well as extensive compute requirements, we selected all datasets with up to 100 features, 100000 samples, and 10 classes \u2013 resulting in 36 datasets. We had to remove the yeast dataset due to insufficient samples per class for 10-fold cross-validation. Of the remaining 35 available datasets, 8 (~23%) failed our memorization tests (see Appendix E) with at least one LLM, making them unsuitable for further evaluation \u2013 resulting in 27 datasets.\nEvaluation Setup. For each dataset, we perform 10-fold cross-validation. For each fold, we run OpenFE and prompt each LLM to generate 20 features. We assess the predictive performance of feature engineering following Zhang et al. (2023) by evaluating LightGBM (Ke et al., 2017) on the original features and the original features plus the newly generated features. Note, due to using a feedback loop, we only add features to the dataset when they improve the predictive performance on validation data (see Appendix A) We measured predictive performance using ROC AUC. Moreover, we mitigate a positional bias of our prompt template by repeating feature generation five times with an arbitrary order of operators. Finally, we compute the frequencies of operators across all new features, in total 27000."}, {"title": "5 RESULTS", "content": "We order our results as follows: first, we demonstrate that a bias exists; then, we show that the bias negatively impacts the performance of feature engineering; and finally, we rule out confounding factors of the prompt template with an additional experiment.\nHYPOTHESIS 1: FEATURE ENGINEERING WITH LARGE LANGUAGE MODELS IS BI-ASED TOWARD SIMPLE OPERATORS.\nFigure 2 illustrates the operators' frequencies for OpenFE and the four LLMs. None of the language models replicate the distribution found by OpenFE. Although, notably, the distribution of Llama3.1 8B and Mistral 7B appear most similar. This discrepancy is particularly noticeable for the most frequently used complex operators by OpenFE, GroupByThenRank and CombineThenFrequencyEnconding. Neither are among the 3 most frequent operators for any LLM."}, {"title": "ADDITIONAL EXPERIMENT. ENGINEERING RANDOM FEATURES WITH LLMS.", "content": "We additionally investigate whether our prompt template influenced our results. Therefore, we adapt our prompt template to mirror a random search. That is, we force an LLM to generate new features by randomly selecting the most appropriate operator. To this end, we employed the same experimental setup as our primary experiments. However, we masked the actual names of the operators in the instructions prompt. Each operator is assigned a numeric label, which the LLM selects. Subsequently, these numeric labels are mapped back to the names of the corresponding operators. Notably, we again shuffle the order of operators five times.\nFigure 6 shows the distribution over the frequency of operators with LLM-based random search for feature engineering. We observe that all models exhibit a significantly higher degree of uniformity compared to Figure 2. Yet, we do not observe total uniformity as expected for a random search, which aligns with observations for LLMs by Hopkins et al. (2023). Moreover, the previously observed bias toward simple operators does not manifest anymore. This is particularly visible for GPT-4o-mini. Mistral 7B has again the highest selection frequency of invalid operator labels, i.e., fails to follow the prompt's instructions. We conclude that our prompt template did not cause the bias toward simple operators. Instead, the content of the prompt, in combination with the LLM, causes the bias."}, {"title": "6 CONCLUSION", "content": "In this work, we propose a method to evaluate whether large language models (LLMs) are biased when used for feature engineering for tabular data. Our method detects a bias based on anomalies in the frequency of operators used to engineer new features (e.g., Add). In our experiments, we evaluated the bias of four LLMs. Our results reveal a bias towards simpler operators when engineering new features with LLMs. Moreover, this bias seems to negatively impact the predictive performance when using features generated by an LLM.\nIn conclusion, the contributions of our work are a method to detect bias in LLMs and evidence that a bias toward simple operators exists. Future work should explore methods to circumvent this bias. Promising methods are in-context learning (e.g., prompt tuning) or fine-tuning the LLM to favor optimal operators. In the long term, after identifying and addressing biases in LLMs, we can fully liberate ourselves from manual feature engineering. This will allow us to leverage LLMs as reliable and efficient automated data science agents for tabular data."}, {"title": "Limitations and Broader Impact.", "content": "Our study on the bias of LLMs still has limitations because it is the first of its kind for automated data science. We detect a bias but do not support practitioners to determine why an LLM might be biased. Similarly, given how frontier LLMs are trained and deployed, we focused on assessing bias in the models' outputs rather than internal mechanisms. Lastly, our study is limited to four LLMs, while practitioners can also choose from many other potentially biased LLMs. Our findings and proposed method do not create any negative societal impacts. Instead, both can have positive societal impacts because they increase our understanding and (mis)trust when using large language models for feature engineering."}]}