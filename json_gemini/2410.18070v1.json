{"title": "TRAINING FREE GUIDED FLOW MATCHING WITH OPTIMAL CONTROL", "authors": ["Luran Wang", "Chaoran Cheng", "Yizhen Liao", "Yanru Qu", "Ge liu"], "abstract": "Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications. One strategy for guiding ODE-based generative models is through optimizing a target loss R(x1) while staying close to the prior distribution. Along this line, some recent work showed the effectiveness of guiding flow model by differentiating through its ODE sampling process. Despite the superior performance, the theoretical understanding of this line of methods is still preliminary, leaving space for algorithm improvement. Moreover, existing methods predominately focus on Euclidean data manifold, and there is a compelling need for guided flow methods on complex geometries such as SO(3), which prevails in high-stake scientific applications like protein design. We present OC-Flow, a general and theoretically grounded training-free framework for guided flow matching using optimal control. Building upon advances in optimal control theory, we develop effective and practical algorithms for solving optimal control in guided ODE-based generation and provide a systematic theoretical analysis of the convergence guarantee in both Euclidean and SO(3). We show that existing backprop-through-ODE methods can be interpreted as special cases of Euclidean OC-Flow. OC-Flow achieved superior performance in extensive experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design.", "sections": [{"title": "INTRODUCTION AND RELATED WORK", "content": "SDE and ODE-based generative models such as diffusion and continuous normalizing flow (CNF) have exhibited excellent performance on various domains such as images (Ho et al., 2020; Esser et al., 2024), audio (Zhang et al., 2023; D\u00e9fossez et al., 2022), and discrete data (Lou et al., 2023; Cheng et al., 2024). Particularly, the simplicity of Riemannian Flow Matching on SO(3) manifold (Chen & Lipman, 2023) empowers de novo generation of small molecules (Song et al., 2024; Xu et al., 2023) and proteins (Yim et al., 2023; Bose et al., 2023; Li et al., 2024), leading to enormous advancement in biomedicine. Controlled generation from pre-trained diffusion and flow matching priors has gained growing interest in numerous fields, as it encompasses a wide range of practical tasks including constrained generation (Giannone et al., 2023), solving inverse problems (Liu et al., 2023; Ben-Hamu et al., 2024), and instruction alignment (Black et al., 2023; Esser et al., 2024).\nThere are several lines of work for guiding diffusion and flow models. Classifier-free guidance (CFG) (Ho & Salimans, 2022) trains conditional generative models that take conditions as input. Reward fine-tuning approaches update the generative model parameters to align with certain target objective functions (Black et al., 2023). Both methods require specialized training routines which are costly and not extendable to new tasks. Training-free guidance on diffusion (Kawar et al., 2022; Chung et al., 2024; Song et al., 2023) alters the scores in the SDE generation process with the gradients from the target function to achieve posterior sampling. These guidances often rely on strong assumptions of the denoising process and require estimating target function gradients w.r.t noised samples which are often intractable. Accurate posterior sampling is only guaranteed for a limited family of objective functions such as linear. Therefore, efforts that deploy such guidance to flow models by bridging the ODE path and SDE path share similar constraints (Pokle et al., 2023; Yim et al., 2024).\nNotably, two recent works showed the effectiveness of guiding pre-trained flow models by differentiating through the ODE sampling process, outperforming popular guidance-based approaches. Particularly, Ben-Hamu et al. (2024) differentiates a loss R(x) through the forward-ODE w.r.t the initial noise xo, which induces implicit regularization by projecting the gradient onto the \"data manifold\" under Gaussian path assumption. This strong confinement to the prior might hinder optimization when the target reward function diverges from the prior distribution. Liu et al. (2023) formulates an optimal control problem where a control term ut at each timestep is solved to guide the ODE trajectory. However, the gradient decomposition trick used in Liu et al. (2023) ignores the running cost of control terms and thus could lead to suboptimal behavior. Despite the good performance, there is a lack of systematic theoretical analysis on the convergence behavior and explicit regularization of the differentiate-through-ODE approaches to better guide algorithm design in this space. Furthermore, existing works predominantly focus on the Euclidean manifold due to its simplicity, and there is a compelling need for a theoretically grounded guided flow matching framework on more complex geometries such as SO(3) which is heavily used in scientific applications.\nTo fill the gap between the practical applications of guided generation and theoretical grounds, we propose OC-Flow, a general, practical, and theoretically grounded framework for training-free guided flow matching under optimal control formulation. Our key contributions are as follows:\n1. We formulate \u201ccontrolled generation with pre-trained ODE-based priors\u201d as an optimal control problem with a control term ut and a running cost that regulates the trajectory proximity to the prior model while optimizing for target loss. Building upon advances in optimal control theory, we develop effective optimization algorithms for both Euclidean and SO(3) space through iterative updates of a co-state flow and control term, with theoretical guarantees under continuous-time formulation.\n2. In Euclidean space, we show that running cost bounds the KL divergence between prior and OC-Flow-induced joint distribution. We develop a simple algorithm for OC-Flow through iterative gradient update and provide convergence analysis. We further demonstrate that Dflow and Flow-grad can be interpreted as special cases of Euclidean OC-Flow, providing a unified view of the problem.\n3. We present one of the first guided flow-matching algorithm on the SO(3) manifold with theoretical grounds. Our approach extends the Extended Method of Successive Approximations (E-MSA) to SO(3) with a rigorous convergence analysis. Additionally, we propose approximation techniques to enable computationally efficient OC-Flow on SO(3).\n4. We provide an efficient and practical implementation of OC-Flow, by introducing the vector-Jacobian product and asynchronous update scheme. We show the effectiveness of our method with extensive empirical experiments, including text-guided image manipulation, controllable generation of small molecules on QM9, and energy optimization of flow-based all-atom peptide design."}, {"title": "PRELIMINARIES AND PERSPECTIVES ABOUT FLOW MATCHING", "content": "Euclidean Flow Matching. Flow matching (Lipman et al., 2022; Liu et al., 2022) provides an efficient framework for training a generative model by approximating the time-dependent vector field associated with the flow represented as V_t : [0, 1] \u00d7 R^d \u2192 R^d. This vector field u_t : [0, 1] \u00d7 R^d \u2192 R^d defines a probability path of the evolution of an initial noisy distribution, denoted by p_0, towards a target distribution, p_1, with the pushforward probability as p_t := (V_t)_*p_0. The dynamics of the vector field that governs this flow can be described by the flow ordinary differential equation (ODE) of the form x_t = u_t(x_t), where we follow the convention to use Newton's notation with respect to time t and the state at time t is given by x_t := \u03a6_t(x_0). Lipman et al. (2022) demonstrates that a tractable flow matching objective can be obtained by conditioning on the target data x_1. The primary goal of conditional flow matching is to train a model, f : [0,1] \u00d7 R^d \u2192 R^d, such that it minimizes the difference between its output and the ground truth conditional vector field as L_{CFM} = E_{t,p(x_0,x_1)}|| f_t (x_t) - u_t (x_t | x_1,x_0)||^2. The trained model f_t can be employed as the marginal vector field during the inference phase. In this context, once a noise sample x_0 is drawn, the system's evolution can be described by the following differential equation:\n$$x_t = f_t^\u2217(x_t), x_0 \u223c p_0(x).$$"}, {"title": "OPTIMAL CONTROL FRAMEWORK FOR GUIDED FLOW MATCHING", "content": "Given a pre-trained flow model, f^p_t(x), parameterized by a neural network, our goal is to determine the optimal control terms \u03b8_t that maximize the reward R(x) while maintaining the proximity of the resulting vector field to the original vector field induced by f^p_t(x). The reward can be customized for diverse tasks such as inverse problem R = ||H(x) \u2013 y||^2, conditional generation R = (f(x) \u2013 c)^2, and constrained generation R = ||x \u2212 y||^2. To ensure proximity, we incorporate a penalty on the state trajectory or control terms for L(\u03b8_t), also known as the running cost. Optionally, one may also introduce a metric d(\u00b7, \u00b7) to penalize the deviation between the new terminal state x_T and the prior terminal state x_1^p. The modified terminal reward function is then defined as: \u03a6(x_T) = R(x_T) \u2013 d(x_T, x_1^p). and scaling the terminal reward by a constant \u03b1, we can formulate the problem as a standard optimal control task:\n$$J(\\theta):=\\alpha\\Phi(x_{T})+\\int_{0}^{T}L(\\theta_{t})dt\\quad \\text{subject to }\\dot{x}_{t}=h_{t}(x_{t},\\theta_{t}).$$\nA fundamental result in optimal control theory is Pontryagin's Maximum Principle (PMP) (Pontryagin (2018)), which provides the necessary conditions for optimal solutions in control problems. Specifically, at the core of PMP is the introduction of the Hamiltonian function, H. This Hamiltonian is defined in terms of the state of the system, the control, and a new entity called the co-state \u00b5, which resides in the cotangent space of the state manifold.\n$$H(t,x,\\mu,\\theta)=\\langle\\mu,h_{t}(x,\\theta_{t})\\rangle-L(\\theta).$$\nThe co-state variables can be thought of as shadow prices that represent the sensitivity of the optimal value function to changes in the state variables. Meanwhile, the terminal reward is also encoded to \u00b5_T. PMP states that for a control to be optimal, the Hamiltonian must be maximized by the evolution of the co-state \u00b5_t. The details of PMP conditions can be found in Appendix B.1."}, {"title": "OC-FLOW ON EULIDEAN MANIFOLD", "content": "We first develop the algorithm and theoretical analysis for OC-flow in Euclidean space. One of the simplest choices for the control term is an additive control (Kobilarov & Marsden (2011)), which directly perturbs the prior trajectory. In fact, with the linear expansion, the additive control could be seen as a general case and is widely used in optimal control. Specifically, with \u03b8_t representing the control input, the new state dynamics and the corresponding running cost can be defined as:\n$$\\dot{x}_{t}=h_{t}(x_{t},\\theta_{t})=f_{t}^{p}(x_{t})+\\theta_{t} \\quad L(\\theta_{t})=\\frac{1}{2}\\|\\theta_{t}\\|^{2}$$\nThe running cost can be interpreted as a constraint on the trajectory and, equivalently, as a constraint on the terminal probability distribution along a Gaussian path. This leads to the following observation:\nProposition 1. Under the Affine Gaussian Probability Paths framework, the per sample \u03b8_t can be interpreted as a function of the data target x_1. The expectation of the running cost \u222b_0^T||\u03b8_t||^2dt provides an upper bound for a constant C times the Kullback-Leibler (KL) divergence between the joint distributions of the data point x\u2081 and the prior terminal point xP, denoted as p\u2081(xP, x1) = P1 (xP|x1)p_{data}(x1), and that of the data point x1 and the controlled terminal point x\u0398, denoted as p1(x\u0398, x1):\n$$E_{x_{1}\\sim p_{data}(x_{1})}\\left[\\int_{0}^{T}\\|\\theta_{t}(x_{1})\\|^{2}dt\\right] \\geq C\\cdot KL(p_{1}(x^{\\Theta},x_{1})\\|p_{1}(x^{P},x_{1})).$$\nOne effective approach for directly applying PMP to optimal control tasks is the Extended Method of Successive Approximations (E-MSA) Li et al. (2018). E-MSA builds upon the basic MSA algorithm (Chernousko & Lyubushin (1982)), which iteratively updates the terms in the PMP conditions (Appendix B.2). The primary enhancement of E-MSA over the basic MSA is its ability to extend convergence guarantees beyond a limited class of linear quadratic regulators (Aleksandrov (1968)).\nA key assumption is the global Lipschitz condition for the functions involved. However, note that this assumption can be relaxed to a local Lipschitz condition if we can demonstrate that xf is bounded, which can be safely assumed provided that appropriate regularization techniques are applied.\nWhen the E-MSA algorithm is applied to the guided controlled generation task as defined earlier, the trajectory of the co-states \u00b5_t can be calculated in closed form. Further, they can be expressed as \u2207\u03a6(x_T). Overall, the following update rule is derived:\nTheorem 2: Assume that the reward function, the prior model, and their derivatives satisfy Lipschitz continuity, bounded by a Lipschitz constant L. Utilizing the E-MSA, for each iteration k, for each constant \u03b3 > 2C with C is a function of L, such that under the addictive control and the running cost defined in Equation 4, the optimal update is following:\n$$\\theta_{t}^{k+1}=\\frac{\\alpha}{\\gamma+1} + \\theta_{t}^{k+1} + \\frac{1}{\\gamma+1}\\nabla\\Phi(x_{t}^{k}).$$\nThis update rule for the control term \u03b8_t guarantees an increase in the objective function defined in equation 2.\n$$J(\\theta^{k+1})-J(\\theta^{k})\\geq\\frac{(1-\\frac{2C}{\\gamma})^{2}}{2C}(\\theta_{t}^{k+1}-\\theta_{t}^{k})^{2} \\geq 0$$\nIn practice, continuous ODEs are solved through discretization. The discretized formulation of our algorithm is outlined in Algorithm 1. In this version, the weight decay term is parameterized as: \u03b2 = \u03b1/ (1+\u03b3) and the learning rate is defined as: \u03b7 = \u03b1 / (1+\u03b3)."}, {"title": "PRACTICAL IMPLEMENTATION AND ACCELERATION", "content": "A significant portion of the computational time in the OC-Flow algorithm is spent on evaluating the gradient \u2207\u03a6(x_T). The computational cost of directly backpropagating through the model to compute the Jacobian is prohibitive due to the complexity of f^p_t, which is parameterized as a deep learning model. Inspired by Liu et al. (2023), the gradient \u2207\u03a6(x_T) can be efficiently computed using a vector-Jacobian product by the double-backwards trick, which is expressed as:\n$$\\nabla x_{k/N}\\Phi = (\\nabla x_{(k+1)/NP}) \\cdot \\Phi x_{(k+1)/N} (x_{k/N}).$$\nAlternatively, since our algorithm does not require a fixed step size and follows a deterministic optimization process, it is compatible with sophisticated optimizers such as LBFGS with line search, which automatically determines a quasi-optimal step size. Note that LBFGS may result in faster convergence but with the cost of increasing memory consumption (Table 1) similar to Ben-Hamu et al. (2024). It is also not compatible with OC-Flow in SO(3). In our experiments, we observed good convergence with the adjoint approach and only used LBFGS in QM9 where the memory consumption is smaller, for a fair comparison with Ben-Hamu et al. (2024)."}, {"title": "ASYNCHRONOUS SETTING FOR FLEXIBLE UPDATE SCHEDULING", "content": "In practice, discretization techniques are employed to simulate the ODEs governing both the state trajectory x_t and the co-states \u00b5_t and operate in a synchronous setting, where the number of time steps for the state trajectory x_t coincides with the number of control terms \u03b8_t.\nHere we show that OC-Flow can be extended to an asynchronous framework, providing greater flexibility in scheduling. We subdivide the time interval \u2206t into N equally spaced subintervals. Let {x_t} denote the state trajectory over the time interval [t, t + \u2206t], and {x_t^i} represent the trajectory when the control term \u03b8_t is applied in the first subinterval. The trajectory can be approximated as follows:\n$$x_{t+\\Delta t} = x_{t} + \\frac{\\Delta t}{N}\\sum_{l=1}^{N-1}f_{t}^{p}(x_{t+\\frac{l\\Delta t}{N}})+\\theta_{t} \\approx x_{t} + \\Delta t\\left(\\frac{1}{N}\\sum_{l=1}^{N-1}f_{t}^{p}(x_{t+\\frac{l\\Delta t}{N}})+\\frac{1}{N}\\theta_{t}\\right).$$\nConsequently, the asynchronous setting allows the algorithm to be applied without modification while enabling finer updates to both the control terms and state trajectories by adjusting the frequency N of control term updates relative to the state trajectory simulation (see Appendix C.3 for the proof and justification of the approximations in Equation 7). In our peptide design experiment, asynchronous setting is applied for efficient computing."}, {"title": "CONNECTION TO OTHER BACKPROP-THROUGH GUIDED-ODE APPROACHES", "content": "Several previous works discussed backprop-through-ODE guidance in flow-matching models. Notable examples include D-Flow (Ben-Hamu et al., 2024) and FlowGrad (Liu et al., 2023). An illustration of their algorithms and ours is shown in Figure 1. In this section, we demonstrate that our framework is more general, and both of these methods can be viewed as special cases of our algorithm.\nFlowGrad formulates the optimization task in a manner similar to our optimal control target in Equation 2. Specifically, it directly applies gradient descent to the control variables:\n$$\\theta_{t}^{k+1}=\\theta_{t}^{k} + \\nabla_{\\theta_{t}}\\Phi(x_{t}^{k})=\\theta_{t}^{k} + (\\nabla x_{t}x_{t+1})^{-1}\\nabla\\Phi(x_{t}^{k}),$$\nwhich can be interpreted as a limiting case of our algorithm in Equation 5, where \u03b3 \u2192 \u221e and given with dt \u2192 0, x_tX_{t+1} \u2192 I. However, as shown in Equation 6, the convergence rate is a complex function of \u03b3, so in practice, \u03b3 is treated as a tunable parameter. FlowGrad's setting \u03b3 \u2192 \u221e may undermine the convergence speed.\nD-Flow optimizes the reward by applying gradient descent to the initial noise x0:\n$$X_{0}^{k+1} = X_{0} + LBFGS(\\nabla_{\\Theta_{0}}\\Phi(x_{1})).$$"}, {"title": "OPTIMAL CONTROL FRAMEWORK FOR GUIDED FLOW MATCHING IN SO(3)", "content": "Most optimization algorithms are designed for Euclidean spaces and encounter difficulties when extended to non-Euclidean settings, such as the topologically and geometrically complex structure of SO(3). In this section, we review the current results for optimal control on SO(3) and extend the applicability of the E-MSA to this manifold, providing rigorous proof of its convergence."}, {"title": "OC-FLOW FOR SO3", "content": "To begin, we define the vector field governing the system dynamics. The state trajectory, influenced by control terms \u03b8_t \u2208 so(3), evolves according to the following differential equation:\n$$\\dot{x}_{t}=x_{t}^{\\ddagger}(f_{t}^{p}(x_{t})+\\theta_{t})$$\nHere we employ the left-invariant vector field, which aligns naturally with the structural characteristics of the Lie group so(3). This alignment is reflected in the system dynamics described by equation 8, where the tangent space is expressed as T_xSO(3) = SO(3) \u00d7 so(3). Under the left-invariant vector field, it can be shown that the Hamiltonian becomes a linear functional in so(3)^* (Jurdjevic (1996), Colombo & Dimarogonas (2020)). With the co-state \u00b5_t \u2208 so(3)^*, the Hamiltonian function H: [0, T] \u00d7 SO(3) \u00d7 so(3)^* \u00d7 so(3) \u2192 R can be redefined as:\n$$H(t,x,\\mu,\\theta)=\\langle\\mu_{t},f_{t}(x_{t},\\theta)\\rangle-L(\\theta)$$\nInspired by the Method of Successive Approximations (MSA), a direct approach to apply PMP conditions on the SO(3) manifold involves iteratively updating the cotangent vector and the state trajectory. At each iteration, we utilize trajectories of \u00b5_t and x_t as shown in Step 4 and Step 5 in Algorithm 2, and then apply an update rule to determine the control term \u03b8_t for the subsequent iteration. Drawing inspiration from our algorithm in Euclidean space, with weight decay \u03b2 and learning rate \u03b7 the update for \u03b8_t can be written as:\n$$\\theta_{t}^{k+1} = \\beta\\theta_{t}^{k} + \\eta \\tilde{\\mu}_{t}^{*k},$$\nwhere ~\u03bc_t^* is defined by <\u00b5^*, v) = \u00b5^* (v) for all v \u2208 so(3). The existence of ~\u00b5^* can be derived from the Riesz Representation Theorem (Goodrich (1970)). This formulation leads to the introduction of the OC-FLow algorithm on SO(3), which provides an optimization procedure for the task of guided flow matching, as detailed in Algorithm 2."}, {"title": "CONVERGENCE OF OC-FLOW ON SO3", "content": "To derive the proof of the convergence of our Algorithm 2, we first establish that under the PMP conditions on SO(3), the objective function J(\u03b8), as defined in equation 2, can be bounded. This is formalized in the following proposition:\nProposition 3: Assume that the reward function, the prior model, and their derivatives satisfy Lipschitz continuity, bounded by a Lipschitz constant L. Then, there exists a constant C > 0 such that for any \u03b8, \u03c6 \u2208 50(3), the following inequality holds:\n$$J(\\theta)+\\int_{0}^{T}\\Delta_{\\phi,\\theta}H(t)dt - C\\int_{0}^{T}\\|\\phi_{t}-\\theta_{t}\\|^{2}dt \\leq J(\\phi),$$\nwhere X\u00ba and P\u00ba satisfy the PMP conditions in equation 19, and \u2206H\u03c6,\u03b8 denotes the change in the Hamiltonian, defined as:\n$$\\Delta H_{\\phi,\\theta}(t) := H(t, x_{t}, \\mu_{t}, \\phi_{t}) - H(t, x_{t}, \\mu_{t}, \\theta_{t}).$$\nProposition 2 provides a lower bound for the difference in the objective function values under two distinct control terms that satisfy the PMP conditions described by the Hamiltonian equations in Appendix B.1.\nHowever, applying this result directly as an optimization algorithm presents several challenges. First, the difference in the Hamiltonian \u2206H\u03c6,\u03b8(t) is not inherently bounded. Second, the term ||\u03c6 \u2013 \u03b8||\u00b2 is non-negative, which complicates the minimization process. To address these issues, inspired by the method of E-MSA, we introduce a positive constant \u03b3 and defining an Extended-Hamiltonian:\n$$H(t,x,\\mu,\\theta,\\phi) = H(t, x, \\mu, \\theta) - \\frac{\\gamma}{2}\\|\\theta - \\phi\\|^{2} = \\langle\\mu, f_{t}(x)+\\theta\\rangle - \\frac{1}{2}\\|\\theta\\|^{2} - \\frac{\\gamma}{2}\\|\\theta - \\phi\\|^{2}.$$\nThe introduction of the Extended-Hamiltonian enables the combination of the original Hamiltonian with the penalty term ||\u03c6 \u2013 \u03b8||\u00b2 into a unified expression that can be optimized jointly. A natural approach to achieve this is by updating \u03b8 to maximize the Extended-Hamiltonian. The resulting update rule is given by:\n$$\\theta_{t}^{k+1} = \\underset{\\theta}{argmax}\\tilde{H}(t,x^{k},\\mu^{k},\\theta,\\theta^{k}) = \\frac{\\gamma}{\\gamma+1}\\theta^{k} + \\frac{1}{\\gamma+1}\\mu_{t}^{k} = \\beta \\theta^{k} + \\eta \\mu_{t}^{k}.$$\nBy performing this maximization step at each iteration, we ensure that the change in the Extended-Hamiltonian, \u2206H, is non-negative, indicating that the algorithm progresses towards an optimal solution. Furthermore, we can show that when the update process converges, i.e., when \u2206H = 0 or equivalently \u2206~H = 0, the algorithm has reached the optimal control point. These insights can be formalized in the following proposition:\nProposition 4: Let X\u00ba and P\u00ba satisfy the PMP conditions . If the update rule follows Algorithm 2, we define \u03f5k := \u222b_0^T \u2206_{\u03b8^{k+1},\u03b8^k} H(t) dt, and \u03f5k is bounded as:\n$$\\epsilon_{k} := \\int_{0}^{T}\\Delta_{\\theta^{k+1},\\theta^{k}} H(t) dt \\quad  lim_{k\\rightarrow \\infty} \\epsilon_{k} = 0.$$\nFurthermore, when \u03f5k = 0, we have \u03b8 = \u03b8^* := arg max_\u03b8 J(\u03b8)\nTo establish convergence, define\n$$\\epsilon_{k} := \\int_{0}^{T}\\Delta\\tilde{H}_{t^{\\theta^{k+1}},\\theta^{k}}(t)dt \\geq 0.$$\nTherefore, by invoking Proposition 3, we can conclude that after each update, the target function is non-decreasing and when the update process terminates, the optimal solution has been attained. This establishes the convergence of the OC-Flow algorithm on the SO(3) manifold."}, {"title": "PRACTICAL IMPLEMENTATION", "content": "In practice, directly optimizing Algorithm 2 using existing ODE methods is challenging due to the nature of the adjoint variable \u03bc_t, which is a linear functional in the dual space so(3)^*. Instead we can optimize \u03bc_t as defined in Section 4.1. We can decompose \u03bc_t into its projections onto a set of orthogonal bases within the so(3) group."}, {"title": "TEXT-GUIDED IMAGE MANIPULATION", "content": "We first validate our OC-Flow on the traditional text-to-image generation task. Previous work has demonstrated the importance of alignment with the given text prompt using either automatic metrics or human preference as the reward (Black et al., 2023; Esser et al., 2024). In our text-guided image manipulation task, we want to guide the generative model pre-trained on the celebrity face dataset CelebA-HQ (Karras, 2017) to three text guidance {sad, angry, curly hair} showing different facial expressions or traits. Following the same setup in Liu et al. (2023), given an input image xg, the reward for alignment with the text prompt can be effectively evaluated by the CLIP model (Radford et al., 2021) pre-trained in a contrastive way to score the similarity between arbitrary image-text pairs. Following (Liu et al., 2023), we adopt the pre-trained Rectified Flow (RF) (Liu et al., 2022) as the generative prior.\nWe choose two state-of-the-art text-guided image manipulation baselines, StyleCLIP (Patashnik et al., 2021) and FlowGrad (Liu et al., 2023). We run StyleCLIP and FlowGrad with their official implementation and default parameter configurations. For ours, we set time step N = 100, step size \u03b7 = 2.5, weight decay = 0.995, and the number of optimization steps M = 15.\nFor qualitative comparison, we show generated examples of different text-guided expressions in Figure 2. Note that the faces in the reference input are mostly smiling. Therefore, due to the large gap between reference and guided distributions, StyleCLIP fails to manipulate with sad. Lacking in regularization, FlowGrad may change the content too much with curly hair. Our OC-Flow generally produces the best results with a good alignment with the text prompt while preserving the generative prior so as to produce reasonable faces that are not distorted much."}, {"title": "MOLECULE GENERATION FOR QM9", "content": "We further instantiate our OC-Flow for controllable molecule generation on the QM9 dataset (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014), a commonly used molecular dataset containing small molecules with up to 9 heavy atoms from C, O, N, F. Following Hoogeboom et al. (2022); Ben-Hamu et al. (2024), we target for conditional generation of molecules with specified quantum chemical property values including polarizability \u03b1, orbital energies \u03b5HOMO, \u03b5LUMO and their gap \u2206\u03b5, dipole moment \u00b5, and heat capacity cv. Such a conditional generation setting of molecules with desired properties has profound practical applications in drug design and virtual screening.\nTo define the loss function, separate classifiers for each property are first trained to predict the property value for the generated molecule (Hoogeboom et al., 2022), and the loss can be then calculated as the mean absolute error (MAE) between the predicted and the reference property values. The pre-trained unconstrained generative model is taken from Song et al. (2024) (EquiFM), a flow-based generative model that uses an equivariant vector field parameterization for generating the atom coordinates and types via the learned flow dynamics. To demonstrate the zero-shot guidance performance on such a conditional generation task, we compare our approach with other gradient-based methods of D-Flow (Ben-Hamu et al., 2024) and FlowGrad (Liu et al., 2023) on the same pre-trained EquiFM. To be comparable to D-Flow, we follow its setting to use the L-BFGS optimizer with 5 optimization steps with linear search. We generate 1000 molecules for each property and report the MAE in Table 3. The unconditional EquiFM is also included as an upper bound for the guided models. It can be seen that our approach, as a generalization of both D-Flow and FlowGrad, consistently outperforms both of them with lower MAEs. We provide guided generation samples in Figure 3 with respect to different target dipole moments. A clear trend from hydrocarbons with more symmetric structures to molecules with more high-electronegativity atoms of oxygen and nitrogen can be observed, indicating an increase in the dipole moment."}, {"title": "PEPTIDE DESIGN", "content": "We evaluate our OC-Flow approach for peptide Backbone design using a test set derived from (Li et al., 2024), which includes 158 complexes clustered based on 40% sequence identity using mmseqs2(Steinegger & S\u00f6ding, 2017). Our experiments focus on PepFlow w/Bb, a model designed to exclusively sample peptide backbones while optimizing translations in Euclidean space and rotations in SO(3) space. The model employs the MadraX force field (Orlando et al., 2024) for energy optimization, and performance is evaluated using several key metrics. These metrics include MadraX energy, which assesses the total energy of the generated peptide structures, along with Rosetta-based measures of stability and affinity. Currently, our stability and affinity metrics are represented by their respective means: stability quantifies the energy states of peptide-protein complexes, while affinity measures the binding energies.\nIn addition, we employ an existing metric, denoted as IMP, which measures the percentage of generated peptides that exhibit lower energy than the original ground truth. Additionally, we use the root-mean-square deviation (RMSD) to evaluate structural accuracy by aligning the generated peptides to their native structures and calculating the C\u03b1 RMSD. To further analyze structural characteristics, we compute the secondary-structure similarity ratio (SSR), which reflects the proportion of shared secondary structures, and the binding site ratio (BSR), which quantifies the overlap between the binding sites of the generated and native peptides on the target protein. Structural diversity is assessed using the average of one minus the pairwise TM-Score (Zhang & Skolnick, 2005) among the generated peptides, representing their dissimilarities.\nWe compare our method to the pre-trained unconditional PepFlow model (Li et al., 2024), serving as a baseline. We also include ablations where our model guides only translations (Euclidean) or rotations (SO(3)). As shown in Table 5, our OC-Flow method, applied to both Euclidean and SO(3) spaces, consistently outperforms the baseline, even though we only optimize for the Madrax target function. This indicates that our algorithm not only achieves higher target function scores but also captures more natural structural configurations. It generates peptide backbones that are more stable, energetically favorable, and diverse, while improving key metrics such as stability, affinity, IMP, diversity, SSR, and BSR. In comparison, optimizing in Euclidean space alone yields only marginal improvements in IMP, while optimizing rotations alone achieves comparable performance. More experimental details can be found in Appendix B.1."}, {"title": "CONCLUSIONS AND DISCUSSION", "content": "In this paper, we propose OC-Flow, a general and theoretically grounded framework for training-free guided flow matching under optimal control formulation. Our framework provides a unified perspective on existing backprop-through-ODE approaches and lays the foundation for systematic analysis of the optimization dynamics of this setting. Extensive empirical experiments demonstrate the effectiveness of OC-Flow. Future extensions of OC-Flow include generalizing beyond additive control terms and bridging connection with fine-tuning regimes where control terms can be solved as learning updates to the model parameters. Another extension could be scaling up the SO(3) OC-Flow to guide generative tasks for larger molecular systems such as protein motif scaffolding. We also note that our practical algorithms utilize discretization which could lead to gaps in convergence analysis derived under continuous formulation. We leave the discussions of discretization analysis as a future direction for study. We hope that our findings can guide algorithm design and motivate further model improvement in guided flow matching."}, {"title": "OC-FLOW ON SO3", "content": "To begin", "equation": "n$$\\dot{x}_{t}=x_{t}^{\\ddagger}(f_{t}^{p}(x_{t})+\\theta_{t})$$\nHere we employ the left-invariant vector field, which aligns naturally with the structural characteristics of the Lie group $\\mathfrak{so}(3)$. This alignment is reflected in the system dynamics described by equation 8, where the tangent space is expressed as $T_{x}SO(3) = SO(3) \\times \\mathfrak{so}(3)$. Under the left-invariant vector field, it can be shown that the Hamiltonian"}]}