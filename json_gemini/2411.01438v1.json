{"title": "SkyServe: Serving AI Models across Regions and Clouds with Spot Instances", "authors": ["Ziming Mao", "Tian Xia", "Zhanghao Wu", "Wei-Lin Chiang", "Tyler Griggs", "Romil Bhardwaj", "Zongheng Yang", "Scott Shenker", "Ion Stoica"], "abstract": "Recent years have witnessed an explosive growth of AI models. The high cost of hosting AI services on GPUs and their demanding service requirements, make it timely and challenging to lower service costs and guarantee service quality. While spot instances have long been offered with a large discount, spot preemptions have discouraged users from using them to host model replicas when serving AI models.\nTo address this, we introduce SkyServe, a system that efficiently serves Al models over a mixture of spot and on-demand replicas across regions and clouds. SkyServe intelligently spreads spot replicas across different failure domains (e.g., regions or clouds) to improve availability and reduce correlated preemptions, overprovisions cheap spot replicas than required as a safeguard against possible preemptions, and dynamically falls back to on-demand replicas when spot replicas become unavailable. We compare SkyServe with both research and production systems on real AI workloads: SkyServe reduces cost by up to 44% while achieving high resource availability compared to using on-demand replicas. Additionally, SkyServe improves P50, P90, and P99 latency by up to 2.6x, 3.1x, 2.7\u00d7 compared to other research and production systems.", "sections": [{"title": "Introduction", "content": "Generative AI has experienced explosive growth in the past several years. These models have enabled a plethora of new applications, such as large language model (LLM) chatbots [1, 2], programming assistants [3], image generation [4], and writing assistants [5]. Many companies [6, 7] offer these models as hosted services on the cloud. A service is composed of multiple model replicas; each replica runs on one or more GPU instances. However, serving these models is challenging: not only do they need to be highly available and serve user requests under tight latency constraints, but they are also expensive to operate. Compared to traditional web services, Al systems have much higher compute requirements [8] and costs [9].\nThere are two reasons for the high cost of serving AI models. First, these models require expensive GPU accelerators. As a result, processing a request can be 10x more expensive than a traditional search engine query [9]. Second, real-life AI workloads have frequent and unpredictable traffic spikes (up"}, {"title": "Background and challenges", "content": "Serving AI Models on spot instances\nServing Systems. In practice, a service (Figure 2) hosting AI models comprises of one or multiple model replicas. Each request to the service is load-balanced and routed to one replica. Each replica exposes the model endpoint with an inference engine (e.g., vLLM [24], TGI [25], Triton [26]) containing logic to invoke models. Within a replica, a model can be partitioned over multiple GPU instances or run on a single GPU instance; there is little traffic across replicas as these replicas can independently serve user requests. We call model replicas running on spot instances as spot replicas, and model replicas running on on-demand instances as on-demand replicas.\nCost savings. Al serving on the cloud is costly [28]. The rise in popularity of AI models [2] demands many GPUs to host them. While GPUs are expensive [9, 29], spot GPU in-stances can be 2\u201312\u00d7 cheaper than on-demand GPU instances (Table 1, [30]), presenting opportunities to reduce the cost of AI serving workloads using spot GPU instances. The cost of spot instances is generally stable over time, though there could be cost differences across zones and regions [31]. Despite significant cost savings, the industry has not seen practical adoption of spot instances in serving AI models, particularly due to spot instance preemptions and unavailability."}, {"title": "Existing single-region systems suffer from limited spot GPU availability.", "content": "Existing systems suffer from limited availability as they primarily focus on single-region deployments:\nUnavailability of spot GPUs in one region. GPUs often experience shortage [37] within a region. We have observed spot GPU unavailability across zones of the same region, either due to the region running out of capacity, or quota issues. For example, in one trace we analyzed (AWS 2, \u00a75.2), 33.1% of time spot GPUs experience unavailability across all zones in a single region. In our end-to-end evaluation (\u00a75.1), 21% of time region us-west-2 has experienced unavailability. This observation generalizes across multiple GPU instance types and is one of the key limitations of prior work: the service can be unavailable if spot GPU instances are simultaneously unavailable in a single region. A single-region system will receive service disruptions when another spot GPU instance cannot be provisioned in the zone or region experiencing preemptions. This significantly limits the practical deployment of AI models on spot instances: in our evaluation with pure spot deployment of AWS Autoscaling Group (\u00a75.1), 49.4%"}, {"title": "Existing spot CPU-focused policies failed to work on spot GPUs.", "content": "Majority of the systems [12-17] on spot instances target CPU instances. In particular, these systems use preemption warnings to mitigate service disruptions. However, they failed to directly work on using spot GPUs to serve AI models. We categorize the limitations of those systems as follows:"}, {"title": "Existing systems with static spot-on-demand mixture either are costly or have poor availability.", "content": "Several systems support serving on both spot and on-demand replicas [22, 23, 32] to mitigate preemptions and unavailability. These systems require setting fixed and predefined node pools of either spot or on-demand instances. If spot replicas are preempted, the traffic will be re-distributed over on-demand replicas in the on-demand node pools. The mix-ture size (i.e. node pool size) is static, for example, AWS Autoscaling Group can statically maintain 10% of on-demand replicas at all times. This allows on-demand replicas to serve as the base capacity for the service in the event of preemption. These systems will try to satisfy the specified node pool size but could fail to do so in the event of unavailability.\nUsing a fixed pool of on-demand replicas is unnecessary and costly when spot replicas are available. During periods with high spot availability, systems with static pools of on-demand and spot replicas still keep the costly on-demand replicas, instead of leveraging more spot replicas and mov-ing to a pure spot deployment. For example, we run AWS Autoscaling Group [22] (ASG) with on-demand node pool of size 1 and spot node pool of size 4 for g5.48xlarge instances. During periods where spot instances are available, ASG maintains one on-demand replica throughout, providing base service capacity. Even with a single on-demand replica, this increases the total cost by 1.56\u00d7 compared to using a pure spot deployment. The on-demand g5.48xlarge replica cost constitutes 52% of the total cost, as its hourly cost ($16.3) is significantly higher than that of spot instance ($4.9). As such, using always-on on-demand replicas can be expensive and unnecessary when spot instances are available.\nA fixed pool of spot instances may fail to provision when spot availability is volatile. When the system fixes the spot node pool, it will continue provisioning for the spot node pool size even when spot instances are unavailable, rather than launch more on-demand instances to cover for lost capacity in the spot pool. As the system retries to fulfill the lost spot"}, {"title": "SkyServe", "content": "We propose SkyServe to address the aforementioned chal-lenges. To overcome limited availability and correlated pre-emptions in a single region (\u00a72.2), SkyServe dynamically provisions spot replicas across regions and clouds based on their preemptiveness. To lower cost with good availability (\u00a72.4), SkyServe adaptively maintains spot and on-demand mixture. We discuss our designs below:"}, {"title": "Placing spot replicas dynamically across regions and clouds", "content": "SkyServe addresses the challenges of limited spot availability in a single region by using spot instances across different regions and clouds. Before discussing how SkyServe selects and provisions spot instances, we discuss and compare prior single-region multi-zone policies in other systems.\nComparing alternative policies. Assume N zones and for each zone i, preemptions follow Poisson distribution with rate parameter \u03bb\u2081. Spot instance lifetime is. The expected number of preemptions E[K;] in zone zi over an observation window T is E[K\u2081] = T\u03bb\u2081, assuming that spot instance lifetime is much greater than the cold start delay d.\nStatic Spread Policy (used by AWS Autoscaling Group [22], MArk [16], GKE [46], Ray Serve [32] in a single region): Consider a simple policy where we maintain an even static spread of n spot replicas to N zones, such that each zone is given number of replicas. The expected total number of preemptions K over T is: E[K] = nT \u03a3\u2081 \u03bb\u2081. \u0395[K] will be dominated by preemptive zones with large \u03bb\u2081. This is not ideal; intuitively, when a zone experiences more preemptions, an intelligent policy should avoid provisioning more spot instances in that zone.\nRound Robin: To mitigate the above issue, we discuss a Round Robin policy. When a spot replica gets preempted in a zone i, it gets relaunched in the next zone in the same region. For a long-running service, the average spot lifetime is \u03a3over N zones. The expected total number of preemptions is: E[K] = nT(_). Since 1 \u2081 is larger"}, {"title": "Dynamic Placement Policy.", "content": "As such, we propose the fol-lowing spot replica placement policy: Dynamic Placement (Algorithm 1). To avoid preemptive zones, the policy intelli-gently tracks which zones are active or preemptive and dynam-ically selects zones from active zones. Let Z be all enabled zones with the required instance type. Z allows SkyServe users to express additional requirements: such as avoiding par-ticular zones or regions for regulation or latency constraints. The algorithm keeps two lists: ZA is a list of active zones initialized to be Z; Zp is a list of preemptive zones initialized to be empty. If a replica is preempted in zone z, z is moved to Zp. If a replica is successfully launched and ready in zone z, z is moved to ZA. Whenever a new replica needs to be launched, it is drawn from ZA, prioritizing zones with fewer current spot placements and zones with lower cost. The policy can addi-tionally probe different zones to maintain Zp and ZA. When"}, {"title": "Adjusting spot and on-demand mixture dynamically based on spot preemptiveness", "content": "Most existing systems [21, 32] only uses static node pools but cannot change the mixture dynamically when preemption happens: launch more spot replicas when spot instances become available, fall back to on-demand replicas when spot market becomes volatile. We have shown in \u00a72.4 that static node pools either result in poor availability or high cost, ne-cessitating an adaptive spot and on-demand mixture based on measured spot instance preemptiveness. To derive the dy-namic mixture policy, let S(z, t) be the number of launched spot replicas at time t in zone z, and O(t) be the number of launched on-demand replicas at t. The total number of launched spot replicas across zones is S(t) = \u2211zez S(z, t). Let NTar (t) be the target number of ready instances decided by the user or an autoscaling policy at t based on the load.\nOverprovisioning with NExtra(t). SkyServe overprovi-sions cheap spot replicas to mitigate preemptions and cold start delay. We use NExtra(t) to denote the number of spot replicas to over-provision at t. These spot replicas serve as buffer in the event of preemptions. Intuitively, even when"}, {"title": "Deciding the number of fallback on-demand replicas.", "content": "Since having fixed on-demand node pools leads to higher costs, we propose a new policy: Dynamic Fallback (Algorithm 2). The policy initializes with NTar(t) + NExtra(t) spot repli-cas. If a spot replica is preempted, the policy launches an on-demand replica and keeps trying to launch NTar(t)+NExtra(t) spot replicas at the same time. Let S,(t) be the number of ready spot replicas, the policy tries to maintain O(t) (but possibly not all ready) on-demand replicas where O(t) = max(NTar (t), NTar(t) + NExtra(t) \u2013 S,(t)). This policy pro-visions on-demand replicas to replenish the lost spot capacity, where O(t) \u2264 NTar(t).\nIntuitively, when we have a spot replica preempted, Dy-namic Fallback quickly launches an on-demand replica. On-demand replicas serve as a reliable fallback; we have observed in \u00a75.1 that on-demand replicas are typically available across regions. While the on-demand replicas are provisioning, the overprovisioned spot replicas ensure little service disruption. When the spot replicas become available, SkyServe scales down these on-demand replicas and instead serves entirely on spot replicas. The cost of Dynamic Fallback is relatively small as these on-demand replicas will be terminated once we have Ntar(t) + NExtra(t) spot replicas ready. In the event of spot unavailability, on-demand replicas (up to Ntar(t)) is necessary to ensure that the service still meets the availability requirement."}, {"title": "Putting these together", "content": "SkyServe first decides Ntar(t) and Nextra(t) based on the traffic. Next, it decides the spot-on-demand mixture (i.e. the number of spot replicas and on-demand replicas at t, or S(t) and O(t)). Lastly, it assigns S(t) spot replicas to different zones, regions, and clouds based on their preemptiveness.\nSkyServe illustration. We illustrate SkyServe with an ex-ample (Figure 7). 4 spot replicas are distributed among three zones (zone 1, 2, 3). SkyServe initially fails to launch spot replicas in zone 2, as such, they are launched in zone 1 and zone 3 as zone 2 is moved to Zp. On-demand replicas are simultaneously launched as a fallback but quickly terminated once spot replicas have been launched and are in service. When zone 3 becomes unavailable, SkyServe moves replicas to zone 1. Similarly, when zone 1 becomes unavailable, Sky-Serve launches replicas in zone 3. Zone 3 later experiences preemptions, prompting SkyServe to re-try in zones 1 and 2."}, {"title": "Omniscient.", "content": "We propose an Omniscient policy that re-quires a complete spot availability trace (infeasible in prac-tice). With Integer Linear Programming (ILP), we represent the policy as a cost-minimization problem. We use S,(z, t), S(z, t) to denote the number of (ready) spot replicas launched in zone z and time t, and Or(t), O(t) to denote the number of (ready) on-demand replicas at time t. C(z, t) denotes the number of launchable spot replica capacity at zone z and time t, typically unknown to users in an online setting. M(t) is a binary variable denoting whether S,(t) + Or(t) \u2265 NTar(t), recording whether the policy has satisfied the target number of replicas. d is the cold start delay. k is the cost ratio between spot and on-demand replicas. Nmax is the maximum required number of replicas. The normalized cost C can be expressed by: C = [zez S(z, t) + kO(t)]. We express a resource availability constraint with AvailTar, the percentage of time at least NTar replicas are ready. Let T be an interval."}, {"title": "System design", "content": "We have implemented a prototype serving system SkyServe (Figure 8) that manages a mixture of spot and on-demand replicas across multiple regions and clouds. It builds on an open-sourced multi-cloud system SkyPilot [31] that provi-sions instances on public cloud providers. SkyServe adds both serving and the spot policy with additionally \u2248 7000 lines of Python code and supports common inference engines such as vLLM [24], TGI [25], Triton [26].\nService Controller. The service controller is responsible for overseeing the entire replica life cycle, including scal-ing up replicas in a zone, reducing extra on-demand replicas when there is a sufficient number of spot replicas or surplus replicas during periods of low request rates, and managing the preemptions of spot replicas or any arising errors (6). The service controller implements readiness_probe, a monitoring tool to periodically assess service status through either a stan-dard health probe or an actual user-defined compute workload (5). It then forwards the ready replica information to the load balancer (2). The service controller periodically polls the cost information via cloud API used in Algorithm 1."}, {"title": "Implementation of SkyServe.", "content": "SkyServe tries to main-tain NTar + NExtra spot replicas, as described in \u00a73.2. These replicas may be in various states: provisioning, initializing, or ready for traffic handling. SkyServe launches on-demand replicas when it does not have NTar + NExtra ready spot repli-cas. SkyServe schedules these on-demand replicas for termi-nation when a sufficient number of spot replicas are ready to accept traffic. SkyServe implements the spot placement"}, {"title": "Autoscaler.", "content": "SkyServe implements a load-based autoscaler that decides the target number of replicas NTar based on the target load per replica QTar. Scaling-up is only triggered when the endpoint is consistently experiencing traffic loads greater than QTar. The autoscaler keeps track of a config-urable past time window (default to 1 minute) and calculates the average request rate to be Rt. The autoscaler proposes a candidate target Ncan = [R] and compare it with the current NTar. If Ncan is consistently larger than the current NTar for a certain amount of time (e.g., 10 minutes), current NTar is set to Ncan. Similarly, NTar is decreased to Ncan if Ncan is consistently smaller than Ntar. SkyServe additionally supports custom policies specified by the user, such as au-toscaling based on GPU utilization or maintaining a minimum amount of base on-demand capacity."}, {"title": "Load Balancer.", "content": "SkyServe's load balancer distributes in-coming traffic (1) and supports both round-robin and routing to replicas with the least number of ongoing requests (7). It also forward the metric (e.g., QPS) to the autoscaler for decision-making (3). It can be extended to route requests to replicas closer to the clients, prioritizing underloaded replicas. We elaborate more in \u00a76. We leave these policies configurable to users."}, {"title": "Evaluation", "content": "We conduct both end-to-end experiments (\u00a75.1) and exper-iments with simulated preemptions (\u00a75.2). In the former, we deploy SkyServe and compare it to several serving systems on the cloud with real-time preemptions; while in the latter, we compare different policies based on real spot availability traces."}, {"title": "End-to-end Results with Real Preemptions on Cloud", "content": "We ran end-to-end experiments that lasted \u224822 hours in total and served 133k requests to compare SkyServe with several production and research systems with the total cost at $4.1k.\nBaselines. We compare with the following systems:\n\u2022 AWS Auto-scaling Group (ASG) [22]: ASG uses fixed node pools (e.g., fixed percentage of spot replicas and on-demand replicas). We set the on-demand percentage to 10% following its official example [53].\n\u2022 MArk [16]: an ML serving system focusing on spot CPU instances and using proactive autoscaling. We modify MArk to make it compatible with spot GPUs\u00b9.\n\u2022 AWS spot node pool (AWSSpot) [22]: A node pool that uses spot instances with autoscaling allocated over multiple zones of the same region."}, {"title": "Experiment Setup.", "content": "We conduct an end-to-end evalua-tion on AWS, where baseline systems are launched concur-rently on the cloud and experience real-time preemptions, unavailability, and cold start delay for a fair comparison. We run two sets of experiments: (1) each replica runs on a g5.48xlarge instance (with 8 A10G GPUs) and consists of Llama-2-70B [56] using vLLM [24]; (2) each replica runs on a g4dn.12xlarge instance (with 4 T4 GPUs) and consists of OPT-6.7B using SpotServe [23]. For work-load, we use the inter-arrival time and query prompts from Arena (\u00a75.2), a real LLM serving workload from Chatbot Arena [57] with bursty traffic (Figure 10c) and varying output lengths. Each serving system processes the same sequence of prompts, where each prompt is different and requires a differ-ent amount of processing time. Request timeouts are set to 100s for Llama-2-70B and 20s for OPT-6.7B, to account for the long computation time of LLM requests. For SkyServe, we launch all replicas in the following regions: us-east-2, us-west-2, eu-central-1, eu-west-2. We choose us-west-2 for other baselines due to more quota, its pop-ularity, and that its costs are lowest among regions. We con-ducted four experiments at different times of the week for the generality of the results, and we categorized the experiment into two groups: Spot Available (91.4\u2013100% spot availability) and Spot Volatile (44.6\u201345.8% spot availability\u00b2). The cost is computed with the real-time price obtained via the cloud provider's API. Request failure rates are recorded by tracking request timeouts due to preemptions and queueing."}, {"title": "Service Quality and Failure Rate.", "content": "We run Llama-2-70B on 8 A10G GPUs (Figure 9). The number of spot and on-demand instances over time successfully provisioned is shown in Figure 10. Compared to ASG, SkyServe improves ser-vice quality (P50, P90, P99 latency) by 1.1-1.6x, 1.1\u20132.5x, 1.4-1.9\u00d7 respectively. ASG maintains a single on-demand replica throughout the experiment. However, it encounters difficulties in acquiring additional spot replicas within one region due to spot unavailability. Consequently, ASG experi-ences a high failure rate of 36.4% in the event of spot volatility. Increasing the number of always-on on-demand replicas in ASG can improve availability; however, it will significantly increase the cost of ASG deployment. Compared to AWSSpot, SkyServe largely improves P50, P90, and P99 latencies by 2.6-3.9x, 2.5-3.1x, and 1.9\u20132.7\u00d7 as it leverages multiple re-gions to expand the available spot capacity and avoid pre-emptive regions. AWSSpot's single-region policy is unable to guarantee enough replicas, leading to a larger 49.4-93.5%"}, {"title": "Cost.", "content": "Compared to ASG, SkyServe lowers cost by 20-24%, as ASG maintains an on-demand replica even when spot in-stances are available. For ASG, in group Spot Volatile, the cost of on-demand replicas comprises 97% of the total cost, since spot instances are less unavailable. For AWSSpot, we observed a provision-then-preempt cycle in preemptive zones (e.g., us-west-2b) when the spot is volatile with increased costs. MArk and AWSSpot over-request in the event of un-availability, and we observed up to 14 replicas in provision-ing status (Figure 11b), likely because both systems target CPU instances and assume that replicas will quickly become ready after which provisioning replicas will be scaled down. Though AWSSpot uses entirely spot, SkyServe lowers cost by 20% in group Spot Available. In group Spot Volatile, as both"}, {"title": "Results with Simulated Preemptions from Real Traces", "content": "We run the following workloads:\n\u2022 Poisson Distribution (Poisson). We construct synthetic workloads with Poisson distribution.\n\u2022 Arena. We use a real LLM serving workload from Chatbot Arena [57] with load fluctuations, bursty traffic (Figure 10c), and dynamic request execution time.\n\u2022 Microsoft Azure Function (MAF). MAF [58] was collected from Azure serverless function invocations in two weeks, and has been used for ML serving re-search [10, 23, 59, 60].\nFollowing prior work [23, 27], we benchmark SkyServe performance based on real spot availability traces and work-load traces. Instead of experiencing real-time preemptions on the cloud (\u00a75.1), spot preemptions are instead injected based on the collected spot availability traces. We compare Sky-Serve with the following policies, similar to prior work [12]:\n\u2022 Even Spread: Evenly spreading spot replicas across zones of different regions, similar to [16, 22, 32].\n\u2022 Round Robin: Launch spot replicas to zones of differ-ent regions, round-robin.\n\u2022 Optimal: Omniscient policy based on ILP (\u00a73.3).\nSpot datasets. We use spot traces from [27]. These traces were previously collected by launching spot GPUs and expe-riencing preemptions and unavailability on the cloud.\n\u2022 AWS 1: A 2-week trace for 4 p3.2xlarge in 3 zones.\n\u2022 AWS 2: A 3-week trace for 16 p3.2xlarge in 3 zones.\n\u2022 AWS 3: A 2-month trace for p3.2xlarge in 9 zones.\n\u2022 GCP 1: A 3-day trace for a2-ultragpu-4g in 6 zones.\nAvailability. We evaluate the resource availability of dif-ferent policies (Figure 13a). Even Spread achieves 27-63% availability, whereas Round Robin achieves 82\u201399% availability. When spot replicas become unavailable, Even Spread and Round Robin cannot ensure that we can quickly find and launch at least NTar ready spot replicas. For example, in AWS 2, spot replicas experience unavailability across all zones. In these cases, Even Spread and Round Robin will suffer un-availability. In contrast, SkyServe achieves high availability (99-100%) because of three reasons. First, Dynamic Fallback ensures that SkyServe uses on-demand replicas when spot replicas become unavailable. Second, it is unlikely to lose many spot replicas before an on-demand fallback replica is ready, thanks to cheap over-provisioning with spot replicas. Third, by tracking preemptive zones, SkyServe can minimize the likelihood of preemptions by avoiding these zones.\nCost. SkyServe reduces cost by 42-54% (Figure 13b) com-pared to using entirely on-demand replicas. The cost for Even Spread and Round Robin is lower, due to more spot pre-emptions and less spot capacity. This is consistent with the end-to-end experiments, where these policies cannot launch enough replicas, suffering from worse service quality with"}, {"title": "Service Quality.", "content": "We report request latency in Fig 14. A static policy (e.g., Even Spread) experiences frequent preemp-tions, resulting in fewer ready replicas than NTar and worse service quality. SkyServe achieves 1.1-3.0\u00d7, 1.0-1.8\u00d7 reduc-tion in average latency compared to Even Spread and Round Robin, and within 5% to the Optimal."}, {"title": "Future Work", "content": "Latency-aware load balancer. For requests that require real-time interactivity and short Time-to-first-token (TTFT), SkyServe can be extended to dynamically route these requests to replicas in the same zone as the client, as well as monitor the replica load and only direct requests to a remote zone if local replicas are overloaded. SkyServe can also keep an on-demand node pool in the client zone for these requests and route accordingly. This ensures that these requests can still be served from the closest replica to the client, while the requests that would otherwise cause overload can be routed to another replica in a remote region. That said, we expect inter-region latency to be largely outweighed by computation time for most requests (\u00a73.1).\nSupport heterogeneous accelerators. GPUs have differ-ent performance-to-cost trade-offs. Expensive GPUs tend to have better performance albeit being less available. While SkyServe supports specifying a set of GPUs, it can be ex-tended to adopt a more intelligent algorithm to leverage ac-celerator heterogeneity. For example, when the spot instance for a higher-end GPU is unavailable, one might switch to a spot instance of a cheaper, lower-end GPU instead."}, {"title": "Related Work", "content": "Spot instances for non-serving workloads. Spot instances have drawn interest from both industry [12, 61] and academia [27, 62, 63]. Prior work explored using spot instances for maintain-ing resource availability [12], web services [64], in-memory storage [65], batch [66, 67] or interactive jobs [68], HPC [69], analysis tasks [40], or elastic services [15]. ML training on spot instances has also been extensively studied [13, 14, 18-20, 70]. For example, Bamboo provides resilience for DNN training on preemptible instances [18]. Varuna [19] trains DNN networks on spot instances with commodity network-ing. However, scheduling spot replicas for serving requires high resource availability and low latency.\nSpot instances for serving workloads. As mentioned, SpotServe addresses a different set of problems from Sky-Serve: preemption-tolerant model parallelization across multi-ple spot instances. Other works that incorporate spot instances are MArk [16], Cocktail [17], and Tributary [15]. However, they focus on serving small ML models using spot CPUs. In \u00a72.3, we showed that spot GPUs are more preemptive than spot CPUs, and systems that target spot CPUs do not perform well in GenAI workloads (\u00a75.1). Further, prior works only consider allocating instances in a single region or zone and result in unavailability. SkyServe instead provisions GPUs across multiple regions and clouds. Snape [12] uses spot CPU"}, {"title": "Conclusions", "content": "Spot GPUs are economically appealing to serve AI workloads, but they have not been widely considered viable for serving due to unpredictable preemptions and long cold start delays. We introduce SkyServe to serve AI workloads on a mixture of spot and on-demand GPUs across regions and clouds. SkyServe diversifies spot replica placements across regions and clouds, overprovisions spot replicas to mitigate preemptions, and proactively uses on-demand replicas when spot replicas are less available. SkyServe provides a unified interface to host AI services on mixtures of spot and on-demand replicas. Through evaluations on real AI workloads, SkyServe saves cost by up to 44% while achieving similar resource availability compared to on-demand deployments, and improves P50, P90, and P99 latency by up to 2.6x, 3.1x, 2.7\u00d7 respectively compared to other systems."}]}