{"title": "DAGPrompT: Pushing the Limits of Graph Prompting with a Distribution-aware Graph Prompt Tuning Approach", "authors": ["Qin Chen", "Liang Wang", "Bo Zheng", "Guojie Song"], "abstract": "The \"pre-train then fine-tune\" approach has advanced GNNs by enabling general knowledge capture without task-specific labels. However, an objective gap between pre-training and downstream tasks limits its effectiveness. Recent graph prompting methods aim to close this gap through task reformulations and learnable prompts. Despite this, they struggle with complex graphs like heterophily graphs. Freezing the GNN encoder can reduce the impact of prompting, while simple prompts fail to handle diverse hop-level distributions. This paper identifies two key challenges in adapting graph prompting methods for complex graphs: (i) adapting the model to new distributions in downstream tasks to mitigate pre-training and fine-tuning discrepancies from heterophily and (ii) customizing prompts for hop-specific node requirements. To overcome these challenges, we propose Distribution-aware Graph Prompt Tuning (DAGPrompT), which integrates a GLORA module for optimizing the GNN encoder's projection matrix and message-passing schema through low-rank adaptation. DAGPrompT also incorporates hop-specific prompts accounting for varying graph structures and distributions among hops. Evaluations on 10 datasets and 14 baselines demonstrate that DAGPrompT improves accuracy by up to 4.79% in node and graph classification tasks, setting a new state-of-the-art while preserving efficiency. Codes are available at GitHub.", "sections": [{"title": "1 Introduction", "content": "In recent years, the schema of \"pre-training then fine-tuning\" on Graph Neural Networks (GNNs) has experienced significant growth, especially in few-shot learning scenarios [10, 17, 27, 44]. Specifically, GNNs are pre-trained in a self-supervised manner on tasks such as graph property reconstruction [8, 12] or contrastive learning [24, 39]. Then, GNNs are adapted to downstream tasks during the fine-tuning. However, a common limitation is that the gap between pre-training and downstream objectives is often overlooked, which hinders the model performance. For example, the pre-training objective may be link prediction, while the downstream objective may be node classification, and these two objectives vary a lot [26]. \u03a4\u03bf address this, recent research has begun incorporating graph prompting techniques [7, 16, 25, 26] to bridge the gap between pre-training and downstream tasks. They propose using prompts to reformulate downstream tasks as pre-training tasks with additional learnable parameters. The pre-trained GNN encoder remains frozen during this process. For example, GPPT [25] reformulates the downstream task, node-classification, to the pre-training task, link-prediction. This reformulation reduces the objective gap between the pre-train and downstream by the alignment of objective forms.\nHowever, existing prompting methods are sub-optimal for graphs with complex distributions, such as heterophily graphs, where connected nodes frequently have different labels [4, 42]. This label disparity creates a profound disconnect between pre-training objectives and downstream tasks. As most pre-training techniques are label-agnostic and rely on graph structure to varying extents, they inherently suffer from this discrepancy. For instance, tasks like link"}, {"title": "2 Preliminary", "content": "Notations. Consider an undirected graph G = {V, 8}, where V represents the set of N nodes and & represents the set of E edges. The graph is described by its adjacency matrix A \u2208 R^{N\u00d7N}, where A_{ij} = 1 if and only if there exists an edge e_{ij} \u2208 & connecting node v_i and node v_j. Additionally, each node v_i \u2208 V is associated with"}, {"title": "3 Method", "content": "In this section, we elaborate on the Distribution-aware Graph Prompt Tuning (DAGPrompT). The framework of DAGPrompT is illustrated in Figure 3, which consists of two stages: (i) Link-prediction-based pre-training. (ii) Graph Low-rank Adaptation with Hop-specific Graph Prompting. We also provide a detailed algo-rithm in Appendix A and a complexity analysis in Appendix B."}, {"title": "3.1 Label-free Pre-Training", "content": "The pre-training strategy is essential for few-shot learning, allowing the model to capture graph structures across diverse domains without labeled data, as shown by several approaches [16, 25, 26, 40]. It also aids in capturing local structures and reduces over-fitting [17]. We adopt link prediction for pre-training due to its advantages: (i) the abundance of inherent edge data in graphs, and (ii) alignment in objective forms between pre-training and downstream tasks, as tasks like node and graph classification can be seamlessly reformulated as link prediction by introducing pseudo-nodes or pseudo-graphs [16, 25].\nConsider a node v in a graph G. For training, a positive node a is selected from the neighbors of v, and a negative node b from the non-neighbors, forming a triplet (v, a, b). Let the GNN encoder f produce the corresponding embeddings s_v, s_a, and s_b. By considering all nodes in G, the pre-training dataset T_{pt} is constructed. The pre-training loss is then defined as:\nL_{pt}(\u0398) = - \\sum_{(v,a,b) \\in T} ln \\frac{exp (sim (s_v, s_a) /\u03c4)}{\\sum_{u\\in \\{a,b\\}} exp (sim (s_v, s_u) /\u03c4)},"}, {"title": "3.2 Distribution-aware Graph Prompt Tuning", "content": "In this subsection, we discuss tuning and prompting the pre-trained GNN f for downstream tasks in a distribution-aware approach. We introduce the Graph Low-Rank Adaptation (GLORA) module, which aligns the projection and message passing scheme of f with the distribution of downstream tasks through low-rank adaptation. This approach preserves the knowledge embedded in the pre-trained weights while adapting to new tasks. We then detail the prompting module, which links diverse downstream tasks to the pre-training objective, ensuring alignment with the unique downstream distributions in a hop-decoupled manner."}, {"title": "3.2.1 Tuning with Graph Low-Rank Adaptation", "content": "Previous works often pre-train GNNs and keep them frozen during prompting, relying on learnable prompts for downstream tasks [7, 16, 25, 26]. While effective in graphs with strong homophily, this approach underperforms in more complex settings, such as graphs with strong heterophily, as shown in Figure 2. Freezing the GNN can lead to performance degradation in such cases, as most pre-training methods are label-agnostic, and downstream objectives often differ from pre-training goals, especially in heterophily graphs. For instance, link prediction favors similar embeddings for connected nodes, which aligns with homophily but fails in heterophily, where connected nodes may have dissimilar characteristics. As illustrated in Figure 1, on heterophily graphs, pre-training without tuning the GNN encoder can result in nodes with different labels being mapped too closely in latent space, making them difficult to distinguish during prompting. However, tuning GNN parameters directly during prompting presents other challenges, including computational inefficiency and the risk of over-fitting due to sparse downstream labels [17]. A theoretical analysis is offered in section 4, with experimental results in subsection E.6. A theoretical analysis of these issues is provided in section 4, with corresponding experimental results in subsection E.6.\nTo efficiently adapt to the distributions of downstream tasks while preserving the knowledge in the pre-trained weights, we introduce the Graph Low-Rank Adaptation (GLORA) module, inspired by LoRA from the NLP field [9]. GLORA targets two components during fine-tuning: (i) the message-passing scheme and (ii) the projection matrices. Formally, for the l-th GNN layer, the fine-tuning process with GLORA is expressed as follows:\nH^{(l)} = (A+P_A Q_A)H^{(l-1)} (W^{(l)} + P^{(l)} Q^{(l)}),"}, {"title": "3.2.2 Hop-specific Graph Prompting", "content": "Unification of Downstream Tasks. We begin the elaboration of our prompting technique by introducing how we unify various downstream tasks. To achieve this, we reformulate all downstream tasks as sub-graph level tasks, as they represent a general and expressive framework for many tasks [26]. This allows us to adapt various downstream tasks to our link-prediction pre-training task. Formally, given a node u in a graph G, we define its k-hop neighborhood as N_k (v) and its embedding (produced by the GNN encoder f) as s_{k,v}. Consequently, we have:\n* Link-Prediction. Given a node triplet (v, a, b) where an edge exists between nodes (v, a) but not between (v, b) does not, it's expected that sim(s_k, S_{k,a}) > sim(s_k, S_{k,b}). Here, the similarity measure (sim) can be computed using methods such as cosine similarity.\n* Node Classification. In a graph with C labels, we construct C pseudo-nodes, with their embeddings initialized as the mean of the embeddings of nodes from the same class in the training set. The label prediction task for a node v is then reduced to identifying the pseudo-node most likely to form an edge with v, transforming the problem into a link prediction task.\n* Graph Classification. For a set of graphs with C labels, we generate C pseudo-graphs, initializing their embeddings as the average of the graph embeddings from the training set. Similar to node classification, predicting a graph's label is formulated as a link prediction problem between the graph and the pseudo-graphs.\nConventional approaches with GNN encoders of L layers typically rely on the final layer embedding H^{(L)}, or a combination of all intermediate embeddings for prompting [7, 16, 25]. However, these methods often fail to account for hop-specific preferences of different nodes, limiting their adaptability. For example, in heterophilic graphs like dating networks where gender is the label, the first-hop neighborhood may exhibit heterophily, while the second-hop neighborhood may show homophily [18, 42]. We illustrate this in Figure 6. Given the varying distributions across hops and their potential differing impact on performance [42], we propose decoupling the graph prompting process in a hop-specific manner. First, we collect intermediate embeddings from GNN layers to construct a more informative sequence than using only H^{(L)}:\nH = [H^{(0)} ||H^{(1)} || ... ||H^{(L)}] \u2208 R^{(L+1)\u00d7N\u00d7d},"}, {"title": "4 Theoretical Analysis", "content": "In this section, we present a theoretical analysis of the GLORA module. Although low-rank adaptation may be less optimal than full-parameter fine-tuning in NLP tasks [28], we demonstrate that in few-shot settings, low-rank adaptation proves to be more effective.\nTHEOREM 1. Let H be a hypothesis class, and D = {(x_i, y_i)} be a dataset of m i.i.d. samples. Suppose the loss function l(h(x), y) is bounded by 0 \u2264 l(h(x), y) \u2264 B. Then, with probability at least 1 \u2013 \u03b4, for all h \u2208 H, we have:\nL(h) \u2013 \u00ce1(h) \u2264 2R_{1}(H) + 3B\\sqrt{\\frac{log(2/\u03b4)}{2m}},"}, {"title": "5 Experiments", "content": "In this section, we evaluate the capability of DAGPrompT by addressing the following key questions:\n* Q1: How does DAGPrompT perform compared to state-of-the-art models on real-world datasets?\n* Q2: How does the internal data distribution, such as heterophily levels, affect DAGPrompT's performance?\n* Q3: How does the number of labels impact DAGPrompT's performance?\n* Q4: How well does DAGPrompT transfer to other graphs?\n* Q5: What is the running efficiency of DAGPrompT?\n* Q6: How do the main components of DAGPrompT influence its performance?\nWe also conduct additional experiments on other backbones, along with a full-shot evaluation, parameter analysis, and visualizations of graph hop-wise distributions and GLORA weights, as detailed in Appendix E."}, {"title": "5.1 Datasets and Settings", "content": "We evaluate DAGPrompT on both few-shot node classification and graph classification tasks. For the few-shot node classification, we use seven datasets of varying scales, types, and heterophily levels. Texas, Wisconsin, Cornell, Chameleon, Squirrel [21], and Arxiv-Year [15] represent well-known heterophily datasets, while Cora [32] is a commonly used homophily graph. The dataset statistics are provided in Table 1. Additionally, we generate synthetic"}, {"title": "5.3 Evaluation on Real-world Datasets (Q1)", "content": "We evaluate DAGPrompT with GCN as the backbone, as shown in Table 3, and draw the following key observations: (i) DAGPrompT consistently outperforms other baselines by a large margin. The performance improvements on heterophily datasets are particularly notable, with up to a 4.79% increase on Texas and an average improvement of 2.43% across all datasets. (ii) Fine-tuning or prompting methods sometimes underperform compared to training from scratch on heterophily graphs. For example, H2GCN, trained from scratch, surpasses most graph prompt methods on Texas, Cornell, and Wisconsin. This supports the claim in section 1 that larger gaps exist between pre-training and downstream tasks in complex graphs, where task reformulation and prompting alone are insufficient to bridge the gap caused by intricate graph distributions. The heterophily in these graphs limits the"}, {"title": "5.4 Evaluation on Data Heterophily (Q2) and Number of Shots (Q3)", "content": "We investigate the impact of varying heterophily levels by generating a series of synthetic graphs, Syn-Chameleon, based on the Chameleon dataset. Following the method in [18], we control the homophily ratio of Syn-Chameleon by adjusting the edges, allowing the homophily ratio to range from 0.9 (strong homophily) to 0.1 (strong heterophily). Models are evaluated on these graphs under a full-shot setting, using 50% of the nodes for training to minimize the effect of label quantity. The results, shown in Figure 4, demonstrate that DAGPrompT consistently outperforms the baselines, especially in strong heterophily scenarios. Notably, heterophily-aware models like GPR-GNN outperform most non-heterophily-aware models, such as GraphPrompt and GPF-Plus-LP, in this setting."}, {"title": "5.5 Evaluation on Transfer Ability (Q4)", "content": "We evaluate the transferability of DAGPrompT in Table 5. For pre-training, we use the Texas dataset as the source domain and test the transfer to downstream tasks on the Cornell, Wisconsin, and Chameleon datasets. Models with the suffix -Scratch are those trained directly on the downstream tasks without pre-training, while models with the suffix -Cross are pre-trained on the source domain and then fine-tuned (or prompted) on the target domains.\nThe results show that pre-training, even across different domains, generally enhances model performance. DAGPrompT exhibits the most significant improvement when transitioning from training from scratch to cross-domain pre-training, highlighting its strong transferability. This makes DAGPrompT particularly useful in cases"}, {"title": "5.6 Efficiency Analysis (Q5)", "content": "We evaluate the efficiency of DAGPrompT on the Chameleon dataset, as shown in Table 6. The results demonstrate that DAG-PrompT is generally efficient, exhibiting fast running speed, low GPU memory consumption, and a small number of tunable parameters. Overall, DAGPrompT ranks highly in terms of time efficiency, memory usage, and parameter efficiency."}, {"title": "5.7 Ablation Study (Q6)", "content": "We perform an ablation study to assess the impact of each component in DAGPrompT in Table 7, by disabling them individually. For the Layer-Specific Prompts, we adopt the last layer only. For the coefficients \u03b3 in Equation 8, we fix all values to 1. Overall, each component contributes to performance to varying extents."}, {"title": "6 Related Works", "content": "In recent years, significant advancements have been made in the development of pre-trained Graph Neural Networks (GNNs). These methods can be broadly categorized into three main types: (i) Graph Property Reconstruction-Based Methods, which focus on reconstruct-ing specific graph properties such as node attributes [8, 10] or links [12, 17]; (ii) Sub-Graph Contrastive Methods, which distinguish posi-tive subgraphs from negative ones [33, 39, 44]; and (iii) Local-Global"}, {"title": "6.2 Heterophily Graph Learning", "content": "Traditional GNNs typically assume homophily (similarity between connected nodes) [20] and are less effective in heterophily graphs, where connected nodes differ significantly [42]. To address this, models such as H2GCN [42] and GPR-GNN [4] enhance message-passing with high-order re-weighting techniques to improve com-patibility with heterophily. LINKX, a simpler model, is optimized for large-scale heterophily learning [15]. There are more approaches refine graph convolution for heterophilous data [3, 5, 14, 18, 30, 41]. However, most heterophily-aware models are designed for training from scratch in label-rich scenarios and face generalization and over-fitting issues in few-shot settings."}, {"title": "7 Conclusion", "content": "In this paper, we push the limits of the graph prompting paradigm to graphs with complex distributions, such as heterophily graphs. We observe that current methods struggle to generalize in these settings and are, in some cases, outperformed by simple models trained from scratch. We identify two key challenges for better generalization on complex graphs: (i) adapting the model to new distributions in downstream tasks to reduce discrepancies between pre-training and fine-tuning due to heterophily, and (ii) aligning model prompts to the hop-specific needs of different nodes. To address these challenges, we propose Distribution-aware Graph Prompt Tuning (DAGPrompT), which includes a GLORA module and a Hop-specific Graph Prompting module, corresponding to the two challenges outlined above. Our experiments across 10 datasets and 14 baselines demonstrate the state-of-the-art performance of DAGPrompT, achieving up to a 4.79% improvement in accuracy."}, {"title": "A Algorithm of DAGPrompT", "content": "We detail the DAGPrompT algorithm from pre-training to prompt-ing in Algorithms algorithm 1 and algorithm 2. For clarity, we present the algorithm using a for-loop structure, though in practice, we process data in batches. The example provided focuses on node"}, {"title": "B Complexity Analysis", "content": "Consider a graph with N nodes and E edges, where d is the hidden dimension, L the number of GNN encoder layers, and C the number of classes.\nThe pre-training complexity of the GNN encoder is O((LE +NK)d), where K is the number of negative samples. In this work, we set K = 1.\nFor prompting, the complexity of DAGPrompT arises from three components: (i) generating layer-wise embeddings from the GNNencoder f, (ii) generating layer-wise class tokens, and (iii) performing similarity calculations. Step (i) incurs a complexity ofO(L(E + Nd\u00b2)), driven by message passing and embedding projection. Step (ii) has a lighter complexity of O (LCd), involving matrixaddition. Step (iii) incurs a complexity of O(LNCd), dominated bysimilarity calculations.\nThe overall complexity of DAGPrompT is O(L(NC+LC+Nd)d+LE). Given that L \u00ab N and C \u00ab d, this simplifies to O(LNd\u00b2+LEd),yielding near-linear complexity with respect to graph size, makingit efficient for large-scale applications."}, {"title": "C Details of Theorem 1", "content": "Let H be a hypothesis class, and let D = {(x_1, y_1), . . ., (x_m, Y_m) }represent a dataset of m independent and identically distributed(i.i.d.) samples drawn from an unknown distribution. The goal is toevaluate the performance of a hypothesis h \u2208 H, which we do byassessing its true risk (or expected error). The true risk is definedas:\nL(h) = E_{(x,y)~D}[l(h(x), y)],"}, {"title": "D Dataset Details", "content": "In this section, we describe the datasets used in our study.\nThe Cora dataset [32] is a widely used citation network charac-terized by strong homophily [20]. In Cora, nodes represent papers,node features are bag-of-words representations derived from thecontent, and edges correspond to citation links. The labels indicatethe subject categories of the papers.\nThe Texas, Wisconsin, Cornell, Chameleon, and Squirrel datasets[21] consist of web pages, where nodes represent individual pages,node features are word embeddings, and edges reflect hyperlinks.Labels for Texas, Cornell, and Wisconsin represent web page cate-gories, while Chameleon and Squirrel labels capture average monthlyweb traffic, grouped into five ranges. Notably, Chameleon and Squir-rel are complex Wikipedia networks, exhibiting a mix of homophilyand heterophily [18]."}, {"title": "E.1 DAGPrompt with GAT as Backbone", "content": "To evaluate the generalization capability of DAGPrompt, we conduct 5-shot and 10-shot node classification experiments using GAT as the backbone encoder\u00b2. The results in Table 8 show that DAG-Prompt consistently outperforms all baselines, providing strong evidence of its robust generalization performance."}, {"title": "E.2 Full-shot Experiments", "content": "We further evaluate DAGPrompt under the full-shot setting, using a training-validation-test split of approximately 50%-25%-25%. The results in Table 9 show that, although the performance improvement is less pronounced compared to the few-shot settings in Table 3, DAGPrompt consistently achieves the best results across all datasets, particularly on those with strong heterophily."}, {"title": "E.3 Parameter Analysis", "content": "We conduct a parameter analysis for \u03b1 and r in Figure 7. The results show that Texas and Cornell generally perform better with larger"}, {"title": "E.4 Visualization of GLoRA Weights", "content": "We visualize the weight distributions of GLORA on the Texas and Cornell datasets in Figure 8. For clarity, we present the final result of A + P_A Q_A, representing the edge weights used during message passing in GNNs. As shown, GLORA strengthens certain connections by assigning weights greater than 1, while weakening others with weights less than 1. This suggests that GLORA refines the graph message-passing scheme to better align with downstream tasks."}, {"title": "E.5 Pre-training Helps the Convergence", "content": "To examine the effect of pre-training on model convergence, we conduct experiments with varying hidden sizes of DAGPrompT on the Chameleon dataset. For each configuration, the model is pre-trained either sufficiently (200 epochs) or minimally (30 epochs). As shown in Figure 9, the results demonstrate that sufficient pre-training not only accelerates convergence (evident from a faster reduction in loss) but also improves final performance (achieving a lower loss). This aligns with findings in NLP fields, where sufficient pre-training reduces the intrinsic dimension of the model, simplifying the learning process for downstream tasks [1]."}, {"title": "E.6 DAGPrompT with Full-parameter Tuning", "content": "We conducted an experiment with two variants of DAGPrompT:DAGPrompT-Full and DAGPrompT-Freeze. DAGPrompT-Full removes the GLORA module and fine-tunes all GNN encoder parameters during prompting, while DAGPrompT-Freeze removes the GLORA module and freezes all GNN encoder parameters during prompting. The results show that both DAGPrompT-Full and DAGPrompT-Freeze perform worse than DAGPrompT. This supports the theory from section 4 that neither zero-parameter nor full-parameter tuning is optimal in few-shot settings."}]}