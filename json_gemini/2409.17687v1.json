{"title": "Graph Edit Distance with General Costs Using Neural Set Divergence", "authors": ["Eeshaan Jain", "Indradyumna Roy", "Saswat Meher", "Soumen Chakrabarti", "Abir De"], "abstract": "Graph Edit Distance (GED) measures the (dis-)similarity between two given graphs, in terms of the minimum-cost edit sequence that transforms one graph to the other. However, the exact computation of GED is NP-Hard, which has recently motivated the design of neural methods for GED estimation. However, they do not explicitly account for edit operations with different costs. In response, we propose GRAPHEDX, a neural GED estimator that can work with general costs specified for the four edit operations, viz., edge deletion, edge addition, node deletion and node addition. We first present GED as a quadratic assignment problem (QAP) that incorporates these four costs. Then, we represent each graph as a set of node and edge embeddings and use them to design a family of neural set divergence surrogates. We replace the QAP terms corresponding to each operation with their surrogates. Computing such neural set divergence require aligning nodes and edges of the two graphs. We learn these alignments using a Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and edge alignments are consistent with each other. Moreover, these alignments are cognizant of both the presence and absence of edges between node-pairs. Experiments on several datasets, under a variety of edit cost settings, show that GRAPHEDX consistently outperforms state-of-the-art methods and heuristics in terms of prediction error.", "sections": [{"title": "Introduction", "content": "The Graph Edit Distance (GED) between a source graph, G, and a target graph, G', quantifies the minimum cost required to transform G into a graph isomorphic to G'. This transformation involves a sequence of edit operations, which can include node and edge insertions, deletions and substitutions. Each type of edit operation may incur a different and distinctive cost, allowing the GED framework to incorporate domain-specific knowledge. Its flexibility has led to the widespread use of GED for comparing graphs across diverse applications including graph retrieval [5, 6], pattern recognition [46], image and video indexing [50, 48] and chemoinformatics [21]. Because costs for addition and deletion may differ, GED is not necessarily symmetric, i.e., GED(G, G') \u2260 GED(G', G). This flexibility allows GED to model a variety of graph comparison scenarios, such as finding the Maximum Common Subgraph and checking for Subgraph Isomorphism [13]. In general, it is hard to even approximate GED [32]. Recent work [5, 6, 19, 55, 39] has leveraged graph neural networks (GNNs) to build neural models for GED computation, but many of these approaches cannot account for edit operations with different costs. Moreover, several approaches [40, 31, 55, 6] cast GED as the Euclidean distance between graph embeddings, leading to models that are overly attuned to cost-invariant edit sequences."}, {"title": "Present work", "content": "We propose a novel neural model for computing GED, designed to explicitly incorporate the various costs of edit operations. Our contributions are detailed as follows.\nNeural set divergence surrogates for GED We formulate GED under general (non-uniform) cost as a quadratic assignment problem (QAP) with four asymmetric distance terms representing edge deletion, edge addition, node deletion and node addition. The edge-edit operations involve quadratic dependencies on a node alignment plan a proposed mapping of nodes from the source graph to the target graph. To avoid the the complexity of QAP [44], we design a family of differentiable set divergence surrogates, which can replace the QAP objective with a more benign one. In this approach, each graph is represented as a set of embeddings of nodes and node-pairs (edges or non-edges). We replace the original QAP distance terms with their corresponding set divergences, and obtain the node alignment from a differentiable alignment generator modeled using a Gumbel-Sinkhorn network. This network produces a soft node permutation matrix based on contextual node embeddings from the graph pairs, enabling the computation of the overall set divergence in a differentiable manner, which facilitates end-to-end training. Our proposed model relies on late interaction, where the interactions between the graph pairs occur only at the final layer, rather than during the embedding computation in the GNN. This supports the indexing of embedding vectors, thereby facilitating efficient retrieval through LSH [25, 24, 12], inverted index [20], graph based ANN [34, 37] etc.\nLearning all node-pair representations The optimal sequence of edits in GED is heavily influenced by the global structure of the graphs. A perturbation in one part of the graph can have cascading effects, necessitating edits in distant areas. To capture this sensitivity to structural changes, we associate both edges as well as non-edges with suitable expressive embeddings that capture the essence of subgraphs surrounding them. Note that the embeddings for non-edges are never explicitly computed during GNN message-passing operations. They are computed only once, after the GNN has completed its usual message-passing through existing edges, thereby minimizing additional computational overhead.\nNode-edge consistent alignment To ensure edge-consistency in the learned node alignment map, we explicitly compute the node-pair alignment map from the node alignment map and then utilize this derived map to compute collective edge deletion and addition costs. More precisely, if (u, v) \u2208G and (u', v') \u2208 G' are matched, then the nodes u and v are constrained to match with u' and v' (or, v' and u') respectively. We call our neural framework as GRAPHEDX.\nOur experiments across several real datasets show that (1) GRAPHEDX outperforms several state-of-the-art methods including those that use early interaction; (2) the performance of current state-of-the-art methods improves significantly when their proposed distance measures are adjusted to reflect GED-specific distances, as in our approach."}, {"title": "Problem setup", "content": "Notation The source graph is denoted by G = (V, E) and the target graph by G' = (V', E'). Both graphs are undirected and are padded with isolated nodes to equalize the number of nodes to N. The adjacency matrices for G and G' after padding are A, A' \u2208 {0,1}^{N\u00d7N}. (Note that we will use MT, not M', for the transpose of matrix M.) The sets of padded nodes in G and G' are denoted by PaddedNodesg and PaddedNodesg, respectively. We construct \u03b7 \u2208 {0, 1}^N, where \u03b7[u] = 0 if u \u2208 PaddedNodesg and 1 otherwise (same for G'). The embedding of a node u \u2208 V computed at propagation layer k by the GNN, is represented as xk(u). Edit operations, denoted by edit, belong to one of four types, viz., (i) node deletion, (ii) node addition, (iii) edge deletion, (iv) edge addition. Each operation edit is assigned a cost cost(edit). The node and node-pair alignment maps are described using (hard) permutation matrices P \u2208 {0,1}^{N\u00d7N} and S \u2208 {0,1}^{\\binom{N}{2}\u00d7\\binom{N}{2}} respectively. Given that the graphs are undirected, node-pair alignment need only be specified across at most \\binom{N}{2} pairs. When a hard permutation matrix is relaxed to a doubly-stochastic matrix, we call it a soft permutation matrix. We use P and S to refer to both hard and soft permutations, depending on the context. We denote PN as the set of all hard permutation matrices of dimension N; [N] as {1, . . ., N} and || A||1,1 to describe \u03a3u\u03c5 |A[u, v]|. For two binary variables C1, C2 \u2208 {0, 1}, we denote J(C1, C2) as (c1 XOR c2), \u0456.\u0435., J(C1, C2) = c1c2 + (1 - c\u2081)(1 \u2013 c2)."}, {"title": "Graph edit distance with general cost", "content": "We define an edit path as a sequence of edit operations 0 = {edit1, edit2, ...}; and O(G, G') as the set of all possible edit paths that transform the source graph G into a graph isomorphic to the target graph G'. Given O(G, G') and the cost associated with each operation edit, the GED between G and G' is the minimum collective cost across all edit paths in O(G, G'). Formally, we write [14, 7]:\nGED(G, G') = \\underset{0=\\{edit_1, ... , edit_{|0|}\\}\\in O(G, G')}{min} \\sum_{i \\in [|0|]} cost(edit_i). (1)\nIn this work, we assume a fixed cost for each of the four types of edit operations. Specifically, we use a, a\u2295, b and b\u2295 to represent the costs for edge deletion, edge addition, node deletion, and node addition, respectively. These costs are not necessarily equal, in contrast to the assumptions made in previous works [5, 31, 55, 39]. Additional discussion on GED with node substitution in presence of labels can be found in Appendix D.\nProblem statement Our objective is to design a neural architecture for predicting GED under a general cost framework, where the edit costs a, a\u2295, b and b\u2295 are not necessarily the same. During the learning stage, these four costs are specified, and remain fixed across all training instances D = {(Gi, G'\u2081, GED(Gi, G'\u2081))} i\u2208[n]\u00b7 Note that the edit paths are not supervised. Later, given a test instance G, G', assuming the same four costs, the trained system has to predict GED(G, G')."}, {"title": "Proposed approach", "content": "In this section, we first present an alternative formulation of GED as described in Eq. (1), where the edit paths are induced by node alignment maps. Then, we adapt this formulation to develop GRAPHEDX, a neural set distance surrogate, amenable to end-to-end training. Finally, we present the network architecture of GRAPHEDX."}, {"title": "GED computation using node alignment map", "content": "Given the padded graph pair G and G', deleting a node u \u2208 V can be viewed as aligning node u with some padded node u' \u2208 PaddedNodesg. Similarly, adding a new node u' to G can be seen as aligning some padded node u \u2208 PaddedNodesg with node u' \u2208 V'. Likewise, adding an edge to G corresponds to aligning a non-edge (u, v) \u2209 E with an edge (u', v') \u2208 G'. Conversely, deleting an edge in G corresponds to aligning an edge (u, v) \u2208 G with a non-edge (u', v') \u2209 G'.\nTherefore, GED(G, G') can be defined in terms of a node alignment map. Let I\u0147 represent the set of all node alignment maps \u03c0 : [N] \u2192 [N] from V to V'. Recall that \u03b7g [u] = 0 if u \u2208 PaddedNodesg and 1 otherwise.\n\\underset{\\pi \\in \\Pi_N}{min} \\frac{1}{2} (\\sum_{u, v} (\\alpha^{\\ominus} \\cdot I[(u,v) \\in E \\land (\\pi(u), \\pi(v)) \\notin E'] + \\alpha^{\\oplus} \\cdot I [(u,v) \\notin E \\land (\\pi(u), \\pi(v)) \\in E'])) \\\\ + \\sum_u (b^{\\ominus} \\cdot \\eta_G[u] (1 - \\eta_{G'} [\\pi(u)]) + b^{\\oplus} \\cdot (1 - \\eta_G[u]) \\eta_{G'} [\\pi(u)]). (2)\nIn the above expression, the first sum iterates over all pairs of (u, v) \u2208 [N] \u00d7 [N] and the second sum iterates over u \u2208 [N]. Because both graphs are undirected, the fraction 1/2 accounts for double counting of the edges. The first and second terms quantify the cost of deleting and adding the edge (u, v) from and to G, respectively. The third and the fourth terms evaluate the cost of deleting and adding node u from and to G, respectively."}, {"title": "GED as a quadratic assignment problem", "content": "In its current form, Eq. (2) cannot be immediately adapted to a differentiable surrogate. To circumvent this problem, we provide an equivalent matricized form of Eq. (2), using a hard node permutation matrix P instead of the alignment map \u03c0. We compute the asymmetric distances between A and PA'PT and combine them with weights a and a\u2295. Notably, ReLU (A \u2013 PA'PT) [u, v] is non-zero if the edge (u,v) \u2208 E is mapped to a non-edge (u', v') \u2208 E' with P[u, u'] = P[v, v'] = 1, indicating deletion of the edge (u, v) from G. Similarly, ReLU (PA'PT - A) [u, v] becomes non-zero if an edge (u, v) is added to G. Therefore, for the edit operations involving edges, we have:\nI [(u, v) \u2208 \u0395\u039b (\u03c0(u), \u03c0(v)) \u2209 E'] = ReLU (A \u2013 PA'P\u00b9) [u, v], (3)\nI [(u, v) \u2209 \u0395 \u2227 (\u03c0(u), \u03c0(v)) \u2208 E'] = ReLU (PA'PT \u2013 A) [u, v]. (4)\nSimilarly, we note that ReLU (ng[u] \u2013 \u03b7\u03c3\u03b9 [\u03c0(u)]) > 0 if u \u2209 PaddedNodesg and \u03c0(u) \u0395 PaddedNodesg, which allows us to compute the cost of deleting the node u from G. Similarly, we"}, {"title": "GRAPHEDX model", "content": "Minimizing the objective in Eq. (7) is a challenging problem. In similar problems, recent methods have approximated the hard node permutation matrix P with a soft permutation matrix obtained using Sinkhorn iterations on a neural cost matrix. However, the binary nature of the adjacency matrix and the pad indicator q still impede the flow of gradients during training. To tackle this problem, we make relaxations in two key places within each term in Eq. (7), leading to our proposed GRAPHEDX model."}, {"title": "Network architecture of EMBED and PERMNET", "content": "In this section, we present the network architectures of the two components of GRAPHEDX, viz., EMBED and PERMNET, as introduced in items (1) and (2) in Section 3.2. Notably, in our proposed graph representation, non-edges and edges alike are embedded as non-zero vectors. In other words, all node-pairs are endowed with non-trivial embeddings. We then explain the design approach for edge-consistent node alignment.\nNeural architecture of EMBED EMBED consists of a message passing neural network MPNN0 and a decoupled neural module MLP\u0189. Given the graphs G,G', MPNNe with K propagation layers is used to iteratively compute the node embeddings {xk(u) \u2208 Rd | u\u2208 V} and {x'k(u) \u2208 Rd | u \u2208 V'}, then collect them into X and X' after padding, i.e.,\nX := {\u0445\u043a (u) | u \u2208 [N]} = MPNN\u0189(G), X' := {x'\u03ba (u') | u' \u2208 [N]} = MPNNo(G'). (16)\nThe optimal alignment S is highly sensitive to the global structure of the graph pairs, i.e., S[e, e'] can significantly change when we perturb G or G' in regimes distant from e or e'. Conventional representations mitigate this sensitivity while training models, by setting non-edges to zero, rendering them invariant to structural changes. To address this limitation, we utilize more expressive graph representations, where non-edges are also embedded using trainable non-zero vectors. This approach allows information to be captured from the structure around the nodes through both edges and non-edges, thereby enhancing the representational capacity of the embedding network. For each node-pair e = (u, v) \u2208 G (and equivalently (v, u)), and e' = (u', v') \u2208 G', the embeddings of the corresponding nodes and their connectivity status_are concatenated, and then passed through an MLP to obtain the embedding vectors r(e), r'(e') \u2208 RD. For e = (u, v) \u2208 G, we computer(e) as follows:\nr(e) = MLP\u0189(X\u03ba(\u03b9) ||x\u03ba (v) || A[u, v]) + MLP0(X\u03ba (v) ||\u03c7\u03ba(\u03b9) || \u0391[v, u]). (17)\nWe can compute r'(e) in similar manner. The property r((u, v)) = r((v, u)) reflects the undirected property of graph. Finally, the vectors r(e) and r'(e') are stacked into matrices R and R', both with dimensions R(\\binom{N}{2})\u00d7D. We would like to highlight that r((u, v)) or r'((u', v')) are computed only once for all node-pairs, after the MPNN completes its final Kth layer of execution. The message passing in the MPNN occurs only over edges. Therefore, this approach does not significantly increase the time complexity.\nNeural architecture of PERMNET The network PERMNET provides Pas a soft node alignment matrix by taking the node embeddings as input, i.e., P = PERMNET(X, X'). PERMNET is implemented in two steps. In the first step, we apply a neural network cf on both K and 'K, and then compute the normed difference between their outputs to construct the matrix C, where C[u, u'] = ||C\u00a2 (\u0425\u0445\u043a (u)) \u2013 c\u00a2 (x'k(u'))||1. Next, we apply iterative Sinkhorn normalizations [16, 35] on exp(-C/T), to obtain a soft node alignment P. Therefore,\nP = Sinkhorn ([exp (- ||C\u2084 (XK (u)) \u2013 C\u00f3 (x'k (u')) ||\u2081/T)] (u,u')\u2208[N]\u00d7[N]) . (18)\nHere, T is a temperature hyperparameter. In a general cost setting, GED is typically asymmetric, so it may be desirable for C[u, u'] to be asymmetric with respect to x and x'. However, as noted in Proposition 1, when we compute GED(G', G), the alignment matrix P' = PERMNET(X', X) should satisfy the condition that P' = PT, where P is computed from Eq. (18). The current form of C supports this condition, whereas an asymmetric form might not, as shown in Appendix D.\nWe construct S \u2208 R(\\binom{N}{2}) \u00d7 R(\\binom{N}{2}) as follows. Each pair of nodes (u, v) in G and (u', v') in G' can be mapped in two ways, regardless of whether they are edges or non-edges: (1) node u \u2192 u' and v \u2013 v' which is denoted by P[u, u']P[v, v']; (2) node u \u2192 v' and v \u2192 u', which is denoted by P[u, v']P[v, u'] Combining these two scenarios, we compute the node-pair alignment matrix S as: S[(u, v), (u', v')] = P[u, u']P[v, v'] + P[u, v']P[v, u']. This explicit formulation of S from P ensures mutually consistent permutation across nodes and node-pairs."}, {"title": "Experiments", "content": "We conduct extensive experiments on GRAPHEDX to showcase the effectiveness of our method across several real-world datasets, under both equal and unequal cost settings. Additiional experimental results can be found in Appendix F."}, {"title": "Setup", "content": "Datasets We experiment with seven real-world datasets: Mutagenicity (Mutag) [18], Ogbg-Code2 (Code2) [23], Ogbg-Molhiv (Molhiv) [23], Ogbg-Molpcba (Molpcba) [23], AIDS [36], Linux [5] and Yeast [36]. For each dataset's training, test and validation sets Dsplit, we generate (Dsplit) + |Dsplit | graph pairs, considering combinations between every two graphs, including self-pairing. We calculate the exact ground truth GED using the F2 solver [29], implemented within GEDLIB [10]. For GED with equal cost setting, we set the cost values to b\u00a9 = b\u00ae = a\u00a9 = a\u00ae = 1. For GED with unequal cost setting, we use b = 3, b\u00ae = 1,a\u00ae = 2, a = 1. Further details on dataset generation and statistics are presented in Appendix E. In the main paper, we present results for the first five datasets under both equal and unequal cost settings for GED. Additional experiments for Linux and Yeast, as well as GED with node label substitutions, are presented in Appendix F.\nBaselines We compare our approach with nine state-of-the-art methods. These include two variants of GMN [31]: (1) GMN-Match and (2) GMN-Embed; (3) ISONET [43], (4) GREED [40], (5)"}, {"title": "Results", "content": "Comparison with baselines We start by comparing the performance of GRAPHEDX against all state-of-the-art baselines for GED with both equal and unequal costs. We make the following observations. (1) GRAPHEDX outperforms all the baselines by a significant margin. For GED with equal costs, this margin often goes as high as 15%. This advantage becomes even more pronounced for GED with unequal costs, where our method outperforms the baselines by a margin as high as 30%, as seen in Code2. (2) There is no clear second-best method. Among the baselines, EGSC and ERIC each outperforms the others in two out of five datasets for both equal and unequal cost settings. Also, EGSC demonstrates competitive performance in AIDS.\nImpact of cost-guided GED Among the baselines, GMN-Match, GMN-Embed and GREED compute GED using the euclidean distance between the graph embeddings, i.e., GED(G, G') = ||XG - XG ||2, whereas we compute it by summing the set distance surrogates between the node and edge embedding sets. To understand the impact of our cost guided distance, we adapt it to the graph-level embeddings used by the above three baselines as follows: GED(G, G') = b+a ||ReLU (XG - XG')||1 + b\u00ae+a\u00ae ||ReLU (xG' - XG)||1."}, {"title": "Conclusion", "content": "Our work introduces a novel neural model for computing GED that explicitly incorporates general costs of edit operations. By leveraging graph representations that recognize both edges and non-edges, together with the design of suitable set distance surrogates, we achieve a more robust neural surrogate for GED. Our experiments demonstrate that this approach outperforms state-of-the-art methods,"}, {"title": "Limitations", "content": "Our neural model for GED affords significant improvements in accuracy and flexibility for modeling edit costs. However, there are some limitations to consider.\n(1) While computing graph representations over \\binom{N}{2} \u00d7 \\binom{N}{2} node-pairs does not require additional parameters due to parameter-sharing, it does demand significant memory resources. This could pose challenges, especially with larger-sized graphs.\n(2) The assumption of fixed edit costs across all graph pairs within a dataset might not reflect real-world scenarios where costs vary based on domain-specific factors and subjective human relevance judgements. This calls for more specialized approaches to accurately model the impact of each edit operation, which may differ across node pairs.\n(3) the current model may not adequately address richly attributed graphs with complex node and edge features. Incorporating such attributes alongside graph structure based GED computation may require further exploration."}, {"title": "Broader impact", "content": "Graphs serve as powerful representations across diverse domains, capturing complex relationships and structural notions inherent in various systems. From biological networks to social networks, transportation networks, and supply chains, graphs provide a versatile framework for modeling interactions between interconnected entities. In domains where structure-similarity based applications are prevalent, GED emerges as a valuable and versatile tool.\nFor example, in bio-informatics, molecular structures can naturally be represented as graphs. GED computation expedites tasks such as drug discovery, protein-protein interaction modeling, and molecular similarity analysis by identifying structurally similar molecular compounds. Similarly, in social network analysis, GED can measure similarities between user interactions, aiding in friend recommendation systems or community detection tasks. In transportation networks, GED-based tools assess similarity between road networks for route planning or traffic optimizations. Further applications include learning to edit scene graphs, analyzing gene regulatory pathways, fraud detection, and more\nMoreover, our proposed variations of GED, particularly those amenable to hashing, find utility in retrieval based setups. In various information retrieval systems, hashed graph representations can be used to efficiently index and retrieve relevant items using our GED based scores. Such applications include image retrieval from image databases where images are represented as scene graphs, retrieval of relevant molecules from molecular databases, etc.\nFurthermore, our ability to effectively model different edit costs in GED opens up new possibilities in various applications. In recommendation systems, it can model user preferences of varying importance, tailoring recommendations based on user-specific requirements or constraints. Similarly, in image or video processing, different types of distortions may have varying impacts on perceptual quality, and GED with adaptive costs can better assess similarity. In NLP tasks such as text similarity understanding and document clustering, assigning variable costs to textual edits corresponding to word insertion, deletions or substitutions, provides a more powerful framework for measuring textual similarity, improving performance in downstream tasks such as plagiarism detection, summarization, etc.\nLastly, and most importantly, the design of our model encourages interpretable alignment-driven justifications, thereby promoting transparency and reliability while minimizing potential risks and negative impacts, in high stake applications like drug discovery."}, {"title": "Discussion on related work", "content": "Heuristics for Graph Edit Distance GED was first introduced in [45]. Bunke and Allermann [14] used it as a tool for non exact graph matching. Later on, [13] connected GED with maximum common subgraph estimation. Blumenthal [7] provide an excellent survey. As they suggest, combinatorial heuristics to solve GED predominantly follows three approaches: (1) Linear sum assignment problem with error-correction, which include [27, 41, 52, 54] (2) Linear programming, which predominantly uses standard tools like Gurobi, (3) Local search [42]. However, they can be extremely time consuming, especially for a large number of graph pairs. Among them Zheng et al. [54] operate in our problem setting, where the cost of edits are different across the edit operations, but for the same edit operation, the cost is same across node or node pairs.\nOptimal transport In our work, we utilize Graph Neural Networks (GNNs) to represent each graph as a set of node embeddings. This transforms the inherent Quadratic Assignment Problem (QAP) of graph matching into a Linear Sum Assignment Problem (LSAP) on the sets of node embeddings. Essentially, this requires solving an optimal transport problem in the node embedding space. The use of neural surrogates for optimal transport was first proposed by Cuturi [16], who introduced entropy regularization to make the optimal transport objective strictly convex and utilized Sinkhorn iterations [49] to obtain the transport plan. Subsequently, Mena et al. [35] proposed the neural Gumbel Sinkhorn network as a continuous and differentiable surrogate of a permutation matrix, which we incorporate into our model.\nIn various generative modeling applications, optimal transport costs are used as loss functions, such as in Wasserstein GANs [1, 3]. Computing the optimal transport plan is a significant challenge, with approaches leveraging the primal formulation [51, 33], the dual formulation with entropy regularization [17, 47, 22], or Input Convex Neural Networks (ICNNs) [2].\nNeural graph similarity computation Most earlier works on neural graph similarity computation have focused on training with GED values as ground truth [5, 6, 19, 40, 55, 39, 53, 31], while some have used MCS as the similarity measure [6, 5]. Current neural models for GED approximation primarily follow two approaches. The first approach uses a trainable nonlinear function applied to graph embeddings to compute GED [5, 39, 6, 55, 53, 19]. The second approach calculates GED based on the Euclidean distance in the embedding space [31, 40].\nAmong these models, GOTSIM [19] focuses solely on node insertion and deletion, and computes node alignment using a combinatorial routine that is decoupled from end-to-end training. However, their network struggles with training efficiency due to the operations on discrete values, which are not amenable to backpropagation. With the exception of GREED [40] and Graph Embedding Network (GEN) [31], most methods use early interaction or nonlinear scoring functions, limiting their adaptability to efficient indexing and retrieval pipelines"}, {"title": "Discussion on our proposed formulation of GED", "content": "Modification of scoring function from label substitution\nTo incorporate the effect of node substitution into account when formulating the GED, we first observe that the effect of node substitution cost b~ only comes into account when a non-padded node maps to a non-padded node. In all other cases, when a node is deleted or inserted, we do not additionally incur any substitution costs. Note that, we consider the case when node substitution cannot be replaced by node addition and deletion, i.e., b~ < b + b\u2295. Such a constraint on costs has uses in multiple applications [9, 38]. Let L denote the set of node labels, and l(u), l'(u') \u2208 L denote the node label corresponding to nodes u and u' in G and G' respectively. We construct the node label matrix L for G as follows: L \u2208 {0,1}^{N\u00d7|L|}, such that L[i, :] = one_hot(l(i)), i.e., L is the one-hot indicator matrix for the node labels, which each row corresponding to the one-hot vector of the label. Similarly, we can construct L' for G'. Then, the distance between labels of two nodes u \u2208 V and u' \u2208 V' can be given as ||L[u, :] \u2013 L'[u', :]||1. To ensure that only valid node to node mappings contribute to the cost, we multiply the above with A(u, u') = AND(\u03b7\u03c2[u], \u03b7\u03c2, [u']). This allows us to write the expression for GED with node label substitution cost as\nGED(G, G') = \\underset{P\\in \\Pi_N}{min} \\frac{\\alpha^{\\ominus}}{2}||ReLU (A-PA'P^T)||_{1,1} + \\frac{\\alpha^{\\oplus}}{2} ||ReLU (PA'P^T - A) ||_{1,1} \\\\+ b^\\ominus ||ReLU (Pq_{G'} - q_G)||_{1} + b^{\\oplus} ||ReLU (q_G - Pq_{G'})||_{1} \\\\ + b~\\sum_u\\sum_{u'} A(u, u') || L[u, :] \u2013 L'[u', :]||\u2081 P[u, u']\nA~(L,L'|P)\nWe can design a neural surrogate for above in the same way as done in Section 3.2, and write\nGED_{0,\\phi}(G, G') = \\alpha^{\\ominus}\\Delta^{\\ominus}(R, R' | S) + \\alpha^{\\oplus}\\Delta^{\\oplus} (R, R' | S) \\\\+b^\\ominus\\Delta^{\\ominus} (X, X' | P) + b^{\\oplus}\\Delta^{\\oplus}(X, X' | P) \\\\+b~\\Delta^{\\sim}(L, L'|P) (19)\nIn this case, to account for node substitutions in the proposed permutation, we use L[u, :] and L'[u', :] as the features for node u in G and node u' in G', respectively. We present the comparison of our method including subsitution cost with state-of-the-art baselines in Appendix F."}, {"title": "Proof of Proposition 1", "content": "Proposition Given a fixed set of values of be, b\u2295, a, a\u2295, let P be an optimal node permutation matrix corresponding to GED(G, G'), computed using Eq. (7). Then, P' = PT is an optimal node permutation corresponding to GED(G', G).\nProof: Noticing that ReLU (c \u2013 d) = max(c, d) \u2013 d, we can write\n||ReLU (A \u2013 PA'P\u00af) ||1,1 = ||max(A, PA'P\u00af) \u2013 PA'P\u00af ||1,1\n= ||max(A, PA'PT) ||1,1 \u2013 2|E'|\nThe last equality follows since max(A, PA'PT) > PA'P\u2122 element-wise, and ||PA'PT ||1,1 = ||A'||1,1 = 2|E'|. Similarly, we can rewrite ||ReLU (PA'P\u2122 \u2013 A) ||1,1, ||ReLU (\u03b7\u03c2 \u2013 \u03a1\u03b7\u03c3')||1, and ||ReLU (\u03a1\u03b7\u03c3' \u2013 qG)||1, and finally rewrite Eq. (7) as\nGED(G, G') = \\underset{P \\in \\Pi_N}{min} \\frac{\\alpha^{\\ominus} + \\alpha^{\\oplus}}{2}||max(A, PA'P^T)||_{1,1} \u2212 \\alpha^{\\oplus} | E'| - \\alpha^{\\ominus} |E| + \\\\+ \\frac{b^{\\ominus} + b^{\\oplus}}{2} ||max(\\eta_G, P\\eta_{G'})||_{1} \u2013 b^{\\ominus}|V'| \u2013 b^{\\ominus}|V| (20)\nGED(G', G) = \\underset{P \\in \\Pi_N}{min} \\frac{\\alpha^{\\ominus} + \\alpha^{\\oplus}}{2}||max(A', P^T AP)||_{1,1} - \\alpha |E| - \\alpha^{\\oplus} | E'| + \\\\+ \\frac{b^{\\ominus} + b^{\\oplus}}{2} ||max(\\eta_{G'}, P^T\\eta_{G} )||_{1} \u2013 b^{\\ominus}|V| \u2013 b^{\\ominus}|V'| (21)"}, {"title": "Connections with other notions of graph matching", "content": "Graph isomorphism: When we set all costs to zero", "isomorphism": "Assume be = 0. Then", "v": "which can be reduced to zero for some permutation P", "subgraph": "From Appendix D.2, we can write that GED(G, G') = minp 0.5(a\u2295 + a\u2295) ||max(A, PA'PT)||1,1+(b\u2295 +b\u2295) ||NG, P\u03b7\u03c3' ||\u2081-a\u2295|E'|-a\u2295|E|-b\u2295|V'| \u2013 b\u2295|V"}]}