{"title": "Divergence-Augmented Policy Optimization", "authors": ["Qing Wang", "Yingru Li", "Jiechao Xiong", "Tong Zhang"], "abstract": "In deep reinforcement learning, policy optimization methods need to deal with\nissues such as function approximation and the reuse of off-policy data. Standard\npolicy gradient methods do not handle off-policy data well, leading to premature\nconvergence and instability. This paper introduces a method to stabilize policy\noptimization when off-policy data are reused. The idea is to include a Bregman\ndivergence between the behavior policy that generates the data and the current\npolicy to ensure small and safe policy updates with off-policy data. The Bregman\ndivergence is calculated between the state distributions of two policies, instead of\nonly on the action probabilities, leading to a divergence augmentation formulation.\nEmpirical experiments on Atari games show that in the data-scarce scenario where\nthe reuse of off-policy data becomes necessary, our method can achieve better\nperformance than other state-of-the-art deep reinforcement learning algorithms.", "sections": [{"title": "Introduction", "content": "In recent years, many algorithms based on policy optimization have been proposed for deep reinforce-\nment learning (DRL), leading to great successes in Go, video games, and robotics [Silver et al., 2016,\nMnih et al., 2016, Schulman et al., 2015, 2017b]. Real-world applications of policy-based methods\ncommonly involve function approximation and data reuse. Typically, the reused data are generated\nwith an earlier version of the policy, leading to off-policy learning. It is known that these issues may\ncause premature convergence and instability for policy gradient methods [Sutton et al., 2000, Sutton\nand Barto, 2017].\nA standard technique that allows policy optimization methods to handle off-policy data is to use\nimportance sampling to correct trajectories from the behavior policy that generates the data to the\ntarget policy (e.g. Retrace [Munos et al., 2016] and V-trace [Espeholt et al., 2018]). The efficiency of\nthese methods depends on the divergence between the behavior policy and the target policy. Moreover,\nto improve stability of training, one may introduce a regularization term (e.g. Shannon-Gibbs entropy\nin [Mnih et al., 2016]), or use a proximal objective of the original policy gradient loss (e.g. clipping\nin [Schulman et al., 2017b, Wang et al., 2016a]). Although the well-adopted method of entropy\nregularization can stabilize the optimization process [Mnih et al., 2016], this additional entropy\nregularization alters the learning objective, and prevent the algorithm from converging to the optimal\naction for each state. Even for the simple case of bandit problems, the monotonic diminishing\nregularization may fail to converge to the best arm [Cesa-Bianchi et al., 2017].\nIn this work, we propose a method for policy optimization by adding a Bregman divergence term,\nwhich leads to more stable and sample efficient off-policy learning. The Bregman divergence"}, {"title": "Preliminaries", "content": "In this section, we state the basic definition of the Markov decision process considered in this work,\nas well as the Bregman divergence used in the following discussions."}, {"title": "Markov Decision Process", "content": "We consider a Markov decision process (MDP) with infinite-horizon and discounted reward, denoted\nby $M = (S, A, P, r, d_0, \\gamma)$, where $S$ is the finite state space, $A$ is the finite action space, $P : S\\times A \\rightarrow$\n$\\Delta(S)$ is the transition function, where $\\Delta(S)$ means the space of all probability distributions on $S$.\nA reward function is denoted by $r : S \\times A \\rightarrow \\mathbb{R}$. The distribution of initial state $s_0$ is denoted by\n$d_0 \\in \\Delta(S)$. And a discount factor is denoted by $\\gamma \\in (0, 1)$.\nA stochastic policy is denoted by $\\pi : S \\rightarrow \\Delta(A)$. The space of all policies is denoted by $\\Pi$. We use the\nfollowing standard notation of state-value $V^{\\pi}(s_t)$, action-value $Q^{\\pi}(s_t, a_t)$ and advantage $A^{\\pi}(s_t, a_t)$,\ndefined as $V^{\\pi}(s_t) = \\mathbb{E}_{\\pi|s_t} \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l+1}, a_{t+l+1})$, $Q^{\\pi}(s_t, a_t) = \\mathbb{E}_{\\pi|s_t, a_t} \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l+1}, a_{t+l+1})$, and\n$A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t)$, where $\\mathbb{E}_{\\pi|s_t}$ means $a_l \\sim \\pi(a|s_l)$, $s_{l+1} \\sim P(s_{l+1}|s_l, a_l), \\forall l \\geq t$,\nand $\\mathbb{E}_{|s_t, a_t}$ means $s_{l+1} \\sim P(s_{l+1}|s_l, a_l)$, $a_{l+1} \\sim \\pi(a|s_{l+1}), \\forall l > t$. We also define the space of\npolicy-induced state-action distributions under $M$ as\n$\\Delta_{\\pi} = {\\mu \\in \\Delta(S \\times A) : \\sum_{a'} \\mu(s', a') = (1 - \\gamma)d_0(s') + \\sum_{s,a} P(s'|s, a)\\mu(s, a), \\forall s' \\in S}$\nWe use the notation $\\mu_{\\pi}$ for the state-action distribution induced by $\\pi$. On the other hand, for each\n$\\mu \\in \\Delta_{\\pi}$, there also exists a unique policy $\\pi_{\\mu}(a|s) = \\frac{\\mu(s, a)}{\\sum_a \\mu(s,a)}$ which induces $\\mu$. We define the state\ndistribution $d_{\\pi}$ as $d_{\\pi}(s) = (1 - \\gamma) \\mathbb{E}_{\\tau \\sim \\pi} \\sum_{t=0}^{\\infty} \\gamma^t 1(s_t = s)$. Then we have $\\mu_{\\pi}(s, a) = d_{\\pi}(s) \\pi(a|s)$.\nWe sometimes write $\\pi_{\\mu_1}$ as $\\pi_1$ and $d_{\\mu_1}$ as $d_1$ when there is no ambiguity.\nIn this paper, we mainly focus on the performance of a policy $\\pi$ defined as\n$J(\\pi) = (1 - \\gamma) \\mathbb{E}_{\\tau \\sim \\pi} \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) = \\mathbb{E}_{d, \\pi}r(s, a)$   (2)\nwhere $\\mathbb{E}_{\\tau \\sim \\pi}$ means $s_0 \\sim d_0$, $a_t \\sim \\pi(a_t|s_t)$, $s_{t+1} \\sim P(s_{t+1}|s_t, a_t)$, $t > 0$. We use the notation\n$\\mathbb{E}_{d, \\pi} = \\mathbb{E}_{s \\sim d(\\cdot), a \\sim \\pi(\\cdot|s)}$ for brevity."}, {"title": "Bregman Divergence", "content": "We define Bregman divergence [Bregman, 1967] as follows (e.g. Definition 5.3 in [Bubeck and\nCesa-Bianchi, 2012]). For $D \\subset \\mathbb{R}^d$ an open convex set, the closure of $D$ as $\\overline{D}$, we consider a\nLegendre function $F : D \\rightarrow \\mathbb{R}$ defined as (1) $F$ is strictly convex and admits continuous first\npartial derivatives on $D$, and (2) $\\lim_{x \\rightarrow \\overline{D}\\backslash D} ||\\nabla F|| = +\\infty$. For function $F$, we define the Bregman\ndivergence $D_F : D \\times D \\rightarrow \\mathbb{R}$ as\n$D_F(x, y) = F(x) - F(y) - \\langle \\nabla F(y), x - y \\rangle$.\nThe inner product is defined as $\\langle x, y \\rangle = \\sum_i x_i y_i$. For $K \\subset D$ and $K \\cap D \\neq 0$, the Bregman projection\n$z = \\arg \\min_{x \\in K} D_F(x, y)$\nexists uniquely for all $y \\in D$. Specifically, for $F(x) = \\sum_i x_i \\log(x_i) - \\sum_i x_i$, we recover the\nKullback-Leibler (KL) divergence as\n$D_{KL}(\\mu', \\mu) = \\sum_{s,a} \\mu'(s, a) \\log \\frac{\\mu'(s, a)}{\\mu(s, a)}$\nfor $\\mu, \\mu' \\in \\Delta(S \\times A)$ and $\\pi, \\pi' \\in \\Pi$. To measure the distance between two policies $\\pi$ and $\\pi'$, we also\nuse the symbol for conditional \u201cBregman divergence\u201d associated with state distribution $d$ denoted as\n$D_d(\\pi', \\pi) = \\sum_s d(s) D_F(\\pi'(\\cdot|s), \\pi(\\cdot|s))$.\n (3)"}, {"title": "Method", "content": "In this section, we present the proposed method from the motivation of mirror descent and then\ndiscuss the parametrization and off-policy correction we employed in the practical learning algorithm."}, {"title": "Policy Optimization and Mirror Descent", "content": "The mirror descent (MD) method [Nemirovsky and Yudin, 1983] is a central topic in the optimization\nand online learning research literature. As a first-order method for optimization, the mirror descent\nmethod can recover several interesting algorithms discovered previously [Sutton et al., 2000, Kakade,\n2002, Peters et al., 2010, Schulman et al., 2015]. On the other hand, as an online learning method,\nthe online (stochastic) mirror descent method can achieve (near-)optimal sample efficiency for a wide\nrange of problems [Audibert and Bubeck, 2009, Audibert et al., 2011, Zimin and Neu, 2013]. In this\nwork, following a series of previous works [Zimin and Neu, 2013, Neu et al., 2017], we investigate\nthe (online) mirror descent method for policy optimization. We denote the state-action distribution at\niteration $t$ as $\\mu_t$, and $l_t(\\mu) = \\langle g_t, \\mu \\rangle$ as the linear loss function for $\\mu$ at iteration $t$. Without otherwise\nnoted, we consider the negative reward as the loss objective $l_t(\\mu) = -\\langle r, \\mu \\rangle$, which also corresponds\nto the policy performance $l_t(\\mu) = -J(\\pi_{\\mu})$ by Formula (2). We consider the mirror map method\nassociated with Legendre function $F$ as\n$\\nabla F(\\mu_{t+1}) = \\nabla F(\\mu_t) - \\eta g_t$\n$\\mu_{t+1} = \\Pi_{\\Delta_{\\Pi}}(\\mu_{t+1})$ (5)\nwhere $\\mu_{t+1} \\in \\Delta(S \\times A)$ and $g_t = \\nabla l_t(\\mu_t)$. It is well-known [Beck and Teboulle, 2003] that an\nequivalent formulation of mirror map (4) is\n$\\mu_{t+1} = \\arg \\min_{\\mu \\in \\Delta_{\\Pi}} D_F(\\mu, \\mu_{t+1})$ (6)\n$= \\arg \\min_{\\mu \\in \\Delta_{\\Pi}} D_F(\\mu, \\mu_t) + \\eta \\langle g_t, \\mu \\rangle$,\n (7)\nThe former formulation (6) takes the view of non-linear sub-gradient projection in convex optimiza-\ntion, while the later formulation (7) can be interpreted as a regularized optimization and is the usual\ndefinition of mirror descent [Nemirovsky and Yudin, 1983, Beck and Teboulle, 2003, Bubeck, 2015].\nIn this work, we will mostly investigate the approximate algorithm in the later formulation (7)."}, {"title": "Parametric Policy-based Algorithm", "content": "In the mirror descent view for policy optimization on state-action space as in Formula (7), we need to\ncompute the projection of $\\mu$ onto the space of $\\Delta_{\\Pi}$. For the special case of KL-divergence on $\\mu$, the\nsub-problem of finding minimum in (7) can be done efficiently, assuming the knowledge of transition\nfunction $P$ (See Proposition 1 in [Zimin and Neu, 2013]). However, for a general divergence and real-world problems with unknown transition matrices, the projection in (7) is non-trivial to implement.\nIn this section, we consider direct optimization in the (parametric) policy space without explicit\nprojection. Specifically, we consider $\\mu_{\\pi}$ as a function of $\\pi$, and $\\pi$ parametrized as $\\pi_\\theta$. The Formula\n(7) can be written as\n$\\theta_{t+1} = \\arg \\min_{\\pi} D_F(\\mu_{\\pi}, \\mu_t) + \\eta \\langle g_t, \\mu_{\\pi} \\rangle$.   (8)\nInstead of solving globally, we approximate Formula (8) with gradient descent on $\\pi$. From the\ncelebrated policy gradient theorem [Sutton et al., 2000], we have the following lemma:\nLemma 1. (Policy Gradient Theorem [Sutton et al., 2000]) For $d_{\\mu}$ and $\\mu_{\\pi}$ defined previously, the\nfollowing equation holds for any state-action function $f : S \\times A \\rightarrow \\mathbb{R}$:\n$\\sum_{s,a} f(s, a) \\nabla_\\theta \\mu_{\\pi}(s, a) = \\sum_{s} d(s) Q^{\\pi}(f)(s, a) \\nabla_\\theta \\pi(a|s)$,\nwhere $Q^{\\pi}$ is defined as an operator such that\n$Q^{\\pi}(f)(s, a) = \\mathbb{E}_{\\pi|s_t=s, a_t=a} \\sum_{l=0}^{\\infty} \\gamma^l f(s_{t+l+1}, a_{t+l+1})$.\nDecomposing the loss and divergence in two parts (8), we have\n$\\nabla_\\theta \\langle g_t, \\mu_{\\pi} \\rangle = \\langle d_\\pi Q^{\\pi}(g_t), \\nabla_\\theta \\pi(a|s) \\rangle$,  (9)\nwhich is the usual policy gradient, and\n$\\nabla_\\theta D_F(\\mu_{\\pi}, \\mu_t) = \\langle \\nabla F(\\mu_{\\pi}) - \\nabla F(\\mu_t), \\nabla_\\theta \\mu_{\\pi} \\rangle = \\langle d_{\\pi} Q^{\\pi} (\\nabla F(\\mu_{\\pi}) - \\nabla F(\\mu_t)), \\nabla_\\theta \\pi(a|s) \\rangle$.   (10)\nSimilarly, we have the policy gradient for the conditional divergence (3) as\n$\\nabla_\\theta D_d(\\pi, \\pi_t) = \\langle d_t(\\nabla F(\\pi) - \\nabla F(\\pi_t)), \\nabla_\\theta \\pi(a|s) \\rangle$,\nwhich does not have a discounted sum, since $d_t$ is fixed and independent of $\\pi \\equiv \\pi_{\\mu}$."}, {"title": "Off-policy Correction", "content": "In this section, we discuss the practical method for estimating $Q^{\\pi}(f)$ under a behavior policy $\\pi_t$. In\ndistributed reinforcement learning with asynchronous gradient update, the policy $\\pi_t$ which generated\nthe trajectories may deviate from the policy $\\pi_\\theta$ currently being optimized. Thus off-policy correction\nis usually needed for the robustness of the algorithm (e.g. V-trace as in IMPALA [Espeholt et al.,\n2018]). Consider\n$\\sum_{s,a} d_{\\pi}(s) Q^{\\pi}(f)(s, a) \\nabla_\\theta \\pi(a|s) = \\mathbb{E}_{(s,a) \\sim d_{\\pi} \\pi_{\\theta}} Q^{\\pi}(f)(s, a) \\nabla_\\theta \\log \\pi(a|s)$\n$= \\mathbb{E}_{(s,a) \\sim d_{\\pi_t} \\pi_t} \\frac{d_{\\pi}(s)}{d_{\\pi_t}(s)} \\frac{\\pi(a|s)}{\\pi_t(a|s)} Q^{\\pi}(f)(s, a) \\nabla_\\theta \\log \\pi(a|s)$\nfor $f = g_t$ or $f = \\nabla F(\\mu_{\\pi}) - \\nabla F(\\mu_t)$. We would like to have an accurate estimation of $Q^{\\pi}(g_t)$ (9)\nand $Q^{\\pi} (\\nabla F(\\mu_{\\pi}) - \\nabla F(\\mu_t))$ (10), and correct the deviation from $d_{\\pi_t}$ to $d_{\\pi}$ and $\\pi_t$ to $\\pi$.\nFor the estimation of $Q^{\\pi}(f)$ under a behavior policy $\\pi_t$, possible methods include Retrace [Munos\net al., 2016] providing an estimator of state-action value $Q^{\\pi}(f)$, and V-trace [Espeholt et al., 2018]\nproviding an estimator of state value $\\mathbb{E}_{a \\sim \\pi} Q^{\\pi}(f)(s, a)$. In this work, we utilize the V-trace (Section\n4.1 [Espeholt et al., 2018]) estimation $v_{s_i} = v_i$ along a trajectory starting at $(s_i, a_i = s, a)$ under $\\pi_t$."}]}