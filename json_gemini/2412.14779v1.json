[{"title": "Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning", "authors": ["Aditya Kapoor", "Sushant Swamy", "Kale-ab Tessera", "Mayank Baranwal", "Mingfei Sun", "Harshad Khadilkar", "Stefano V. Albrecht"], "abstract": "In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR\u00b2), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR2 decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR\u00b2 is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR\u00b2 stabilizes and accelerates the learning process. Additionally, we show that when TAR\u00b2 is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods.", "sections": [{"title": "Introduction", "content": "In cooperative multi-agent reinforcement learning (MARL), multiple autonomous agents learn to interact and collaborate to execute tasks in a shared environment by maximizing a global return (Busoniu et al., 2008). MARL has shown considerable potential in solving decentralized partially observable Markov decision processes (Dec-POMDPs) (Oliehoek & Amato, 2016; Amato, 2024; Zhang et al., 2021), where each agent has access to only local information (partial observation) and need to select actions based on their local action-observation (or sometimes only observation) histories to maximize the global (team) return. Applications of MARL include complex video games such as StarCraft-II (Vinyals et al., 2019), Defense of the Ancients (DOTA) (Berner et al., 2019), Google Football (Kurach et al., 2020), and Capture the Flag (CTF) (Jaderberg et al., 2019), and real world applications of warehouse logistics (Krnjaic et al., 2022; Agrawal et al., 2023), e-commerce (Shelke et al., 2023; Baer et al., 2019), robotics (Sartoretti et al., 2019; Damani et al., 2021), and routing problems (Zhang et al., 2018; Vinitsky et al., 2020; Zhang et al., 2023b). These applications illustrate the potential of MARL to develop sophisticated strategies and behaviors through coordinated teamwork and collaboration.\nDespite these successes, cooperative multi-agent systems face a significant challenge in credit assignment, which is crucial for learning effective policies Foerster et al. (2018). Credit assignment"}, {"title": "Related Works", "content": "In this section, we review various techniques proposed to address temporal and agent credit assignment in both single-agent and multi-agent systems. We begin by discussing potential-based reward shaping, a method that provides theoretical guarantees of sample-efficient learning of optimal policies in single-agent (Ng, 1999) and multi-agent (Xiaosong Lu, 2011; Devlin & Kudenko, 2011) settings. The use of potential based shaping methods have shown to accelerate the learning process."}, {"title": "Temporal Credit Assignment", "content": "Temporal credit assignment focuses on decomposing sparse or episodic environment rewards into dense reward functions by attributing credit to each time step in an episode.\nRUDDER (Arjona-Medina et al., 2019) and its variants (Patil et al., 2020) use contribution analysis to break down episodic rewards into per-time-step rewards by computing the difference between predicted returns at successive time steps. Similarly, Zhang et al. (2023a) perform return-equivalent contribution analysis. Liu et al. (2019) leverage auto-regressive architectures from natural language processing, such as Transformers (Vaswani et al., 2017), to attribute credit to every state-action tuple in the trajectory. Methods by Efroni et al. (2021) and Ren et al. (2021) learn proxy reward functions via trajectory smoothing based on reinforcement learning algorithms that utilize least-squares error. Harutyunyan et al. (2019) introduce a new family of algorithms that use new information to assign credit in hindsight. Han et al. (2022) redesign the value function to predict returns for both historical and current steps by approximating these decompositions. Zhu et al. (2023) propose a bi-level optimization framework to learn a reward redistribution for effective policy learning. These methods have been primarily developed for single-agent settings and may not scale well to multi-agent reinforcement learning (MARL) due to the exponential growth in the joint observation-action space.\nIn multi-agent settings, recent works have also addressed temporal credit assignment. IRCR (Gangwani et al., 2020) developed a count-based method to learn a proxy reward function for both single and multi-agent settings. AREL (Xiao et al., 2022) uses attention networks to perform reward redistribution, while She et al. (2022) employ an attention encoder network followed by a decoder to address both agent and temporal credit assignment in delayed reward settings."}, {"title": "Agent Credit Assignment", "content": "Most prior works focus on agent credit assignment in multi-agent systems. Devlin et al. (2014) and Foerster et al. (2018) employ difference rewards to assess each agent's contribution to the global reward. Value Decomposition Networks (VDN) (Sunehag et al., 2017) decompose the joint value function into agent-specific value functions, assuming additivity. Rashid et al. (2020) introduce monotonicity constraints on the joint Q function to learn individual Q values for each agent. Son et al. (2019) generalize this approach to decompose joint Q functions into agent-specific Q functions. Wang et al. (2020) leverage Shapley values to model the joint Q function for agent credit assignment. Zhou et al. (2020) propose an entropy-regularized actor-critic method to efficiently explore multi-agent credit assignment. Freed et al. (2021) use Transformer attention mechanisms in the critic of an actor-critic method to identify relevant agent subgroups for effective multi-agent credit assignment. However, these techniques do not address temporal credit assignment and are therefore inadequate for learning optimal policies in episodic or highly delayed reward settings.\nIn summary, while significant progress has been made in addressing either agent or temporal credit assignment, the combined challenge of both remains underexplored. Our proposed Temporal-Agent Reward Redistribution (TAR\u00b2) aims to fill this gap by effectively handling both agent and temporal credit assignment, enabling efficient learning in multi-agent environments with sparse or delayed rewards."}, {"title": "Background", "content": "In this section, we introduce our problem setup within the framework of a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek & Amato, 2016; Amato, 2024). We describe how agents operate under partial observability and must make decisions based on local observations and histories. We then discuss the episodic multi-agent reinforcement learning (MARL) setting, where agents receive rewards only at the end of each episode, posing a significant challenge for credit assignment. To address this, we explore potential-based reward shaping for multi-agent systems (Ng, 1999; Devlin & Kudenko, 2011), a technique that reshapes the reward function to facilitate learning while preserving the optimal policy. Finally, we analyze how imperfect credit assignment impacts the variance of the policy gradient in multi-agent systems. We show that improper credit distribution among agents leads to high variance in advantage estimates, which in turn exacerbates the learning process and hinders the convergence to optimal policies."}, {"title": "Decentralized Partially Observable Markov Decision Processes (Dec-POMDP)", "content": "A Dec-POMDP is represented by a tuple $M = (S,A,P,T, O, N, R\u03c2, \u03c1\u03bf, \u03b3)$ where $s \u2208 S$ is the environment state space, $a \u2208 A$ is the joint action space denoted by $A := A_1 \u00d7 A_2 \u00d7 ... \u00d7 A_N$ and $P:S \u00d7 A \u00d7 S \u2192 [0,1]$ is the state transition function. $r_{global,t} ~ R(s_t, a_t) : S \u00d7 A \u2192 R$ is the global reward shared among agents at every timestep of the trajectory. $p_0$ is the initial state distribution and $\u03b3\u2208 [0,1]$ is the discount factor. $\u03c0 = \u03a0_1\u03c0_i$ is the joint policy of the multi-agent system which comprises of independent agent policies $\u03c0_i$. Each agent $i \u2208 {1...N}$ receives an observation $o_i \u2208 O_i$ from the observation function $T(s, i) : S \u00d7 N \u2192 O$. Because the state is not directly observable, it is typically beneficial for each agent to remember a history of its observations or observations-actions. $h_t \u2208 H := H_1 . . . H_N$ is the set of agent observation (-action) histories up to the current time step t where $h_{i,t} \u2208 H_i$ and defined as $h_{i,t} = {o_{i,1},a_{i,1}, ..., o_{i,t}}$ denotes agent i's history and $h_{-i,t}$ is the history of all other agents except agent i. At each time step every agent selects an action $a_i \u2208 A_i$ according to it's policy $\u03c0_i: H_i \u00d7 A_i \u2192 [0,1]$. $T = {o_{0,1}, a_{0,1}, \u2026\u2026o_{0,N}, a_{0,N}.......o_{|\u03c4|,1}, a_{|\u03c4|,1}..o_{|\u03c4|,N}, a_{|\u03c4|,N}}$ is the multi-agent trajectory where $|\u03c4|$ is the horizon length of the trajectory. The goal of the agents is to determine their individual optimal policies that achieve maximum global return $E_{s0~\u03c1_0,s~P,ai~\u03c0i\u2026} [\u03a3_{t=1}\u03b3^tr_t]$"}, {"title": "Return Decomposition in Episodic Multi-Agent Reinforcement Learning", "content": "In most MARL systems, each agent receives a global reward $r_{global,t}$ after executing the joint action $a_t$ in state $s_t$. However, in episodic MARL setups, agents only receive a global reward signal from the environment at the end of the trajectory, known as the episodic reward or trajectory return $r_{global,episodic}(T)$. The objective in such environments is to maximize the trajectory return, $E_{s0~p_0,s~P,ai~\u03c0i..}(r_{global,episodic}(T))$. Delayed reward settings introduce significant bias and variance (Ng, 1999) during the learning process, exacerbating sample inefficiency."}, {"title": "Potential-based reward shaping", "content": "Ng (1999) presented a single-agent reward shaping method to address the credit assignment problem by introducing a potential-based shaping reward to the environment. The combination of the shaping reward with the original reward can enhance the learning performance of a reinforcement learning algorithm and accelerate the convergence to the optimal policy. Devlin & Kudenko (2011) and Xiaosong Lu (2011) extended potential-based reward shaping to multi-agent systems.\nTheorem 1. Given an n-player discounted stochastic game $M = (S, A_1,..., A_n, T, \u03b3, R_1, ..., R_n)$, we define a transformed n-player discounted stochastic game $M' = (S, A_1, ..., A_n, T, \u03b3, R_1 + F_1,..., R_n + F_n)$, where $F_i \u2208 S \u00d7 S$ is a shaping reward function for player i. We call $F_i$ a potential-based shaping function if $F_i$ has the form:"}, {"title": null, "content": "$F_i(s, s') = \u03b3\u03a6_i(s') \u2013 \u03a6_i(s)$,\nwhere $\u00de_i: S \u2192 R$ is a potential function. Then, the potential-based shaping function $F_i$ is a necessary and sufficient condition to guarantee the Nash equilibrium policy invariance such that:\n\u2022 (Sufficiency) If $F_i (i = 1,...,n)$ is a potential-based shaping function, then every Nash equilibrium policy in M' will also be a Nash equilibrium policy in M (and vice versa).\n\u2022 (Necessity) If $F_i (i = 1,...,n)$ is not a potential-based shaping function, then there may exist a transition function T and reward function R such that the Nash equilibrium policy in M' will not be the Nash equilibrium policy in M.\nIn summary, potential-based reward shaping ensures that Nash equilibrium policies are preserved, enhancing learning without altering the strategic dynamics. This principle underpins our proposed reward redistribution method, which we will validate in the following sections, demonstrating its effectiveness in multi-agent reinforcement learning."}, {"title": "Impact of Faulty Credit Assignment on Policy Gradient Variance", "content": "To understand the impact of imperfect credit assignment, we analyze the effect of other agents on the policy gradient update of agent i. Consider an actor-critic gradient estimate for a multi-agent system in a Dec-POMDP setting, computed using a state-action sample from an arbitrary timestep t. We make no assumptions about the policy parameters of the agents in the multi-agent system. Ideally, the policy gradient update for agent i should be computed using\n$\u2207_\u03b8J(\u03b8, h) = \u2207_{\u03b8_i} log \u03c0_i(a_i|h_i)E_{\u00ach_i,\u00aca_i} [A(h, a)]$\nComputing $E_{\u00ach_i,\u00aca_i} [A(h, a)]$ ischallenging to compute due to the high dimensionality, dependency on other agents, and the computational complexity involved in accurately modeling and estimating the interdependent histories and actions of multiple agents. However, in practice multi-agent policy-gradient algorithms like MAPPO (Yu et al., 2022), MADDPG (Lowe et al., 2017) etc employ $A(h, a)$ to compute the policy gradient update for each agent. As a result, the credit assignment problem manifests as high variance in advantage estimates, leading to slower learning because of noisier policy gradient estimates.\nMulti-agent policy gradient methods approximate the true advantage by computing A, which is actually a stochastic advantage estimation of taking a joint action a while observing the joint agent history h, and following the joint policy \u03c0. The advantage function is typically defined as $A^\u03c0 (s, a) = Q^\u03c0 (s,a) - V^\u03c0(s)$, where $Q^\u03c0(s,a)$ and $V^\u03c0(s)$ are the state-action value function and state-value function, respectively (Sutton & Barto, 1998). However, in practice the state-action value function and state-value function are approximated using $Q^\u03c0 (h, a)$ and $V^\u03c0 (h)$ in Dec-POMPDs and there are many ways to compute A, generally all involving some error, as the true value functions are unknown (Sutton & Barto, 1998; Schulman et al., 2015).. Intuitively, this is the centralized advantage function which measures how much better it is to select a joint action a than a random action from the joint policy \u03c0, while in state s. Besides, to update the policy of agent i, we need to compute the advantage of selecting action $a_i$, taking into account the specific contribution and context of agent i within the multi-agent system. Perfect credit assignment would be possible if the advantage function could be computed perfectly for each agent, as it directly measures how a particular action of an agent impacted the total reward obtained by the group.\nThe conditional variance of $Var (\u2207_{\u03b8_i}J|h, a)$, given h and a, is proportional to the variance of \u00c2. While this statement typically implies multiple samples are considered to estimate the variance accurately, we can gain insight into the variance introduced by the contributions of other agents by initially"}, {"title": null, "content": "focusing on a single sample scenario.\n$Var(\u2207_\u03b8J|h, a) = (\u2207_{\u03b8_i}log \u03c0_i(a_i|h_i))^2 Var(A|h, a)$.\nIt is therefore evident that the variance of the policy gradient update is directly proportional to the variance of the advantage estimate: $Var(\u2207_\u03b8J|h, a) \u221d Var(A|h, a)$.\nLet us analyze the variance of the advantage function in cooperative multi-agent setting,\n$A(h, a) = Q(h, a) \u2013 V(h)$\nLet us assume that agent i's reward contribution at an arbitrary time-step t is denoted by $r_{i,t}$ and the episodic reward described in subsection 3.2 can be derived using $r_{global, episodic}(T) = \u03a3_{t=1}^T \u03a3_{i=1}^N \u03b3^{i,t} r_{i,t}h, a$. From this we can rewrite Q(h, a) and V(h) as $Q(h, a) = E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{i=1}^N r_{i,t}h, a)]$ and $V(h) = E_\u03c0[Q(h, a)]$\n$A(h, a) = E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{i=1}^N r_{i,t}h, a)] \u2013 E_\u03c0[E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{i=1}^N r_{i,t}h)]]$\nBased on the linearity of expectations on $\u03a3_{t=1}^T \u03b3^{i,t} r_{j,t} = \u03a3_{t=1}^T \u03b3^{i,t} r_{i,t} + \u03a3_{t=1}^T \u03b3^{i,t} r_{j,t}$\n$A(h, a) = (E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t r_{i,t}h, a] + E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{j\u2260i}^N r_{j,t}h, a)]) \u2013 (E_\u03c0 [E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t r_{i,t}h]] + E_\u03c0 [E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{j\u2260i}^N r_{j,t}h)]])$\n$A(h,a) = (E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t r_{i,t}h, a] \u2013 E_\u03c0 [E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t r_{i,t}h]]) + (E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{j\u2260i}^N r_{j,t}h, a)] \u2013 E_\u03c0 [E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{j\u2260i}^N r_{j,t}h)]])$"}, {"title": null, "content": "The advantage estimate considering only the contribution of agent i is $A_i = E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t r_{i,t}h, a] \u2013 E_\u03c0 [E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t r_{i,t}h]]$ the only advantage term that should be considered while calculating the policy gradient update for agent i whereas the advantage estimate due to other agent contributions $A_{\u00aci} = E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{j\u2260i}^N r_{j,t}h, a)] \u2013 E_\u03c0 [E_{s0~\u03c1_0,s~P,a_1~\u03c0_1..} [\u03a3_{t=1}^T \u03b3^t (\u03a3_{j\u2260i}^N r_{j,t}h)]]$ act as noise. Thus,\n$A(h, a) = A_i(h, a) + A_{\u00aci}(h, a)$\nUsing variance of the sum of random variables,\n$Var(A(h, a)) = Var(A_i(h, a)) + Var(A_{\u00aci}(h, a)) + 2Cov(A_i(h, a), A_{\u00aci}(h, a))$\nTo express the equation in terms of variance, we use the Cauchy-Schwarz inequality, which states that for any two random variables Ai and A\u00aci:\n$Cov(A_i(h, a), A_i(h, a)) \u2264 \u221aVar(A_i(h, a))Var(A_{-i}(h, a))$"}, {"title": null, "content": "By substituting this inequality, we get an upper bound on our equation,\n$Var(A(h, a)) \u2264 Var(A_i(h, a)) + Var(A_{\u00aci}(h, a)) + 2\u221aVar(A_i(h, a))Var(A_{\u00aci}(h, a))$\n$Var(A(h, a)) \u2264 (\u221aVar(A_i(h, a)) + \u221aVar(A_{\u00aci}(h, a)))^2$\nThe above equation shows that the variance of the policy gradient update grows approximately linearly with the number of agents in the multi-agent system. This increase in variance reduces the signal-to-noise ratio of the policy gradient, necessitating more updates for effective learning. Proper credit assignment can mitigate this issue by enhancing the signal-to-noise ratio, thereby facilitating more sample-efficient learning."}, {"title": "Method", "content": "In this paper, we address the challenge of temporal and agent credit assignment in fully cooperative multi-agent systems with episodic global rewards. Our goal is to learn a reward redistribution function that preserves the optimal policy of the original reward function of the environment. We aim to achieve this by defining a reward redistribution function that decomposes the episodic trajectory reward, also known as trajectory return, to each agent based on their contribution to the team's outcome at every time step.\nWe premise that the joint history and action at the final timestep of the multi-agent trajectory serve as a good proxy for predicting the episodic global reward. Consequently, we predict the per timestep reward for each agent by assessing the contribution of its state-action tuple towards generating this joint history of the multi-agent system.\nAssumption 1. The reward redistribution function $r_{it}$, which assigns the reward received by agent i at time step t, is determined by analyzing the importance of each state-action tuple against the joint history and action at the final timestep that predicts the episodic global reward."}, {"title": null, "content": "$r_{global, episodic}(T) = \u03a3_{i=1}^N \u03a3_{t=1}^T r_{i,t} (h_{i,t}, a_{i,t}, h_{\u00aci,t}, a_{\u00aci,t}, h_{|\u03c4|}, a_{|\u03c4|}),$\n(1)\nThis assumption aligns with the framework adopted by prior works (Xiao et al., 2022; Ren et al., 2021; Efroni et al., 2021) which also assumes that the episodic return has some structure in nature, e.g., a sum-decomposable form."}, {"title": "Assembling the reward function", "content": "We propose a method to redistribute the trajectory returns temporally, assigning credit to each time step in the multi-agent trajectory. Subsequently, the temporally redistributed rewards are further decomposed across agents based on their individual contributions, ensuring that the relationship expressed in (1) is maintained.\nGiven the dual nature of credit assignment-attributing relative credit to: (1) each time step in the multi-agent trajectory, and (2) each agent at every time step we can derive the relationship between the trajectory return, $r_{global,episodic}$, and the redistributed reward received by agent i at time step t, $r_{i,t}$.\nTo formalize, we assume the following process:\nAssumption 2. The trajectory return $r_{global, episodic}$ is redistributed temporally to obtain rewards for each time step, $r_{global,t}$, such that:"}, {"title": null, "content": "$r'_{global, episodic}(T) = \u03a3_{t=1}^T r_{global,t} (h_t, a_t, h_{|\u03c4|}, a_{|\u03c4|}).$"}, {"title": null, "content": "Subsequently, these temporally redistributed rewards are decomposed across agents:\n$r_{global,t}(h_t, a_t) = \u03a3_{i=1}^N r_{i,t} (h_{i,t}, a_{i,t}, h_{-it, a-i,t}, h_{|\u03c4|}, a_{|\u03c4|}),$\nensuring that:\n$r_{global, episodic}(T) = \u03a3_{i=1}^N \u03a3_{t=1}^T r_{i,t} (h_{i,t}, a_{i,t}, h_{-it, a-i,t}, h_{|\u03c4|}, a_{|\u03c4|}).$\nThis two-step decomposition process first temporally and then across agents ensures that each agent's contribution at each time step is appropriately credited, thereby providing a robust framework for reward redistribution in multi-agent systems.\nLet's define a function $w_t ~ W_w(h_t, a_t, h_{|\u03c4|}, a_{|\u03c4|}) : (H \u00d7 A) \u00d7 (H_{|\u03c4|} \u00d7 A_{|\u03c4|}) \u2192 R$ that redistributes the rewards across the temporal axis of the multi-agent trajectory. Thus, we can express the multi-agent temporal reward at an arbitrary time step t as\n$r_{global,t} = W_tr_{global,episodic}(T)$\nSimilarly, let's define a function $w_{it} ~ W_k(h_{i,t}, a_{i,t}, h_{\u00aci,t}, a_{\u00aci,t}) : H_i \u00d7 A_i \u00d7 H_{\u00aci} \u00d7 A_{\u00aci} \u2192 R$ that redistributes the temporal rewards at an arbitrary time step t across agents. Hence, now we can express the reward that agent i receives as\n$r_{i,t} = W_{i,t}r_{global,t}$\nFinally, deriving the relationship between $r_{i,t}$ and $r_{global,episodic}(T)$ for an arbitrary time-step t\n$r_{i,t} = W_{t,i}W_tr_{global, episodic (T)}$\nBased on the definition of reward redistribution function\n$\u03a3_{i=1}^N \u03a3_{t=1}^T r_{i,t}(h_{i,t}, a_{i,t}, h_{\u00aci,t}, a_{\u00aci,t}, h_{|\u03c4|}, a_{|\u03c4|}) = r_{global,episodic}(T)$\n$(\u03a3_{i=1}^N \u03a3_{t=1}^T W_{t, i}W_tr_{global, episodic}(T) = r_{global,episodic}(T)$\n$\u03a3_{i=1}^N \u03a3_{t=1}^T W_{t, i}W_t \u03a3\u03a3 = 1$\n$\u03a3_{t=1}^T (\u03a3 W_t) \u00d7 W_t = 1$\nThe solution for the above equation is,\n$\u03a3_{i=1}^N w_{it} = 1$\n(2)\n$\u03a3_{t=1}^T w_t = 1$\n(3)"}, {"title": "Optimal Policy Preservation", "content": "To construct the new reward function $R_{\u03c9,\u03ba}$, we incorporate the original reward $R_s$ and the redistributed credit $r_{it}$ that each agent i receives at time step t. This redistributed credit reflects the agent's relevance to the final outcome of the multi-agent system. The relevance of each state-action tuple is determined by the reward redistribution functions $W_w$ and $W_t$, which effectively assign the trajectory rewards temporally and across agents.\n$R_{\u03c9,\u03ba} (St, at, St+1) = R_\u03c2(St, at, St+1) + r_{i,t}$\n(4)\n$R_{\u03c9,\u03ba} (St, at, St+1) = R_\u03c2(St, at, St+1) + W_{t,i}W_tr_{global,episodic}(T)$\nThis formalizes the reward redistribution process, ensuring that each agent receives a reward proportionate to its contribution to the overall team performance at each timestep."}, {"title": "Optimal Policy Preservation", "content": "To ensure that the optimal policy learned using the densified reward function is also optimal in the environment's original reward function, we establish the following theorem:\nTheorem 2. Let's consider two Dec-POMDPS as defined in subsection 3.1, $M_{env} = (S,A,P,T, O, N, R_\u03c2, \u03c1_0, \u03b3)$ and $M_{rrf} = (S,A,P,T,O,N,R_{\u03c9,\u03ba}, \u03c1_0, \u03b3)$. The only distinction between $M_{env}$ and $M_{rrf}$ are the reward functions. If $\u03c0_\u03b8$ is the optimal policy in $M_{rrf}$ then $\u03c0_\u03b8$ is also optimal in $M_{env}$.\nProof. We know that theta is optimal in Mrrf. For theta to be optimal in Menv, we need to show that $R_{\u03c9,\u03ba} = R + F(St, at, St+1)$ where $F(st, at, st+1)$ is a potential based shaping function which is a necessary and sufficient condition for optimal policy preservation 3.3.\nIt is therefore sufficient to show that the equation (4) takes the form $R_{\u03c9,\u03ba}(St, at, St+1) = R_s(St, at, St+1) + \u03b3\u03c6(st+1) \u2013 \u03c6(st)$. Comparing this format to equation (4), assuming \u03b3 = 1 we arrive at $\u03c6(st+1) - \u03c6(st) = W_{t,i}W_tr_{global,episodic}(T)$. This relation holds for $\u03c6(st) = r_{global, episodic}(T)(\u03a3_{\u03c4'=0}^\u03c3 w_{\u03c4'}w_{t'})$.\nThis result ensures that if an arbitrary policy $\u03c0_\u03b8$ when trained using the reward function $R_{\u03c9,k}$ in $M_{rrf}$ converges to an optimal policy $\u03c0_\u03b8$ then will also be optimal for the original reward function $R_s in M_{env}$."}, {"title": "Policy Gradient Update Equivalence with Reward Redistribution", "content": "In this subsection", "function": "n$\u2207_{\u03b8_k"}, "E_{\u03c0_{\u03b8_k}} [\u03a3_{t=1}^T R_{k,t}"], "\u03b4": "H \u00d7 A \u00d7 H|\u03c4| \u00d7 A|\u03c4| \u2192 R\u2265 0 is a non-negative scalar-valued function conditioned on the trajectory. For brevity", "have": "n$\u2207_{\u03b8_k"}, {"as": "n$\u2207_{\u03b8_k} E_{\u03c0_{\u03b8_k}} [r_{global, episodic (T)}] = \u2207_{\u03b8_k} E_{\u03c0_{\u03b8_k}} [\u03a3_{t=1}^T r_{k,t}] + \u2207_{\u03b8_k} E_{\u03c0_{\u03b8_k}} [\u03a3_{t=1}^T w"}]