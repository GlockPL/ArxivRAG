{"title": "Knoop: Practical Enhancement of Knockoff with Over-Parameterization for Variable Selection", "authors": ["Xiaochen Zhang", "Yunfeng Cai", "Haoyi Xiong"], "abstract": "Variable selection plays a crucial role in enhancing modeling effectiveness across diverse fields, addressing the challenges posed by high-dimensional datasets of correlated variables. This work introduces a novel approach namely Knockoff with over-parameterization (Knoop) to enhance Knockoff filters for variable selection. Specifically, Knoop first generates multiple knockoff variables for each original variable and integrates them with the original variables into an over-parameterized Ridgeless regression model. For each original variable, Knoop evaluates the coefficient distribution of its knockoffs and compares these with the original coefficients to conduct an anomaly-based significance test, ensuring robust variable selection. Extensive experiments demonstrate superior performance compared to existing methods in both simulation and real-world datasets. Knoop achieves a notably higher Area under the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for effectively identifying relevant variables against the ground truth by controlled simulations, while showcasing enhanced predictive accuracy across diverse regression and classification tasks. The analytical results further backup our observations.", "sections": [{"title": "1 Introduction", "content": "Variable selection, especially supervised variable/feature selection, plays a crucial role in enhancing modeling effectiveness across diverse fields, such as genetics and biological science [1], especially when dealing with high-dimensional data characterized by complex correlations and heterogeneity. By forming a parsimonious subset of key variables, this process addresses challenges like the curse of dimensionality and the presence of irrelevant and redundant variables. It thus improves the stability of 1"}, {"title": "2 Backgrounds and Preliminaries", "content": ""}, {"title": "2.1 Variable Selection via Linear Models", "content": "Consider a dataset with n independent and identically distributed observations of p variables, which can be collectively represented in matrix form as $X \\in R^{n \\times p}$ for the variable matrix and $y \\in R^{n}$ for the response vector. An unknown projection vector $\\beta = (\\beta_1,..., \\beta_p) \\in R^{p}$ is assumed to represent a linear relationship as follow\n\n$y = X\\beta + \\varepsilon$ and $\\varepsilon \\sim N(0, \\sigma^{2}I)$ .\n\nThe objective of variable selection is to infer the non-zero coefficients in $\\beta$, so as to understand the relationship between the variables and their responses. 3"}, {"title": "2.2 Knockoff Methods", "content": "Generally, two categories of Knockoffs fixed-X and model-X methods have been studied [9, 10]. While the fixed-X Knockoff filter generates synthetic variables with marginal correlations, independent of other variables, our work focuses on the model-X Knockoff filter generating knockoffs by modeling the joint distribution of all variables, thus managing more complex dependencies. The model-X knockoff variables $\\tilde{X}$ replicate the distribution of the original variables X [10]. These variables meet two crucial criteria: Exchangeability: $(X,\\tilde{X})\\overset{swap(S)}{=} (\\tilde{X}, X)$, achieved by switching each $X_j$ with $\\tilde{X_j}$; in any subset $S$ of indices and Independence: Ensuring that $\\tilde{X}$ is independent of the response vector y, conditional on X. In the model-X Knockoff filter, the impor- tance of each variable $X_j$ is assessed through a statistic $W_j = w_j([X, \\tilde{X}], y)$, where $w_j$ is a function indicating the relevancy of variables. Importantly, the sign of $w_j$ is reversed when $X_j$ and $\\tilde{X_j}$ are swapped within S.\n\n$w_j ([X, \\tilde{X}]^{\\text{swap}(S)},y) = \\begin{cases}\nw_j ([X, \\tilde{X}], y), & j \\notin S, \\\\\n-w_j ([X, \\tilde{X}], y), & j \\in S.\\end{cases}$\n\nA threshold $\\tau$ is established based on the condition as follows.\n\n$\\tau = \\min \\left \\{ t>0: \\frac{1+ \\#\\{j: W_j \\leq -t\\}}{\\#\\{j: W_j \\geq t\\}} \\leq q \\right \\},$ 4"}, {"title": "2.3 Ridge Regression and Sparse Linear Models", "content": "When selecting variables from data, one approach is to assess the significance of the coefficients in regression models, as follows.\n\n\u2022 Ridge Regression [13]: Ridge Regression integrates an $l_2$ penalty into the loss function, effectively shrinking regression coefficients towards zero. This regulariza- tion helps mitigate overfitting by penalizing the magnitude of the coefficients. The 4"}, {"title": "2.4 Ridgeless Regression and Overparameterization", "content": "Ridgeless regression extends the ordinary least squares estimator to provide a unique solution in ultra-high dimensional settings. The coefficients, $\\beta$, can be obtained through a limiting process as $\\lambda \\rightarrow 0$:\n\n$\\beta = \\lim_{\\lambda \\rightarrow 0} (X^{T} X + \\lambda I)^{-1}X^{T}y,$ 5\n\nTheoretical research has explored the conditions under which over-parameterized Ridgeless regression models attain advantages of estimation and prediction [17]."}, {"title": "3 Methodology: Over-Parameterized Knockoff", "content": "This section presents the overall framework and core algorithms of Knoop."}, {"title": "3.1 Overall Framework Design", "content": "In this section, we describe the comprehensive design of the Knoop algorithm (Algo- rithm 1). The algorithm accepts the training set (X,y) as input and outputs the selection result of the variables. The method comprises four principal steps: hier- archical generation of multi-layered knockoffs, Ridgeless regression, anomaly-based significance test, and variable selection. Each step is elaborated upon as follows.\n\nRecursive Generation of Multi-Layered Knockoffs\n\nGiven the training set (X, y) as input, Knoop employs a unique recursive generation approach to generate enhanced knockoff variable matrix $\\tilde{X}$, outlined in lines 1 of Algorithm 1. This approach generates a multi-layered knockoff matrix, $\\tilde{X}$, through a recursive process, thereby increasing the model capacity with a larger number of dimensions in input features for regression. The resulting $\\tilde{X}$ is represented as:\n\n$\\tilde{X} = [X_1, ..., X_p, K_{1,1},..., K_{1,p},..., K_{k_{max},1},..., K_{k_{max},p}].$ 5"}, {"title": "3.2 Recursive Generation of Multi-Layered Knockoffs", "content": "This section outlines the proposed algorithm for recursive generation of multi-layered knockoffs, detailed in Algorithm 2. Given a data matrix based on the n samples of the original p variables, denoted as $X \\anxp$ matrix, and the number of layers desired $l_{max}$, this step would recursively generate $2^{l_{max}} - 1$ sets of knockoff variables, totalling $(2^{l_{max}} - 1)p$ variables, from the original variables. It returns a $n \\times (2^{l_{max}}p)$ knockoff matrix, denoted as $\\tilde{X}$, including both original variables and generated ones. Note that to simplify our analysis in the rest of our paper, we use $k_{max} = 2^{l_{max}} - 1$.\nInitially, the matrix $k^1$, denoted as the matrix of knockoff counterparts of the origi- nal variable matrix X, is computed using computeKnockoff(\u00b7) and then horizontally concatenated with X to form the first layer K\u00b9 (lines 1-2 in Algorithm 2):\n\n$K^{1} = [X \\vert k^{1}]$ 7"}, {"title": "3.2.2 Algorithm Analysis", "content": "While traditional Knockoff methods focus on ensuring exchangeability between each original variable and its knockoff, Knoop is expected to maintain such exchange- ability for multiple sets of knockoffs generated recursively from the same original variable. Here, we provide a brief analysis on the exchangeability of generated vari- ables to the original ones [20, 21]. To establish our analysis, we first make the following assumptions.\n\nA1 We adopt the knockoff generation procedure described in Model-X Knockoff fil- ters [10]. Specifically, for any input matrix $X_{inp}$ and its generated knockoff matrix $X_{gen}$ = computeKnockoff($X_{inp}$), in addition to the exchangeability between each column in $X_{inp}$ and its counterpart in $X_{gen}$, we assume that each column in $X_{inp}$ and $X_{gen}$ is distinct.\n\nA2 We assume the transitivity of exchangeability among multiple random matrices. Let denote V1, V2, and V3 as three independent random matrices. Suppose V\u2081 and V2 exchangeable and V2 and V3 are exchangeable. We say V\u2081 and V3 should be exchangeable.\n\nGiven the input matrix $X = [X_1,..., X_p]$, Algorithm 2 outputs a matrix $\\tilde{X}$ with the structure $\\tilde{X} = [X_1,..., X_p, K_{1,1}, ..., K_{1,p}, ..., K_{k_{max}, 1},..., K_{k_{man},p}]$. We say that for each $j = 1, 2,..., k_{m\u0430x}$, each $j' = 1, 2,..., k_{max}$ and $j \\neq j'$, the matrix X and the sub-matrix 8"}, {"title": "3.3 Anomaly-based Significance Test", "content": "This section outlines the procedure of anomaly-based significance test based on the coefficients of Ridgeless regression. Specifically, Knoop considers an original variable significant (relative to the response variable) when its coefficient becomes an outlier in the total kmax + 1 coefficients of the original variable together with its kmax knockoff counterparts. To the end, the proposed method leverages the coefficient differences between the original variables and their knockoff counterparts to estimate the p-values, prioritizes variables by their significance, and outputs a ranking list of variables for selection purposes.\n\nGiven that the kmax knockoff counterparts of each original variable are independent and identically distributed, we make assumptions as follows.\n\nA3 This work follows existing studies on the distribution of regression coefficients [22- 24] and assumes that for any original variable, the regression coefficients of its 10"}, {"title": "3.3.2 Estimation of p-values based on Coefficient Differences", "content": "To implement the above significance test, this step employs a Z-statistic approach [23, 24] to estimate the p-value of the original features. Given the vector of coefficients, $\\beta$, as the input, the Z-statistic approach first calculates the normalized difference between coefficients for the ith original variable as follows.\n\n$\\hat{Z}_{i} := \\frac{\\sqrt{k_{max} - 1}(\\beta_{i} - \\bar{\\beta_{i}})}{\\hat{S}_{\\beta_{i}}},$ 11\n\n$\\mathbb{P}_{i} := 2(1 - \\Phi(|\\hat{Z}_{i}|)),$ 11"}, {"title": "3.3.3 Algorithm Analysis", "content": "We formulate our main analytical result as the control of False Discovery Rate (FDR) .\n\nUnder assumptions A3 and A4, the anomaly-based significance test controls the FDR at level a if the p-values of the original variables are estimated using the Z- statistic approach and the Benjamini-Hochberg (BH) procedure [18] is applied to select significant variables based on the estimated p-values at a target FDR level a.\n\n$FDR := E\\left [\\frac{V}{R} \\vert R>0\\right ]\\cdot P(R > 0)$ 12"}, {"title": "4 Experiments", "content": "This section uses both controlled simulations and applied real-world scenarios to eval- uate the capabilities of Knoop, assessing the effectiveness in selecting variables for enhanced prediction."}, {"title": "4.1 Simulation with Synthesized Datasets", "content": "We setup two categories of experiments with the synthesized datasets based on simulations (in Algorithm 4) as follows.\n\n\u2022 Low-dimensional experiments: These experiments investigate various parame- ter settings for low-dimensional data generated from simulations. The data matrix X is drawn from a multivariate normal distribution with covariance matrix $\\Sigma_{i,j} = \\rho^{\\vert i-j\\vert}$, where $\\rho = 0.25$, and the error variance is set to $\\sigma^2 = 1$. The experi- ments vary the number of variables $p \\in \\{80, 100, 150, 180\\}$, non-zero components $p_{real} \\in \\{10, 20, 30, 40, 50\\}$, and sample size $n \\in \\{85, 100, 120\\}$. Each experimental setting is repeated at least 20 times to ensure robust results.\n\n$\nX \\leftarrow \\text{nxp matrix with i.i.d. rows from } N(0, \\Sigma)$\n$\nX \\leftarrow \\text{vector with i.i.d. entries from } N(0, \\sigma^{2})$\n$\nX \\leftarrow X \\beta + \\epsilon$ 13"}, {"title": "4.1.1 Experiment Results", "content": "The experiment results for variable selection, as presented in Table 1, showcase the performance of different methods in both low-dimensional and high-dimensional set- tings. Across all 13 experimental settings, Knoop achieves the highest AUC values in 7 of them and takes the second places in 5 of them, indicating its superior ability to prioritize significant variables. Especially, for the low-dimensional experiments (set- tings 1-10), Knoop always achieves the highest or the second highest AUC values. Similarly, in the high-dimensional experiments (settings 11-13), Knoop maintains its superior performance, particularly when the sample size is sufficient (settings 11 and 14"}, {"title": "4.1.2 Discussions", "content": "Knoop and Knockoff methods generally outperform Lasso or ElasticNet in variable selection, achieving higher AUC, due to their specific design to control FDRs while maintaining power via so-called \"selective inference\" [25]. Utilizing coefficients from Lasso and ElasticNet for variable selection can even result in an AUC below 0.5, indica- tive of performance inferior to random guessing. Methods like Lasso or ElasticNet can still fit and predict data well, even when their non-zero coefficients are not assigned to the true variables, due to the inconsistency in coefficient estimates [26]. Specifically, in high-dimensional scenarios where an extremely sparse model is fitted, almost all the coefficients are estimated as 0. Consequently, all variables with zero coefficients may be considered to be equally irrelevant, making it impracticable to distinguish or select meaningful variables based solely on their coefficients. 15"}, {"title": "4.2 Evaluation with Realistic Datasets", "content": "Table 2 provides the profile of every dataset for evaluation, including the number of variables, the number of samples, and the context of tasks. To carry out the experiments, we use following two categories of baseline algorithms for comparisons.\n\n\u2022 Statistical methods such as model-X Knockoff, multiple Knockoff, Ridge regression, Lasso, and ElasticNet are utilized to select variables by their statistical significance. For every comparison here, Knoop and these methods are set to selected a fixed- length of variables in every experiment (please refer to Section 3.1).\n\nNote that Knoop employs a hierarchical knockoff structure spanning lmax = 4 layers (totaling 15 sets of knockoff matrices). For classification tasks, we use misclassification rate for evaluation and comparison. For regression tasks, we report mean squared error (MSE) as the measurements of performance. 15"}, {"title": "4.2.1 Experiment Results", "content": "In Table 3, we present the comparison results between Knoop and statistical variable selection methods under the settings of fixed-length selection (1-10 selected variables). Due to the page length, we only present 1, 2, 4, and 8 here. Specifically, we assess the misclassification rates of chosen variables through logistic regression model, alongside analyzing the MSE of these variables when employing linear regression for prediction. The results demonstrate that Knoop consistently outperforms the other methods in terms of misclassification rate and MSE when selecting a small number of variables (1, 2, or 4). This superior performance is particularly evident in the Communities and Crime dataset, where Knoop achieves significantly lower MSE values compared to the other methods. However, as the number of selected variables increases to 8, the performance of Knoop becomes comparable to or slightly worse than some of the other methods, such as Lasso and ElasticNet, in certain datasets like the Alon and Superconductivity datasets. Note that the misclassification rate on Alon dataset could be even higher than 0.5, as the samples are extremely class-imbalanced.\n\nTable 4 compares the performance of Knoop against several global optimization- based feature selectors for the four datasets, under varying-length selection settings. The results demonstrate that Knoop not only achieves the highest accuracy (mea- sured by misclassification rate or MSE) when selecting features for a specific machine learning algorithm on the majority of datasets but also almost delivers the highest accuracy across all machine learning models for each dataset. We are encouraged to see Knoop outperforms the global optimization-based approaches for feature selection, 16"}, {"title": "4.2.2 Discussions", "content": "Comparing to sparse linear models and global optimization-based feature selectors, the core advantage of Knoop (as well as the Knockoff baselines) is its capability in testing the conditional independence between variables and the response for predic- tion purposes [7]. Apparently, filtering the variables that are conditional independent with the responses is more effective than just estimating the sparsest set of coeffi- cients, resulting in more generalizable models and preventing models learning from noise as signal. Furthermore, such test helps in distinguishing causation from correla- tion. This distinction is crucial in scenarios where understanding causal relationships 17"}, {"title": "5 Discussion and Conclusions", "content": "This work introduces Knoop to enhance variable selection, which addresses the model capacity issue in regression models used in the common Model-X Knockoff filter. The proposed approach first generates multiple sets of knockoff variables through recursive generation, and then integrates every original variable and its multiple knockoff coun- terparts into a Ridgeless regression. Knoop refines coefficient estimates with better capacity, and employs an anomaly-based significance test for robust variable selection. Knoop can either pickup a predefined number of variables or optimizes the number of selected variables by cross-validation. Extensive experiments demonstrate supe- rior performance compared to existing methods in relevant variable discovery against ground truth variables controlled by simulations and supervised feature selection for classification/regression tasks."}, {"title": "Declaration", "content": "\u2022 Funding - Not applicable\n\u2022 Conflicts of interest/Competing interests - Not applicable\n\u2022 Ethics approval - No data have been fabricated or manipulated to support your conclusions. No data, text, or theories by others are presented as if they were our own. Data we used, the data processing and inference phases do not contain any user personal information. This work does not have the potential to be used for policing or the military.\n\u2022 Consent to participate - Not applicable\n\u2022 Consent for publication - Not applicable\n\u2022 Availability of data and material - Experiments are based on publicly available open-source datasets.\n\u2022 Code availability - All codes will be released after acceptance.\n\u2022 Authors' contributions - H.X contributed the original idea. X.Z conducted experi- ments. X.Z and H.X wrote the manuscript. Y.C involved in discussion and wrote part of the manuscript. X.Z and H.X share equal technical contribution. 18"}]}