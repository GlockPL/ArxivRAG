{"title": "A Methodology to Study the Impact of Spiking Neural Network Parameters considering Event-Based Automotive Data", "authors": ["Iqra Bano", "Rachmad Vidya Wicaksana Putra", "Alberto Marchisio", "Muhammad Shafique"], "abstract": "Autonomous Driving (AD) systems are considered as the future of human mobility and transportation. Solving computer vision tasks such as image classification and object de-tection/segmentation, with high accuracy and low power/energy consumption, is highly needed to realize AD systems in real life. These requirements can potentially be satisfied by Spiking Neural Networks (SNNs). However, the state-of-the-art works in SNN-based AD systems still focus on proposing network models that can achieve high accuracy, and they have not systematically studied the roles of SNN parameters when used for learning event-based automotive data. Therefore, we still lack understanding of how to effectively develop SNN models for AD systems. Toward this, we propose a novel methodology to systematically study and analyze the impact of SNN parameters considering event-based automotive data, then leverage this analysis for enhancing SNN developments. To do this, we first explore different settings of SNN parameters that directly affect the learning mechanism (i.e., batch size, learning rate, neuron threshold potential, and weight decay), then analyze the accuracy results. Afterward, we propose techniques that jointly improve SNN accuracy and reduce training time. Experimental results show that our methodology can improve the SNN models for AD systems than the state-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARS dataset, and it can also achieve iso-accuracy (i.e., ~85% with standard deviation less than 0.5%) while speeding up the training time by 1.9x. In this manner, our research work provides a set of guidelines for SNN parameter enhancements, thereby enabling the practical developments of SNN-based AD systems.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the interest in the autonomous driving (AD) systems has increased significantly as these systems potentially improve the efficiency of human mobility and transportation [1] [2]. The development of AD systems in real life necessitates the capabilities of solving com-puter vision tasks such as image classification and object detection/segmentation from images/videos [3]. Currently, the state-of-the-art accuracy for computer vision tasks is achieved through advanced neural network algorithms, such as Deep Neural Networks (DNNs) and Spiking Neural Net-works (SNNs). Therefore, the employment of neural network algorithms for AD systems is prevalent [4]. Besides the above-discussed functionalities, AD systems also require low energy consumption to preserve their battery lifespan, and real-time output to provide fast decision. These requirements can potentially be fulfilled by SNNs, since SNNs offer low power/energy computation due to sparse event-based operations [5]-[7], high accuracy due to effective learning mechanism [1] [2] [8] [9], and low latency due to efficient neural/spike coding and operational timesteps [10]\u2013[13]. Furthermore, to maximize the benefits of sparse operations in SNNs, the workload should be presented in the form of event-based data, because this data format is suitable with the data representation in SNNs (i.e., spikes). Therefore, to ensure the practicality of the SNN models for AD systems, the SNN developments should also consider event-based automotive data, such as the NCARS dataset [14] [15]. Motivated by the potentials of SNN-based AD systems, our targeted research problems are the following.\n\u2022 How can we investigate, analyze, and understand the impact of different SNN parameters on the final accu-racy?\n\u2022 How can we effectively leverage this analysis and the obtained knowledge for developing SNN models for event-based automotive data?\nThe efficient solution to this problem will provide guidelines and insights for developing effective SNN models that can achieve high accuracy with fast training time for AD systems. To address these problems, we make two important contri-butions in this paper: (1) detailed investigation and analysis on how different SNN parameters and settings impact the accuracy, and (2) a novel methodology to develop SNN models considering the key observations from the above analysis; which will be discussed further in Section I-B."}, {"title": "A. State-of-the-Art and Their Limitations", "content": "Currently, the state-of-the-art works in SNN-based AD systems [1] [2] still focus on proposing network models to achieve high accuracy on event-based automotive data. However, they have not investigated the impact and roles of different SNN parameters on the learning quality, and hence lack understanding on how to effectively develop SNN models for AD systems. Consequently, the existing techniques for developing SNNs for AD systems may not be efficient, and their accuracy may be sub-optimal. To demonstrate these limitations, we perform an experimental case study. Here, we implement CarSNN [1] and perform experiments with its default parameter setting. Note, the details for the experimental setup and parameter settings will be discussed in Sections IV and V. The experimental results with key observations are presented in Fig. 1.\nThese results show that with a simple tuning of the learning rate, the SNN accuracy for the NCARS dataset is improved, and high accuracy can be obtained with fewer training epochs compared to the state-of-the-art. These limi-tations expose the need to understand the impact of SNN pa-rameters when learning event-based automotive data so that the knowledge can be used for practical SNN developments for AD systems."}, {"title": "II. BACKGROUND", "content": "A. Spiking Neural Networks (SNNs)\nRecently, SNNs have demonstrated great success and potential to be deployed in autonomous systems due to their low energy consumption and high performance. Inspired by nature and biology, the artificially-designed SNNs mimic the event-based decision-making process of the biological brain. As shown in Fig. 3, they receive input in the form of a spike train and process the information through spiking neurons. SNNs typically employ a specific neuron model, such as the Leaky Integrate-and-Fire (LIF), whose neuronal dynamics can be stated as the following.\n$\\tau \\frac{dVm(t)}{dt} = -(Vm(t) \u2013 Vr) + I(t)$ (1)\nif $Vm \\geq Vth$ then $Vm \\leftarrow Vr$ (2)\nHere, $Vm$ refers to the membrane potential of a neuron at time-t, $Vr$ refers to the reset potential of a neuron, and $\\tau$ refers to the time constant of membrane potential decay. $I$ refers to the inputs. Meanwhile, $Vth$ refers to the neurons' threshold potential (or simply threshold potential).\nCompared to conventional DNNs, SNNs have advantages in terms of low power/energy consumption, low computation latency, as well as a straightforward interface with event-based cameras. Based on these benefits, the deployment of SNNs for AD systems is highly desired."}, {"title": "B. SNN Learning Mechanism", "content": "Overview: Training SNNs is non-trivial due to the intrin-sic non-differentiable nature of the spiking loss function [16]. To overcome this issue, there are two possible solutions proposed in the literature for training SNNs with supervised settings, namely DNN-to-SNN conversion [17] and direct SNN training under surrogate gradient [18]. Since the first approach requires training non-spiking DNNs on the same data, it limits its applicability to static data and cannot be directly used on event-based data. Meanwhile, the second approach trains the network directly in the spiking domain through a surrogate gradient-based learning rule to address the non-differentiable issue in SNNs. In this category, the"}, {"title": "III. THE PROPOSED METHODOLOGY", "content": "Our proposed methodology systematically investigates the impact of SNN parameters on the learning quality, and then leverages the knowledge for effective SNN developments for AD systems. Its overview is shown in Fig. 5, and the details of each step will be discussed in the subsequent subsections.\n\nA. Determining the SNN Parameters for Investigation\nIt aims at selecting parameters that are considered impor-tant (effective) for the learning process To do this, we first study the learning mechanism employed in SNN processing, then identify SNN parameters that are directly involved and/or effectual in the learning process.\nIn this work, we employ the STBP learning rule [19], which considers both spatial and temporal information. Here, the amount of information employed for updating the weights during the learning process mainly depends on the batch size (B). To calculate its loss function, the STBP rule leverages the spiking activity which mainly depends on the threshold potential (Vth). The reason is that, the threshold potential defines the range of value for neuronal dynamics to trigger"}, {"title": "B. Exploring the Impact of Different SNN Parameter Values", "content": "It aims at exploring different settings (values) of the selected SNN parameters, and analyzes their impact on the accuracy. Here, each parameter has its own range of exploration settings due to the nature of their functionalities. To decide the range of values for each parameter, we first consider the parameter settings in the state-of-the-art works as the default settings. For instance, the default setting for the SNN architecture with STBP rule [1] considers the following set of values: B = 40, lr = 1e-3, Vth = 0.4, and Wdecay = 0. The experimental results for employing the default setting are presented in Fig. 6. The results show that, in general, the accuracy curve consists of two regions: (1) transition region, where the accuracy is still not stable as the network is still learning new information over training time, and (2) stable region, where the accuracy does not change much as the network does not necessarily learn new information. This curve indicates how fast the training phase reaches stable accuracy. We define the stable region as the region that has accuracy with a standard deviation of less than 0.5% accuracy over the last 10 training epochs, since the typical range of acceptable accuracy variation in the neural network community is within 1% accuracy [22] [23].\nAfterward, we explore different values for each investi-gated parameter, which can be smaller or larger than its default setting. In this manner, we can observe and analyze the correlation between the accuracy and the value changes. The details of exploration values are defined in the following.\n\u2022 \u0392\u2208 {10, 20, 30, 40, 50, 60, 70}.\n\u2022 Ir \u2208 {5e-4, 1e-4, 1e-3, 5e-3, 7.5e-3, le-2, 5e-2}.\n\u2022 Vth \u2208 {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7}.\n\u2022 Wdecay \u2208 {0,0.2, 0.5, 0.75, 1, 2, 4}.\nBased on these settings, we perform experiments that tune"}, {"title": "C. Parameter Enhancements for Improving the SNN Learning Quality", "content": "It aims at improving the SNN learning quality, i.e., in-creasing the accuracy and/or reducing the training time. To do this, we first analyze the experiment results from the previous exploration step to identify the effective value for each parameter. The effective parameter value is typically characterized by the one that leads to high accuracy under a relatively short training time. Therefore, our strategy is to perform parameter enhancements by tuning the selected parameters to follow the effective values based on the exper-imental results. Note, the experimental results and related discussion for the SNN parameter enhancements will be provided in Section V-E and Section V-F."}, {"title": "IV. EVALUATION METHODOLOGY", "content": "Fig. 7 illustrates the experimental setup for evaluating our methodology. Here, we employ a Python-based implementa-tion that runs on an Nvidia RTX 6000 Ada GPU machine, and the generated outputs are the accuracy (i.e., training and test) and the log of experiments (e.g., number of epoch and loss scores). The Python-based implementation is built using the numpy, torch, and torchvision library packages. We employ the SNN architecture described in Table I with the STBP learning rule, the NCARS dataset as the workload, and 200 epochs for the training phase. We consider the state-of-the-art work, i.e., CarSNN [1] with the following default setting: B = 40, lr = 1e-3, Vth = 0.4, and Wdecay = 0, as the reference comparison partner."}, {"title": "A. Impact of the Batch Size", "content": "The experimental results for the impact of batch size B are presented in Fig. 8. The results show that, in general, most of the batch sizes have fluctuating accuracy during the early training phase (i.e., \u2264 60 epoch); see label \u2460. Such accuracy fluctuations happen because in the early training phase, the network still starts learning new information and needs to jump out of local minima to reach better points in the SNN loss landscape. Furthermore, we observe that networks with smaller batch sizes tend to face more fluctuations than the ones with bigger batch sizes. The reason is that, updating the weights under a smaller batch size may lead the network to update its weights towards specific training directions in the early training phase, which may not fully represent unseen features. After training the network for some time, the accuracy starts to saturate in the later training phase; see label 2. In this training phase, the notable difference is that smaller batch sizes tend to have better accuracy than bigger batch sizes. The reason is that, the smaller batch sizes provide more fine-grained information updates than the bigger ones, and considering that the NCARS dataset only has two classes (i.e., car and background), the network tends to train more effectively with small batch sizes while minimizing the overfitting. Based on these observations, we select B = 20 as the effective value for SNN enhancements, since it quickly leads to stable and high accuracy over the training phase."}, {"title": "B. Impact of the Learning Rate", "content": "The experimental results for the impact of learning rate Ir are presented in Fig. 9. The results show that, in general, most of the learning rates have fluctuating accuracy during the early training phase (i.e., \u2264 60 epoch) since the network still starts learning new information; see label \u2462. In this early training phase, we observe that learning rates 7.5e-3 and le-2 can quickly reach accuracy without notable fluctua-tions, while smaller and bigger learning rates face significant fluctuations. The reason is that, smaller and bigger learning rates struggle to find local minima, while the learning rates of 7.5e-3 and le-2 are in the right range of values to generalize well. These trends stay consistent until later training phase, as shown by label \u2463, indicating that the learning rates of 7.5e-3 and le-2 are effective in training the SNN to high accuracy values compared to other learning rates. To decide which learning rate value to select as the adjustment setting, we compare the accuracy obtained by learning rates 7.5e-3 and le-2. Overall, Fig. 9 shows that a learning rate of 7.5e-3 achieves comparable accuracy to the learning rate of le-2 over the training phase, and in the range of 20-40 training epochs, the learning rate of 7.5e 3 has slightly better results, meaning that it potentially enables a faster learning process. Therefore, in this work, we select lr = 7.5e-3 as the effective value for SNN enhancements. Later, we will also show in Section V-F that employing lr = 1e-2 can also achieve good accuracy, demonstrating the effectiveness of our methodology."}, {"title": "C. Impact of the Threshold Potential", "content": "The experimental results for the impact of threshold potential Vth are presented in Fig. 10. The results show that, most of the given Vth values face notable accuracy fluctuations in the early training phase (i.e., \u2264 60 epoch) due to the impact of learning new information; see label 5. The accuracy trends become more distinguishable in the later training epochs (i.e., epochs 140-200); see label \u2465. Here, we observe that most of the threshold potential values (i.e., 0.1, 0.2, 0.3, 0.4, 0.5, and 0.6) achieve comparable accuracy, while the threshold potential of 0.7 has lower accuracy than the others. The reason is that a large threshold potential"}, {"title": "D. Impact of the Weight Decay", "content": "The experimental results for the impact of weight decay Wdecay are presented in Fig. 11. The results show that only a weight decay rate of 0 offers high accuracy, as in this setting, the knowledge learned by the learning process stays in the network model, thereby providing proper synaptic weights for the inference phase; see label \u2466. Meanwhile, other weight decay rates (i.e., > 0) suffer from accuracy degradation, as these settings make the knowledge learned by the learning process decay over time and may disappear eventually, thereby corrupting the synaptic weights for the inference phase; see label \u2466. Hence, in this work, we select Wdecay = 0 as the effective value for SNN enhancements."}, {"title": "E. Impact of the SNN Parameter Enhancements", "content": "Based on previous analysis from Section V-A until Sec-tion V-D, we propose a set of parameter values for enhancing SNNs for an event-based automotive dataset: B = 20, lr =7.5e-3, Vth = 0.5, and Wdecay = 0. The experimental results for our parameter enhancements are presented in Fig. 12. The results show that, in general, our parameter enhancements improve the SNN accuracy from the state-of-the-art over the training epoch, demonstrating the effectiveness of our parameter settings. Our enhanced SNN model has a better accuracy range over the training phase (71.28% - 86.64% accuracy) than the state-of-the-art which has a range of 57.06%- 85.84% accuracy. In the early training phase, the state-of-the-art work faces significant accuracy fluctuations as its parameter values are crafted to achieve stable accuracy after running hundreds of training epochs; see label \u2467. Meanwhile, our parameter enhancements can achieve a rela-tively smooth learning curve (i.e., accuracy) in the transition region, since we craft its parameter values with the ones that iteratively find the local minima over the training phase; see label 8. These trends stay consistent until the later training phase, since our parameter enhancements achieve notable accuracy improvements as compared to the state-of-the-art work; see label \u2468. Furthermore, our proposed parameter enhancements can reach a stable region faster than the state-of-the-art work (i.e., by 1.9x speed up). Here, the state-of-the-art work achieves 85.24% accuracy after performing 125 training epochs, while our enhanced SNN model achieves 85.99% accuracy after performing 66 training epochs; see label 10. The reason for all these improvements is that we employ parameter enhancements that combine benefits from the selected values, thus maximizing their advantages together for improving SNN accuracy and training time."}, {"title": "F. Further Discussion", "content": "To further demonstrate the effectiveness and generality of our methodology, we perform evaluations considering different parameter settings as the following.\n\u2022 Setting-1: B = 20, lr = 7.5e\u22123, Vth = 0.5, Wdecay = 0.\n\u2022 Setting-2: B = 20, lr = 1e\u00af2, Vth = 0.5, Wdecay = 0. Setting-1 is simply the same as our proposed one discussed in the previous subsection (i.e., Section V-E), while Setting-2 is similar to Setting-1 but with a different learning rate. We select a learning rate of le-2 for Setting-2 because from the observations in Section V-B, this learning rate can achieve comparable accuracy to our selected learning rate value (i.e., 7.5e-3). The experimental results for this case study are presented in Fig. 13. The results show that, in general, Setting-2 leads the enhanced SNN model to achieve a better accuracy range over the training phase (i.e., 75.48% - 86.46% accuracy) than the state-of-the-art work as indicated by label 11 and 12, demonstrating the effectiveness of its parameter setting. Furthermore, Setting-2 can also reach a stable region faster than the state-of-the-art work with 85.17% accuracy, similar to that of Setting-1. The reason for all these improvements is that the SNN parameter values in Setting-2 are also obtained through our methodology, hence combining benefits from the selected parameter values for improving SNN models with event-based autonomous data."}, {"title": "VI. CONCLUSION", "content": "In this work, we propose a novel methodology to sys-tematically understand the impact of SNN parameters on the learning quality, by identifying the practical SNN parameters to explore, and parameter enhancements. We investigate the impact of batch size, learning rate, threshold potential, and weight decay. The experimental results show that our methodology improves the SNN learning quality, as the enhanced SNN achieves higher accuracy (i.e., 86%) than the state-of-the-art, and it can speed up the training phase by 1.9x while achieving iso-accuracy (i.e., around 85% with standard deviation < 0.5%). In this manner, our methodology provides effective guidelines and insights for developing highly accurate and efficient SNNs for AD systems."}]}