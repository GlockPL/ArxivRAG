{"title": "The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks", "authors": ["Niyar R Barman", "Krish Sharma", "Ashhar Aziz", "Shashwat Bajpai", "Shwetangshu Biswas", "Vasu Sharma", "Vinija Jain", "Aman Chadha", "Amit Sheth", "Amitava Das"], "abstract": "The rapid advancement of text-to-image generation systems, exemplified by models like Stable Diffusion, Midjourney, Imagen, and DALL-E, has heightened concerns about their potential misuse. In response, companies like Meta and Google have intensified their efforts to implement watermarking techniques on AI-generated images to curb the circulation of potentially misleading visuals. However, in this paper, we argue that current image watermarking methods are fragile and susceptible to being circumvented through visual paraphrase attacks. The proposed visual paraphraser operates in two steps. First, it generates a caption for the given image using KOSMOS-2, one of the latest state-of-the-art image captioning systems. Second, it passes both the original image and the generated caption to an image-to-image diffusion system. During the denoising step of the diffusion pipeline, the system generates a visually similar image that is guided by the text caption. The resulting image is a visual paraphrase and is free of any watermarks. Our empirical findings demonstrate that visual paraphrase attacks can effectively remove watermarks from images. This paper provides a critical assessment, empirically revealing the vulnerability of existing watermarking techniques to visual paraphrase attacks. While we do not propose solutions to this issue, this paper serves as a call to action for the scientific community to prioritize the development of more robust watermarking techniques. Our first-of-its-kind visual paraphrase dataset and accompanying code are publicly available.", "sections": [{"title": "1 Watermarking AI-Generated Images: The Necessity", "content": "With the rapid proliferation of AI-generated visual content from models such as Stable Diffusion (Rombach et al. 2022a; Podell et al. 2023a), DALL-E (Ramesh et al. 2021, 2022), Midjourney (Holz 2022), Imagen (Saharia et al. 2022), among others, and their dangerous potential for misuse by malicious actors, the field of image watermarking has become a critical area of research. Given that, as of 2020, approximately 3.2 billion images and 720,000 hours of video are uploaded to social media platforms daily (T.J. Thomson 2020), the volume of visual content is staggering. When considering how AI-generated visuals can significantly contribute to misinformation strategies by serving as deceptive evidence for fabricated anomalies, the demand for robust watermarking techniques for AI-generated content becomes more pressing than ever. Governments worldwide have initiated discussions and implemented measures to develop policies concerning AI systems. The European Union (European-Parliament 2023) has taken a decisive step by enacting legislation, while the United States (White-House 2023) and other countries have introduced preliminary proposals for a regulatory framework for AI. A primary concern among policymakers is that \"Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality\" (Janjeva et al. 2023). Moreover, AI policymakers have raised significant concerns regarding the use of automatic labeling or invisible watermarks as technical solutions to the challenges posed by generative AI-enabled disinformation. Nevertheless, persistent concerns remain about the susceptibility of these measures to deliberate tampering and the potential for malicious actors to circumvent them entirely.\nIn response to the increasing concern over AI-generated misinformation, companies such as Meta, Google, and OpenAI have begun exploring methods to watermark their generated image content. Meta recently announced its strategy (Fernandez et al. 2023a) to address AI-generated misinformation, emphasizing three primary approaches: (i) the inclusion of visible markers on images, (ii) the application of invisible watermarks, and (iii) the embedding of metadata within image files. This paper contends that these strategies are inadequate in the context of advanced generative Al systems. For example, with the rapid progression of image inpainting systems (Jeevan, Kumar, and Sethi 2023; Zheng et al. 2022; Li et al. 2022; Wang, Yu, and Zhang 2022), detecting and removing visible markers has become increasingly straightforward, as illustrated in Figure 2. Similarly, metadata, which comprises additional tags, can be easily stripped from files using a simple wrapper, as demonstrated in the detailed example provided in Appendix 7.1.\nWatermarking techniques originated within the computer vision community; however, recent advancements in LLMs have spurred interest in the development of text watermarking methods. Last year, OpenAI alluded to the development"}, {"title": "2 Related Work: State-of-the-art Image Watermarking and Detection Methods", "content": "Watermarking techniques are broadly classified into two categories: (i) static (i.e., non-learning) watermarking methods and (ii) learning-based (dynamic) watermarking methods. Static watermarking refers to embedding a watermark into an image in a fixed, unchanging manner. Once the watermark is embedded, it remains the same regardless of any subsequent use or manipulation of the image. Dynamic watermarking, on the other hand, refers to a more flexible approach where the watermark can change or adapt based on certain conditions or during the image's usage. This type of watermarking is often used in scenarios where the watermark needs to convey additional information, such as the time of access, user identity, or location, and can be embedded in real-time. Dynamic watermarking can be more difficult to detect and remove because the watermark isn't static or predictable.\n2.1 Static Watermarking Methods\nThe most common way of creating a static watermark is to apply some type of Frequency domain transform and then altering certain frequency coefficients of the image or its image blocks via adding a bit of the watermark. The watermarked image is obtained via inverse transform of this transformed image. like Discrete Wavelet Transform (DWT) (Lai and Tsai 2010) to decompose an image into several frequency sub-bands, then applying another transform like the Discrete Cosine Transform (DCT) (Yuan et al. 2020) to each block of some of the sub-bands, and finally altering certain frequency coefficients of each block via adding a bit of the watermark. The watermarked image is obtained via inverse transform. We won't study these methods further in this work due to these approaches being extremely easy to detect and very outdated, except for DwtDctSVD (Navas et al. 2008b), included solely for academic comparison.\nDwtDctSVD The DwtDctSVD (Navas et al. 2008b) watermarking algorithm uses various techniques to embed a watermark into an image, including Discrete Wavelet Transform (DWT), Discrete Cosine Transform (DCT), and Singular Value Decomposition (SVD). These methods decompose the image into frequency bands, allowing the watermark to be embedded in specific regions that are less prone to common image processing operations. The watermark is embedded in middle-frequency bands to balance robustness and imperceptibility. However, the watermark can be removed or degraded by manipulating the target frequency bands through filtering or compression, altering the singular values obtained from SVD, or applying visual paraphrasing techniques such as random pixel swapping or contrast changes. These methods can destroy or weaken the watermark, rendering it less effective or totally removed.\n2.2 Learning-based Watermarking Methods\nA typical learning based watermarking method has three key components: watermark (w), encoder (E), and decoder (D). An encoder takes an image X and watermark w as inputs and produces an watermarked image (Xw). So, $X_\\omega = E(X, w)$ and a decoder takes Xw as an input and produces $\\hat{\\omega} = D(X_\\omega)$. $\\hat{\\omega_i} = [\\hat{\\omega}_i \\geq T]$, where $[.]$ represents the indicator function and $T$ is a threshold value we decide based on the problem requirements.\nThe following paragraphs describe the five state of the art learning based watermarking techniques we selected for comparison with visual paraphrasing.\nHiDDen: A Watermarking Method for Images The HiDDen paper (Zhu et al. 2018) proposes a watermarking technique where an encoder embeds a secret message into a cover image, which is then noised and decoded to retrieve the message. To ensure robustness, the encoder and decoder are trained to minimize losses related to image similarity, message accuracy, and adversarial detection. However, the method has weaknesses that can be exploited, such as the noise layer's impact on the encoded message and the complexity of balancing multiple loss functions. Visual paraphrasing, which alters the image while preserving its semantic content, can manipulate these weaknesses to distort the encoded message or make the watermark undetectable.\nStable Signature The Stable Signature method (Fernandez et al. 2023b) introduces a novel watermarking technique for images generated by latent diffusion models (LDMs) (Rom-"}, {"title": "2.3 Traditional De-Watermarking Techniques", "content": "In addition to the discussed watermarking methods, certain traditional image alteration techniques can also function as de-watermarking attacks, as explored by previous researchers. We have included the following techniques in our study for comparison purposes.\nBrightness: Altering the brightness (Verma, Singh, and Kumar 2009) of an image is a simple yet effective method for attempting to reduce the visibility of watermarks. By increasing or decreasing the brightness, the contrast between the watermark and the underlying image can be diminished, making the watermark less noticeable. However, this method can also degrade the overall quality of the image, potentially affecting important visual details. For our experiments, we selected a brightness level increased by a factor of 2.\nRotation: Rotating (Luo et al. 2022) an image is another technique used to obscure watermarks, especially those that are positioned in a fixed location. By rotating the image, the watermark may be repositioned to an area where it is less visible or more easily cropped out. While rotation can effectively reduce watermark visibility, it can also distort the original image content, particularly if the rotation angle is significant. For our experiments, the images were rotated by \u00b145\u00b0.\nJPEG Compression: JPEG compression (Jia, Fang, and Zhang 2021) is a common technique that reduces the file size of an image by discarding some of its data, which can incidentally affect the visibility of watermarks. The lossy nature of JPEG compression can blur or distort the watermark, making it less discernible. However, this technique may also lead to a loss of image quality, particularly when high compression levels are used. For our experiments, we set the quality setting to a reduced level of 50.\nGaussian Noise: Adding Gaussian noise (Li et al. 2024) to an image is a method that introduces random variations in pixel intensity, which can help in reducing the clarity of watermarks. The noise can obscure the fine details of the watermark, blending it into the background. While this approach can be effective, it may also degrade the visual quality of the image, making it appear grainy or less sharp. In our experiments, noise with a standard deviation of 0.05 was added to the images."}, {"title": "3 Visual Paraphrasing", "content": "Paraphrasing is a well-established area of research within natural language processing (NLP). For instance, sentences such as \"What is your age?\" and \"How old are you?\" convey identical meanings despite their differing linguistic structures, thus constituting paraphrases of each other. In contrast, the concept of visual paraphrasing has not been as extensively explored, likely due to the recent emergence of text-to-image generation systems such as Stable Diffusion and Midjourney. These systems are capable of producing slight variations of a given image that maintain the same semantic content while differing in visual presentation. A related concept is visual entailment, which concerns image-sentence pairs where the image serves as the premise, as opposed to a sentence in traditional Visual Entailment tasks (Xie et al. 2019). The objective in visual entailment is to determine whether the image semantically supports the text. However, given the significant differences between visual entailment and visual paraphrasing, this discussion will not explore visual entailment further. For example, as illustrated in Figure 4, all generated images are visual paraphrases of the input image.\nThe process of visual paraphrasing begins with the generation of a caption for the image, followed by the application of image-to-image diffusion techniques. This two-step approach ensures that the output images retain the semantic integrity of the original while allowing for variations in visual presentation. The effectiveness of visual paraphrasing is governed by adjusting two key parameters: paraphrase strength and guiding scale, as described below.\nGenerating Caption When an image encountered in the wild is suspected to have been generated by AI, the original prompt used to create it is typically unavailable. To address this challenge, we employed KOSMOS 2 (Peng et al. 2023) to generate a textual description or a brief caption of the image. KOSMOS 2, along with other image captioning models (You et al. 2016), is particularly effective at producing detailed textual descriptions of images. This generated caption then serves as the textual conditioning input for the image-to-image diffusion models, which are discussed in the following section. By utilizing the extracted textual context as guidance, the diffusion model reconstructs the image while preserving its semantic content, thereby achieving visual paraphrasing.\nImage-to-Image Diffusion At the core of visual paraphrasing lies the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). This technique, employed in generative models, transforms images while maintaining their underlying structure and semantic information. The diffusion process involves two key stages: the forward diffusion process and the reverse diffusion process. In the forward diffusion process, an image is gradually corrupted by adding noise, eventually reaching a state of complete noise. Mathematically, this process is described as follows: $x_t = \\sqrt{a_t}x_{t-1} + \\sqrt{1 - a_t} \\epsilon_t$, where $x_t$ is the image at time step t, $a_t$ is a noise scaling factor, and $\\epsilon_t$ is the noise sampled from a Gaussian distribution. In the reverse diffusion process, the model attempts to remove the noise step by step, reconstructing the original image from the noisy version. This is achieved using a learned denoising function $\\epsilon_\\theta$: $x_{t-1} = \\frac{1}{\\sqrt{a_t}}(x_t - \\sqrt{1 - a_t} \\epsilon_\\theta(x_t, t))$. This iterative denoising continues until the model produces an image that closely resembles the original in both visual and semantic terms. In this context, two controls are utilized: (i) the original image and (ii) the generated caption. The number of inference steps, denoted by T, is a critical factor in this process. Increasing the number of steps generally results in more refined reconstructions, yielding higher-quality images, albeit with greater computational demands. In this scenario, we employed the default setting of 50 inference steps.\nStrength of Paraphrase The strength of paraphrasing in visual paraphrasing, ranging from 0 to 1, determines the extent to which the original image's features are preserved versus the introduction of new variations. Achieving the right balance is crucial to ensure that the paraphrased image remains semantically consistent with the original while varying certain attributes effectively, as outlined in the following points:\n\u2022 A higher strength value allows the model greater creative latitude, enabling it to produce an image that significantly deviates from the original. At a strength value of 1.0, the original image is largely disregarded, resulting in a completely transformed output.\n\u2022 Conversely, a lower strength value maintains closer fi-"}, {"title": "4 Performance with De-Watermarking", "content": "After visually paraphrasing a watermarked image, the next crucial step in evaluation involves answering two key questions: (i) To what extent has the visually paraphrased image distorted the original content? Is the distortion too severe to be acceptable, or does it remain within an acceptable range? (ii) How effectively has the paraphrased image removed the watermark from the original image?\nSemantic Distortion Semantic distortion refers to the extent to which visual paraphrasing alters the original meaning or content of an image. To quantify this, we employed the continuous Metric Matching Distance (CMMD) score (Jayasumana et al. 2024), which measures the similarity between the original and paraphrased images. Figure 5 includes a comparison of CMMD scores across various paraphrasing strengths and guidance scale values, illustrating the trade-off between de-watermarking effectiveness and semantic preservation. Our analysis reveals a complex relationship: low-strength paraphrasing typically results in minimal semantic distortion but is less effective at removing watermarks. As paraphrasing strength increases, we observe more successful watermark removal but at the cost of increased semantic distortion. The optimal balance point varies depending on the specific image content and watermarking technique employed. An extended version of Figure 5, which includes all discussed watermarking techniques, is provided in the appendix as Figure 8.\nDetectability Rate: The detectability rate is a crucial metric in assessing the effectiveness of watermark detection methods after visual paraphrasing. Our experiments reveal a clear inverse relationship between the strength of visual paraphrasing and the detectability of watermarks. As the intensity of paraphrasing increases, we observe a significant decline in the ability to detect and extract the original watermarks. This trend is consistent across various watermarking techniques, though some algorithms demonstrate more resilience than others. Detail results are presented in Table 1.\nExperiment Setup: For each attack, we report the watermark probability post-attack. Additionally, we determine the success of watermark detection by applying a threshold on the obtained probability. These threshold values were derived from the original publications of each watermarking method. The results are reported in Table 1. For methods that embed the watermark in the image generation process, such as Tree-Ring and Gaussian Shading, the given captions in the subset were used to generate new watermarked images using Stable Diffusion XL (Podell et al. 2023b). All watermarking methods were tested at their default settings as specified in the original publications."}, {"title": "4.1 Visual Paraphrasing vs. Information Loss", "content": "While we have already discussed measuring semantic distortion using the CMMD score, we critically contend that CMMD may have limitations in capturing significant information loss. With this consideration in mind, we designed a human annotation task. The objective of this task is to obtain annotations from human users regarding the acceptability of these automatically paraphrased images. Furthermore, as previously discussed, there are two controlling factors in visual paraphrasing, namely, the strength of the paraphrase and the guiding scale. A pertinent question arises: Are there upper limits on these two parameters that should not be exceeded, beyond which the generated paraphrases start to exhibit excessive distortion?"}, {"title": "5 Conclusion", "content": "In this study, we empirically demonstrate that existing image watermarking techniques are fragile and susceptible to circumvention via visual paraphrase attacks. To facilitate further research, we are releasing the first-of-its-kind visual paraphrase dataset, along with the accompanying code for all state-of-the-art watermarking methods. This work underscores the urgent need for the scientific community to prioritize the development of more robust watermarking strategies. We anticipate that this research will serve as a benchmark for future efforts to create watermarking methods resilient to visual paraphrase attacks."}, {"title": "6 Ethical Considerations", "content": "The development of visual paraphrasing methods that can bypass state-of-the-art watermarking techniques raises important ethical considerations. While our research aims to advance image processing and improve watermarking resilience, we acknowledge the potential for misuse, such as unauthorized removal of watermarks from copyrighted images. To mitigate these risks, we will responsibly disclose our findings to stakeholders, restrict access to our methodologies and tools to legitimate entities, and advocate for the establishment of ethical guidelines for the use of visual paraphrasing tools. Our goal is to conduct research that aligns with the highest ethical standards, promotes collaborative improvements in watermarking technologies, and respects intellectual property rights and broader societal values."}, {"title": "Frequently Asked Questions (FAQs)", "content": "* How did you determine the optimal combination of paraphrase strength (s) and guiding scale (gs), given the multiple possibilities, such as higher s with higher gs, or other variations?\nWe conducted a series of rigorous experiments to explore various combinations of paraphrase strength (s) and guiding scale (gs), shown in Figure 8. By systematically varying these parameters, we were able to identify the configurations that produced the highest Mean Opinion Scores (MOS) for paraphrase acceptability.\n* Is the optimal combination of paraphrase strength (s) and guiding scale (gs) dependent on the model?\nYes, the optimal combination of paraphrase strength (s) and guiding scale (gs) can vary depending on the model. Different models have unique architectures and training data, which influence how they respond to variations in these parameters. Therefore, fine-tuning these settings for each specific model is crucial to achieving the best balance between maintaining image semantics and ensuring high visual quality.\n* Why Gaussian Shading is the most resilient towards Visual Paraphrase attack? What we learn from it?\n\u27a1Gaussian Shading is the most resilient to visual paraphrase attacks because it smooths out high-frequency details and textures in an image, which are typically exploited in such attacks to alter the visual appearance while preserving recognizability. By applying Gaussian shading, the image becomes less susceptible to small perturbations and subtle modifications, which are commonly used in visual paraphrase attacks to create misleading variations. This resilience teaches us that the robustness of image processing techniques can be significantly enhanced by focusing on reducing the sensitivity to fine details and focusing on the broader, less granular features of the image, thus improving the security and reliability of image recognition systems.\n* Why did you compare only six methods?\nWe have focused on methods that have demonstrated strong performance in recent literature, particularly emphasizing dynamic approaches. While we have mostly omitted older static watermarking methods, we have included results on the DwtdctSvd technique due to its popularity and relevance compared to others within the same category.\n* On average, at what values of strength and guidance scale does the generated image deviate significantly from the original image?\nThe generated image begins to deviate significantly from the original when the strength value exceeds 0.8. Similarly, notable deviations occur when the guidance scale is set to values below 4 or above 13. These settings allow the model more flexibility, leading to greater alterations in the image's appearance while potentially straying from the original content and context.\n* You use KOSMOS-2 for caption generation. How would the performance of the visual paraphrasing attack be affected if a different captioning model was used, especially one with varying levels of detail and accuracy?\n\u27a1While KOSMOS-2 is a strong performer, different captioning models could indeed influence the attack's effectiveness. A less detailed caption might lead to more significant semantic distortion during the visual paraphrasing process, potentially hindering the removal of the watermark. Conversely, a highly accurate and detailed caption could improve the attack by providing more precise guidance to the image-to-image diffusion model, leading to better preservation of semantic content while still removing the watermark. Further research could explore the impact of various captioning models with varying levels of accuracy and detail on the success of visual paraphrasing attacks.\n* The paper focuses on diffusion-based watermarking techniques. How do you think your visual paraphrasing attack would perform against GAN-based watermarking methods, or those that employ steganographic techniques in the spatial domain?\nIn this paper our focus was on diffusion models due to their prominence in current watermarking research. GAN-based or spatial domain watermarking techniques might exhibit different vulnerabilities. GAN-based methods could be more robust due to their adversarial training nature, potentially making it harder to generate paraphrases that both remove the watermark and maintain image fidelity. Spatial domain techniques might be vulnerable to subtle pixel manipulations introduced during visual paraphrasing. Further investigation is needed to assess the effectiveness of our attack against these alternative watermarking approaches.\n* You primarily evaluate the attack based on CMMD and detectability. Are there other metrics, especially those focused on perceptual similarity or specific watermarking features, that could provide a more comprehensive evaluation?\nCMMD and detectability provide a good starting point, but other metrics could enhance the evaluation. Perceptual similarity metrics like LPIPS (Learned Perceptual Image Patch Similarity) could capture subtle differences in visual appearance missed by CMMD. Analyzing specific watermarking features, like frequency distribution changes or alterations in specific latent space dimensions, could offer more granular insights into the attack's impact. Incorporating these additional metrics would provide a richer understanding of the attack's efficacy.\n* You mention the potential for adversarial training to improve watermarking robustness. Can you elaborate on how adversarial training could be specifically tailored to defend against visual paraphrasing attacks?\nAdversarial training could be a powerful defense mechanism. We envision training the watermarking encoder and decoder against a dataset of visually paraphrased images. This would expose the model to the types of perturbations introduced by our attack, forcing it to learn more robust embedding strategies. The training process could involve generating paraphrases using different strengths and guidance scales to ensure generalization across a variety of attack parameters.\n* The paper acknowledges the ethical implications of visual paraphrasing. What specific measures, beyond responsible disclosure, can be taken to prevent the misuse of this technique for malicious purposes like copyright infringement?\nBeyond responsible disclosure, we could explore incorporating \"detection mechanisms\" within the visual paraphrasing tool itself. This could involve training a classifier to identify watermarked images and either prevent their paraphrasing or add a persistent notification indicating potential copyright protection. Another avenue could be developing a collaborative platform where researchers can share newly developed watermarking techniques and test their resilience against visual paraphrasing, fostering a continuous improvement cycle in watermarking robustness.\n* The paper claims that visual paraphrasing is a novel approach to remove watermarks. However, image editing techniques like inpainting and masking have been around for a while. How does visual paraphrasing differ from these existing techniques?\n\u27a1While inpainting and masking can be used to remove visible watermarks, they often leave noticeable artifacts or require precise manual intervention. Visual paraphrasing, on the other hand, leverages the capabilities of image-to-image diffusion models to generate visually similar images guided by a text caption. This process aims to preserve the semantic content while subtly altering the image, making it more challenging to detect and remove the watermark. It can achieve a higher level of realism and detail compared to inpainting and masking while being less susceptible to detection.\n* The paper mainly focuses on visual paraphrasing with Stable Diffusion. Have the authors explored the efficacy of other image-to-image diffusion models or other generative Al models for this task?\nOur paper primarily uses Stable Diffusion for its established capabilities and accessibility. While we acknowledge the potential of other image-to-image diffusion models and generative AI systems for visual paraphrasing, we haven't yet extensively tested them. This is a potential area for future research, examining the effectiveness of different models for watermark removal and the potential impact of different model architectures on the results."}, {"title": "7 Appendix", "content": "This section provides supplementary material in the form of additional examples, implementation details, etc. to bolster the reader's understanding of the concepts presented in this work.\n7.1 Stripping Metadata\nWhile attaching metadata to images is one proposed method for identifying AI-generated content, this approach is vulnerable to simple removal techniques. Here we demonstrate how easily metadata can be stripped from image files, rendering this method ineffective for long-term content attribution.\nRemoving Metadata Using ExifTool To illustrate the simplicity of metadata removal, we'll use ExifTool (Harvey 2024), a popular and freely available command-line application for reading, writing, and editing metadata in various file types, including images.\n1. Consider an AI-generated image with embedded metadata identifying its source.\n$ exiftool sample_image.jpg\nFile Name\n: sample_image.jpg\nFile Size\n: 2.5 MB\nFile Type\n: JPEG\nAI Generator\n: Midjourney v5\nCreation Time\n: 2023:08:15 14:30:22\nImage Width\n: 1024\nImage Height\n: 1024\n2. Using the -al1= ExifTool command, we can strip all metadata from the image:\n$ exiftool -all= sample_image.jpg\n1 image files updated\n3. Checking the image again, we see that all metadata has been removed:\n$ exiftool sample_image.jpg\nFile Name\n: sample_image.jpg\nFile Size\n: 2.5 MB\nFile Type\n: JPEG\nImage Width\n: 1024\nImage Height\n: 1024\nThis demonstration shows that with a single command, all identifying metadata can be eliminated from an image file. The process is quick, requires no specialized knowledge, and can be easily automated for batch processing."}, {"title": "7.7 An Interesting Observation \u2013 Fourier behaviors", "content": "Figure 10 illustrates the watermark patterns embedded in the Fourier space by various methods. Notably, the tree ring and zodiac watermark methods display distinct and recognizable characteristics in this domain, which are not observed in the Gaussian shading watermark. The exact contribution of these characteristics to the resilience of the watermarks against paraphrase attacks remains unclear at this stage. Further investigation into how these specific features contribute to watermark robustness will be a focus of our future work."}]}