{"title": "The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks", "authors": ["Niyar R Barman", "Krish Sharma", "Ashhar Aziz", "Shashwat Bajpai", "Shwetangshu Biswas", "Vasu Sharma", "Vinija Jain", "Aman Chadha", "Amit Sheth", "Amitava Das"], "abstract": "The rapid advancement of text-to-image generation systems, exemplified by models like Stable Diffusion, Midjourney, Imagen, and DALL-E, has heightened concerns about their potential misuse. In response, companies like Meta and Google have intensified their efforts to implement watermarking techniques on AI-generated images to curb the circulation of potentially misleading visuals. However, in this paper, we argue that current image watermarking methods are fragile and susceptible to being circumvented through visual paraphrase attacks. The proposed visual paraphraser operates in two steps. First, it generates a caption for the given image using KOSMOS-2, one of the latest state-of-the-art image captioning systems. Second, it passes both the original image and the generated caption to an image-to-image diffusion system. During the denoising step of the diffusion pipeline, the system generates a visually similar image that is guided by the text caption. The resulting image is a visual paraphrase and is free of any watermarks. Our empirical findings demonstrate that visual paraphrase attacks can effectively remove watermarks from images. This paper provides a critical assessment, empirically revealing the vulnerability of existing watermarking techniques to visual paraphrase attacks. While we do not propose solutions to this issue, this paper serves as a call to action for the scientific community to prioritize the development of more robust watermarking techniques. Our first-of-its-kind visual paraphrase dataset and accompanying code are publicly available.", "sections": [{"title": "Watermarking AI-Generated Images: The Necessity", "content": "With the rapid proliferation of AI-generated visual content from models such as Stable Diffusion (Rombach et al. 2022a; Podell et al. 2023a), DALL-E (Ramesh et al. 2021, 2022), Midjourney (Holz 2022), Imagen (Saharia et al. 2022), among others, and their dangerous potential for misuse by malicious actors, the field of image watermarking has become a critical area of research. Given that, as of 2020, approximately 3.2 billion images and 720,000 hours of video are uploaded to social media platforms daily (T.J. Thomson 2020), the volume of visual content is staggering. When considering how AI-generated visuals can significantly contribute to misinformation strategies by serving as deceptive evidence for fabricated anomalies, the demand for robust watermarking techniques for AI-generated content becomes more pressing than ever. Governments worldwide have initiated discussions and implemented measures to develop policies concerning AI systems. The European Union (European-Parliament 2023) has taken a decisive step by enacting legislation, while the United States (White-House 2023) and other countries have introduced preliminary proposals for a regulatory framework for AI. A primary concern among policymakers is that \"Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality\" (Janjeva et al. 2023). Moreover, AI policymakers have raised significant concerns regarding the use of automatic labeling or invisible watermarks as technical solutions to the challenges posed by generative AI-enabled disinformation. Nevertheless, persistent concerns remain about the susceptibility of these measures to deliberate tampering and the potential for malicious actors to circumvent them entirely.\nIn response to the increasing concern over AI-generated misinformation, companies such as Meta, Google, and OpenAI have begun exploring methods to watermark their generated image content. Meta recently announced its strategy (Fernandez et al. 2023a) to address AI-generated misinformation, emphasizing three primary approaches: (i) the inclusion of visible markers on images, (ii) the application of invisible watermarks, and (iii) the embedding of metadata within image files. This paper contends that these strategies are inadequate in the context of advanced generative Al systems. For example, with the rapid progression of image inpainting systems (Jeevan, Kumar, and Sethi 2023; Zheng et al. 2022; Li et al. 2022; Wang, Yu, and Zhang 2022), detecting and removing visible markers has become increasingly straightforward, as illustrated in Figure 2. Similarly, metadata, which comprises additional tags, can be easily stripped from files using a simple wrapper, as demonstrated in the detailed example provided in Appendix 7.1.\nWatermarking techniques originated within the computer vision community; however, recent advancements in LLMS have spurred interest in the development of text watermarking methods. Last year, OpenAI alluded to the development"}, {"title": "Related Work: State-of-the-art Image Watermarking and Detection Methods", "content": "Watermarking techniques are broadly classified into two categories: (i) static (i.e., learning-free) watermarking methods and (ii) learning-based (dynamic) watermarking methods. Static watermarking refers to embedding a watermark into an image in a fixed, unchanging manner. Once the watermark is embedded, it remains the same regardless of any subsequent use or manipulation of the image. Dynamic watermarking, on the other hand, refers to a more flexible approach where the watermark can change or adapt based on certain conditions or during the image's usage. This type of watermarking is often used in scenarios where the watermark needs to convey additional information, such as the time of access, user identity, or location, and can be embedded in real-time. Dynamic watermarking can be more difficult to detect and remove because the watermark isn't static or predictable."}, {"title": "Static Watermarking Methods", "content": "The most common way of creating a static watermark is to apply some type of Frequency domain transform and then altering certain frequency coefficients of the image or its image blocks via adding a bit of the watermark. The watermarked image is obtained via inverse transform of this transformed image. like Discrete Wavelet Transform (DWT) (Lai and Tsai 2010) to decompose an image into several frequency sub-bands, then applying another transform like the Discrete Cosine Transform (DCT) (Yuan et al. 2020) to each block of some of the sub-bands, and finally altering certain frequency coefficients of each block via adding a bit of the watermark. The watermarked image is obtained via inverse transform. We won't study these methods further in this work due to these approaches being extremely easy to detect and very outdated, except for DwtDctSVD (Navas et al. 2008b), included solely for academic comparison."}, {"title": "DwtDctSVD", "content": "The DwtDctSVD (Navas et al. 2008b) watermarking algorithm uses various techniques to embed a watermark into an image, including Discrete Wavelet Transform (DWT), Discrete Cosine Transform (DCT), and Singular Value Decomposition (SVD). These methods decompose the image into frequency bands, allowing the watermark to be embedded in specific regions that are less prone to common image processing operations. The watermark is embedded in middle-frequency bands to balance robustness and imperceptibility. However, the watermark can be removed or degraded by manipulating the target frequency bands through filtering or compression, altering the singular values obtained from SVD, or applying visual paraphrasing techniques such as random pixel swapping or contrast changes. These methods can destroy or weaken the watermark, rendering it less effective or totally removed."}, {"title": "Learning-based Watermarking Methods", "content": "A typical learning based watermarking method has three key components: watermark (w), encoder (E), and decoder (D). An encoder takes an image X and watermark w as inputs and produces an watermarked image (Xw). So, $Xw = E(X, w)$ and a decoder takes Xw as an input and produces $\\hat{w} = D(Xw)$. $\\hat{w}_{i} = [\\hat{W}_{i} \\geq \\tau]$, where [.] represents the indicator function and \u03c4 is a threshold value we decide based on the problem requirements.\nThe following paragraphs describe the five state of the art learning based watermarking techniques we selected for comparison with visual paraphrasing."}, {"title": "HiDDen: A Watermarking Method for Images", "content": "The HiDDen paper (Zhu et al. 2018) proposes a watermarking technique where an encoder embeds a secret message into a cover image, which is then noised and decoded to retrieve the message. To ensure robustness, the encoder and decoder are trained to minimize losses related to image similarity, message accuracy, and adversarial detection. However, the method has weaknesses that can be exploited, such as the noise layer's impact on the encoded message and the complexity of balancing multiple loss functions. Visual paraphrasing, which alters the image while preserving its semantic content, can manipulate these weaknesses to distort the encoded message or make the watermark undetectable."}, {"title": "Stable Signature", "content": "The Stable Signature method (Fernandez et al. 2023b) introduces a novel watermarking technique for images generated by latent diffusion models (LDMs) (Rombach et al. 2022b), building on the process of progressively denoising a latent image representation. Watermarking is achieved by subtly modifying this latent representation in a way that remains invisible to the human eye but can be detected by a pretrained watermark extractor network. The core of the technique lies in refining the LDM decoder to produce images that exhibit a specific signature when analyzed by the watermark extractor. This involves minimizing a loss function that balances the reconstruction loss, which measures the difference between the generated and target images, and the watermark loss, which gauges the discrepancy between the generated image's signature and the desired watermark signature, controlled by a hyperparameter \u03bb (Gower et al. 2019). The method employs both standard training using SGD and adversarial training to enhance robustness against post-processing."}, {"title": "Tree Ring Watermark", "content": "The proposed tree-ring watermarking technique (Wen et al. 2023) embeds a watermark into the frequency domain of the initial noise vector using Fast Fourier Transform (FFT) (Heckbert 1995), followed by a diffusion process. To detect the watermark, the inverse diffusion process is applied, and an Inverse Fast Fourier Transform (IFFT) (Heckbert 1995) is performed. The L1 distance between the inverted noise vector and the key in the Fourier space is then compared to determine if the image is watermarked. Any attempts to disrupt the watermark through frequency manipulation or adversarial attacks result in loss of image details, rendering the image unusable. This approach aims to retain the image's essence while allowing for changes to the pixel values, similar to paraphrasing in text."}, {"title": "ZoDiac Watermarking", "content": "ZoDiac (Zhang et al. 2024) is a zero-shot watermarking technique that utilizes pre-trained diffusion models to embed watermarks into images while maintaining visual similarity. The method consists of three main steps: initializing a trainable latent vector using the DDIM inversion process (Song, Meng, and Ermon 2022) to reproduce the original image, encoding a concentric ring-like watermark into the latent vector's Fourier space and refining it using a custom reconstruction loss, and adaptively enhancing the visual quality of the watermarked image by mixing it with the original image to meet a desired quality threshold. Unlike tree-ring watermarking, ZoDiac can be used to watermark existing images, making it a versatile and effective watermarking technique."}, {"title": "Gaussian Shading", "content": "The Gaussian Shading watermarking method (Yang et al. 2024) offers a performance-lossless approach to embedding watermarks in images generated by diffusion models by operating entirely within the latent space, preserving the statistical distribution of latent representations. The process involves randomizing the watermark W using a stream cipher like ChaCha20 (Bernstein 2008) to create an encrypted watermark W', which is then embedded into the latent space z during the diffusion process through the equation $z' = z + \\sigma \\cdot W'$, where \u03c3 is a scaling factor. This technique ensures that the quality of watermarked images is indistinguishable from non-watermarked ones, supporting high watermark capacity and robustness against attacks such as noise and lossy compression.\nIn addition to the six techniques previously mentioned, the method proposed in DeepMind (2023) appears promising. However, due to the unavailability of its code, we are unable to include it in our study."}, {"title": "Traditional De-Watermarking Techniques", "content": "In addition to the discussed watermarking methods, certain traditional image alteration techniques can also function as de-watermarking attacks, as explored by previous researchers. We have included the following techniques in our study for comparison purposes."}, {"title": "Brightness", "content": "Altering the brightness (Verma, Singh, and Kumar 2009) of an image is a simple yet effective method for attempting to reduce the visibility of watermarks. By increasing or decreasing the brightness, the contrast between the watermark and the underlying image can be diminished, making the watermark less noticeable. However, this method can also degrade the overall quality of the image, potentially affecting important visual details. For our experiments, we selected a brightness level increased by a factor of 2."}, {"title": "Rotation", "content": "Rotating (Luo et al. 2022) an image is another technique used to obscure watermarks, especially those that are positioned in a fixed location. By rotating the image, the watermark may be repositioned to an area where it is less visible or more easily cropped out. While rotation can effectively reduce watermark visibility, it can also distort the original image content, particularly if the rotation angle is significant. For our experiments, the images were rotated by \u00b145\u00b0."}, {"title": "JPEG Compression", "content": "JPEG compression (Jia, Fang, and Zhang 2021) is a common technique that reduces the file size of an image by discarding some of its data, which can incidentally affect the visibility of watermarks. The lossy nature of JPEG compression can blur or distort the watermark, making it less discernible. However, this technique may also lead to a loss of image quality, particularly when high compression levels are used. For our experiments, we set the quality setting to a reduced level of 50."}, {"title": "Gaussian Noise", "content": "Adding Gaussian noise (Li et al. 2024) to an image is a method that introduces random variations in pixel intensity, which can help in reducing the clarity of watermarks. The noise can obscure the fine details of the watermark, blending it into the background. While this approach can be effective, it may also degrade the visual quality of the image, making it appear grainy or less sharp. In our experiments, noise with a standard deviation of 0.05 was added to the images."}, {"title": "Visual Paraphrasing", "content": "Paraphrasing is a well-established area of research within natural language processing (NLP). For instance, sentences such as \"What is your age?\" and \"How old are you?\" convey identical meanings despite their differing linguistic structures, thus constituting paraphrases of each other. In contrast, the concept of visual paraphrasing has not been as extensively explored, likely due to the recent emergence of text-to-image generation systems such as Stable Diffusion and Midjourney.\nThese systems are capable of producing slight variations of a given image that maintain the same semantic content while differing in visual presentation. A related concept is visual entailment, which concerns image-sentence pairs where the image serves as the premise, as opposed to a sentence in traditional Visual Entailment tasks (Xie et al. 2019). The objective in visual entailment is to determine whether the image semantically supports the text. However, given the significant differences between visual entailment and visual paraphrasing, this discussion will not explore visual entailment further. For example, as illustrated in Figure 4, all generated images are visual paraphrases of the input image.\nThe process of visual paraphrasing begins with the generation of a caption for the image, followed by the application of image-to-image diffusion techniques. This two-step approach ensures that the output images retain the semantic integrity of the original while allowing for variations in visual presentation. The effectiveness of visual paraphrasing is governed by adjusting two key parameters: paraphrase strength and guiding scale, as described below."}, {"title": "Generating Caption", "content": "When an image encountered in the wild is suspected to have been generated by AI, the original prompt used to create it is typically unavailable. To address this challenge, we employed KOSMOS 2 (Peng et al. 2023) to generate a textual description or a brief caption of the image. KOSMOS 2, along with other image captioning models (You et al. 2016), is particularly effective at producing detailed textual descriptions of images. This generated caption then serves as the textual conditioning input for the image-to-image diffusion models, which are discussed in the following section. By utilizing the extracted textual context as guidance, the diffusion model reconstructs the image while preserving its semantic content, thereby achieving visual paraphrasing."}, {"title": "Image-to-Image Diffusion", "content": "At the core of visual paraphrasing lies the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). This technique, employed in generative models, transforms images while maintaining their underlying structure and semantic information. The diffusion process involves two key stages: the forward diffusion process and the reverse diffusion process. In the forward diffusion process, an image is gradually corrupted by adding noise, eventually reaching a state of complete noise. Mathematically, this process is described as follows: $x_{t} = \\sqrt{A_{t}}x_{t-1} + \\sqrt{1 - a_{t}}\\epsilon_{t}$, where $x_{t}$ is the image at time step t, $a_{t}$ is a noise scaling factor, and $\\epsilon_{t}$ is the noise sampled from a Gaussian distribution. In the reverse diffusion process, the model attempts to remove the noise step by step, reconstructing the original image from the noisy version. This is achieved using a learned denoising function $\\epsilon_{\\theta}$: $x_{t-1} = \\frac{1}{\\sqrt{A_{t}}} (x_{t}-\\sqrt{1 - A_{t}}\\epsilon_{\\theta}(x_{t}, t))$. This iterative denoising continues until the model produces an image that closely resembles the original in both visual and semantic terms. In this context, two controls are utilized: (i) the original image and (ii) the generated caption. The number of inference steps, denoted by T, is a critical factor in this process. Increasing the number of steps generally results in more refined reconstructions, yielding higher-quality images, albeit with greater computational demands. In this scenario, we employed the default setting of 50 inference steps."}, {"title": "Strength of Paraphrase", "content": "The strength of paraphrasing in visual paraphrasing, ranging from 0 to 1, determines the extent to which the original image's features are preserved versus the introduction of new variations. Achieving the right balance is crucial to ensure that the paraphrased image remains semantically consistent with the original while varying certain attributes effectively, as outlined in the following points:\n\u2022 A higher strength value allows the model greater creative latitude, enabling it to produce an image that significantly deviates from the original. At a strength value of 1.0, the original image is largely disregarded, resulting in a completely transformed output.\n\u2022 Conversely, a lower strength value maintains closer fidelity to the original image, preserving much of its details."}, {"title": "Guidance Scale", "content": "The guidance scale parameter determines the extent to which the generated image aligns with the details specified in the text prompt. This parameter plays a crucial role when the paraphrasing process is guided by textual descriptions, as it regulates the balance between strict adherence to the prompt and permitting creative variations, as demonstrated by the following points:\n\u2022 A higher guidance scale value ensures that the generated image closely follows the prompt, resulting in an output that is strongly influenced by the provided text.\n\u2022 A lower guidance scale value allows for greater flexibility, enabling the model to deviate from the prompt and introduce more creative variations in the generated image."}, {"title": "Performance with De-Watermarking", "content": "After visually paraphrasing a watermarked image, the next crucial step in evaluation involves answering two key questions: (i) To what extent has the visually paraphrased image distorted the original content? Is the distortion too severe to be acceptable, or does it remain within an acceptable range? (ii) How effectively has the paraphrased image removed the watermark from the original image?"}, {"title": "Semantic Distortion", "content": "Semantic distortion refers to the extent to which visual paraphrasing alters the original meaning or content of an image. To quantify this, we employed the continuous Metric Matching Distance (CMMD) score (Jayasumana et al. 2024), which measures the similarity between the original and paraphrased images. Our analysis reveals a complex relationship: low-strength paraphrasing typically results in minimal semantic distortion but is less effective at removing watermarks. As paraphrasing strength increases, we observe more successful watermark removal but at the cost of increased semantic distortion. The optimal balance point varies depending on the specific image content and watermarking technique employed."}, {"title": "Detectability Rate", "content": "The detectability rate is a crucial metric in assessing the effectiveness of watermark detection methods after visual paraphrasing. Our experiments reveal a clear inverse relationship between the strength of visual paraphrasing and the detectability of watermarks. As the intensity of paraphrasing increases, we observe a significant decline in the ability to detect and extract the original watermarks. This trend is consistent across various watermarking techniques, though some algorithms demonstrate more resilience than others."}, {"title": "Experiment Setup", "content": "For each attack, we report the watermark probability post-attack. Additionally, we determine the success of watermark detection by applying a threshold on the obtained probability. These threshold values were derived from the original publications of each watermarking method. The results are reported in Table 1. For methods that embed the watermark in the image generation process, such as Tree-Ring and Gaussian Shading, the given captions in the subset were used to generate new watermarked images using Stable Diffusion XL (Podell et al. 2023b). All watermarking methods were tested at their default settings as specified in the original publications."}, {"title": "Visual Paraphrasing vs. Information Loss", "content": "While we have already discussed measuring semantic distortion using the CMMD score, we critically contend that CMMD may have limitations in capturing significant information loss. With this consideration in mind, we designed a human annotation task. The objective of this task is to obtain annotations from human users regarding the acceptability of these automatically paraphrased images. Furthermore, as previously discussed, there are two controlling factors in visual paraphrasing, namely, the strength of the paraphrase and the guiding scale. A pertinent question arises: Are there upper limits on these two parameters that should not be exceeded, beyond which the generated paraphrases start to exhibit excessive distortion?\nOur research indicates that for strength, the modal MOS is observed at a value of 0.4, signifying optimal acceptability. Conversely, the lowest acceptability is recorded at a MOS of 0.8, where the paraphrases are deemed least acceptable. Regarding the guidance scale, the highest MOS frequencies are noted at values of 1 and 3, which suggests that these settings yield the most acceptable results. In contrast, a guidance scale of 13 results in the least acceptable paraphrases. These observations highlight the essential role of adjusting both strength and guidance scale to enhance the acceptability of paraphrases."}, {"title": "Conclusion", "content": "In this study, we empirically demonstrate that existing image watermarking techniques are fragile and susceptible to circumvention via visual paraphrase attacks. To facilitate further research, we are releasing the first-of-its-kind visual paraphrase dataset, along with the accompanying code for all state-of-the-art watermarking methods. This work underscores the urgent need for the scientific community to prioritize the development of more robust watermarking strategies. We anticipate that this research will serve as a benchmark for future efforts to create watermarking methods resilient to visual paraphrase attacks."}, {"title": "Ethical Considerations", "content": "The development of visual paraphrasing methods that can bypass state-of-the-art watermarking techniques raises important ethical considerations. While our research aims to advance image processing and improve watermarking resilience, we acknowledge the potential for misuse, such as unauthorized removal of watermarks from copyrighted images. To mitigate these risks, we will responsibly disclose our findings to stakeholders, restrict access to our methodologies and tools to legitimate entities, and advocate for the establishment of ethical guidelines for the use of visual paraphrasing tools. Our goal is to conduct research that aligns with the highest ethical standards, promotes collaborative improvements in watermarking technologies, and respects intellectual property rights and broader societal values."}, {"title": "Appendix", "content": "This section provides supplementary material in the form of additional examples, implementation details, etc. to bolster the reader's understanding of the concepts presented in this work."}, {"title": "Stripping Metadata", "content": "While attaching metadata to images is one proposed method for identifying AI-generated content, this approach is vulnerable to simple removal techniques. Here we demonstrate how easily metadata can be stripped from image files, rendering this method ineffective for long-term content attribution."}, {"title": "Removing Metadata Using ExifTool", "content": "To illustrate the simplicity of metadata removal, we'll use ExifTool (Harvey 2024), a popular and freely available command-line application for reading, writing, and editing metadata in various file types, including images.\n1. Consider an AI-generated image with embedded metadata identifying its source.\n2. Using the -al1= ExifTool command, we can strip all metadata from the image:\n3. Checking the image again, we see that all metadata has been removed:\nThis demonstration shows that with a single command, all identifying metadata can be eliminated from an image file. The process is quick, requires no specialized knowledge, and can be easily automated for batch processing."}]}