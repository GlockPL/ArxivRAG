{"title": "Adversarial Attacks on Machine Learning-Aided Visualizations", "authors": ["Takanori Fujiwara", "Kostiantyn Kucher", "Junpeng Wang", "Rafael M. Martins", "Andreas Kerren", "Anders Ynnerman"], "abstract": "Research in ML4VIS investigates how to use machine learning (ML) techniques to generate visualizations, and the field is rapidly growing with high societal impact. However, as with any computational pipeline that employs ML processes, ML4VIS approaches are susceptible to a range of ML-specific adversarial attacks. These attacks can manipulate visualization generations, causing analysts to be tricked and their judgments to be impaired. Due to a lack of synthesis from both visualization and ML perspectives, this security aspect is largely overlooked by the current ML4VIS literature. To bridge this gap, we investigate the potential vulnerabilities of ML-aided visualizations from adversarial attacks using a holistic lens of both visualization and ML perspectives. We first identify the attack surface (i.e., attack entry points) that is unique in ML-aided visualizations. We then exemplify five different adversarial attacks. These examples highlight the range of possible attacks when considering the attack surface and multiple different adversary capabilities. Our results show that adversaries can induce various attacks, such as creating arbitrary and deceptive visualizations, by systematically identifying input attributes that are influential in ML inferences. Based on our observations of the attack surface characteristics and the attack examples, we underline the importance of comprehensive studies of security issues and defense mechanisms as a call of urgency for the ML4VIS community.", "sections": [{"title": "1 Introduction", "content": "Visualizations play a critical role in depicting relationships or patterns within an underlying dataset such that analysts can effectively explore, interact, and communicate the data. Recently, researchers are actively investigating various applications of using machine learning (ML) techniques when generating visualizations optimized for certain tasks (e.g., chart recommendation [1], text-chart translation [2], and interaction prediction [3]). This approach is often referred to as ML for visualization, or ML4VIS for short [4]. Despite the wide usage, we argue that the current research focus on ML4VIS is imbalanced. The majority of ML4VIS research mainly focuses on the benefits these techniques provide [4-6], and consequently largely overlooks the security issues these techniques introduce.\nCritically understanding security issues related to ML4VIS requires a more holistic perspective that is both visualization- and ML-oriented. This dual perspective is necessary as the joint use of ML and visualization techniques may cause additional security problems that are not prevalent in each respective field. However, relevant research investigations on these matters are currently disconnected. For example, though some visualization research has explored the vulnerabilities of visualization approaches, the focus of these investigations targets more general visualization practices (e.g., the influence of the choice of the aspect ratio on the recognition of correlation patterns [7, 8]). These insights do not consider the specific security issues that ML approaches introduce (e.g., manipulating data transformation when generating ML-aided visualizations). Similarly, though the ML community is intensively studying the vulnerability of"}, {"title": "2 Background and Related Work", "content": "Addressing security issues in ML-aided visualizations requires a dual set of ML and visualization considerations. We describe relevant works related to adversarial attacks and vulnerabilities from ML and visualization research. We also discuss a general overview of ML-aided visualizations."}, {"title": "2.1 Adversarial Attacks on Machine Learning Models and Defenses", "content": "With ML techniques actively used in real-world settings, Dalvi et al. [17] posits a critical research agenda that can address the issue of how \"the presence of an adversary actively manipulating the data [can] defeat the data miner\". As deep NNs are rapidly being used in various domains, such as vision and speech recognition, a significant portion of ML research is devoted to addressing security issues in NNs. One notable early result is an adversarial example designed for convolutional NN (CNN) models by Szegedy et al. [18]. They demonstrated that an adversarial example can be easily constructed by adding a human-imperceptible perturbation into an input image. This perturbation can readily cause image misclassification even with state-of-the-art CNN models. These adversarial examples pose critical issues in real-world settings where, for example, an adversary may craft a stop traffic sign that an autonomous car will misclassify as a yield sign [19]. Researchers have since studied a multitude of efficient and intense adversarial examples on CNNs.\nAdversarial examples on CNNs can be generally categorized as either white-box attacks or black-box attacks depending on the adversary's knowledge about the model of interest. When an adversary knows detailed information about a target CNN model (i.e., white-box attacks), they can efficiently construct adversarial examples by referring to the gradients of the model's loss function [20, 21]. In contrast, black-box attacks are when the adversary has limited information available on a target model. In these cases, the adversary generally has two strategies. First, an adversary can build a substitute model that performs the same classification task as the target model. The substitute model can then be used to generate adversarial examples like in white-box attacks [18]. Another common black-box attack strategy is to reverse engineer a target model from the collected input-output relationships by sending queries to the model [19]. Research continues to highlight new attack methods that have grown and diversified for other NNs, such as recurrent NNs (RNNs) and graph NNs (GNNs) [22].\nBesides crafting adversarial examples, data poisoning [23] (i.e., adding malicious training data) is another effective way to corrupt the model as exemplified by the problematic tweets made by Microsoft's chatbot Tay [24]."}, {"title": "2.2 Vulnerabilities of Visualizations", "content": "Visualizations can become obscure, misleading, and even deceptive as a consequence of poorly prepared data [35], problematic visual designs [36, 37], viewers' cognitive bias [38, 39], or any combinations of these. For example, a visualization with missing value imputations that is not suited for target analysis tasks can decrease viewers' performance [35]. Subtle skews in visualizations, such as 3D effects on pie charts, can lead to wrong conclusions [36]. A viewer's belief in the existence of correlations between two variables (e.g., the numbers of guns and crimes) can also influence the cognition of the correlation strength [39]. The choice of a visual representation in itself can additionally lead to bias in estimating a user's confidence in the presented visual representations [40]. McNutt et al. [8] provided a conceptual model that frames these visualization vulnerabilities along the visual analytics pipeline.\nBy exploiting these visualization vulnerabilities, adversaries can easily create malicious visualizations. Correll and Heer [7] discussed a man-in-the-middle attack on visualizations: an attack from a malicious visualization designer who aims to distract communication between data and viewers. Adversaries (i.e., designers) can intentionally break visualization conventions [41] to create visualizations with problematic designs. Our work argues that even when adversaries do not have such a strong capability of manipulating visualization designs, various practical attacks can be imposed on ML-aided visualizations (see Sec. 3 and 4).\nTo the best of our knowledge, no existing work explicitly studied defense strategies against adversarial attacks on visualizations. Existing works targeted on detecting flaws in visualizations [42], mitigating cognitive biases [43, 44], and testing the robustness of visualizations [8]. For example, Chen et al. [42] developed a linting tool to detect erroneous visual specifications. McNutt et al. [8] introduced metamorphic testing for visualization, which measures how strongly a change to the input data can perturb a visualization outcome. This current state of literature presents security issues that are being overlooked within the visualization community."}, {"title": "2.3 Machine Learning-Aided Visualizations", "content": "Existing works [4-6] provide comprehensive surveys on ML4VIS related to visual analytics, information visualization, and scientific visualization. Based on these surveys, the following facts emphasize the importance of needing future studies regarding adversarial attacks on ML-aided visualizations.\nIncrease of research interest in ML4VIS. Research on ML4VIS is rapidly growing. Until 2015, only a few ML4VIS-related papers were published annually. Since then, the number of ML4VIS-related publications radically increased [4]. This trend indicates an increasing interest in, and need for, using ML for visualization.\nExistence of broad and critical attack targets. ML4VIS approach is employed for various tasks, including data processing for visualization, feature extraction, visualization generation, user behavior prediction, and interpretation support of visualized results. Also, ML4VIS approaches are utilized in life-critical applications, such as 3D medical image analysis and reconstruction [45-47] as well as cancer genome analysis [48]. Because visualization is a fundamental tool to communicate and analyze data, by attacking ML-aided visualizations, adversaries could significantly and negatively impact critical applications in areas such as business, politics, and medicine."}, {"title": "3 Attack Surface of Machine Learning-Aided Visualizations", "content": "We analyze the attack surface (i.e., set of potential entry points for attacks) of ML-aided visualizations. For this analysis, we use Wang et al.'s ML4VIS pipeline [4], which is derived from an extensive survey on ML4VIS approaches, to review the potential processes, inputs, and outputs involved in ML-aided visualizations."}, {"title": "3.1 ML4VIS Pipeline", "content": "As shown in Fig. 1, Wang et al.'s ML4VIS pipeline consists of seven processes that can be individually aided by ML: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, User Profiling, and VIS Reading. Note that we consider that visualizations are ML-aided if they utilize ML for any of these processes. ML does not have to be involved in all the processes. For example, a user may utilize ML for data processing but still manually design the visual encodings to produce a visualization. Below, we briefly describe each process.\nData Processing4VIS prepares data for the subsequent visualization processes. A typical ML method used for this process is dimensionality reduction (DR). This process can use raw data, existing visualizations, or both as input. For example, DR can utilize the previously generated visualization result to perform visually consistent updates [49, 50]. It is worth noting that NN-based DR methods are becoming more actively developed for Data Processing4VIS [51-55].\nData-VIS Mapping encodes processed data into visualizations by assigning visual marks and channels. For this process, ML can recommend marks and channels suitable for a given dataset or accelerate the encoding process. These enhancements are often achieved by training NNs using supervised learning on labeled training data (e.g., data tables and their suitable visual marks) [1, 56, 57].\nInsight Communication aims to produce visualizations that effectively convey insights found in the data. Wang et al. [4] distinguish this process from Data-VIS mapping based on whether the insights are used as inputs. Because insights are frequently represented"}, {"title": "3.2 Characterization of Attack Surfaces", "content": "We consider that all employed processes, inputs, and outputs in Wang et al.'s ML4VIS pipeline compose an attack surface given how adversaries may be able to manipulate inputs, corrupt ML/non-ML processes, and tamper with the output results. Probable attack surfaces across different ML-aided visualizations reveal critical, unique characteristics in ML-aided visualizations:\nC1. ML inputs and outputs specific to visualization. Though existing ML-aided visualizations usually customize and utilize NNs developed from ML research [4-6], the inputs used for the training and inference phases are often specific to the visualization (e.g., [3, 59, 62]). Target outputs and loss functions to produce such visualization outputs can also be unique for ML-aided visualizations. For example, ML-aided visualizations often adapt NNs to analyze visualizations based on their visual marks and channels rather than pixel-based images [56, 57]. Consequently, adversaries might design attack methods for ML-aided visualizations that are significantly different from those studied in the ML field, and existing vulnerability assessment and defense methods for NNs [9, 10] might not be as effective for such attacks.\nC2. Exposure of ML outputs (and inputs) to adversaries. Visualizations are usually intended to be reviewed by users. The processes within the ML4VIS pipeline, such as Data-VIS Mapping, Insight Communication, and Style Imitation, all produce visualizations as their outputs. Thus, when these processes employ ML, the visualizations are inference results that are abundant with information, which will inherently be observed by users as well as adversaries. When the data generated by Data Processing4VIS is visualized without additional processes (e.g., directly visualizing DR results as 2D scatterplots [49, 54]), Data Processing4VIS also suffers from the same issue. Moreover, interactive visualizations often support details-on-demand [64], enabling adversaries to access detailed information of raw or processed input data. This attack surface characteristic can enable adversaries to gain further knowledge of ML models (i.e., contributing to the reconnaissance stage of cyberattacks [65]). In contrast, C1 provides new opportunities to create attacks specific to visualization (i.e., contributing to the weaponization stage [65]).\nC3. Long, complex pipeline with interdependent processes. ML-aided visualizations may involve a large number of interdependent processes. This interdependency introduces additional opportunities for adversaries to amplify a cascade of attacks throughout the pipeline. For instance, adversaries may be able to"}, {"title": "4 Concrete Adversarial Attack Examples", "content": "To showcase possible threats while highlighting the uniqueness of the attack surface, we designed attacks on state-of-the-art ML-aided visualizations [52, 56]. A summary is presented in Table 1. Across the attacks, we make different assumptions that cover different levels of the adversaries' capabilities (e.g., black-box attack vs. white-box attack). The source code used for our attacks is available online [69].\nOur attacks are on two representative methods that focus on different parts of the ML4VIS pipeline: (1) parametric UMAP (PUMAP) [52] for Data Processing4VIS and (2) MultiVision [56] for Data-VIS Mapping. These methods are selected for three reasons. First, we aim to cover multiple different processes in the ML4VIS pipeline so we can highlight all of the identified attack surface"}, {"title": "4.1 Attack Target 1: Parametric DR for Data Processing4VIS", "content": "We first provide the background of the attack target. Nonlinear DR methods, such as t-SNE and UMAP, are commonly used for visual analysis of high-dimensional data [70]. Using NNs, parametric extensions of nonlinear DR methods have been developed (e.g., parametric t-SNE [51, 53] and PUMAP [52]). Unlike its nonparametric counterpart, the parametric method produces a parametric mapping that projects data instances onto a low-dimensional space.\nFig. 2 compares pipelines employed by UMAP and PUMAP. Conventional UMAP first constructs a graph representation of the input high-dimensional data. The subsequent step is an iterative optimization that does not involve NNs. Using this optimization, UMAP layouts the instances onto a low-dimensional space (often in 2D) so that instances similar in the graph representation are spatially proximate (refer to [71] for details). There is no direct, numerical connection between high-dimensional data and its low-dimensional representation. Thus, UMAP does not provide parametric mapping. In contrast, PUMAP feeds high-dimensional data to the MLP's input layer, learns the hidden layers' neuron weights for parametric mapping, and produces low-dimensional coordinates from the output layer. PUMAP"}, {"title": "4.1.1 Attack Manipulating One Attribute Value", "content": "Attack design. For our first attack, using PUMAP as an example, we demonstrate a rudimentary attack that can be performed even with limited adversarial capability. This basic example provides evidence of how"}, {"title": "4.1.2 Attack Using a Substitute Model", "content": "Attack design. When adversaries can observe various combinations of inputs and the corresponding outputs for parametric DR, they can construct a substitute model that can craft adversarial inputs flexibly and effectively. Interactive visualization systems showing DR results have a high likelihood of this situation occurring. These systems often provide training data information on demand to allow users to examine the DR results [74]. Therefore, we assume that it is possible and reasonable that an adversary's goal is to construct a substitute model to the attack target model and generate deceptive visualizations.\nFig. 4 shows our architecture for a substitute model attack. The substitute model learns the parameters that produce a nearly identical low-dimensional representation to the one produced by the attack target model. This learning is achieved by setting a loss function that minimizes the positional differences (e.g., the sum of pairwise Euclidean distances) between the instances in the low-dimensional representations that are produced by the attack target and substitute model. To construct this substitute model, adversaries do not need to know how the attack target model's parametric mapping is learned (e.g., the use of PUMAP). In addition, the substitute model's NN architecture and implementation do not have to be the same as the attack target model's. Adversaries need to only have sufficient neurons/parameters to mimic the attack target's parametric mapping. Since adversaries can access all information of the substitute model (e.g., gradients), they can efficiently craft adversarial inputs so that the inputs are projected at specific positions in the substitute model's low-dimensional representation. Then, adversaries can feed the crafted adversarial inputs to the attack target model, where the inputs would be projected closely to the aimed positions.\nAttack results and analyses. We demonstrate attacks on the same ML model as Sec. 4.1.1 (i.e., PUMAP trained with the Wine dataset). We construct our substitute model with an MLP of three 50-neuron hidden layers, using PyTorch. Note: PUMAP employs TensorFlow and an MLP of three 100-neuron hidden layers.\nWe first showcase how to create adversarial inputs. As shown in Fig. 5-a1, we can create an adversarial input that is projected onto a specific position (e.g., (-2.5, -2.5)) with the substitute model. The adversarial inputs can be optimized by adjusting the input's attribute values based on their gradients. The gradients are related to the error between the aimed and projected positions. When we feed the crafted adversarial input to PUMA\u0420, as shown in Fig. 5-a2, the adversarial input is placed close to the aimed position. Due to the linearity of NNs, we expect that similar placement could be achieved without manipulating many attributes. For example, two"}, {"title": "4.2 Attack Target 2: Chart Recommendation for Data-VIS Mapping", "content": "Chart recommendation is a canonical use of ML for Data-VIS Mapping. Given input data, chart recommendation systems offer suggestions and generate the appropriate charts. By using large datasets of collected charts (e.g., [1]), researchers applied supervised learning to NNs and developed various chart recommendation systems [56, 57].\nWe use MultiVision [56] as a representative example of chart recommendation systems. Given a data table, MultiVision is designed to rank and recommend multiple charts. See Fig. 7 for an overview of MultiVision's"}, {"title": "4.2.1 Knowledge-Based Attack", "content": "Attack design. We perform a simple, but critical attack on Wu et al.'s pre-trained MultiVision [56]. Here, we assume that adversaries know or can guess some basic specifications related to the employed ML model. This assumption fits into cases when adversaries can recognize that the recommendation system uses a similar approach to MultiVision. Even if adversaries partially know the"}, {"title": "4.2.2 Attack Referring to Gradients", "content": "Attack design. When the chart recommendation system is a white box, adversaries can craft adversarial inputs more efficiently. MultiVision employs two different NNs to recommend column sets and chart types respectively. We refer to the gradients for column features toward decreasing the scores corresponding to the highest-ranked chart. Then, we manipulate an input data table to change the column features that have large gradient magnitudes.\nAttack results and analyses. We craft an adversarial input for the Gapminder dataset. As shown in Fig. 8-al, MultiVision originally recommended life_expect (x-axis), fertility (y-axis), and year (line width) as the column set; line chart as the chart type. By accessing the NNs used in MultiVision, we extract the aforementioned gradients to disturb this line chart recommendation."}, {"title": "4.2.3 Attack Propagating across Multiple Processes", "content": "Attack design. This attack focuses on creating adversarial examples for visualizations that utilize NNs for multiple ML4VIS processes. We consider a system that first performs Data Processing4VIS using PUMAP to derive 2D features from the raw data. Then, the subsequent Data-VIS Mapping process employs MultiVision to select the appropriate chart to visualize the 2D features. We assume that when attacking the target system, adversaries can add new instances to the raw data but cannot directly change a data table input used in MultiVision. We also assume that adversaries are capable of accessing a pre-trained MultiVision model (e.g., the pre-trained model available from an online repository). With these two capabilities, adversaries aim to induce misprediction for the chart recommendation.\nSimilar to Sec. 4.2.2, we can refer to the gradients of the pre-trained MultiVision models. However, when considering the first capability of adversaries, we can only indirectly influence attribute values of the input data table. For example, we cannot change the column order and column names. When MultiVision extracts the column features, attribute values (in our case, 2D features from PUMAP) are converted into statistics that are interdependent from each other (e.g., the ratios of negative values and skewness). Consequently, it is not trivial to associate these statistics' gradients with the data table's attributes. Instead, we generate various 2D features with multiple different magnitudes of values and select one that changes the ranks of the chart type or column set scores. Then, to find an adversarial instance that PUMAP transforms to the selected 2D feature, we build and exploit a substitute model of PUMAP by taking the same approach as Sec. 4.1.2."}, {"title": "5 Discussion: Toward Secure Machine Learning-Aided Visualization", "content": "We now discuss the implication of the attacks demonstrated in Sec. 4 as well as highlight potential threats in the real world. We then discuss the insights we gained by studying these concrete attacks. Lastly, we provide a set of suggestions to help further investigate the vulnerabilities of ML-aided visualizations to move toward a more secure use."}, {"title": "5.1 Discussion on the Performed Attack Examples", "content": "Possible critical scenarios in the real world. Although we performed the attacks using demonstrative datasets and ML-aided visualizations, these attacks can be easily applied to various real-life scenarios.\nAdversaries can directly influence NN models used in various domains. For example, in the case where NN-based parametric DR is used to monitor a product in real time [49], adversaries might notice that the product weight has an influence on the projection. This attack would result similarly to the one-attribute attack example. Then, they might physically control the weight to hide any other abnormal product status. They may also intentionally cause problematic x, y-axes scaling in visualizations to conceal any subsequent attacks from"}, {"title": "5.2 Suggestions for Future Research", "content": "Enhance studies on attacks specific to ML-aided visualizations. To develop defense strategies, we first need to analyze possible attacks that exploit the vulnerabilities of ML-aided visualizations. We expect that various attacks can be specific to ML-aided visualizations based on our observations on their attack surface (Sec. 3.2). As a primary step toward a better understanding of ML4VIS' vulnerability, this work demonstrates several critical attacks. As discussed in Sec. 4, only a limited work makes their source code, training and testing datasets, and pre-trained ML models available for the public [69]. In contrast, in the ML field, they are often publicly available given how they are vital to efficiently and accurately study limitations of ML models [20, 21]. Thus, we encourage more visualization researchers to make efforts to provide full accounts of data and software publicly, and thereby enable further analyses of ML-aided visualizations. Also, this immaturity of security studies in ML4VIS indicates our study's limitation. The current characterization of the attack surface is based on an abstract-level analysis and may not reflect a variety of possible real attacks (since they are yet unseen).\nIdentify and inform potential vulnerabilities. Furthermore, we suggest that researchers routinely identify and publicly inform potential vulnerabilities in their ML-aided visualizations. Authors have intimate knowledge of their methods (e.g., designs, algorithms, and datasets). For example, adding discussions on the vulnerabilities in the respective publications would largely benefit risk assessments. If we believe the developed visualizations provide significant value (e.g., having a large number of users and deriving highly valuable knowledge) [80], these discussions are crucial in light of potential threats. Although discussing the vulnerabilities involves a risk of distributing information about attack entries, unsolved critical issues should be reported before the research results are applied to practical applications. In addition to these individual efforts, there is a growing need to develop methods that systematically evaluate vulnerabilities [8], which would reduce the need for time-consuming manual inspections.\nInvestigate the role of human intervention. Lastly, we pose another two open questions: Should"}, {"title": "6 Conclusion", "content": "We systematically reviewed the vulnerabilities of ML-aided visualizations. We described the uniqueness of the attack surface of ML-aided visualizations and demonstrated security risks with five concrete examples of adversarial attacks. Our results show that more research efforts are needed to address security aspects and advance defense strategies against these adversarial attacks.\nThis work also suggests several future research directions, such as investigating diverse adversarial attacks, systematically testing to evaluate the robustness of ML-aided visualizations, and evaluating the human role in defense against adversarial attacks. In addition to pursuing these directions, we suggest future research to study the interrelationships between security and other closely related topics, such as privacy preservation and trust building [79] in ML-aided visualizations. This work contributes as a stepping stone toward a holistic study for both maximizing benefits and minimizing risks in the use of ML for visualizations."}]}