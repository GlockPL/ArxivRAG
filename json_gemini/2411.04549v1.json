{"title": "Vision Language Models are In-Context Value Learners", "authors": ["Yecheng Jason Ma", "Joey Hejna", "Ayzaan Wahid", "Chuyuan Fu", "Dhruv Shah", "Jacky Liang", "Zhuo Xu", "Sean Kirmani", "Peng Xu", "Danny Driess", "Ted Xiao", "Jonathan Tompson", "Osbert Bastani", "Dinesh Jayaraman", "Wenhao Yu", "Tingnan Zhang", "Dorsa Sadigh", "Fei Xia"], "abstract": "Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and advantage-weighted regression \u2013 all without any model training or finetuning.", "sections": [{"title": "1. Introduction", "content": "Predicting temporal progress from visual trajectories is an important task for embodied agents that interact with the physical world. A robot capable of generalizable progress estimation can in principle discern desirable and undesirable behaviors to learn visuomotor skills in new environments. This is most often studied in reinforcement learning literature [51], where progress estimation is equivalent to universal value learning under specific choices of reward function. However, universal value estimation comes with a number of key challenges: (1) broad generalization to new tasks and scenes, (2) the ability to accurately estimate state in partially observed environments, and (3) temporal consistency (i.e. satisfying the Bellman equation) over long horizons. Most existing methods trained on relatively small amounts of vision-only data [8, 40, 1] lack the semantic, spatial, and temporal understanding needed to ground task progress in the space-time manifold of video, preventing generalization. Moreover, they often reason over single frames, inducing a high-degree of uncertainty in partially observed environments which in turn can effect the consistency of predictions for poorly estimated states. However, these challenges are not insurmountable: modern vision language models (VLMs) exhibit marked generalization and reasoning capabilities, potentially making them useful for value estimation.\nThough not often considered as candidates for value estimation, VLMs excel at its aforementioned core challenges. First, state-of-the-art VLMs have exhibited strong spatial reasoning and temporal"}, {"title": "2. Related Work", "content": "Reward and value foundation models. Several works have tried to learn transferable reward and value functions from diverse data. Early works learned models using robot [52] or even human videos with discriminators [8], contrastive learning [3] or offline RL [40, 41, 4] to guide manipulation tasks. With the advent of recent language and vision foundation models, several works have integrated them into various robotic applications such as semantic planning [1, 27, 56, 70, 14], imitation learning [6, 57], and symbolic programming [58, 36, 56, 63, 26, 39, 55, 14, 38, 67]. Most related to our work, LLMs and VLMs have been used as reward models. Kwon et al. [34], Mahmoudieh et al. [43] use language models to provide reward values for RL agents, while Klissarov et al. [32], Wang et al. [64], Kwon et al. [34] use them to provide preference feedback. Ma et al. [42], Yu et al. [69], Xie et al. [66] even have LLMs generate their code. These works use only the language capabilities of foundation models. More recent works directly use VLMs as zero-shot reward models [50] or success detectors [15, 23]. Critically, in these works the VLM acts only as an (often sparse) reward function which predicts success, and not a value function that predicts task progress. Though some works use chain-of-thought prompting [61] or active learning [33], they generally do not make use of the autoregressive, long-context, or in-context learning capabilities of state-of-art VLMs. As a consequence, they often evalaute reward prediction only on simple and simulated tasks. To our knowledge, we are the first to demonstrate that VLMs are capable of generalizable per-frame value estimation on real world tasks which can be used for downstream tasks like dataset selection.\nIn-context learning for robotics. In-context learning has been explored in the robot learning literature, primarily focusing on action generation [16, 19, 11, 68, 13, 37, 20]. However, all these prior works require explicit, and often extensive training, on their robot tasks in order to realize in-context learning capabilities, and generalization is achieved only on narrow distribution of tasks. In contrast, we demonstrate that visual value estimation already enjoys flexible multi-modal in-context learning from pre-trained VLMs without any robot specific fine-tuning."}, {"title": "3. Generative Value Learning", "content": "In this section, we introduce Generative Value Learning, GVL. At a high level, GVL frames value estimation as an autoregressive next-token prediction problem in which a VLM is tasked with outputting the task progress for a batch of shuffled trajectory frames.\nProblem setup. We model robotics tasks as goal-conditioned partially observed Markov decision processes [48]: M(\u0444) := (0, A, R, P, T\u00b5, G) with observation space O, action space A, reward function R, transition function P, task horizon T, initial state distribution \u03bc(o), and goal space G that specifies the task semantically. Conditioned on a task g an agent \u03c0 : O \u2192 A aims to maximizes its value function, or the expected cumulative reward over the task horizon, V\u201d (01; g) = \u0395\u03bc,\u03c0,p[r(01; g) + \u00b7\u00b7\u00b7 + r(or; g)]. However, reward and value functions can be difficult to define for robotics applications given their heterogeneity. Given this, a popular universal notion of value is task progress [52, 53, 18, 60, 35]. This kind of temporal value function maps an observation and goal specification to a real number between 0 and 1: V : 0 \u00d7 G \u2192 [0, 1], where initial observations of the environment have value 0 and goal-satisfying observations have value 1. Under this definition, an expert trajectory \u03c4 = (01, ..., OT) ~ \u03a0\u0395, has value function V\u314c (ot; g) = \u2252. In this work, our goal is to obtain such a temporal value function V that can predict such task progress v\u2081, . . . Ur for each frame of video 01, ..., OT.\nThough we seek to leverage priors imbued in large foundation models, as shown in Section 4 simply prompting a VLM with video frames fails to produce meaningful estimates. To make VLMs amenable to value prediction, we propose three key components that comprise the GVL method: 1) autoregressive value prediction, 2)input observation shuffling, and 3) in-context value learning."}, {"title": "1. Autoregressive value prediction.", "content": "Traditionally, value functions V(\u00b7) : 0 \u2192 R are trained to be self-consistent by enforcing the bellman equation\nV\" (ot) = R(ot) + \u0395\u03c0,\u03c1 [V(Ot+1)].\nWhen parameterizing a value function as a feed-forward neural network, this is typically done by minimizing the mean-squared error of the equality above. As values for different observations within the same trajectory are related via the bellman equation, the resulting value function remains consistent even if we query it with only a single observation. VLMs on the other hand are not inherently trained with any consistency objective. Thus, if we independently query a VLM with different observations from the same trajectory it is likely to produce inconsistent values. Our insight is that providing the entire trajectory as input instead of just a single observation offers VLMs greater opportunity to generate self-consistent value estimates. Concretely, given a language description of the task ltask we ask the VLM to auto-regressively generate values given the entire video as context:\nUt = VLM(01, . . ., OT; U1, . . ., Ut\u22121; ltask), \u2200t \u2208 [2, T].\nWe abbreviate this auto-regressive prediction process as v\u2081, . . ., U\u2081 = VLM(01, ..., or; ltask). This simple mechanism allows the VLM to attend to all previous predictions and frames when making the next value prediction, enabling it to produce globally consistent estimates over long-horizon sequences without needing to be trained like classical feed-forward value functions. Though this design choice enables VLMs to produce consistent values, it doesn't necessitate that the values are meaningfully. Naively prompting a VLM in this manner tends to produce linear, monotonic value functions for every single video, regardless of optimality."}, {"title": "2. Input observation shuffling.", "content": "Empirically we find that when presented a choronological sequence of frames VLMs discover the short-cut solution of outputting monotonically increasing values, often ignoring the task description or the actual quality of the trajectory. One hypothesis is that as VLMs are trained on ordered video frames for captioning and question answering, the chronology itself is a cue for downstream tasks unrelated to value prediction. As a consequence, model naive prompting results in unfaithful low-quality value predictions. To break this temporal bias, we propose randomly shuffling the input frames. In this manner, GVL forces the VLM to pay attention to each individual frame and output faithful value predictions using all information provided in context. Concretely, GVL prompts a VLM as:\nV\u0129,..., U\u017e = VLM(0\u0129,\u2026\u2026\u2026, 07; ltask, 01), where (\u0128,...,T) = permute(1,...,T).\nwhere the permute operator randomly shuffles the temporal indicies. Note however, that we cannot shuffle every frame. If we do so, then the arrow of time in the original video can be ambiguous \u2013 i.e., in many cases, the reverse video is also physically plausible, making is the ground-truth order impossible to predict. Thus, as in the above equation we condition the VLM on the first input frame allow it to use the first observation as an anchor point for all other shuffled frames."}, {"title": "3. In-context value learning.", "content": "While auto-regressive prediction and shuffling are enough to obtain good performance, GVL can perform even better by leveraging the appealing properties of VLMs. Notably, large models often exhibit in-context learning, where tasks can be learned by simply providing examples [7]. This enables flexible and versatile in context value learning, by which GVL's predictions can steadily improve by providing examples at test time without any model fine-tuning. In particular, we can simply prepend shuffled videos and their ground-truth task progress as in-context examples to boost the value prediction quality via few-shot learning:\nV\u0129, ..., v\u0129 = VLM (0\u0129,..., 0\u1ef9, ltask | permute ((01, V1), (02, U2), . . ., (0\u043c,\u0438\u043c)))"}, {"title": "Practical implementation.", "content": "To predict temporal value functions in practice, GVL asks the VLM to output integer-valued percentage numbers between 0 and 100. Given that real-world robot video datasets are of different lengths and taken at different frequencies, we subsample all videos so that there are 30 frames in the input sequence to ensure comparable findings across datasets. See the Appendix for the full prompt and implementation."}, {"title": "4. Experiments", "content": "We conduct large scale experiments assessing GVL's value prediction generalization and in-context learning capabilities. Specifically, we study the following questions:\n1. Can GVL produce zero-shot value predictions for a broad range of tasks and embodiments?\n2. Can GVL improve from in-context learning?\n3. Can GVL be used for other downstream robot learning applications?\nIn all our experiments, we use Gemini-1.5-Pro [22] as the backbone VLM for GVL; we ablate this model choice and find GVL effective with other VLMs as well. After thorough study of GVL's value prediction capabilities, we study several downstream applications in visuomotor policy learning, aiming to improve data quality at dataset, trajectory, and individual transition levels."}, {"title": "Evaluation metric.", "content": "Our goal is to evaluate GVL value estimation at scale on as many robot datasets as possible, holistically testing its generalization capabilities and understanding its limitations. This makes it difficult to use traditional evaluation metrics for value functions, such as observing downstream learned policy performance, as they require value functions that are specifically trained or finetuned for individual tasks and embodiments. This quickly becomes very expensive for universal value functions that are intended for use across a large set of diverse real-world tasks and robots, many of which the practitioner may not have access to. Prior works on large-scale value learning have resorted to visually observing the smoothness of the value curve on expert trajectories as a qualitative \"eye-test\" for model generalization [40, 41, 29], but such evaluation is conducted on only few selected videos. We formalize and scale up this intuitive approach and introduce a lightweight, yet predictive method for evaluating value models: Value-Order Correlation (VOC). This metric computes the rank correlation between the predicted values and the chronological order of the input expert video:\nVOC = rank-correlation (argsort(v\u012b, ..., v\u012b); arange(T));\nVOC ranges from \u22121 to 1, where 1 indicates that the two orderings are perfectly aligned. Expert quality demonstrations, by construction, have values that monotonically increase with time, and thus a good value model should have high VOC scores when evaluated on expert videos. On the other hand, fixing a good value model, low-quality trajectories should have low VOC scores. This is because sub-optimal trajectories often contain high repetition of visually similar frames due to the presence of redundant, re-attempt actions or poorly-placed cameras. As such, the values along the trajectories should not be monotonic, resulting in low correlation with the ground-truth timestep order. As we will show in our experiments, this value rank correlation metric has strong predictive power for the quality of the values as well as downstream policy learning performance, validating its usefulness as a standalone evaluation metric for value predictions."}, {"title": "4.1. Large-scale real-world evaluation", "content": "To study GVL's zero-shot value prediction capability, we evaluate its VOC on two large expert robotics datasets.\nOpen X-Embodiment dataset. First, we consider the Open X-Embodiment (OXE) dataset [45]. an aggregation of trajectory data from 50 standalone academic robot datasets that consists of diverse tasks, robots, and camera viewpoints. For each of the 50 datasets, we randomly sample 20 trajectories and evaluate GVL zero-shot on each of the sampled trajectories. Note that not all OXE datasets have language task annotations, so we use the last frame of the trajectory as the goal specification when text annotation is not provided. To better contextualize GVL's value prediction quality, we compare to a state-of-the-art multi-modal value model LIV [41], a contrastive vision-language model [49] fine-tuned with value learning objective on human videos for in-the-wild value estimation. LIV predicts the temporal value of an input observation by computing its embedding distance to the embedding of the goal image or task description.\nFor evaluation, we plot the histogram of all 1000 (50\u00d720) Value Order Correlation (VOC) scores in Fig. 3, split by goal modalities. Given that most OXE datasets contain human-collected expert demonstrations, good value models should have high VOC scores; however, we acknowledge that there are sub-optimal trajectories within OXE that can introduce noise in our results. After we first establish GVL as an effective universal value model. we will present how GVL can be used to detect low-quality data in Section 4.3. As shown in Fig. 3, on both goal modalities, GVL consistently generates VOC scores that heavily skew to the right, indicating that it is able to zero-shot recover the temporal structure hidden in the shuffled demonstration videos, i.e., coherent value predictions. GVL's performance is also markedly better than LIV on language goals (Fig. 3 left). Here, LIV's predictions are random, suggesting that its embedding space does not contain sufficient knowledge for predicting dense values for arbitrary unseen robot videos. On image goals, LIV's prediction problem is arguably simpler because an embedding space that simply captures image similarity can result in ascending values that correlate with timesteps. Even then, GVL generates better quality value predictions as judged by slightly higher VOCs (Fig. 3 right). In summary, GVL can indeed effectively utilize the world knowledge afforded by the backbone VLM to achieve effective value predictions zero-shot for the breadth of real-world robotic tasks and datasets.\nChallenging bimanual datasets. OXE datasets primarily focus on simpler, short horizon single-arm tasks. To further stress test GVL, we evaluate on a new diverse dataset of 250 distinct household tabletop tasks on the bi-manual ALOHA systems [71, 2]. This dataset includes highly challenging, long-horizon skills, such as removing three gears sequentially from a NIST board, folding a dress in eighth-fold, hanging a t-shirt on a cloth rack. See the bottom right of Fig. 2 for representative ALOHA"}, {"title": "4.2. Multi-Modal In-Context Value Learning", "content": "As the diverse ALOHA dataset is significantly more challenging, we explore whether GVL can benefit from in-context learning, where additional shuffled observation-value pairs are presented in the VLM context window (Eq. 4).\nFew-shot in-context learning. First, we collect an additional demonstration for each of the 250 tasks and use its shuffled value-observation pairs as context for one-shot GVL value prediction for the same set of 500 evaluations. As seen in Fig. 4, with one in-context trajectory, GVL's performance substantially improves with 90% positive VOCs and a median VOC of 0.37. We further investigate whether performance can improve with more in-context examples on a represented subset of 13 tasks for which have more than 500 demonstrations. For these tasks, we evaluate few-shot GVL on 500 distinct trajectories per task with up to 5 in-context examples. The average VOCs over tasks and trajectories is shown in Fig. 4 (Right). We see that GVL demonstrates appealing in-context scaling as the average score steadily improves as we increase the number of in-context examples. Even with 5 in-context trajectories, meaning 150 total shuffled images, GVL is able to utilize its full context and exhibit strong generalization. This result demonstrates how state-of-art long-context-window VLMs, such as Gemini-1.5-Pro, can be re-purposed to make for general-purpose value functions with impressive test-time improvement capability, quickly mastering value predictions with minimal supervision.\nCross-embodiment in-context learning. Examples in-context are not limited to robot demonstrations. One advantage of GVL is that it can still benefit from in-context learning even when the demonstrations come from a different embodiment. Specifically, we record humans performing the same tasks as the ALOHA robot demonstrations and then use these human demonstrations as in-context examples for value prediction. As shown in Fig. 6, GVL with one cross-embodiment in-context example can effectively improve over its zero-shot counterpart. In the Appendix, we also show that GVL can similarly benefit from cross-task in-context learning. In conclusion, GVL presents a versatile framework for in-context value learning that can scale up to even the most challenging manipulation tasks."}, {"title": "4.3. GVL Applications", "content": "As GVL can generate high-quality value estimates, it can be applied to a number of downstream tasks including dataset quality estimation, success detection, and weighted imitation learning.\nDataset Quality Estimation. Robotic action models are increasingly trained on large mixtures of datasets [45, 59, 31] and selecting the right mixture is critical for policy performance [24]. However, dataset mixing is often done in an ad-hoc fashion by visual inspection [59]. Having validated that GVL is an effective zero-shot value model, we investigate whether we can in turn use GVL's VOC scores to determine dataset quality within OXE. To this end, for each OXE dataset in Fig. 3, we compute the average correlation scores for its sampled trajectories and present the ranking of the average score in Appendix B. In Table 1, we present a subset of selected representative large-scale datasets in OXE. We see that datasets have large spread in their VOC scores, but these scores are interpretable and match human intuitions. Specifically, datasets collected from human teleoperators with relative fixed camera placements, such as RT-1 [5], Dobb-E [54], and Bridge [17, 62], have high VOC scores, despite their diversity in scenes and tasks. In contrast, datasets with autonomous data collection via scripted motions or motor babbling, such as QT-OPT [28] and RoboNet [12], contain high number of suboptimal trajectories that do not exhibit smooth temporal structure to be re-shuffled.\nInterestingly, DROID [30], a recent large household manipulation ataset is ranked very low, consistent with prior works [31] that found that removing DROID from large action model training improved final performance. After inspecting trajectories from DROID with a low VOC score from GVL we found that many have poor camera angles that do not capture robot motion or have the arm or manipulated objects heavily occluded. These observations indicate that GVL VOC can be indicative of dataset quality.\nSuccess detection and filtered imitation learning. Next we consider more granular intra-dataset quality control by investigating how GVL can be used as a success detector for trajectory filtering, enabling filtered imitation learning on mixed quality datasets. As discussed, good value models should return low VOC scores on unsuccessful trajectories; in particular, it is difficult for GVL to re-shuffle frames within suboptimal trajectories which often contain irregular or repetitive behavior. Thus, we can use GVL for success detection by filtering trajectories that have VOC scores below certain numerical threshold; we refer to this procedure as GVL-SD. We evaluate GVL-SD on six simulated bimanual dexterous manipulation tasks on the ALOHA system (see Fig. 10). Simulation is well-suited for this experiment because we can naturally control for data quality and reproducibility. More specifically, for each task, we construct a mixed quality dataset by rolling out a pre-trained policy of roughly 50% success rate for 1000 episodes, mirroring real-world autonomous data collection settings with high failure rate [28]. We compare to SuccessVQA [15], which poses"}, {"title": "4.4. Ablations", "content": "Finally, we ablate key algorithmic design choices of GVL to validate their necessity. In the Appendix, we additionally demonstrate that GVL's performance is robust to the choice of backbone VLMs as well as input camera viewpoint.\nIs autoregressive value prediction necessary? We consider an ablation that simply asks the VLM to predict values of input observations one by one without GVL's autoregressive batch prediction mechanism. This ablation, which we refer to as VLM (Single Frame), essentially poses value estimation as a VQA problem. We compare this ablation to GVL on a subset of RT-1 dataset as in Section 4.1; the average VOC for VLM (Single Frame) is a mere \u20130.08, a significant drop from GVL's 0.74 on RT-1 dataset. As seen, pre-trained VLMs by themselves are poor value estimators, generating inconsistent values that are too noisy to be used in practice.\nIs input observation shuffling necessary? As discussed, we find that removing shuffling collapses ICV's predictions into generating degenerate values; that is, regardless of the quality of the provided trajectory, GVL tends to predict monotonically increasing values, resulting in inflated VOC scores that cannot be used to discriminate successful and failure trajectories.; see Fig. 7 (Right). To further qualitatively illustrate this phenomenon, in Fig. 11 in the Appendix, we overlay raw GVL value predictions with frame shuffling and lackthereof to understand the spread of the value curves. We see that the overlay for original GVL looks \u201cmessy\u201d, suggesting that GVL outputs varied value curves that better capture the heterogeneity of the queried video qualities. In contrast, without frame shuffling, GVL predictions indeed collapses onto a few linear ascending patterns."}, {"title": "5. Conclusion", "content": "We have introduced Generative Value Learning (GVL), a universal value function via VLM autore-gressive value prediction on shuffled video frames. GVL can zero-shot output dense and high-quality value predictions for diverse and challenging real-world robotic tasks, spanning various robot embodiments and task categories. With few-shot learning from the same task, different task, or different embodiment, GVL performance steadily improves. We have demonstrated several use cases of using GVL to perform dataset, trajectory, and transition selection to improve downstream policy learning performance and generalization. We believe that GVL takes an important step in using foundation models supervision for robot learning.\nLimitations and future work. We have not investigated whether pre-trained VLMs can be fine-tuned to perform better value predictions. In addition, though we test on diverse camera viewpoints, we have not yet investigated whether multi-view observations can improve value prediction quality. In addition, our evaluation metric Value-Order Correlation is most suitable for a-periodic tasks for which there exists a unique ordering of frames from an expert demonstration. Tasks such as wiping or stirring may be hard to discern. Though these limitations present avenues for future work, we believe GVL is a step towards improved in-the-wild value estimation."}, {"title": "A. Prompt", "content": "In this section, we provide the full prompt provided to the VLM for GVL predictions. The same prompt is used for all OXE datasets.\nYou are an expert roboticist tasked to predict task completion percentages for frames of a robot for the task of {task_description}. The task completion percentages are between 0 and 100, where 100 corresponds to full task completion. We provide several examples of the robot performing the task at various stages and their corresponding task completion percentages. Note that these frames are in random order, so please pay attention to the individual frames when reasoning about task completion percentage.\nInitial robot scene: [IMG]\nIn the initial robot scene, the task completion percentage is 0.\nNow, for the task of {task_description}, output the task completion percentage for the following frames that are presented in random order. For each frame, format your response as follow: Frame {i}: Frame Description: {}, Task Completion Percentages:{}%\nFrame 1: [IMG]\nFrame n: [IMG]"}, {"title": "B. GVL OXE Dataset VOC Breakdown", "content": "In this section, we provide the full list of average VOC score for each OXE dataset. In Appendix B, we provide the VOC scores for GVL with Gemini-1.5-Pro as the backbone VLM. In Table 5, we provide the VOC scores for GVL with GPT-40 as the backbone VLM."}, {"title": "C. Simulation Tasks", "content": "In Figure 10, we illustrate the six simulation tasks used for the success detection and filtered imitation learning experiment. For each task, we use VR teleoperation to collect 500 trajectories for initial policy training. After the policy converges, we rollout the last checkpoint for 1000 imtes, resulting in naturally balanced mix-quality datasets of about half success and half failure trajectories."}, {"title": "D. Additional Results", "content": "In this section, we present additional results and analysis.\nGVL and No-Shuffling ablation qualitative comparison. As shown in Fig. 11, GVL generates value predictions that are varied over time; in contrast, without frame shuffling, the predictions all collapses onto a few monotonic patterns."}]}