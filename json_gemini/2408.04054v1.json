{"title": "NAVINACT: Combining Navigation and Imitation Learning for Bootstrapping Reinforcement Learning", "authors": ["Amisha Bhaskar", "Zahiruddin Mahammad", "Sachin R Jadhav", "Pratap Tokekar"], "abstract": "Reinforcement Learning (RL) has shown remarkable progress in simulation environments, yet its application to real-world robotic tasks remains limited due to challenges in exploration and generalization. To address these issues, we introduce NAVINACT, a framework that chooses when the robot should use classical motion planning based navigation and when it should learn a policy. To further improve the efficiency in exploration, we use imitation data to bootstrap the exploration. NAVINACT dynamically switches between two modes of operation: navigating to a waypoint using classical techniques when away from the objects and reinforcement learning for fine-grained manipulation control when about to interact with objects. NAVINACT consists of a multi-head architecture composed of ModeNet for mode classification, NavNet for waypoint prediction, and InteractNet for precise manipulation. By combining the strengths of RL and Imitation Learning (IL), NAVINACT improves sample efficiency and mitigates distribution shift, ensuring robust task execution. We evaluate our approach across multiple challenging simulation environments and real-world tasks, demonstrating superior performance in terms of adaptability, efficiency, and generalization compared to existing methods. In both simulated and real-world settings, NAVINACT demonstrates robust performance. In simulations, NAVINACT surpasses baseline methods by 10-15% in training success rates at 30k samples and by 30-40% during evaluation phases. In real-world scenarios, it demonstrates a 30-40% higher success rate on simpler tasks compared to baselines and uniquely succeeds in complex, two-stage manipulation tasks. Datasets and supplementary materials can be found on our website.", "sections": [{"title": "1 Introduction", "content": "In recent years, reinforcement learning (RL) has made significant strides, achieving remarkable success across various domains [21\u201323]. Despite advances in RL, learning behaviors efficiently that generalize to new settings is still a challenge. RL methods are often constrained to short-horizon tasks due to the inherent challenges of exploration in high-dimensional, continuous action spaces, which are characteristic of robotic applications. In these scenarios, reward signals tend"}, {"title": "2 Related Work", "content": "This section explores substantial contributions in reinforcement learning (RL) aimed at enhancing sample efficiency and generalizability, with a focus on methods integrating human demonstrations and strategies to counter distribution shifts in imitation learning (IL). We also examine how leveraging reference policies can optimize RL outcomes."}, {"title": "2.1 Incorporating Model Priors and Refining Action Spaces", "content": "Research has increasingly concentrated on embedding structural priors within models to stabilize performance across variable conditions. Object-centric representations and pretrained state models from diverse task datasets have proven to enhance generalization and efficiency [1\u20133]. Approaches like [29] HYDRA have refined action spaces to align with these insights. However, NAVINACT goes a step further by dynamically switching action modes, integrating both high-level strategic planning and precise low-level task executions. This not only enhances navigation and manipulation capabilities in intricate settings but also addresses the challenges of generalization and extensive data requirements found in temporal action abstractions and parameterized motion primitives [4,7\u20139,11,12]. Building on the foundations laid by previous models, NAVINACT effectively minimizes errors and advances the capabilities of action representation in complex environments, making it a significant advancement in both model integration and action space refinement."}, {"title": "2.2 Planning-Driven Reinforcement Learning", "content": "Integrating motion planning with RL has been explored to combine the benefits of both paradigms [13\u201319]. For example, GUAPO [13] focuses on single-stage tasks, maintaining RL agents within low pose-estimator uncertainty areas, while Plan-Seq-Learn decomposes complex tasks into sequences of subtasks guided by large language models (LLMs). However, these methods often come with high inference costs or inefficiencies in task decomposition. Our NAVINACT method contrasts these approaches by offering an end-to-end learning solution that is not only faster but also more adaptable to real-time applications, with the potential for further enhancement through LLM integration."}, {"title": "2.3 Reinforcement Learning with Prior Demonstrations", "content": "Sample-efficient RL often struggles in sparse reward environments, where initial learning cues are vital. Integrating prior data or human demonstrations into the replay buffer an approach employed in methods like Reinforcement Learning from Prior Data (RLPD) [31] and IBRL [28]-has proven effective. These methods enhance RL algorithms by oversampling demonstrations during training, significantly improving performance in continuous control domains with techniques like normalization and Q-ensembling [2, 28]. Our NAVINACT framework builds on these principles by dynamically adjusting its strategy based on environmental cues and task demands, using a multi-headed architecture that optimizes both navigation and interaction. This approach not only leverages prior demonstrations for improved initial guidance but also adapts in real-time to enhance task execution and generalization in complex robotic manipulation tasks."}, {"title": "2.4 Advancing RL Sample Efficiency", "content": "Recent advancements in RL have prioritized enhancing sample efficiency through innovative regularization strategies. Techniques like RED-Q [20] and DropoutQ"}, {"title": "3 NAVINACT", "content": "We consider a standard Markov decision process (MDP) consisting of state space $s \\in S$, continuous action space $A = [-1,1]^d$, deterministic state transition function $T: S \\times A \\rightarrow S$, sparse reward function $R : S \\times A \\rightarrow {0,1}$ that returns 1 when the task is completed and 0 otherwise, and discount factor $\\gamma$."}, {"title": "3.1 ModeNet: A Vision-Based Mode Classification Network", "content": "ModeNet is a deep learning architecture that classifies the operational mode of the robot based on visual input. It determines whether the robot should operate"}, {"title": "3.2 NavNet: A Vision-Based Waypoint Prediction Network", "content": "NavNet is designed to predict waypoints for robotic manipulation tasks by processing both visual and proprioceptive inputs. It generates high-level action outputs that guide the robot towards the target objects in the environment. NavNet combines a convolutional neural network (CNN) for visual processing with fully connected layers that integrate visual features with proprioceptive data, Fig. 3. This design allows for efficient waypoint prediction, facilitating robust navigation in complex environments. Once we have the predicted waypoint, we use a sampling-based motion planner, AIT* [39], to perform motion planning."}, {"title": "3.3 Interact Net: Low-Level Action Prediction", "content": "InteractNet integrates imitation learning (IL) with reinforcement learning (RL) to enhance sample efficiency in robotic task execution. Initially, it trains using expert demonstrations to establish a baseline IL policy. This IL policy guides the RL training phase by recommending high-quality actions, aiding in effective learning through techniques like Temporal Difference (TD) and off-policy RL methods such as TD3. Actions are selected based on the highest Q-values, comparing inputs from both IL and RL strategies."}, {"title": "4 Experiments", "content": "In this section, we present a comprehensive evaluation of NAVINACT, focusing on its sample efficiency, generalizability, and performance in real-world scenarios. We conducted experiments in three challenging simulation environments (Fig. 8) and two complex real-world tasks (Fig. 10)."}, {"title": "4.1 Training and evaluation of ModeNet and NavNet", "content": "Can we predict the modes and waypoints accurately?\nModeNet is trained on a dataset comprising 1,500 environmental camera images for simulation. The ground truth labels were determined by a distance threshold between the end-effector and the object, effectively deciding whether the robot should be in a navigation mode (moving towards an object) or an interaction mode (engaging directly with an object). Data augmentation techniques like rotation and contrast adjustment were applied during training. The network utilizes cross-entropy loss and is optimized with Adam. The network's performance is evaluated using metrics such as accuracy, precision, recall, and F1 score. ModeNet achieved an accuracy score of 0.89, an F1 score of 0.85, a recall score of 0.9, and a precision score of 0.8, demonstrating its effectiveness in accurately classifying operational modes for robotic tasks.\nNavNet NavNet is trained on the same dataset of 1,500 environmental camera images as ModeNet. The ground truth for NavNet involves waypoints defined as positions 2 cm above the objects, guiding the robot on where to navigate"}, {"title": "4.2 Simulation Experiments", "content": "Our evaluation suite includes three tasks from MetaWorld [37], each using sparse 0/1 task completion rewards at the end of each episode. These tasks are a subset of those evaluated in IBRL [28] and cover medium and hard difficulty levels as categorized in [35].\nThe specific environments used are assembly-v2, box-close-v2, and coffee-push-v2. Given the absence of human demonstrations in MetaWorld, we utilized the demonstration datasets generated by IBRL, which were created using scripted expert policies from [37]. Although these tasks are relatively simple, they are sufficient to differentiate between stronger methods, especially in terms of sample efficiency and generalizability.\nImplementation of NAVINACT and Baselines:\nNAVINACT employs Behavior Cloning (BC) for imitation learning (IL) and Twin Delayed Deep Deterministic Policy Gradient (TD3) for reinforcement learning (RL). A ResNet-18 vision encoder is used for BC, and random-shift image augmentation is applied to enhance RL performance. NAVINACT is benchmarked against three baselines: IBRL [28], RL-MN (RL with ModeNet and NavNet), and standard RL.\nIBRL uses a pre-trained IL policy, $\\mu_{\\psi}$, to aid exploration during RL and to assist in target value estimation during Temporal Difference (TD) learning. Likewise, NAVINACT also uses IL policy within the InteractNet, that speeds up RL during object interaction. RL with ModeNet and NavNet, which we refer as RL-MN, incorporates ModeNet and NavNet to accelerate exploration by predicting waypoints, reducing the number of exploration steps needed in RL. Standard RL utilizes TD3 but does not use any IL or hierarchical mode prediction components."}, {"title": "4.3 Results", "content": "Following the training of NAVINACT and the baselines, we evaluate its performance on unseen environments and compare it against baseline methods. Performance is assessed using training and evaluation scores, which indicate sample efficiency and generalizability. Fig. 8 illustrates the comparative results for the all"}, {"title": "4.4 Real-world Experimental Setup", "content": "We design two tasks named Lift and pick-and-place as shown in Fig.10a, 10b, 10c. The tasks use a UR3e robot. The robot is equipped with a Robotiq hand-e gripper and a realsense camera mounted on the wrist. We also use another realsense camera as environment camera for the experiment as shown in Fig. 9.\nActions are 4-dimensional consisting of 3 dimensions for difference of end-effector position (deltas), under a Cartesian Delta controller and 1 dimension for absolute position of the gripper. Policies run at 6 Hz. For each task, we collect a small number of prior demonstrations via teleoperation with a passive arm Gello [36], that we modified for UR3e controller, and then run different RL methods for a fixed number of interaction steps. All methods use the exact same hyper-parameters and network architectures as in metaworld tasks. We illustrate these tasks in Fig. 10 and briefly describe them here.\nLift: The objective is to pick up a foam block. The initial location of the block is randomized over roughly 22cm \u00d7 (20cm - 25cm) trapezoid, which covers the entire area visible from the wrist-camera when the robot is at the home position. See Fig. 10a, Fig. 10b for reference. We collect 10 demonstrations for this task due to its simplicity. It uses wrist-camera images as observations. We detect whether the gripper is holding the block by checking if the gripper width is static and the desired gripper width is smaller than the actual gripper width. The success detector returns 1 if the end effector has move upward by at least 2cm while holding the block. The maximum episode length is 120.\nPick-and-Place: The objective is to pick a soft toy on a wooden block. The initial location of the soft toy and the block is in a fixed position. The soft toy is initialized so that its visible from the wrsit camera view, see Fig.10c. We use 20 prior demonstrations. This task uses environment camera images as observations as well because the wrist-camera loses sight of the block after picking up the soft-toy. We detect whether the gripper is holding the oft-toy by checking if the gripper width is static and the desired gripper width is smaller than the actual gripper width. The success returns 1 when the gripper width is static, the end-effector position are near the threshold of desired block position. The maximum episode legnth is 300.\nAs the primary goal of our real-world evaluations is to compare sample-efficiency and performance of various algorithms, we design rule-based success detectors and perform manual reset between episodes to ensure accurate reward and initial conditions. Note that sparse 0/1 reward from the success detector is the only source of reward."}, {"title": "4.5 Real-world Experiment Results", "content": "Does NAVINACT Work Effectively in Real-World Scenarios? Fig. 10 presents the training curves of NAVINACT compared to IBRL and across all"}, {"title": "5 Discussion", "content": "In this paper, we presented NAVINACT, a hierarchical policy framework that combines NavNet for waypoint planning, ModeNet for dynamic mode switching, and InteractNet for fine-grained manipulation. By integrating imitation learning (IL) with reinforcement learning (RL), NAVINACT effectively addresses challenges of sample efficiency and distributional shift, outperforming existing methods. Through extensive simulations and real-world experiments, we demonstrated NAVINACT's superior performance, generalizability, and adaptability across various tasks. Our approach learns faster and requires fewer interaction steps due to the innovative integration of mode and waypoint prediction, making it a scalable and robust solution for complex robotic manipulation in real-world scenarios."}, {"title": "5.2 Limitation and Future Work", "content": "While NAVINACT demonstrates significant advancements in sample efficiency and generalizability, one limitation of our approach is the reliance on collecting high-quality demonstration data, which can be time-consuming and expensive. This dependence on expert-generated data constrains the scalability and applicability of our framework across a wider range of tasks and environments. Future work could focus on developing methods that allow non-experts to collect usable data quickly and efficiently, potentially through resources such as videos in the wild."}]}