{"title": "A GENERAL RETRIEVAL-AUGMENTED GENERATION\nFRAMEWORK FOR MULTIMODAL CASE-BASED REASONING\nAPPLICATIONS", "authors": ["Ofir Marom"], "abstract": "Case-based reasoning (CBR) is an experience-based approach to problem solving, where a repos-\nitory of solved cases is adapted to solve new cases. Recent research shows that Large Language\nModels (LLMs) with Retrieval-Augmented Generation (RAG) can support the Retrieve and Reuse\nstages of the CBR pipeline by retrieving similar cases and using them as additional context to an\nLLM query. Most studies have focused on text-only applications, however, in many real-world\nproblems the components of a case are multimodal. In this paper we present MCBR-RAG, a general\nRAG framework for multimodal CBR applications. The MCBR-RAG framework converts non-text\ncase components into text-based representations, allowing it to: 1) learn application-specific latent\nrepresentations that can be indexed for retrieval, and 2) enrich the query provided to the LLM by\nincorporating all case components for better context. We demonstrate MCBR-RAG's effectiveness\nthrough experiments conducted on a simplified Math-24 application and a more complex Backgam-\nmon application. Our empirical results show that MCBR-RAG improves generation quality com-\npared to a baseline LLM with no contextual information provided.", "sections": [{"title": "Introduction", "content": "Case-based reasoning (CBR) is an experience-based approach to problem solving, utilizing a repository of previously\nsolved cases to address new problems through a four-phase pipeline: Retrieve, Reuse, Revise, and Retain [1, 2]. A\nsolved case consists of a problem and a solution, while a new case includes only a problem. The first two phases aim\nto retrieve similar cases from the repository and adapt their solutions for the new problem. The Revise phase then\nensures the correctness of the new solution, while the Retain phase expands the repository by incorporating newly\nsolved cases.\nRecent advancements in Large Language Models (LLMs), particularly those enhanced with Retrieval-Augmented\nGeneration (RAG), demonstrate significant promise in supporting the Retrieve and Reuse stages of the CBR pipeline\n[3, 4, 5, 6]. While these studies employ varied methodologies tailored for specific applications, they fundamentally\nshare the same core principles.\nThe Retrieve stage utilizes RAG retrieval, indexing solved case problems as neural embeddings. When a new case is\npresented, its problem is also converted to an embedding, enabling a similarity measure to identify related cases. The\nReuse phase then employs RAG generation, crafting a query for the LLM to produce a solution for the new case while\nintegrating knowledge from retrieved cases as additional context.\nWhile promising results have emerged from this research, the focus has primarily been on text-only applications such\nas legal Q&A, medical diagnostics, code generation, and logical fallacy detection [3, 4, 5, 6]. In such applications, case\nproblems can be effectively converted to neural embeddings using pretrained transformer-based language models, such\nas BERT or GPT [7, 8]. However, many CBR applications naturally encompass data other than text such as image,\naudio, and video formats [9, 10, 11]."}, {"title": "Background", "content": ""}, {"title": "Case-Based Reasoning", "content": "In case-based reasoning (CBR) [1, 2] a case is a tuple $C = (P, S, R)$ where $P = (p_1, p_2, ..., p_n)$ denotes the problem\nmade up of n components, S is the solution, and R is the result of S. Let $C = (C_1, C_2, ..., C_N)$ denote a repository\nof N solved cases. Given a case $C_i$ for $i \\in [1, N]$ denote by $P_i, S_i$ and $R_i$ the problem, solution and result of $C_i$\nrespectively; furthermore, denote by $p_{i,j}$ the problem component $j \\in [1,n]$ for case i. Let $C = (P, \\phi, \\phi)$ be a new\ncase with $P = (p_1,p_2,...,p_n)$, where $\\phi$ denotes that we do not yet have a solution or result for the new case. Then\nthe CBR pipeline operates in four phases:\nRetrieve: the system searches its case repository for cases that are similar to the new case. Formally, let $p(P, P_i)$\ndenote a similarity function that measures how similar the problem of cases $C$ and case $C_i$ are. Then the $k > 0$ most\nsimilar cases, denoted $C_k$, are given by:\n$C_k = arg \\mathop{topk}\\limits_{C_i \\in C} p(P, P_i).$  (1)\nReuse: the retrieved cases are reused to find a solution to the new case. Formally, let Reuse be a function that reuses\nthe solutions of similar cases $C_k$ to find a solution S to P. Then\n$S = Reuse(P,C_k).$ (2)\nRevise: the result of the proposed solution is obtained and, if found to be incorrect, revised. Formally, let $R^*$ denote\nthe correct result of C and Eval be a function that evaluates a given solution to produce a result. Then, if $Eval(S) = R$\n$\\neq R^*$, revise $S$ using a revision function Revise:\n$S \\leftarrow Revise(S),$ (3)\nsuch that $R = R^*$.\nRetain: once the new case is solved, add it to the repository of solved cases. Formally, if $R = R^*$ then add $C =(P, S, R)$ to $C$:\n$C \\leftarrow C\\cup\\{C\\}.$ (4)"}, {"title": "Retrieval-Augmented Generation", "content": "Let x be a sequence of input tokens, and let $q = Query(x)$ represent a function that generates a query q from x. Let L\ndenote a Large Language Model (LLM), such that $y = L(q)$ corresponds to the LLM's response given the query q.\nRetrieval-Augmented Generation (RAG) [16] is a technique designed to enhance the quality of the LLM's response by\nincorporating additional context from an external document memory. Specifically, given a collection of M documents\n$D = \\{d_1, d_2,..., d_m \\}$, the method retrieves the top $k > 0$ most similar documents to the query, denoted $D_k$, using:\n$D_k = arg \\mathop{topk}\\limits_{d \\in D} CosineSim(E(q), E(d)),$ (5)\nwhere E is a function that maps text to an embedding space, and $CosineSim(a, b) = \\frac{a\\cdot b}{||a||||b||}$ is the cosine similarity\nbetween vectors a and b.\nOnce the relevant documents are retrieved, an augmented query can be constructed: $q_{RAG} = Query(x, D_k)$. This\nquery incorporates both the input tokens x as well as $D_k$ as additional context. The LLM then generates a response\nbased on this augmented query:\n$y_{RAG} = L(q_{RAG}).$ (6)"}, {"title": "Methodology", "content": "Research has demonstrated that RAG can be used to handle the first two phases of the CBR pipeline [3, 4, 5, 6]. In\nparticular, if we consider cases that are text-only, then RAG retrieval, given by Equation 5, can cater for the CBR\nRetrieve phase, given by Equation 1, by converting case problems to embeddings. Meanwhile, RAG generation, given\nby Equation 6, can cater for the CBR Reuse phase, given by Equation 2, by passing an augmented query to an LLM\nthat generates a solution to a new case and providing the retrieved solved cases as additional context.\nUnfortunately, the Revise and Retain phases cannot be handled generally, as automatically inferring whether an LLM\nhas generated an incorrect solution and then revising the solution accordingly is not necessarily straightforward.\nFor\nthe purposes of this paper, we focus on the Retrieve and Reuse phases in presenting our general framework, leaving\nthe latter two phases as application specific.\nTo formalize MCBR-RAG, we assume a multimodal CBR setting where the components $p_j$ of a case problem $P = (p_1, p_2,..., p_n)$ each has arbitrary modality, while the solution. S, of a case is text-based. Let $\\{T_j\\}_{j=1}^{n}$ be a collection\nof n text generation functions, such that $T_j(p_j) = t_j$ returns a text-based representation of $p_j$. Let $\\{L_j\\}_{j=1}^{n}$ be a\ncollection of n latent representation functions, such that $L_j (p_j) = l_j$ returns a latent representation of $p_j$.\nRetrieve: to process this phase under our proposed setting, we convert the problem components of a case to their\nlatent representations and compute a weighted average over the cosine similarities of the individual components.\n$C_k = arg \\mathop{topk}\\limits_{C_i \\in C} \\sum\\limits_{j=1}^{n} w_jCosineSim(L_j(p_j), L_j (p_{i,j})),$ (7)\nwhere $w_j > 0$ and $\\sum\\limits_{j=1}^{n} w_j = 1$. Using a weighted average allows control over the importance of the different\nproblem components.\nReuse: to process this phase under our proposed setting, we convert the problem components of a case to their text-\nbased representations and pass these as additional context to an LLM.\nLet $T(P) = (T_1(p_1), T_2(p_2),...,T_n(p_n))$ be a text-based representation of a case problem. Let $T(C) = (T(P), S, R)$ be\na case whose problem has been converted to text. Then we can build an augmented query $q_{RAG} = Query(T(P), T(C_k))$, where $T(C_k) = \\{T(C)\\}_{c \\in C_k}$, and obtain a solution to a new case\n$S = L(q_{RAG}).$ (8)"}, {"title": "Applications", "content": ""}, {"title": "Math-24", "content": "We first introduce a simplified Math-24 application that serves to illustrate the fundamental aspects of MCBR-RAG.\nA Math-24 puzzle is comprised of a card that contains 4 numbers (a, b, c, d) with $1 < a < b < c \\leq d \\leq 13$. The goal\nis to use these four numbers to make 24, where all four numbers must be used exactly once and one may use any of\nthe addition, subtraction, multiplication and division operators. Figure 1 shows four examples of Math-24 puzzles.\nIn total, there are 1362 puzzles with at least one solution [17]. However, for the purposes of this application, we\nrestrict our attention to the subset of puzzles that have a solution in the form $(x_1 x_2) \\times (x_3 x_4) = 24$ where $\\in \\{+, -, \\times, \\div\\}$. This is a common strategy to solve Math-24 puzzles and involves figuring out how to obtain\n$\\24\\}$, or $\\{4, 6\\}$ from pairs of puzzle numbers, then multiplying them together to get 24. This leaves\nus with a total of 466 puzzles.\nIn transforming Math-24 into a CBR application, a case is defined as a tuple $(P, S, R)$. Here, P represents an image\nof a Math-24 puzzle card; S is a set containing the solutions to P; and R is a set of the results for these solutions,\nwith each result being 24 if the solution is correct. To handle this application within the MCBR-RAG framework, we\nrequire functions for both text generation and latent representation.\nText generation: We generate 40,000 card images, each with 4 random numbers between 1 and 13. We then train\na convolutional neural network (CNN) that takes as input an image of a card and has four softmax outputs. Each\nsoftmax output has 13 units that predicts a respective number on the card. Due the simple nature of this task, our\nmodel achieves 100% accuracy on an independent test set of 10,000 cards. Once the CNN is trained we can pass\na Math-24 card image to the model and, from the resulting predictions, generate a text-based representation for the\npuzzle numbers on the card. See Figure 2 for an illustration of the CNN.\nLatent representation: to learn a useful latent representation for this application, we train a fully-connected feedfor-\nward neural network (FFNN) treating the problem as a multi-label classification task. The input to the network is the\ntext-based representation of the puzzle, which we predict using our text generator. Given our small dataset of only\n466 cases, we include additional aggregate features to aid classification. In particular, we count the number of pairs\nthat can make 1, 2, 3, 4, 6, 8, 12, or 24 using any of the allowed operations. We do this both globally and per puzzle\nnumber. For example, consider the puzzle (4, 5, 9, 10). If we compute all pair combinations that can make 1, we find"}, {"title": "Backgammon", "content": "In this section, we introduce a more complex Backgammon application. Backgammon is a two-player game where\nplayers (O and X) move their pieces (called checkers) around triangles (called points) on a board. Given the current\nchecker configuration (called a position), the player whose turn it is rolls a pair of dice, selects a move from the\nresulting legal set of moves and reconfigures the checkers accordingly. After playing their move, a new position is\nreached and the turn switches to the opposing player.\nNumerous books have been written on Backgammon by expert players. Such books often structure their lessons by\nshowing an image of a board position and a roll for a given player, then provide an analysis of the pros and cons of a\nsubset of legal moves.\nConsidering this typical structure, we can convert Backgammon lessons into a CBR applications by defining a case\nC = (P, S, R). Here, P = (z, p, r, m), where z the image of the current position p is the player on roll, r is the"}, {"title": "Experiments", "content": "In this section, we run experiments on our Math-24 and Backgammon applications introduced in Section 4 to evaluate\nretrieval and generation quality. All our reported results average over 10 independent runs where, in each run, we\nrandomly hold back 30 cases for testing. For retrieval quality we use the metrics Precision@k, Recall@k, F1-Score@k,\nNDCG@k (Normalized Discounted Cumulative Gain) and MRR@k (Mean Reciprocal Rank) where we average each\nmetric over $k \\in \\{1,2,3,4,5\\}$.\nFor Backgammon, in order to prevent data leakage when evaluating a test case, we exclude all solved cases whose\nmoves originate from the same analysis as that test case. This ensures that our test cases are not influenced by their\nown analysis, which we assume to be unknown.\nFor retrieval quality, we consider two labeling schemes to determine if a retrieved case is relevant:\na) Solution Category Only (SCO): a retrieved case is classified relevant if it contains any solution category\npresent in the test case.\nb) Solution Category & Decomposition (SCD): a retrieved case is classified as relevant if it contains any\nsolution category and decomposition present test case.\nWe also consider two similarity measures using different inputs for cosine similarity as described in Section 4.1:\na) Features: this measure uses the 40 input features of the retrieved case and test case.\nb) Latent: this measure uses the latent representations of the retrieved case and test case."}, {"title": "Final Remarks", "content": "This paper introduced MCBR-RAG, a general RAG framework designed for multimodal CBR applications. The\nframework addresses multimodal problem components through two core functions: a text generation function, which\ntransforms problem components into text, and a latent representation function, which produces latent representations\nof these components. These functions facilitate the Retrieve and Reuse phases of the CBR pipeline through RAG\nretrieval and RAG generation, respectively.\nWe began by formalizing MCBR-RAG in an abstract setting to demonstrate its generalizability across various appli-\ncations. This formalization provides a foundation for adapting the framework to a wide range of multimodal CBR\nscenarios. We then applied MCBR-RAG to two specific tasks: a simplified Math-24 application and a more complex\nBackgammon application, where we outlined practical methods for learning the necessary text generation and latent\nrepresentation functions. Finally, our experiments demonstrated that, in both applications, MCBR-RAG produced\nhigher generation quality across various LLMs compared to a baseline that did not incorporate contextual information\nin the LLM query."}]}