{"title": "To what extent is ChatGPT useful for language teacher lesson plan creation?", "authors": ["Alex Dornburg", "Kristin J. Davin"], "abstract": "The advent of generative Al models holds tremendous potential for aiding teachers in the generation of pedagogical materials. However, numerous knowledge gaps concerning the behavior of these models obfuscate the generation of research-informed guidance for their effective usage. Here we assess trends in prompt specificity, variability, and weaknesses in foreign language teacher lesson plans generated by zero-shot prompting in ChatGPT. Iterating a series of prompts that increased in complexity, we found that output lesson plans were generally high quality, though additional context and specificity to a prompt did not guarantee a concomitant increase in quality. Additionally, we observed extreme cases of variability in outputs generated by the same prompt. In many cases, this variability reflected a conflict between 20th century versus 21st century pedagogical practices. These results suggest that the training of generative AI models on classic texts concerning pedagogical practices may represent a currently underexplored topic with the potential to bias generated content towards teaching practices that have been long refuted by research. Collectively, our results offer immediate translational implications for practicing and training foreign language teachers on the use of AI tools. More broadly, these findings reveal the existence of generative AI output trends that have implications for the generation of pedagogical materials across a diversity of content areas.", "sections": [{"title": "1. Introduction", "content": "Language education is the only subject in which the language is both the content and the vehicle of instruction. Teachers typically face a range of proficiency levels in each class, with students developing their language skills at differing rates from their peers (Davin et al., 2014). To mitigate the challenges intrinsic to instruction in this content area, best practices for teaching languages have changed dramatically over the last century. For example, audiolingual methods in which learners memorized dialogues and grammar translation approaches in which students translated texts have been replaced with communicative approaches in which students use the language for real-world tasks (Lightbown & Spada, 2021). Such research-driven approaches have greatly improved the efficacy of classroom instruction. However, a consistent lack of resources continues to exacerbate teachers both in and out of the classroom. Foreign language education often exists as a marginalized content area with fewer resources than subjects such as math and science (Cawelti, 2006). In Anglophone contexts especially, programs typically have either outdated curricular materials and textbooks or none at all. In many schools, teachers are tasked with developing their own lesson plans and assessments (Schwartz, 2022). Thus, resources that can aid teachers in instructional design are critically needed.\nThe recent advent of readily available large language models (LLMs) such as ChatGPT are poised to dramatically impact how teachers approach instructional design. Such trends have already emerged in other fields such as consulting, where those trained in the use of AI tools have a documented 40% increase in work quality and 25% increase in task completion speed as compared to those who do not (Dell'Acqua et al., 2023). However, AI tools have not yet been widely embraced in foreign language programs. Many foreign language educators are cautious of this technology (Bekou et al., 2024; Kohnke et al., 2023), in part due to declining enrollments (Lusin et al., 2023) and recent headlines regarding the replacement of foreign language departments with commercial software that uses AI (e.g. Duolingo; Petit, 2023). This reticence and the nascent nature of generative AI is now creating a knowledge gap that threatens to further isolate foreign language teachers from this rapidly evolving field.\nUnderstanding how to prompt AI models and the possible limitations of the resulting outputs are critical components of AI competency. Prompts are the interface between human intent and machine output, usually manifesting as questions or instructions given to an Al model with the goal of eliciting a specific response (Giray, 2023; Short & Short, 2023). In its simplest form, a user prompts an LLM like ChatGPT with a simple command and obtains an output (Wei et al., 2023). For LLMs to successfully complete complex tasks, the ability to engineer sophisticated prompts that contain a high level of specificity is critical (Giray, 2023; Zhong et al., 2023). Numerous AI prompt engineering strategies have been developed to guide generative AI solutions to desired outputs (Bozkurt & Sharma, 2023; White, Fu, et al., 2023). However, variability in outputs from users using the same prompt may be correct for one and contain errors for another. The degree to which such errors are to be expected or if they reflect general weaknesses of current generative AI models when generating pedagogical materials remains unknown.\nThe purpose of the present study was to provide the first assessment of how prompt specificity influences trends in output variability and weaknesses when using generative AI to write lesson plans for language teachers. We used zero-shot prompting, in which a user inputs a single prompt and does not engage in dialogue with the chatbot. This approach provides a baseline expectation of the model's output and is the most likely to be employed by non-AI"}, {"title": "2. Literature Review", "content": "2.1 What are Large Language Models?\nA primary objective of natural language processing (NLP), a subfield of Artificial Intelligence, is to enable machines to understand, interpret, and generate human language for task performance (Chowdhary, 2020). The recent release of LLMs such as ChatGPT has placed unprecedented attention on our ability to create models that allow machines to mimic human language (Roe & Perkins, 2023). This ability is due in part to advances in deep learning, in which networks of nodes that mirror our conceptual understanding of human neural networks communicate and extract meaningful content from unstructured input data (Roumeliotis & Tselikas, 2023; Serban et al., 2016). Understanding of input text is made possible by pre-training models on aggregations of millions of pages of text from books, websites, articles, and other sources (Wu et al., 2023). This pre-training provides a foundational basis for capturing semantic nuances of human language that can be fine-tuned for a wide range of specific applications that span content creation (Cao et al., 2023), language translation (Gu, 2023; Li et al., 2023), and writing assistance (Ba\u0161i\u0107 et al., 2023; Imran & Almusharraf, 2023) to name but a few. Central to the efficacy of such applications is the prompt of the user, which embeds task descriptions as input that guides the computational response of the AI model (Lee et al., 2023; White, Hays, et al., 2023).\n2.2. Prompting in LLMs\nPrompts act as the primary user-based input that LLMs such as ChatGPT respond to when generating output. A prompt may simply state a question or task command such as \u201cWrite a haiku about sharks.\" In response, ChatGPT will generate the haiku. If a haiku about any aspect of sharks is the desired output, then the user will have achieved their goal. However, such general examples are rarely the desired output. Instead, users of LLMs in professional settings have highly specialized tasks for which they need specific output. This need for specificity has led researchers to urge users of generative AI to understand and master fundamental concepts of prompt engineering to effectively leverage LLMs (Giray, 2023; Hatakeyama-Sato et al., 2023; Heston & Khun, 2023). Effective prompts are often comprised of four components (Giray, 2023):\n(1) Instruction: A detailed directive (task or instruction) that steers the model's actions towards the intended result.\n(2) Context: Supplementary data or context that supply the model with foundational understanding, thereby enhancing its ability to produce accurate outputs.\n(3) Input data: This serves as the foundation of the prompt and influences the model's perception of the task. This is the query or information we seek to have the model analyze and respond to.\n(4) Output indicator: This sets the output type and format, defining whether"}, {"title": "2.3 Variation in Outputs", "content": "The ability of AI to generate non-deterministic outputs from the same prompt has been lauded as a major achievement, but it also underscores a need for caution. This ability enables the LLM to weigh the importance of words in a sentence and generate outputs based on probability distributions (Lubiana et al., 2023). Central to this architecture is the temperature parameter which acts as a dial for the model's creative output. At low temperature values, words with higher probabilities are chosen and the model output becomes more deterministic (Lubiana et al., 2023). At high temperature values, the model explores a broader range of possible responses that include novel and less expected outputs (Davis et al., 2024). However, even at low temperature settings near or at zero, models like ChatGPT have been found to return non-deterministic and sometimes erroneous results by chance. Jalil et al (2023) recently found that even at a temperature setting of zero, ChatGPT provided non-deterministic answers to simple prompts related to software curriculum nearly 10% of the time with an error rate of over 5%. As the default temperature setting for the public release of ChatGPT likely to be used by educators is around 0.7, this finding suggests that assessments of this tool in education should include the potential for non-determinism. Unfortunately, how variability in output responses to the same prompt impact the design of language teacher instructional materials remains unexplored."}, {"title": "2.4 Approaches to Prompting for Lesson Planning", "content": "The promise of using LLMs for foreign language teacher material development was recognized not long after the public release of ChatGPT (Hong, 2023; Kartal, 2023; Koraishi, 2023; Kostka & Toncelli, 2023). Since then, a range of prompting techniques such as zero-shot (Kojima et al., 2022; Sanh et al., 2021), few-shot (Brown et al., 2020; Kojima et al., 2022), chain-of-thought (Wang et al., 2022), tree-of-thought (Yao et al., 2023), and even autotune solutions (Khattab et al., 2023) have been developed. However, whether teachers require training in complex prompting strategies for routine tasks remains unclear. Corp and Revelle (2022) explored the ability of eight pre-service elementary school teachers to use zero-shot prompting with ChatGPT for lesson plan creation and found it to be feasible after a short tutorial. Zero-shot approaches have repeatedly been shown to produce quality outputs when prompted effectively (Ateia & Kruschwitz, 2023; D. Hu et al., 2024; Y. Hu et al., 2023), including in an evaluation of materials generated for physics classes that found no statistical difference in output between zero-shot prompting and other more complex approaches (Yeadon & Hardy, 2024). More recently, Karaman & G\u00f6ksu (2024) evaluated the effectiveness of lesson plans generated by ChatGPT using zero-shot prompting on third graders' mathematical performance over five weeks, noting a boost in math scores for the ChatGPT group. Although results such as this underscore the potential benefits of incorporating AI-developed lesson plans into the educational repertoire, whether the alignment of those plans with teaching objectives and standards is reliable has been questioned (Koraishi, 2023; Lo, 2023) leaves this question unanswered."}, {"title": "3. Methods", "content": "The present study sought to analyze the extent to which zero shot prompting was useful for language teacher lesson plan creation. Specifically, the study was guided by the following research questions:\nR1. To what degree does increasing the specificity of prompts impact the structure and content of AI-generated lesson plans?\nR2.How does the specificity of a prompt influence the consistency of AI-generated responses?\nR3. Does ChatGPT demonstrate any overall strengths or weaknesses in lesson plan design, regardless of prompt specificity?\n3.1 Approach\nWe used ChatGPT 4.0 (OpenAI, 2024) to assess how increasing prompt specificity impacted the alignment of outputs with lesson plan criteria that are given to pre-service L2 teachers during their training at the University of North Carolina at Charlotte. We designed five increasingly specific zero-shot prompts following general guidelines of prompt design (Giray, 2023) and recorded the resulting outputs. Each prompt was input into 10 unique chats to additionally assess the resulting non-determinism of outputs between the same prompt. The resulting 50 outputs were scored for the presence/absence of criteria and subject to a range of statistical analyses that allowed for the visualization of trends, assessment of variability between prompt groups, and a series of analyses in dissimilarity. These analyses aimed to reveal whether specific features of the prompt design yielded outputs that were more or less aligned with target criteria for the lesson plan design. This approach allowed us to provide an assessment of zero-shot prompting for language teacher lesson plan design.\n3.2 Prompt Design\nThe five prompts iteratively built specificity toward constructing a lesson plan that aligns with target criteria used in the foreign language teacher licensure program at the University of North Carolina at Charlotte. The lesson plan template and scoring rubric aligned to the requirements of Pearson's edTPA, a performance assessment that teachers in North Carolina and many other states must pass for teacher licensure. Prompts were designed to adhere to the guidance of prompt design that include instruction, context, input data, and an output indicator (Giray, 2023). They iteratively increased in complexity."}, {"title": "3.3 Dataset creation", "content": "All prompts were used as input for ChatGPTv 4.0 (OpenAI, 2024) with the resulting lesson plan output saved to a text file. Each prompt (P.1-P.5) was input ten times to capture non-deterministic output arising from the temperature parameter. Each prompt and prompt iteration occurred on new chats to ensure no influence of the prior prompt on the output. All text files were labeled by prompt. For example, P1.1 represented the first time Prompt 1 was entered, P1.2 represented the second time Prompt 1 was entered, and P5.5 represented the fifth time Prompt 5 was entered. In sum, 50 lesson plans, 10 for each of the five prompts, were generated. We then scored each output for the presence (1) or absence (0) of components indicated in P.5, yielding a binary presence/absence matrix for data analyses."}, {"title": "3.4 Statistical Analyses", "content": "All statistical analyses were conducted in R v4.2.1 \u201cBird Hippy\u201d (R Development Core Team, 2021). To investigate how increasing the specificity of prompts impacted the structure and content of AI-generated lesson plans (R1), we first assessed general trends in the presence or absence of target lesson components. Trends in the total number of target components captured by each prompt across iterations were visualized and group means compared using ANOVA. This allowed us to test if the prompt types were significantly different in terms of generating outputs more aligned with the target criteria. Pairwise t-tests between groups were conducted using the correction method of Benjamini and Hochberg (Benjamini & Hochberg, 1995) to mitigate the potential for false discovery. These analyses were additionally repeated on the word counts between prompt categories to assess if specificity abridged the resulting output.\nAs an ANOVA only offers a perspective on group means, we additionally utilized several statistical approaches to analyze the dissimilarity between prompt outputs that can reveal separation of outputs that may be masked by comparisons of group means. As euclidean distance measures exhibit known pathologies when handling presence/absence matrices (Ricotta & Podani, 2017), we quantified dissimilarity using Jaccard distances, which are appropriate for binary data (Hao et al., 2019), using the vegan v2.6.4 package (Dixon, 2003). To visualize overlap between prompt outputs, we used nonmetric multidimensional scaling (NMDS), treating"}, {"title": "4. Results", "content": "4.1 To what degree does increasing the specificity of prompts impact the structure and content of Al-generated lesson plans?\nThere was a marginally significant difference in overall trends of prompt scores between prompt groups (p=0.0442; F=2.668; DF=4), though significance was not supported in multiple test corrected pairwise tests (p > 0.12). Adding detailed instructions had the effect of reducing the spread of variance and in some cases raising the mean value of scores (Figure 1). However, the mean scores did not increase linearly as a function of prompt detail. When the lesson template was first provided in P.3, the mean score dropped from 21.2 out of 25 from P.2 to 19.8. For P.4, when the prompt included the directive to address the ACTFL world-readiness standards, the resulting score decreased from 19.8 (P.3) to 19.6 (P. 4), remaining lower than the average scores from P.2. However, the addition of the checklist of criteria in P.5 raised the mean to the overall highest of 21.6. In addition, adding specificity has a significant impact on the"}, {"title": "4.2 How does the specificity of a prompt influence the consistency of AI-generated responses?", "content": "Scoring of prompt output revealed high variance in scores within prompt groups (Figure 3). For example, scores resulting from P.1 ranged from 23/25 to 16/25, with an average score of 20.1/25. Likewise, scores for P.5, which contained the scoring criteria ranged from a perfect score of 25 out of 25 to 20/25. In general, outputs generated from identical prompts varied by at least five elements. Corresponding to the high variance observed in the raw score, estimation of Jaccard distances provided little indication of distinct clustering by prompt type that would indicate strong dissimilarity between groups (Figure 4). Instead, within each prompt group, there were examples of highly divergent replicates as well as replicates that were highly similar to replicates from other prompt groups (Figure 4A). In other words, the convergence and divergence patterns observed in the distance matrix reveal a mixture of similarity and variability between and within prompt groups. For example, the tenth replicate of prompt 5 (5.10) was highly similar to the sixth replicate of prompt 2 (2.6), indicating the potential of a less detailed prompt output to converge in score with one generated using a higher detailed prompt by chance. In contrast, the third output generated using P.1 in independent chats (P1.3) was highly dissimilar to almost all other outputs (Figure 4A). This reveals the potential for a lack of rigid uniformity in how each distinct prompt type influenced outputs.\nThe dendrogram estimated using hierarchical clustering revealed a similar pattern to the raw distance matrix. Some prompt replicate outputs were highly dissimilar to the outputs from the other prompts (Figure 4B). Overall, there was some degree of differentiation between the prompt groups, with P.1 and P.2 having a higher distance on average from the P.3-P.5 replicates. However, the dendrogram also revealed numerous cases of convergence between prompt group replicates (Figure 4B). For example, 3 replicates from P.4 (P.4.9, P.4.5, P.4.3) were identical in scoring to a replicate from P.5 (P.5.3), two replicates from P.3 (P.3.4, P.3.2) and one replicate from P.1 (P.1.9). Similar cases of convergence in scoring groups were found throughout the dendrogram. Collectively, these results do not support the hypothesis that more specific prompts"}, {"title": "4.3 Does ChatGPT demonstrate any overall strengths or weaknesses in lesson plan design, regardless of prompt specificity?", "content": "Assessing patterns of missing components in the scoring rubrics from prompt outputs revealed high heterogeneity between categories (Figure 5). Categories present in all outputs included meaningful context, teacher input aligning with lesson objectives, and activities appropriate to students' proficiency level. Several categories were also present in almost all cases except a few replicates of P.1 or P.2 including fostering student engagement, showing a connection to the learner's world, establishing a purpose for the lesson in the warm-up, and the integration of ACTFL standards into the closure and independent practice (Figure 5).\nHowever, several categories were conspicuously absent between prompt groups. For example, cultural connection and ACTFL standards being integrated into teacher input were largely restricted to the outputs generated by P.5. Warm-up serving to activate prior knowledge and integration of the ACTFL standards into the focus and review were largely restricted to groups P.1 and P.2. Teacher input engaging learners in interaction was restricted largely to P.1, P.2, and P.5. In all of these examples of heterogeneity between prompt outputs, the appearance of rubric elements was often restricted to only around 50% of the outputs, indicating non-determinism in generated responses."}, {"title": "5. Discussion", "content": "Our results demonstrate several significant aspects of ChatGPT's output in regards to prompt specificity, variability, and possible weaknesses that can guide effective usage. Overall, these results suggest a high degree of utility for developing foreign language teacher lesson plans, confirming existing research that zero-shot approaches can produce quality outputs (Ateia and Kruschwitz 2023; Hu et al. 2023, 2024). However, our results underscored that simply providing additional context and specificity to a prompt did not guarantee a concomitant increase in output quality. On the one hand, we observed a moderate degree of convergence in the outputs between the prompt categories, with several features of the scoring criteria present in all prompt outputs. On the other hand, we observed extreme cases of variability in which the same prompt yielded outputs perfectly or almost perfectly aligned with desired outcomes as well as prompts that missed numerous criteria. In several cases, this variability reflected outputs aligned with anachronistic pedagogical practices that no longer reflect best-practices. This suggests the presence of possible biases in the neural network stemming from training on historic data that may steer users towards research-rebuked teaching practices. Whether such biases permeate other aspects of instructional design requires additional study.\n5.1 Increasing specificity does not always lead to increasing quality\nDetermining the necessary specificity for a desired output is considered a critical aspect of prompting for AI models (Krause, 2023). However, we found that the relationship between the specificity of prompt and output score or generated lesson plans was not linear. For example, the average score of the output provided as a result of P.3 decreased from P.2 and remained virtually unchanged (0.2 difference) for P.4. In P.3, we provided a lesson plan template for the first time. This embedded description of the task guided the computational response of the AI model towards less detailed plans. Additionally, the opening section of a lesson plan, called the Warm-up or Focus and Review, was labeled as the hook (Wiggins & McTighe, 2005). ChatGPT seemed to interpret this terminology as input that is teacher-centered and does not require student interaction, which resulted in lower scores for related portions of the lesson plan. Consequently, once the lesson plan template was included as input beginning with P.3, the warmup/hook became almost formulaic. Though technically more specific, this decrease in score suggests that simply including a lesson plan template for context is not an effective strategy for increasing quality. Instead, including scoring criteria increased output quality Just as students are more effective when given scoring criteria along with the description of an assignment (Jonsson & Svingby, 2007), so too was ChatGPT. This was readily observed with the addition of the checklist with required criteria in P.5 that raised the mean to the overall highest of 21.6/25, suggesting that such input may be critical for optimizing effective AI-based lesson plan generation.\nTranslating these findings into practical implications for teachers highlights just how much context matters. When prompting ChatGPT to create foreign language lesson plans, teachers should include a meaningful context, lesson objectives, and the scoring criteria in the prompt. The inclusion of the first two in the prompt resulted in high scores for these categories across all prompts. The inclusion of scoring criteria in P.5 resulted in the highest average score on output for that prompt. If one's school or district requires the lesson in a particular format, then including that format can be useful as well. However, specialized terminology like the term"}, {"title": "5.2 Variable responses are an inherent feature of AI output", "content": "Generative Al models break down prompts into textual representation and logic to parse the tasks required of them. As users add specificity and context to a prompt, this provides additional logic to guide the model towards desirable outputs (Giray, 2023). However, our work underscores that outputs from the same prompt are often not deterministic. In our case, scoring of prompt output revealed high variance in scores within prompt groups (Figure 1A). Simply by chance, some outputs from the same prompt missed over 25% of scoring criteria while others were over 90% complete. Similar variability and lack of deterministic output has been observed in other fields and is often attributed to similar weights between word pattern choices or the temperature value in the transformer model (Ouyang et al., 2023). Even at temperature settings of 0, models like ChatGPT have been found to return non-deterministic and erroneous outputs (Lubiana et al. 2023). Emphasizing to teachers the critical importance of evaluating and engaging with AI outputs should be an essential component of training AI competency, as instructional materials generated by teachers using the same prompt have the potential to widely vary in quality.\nA possible approach to mitigating variability could involve training teachers to use additional follow-ups to improve output. In the present study, we did not engage in back-and-forth dialogue with the chatbot as this begins to introduce too much non-tractable variability. In contrast, once a chatbot produces a lesson plan, a teacher might follow-up with a prompt like, \u201cPlease engage students in interaction during the Teacher Input\u201d or \u201cPlease script out questions for the teacher to ask during the Teacher Input\u201d or \u201cPlease add a Closure in which the teacher asks students how dining practices might differ in Costa Rica and the United States.\u201d However, it is unlikely that such an approach would always yield desired outcomes. Based on the various temperature settings in generative models such as ChatGPT, there is no guarantee that outputs will be of a high quality in every instance. This is true even with refinement and iterative prompting and coincides with a general impediment to AI tool use: adopting AI content without critical evaluation (Dell'Acqua et al., 2023). For teachers, we advocate that this potential for a lack of engagement (Dell'Acqua et al., 2023) with AI tools and their output should be considered a key area of AI literacy and competency training. In practice, this would entail not only training teachers on how to write specific prompts, but also how to evaluate and revise the output produced. It is not clear how teachers currently using generative AI tools are evaluating outputs, and research on this topic is needed."}, {"title": "5.3 Weaknesses in ChatGPT's output often reflected historic shifts in instructional design", "content": "Generative Al models such as ChatGPT were trained on vast collections of text to develop mathematical associations between words that form the basis of each output. However, such training can induce algorithmic biases towards over-represented associations that do not reflect current knowledge or practices. Algorithmic bias may be particularly problematic for education, as there have been dramatic shifts in pedagogical practices between the 20th and 21st century. Our results reveal several potential sources of bias that can impact foreign language lesson plan generation. One of the most striking of those aspects involved including tasks that prompt students to make connections between their own cultural perspectives and those of the target culture in the target language. This checklist criteria were met in only five of the 50 lesson plans. Even when it was included in the input scoring rubric for P.5, output only included this objective 50% of the time. This output reflects the often-criticized historic practice in foreign language instruction of prioritizing the teaching of language at the expense of culture (Kramsch, 1991), and also the historic difficulties preservice teachers experienced with cultural comparisons as lesson components on their licensure exam (Hildebrandt & Swanson, 2014). This finding supports the view that the teaching of culture is an area within which current AI falls short (Kern, forthcoming), and suggests that this aspect of lesson planning requires human attention when using generative AI.\nIn addition to the integration of culture, theories of language acquisition and development have changed dramatically in the second half of this century (Lightbown & Spada, 2021). However, ChatGPT output related to engaging students in communicative activities in all sections of the lesson plan sometimes reflected approaches not aligned with contemporary research-backed practices. In particular, the audio-lingual method, a behaviorist approach popular in the 1970's in which learners listened to and repeated pre-scripted dialogues, seemed to influence some of the lesson plans. In one case, students were given a role play script to rehearse and present. In others, the plans asked the teacher to show flashcards for students to practice pronunciation. These historic practices appeared regardless of prompt specificity, even though the majority of output guided practice and independent practice activities were communicative and engaged students in interaction. Extrapolating from our results and considering the number of teachers who could use ChatGPT for lesson plan design, there is a high chance that similar relics of outdated methods will appear across outputs. Should users not look critically at generated output, this raises the concern that these atavisms might become pervasive features of pedagogical materials."}, {"title": "6. Conclusion", "content": "Many teachers and teacher educators are resistant to ChatGPT without realizing the intense pressure that language teachers face daily. Foreign language teachers often do not have access to high-quality curricula and there is a global trend towards abridging teacher preparation programs to eliminate barriers to the profession. Our results demonstrate that ChatGPT can streamline lesson planning thereby mitigating an aspect of professional burden. However, awareness of possible biases towards 20th century pedagogical practices that can occur by chance mark an urgent need in teacher AI literacy. Current rates of error that can be expected stochastically in foreign language instructional materials remain unknown because evaluations of models such as ChatGPT have been focused on single prompt outputs. Our work suggests that historical biases could be pervasive and should be expected to occur in output by chance. To avoid reintroducing research-rejected practices into modern curricula, it is essential that teachers be trained to modify and revise outputs."}]}