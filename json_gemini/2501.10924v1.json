{"title": "Adaptive Target Localization under Uncertainty using\nMulti-Agent Deep Reinforcement Learning with\nKnowledge Transfer", "authors": ["Ahmed Alaghaa", "Rabeb Mizounibe", "Shakti Singhb,c", "Jamal Bentaharb,d,a", "Hadi Otrokb,c"], "abstract": "Target localization is a critical task in sensitive applications, where multiple sensing\nagents communicate and collaborate to identify the target location based on sensor\nreadings. Existing approaches investigated the use of Multi-Agent Deep Reinforce-\nment Learning (MADRL) to tackle target localization. Nevertheless, these methods do\nnot consider practical uncertainties, like false alarms when the target does not exist or\nwhen it is unreachable due to environmental complexities. To address these drawbacks,\nthis work proposes a novel MADRL-based method for target localization in uncertain\nenvironments. The proposed MADRL method employs Proximal Policy Optimization\nto optimize the decision-making of sensing agents, which is represented in the form\nof an actor-critic structure using Convolutional Neural Networks. The observations\nof the agents are designed in an optimized manner to capture essential information in\nthe environment, and a team-based reward functions is proposed to produce coopera-\ntive agents. The MADRL method covers three action dimensionalities that control the\nagents' mobility to search the area for the target, detect its existence, and determine its\nreachability. Using the concept of Transfer Learning, a Deep Learning model builds\non the knowledge from the MADRL model to accurately estimating the target location\nif it is unreachable, resulting in shared representations between the models for faster\nlearning and lower computational complexity. Collectively, the final combined model\nis capable of searching for the target, determining its existence and reachability, and\nestimating its location accurately. The proposed method is tested using a radioactive\ntarget localization environment and benchmarked against existing methods, showing\nits efficacy.", "sections": [{"title": "1. Introduction", "content": "Target localization is a task concerned with determining the precise location of a\nspecific target within a certain environment. It is a fundamental task in many applica-"}, {"title": "2. Problem Definition", "content": "In target localization problems, a team of N agents is deployed in an Area of Interest\n(AoI) with the task of cooperating to find the unknown target location in a timely\nand resource-efficient manner. Each agent is equipped with sensors that help acquire\ninformation (observations) about the target, the environment, and the other agents. The\nobservations are acquired progressively and utilized for decision-making to take actions\nin the environment, with the aim of reaching the unknown target location. At each time\nstep, the agents update their observations and use the updated information to act in the\nenvironment. It is assumed that agents can communicate and share information, as well\nas store previously collected observations.\nFigure 1 shows representations of the different possible scenarios for the target lo-\ncalization problem in environments with uncertainties. In realistic environments, the\nagents might encounter several obstacles that hinder their movements and affect their\ndecision-making and planning (Fig. 1a). In addition to obstructing the movement of\nthe agents, obstacles also attenuate the sensor readings, making it more difficult to de-\ntect signals emitted from the target. As a result, an agent with a higher reading might\nhave to travel a larger distance to reach the target when compared to an agent with a\nlower reading. For example, the black agent in Fig. la has a higher sensor reading than\nthe red agent (since it is closer to the target), but the red agent needs less time to reach\nthe target. Such cases increase the difficulty of the search process and require more co-\noperation and planning from the agents. Moreover, since target localization is common\nin applications with harsh environments (i.e. search and rescue and fire localization),\nthere could be cases where the target can be detected but is unreachable (Fig. 1b).\nIn such cases, the agents are required to cooperate to determine that the target exists\nbut is unreachable and then provide an estimate of the target location. Finally, there\nare cases where the agents are deployed with uncertainty about the target's existence\n(Fig. 1c). Here, instead of blindly searching the entire Aol, the agents should coop-\nerate to intelligently determine whether a target exists or not with minimum resource\nconsumption.\nIn all scenarios, it is desired for the task to be executed as quickly as possible and\nwith low resource consumption, which requires the agents to cooperate. Although the\ntarget location is unknown, the agents can use their observations to quickly localize the\ntarget. Some of these observations, such as sensor readings, can guide the agents to-\nwards areas with high probabilities of containing the target. Other observations, such as\nthe distribution of other agents and the layout of the environment, can help the agents\ncoordinate and optimize their collective search paths as well as their resource con-\nsumption. In this context, agents can cooperate to expedite localization or to conserve\nresources. Resource conservation can be achieved if agents assess their contributions\nto the task and remain idle when they are not needed. For the scenario in Fig. la as an\nexample, given the layout of the environment and that the red and black agents have\ninformative sensor readings, the green agent should intelligently decide to maintain an\nidle state while the other agents search the environment, since it cannot contribute to\nthe task. In cases where all the agents do not have informative readings, they need to\ncooperate and spread to better search the environment to collect more readings or reach\nconclusions about the existence and reachability of the target."}, {"title": "3. Related Work", "content": "The initial works addressing the target localization problem focused on reducing\nthe target location estimation error through advanced mathematical data fusion tech-"}, {"title": "4. Proposed System", "content": "This section presents the MADRL method used to intelligently produce sensing\nagents capable of tackling the different complex scenarios of target localization. An\noverview of the final proposed model is presented in Fig. 2. At each timestep, and for\na given agent, the MADRL model translates its observations into one of three possible\naction dimensionalities: Movement, Detection, and Reachability. Movement actions\nare responsible for controlling the mobility of the sensing agent in the environment.\nA detection action is concerned with flagging the existence of a target in the Aol,\nwith the aim of conserving resources if the target does not exist. A reachability action\ndetermines if a target exists but is unreachable due to environment complexity. Given\nthat the target is unreachable, an estimation process is triggered, which is responsible\nfor estimating the location of the unreachable target. The key challenge here is to\nproduce all actions, as well as the estimation process, in one AI model capable of\ntranslating an agent's observations, while ensuring its cooperation with other sensing\nagents. This section formulates the MADRL problem and discusses the modeling of\nthe MADRL methods used to obtain the final model."}, {"title": "4.1. MADRL Formulation and Policy Optimization", "content": "In the context of MADRL, Markov Games are generally used to extend Markov\nDecision Processes (MDPs) into multi-agent settings [30, 31, 32]. In the problem of\ntarget localization, the state at a given instant is defined by the distribution of agents\nand obstacles, as well as the target location. Given that the location of the target is\nunknown, the problem is represented as a Partially Observable Markov Game (POMG).\nThe different components of a POMG include a set of S finite states, finite action sets\n$A_1, A_2, ..., A_n$ for each of the N agents, finite observation sets $O_1, O_2, ..., O_N$, a state\ntransition function $P(s', o | s, a)$ that computes the probability of ending up in state s'\nwith observation o after taking action a in state s, a reward function R : S \u00d7 A \u2192 R,\nand a discount factor \u03b3 \u2208 [0, 1]. Here, a = ($a_1$, ..., $a_n$) and o = ($o_1$, ..., $o_N$) denote joint\nactions and observations from the N agents at a given instant. In MADRL settings,\nthe target localization task occurs in discrete steps. During each step, agent i uses its\npolicy $\u03c0_i$ : $O_i$ \u00d7 $A_i$ \u2192 [0, 1] to translate an observation $o_i$ \u2208 $O_i$ into an action $a_i$ \u2208 $A_i$,\nand receives a reward $r_i$. The primary aim of each agent is to maximize the cumulative\nrewards earned throughout the duration of an episode.\nUsing the collected experiences, PPO [17] is utilized to update the decision-making\npolicies of the agents. PPO is a policy gradient (PG) method that has two components,\nan actor and a critic. The actor (policy) network takes the current observations as in-\nput and produces a probability distribution over the possible actions. The critic (value\nfunction) predicts the future rewards, which is used in updating the actor network. The\nobjective is to optimize the actor policy $\u03c0_\u03b8$, parametrized by \u03b8, to maximize the cumula-\ntive rewards in an episode. PPO strikes a balance between simplicity and performance\nby using a clipped surrogate objective that ensures stable and efficient policy updates,\nwhich is given as:\n$L^{CLIP}(\u03b8) = \\hat{E}_t [min(r_t(\u03b8)\\hat{A}_t, clip(r_t(\u03b8), 1 \u2013 \u03b5, 1 + \u03b5)\\hat{A}_t)]$\nwhere \u03b5 is a hyperparameter that controls the clipping. On the other hand, $r_t(\u03b8) = \\frac{\u03c0_\u03b8(a_t/s_t)}{\u03c0_{\u03b8old}(a_t/s_t)}$\nis a probability ratio between the old and the current policy of taking a given\naction. The advantage function estimate ($\\hat{A}_t$) quantifies how good taking a specific\naction $a_t$ in state $s_t$ is. Here, $\\hat{A}_t$ is estimated using Generalized Advantage Estimate\n(GAE) [33]. After collecting H steps of experience in the environment (horizon length),\nPPO is used to update the policy, where the estimator of $\\hat{A}_t$ is computed as:\n$\\hat{A}^{GAE(\u03b3,\u03bb)} = \\sum_{l=0}^H(\u03b3\u03bb)^l \u03b4_{t+l+1}$,\n$\u03b4_{t+1} := r_{t+1} + \u03b3V(s_{t+l+1}) - V(s_{t+l})$\nwhere $\u03b4_{l+1}$ is the Temporal Difference (TD) error, \u03b3 \u2208 [0, 1] is the discount factor that\ncontrols the weight of future rewards, \u03bb \u2208 [0, 1] is a parameter that controls the bias-\nvariance trad-off, and V($s_t$) is the value function estimate.\nAs per the original work in [17], the policy surrogate is further improved by con-\nsidering two additional terms, namely $L^{VF} (\u03b8)$ and S [$\u03c0_\u03b8$]($s_t$), which results in:\n$L^{CLIP+VF+S} (\u03b8) = \\hat{E}_t [L^{CLIP} (\u03b8) \u2013 c_1L^{VF} (\u03b8) + c_2S [\u03c0_\u03b8](s_t)]$\nwhere $c_1$ and $c_2$ are coefficients. Here, $L^{VF} = (V_\u03b8(s_t) \u2013 V_{targ})^2$ is the squared-error\nloss that ensures the value function is accurately approximated, while S encourages\nexploration by increasing the entropy of the policy."}, {"title": "4.2. Observation Space", "content": "An efficient decision-making process in target localization depends on the collected\nsensor readings, the distribution of sensing agents and the areas they visited, and the\nlayout of the environment and its complexities. For efficient execution of the given\ntask, all such observations need to be tracked and stored over time. To tackle this, the\nobservations of each agent are represented as 2D maps that capture both current and\npast spatial information collected throughout the task. These maps encapsulate infor-\nmation about the agents' locations, the collected readings, previously visited areas, and\nthe layout of the environment, which are needed for the decision-making process. To\nachieve this, the Aol is regarded as an h \u00d7 w grid, and the observations are represented\nas stacks of h \u00d7 w maps showing spatial information throughout the AoI. In this work,\nat a given timestep t, the agent collects information and updates 5 observations, which\nare shown in Fig. 3. Among these observations, the Location observation indicates\nthe location of the agent in the AoI. The Team Distribution observation highlights the\nlocations of other agents in the team with respect to the AoI. The Visit History obser-\nvation records the visiting frequency of the grid elements by the team members. The\nReadings History observation tracks the readings collected and their spatial pattern in\nthe AoI. The Environment Layout observation highlights the different obstacles in the\nenvironment. All of those observations, given as 2D maps, undergo normalization be-\nfore going to the agent's policy which ensures learning stability and faster convergence\n[34].\nIt is worth mentioning that due to temporal aspect of the problem, using methods\nin Recurrent Neural Networks (RNNS), such as Gated Recurrent Units (GRUs) and\nLong Short-Term Memory (LSTM), are popular in DRL applications. This is because\nsuch methods efficiently capture the temporal aspects of the decision-making process,\nallowing agents to correlate long sequences of actions. However, target localization\nproblems are also spatial in nature, which is an added complexity when using RNNs.\nInstead, the proposed approach accumulates historical data in 2D maps to remove the\ntemporal dependency. For a given timestep, the agent has access to its previously\ncollected and stored data, in addition to new data collected in the current step. This\nallows the agent to act solely based on its current observation (that already includes\npreviously accumulated data), which transforms the problem into a less complex one\nthat is focused purely on spatial correlation leveraging simple CNNs. This is also\ncommon in existing literature as shown in [10, 35]. Here, the collected information\ncould be accumulated over time throughout the task by just updating each observation\nwithout changing its dimensionality. Target localization tasks rely heavily on the spatial\ncorrelation of sensor readings and agents' distribution, which can only be captured in\n2D maps. The aforementioned observations are meant to guide the agents towards\nefficient target localization. Ideally, the readings history map should guide the agent in\nthe direction of higher data readings. At each step, the agents update this map with the\nlatest sensor readings. However, in many cases, the agents might not have sufficient\nreadings (because of being distant from the target for instance) and hence require more\nexploration of the environment. Here, the location, team distribution, and visit history\nmaps help the agents coordinate to explore the environment efficiently. Knowing the\nteam distribution also helps an agent preserve resources when possible if it is evident\nthat the other agents are getting closer to the target and that the agent can no longer\nbe of benefit to the search task. The environment layout map is essential in translating\nall the aforementioned observations since it significantly affects the readings collected\nas well as the environment exploration planning process. In this work, one assumption\nis that the agents can communicate and share information to be used in updating the\naforementioned maps.\nTo further optimize the learning and decision-making processes and reduce their\ncomplexity, the five observations undergo further pre-processing steps. Here, the aim\nis to reduce the dimensionality of the observations while maintaining their informa-\ntiveness. To this end, as seen in Fig. 3, the five original observations are reduced into\nten optimized ones, which are split into global and local observations. The agent can\nuse the local observations to take actions based on its surroundings, such as follow-\ning higher readings or avoiding obstacles. Global observations summarize the environ-\nment's spatial features, which help in making coordination and planning decisions. Out\nof the reduced observations, the first 9 have a dimension of n \u00d7 n, where h, w > n > 1\nand n is odd. Here, n is considered a tunable hyperparameter, where smaller values\nentail lower computational complexity and higher information loss, when compared\nto higher values. At timestep t, local observations are acquired by capturing an\u00d7n\nwindow that is centered at the agent's position from the corresponding original ob-\nservation, capturing surrounding information. Global observations are computed by\ndownsampling the original observations using bi-linear interpolation. For the environ-\nment layout, the use of generic downsampling proved inefficient due to significant loss\nof information, particularly with the increasing complexity of the environment with\nmore obstacles and narrow openings. To circumvent this issue, we create embeddings\nthat encapsulate essential information from the environment at a lower dimensionality\nusing Convolutional AutoEncoders (CAE). A CAE is composed of an encoder and a\ndecoder. An encoder is a CNN responsible for converting the input map into a 1D vec-\ntor (embedding) through several convolution and fully connected layers. The decoder is\nresponsible for attempting to generate the original input map starting from the embed-\nding. The decoder's job is to assess the power of the encoder in creating embeddings\nthat properly represent the original input at a lower dimensionality. In this work the\nencoder consists of 3 convolutional and 2 fully connected (FC) layers, where the last\nFC layer represents the embedding. The decoder takes the embedding as an input into\n2 FC layers followed by 2 convolutional layers that produce the reconstructed map.\nA synthetic dataset of different environment layouts is used to train the CAE before\nthe MADRL training. The training of the CAE aims to minimize the reconstruction\nerror between the input and the reconstructed map. After the training is completed,\nthe encoder can generate embeddings for the environment layout, while the decoder is\ndiscarded."}, {"title": "4.3. Action Space", "content": "To address the complex scenarios of target localization discussed in Section 2, the\naction space at a given step for an agent is divided into mobility, detection, and reach-\nability actions. In terms of mobility, the agent has a fixed speed and decides on the\ndirection of movement. Given D possible discrete directions {1, 2, di, ..., D}, the\nmovement angle is given as:\n$0 = 2 \\frac{d_i}{D} 2\u03c0$\nwhere the hyperparameter D determines how detailed the movement is. In this work,\nwe use D = 8, which allows the agent to move in one of the cardinal or ordinal di-\nrections. Assuming a fixed speed, the agent can move a fixed distance in one of these\ndirections with the aim of contributing to the localization task. It has been found in this\nwork that discretizing the direction of movement into 8 possible values is sufficient.\nOn the other hand, choosing to stay idle, if needed, helps in preserving resources.\nThe detection and reachability actions are each represented by a binary value to\nflag the existence of the target and its reachability, respectively. At a given time step\nt, an agent could determine that the target does not exist and hence set the existence\nflag to 1. Similarly, an agent could determine that a target is unreachable by setting the\nreachability flag to 1.\nThe decision-making process is discretized, where at each step, an agent can choose\nonly one of the 11 actions. If the majority of the agents declare that the target does\nnot exist, the search process stops. Similarly, if the majority declare that the target is\nunreachable, a target estimation process is triggered, which estimates the target location\n(to be explained later in Section 4.5). In certain time steps, some actions may not\nbe possible, such as moving outside the boundaries of the Aol or into a wall. Such\ninvalid actions are masked out during the decision-making process, where the agent\nonly chooses from the available actions."}, {"title": "4.4. Policy Networks and Learning Process", "content": "As discussed in Section 4.1, we use PPO to train the MADRL agents. Generally,\nthe actor and critic in PPO are represented by Deep Neural Networks (DNNs). In this\nwork, CNNs are used for the actor and critic, because the observations of each agent\nare represented as 2D maps. CNNs are crucial for the target localization problem as\nthey effectively correlate spatial features within the input maps.\nFig. 4 shows the architecture used for the actor (upper part of the figure), which\nis similar to the LeNet-5 architecture [36]. The network takes the first 9 reduced ob-\nservations as input, which are then processed in convolution and max pooling layers\nfor feature extraction. The embedding for the environment layout observation is con-\ncatenated with the processed and flattened observations, before being fed into the fully\nconnected layers, . The fully connected layers produce 11 outputs, which are fed into a\nsoftmax function that produces a probability distribution for the possible actions. Dur-\ning the decision-making process, the agent samples an action from this distribution,\nwhich is then executed in the environment.\nThe proposed MADRL method uses PPO in multi-agent settings, sometimes re-\nferred to as Multi-Agent PPO (MAPPO), with Centralized Learning and Distributed"}, {"title": "Execution (CLDE) [37]", "content": "Here, a copy of the actor is given to each agent, where agents\nact independently based on their observations. During the training stage, a centralized\ncritic is used to assess the experiences collected by the agents. The architecture of the\ncritic is similar to that of the actor, but with 1 output representing the value function.\nUsing a centralized critic is essential in addressing the non-stationarity problem that\nis common in MADRL due to the influence agents have on each other's view of the\nenvironment [38]. The value function, which is the critic's output, is used during the\nPPO process to update the actor and the critic.\nA team-based shaped reward function is proposed in this work to guide the learn-\ning. A team-based (joint) reward function gives equal rewards to the agents according\nto the collective behavior. As a result, the agents would be motivated to act in a way\nthat benefits the entire team. A shaped reward gives more frequent feedback to the\nagents during the task, which helps speed up the learning process. Following the joint\naction taken by the agents in a given time step, the environment returns to all the agents\na similar reward. To achieve all the desired behaviors, the reward function after step t\nis given as:\n$\\begin{cases}\n-Q & \\text{if flags are incorrect} \\\\\n-v + 1 & \\text{if min}(D_t) < \\text{min}(D_{t-1}) \\\\\n-v-1 & \\text{otherwise}\n\\end{cases}$\nwhere Q is a large penalty, v is the total number of mobility actions taken, and D\u2081\nrepresents the set of distances between the agents and the target. In the function, the\nfirst condition gives the team a high negative reward (in this work Q = 500) if a target\nis incorrectly declared non-existent or unreachable. Alternatively, in the second and\nthird conditions, the agents are rewarded according to their proximity to the target and\ntheir resource consumption. At each timestep, the agents are penalized by (-v), where\nv is the number of agents who moved in the environment. This indicates resource\nconsumption, which is essential in pushing the agents towards finishing the task as fast\nas possible. It also motivates the agents to only move in the environment if needed\nand stay idle otherwise. The agents receive an additional positive or negative reward\n(\u00b11) based on their proximity to the target. The reward would be positive if they have\nmoved closer to the target from step t - 1 to t, otherwise they receive a negative reward.\nThe team is considered to have moved closer to the target if the nearest agent(s) at step\nt - 1 took a mobility action towards the target at step t, meaning min(D\u2081) < min(Dt-1).\nDue to the environment complexity and the existence of obstacles, it is inaccurate to\ncompute the travel distance between an agent and the target using generic methods like\nManhattan and Euclidean distance. Instead, Breadth First Search (BFS) is used, which\ndetermines reward calculations. BFS explores different paths starting from an initial\nnode until the goal is reached, aiming to find the shortest path. It is worth mentioning\nthat the agents have no knowledge of the location of the target, and act only based on\ntheir observations. The reward function serves as a feedback mechanism, used only\nduring the training stage, to improve the policies of the agents using PPO.\nThe training process is done using a CLDE method [37], in which the MADRL\nmodel is optimized is based on PPO. In this method, the agents act in a distributed\nmanner in the environment according to their actor network copies. The experiences\ncollected by the agents are then gathered and used centrally for updating the actor and\ncritic. Once the training is over (which usually occurs in simulations), the final model\nis deployed on the sensing agents which act independently based on their observations.\nAlgorithm 1 explains the training process in CLDE-based PPO. The learning takes\nplace over episodes of the problem, where the environment is reset to a different initial\nstate at the beginning of each episode. During episode step i, each agent j samples an\naction a by feeding its observations o to its actor network. The agents then step in\nthe environment with their actions, which then returns a reward value r\u00b9, a new set of\nobservations (0)+1, and a termination flag d' indicating if the episode has ended. When\nthe number of training timesteps reaches the horizon (H), the actor and critic networks\nare updated using the gathered experiences. The process repeats for a pre-defined total\nnumber of time steps."}, {"title": "4.5. Target Estimation with Transfer Learning", "content": "The MADRL model discussed in Section 4.4 is responsible for regulating the con-\ntinuous decision-making process for each agent throughout the localization task. How-\never, if a target is determined unreachable, it is essential to provide an estimate for\nits location. To reduce the complexities of the MADRL training process and the final\nmodel, the target estimation is not performed at each time step, but only when target\nunreachability is determined. To achieve this, a separate model is trained using a typi-\ncal DL process with TL. Here, based on the observations collected until unreachability\nis determined, the aim is to provide an estimate for the (x,y) coordinates of the target.\nA dataset is pre-built using sets of observations and the corresponding target locations"}, {"title": "Algorithm 1", "content": ": The training process of CLDE PPO\nInput: Initialized actor and critic\n1: for Step \u2208 TotalSteps do:\n2:  0\u00ba = ResetEnvironment()\n3:  for i = 0, 1, 2, ..., MaxEpisodeLength:\n4:   for j = 1, 2, ..., N:\n5:    a = Sample(Actor(0))\n6:   end for\n7:   a = [a\u2081, a, ...]\n8:   oi+1, ri, di = Step(a')\n9:   Save Experiences\n10:   if Step % H == 0 then:\n11:    Update_PPO()\n12:   end if\n13:   if d' == 1 then break\n14:  end for\n15: end for\nto be estimated, which are used to train the DL model to estimate the location based\non the observations. To reduce the training and deployment complexities, we use TL\nto utilize knowledge from the previously obtained MADRL model, as shown in Fig.\n4. Here, rather than training a new DL model from scratch for target estimation, the\nweights of the MADRL actor network are initially copied into the new DL model.\nDuring the training process, these layers are frozen, i.e. not trained, and only the last\nfully connected layer is trained. This is viable because the initial layers, especially\nthe convolution layers, have already been trained to extract features related to target\nlocalization in the MADRL process. Such features would still be similar in the target\nestimation model, and hence only a final classification layer would be sufficient. Fol-\nlowing the training process, and since both the MADRL and DL models have common\ninitial layers, they can be combined into one model with two output heads; one for\ncontinuous decision making (mobility, detection, and reachability) and one for target\nestimation, as shown in Fig 5. Both action heads take the same set of extracted fea-\ntures from the initial layers, but only differ in the weights of final layer, with the target\nestimation output head only getting triggered when unreachability is determined.\nIt is worth mentioning that TL is used solely during the training stage, which gen-\nerally occurs offline before deployment, managed by the system administrator. Hence,\nthere are no privacy concerns during the training when it comes to transferring knowl-\nedge from the MADRL model to the estimation model. In case this work is extrapolated\nsuch that the training itself happens in a distributed manner with knowledge exchange\nbetween the agents, existing methods for privacy preservation can be used, such as\ndifferential privacy for TL in MADRL [39, 40]."}, {"title": "5. Simulation and Evaluation", "content": "Extensive experiments are conducted in this section to validate the efficiency of the\nproposed method, as well as benchmark it against existing works in the literature. All\nthe simulations have been conducted using an Intel E5-2650 v4 Broadwell workstation\nequipped with a 128 GB RAM, an 800 GB SSD, and an NVIDIA P100 Pascal GPU\n(16 GB HBM2 memory)."}, {"title": "5.1. Simulation Environment", "content": "To validate the proposed methods, a sample environment of radioactive source lo-\ncalization is used, where the agents are tasked with localizing a radiation source in an\narea of size 1km \u00d7 1km. The environment is simulated using radiation physics [19, 41],\nwhere the readings collected by the detectors carried by each agent follow a Poisson\ndistribution. Given a certain source strength S and a distance d from the radiation\nsource to the detector, the photon counts per minute (CPM) are given as:\nCPM C $\\frac{S}{d^2}$\nThis essentially indicates that radiation sources with higher strength can be better\ndetected from a longer distance. Due to the existence of obstacles, the CPM collected\nby a certain detector is influenced based on the number of obstacles. Generally, each\nobstacle has an attenuation factor that represents the absorption/reflection of photons.\nIn this work, we assume constant attenuation of \u00b5 = 0.1 for each obstacle. While the\nproposed methods are tested on a radiation localization environment, they are still valid\nfor any localization environment. This is mainly because data collected in localization\nproblems are always a function of the distance between the target and the agent; the\ncloser the agent is the higher the readings, based on the Inverse Square Law. Several\napplications are based on these representations, including radiation, sound, and heat\nlocalization [12, 42].\nFor all the following experiments, each model is trained for 30 million environment\nsteps using MADRL. At the beginning of each episode, the target location, the agents'\ninitial distribution, and the environment layout (with varying number of obstacles) are"}, {"title": "5.2. MADRL Performance Analysis", "content": "The performance of the proposed MADRL model is analyzed in terms of the col-\nlected reward", "strength": 1}, {"strength": 1}]}