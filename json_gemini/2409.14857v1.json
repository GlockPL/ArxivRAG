{"title": "Embedding Knowledge Graphs in Function Spaces", "authors": ["Louis Mozart Kamdem Teyou", "Caglar Demir", "Axel-Cyrille Ngonga Ngomo"], "abstract": "We introduce a novel embedding method diverging from conventional approaches by operating within function spaces of finite dimension rather than finite vector space, thus departing significantly from standard knowledge graph embedding techniques. Initially employing polynomial functions to compute embeddings, we progress to more intricate representations using neural networks with varying layer complexities. We argue that employing functions for embedding computation enhances expressiveness and allows for more degrees of freedom, enabling operations such as composition, derivatives and primitive of entities representation. Additionally, we meticulously outline the step-by-step construction of our approach and provide code for reproducibility, thereby facilitating further exploration and application in the field.", "sections": [{"title": "1 INTRODUCTION", "content": "A knowledge graph (KG) serves as a structured representation of data, aiming to encapsulate and convey real-world knowledge [19]. Comprising triples that articulate facts about the world, KG data are pivotal, in organizing information. For example, the triple (Obama, president_of, America) represents the fact that Barack Obama was the president of the United States. Consequently, KGs find applications in various domains such as web search [35], recommender systems [28, 40], and natural language processing [31].\nGiven that KGs are essentially composed of strings or characters, knowledge graph embedding (KGE) involves mapping entities and relations from a KG into a vector space [39]. This transformation facilitates computational operations, enabling the application of machine learning and deep learning techniques to extract insights from the KG. Hence, an effective KGE model should strive to maintain the properties and semantics inherent in the original KG.\nKnowledge graphs are typically represented as K \u2286 & \u00d7 R \u00d7 &, where & and R represent sets of entities and relations respectively. They are commonly embedded in d-dimensional vector spaces V such as Rd (real numbers), Cd (complex numbers), or even Hd (quaternions) [3, 7, 38]. While such embeddings offer a low-dimensional representation, they treat entities and relations as static vectors, which may limit their ability to capture some dynamics in the knowledge graph. For instance, in real-world applications, relationships between entities can change over time exemplifying temporal dynamics (e.g. transition from \"is_friends_with\" to \"was_friends_with.\") [20, 27]. They can also depend on context, such as in movie recommendations, where the relevance of a movie may be context-dependent (e.g., \"is_interested_in_watching\u201d may vary based on user preferences [5]. Static embeddings may fail to represent such dynamic behaviours accurately, leading to suboptimal performance in tasks like link prediction or knowledge graph completion [25].\nTo mitigate the aforementioned drawback of the existing embedding approaches, in this work, we propose the use of functions to represent entities and relations of a KG, i.e., embedding them in function space. Functional representations of entities and relations provide a compelling alternative to static embeddings. For instance, functions can be time-dependent, they can support compositionality, enabling the combination of simpler functions to represent more complex relationships and entities. Moreover, functions offer a richer and more expressive representation compared to vectors, allowing the modelling of complex interactions and dependencies within KGs. Also, functions offer greater interpretability, as they can be analyzed and understood based on their mathematical properties and behaviour. By examining the functional forms, it would be"}, {"title": "2 RELATED WORK", "content": "As a reminder, several categories of KGE models can be found in the literature. This includes:\nTranslational models. The foundational work in translational models was established by TransE [3]. In TransE, given a triple (h, r, t) \u2208 K, the optimization objective involves minimizing the score ||h + r \u2212 t||, where h, r, t \u2208 Rd. A higher score is assigned if (h, r, t) holds in K, and a lower score otherwise. To resolve TransE shortcomings e.g. TransE cannot model reflexive relationship [8, 21, 41]. Some variants have been proposed with similar ideas but different projection strategies. TransH [41] embeds knowledge graphs by projecting entities and relations onto hyperplanes\nRotational models. In contrast to translational models, rotational models [5], use the power of bilinear transformation for capturing complex interactions and correlations between entities and relations in knowledge graphs. This category of model was inaugurated by RESCAL [31] which models entities and relations using matrices, representing relationships as bilinear interactions between entity vectors. DistMult [43] simplifies RESCAL by Representing entities and relations as low-dimensional vectors instead of matrices, employing the dot product to compute scores for triples. Although DistMult is very accurate in handling symmetric relations, it performs poorly for anti-symmetric relations. ComplEx [38] tackles this drawback and uses complex embeddings to model asymmetric relations, capturing both the interactions and correlations between entities and relations. Similarly to Complex, RotatE [36] embeds entities and relations into complex space, representing relationships as rotations from head to tail entities. A variation of RotatE is RotE [7] which focuses on learning rotation operations to capture relational patterns. QuatE [45], extends RotatE, DistMult and Complex by using quaternion embeddings, enabling more expressive representations of relationships in hypercomplex space. OctE Further extends QuatE by employing octonion embeddings, offering even richer and more complex representations of relationships. However, the scaling effect in the octonion and quaternion space can be a bottleneck for QuatE and OctE hence, QMult and OMult [10] solve this issue thanks to the batch normalization technique. Dual quaternion methods like DualE [5] use dual quaternions [24] to embed entities and relations, offering a representation that combines the advantages of translation and rotation.\nHyperbolic models. The literature also features hyperbolic embedding methods that leverage hyperbolic geometry's properties to capture hierarchical structures in KGs. Among them, RotH [7] extends Rotate by representing entities and relations in hyperbolic space. MuRP [1], which maps entities and relations from a knowledge graph onto a hyperbolic space. MuRE [1], which is a variant of MuRP, operates in Euclidean space, offering a simpler alternative for certain types of knowledge graphs.\nDeep learning models. This group of models uses convolutional neural networks' power to encode entity and relation information simultaneously. Among them we have ConvE [12] which applies 2D convolutional filters to capture local patterns in the entity-relation space, followed by max-pooling to extract global features. Similar to ConvE, ConvO and ConvQ [10] also employ convolutional neural networks, but they are built upon OMult and QMult respectively."}, {"title": "3 FUNCTION SPACE", "content": "A function [16, 18, 26] can be defined as a mathematical relationship or correspondence between two sets of elements. In other words, a function maps each input value to a unique output value. Formally, a function f from a set X (the domain) to a set Y (the range) is denoted as f: X \u2192 Y. It associates each element x in the domain X with one element y in the range Y, written as y = f(x).\nA function space [26] denoted F is a space that consists of functions as its elements. In other words, it is a set of functions with certain properties defined on a given domain. That is, F(X, Y) =\n{f, f : X \u2192 Y}. They can vary widely depending on the specific properties and structures imposed on the functions within them. In this work we focus on function space that consists of p-integrable functions denoted LP [6].", "subsections": [{"title": "3.1 Integrable Functions", "content": "A function f is said to be p-integrable (p \u2208 N) over a domain \u03a9 and we note f \u2208 LP (\u03a9) iff the function f to the power p is Lebesgue integrable [26] that is,\nf\u2208LP (\u03a9) \u21d4 So ||f(x)||Pdx < +\u221e.\nFor any 1 \u2264 p < \u221e, LP spaces are also Banach space. That is, they are all vector space with an equipped norm || . ||p defined as:\nVf \u2208 LP (2), ||f||p = ( So ||f(x)||Pdx) 1/p.\nHowever, it's noteworthy that they do not universally qualify as Hilbert spaces, except in the special case where p = 2, as highlighted by Young [44]. In such case they are equipped with a scalar product (, )L2(\u03a9) defined as:\nVf, g\u2208 L\u00b2 (\u03a9), (f, 9) L2 (2) = So f(x)g(x)dx.\nA straight example of integrable functions is polynomial functions."}, {"title": "3.2 Polynomial Functions", "content": "A polynomial function represents a distinctive class of mathematical functions expressed as the sum of individual terms. We denote the space Rdeg [x] the space of all polynomial functions of degree at most deg \u2208 N with coefficients in R. That is,\nVP \u2208 F (R), P\u2208 Rdeg [x] \u21d4\u2203(ai) deg i=0 \u2208 R, P(x) = \u2211degi=0 aixi.\nClearly, for any bounded domain \u03a9\u2286 R, Rdeg[x] \u2286 L\u00b2(\u03a9), V deg \u2208 N. Therefore, given P1, P2 \u2208 Rdeg [x] such that P\u2081(x) = \u2211degi=0 aixi, and P2(x) = \u2211degi=0 bixi their scalar product can be computed as:\n(P1(x), P2(x)) L2 (\u03a9) = So P1(x)P2(x)dx\n= \u2211deg i,j=0 aibj (Mi+j \u2212 mitj) / 1+i+j\nHere, M and m are respectively the upper and lower bounds of \u03a9. See the appendix in Section 7 to see how this is derived. We can then derive the norm of all polynomial functions as:\n||P1(x)||\u00b2 L2 [0,1] = (P1(x), P1(x)) L2[0,1]\n= \u2211degi=0 a2i/ 1 + 2i + \u2211deg i,j=0 aiaj/ 1+i+j"}, {"title": "3.3 Integral Approximation", "content": "Integral approximation is a numerical technique used to approximate the value of a definite integral by partitioning the interval of integration into subintervals and approximating the area under the curve using rectangles (for rectangle methods) and trapezoid (for trapezoid methods) [4]\nHowever, these approaches have the main disadvantage of not guaranteeing a maximal error. To address this limitation, we follow the methodology of works on numerical integration, by employing adaptive Gaussian quadrature [17] with 5 nodes for the approximation of integrals. In adaptive Gaussian quadrature, an integral over the domain [-1, 1] can be approximated as follows:\nS-1 1 f(x)dx = \u22115k=1 wkf(xk),"}]}, {"title": "4 METHODOLOGY", "content": "Let us consider the problem of embedding a KG into a d- dimensional vector space, ensuring compatibility with existing methods. In Table 2, we show the embedding space where each of the state-of-the-art models operates. We initiate the exploration of functional embeddings by employing polynomial functions. This choice is grounded in the fundamental theorem of real analysis, known as the Weierstrass approximation theorem, which asserts that any function, no matter how complex, can be accurately approximated by polynomial functions [32, 33]. Given their simplicity and universal approximating power, polynomial functions serve as an ideal starting point for our investigation into functional embeddings. We then extend this idea into complex spaces, leveraging the expressive power of trigonometric functions. This serves as a natural progression before delving into more complex function representations, notably employing Neural Networks which is a kind of generalization of polynomial embeddings."}, {"title": "4.1 Embedding with Polynomial Functions", "content": "We first show how to embed KGs using polynomial functions. Specifically, when embedding in polynomial space Rn [x], our approach, FMULT, seamlessly transforms into FMULTn, where n denotes the degree of the polynomial utilized for representing the embeddings.\n\u2022 Let (h, r, t) \u2208 K\n\u2022 We compute the embedding of h, r and t in Rm [x] where m = deg+1 and deg \u2208 N is a hyper-parameter s.t. 0 \u2264 deg \u2264 d-1 as follows:\nh(x) = \u2211degi=0 aixi, r(x) = \u2211degi=0 bixi,t(x) = \u2211degi=0 Cixi\nwhere a(.), b(.) and c(.) \u2208 Rm. Hence,\nFMULTdeg ((h, r, t)) = (h(x) \u2297 r(x), t(x)) L2 (\u03a9)\n= So h(x)r(x)t(x)dx\n= \u2211degi,j,k=0 aibjckSo xi+j+kdx\n= \u2211degi,j,k=0 aibjck(M1+i+j+k \u2212 m1+i+j+k) / 1+i+j+k\nwhere M and m represent the upper and lower bounds of \u03a9."}, {"title": "4.2 Embedding with Trigonometric Functions", "content": "As shown with the Complex model [38], the complex space is a prominent space to compute the embeddings as the imaginary part can be used to model anti-symmetric relations. We extend this idea by using a trigonometric function to represent the embedding. Let be C[x] = {fr + ifc, where i2 = \u22121 and fr, fc \u2208 L\u00b2(\u03a9)}. For any f\u2208 C[x], We have\n||f||2 = So1 f(x)f\u2217(x)dx = ||fr||2L2(\u03a9) + ||fc||2L2(\u03a9)\nwhere f\u2217 = fr \u2212 ifc.\nFor m \u2208 2N, we define the space\nCm [x] = {\u2211nk=0 akeikx, where ak \u2208 Cm/2\u2286 C[x]}\nWhen computing the embedding in complex space, FMULT reduces to FMULT. For simplicity, here we choose \u03a9 = [0, 1].\n\u2022 Let (h, r, t) \u2208 \u039a\n\u2022 We compute the embedding of h, r and t in Cm [x] where m = deg+1 and deg \u2208 N is a hyper-parameter s.t. 0 \u2264 deg \u2264 d-1.\nh(x) = \u2211degi=0 akeikx, r(x) = \u2211degi=0 brekix, t(x) = \u2211degi=0 ckekix\nwhere, a(.), b(.) and c(.) \u2208 Cm/2. Hence,\nFMULT (h, r, t) = Re (h(x) \u2297 r(x), t(x)) L2 (\u03a9)\n= Re ao\u02c9boco + \u2211deg u,v,w=1 aubvw(1 \u2212 ei(u+v+w)) / u + v + W\nStep-by-step derivation of FMULT is provided in the Appendix. Some direct consequences of these scoring formulations are the following theorems:"}, {"title": "4.3 Embedding with Neural Networks", "content": "Building upon the foundation laid by polynomial functions, we now delve into a more sophisticated approach by harnessing the power of Neural Networks for embedding. While polynomial functions offer simplicity and certain approximation capabilities, Neural Networks present a paradigm shift with their ability to capture highly complex and nonlinear relationships inherent in the data as mentioned by Dongare et. al. in [13]. Here, we explore how entities and relations in KGs can be represented as Neural Networks. Figure 1 shows a summarization of FMULT with a single layer.", "subsections": [{"title": "4.3.1 Entities representation.", "content": "Considering the problem of embedding into a d-dimensional vector space, for u = h, r or t, we initialize the weights and bias of the neural network as follows:\nWu = (Wu1 Wu2 ... Wud/2), bu = (bu1 bu2 ... bud/2)T\ni.e. Wu, bu \u2208 Rd/2\nWe represent u as a Neural Network with n layers as:\nu(x) = (\u03c3(Wunx + bun)o ... \u03bf \u03c3(Wu1x + bu1) / \u03c3(Wu2nx + bu2n)o ... \u03bf \u03c3(Wu(n\u22121)+1 x + bu(n\u22121)+1)/ ... / \u03c3(Wuknx + bukn) ... o \u03c3(Wu(k\u22121)n+1 x + bu(k\u22121)n+1)"}, {"title": "4.3.2 Scoring Function Derivation.", "content": "Given a triple (h, r, t) \u2208 K, we define the scoring function of FMULT as follow:\nFMULT((h, r, t)) = (h \u2297 r(x), t(x)) L2(\u03a9)\n= \u2211(k\u22121)n+1 i=1 (h(i) or(i)(x), t(i)(x)) L2(\u03a9)\nhere, h(i)(.), r(i)(.) and t(i)(.) represent the i-th components of h(.), r(.) and t(.) respectively i.e.\nh(i)(x) = (W h i n x + b h i n )o ... \u03bf (W h(i\u22121)n+1 x + b h(i\u22121)n+1)\nr(i)(x) = (W r i n x + b r i n )o ... \u03bf (W r(i\u22121)n+1 x + b r(i\u22121)n+1)\nt(i)(x) = (W t i n x + b t i n )o ... \u03bf (W t(i\u22121)n+1 x + b t(i\u22121)n+1)\nthus,\nFMULT((h, r, t)) = \u2211(k\u22121)n+1 i=1 So h(i)(r(i)(x)) \u00d7 t(i)(x)dx\n= \u2211(k\u22121)n+1 i=1 \u22115j=1 wjh(i)(r(i)(xj)) \u00d7 t(i)(xj)dx,"}]}, {"title": "5 EXPERIMENTS", "content": "In our experiments, we consider our approaches for link prediction tasks, thereby providing a compelling comparison with state-of-the-art models. Prior to performing this task, we first performed an exhaustive grid search to fine-tune FMULT 's and FMULTn's parameters, seeking the optimal configuration. This phase was the most challenging aspect of our research, and the results presented on large datasets do not reflect the optimal parameters due to the substantial time investment required for this process.\nWe perform the evaluations considering benchmark datasets KINSHIP, COUNTRIES, UMLS, NELL-995-h100, NELL-995-h50 and NELL-995-h75. The KINSHIP dataset is a KG data that focuses on familial relationships [14]. It typically contains information about familial connections between individuals, such as parent-child relationships, sibling relationships, grandparent-grandchild relationships, and so on. The UMLS dataset is a KG data which focuses on bio-medicine [37]. Each entity is a medical concept, and the edges represent semantic relationships between these concepts, such as \"is-a,\" \"part-of,\" \"treats,\" etc. The COUNTRIES dataset typically refers to a collection of data related to countries around the"}, {"title": "6 RESULTS AND DISCUSSION", "content": "In Figure 2, we show the embeddings of the COUNTRIES dataset in two dimensions, showcasing the representation of each entity across different variants of the FMULT method and the DistMult model. As expected, DistMult displays the embeddings as 2-dimensional vectors. When applying a sigmoid activation function to FMULT,"}, {"title": "6.1 Embedding Dynamics", "content": "In Figure 2, we show the embeddings of the COUNTRIES dataset in two dimensions, showcasing the representation of each entity across different variants of the FMULT method and the DistMult model. As expected, DistMult displays the embeddings as 2-dimensional vectors. When applying a sigmoid activation function to FMULT, it effectively uses this function to represent the computed embeddings. Conversely, FMULT uses trigonometric functions, resulting in circular embeddings with varying centres and diameters determined by the trained embeddings. Further, FMULT2 employs squared functions to represent the embeddings, resulting in shapes"}, {"title": "6.2 Link Prediction Results: Comparison with\nthe state-of-the-arts", "content": "Table 4 presents the results on the UMLS, KINSHIP, and NELL-995-h100 datasets. On the UMLS dataset, FMULT excels, achieving the highest scores across all metrics, tying with MuRE and outperforming other state-of-the-art models. FMULTn does not have high performance, however, remains competitive. Since the UMLS data contains hierarchical relationships that are often complex, we can conclude that representing entities as neural networks is highly effective in capturing complex and non-linear relationships, which is likely why FMULT achieved outstanding results across all metrics. However, polynomial functions may not be as flexible as neural networks in capturing the nuances of hierarchical and medical data. This might explain why FMULTn performed lower than FMULT and other state-of-the-art models. On the KINSHIP data FMULT leads in MRR, H@1, and H@3, outperforming all other models, while FMULTn lags behind most state-of-the-art models. Note that the KINSHIP dataset is known for its numerous symmetric relationships, such as parent-child and sibling relationships. The result suggests that neural networks can seamlessly handle symmetry, which might explain why FMULT leads in most metrics. However, polynomial functions may not be particularly adept at capturing symmetric relationships, which could explain the middling performance of FMULTn on this dataset leading to lower scores compared to neural networks. A similar observation can be found on the NELL-995-h100 where FMULT demonstrates outstanding performance, leading in MRR, H@1, and H@3, and achieving near-perfect scores in H@10. FMULTn also performs well on this dataset, closely following the top models.\nIn Table 5, we describe the results of the WN18RR and FB15k-237 datasets. On the WN18RR dataset, FMULTn is trained with n = 0 and performed as expected similarly to DistMult but does not outperform the state-of-the-art models. However, FMULT shows a stronger performance in H@3 and H@10, indicating it is particularly effective in retrieving the correct tail entity within the top 3 and top 10 predictions of the dataset. This implies that while it may struggle to predict the exact head entity (lower H@1), it captures useful information that positions the correct entity closer to the top in ranked lists. Duale and Rotate outperform both FMULT and FMULTn in terms of MRR and H@1, which highlights the efficiency of models that capture rotational and dual embeddings for this dataset. On the FB15k-237, FMULTn is trained using one-degree polynomials (n = 1) and this significantly outperforms other models. Meanwhile, we observe a very poor performance of FMULT. This extreme suboptimal performance is due to inappropriate hyperparameter settings for this dataset. Given the large size of the data, experimenting with various settings is very time-consuming.\nTable 6 presents the link prediction results on the NELL-995-h75, NELL-995-h50, and Countries datasets. On the NELL-995-h75, FMULT excels with second-best performance across all metrics, closely following QMult. FMULTn also performs competitively, especially in H@10. On the NELL-995-h50 dataset, FMULT and FMULTn both show strong performance, but in slightly different ways: FMULT is most effective at predicting H@10 showing best performance with HolE and Complex due to the flexibility of neural networks. While FMULTn excels in accurately identifying the most relevant connections like H@1 and H@3 and maintaining a high overall performance (MRR), indicating the efficacy of polynomial functions in capturing a mix of relationship complexities contained in the NELL-995-h50 dataset. This analysis highlights the complementary strengths of the two approaches, with neural networks providing broader predictive power and polynomial functions offering precision in ranking relevance on this particular dataset. On the Countries data, apart from TuckER, all other approaches perform incredibly well which is because the data itself contains only two relations. However, both FMULT and FMULTn demonstrate strong performance, with FMULTn achieving near-perfect scores across all metrics. Overall, both FMULT and FMULTn demonstrate competitive performance against state-of-the-art models, with FMULT often achieving second-best results and FMULTn excelling in several metrics, particularly in the NELL-995-h50 and Countries datasets. FMULT excels in datasets with complex, hierarchical (e.g. UMLS), or symmetric relationships (KINSHIP, WN18-RR), outperforming state-of-the-art models, while FMULTn remains competitive but less effective due to its reliance on polynomial functions. Conversely, FMULTn significantly outperforms in datasets suited to polynomial representations (e.g. NELL-995-h50) but may underperform if hyperparameters are not well-tuned. Both approaches demonstrate robust performance in datasets with fewer relationships (e.g. COUNTRIES), with FMULTn particularly excelling in precision."}, {"title": "7 CONCLUSION", "content": "In this work, we have developed three novel embedding methods that operate in function space: FMULTn, FMULT, and FMULT. This marks the first time that functional representations have been employed for embedding entities and relations in knowledge graphs. Our experimental results on eight benchmark datasets demonstrate that either polynomial-based embeddings (FMULTn) or neural network-based embeddings (FMULT) can significantly improve state-of-the-art results in knowledge graph completion.\nThe promising outcomes suggest that functional representations provide a more flexible and expressive framework for capturing the complex dynamics within knowledge graphs. While this study has focused on FMULTn and FMULT, the potential of the trigonometric function-based approach (FMULT) remains unexplored and will be the subject of future work."}, {"title": "A SCORES DERIVATION", "subsections": [{"title": "A.1 Scalar Product and Norm Derivation of Polynomial Functions", "content": "Given P1, P2 \u2208 Rdeg[x] such that P1(x) = \u2211degi=0 aixi, and P2(x) = \u2211degi=0 bixi we have:\n(P1(x), P2(x)) L2(\u03a9) = So P1(x)P2(x)dx\n= So (\u2211degi=0 aixi)(\u2211degj=0 bjxj)dx\n= So \u2211degi,j=0 aibjxi+jdx\n= \u2211degi,j=0 aibjSo xi+jdx\n= \u2211degi,j=0 aibj (sup \u03a9i+j \u2212 inf \u03a9i+j) / 1+i+j\nHere, sup \u03a9 and inf \u03a9 are respectively the upper and lower bounds of \u03a9."}, {"title": "A.2 Full Scoring Function Derivation of FMULT (h, r, t)", "content": "\u2022 Let (h, r, t) \u2208 \u039a\n\u2022 We compute the embedding of h, r and t in Cheg [x] where m = deg+1 and deg \u2208 N is a hyper-parameter s.t. 0 \u2264 deg \u2264 d-1.\nh(x) = \u2211degi=0 akeikx, r(x) = \u2211degi=0 bkekix, t(x) = \u2211degi=0 ckekix\nwhere, a(.), b(.) and c(.) \u2208 Cm/2. Hence,\nFMULT (h, r, t) = A\nA = Re (h(x) \u2297 r(x), t(x)) L2 (\u03a9)\n= Re(So (\u2211degi=0 akekix)(\u2211degj=0 bekix)(\u2211degk=0 ckekix))dx)\n= Re(So (\u2211\u03c9=0deg ao\u02c9boco + \u2211degu,v,w=1 aubvw ei(u+v+w)x))dx)\n= Re(aoboco||\u03a9||) + Re (So (\u2211degu,v,w=1 aubvw ei(u+v+w)x)))\n= Re aoboco||\u03a9|| + Re (\u2211degu,v,w=1 aubvw (ei(u+v+w) sup \u03a9 \u2212ei(u+v+w) inf \u03a9) / i(u+v+w))"}]}, {"title": "B PROOF OF THEOREMS", "content": "\u2022 If ||\u03a9|| = 1, FMULT0 ((h, r, t)) = DistMult((h, r, t))\nPROOF. We assume ||\u03a9|| = 1. If deg = 0, the polynomials reduce to d-dimentional constant vectors (as m = 0+1 = d) i.e.,\nh(x) = ao, r(x) = bo, t(x) = Co.\nTherefore,\nFMULT0 ((h, r, t)) = So aobocodx\n= aoboco ||\u03a9||\n= aoboco\n= DistMult((h, r, t))", "subsections": [{"title": "", "content": "\u2022 If ||\u03a9|| = 1, FMULT((h, r, t)) = ComplEx((h, r, t))\nPROOF.\nFrom Subsection A.2, we can directly derive that if deg = 0, the second term of the summation vanish. We therefore remain with FMULT(h, r, t) = Re(aoboco||\u03a9||) and since by hypothesis, ||\u03a9|| = 1 we get FMULT(h, r, t) = Re(aoboco) with ao, bo and co \u2208 Cd/2 which is the definition of Complex.", "subsections": [{"title": "", "content": "\u2022 FMULTn ((h, r, t)) = FMULTn ((t, r, h))\nPROOF. Using the definition of FMULTn:\nFMULTn ((h, r, t)) = So h(x)r(x)t(x) dx\nBy swapping h and t:\nFMULTn ((t, r, h)) = So t(x)r(x)h(x) dx\n= So h(x)r(x)t(x) dx\n= FMULTn ((h, r, t))"}, {"title": "", "content": "\u2022 FMULT ((h, r, t)) = FMULTn ((t, r, h))\nPROOF. Using the definition of FMULT:\nFMULT ((h, r, t)) = Re (So h(x)r(x)t(x) dx)\nBy swapping h and t:\nFMULT ((t, r, h)) = Re (So t(x)r(x)h(x) dx)\n= Re (So h(x)r(x)t(x)dx)\n= FMULT ((h, r, t))."}]}]}]}