{"title": "Analyzing Patient Daily Movement Behavior Dynamics\nUsing Two-Stage Encoding Model", "authors": ["Jin Cui", "Alexander Capstick", "Payam Barnaghi", "Gregory Scott"], "abstract": "In the analysis of remote healthcare monitoring data, time series representation\nlearning offers substantial value in uncovering deeper patterns of patient behavior,\nespecially given the fine temporal granularity of the data. In this study, we focus\non a dataset of home activity records from people living with Dementia. We\npropose a two-stage self-supervised learning approach. The first stage involves\nconverting time-series activities into text strings, which are then encoded by a\nfine-tuned language model. In the second stage, these time-series vectors are bi-\ndimensionalized for applying PageRank method, to analyze latent state transitions\nto quantitatively assess participants behavioral patterns and identify activity biases.\nThese insights, combined with diagnostic data, aim to support personalized care\ninterventions.", "sections": [{"title": "Introduction", "content": "In remote healthcare monitoring applications, the use of wearables and Internet of Things (IoT)\ndevices to continuously collect time-series data, often with second-level accuracy or finer, has become\nincreasingly common. However, the sheer scale of such data makes it difficult for human experts\nto analyze or use directly, necessitating the use of time-series deep learning techniques for effective\nanalysis and diagnosis.\nTraining on large volumes of unlabeled time-series data poses a significant challenge. Semi-supervised\nand unsupervised methods are typically employed to encode and extract data features for downstream\ntasks like classification or regression, demonstrating their ability to capture deep features. Semi-\nsupervised methods, such as nearest neighbor contrastive learning and temporal relation prediction,\nefficiently utilize both labeled and unlabeled data, improving the quality of representations for\ndownstream tasks like classification (Kim et al., 2024; Fan et al., 2021). Unsupervised methods focus\non learning robust representations without relying on labels, often leveraging contrastive learning\ntechniques and innovative data augmentations to capture key temporal patterns (Franceschi et al.,\n2019; Lee et al., 2024). Attention mechanisms and domain-adaptive techniques further enhance the\ninterpretability of encoded features, aligning them more closely with human intuition and domain-\nspecific insights (Lyu et al., 2018). However, this strategy faces two challenges: first, labeling criteria\nfor time-series data is often vague, which can significantly impact model performance; second, the\nencoded data remain vast, unintuitive, and difficult to interpret (Ye and Ma, 2023; Hill et al., 2022).\nIn this work, we focus on time-series data characterized by irregular discrete values. Extending\nthe methods introduced in Capstick et al. (2024), we present preliminary results of a second-order\nrepresentation learning method designed to aid in clustering, identifying similar clinical cases, and\nuncover patients' interpretable behavioral patterns. This is achieved through a large language model\nencoding combined with a two-dimensional vectors representation and transfer pattern analysis."}, {"title": "1.1 Background", "content": "Time-series forecasting is primarily to predict future values based on previously observed data points.\nTraditional statistical methods, most notably the Autoregressive Integrated Moving Average (ARIMA)\nmodel, have long been utilized due to their mathematical simplicity and flexibility in application\n(Rizvi, 2024; Kontopoulou et al., 2023). While ARIMA remains a staple for scenarios where data\nexhibits linear patterns, recent developments in machine learning have introduced sophisticated\nmodels capable of capturing non-linear dependencies, thus offering potential improvements in\nforecasting accuracy and robustness (Masini et al., 2023; Rhanoui et al., 2019).\nThe advent of the Generative Pre-trained Transformer (GPT) by OpenAI marked a significant mile-\nstone in the field of natural language processing (Brown et al., 2020), catalyzing a wave of innovations\nin large language models (LLMs). Large Language Models (LLMs) have profoundly transformed nat-\nural language processing and are increasingly being considered for diverse applications beyond text,\nsuch as time series data analysis. The study by (Bian et al., 2024) presents a framework that adapts\nLLMs for time-series representation learning by conceiving time-series forecasting as a multi-patch\nprediction task, introducing a patch-wise decoding layer that enhances temporal sequence learning.\nSimilarly, (Liu et al., 2024) propose a model which leverages the autoregressive capabilities of LLMs\nfor time series forecasting. In Capstick et al. (2024), the authors apply a GPT-based text encoder to\nstring representations of in-home activity data to enable vector searching and clustering. Using a\nsecondary modelling stage, we extend these ideas to enable further analysis and interpretability.\nPageRank, originally developed to rank web pages, is an algorithm designed to assess the importance\nof nodes within a directed graph by analyzing the structure of links within networks (Page et al., 1999).\nWhile it was initially created for search engines, its application has since expanded across various\ndisciplines. For instance, in biological networks, (Iv\u00e1n and Grolmusz, 2011) employed personalized\nPageRank to analyze protein interaction networks, providing scalable and robust techniques for\ninterpreting complex biological data. Similarly, (B\u00e1nky et al., 2013) introduced an innovative\nadaptation of PageRank for metabolic graphs. This cross-disciplinary application of PageRank\nhighlights its potential for analyzing complex systems beyond its original domain."}, {"title": "1.2 Our Contribution", "content": "We propose an integrated approach for discovering latent states of activity. This method comprises\nseveral key steps:\n1. Temporal Data Preprocessing: The raw temporal data is first preprocessed to remove noise\nand standardize the data for consistency.\n2. Language Model Encoding: A language model is trained on our dataset to encode the\npreprocessed temporal data into high-dimensional vector representations. To enhance the\nmodel's learning capability, we perform pseudo labeling using one-hot similarity. This\nallows the model to better capture temporal dependencies and patterns in the data."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Mathematical Foundations of the Model", "content": "Given a discrete data sample $X = {X_1,X_2, ..., X_n}$, the following steps describe the transformation\nprocess:\n1. Sampling and Text Conversion: Each sample $x_i$ is converted into a text representation $T(x_i)$.\n2. Language Model Encoding: A pre-trained language model $f_{LM}$ is applied to obtain high-\ndimensional vector embeddings for the text data:\n$h_i = f_{LM}(T(x_i)), h_i \\in R^d$.\n3. Dimensionality Reduction: The high-dimensional embeddings are projected into a 2D space\nusing a dimensionality reduction method $\\Phi$, such as t-SNE:\n$z_i = \\Phi(h_i), z_i \\in R^2$.\n4. PageRank and Deep State Vector Extraction: A transition matrix $P$ between points in 2D space\nis constructed, and the PageRank algorithm is applied to further reduce the dimensionality:\n$v_i = PageRank(P), v_i \\in R^k, k\\ll d$.\nThe final low-dimensional vectors $v_i$ capture deep semantic relationships from the original data."}, {"title": "2.2 The Dataset", "content": "We obtained a dataset collected from 134 people diagnosed with dementia, capturing their home\nlocation movement data between July 1, 2021, and January 30, 2024. The dataset records the time\nentering different rooms and sleeping mats, alongside clinical metrics such as MMSE (Kurlowicz\nand Wallace, 1999), ADAS-Cog (Kueper et al.) scores from regular tests. It also includes details on\nvarious factors such as demographic data, comorbidities, and other medical information. The dataset\ncontains a total of 66,096 recording days. A more detailed description of the dataset is provided in\nAppendix A.2. After excluding patients with missing data, the final dataset used for further analysis\ncontained 50 participants with complete information."}, {"title": "2.3 Our Framework", "content": "Our framework consists of several key stages: data preprocessing and encoding, latent state discovery,\nand transition pattern analysis, as shown in Figure 1. First, we preprocess the raw temporal data to\nremove noise and ensure consistency. This process is illustrated in Figure 2. We then utilize the all-\nMiniLM-L12-v2 model (Muennighoff et al., 2023) as the language model encoder. This model excels\nat capturing similarities in textual information, making it suitable for analyzing similarities between\nrecorded dates and uncovering potential relationships. We fine-tune the model using its pretrained\nweights to adapt to the specific characteristics of our dataset. The preprocessed temporal data is then\nencoded into 384-dimensional vector representations, capturing the inherent temporal dependencies\nand patterns within the data. Given the unlabeled nature of our temporal data, we adopt a Cluster-\nbased Contrastive Sample Selection method and triplet loss for training and evaluation. Further"}, {"title": "3 Experiments", "content": "After clustering the text vectors of the test set using K-means, we identified the optimal clustering\nresult at 5 clusters (Appendix A.5), suggesting five latent states across all single-day, single-participant\nbehavioral patterns. Figure 8 shows the clustering outcomes following dimension reduction of the\nembeddings using t-SNE. By examining the transformation of individual vectors in two dimensions,\nwe can visualize the behavioral trajectories of different participants within the embedding space (see\nAppendix A.6 for more participant visualizations). Collaborating with clinical experts, we can explore\nthe semantics represented by these clusters and their relationship to patient medical characteristics.\nBy integrating other participant data \u2013 such as housing type and whether they live alone, we can begin\nto infer participant behavior and care needs.\nMore significantly, by applying the random wandering model and the PageRank algorithm to these\ntwo-dimensional plots, in combination with clinical expert opinions and diagnostic results, we can\nquantitatively assess the deeper semantics represented by the vector clusters, or latent states. By\nreducing the complex vector matrix into a simplified (1,5) vector(Appendix A.4.5), we can explore\nsemantic characteristics to each of the dimensions. For example, once a unique deep vector is\ngenerated for each participant, we can easily identify the disease type, age, MMSE, ADAS-Cog\nscores, and their rate of change for the three patients most and least similar to any given participant.\nThis revealed that the clinical differences between similar groups were indeed smaller in features\nlike age, change in ADAS-Cog score (Appendix A.7). Furthermore, clustering the latent vectors\nof all 50 participants highlighted pronounced differences between clusters, each offering potential\nexplanations for the link between activity data and clinical diagnosis (Appendix A.8).\nLooking forward, now that we have established a process for encoding deep vectors, we could explore\ntransforming this approach into a generative model. Such a model could be used to generate sensitive\nand hard-to-obtain medical datasets for purposes like data augmentation or alignment, a strategy\nproven effective in the training of large language models(Li et al., 2023)."}, {"title": "4 Conclusion", "content": "In conclusion, our initial results demonstrate that by applying our framework, we show that our\nlatent states vector based on patients daily activity patterns can be useful for exploring behavior\ndynamics. While these findings offer a promising approach to exploring the relationship between\nbehavior and clinical characteristics, further research is needed to refine the model and validate its\nbroader applications, including potential use in medical data augmentation."}, {"title": "A.2 Detailed Description of The Dataset", "content": "The dataset used for in-home activity monitoring was collected via passive infrared sensors installed\nat multiple locations in the homes of individuals with dementia, along with sleep pads placed under\ntheir mattresses. These infrared sensors detect motion within a range of up to nine meters, at a\nmaximum angle of forty-five degrees diagonally upward. Sensors were placed in lounges, kitchens,\nhallways, bedrooms, and bathrooms, allowing for detailed tracking of participants' movements within\nand between these areas.\nWe analyzed data recorded between July 1, 2021, and January 30, 2024, amounting to 66,096\nparticipant-days for 134 individuals. Figure 3 illustrates the distribution of logged days per participant.\nEach data point includes the participant ID, a timestamp (accurate to the second), the location of\ndetected activity, or sleep pad data indicating whether the participant entered or left their bed. The\ndataset contains a total of 24,467,307 individual records, as depicted in Figure 4. Figure 5 shows the\nthree month interval distribution of records.\nIn addition to activity data, we had access to diagnostic information for the 134 participants, including\nbirthdate, gender, living situation (whether they lived alone), ethnicity, and dementia diagnosis."}, {"title": "A.3 Detailed Training Process", "content": "Since the sensor data are recorded with second-level precision, each participant generates 86,400\ndata points per day, far exceeding the input token limit of the language model we are using (which\nallows a maximum of 256 tokens). To address this, the raw data were downsampled by extracting\ndiscrete values at 20-minute intervals, reducing the data points per day to 72. After converting these\ndata points to strings, the token count is 72, which falls within the model's token limit.\nGiven the unlabeled nature of our temporal data, we employ a cluster-based contrastive sample\nselection approach for model training. This method leverages the inherent structure within the data to\ncreate meaningful positive and negative sample pairs. The detailed steps are as follows:\n1. One-hot Encoding: Convert all daily string representations into one-hot encoded vectors.\n2. Clustering: Apply K-means clustering to the one-hot encoded vectors to group similar daily\npatterns into clusters.\n3. Target Day Selection: Choose a specific day as the target for comparison.\n4. Similar Sample Selection: For the target day, select a similar sample that meets all the\nfollowing criteria:\n\u2022 From the same participant\n\u2022 Within a 30-day window of the target day\n\u2022 Belongs to the same cluster as the target day\n5. Dissimilar Sample Selection: Randomly select any other sample that does not meet the\ncriteria for similar sample selection.\nWe selected a 30-day interval for positive sample selection for two key reasons: first, k-means\nclustering of the encoded vectors yielded the best results with a 30-day window, as is shown in A.3;\nsecond, many patients undergo regular physical checkups, such as urine tests, on a monthly basis,\naligning well with this time frame."}, {"title": "A.4 PageRank Model for a Single Patient", "content": ""}, {"title": "A.4.1 Model Definition", "content": "We aim to compute the PageRank model fit and entropy value for a single patient based on their\nembeddings and cluster labels. The process involves defining a transition matrix based on distances\nbetween clusters, computing the PageRank scores."}, {"title": "A.4.2 Transition Matrix Construction", "content": "Given:\n\u2022 X: Patient embeddings (shape: (n_samples, 2)).\n\u2022\ny: Patient cluster labels (shape: (n_samples,)).\n\u2022 num_clusters: Number of clusters.\n\u2022 threshold: Distance threshold for defining transitions.\nThe transition matrix T is computed as follows:\n$T_{ij} = \\frac{\\sum_{k \\in C_j} \\sum_{l \\in C_i} I \\{d(k,l) \\leq threshold\\}}{\\sum_{l \\in C_i} \\sum_{m \\in C} I \\{d(l,m) \\leq threshold\\}}$ (1)\nwhere:\n\u2022 $C_i$ and $C_j$ are the sets of samples in clusters $i$ and $j$, respectively.\n\u2022 d(k, l) denotes the distance between samples $k$ and $l$.\n\u2022 I{.} is an indicator function that equals 1 if the condition is true and 0 otherwise."}, {"title": "A.4.3 PageRank Computation", "content": "The PageRank vector p is computed iteratively using:\n$p^{(t+1)} = \\frac{1- \u03b1}{num_{clusters}} + \u03b1T^Tp^{(t)}$ (2)\nwhere a is the damping factor (typically 0.85), and $T^T$ is the transpose of the transition matrix. The\nprocess continues until convergence:\n$||p^{(t+1)} \u2013 p^{(t)} ||_1 < tol$ (3)\nwhere tol is a predefined tolerance for convergence."}, {"title": "A.4.4 Algorithm Summary", "content": "Algorithm 1 explains the specific implementation steps of the PageRank we use."}, {"title": "A.8.1 Key Observations", "content": "Inverse Relationship: The ADAS-Cog score, which measures cognitive impairment, tends to\ndecrease as the MMSE score increases. A higher MMSE score indicates better cognitive function,\nand correspondingly, a lower ADAS-Cog score implies less cognitive impairment."}, {"title": "A.8.2 Cluster Distribution", "content": "\u2022 Cluster 1 (Dark Blue): Data points are spread across a wide range of ADAS-Cog scores\nfrom 50 to 80 and correspond to MMSE scores between 5 and 25. This cluster likely\nrepresents individuals with higher cognitive impairment.\n\u2022 Cluster 2 (Light Blue): Data points are mainly grouped between MMSE scores of 20 and\n30, with ADAS-Cog scores ranging between 40 and 70.\n\u2022 Cluster 3 (Green): This cluster includes individuals with intermediate MMSE and ADAS-\nCog scores, generally between 10 to 25 on the MMSE scale and 30 to 60 on the ADAS-Cog\nscale.\n\u2022 Cluster 4 (Yellow): Represented sparsely with fewer points, indicating individuals with\nhigher ADAS-Cog scores (around 50\u201380) and lower MMSE scores (10\u201315).\n\u2022 Cluster 5 (Orange) and Cluster 6 (Red): These clusters consist of individuals with\ngenerally higher MMSE scores (15 to 30) and lower ADAS-Cog scores, signifying lesser\ncognitive impairment."}]}