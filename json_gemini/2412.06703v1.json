{"title": "Source Separation & Automatic Transcription for Music", "authors": ["Bradford Derby", "Lucas Dunker", "Samarth Galchar", "Shashank Jarmale", "Akash Setti"], "abstract": "Source separation is the process of isolating individual sounds in an auditory mixture of multiple sounds [1], and has a variety of applications ranging from speech enhancement and lyric transcription [2] to digital audio production for music. Furthermore, Automatic Music Transcription (AMT) is the process of converting raw music audio into sheet music that musicians can read [3]. Historically, these tasks have faced challenges such as significant audio noise, long training times, and lack of free-use data due to copyright restrictions. However, recent developments in deep learning have brought new promising approaches to building low-distortion stems and generating sheet music from audio signals [4]. Using spectrogram masking, deep neural networks, and the MuseScore API, we attempt to create an end-to-end pipeline that allows for an initial music audio mixture (e.g...wav file) to be separated into instrument stems, converted into MIDI files, and transcribed into sheet music for each component instrument.", "sections": [{"title": "INTRODUCTION", "content": "Music source/stem separation, the task of separating an auditory mixture into its individual instruments or sound sources, is a power tool with applications from automatic lyric transcription and music similarity analysis to real-time sheet music generation from audio. The successful separation of components such as vocals, bass, drums, and other accom- paniments can enable musicians and researchers to perform tasks such as genre classification, audio alone transcription, and remixing of popular songs. However, accurate separation and corresponding Automatic Music Transcription (AMT) remain a difficult problem due to audio data size, limited availability of copyrighted music, and intricate relationships between sound sources in songs.\nIn recent years, deep learning models have shown increas- ing promise in effective source separation and AMT. With greater availability for computing power and the ability to experiment with model architectures, the ceiling for learning complex representations of audio data has been raised. We envision an end-to-end pipeline that allows a musician or producer to separate a song into its stems and generate sheet music for their specific instrument or create remixes, via MIDI representations and the MuseScore API. This workflow not only simplifies the creative and analytical processes in music production, but also broadens access to tools for audio manipulation and transcription."}, {"title": "RELATED WORK", "content": "The fields of audio source separation and automatic mu- sic transcription has evolved considerably over the past decade, driven in large part by advances in machine learning, greater compute, and the availability of improved datasets (MUSDB18). As presented in [1] and [9], time-frequency representations and masking strategies can isolate individ- ual sound sources from a mixture. These early approaches showed that spectrogram transformations when combined with masks can isolate stems from complex audio inputs.\nBuilding on this earlier work, more recent studies have explored the use of deep neural architectures to address both the complexity of multi-instrument mixtures and variability of audio recordings. In [4] researchers proposed convolu- tional encoder-decoder networks that operate directly on the spectrograms to reconstruct target sources with minimal pre or post processing. Similarly, in [7] researchers employed a deep neural network architecture known as Wave-U-Net to perform music source separation directly on raw waveforms rather than spectrograms. Their model, adapted from the U- Net encoder-decoder structure, learned to reconstruct isolated sources-such as vocals-from the mixture by leveraging skip connections. The study in [3] utilized RNN with GRU for source separation and LSTM for chord estimation to map spectrogram inputs into time-aligned sequences of chords and separated audio sources. Unlike earlier attempts, these deep architectures displayed enhanced recognition of over- lapping pitches and nuances.\nAs a result of these tools and advancements, researchers have been able to progressively enhance the performance of their architectures, blending insights from masking-based studies [1,9], vocal centric enhancements [2], AMT innova- tions [3], and deep learning breakthroughs [4,7]. Collectively, these works illustrate an evolving ecosystem of research, moving the field increasingly closer to high-quality products that can isolate, interpret, and transcribe musical recordings with impressive accuracy."}, {"title": "SOURCE SEPARATION PREPROCESSING", "content": "For our source separation pre-processing we rely on the MUSDB18 dataset, which stores full-length music tracks across various genres along with their isolated vocals, bass, drums, and other stems [5]."}, {"title": "Short-time Fourier Transform (STFT)", "content": "The STFT of a waveform is calculated by applying a Fourier transform in a sliding window through the duration of the input signal. The outputs of the STFT, therefore, are the time t and frequency f bins across the entirety of the waveform. Each entry in the STFT contains a magnitude component and a phase component, allowing for conver- sion back to waveform via the inverse Short-time Fourier Transform (iSTFT). The time-frequency representation of the audio signal is crucial for isolating and separating different"}, {"title": "Summing Accompaniment Sources", "content": "Previous work has attempted to train single models to separate all sources in an auditory mixture [7]. While this is possible with the approach we describe and use below, we focus on separating a single source (vocals) from the rest (bass, drums, other). To adjust our data samples accordingly, we sum the latter sources in the training dataset using the nussl Python audio source separation library [8]. Through this method, we clearly define the task of our model to be predicting the vocals source, whereas the remainder of the mixture should be considered an aggregate accompaniment."}, {"title": "Creating New Source Mixtures for Increased Dataset Size", "content": "The raw MUSDB18 dataset consists of 150 music tracks of 10 hours in total duration; this quantity of data is too low to train an accurate and generalized stem separation model. To counteract this, we take the same approach as in previous literature: we do on-the-fly mixing of raw sources in the dataset to create novel mixtures. While we are still limited by the number of raw source signals available in the dataset, this technique exponentially increases the number of varied signals we can use for training and evaluation."}, {"title": "SOURCE SEPARATION MODEL", "content": "Our approach to stem separation relies on masking, which refers to the application of a filter over a spectrogram. The values in the mask lie in the continuous interval [0.0, 1.0], and therefore determine what proportion of the energy in the original mixture should be contributed by a specific TF-bin, and therefore by a specific, distinct sound.\nFor a mixture with N sources where a mask is applied to separate each one, the element-wise sum of all masked signals Mi should result in the original mixture S.\n$S = \\sum_{i=1}^{N} Mi$\n(1)"}, {"title": "Algorithm", "content": "Our stem separation model is a deep neural network that learns to predict the mask to apply over an input mixture signal, therefore isolating the vocals stem. To do this, our model first calculates the log-magnitude representation of the input mixture. This is done to further spread out the magnitude distribution and reflect the logarithmic human perception of volume, making the data friendlier for training with a neural network:\n$S_{log-mag} = 10. (log_{10} (max (S^2, \\amin)) \u2013 log_{10} (max (\\amin, r^2)))$\nWhere:\n\u2022 S: Input magnitude spectrogram\n\u2022 @min: Small constant for numerical stability\n\u2022 r: Reference value for dB scaling\nA batch normalization is then applied over the signal, allowing for sigmoid and other activations to be less saturated across the full range of mixture inputs. This brings the mean of the data close to 0 and the variance close to 1, resulting in more effective learning for the neural network. Next, we use long short-term memory (LSTM) layers to extract relevant features from the audio signal and process the input. Finally, we apply an activation to this output and map it to a mask that is applied to the mixture signal in order to obtain the vocals estimate."}, {"title": "SOURCE SEPARATION RESULTS", "content": "The source separation model was trained for 200 epochs using the MUSDB18 dataset, with loss calculated as the"}, {"title": "Algorithm", "content": "Input: D = music audio mixture dataset (e.g. MUSDB18)\nOutput: V = calculated estimate of vocals stem\nLet M= estimated mask to isolate the vocals stem\nfor each input signal S in D do\n$S_{log-mag}$ = LogMagnitude(S)\n$S_{normalized}$ = BatchNorm($S_{log-mag}$)\n$S_{output}$ = LSTM($S_{normalized}$)\nM = Embedding($S_{output}$)\nV=MXS\nend for\nmagnitude differences between the estimated and true vocals and accompaniment stems. Hyperparameters were tuned over dozens of attempts at model training in an effort maximize metric-based and subjective measures of quality.\nWe divided our dataset into train, validation, and testing subsets before initializing our model, and then used our testing subset for evaluating our model's performance.\nOur evaluation metrics are as follows, as recommended from the nussl Python audio source separation library. For all metrics, the higher the score, the better."}, {"title": "AMT PREPROCESSING", "content": "Our transcription work leverages the MAESTRO dataset, which provides audio and labelled MIDI files aligned with ~3ms accuracy. Due to resource constraints, we have used only 1/4 of the dataset for training and evaluation.\nThe audio files are resampled to 22.05 kHz and converted into Constant-Q Transform (CQT) spectrograms, which use a logarithmic frequency scale ideal for music signals. CQT spectrograms are derived from the Short-Time Fourier Trans-form (STFT) by adapting the frequency resolution for each bin. Unlike STFT, where the resolution is fixed across the spectrum, CQT uses a constant ratio of frequency to resolution, allowing lower frequencies to have higher res-olution and higher frequencies to have coarser resolution. Each spectrogram is processed to include 84 frequency bins, capturing the full piano range (7 octaves). The magnitude values are then log-transformed to reflect human auditory perception."}, {"title": "AMT MODEL", "content": "Our approach uses a deep learning model to predict a binary piano roll representation from the processed CQT spectrograms. The spectrogram undergoes a convolutional process to extract spatial features in the frequency-time domain.\nBatch normalization is applied to ensure stable and ef- ficient training by normalizing the input distribution. The convolutional layers are followed by reshaping the feature map and aligning time steps for sequential processing.\nThe reshaped data then passes through bidirectional LSTM layers, which model temporal dynamics in both forward and backward directions, ensuring accurate capture of note onsets and offsets. Finally, the output is processed through a TimeDistributed dense layer with a sigmoid activation func- tion to predict the probability of each piano key activating at every frame. By normalizing and scaling these probabilities, a binary piano roll is generated, which is then converted into a MIDI representation for further analysis and evaluation."}, {"title": "WAV-to-MIDI Transcription Process", "content": "Input: Dataset D containing audio files\nOutput: MIDI representation of each audio file\nfor each audio file A \u2208 D do\nC\u2190 CQT(A) \nClog= LogAmplitude(C) \nCnorm\u2190 BatchNorm(Clog) \nFconv\u2190 Conv2D(Cnorm) \nFpooled\u2190 MaxPooling(Fconv) \nFseq = Reshape(Fpooled) \nFbiLSTM \u2190 BiLSTM(Fseq) \nPprob\u2190 TimeDistributed(Dense(FbiLSTM))\nPbinary \u2190 Threshold($P_{prob}$, T = 0.5)\nMIDI \u2190 ConvertToMIDI(Pbinary)\nend for"}, {"title": "AMT LOSS & METRICS", "content": "To handle class imbalance in our system, we are using Focal Loss, which emphasizes hard-to-predict samples, re-ducing the bias introduced by an overwhelming majority of inactive frames. The focal loss equation is given by:\n$L_{focal} = -\\alpha * (1- p_t)^\\gamma log(p_t)$\nwhere:\n$p_t = Y_{true} * Y_{pred} + (1 - Y_{true})(1 \u2013 Y_{pred})$\n\u2022 \u03b1: Balances the importance of classes. a = 0.35.\n\u2022 \u03b3: Modulates focus on hard samples. y = 3.0."}, {"title": "Metrics", "content": "Unlike typical machine learning tasks, music transcription involves highly imbalanced data due to the predominance of silent frames. In such cases, accuracy can be misleading, as they give disproportionate weight to such frames. As such, we evaluate the model using custom precision, recall, and F1-score metrics tailored to AMT, ensuring silent frames do not dominate the evaluation. These metrics focus exclusively on active frames, i.e., frames where either the ground truth or prediction contains active notes. These metrics provide a more meaningful assessment of transcription accuracy by ignoring the aformentioned silent regions."}, {"title": "AMT EVALUATION & RESULTS", "content": "To evaluate the performance of the transcription model, we focus on frame-level metrics and onset metrics. These metrics were chosen as they sufficiently capture the model's ability to detect active notes across frames and align with the performance of active region and note-level metrics, which yielded identical values in our evaluations."}, {"title": "AMT MIDI CONVERSION", "content": "The MIDI generation phase transforms the output predic- tions of the transcription model into a playable MIDI file."}, {"title": "Input Processing", "content": "\u2022 CQT \u2013 To ensure data compatibility with the model, the input audio is converted to CQT, capturing the harmonic features suitable for musical transcription.\n\u2022 Segmentation \u2013 The CQT features are segmented into overlapping windows matching the preprocessing steps, ensuring that the model processes the audio efficiently and reconstructs the entire sequence without loss of information."}, {"title": "Model Inference", "content": "The trained transcription model predicts a piano roll rep- resentation from the segmented CQT features. The piano roll is a binary matrix where rows correspond to MIDI pitches (21 to 108) and columns represent frames in time. Each entry indicates whether a particular pitch is active at a given time frame. Segments are concatenated to form the complete piano roll for the entire audio file."}, {"title": "Piano Roll to MIDI Conversion", "content": "Frame Timing Calculation - each frame is mapped to a time interval:\n$Time Per Frame = \\frac{Hop Length}{Sample Rate}$\n(5)\nWhere Hop Length = 512 and Sample Rate = 22050, resulting in a Time Per Frame of approx. 23.2 ms.\nNote detection: For each pitch (row in piano roll), active frames (>0.5) are grouped into contiguous \u201cnote-on\u201d events. Each group is translated into a note with:\n\u2022 Start Time: The first active frame in the group.\n\u2022 End Time: The last active frame in the group plus one.\n\u2022 Pitch: The row index of the piano roll, offset by 21 to match the MIDI pitch range.\n\u2022 Velocity: Set to a fixed intensity of 100. (Our model does not predict velocity).\nMIDI File Construction: Each detected note is added to a single-instrument MIDI file representing the transcription output. The MIDI file is written to disk using the pretty_midi library."}, {"title": "MIDI TO SHEET MUSIC NOTATION", "content": "To transform the MIDI files generated by our AMT model into readable sheet music notation, we integrate MuseScore into our pipeline. MuseScore is an open-source music nota- tion software that converts various music file types such as MIDI and MusicXML into sheet notation. We developed a service and controller layer that interfaces with MuseScore's command-line tool; specifically, we utilized the export_to command which provides functionality to export from MIDI to sheet music notation. The MuseScore application was installed locally, while the MuseScore itself executable was added to our system environment variables. These envi- ronment variables are required to run the export_to CLI command in the pipeline.\nAn example of the CLI command with required parame- ters: mscore example.mid -o example.pdf.\nInternally, MuseScore performs several steps to convert MIDI data into sheet music notation:"}, {"title": "MuseScore API Functionality", "content": "MuseScore reads the MIDI file and parses it into Mi- diEvent objects encoding the note-on and note-off instances, control changes, tempo indications, and other metadata. These events are then organized into a MidiTrack object. After initialization, the MidiTrack and MidiEvent objects are then converted to Score objects which represent the musical notation. This is done through a process including pitch map- ping, quantization, articulation, and dynamics interpretation.\nThe MidiMapper class provides the capability to map MIDI pitches to musical notes considering transpositions. After mapping pitches, MuseScore applies rhythmic quanti- zation to align MIDI note durations and timings to musical notation values (whole, half, and quarter notes) using the Quantizer class. After quantization, the articulations and dynamics are transcribed. While MuseScore maps MIDI velocity values to these notational elements, it can miss expressive markings often found in human-generated sheet music. After the score is processed, the final documented layout is corrected, ensuring spacing and other constraints before the results is \"painted\" to a PDF file."}, {"title": "CONCLUSIONS & FUTURE WORK", "content": "In summation, we have successfully created a process for separating the vocal data from an audio file, converting audio data into MIDI data, and displaying MIDI data as sheet music. While our pipeline would substantially benefit from contributing more time and resources into improving accuracy and ease-of-use, we're satisfied with the work we've completed so far and are confident that our models lay the groundwork for future contributions and revisions.\nOur pipeline currently has a mismatch between our desired output for stem separation and our desired input for AMT; we trained our stem separator model to isolate vocal data, while our AMT model was trained on piano audio. While we initially aimed to train our AMT model on vocal data as well, we were unable to find a suitable dataset that matched vocal sounds with their corresponding notes.\nA primary limitation of our model performances is our lack of long-term training time; the input data we used for our stem separator was clipped to 7-second intervals, as this was the maximum size for which we could still utilize the entire dataset and train our model in a reasonable amount of time. With the Northeastern Discovery Cluster, we estimate that training our model on the entire MUSDB18 dataset for 200 epochs would take approximately 1 week, substantially increasing our model's performance.\nHowever, our primary takeaway for this project is that a full pipeline for stem separation, automatic music transcrip- tion, and sheet music generation is completely possible, and has great potential in the future to become a fully viable product for creative and commercial use."}, {"title": "GITHUB REPOSITORY", "content": "The GitHub repository containing our code is pub- lic and can be found at https://github.com/Lucas-Dunker/ Stem-Separator-AMT/tree/main. All code has been separated into modular components and organized into Python files. We began much of our initial work using Jupyter Notebooks, and used them for our demonstration during our project presentation; while the code in these notebooks is now outdated, we have left them in our repository for archival reasons and contributor reference."}, {"title": "TEAM CONTRIBUTIONS STATEMENT", "content": "Bradford was responsible for creating our code that con- verts MIDI data to sheet music using MuseScore and setting up the structure of our presentation. Lucas and Shashank were responsible for setting up the stem separation model and transcribing our reports to Latex. Samarth and Akash were responsible for setting up the AMT model and ensuring our input/output data throughout our pipeline aligned with our expectations."}]}