{"title": "Accelerating Large Language Models through Partially Linear Feed-Forward\nNetwork", "authors": ["Gansen Hu", "Zhaoguo Wang", "Jinglin Wei", "Wei Huang", "Haibo Chen"], "abstract": "Large language models (LLMs) demonstrate remarkable ca-\npabilities but face deployment challenges due to their massive\nparameter counts. While existing compression techniques like\npruning can reduce model size, it leads to significant accu-\nracy degradation under high compression ratios. We present\na novel perspective inspired by constant folding in compiler\noptimization. Our approach enables parameter reduction by\ntreating activation functions in LLMs as linear functions.\nHowever, recent LLMs use complex non-linear activations\nlike GELU that prevent direct application of this technique.\nWe propose TARDIS, which enables optimization of LLMs\nwith non-linear activations by partially approximating them\nwith linear functions in frequently occurring input ranges.\nFor outlier inputs, TARDIS employs an online predictor to\ndynamically fall back to original computations.\nOur experiments demonstrate that TARDIS achieves 80%\nparameter reduction in feed-forward networks, while signifi-\ncantly outperforming state-of-the-art pruning methods Wanda\nand RIA with up to 65% higher accuracy. In practical deploy-\nments for a 7B model, TARDIS achieves 1.6x end-to-end\ninference speedup when integrated with the vLLM serving\nsystem, and 1.4x speedup with the widely adopted Hugging-\nFace implementation, while incurring only a 10.9% accuracy\ntrade-off.", "sections": [{"title": "1 Introduction", "content": "Recently, transformer-based large language models (LLMs)\nhave revolutionized natural language processing, demonstrat-\ning remarkable capabilities across diverse tasks. Deploying\nthese models for inference is notoriously challenging due to\ntheir enormous number of parameters [58,60, 69], often reach-\ning billions or even trillions [18]. To address this, various\ntechniques have been proposed to compress LLMs, includ-\ning pruning [43, 53, 68], quantization [28,41,63], knowledge\ndistillation [31,55], and low-rank approximation [22, 39, 50].\nWhile pruning has gained popularity due to its simplicity [62],\nit often leads to significant accuracy degradation under high\ncompression ratios - a 80% compression could result in up to\n70% accuracy loss.\nInspired by the success of constant folding [1] in traditional\ncompiler optimizations, we propose a novel approach to LLM\ncompression that maintains high accuracy even under aggres-\nsive compression ratios. Constant folding, a fundamental com-\npiler optimization technique that reduces both program code\nsize and runtime computation costs, has been successfully em-\nployed across numerous programming languages including\nC/C++ [4], Python [5], and SQL [9]. We discover a unique\nopportunity to adapt this technique for LLM compression.\nThe architecture of an LLM primarily consists of two core\ncomponents: the Multi-Head Attention (MHA) block and\nthe feed-forward network (FFN) block, with the FFN block\naccounting for 67% to 80% [19] of the total parameters. The\nFFN block comprises two matrix multiplications separated\nby a non-linear activation function (e.g., ReLU or GELU).\nOur key observation is that through linear approximation\nof the non-linear activation function, we can leverage the as-\nsociative property of matrix multiplication to merge the two\nFFN matrices into a single, compact matrix. This transforma-\ntion enables a reduction in FFN parameters by up to 87.5%\nin theory (detailed in Section 3.1).\nHowever, modern LLMs predominantly use sophisticated\nnon-linear activation functions, particularly GELU [33]\n(adopted in GPT series models [21]) and SiLU [27] (used\nin LLaMA2 [57]). These complex functions are crucial for\ncapturing intricate patterns in the data, and naively approxi-\nmating them with linear functions leads to severe performance\ndegradation. Our experiments with a 7B model show that a\nsimple linear approximation of GELU results in a 75% drop\nin model accuracy.\nIn this paper, we present TARDIS, a novel approach that\nenables constant folding in LLMs with non-linear activation\nfunctions at low approximation error. Our key insight is that\nthe inputs to these activation functions in LLMs are typi-\ncally concentrated within a narrow range. This distribution\npattern allows us to partially approximate the non-linear ac-"}, {"title": "2 Background and Motivation", "content": "In this section, we first cover the basics of LLM inference and\nhighlight the performance bottlenecks introduced by massive\nparameters. We then review existing approaches to reducing\nLLM size and identify their constraints."}, {"title": "2.1 Large Language Model Inference", "content": "LLM Inference Overview. LLM inference generates output\ntokens in response to input tokens through two main phases.\nIn the first phase, as shown in Figure 1a, the model processes\nall initial input tokens through its N layers sequentially to\nestablish context. In the next phase, the model generates out-\nput tokens one at a time in an auto-regressive manner, where\neach generated token becomes part of the input sequence for\npredicting the next token. This process continues until either\nthe desired output length is reached or a stop condition is met.\nOperations in LLM Inference. The transformer architecture,\nwhich forms the backbone of LLMs, consists of two main\ncomponents: the multi-head attention (MHA) block and the\nfeed-forward network (FFN) block. The MHA block enables\nthe model to focus on different parts of the input sequence\nsimultaneously through multiple attention heads, while the\nFFN block, composed of two linear transformations with an\nactivation function in between, processes the features from the\nattention block. As Figure la shows, an LLM typically stacks\nmultiple transformer layers, where each layer sequentially\nprocesses the input through its MHA and FFN blocks. During\ninference, the model parameters for both blocks need to be\nloaded from memory before performing computations on\ninput tokens."}, {"title": "2.2 Cost of LLM Inference", "content": "Despite their remarkable capabilities, deploying LLMs for\ninference tasks remains challenging due to their massive pa-\nrameter counts [41, 42, 58, 60]. This large parameter count\nsignificantly impacts inference performance.\nTo quantify this impact, we analyzed the inference time\nbreakdown of Falcon-7B using the SharedGPT dataset [14],\nwhich contains 90K real-world chat histories between humans\nand LLMs. 80% of the total parameters are in the FFN blocks\nof Falcon-7B. Using the dataset's average lengths (91 tokens\nfor human input, 178 tokens for LLM response), we measured\nthe inference time with the vLLM inference engine [7] on\nan RTX 4090, where attention blocks take 0.9s, and the FFN\nblocks takes 2.1s, showing the bottleneck is in FFN blocks.\nWe also performed a therotical analysis of the time spent\non computation and I/O operations (performed on the 1TB/s\nbandwidth VRAM of RTX 4090) in both MHA and FFN\nblocks. Figure 1b shows that parameter loading I/O dominates\nthe inference time, with the FFN I/O alone consuming 78.2%\nof the total time. This high I/O overhead stems from LLMs'\nauto-regressive nature: generating each token requires loading\nall model parameters into the hardware's cache (Figure 1a),\nresulting in repeated, expensive I/O operations [58, 60, 69]."}, {"title": "2.3 LLM Compression", "content": "To reduce the I/O overhead during LLM inference, researchers\nhave developed various compression techniques. These tech-"}, {"title": "3 Opportunity and Challenge", "content": "Inspired by the success of constant folding in compiler op-\ntimizations, we discover a novel opportunity to reduce I/O\ncosts by simplifying FFN blocks in LLMs through activation\nfunction modification. Our approach achieves higher accuracy\nunder aggressive compression compared to state-of-the-art\npruning methods. Rather than directly manipulating model\nparameters like existing pruning techniques, we focus on sim-\nplifying the activation functions in FFN blocks, which enables\nparameter reduction through constant folding."}, {"title": "3.1 Opportunity", "content": "A key insight for simplifying FFN blocks in LLMs is that their\nactivation functions can be approximated as linear functions.\nThe FFN block in transformer models consists of two weight\nmatrices $W_1$, $W_2$, and an activation function between them.\nAs shown in Figure 3, a simple FFN block with activation\ncomputes:\n$FFN(x) = \\sigma(xW_1)W_2$\nwhere matrix $W_1$ is of size $d\\times h$ and $W_2$ is $h \\times d$, $\\sigma$ is the acti-\nvation function. A function $f(x)$ is linear if and only if it can\nbe expressed as $f(x) = ax + b$, where a and b are constants.\nWhen the activation function is approximated by a linear func-\ntion, the FFN becomes the following linear transformation:\n$FFN(x) = a((xW_1)+b\\textbf{1})W_2$\nThis linear representation of activation function enables\nus to leverage matrix computation properties to optimize\nthe FFN block. Specifically, we can reorder the computation\nfrom $a(xW_1)W_2$ to $x(aW_1W_2)$ using matrix associativity. This\nreordering would not be possible with non-linear activation\nfunctions. The key advantage is that $aW_1W_2$ contains only\nconstants and can be pre-computed into a single matrix C be-\nfore inference. As illustrated in Figure 3, with input $[x_1,x_2]$,\nthe FFN computes $t= [a(-2x_1+3x_2), a(x_1+2x_2)]$. By pre-\ncomputing $aW_1 W_2$ into matrix $C = \\begin{pmatrix} 3a & 2a \\ -1a & 1a\\end{pmatrix}$, we can simplify\nthe entire FFN into a single matrix multiplication $xC$ that\nproduces the same result $t$. This transformation significantly\nreduces the parameter count in FFN blocks. Originally, an"}, {"title": "3.2 Challenge", "content": "While constant folding in LLMs shows promise, a signifi-\ncant challenge arises from the widespread adoption of sophis-\nticated non-linear activation functions in modern language\nmodels [23, 24, 27, 33, 35]. Our analysis of the top 15 text\ngeneration models on HuggingFace [10], a leading model\nrepository, reveals that all models use complex activations - 7\nemploy GELU and 8 use SiLU. These sophisticated activation\nfunctions make naive linear approximation challenging.\nConsider GELU as an example. As illustrated in Figure 4,\nGELU exhibits a smooth but intricate input-output relation-\nship. When we attempt to approximate GELU with a simple\nlinear function (as described in Section 3.1), the model suffers\nsubstantial accuracy degradation. To validate this, we modi-\nfied Falcon-7B by replacing its GELU activation with a linear\nfunction in the FFN blocks, derived through linear regression\non the original activation's input-output data. When evaluated\non the Lambada [12] benchmark, the model's accuracy plum-\nmeted from 75% to 0, making it impractical for real-world\napplications."}, {"title": "4 Insights and Overview", "content": "Through careful analysis of LLM internals, we have identified\ntwo key insights that enable effective approximation of non-\nlinear activation functions with linear ones while preserving\nmodel accuracy."}, {"title": "4.1 Insights", "content": "Insight 1. Skewed distribution of activation function in-\nputs. The input distribution for activation functions in LLMs\nexhibits a highly skewed pattern, with inputs predominantly\nconcentrated within a narrow range of the input space. To vali-\ndate this observation, we analyzed three widely-used language\ntask datasets: Wikitext-2 [45], C4 [49], and Penn Treebank\n(PTB) [44]. A FFN block consists of multiple neurons, where\neach neuron processes input through its weights and activa-\ntion function. The activation input distribution at the neuron\nlevel is crucial for understanding model behavior [42, 52]. We\nexamined this distribution within FFN weight matrices. Each\nweight matrix can be decomposed into individual neurons,\nwhich represent distinct computational units. For example, in\nFigure 3, $W_1$ comprises two neurons represented as columns:\n$\\begin{pmatrix}3 \\ -1 \\end{pmatrix}$ and $\\begin{pmatrix}1 \\ 2\\end{pmatrix}$. Using 20K random tokens from each dataset,\nwe collected comprehensive statistics.\nFigure 5 illustrates the input activation distribution for 50\nrandomly sampled neurons from two layers of Falcon-7B\nacross these datasets. Our analysis reveals consistent distribu-\ntion patterns within the same layer across different datasets.\nNotably, approximately 65% of activation inputs for each neu-\nron are concentrated within just 20% of the total input range,\ndemonstrating significant skewness. This pattern holds true\nacross various model architectures. Table 1 presents the aver-\nage range portion containing 65% of activation inputs relative\nto the total range for four LLMs: Falcon-7B/40B, Bloomz-\n7B1, and LLaMA2-7B. While the first three models employ\nGELU activation, LLaMA2-7B uses SiLU. Across all models\nand datasets, this portion consistently remains between 18-\n20%. These findings align with previous research [42, 46, 52],\nconfirming that skewed activation input distribution is an\ninherent characteristic of LLMs. This insight reveals that\nactivation function inputs follow a power law-like distribu-\ntion, enabling effective approximation of non-linear activation\nfunctions within a compact \"hot\" input range.\nInsight 2. Non-uniform distribution of layer and neuron\nimportance. Not all layers and neurons contribute equally\nto the model's performance. Using the same methodology\nas in the previous insight, we collected the Mean Square\nError (MSE) of approximating the activation function with\nlinear functions for each neuron in each layer. Specifically,\nfor each layer and neuron, we first identify the input range\ncontaining a certain portion (e.g., 85%) of activation inputs"}, {"title": "4.2 Overview", "content": "Armed with these insights, we propose TARDIS, the first sys-\ntem that enables constant folding in LLMs while preserving\nmodel capabilities and reducing parameter size. Our approach\nleverages two key observations: 1) the skewed distribution of\nactivation inputs allows us to focus approximation on the most\nfrequently accessed input ranges; 2) the varying importance\nof different layers and neurons suggests that approximation\nthresholds should be adapted accordingly.\nAs illustrated in Figure 7, TARDIS takes three inputs: an\nLLM, a calibration dataset, and a threshold t that defines what\nportion of inputs should fall within the linear approximated\nrange. The system generates two key outputs: a compressed"}, {"title": "5 TARDIS Design", "content": "In this section, we present the detailed design of TARDIS,\nwhich comprises offline and online components. The offline\ncomponent consists of three key aspects: approximating non-\nlinear activation functions with linear functions (Section 5.1),\ngenerating constant folded matrices (Section 5.2), and con-\nstructing predictors (Section 5.3). The online component de-\ntails how these elements work together during inference (Sec-\ntion 5.4)."}, {"title": "5.1 Non-linear Function Approximation", "content": "TARDIS aims to reduce FFN parameter counts and speedup\ninference by replacing complex non-linear activation func-\ntions with simple linear functions where possible. At a high\nlevel, our non-linear activation function replacing approach\nworks as follows. For each FFN layer that consists of two\nweight matrices (W\u2081 and W2) and a non-linear activation func-\ntion \u03c3 in between, we 1) break down the computation of the\nentire FFN into computations on individual neurons, where\neach neuron corresponds to a column in W\u2081 and a row in W2.\n2) For each neuron, analyze its input patterns to the activation\nfunction using a calibration dataset. 3) Replace the neuron's\nnon-linear activation with linear functions ($y = ax + b$) within\nits most frequently observed input range. The linear functions\nare obtained using least squares regression.\nAn intuitive design would be to replace each neuron's non-\nlinear activation with multiple linear ranges, as different input"}, {"title": "5.1.1 Single Range Approximation Strategy", "content": "To overcome the exponential storage overhead of multi-range\napproximation, TARDIS adopts a pragmatic single-range\nstrategy. For each neuron, we approximate its non-linear ac-\ntivation using one linear function within a carefully chosen\ninput range.\nMore specifically, for each neuron $n$, we aim to find a lin-\near function $y = ax + b$ that best approximates the activation\nwithin a range $[l_1,l_2)$. The approximation function $\\phi_n$ is de-\nfined as:\n$\\phi_n(x) = \\begin{cases}ax+b, & l_1 \\leq x < l_2 \\\\ \\sigma(x), & \\text{otherwise}\\end{cases}$\nTo quantify the quality of our approximation, we define the\nerror metric for the entire FFN layer as: $err(x) = \\sum_n err_n(x)$\nwhere each neuron's error contribution is measured using the\nL2 distance [56] between original and approximated outputs:\n$err_n(x) = ||\\sigma(xW_{1:,n})W_{2n,:} - \\phi_n(xW_{1:,n})W_{2n:}||_2$\nAdaptive Thresholding. Given the definition of FFN approx-\nimation error, TARDIS needs to determine appropriate linear\napproximation ranges for each neuron while satisfying the\nuser-specified in-range threshold t. A key challenge is decid-\ning how much of each neuron's input should be covered by\nthe linear approximation.\nBased on our insight in Section 4.1 that different layers\nand neurons contribute unequally to the model's performance,\nwe develop an adaptive thresholding mechanism that assigns"}, {"title": "5.2 Constant Folded Matrix Generation", "content": "Given the approximated linear ranges for each neuron in each\nlayer, TARDIS generates constant folded matrices C and bias\nvectors B through a systematic transformation process.\nFor each FFN layer, TARDIS first substitutes the activa-\ntion function with the approximated linear function $\\phi_n$ for\neach neuron n. This transformation creates a sequence of"}, {"title": "5.3 Predictor Generation", "content": "While Section 5.2 demonstrates how to generate optimized\nmatrices for linear approximation, we need a practical mech-\nanism to handle cases where inputs fall outside the approxi-\nmated range (Section 4.1).\nTo address this, TARDIS introduces a speculative approx-\nimation with result fixing scheme. The key idea is to avoid\nexpensive range checks before computation. Instead, TARDIS\noptimistically assumes all inputs fall within the linear range\nand proceeds with fast matrix operations. It then employs a\nlightweight predictor to identify which results need correction,\nonly fixing the inaccurate ones (Section 5.4).\nThe predictor's role is straightforward - it takes the FFN\nblock input and determines which neurons received inputs\noutside their approximated linear ranges. To implement this ef-\nficiently, TARDIS creates a compressed version of the weight\nmatrix $W_1$, which contains just enough information to make\nthese predictions without the overhead of full matrix opera-\ntions. For compressing $W_1$, TARDIS uses quantization tech-\nniques [29]. While alternatives like training smaller predic-\ntion networks are possible [42, 52], quantization provides an\neffective balance of accuracy and simplicity. TARDIS can\nincorporate various LLM compression methods like pruning\nor knowledge distillation, as long as they preserve sufficient\ninformation to predict out-of-range inputs. This enables high\nperformance while maintaining accuracy through selective\nresult fixing."}, {"title": "5.4 Inference Runtime", "content": "The TARDIS operates in two phases: an offline phase that\ngenerates the constant folded matrix (Section 5.2) and predic-\ntor (Section 5.3), and an online phase that performs inference\nthrough speculative approximation and result fixing.\nSpeculative Approximation. During this step, TARDIS ap-\nproximates the FFN computation using a pre-computed con-\nstant folded matrix. This simplifies the complex FFN com-\nputation into a simple matrix multiplication and addition op-\neration: FFN(x) \u2248 xC + B. This approximation significantly\nreduces the computational complexity compared to the origi-\nnal FFN computation.\nResult Fixing. Not all neurons can be accurately approxi-\nmated using the simplified computation above. The predictor\nidentifies these cases, and TARDIS corrects the results for\nthese specific neurons using the original computation method.\nThis correction involves two steps. First, we remove the ap-\nproximate results for the identified neurons. Then, we com-\npute and add back their actual results using the original FFN\ncomputation."}, {"title": "6 Implementation", "content": "We implemented TARDIS as a prototype using PyTorch [6].\nWe integrated TARDIS into the HuggingFace Transformer\nlibrary (v4.41.0) [10] and vLLM inference engine (v0.6.6) [7].\nThe system consists of two main components:\nOffline Component. The offline preprocessing pipeline, im-\nplemented in approximately 900 lines of Python code, handles\nnon-linear function approximation and constant folding ma-\ntrix generation. We leveraged cuML [11], a GPU-accelerated\nmachine learning library, for training the linear regression\nmodel and performing kernel density estimation efficiently.\nThe resulting constant-folded matrix is serialized to a bi-\nnary format for runtime loading. The predictor is obtained by\nGPTQ [29] with 2-bit precision, which is also stored in binary\nformat, and loaded before inference.\nRuntime Component. The inference runtime consists of two\nparts: (1) a Python library implementing speculative approxi-\nmation and result fixing logic (approximately 150 lines), and\n(2) a specialized NVIDIA CUDA kernel [2] for efficient result\nfixing. The kernel performs selective loading of FFN weights\nbased on predicted neuron indices. We optimized the memory\nefficiency by memory coalescing [32] and vectorized shared\nmemory access [13]."}, {"title": "7 Evaluation", "content": "Our evaluation mainly focuses on the following questions:"}, {"title": "7.1 Settings", "content": "Models. We evaluated TARDIS on five large language mod-\nels ranging from 1.6B to 11.1B parameters: Falcon-7B [19],\nFalcon2-11B [15], BLOOMZ-7B [38], GPT2-XL [3], and\nOPT-6.7B [17]. These models represent different architec-\ntures and training objectives while being widely adopted in\nreal-world applications. Each model employs a standard FFN\narchitecture with either GELU or ReLU activation functions.\nTable 2 summarizes the key characteristics of these models,\nincluding parameter count, architecture details, and activation\nfunctions.\nEvaluating Benchmarks. Following prior works evaluating\nLLM performance [28, 41, 43, 53], we evaluate models on\nlanguage generation and zero-shot tasks with lm-evaluation-\nharness [30]. For language generation, we measure perplexity\non Wikitext-2 [45], C4 [49], and Penn Treebank (PTB) [44].\nOn language generation tasks, perplexity measures how well\na model predicts text by calculating the exponential of the\naverage negative log-likelihood. Lower perplexity indicates\nbetter performance, with a perplexity of 1 representing per-\nfect prediction. For zero-shot evaluation, we test accuracy on\nPIQA [20], Lambada [12], and ARC-Challenge [16], which\ncover reasoning, word prediction, and question answering\ntasks respectively. Higher accuracy indicates better model\nperformance.\nSetup. We conducted all our experiments on a single NVIDIA\nRTX 4090D GPU with 24GB memory. Unless specified, the\ncalibration dataset we used consists of 8 samples randomly\nchoosen from the first shard of C4, each with 2048 tokens. C4"}, {"title": "7.2 Folded Model's Performance", "content": "We first quantifies the performance of the models folded by\nTARDIS on language generation tasks and zero-shot tasks,\nand compare with the baselines with varying compression\nratios.\nSummary. Our evaluation reveals that TARDIS outperforms\nWanda and RIA across different compression ratios:\n1) At lower compression ratios (50% and 70%), TARDIS\ndemonstrates marginally better performance in both language\ngeneration and zero-shot tasks, achieving 4.6\u00d7 and 3.1\u00d7 lower\nperplexity, 1.47\u00d7 and 1.26\u00d7 higher accuracy on average.\n2) At higher compression ratio (80%), TARDIS shows sub-\nstantial improvements over the baselines, achieving up to\n215x and 406\u00d7 lower perplexity on average, while maintain-\ning an average of 20.1% and 19.1% absolute improvement,\nwith up to 65.7% higher accuracy on zero-shot tasks."}, {"title": "7.3 Range Assignment Algorithm", "content": "Recall that the range assignment algorithm is used to deter-\nmine the linear range for each neuron in the model (Sec-\ntion 5.1), which is one of the key components of TARDIS. In\nthis section, we evaluate the preciness, effectiveness, and sen-\nsitivity in terms of calibration dataset of the range assignment\nalgorithm."}, {"title": "7.4 Inference Speedup", "content": "We evaluate the practical inference speedup achieved by\nTARDIS on Falcon-7B across different compression ratios.\nWe measure both FFN block-level and end-to-end speedup us-\ning the vLLM and HuggingFace frameworks. For evaluation,\nwe perform text generation starting with 8 tokens and generat-\ning 192 additional tokens, comparing inference times across\ncompression ratios from 10% to 80%. As shown in Figure 13,\nTARDIS begins providing inference benefits at 30% compres-\nsion. The speedup increases with higher compression ratios,\nreaching 1.86\u00d7 FFN speedup at 80% compression, with end-\nto-end inference speedups of 1.39\u00d7 on HuggingFace and\n1.59\u00d7 on vLLM. vLLM achieves better speedup due to its\noptimized attention mechanism [37] - attention takes 55% of\ninference time in HuggingFace but only 31% in vLLM.\nWhen processing sequences with many initial tokens but\ngenerating few output tokens (a less common scenario [14]),\nTARDIS shows limited performance gains. This is because\nduring the initial processing phase, each input token tends to\nactivate different neurons, reducing the sparsity patterns that\nTARDIS leverages for acceleration."}, {"title": "7.5 Performance Breakdown", "content": "We analyze the inference time breakdown of TARDIS by\nmeasuring four key components: predictor execution, con-\nstant folded matrix computation, result fixing, and auxiliary"}, {"title": "7.6 Influence of Predictor Size", "content": "The predictor's accuracy in TARDIS, influenced by its size,\naffects model performance [52]. We evaluated Falcon-7B's\nperplexity on WikiText2 with varying predictor sizes, con-\ntrolled by the number of bits used in quantization [29] of\nthe weight matrix in FFN. Figure 15 shows that perplexity\ndecreases as predictor size increases, with a maximum dif-\nference of 0.12. This suggests that TARDIS can use a small\npredictor to balance performance and memory footprint."}, {"title": "7.7 Precision Effects on Reordering FFN", "content": "Floating point arithmetic exhibits commutativity but not per-\nfect associativity or distributivity due to rounding errors in\nfinite precision operations [8]. While TARDIS applies these\nproperties during constant folding, we empirically evaluate"}, {"title": "8 Related Works", "content": "LLM Compression. Recent LLM compression approaches\nfall into four categories: 1) Pruning removes weights through\nstructural pruning [43, 68], unstructural pruning [42, 53],\nand contextual pruning [42, 52]. While effective, pruning\noften suffers from accuracy degradation at high compres-\nsion ratios, and contextual pruning is mainly limited to\nReLU-based models. 2) Quantization reduces parameter pre-\ncision [28, 34, 41, 59] by converting floating-point weights to\nlower bit representations. 3) Knowledge distillation transfers"}, {"title": "9 Discussion and Limitations", "content": "The primary limitation of TARDIS lies in its handling of\nLLMs that use GLU-variant Feed-Forward Networks [51]\n(Gated-FFN), which make up about 53% of the most pop-\nular text-generation models on HuggingFace. Models like\nLLaMA3.1 [26] and Qwen 2.5 [61] use a FFN variant with\nquadratic operations: FFN(x) = (xW\u2081) \u2299 (xW2)W3, where\n\u2299 is elementwise multiplication. This architecture poses signifi-\ncant challenges for matrix folding since the folded matrices\ngrow exponentially. Applying our approach to LLaMA-2-7B\nwould increase parameters by 254\u00d7, undermining our goal of\nreducing computational costs. The Multi-head attention block\nencounters similar limitations. Future works could focus on\ndeveloping specialized folding techniques that can efficiently\nhandle these quadratic operations while maintaining reason-\nable memory requirements."}, {"title": "10 Conclusion", "content": "We present TARDIS, a novel approach to compress LLMs\nby applying constant folding optimization to feed-forward\nnetworks. Our method approximates non-linear activations\nwith linear functions in common input ranges, enabling pa-\nrameter reduction through matrix multiplication reordering.\nExperiments show TARDIS reduces FFN parameters by up to\n80% while maintaining accuracy, achieving 1.6\u00d7 inference\nspeedup on vLLM and 1.4\u00d7 on HuggingFace."}]}