{"title": "On Adversarial Robustness of Language Models in Transfer Learning", "authors": ["Bohdan Turbal", "Anastasiia Mazur", "Jiaxu Zhao", "Mykola Pechenizkiy"], "abstract": "We investigate the adversarial robustness of LLMs in transfer learning scenarios. Through comprehensive experiments on multiple datasets (MBIB Hate Speech, MBIB Political Bias, MBIB Gender Bias) and various model architectures (BERT, ROBERTa, GPT-2, Gemma, Phi), we reveal that transfer learning, while improving standard performance metrics, often leads to increased vulnerability to adversarial attacks. Our findings demonstrate that larger models exhibit greater resilience to this phenomenon, suggesting a complex interplay between model size, architecture, and adaptation methods. Our work highlights the crucial need for considering adversarial robustness in transfer learning scenarios and provides insights into maintaining model security without compromising performance. These findings have significant implications for the development and deployment of LLMs in real-world applications where both performance and robustness are paramount.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have become pivotal in natural language processing (NLP), demonstrating remarkable performance across various tasks. Transfer learning, a technique leveraging pre-trained models for new tasks, has significantly contributed to this success [5]. However, the intersection of transfer learning and adversarial robustness in LLMs remains understudied, presenting a critical gap in understanding models' security and reliability.\nWhile transfer learning efficiently applies pre-trained models to new domains, it may inadvertently introduce or amplify vulnerabilities to adversarial attacks. These attacks pose significant threats to model deployment in real-world scenarios. Despite the widespread adoption of transfer learning, there is a notable lack of comprehensive research on how these adapted models perform against adversarial attacks.\nPrevious studies have primarily focused on the robustness of models in their initial training or fine-tuning stages [12] [15] [1], often in controlled environments. This approach overlooks the potential risks emerging from more complex training sequences, particularly those involving multiple pre-training stages as in transfer learning scenarios. The impact of transfer learning on model robustness is nuanced and multifaceted. While some research suggests that post-fine-tuning can lead to decreased robustness [15], other findings indicate that incorporating additional data from the target dataset can enhance robustness [14]. However, in transfer learning scenarios involving pretraining on related but distinct domains, the impact on robustness becomes more complex and warrants careful investigation.\nOur study aims to address the following key research questions:\nRQ1: How does transfer learning affect LLMs' performance and robustness overall?"}, {"title": "Experimental Design", "content": "We focus on the classification task of detecting biased text. We selected the MBIB Hate Speech [13], MBIB Political Bias, and MBIB Gender Bias datasets for their relevance to real-world applications and the importance of robustness in these domains."}, {"title": "Datasets", "content": "We selected three distinct datasets that share a common theme of detecting bias in textual data but address different subdomains within this broader context. This choice allows us to meaningfully explore the impact of transfer learning, as it involves transferring knowledge across related yet distinct types of biases. Each dataset has 2 classes: biased and non-biased, and is balanced. In general, the ability to accurately detect and mitigate various forms of bias is crucial to develop fair and ethical AI systems that can be safely deployed in diverse real-world applications [6]. Data sets are selected from [13] and are as follows:\nMBIB Hate Speech (HS) focuses on identifying hate speech in text.\nMBIB Political Bias (PB) is used to detect political bias in textual data.\nMBIB Gender Bias (GB) helps evaluate the model's ability to recognize gender bias in text.\nFor each dataset, we create a large training set (12,000) for pre-training, a small training set (600) for target task fine-tuning, a validation set (1,000) and a test set (1,000)."}, {"title": "Evaluation Metrics", "content": "We use the following metrics to assess model performance and robustness:\nOriginal Accuracy (OAcc): Main usual metric to evaluate the performance of the model in classification.\nAttack Success Rate (ASR): Percentage of True Positive and True Negative examples that were hacked by the attack, this metric can serve as a basic evaluation of the robustness of the model.\nAccuracy Under Attack (AUA): The accuracy of the model after attack. This metric can be considered a 'safety' metric for the model. For instance, if the model's accuracy (Acc) significantly increases while the Attack Success Rate (ASR) only mildly increases, the AUA may show improvement even though the model has become less robust overall."}, {"title": "Parameters Setting", "content": "For the pre-training phase, we trained the models for 1 epoch on the larger subset of the dataset. During the fine-tuning phase on the target dataset, the models were trained for up to 6 epochs, with the best model selected based on the accuracy of the validation set. We used the Adam optimizer, adjusting the learning rate between $5 \\times 10^{-6}$ and $4 \\times 10^{-4}$ depending on the specific model, to ensure optimal convergence during training."}, {"title": "Training Procedure", "content": "We measured the performance under two conditions:\nUsual Fine-Tuning: Fine-tuning the model directly on the target dataset.\nTransfer Learning: Fine-tuning the model on one large dataset followed by transfer learning to the target task using the smaller dataset.\nThis setup allowed us to measure the influence of each training sequence on both accuracy and adversarial robustness, providing insights into the trade-offs involved in using transfer learning for LLMs in classification tasks aimed at detecting biased versus non-biased text. The overall experiment setup is displayed in Figure 1."}, {"title": "Experimental Setup", "content": "In our experiments, we conducted two main types of experiments:\nStandard Transfer Learning: Fine-tuning models on one dataset followed by transfer to another.\nAdversarial Training with Transfer Learning: Incorporating adversarial examples (10% of training data) during the transfer learning process."}, {"title": "Models", "content": "We evaluated a range of LLMs to assess the impact of model size and architecture on robustness:\nBERT Base (110M), BERT Large (340M), ROBERTa Base (125M), ROBERTa Large (355M), GPT-2 (117M), GPT-2 Medium (345M), and GPT-2 Large (762M) and also large models like Gemma 2b (2B), Phi-2 (2.7B), and GPT2-XL (1.5B)."}, {"title": "Adversarial Attack Methods", "content": "We employ two attack methods in our experiments:\nTextFooler [8]: A word-level adversarial attack method for text classification. It uses word deletion impact for importance ranking, word embeddings for synonyms, and Universal Sentence Encoder for semantic similarity constraints.\nA2T [16]: A computationally efficient adversarial attack method. It uses gradient-based word importance ranking, counter-fitted word embeddings for synonyms, and DistilBERT for semantic similarity constraints."}, {"title": "Results and Analysis", "content": "Our experiments yielded several key insights into the impact of transfer learning on the robustness and performance of LLMs. We'll discuss our findings in relation to each research question."}, {"title": "RQ1: Transfer learning robustness", "content": "We evaluated various LLMs using TextFooler (black-box) and A2T (white-box) adversarial attacks. The results, presented in Table 1, reveal a concerning trend:\nIncreased Vulnerability: In most cases, especially for smaller models, the Attack Success Rate (ASR) increased after transfer learning, regardless of changes in accuracy (OAcc). It suggests that even when models demonstrated enhanced performance in terms of accuracy, their overall robustness against adversarial attacks often decreased.\nPerformance-Robustness Trade-off: Even when models showed improved accuracy, their robustness against adversarial attacks often decreased. For example, on the Hate Speech dataset, GPT-2 experiences a mean 20.4% increase in ASR alongside a 3.67% increase in accuracy. This finding raises significant concerns about LLM security, as improvements in accuracy during training might lead developers to overlook other critical parameters like robustness.\nTable 2 presents the percentage of unaveraged sequences with increased ASR, confirming this robustness decline trend. The complete raw data is available in Appendix B."}, {"title": "LoRA and Larger Models", "content": "For large models with billions of parameters, we used LoRA due to its efficiency in adapting these models, as conventional fine-tuning often requires extensive computational resources that may not be readily available in typical settings. When applying LoRA to these larger models, we observed mixed results. Some sequences showed decreased robustness, while others demonstrated increased robustness (e.g., political bias dataset for Phi-2 and Gemma 2b), result are presented in Table 1 and Table 2.\nThe impact of LoRA on robustness is complex due to its unique approach: introducing and randomly initializing a small set of additional parameters rather than fine-tuning existing ones. This may lead to different robustness outcomes compared to standard fine-tuning. While transfer learning here can still reduce robustness through issues like false memories [4] or shortcut learning [3], catastrophic forgetting [10] may not contribute significantly to the results in this specific setting. This is because, with the random initialization of LoRA adapter parameters and the freezing of other parameters, there is no pre-existing information in the adapters that could be distorted or lost during the transfer learning process, thus potentially altering the dynamics of how robustness changes during transfer learning."}, {"title": "RQ2: Impact of Model Size, Architecture, and Training Procedures", "content": "We examined how different model sizes, architectures, and training procedures (including LoRA for larger models) influenced the transfer learning effect on robustness."}, {"title": "Model Size and Architecture Influence", "content": "Larger Models Show Better Resilience: As illustrated in Figure 2, larger models within each family (GPT-2, BERT, ROBERTa) exhibited smaller increases in ASR due to transfer learning.\nInitial ASR Variations: The mean initial ASR (before transfer learning) didn't follow a consistent pattern across model families Figure 3:\nDecreased with size in BERT and RoBERTa, but increased with size in GPT-2 models.\nOverall ASR range remained similar (0.42 to 0.47) across autoregressive and encoder-based models, indicating that both of them exhibit comparable levels of robustness against adversarial attacks."}, {"title": "RQ3: Real-world implications", "content": "As we showed, often ASR increases in parallel to OAcc, which indicates a potential trade-off of using Transfer learning between performance and safety. Often standard metrics like OAcc are prioritized, while other safety metrics are overlooked, leading to vulnerable models being deployed. Based on our findings, we highlight the necessity of applying additional techniques and adversarial testing to mitigate this issue, particularly when fine-tuning smaller models. For Larger Models with LoRA, use transfer learning cautiously, as effects on robustness can vary."}, {"title": "RQ4: Trade-off Between Robustness and Accuracy", "content": "Experimental Setup This experiment explores the balance between robustness and accuracy in LLMs under adversarial attacks during transfer learning. Two methods are compared:\nIterative Transfer Learning with Adversarial Attacks: The model is sequentially trained and evaluated on three datasets (Hate speech (HS), Political bias (PB) and Gender bias (GB)), with a final evaluation across all datasets to track performance over time.\nAdversarial Training with Transfer Learning: Adversarial samples (10%) are included during training to enhance robustness, with performance assessed across all datasets.\nResults in Table3 show how attacks (A2T, TextFooler) impact Original Accuracy (OAcc), Accuracy Under Attack (AUA), and Attack Success Rate (ASR). \"FE\" denotes Final Evaluation, and \"PC\" represents Percent Change relative to earlier evaluations."}, {"title": "Conclusion", "content": "Our research contributes to the understanding of the adversarial robustness of LLMs in the context of transfer learning. Our empirical analysis reveals nuanced dynamics in the relationship between traditional performance metrics, such as accuracy, and the robustness of models against adversarial attacks. Interestingly, we observed instances where improvements in conventional metrics were accompanied by a decrease in adversarial robustness, suggesting a potential trade-off between performance enhancement and vulnerability to adversarial manipulations. This counterintuitive finding underscores the complexity of model behavior in transfer learning scenarios and raises questions about the underlying causes, which may include phenomena such as catastrophic forgetting or the acquisition of misleading \u201cfalse memories\u201d during pre-training. Notably, our results indicate that larger models may exhibit a reduced susceptibility to this trend, hinting at an inherent robustness associated with scale."}, {"title": "Social Impact Statement", "content": "Our research rigorously examines the balance between performance enhancements and security vulnerabilities in large language models (LLMs) using transfer learning. This analysis has highlighted the need for training methodologies that prioritize both model effectiveness and security.\nAs LLMs become more common in sectors like healthcare, finance, and public services, it is crucial to protect these systems from sophisticated adversarial threats. Our findings show that while transfer learning can improve model performance, it can also introduce and magnify vulnerabilities that malicious actors could exploit, necessitating a reevaluation of current training practices.\nWe advocate for incorporating comprehensive adversarial training and robustness assessments during the AI development. By adopting these practices, developers can better manage the trade-offs between accuracy and security, ensuring that improvements in LLM capabilities do not compromise their defense.\nOur study reveals interesting nuances in the interaction between transfer learning, performance, and security. We observed instances where transfer learning not only contributed to performance improvements but also bolstered the models' defenses against adversarial attacks under certain conditions. These insights suggest that transfer learning, when applied thoughtfully, might offer opportunities to simultaneously enhance both the effectiveness and the security of LLMs, meriting deeper investigation into these phenomena."}, {"title": "Transfer learning experiments raw data", "content": "While Table 1 presented averaged results, the raw data provides more granular insights into the behavior of different model architectures and attack methods.\nTable 4 through Table 13 demonstrate an inverse relationship between model size and vulnerability to adversarial attacks post-transfer learning within the GPT-2 family. GPT-2 exhibits ASR increases up to 80% for some sequences, whereas GPT-2 XL's maximum ASR increase is approximately 22%.\nTable 7 and Table 8 show that BERT models, despite their bidirectional architecture, display vulnerability patterns similar to GPT-2, with BERT Large showing marginally improved robustness.\nROBERTa models (Table 9 and Table 10) exhibit an noteworthy characteristic: while generally more robust than BERT, they still incur significant ASR increases, particularly against the a2t attack. This suggests that RoBERTa's enhanced pretraining does not necessarily confer improved adversarial robustness in transfer learning scenarios.\nThe results for Phi-2 and Gemma 2B (Table 11 and Table 12) are particularly noteworthy. These LoRA-tuned models show highly variable results, with some sequences demonstrating improved robustness post-transfer. This variability indicates a complex interaction between LoRA's adaptation mechanism and adversarial vulnerability, warranting further investigation.\nThese raw results not only corroborate our main findings but also elucidate the nuanced impact of model architecture, size, and fine-tuning method on adversarial robustness in transfer learning contexts."}]}