{"title": "FPE-LLM: Highly Intelligent Time-Series Forecasting and Language Interaction LLM in Energy Systems", "authors": ["Zihang Qiu", "Chaojie Li", "Zhongyang Wang", "Huadong Mo", "Renyou Xie", "Guo Chen", "Zhaoyang Dong"], "abstract": "This paper introduces Fusion PEFT Energy LLM (FPE-LLM), a large language model (LLM) fine-tuned for energy system forecasting using a combination of Prefix and Lora Parameter-Efficient Fine-Tuning (PEFT) methods. FPE-LLM addresses three key challenges in the energy system and LLM fields: 1. Enhancing few-shot learning for handling extreme environmental conditions. FPE-LLM can leverage both textual and time-series data to achieve accurate predictions in few-shot contexts. 2. Reducing dependence on expert input to improve efficiency. FPE-LLM can provide guidance and results on related problems, acting like an expert system. Even non-experts can use FPE-LLM to complete all tasks related to forecasting and its associated tasks. 3. Mitigating hallucination risks through standardized fine-tuning. We validated this through multi-task learning and the self-reasoning characteristics of LLMs.\nOur research opens the door to fully realizing the intelligent potential of FPE-LLM in the energy forecasting field. With the injection of more knowledge and data, FPE-LLM is expected to replace a significant amount of manual work and contribute to the stability and efficiency of energy forecasting.", "sections": [{"title": "I. INTRODUCTION", "content": "FORECASTING is the foundation of all energy systems, with all planning and scheduling tasks for future energy systems being based on forecasting results. Energy system forecasting tasks include but are not limited to demand forecasting, price forecasting, renewable energy integration, load forecasting, reliability and stability assessment of energy systems, and frequency regulation forecasting. At present, the commonly used forecasting models in energy systems are mainly divided into pure mathematical methods for predictive modeling and AI-based neural network predictive modeling.\nPrediction models based on pure mathematics, such as linear regression, moving average, autoregressive, Bayesian regression, vector autoregression model, Kalman filter, and seasonal decomposition, have strong interpretability due to their well-established theoretical foundations and the observability of their computational processes [1-3]. In contrast, neural network predictions, such as recurrent neural networks, generative adversarial networks, spatial-temporal graph neural networks, and transformers, are better at handling nonlinear data and have strong generalization and scalability [4-6].\nHowever, both mathematical prediction models and neural network prediction models have some common limitations.\nFirst, in the case of extreme weather or events, traditional models struggle to capture these features due to the lack of sufficient data on extreme scenarios, making them inadequate in expressing sudden situations. Second, traditional prediction models have limited ability to handle multimodal heterogeneous data, especially textual heterogeneous data, meaning they cannot effectively utilize information from regulations and texts in various fields. Additionally, traditional prediction models can only provide numerical prediction results, making it difficult to determine whether the results align with the current real-world scenario. Furthermore, they require additional expert input for subsequent data analysis and decision-making, which not only increases time costs but also introduces subjectivity and inconsistency, impacting the overall efficiency and reliability of the forecasting process.\nThe aforementioned issues can be addressed by large language models (LLMs) [7-12]. For capturing extreme weather and events, natural language provides more flexible descriptions of extreme events, and since LLMs are trained on vast datasets, they can roughly understand the impact of various types of extreme events on prediction results through similar data in their knowledge base. This allows LLMs to better capture rare extreme events in the dataset. After fine-tuning on relevant scenario-specific corpora, the extent to which extreme events in a given scenario affect the current task can be better quantified and described [9, 10], showcasing excellent few-shot inference capabilities [13, 14]. In terms of handling multimodal heterogeneous data that includes textual information, LLMs have a natural advantage. They can not only integrate new corpus knowledge through fine-tuning methods [15, 16], but also handle numerical modal data by incorporating pre-trained modules (such as prefix, patch, etc.) [12, 17]. Moreover, LLMs have the ability to process context, meaning they can not only provide prediction results but also engage in follow-up dialogue based on those results [18]. This eliminates the need for expert input for post-prediction tasks. Even non-experts, without deep understanding of the energy forecasting field, can inquire and interact with LLMs to understand how to carry out subsequent tasks.\nAs a relatively new area of research, current work on LLMs has not yet fully achieved all of the above-mentioned capabilities. One major issue is that existing temporal large language models lack language interaction capabilities [11, 12, 17, 19, 20]. On the other hand, LLMs fine-tuned for"}, {"title": "II. FRAMEWORK OF FPE-LLM", "content": "As illustrated in Fig.1, FPE-LLM adopts a flexible channel architecture. Based on the modality and structure of the input data, FPE-LLM determines which channels to activate, thereby enabling the simultaneous training of heterogeneous multimodal data. The scheme for activating the three channels in Fig.1 is represented by locks in three different colors.\nAdditionally, FPE-LLM adopts the FP tuning method, which combines prefix tuning and LoRa fine-tuning. The positions of the prefix block and LoRa block in FPE-LLM are shown in the figure."}, {"title": "A. FPE-LLM architecture", "content": "FPE-LLM architecture includes the following components:\nPrefix Tuning Block: This block is used to handle pure time series information. The input time-series numerical data are normalized. The prefix tuning block can also be replaced by the patch fine-tuning module. The key task in this part is to process and transmit temporal information, which plays a guiding role in word associations within the high-dimensional vector space of FPE-LLM.\nIn this module, we adopted the attention mechanism:\n$a(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$\nwhere Q, K, V are query matrix, key matrix, and value matrix respectively. a is the attention output.\n$ \\{\\begin{array}{l}\\alpha_h = ReLU (W_1 \\cdot a + b_1), \\\\h_n = ReLU(W_n \\cdot h_{n-1} + b_n),\\end{array} $\nFormula.2 represents an MLP connected after the attention output, where $h_n$ and $b_n$ are the output and bias of the n-th layer of the MLP, respectively. The ReLU activation function is used to eliminate negative values.\nLLM Text Preprocessing Layer: The input textual in-formation includes identity setting, textual information, and questions, which together form the prompt for FPE-LLM.\n$ \\{\\begin{array}{l}\\{W_1,W_2,..., W_{mi}\\} = Tokenizer(text), \\\\V_i = E(w_i), \\\\h^T = \\{V_1, V_2,\u2026, V_{mi} \\},\\end{array} $\nAs in formula.3, the textual information is first tokenized by the LLM's tokenizer into a sequence of words, where $m^i$ is the number of input words. Each word $w_i$ is mapped to a high-dimensional vector space $v_i$ through the embedding matrix E, where E is a $V \\times d$ matrix, V is the size of the LLM's vocabulary, and d is the dimension of the embedding vectors. Ultimately, these high-dimensional vector sequences form the textual information $h^T$ that is input into the LLM.\nEmbedding Layer: This layer comprises numerical input, placeholder tokens, and text tokens. Placeholder tokens are used to separate numerical input and text tokens dimensionally and are represented by invalid characters composed of fixed nagative numbers, these invalid characters function similarly to inputting a large number of spaces when interacting with an LLM to separate different tasks, which helps the LLM recognize the differences in data types by separating different categories of data dimensionally, as formula.4. $h^p$ is placeholder sequence. $h^E$ is the concatenated input of the LLM.\n$ \\{\\begin{array}{l}h^P = InvalidChar, \\forall i, j, \\\\h^E = SeqEmbedding[h^N : h^P : h^T] \\end{array} $\nBody with LoRa: LoRa blocks are used for fine-tuning without altering the original model's performance. The role of LoRa fine-tuning is to inject domain knowledge from the energy field and reconstruct the associations of FPE-LLM's knowledge in the high-dimensional vector space, thereby achieving fine-tuning of textual content. It can handle specific tasks and conduct conversations with LoRa activated, or directly use the pre-trained base model for conversations when LoRa is not activated, as in formula.5.\n$O^{LLM} = LLM \\& LoRa(h^E),$\nwhere $O^{LLM}$ is the output of LLM deep neural network.\n$ \\{\\begin{array}{l}W = W_0 + \\Delta W, \\\\\\Delta W = A B^T.\\end{array} $"}, {"title": "III. HETEROGENEOUS MULTIMODAL DATA ALIGNMENT", "content": "This paper involves three types of heterogeneous data: time series numerical data, textual data and messages, and numerical + textual multimodal data. As shown in Fig.2, there are three channels for heterogeneous data, corresponding to the green, red, and blue lock symbols in Fig.1. These heterogeneous channels enable FPE-LLM to process inputs of different structures. It can be observed that mode 1 and mode 3 have the same structure at the output position of FPE-LLM, allowing them to be converted into textual outputs simultaneously. This also enables FPE-LLM to simultaneously perform two types of training: time series prediction (numerical + text) and the injection of domain-specific textual knowledge (purely text)."}, {"title": "IV. FUNCTION CALLING", "content": "Any functions\nto be executed?"}, {"title": "V. CASE STUDY", "content": "The case study focuses on the three main contributions. The first subsection introduces the datasets used in three forecast-ing scenarios within the energy system. The second subsection presents the accuracy performance in these scenarios. The third section, addressing the contribution related to forecasting accuracy under extreme environmental changes, demonstrates how sudden environmental changes at specific points impact the prediction accuracy of FPE-LLM through CoT (Chain of Thought) reasoning. The third subsection also showcases FPE-LLM's deep communication and ToT (Tree of Thought) rea-soning capabilities, highlighting its responses under three ToT sub-tasks: fine-tuning with fixed response formats, knowledge injection without fixed response formats, and completely zero-shot learning. The fourth subsection focuses on identifying hallucinations, where we demonstrate how changing the multi-task coefficient affects the forms of hallucination. We confirm that fine-tuning with fixed response formats in specialized domains helps in detecting hallucinations. In the final section, we conducted statistical ANOVA analysis, which confirms the reliability of FPE-LLM's responses.\nIn the fine-tuning process, we used GLM4-9b [34] as the base LLM model. We used an L20 48GB GPU for fine-tuning."}, {"title": "A. Data Distribution", "content": "We applied FPE-LLM to three classic energy system forecasting scenarios: load forecasting, PV forecasting, and electricity price forecasting. The load forecasting data was selected and provided by Ausgrid, consisting of data from 300 home customers without PV installations in the NSW state of Australia in 2018. We considered natural features such as holidays, temperature, visibility, humidity, dew point, wind speed, precipitation, and direct sunlight intensity. Notably, the features of holidays, temperature, dew point, and wind speed were not only considered in the time series data but also included as corresponding textual descriptions as input. Additionally, we used a random forest model for preliminary forecasting, and the results were incorporated as part of the textual input of FPE-LLM. In this scenario, we used 16-step (8 hours) features as input for the time series part.\nThe PV data was provided by China Southern Power Grid, coming from a photovoltaic plant with a capacity of 798 kWh located in Ningbo, China, at coordinates longitude 121.2944, latitude 29.32238, in 2022. In the numerical time series input, we considered a large number of natural features related to wind speed, sunlight, and climate. In the textual descriptions of FPE-LLM, we included details about sunlight intensity, cloud changes, wind speed information, cloud thickness, and visibility. In this scenario, we used 16-step (4 hours) features as input for the time series part.\nThe electricity price data was sourced from publicly avail-able data on the AEMO website for NSW state in 2022 [35]. The features included electricity prices from various markets, such as the ancillary services (FCAS) (6s, 60s, 5min prices for raise and lower services), the electricity market, demand fluctuations, and more. Due to the lack of distinct features in the electricity price data, we only textualized situations involving high buy and sell prices (buy prices above 1000 and sell prices above 500). In this scenario, we used 48-step (4 hours) features as input for the time series part."}, {"title": "B. FPE-LLM Forecast Performance", "content": "This section demonstrates the prediction accuracy of FPE-LLM under normal conditions. Considering that the loss func-tion of LLMs is better suited for handling classification tasks, we adopted a multi-task learning approach and separately present the accuracy for both classification tasks and regression tasks. Task 1 involves interval prediction, which is a textual classification problem. Task 2 is a textual regression problem.\n$f(P_t) \\{\\begin{array}{ll}0, & \\text { if } P_t = 0 \\\\ \\frac{i}{N} \\epsilon_i< P_t < \\epsilon_{i+1}, & \\text { for } i = 1, \\dots N.\\end{array} $\nAs an example, formula.11 represents our classification method for the PV classification task. Here, $P_t$ denotes the PV power generation at time t, $E_r$ is the rated grid-connected capacity of the PV plant, and N is the number of intervals into which $E_r$ is divided. Typically, we divide the rated capacity of 798 kW into 100 intervals, plus an additional interval for when $P_t = 0$, resulting in a total of 101 classes: $\\{0, 0 \\sim 1\\%, 1 \\sim 2\\%, \\dots, 99 \\sim 100\\%\\}$."}, {"title": "C. FPE-LLM Few-Shot Performance under CoT", "content": "The greatest advantage of FPE-LLM lies in its performance on few-shot tasks. In the load and PV data, there are some extremely rare few-shot data points. For example, in the load data, there are few-shot weather data; and in the PV data, there are specific holidays and rare weather data.\nTaking weather conditions in PV data as an example, in the 2022 weather forecast data for Ningbo Province, certain weather patterns, such as \"heavy rain turning to clear,\" \"clear turning to heavy rain,\" \"cloudy turning to overcast,\" and \"haze\", are rare few-shot weather features. In 2022, with data collected at 15-minute intervals, \"heavy rain turning to clear\""}, {"title": "D. FPE-LLM ToT: Post-pred Sub-task Performance", "content": "FPE-LLM is capable of handling post-prediction tasks based on the ToT framework. This ability enables the prediction and subsequent tasks to no longer rely on experienced engineers. Even non-experts can use FPE-LLM to handle professional tasks related to predictions, significantly reducing labor costs associated with the need for highly skilled personnel. The ToT response framework of FPE-LLM is shown in Fig.9. After obtaining the prediction results, if hallucination occurs, the model performs a re-reasoning; if no hallucination is detected, the model proceeds to handle the post-prediction tasks.\nWe have categorized the post-prediction tasks into three types: 1. Related knowledge fine-tuned, and during fine-tuning,"}, {"title": "E. Fixed Formats: A Method for Identifying Hallucinations", "content": "Hallucination is an unavoidable issue when LLMs engage in text-based communication. However, there is currently a lack of methods to detect hallucination. This chapter explains how we identify obvious instances of hallucination.\nAs shown in Fig.11, first, we fine-tuned FPE-LLM's predic-tion tasks with a fixed output format. After doing so, whenever keywords like \"<timestamp>\u201d and \u201cPV forecast\" appear in the prompt, FPE-LLM will try to respond in the format used during the fine-tuning process. We found that by adjusting the multi-task coefficient $w$, the output format of FPE-LLM can be affected, causing it to no longer follow the format used during fine-tuning. This is also considered a form of hallucination. Such hallucinations were identified using cosine similarity as shown in formula.12 [37, 38].\n$(E^{LLM}, O^{LLM}) = 1 \u2013 \\frac{E^{LLM} \\cdot O^{LLM}}{\\|E^{LLM} \\| \\cdot \\|O^{LLM} \\|}.$\nHere, $\\|E^{LLM} \\|$ is the high-dimensional vector represen-tation of the expected FPE-LLM output after fine-tuning. $\\|E^{LLM}\\|$ and $\\|O^{LLM} \\|$ are the norm of the corresponding vectors. is cosine similarity. The larger the value of, the greater the similarity of the sentences. Typically, when is less than 0.8, we consider the output to be a hallucination.\nIn our task setup, Task 1 is the CLS task, and Task 2 is the REG task. The larger the multi-task coefficient $w$, the higher the weight of the CLS task and the lower the weight of the REG task; conversely, as $w$ decreases, the CLS weight decreases while the REG weight increases. It can be observed that, regardless of the task, when its task weight decreases, hallucinations that would not normally occur and that do not conform to the fine-tuning format start to appear. Additionally, the lower the task weight, the more frequent these hallucinations become.\nAmong the many hallucinations that occurred in our exper-iments, we selected the shortest one to display in Fig.11. It can be seen that such hallucinations typically manifest in the form of self-reasoning. This indicates that when hallucination occurs, the prediction results of FPE-LLM no longer depend on multimodal input but rely solely on the textual description. Due to the model's insufficient ability to process multimodal data during hallucination, the prediction accuracy also de-creases. The lower the task weight, the higher the proportion of hallucination, and the higher the MAE in the predictions.\nFrom this experimental result, it can be concluded that when fine-tuning LLM for prediction tasks using a fixed-text format, no unexpected, non-standard output will occur under normal circumstances. However, if FPE-LLM cannot extract enough information from the input data, it responds to the prediction results in a self-reasoning manner with unexpected, non-standard output. When this happens, we know that a certain portion of the data has become ineffective, and the current prediction result is a hallucination.\nBy combining techniques such as standardized fixed format fine-tuning, multi-task learning, and the self-reasoning charac-teristics of LLMs, FPE-LLM's hallucinations become apparent as the model no longer adheres to the standardized output"}, {"title": "F. ANOVA Analysis: Reliability of FPE-LLM Responses", "content": "Since FPE-LLM uses probabilistic distribution predictions, its textual output, even with fixed format fine-tuning, still exhibits some degree of randomness. Therefore, we conducted 100 experiments on each dataset and performed ANOVA analysis to calculate the confidence interval for each time point in the test set [39], as shown in formula.13.\n$\\begin{aligned}S S T & =\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2} \\\\S S B & =\\sum_{j=1}^{k} n_{j}\\left(\\bar{X}_{j}-\\bar{X}\\right)^{2} \\\\S S W & =\\sum_{j=1}^{k} \\sum_{i=1}^{n_{j}}\\left(X_{i j}-\\bar{X}_{i}\\right)^{2} \\\\M S B & =\\frac{S S B}{k-1} \\\\M S W & =\\frac{S S W}{n-k} \\\\F & =\\frac{M S B}{M S W}\\end{aligned}$\n$X_i$ is the i-th Observations. $\\bar{X}$ is the mean value. n is the number of observations. SST is total sum of squares, reflecting the total variance. The total variance can be separated into between-groups variance and within-groups variance. k is the number of experimental sets. $n_j$ is the number of observations in the j-th experimental sets. SSB is the sum of squares between groups, which reflects the between-groups variance. SSW is the sum of squares within groups, which reflects the within-group variance. F is the F-statistic value.\nDue to the high similarity of the results, we used PV data as an example. We calculated the F-statistic value to be 0.00473. Using this F-statistic, we looked up the F-distribution table to obtain the P-value. In the results of the PV prediction, the P-value was 0.99999. The small F-statistic and high P-value indi-cate that there is no significant statistical difference among the 100 sets of prediction results. This suggests that the prediction results are very stable, with minimal fluctuations, indicating hallucination occurrences are rare. The results demonstrate that our FPE-LLM predictions are stable and reliable.\n$\\begin{aligned}\\bar{x} & =\\frac{\\sum_{i=1}^{n} x_{i}}{n} \\\\S E M & =\\frac{\\sigma}{\\sqrt{n}} \\\\C I & =(\\bar{x}-\\epsilon \\cdot S E M, \\bar{x}+\\epsilon \\cdot S E M) \\\\P(\\mu \\epsilon[\\bar{x}-\\epsilon_{0.025} \\cdot S E M, \\bar{x}-\\epsilon_{0.025} \\cdot S E M]) & =0.95.\\end{aligned}$\nBased on the ANOVA analysis, we obtain the confidence interval based on formula.14. SEM is the sample standard"}]}