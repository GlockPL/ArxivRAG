{"title": "LLM-based Online Prediction of Time-varying Graph Signals", "authors": ["Dayu Qin", "Yi Yan", "Ercan Engin Kuruoglu"], "abstract": "In this paper, we propose a novel framework that leverages large language models (LLMs) for predicting missing values in time-varying graph signals by exploiting spatial and temporal smoothness. We leverage the power of LLM to achieve a message-passing scheme. For each missing node, its neighbors and previous estimates are fed into and processed by LLM to infer the missing observations. Tested on the task of the online prediction of wind-speed graph signals, our model outperforms online graph filtering algorithms in terms of accuracy, demonstrating the potential of LLMs in effectively addressing partially observed signals in graphs.", "sections": [{"title": "Introduction", "content": "The application of Large Language Models (LLMs) in graph data processing and graph learning tasks has increasingly garnered research attention. GraphLLM, a graph learning model that integrates with large language models (LLMs), significantly enhances the ability of LLMs to reason with graph data by boosting accuracy and reducing the context length required for graph reasoning tasks (Chai et al. 2023). The NLGraph benchmark is introduced to assess the graph reasoning abilities of language models, showing promising results on simple tasks but highlighting challenges with more complex problems, suggesting the need for further improvement (Wang et al. 2024).\nIn our work, we designed a framework that uses LLM to predict missing values in time-varying graph data by leveraging spatial and temporal smoothness. The LLM model aggregates signals from neighboring nodes and prior time steps, outperforming traditional methods in accuracy."}, {"title": "Methodology", "content": "The proposed methodology leverages the smoothness assumption of data on graph nodes in both spatial and temporal dimensions on a time-varying regression task of online prediction. For a graph G with N nodes, we assume that the time-varying signal (data value) on the graph nodes x[t] exhibits smoothness over time and space. This implies that for an observation o[t] containing missing node observations, the ground truth \u00e6[t] can potentially be inferred from the temporal trends or spatial proximity to other observed values. The Large Language Model (LLM) is trained to recognize and predict patterns that align with this smoothness assumption. In the context of graph-based formulation, a transductive setting naturally appears where observed data and unobserved data coexist, enabling the LLM to exploit the intrinsic structure of the graph for more accurate predictions on the unobserved nodes when the topological context is fed into LLM along with the data (Liu and Wu 2023).\nTo predict missing observations, the LLM is used as a message-passing mechanism realized by localized aggregation at each individual node:\n$$agg(x_v) = \\Omega(\\{(x_v[t], x_u[t]) \\mid u \\in N_v\\}),$$\nwhere for the node v in G, x_v[t] is the signal on v and it has N 1-hop neighboring nodes (including self-loop) denoted with the subscript u = 1... N. We divide the global node observations o[t] and the estimation [t] into localized representation, where for each node v, we record only its own signal and its one-hop neighboring signals to feed into the context and the prompt of LLM. At each time step t, the current partial observation or [t] of a node, along with historical data (either observed or reconstructed), is provided as the input to the LLM. The LLM serves as a localized aggregation where its task T(vn) is to interpret input from the neighbors of a missing node v and the previous estimate of \u00cen [t-1], then inferring the missing observation \u00een [t]. Since this aggregation is defined locally on each node, the aggregation process is fed iterative to LLM for all N nodes, which in turn is mapped back onto each missing node observation. The algorithmic procedure is shown in Algorithm 1.\nIn Algorithm 1, the process initiates by identifying any new observations at time t. For each missing node un in G, values from its own previous estimate and those of its neighbors in the current observations o[t] are collected. This collected data, including neighbor values from past estimations at t 1, informs the LLM task T(vn), which then predicts the current value [t] for the missing node. This prediction"}, {"title": "Experiment Results and Discussion", "content": "In our experiments, we employed time-varying hourly wind speed graph signals with N = 197 nodes and T = 95 time points to validate the proposed methodology from (Yan, Kuruoglu, and Altinkaya 2022). The data is uniformly set 30% of the nodes to be missing. The GPT-3.5-turbo model from the OpenAI API was utilized to predict these missing values based on the temporal and spatial patterns in the data. GPT-3.5-turbo is an advanced LLM, part of the GPT-3 family, that excels in understanding and generating human-like text. It can handle complex prompts and generate highly accurate predictions, making it suitable for tasks involving pattern recognition and missing data prediction.\nThe GPT-3.5-turbo model was provided with the observations from the previous node to predict the missing values for the same node at the next time point. By inputting only one time point at a time, we can achieve online prediction and prevent the LLM from peeking into future data. Additionally, in the prompt, we explicitly instruct it not to use chat memory. Each algorithm is repeated 5 times. Then we can calculate the MSE  = 15\u039d\u03a4 \u03a35\u039d\u03a4i=1 \u03a3T=1 (xi[t] - Xi[t])2 of the predicted value and evaluate the results. In the experiments, we used GLMS (D. Lorenzo et al. 2016) and G-Sign (Yan, Kuruoglu, and Altinkaya 2022) as baseline GSP algorithms for comparison. The result is shown as follows:\nBased on the MSE comparison between different algorithms, the results demonstrate that the GPT-3.5-turbo model\n\n\n\n\nsignificantly outperforms both the GLMS and G-Sign algorithms in handling missing data within graph structures. This may be attributed to the fact that adaptive filters are primarily designed for noise reduction and are less effective for pure noiseless spatiotemporal prediction tasks compared to powerful models like GPT, which suggests that the GPT model holds substantial potential in addressing challenges related to partially observed time-varying graph signals.\nLimitations There are rare occasions (occurred once during our 5 experiment runs) where the model outputs NaN values and the cause remains unclear. In such cases, a practical workaround is employed: the missing values are substituted with the average of the previous time points. Additionally, batch-feeding N nodes tasks into LLM often results in the number of returns not being N. While the GPT model demonstrates substantial potential, addressing these limitations through complementary strategies remains necessary to further improve its robustness in scenarios involving noisy, time-varying, and incomplete data."}]}