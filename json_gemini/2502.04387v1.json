{"title": "FedP2EFT: Federated Learning to Personalize Parameter Efficient Fine-Tuning for Multilingual LLMs", "authors": ["Royson Lee", "Minyoung Kim", "Fady Rezk", "Rui Li", "Stylianos I. Venieris", "Timothy Hospedales"], "abstract": "Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP2EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP2EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP2EFT largely outperforms existing personalized fine-tuning methods, while complementing a range of existing FL methods.", "sections": [{"title": "1. Introduction", "content": "Federated learning (FL) makes it possible to train multilingual large language models (LLMs) across different geographical regions, protecting linguistic diversity for low-resource languages (Zhao et al., 2024) while being compliant with privacy regulations (Lim et al., 2020), e.g., General Data Protection Regulation (GDPR). Despite the impressive capabilities demonstrated by these models across various languages, their performance can vary significantly depending on the specific language (Rust et al., 2021) and the data quantity (Adelani et al., 2021) on each client.\nMoreover, the majority of existing FL-based multilingual LLM approaches have thus far focused on training a single global model (Iacob et al., 2025; Weller et al., 2022; Ye et al., 2024), limiting their performance on specific languages. Concretely, scaling a single model to different languages is challenged by issues such as the Curse of Multilingual-ity (Conneau et al., 2020), where adding more languages often leads to diminishing returns, and Negative Interference (Wang et al., 2020b), where diverse languages compete for limited model capacity. From a personalized FL perspective, learning a global model often increases the initial performance at the expense of personalized performance, e.g., fine-tuning from the global model (Jiang et al., 2019).\nNaturally, personalized FL approaches can help bridge the gap and improve language personalization. However, existing techniques are either too costly to be applied to LLMs, e.g., the use of meta-learning (Chen et al., 2018) and hypernetworks (Shamsian et al., 2021), or rely on suboptimal hand-crafted personalization strategies, e.g., manual choice of personalized and language-specific layers (Iacob et al., 2025; Zhao et al., 2024; Wu et al., 2024; Qi et al., 2024), parameter-efficient fine-tuning (PEFT) adapter structures (Yang et al., 2024a;b), or clustering based on class labels and/or language commonality (Mansour et al., 2020;\nIntuitively, optimizing personalization in personalized FL often necessitates dataset- and task-specific methods. The optimal level of personalization varies significantly depending on the characteristics of the data and the specific FL scenario (Chen et al., 2022; Ye et al., 2024). For instance, an English-pretrained LLM may require stronger personalization, e.g., higher learning rates, when fine-tuning on German than on English. The optimal personalization strategy (PS) is thus contingent upon the specific task, the client, and the given base model (Lee et al., 2023).\nIn this paper, we address the issues above by learning to personalize PEFT for each client. To this end, we propose FedP2EFT, a method that enables clients to collaboratively learn language personalization strategies using FL. Specifically, we federatedly train a PS generator (PSG), as depicted in Fig. 1(b), which allows all participating clients to collaboratively learn the optimal mapping between local meta-data to optimal LoRA (Hu et al., 2022) ranks. Fig. 1(a) illustrates the personalization process of FedP2EFT on a single client during inference, where per-layer LoRA ranks are generated depending on the initial base model, the client's dataset, and their resource budget. These personalized LORA modules are then used to apply PEFT on the base model and yield the personalized model.\nAs FedP2EFT focuses on improving the PEFT process per-dataset/task/client, it is directly pluggable to any starting base model, which may or may not be federatedly trained. This includes off-the-shelf pretrained models, FL approaches that learn a single global model (McMahan et al., 2017; Oh et al., 2022; Karimireddy et al., 2020; Acar et al., 2021), and even personalized FL approaches that deploy personalized layers, e.g., monolingual tokenizer and embeddings (Iacob et al., 2025), personalized LoRA adapters (Qi et al., 2024; Wu et al., 2024; Yang et al., 2024b), and language embeddings (Silva et al., 2023). Through our experiments (Section 4), we show that our method 1) largely outperforms both existing non-FL LORA rank selection and FL-based learning-to-personalize techniques, and 2) complements well with a range of existing FL approaches."}, {"title": "2. Related Work", "content": "Multilingual LLMs (MLLMs). Existing efforts in multilingual LLMs often underperform on low-resource languages due to 1) data scarcity (Xu et al., 2024b), 2) the model's limited capacity to learn the intricacies of multiple languages (Conneau et al., 2020), and 3) negative transfer learning among languages (Wang et al., 2020b). Common ways to counteract these challenges include the use of separate vocabulary and embeddings (Artetxe et al., 2020), hand-crafted adapters (Pfeiffer et al., 2020), automatic data annotation (Dubey et al., 2024), clustering and merging languages with similar representations (Chung et al., 2020), among other contributions (Wang et al., 2020a; Conneau et al., 2019). Our work is orthogonal to these approaches and builds upon recent FL-based MLLMs (Zhao et al., 2024; Iacob et al., 2025; Ye et al., 2024), which utilize FL to tap into previously inaccessible low-resource data sources.\nPersonalized Federated Learning. To obtain personalized client-specific models, various approaches have been proposed, including the use of personalized layers (Arivazhagan et al., 2019), meta-learning (Chen et al., 2018), model mixtures (Marfoq et al., 2021), hypernetworks (Shamsian et al., 2021), transfer learning between global and local models (Shen et al., 2020), among other contributions (Deng et al., 2020). Some of these techniques have also been adopted for LLMs, e.g., personalized LoRAs (Yang et al., 2024b), hypernetworks for client embeddings (Silva et al., 2023), and mixtures of LoRA experts (Zhang et al., 2024b). Our work complements these approaches as personalized models can benefit from further fine-tuning as shown in Section 4.2.1.\nFederated Hyperparameter Optimization (HPO). Most federated approaches to HPO do not utilize the client dataset for personalized hyperparameters. Instead, they employ a single set of hyperparameters across all clients based on the local validation loss evaluated before FL (Zhou et al., 2023; Holly et al., 2022) or sample from federatedly learnt hyper-parameter categorical distributions for each client (Khodak et al., 2021). An exception to this is FedL2P (Lee et al., 2023) which utilizes a PSG for personalized per-layer learning rates and batch normalization hyperparameters. We compare with FedL2P in our experiments.\nPEFT Structure Learning. Contrary to the conventional approach of distributing uniform adapter modules across all layers, a recent line of work allows different LoRA ranks to be used across a model's weight matrices. Using fine-grained per-layer rank selection, existing methods include SVD-based LoRA reformulation followed by importance-based rank assignment (Zhang et al., 2023), trainable rank-gating units (Ding et al., 2023), selectively employing parallel weight modules (Song et al., 2024), meta-learning-based (Zhang et al., 2024a) and black-box optimization techniques (Tribes et al., 2024), specialized training recipes for multi-rank LoRA modules that allow flexible extraction of a range of ranks (Valipour et al., 2023), and coarse-grained dropping of LoRA-enhanced layers (Yao et al., 2024). While these methods can be effective in centralized setups, they typically require an excessive number of optimization steps, which is prone to overfitting in FL settings, where clients have limited amount of data."}, {"title": "3. Our Approach", "content": "3.1. Preliminaries & Motivation\nIn personalized FL, the goal is to minimize each client's local objective $\\mathbb{E}_{(x,y)\\sim P^{i}} \\mathcal{L}^{i}(\\Phi^{i}; x, y)$ where $P^{i}$ represents the data distribution of the i-th client, x and y are the input data and labels, respectively, and $\\mathcal{L}^{i}(\\Phi^{i}; x, y)$ is the loss function for client i given model parameters $\\Phi^{i}$. This is typically achieved via fine-tuning (Matsuda et al., 2022; Chen et al., 2022) a base model, with parameters $\\Phi_{BM}$ and a set of hyperparameters, e.g. learning rate. Note that $\\Phi_{BM}$ may differ across clients if it is already personalized, e.g. if $\\Phi_{BM}^{i}$ is obtained using a personalized FL algorithm.\nFine-tuning LLMs, however, is unprecedentedly compute and memory intensive, and prone to overfitting. As such, the majority of existing federated LLM works (Zhao et al., 2024; Sun et al., 2024) rely on PEFT methods, with LoRA (Hu et al., 2022) being a prevalent choice due to its efficiency and performance. Specifically, for a frozen weight matrix $W\\in \\mathbb{R}^{d\\times e}$, LORA introduces low-rank matrices $B \\in \\mathbb{R}^{d\\times r}$ and $A \\in \\mathbb{R}^{r\\times e}$ where $r < min(d, e)$. The adapted weights are then expressed as: $W' = W + \\alpha_{lora} BA$ where $\\alpha_{lora}$ is a hyperparameter and only B and A are trained during fine-tuning. Although effective, these FL works rely on a fixed hand-crafted PS, e.g., a manually defined LoRA rank on hand-picked layers, for all clients, leading to suboptimal personalized models.\n3.2. Personalized PEFT\nWe, instead, propose using a different PS for each client. Common hyperparameter choices from previous federated HPO approaches (Section 2) include learning rates and batch normalization (BN) hyperparameters. While these hyperparameters have been shown to be effective for handling data heterogeneity in popular vision and speech benchmarks (Li et al., 2021; 2016; Arivazhagan et al., 2019), they are less consequential or not applicable when fine-tuning LLMs. This stems from the fact that LLMs are often fine-tuned using adaptive optimizers, e.g. Adam, which are more robust to the learning rate (Zhao et al., 2025), and BN layers are not typically used. A more critical hyperparameter choice shown to be effective, especially for cross-lingual transfer learning (Pfeiffer et al., 2020), is the PEFT adapter structure; specifically which layers to introduce LoRAs in and what ranks to utilize (Zhang et al., 2023; 2024a).\nAdapting BayesTune for LoRA Rank Selection. Building upon BayesTune (Kim & Hospedales, 2023), a Bayesian sparse model selection approach, we formulate PEFT personalization as a sparse LoRA rank selection problem and propose BayesTune-LoRA. Concretely, we introduce rank-wise latent variables $\\lambda_{l} \\in \\mathbb{R}^{+}$, i > 0, $\\forall i = 1,2,...,r$ for each LoRA matrix: $B_{l}AA_{l}$. Let $ \\Lambda = {\\lambda_{1},...,\\lambda_{L}}$ be the\nset of all $\\Lambda$ where $\\lambda_{l}$ represents the rank-wise scales for layer l in a model with L LORA modules (similarly for A and B). Using BayesTune, the values for $\\theta = (\\Lambda, A, B)$ are optimized as:\n$\\theta^{*} = arg \\min_{\\theta}  \\mathcal{L}_{CE}(\\theta; \\mathcal{D}) + \\mathcal{L}_{s}(\\Lambda, B) + \\mathcal{L}_{p}(\\lambda)$  (1)\nwhere $\\mathcal{D}=\\{(x_{i}, y_{i})\\}_{i=1}^{N}$ is the train dataset, N the size of $\\mathcal{D}$, $\\mathcal{L}_{CE}(\\theta; \\mathcal{D})$ the cross-entropy loss, $\\alpha_{p}$ and $\\alpha_{s}$ hyperparameters, $\\mathcal{L}_{s}$ the logarithm of the Laplace distribution (prior imposed on $p(B|A)^{1}$), $f(||B_{l,i}||_{1}; \\mu, b) = \\frac{1}{2b} exp{-\\frac{||B_{l,i}||_{1}-\\mu}{b}}$ with $\\mu = 0$ (B is initialized to 0 in LoRA) and $b = \\lambda_{l,i}$:\n$\\mathcal{L}_{s}(\\Lambda, B) = \\sum_{l=1}^{L}\\sum_{i=1}^{r} (log\\lambda_{l,i} + \\frac{||B_{l,i}||_{1}}{\\lambda_{l,i}} + log2)$ (2)\nand $\\mathcal{L}_{p}$ is the logarithm of the Gamma distribution (hyper-prior imposed on $\\lambda$),  $G(\\lambda_{l,i}; \\alpha_{g}, \\beta_{g}) = \\frac{\\beta_{g}^{\\alpha_{g}} \\lambda^{\\alpha_{g}-1} e^{-\\beta_{g} \\lambda_{l,i}}}{\\Gamma(\\alpha_{g})}$, where $\\alpha_{g} = 0.01, \\beta_{g} = 100$ following the hyperparameters set by the original authors:\n$\\mathcal{L}_{p}(\\Lambda) = \\sum_{l=1}^{L}\\sum_{i=1}^{r} (0.99\\cdot log \\lambda_{l,i} + 100 \\cdot \\lambda_{l,i} -0.01 \\cdot log (100) + log \\Gamma(0.01))$  (3)\nIn practice, we can save computations by removing all constants and the duplicate term log $\\lambda$, resulting in the following approximated penalty losses:\n$\\mathcal{L}_{s}(\\Lambda, B) = \\sum_{l=1}^{L}\\sum_{i=1}^{r} \\frac{||B_{l,i}||_{1}}{\\lambda_{l,i}}$  (4)\n$\\mathcal{L}_{p}(\\Lambda) = \\sum_{l=1}^{L}\\sum_{i=1}^{r} (log \\lambda_{l,i} + 100 \\cdot \\lambda_{l,i})$  (5)\nRoughly speaking, $\\mathcal{L}_{p}$ encourages small $\\lambda$ while $\\mathcal{L}_{s}$ encourages larger $\\lambda$ for larger LORA B (per column) updates. Hence, minimizing the losses in Eq. (1) encourages larger $\\lambda$ in more significant ranks.\nPersonalizing PEFT with BayesTune-LoRA. For each client, we attach BayesTune-LoRA modules, $\\theta$, to all linear layers of its base model with rank $r_{init} = \\alpha_{r-mul} r_{max\\_target}$ where $r_{max \\_target}$ is the maximum inference resource budget and $r_{init}$ is the initial rank before pruning. $\\theta$ is then optimized using Adam (Kingma & Ba, 2015) as per Eq. (1).\nAfter training, we freeze the resulting $\\Lambda$ and use it for per-\n3.3. FedP2EFT: FL to Personalize PEFT\nThe limited data available to each client in FL makes it difficult to train an effective PS in isolation, frequently resulting in overfitting. Following FedL2P (Lee et al., 2023), we mitigate this by federatedly learning a common PSG that generates client-wise PS. Concretely, we use a small, one hidden layer multilayer perceptron (MLP) with parameters $\\phi$ that takes as input the client meta-data and outputs an estimated PS as follows:\n$\\hat{\\Lambda} = MLP(\\phi; E(h_{0}), SD(h_{0}), ..., E(h_{L-1}), SD(h_{L-1}))$  (6)\nwhere $h_{l-1}$ is the input feature to the l-th layer in the base model, and E(\u00b7) and SD(\u00b7) are the mean and standard deviation (SD), respectively.\nIn contrast to FedL2P, which adopts a computationally demanding meta-learning approach to train MLP, we take a two-stage strategy for each client: 1) first, learn $\\Lambda$, followed by 2) regression learning of MLP to target the learned $\\Lambda$.\nFederated Training of FedP2EFT. Fig. 2 shows the entire FedP2EFT algorithm during federated training. For each federated round, each sampled participating client i receives $\\phi$ from the server and loads them into its MLP. They then\u25cf perform a forward pass of the local train dataset on their base model and a forward pass of the MLP with the resulting features as per Eq. (6). \u25cf The estimated $\\hat{\\Lambda}^{i}$ is plugged into our proposed BayesTune-LoRA (Section 3.2) and fine-tuning is performed as per Eq. (1) for s steps (Stage 1). The resulting $\\Lambda^{i, s}$ is used as an approximated ground-truth for regression learning of MLP to target the learned $\\hat{\\Lambda}^{i, s}$, where $\\mathcal{L}_{1}$ is the $\\mathcal{L}_{1}$ loss (Stage 2). Finally, $\\phi$ is sent back to the server for aggregation. As there is no single aggregation method that outperforms all others in every situation (Matsuda et al., 2022; Chen et al., 2022; Ye et al., 2024), we utilize FedAvg (McMahan et al., 2017). The aggregated $\\phi$ is then sent to clients for the next round.\nAt the end of federated training, the learned $\\phi$ can be deployed to any client, seen or unseen during federated training. Note that unlike FedL2P, which requires federated training for every target rank, FedP2EFT inherits the property of BayesTune-LoRA; federated training is a one-time cost for all ranks $\\leq r_{max target}$."}, {"title": "4. Evaluation", "content": "4.1. Experimental Setup\nWe conduct experiments on multilingual scenarios, where clients with diverse high- and low-resource languages can collaboratively learn how to personalize a given base model to better cater to their language preferences. In all experiments, we divide clients in two pools, seen and unseen, where only the clients in the seen pool actively participate in federated training. We set the maximum number of communication rounds for training the PSG to 150, randomly sampling 10% of participating clients every round. We use Adam (Kingma & Ba, 2015) as the default optimizer for all our experiments. We evaluate on resource budgets r = 2, 4, 8, 16 where the total rank budget is r \u00b7 L. We summarize the FL scenarios that we consider in our experiments, leaving comprehensive details in Appendix A.\n4.1.1. TASKS, MODELS, AND DATASETS\nText Classification. We adopt the pretrained multilingual BERT (Devlin et al., 2018) (mBERT) for all text classification experiments. For datasets, we introduce additional data heterogeneity to the simulated FL setups, XNLI (Conneau et al., 2018) and MasakhaNEWS (Adelani et al., 2023), proposed in PE_FL (Zhao et al., 2024).\n4.1.2. COMPLEMENTARY APPROACHES\nWe show FedP2EFT's compatibility with both off-the-shelf models and models trained using existing FL methods. Concretely, given a pretrained model, we obtain a base model using one of the following approaches:\nStandard FL. We further train the pretrained model federatedly on the seen pool, either using existing PEFT methods or full fine-tuning (Ye et al., 2024; Sun et al., 2024).\nPersonalized FL. We adopt two recent personalized FL works: i) FedDPA-T (Yang et al., 2024b), which learns\n4.1.3. BASELINES\nGiven a base model, we compare FedP2EFT with existing fine-tuning and learning to personalize approaches.\nLORA PEFT. We deploy LoRA (Hu et al., 2022) on all linear layers of the model with a fixed rank r.\nNon-FL Rank Selection. We compare with AdaLoRA (Zhang et al., 2023) and our proposed LoRA-variant of BayesTune (Kim & Hospedales, 2023), BayesTune-LoRA (Section 3.2), which optimizes A separately for each client.\nFL to Personalize. We compare with FedL2P (Lee et al., 2023) which trains a MLP federatedly to output per-client learning rates for each LoRA module.\nFor each baseline, we either follow best practices recommended by the corresponding authors or employ a simple grid search and pick the best performing hyperparameters. Full details in Appendix. A.\n4.2. Results on Text Classification\nWe evaluate our approach in a typical FL setup, where the pretrained model is first trained using Standard FL with full fine-tuning and the resulting base model is then personalized to each client. Tables 1 & 2 show the mean and standard deviation (SD) of the accuracy for each language in our MasakhaNEWS setup for seen and unseen pool respectively (similarly for XNLI in Appendix Tables 8 & 9). In addition, we also show the number of languages, labelled \u201cWins\u201d, an approach is best performing for each budget r.\nThe results in all four tables show that federated learning to personalize methods (FedL2P and FedP2EFT) outperform the other baselines in most cases. Non-FL rank selection approaches (AdaLoRA and BayesTune-LoRA), on the other hand, tend to overfit and/or struggle to learn an optimal rank structure given the limited number of samples in each client. Comparing FedL2P and FedP2EFT, FedP2EFT largely surpass FedL2P with a few exceptions, indicating that learning to personalize LoRA rank structure is the better hyperparameter choice than personalizing learning rates; this finding is also aligned with recent LLM-based optimizer findings (Zhao et al., 2025), which shows that Adam's performance is robust with respect to its learning rate.\n4.2.1. FEDP2EFT'S COMPLEMENTABILITY WITH PERSONALIZED FL WORKS.\nApart from Standard FL, we show that FedP2EFT can be plugged into existing personalized FL works that trains both a subset of the pretrained model and personalized layers for each client. Tables 3 and 4 show that FedP2EFT outperforms baselines in almost all cases in our XNLI setup given a base model trained using FedDPA-T (Yang et al., 2024b) and DEPT(SPEC) (Iacob et al., 2025) respectively. In short, FedP2EFT can be integrated into a larger family of existing personalized FL approaches, listed in Section 2, to further improve personalization performance.\n4.3. Results on Instruction-Tuning Generation\nWe evaluate our approach on the more challenging real-world multilingual benchmark, Fed-Aya. Tables 6 and 7 show the average METEOR (Banerjee & Lavie, 2005)/ROUGE-1/ROUGE-L (Lin, 2004) of each language given the off-the-shelf instruction finetuned Llama-3.2-3B (Llama-3.2-3B-Instruct) for seen and unseen clients respectively. Similarly, in Appendix Tables 10 and 11, we show the same tables given a pretrained MobileLLaMA-1.4B model trained using Standard FL with LoRA following the training recipe from FedLLM-Bench (Ye et al., 2024). These two models represent scenarios where the base model may or may not be trained using FL. Similarly to our text classification results, we report \"Wins\" if an approach has a better performance in at least 2 out of 3 metrics.\nIn all four tables, FedP2EFT outperforms baselines in most scenarios. We also observe that FedL2P underperforms standard baselines in most cases, a phenomenon also observed for our XNLI setup when the base model is trained with FedDPA-T (Tables 3). We hypothesize that the inner-loop optimization in FedL2P fail to reach a stationary point due to the inherent task difficulty (Fed-Aya) or a less-performant base model, resulting in a sub-optimal hypergradient and downstream performance.\n4.3.1. LIMITATIONS OF FEDP2EFT.\nIn some cases, FedP2EFT falls short, especially in the recall performance (ROUGE), such as Russian (ru) and French (fr) for unseen clients for both base models. These cases highlight a couple of limitations of our approach: i) ru is not seen by PSG during federated training; there are no ru samples in any clients in the seen pool and ii) none of the clients in the unseen pool have fr as a predominant language (Fig. 3). In the case of ru, there are no other languages that are similar to ru in the seen pool, resulting in worse performance. Hence, we do not expect a similar outcome in datasets with a more diverse pool of clients.\nFor fr, as the number of predominant language samples are orders of magnitude higher than that of fr samples, the generated $\\Lambda$ are catered towards the predominant language. A simple solution to counteract this limitation is to extend\n4.4. Cost of FedP2EFT\nTable 5 shows the mean latency, in seconds, and the peak memory usage across 100 runs on the first client in the seen pool for r = 16 using a single Nvidia A100 GPU. Non-FL baselines do not incur a federated training cost while FL approaches requires training the PSG. Comparing FedL2P and FedP2EFT, FedP2EFT does not require expensive second-order optimization, resulting in better efficiency. We also note that FedL2P needs to be run for every rank while FedP2EFT runs once for all targeted ranks.\nFor communication costs, not shown in the table, FedP2EFT is more costly as it predicts per LoRA rank while FedL2P predicts per layer. Nonetheless, these costs are negligible compared to running FL on the base model; FedL2P uses 0.02% and 0.002% and FedP2EFT uses 0.2% and 0.16% of the parameters of mBERT and Llama-3.2-3B respectively.\nDuring inference, FL-based approaches incur an additional forward pass of base model and the PSG compared to non-FL approaches. Memory-wise, FedP2EFT results in the smallest memory footprint for autoregressive generation as the PSG learns not to attach LoRA modules $\\lambda_{l} = 0$ on some layers, skipping matmul operations entirely.\n4.5. Further Analysis\nIn this section, we further analyze $\\Lambda$ and how they differ across languages. Surprisingly, we find that FedP2EFT learns language-agnostic rank structures. In other words, depending on the task and the base model, the rank structure of $\\Lambda$ is fixed across languages. For instance, in the case where r = 2, FedP2EFT allocate ranks to dense layers instead of attention blocks. With more budget, e.g., r = 16, FedP2EFT allocates more rank to either the query attention layer or the value attention layer depending on the setup. We show these rank structures across all setups for r = 2 and r = 16 in Appendix Fig. 6-15.\nWhile the rank structure is the same across languages, the rank-wise scales (absolute values of $\\Lambda$) differ. Following FedL2P, we visualize the difference in $\\Lambda$ for different languages using the normalized mean distance, d(j, k), between all clients pairs holding data for languages j and k."}, {"title": "5. Conclusion", "content": "In this work, we tackle language personalization through FedP2EFT, a federated learning method that learns how to perform PEFT on heterogeneous data. We show that our proposed federated learning-to-personalize approach is easily pluggable to off-the-shelf LLMs and standard and personalized FL methods alike, surpassing other personalized fine-tuning baselines in most cases. Our results show that FedP2EFT automatically learns model- and task-specific language-agnostic LoRA rank structures as well as effective cross-lingual transfers, where both diverse low- and high-resource languages can share similar LoRA rank magnitudes. Despite clear advantages, our approach falls short in personalizing for each client's minority languages, as the personalized solution is skewed towards their predominant language. Nonetheless, our work is a significant step towards successfully merging the benefits of multilingual learning and personalized FL."}, {"title": "A. Detailed Experimental Setup", "content": "For reproducibility and completeness, we provide comprehensive details of all setups, datasets, tasks, models, baselines, and hyperparameters. Code is in the process of being released.\nA.1. Tasks, Datasets, and Data Partitioning\nXNLI (Conneau et al., 2018) A natural language inference benchmark dataset for evaluating cross-lingual understanding covering 15 diverse languages including both high- and low-resources languages: English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu. XNLI consists of premise-hypothesis pairs, labeled as entailment, contradiction, or neutral across different languages. We sample 2k instances from the XNLI train split and 500 instances from the test split for each pool. The data is then divided equally among 20 clients for each language using the latent Dirichlet allocation (LDA) partition with $\\alpha = 0.5$. Hence, the total number of clients is 600 (15 languages \u00b7 20 clients per language \u00b7 2 pools).\nMasakhaNEWS (Adelani et al., 2023) A news topic classification benchmark designed to address the lack of resources for African languages. It covers 2 high-resource languages, English and French, and 14 low-resource languages, namely Amharic, Hausa, Igbo, Lingala, Luganda, Naija, Oromo, Rundi, chiShona, Somali, Kiswahili, Tigrinya, isiXhosa, and Yor\u00f9b\u00e1. Each sample contains a headline, the body text, and one of the 7 labels: business, entertainment, health, politics, religion, sports, or technology. We first combine all samples from the MasakhaNEWS train and validation split to form our train set, and use the MasakhaNEWS test split as our test set. We then split both train and test in each of the 16 languages by half for each pool. Following our XNLI setup, we adopt LDA $\\alpha = 0.5$ and divide each language's data equally into 10 clients. Hence, the total number of clients is 320 (16 languages \u00b7 10 clients per language \u00b7 2 pools). Note that unlike XNLI, the number of samples for each language differs, hence there is quantity skew across clients.\nFed-Aya (Ye et al., 2024) A federated instruction tuning benchmark, based on Aya (Singh et al., 2024), where the data is annotated by contributors and partitioned by annotator ID. Following FedLLM-Bench (Ye et al., 2024), we focus on 6 high-resource languages, English, Spanish, French, Russian, Portuguese, Chinese, and 2 low-resource languages, Arabic and Telugu. Additionally, we filter out the other languages from the dataset. Out of 38 clients, we select 8 for our unseen pool, client_ids = [21, 22, 23, 24, 25, 26, 27, 34] and the rest goes into our seen pool. Each client can have up to 4 languages where the number of data samples can range from a hundred to over a"}, {"title": "A.2. Models, Tokenizers, and Data Preprocessing", "content": "mBERT (Devlin et al.", "prompt": "nalpaca_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\n{"}, "n### Response: {}{}\"\"\"\nLlama-3.2-3B (Dubey et al., 2024). We use the off-the-"]}