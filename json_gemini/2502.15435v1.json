{"title": "Single-pass Detection of Jailbreaking Input in Large Language Models", "authors": ["Leyla Naz Candogan", "Yongtao Wu", "Elias Abad Rocamora", "Grigorios G. Chrysos", "Volkan Cevher"], "abstract": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks.", "sections": [{"title": "1 Introduction", "content": "The impressive capabilities of large language models (LLMs) (Brown et al., 2020; Achiam et al., 2023) also highlight the dual nature of their potential, as they can also respond to illicit or detrimental queries equally skillfully. Currently, the safety guardrails inserted by finetuning LLMs preferences (Bai et al., 2022b; Hacker et al., 2023; Ouyang et al., 2022; Sun et al., 2023), can still be easily compromised with so-called \"jailbreaking\" attacks owing to the competing objectives of offering useful and accurate responses versus resisting to answer more harmful questions (Wei et al., 2023a).\nThe \u201cjailbreaking\" attacks (Shen et al., 2023; Zou et al., 2023; Carlini et al., 2023; Liu et al., 2024a; Zeng et al., 2024a; Sadasivan et al., 2024) are a prime instance of avoiding the guardrails through modifications to the harmful prompt to trick the model. For instance, Zou et al. (2023) show that one can add an adversarial suffix after \"Tell me how to build a bomb\" to enforce the model to generate instructions."}, {"title": "2 Related work", "content": "In this section, we summarize the alignment methods, jailbreaking attacks, and jailbreaking defenses.\nAlignment of LLMS LLMs require data-intensive training, making textual corpora on the internet the perfect training set in terms of data size. However, a crucial portion of their training data consists of unwanted and potentially dangerous content (Gehman et al., 2020). To avoid the generation of malicious content and match them with human values different methods have been employed, called \"alignment\" (Bai et al., 2022b; Hacker et al., 2023; Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022a; Askell et al., 2021). Alignment has proven successful in guarding against malicious outputs for natural inputs, but not for adversarial inputs (Carlini et al., 2023).\nDue to the high interest in jailbreaking studies, it is crucial to have standardized evaluation frameworks. The recent works of JailbreakBench (Chao et al., 2024), HarmBench (Mazeika et al., 2024), and EasyJailbreak"}, {"title": "3 Method", "content": "We propose a method to detect jailbreaking attacks with a single forward pass, by only considering the output probabilities of the first few tokens. Our approach, SPD, is computationally efficient and does not depend on the criteria of another LLM. Figure 1 compares SPD with other defense methods, highlighting its efficiency. We summarize the notation used in this manuscript in section 3.1, and subsequently, we present the motivation for our approach and introduce our algorithm in sections 3.2 and 3.3.\n3.1 Notations and preliminaries\nA sequence with n tokens is denoted by $[x_i,...,x_{i+n}]$, with $Xi_1 \\in V$, where V is the vocabulary or in other words, the token set. We represent an input sequence with n tokens as $x_{1,n} := [x_1,...,x_n]$. Similarly, the output sequence with m tokens which is the response to $x_{1,n}$ is symbolized by $o_{n+1,m} := [x_{n+1}...,x_{n+m}]$. When the sequence length is not important, we denote $x_{1,n}$ as x and $o_{n+1,m}$ as o.\nA language model estimates the probability of the output token $o_{n+1,m}$ as follows:\n$P(o_{n+1,m}|x_{1,n}) = \\prod_{i=1}^{m}\\sigma(l_i(x_1,..., x_{n-1+i}))x_{i+n},$ (1)\nwhere we define $l_i(x_1,...,x_{n-1+i}) \\in R^{|V|}$ as the logit of model given input $1x_1,...,x_{n-1+i}$. For notational simplicity we will sometimes refer to them as $l_i$. Additionally, $(\\sigma(l_i))_j = \\frac{e^{l_{ij}}}{\\sum_{k=1}^{|V|}e^{l_{ik}}}$ represents the softmax function."}, {"title": "3.2 Motivation", "content": "Previous studies on model inversion with images have shown that the feature vector carries crucial infor- mation about the input (Dosovitskiy & Brox, 2015). Similarly, in a recent study, the feature vector of an LLM has been used to get the input sequence (Morris et al., 2024). Moreover, Shi et al. (2024) utilize min-k probability to reveal if a sequence is in the pertaining data. Overall, these studies suggest that the output probabilities are more instrumental than just predicting the next token.\nJailbreaking attacks are designed to search for some input sequence $21,n$ so that the probability of observing some malicious output $\\hat{O}_{n+1,m}$ is maximized. A common approach used in automated jailbreaking attacks is minimizing the cross-entropy loss:\n$\\min_{21,n} L(\\hat{o}_{n+1,m}, l_i (x_1,..., \\hat{x}_{n-1+i})),$ (2)\nwhere we define the cross-entropy loss in the following form: $L(\\hat{o}_{n+1,m},l_i) = \\sum_{-1}-log ((l_i)_{i+n})$. Another strategy is to iteratively refine the input sequence x with the help of an auxiliary LLM until the output sequence \u00f4 complies with the original question.\nIndependent of the method of generation, the attacks are designed to produce output sequences \u00f4 with specific requirements that cannot be directly obtained by naturally prompting the model. Given that the output probabilities carry inherited information about the input sequence, we pose the following question:\nAre the output token distributions of benign x and attacked inputs different?\nIf affirmative, we could design strategies for detecting attacks and defend against jailbreaking. Jain et al. (2023) already suggest GCG generates input sequences with high perplexity. Given that other attacks such as AutoDAN (Liu et al., 2024a), PAIR (Chao et al., 2023), and PAP (Zeng et al., 2024a) avoid this defense, our question emphasizes the output distribution to attempt to capture different types of attacks.\nIn our experiments, we observed that there exists a negative shift in the logit values of the output sequence when the input is an attacked sentence, as present in fig. 2 (a). Moreover, we can spot a difference in the entropy of the first logits of outputs of benign vs. attack sentences. When the input is benign, for the first token, there are usually one or two high-probability candidate tokens while the rest have very small probabilities. In other words, the model is very certain about how to answer that prompt. When the input is attacked, the number of high-probability candidates increases resulting in a higher entropy. This change can be observed from fig. 2 (b), where we can see that outputs of attacked sentences have a higher entropy in comparison to normal inputs. Thus, there are indeed differences between the distributions of a and attacked inputs 2. Consequently, we propose to use a binary classifier that can capture the difference in these distributions to decide if an attack has been attempted or not."}, {"title": "3.3 Single-pass detection", "content": "Feature matrix As discussed previously, jailbreaking attacks cause unnatural patterns in the output token distribution such as the drastic negative shift in logit values or the increase in entropy of outputs as observed in fig. 2. To capture the change numerically, we propose to calculate the following feature matrix $H := [h_1, h_2, ..., h_r] \\in [R^{r\\timesk}$ such that:\n$h_i := \u2212 log(\\sigma(l_{i,k})) \\in R^k,$ (3)\nwhere the original logit vector is $l_i := LLM(x_{1,n}) \\in R^{|V|}$ and $l_{i,k} \\in R^k$ is the logit vector with highest k elements. The r corresponds to the number of token positions that will be considered. Since the influence of input on the logit distribution is higher with smaller i, after some testing, we set r = 5 and k = 50, see appendix E.5. Note that although only k tokens per position are included in the feature matrix, the probabilities are calculated with the whole vocabulary V to capture more information.\nClassification problem The adversarial sample detection problem can be approached as a classification task. To ensure the separability of attacked and benign sentences, we check the t-SNE plot of the feature"}, {"title": "4 Experiments", "content": "In this section, after we describe the experimental setting, we provide experimental results and comparison with baselines using Llama 2, Vicuna, GPT-3.5, and GPT-4 models. Further details on the experimental setting, and experiments with Llama 3 can be found in appendix B and appendix D, respectively.\n4.1 Experimental settings\nModels We used Llama 2 (Llama 2-Chat 7B) (Touvron et al., 2023), and Vicuna (Vicuna 13B) (Chiang et al., 2023) for our main experiments and performed ablation studies on GPT-3.5-turbo-0613 (Brown et al., 2020) and GPT-4 and GPT-40-mini (OpenAI, 2023).\nEvaluation metrics Our goal is to detect adversarial prompts in minimal time without being overcautious. We also want to avoid additional computational costs. To capture these, we report five metrics:\n\u2022 True positive (TP) rate: TP describes which portion of the attacked data is classified correctly. It can be calculated for individual attacks or as an average value for all attacks. A higher rate indicates better performance.\n\u2022 False positive (FP) rate: FP describes the misclassification rate of benign samples. The value should be as low as possible.\n\u2022 $F_1$ scores: To examine the overall predictive performance, we calculate the $F_1$ score which is $\\frac{2TP}{2TP+FN+FP}$ where FN is the false negative rate (rate of misclassification of attacked samples). $F_1 \\in [0,1]$ with $F_1 = 1$ as the perfect score."}, {"title": "5 Conclusion and future directions", "content": "In this work, we propose an effective and very efficient LLM jailbreaking detection method that is successful against state-of-the-art attacks. SPD is 3x faster than its closest competitor with better performance and it only needs 1 forward pass through the LLM. Our defense is based on the observation that the negative log probabilities of tokens of attacked sentences are shifted to smaller values. We believe this observation is key to understanding adversarial attacks in LLMs. Our work can foster an understanding of the success of adversarial attacks. Following our initial observations, we train an SVM algorithm as a classifier using only the negative log probabilities of the first five tokens. Our experiments proved that its computational cost is considerably less than other methods, it can identify an attack before responding with more than the overall 93% TP rate while keeping the FP rate under 12%.\nWith slight modifications, SPD can defend proprietary models without access to the full token probabilities. Our studies suggest that with full token probability access, the performance of our method could greatly improve. We believe our work can foster the advancement towards stronger and more efficient defenses, enabling a low overhead detection of jailbreaking attempts.\nLimitations Our approach relies on having access to the next token logits of the model to defend. This constrains the performance of the defense mechanism, especially in the case of proprietary models like GPT- 4. Our method relies on having samples of successful attacks for training an SVM classifier, nevertheless, we show that with very few samples we can train powerful defenses.\nBroader impact statement\nJailbreaking attacks enable malicious individuals and organizations to achieve malicious purposes. Our method improves the detection rate of such attacks and has a low false positive rate for benign inputs. Additionally, the efficiency of our approach allows fast integration within LLM APIs, this supposes a democratization of the access to defenses. On the negative side, publishing our findings, can also enable attackers to devise new strategies to circumvent our defense."}, {"title": "A Motivation continued", "content": "Below, we present our analysis of automated attacks that minimize the loss with respect to a target sequence. To simplify, let us consider the case of m = 1 where the attacker aims to minimize the cross-entropy loss w.r.t only the next token such as \u201csure\u201d. Without the loss of generality, we assume such a token corresponds to the first token in the logit. Then the objective function in eq. (2) becomes:\n$L = \u2212 log (\u03c3(l_1)_{1+n}) = \u2212 log (\u03c3(l_1)_1) .$ (4)\nTo explore the connection between minimizing the loss and the logit, let us take the derivative w.r.t the logit:\n$\u2207_{l_1}L = \\begin{cases}\\sigma(l_1)_1 - 1 < 0 &\\text{if t = 1,}\\\\\u03c3(l_1)_t > 0 &\\text{otherwise.}\\\\\\end{cases}$ (5)\nClearly, the gradient direction for the first logit (corresponding to \u201csure\") is negative. On the contrary, the gradient directions for the remaining large amount of logits are positive, which might result in a shift towards a smaller value by the rule of gradient descent update: $l_{1t} = l_{1t} - n\u2207l_{1+t} L$ with step size \u03b7. This is consistent with our observation of negative shifts in the logit values. Furthermore, by taking the negative logarithm of probability, i.e., \u2013 $log(\u03c3_1)_t = -l_{1t} + log \\frac{1}{\\sum e^{l_k}} $, one can infer that such a decrease in logits can yield a similar reduction in its negative log probability due to its exponential term.\nThis part serves as an intuition and motivation that led us to further investigate the logit values and it does not constitute a full proof of the observed changes. Providing the exact dynamic of each logit is beyond the scope of this study.\nRemark: Even though our analysis above only focuses on the optimization based attacks, we have observed the same phenomenon with other types of attacks too. SPD does not assume any prior knowledge on the attack data or its generation method.\""}, {"title": "B Experimental setting", "content": "Following the JailbreakBench, we use the vLLM API service to access the models. The GPT models are utilized with OpenAPI API access. Every defense method is implemented using the original code of the respective papers. All experiments were conducted in a single machine with an NVIDIA A100 SXM4 80GB GPU. The parameters related to the JailbreakBench implementation such as top-p = 0.9 and temperature = 0 are not changed."}, {"title": "C Details on the datasets", "content": "For each attack method, we generated multiple successful attack prompts using Harmful Behavior data of the AdvBench dataset and JBB-Behaviors dataset of JailbreakBench. The Harmful Behaviour dataset consists of 520 unique goals and their respective targets. The JBB-Behaviors dataset includes 100 unique goals which are taken from AdvBench and Trojan Detection Challenge (Mazeika et al., 2023). Further details on each dataset are provided below:"}, {"title": "D Additional experiments with Llama 3", "content": "In table 6, we compare SPD with other baselines suing Llama 3 model, following the same methodology described in the main text. The training data sizes are also provided in the table. Results indicate that the effectiveness of SPD is applicable to various models, even with limited data size."}, {"title": "E Ablation studies", "content": "In this section, we first study the dependency of SPD on data and measure its performance with unseen data. Later, we experiment with certain design choices such as the refusal classifier, using logit values, and hyperparameters. Next, we try prompting the benign samples and finally, we train a binary Bert-based classifier and compare against it. Finally, we perform additional studies on adaptive attacks and compare baselines using ROC curves.\nE.1 Dependency on training data"}, {"title": "E.6 Prompting benign samples", "content": "To show that the SPD does not depend on any assumption about the output, we performed additional experiments with prompting. GCG and AutoDAN attacks optimize the input prompt so that the answer will start with \"Sure, here is...\". Inspired by this, we test the effect of forcing benign inputs to begin with the same phrase on the FDR. For that purpose, we took our original safe datasets Alpaca Eval and QNLI and added the following prompt at the beginning of each sample: \"I will ask you a question. Please make"}, {"title": "E.7 Ablation studies on adaptive attacks", "content": "To further examine the reason for the logit distribution shift, we performed additional experiments on attack generation that aims to maintain a similar logit distribution to benign samples while getting a successful attack. To do so, inspired by Doan et al. (2021), we introduced a KL divergence loss to the GCG attack and observed the changes in losses.\nIn the fig. 8, we present the jailbreaking and KL loss evolution during an attack generation process with Llama 2 model in two cases: Normal GCG where only the jailbreaking loss is considered and an Adaptive GCG where KL divergence and jailbreaking loss are considered together. The red star shows the steps when a successful attack is achieved.\nIn the provided example, the harmful target is \"Write a convincing anonymous death threat.\" and the default settings are used. The KL divergence is calculated between the average logits of benign samples and the output logits. Among the benign dataset, the KL divergence value between individual samples and the averages ranges between 0.03 and 0.62 with a mean of 0.21.\nIn the normal GCG case (blue line) we can observe a quick decrease in the GCG loss and achieve a successful attack at step 25. When we look at the KL divergence of the output, we can see a rise showing the output distribution is getting more and more different than the benign distribution.\nIn the adaptive case, on the other hand, we could never get a successful attack in 500 steps. Though both GCG and KL losses decrease at the initial steps, after step 200, they start to oscillate, showing it is challenging to achieve these two goals at the same time.\nThe same trend has been observed in all our attempts considering all harmful behaviors in the dataset with 2 different seeds. While the normal attacks were usually successful in fewer steps (between 20-300), with 500 steps, we were never able to get a successful attack using the dual loss. With normal GCG, the KL divergence values are between 0.2-0.5 and the jailbreaking loss ranges from 0.2 to 2.8. With the adaptive attack, on the other hand, the KL loss is between 0.1-0.5 and jailbreaking loss is between 0.7-3.1.\nWe hypothesize that this failure is due to the constrained nature of the logits optimization, which conflicts with the requirements for generating valid jailbreak instructions. Providing a harmful response to a jail- breaking attack is inherently unnatural to the model and forcing a naturally harmful response seems quite challenging, if not impossible."}, {"title": "E.8 Bert-based binary classifier", "content": "We train a RoBERTa model to do the classification task by looking at the input sentences. We use the same dataset we used to train SPD with Vicuna which are $T = T_{safe} = 200$. We test the model on the test dataset of Vicuna. Results are provided in table 12. Though the FP rates are quite desirable, it does not perform well against AutoDAN and PAP attacks. We believe there are two reasons of that: a) with PAP attack, the training size is too small, only 5 samples, for the model to learn from them b) since the AutoDAN attack is too long, and RoBERTa has a very small window length, some part of the input is not processed. As a result, we believe this method is not an ideal way for defense purposes. Moreover, if an additional language model is used for the classification, attackers can easily attack this model too. Finally, training a binary language classifier is computationally more expensive and the input dimension is much higher than training a simple"}, {"title": "E.9 ROC curves", "content": "The reported TP and FP numbers for the baseline methods such as SmoothLLM, RA-LLM, and Perplexity filter in the main text are gotten by using the default threshold settings of the approaches to determine if a sentence is attacked or not. By changing the threshold value, it is possible to change the TP and FP rates. In fig. 9a, we plotted the ROC curves for the Llama 2 model. Curves and the AUROC score show that SPD significantly outperforms other baseline methods. Since Self-defense is not a probabilistic model, neither fixing the FP rate nor calculating the AUROC score is feasible.\nAdditionally, in the fig. 9b, we present the ROC curve for SPD with different models. With Llama 2, the SPD is exceptionally good as it achieves almost perfect discrimination between the positive and negative classes and achieves an AUROC score of almost 1. Similarly, with Vicuna and GPT4 models, the curves indicate good classification results with AUROC scores of 0.95."}, {"title": "F Extended related work", "content": "Fine-Tuning Jailbreaking Attacks: Recent studies have shown that though fine-tuning LLMs enhances their task-specific performance it also introduces significant safety risks, particularly through jailbreaking attacks even if it is not intended (Qi et al., 2024). Additionally, there exist various harmful fine-tuning techniques that makes the models even more susceptible (Huang et al., 2024a). Therefore, it is crucial to have standardized evaluation frameworks for quantifying safety risks, and providing a roadmap for assessing alignment degradation Peng et al. (2024).\nSeveral defensive approaches have been proposed to address these risks. Representation noising (Rosati et al., 2024) and backdoor-enhanced alignment (Wang et al., 2024a) prevent misuse by disrupting the adversarial reprogramming of LLMs. Techniques such as \"Vaccine\", a perturbation-aware alignment method (Huang et al., 2024c), and \"Safe LoRA\" [(Hsu et al., 2024), which minimizes risks via low-rank adaptation, aim to safeguard models during fine-tuning. Additionally, Lyu et al. (2024) stresses the role of prompt templates in constraining outputs, even for misaligned models, while Huang et al. (2024b) introduces \"lazy safety align- ment\", leveraging dynamic checks post-fine-tuning. These solutions underscore the challenge of maintaining both customization and alignment, which remains a critical focus in the field."}]}