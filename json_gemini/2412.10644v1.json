{"title": "Model-Driven Deep Neural Network for Enhancing Direction Finding with Commodity 5G gNodeB", "authors": ["SHENGHENG LIU", "ZIHUAN MAO", "XINGKANG LI", "MENGGUAN PAN", "PENG LIU", "YONGMING HUANG", "XIAOHU YOU"], "abstract": "Pervasive and high-accuracy positioning has become increasingly important as a fundamental enabler for intelligent connected devices in mobile networks. Nevertheless, current wireless networks heavily rely on pure model-driven techniques to achieve positioning functionality, often succumbing to performance deterioration due to hardware impairments in practical scenarios. Here we reformulate the direction finding or angle-of-arrival (AoA) estimation problem as an image recovery task of the spatial spectrum and propose a new model-driven deep neural network (MoD-DNN) framework. The proposed MoD-DNN scheme comprises three modules: a multi-task autoencoder-based beamformer, a coarray spectrum generation module, and a model-driven deep learning-based spatial spectrum reconstruction module. Our technique enables automatic calibration of angular-dependent phase error thereby enhancing the resilience of direction-finding precision against realistic system non-idealities. We validate the proposed scheme both using numerical simulations and field tests. The results show that the proposed MoD-DNN framework enables effective spectrum calibration and accurate AoA estimation. To the best of our knowledge, this study marks the first successful demonstration of hybrid data-and-model-driven direction finding utilizing readily available commodity 5G gNodeB.", "sections": [{"title": "1 INTRODUCTION", "content": "High-accuracy positioning holds promise in supporting a diverse range of services within the forthcoming-generation of wireless communications [1]. The accurate position aids mobile devices in optimizing resource allocation and network configuration, thereby reducing communication latency and energy consumption [2]. Additionally, location awareness assumes a critical stance in bolstering security and privacy protection, facilitating identity authentication and safeguarding sensitive data in the sphere of cyber-physical systems. The advent of high-accuracy positioning services will transcend the conventional boundaries of mobile user lifestyles and usher in transformative prospects across diverse niche markets [3]. While a variety of techniques are available to accomplish this goal, including the global positioning system (GPS) [4], ultra wide-band (UWB) [5], and vision-inertial fusion [6], each manifests distinct advantages and disadvantages. Nevertheless, none singularly achieves comprehensive coverage across both indoor and outdoor scenarios without the need for supplementary device installation. Recent developments have witnessed a notable surge in the deployment of fifth-generation (5G) base stations (a.k.a. gNodeBs or gNBs) in both indoor and outdoor environments to meet the demands for service quality and extensive coverage [7], with their numbers projected to continue growing in the foreseeable future. This presents a viable opportunity to realize high-precision positioning by leveraging the 5G infrastructure itself, thereby obviating the necessity for additional hardware [8]. Consequently, the integration of high data-rate communication and high accuracy positioning assumes an unprecedentedly heightened significance [9].\nThe positioning methods has been extensively explored and can be broadly categorized into two main groups: fingerprint-based methods and geometric-based methods. The fingerprint-based methods capitalizes on the uniqueness of the wireless channel that interconnects users and access point (AP), which is shaped by the intricate scattering environments [10]. However, the dynamic nature of the propagation medium introduces a discrepancy between the receive signal strength indicator (RSSI) and the outdated fingerprint database, thereby compromising the robustness of fingerprint-based methods [11]. To overcome this issue, various strategies have been proposed that involve fusing derivative fingerprints to extract the resilient component of RSSI and enhance the accuracy of positioning [12]. However, the computational complexity of these methods grows exponentially with the expansion of the database, thus posing a formidable challenge in terms of the requisite computation resources [13]. In contrast to the fingerprint-based methods, geometric-based methods impose minimal requirements concerning environmental dynamics [14]. The 3rd Generation Partnership Project (3GPP) Release 16 [15], which reached its finalization in 2021, introduced several geometric-based schemes tailored for 5G. These encompass downlink-time-difference-of-arrival (DL-TDoA), uplink-time-difference-of-arrival (UL-TDoA), downlink-angle-of-departure (DL-AoD), uplink-angle-of-arrival (UL-AoA), and multi-round trip time (Multi-RTT) positioning techniques. Direction finding is a frequently visited problem in the realm of multichannel signal processing. When assuming an idealized mathematical model with perfect system response,"}, {"title": "2 PRELIMINARY", "content": "We investigate the task of estimating the bearing of mobile terminals using commodity gNodeB by measuring the AoA of emitted electromagnetic signals as they impinge upon the receiving array. Our study considers multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) air interface, as it is the dominant system configuration for 5G broadband wireless communications. We first quantify the field of view (FoV) using a uniform discrete direction set denoted as $\\theta = [\\theta_1, ..., \\theta_l, ..., \\theta_L]^T$, and the angular interval is represented by $\\Delta\\theta = \\theta_l - \\theta_{l-1}$."}, {"title": "2.1 Direction Finding Based on Ideal Model", "content": "Suppose that the gNodeB employs an M-element uniform linear array with a unit inter-element spacing denoted as d. Then, the output of the m-th sensor and the k-th subcarrier is formulated in an over-complete form as\n$\\begin{equation}\nX_{m,k}(t) = \\sum_{l=1}^{L} S_{l,k}(t) \\exp \\{-j2\\pi[(f_0+(m-1) \\Delta f)((m-1)d \\sin \\theta_l)+(k-1) \\Delta f r_l]/c\\},\\tag{1}\n\\end{equation}$\nwhere c is the speed of light and $s_{l,k}(t)$ represents the complex waveform of the uplink-sounding reference signal (UL-SRS) impinging upon the system from an AoA $\\theta_l$ and a range $r_l$, during a specific time period denoted as t = 1, . . ., T. It merits mention that the SRS takes the form of an OFDM signal, distinguished by a pre-determined Zadoff-Chu sequence uniquely designated for each individual terminal [36]. Given the narrow-band system assumption, we have $\\Delta f \\ll f_0$ and, thus, the noisy receive signal can be further expressed as\n$\\begin{equation}\nX_{m,k}(t) = \\sum_{l=1}^{L} s_{l,k} (t)e^{-j2\\pi f_0 (m-1)d \\sin \\theta_l/c} + n_{m,k} (t),\\tag{2}\n\\end{equation}$\nwhere $n_{m,k}(t)$ denotes the additive white Gaussian noise (AWGN). Stacking $x_{m,k}(t)$ for all m =\n1, . . ., M, the receive signal vector of the antenna array for the k-th subcarrier is\n$\\begin{equation}\nx_k (t) = \\sum_{l=1}^{L} \\mathbf{a}(\\theta_l, d)s_{l,k} (t) + \\mathbf{n}(t) = \\mathbf{A}(\\boldsymbol{\\theta}, d)\\mathbf{s}_k (t) + \\mathbf{n}_k(t),\n\\tag{3}\n\\end{equation}$\nwhere $\\mathbf{a}(\\theta_l, d) = [1, ..., e^{j2\\pi f_0 (M-1)d \\sin \\theta_l/c}]^T$ is the steering vector. We let $\\mathbf{A}(\\boldsymbol{\\theta}, d)= [\\mathbf{a}(\\theta_1, d), . . .,\\mathbf{a}(\\theta_L, d)]$ denote the array manifold corresponding to the over-complete direction set $\\boldsymbol{\\theta}$, which is exactly determined by the AoA of the user and the ideal array configuration. By futher employing the fast Fourier transform (FFT) technique, we are able to obtain the channel frequency response (CFR), or the frequency snapshot, of the k-th subcarrier as\n$\\begin{equation}\n\\mathbf{h}(k) = \\mathbf{A}(\\boldsymbol{\\theta}, d)\\mathbf{s}(k) + \\mathbf{n}(k), k = 1, ..., K.\n\\tag{4}\n\\end{equation}$\nHere $\\mathbf{s}(k)$ and $\\mathbf{\\tilde{n}}(k)$ respectively represents the signal and noise in frequency domain. The sample covariance matrix of the received signal is calculated as $R = \\sum_k \\mathbf{h}(k)\\mathbf{h}^H(k)$, which will be used as input for the estimators. Popular model-driven estimators include digital beamforming (DBF) and multiple signal classification (MUSIC) algorithms, which estimate the AoA respectively as\n$\\begin{equation}\n\\hat{\\theta}_{DBF} = \\arg \\max_\\theta \\mathbf{a}^H(\\theta)\\mathbf{R}\\mathbf{a}(\\theta) \\quad \\text{and} \\quad \\hat{\\theta}_{MUSIC} = \\arg \\max_\\theta \\frac{1}{\\mathbf{a}^H(\\theta)\\mathbf{U}_N\\mathbf{U}^H_N\\mathbf{a}(\\theta)}\n\\tag{5}\n\\end{equation}$\nThe noise subspace $U_N$ used in MUSIC is constructed using eigenvectors associated with the least eigenvalues of the matrix R."}, {"title": "2.2 Angular-Dependent Phase Errors", "content": "Nevertheless, real-world systems inevitably face hardware impairments, leading to a non-ideal response of the antenna array. As a result, the estimation performance of conventional methods suffers degradation when deployed in practical scenarios. Several types of array error are identified to capture the impact of hardware impairments, primarily including element position errors, gain and phase inconsistencies, and mutual coupling. The cumulative influence of these errors is holistically modelled as\n$\\begin{equation}\n\\mathbf{h} = \\mathbf{C} \\mathbf{F} \\mathbf{A}(\\boldsymbol{\\theta}, d + \\Delta d)\\mathbf{s} + \\mathbf{n},\n\\tag{6}\n\\end{equation}$\nwhere $\\Delta d$ represents the element position error, and the mutual coupling coefficients between the array elements are captured by a Toeplitz matrix denoted as C. Additionally, the gain and phase inconsistencies are incorporated by a matrix $\\mathbf{F}=\\text{diag}\\{\\gamma_1,. . ., \\gamma_L \\}$, with $\\gamma_l$ being the inconsistency coefficient of the l-th RF channel. Under the specific signal model pertaining to hardware impairments, tailored algorithms are developed with the aim of automatically calibrating the phase error"}, {"title": "3 NETWORK FRAMEWORK AND PROBLEM FORMULATION", "content": "Purely model-driven estimators usually struggle to address the challenges posed by a multitude of real-world nonlinear phase errors. In this Article, we propose the model-driven deep neural network (MoD-DNN), a new data-and-model-driven framework for AoA estimation using commodity 5G gNodeB while accounting for the presence of hardware impairments. The proposed framework, as visualized in Fig. 2, comprises three cascade-connected modules: (1) a multi-task autoencoder-based beamformer, (2) a coarray spectrum generation module and (3) a model-driven deep learning-based spatial spectrum reconstruction module.\nFirst, we design a frequency diverse multi-task autoencoder for spatial beamforming. Through encoding and multi-task decoding, the input signal is filtered into different adjacent subregions, thereby enhancing the consistency of hardware impairments within each subregion. In the coarray spectrum generation module, we transform the covariance matrix of the output into a vectorized representation and reformulate the signal into a coarray format. Subsequently, the digital beamforming (DBF) technique is applied to obtain the image of the coarray spatial spectrum. This transformation of data from a manifold to an image enables the modeling of the problem as an inverse problem [37], where the objective is to map the spatial spectrum to the sparse receive signal. In order to calibrate the impact of hardware impairments, the resulting spectrum is fed into a convolutional neural network (CNN). By incorporating the sparse constraint into the conjugate gradient algorithm, the spatial spectrum is reconstructed using the proposed sparse conjugate gradient (SCG) algorithm, which provides closed-form solutions. Finally, an iterative optimization process between the CNN and SCG is proposed, resulting in the integration of the model-driven deep learning-based spatial spectrum reconstruction (MoDL-SSR) module, which effectively enhances the performance of spectrum calibration and AoA estimation.\nIn the subsequent subsections, we provide a comprehensive elucidation of each module within the proposed framework."}, {"title": "3.1 Multi-Task Autoencoder-Based Beamformer", "content": "As is widely recognized, data consistency holds paramount significance for the efficacy of deep learning methodologies. In the context of the AoA estimation addressed in this study, consistency can be understood as the underlying pattern exhibited by phase errors across various AoAs and frequency points. Drawing upon the measurements conducted within the anechoic chamber, as depicted in Extended Data Figs. 2b-d, the phase error curves distinctly exhibit continuity and regularity within a confined spatial subregion. This empirical evidence reveals that the hardware impairments exhibit similar characteristics across different AoA values within a limited angular range. In light of this finding, we first devise a multi-task autoencoder as beamformers to effectively filter the received signal, aiming to enhance its consistency within the aforementioned small subregion. Compared to the MUSIC-based decision mechanism [38] which is susceptible to hardware impairment at the boundary of subregions, this approach ensures that the subsequent module receive accurate inputs, which directly impacts the ability. Additionally, as we identify the inconsistency of array imperfections across various frequency points, a frequency diverse autoencoder is further designed for different subcarrriers. The structure of the proposed multi-task autoencoder is illustrated in Fig. 2a. The autoencoder first compresses the received signal into a lower-dimensional representation, allowing for the extraction of principal components, followed by the reconstruction of the encoded signal back to its original dimension within P adjacent subregions.\nIn the network architecture sketched in Fig. 2, we consider a single-layer encoder and decoder configuration. In this setup, the k-th snapshot of the CFR-form received signal $\\mathbf{h}(k)$ first undergoes compression using the fully connected encoder, resulting in\n$\\begin{equation}\n\\mathbf{c}(k) = f_\\chi [\\mathbf{C}\\mathbf{h}(k) + b_k],\n\\tag{8}\n\\end{equation}$\nwhere $\\mathbf{C}_k \\in \\mathbb{R}^{2M\\times|c|}$ denotes the feedforward computations performed by the encoder for the k-th snapshot. $\\mathbf{h}(k) \\in \\mathbb{R}^{2M}$ is the real-value form of the received signal by stacking the real and complex parts of $\\mathbf{h}(k)$. The additive bias vector for the encoding operation is denoted as $b_k$, and $f_\\chi [\\cdot]$ represents the elementwise activation function applied to the encoding layer. Following the"}, {"title": "3.2 Coarray Spectrum and Inverse Problem Formulation", "content": "In the preceding subsection, we obtained the received signal H. However, directly extracting features from such representative manifold-form signals, often characterized by high dimensions and intricate topological structures, can be very challenging. In contrast, the spatial spectrum, presented in the form of an image, holds the capability to decipher the attributes of hardware impairments from diverse facets such as beamwidth, kurtosis, and skewness. Hence, by converting the received signal into spatial spectrum, we gain the advantage of tapping into the inherent structure and underlying correlations intrinsic to H, thereby providing a more revealing insight into the signal features.\nConsidering the discrete AoA set $\\boldsymbol{\\theta}$, the covariance matrix of the received signal can be represented as\n$\\begin{equation}\n\\mathbf{R} = E \\big[\\mathbf{h}(k)\\mathbf{h}^H(k)\\big] = \\sum_{l=1}^{L} \\eta_l\\mathbf{a}(\\theta_l)\\mathbf{a}^H(\\theta_l) + \\sigma_n^2 \\mathbf{I},\n\\tag{11}\n\\end{equation}$\nwhere $\\eta_l$ signifies the signal power for direction $\\theta_l$, $\\sigma_n^2$ represents the power of the AWGN, and $\\mathbf{I}$ denotes the identity matrix. We further rewrite the m-th column of $\\mathbf{R}$ as\n$\\begin{equation}\n\\mathbf{r}_m = \\sum_{l=1}^{L} \\eta_l\\mathbf{a}(\\theta_l) \\big(\\mathbf{a}^H(\\theta_l)\\mathbf{e}_m\\big) + \\sigma_n^2 \\mathbf{e}_m = \\mathbf{A}_m \\boldsymbol{\\eta} + \\sigma_n^2 \\mathbf{e}_m,\n\\tag{12}\n\\end{equation}$\nwhere $\\mathbf{A}_m = [\\mathbf{a}(\\theta_1)\\big(\\mathbf{a}^H(\\theta_1)\\mathbf{e}_m\\big),...,\\mathbf{a}(\\theta_L)\\big(\\mathbf{a}^H(\\theta_L)\\mathbf{e}_m\\big)]$, $\\boldsymbol{\\eta} = [\\eta_1, ..., \\eta_L]^T$ can be interpreted as the sparse spatial spectrum. The vector $\\mathbf{e}_m \\in \\mathbb{C}^M$ is constructed such that its m-th element is 1 while all other elements are zero. Vectorizing the covariance matrix by stacking all the columns of $\\mathbf{R}$, we obtain the corresponding coarray signal\n$\\begin{equation}\n\\mathbf{y} = [\\mathbf{r}_1; ... ;\\mathbf{r}_M] = [\\mathbf{A}_1; ... ; \\mathbf{A}_M]\\boldsymbol{\\eta} + \\sigma_n^2 [\\mathbf{e}_1; ... ; \\mathbf{e}_M] = \\mathbf{\\tilde{A}}\\boldsymbol{\\eta} + \\sigma_n^2 \\mathbf{\\tilde{e}}\n\\tag{13}\n\\end{equation}"}, {"title": "4 MODL-SSR-BASED SPECTRUM CALIBRATION AND AOA ESTIMATION", "content": "This section is dedicated to the construction of a specialized neural network aimed at rectify the impact of angular-dependent phase errors within H. To handle the image-form spectrum generated in the previous subsection, we implement and train a CNN tailored for each subregion to effectively calibrate the phase errors present in the coarray signal. However, obtaining accurate spatial spectrums as the ground truth for network training presents a notable labeling complexity. To address this challenge, a modified SCG algorithm is devised to reconstruct the sparse spatial spectrum. One-hot vectors are then employed as surrogate labels. In addition, an iterative feedback"}, {"title": "4.1 Alternative Optimization Formulation", "content": "To estimate the AoA from the observed coarray signal affected by hardware impairments, we propose a model-driven deep neural network that incorporates iterations between deep learning and signal processing methods. The overall iteration is depicted in Fig. 3a. We start by recasting the reconstruction of the spatial spectrum as the following optimization problem\n$\\begin{equation}\n\\boldsymbol{\\eta} = \\arg \\min_{\\boldsymbol{\\eta}} \\|\\mathbf{P}\\boldsymbol{\\eta} - \\mathbf{\\hat{\\eta}}\\|^2 + \\lambda\\|\\mathcal{E}_w(\\boldsymbol{\\eta}) \\|^2,\n\\tag{15}\n\\end{equation}$\nwhere $\\mathcal{E}_w(\\boldsymbol{\\eta})$ represents a regularization term that accounts for the combined effects of noise and hardware impairments, and it can be learned using a w-weighted CNN. The regularization term can be rewritten as\n$\\begin{equation}\n\\mathcal{E}_w(\\boldsymbol{\\eta}) = (\\mathbf{I} - C_w)(\\boldsymbol{\\eta}) = \\boldsymbol{\\eta} - C_w(\\boldsymbol{\\eta}),\n\\tag{16}\n\\end{equation}$\nwhere $C_w(\\boldsymbol{\\eta})$ corresponds to a calibrated version of $\\boldsymbol{\\eta}$ after mitigating the impact of hardware impairments and noise. By substituting (16) into (15), we obtain the optimization problem in the i-th iteration:\n$\\begin{equation}\n\\boldsymbol{\\eta}^{i+1} = \\arg \\min_{\\boldsymbol{\\eta}} \\|\\mathbf{P}\\boldsymbol{\\eta} - \\mathbf{\\hat{\\eta}}\\|^2 + \\lambda\\|\\boldsymbol{\\eta} - C_w^i(\\boldsymbol{\\\u03b7}) \\|^2.\n\\tag{17}\n\\end{equation}$\nThe calibration term $C_w(\\boldsymbol{\\\u03b7}^i + \\Delta \\boldsymbol{\\\u03b7})$ can be further approximated by employing the Taylor series expansion, i.e.,\n$\\begin{equation}\nC_w(\\boldsymbol{\\\u03b7}^i + \\Delta \\boldsymbol{\\\u03b7}) = C_w^i(\\boldsymbol{\\\u03b7}^i) + \\mathbf{J}\\Delta \\boldsymbol{\\\u03b7},\n\\tag{18}\n\\end{equation}$\nwhere J is the Jacobian matrix. Upon introducing the updated variable $\\boldsymbol{\\\u03b7} = \\boldsymbol{\\\u03b7}^i+\\Delta \\boldsymbol{\\\u03b7}$, the regularization term is further expressed as\n$\\begin{equation}\n\\|\\boldsymbol{\\\u03b7} - C_w^i(\\boldsymbol{\\\u03b7}^i + \\Delta \\boldsymbol{\\\u03b7}) \\|^2 \\overset{\\Delta \\boldsymbol{\\\u03b7} \\to 0}{\\approx} \\|\\boldsymbol{\\eta} - C_w^i(\\boldsymbol{\\\u03b7}^i) \\|^2 + \\lambda\\|\\mathbf{J}\\Delta \\boldsymbol{\\\u03b7}\\|^2.\n\\tag{19}\n\\end{equation}$\nThen, the optimization problem (17) can be transformed into the following equivalent:\n$\\begin{align}\n\\mathbf{z}^i &= C_w^i(\\boldsymbol{\\\u03b7}^i),\\tag{20a}\\\\\n\\boldsymbol{\\\u03b7}^{i+1} &= \\arg \\min_{\\boldsymbol{\\\u03b7}} \\|\\mathbf{P}\\boldsymbol{\\eta} - \\mathbf{\\hat{\\\u03b7}}\\|^2 + \\lambda\\|\\boldsymbol{\\\u03b7} - \\mathbf{z}^i \\|^2.\\tag{20b}\n\\end{align}$\nThe iterative process is presented in Fig. 3b. By examining the alternative optimization in (20), we highlight the following remarks."}, {"title": "4.2 SCG-Based SSR Layer", "content": "Next, we elaborate the SCG algorithm used in the SSR layer. The goal is to reconstruct the spatial spectrum $\\boldsymbol{\\\u03b7}$ by solving the sub-problem (20b) through the use of the following normal equation\n$\\begin{equation}\n(\\mathbf{P} + \\lambda \\mathbf{I})\\boldsymbol{\\\u03b7} = \\mathbf{\\hat{\\eta}} + \\lambda \\mathbf{z}^i.\n\\tag{21}\n\\end{equation}$\nThe CG algorithm is commonly employed in forward models of this type to efficiently tackle such problems. In the following, we first provide a brief overview of the CG algorithm. Specifically, in each iteration of the CG algorithm, a P-conjugate direction $\\mathbf{c}(n)$ is selected from the negative of the gradient vector $\\mathbf{g}(n)$. The solution is then updated along the direction $\\mathbf{c}(n)$ with a step size denoted as $\\alpha(n)$. Concretely, the CG steps are given as follows\n$\\begin{align}\n\\alpha(n) &= \\frac{\\mathbf{g}^T(n)\\mathbf{c}(n)}{\\mathbf{c}^T (n)\\mathbf{P}\\mathbf{c}(n)},\\tag{22a}\\\\\n\\boldsymbol{\\\u03b7}(n + 1) &= \\boldsymbol{\\\u03b7}(n) + \\alpha(n)\\mathbf{c}(n),\\tag{22b}\\\\\n\\beta(n) &= \\frac{\\big(\\mathbf{g}(n + 1) - \\mathbf{g}(n)\\big)^T\\mathbf{g}(n + 1)}{\\mathbf{g}^T(n)\\mathbf{g}(n)},\\tag{22c}\\\\\n\\mathbf{c}(n + 1) &= -\\mathbf{g}(n + 1) + \\beta(n)\\mathbf{c}(n).\\tag{22d}\n\\end{align}$\nWe readily see that all steps of the CG algorithm have closed-form solutions, which removes the need for additional parameter training. Moreover, the gradients can be backpropagated from the CG sub-blocks during the iterations, enabling end-to-end training. By leveraging the CG algorithm, the MoDL-SSR modules feature sub-blocks that include a numerical optimization layer with a low computational cost.\nWhile the CG algorithm efficiently solves the problem, we must acknowledge that the original form of the normal equation is given by $\\mathbf{y} = \\mathbf{\\tilde{A}}\\boldsymbol{\\eta}$, where $\\mathbf{\\tilde{A}}$ represents an over-complete coarray manifold associated with the discrete AoA set $\\boldsymbol{\\theta}$. This implies that the inverse problem is inherently underdetermined, leading to a wide beam-width for the solution when using the CG algorithm alone. In this case, the utilization of one-hot vectors as training labels remains impractical. To counteract this, we leverage the inherent sparsity of the solution to narrow the beam-width. To this end, we introduce a sparsity-constrained CG algorithm tailored for spectrum reconstruction. In order to fully utilize the sparsity of the incident signal, we introduce an additional regularization term to the optimization problem (20b) in the form of a sparsity function $s(\\boldsymbol{\\\u03b7})$ as\n$\\begin{equation}\n\\boldsymbol{\\\u03b7}^{i+1} = \\arg \\min_{\\boldsymbol{\\\u03b7}} \\|\\mathbf{P}\\boldsymbol{\\\u03b7} - \\mathbf{\\hat{\\\u03b7}}\\|^2 + \\lambda\\|\\boldsymbol{\\\u03b7} - \\mathbf{z}^i \\|^2 + \\mu s(\\boldsymbol{\\\u03b7}),\n\\tag{23}\n\\end{equation}$\nwhere $\\mu$ denotes the regularization coefficient to enforce the sparsity constraint. The objective is then to minimize this problem through an iterative method that alternates between CG solutions and sparsity modification. The first two terms in (23) are minimized using CG method to obtain an intermediate solution\n$\\begin{equation}\n\\mathbf{\\tilde{\\eta}}(n + 1) = \\boldsymbol{\\\u03b7}(n) + \\alpha(n)\\mathbf{c}(n).\n\\tag{24}\n\\end{equation}$\nThen, the solution $\\mathbf{\\tilde{\\eta}}(n + 1)$ is modified using proximal gradient search as\n$\\begin{equation}\n\\boldsymbol{\\\u03b7} = \\arg \\min_{\\boldsymbol{\\\u03b7}} \\|\\boldsymbol{\\\u03b7} - \\mathbf{\\tilde{\\\u03b7}}(n + 1)\\|^2 + \\mu s(\\boldsymbol{\\\u03b7}).\n\\tag{25}\n\\end{equation}"}, {"title": "4.3 Training of CNN-based Calibrator", "content": "In the previous subsection, we introduced the SCG algorithm, capable of recovering the sparse AoA solution from an ideal spatial spectrum. The final and crucial phase in achieving a closed-loop process involves the design of a neural network to calibrate the spatial spectrum with phase errors. In this subsection, we present the architecture of the CNN used as the spectrum calibrator in (20a). As shown in Fig. 3a, the spatial spectrum $\\boldsymbol{\\\u03b7}^i$ is passed to the CNN input layer. The CNN consists of P layers, each layer comprising a convolution operation followed by batch normalization and a rectified linear unit (ReLU) activation function. The final layer does not employ ReLU activation, allowing the network to learn the negative part of the array imperfection patterns. In this network framework, a four-layer CNN structure is employed, with each dimension being 32 \u00d7 1, and the"}, {"title": "5 PERFORMANCE EVALUATION VIA SIMULATIONS", "content": "We evaluate the performance of the proposed MoD-DNN method for AoA estimation in the presence of hardware impairments. The CSI data used in the numerical studies is generated using a state-of-the-art link-level wireless communications simulator [39]. The simulator is capable of accommodating customized hardware impairment functions and can replicate channels aligned with the LoS-only scenario as stipulated in the 3GPP TR 38.901 standard [40]. Our focus centers on a typical four-antenna gNodeB configuration, which mirrors the most common setup in current commercial 5G deployments in indoor environment. The system and waveform parameters used in the simulations are detailed in Table 1. We consistently employ the indoor factory LoS channel at a Sub-6 GHz working frequency (denoted as 3GPP_38.901_InF_LoS) across all simulations.\nTo mitigate storage constraints, we uniformly sample 16 subcarriers from the CSI dataset. Illustrations of the angular-dependent phase error for this configuration can be found in Fig. 1b, while the comprehensive data for each subcarrier is accessible at [41]. The AoA of the user equipment (UE) varies within the range of [\u221260\u00b0, 60\u00b0], with a uniform interval $\\Delta\\theta = 0.1\u00b0$. Each AoA value is associated with 60 time slots, resulting in a total of 1201 \u00d7 50 = 60050 groups of CSI data for analysis. During the network training using PyTorch, we employ the Adam optimization scheme to iteratively update network parameters, which allows the computation of the weight gradients using backpropagation [42]. The learning rate and mini-batch size are set as 0.01 and 64, respectively. Subsequently, the learning rate undergoes reduction by a factor of 0.5 every 5 epochs. Empirical choices for the regularization coefficient $\\lambda$ and the weight coefficient $\\epsilon$ are 0.1 and 0.5, respectively."}, {"title": "5.1 Performance of Multi-task Autoencoder", "content": "In the first set of tests, we evaluate the beamforming performance achieved by the proposed frequency diverse multi-task autoencoder. As a comparison, we also include a multi-task autoencoder that directly processes the original CSI across all frequency points, disregarding the phase differences between subcarriers. For convenience, we refer to these as Autoencoder I and Autoencoder II, respectively. We collect 1201 \u00d7 40 = 48040 groups of simulated CSI data for training, and an additional 1201 \u00d7 10 = 12010 groups for validation, with a fixed input signal-to-noise ratio (SNR) of 10 dB. To substantiate the effectiveness of the proposed spatial filtering method while conserving computational resources, the spatial scope of [\u221260\u00b0, 60\u00b0] is evenly divided into P = 4 subregions, i.e., [\u221260\u00b0, -30\u00b0), [-30\u00b0, 0\u00b0), [0\u00b0, 30\u00b0) and [30\u00b0, 60\u00b0]. The detailed autoencoder settings can be found in TABLE 2. Note that, the number of the subregions can be conveniently adjusted according to different requirements. The spatial responses of the multi-task autoencoder are provided in Figs. 4a-d. The amplitude and phase responses of the multi-task autoencoder-based beamformers are defined as\n$\\begin{equation}\nr_{amp} = \\frac{|\\mathbf{h}^H(k)\\mathbf{h}_p(k)|}{\\|\\mathbf{h}(k) \\|^2\\|\\mathbf{h}_p(k) \\|^2} \\quad \\text{and} \\quad r_{phase} = |\\frac{\\mathbf{h}^H(k)\\mathbf{h}_p(k)}{\\mathbf{h}^H(k)\\mathbf{h}_p(k)}|.\n\\tag{33}\n\\end{equation}$\nFigs. 4a and c show the amplitude responses of Autoencoder I and Autoencoder II, respectively. It is evident that both Autoencoder I and Autoencoder II demonstrate a high degree of agreement in their amplitude responses with the anticipated output across the segmented subregions. Furthermore, their responses exhibit rapid attenuation at the subregion boundaries. In terms of phase response, as depicted in Figs. 4b and d, it becomes apparent that Autoencoder I shows a clearer inconsistency with the input beyond the designated subregion, which indicates a performance superiority compared to Autoencoder II. This superiority can be attributed primarily to the frequency diverse scheme, which"}, {"title": "5.2 Performance of Spectrum Calibrator", "content": "In this subsection, we first present the spectrums generated by coarray DBF and SCG algorithm, which are respectively the input and output of MoDL-SSR module. For a UE AoA of -15\u00b0, Fig. 1c indicates that the azimuth of UE can be directly estimated from the coarray spectrum. However, the accuracy deteriorates evidently in the presence of hardware impairment. Then, we assess the effectiveness of the proposed SCG algorithm. As demonstrated in Fig. 1c, the application of the sparsity constraint leads to a narrower spectrum width. Nonetheless, the obtained result is not entirely sparse due to the mismatch between the real and ideal coarray manifolds. Similar outcomes are observed in Fig. 1c, when the AoA of the UE is \u221245\u00b0, 15\u00b0 and 45\u00b0. This reaffirms the capability of the SCG algorithm to achieve reasonably accurate results despite the presence of hardware"}, {"title": "5.3 Performance of AoA estimator", "content": "To enable a rigorous quantitative assessment and comparative analysis of estimation performance among various methods, we present a comprehensive statistical evaluation of the proposed MoD-DNN algorithm. In particular, we benchmark its performance against the MUSIC algorithm and the DeepMUSIC algorithm [31]. Furthermore, we extend our inquiry to include a purely data-driven CNN [43]. Our prior investigations have explored different data-driven backbones [38, 43] for similar tasks. However, in the context of this research, CNN is selected as a representative method for a more intuitive comparative analysis. Drawing parallels to the proposed MoD-DNN, it is noteworthy that the DeepMUSIC algorithm also produces a spatial spectrum, whereas CNN directly yields AoA estimates.\nOur investigation begins by examining the root-mean-square-error (RMSE) a function of the SNR. As shown in Fig. 4f, both the MUSIC and DeepMUSIC methods exhibit relatively consistent performance levels despite variations in SNR. The saturated floors of the RMSE performance exhibited by MUSIC and DeepMUSIC can be attributed to their reliance on the ideal array response model. Due to the lack of specific calibration for angular-dependent errors, both MUSIC and DeepMUSIC experience performance degradation resulting from basis mismatch. In contrast, both the CNN and the proposed MoD-DNN demonstrate decreasing RMSEs as the SNR increases, with MoD-DNN consistently outperforming the CNN. This notable superiority stems from the effective"}, {"title": "5.4 Computational Complexity", "content": "As the final part of our simulation evaluation", "modules": "the multi-task autoencoder module, the spatial spectrum generation module, and the MoDL-SSR module. For the first module, the multi-task autoencoder, the computational complexity"}]}