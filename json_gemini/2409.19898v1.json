{"title": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMS", "authors": ["Yuho Lee", "Taewon Yun", "Jason Cai", "Hang Su", "Hwanjun Song"], "abstract": "Existing benchmarks for summarization quality evaluation often lack diverse input scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and struggle with subjective and coarse-grained annotation schemes. To address these shortcomings, we create UNISUMEVAL benchmark, which extends the range of input context (e.g., domain, length) and provides fine-grained, multi-dimensional annotations. We use AI assistance in data creation, identifying potentially hallucinogenic input texts, and also helping human annotators reduce the difficulty of fine-grained annotation tasks. With UNISUMEVAL, we benchmark nine latest language models as summarizers, offering insights into their performance across varying input contexts and evaluation dimensions. Furthermore, we conduct a thorough comparison of SOTA automated summary evaluators. Our benchmark data will be available at https://github.com/DISL-Lab/UniSumEval-v1.0.", "sections": [{"title": "1 Introduction", "content": "Despite the enhanced quality of text summarization by large language models (LLMs), they still face persistent challenges like hallucination, information omission, and verbosity (Fabbri et al., 2022; Laban et al., 2023). This multifaceted nature of text summaries inevitably demands manual evaluation by human experts, a labor-intensive and costly process. To streamline this evaluation process, recent efforts aim to design human-like automatic evaluators, such as G-Eval (Liu et al., 2023a) and FineSurE (Song et al., 2024), which achieve a satisfactory correlation with human judgments.\nSuch evaluators are typically validated by examining their consistency with human judgments on established benchmark datasets, such as FRANK (Pagnoni et al., 2021) and TofuEval (Tang et al., 2024b). Yet, these benchmark datasets have limitations in terms of input diversity, granularity of human annotations, and evaluation dimensions.\nFirstly, most existing benchmarks are restricted solely to a single domain. The predominant focus is often on the news domain such as SummEval (Fabbri et al., 2021) and AggreFact (Tang et al., 2023). This deficiency constrains the accurate evaluation of automated evaluators by failing to capture diverse input contexts across various domains.\nSecondly, there is a lack of datasets that consider varying input types and lengths simultaneously. While these two factors have a significant impact on the summary quality, existing datasets are often limited to short, non-dialogue texts (Bhandari et al., 2020; Pagnoni et al., 2021; Laban et al., 2022). Without considering these factors, they cannot adequately assess distinct perspectives across different input types and lengths. This includes correctly attributing statements to speakers in dialogue, preventing false information in articles with personally identifiable information (PII) redacted, and pinpointing key information in long texts.\nThirdly, no comprehensive datasets exist for fine-grained, multi-dimensional summarization evaluation. Some benchmarks offer fine-grained annotations, such as fact verification at the sentence-level (Pagnoni et al., 2021; Laban et al., 2022; Zhu et al., 2023) and alignment at the key-fact\u00b9 level (Bhandari et al., 2020; Tang et al., 2024b), yet they suffer from either a limited evaluation dimension or coarse-grained human labels.\nIn this paper, we create UNISUMEVAL in Figure 1, the first one-size-fits-all benchmark for fine-grained, multi-dimensional evaluation of automated evaluators. It includes: Text Inputs encompassing nine distinct domains (e.g., news, report,"}, {"title": "2 Related Work", "content": "Evaluation Benchmarks. Existing benchmarks in summary quality evaluation have predominantly concentrated on assessing the performance of automated metrics in evaluating faithfulness (Bhandari et al., 2020; Pagnoni et al., 2021; Laban et al., 2022; Tang et al., 2023). These benchmarks have generally been limited to short and non-dialogue texts. This limitation has spurred the development of new benchmarks that specifically address either longer texts (Krishna et al., 2023) or dialogue-based texts (Gao and Wan, 2022; Zhu et al., 2023; Laban et al., 2023; Tang et al., 2024b). Additionally, another segment of benchmarks has expanded beyond the dimension of faithfulness to include relevance and coherence (Fabbri et al., 2021; Gao and Wan, 2022; Tang et al., 2024b), also enhancing the granularity of annotations (Zhu et al., 2023).\nRecently, more advanced benchmarks have been developed, utilizing the power of LLMs. SummEd-"}, {"title": "3 UNISUMEVAL Pipeline", "content": "Our data creation pipeline consists of four consecutive steps in the following sections. Table 1 contrasts UNISUMEVAL with existing benchmarks across various aspects, including input diversity, annotation schemes, and data generation. The statistics of UNISUMEVAL are provided in Appendix A."}, {"title": "3.1 Input Text Sourcing", "content": "We use nine source datasets to construct our benchmark dataset: Wikihow (lifestyle) (Koupaee and Wang, 2018), CNN/DM (news) (Nallapati et al., 2016), GovReport (report) (Huang et al., 2021), PubMed (medical literature) (Cohan et al., 2018), SQUALITY (science fiction) (Wang et al., 2022), MultiWOZ (booking conversation) (Zang et al., 2020), DialogSum (daily life conversation) (Chen et al., 2021), MediaSum (interview) (Zhu et al., 2021), and MeetingBank (meeting) (Hu et al., 2023). This selection ensures that each source dataset covers nine distinct domains and maintains a balanced distribution of text types (dialogue, non-dialogue) and lengths (short text, long text)."}, {"title": "3.2 Summary Generation and Selection", "content": "Summary Generation. We randomly sample 200 input texts from the test set of each source dataset. Then, we generate summaries using the nine latest language models as summarizers, chosen for their widespread usage. These models are classified into three categories: non-LLMs, including finetuned BART-Large (Lewis et al., 2020) and FlanT5-Large (Chung et al., 2024), each with fewer than 700M parameters; open-source LLMs, including Phi-2 (Javaheripi et al., 2023), Llama2-13B-Chat (Touvron et al., 2023), instruction-tuned Mistral7B (Jiang et al., 2023) and Mixtral-8x7B (Jiang et al., 2024); and proprietary LLMs, including GPT-3.5-turbo, GPT-4-turbo (Achiam et al., 2023), and Claude2.1. See Appendix B.1 for model details and prompts used to generate summaries.\nHallucinogenic Text Selection. The latest models, such as Claude2.1 and GPT-4-turbo, generate hallucinations that are subtle and less common. Hence, to create a more challenging benchmark, we identify hallucinogenic texts, which have the potential"}, {"title": "3.3 Fine-Grained Annotation Tasks", "content": "We collect fine-grained human labels for multi-dimensional aspects of summary evaluation. The conventional dimensions like coherence and relevance is not adequate for fine-grained evaluation, due to the ambiguity in their definitions. Thus, we follow the three fine-grained dimensions suggested in the recent work (Song et al., 2024), namely faithfulness, completeness, and conciseness.\nFaithfulness is assessed at the sentence level by fact verification, a task of assigning a binary label (Yes/No) indicating whether a sentence has factual errors across four predefined categories. These include Out-of-Article Error as an extrinsic error, and Entity Error, Sentence Error, Relation Error as the three subcategories of intrinsic errors (see Appendix C for more details on the error taxonomy). If the response is Yes, we then ask the respondents to identify error types using a multichoice form.\nIn contrast, completeness and conciseness are annotated at the key-fact level using two different tasks. More specifically, we carefully generate the list of potential key-facts using multiple LLMs\u00b3 and then conduct key-fact validation, a human annotation task for verifying if the information in each generated key-fact is significant, factually correct, and relevant with respect to its source text. It enables the identification of human verified key-facts from the generated ones. Next, we perform another annotation task of key-fact alignment, matching each human key-fact to all summary"}, {"title": "3.4 AI-Assisted Manual Evaluation", "content": "Unlike key-fact alignment, which only requires annotators to align key-facts and summary sentences, fact verification and key-fact validation tasks necessitate a thorough understanding of the input text. This issue evidently leads to a significant drop in IAA for manual evaluation when input texts are lengthy, as humans struggle to manage large volumes of information simultaneously (Krishna et al., 2023). Hence, we devise AI-assisted manual evaluation in Figure 2, which helps achieve high IAA among non-expert human annotators for long texts.\nWe apply contextual mapping, a task of highlighting sentences in an input text that rank in the top 30% for similarity\u00b3 with a target summary sentence in fact verification (or a key-fact in key-fact validation) being evaluated. This aids annotators by providing contextual cues and narrowing down the relevant sections of the input text. Additionally, for fact verification, we plug in machine-based reasoning, the assistance of providing the decision by an automatic evaluator. This is based on the inferred"}, {"title": "3.5 Annotation Procedure", "content": "We conduct AI-assisted manual evaluation using MTruk on our three tasks. We select annotators who pass an English qualification test, with an approval rating above 95% and at least 1,000 accepted HITs. We collect human annotations from three independent qualified annotators for 8,133 summary sentences in fact verification, and for 2,673 keyfacts in key-fact validation. The 2,673 potential key-facts are reduced to 2,509 human key-facts. Consequently, annotations for key-fact alignment are collected for all possible pairs between human key-facts and summary sentences of the same input text, i.e., 101,013 pairs in total. Annotators are paid 50% above the U.S minimum wage and receive $25 bonuses for every 500 HITs. See Appendix G for more details on our annotation tasks."}, {"title": "4 UNISUMEVAL Quality Assessment", "content": "Evaluation Metric. We report IAA for three fine-grained annotation tasks in UNISUMEVAL: fact verification, key-fact validation and alignment. We use Krippendorff's \u03b1 (Krippendorff, 2011) by default, but for key-fact validation, where there is significant label imbalance, we use Gwet's AC1 (Wongpakaran et al., 2013) due to its enhanced robustness."}, {"title": "4.1 Inter-Annotator Agreement", "content": "Overall Assessment. Table 2 shows the IAA scores of UNISUMEVAL across nine data domains. In Table 3, we compare UNISUMEVAL with exist"}, {"title": "5 Benchmarking Summarizers", "content": "Evaluation Dimension. We evaluate the nine summarizers across five crucial evaluation dimensions for text summarization. In addition to faithfulness, completeness, and conciseness, we add two more dimensions: domain stability, the consistency of a summarizer's performance across the nine domains; abstractiveness, the extent to which a summary generates novel sentences or phrases, leading to a more coherent and condensed summary."}, {"title": "Evaluation Metric", "content": "We report percentage scores (in Section 3.3) of faithfulness, completeness, and conciseness, computed by using fine-grained human annotations. For domain stability, we calculate the average of the three percentage scores to obtain a composite score, and then measure domain inconsistency by computing the gap between the highest and lowest composite ones. For abstractiveness, we use the average of novel 1/3/5-grams following Song et al. (2023). See Appendix D.1 for their detailed definitions."}, {"title": "5.1 Comparison over Nine Summarizers", "content": "Figure 3 shows the overall performance rankings aggregated across the all text domains, types, and lengths in UNISUMEVAL. The proprietary LLMs notably outperform the non-LLMs and open-source LLMs across the all aspects. GPT-4-turbo is the best summarizer in general, while Claude2.1 has the best faithfulness and domain stability. Proprietary LLMs also exhibit an interesting behavior \u2013 no statistical relationship between faithfulness and abstractiveness \u2013 contradicting recent findings (Maynez et al., 2020; Ladhak et al., 2022) that summaries with higher abstractiveness are more prone to trigger hallucination. Statistical analysis on this can be found in Appendix F.3."}, {"title": "5.2 Impact of Domain, Type, and Length", "content": "Table 4 breaks down the performance in Figure 3, highlighting the impact of input contexts \u2013 input domain, type, and length \u2013 on each summarizer.\nFirstly, the superiority in summarization quality among the summarizers varies depending on input domain, type, and length. The general tendency among the three summarizer categories is consistent, while within each category, there are considerable changes in the summarizers' rankings (see the color changes for each column).\nSecondly, most summarizers experience a significant performance drop in completeness and conciseness with lengthy input texts. Particularly, such a drop is more noticeable when dealing with non-dialogue than dialogue. This confirms that identifying key-facts in summary generation is more challenging with longer input texts."}, {"title": "5.3 Impact of PII Redaction", "content": "We investigate how PII redaction impacts summarization quality using recent summarizers. This is a crucial aspect, because it is very common in industrial use cases, such as call centers and legal service. To construct a redacted dataset, we select a subset of UNISUMEVAL \u2013 25 hallucinogenic texts from the MultiWOZ (booking) data which contain significant amounts of PII-related entities, including phone numbers and addresses. We manually redact the entities by replacing them with their corresponding category name, i.e., <PHONE-NUMBER-1>. On average, eight entities are redacted per input dialogue. See Appendix E for the detailed protocol.\nTable 5 shows the faithfulness scores before and after PII redaction into input texts, where the top-2 summarizers are selected from each category. PII redaction negatively affects the faithfulness of the all summarizers. The non-LLMs are more susceptible to PII redaction than the open-source"}, {"title": "5.4 Factuality Error Analysis", "content": "In Figure 4, we examine how the distribution of error types varies across different input contexts for each summarizer group. The proprietary LLMs exhibit a lower rate of intrinsic errors, while the non-LLMs exhibit a higher rate of them in all input contexts. Notably, the proprietary LLMs show no relation errors across most context types except in long dialogue texts. This suggests that the higher faithfulness scores of the proprietary LLMs in Table 4 are likely due to their much lower intrinsic error rates compared with the others."}, {"title": "6 Benchmarking Auto-Evaluators", "content": "Evaluator Selection. We benchmark SOTA automated evaluators on UNISUMEVAL. The set of compared evaluators varies according to the target evaluation dimension. For faithfulness, we include QA-based models: UniEval (Zhong et al., 2022) and QAFactEval (Fabbri et al., 2022), NLI-based models: Summac-Conv (Laban et al., 2022), AlignScore (Zha et al., 2023), and MiniCheck (Tang et al., 2024a), and LLM-based models at various levels of granularity: G-Eval (Liu et al., 2023a) for summary level, G-Eval+4 for sentence level and FactScore (Min et al., 2023) for atomic level. For completeness, we include NLI-based models: Lite\u00b3Pyramid (Zhang and Bansal, 2021), A\u00b3CU"}, {"title": "6.1 Alignment with Human Score", "content": "Tables 6-8 report the agreement between automated evaluators and humans in faithfulness, completeness, and conciseness. In the booking domain, evaluators that require reference summaries do not report results since these summaries are unavailable. In general, the LLM-based evaluators, G-Eval+, show the highest agreement in all dimensions. The agreement with the human scores appears to vary across different data domains.\nIn the faithfulness evaluation (Table 6), the increasing granularity over the three LLM-based methods do not guarantee improved performance. Specifically, the atomic level evaluator, FactScore, does not perform well due to the difficulty in atomic fact generation \u2013 often producing numerous overlapping or factually incorrect atomic facts, which can cascade into further errors by LLMs. Moreover, contrary to prior findings that NLI-based evaluators perform well (Tang et al., 2024b), they show much lower agreement compared to G-Eval when evaluating faithfulness on our hallucinogenic texts. Hence, non-LLM evaluators tend to perform poorly at verifying LLM-generated hallucinations.\nAdditionally, the agreement with human faithfulness evaluation is significantly affected by domain characteristics. In particular, QA- and NLI-based methods, which require training on specific data, lack domain generalization in automatic evaluation. They exhibit negative correlations with human judgements on more than half of the do-"}, {"title": "7 Conclusion", "content": "We introduce UNISUMEVAL, a benchmark dataset featuring hallucinogenic texts from nine domains, spanning non-dialogue to dialogue and short to long texts, paired with their summaries generated by nine recent summarizers. Built using a data creation pipeline with AI assistance, UNISUMEVAL includes high-quality, fine-grained human annotations that enable in-depth studies on the multidimensional performance of summarizers. Additionally, based on our benchmark, we provide a thorough assessment of automated evaluators for text summarization, revealing weaknesses related to specific domains and evaluation dimensions."}, {"title": "Limitations", "content": "Our work has some limitations. First, although UNISUMEVAL covers three comprehensive evaluation dimensions for summarization quality, additional dimensions like harmfulness or bias could enhance the nuanced assessment of summaries. Sec-"}, {"title": "Ethics Statement", "content": "We actively addressed annotators' queries during the annotation process, ensuring faithful communication. Annotators were compensated at a rate 50% above the average American minimum wage and received bonuses for consistent, high-quality work. Our dataset excludes any information that could potentially disclose the annotators' personal details."}, {"title": "Scientific Artifacts", "content": "We utilized nine language models to generate summaries on UNISUMEVAL. Apart from the paid APIs like OpenAI and AWS Bedrock, we used readily available checkpoints on Huggingface. All the details are summarized in Table 10."}]}