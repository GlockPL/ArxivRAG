{"title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension", "authors": ["Yongdong Luo", "Xiawu Zheng", "Xiao Yang", "Guilin Li", "Haojia Lin", "Jinfa Huang", "Jiayi Ji", "Fei Chao", "Jiebo Luo", "Rongrong Ji"], "abstract": "Existing large video-language models (LVLMs) struggle\nto comprehend long videos correctly due to limited context.\nTo address this problem, fine-tuning long-context LVLMs\nand employing GPT-based agents have emerged as promis-\ning solutions. However, fine-tuning LVLMs would require\nextensive high-quality data and substantial GPU resources,\nwhile GPT-based agents would rely on proprietary models\n(e.g., GPT-40). In this paper, we propose Video Retrieval-\nAugmented Generation (Video-RAG), a training-free and\ncost-effective pipeline that employs visually-aligned auxil-\niary texts to help facilitate cross-modality alignment while\nproviding additional information beyond the visual con-\ntent. Specifically, we leverage open-source external tools\nto extract visually-aligned information from pure video data\n(e.g., audio, optical character, and object detection), and in-\ncorporate the extracted information into an existing LVLM\nas auxiliary texts, alongside video frames and queries, in\na plug-and-play manner. Our Video-RAG offers several\nkey advantages: (i) lightweight with low computing over-\nhead due to single-turn retrieval; (ii) easy implementa-\ntion and compatibility with any LVLM; and (iii) signifi-\ncant, consistent performance gains across long video un-\nderstanding benchmarks, including Video-MME, MLVU,\nand LongVideoBench. Notably, our model demonstrates su-\nperior performance over proprietary models like Gemini-\n1.5-Pro and GPT-40 when utilized with a 72B model.", "sections": [{"title": "1. Introduction", "content": "With the advancements in Large Language Models (LLMs),\nnumerous studies have been conducted to enhance their\nability to comprehend and process videos [2, 3, 10, 15-\n18, 21-23, 45, 48, 50], collectively termed Large Video-\nLanguage Models (LVLMs). Although current LVLMs\nhave demonstrated promising performance in understand-\ning short videos, effective comprehension of extremely long\nvideos continues to be a major challenge.\nTo address this challenge, recent studies [33, 39, 43,\n47, 53] have sought to extend the reasoning context length\nof LVLMs, essentially finetuning long-context LVLMs for\nlong video understanding. LongVA [47] first introduces\nincreasing the token capacity of an LLM and transferring\nits long-context comprehension capabilities to video data.\nHowever, training such a model requires pre-training on an\nextended corpus, and often there are distribution shifts be-\ntween deployment videos and finetuning videos. As demon-\nstrated in Video-MME [6], LongVA declines when increas-\ning the video frame sampling rate from 128 to 384 (52.6%\n\u2192 51.8%). This outcome suggests that simply increasing\nthe number of sampled frames not only leads to information\nredundancy but also imposes additional challenges for the\nmodel to handle complex reasoning. Retrieval-Augmented\nGeneration [13] (RAG) is a technique that enhances genera-\ntive tasks by retrieving relevant documents from an external\ncorpus, thus improving response quality in LLMs. Recent\nstudies have begun exploring the integration of RAG with"}, {"title": "2. Related Work", "content": "video-based tasks [1, 25, 46], employing tools to process\nvideos in long contexts and sending them to a proprietary\nmodel for generation, which is known as the GPT-based\nAgent method. However, they come with serval limita-\ntions. First, most of them process long video content as\nplain text, subsequently utilizing the RAG mechanisms to\nretrieve relevant documents for LLMs. Therefore, they lack\nalignment with the visual context of the video, resulting in\na loss of critical visual information. Second, they are often\nresource-intensive in multi-turn interactions and typically\nrequire powerful LLMs to function as the driving force, thus\nlimiting their flexibility and generative capabilities. Exe-\ncuting the whole Video-MME [6] using VideoAgent [4] re-\nquires approximately 20 days and incurs a substantial con-\nsumption of GPT-40 API tokens.\nIn this study, we propose Video-RAG, an effective RAG\npipeline that can be seamlessly integrated with any LVLM.\nSpecifically, instead of simply increasing the number of\nsampled video frames, we propose to replace the corre-\nsponding extended visual tokens with auxiliary texts ex-\ntracted from pure video data by invoking open-source foun-\ndation models, such as optical character recognition (OCR),\nautomatic speech recognition (ASR), and object detection.\nThese auxiliary texts are more aligned with the visual con-\ntext while providing additional information beyond the vi-\nsual data, as demonstrated in [4, 19]. Besides dealing with\nthe context windows limit of LVLMs, we employ RAG in\nVideo-RAG to filtering auxiliary texts, ensuring their rel-"}, {"title": "2.1. Large Video-Language Models", "content": "With the rapid advancement of large language models\n(LLMs), there has been increasing interest in developing\ngeneralist video models capable of handling a wide range of\nvideo-related tasks. Video-ChatGPT [26] extracts features\nfrom individual frames and aggregates them through both"}, {"title": "2.2. Long-context Large Video-Language Models", "content": "Recent approaches have sought to expand the context win-\ndow size to enhance detailed video understanding. LongVA\n[47] and Long-LLaVA [43] address this by continuously\ntraining LLMs on extended textual data, to transfer their\nlong-text comprehension capabilities to video processing.\nINTP [33] introduces a video token rearrangement tech-\nnique while proposing a training-free method for extend-\ning the LLM context window, allowing LVLMs to process\nincreased visual tokens. However, these methods face chal-\nlenges in striking a balance between the high computational\ncosts associated with sampling video frames and the limited\nperformance improvements achieved. Due to the inherent\nredundancy in video content and constraints on model ca-"}, {"title": "2.3. GPT-based Agent Video Understanding", "content": "Initial efforts [7, 27, 36, 40, 44] have employed LLMs to in-\nteract with tools to process visual information as structured\nlong context for question-answering. MM-VID [19] en-\nhances long video understanding by aligning video frames\nwith corresponding text descriptions. VLog [12] leverages\nmultimodel pre-trained models to capture and interpret vi-\nsual and audio information, summarizing it into documents\nfor video comprehension. VideoAgent [4], DrVideo [25],\nand OmAgent [46] integrate multimodal inputs and enable\ndynamic querying of video segments to support long video\nreasoning tasks. However, these methods take an extremely\nlong time to process videos while relying on proprietary\nmodels (e.g., GPT-40), thus limiting their efficiency and\nadaptability to other open-source frameworks."}, {"title": "3. Method", "content": "We propose a novel, training-free pipeline for large video-\nlanguage models (LVLMs), named Video-RAG, which can\nbe integrated into any LVLM. As illustrated in Figure 3, our\npipeline comprises three key phases: (i) Query Decouple:\nIn this phase, the user's query is decomposed into a retrieval"}, {"title": "3.1. Large Video-Language Model", "content": "Given a video V, a frame sampler first sample N frames\nF. Most existing methods uniformly sample frames from\na video for both effectiveness and simplicity. Then, video\nfeatures are extracted as F = VisualEnc(F), where\nVisualEnc is an image-based visual encoder, such as\nCLIP-L [29]. Finally, the video features F and the user's\nquery Q are fed into the LVLM to generate an output O:\nO = LVLM(F, Q)"}, {"title": "3.2. Query Decouple", "content": "In this phase, upon receiving a user's query about the video,\nthe LVLM begins by decoupling the query and generating\nretrieval requests, denoted as R, for auxiliary texts. During\nthis phase, the LVLM processes only textual information,\nwithout access to video frames, and the output requests are\nformatted in JSON. We prompt the LVLM using a decou-\npling prompt P to generate the following retrieval requests\nas necessary: (i) Rasr: Requests about automatic speech\nrecognition, to extract audio information from the video that\nmay pertain to the query. (ii) Rdet: Requests for identifying\nphysical entities within the video that may assist in answer-\ning the query. (iii) Rtype: Requests for details about the\nlocation, quantity, and relationships of the identified phys-\nical entities. These requests, which may be NULL (indi-\ncating that the corresponding information is not required),\nare then parsed and forwarded to the auxiliary text retrieval\nphase. The entire process can be formally described as:\nR = LVLM(P, Q), R = {Rasr, Rdet, Rtype}"}, {"title": "3.3. Auxiliary Text Generation", "content": "In this phase, we first generate the auxiliary texts from the\nvideo and then retrieve them to assist the LVLMs accord-\ning to the retrieval requests R. As the length of the video\nincreases, the number of tokens generated from the pro-\ncessed data also grows, leading to an increase in redun-\ndant information. Additionally, current open-source mod-\nels are constrained by the limited length of their context\nwindows, which may prevent them from fully processing\nall auxiliary texts. To address this issue, we draw inspira-\ntion from Retrieval-Augmented Generation (RAG) [13], re-\ntrieving only the auxiliary texts relevant to the user's query."}, {"title": "3.4. Auxiliary Text Retrieval", "content": "During the retrieve phase, we employ the Contriever\nframework to encode the user's query and the parsed re-\nquests for OCR and ASR into text embeddings, then\nconcatenating to form the final query request Ereq =\nContriever(Concat(R, Q)), R\u2208 {Rocr, Rasr}.\nThen we retrieve the auxiliary texts from DB =\n{DBocr, DBasr} by the FAISS tool, which computes the\nvector similarity between the query and text chunks stored\nin the database. Text chunks with a FAISS similarity score\ngreater than threshold t are indexed as the retrieval results\nA \u2208 {Aocr, Aasr}. The process can be formulated as:\nA FAISS_similarity(DB,Ereq) > t\nThe information stored in the DET database undergoes\nan initial retrieval process. Since the text generated by\nthe detection model is in a raw format (\"category: [x_min,\ny_min, length, width]\"), it challenges LVLMs to understand\nthe relative relationships between objects. To address this\nissue, we preprocess the object information using a scene\ngraph, which helps to represent spatial and relational in-\nformation more explicitly. This preprocessing allows us to\nconstruct more coherent and semantically meaningful texts,\ndenoted as Ae, which are more readily interpretable by\nLVLMs. We incorporate three types of object information\nfor each video keyframe: (i) Object Location Aloc: This\nrefines the positional information of the object, formatted\nas: \"Object {node ID} is a {object category} located at co-\nordinates [x, y] with dimensions {length \u00d7 width}\" (ii) Ob-\nject Counting Acnt: This counts the number of objects and\ngenerates text in the following format: \"Object counting:\n{object category}: {number}\" (iii) Relative Positional\nRelationships Arel: This captures the relative spatial rela-"}, {"title": "3.5. Integration and Generation", "content": "After obtaining different types of auxiliary texts, we or-\nganize them chronologically using natural language to\ncreate a unified auxiliary input, denoted as Am =\nConcat(Aocr, Aasr, Adet). These merged auxiliary in-\nputs, along with the user's query and the sampled video\nframes, are then fed into the LVLM to produce the final\nresult. The overall process can be formulated as:\n0 = LVLM(F, Concat(Am, Q))"}, {"title": "4. Experiments", "content": "4.1. Datasets\nVideo-MME [6] is a widely used benchmark for assessing\nthe ability of LVLMs to handle detailed videos in real-world\nscenarios. It is divided into three subsets based on video\nlength, with durations ranging from 11 seconds to 1 hour.\nMLVU [52] is a long video understanding benchmark with\na large wide of 9 distinct tasks. It is created based on long\nvideos of diversified lengths, ranging from 3 minutes to 2\nhours with about 12 minutes average video length.\nLongVideoBench [41] is a benchmark designed to ac-\ncurately retrieve and reason over detailed multimodal in-\nformation from long videos, with 6,678 human-annotated\nmultiple-choice questions in 17 fine-grained categories."}, {"title": "4.2. Implementation Details", "content": "We performed all experiments on NVIDIA A100 80G\nGPUs. During the auxiliary text generation phase, we first\nfilter the detection requests Rdet generated by the LVLM\nto ensure they correspond to CLIP-sensitive physical en-\ntities, avoiding the inclusion of abstract concepts. In the\nauxiliary text retrieval phase, we set both the CLIP sim-\nilarity threshold and the FAISS similarity threshold t to\n0.3. We employ the IndexFlatIP as the similarity calcu-"}, {"title": "4.3. Main Results", "content": "Video-MME. We evaluate our Video-RAG in four\n7B open-source LVLMs, including Video-LLaVA [17],\nLLaVA-NeXT-Video [48], LongVA [47], Long-LLaVA\n[43], and two 72B LVLM Qwen2-VL [38] and LLaVA-\nVideo [49]. To ensure a fair assessment of the performance\nimprovements introduced by our pipeline, and given the re-\nsource constraints (especially 72B LVLMs), we reproduced\nthe performance of all open-source models under the 32-\nframe setting. Results are shown in Table 1. Specifically,\nafter applying our Video-RAG in 72B LVLM, we perform\nbetter than the SOTA proprietary model Gemini-1.5-Pro\n[31] (75.7% vs. 75.0%). Across the six LVLMs used in our\nexperiments, we gained an average performance boost of\n8.0%, especially a significant gain on long videos, demon-\nstrating our pipeline's effectiveness. This performance im-"}, {"title": "4.4. Ablation Studies", "content": "Effect of different sampling frame number. To explore\nthe effect of the number of sampling frames on Video-RAG,\nwe experience sampling frames number 8, 16, 32, and 64\nframes in 7B model Long-LLaVA [43], results are shown\nin Figure 4. As demonstrated, Video-RAG consistently de-\nlivers performance improvements across all frame rates es-\npecially in long videos, with these gains increasing as the\nframe rate rises. Furthermore, the results indicate that the\nhighest accuracy without Video-RAG is achieved when 32\nframes are sampled from the video. Therefore, we adopt\nthis configuration in the subsequent ablation experiments.\nEffect of different components of Video-RAG. To explore\nthe effectiveness of auxiliary texts, we incrementally add\nobject detection, OCR, and ASR as auxiliary texts after re-"}, {"title": "4.5. Qualitative Evaluation", "content": "We present qualitative results in the case of Video-MME\n[6] in Figure 5 and Figure 6. As illustrated, augmenting\nLLaVA-Video with external tools to process and retrieve\nauxiliary texts from videos significantly enhances its ability\nto reduce visual hallucinations, thereby enabling more accu-\nrate responses to user queries. Grad-CAM [32] and t-SNE\n[37] visualization results also show that applying Video-\nRAG helps the LVLM's cross-modality alignment."}, {"title": "5. Conclusion", "content": "In this paper, we present Video-RAG for effective long\nvideo understanding by integrating retrieved auxiliary texts"}], "equations": [{"equation": "O = LVLM(F, Q)"}, {"equation": "R = LVLM(P, Q), R = {Rasr, Rdet, Rtype}"}, {"equation": "Tocr = EasyOCR(F)"}, {"equation": "DBocr FAISS Eocr = Contriever(Tocr)"}, {"equation": "Tasr = Whisper(U)"}, {"equation": "DBasr FAISS Easr = Contriever(Tasr)"}, {"equation": "Fkey CLIP_similarity(Rdet, F) > t"}, {"equation": "DBdet \u2190 Tdet = APE(Fkey, Rdet)"}, {"equation": "AFAISSIndex FAISS_similarity(DB,Ereq) > t"}, {"equation": "Adet SceneGraph(DBdet)"}, {"equation": "Adet = Rtype(P(Adet)) \u2208 P(Adet)"}, {"equation": "0 = LVLM(F, Concat(Am, Q))"}]}