{"title": "Unified Local and Global Attention Interaction Modeling for Vision Transformers", "authors": ["TAN NGUYEN", "COY D. HELDERMON", "COREY TOLER-FRANKLIN"], "abstract": "We present a novel method that extends the self-attention mechanism of a vision transformer (ViT) for more accurate object detection across diverse datasets. ViTs show strong capability for image understanding tasks such as object detection, segmentation, and classification. This is due in part to their ability to leverage global information from interactions among visual tokens. However, the self-attention mechanism in ViTs are limited because they do not allow visual tokens to exchange local or global information with neighboring features before computing global attention. This is problematic because tokens are treated in isolation when attending (matching) to other tokens, and valuable spatial relationships are overlooked. This isolation is further compounded by dot-product similarity operations that make tokens from different semantic classes appear visually similar. To address these limitations, we introduce two modifications to the traditional self-attention framework; a novel aggressive convolution pooling strategy for local feature mixing, and a new conceptual attention transformation to facilitate interaction and feature exchange between semantic concepts. Experimental results demonstrate that local and global information exchange among visual features before self-attention significantly improves performance on challenging object detection tasks and generalizes across multiple benchmark datasets and challenging medical datasets. We publish source code and a novel dataset of cancerous tumors (chimeric cell clusters).", "sections": [{"title": "1 INTRODUCTION", "content": "Recent object detection models [3, 20, 23, 25, 28, 36, 37, 49, 52] are able to capture robust, representative, high-level semantic features across diverse datasets for accurate localization and classification of objects. These architectures incorporate learning-based visual feature encoders that are critical for perception object detection, the process of identifying and interpreting visual information to recognize objects. Transformer architectures are at the forefront of these models, achieving state-of-the-art results on many object detection benchmarks [3, 21, 26, 27, 42, 47, 48]. One of the reasons transformer encoders have been successful at object detection lies in their ability to model long-range dependencies between visual elements through the attention mechanism. This capability makes them well-suited for visual detection tasks, where understanding spatial relationships at different scales and ranges is essential.\nDespite its recent successes and broad adoption, the transformer self-attention mechanism has inherent limitations when operating on complex datasets where different semantic objects exhibit visually similar appearances. Medical datasets with cancerous tumors in tissue scans or tumors in brain MRI images are examples. Queries, keys, and values for objects from different classes can become indistinguishable. Consequently, the attention map struggles to focus on relevant regions, and thus spans indiscriminate attention to non-relevant objects. In the case of cancer tumor detection, failing to differentiate between visually similar but conceptually different tissues could lead to false positives, inaccurate diagnoses and unnecessary invasive procedures.\nTo address these limitations, we propose a method that extends the self-attention mechanism in Vision Transformers to enable feature tokens to interact at both local and global scales before self-attention is applied. Inspired by complementary properties in localized convolutional interactions [13, 46] and global attention, our technique facilitates feature exchange, and allows tokens to develop more complex and distinct representations based on their true semantic class. The approach incorporates two neural modules before the global attention step: (1) Aggressive Convolutional Pooling that iteratively applies depth-wise convolution and pooling operations to allow each feature token to capture both local and global interaction, and (2) a Conceptual Attention Transformation implemented by a novel Conceptual Attention Transformer that leverages high-level conceptual knowledge [44] through a novel backward flow attention mechanism to provide a global perspective that complements local convolution interactions.\nWe leverage convolution applied in early stages to produce more distinct, well-differentiated features that effectively reduce smoothing caused by isolated feature interactions at later stages within the self-attention mechanism. The enriched features are further refined using conceptual attention transformation with a unique projection layer that integrates the input with the semantic conceptual tokens [44]. The results produce visual tokens with improved contextual understanding and feature representation. Ultimately, self-attention is applied to these distinctive features that are stronger aligned with their true semantic classes.\nOur Enhanced Interaction Vision Transformer architecture shows substantial performance improvement for object detection over state-of-the-art transformer models for a broad range of self-attention module formations. Our contributions include:\n\u2022 A novel aggressive depth-wise convolutional pooling module that combines local interactions with global interactions before self-attention (Section 5).\n\u2022 A new conceptual attention transformation with a unique projection layer that integrates model inputs with semantic conceptual tokens for enhanced feature representation and interaction (Section 6)."}, {"title": "2 BACKGROUND", "content": "This section provides background information on aspects of transformer models and self attention that motivate our work.\nTransformer-based object detectors have the flexibility to learn universal features without implicit constraints of inductive bias inherent to CNN-based models like translational equivariance and locality [7]. Self attention, the fundamental mechanism of operation for transformers [7, 41] effectively captures global information, granting each feature token a global receptive field. This capability is crucial for object detection, enabling the model components responsible for high level vision tasks to comprehend spatial relationships and extract meaningful spatial semantics for accurate detection. Several studies, which we discuss in Section 3, are relevant to our investigation of the effectiveness of transformer-based encoders for object detectors.\nHowever, there are challenges. In multi-headed attention, feature tokens are projected through a linear aggregation along the channel-wise dimension to compute queries, keys, and values for self-attention (Equation 1). This can lead the network to rely more on positionally encoded information rather than extracting robust and representative features for downstream tasks where objects appear visually similar. In this formulation, feature tokens are treated in isolation when attending to others. Thus, for visually similar objects, their corresponding queries, keys, and values become nearly identical.\n$K = W_KX$\n$V = W_vX$\n$Q = W_oX$\n$\\text{attn} = \\sigma\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right)$\n(1)\nWhere the matrices $W_K$, $W_V$, and $W_Q$ represent linear projection matrices without bias terms. The function $\\sigma$ is applied to the result of the matrix multiplication between the query matrix $Q$ and the key matrix $K$, scaled by $\\sqrt{d_k}$. This scaling factor normalizes the result based on the dot product in the channel dimension of size $d_k$ for both $Q$ and $K$, resulting in the attention matrix $\\text{attn}$. The attention matrix $\\text{attn}$ quantifies the relevance of each feature in the value vector $V$ for every query in $Q$.\nVision transformers, the focus of our work, are designed for high-level computer vision operations on image data. ViTs scale efficiently when trained on large volumes of data. Thus, pre-trained ViTs are good foundational models able to transfer information learned from extensive datasets for detection tasks on mid-size and small image recognition benchmark datasets with prediction rates comparable to state-of-the-art CNN models [1, 7, 18, 35]. We aim to use these models as a starting point for a technique that enhances feature representation for robust object detection in complex datasets where the target object is ambiguous (concealed-object datasets)."}, {"title": "3 PREVIOUS WORK", "content": "In this section, we review prior work, with a focus on object detection methods closely related to our approach.\nObject detection networks are broadly categorized into multi-stage detectors [2, 10, 11, 15, 37] and one-stage detectors [23, 25, 36]. Both approaches rely on a feature extraction to capture high-level semantic features that represent a variety of objects. Before the advent of transformers, the original approaches developed efficient convolution-based feature extractors as visual encoders tailored for object detection tasks [16, 32, 34, 39, 40]. Today, these designs complement and enhance overall performance in transformer-based architectures [46]. In multi-stage detectors, features are processed by an additional region proposal network (RPN) [37] that generates a set of potential regions of interest. Features corresponding to these regions are then pooled [22] to incorporate multi-scale feature representations before performing the final detection. One-stage detectors bypass the RPN and directly generate detection anchors to simultaneously classify and localize objects. Recent autoregressive decoder methods [3] further eliminate the reliance on RPNs and anchor generation. These transformer-based decoders bypass the need for traditional non-maximum suppression, previously essential for both multi-stage and one-stage detectors.\nTransformer-based Object Detection: Previous studies on vision transformers predominantly utilize the vanilla self-attention mechanism, defined using queries, keys, and values (Equation 1). This mechanism often generates token-wise attention maps that exhibit excessive uniformity [51]. This uniformity leads to dense aggregation of patch embeddings, that results in overly similar token representations\u2014a phenomenon we refer to as the smoothing effect in self-attention. This effect is particularly pronounced in medical datasets, where objects from different classes often appear visually similar, and in natural datasets involving concealed or camouflaged objects. Prior works [45, 51] addressed this issue by enriching the attention maps after the self-attention computation. In this work, we assert that adding additional context to feature representations prior to self-attention is an essential complementary step. This strategy enhances the expressiveness of attention maps, optimizes the aggregation of value features, and significantly improves overall performance.\nVision Transformers: Recent advancements that adopt transformers for vision tasks have significantly enhanced the effectiveness of Vision Transformers as object detectors. Standard ViT object detection models [21] use a straight forward adaptation of a transformer model with minimal modifications. These original models achieve competitive results. Subsequent research improved feature extraction capabilities. The Swin Transformer [26, 27] introduced a shifted window mechanism to reduce the computational overhead of global attention and incorporated a hierarchical structure"}, {"title": "4 OVERVIEW", "content": "Figure 1 illustrates our system architecture adapted for the baseline (standard) vision transformer module. Two distinct interaction modules, Aggressive Convolutional Pooling (ACP) and Conceptual Attention Transformation (CAT) enable feature tokens to interact before self-attention. These modules may be integrated into a wide range of ViT architectures. We position aggressive convolutional pooling before the conceptual attention transformation unit as convolutional operations utilize local kernels that capture localized interactions that complement the global attention mechanism. Convolutional properties enhance feature complexity early in the process, reducing smoothing effects when global attention is applied. This additional enhancement transforms the input feature maps so that queries, keys, and values represent distinct features that encode their interrelationships. Enhanced feature complexity during the dot-product similarity within the attention mechanism leads to more easily detected differences between visually similar objects across different semantic classes."}, {"title": "5 AGGRESSIVE ATTENTION POOLING", "content": "We propose a novel aggressive depth-wise convolutional pooling layer before self-attention to enhance feature representations with both local and global context. Our strategy begins with the depth-wise convolution operation (DWConv) in Equation 2 from Local Perception Units (LPUs) [13, 47, 48], and extends it with an iterative pooling scheme that significantly increases the effective respective field for global interactions, rather than local-only operations that occur within the convolutional kernel\u0161s window. Like LPUs, our aggressive attention pooling method occurs before multi-head self-attention (MHSA).\n$LPU(X) = DWConv(X) + X$\n$Xout = MHSA(\\omega)$\n(2)\n(3)\nwhere w is the output from the LPU operation.\nLet $X_o \\in \\mathbb{R}^{C_o \\times H \\times W}$ denote the initial input feature map. At each step, $X_{i+1} \\in \\mathbb{R}^{2C_i \\times H_i/2 \\times W_i/2}$, where the spatial dimensions are reduced by a factor of 2 and the channel size is doubled to preserve information. Using Equation 2, we iteratively apply a sequence of convolutions with small kernel sizes interspersed with our additional max pooling operations. Our modification generalizes the LPU by repeating the LPU operation $n_{lpu} \\leq log_2(min(H, W))$ times, where $H$ and $W$ represent the height and width of the feature map, respectively. Iterations beyond this threshold reduce one of the spatial dimensions to one. Average pooling is then applied to the spatial dimension to compute the final feature map $X_f \\in \\mathbb{R}^{C_f \\times 1 \\times 1}$, where $C_f \\leq C_o \\cdot 2^{log_2(min(H_o, W_o))}$. The result is a feature map with a global receptive field. Conversely LPUs have smaller receptive fields limited to $k \\times k$, where $k$ is the convolution kernel size. Algorithm 1 summarizes the process, where $X$ is the model input, upscale is a small super-resolution network, and out is a linear projection for $X$, and stores the summation of all feature maps at different scales.\nLet us denote $f_{conv}$ as the convolutional operation at step $i$ that computes a new feature map $x^i$ of shape ($C \\cdot 2^i, H/2^i, W/2^i$) from an input feature of shape (C, H, W). The function $f_{conv}$ comprises a sequence of operations in the following order: Convolution with kernel size 3 \u00d7 3, ReLU activation function, and Max pooling with kernel size 2 x 2.\nWith this aggressive pooling layer, we obtain M \u2264 log2(min(H, W)) feature maps. These M features provide a global understanding of the surrounding context. It is important to note that each subsequent feature map $x^{i+1} = f_{conv}(x^i)$ has twice the receptive field of $x^i$. Consequently, the final feature map $x^M$ possesses the largest receptive field which can approximate the global receptive field when $M = log_2(min(H, W))$. We integrate these features with the input features, to gain combined local and global information at different scales in a framework inspired by the Feature Pyramid Network (FPN) architecture [22].\nFinally, we aggregate all $x^i$, where $i \\in \\{1, ..., M\\}$, into the input $x^0$. A naive approach would first perform concatenation, and then use a linear projection to map the concatenated feature into a C-dimensional feature space. However, we found this approach to be suboptimal, causing a large memory usage footprint. Instead, we propose a mathematically equivalent memory-efficient alternative that computes the sum of individual linear projections for each $x^i$. The linear projection is used to map the $C \\cdot 2^i$ channels of features into C channels. To resolve mismatched spatial shapes, we employ a sequence of upscaling modules, each increasing the spatial resolution by a factor of 2, followed by a convolution operation. Thus, each block consists of an upscaling step followed by a convolution. At most, log2(min(H, W)) such blocks are required to restore the pooled features to their original size. We apply nearest tensor neighbor interpolation to address potential shape mismatches caused by max pooling when the spatial resolution is not divisible by 2 as expressed as in Equation 4:\n$Y = f_{proj} (x^0) + \\sum_{i=1}^M  f_{upscale} f_{upscale} (f_{conv} (x^{i-1}))$\n(4)\nwhere $f_{upscale}$ is a small upscaling network that performs a sequence of convolution and upscaling operations to restore both the original channels and spatial dimensions for smaller feature maps."}, {"title": "6 CONCEPTUAL ATTENTION TRANSFORMATION", "content": "Kernels in the convolutional pooling layer increasingly limit spatially-distant interaction. We complement this effect with an attention-based module to enhance global feature interaction. We aim to leverage spatial attention to transform feature maps into a compact set of semantic tokens. Although our conceptual attention transformation (Figure 2) is inspired by the high-level conceptual features introduced in the Visual Transformer architecture [44], several novel contributions make our approach unique. A clear departure from prior work is a novel projection layer that integrates the input with the semantic conceptual tokens.\nLet X represent the input feature map of shape (C, H, W), where C, H, and W denote the number of channels, height, and width, respectively. The transformation stage begins by incorporating positional embedding information into the input features X and applying a convolutional operation with a kernel size of 3x3 for mixing. This operation is expressed in Equation 5:\n$X_p = Conv(X + PE)$\n(5)\nWe transform the positionally mixed features Xp into L conceptual representations, which can be learned in either an input-independent or input-dependent manner. For input-independent concepts, a linear projection maps the C features into L concepts, enabling the computation of similarity scores between each feature and each concept. This projection defines a hyperspace in $R^C$, where the dot product measures the relevance of features to each concept. For input-dependent concepts, the process begins by extracting conceptual representations from the input features. This is achieved through a sequence of convolutional layers with a kernel size of 3x3, followed by max pooling with a size of 2x2 and average pooling along the spatial dimensions. This sequence reduces the input feature map into a single feature token of shape (C, 1). The reduced features are then projected into (C, L) using a linear layer, producing L concepts, each represented in RC.\nConcepts (Figure 3) are used to compute similarity scores for each visual feature in the input as shown in Equation 6:\n$T_{attns} = W_{con}X_p$\n(6)\nHere, attns represents the unnormalized conceptual attention transformation, and $W_{con} \\in \\mathbb{R}^{C \\times L}$ denotes the conceptual hyperplanes in $R^C$. The unnormalized attention attns is passed through a Soft-max layer to compute the conceptual attention map attn $\\in \\mathbb{R}^{L \\times HW}$, as defined in Equation 7:\n$attn = Softmax(attns)$\n(7)\nThis attention map indicates, for each concept $l \\in \\{1, ..., L\\}$, an attention matrix of shape (1, HW) that identifies which features of the input Xp are relevant to concept l. Using this attention, the features for each concept are computed through attention pooling, as shown in Equation 8:\n$T_c = attn \\cdot X_p$\n(8)\nHere, $T_c \\in \\mathbb{R}^{L \\times C}$ are the conceptual tokens, and $X \\in \\mathbb{R}^{HW \\times C}$ is the reshaped tensor for Xp.\nTo integrate information from the conceptual tokens back into the input features, the conceptual attention map, attn, is reused to compute the backward flow contribution of each visual token to the conceptual tokens. For each concept $l \\in \\{1, ..., L\\}$, the attention map $attn_l \\in \\mathbb{R}^{HW}$ distributes contributions across HW visual tokens, indicating the degree to which each visual token contributes to a concept. This backward flow is computed as described in Equation 9:\n$attn_\u00b5 = W_m.(attn + \\alpha)$\n(9)\nThe stochasticity term \u03b1 introduces variance into the backward flow to ensure tokens are updated with sufficient diversity to mitigate smoothing that occurs when tokens become increasingly similar after each update. This occurs when dot products incorrectly map highly similar features or features that exhibit similar backward contributions to the same concept. We can learn \u03b1 independent of input features using positional information as a positional bias. It can also be learned in a feature-dependent manor by applying aggressive convolutional pooling to compute a parameter of shape (H, W, C). This parameter is projected via convolutional to produce L channels from C channels, yielding \u03b1.\nUsing the contribution attention attnu, we multiply this value with the conceptual tokens Tc to compute the mixed value defined in Equation 10:\n$\\phi = GELU(W_m(attn_\u00b5 . T_c))$\n(10)"}, {"title": "7 EXPERIMENTAL SETUP", "content": "To demonstrate the versatility of our enhanced architecture, we evaluated its performance for object detection tasks using three transformer object detection frameworks: the standard Vision Transformer (ViT) [21], the Swin Transformer [27], and the Deformable Attention Transformer (DAT++) [47, 48]. These architectures represent a diverse set of self-attention mechanisms, ranging from standard self-attention to advanced techniques such as shifted window attention and deformable attention. Our analysis includes several benchmark datasets, with a particular emphasis on medical datasets including one new dataset of cancerous tumors (chimeric cell clusters) which we contribute to the research community. The results demonstrate that our module can seamlessly integrate with diverse transformer architectures and significantly enhance their overall performance, particularly on complex dataset domains. We discuss the implementation details of our interaction enhancement modules for different vision transformer models in Section 7.2. The datasets and training procedure are provided in Section 7.1.\n7.1 Datasets\nFigure 4 shows the range of benchmark datasets for our evaluation. For each dataset, we minimize architectural modifications, only adjusting the patch embedding size, input resolution, and the number of output classes according to the provided configurations. Our evaluation includes testing on our new CCellBio dataset, available with this publication, which contains 26,991 training images and 3,643 test images of cancer tumors in tissue scans along with ground truth annotations. In addition, we assess our models on several publicly available datasets: COD10K-V2 [8, 9], Brain Tumor Detection [6], VinDr-CXR [12, 31, 31], NIH-ChestXRay [43], which includes nearly 1,000 images with bounding box annotations for 8 categories, and the RSNA Pneumonia Detection Dataset [30, 33, 38, 43], which includes 7,644 training images and 1,911 test images for detecting lung opacity.\nWe apply data augmentation in a consistent manor across all datasets studied in the following sequence: random flip with a probability of 50%, random resizing within a ratio range of 0.1 to 2.0 maintaining aspect ratio, and random cropping. Next, we remove annotations with spatial dimensions (height or width) smaller than 10-2 for more stable training before mean value normalization (123.675, 116.28, 103.53), and standard deviation values of (58.395, 57.12, 57.375) for the three color channels respectively. Finally, padding is applied to match the defined image size (114, 114, 114) for the height, width, and channels respectively.\n7.2 Model Configurations\nOur study centers on improving the transformer backbone architecture, a model used for feature extraction for higher level computer vision tasks; in our case object detection. Thus, we evaluated the aforementioned transformer models (standard Vision ViT [21], Swin [27], and DAT++ [47, 48]) within the RetinaNet framework [23], a single, unified network composed of a backbone network and two task-specific subnetworks. Nevertheless, the proposed approach is versatile and can be extended to other detection frameworks. We now describe a comprehensive evaluation approach that underscores the adaptability of our module across diverse transformer architectures, affirming its potential for broader generalization.\nBackbone Configurations: For each transformer backbone, we derive two versions: the original architecture (ViT, Swin, and DAT), and a corresponding modified version incorporating our enhanced interaction modules (EI-ViT, EI-Swin, and EI-DAT). To ensure a fair evaluation, the enhanced architecture configuration is preserved to match that of the original architecture, and we increased the hidden dimensions of the baseline model to approximate the parameter"}, {"title": "8 RESULTS & ANALYSIS", "content": "Here we present results of our benchmark evaluations for the CCellBio, COD10K-V2, and Brain Tumor and RSNA Pneumonia datasets (Tables 3, 4, 5, and 7 respectively). We begin with a summary of the findings discussed in this section. ACP and CAP enhanced interaction components:\nImprove both mAP and AR metrics across five challenging detection datasets. (Tables 3, 4, 5, 6, and 7) and are more effective in improving mAP than AR (Figures 7 and 6).\nImprove feature representation and reduce over-smoothing. by allowing features to interact both locally and globally evidenced by"}, {"title": "8.1 Quantitative Benchmark Dataset Analysis", "content": "CCellBio: The evaluation on the CCellBio dataset, which focuses on cancer tumor detection in stained tissue scans, demonstrates consistent improvements with the enhanced interaction architectures across most metrics. EI-VIT outperformed the baseline with a 9.14% increase in mAP, a 5.69% improvement in mAP50, a 14.58% improvement in mAP75, and a 5.71% improvement in AR. Similarly, EI-DAT achieved a 1.92% increase in mAP, a notable 3.49% improvement in mAP50, and a 1.50% improvement in AR. EI-SWIN consistently outperformed the baseline across all metrics. It demonstrated a 7.85% increase in mAP, a 6.21% improvement in mAP50, a 14.65% increase in mAP75, and a 5.50% improvement in AR. These results confirm the generalizability and effectiveness of the proposed interaction modules across different backbone architectures and underscore their potential for improving cancer tumor detection in medical imaging applications.\nCOD10K-V3: Enhanced interaction architectures consistently outperformed their respective baselines across all metrics on the COD10K-V3 dataset. EI-VIT achieved a significant improvement with a 21.05% increase in mAP, a 17.45% increase in mAP50, and a 166.67% increase in mAP75. However, there was a 0.36% decline in relative performance compared to the baseline VIT model due to challenges processing deformable points which we discuss in Section 10. EI-DAT showed a 15.15% improvement in mAP, a 9.66% improvement in mAP50, a 42.86% increase in mAP75, and a 2.99% gain in AR, AR300, and AR1000. The most notable performance gain was observed with EI-SWIN which achieved a 103.03% improvement in mAP, a 73.85% increase in mAP50, a 375.00% gain in mAP75, and a 24.91% increase across AR. Thus we improve AR and precision at higher IoU thresholds (mAP75), which are crucial for detecting concealed objects in challenging scenarios such as those in the COD10K-V2.\nBrain Tumor: The enhanced architectures outperform the baseline on all mAP and AR metrics for brain tumor detection as shown in Table 5. Specifically, ViT shows relative performance gains of 21.73%, 13.93%, 37.07%, and 7.84% for mAP, mAP50, mAP75, and AR, respectively. The DAT model also shows positive improvements with 2.71%, 1.8%, 5.68%, and 0.76% gains. The Swin Transformer demonstrates the largest improvements, with gains of 63.92%, 46.64%, 87.13%, and 15.65% for mAP, mAP50, mAP75, and AR, respectively."}, {"title": "8.2 Qualitative Feature and Attention Analysis:", "content": "Feature Analysis:. We conducted a detailed analysis of the feature maps generated by the baseline ViT architecture and its enhanced counterpart with our interaction modules on feature representations within the CCellBio dataset to evaluate the impact of our proposed enhancements. Using Principal Component Analysis (PCA), we analyzed the output feature maps from four transformer blocks of both the baseline ViT and EI-ViT model, shown in Figure 8. These Transformer stages are designed to compute features at different spatial resolutions, providing multi-scale feature representations. Note that in each stage, there are multiple transformer blocks. In our setup, we decided to use the plain ViT architecture which contains 3 blocks per stage."}, {"title": "9 ABLATION STUDY", "content": "To understand the individual contributions of aggressive convolutional pooling and conceptual attention transformation in enhancing the model\u0160s detection capabilities, we evaluate the model's performance using each component in isolation, We assess how the interactions facilitated by each influence overall performance. Additionally, we conduct experiments by varying the number of convolutional pooling blocks and the number of concepts independently. In this experiment, we focus on the ViT architecture, as it represents the foundational transformer model, and evaluate its performance on the CCellBio dataset. Using ViT as the baseline provides a clear framework for comparison, allowing us to assess the effects of the introduced components within a well-established context.\nIsolation Assessment. We begin by training the ViT model with either the ACP layer or the CAT independently to assess the contribution of each component to the overall performance. The recorded benchmarks are presented in Figure 12, which shows the performances for mAP50, mAP75, and AR for the baseline ViT, EI-ViT, and ViT models with isolated ACP and CAT components.\nOur observations reveal that removing the aggressive convolutional pooling results in a 5.56% increase in mAP50 and a 5.07% increase in AR compared to the baseline ViT. However, compared to the EI-ViT model, this leads to a reduction of 0.13% in mAP50 and 0.65% in AR. Removing the conceptual attention transformer has a more noticeable impact, reducing mAP50 by 1.22% and AR by 1.06% compared to the EI-ViT model. When only the ACP layer is included, the mAP75 metric improves by 17.36%. Similarly, including only the CAT results in a 17.01% improvement in mAP75 compared to the baseline, outperforming the EI-ViT architecture as well. This is because competing features for larger objects extracted by the convolution and attention mechanisms when both components are present make it more challenging for the network to perform well on the mAP75 metric. Additionally, we conducted extra epoch training with both the ViT-CAT and ViT-ACP models. While the performance of the isolated aggressive convolutional pooling ViT-ACP did not improve further, we observed that the isolated ViT-CAT continued to lower losses and gained extra points in the benchmark, even outperforming the EI-ViT for the CCellBio dataset across all metrics. Specifically, there was a 1.83% improvement in mAP, 0.13% in mAP50, 4.55% in mAP75, and 0.20% in AR when compared to the EI-ViT, which contains both the convolutional pooling and conceptual attention transformer layers. Our studies reveal that at lower epoch training, the utilization of the ACP component helps the network converge faster and achieve high KPIs. However, when training for more epochs, the CAT component can learn more relevant features, removing the need for the ACP component and achieving the higher KPIs."}, {"title": "Aggressive Convolutional Pooling", "content": "We explore the impact of varying the number of convolutional pooling layers on overall network performance. This technique is incorporated into the baseline ViT architecture, and the network's performance is evaluated using AR and mAP at two different thresholds, mAP50 and mAP75. The results shown in Figure 14 reveal a clear relationship between the number of convolutional pooling layers and the network's performance. When aggressive convolutional pooling is applied, there is a noticeable improvement in the mAP50 and AR, particularly when the number of pooling layers is kept relatively low. When the number of pooling layers is limited to four, the network improves its ability to locate objects and recall true positives. An intriguing observation shows that the mAP75 metric performance drops below the baseline when the number of layers is set to 1 or 7, suggesting that too few or too many pooling layers hinder the model\u0160s ability to handle object localization at higher thresholds.\nThe best performance across all three metrics is achieved when the number of pooling layers is set to two. With this configuration, the network shows a relative improvement of 4.47%, 17.01%, and 4.23% in mAP50, mAP75, and AR, respectively. This indicates that the optimal number of convolutional pooling layers lies in a balanced configuration, where pooling is applied strategically to extract essential features without overly distorting the network\u0161s capacity to retain fine-grain details.\nDespite the overall improvements seen with the introduction of aggressive convolutional pooling, performance peaks at two layers. Adding additional layers beyond this point results in diminishing returns. In fact, when more than two pooling layers are applied, the network\u0161s performance begins to decline. We hypothesize that this decline in performance is due to the aggressive application of pooling operations after every convolutional layer. This rapid pooling may cause the network to discard or overly simplify important spatial information, making it harder for the model to preserve the contextual details necessary for effective object detection and localization. Even with the use of residual connections, the frequent pooling operations may hinder the network's ability to learn discriminative features and retain relevant information across deeper layers. While aggressive convolutional pooling can improve the performance of the ViT architecture in certain cases, it is crucial to find the optimal balance between pooling layers. These findings underscore the importance of carefully controlling the amount of pooling applied, as it can have a substantial impact on the network's ability to generalize and perform well on object detection tasks."}, {"title": "Conceptual Attention Transformation", "content": "To evaluate the influence of the number of concepts on performance, we conduct a comprehensive analysis using the baseline ViT architecture, enhanced solely by the CAT layer. Performance metrics include mAP50, mAP75, and AR, with the number of concepts systematically varied from 32 to 512. The results are presented in Figure 13. All three metrics (mAP50, mAP75, and AR) exhibit consistent and positive correlations between the number of concepts performance. As the number of concepts increase, the network shows a marked improvement in its ability to detect objects, with particular gains in recognizing smaller or more localized objects that require finer-grained attention. For mAP50, the performance boost is the most notable, with a relative improvement of up to 6.37% when the number of concepts is increased to 512. This suggests that a larger pool of concepts enables the model to capture more detailed objects features within the scene, leading to better localization and overall improved detection accuracy.\nA similar pattern is observed for the mAP75 and AR metrics. At 512 concepts, the network demonstrates relative performance gains of 16.32% and 4.12%, respectively, indicating that a comprehensive conceptual representation, provided by a higher number of concepts, yields more accurate results at higher intersection over Union (IoU) thresholds and improves recall. These performance gains reflect the model's ability to maintain high precision while simultaneously improving its recall, especially in cases where the object boundaries are more challenging to detect, or where a more comprehensive understanding of the object context is required."}, {"title": "10 LIMITATIONS", "content": "In this section, we discuss limitations of our approach. We observed that the incorporation of the enhanced interaction mechanism may hinder the EI-DAT model's ability to effectively learn deformable points. In this case, the query matrix is designed to learn and generate deformable points. These offset points are then used to interpolate feature maps. However, additional complexities added by the model enhancement make it difficult to compute queries accurately. This causes a decline in model performance that ultimately impacts its mAP performance as it struggles to focus on relevant regions."}, {"title": "11 CONCLUSION", "content": "In this work, we introduced an enhanced interaction modeling approach for the vision-transformer backbone in object detectors. Our findings demonstrate that enabling interaction prior to self-attention improves performance across multiple challenging medical and concealed object detection datasets and for a diverse set of evaluation metrics. We showed that incorporating our aggressive attention pooling and conceptual attention transformation alters self-attention behavior by allowing it to learn more distinct features and attention maps when compared to original baseline models. The modified models distinguish and represent these features and attention maps more clearly in the feature space, even when objects of different classes have similar visual appearances. Such interactions"}]}