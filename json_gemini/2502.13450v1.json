{"title": "Interleaved Gibbs Diffusion for Constrained Generation", "authors": ["Gautham Govind Anil", "Sachin Yadav", "Dheeraj Nagaraj", "Karthikyan Shanmugam", "Prateek Jain"], "abstract": "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling framework for mixed continuous-discrete data, focusing on constrained generation problems. Prior works on discrete and continuous-discrete diffusion models assume factorized denoising distribution for fast generation, which can hinder the modeling of strong dependencies between random variables encountered in constrained generation. IGD moves beyond this by interleaving continuous and discrete denoising algorithms via a discrete time Gibbs sampling type Markov chain. IGD provides flexibility in the choice of denoisers, allows conditional generation via state-space doubling and inference time scaling via the ReDeNoise method. Empirical evaluations on three challenging tasks-solving 3-SAT, generating molecule structures, and generating layouts-demonstrate state-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT out of the box and achieves state-of-the-art results in molecule generation without relying on equivariant diffusion or domain-specific architectures. We explore a wide range of modeling, and interleaving strategies along with hyperparameters in each of these problems.", "sections": [{"title": "1. Introduction", "content": "Autoregressive models have been highly successful at modeling languages in a token by token fashion. While finetuned autoregressive (AR) models can produce realistic texts and maintain lengthy human-like conversations, they are known to fail at simple planning and reasoning tasks. One hypothesis is that AR generation is not suited for generating tokens where non-trivial constraints have to be satisfied. There have been efforts such as Chain-of-Thought prompting (Wei et al., 2022) and O1 (OpenAI, 2024) which force the model to \"think over\" the solution in many steps before answering.\nDiffusion models, another class of generative models, start with pure noise and slowly denoise to obtain a sample from the desired distribution (Ho et al., 2020; Song et al., 2020). While its outstanding applications have been in the context of generating images (i.e, continuous data) (Saharia et al., 2022; Rombach et al., 2022), it has been successfully extended to discrete data (Austin et al., 2021; Lou et al., 2023). This model has shown promising results in planning and constrained generation in a wide range of tasks, such as layout generation, molecule generation, 3SAT, SuDoKu (Ye et al., 2024) and traveling salesman problem (Zhang et al., 2024), outperforming AR models. This is attributed to diffusion models being able to parse the entire set of generated tokens multiple times during denoising.\nAlgorithms based on D3PM (Austin et al., 2021) as presented in prior works (Inoue et al., 2023; Ye et al., 2024) and mixed mode diffusion based works such as (Hua et al., 2024) assume that the denoising process samples from a product distribution of the tokens, which seems unreasonable in cases of constrained generation where the tokens can be highly dependent. It would be desirable if partial denoising of a token (continuous or discrete) is dependent on current denoised status of all other tokens. Alternative proposals such as Concrete Score Matching (Meng et al., 2022), SEDD (Lou et al., 2023), symmetric diffusion ((Zhang et al., 2024)) and Glauber Generative Model (GGM) (Varma et al., 2024) do not assume such a factorization. Symmetric diffusion considers the special case of generating permutations using riffle shuffle as the noising process and derives algorithms for denoising it exactly. This demonstrates gains in a variety of planning problems. GGM is a discrete diffusion model which denoises a lazy random walk exactly by learning to solve a class of binary classification problems.\nGibbs Sampler is a Markov chain which samples jointly distributed random variables by resampling one co-ordinate at a time from the accurate conditional distribution. This has been studied widely in Theoretical Computer Science, Statistical Physics, Bayesian Inference and Probability Theory (Geman & Geman, 1984; Turchin, 1971; Gelfand & Smith, 1990; Martinelli, 1999; Levin & Peres, 2017). While the original form gives a Markov Chain Monte Carlo (MCMC) algorithm, (Varma et al., 2024) considered a learned, time dependent, systematic scan variant of the Gibbs sampler for generative modeling over discrete spaces.\nIn this work, we extend the principle of time dependent"}, {"title": "2. Preliminaries", "content": "Notation Let $\\mathcal{X}$ be a finite set, let $L$ be the sequence length such that $L = L_1 + L_2$, $L_1, L_2 \\in \\mathbb{N} \\cup \\{0\\}$. Let $d_{L_1+1},...,d_L \\in \\mathbb{N}$ be the continuous dimensions. We let"}, {"title": "3. Interleaved Gibbs Diffusion", "content": "We now describe the Interleaved Gibbs Diffusion (IGD) framework for sampling from a target distribtuion $\\pi$ over $\\mathcal{S}_L$, given access to discrete and continuous denoisers which satisfy certain properties. We first describe the forward noising process and then the reverse denoising process using the given denoisers. In IGD, both the forward noising and reverse denoising processes operate one element at a time. Our noising process is illustrated in Figure 1."}, {"title": "3.1. Forward Noising Process", "content": "The forward noising process takes a sample $s$ from the target distribution $\\pi$ and applies a discrete time Markov chain to obtain the trajectory $s^{(0)}, s^{(1)}, ..., s^{(T)}$, where $T$ is the total number of timesteps. We refer to $t$ as the sequence time. Note that $s^{(0)} = s$. For each $t$, we choose a position $i_t \\in \\{1,2, ..., L\\}$ to be noised at sequence time $t$. In this work, we choose $i_t$ in a round-robin fashion from some permutation of $\\{1, 2, ..., L\\}$ so that all positions are noised exactly once after every $L$ sequence timesteps; we call this permutation the interleaving pattern. Given $i_t$, the corresponding sequence element $s_{i_t}$ can either be discrete"}, {"title": "Discrete Noising", "content": "If $s_{i_t}$ is discrete (i.e, $i_t \\leq L_1$), following (Varma et al., 2024), we consider token $\\phi \\notin \\mathcal{X}$ and define a probability distribution $\\Pi_t$ over $\\mathcal{X} \\cup \\{\\phi\\}$. Note that $\\Pi_t$ depends on the sequence time $t$. We refer to $\\Pi_t$ as the discrete noise schedule. Then the discrete noising process is as follows:\nSample $z_t \\sim \\Pi_t$ independent of $s_{i_t}^{(t)}$. Then we have:\n\\[s_j^{(t+1)} = \\begin{cases} z_t, & \\text{if } j = i_t \\text{ and } z_t \\neq \\phi \\\\ s_j^{(t)}, & \\text{otherwise} \\end{cases}\\]"}, {"title": "Continuous Noising", "content": "If $s_{i_t}$ is continuous (i.e, $L_1 < i_t < L_2$), we use $m_{i_t}$ to denote the number of times position $i_t$ has been visited by sequence time $t$ (including the visit at $t$). Let $\\overline{m} = \\text{maxim}_{i_t}$. Define $[\\beta_j]_{j=1}^{\\overline{m}}$ to be a monotonically increasing sequence, which we refer to as the continuous noise schedule. Then, the continuous noising process is given by:\n\\[s_{i_t}^{(t+1)} = (\\sqrt{1 - \\beta_{m_{i_t}}}) s_{i_t}^{(t)} + (\\sqrt{\\beta_{m_{i_t}}}) \\epsilon^{(t)}\\]\nwhere $\\epsilon^{(t)} \\sim \\mathcal{N}(0, I)$. Note that $s_j^{(t+1)} = s_j^{(t)} \\forall j \\neq i_t$."}, {"title": "Co-ordinate wise independent noising", "content": "The noising process of any element $s_{i_t}$ at any time $t$ is independent of other elements; this allows us to sample $s^{(t)}$ at any time $t$ directly from $s^{(0)}$ without having to compute $s^{(1)}, s^{(2)}, ..., s^{(t)}$ sequentially (Algorithm given in Appendix C)."}, {"title": "3.2. Reverse Denoising Process", "content": "The reverse denoising process takes a sample $\\hat{s}^{(T)}$ from $P_T$ as the input and applies a discrete time Markov chain to obtain the trajectory $\\hat{s}^{(T)}, \\hat{s}^{(T-1)}, ..., \\hat{s}^{(0)}$, where $T$ is the total number of sequence timesteps. Recall that $i_t$ denotes the position which was noised at time $t$ during the forward process.\nGiven $\\hat{s}^{(t+1)}$, we set $\\hat{s}_{-i_t}^{(t)} = \\hat{s}_{-i_t}^{(t+1)}$. Depending on whether $\\hat{s}_{i_t}^{(t+1)}$ is discrete (resp. continuous) we use the discrete denoiser (resp. continuous denoiser) to samples $\\hat{s}_{i_t}^{(t)}$ ($s_{i_t}^{(t+1)}$ is the sample from the forward process at time $t + 1$):\nDiscrete Denoiser is a (learned) sampling algorithm which can sample from $P_{t,i_t} (\\cdot | s)$, a probability distribution over"}, {"title": "3.3. ReDeNoise Algorithm", "content": "Inspired by (Meng et al., 2021), we now propose a simple but effective mechanism for quality improvements at inference time. Given a sample obtained through complete reverse process $\\hat{s}^{(0)}$, we now repeat the following two steps $N_R$ times: (1) Noise $\\hat{s}^{(0)}$ for $T_R$ rounds to obtain $\\hat{s}^{(T_R)}$. (2) Denoise $\\hat{s}^{(T_R)}$ back to $\\hat{s}^{(0)}$. While $N_R$ decides the number of times the noise-denoise process is repeated, $T_R$ decides how much noising is done each time. These are hyperparameters which can be tuned to suit the task at hand."}, {"title": "3.4. Conditional Generation", "content": "We train the model for conditional generation - i.e, generate a subset of the co-ordinates conditioned on the rest. We adopt the state-space doubling strategy, inspired by (Levi et al., 2023). A binary mask vector is created indicating whether each element in the sequence is part of the conditioning or not; for vectors in $\\mathbb{R}^{d_i}$, a mask is created for each element in the vector. The mask is now embedded/projected and added to the discrete/continuous embedding and fed into the model while training. Further, during the forward and reverse processes, the conditioned elements are not noised/denoised."}, {"title": "4. Training the Denoisers", "content": "Having established the IGD framework, we now describe strategies to train the discrete and continuous denoisers, which have been black boxes in our discussion so far."}, {"title": "4.1. Training the Discrete Denoiser", "content": "Throughout this subsection, we use $g_\\theta$ to denote a parameterised neural network which is trained to be the discrete denoiser. $g_\\theta$ takes input from the space $\\mathcal{S}_L \\times \\{0, 1, ...,T-1\\}$ and outputs logits in the space $[0, 1]^{|\\mathcal{X}|}$. We now describe two strategies to train $g_\\theta$:"}, {"title": "4.1.1. |X|-ARY CLASSIFICATION", "content": "In this approach, the objective is to learn $\\mathbb{P}\\big(s_{i_t}^{(t)} = \\cdot |s^{(t+1)} = s^{(t)}\\big)$. So, we directly train the model to predict $s_{i_t}^{(t)}$ given $s^{(t+1)}$. Since there are $|\\mathcal{X}|$ discrete tokens in the vocabulary, this is a $|\\mathcal{X}|$-ary classification problem, where the input is $s^{(t+1)}$ and the corresponding label is $s_{i_t}^{(t)}$. Hence, we minimize the cross-entropy loss: $\\mathcal{L}_{CE}(\\theta; s^{(t+1)},t) = -\\log \\big(g_{\\theta, s_{i_t}^{(t)}} (s^{(t+1)},t)\\big)$ where $g_{\\theta, x}(\\cdot)$ denotes the logit corresponding to token $s_{i_t}^{(t)}$."}, {"title": "4.1.2. BINARY CLASSIFICATION", "content": "In this approach, the objective is to learn $\\mathbb{P}\\big(s_{i_t}^{(t)} = x |s^{(t+1)} = s^{(t)}\\big)$. We adapt Lemma 3.1 from (Varma et al., 2024) to simplify this objective:\nLemma 4.1. Let $s \\in \\mathcal{S}_L$. Then, for $x \\in \\mathcal{X}$ and discrete $s_{i_t}^{(t)}$, we can write $\\mathbb{P}\\big(s_{i_t}^{(t)} = x |s^{(t+1)} = s^{(t)}\\big)$ as:\n\\[ \\frac{\\mathbb{P}(z_t = x)}{\\mathbb{P}(z_t = \\phi)} = \\frac{\\mathbb{P}(z_t = x | s^{(t+1)} = s_{i_t}^{(t+1)}, s_{-i_t}^{(t+1)} = s_{-i_t}^{(t)})}{\\mathbb{P}(z_t = \\phi | s^{(t+1)} = s_{i_t}^{(t+1)}, s_{-i_t}^{(t+1)} = s_{-i_t}^{(t)})} - 1,\\]\nwhere $(s^{(0)},...s^{(T)},)$ is obtained from forward process.\nHence, it is sufficient for the model to learn $\\mathbb{P}\\big(z_t = x | s^{(t+1)} = s_{i_t}^{(t+1)}, s_{-i_t}^{(t+1)} = s_{-i_t}^{(t)}\\big)$ for all $x \\in \\mathcal{X}$."}, {"title": "4.2. Training the Continuous Denoiser", "content": "In continuous diffusion, the noising (and denoising) process follows an SDE; the entire process happens in an uninterrupted fashion. However, in IGD, the noising and denoising happen with interruptions, because of the sequential nature. Thus, in the reverse process, the conditioning surrounding a continuous element changes every time it is picked for denoising. By an adaptation of the standard Tweedie's formula and exploiting the fact that forward noising process for every element is independent of other elements, we show that using the current conditioning and estimating the cumulative noise added from the beginning (across interruptions) still reverses the continuous elements in an interleaved manner. This is the novelty behind Lemma 4.2.\nSuppose we are given a sample $s^{(t)}$ from the distribution at time $t$. Let $\\doteq$ denote equality in distribution. Suppose $x_0 = s_{i_t}^{(t)} \\in \\mathbb{R}^{d_{i_t}}$ and consider the Orstein-Uhlenbeck Process $dx = -x \\cdot d\\tau + \\sqrt{2}dB_\\tau$, with standard Brownian motion $B_\\tau$. Then $x_{T_0} \\doteq s_{i_t}^{(t+1)} | s_{i_t}^{(t)}$ whenever $T_0 = \\sum_{m} \\frac{1}{2} \\log \\frac{1}{1-\\beta_{m_t}}$. Based on the observations in (Song et al., 2020; Ho et al., 2020), the reverse SDE given by\n\\[dx_{\\text{rev}} = x_{\\theta} d\\tau + \\big(2\\nabla \\log q_{T_0 - \\tau} (x_{\\text{rev}} | s^{(t+1)})\\big) + \\sqrt{2}dB_\\tau^{(1)}\\]\nis such that if $x_{\\text{rev}}^0 \\doteq s_{i_t}^{(t+1)}$ then $p(s_{i_t}^{(t)} = \\cdot| s_{i_t}^{(t+1)}) \\doteq p(x_{\\text{rev}}^T | s_{i_t}^{(t+1)})$ where $q_{T_0 + \\tau} (\\cdot | s^{(t+1)})$ is the conditional density function of $x_\\tau$.\nWe use DDPM (Ho et al., 2020) to sample from $\\mathbb{P}(s_{i_t}^{(t)} = \\cdot | s_{i_t}^{(t+1)})$ by learning the score function $\\nabla \\log q_{T_0 + \\tau} (\\cdot | s^{(t+1)})$ and then discretizing the reverse SDE in Equation (1).\nTo obtain a more precise discretization, we divide the noising at sequence timestep $t$ into $K_{i_t}$ element timesteps (whenever $s_{i_t}$ is a continuous vector). We define $s_{i_t,0}^{(t)} = s_{i_t}^{(t+1)}$, and for $k \\in [0, 1, ..., K_{i_t} - 1]$:\n\\[ s_{i_t,k+1}^{(t)} \\sim \\mathcal{N} \\Big((\\sqrt{1 - \\beta(t, k)}) s_{i_t,k}^{(t)}, (\\beta(t, k)) I \\Big)\\]\nwhere $\\beta$ is a continuous noise schedule which outputs a scalar given $(t, k)$ as input. Following the popular DDPM"}, {"title": "5. Model Architecture", "content": "Inspired by (Peebles & Xie, 2023), we use a transformer-based architecture closely resembling Diffusion Transformers (DiTs) for the model. Since DiT has been designed for handling discrete tokens, we modify the architecture slightly to accommodate continuous vectors as well. However, we keep modifications to a minimum, so that the proposed architecture can still benefit from DiT design principles. Our proposed architecture, which we refer to as Discrete-Continuous (Dis-Co) DiT, is illustrated in Figure 2."}, {"title": "6. Experiments", "content": "We evaluate the IGD framework on three different tasks: Layout Generation, Molecule Generation and the Boolean Satisfiability problem. While the first two tasks involve generating both discrete tokens and continuous vectors, 3SAT involves only discrete tokens. Nevertheless, all three problems are constrained generation problems and can hence benefit from the exact reversal of IGD framework."}, {"title": "6.1. Layout Generation", "content": "Layout generation aims to generate coherent arrangements of UI elements (e.g., buttons, text blocks) or document components (e.g., titles, figures, tables) that satisfy both functional requirements and aesthetic principles. This problem is important in various applications of graphic design and interface prototyping.\nFormally, each layout is a set of $N$ elements $\\{e_i\\}_{i=1}^N$. Each element $e_i$ is represented by a discrete category $t_i \\in \\mathcal{N}$ and a continuous bounding box vector $p_i \\in \\mathbb{R}^4$. We use the parameterization $p_i = [x_i, y_i, l_i, w_i]$, where $(x_i, y_i)$ represents the upper-left corner of the bounding box, and $(l_i, w_i)$ its length and width, respectively."}, {"title": "6.2. Molecule Generation", "content": "Molecule generation aims to synthesize new valid molecular structures from a distribution learned through samples. Recently, generative models trained on large datasets of valid molecules have gained traction. In particular, diffusion-based methods have shown strong capabilities in generating discrete atomic types and their corresponding 3D positions.\nWe represent a molecule with $n$ atoms by $(z_i, p_i)_{i=1}^n$, where $z_i \\in \\mathbb{N}$ is the atom's atomic number and $p_i \\in \\mathbb{R}^3$ is the position. We focus on organic molecules with covalent bonding, where bond orders (single, double, triple, or no bond) between atoms are assigned using a distance-based lookup table following (Hoogeboom et al., 2022)."}, {"title": "6.3. Boolean Satisfiability Problem", "content": "The Boolean Satisfiability (SAT) problem is the task of determining whether there exists a binary assignment to"}, {"title": "A. Proofs", "content": ""}, {"title": "A.1. Lemma 3.1", "content": "Statement:\nDenote the distribution of $s^{(t)}$ by $P_t$. Suppose $\\Pi_t(\\cdot|\\mathcal{X}) = \\Pi(\\cdot|\\mathcal{X})$ for all $t$, $\\Pi_t(\\phi) \\leq 1 - \\epsilon$ for some $\\epsilon > 0$ and $\\lim_{T\\rightarrow\\infty} \\sum_j \\log (1 - \\beta_j) = -\\infty$. As $T \\rightarrow \\infty$, $P_T$ converges to the product distribution: $\\Pi (\\cdot|\\mathcal{X})^{\\mathbb{1} \\times L_1+1} \\mathcal{N} (0, I_{d_1})$.\nProof:\nWe closely follow the proof of Lemma 1 in (Varma et al., 2024).\nNote that the forward noising for each element is independent of all other elements. Hence, it suffices to consider the noising of each element separately.\nConsider a discrete element. By assumption, the probability of not choosing $\\phi$:\n\\[1 - \\Pi(\\phi) \\geq \\epsilon\\]\nwhere $\\epsilon > 0$ for all. Further, when $\\phi$ is not chosen at time t, then the distribution of the discrete token is $\\Pi(\\cdot|\\mathcal{X})$ for all time > t independent of other tokens. The probability of choosing only $\\phi$ until time t is at most $(1 - \\epsilon)^t$ and this goes to 0 as t $\\rightarrow \\infty$. Therefore with probability 1, asymptotically, every discrete element is noised to the distribution $\\Pi (\\cdot|\\mathcal{X})$.\nConsider a continuous vector at position i. From the definition of the forward process, we have:\n\\[s_{i_t}^{(t+1)} = (\\sqrt{1 - \\beta_{m_{i_t}}}) s_{i_t}^{(t)} + (\\sqrt{\\beta_{m_{i_t}}}) \\epsilon^{(t)}\\]\nwhere $\\epsilon \\sim \\mathcal{N}(0, I)$. Merging the Gaussians, we have:\n\\[s_{i_t}^{(t+1)} = (\\sqrt{\\tilde{\\alpha}_{m_t}}) s_{i_t}^{(0)} + (\\sqrt{1 - \\tilde{\\alpha}_{m_t}}) \\epsilon\\]\nwhere:\n\\[\\tilde{\\alpha}_{m_t} = \\prod_{j=0}^{m} (1 - \\beta_j)\\]\nSince $m_i$ denotes the number of times the position i is visited by sequence time t, $m_i \\rightarrow \\infty$ as t $\\rightarrow \\infty$. Hence, from the assumption $\\lim_{T\\rightarrow\\infty} \\sum_j \\log (1 - \\beta_j) = -\\infty$, we have $\\lim_{t\\rightarrow\\infty} \\tilde{\\alpha}_{m_t} = 0$ and hence the continuous vector will converge to an independent Gaussian with variance 1 per continuous dimension."}, {"title": "A.2. Lemma 3.2", "content": "Statement:\nAssume $\\hat{s}^{(T)} \\sim P_T$ and assume we have access to ideal discrete and continuous denoisers. Then, $\\hat{s}^{(0)}$ obtained after $T$ steps of reverse denoising process, will be such that $\\hat{s}^{(0)} \\sim \\pi$.\nProof:\nRecall that $s^{(t)} \\in \\mathcal{S}_L$ denotes the the sequence at sequence time $t$ of the forward process. Further, $P_t$ denotes the probability measure of $s^{(t)}$ over $\\mathcal{S}_L$. $\\hat{s}^{(t)} \\in \\mathcal{S}_L$ denotes the the sequence at sequence time $t$ of the reverse process and let $\\hat{P}_t$ denote the probability measure of $\\hat{s}^{(t)}$ over $\\mathcal{S}_L$.\nWe now prove the lemma by induction. Assume that $\\hat{s}^{(t+1)} \\overset{d}{\\sim} s^{(t+1)}$, i.e., $\\hat{P}_{t+1} = P_{t+1}$. Consider a measurable set $A$ such that $A \\subseteq \\mathcal{S}_L$. Let $y \\sim P_{t+1}$. From the measure decomposition theorem, we have:\n\\[\\mathbb{P}(\\hat{s}^{(t)} \\in A) = \\int_y \\mathbb{P} \\big(\\hat{s}^{(t)} \\in A| \\hat{s}^{(t+1)} = y\\big) dP_{t+1}(y)\\]\nFrom the induction assumption, we can rewrite this as:\n\\[\\mathbb{P}(\\hat{s}^{(t)} \\in A) = \\int_y \\mathbb{P} \\big(s^{(t)} \\in A| \\hat{s}^{(t+1)} = y\\big) d\\hat{P}_{t+1}(y)\\]\nFrom the definition of the reverse process, we know that $\\hat{s}_{-i_t}^{(t)} = \\hat{s}_{-i_t}^{(t+1)}$. Therefore, we have:\n\\[\\mathbb{P} \\big(\\hat{s}_{i_t}^{(t)} \\in A_{-i_t} (y_{-i_t}) |\\hat{s}^{(t+1)} = y\\big) = \\mathbb{P} \\big(s_{i_t}^{(t)} \\in A_{-i_t} (y_{-i_t}) |s^{(t+1)} = y\\big)\\]\nwhere $A_{-i_t} (y_{-i_t}) = \\{x_{i_t} : x \\in A, x_{-i_t} = y_{-i_t}\\}$. Depending on the reverse process chosen, we have:\nWe have:\nAnd hence:\nCase 2: $\\mathbb{P} \\big(\\hat{s}_{i_t}^{(t)} \\in A_{-i_t} (y_{-i_t}) |s^{(t+1)} = y\\big) = \\mathbb{P} \\big(s_{i_t}^{(t)} \\in A_{-i_t} (y_{-i_t}) |s^{(t+1)} = y\\big)$\nWe have:\nAnd hence:"}, {"title": "A.3. Lemma 4.2", "content": "Statement: Under the considered forward process where noising occurs independently, we have:\n\\[\\nabla_{s_{i_t}^{(t,k+1)}} \\log q(s_{i_t}^{(t,k+1)} | s_{i_t}^{(t,k+1)}) = \\frac{1}{\\sqrt{1 - \\alpha}} \\mathbb{E} [\\epsilon | s_{i_t}^{(t,k+1)}]\\]\nProof: Let us split $s_{i_t}^{(t,k+1)} = [s_{i_t}^{(t,k+1)} | s_{i_t}^{(t,k+1)}]$. Note that, $s_{i_t}^{(t,k+1)}$ is the continuous part that is being de noised.\n\\nabla log q(s,k+1)|s,k+1)) =\n\\[\\sum_{\\epsilon} \\frac{q(s_{i_t}^{(t,k+1)} | s_{i_t}^{(t,k+1)}) q(s_{i_t}^{(t,k+1)})}{\\int s_{i_t}^{(t,k+1)} q (s_{i_t}^{(t,k+1)}| s_{i_t}^{(t,k+1)}) q(s_{i_t}^{(t,k+1)})}\\]\n\\nabla+1)(+)()\n\\[\\sum_{\\epsilon} \\frac{q(s_{i_t}^{(t,k+1)} | s_{i_t}^{(t,k+1)}) q(s_{i_t}^{(t,k+1)})}{\\int s_{i_t}^{(t,k+1)} q (s_{i_t}^{(t,k+1)}| s_{i_t}^{(t,k+1)}) }\\]\n\\[\\sum_{\\epsilon} \\frac{\\int s_{i_t}^{(t,k+1)} q (s_{i_t}^{(t,k+1)}| s_{i_t}^{(0)})q(s_{i_t}^{(t,k+1)}| s_{i_t}^{(0)})q(s_{i_t}^{(0)})ds(0)}{q(s_{i_t}^{(t,k+1)}, s_{i_t}^{(t,k+1)})}\\]\n\\[\\sum_{\\epsilon} \\frac{q (s_{i_t}^{(t,k+1)}| s_{i_t}^{(0)})q(s_{i_t}^{(k+1)}| s_{i_t}^{(0)})q(s_{i_t}^{(0)})ds(0)}{q(s_{i_t}^{(t,k+1)}, s_{i_t}^{(t,k+1)})}\\]\n[0))ds(0) (0)q(s,k+1),,k+1)] =\n\\\\[\\mathbb{E} \\nabla {i}^{()|(+1) } ds(, )\\]\nq(), )\n\\[\\int {i}^{(, )-1} , (,1), () {i}(, 1)}\\]\nIn the RHS of the chain in (4), we observe that $\\hat{s}_{i_t}^{t,k+1}$ is being conditioned on and given $\\hat{s}_{i_t}^{t,k+1}$, $\\epsilon$ is a function only of $s_{i_t}^{(0)}$ from (2). Therefore, the above chain yields:\n\\[\\nabla_{s_{i_t}^{(t,k+1)}} \\log q(s_{i_t}^{(t,k+1)} | s_{i_t}^{(t,k+1)}) = \\frac{1}{\\sqrt{1 - \\alpha}} \\mathbb{E} [-\\epsilon | s_{i_t}^{(t,k+1)}]\\]\nThis is the exactly $\\frac{\\nabla_x}{9xy}$ if the estimator was a perfect MMSE estimator.\nJustifications:- (a) observe that conditioned on $s^{(0)}$, how it-th element is noised in the forward process is independent of all other elements. This gives rise to the conditional independence.(b) We exchange the integral and the $\\nabla$ operator. Let q(x|y) be conditionally Gaussian, i.e. $x|y \\sim \\mathcal{N}(\\sqrt{\\bar{\\alpha}} y; (1 - \\bar{\\alpha}))$, then it is a property of the conditional Gaussian random variable that $\\nabla_x q(x|y) = \\frac{(x - \\sqrt{\\bar{\\alpha}} y)}{(1 - \\bar{\\alpha})} * q(x|y)$. Taking x = $s_{i_t}^{t,k}$ and y = $s_{i_t}^{(0)}$ from (2), we see that:"}, {"title": "B. Connection between \u03b2, \u03b2 and \u0101", "content": "From subsection 3.1, we have:\n\\[s_{i_t}^{(t+1)} = (\\sqrt{1 - \\beta_{m_{i_t}}}) s_{i_t}^{(t)} + (\\sqrt{\\beta_{m_{i_t}}}) \\epsilon^{(t)}\\]\nAnd from subsection 4.2, we have:\n\\[ s_{i_t}^{(t,k+1)} = (\\sqrt{1 - \\beta(t, k)}) s_{i_t,k}^{(t)} + (\\sqrt{\\beta(t, k)}) \\epsilon \\]\nDefine $a(t, k) = 1 - \\beta(t, k)$. Then, we have:\n\\prod_1, () (()) =\n1 - (1)1 +\nwhere we have merged the Gaussians. Setting k = - 1 and comparing (6) and (7), we have:\n1-\nRecall from subsection 4.2 that:\n)+(v1- () e=\nAgain rewriting (7) by merging gaussians, we have:\n(71 45010-1 (1) +4-41 (+))\nComparing with (9), we have:"}, {"title": "C. Forward process: Generating s(t) directly", "content": "For training the model for denoising at sequence time t (and element time k if we are denoising a continuous vector)", "to": "n\u2022 $(s_{i_t}^{(t-1)}, s_{i_t}^{(t)})$ if $s_{i_t}^{(t)}$ is discrete\n\u2022 $(s_{i_t,k}^{(t)}, \\epsilon)$ if $s_{i_t,k}^{(t)}$ is continuous\nNote that $\\epsilon$ is as defined in (2). Once you have $s_{i_t}^{(t-1)}$, $s_{i_t}^{(t)}$ can be generated by applying one discrete noising step, by sampling $z"}]}