{"title": "\"It depends\": Configuring AI to Improve Clinical Usefulness Across Contexts", "authors": ["Hubert D. Zaj\u0105c", "Jorge M. N. Ribeiro", "Silvia Ingala", "Simona Gentile", "Ruth Wanjohi", "Samuel N. Gitau", "Jonathan F. Carlsen", "Michael B. Nielsen", "Tariq O. Andersen"], "abstract": "Artificial Intelligence (AI) repeatedly match or outperform radiologists in lab experiments. However, real-world implementations of radiological AI-based systems are found to provide little to no clinical value. This paper explores how to design AI for clinical usefulness in different contexts. We conducted 19 design sessions and design interventions with 13 radiologists from 7 clinical sites in Denmark and Kenya, based on three iterations of a functional AI-based prototype. Ten sociotechnical dependencies were identified as crucial for the design of AI in radiology. We conceptualised four technical dimensions that must be configured to the intended clinical context of use: Al functionality, AI medical focus, AI decision threshold, and AI Explainability. We present four design recommendations on how to address dependencies pertaining to the medical knowledge, clinic type, user expertise level, patient context, and user situation that condition the configuration of these technical dimensions.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) models repeatedly match or outright outperform radiologists in narrowly defined detection tasks [4, 53, 60, 63]. There are multiple studies claiming that AI-based systems enhance radiologists' work, either by increasing accuracy or reducing time spent on each examination [48]. These claims, however, are based on retrospective evaluations conducted in laboratory settings. When looking closer into the state of the art of clinician-facing AI, the claims of utility weaken [93]. For example, Roberts et al. [61] found that out of the 62 AI models detecting and predicting COVID-19 on chest X-rays and CT scans that were described in the literature, none were deemed to be useful for clinical purposes. Furthermore, evaluations of the handful of systems approved by the authorities in the United States and European Union [1] revealed that their clinical impact when integrated into practice remains mostly unclear [79, 84]. A similar study by Lehman et al. [47] showed no improvement in patient outcomes after the successful integration of an AI-based support tool for mammography screenings. Strohm et al. claimed that one of the primary causes of Al's lack of success in radiology until now is due to \"uncertain added value for clinical practice of AI applications\" [73]. What these studies show is that the clinical usefulness of hitherto AI-based support systems is limited. Researchers with diverse backgrounds (AI, Health, and Human-Computer Interaction (HCI)) investigated what makes AI-based support systems clinically useful. Based on the previous work, we define clinical usefulness as the overarching quality of AI-based support systems emerging from the interplay of their real-world performance, clinical efficacy, local applicability, and end-user acceptance in a situated clinical context for concrete end-users. First, robust performance in real-world settings is essential, as subpar performance has been found to increase workload and disrupt clinical routines [80, 82, 93]. Second, the evaluations, primarily assessing technical performance metrics through randomised clinical trials (RCTs), must encompass tangible clinical outcomes and patient benefits. Health researchers have been advocating for more flexible assessment methodologies aligned with the iterative nature of AI deployment [11, 42, 49]. Third, end-user acceptance, supported by qualities like trust and usability, emerges as pivotal for successful use in clinical practice [18, 19, 39]. Altogether, for an AI-based system to be clinically useful, it must perform well, benefit patients, and be accepted by clinical end-users working in different clinical contexts.\nIn this paper, we investigate how to design Al for clinical usefulness in different clinical contexts. This study was conducted as a part of a larger research and development project focused on innovating an AI-based system to assist examinations of chest X-rays in Denmark and Kenya. Here, we define innovation as the entirety of work conducted to create an AI-based system, from creating the datasets the Al is trained on through design and development to its integration and use in practice. We conducted 19 design sessions and design interventions (online and collocated) with 13 radiologists from 7 clinical sites in Denmark and Kenya. Throughout the design study, we explored a range of user interface mock-ups and three versions of a web-based prototype of an AI-based support system with prioritisation and decision-support functionalities."}, {"title": "2 RELATED WORK", "content": "The hitherto evidence of Al's positive influence on clinical practice is limited [51, 75, 80]. Research on the real-world effect of AI in healthcare tends to be discrete and focusing on confined goals [93]. However, to provide clinical value AI-based systems have to dovetail contributions from Human-Computer Interaction, AI, and Health into a cohesive vision [30, 82, 93].\nFirst, clinical usefulness necessitates robust performance [93]. This primarily has to be true in real-world settings, retrospective evaluations in lab environments do not speak to the final performance of a system. For example, in a real-world evaluation of an acclaimed ML model for detecting diabetic retinopathy, 21% of all cases were deemed ungradable [8]. Poor performance also leads to increased workload [57, 64, 82], additional time spent on discerning false positive predictions [67, 68], or breakages to work routines [31]. Van Leeuwen et al. [80] reported that out of 100 CE-approved radiological AI-based systems, 64 showed no peer-reviewed evidence of clinical efficacy. Most evidence for the remaining 36 systems focused on diagnostic accuracy, not real-world clinical outcomes.\nSecond, clinical usefulness necessitates clinical efficacy [42]. However, randomised clinical trial (RCT) - a focused, systematic, rigorous, and insulated method commonly used to evaluate the validity of clinical interventions independent of external confounders - is often following the traditional sequential paradigm of work characteristic for drug development [13]. In this tradition, the intervention is evaluated only when deemed complete [21]. When translating this mentality to AI-based systems, not only does it hinder innovation, but it also results in the evaluation of AI through the measure of technical performance [49]. While technical performance is the backbone of useful AI, clinical efficacy is not its immediate consequence [11, 69]. For example, Lehman et al. [47] conducted a prospective evaluation of a computer-aided detection system supporting mammography reporting. Researchers concluded that the"}, {"title": "2.2 System Configurability", "content": "Configurability has been long considered crucial to the appropriation of IT systems [26, 27, 45]. There are two types of configurability that should be explored in the context of this study: before-use and in-use [36].\nBefore-use configurability typically involves the active participation of end-users in the design processes, aiming to tailor systems to their specific needs and preferences [36]. Various methods and approaches have emerged to facilitate meaningful engagement with end-users, such as participatory design techniques [45]. Acquiring an understanding of work practices and work environment, but also technology aspects of a future system and changes it may introduce, is critical for developing systems that effectively respond to user needs [43]. This understanding enables developers and designers to implement systems that are not only technically sound but also contextually appropriate.\nHowever, according to Stewart and Williams [72], the paradigm of user-centred design does not properly answer the challenges of implementing useful systems. Rather, the final usefulness of a system is created iteratively through the acts of in-use configuration."}, {"title": "3 METHODOLOGY", "content": "In this paper, we explored how to design radiological AI-based systems for clinical usefulness across contexts. This study was part of a larger project set to design and develop an AI-based support tool for radiologists examining chest X-rays, funded by the Innovation Fund Denmark (0176-00013B). The project is a multidisciplinary collaboration between the Department of Computer Science at the"}, {"title": "3.1 Research Through Design: Design Interventions with Working Prototypes", "content": "To explore the clinical usefulness of AI in different radiology contexts, we undertook a research through design approach [96]. We conducted three iterations based on a series of design sessions and design interventions using mock-ups of user interfaces (Prototype I) and working prototypes (II and III) (Fig. 1, Fig. 2, and Fig. 3). The three iterations were determined by decisions to deploy major changes in the web-based prototypes, i.e. version 1-3, followed by gathering feedback from the participants. The design sessions were carried out both online and collocated with radiologists in hospital offices. During these sessions, we obtained medical domain knowledge, typically by clarifying questions about radiology work and X-rays, but we also collectively explored the design space through a range of mock-ups and prototypes. The design interventions were carried in-situ with the performative purpose of exploring how"}, {"title": "3.1.1 Prototypes.", "content": "As part of the greater project, a deep learning-based model was developed by machine learning engineers at Unumed ApS to detect selected radiological findings [92]. The AI model was developed using a convolutional neural network. The first prototype was merely a proof of concept, not designed to collect feedback from external domain experts. It was developed to guide future work in terms of model development and data labelling. However, inspired by earlier research [92, 93], we considered it an opportunity to engage in more concrete discussions on the merit of clinical usefulness with medical professionals at an early stage of the innovation work.\nThe second and third iteration of the prototype consisted of an interactive web application designed to emulate a DICOM viewer. The web application integrated with the AI model developed within the bigger project. This connection enabled us to work with real data and, thus, explore with fidelity the interactions of the radiologists with the system. For the design interventions, radiologists were given access to the prototype, either in-person or remotely. They were requested to choose the next examination to report, following their usual practice and using information displayed in the prototype. Then, they were asked to interpret the selected examination without the use of AI and with Al decision support. Moreover, they were asked to configure the AI tool using available options to fit their practice. Finally, they were encouraged to explore the prototype independently and interact with any element of the user interface."}, {"title": "3.2 Analysis Positionality", "content": "The data analysis was conducted by the first and last authors (HDZ and TOA) with backgrounds in Health Informatics, HCI, and AI (5+ & 15+ years of experience). Moreover, before the analysis of the data from the design sessions and design interventions, the two co-authors concluded extensive ethnographic investigations into the work practices of radiologists from the visited sites with a particular outlook on opportunities for AI support (described in a manuscript prepared for publication). First-hand experience with the work practices and similarities and differences across clinical settings informed the initial analysis of this data."}, {"title": "3.3 Data Analysis", "content": "We used reflective thematic analysis [17] to analyse collected data (transcriptions of the design interventions). The analysis took place in Dovetail - a web application for qualitative data analysis. Except for the transcription software, no AI-based analysis support was used in this study. The two authors familiarised themselves with the collected data after every iteration of the design sessions and design interventions when deciding on the next focus. Moreover, the two authors, prior to coding, based on their fieldwork experience (60+ hours) and a literature review [93], devised three bucket themes to support the later organisation of codes: type of clinical site, domain expertise of medical professionals, and patient and situational context. Additionally, a fourth residual category was added not to limit coding. Next, to test the bucket themes, the two authors coded one transcript each for any references to challenges, preferences, dependencies, and configurations in relation to AI and their clinical practice. After this test, the fourth bucket theme was renamed to technical dependencies. The first author coded the remaining transcripts following the same directions. The two authors met weekly to discuss the coverage of the coding and future conceptualisation of themes. The themes were created within their respective bucket themes based on their grounding in the clinical context. Importantly, the division of codes between the bucket themes was never final and was used only to support analysis of the significant amount of codes (n=260). Through discussion, reflection on data across the interventions, and fieldwork experience, the authors iteratively clarified themes and reorganised data, moving away from the original bucket themes (while maintaining their initial assignment known). This interpretative work was conducted twice, creating ten reflective themes. The ten themes were framed as dependencies conditioning four specific design decisions that formed an AI-based support design space."}, {"title": "4 CONFIGURING FOUR TECHNICAL DIMENSIONS OF CLINICALLY USEFUL RADIOLOGICAL AI", "content": "We identified ten dependencies that emerge from the social dimensions of clinical AI and condition the configuration of four technical dimensions of clinical AI for radiology (see Fig. 4). Each of the technical dimensions needs to be configured in relation to the local clinical context to achieve clinical usefulness. In this section, we will briefly explain the social dimensions of clinical AI to then explore in-depth the conceptualised dependencies."}, {"title": "4.1 Social Dimensions of Clinical AI", "content": "Medical knowledge. This dimension includes concepts and definitions relevant to the medical domain addressed by the innovated AI-based system, for example, the meaning of radiological findings detected by our Al-based system. Familiarity with them supports meaningful collaboration between designers, developers, and medical professionals and reduces the risk of incorrect assumptions throughout the innovation process.\nClinic type. This social dimension addresses types of clinical sites. Imaging clinics, general hospitals, and specialised hospitals provide unique healthcare services and, thus, cater to the needs of patients with different conditions. Moreover, the type of clinical site determines the available resources, the speciality of medical professionals working there, their workflows, and their goals.\nUser expertise level. All medical professionals have different domain expertise. This is evident when comparing junior to senior medical professionals. However, it was also observed between board-certified radiologists. The level of expertise also determines the workload and clinical responsibilities.\nPatient context. This context encompasses the current location of a patient (in or out of a hospital) and their medical history. Patients are the centre of medical work. Their health and well-being are the priority. Thus, by extension, any system supporting healthcare professionals should support patients and depend on their context.\nUser situation. This dimension pertains to the workload, available time, and resources of medical professionals. While the other four dependencies describe relatively stable medical practice, situational context introduces a temporal factor to the work done and may affect the priorities of medical professionals."}, {"title": "4.2 AI Functionality", "content": "Which Al functionality should the system provide? Answering this question defines this technical dimension. The functionalities explored during design interventions (prioritisation and decision support, see, Fig. 5) were linked to the AI model developed for the project this study was a part of. We explored the conditions for these functionalities to provide clinical value and propose a third functionality: quality assurance, which originated during the design interventions.\nDependency 1: AI functionality depends on clinic type. Each clinical site has different (1) positions within the healthcare system, (2) amounts of resources available, and (3) workloads related to the size of a clinic. This is why it is important to ensure that AI functionality is implemented in a way that makes sense for the clinical site in which it will be deployed.\nFirst, while every radiologist puts the well-being of their patients first, the healthcare systems that they are a part of operate under different incentives. Public and private clinics face different challenges and may require adjusted AI functionalities, for example, The number of cases in court, medical and legal cases, is way more than what you would get in the public sector. So from the medical director's office [point of view], they would want .... any small thing to be flagged so that we don't get into problems later... it would be different in K5, compared to the public sector, where even if you missed this, people are rarely taken to court but in a private setting... if they"}, {"title": "Dependency 2: AI functionality depends on user expertise level.", "content": "The value of support in detecting findings on a medical examination decreases with increasing experience. Instead, the assigned workload increases with seniority. Thus, prioritisation and quality assurance functionalities gain importance.\nRadiological AI-based decision support typically presents a list of findings detected on an examination accompanied by an X\u0391\u0399 visualisation, as also explored in our prototypes. While this mode of support seems straightforward, it misses the reality of clinical practice. Senior radiologists spend a very short time interpreting chest X-rays. To ask them to revisit every examination to discern the validity of AI predictions is wishful. However, when discussing the potential value of AI-based decision support, they focused on quality assurance. Thus, AI should be treated not as an all-knowing peer who is going to point out every finding on an examination but as a safety net that activates only in time of need. For example, It could read the text we write and say: \"Oh, you missed that.\" That could be good [I11, Senior, Imaging clinic, Denmark]. This way, the envisioned system would not require the mental effort and time to discern Al output but would inform a radiologist about potentially missed findings based on the report they were writing.\nOn the other hand, junior radiologists in clinical settings usually take significantly more time to report every examination. Moreover, all of their reports have to be confirmed by a senior colleague. For them, reporting serves as a primary learning exercise. In this context, they envisioned using AI support not as a quality assurance but as a new source of information used to draw their own conclusions. I would take a look at a chest X-ray, formulate my opinion, and then see what the AI says... If it agrees... good, if it disagrees or finds something that I hadn't, I'll examine it critically... I like getting almost overwhelmed by data, and I sort it out afterwards... [S14, Junior, Specialised hospital, Denmark]. These two perspectives highlight how workflow, workload, and the act of detecting findings on a medical examination changes with expertise. The educational value created for junior radiologists by verbose explanations of Al's predictions may become a burden for senior radiologists who expect minimal disruption to their existing workflows."}, {"title": "#1 Recommendation:", "content": "Enable users to select preferred AI functionality."}, {"title": "4.3 AI Medical Focus", "content": "Which radiological findings should the AI detect? This is where our participants, for the first time, responded, starting with \"It depends...\" (see Fig. 6). Let's explore how to ensure the detected findings are clinically useful in the real world.\nDependency 3: AI medical focus depends on clinic type. Different clinics take care of different types of patients suffering from different conditions. Types of patients seen in different clinical settings result in a local prevalence of observed radiological findings. As a result, a single fit-them-all system that detects an arbitrarily selected set of findings is not going to provide a similar quality of support across the different clinical contexts."}, {"title": "Dependency 4: AI medical focus depends on user expertise level.", "content": "Junior radiologists may interpret a single X-ray for up to tens of minutes. Whereas, according to our senior participants, interpreting a chest X-ray takes around 1 to 2 minutes. This means that with experience, many findings become \"obvious\" and are no feat to detect. When discussing the decision support functionality of our prototypes and previous systems that our participants had piloted, the common complaint related to the detection of \"obvious\" radiological findings, which took additional time to discern.\nIf it's an obvious finding, we'll see that one quickly, and we all agree on it. The problem comes when it's something more subtle [106, Senior, Big General Hospital, Kenya]. Detecting the difficult or \"subtle\" radiological findings is where the value lies for senior radiologists. However, the less experienced, the more support a radiologist may accept. This was captured by P01: Maybe it'll help the resident radiologist in the first or second year, but I don't think"}, {"title": "Dependency 5: AI medical focus depends on patient context.", "content": "Radiologists are not interpreting medical imaging to find every possible finding. Rather, they are interpreting them to help the ordering clinicians take action in patient management. Such actions usually occur when a new condition is being diagnosed, or a patient's health may be at risk. However, the clinical meaning of certain radiological findings depends on the location of a patient. This means that a finding may be expected when observed in an examination of a patient who is admitted to a hospital. Whereas the same finding observed in an examination of a patient who is not admitted to a hospital may warrant immediate action.\nOur participants stressed that useful prioritisation should consider patients' medical history to filter out already-known findings, which our prototype could not do. But how urgent is it? We know that pneumothorax has decreased. It's a big heart, but it's much smaller than it was a week ago. It has [pleural] effusion, but much, much less than it was a week ago. That's the thing we miss with this [I11, Senior, Specialised Hospital / Imaging Clinic, Denmark]. In this quote, P11 explains that the examination they looked at may not be urgent at all despite the fact that the AI correctly detected three findings, one of them (pneumothorax) being life-threatening. These findings would not be urgent if they were already known to the ordering clinician. In such a case, the patient would have already been undergoing treatment, and this examination's sole purpose was to control its progress. In specialised hospitals and bigger general hospitals, patients often have taken several X-rays to monitor the progress of treatment. This means that the same findings, but of different severity, will be visible on their examinations. The ability to assess the detected findings in the light of patient history is crucial to correctly prioritise findings that warrant clinical action.\nWhen looking at radiologists' work from the perspective of contributing to the broader clinical work, it is counterproductive to prioritise findings that clinicians taking care of a patient are already aware of. In other words, a radiological finding may be relevant to detect on examinations from patients who are not admitted to a hospital, but not so much for patients currently admitted. A senior radiologist explained, It depends on the findings, and it depends on the patient... some findings in the out-patients would be more important to be prioritised than if they're in-house. Because if they're in-house, then I would suspect that someone not from the radiology department would have looked at them. If it's out-patient, then nobody has looked at them... [I10, Senior, Specialised Hospital, Denmark]. Whereas, as explained by P10, patients referred from outside of a hospital are more likely to have conditions that their doctors are unaware of. Thus, the location of the patient is crucial to selecting which findings are relevant to receiving support from an AI-based system."}, {"title": "4.4 AI Decision Threshold", "content": "At what certainty level should the AI inform a user about detected findings? Specifying when a radiological AI-based system should inform a user about a finding is usually done by specifying a decision threshold (see Fig. 7). Selecting a specific threshold value determines the measured performance of an Al model captured by evaluation metrics like specificity, sensitivity, or positive and negative predictive values. Arguably, in practice, a decontextualised performance value is less important than the practical consequences of selecting a specific threshold level. Every time an AI model detects a finding (based on a selected threshold), a radiologist may have to take action to assess it. The balance between clinical value and additional burden is thus closely tied to how well the threshold is configured to match the local clinical context. We conceptualised four dependencies that influence the configuration of the AI decision threshold.\nDependency 6: AI decision threshold depends on medical knowledge. While some of the radiological findings are well understood across the contexts, some definitions are more subjective and their meanings change across countries. Infiltration or consolidation are two examples of radiological findings which have been found to be used differently in clinical practice in Denmark and Kenya. Moreover, some of the findings were too vague for the radiologists to decide how to assess them, for example, I think vascular changes can mean one of two things if it's the big vessels - I think it's important to have it, e.g., if the computer can say the aorta is big... It could also be about... the small vessels, and then it's more like stasis. Then it's quite different [117, Junior, Specialised hospital, Denmark]. The underlying definition of a finding, in this described case, affected how P13 understood the condition and what threshold level they deemed"}, {"title": "Dependency 7: AI decision threshold depends on user expertise level.", "content": "Often, junior and senior radiologists are juxtaposed as two groups of AI support end-users with different needs. This is also visible in the strategy for selecting thresholds.\nWhen used by junior radiologists, both junior and senior radiologists (who supervised them) leaned towards accepting AI predictions only with a high degree of certainty. As explained before, the interpretation of chest X-rays is uniquely subjective. It takes experience to report them with a high degree of certainty. In this context, an uncertain AI prediction would jeopardise the learning process and introduce more confusion, resulting in more work for the junior students and their supervisors.\nOn the other hand, allowing senior radiologists to set the threshold for different findings according to their personal preferences could entice them to utilise the system in their own way. P05, who also had senior administrative experience, explained that senior radiologists do not always have the same level of expertise and may need different levels of support. This would be amazing. I wouldn't want to do it [adjust threshold] at the administrative level because not all the radiologists in the department have the same capabilities. So I'd rather let people set it for themselves [S18, Senior, Big general hospital, Kenya]. By enabling users to select the Al decision threshold on their own, they could build trust by incrementally including Al in their own practice.\nDependency 8: AI decision threshold depends on patient context. Diverging from a fixed threshold level defined at a system level towards finding-level threshold specification may boost the clinical usefulness of AI-based systems for radiology. A finding-level threshold specification would allow radiologists to stratify which findings in a given context are more relevant.\nThey could do it by lowering the threshold. A lower threshold would be associated with a higher rate of false positive prediction for that particular radiological finding. Thus, more work for radiologists. However, for a subset of findings, our participants were willing to accept more false positive detections if it would benefit their patients. For pneumothorax, I would probably lower the threshold because you would want to find every pneumothorax there is. But for some other stuff, like fibrosis, I would probably have a higher threshold because that's not critical [S13, Senior, Specialised hospital, Denmark]. Based on design interventions with the third prototype, our participants saw a utility in such fine-grained configuration. I think the relevance of certainty [threshold] is the clinical implication of the diagnosis. So, something like a pneumothorax needs some form of intervention... whereas on a suspected infection, a clinician may go ahead and treat it even if the X-ray is normal. So that's why it may not be such a big deal whether I call a pneumonia or not. Whereas a pneumothorax might need a chest tube insertion. It's a do-or-die call [S19, Senior, Specialised hospital, Kenya]. As shown in this quote, the clinical implications for a patient made radiologists more accepting of false positives. Meanwhile, findings that were less severe or that could be discerned using other indicators, e.g., clinical"}, {"title": "Dependency 9: AI decision threshold depends on user situation.", "content": "Radiologists' approach to AI support changes with time. In this paper, we uncovered two temporal aspects that affected how radiologists thought of configuring Al decision thresholds: the time spent using the system and the rhythm of clinical work.\nOne of the common comments when discussing the threshold with our participants was about its arbitrary nature. Radiologists wondered what the real-life consequences of changing the threshold would be. Based on these concerns, our final prototype included an estimate of false positive predictions. These values, while more relatable, were still considered difficult to imagine in real practice both for senior and junior radiologists. I mean, it's a bit arbitrary at this moment because you don't have any idea what the effect is [117, Junior, Specialised hospital, Denmark]. It would be nice to be able to adjust this... try all this out and see in real life how many cases it's missing or over-calling [S19, Senior, Specialised hospital, Kenya]. These quotes highlight that such essential development tasks as selecting a threshold have little to no basis in clinical practice. They uncover a need for a better translation between the domains of AI and Health to support meaningful configuration. Currently, this translation has to be conducted through real-world experimentation in the final context of use. This way, medical professionals may gain a practical understanding of what the changes to the threshold mean and further purposefully and consciously adjust it to fit their work.\nThe second temporal aspect of selecting an appropriate threshold relates to the routine of end users. Radiologists saw an advantage in adjusting the threshold depending on their workload. For example, a specialised radiologist from a busy specialised hospital mentioned, on Fridays, we tend to be more active because if you leave a long list on Friday, the turnaround time will be way longer - there is very low coverage over the weekend [few on-call doctors]... and then Monday tends to be very busy [I04, Senior, Specialised hospital, Kenya]. During this conversation, the radiologist concluded that lowering the threshold could help them ensure that no examinations with critical findings were left to be reported after the weekend. These two aspects highlight that what radiologists consider a useful level of detection (including false positive predictions) may vary throughout the use."}, {"title": "#3 Recommendation:", "content": "Enable users to adjust AI thresholds."}, {"title": "4.5 AI Explainability", "content": "How should the AI explain its decisions? Understanding AI predictions supports building trust towards AI-based systems. In this study, we explored three visual ways of explaining AI predictions: heat maps, bounding boxes, and arrows (see Fig. 8). We discovered that no single method can support the explainability of all the radiological findings."}, {"title": "Dependency 10: AI explainability depends on medical knowledge.", "content": "The visual appearance of radiological findings dictates the best way to highlight them for radiologists for inspection. Radiologists discern between different radiological findings based on their visual appearance. Their presentation ranges from barely visible nodules to diffused opacities (areas of less transparency) present across both lungs. The breadth of visual impressions suggests the need for flexibility, I think that both ways of displaying the findings are fine, but for different pathologies. I mean, the heat map makes sense in this case for pneumothorax because it's a very extensive finding. And for the fracture, it makes sense to see it with a box, whereas the heat map doesn't make that much sense. It becomes too blurry... [S13, Senior, Specialised hospital, Denmark]. Radiologists preferred bounding boxes for more contained findings, whereas the more diffused, the more inclined they were towards the heat map. An important factor when designing XAI for chest X-rays is allowing for inspection of the underlying examination. The main purpose of XAI is to direct radiologists' attention to the detected findings. To assess the validity of a prediction, radiologists have to inspect the examination itself without additional overlays."}, {"title": "5 DISCUSSION", "content": "In this paper, we investigated how to design AI for clinical usefulness in different clinical contexts of radiology practice. Based on extended research through design study, we provided four practical recommendations on addressing ten dependencies emerging from the social dimensions of clinical practice (Fig. 9). By engaging radiologists in two different countries from the Global North and Global South, respectively, we found that the radiologists' practices in Kenya and Denmark were rather similar, possibly due to resemblances in medical education, scientific models, and ethics. The social dimensions derived from this study therefore orient towards similarities that cut across country specifics. However, the types of healthcare systems and present IT infrastructures differed quite substantially and need to be accounted for during AI innovation, which goes beyond this study. In this section, we will discuss how these recommendations may be enacted during the innovation process of clinical AI for different clinical contexts."}, {"title": "5.1 Enable users to select preferred AI functionality", "content": "Configuring clinical AI-based support systems to suit local environments is essential, as one-size-fits-all approaches often fail to address their unique needs [34, 39, 58, 89]. In this study, we discovered how social dimensions of clinical practice condition what kind of AI functionality is considered useful. We argue that for AI to match local requirements, it needs to be configured throughout the innovation process with the intended context of use in mind, i.e., the expertise of the end-users and the work performed in their clinical site. This is especially relevant, as clinical AI is often afflicted by the problem of late realisation [33, 88].\nWhen addressing expertise-related needs for support, previous research in radiology showed that AI-based systems have different effects on junior and senior radiologists [95]. Even more, Tong et al. [78] investigated two strategies, what they called \"optimised\" and \"all-AI\", for AI support of junior and senior radiologists in thyroid nodule management. They reported that the best results were obtained when the type of support was configured to the expertise of the radiologist. However, our study showed that personal preferences play a deciding factor only if the AI functionality is appropriate in the context of the local clinic. Selecting the best way to prioritise findings will not make sense if there are only a few examinations to prioritise to begin with. AI-based systems should be designed to respond to fit the utility gap in a clinic and then be configured to the varying needs and preferences of different end-users, depending on their level of experience, knowledge, and confidence."}, {"title": "5.2 Enable users to select radiological findings", "content": "The innovation of AI-based systems is often initiated and defined by technical opportunities, e.g., access to medical data [70, 71, 87, 94]. As such, the medical and social aspects of the systems are sometimes addressed only after the technology has gone through several rounds of development [2]. This inadvertently means that certain assumptions about the medical focus are made [92]. Present radiological AI models tend to detect findings relevant to the local radiologists involved in the data creation process [38, 54, 83]. However, we showed that the prevalence and clinical meaning of radiological findings varies based on the clinic type and patient context. This affects the usefulness of clinical systems in different settings and their transferability [55, 92]. Thus, it is critical to investigate the intended clinical context of use prior to deciding on the medical focus of the AI-based system and to allow medical professionals to set the scope of support relevant to them and their practice.\nMoreover, the clinical meaning of radiological findings is tied to the patient context and not only the type of medical condition, i.e.,"}, {"title": "5.3 Enable users to adjust AI thresholds", "content": "Selecting AI decision threshold has significant ethical [12], performance [65], and clinical [81] consequences for AI-based systems, and it has been a notable research topic in the AI and Health communities. Recently, it gained footing in the HCI design community. Kocielnik et al. [44] explored how the decision threshold affects the number of false positive and false negative predictions, significantly altering users system perception. While from the technical point of view, the accuracy may be the same, the distribution of false positives and false negatives may have severe clinical consequences. Our participants warned that false positive predictions require additional time and resources to discern and that the potential benefits of AI often do not justify this additional cost, resulting in the failure of the AI-based systems in clinical practice [5, 7, 50, 76].\nHowever, until AI reaches 100% accuracy, false positive predictions are the reality of AI-based systems. Improving performance is only one way of addressing them. In this paper, we offer another"}, {"title": "5.4 Enable users```json\n{\n  "}, {"title": "\"It depends\": Configuring AI to Improve Clinical Usefulness Across Contexts", "authors": ["Hubert D. Zaj\u0105c", "Jorge M. N. Ribeiro", "Silvia Ingala", "Simona Gentile", "Ruth Wanjohi", "Samuel N. Gitau", "Jonathan F. Carlsen", "Michael B. Nielsen", "Tariq O. Andersen"], "abstract": "Artificial Intelligence (AI) repeatedly match or outperform radiologists in lab experiments. However, real-world implementations of radiological AI-based systems are found to provide little to no clinical value. This paper explores how to design AI for clinical usefulness in different contexts. We conducted 19 design sessions and design interventions with 13 radiologists from 7 clinical sites in Denmark and Kenya, based on three iterations of a functional AI-based prototype. Ten sociotechnical dependencies were identified as crucial for the design of AI in radiology. We conceptualised four technical dimensions that must be configured to the intended clinical context of use: Al functionality, AI medical focus, AI decision threshold, and AI Explainability. We present four design recommendations on how to address dependencies pertaining to the medical knowledge, clinic type, user expertise level, patient context, and user situation that condition the configuration of these technical dimensions.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) models repeatedly match or outright outperform radiologists in narrowly defined detection tasks [4, 53, 60, 63]. There are multiple studies claiming that AI-based systems enhance radiologists' work, either by increasing accuracy or reducing time spent on each examination [48]. These claims, however, are based on retrospective evaluations conducted in laboratory settings. When looking closer into the state of the art of clinician-facing AI, the claims of utility weaken [93]. For example, Roberts et al. [61] found that out of the 62 AI models detecting and predicting COVID-19 on chest X-rays and CT scans that were described in the literature, none were deemed to be useful for clinical purposes. Furthermore, evaluations of the handful of systems approved by the authorities in the United States and European Union [1] revealed that their clinical impact when integrated into practice remains mostly unclear [79, 84]. A similar study by Lehman et al. [47] showed no improvement in patient outcomes after the successful integration of an AI-based support tool for mammography screenings. Strohm et al. claimed that one of the primary causes of Al's lack of success in radiology until now is due to \"uncertain added value for clinical practice of AI applications\" [73]. What these studies show is that the clinical usefulness of hitherto AI-based support systems is limited. Researchers with diverse backgrounds (AI, Health, and Human-Computer Interaction (HCI)) investigated what makes AI-based support systems clinically useful. Based on the previous work, we define clinical usefulness as the overarching quality of AI-based support systems emerging from the interplay of their real-world performance, clinical efficacy, local applicability, and end-user acceptance in a situated clinical context for concrete end-users. First, robust performance in real-world settings is essential, as subpar performance has been found to increase workload and disrupt clinical routines [80, 82, 93]. Second, the evaluations, primarily assessing technical performance metrics through randomised clinical trials (RCTs), must encompass tangible clinical outcomes and patient benefits. Health researchers have been advocating for more flexible assessment methodologies aligned with the iterative nature of AI deployment [11, 42, 49]. Third, end-user acceptance, supported by qualities like trust and usability, emerges as pivotal for successful use in clinical practice [18, 19, 39]. Altogether, for an AI-based system to be clinically useful, it must perform well, benefit patients, and be accepted by clinical end-users working in different clinical contexts.\nIn this paper, we investigate how to design Al for clinical usefulness in different clinical contexts. This study was conducted as a part of a larger research and development project focused on innovating an AI-based system to assist examinations of chest X-rays in Denmark and Kenya. Here, we define innovation as the entirety of work conducted to create an AI-based system, from creating the datasets the Al is trained on through design and development to its integration and use in practice. We conducted 19 design sessions and design interventions (online and collocated) with 13 radiologists from 7 clinical sites in Denmark and Kenya. Throughout the design study, we explored a range of user interface mock-ups and three versions of a web-based prototype of an AI-based support system with prioritisation and decision-support functionalities."}, {"title": "2 RELATED WORK", "content": "The hitherto evidence of Al's positive influence on clinical practice is limited [51, 75, 80]. Research on the real-world effect of AI in healthcare tends to be discrete and focusing on confined goals [93]. However, to provide clinical value AI-based systems have to dovetail contributions from Human-Computer Interaction, AI, and Health into a cohesive vision [30, 82, 93].\nFirst, clinical usefulness necessitates robust performance [93]. This primarily has to be true in real-world settings, retrospective evaluations in lab environments do not speak to the final performance of a system. For example, in a real-world evaluation of an acclaimed ML model for detecting diabetic retinopathy, 21% of all cases were deemed ungradable [8]. Poor performance also leads to increased workload [57, 64, 82], additional time spent on discerning false positive predictions [67, 68], or breakages to work routines [31]. Van Leeuwen et al. [80] reported that out of 100 CE-approved radiological AI-based systems, 64 showed no peer-reviewed evidence of clinical efficacy. Most evidence for the remaining 36 systems focused on diagnostic accuracy, not real-world clinical outcomes.\nSecond, clinical usefulness necessitates clinical efficacy [42]. However, randomised clinical trial (RCT) - a focused, systematic, rigorous, and insulated method commonly used to evaluate the validity of clinical interventions independent of external confounders - is often following the traditional sequential paradigm of work characteristic for drug development [13]. In this tradition, the intervention is evaluated only when deemed complete [21]. When translating this mentality to AI-based systems, not only does it hinder innovation, but it also results in the evaluation of AI through the measure of technical performance [49]. While technical performance is the backbone of useful AI, clinical efficacy is not its immediate consequence [11, 69]. For example, Lehman et al. [47] conducted a prospective evaluation of a computer-aided detection system supporting mammography reporting. Researchers concluded that the"}, {"title": "2.2 System Configurability", "content": "Configurability has been long considered crucial to the appropriation of IT systems [26, 27, 45]. There are two types of configurability that should be explored in the context of this study: before-use and in-use [36].\nBefore-use configurability typically involves the active participation of end-users in the design processes, aiming to tailor systems to their specific needs and preferences [36]. Various methods and approaches have emerged to facilitate meaningful engagement with end-users, such as participatory design techniques [45]. Acquiring an understanding of work practices and work environment, but also technology aspects of a future system and changes it may introduce, is critical for developing systems that effectively respond to user needs [43]. This understanding enables developers and designers to implement systems that are not only technically sound but also contextually appropriate.\nHowever, according to Stewart and Williams [72], the paradigm of user-centred design does not properly answer the challenges of implementing useful systems. Rather, the final usefulness of a system is created iteratively through the acts of in-use configuration."}, {"title": "3 METHODOLOGY", "content": "In this paper, we explored how to design radiological AI-based systems for clinical usefulness across contexts. This study was part of a larger project set to design and develop an AI-based support tool for radiologists examining chest X-rays, funded by the Innovation Fund Denmark (0176-00013B). The project is a multidisciplinary collaboration between the Department of Computer Science at the"}, {"title": "3.1 Research Through Design: Design Interventions with Working Prototypes", "content": "To explore the clinical usefulness of AI in different radiology contexts, we undertook a research through design approach [96]. We conducted three iterations based on a series of design sessions and design interventions using mock-ups of user interfaces (Prototype I) and working prototypes (II and III) (Fig. 1, Fig. 2, and Fig. 3). The three iterations were determined by decisions to deploy major changes in the web-based prototypes, i.e. version 1-3, followed by gathering feedback from the participants. The design sessions were carried out both online and collocated with radiologists in hospital offices. During these sessions, we obtained medical domain knowledge, typically by clarifying questions about radiology work and X-rays, but we also collectively explored the design space through a range of mock-ups and prototypes. The design interventions were carried in-situ with the performative purpose of exploring how"}, {"title": "3.1.1 Prototypes.", "content": "As part of the greater project, a deep learning-based model was developed by machine learning engineers at Unumed ApS to detect selected radiological findings [92]. The AI model was developed using a convolutional neural network. The first prototype was merely a proof of concept, not designed to collect feedback from external domain experts. It was developed to guide future work in terms of model development and data labelling. However, inspired by earlier research [92, 93], we considered it an opportunity to engage in more concrete discussions on the merit of clinical usefulness with medical professionals at an early stage of the innovation work.\nThe second and third iteration of the prototype consisted of an interactive web application designed to emulate a DICOM viewer. The web application integrated with the AI model developed within the bigger project. This connection enabled us to work with real data and, thus, explore with fidelity the interactions of the radiologists with the system. For the design interventions, radiologists were given access to the prototype, either in-person or remotely. They were requested to choose the next examination to report, following their usual practice and using information displayed in the prototype. Then, they were asked to interpret the selected examination without the use of AI and with Al decision support. Moreover, they were asked to configure the AI tool using available options to fit their practice. Finally, they were encouraged to explore the prototype independently and interact with any element of the user interface."}, {"title": "3.2 Analysis Positionality", "content": "The data analysis was conducted by the first and last authors (HDZ and TOA) with backgrounds in Health Informatics, HCI, and AI (5+ & 15+ years of experience). Moreover, before the analysis of the data from the design sessions and design interventions, the two co-authors concluded extensive ethnographic investigations into the work practices of radiologists from the visited sites with a particular outlook on opportunities for AI support (described in a manuscript prepared for publication). First-hand experience with the work practices and similarities and differences across clinical settings informed the initial analysis of this data."}, {"title": "3.3 Data Analysis", "content": "We used reflective thematic analysis [17] to analyse collected data (transcriptions of the design interventions). The analysis took place in Dovetail - a web application for qualitative data analysis. Except for the transcription software, no AI-based analysis support was used in this study. The two authors familiarised themselves with the collected data after every iteration of the design sessions and design interventions when deciding on the next focus. Moreover, the two authors, prior to coding, based on their fieldwork experience (60+ hours) and a literature review [93], devised three bucket themes to support the later organisation of codes: type of clinical site, domain expertise of medical professionals, and patient and situational context. Additionally, a fourth residual category was added not to limit coding. Next, to test the bucket themes, the two authors coded one transcript each for any references to challenges, preferences, dependencies, and configurations in relation to AI and their clinical practice. After this test, the fourth bucket theme was renamed to technical dependencies. The first author coded the remaining transcripts following the same directions. The two authors met weekly to discuss the coverage of the coding and future conceptualisation of themes. The themes were created within their respective bucket themes based on their grounding in the clinical context. Importantly, the division of codes between the bucket themes was never final and was used only to support analysis of the significant amount of codes (n=260). Through discussion, reflection on data across the interventions, and fieldwork experience, the authors iteratively clarified themes and reorganised data, moving away from the original bucket themes (while maintaining their initial assignment known). This interpretative work was conducted twice, creating ten reflective themes. The ten themes were framed as dependencies conditioning four specific design decisions that formed an AI-based support design space."}, {"title": "4 CONFIGURING FOUR TECHNICAL DIMENSIONS OF CLINICALLY USEFUL RADIOLOGICAL AI", "content": "We identified ten dependencies that emerge from the social dimensions of clinical AI and condition the configuration of four technical dimensions of clinical AI for radiology (see Fig. 4). Each of the technical dimensions needs to be configured in relation to the local clinical context to achieve clinical usefulness. In this section, we will briefly explain the social dimensions of clinical AI to then explore in-depth the conceptualised dependencies."}, {"title": "4.1 Social Dimensions of Clinical AI", "content": "Medical knowledge. This dimension includes concepts and definitions relevant to the medical domain addressed by the innovated AI-based system, for example, the meaning of radiological findings detected by our Al-based system. Familiarity with them supports meaningful collaboration between designers, developers, and medical professionals and reduces the risk of incorrect assumptions throughout the innovation process.\nClinic type. This social dimension addresses types of clinical sites. Imaging clinics, general hospitals, and specialised hospitals provide unique healthcare services and, thus, cater to the needs of patients with different conditions. Moreover, the type of clinical site determines the available resources, the speciality of medical professionals working there, their workflows, and their goals.\nUser expertise level. All medical professionals have different domain expertise. This is evident when comparing junior to senior medical professionals. However, it was also observed between board-certified radiologists. The level of expertise also determines the workload and clinical responsibilities.\nPatient context. This context encompasses the current location of a patient (in or out of a hospital) and their medical history. Patients are the centre of medical work. Their health and well-being are the priority. Thus, by extension, any system supporting healthcare professionals should support patients and depend on their context.\nUser situation. This dimension pertains to the workload, available time, and resources of medical professionals. While the other four dependencies describe relatively stable medical practice, situational context introduces a temporal factor to the work done and may affect the priorities of medical professionals."}, {"title": "4.2 AI Functionality", "content": "Which Al functionality should the system provide? Answering this question defines this technical dimension. The functionalities explored during design interventions (prioritisation and decision support, see, Fig. 5) were linked to the AI model developed for the project this study was a part of. We explored the conditions for these functionalities to provide clinical value and propose a third functionality: quality assurance, which originated during the design interventions.\nDependency 1: AI functionality depends on clinic type. Each clinical site has different (1) positions within the healthcare system, (2) amounts of resources available, and (3) workloads related to the size of a clinic. This is why it is important to ensure that AI functionality is implemented in a way that makes sense for the clinical site in which it will be deployed.\nFirst, while every radiologist puts the well-being of their patients first, the healthcare systems that they are a part of operate under different incentives. Public and private clinics face different challenges and may require adjusted AI functionalities, for example, The number of cases in court, medical and legal cases, is way more than what you would get in the public sector. So from the medical director's office [point of view], they would want .... any small thing to be flagged so that we don't get into problems later... it would be different in K5, compared to the public sector, where even if you missed this, people are rarely taken to court but in a private setting... if they"}, {"title": "Dependency 2: AI functionality depends on user expertise level.", "content": "The value of support in detecting findings on a medical examination decreases with increasing experience. Instead, the assigned workload increases with seniority. Thus, prioritisation and quality assurance functionalities gain importance.\nRadiological AI-based decision support typically presents a list of findings detected on an examination accompanied by an X\u0391\u0399 visualisation, as also explored in our prototypes. While this mode of support seems straightforward, it misses the reality of clinical practice. Senior radiologists spend a very short time interpreting chest X-rays. To ask them to revisit every examination to discern the validity of AI predictions is wishful. However, when discussing the potential value of AI-based decision support, they focused on quality assurance. Thus, AI should be treated not as an all-knowing peer who is going to point out every finding on an examination but as a safety net that activates only in time of need. For example, It could read the text we write and say: \"Oh, you missed that.\" That could be good [I11, Senior, Imaging clinic, Denmark]. This way, the envisioned system would not require the mental effort and time to discern Al output but would inform a radiologist about potentially missed findings based on the report they were writing.\nOn the other hand, junior radiologists in clinical settings usually take significantly more time to report every examination. Moreover, all of their reports have to be confirmed by a senior colleague. For them, reporting serves as a primary learning exercise. In this context, they envisioned using AI support not as a quality assurance but as a new source of information used to draw their own conclusions. I would take a look at a chest X-ray, formulate my opinion, and then see what the AI says... If it agrees... good, if it disagrees or finds something that I hadn't, I'll examine it critically... I like getting almost overwhelmed by data, and I sort it out afterwards... [S14, Junior, Specialised hospital, Denmark]. These two perspectives highlight how workflow, workload, and the act of detecting findings on a medical examination changes with expertise. The educational value created for junior radiologists by verbose explanations of Al's predictions may become a burden for senior radiologists who expect minimal disruption to their existing workflows."}, {"title": "#1 Recommendation:", "content": "Enable users to select preferred AI functionality."}, {"title": "4.3 AI Medical Focus", "content": "Which radiological findings should the AI detect? This is where our participants, for the first time, responded, starting with \"It depends...\" (see Fig. 6). Let's explore how to ensure the detected findings are clinically useful in the real world.\nDependency 3: AI medical focus depends on clinic type. Different clinics take care of different types of patients suffering from different conditions. Types of patients seen in different clinical settings result in a local prevalence of observed radiological findings. As a result, a single fit-them-all system that detects an arbitrarily selected set of findings is not going to provide a similar quality of support across the different clinical contexts."}, {"title": "Dependency 4: AI medical focus depends on user expertise level.", "content": "Junior radiologists may interpret a single X-ray for up to tens of minutes. Whereas, according to our senior participants, interpreting a chest X-ray takes around 1 to 2 minutes. This means that with experience, many findings become \"obvious\" and are no feat to detect. When discussing the decision support functionality of our prototypes and previous systems that our participants had piloted, the common complaint related to the detection of \"obvious\" radiological findings, which took additional time to discern.\nIf it's an obvious finding, we'll see that one quickly, and we all agree on it. The problem comes when it's something more subtle [106, Senior, Big General Hospital, Kenya]. Detecting the difficult or \"subtle\" radiological findings is where the value lies for senior radiologists. However, the less experienced, the more support a radiologist may accept. This was captured by P01: Maybe it'll help the resident radiologist in the first or second year, but I don't think"}, {"title": "Dependency 5: AI medical focus depends on patient context.", "content": "Radiologists are not interpreting medical imaging to find every possible finding. Rather, they are interpreting them to help the ordering clinicians take action in patient management. Such actions usually occur when a new condition is being diagnosed, or a patient's health may be at risk. However, the clinical meaning of certain radiological findings depends on the location of a patient. This means that a finding may be expected when observed in an examination of a patient who is admitted to a hospital. Whereas the same finding observed in an examination of a patient who is not admitted to a hospital may warrant immediate action.\nOur participants stressed that useful prioritisation should consider patients' medical history to filter out already-known findings, which our prototype could not do. But how urgent is it? We know that pneumothorax has decreased. It's a big heart, but it's much smaller than it was a week ago. It has [pleural] effusion, but much, much less than it was a week ago. That's the thing we miss with this [I11, Senior, Specialised Hospital / Imaging Clinic, Denmark]. In this quote, P11 explains that the examination they looked at may not be urgent at all despite the fact that the AI correctly detected three findings, one of them (pneumothorax) being life-threatening. These findings would not be urgent if they were already known to the ordering clinician. In such a case, the patient would have already been undergoing treatment, and this examination's sole purpose was to control its progress. In specialised hospitals and bigger general hospitals, patients often have taken several X-rays to monitor the progress of treatment. This means that the same findings, but of different severity, will be visible on their examinations. The ability to assess the detected findings in the light of patient history is crucial to correctly prioritise findings that warrant clinical action.\nWhen looking at radiologists' work from the perspective of contributing to the broader clinical work, it is counterproductive to prioritise findings that clinicians taking care of a patient are already aware of. In other words, a radiological finding may be relevant to detect on examinations from patients who are not admitted to a hospital, but not so much for patients currently admitted. A senior radiologist explained, It depends on the findings, and it depends on the patient... some findings in the out-patients would be more important to be prioritised than if they're in-house. Because if they're in-house, then I would suspect that someone not from the radiology department would have looked at them. If it's out-patient, then nobody has looked at them... [I10, Senior, Specialised Hospital, Denmark]. Whereas, as explained by P10, patients referred from outside of a hospital are more likely to have conditions that their doctors are unaware of. Thus, the location of the patient is crucial to selecting which findings are relevant to receiving support from an AI-based system."}, {"title": "4.4 AI Decision Threshold", "content": "At what certainty level should the AI inform a user about detected findings? Specifying when a radiological AI-based system should inform a user about a finding is usually done by specifying a decision threshold (see Fig. 7). Selecting a specific threshold value determines the measured performance of an Al model captured by evaluation metrics like specificity, sensitivity, or positive and negative predictive values. Arguably, in practice, a decontextualised performance value is less important than the practical consequences of selecting a specific threshold level. Every time an AI model detects a finding (based on a selected threshold), a radiologist may have to take action to assess it. The balance between clinical value and additional burden is thus closely tied to how well the threshold is configured to match the local clinical context. We conceptualised four dependencies that influence the configuration of the AI decision threshold.\nDependency 6: AI decision threshold depends on medical knowledge. While some of the radiological findings are well understood across the contexts, some definitions are more subjective and their meanings change across countries. Infiltration or consolidation are two examples of radiological findings which have been found to be used differently in clinical practice in Denmark and Kenya. Moreover, some of the findings were too vague for the radiologists to decide how to assess them, for example, I think vascular changes can mean one of two things if it's the big vessels - I think it's important to have it, e.g., if the computer can say the aorta is big... It could also be about... the small vessels, and then it's more like stasis. Then it's quite different [117, Junior, Specialised hospital, Denmark]. The underlying definition of a finding, in this described case, affected how P13 understood the condition and what threshold level they deemed"}, {"title": "Dependency 7: AI decision threshold depends on user expertise level.", "content": "Often, junior and senior radiologists are juxtaposed as two groups of AI support end-users with different needs. This is also visible in the strategy for selecting thresholds.\nWhen used by junior radiologists, both junior and senior radiologists (who supervised them) leaned towards accepting AI predictions only with a high degree of certainty. As explained before, the interpretation of chest X-rays is uniquely subjective. It takes experience to report them with a high degree of certainty. In this context, an uncertain AI prediction would jeopardise the learning process and introduce more confusion, resulting in more work for the junior students and their supervisors.\nOn the other hand, allowing senior radiologists to set the threshold for different findings according to their personal preferences could entice them to utilise the system in their own way. P05, who also had senior administrative experience, explained that senior radiologists do not always have the same level of expertise and may need different levels of support. This would be amazing. I wouldn't want to do it [adjust threshold] at the administrative level because not all the radiologists in the department have the same capabilities. So I'd rather let people set it for themselves [S18, Senior, Big general hospital, Kenya]. By enabling users to select the Al decision threshold on their own, they could build trust by incrementally including Al in their own practice.\nDependency 8: AI decision threshold depends on patient context. Diverging from a fixed threshold level defined at a system level towards finding-level threshold specification may boost the clinical usefulness of AI-based systems for radiology. A finding-level threshold specification would allow radiologists to stratify which findings in a given context are more relevant.\nThey could do it by lowering the threshold. A lower threshold would be associated with a higher rate of false positive prediction for that particular radiological finding. Thus, more work for radiologists. However, for a subset of findings, our participants were willing to accept more false positive detections if it would benefit their patients. For pneumothorax, I would probably lower the threshold because you would want to find every pneumothorax there is. But for some other stuff, like fibrosis, I would probably have a higher threshold because that's not critical [S13, Senior, Specialised hospital, Denmark]. Based on design interventions with the third prototype, our participants saw a utility in such fine-grained configuration. I think the relevance of certainty [threshold] is the clinical implication of the diagnosis. So, something like a pneumothorax needs some form of intervention... whereas on a suspected infection, a clinician may go ahead and treat it even if the X-ray is normal. So that's why it may not be such a big deal whether I call a pneumonia or not. Whereas a pneumothorax might need a chest tube insertion. It's a do-or-die call [S19, Senior, Specialised hospital, Kenya]. As shown in this quote, the clinical implications for a patient made radiologists more accepting of false positives. Meanwhile, findings that were less severe or that could be discerned using other indicators, e.g., clinical"}, {"title": "Dependency 9: AI decision threshold depends on user situation.", "content": "Radiologists' approach to AI support changes with time. In this paper, we uncovered two temporal aspects that affected how radiologists thought of configuring Al decision thresholds: the time spent using the system and the rhythm of clinical work.\nOne of the common comments when discussing the threshold with our participants was about its arbitrary nature. Radiologists wondered what the real-life consequences of changing the threshold would be. Based on these concerns, our final prototype included an estimate of false positive predictions. These values, while more relatable, were still considered difficult to imagine in real practice both for senior and junior radiologists. I mean, it's a bit arbitrary at this moment because you don't have any idea what the effect is [117, Junior, Specialised hospital, Denmark]. It would be nice to be able to adjust this... try all this out and see in real life how many cases it's missing or over-calling [S19, Senior, Specialised hospital, Kenya]. These quotes highlight that such essential development tasks as selecting a threshold have little to no basis in clinical practice. They uncover a need for a better translation between the domains of AI and Health to support meaningful configuration. Currently, this translation has to be conducted through real-world experimentation in the final context of use. This way, medical professionals may gain a practical understanding of what the changes to the threshold mean and further purposefully and consciously adjust it to fit their work.\nThe second temporal aspect of selecting an appropriate threshold relates to the routine of end users. Radiologists saw an advantage in adjusting the threshold depending on their workload. For example, a specialised radiologist from a busy specialised hospital mentioned, on Fridays, we tend to be more active because if you leave a long list on Friday, the turnaround time will be way longer - there is very low coverage over the weekend [few on-call doctors]... and then Monday tends to be very busy [I04, Senior, Specialised hospital, Kenya]. During this conversation, the radiologist concluded that lowering the threshold could help them ensure that no examinations with critical findings were left to be reported after the weekend. These two aspects highlight that what radiologists consider a useful level of detection (including false positive predictions) may vary throughout the use."}, {"title": "#3 Recommendation:", "content": "Enable users to adjust AI thresholds."}, {"title": "4.5 AI Explainability", "content": "How should the AI explain its decisions? Understanding AI predictions supports building trust towards AI-based systems. In this study, we explored three visual ways of explaining AI predictions: heat maps, bounding boxes, and arrows (see Fig. 8). We discovered that no single method can support the explainability of all the radiological findings."}, {"title": "Dependency 10: AI explainability depends on medical knowledge.", "content": "The visual appearance of radiological findings dictates the best way to highlight them for radiologists for inspection. Radiologists discern between different radiological findings based on their visual appearance. Their presentation ranges from barely visible nodules to diffused opacities (areas of less transparency) present across both lungs. The breadth of visual impressions suggests the need for flexibility, I think that both ways of displaying the findings are fine, but for different pathologies. I mean, the heat map makes sense in this case for pneumothorax because it's a very extensive finding. And for the fracture, it makes sense to see it with a box, whereas the heat map doesn't make that much sense. It becomes too blurry... [S13, Senior, Specialised hospital, Denmark]. Radiologists preferred bounding boxes for more contained findings, whereas the more diffused, the more inclined they were towards the heat map. An important factor when designing XAI for chest X-rays is allowing for inspection of the underlying examination. The main purpose of XAI is to direct radiologists' attention to the detected findings. To assess the validity of a prediction, radiologists have to inspect the examination itself without additional overlays."}, {"title": "5 DISCUSSION", "content": "In this paper, we investigated how to design AI for clinical usefulness in different clinical contexts of radiology practice. Based on extended research through design study, we provided four practical recommendations on addressing ten dependencies emerging from the social dimensions of clinical practice (Fig. 9). By engaging radiologists in two different countries from the Global North and Global South, respectively, we found that the radiologists' practices in Kenya and Denmark were rather similar, possibly due to resemblances in medical education, scientific models, and ethics. The social dimensions derived from this study therefore orient towards similarities that cut across country specifics. However, the types of healthcare systems and present IT infrastructures differed quite substantially and need to be accounted for during AI innovation, which goes beyond this study. In this section, we will discuss how these recommendations may be enacted during the innovation process of clinical AI for different clinical contexts."}, {"title": "5.1 Enable users to select preferred AI functionality", "content": "Configuring clinical AI-based support systems to suit local environments is essential, as one-size-fits-all approaches often fail to address their unique needs [34, 39, 58, 89]. In this study, we discovered how social dimensions of clinical practice condition what kind of AI functionality is considered useful. We argue that for AI to match local requirements, it needs to be configured throughout the innovation process with the intended context of use in mind, i.e., the expertise of the end-users and the work performed in their clinical site. This is especially relevant, as clinical AI is often afflicted by the problem of late realisation [33, 88].\nWhen addressing expertise-related needs for support, previous research in radiology showed that AI-based systems have different effects on junior and senior radiologists [95]. Even more, Tong et al. [78] investigated two strategies, what they called \"optimised\" and \"all-AI\", for AI support of junior and senior radiologists in thyroid nodule management. They reported that the best results were obtained when the type of support was configured to the expertise of the radiologist. However, our study showed that personal preferences play a deciding factor only if the AI functionality is appropriate in the context of the local clinic. Selecting the best way to prioritise findings will not make sense if there are only a few examinations to prioritise to begin with. AI-based systems should be designed to respond to fit the utility gap in a clinic and then be configured to the varying needs and preferences of different end-users, depending on their level of experience, knowledge, and confidence."}, {"title": "5.2 Enable users to select radiological findings", "content": "The innovation of AI-based systems is often initiated and defined by technical opportunities, e.g., access to medical data [70, 71, 87, 94]. As such, the medical and social aspects of the systems are sometimes addressed only after the technology has gone through several rounds of development [2]. This inadvertently means that certain assumptions about the medical focus are made [92]. Present radiological AI models tend to detect findings relevant to the local radiologists involved in the data creation process [38, 54, 83]. However, we showed that the prevalence and clinical meaning of radiological findings varies based on the clinic type and patient context. This affects the usefulness of clinical systems in different settings and their transferability [55, 92]. Thus, it is critical to investigate the intended clinical context of use prior to deciding on the medical focus of the AI-based system and to allow medical professionals to set the scope of support relevant to them and their practice.\nMoreover, the clinical meaning of radiological findings is tied to the patient context and not only the type of medical condition, i.e.,"}, {"title": "5.3 Enable users to adjust AI thresholds", "content": "Selecting AI decision threshold has significant ethical [12], performance [65], and clinical [81] consequences for AI-based systems, and it has been a notable research topic in the AI and Health communities. Recently, it gained footing in the HCI design community. Kocielnik et al. [44] explored how the decision threshold affects the number of false positive and false negative predictions, significantly altering users system perception. While from the technical point of view, the accuracy may be the same, the distribution of false positives and false negatives may have severe clinical consequences. Our participants warned that false positive predictions require additional time and resources to discern and that the potential benefits of AI often do not justify this additional cost, resulting in the failure of the AI-based systems in clinical practice [5, 7, 50, 76].\nHowever, until AI reaches 100% accuracy, false positive predictions are the reality of AI-based systems. Improving performance is only one way of addressing them. In this paper, we offer another"}, {"title": "5.4 Enable users"}]}]}