{"title": "Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning", "authors": ["Minbo Ma", "Kai Tang", "Huan Li", "Fei Teng", "Dalin Zhang", "Tianrui Li"], "abstract": "Multivariate Time Series Forecasting (MTSF) has long been a key research focus. Traditionally, these studies assume a fixed number of variables, but in real-world applications, Cyber-Physical Systems often expand as new sensors are deployed, increasing variables in MTSF. In light of this, we introduce a novel task, Expanding-variate Time Series Forecasting (EVTSF). This task presents unique challenges, specifically (1) handling inconsistent data shapes caused by adding new variables, and (2) addressing imbalanced spatio-temporal learning, where expanding variables have limited observed data due to the necessity for timely operation. To address these challenges, we propose STEV, a flexible spatio-temporal forecasting framework. STEV includes a new Flat Scheme to tackle the inconsistent data shape issue, which extends the graph-based spatio-temporal modeling architecture into 1D space by flattening the 2D samples along the variable dimension, making the model variable-scale-agnostic while still preserving dynamic spatial correlations through a holistic graph. Additionally, we introduce a novel Spatio-temporal Focal Learning strategy that incorporates a negative filter to resolve potential conflicts between contrastive learning and graph representation, and a focal contrastive loss as its core to guide the framework to focus on optimizing the expanding variables. To evaluate the effectiveness of STEV, we benchmark EVTSF performance on three real-world datasets from various domains and compare it against three potential solutions employing state-of-the-art (SOTA) MTSF models tailored for EVSTF. Experimental results show that STEV significantly outperforms its competitors, especially in handling expanding variables. Notably, STEV, with only 5% of observations during the expanding period, is on par with SOTA MTSF models trained with complete data. Further exploration of various expanding scenarios underscores the generalizability of STEV in real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Multivariate Time Series (MTS), reflecting the fluctuation of multiple sensors' measurements over time, plays a vital role in understanding and optimizing Cyber-Physical Systems (CPS) across various domains [1, 31, 41]. As a fundamental task and a research hotspot in MTS utilization, MTS Forecasting (MTSF) necessitates a keen consideration of both the temporal dynamics of each variable and the spatial correlations across variables [13, 16, 36]. While recent advancements in MTSF have focused on developing novel deep learning networks to enhance forecasting accuracy, progress has nearly plateaued, often neglecting critical real-world requirements.\nIn this study, we instead draw attention to a frequently overlooked scenario in MTS research: Expanding-variate Time Series (EVTS), where the number of variables dynamically increases over time. Specifically, we explore situations where an established CPS evolves with newly deployed sensors, which are expected to function with their supporting model as soon as possible. Consider a traffic management system, as illustrated in Fig. 1. Initially, sensors (black dots) monitor traffic conditions within a designated area (yellow shadow). As urban development progresses, additional sensors (red dots) are deployed, expanding the monitoring area (expanded yellow shadow). In this evolving context, we define the initially deployed sensors as continual sensors, and their measurements as continual variables, while the newly deployed sensors and their measurements are termed expanding sensors and expanding variables, respectively.\nWe will focus on the EVTS Forecasting (EVTSF) task, which aims to accurately forecast future values of both continual and expanding variables. In particular, the newly installed sensors are expected to perform forecasting upon deployment. Early forecasting is highly beneficial. For instance, in transportation, timely predictions of future traffic conditions in newly monitored regions can help reduce accidents during critical early stages. Similarly, in power systems, substantial ice accumulation on transmission lines can cause widespread power outages, as has been extensively documented in both China [8] and the United States [24]. To mitigate such risks, monitoring devices like tension sensors are deployed to detect ice buildup, where tension forecasting supports prompt de-icing measures. In both scenarios, initiating forecasts immediately after sensor deployment is essential for effective risk prevention and mitigation.\nDespite its many advantages, this task not only shares the common challenge of capturing spatio-temporal dependencies but also faces two additional major challenges:\n\u2022 Challenge 1: Inconsistent data shapes. As new variables are introduced, the shape of the data changes, as shown in Fig. 1. This necessitates that deep model training adapts to varying dimensions within the same mini-batch, a stark contrast to the equal-sized data shapes assumed by traditional training methods. An intuitive strategy is to treat EVTS as a set of independent univariate times series (UTS) and perform forecasting for each UTS individually. However, this approach fails to leverage correlations among variables, limiting its effectiveness. Another strategy is to apply the padding operation to standardize sample shapes, but this often introduces noise, resulting in inevitably inferior performance (refer to the evaluation in Section 4.2). Besides, several studies [4, 29] employ continual learning [27], which sequentially updates models using newly available data. While this approach is feasible in dynamic settings, its effectiveness is constrained by the catastrophic forgetting problem [12], i.e., neural models significantly forget previously learned information upon learning new data.\n\u2022 Challenge 2: Imbalanced spatio-temporal learning. Newly deployed sensors need to start providing accurate forecasts as soon as possible. However, collecting enough data to fully capture the new spatio-temporal patterns takes time. This leads to a significant imbalance in the volume of data available, with the long-term observations (P1 + P2 in Fig. 1) of continual variables and only very short-term data (P2 in Fig. 1) of expanding variables. As a result, models trained under these conditions are often biased toward the well-represented continual variables, resulting in less accurate predictions for the newly added expanding variables. Conventional methods focus on increasing samples of expanding variables through techniques like oversampling and data augmentation. However, these approaches can easily lead to overfitting, especially when the synthetic data fails to adequately represent the expanding variables. Besides, previous studies have employed large language model fine-tuning [42] and channel-independent approaches [19] to enhance models with few-shot learning capabilities, they face challenges in capturing sequential dependencies in time series and effectively modeling spatial dependencies [25].\nTo tackle these challenges, we propose STEV, a generic Spatio-Temporal neural forecasting framework for EVTSF. For Challenge 1, we introduce a simple yet effective Flat Scheme (FLATS), which flattens EVTS along the variable dimension, transforming it into UTS, thereby making the model variable scale-agnostic. To preserve the spatial correlations in the original EVTS, we then present a holistic graph where isolated subgraphs maintain these spatial relationships. For Challenge 2, Inspired by the success of contrastive learning (CL) in capturing powerful representations from limited data [3, 32, 38], we introduce Spatio-temporal Focal Learning (STFL) to guide the model in learning more effective representations for the undersampled expanding variables. Our approach begins with negative sample filtering, designed to resolve conflicts between the negative sample generation strategy and the spatial neighbouring similarities. Building on this, we then introduce Focal Contrastive Learning (FOCALCL), which incorporates a newly designed focal temperature factor into the contrastive learning loss, ensuring that the optimization process prioritizes expanding variables, thereby enhancing learning effectiveness. Extensive experiments on three real-world EVTS datasets confirm that STEV not only outperforms potential state-of-the-art (SOTA) solutions but also demonstrates robustness across various expanding scenarios. In summary, our contributions are fourfold:\n(1) We introduce an emerging task, EVTSF, addressing a frequently overlooked aspect of CPS, that is evolving with sensing expansion. In response, we present STEV, a pioneering framework designed to tackle the unique challenges of EVTSF. (Sections 2 & 3)\n(2) We develop FLATS, a solution to the challenge of inconsistent-shaped mini-batches specific to EVTSF. This straightforward yet effective method preserves original spatial correlations while transforming variable-shaped data into a consistent format. (Section 3.2)\n(3) To address the issue of imbalanced spatio-temporal learning, we propose STFL, an unsupervised auxiliary task that enhances the learning of spatio-temporal representations for the undersampled expanding variables. (Section 3.3)\n(4) Our experiments involve creating three diverse EVTS datasets across various domains to evaluate the STEV framework. The results show that STEV delivers performance comparable to SOTA MTSF methods using complete data, even when leveraging only 5% of the observed data. (Section 4)"}, {"title": "2 PROBLEM DEFINITION", "content": "We focus on defining the EVTSF problem within an individual expansion process. Consecutive expansions can be treated as repeated instances of this process.\nDEFINITION 1 (EXPANDING-VARIATE TIME SERIES). Let $V_1$ and $V_2$ denote the sets of variables before and after expansion, respectively. $V_1$ includes only continual variables, while $V_2$ encompasses both continual and expanding variables, thus $V_1 \\subset V_2$. The EVTS dataset $\\mathcal{D}$ consists of two main components: MTS data before variable expansion $D_1 \\in \\mathbb{R}^{|V_1|\\times|P_1|}$ and after variable expansion $D_2 \\in \\mathbb{R}^{|V_2|\\times|P_2|}$, i.e., $\\mathcal{D} = (D_1, D_2)$. To ensure seamless forecasting upon new sensor installation, the observed time steps after expansion should be significantly fewer than those before expansion, i.e., $|P_2| < |P_1|$.\nDEFINITION 2 (EXPANDING-VARIATE TIME SERIES FORECASTING). We aim to build an EVTSF function $F$ that forecasts the future values for both continual and expanding variables. Formally,\n$\\mathbb{Y} = F(X)$, (1)\nwhere $X \\in \\mathbb{R}^{|V_2|\\times H}$ represents the historical observations and $\\mathbb{Y} \\in \\mathbb{R}^{|V_2|\\times Q}$ represents the predictions, $H$ and $Q$ denote the historical and forecasting horizons, respectively.\nPROBLEM (EXPANDING-VARIATE TIME SERIES FORECASTING). We define the data-driven optimization problem for $F^*(\\cdot)$ with the parameter $\\Theta$ as follows:\n$\\mathcal{F} = \\underset{F_{\\Theta}}{argmin}  E_{\\mathbb{X}_i,\\mathbb{Y}_i\\sim \\mathcal{D}}[\\mathcal{L}(F_{\\Theta}(\\mathbb{X}_i), \\mathbb{Y}_i)]$, (2)\nwhere $\\mathbb{X}_i \\in \\mathbb{R}^{N\\times H}$, $\\mathbb{Y}_i \\in \\mathbb{R}^{N\\times Q}$ are drawn from $\\mathcal{D}$ using the sliding window technique, and $\\mathcal{L}(\\cdot)$ is the objective loss function. Notably, we mix and shuffle the data from before and after the variable expansion during the model optimization process. Therefore, the variable dimension $N$ in Equation (2) equals to $|V_1|$ if the samples are from before expansion, and $|V_2|$ otherwise."}, {"title": "3 METHOD", "content": "Figure 2 provides an overview of STEV, highlighting two innovations integrated into general MTSF models: FLATS and FOCALCL. As discussed in Challenge 1, EVTS data contains varying numbers of variables, whereas conventional methods generally handle regular tensors. To address this issue, we propose FLATS to flatten the EVTS along the variable dimension, enabling a model to handle an arbitrary number of variables. We additionally present a holistic isolated graph to preserve the spatial correlations within variables, which are disordered when flattening. For each subgraph within the holistic isolated graph, we introduce the dynamic graph learning (DGL) scheme to adaptively represent the time-dependent spatial correlations. Addressing Challenge 2, we draw inspiration from contrastive learning's ability to derive robust representations from limited labeled data. We propose STFL with a negative sample filter and a FOCALCL to derive robust and discriminative representations. These two custom mechanisms enable the model to avoid hard negative samples and focus on optimizing representations of expanding variables with limited observed data. The final optimization objective $\\mathcal{L}$ is a combination of traditional forecasting loss $\\mathcal{L}_{error}$ and FOCALCL loss $\\mathcal{L}_{cl}$.\nThe variable expansion causes inconsistencies in sample data shapes within a mini-batch, rendering traditional MTSF training methods that depend on uniform shapes ineffective for the EVTSF task. One straightforward solution to this issue is the padding scheme, where samples are padded with constant values (e.g., zeros) to achieve uniformity, and a graph is to represent inter-variable correlations, as shown in the upper part of Figure 3. However, this approach inevitably introduces disturbances from the padding values, which persist despite applying a mask to the loss function to minimize their impact. To address this problem, we propose a simple yet effective FLATS.\nSuppose we have a mini-batch EVTS samples $X = \\{\\mathbb{X}_i\\}_{i=1}^B$ (Figure 3 left), where $B$ is the batch size and $\\mathbb{X}_i$ is an EVTS sample. $\\mathbb{X}_i \\in \\mathbb{R}^{|V_1|\\times H}$ if it is before expansion and $\\mathbb{X}_i \\in \\mathbb{R}^{|V_2|\\times H}$ if it is after expansion. To handle the inconsistent data shapes, we flatten $\\mathbb{X}$ along the variable dimension, transforming it into a new mini-batch $X' = [\\mathbb{X}'_i|i = 1, 2, ..., B']$ (Figure 3 bottom), where $\\mathbb{X}'_i \\in \\mathbb{R}^{H}$ is an UTS and $B' \\in [B\\times|V_1|, B\\times|V_2|]$ represents the total count of samples in the mini-batch $X'$. This flattening process converts the original batch $X$ with different-shaped MTS samples into a new batch $X'$ with uniform-shaped UTS samples, ensuring consistency in data shape for effective model training. However, this process loses the correlations among variables in the original samples. Therefore, we propose a holistic isolated graph to preserve the essential relationships between variables, enabling seamless spatial feature extraction.\nThe original EVTS $X$ uses a graph structure to represent spatial correlations among variables. Inspired by the advanced mini-batching module of PyG [9], which efficiently handles multiple graphs with varying numbers of nodes by stacking them diagonally, we propose the holistic isolated graph to maintain these spatial correlations within the flattened samples.\nIn the original graph (Figure 3 top), each sample shares a uniform structure. Differently, the holistic isolated graph (Figure 3 bottom) comprises a disconnected collection of $B$ distinct subgraphs $\\{G_i\\}_{i=1}^B$, each corresponding to consecutive UTS samples in $X'$, which align with their pre-expanding sample $\\mathbb{X}_i$ in $X$. For instance, in the mini-batch in Figure 3, before flattening, the original graph represents the spatial correlations of the post-expansion sample that is $\\mathbb{X}_3$, containing variables 1 to 4. After flattening, samples $\\mathbb{X}'_7$ to $\\mathbb{X}'_{10}$ correspond to the original sample $\\mathbb{X}_3$ and share a subgraph $G_3$, while the other flat samples have their own subgraphs. This method not only ensures the spatial relationships are retained but also keeps a specific subgraph for each original sample.\nThen we introduce the DGL module to construct the subgraph $G_i$. Considering that in real-world CPS, pre-defined spatial correlations are not always available, we propose to construct learnable graph structures. In addition, the spatial correlations are not static but reveal periodic characteristics as time evolves due to regular production activities [18]. The adjacency matrix of the learnable time-aware subgraph $G_i$ at time $t$ is derived as follows:\n$A_{t,i} = SPARSE(E_t E_t^\\top), where E_t = CAT(E, e_t)$. (3)\nHere, $E \\in \\mathbb{R}^{N\\times d}$ represents the $d$-dimensional learnable embeddings of the $N$ variables, and $e_t = CAT(e_{tod}, e_{dow})$ is a joint periodic embedding combining the time-of-day (tod) and the day-of-week (dow). The $CAT(\\cdot,)$ operation concatenates these embeddings concatenation operation. The periodic embedding is also learnable during training and indexable during inference. The $SPARSE()$ function applies a sigmoid activation followed by a filter operation that retains positive correlations, enhancing generalization while reducing computational overhead. This DGL scheme ensures that the spatial correlations within the subgraph $G_i$ are both adaptive and reflective of temporal periodicities.\nAfter flattening the original mini-batch $X$, we use a spatio-temporal feature extractor (STFE) to derive meaningful features. The STFE processes the flattened mini-batch $X'$ and the corresponding holistic isolated graphs $\\{G_i\\}_{i=1}^B$, producing the output $H = STFE(X', G)$. Here, $H \\in \\mathbb{R}^{B'\\times C_{out}}$ with $C_{out}$ representing the output dimension. This extractor employs a deep residual architecture comprising multiple blocks, each containing multi-layer 1D dilated convolutions [37] to capture temporal dynamics across different receptive fields, and Chebyshev spectral graph convolutions [7] to extract intricate spatial features. Detailed of the STFE can be found in Appendix A.1.\nAs indicated in Challenge 2, the issue of data imbalance tends to push the STFE towards focusing on continual variables with abundant data, resulting in insufficient representations of data-limited expanding variables. Traditionally, over-sampling and data augmentation are employed to address such imbalances by generating more samples for the minority class. However, merely increasing the number of minority samples does not necessarily enrich their semantic information and may lead to overfitting. Additionally, augmenting time series data poses a particular challenge, as it requires introducing diversity without corrupting the inherent temporal patterns. To circumvent these issues, we introduce a novel spatio-temporal focal learning approach. This method incorporates an innovative unsupervised FOCALCL mechanism with negative pair filtering, specifically designed to adapt to the FLATS and enhance the feature extraction for expanding variables.\nVanilla contrastive learning aims to learn temporal representations and capture variable heterogeneity by pulling positive samples closer together and pushing negative samples apart. It has shown superiority in robust representation learning when only limited data is available [14, 38]. The core components of contrastive learning include the construction of positive and negative sample pairs and the contrastive loss. Formally, to construct positive and negative sample pairs, a time series augmentation technique generates an additional view $\\mathbb{X}'_{aug} \\sim p(\\mathbb{X}'_{aug}|\\mathbb{X})$. The augmentation conditional distribution $p(\\mathbb{X})$ reflects the transformational impact of augmentation techniques on the temporal dynamics of the time series. A sample and its augmented counterpart are treated as a positive pair, i.e., $(\\mathbb{X}'_i, \\mathbb{X}'_{aug})$, while the sample and the augmented counterparts of the other samples within the mini-batch are treated as negative pairs, i.e., $\\{(\\mathbb{X}'_i, \\mathbb{X}'_{aug})\\}_{j=1}^{B'}, j\\neq i$. A projection layer maps the spatio-temporal features of the original $H$ and augmented samples $H_{aug}$ into latent space $Z$ and $Z_{aug}$, respectively. The contrastive loss $\\mathcal{L}_{cl}$ for sample $\\mathbb{X}'_i$ is then formulated as:\n$\\mathcal{L}_{cl}(\\mathbb{X}'_i) = -log\\frac{exp(s_{i,i}/\\tau)}{\\sum_{j=1}^{B'}exp(s_{i,j}/\\tau)}$, (4)\nwhere $s_{i,j} = sim(Z_i, Z_{aug})$ denotes the dot-product similarity measuring the feature similarity, and $\\tau$ is the temperature factor that controls the magnitude of penalties on hard negative samples [30].\nHowever, vanilla spatio-temporal contrastive learning introduces a unique dilemma regarding the treatment of negative sample pairs. Specifically, as shown in Figure 4 left, a sample $\\mathbb{X}'_i$ has its negative pair counterpart $\\mathbb{X}'_{aug}$, where $j \\neq i$. The learning process aims to push these samples apart, that is to make them dissimilar. However, a complication arises when $\\mathbb{X}'_i$ and $\\mathbb{X}'_j$ belong to the same subgraph $G_i$ and are connected by an edge (Figure 4 right). In this case, $Z_i$ and $Z_{aug}$ should exhibit similar behaviours and thus should be pulled closer together during learning. Experimentally, our ablation study supports this claim, where the performance decreases when vanilla contrastive learning is directly applied (refer to results in Figure 5).\nIn light of this, we propose to include a negative pair filter into vanilla contrastive learning that adaptively excludes negative sample pairs belonging to the same subgraph. Consequently, Equation (4) is re-formulated as:\n$\\mathcal{L}_{cl}(\\mathbb{X}'_i) = -log\\frac{exp(s_{i,i}/\\tau)}{\\sum_{j=1, j \\notin G_i}^{B'}exp(s_{i,j}/\\tau)}$, (5)\nwhere $G_i$ denotes the subgraph containing sample $\\mathbb{X}'_i$.\nWhen constructing negative pairs, we find that imbalanced data would lead to generating hard negative samples, which are excessively similar to positive samples. This makes it challenging for vanilla contrastive learning to learn discriminative representations. To understand this issue formally, consider the following: let $(\\mathbb{X}'_i, \\mathbb{X}'_{aus})$ be a negative pair, and $(\\mathbb{X}'_i, \\mathbb{X}'_{aug})$ be a positive pair, where $\\mathbb{X}'_i$ is an expanding variable sample and $\\mathbb{X}'_{aug}$ is an augmented sample of a continual variable. When the STFE is optimized with imbalanced data, it would focus on continual variables, overshadowing the distinct patterns of expanding variables. This results in an increase in the similarity between the negative pair embeddings $(Z_{aug}, Z_{aus})$, which diminishes the contrast between positive pair embeddings $(Z_{aug}, Z_{aug})$ and negative pair embeddings $(Z_{aug}, Z_{aus})$. Consequently, vanilla contrastive learning becomes less effective.\nTo alleviate this problem, our approach is to concentrate on the samples of expanding variables to compensate for the lack of optimization due to insufficient data. As pointed out by [30], the temperature $\\tau$ in Equation (5) can control the penalty strength on the above hard negative samples. To improve the optimization of expanding variables, we further introduce a focal temperature factor. This factor adjusts the temperature $\\tau$ dynamically, thereby refining the learning process for these critical samples. The focal temperature factor can be formulated as follows:\n$\\tau_i = \\begin{cases} \\alpha * \\tau, 0 < \\alpha < 1.0, & \\text{if } \\mathbb{X}'_i \\text{ is expanding variable}, \\\\ \\tau & \\text{others}. \\end{cases}$ (6)\nFinally, the output layer employs 1 \u00d7 1 convolutions to map the spatio-temporal features $H$ to the desired $Q$ forecasting timesteps. The overall loss function integrates the focal contrastive loss (using Equation (6) as the temperature $\\tau$ in Equation (5))with the mean absolute forecasting error loss. Formally, the joint loss is defined as:\n$\\mathcal{L} = \\frac{1}{B'} \\sum_{i=1}^{B'} \\left( -log\\frac{exp(s_{i,i}/\\tau_i)}{\\sum_{j=1,j \\notin G_i}exp(s_{i,j}/\\tau_i)} + \\frac{1}{2} ||\\mathbb{Y}_i - \\hat{\\mathbb{Y}}_i|| \\right)$. (7)\nwhere $\\mathbb{Y} = (y_1,..., y_{B'})$ represents the ground truths."}, {"title": "4 EXPERIMENTS", "content": "In this section, we comprehensively evaluate the proposed STEV model to investigate its forecasting performance, robustness to various expanding scenarios, and sensitivity to key parameters. Specifically, our experiments aim to answer the following research questions:\n\u2022 RQ1: How does STEV perform on real-world EVTS datasets compared to SOTA methods tailored to EVTSF? (see Section 4.2)\n\u2022 RQ2: What is the impact of each component of STEV on its performance? (see Section 4.3)\n\u2022 RQ3: Does STEV have an advantage over other methods for addressing data imbalance, such as oversampling and augmentation? (see Section 4.3)\n\u2022 RQ4: How does STEV perform under different variable expanding scenarios? (see Section 4.4)\nWe used three real-world datasets across different domains for experiments: Electricity [34], PeMS [15], Weather [21]. As these datasets originally had full observations for all variables, we customized them to adapt to EVTSF. Table 1 shows each dataset's statistics and data splits. For all datasets, both H and Q are set to 12. The details are as follows: (i) EElectricity: This dataset records hourly electricity consumption in kWh of 321 clients from 2012 to 2014. We randomly selected 261 clients as the continual variables over the pre-expanding period (P1), and the remaining 60 clients as the expanding variables over the post-expanding period (P2). (ii) EPEMS: The PeMS dataset consists of traffic flow data in District 7 of California. Since the sensor location is available, we first selected a central sensor as an anchor and then filtered 296 sensors as the continual variables, with the remaining 151 sensors as expanding variables, based on the spatial distance from the anchor. (iii) EWeather: Provided by WeatherBench, this dataset contains 410 nodes on the Earth sphere. We randomly selected 310 nodes as continual variables, and the remaining as expanding variables.\nIn real-world applications, the expansion process varies across different contexts and requirements. Thus, We also tailored the datasets accordingly and explored various expanding scenarios (see Section 4.4). More details can be found in Appendix A.2.\nWe investigated and implemented a series of existing methods that can be tailored for EVTSF, classifying them into three types: (1) UTS Forecasting (UTSF): these methods forecast EVTS as individual UTS, naturally avoiding the inconsistent shape issue; (2) First-padding-then-masking MTSF (FPTM): these methods first standardize data shapes through padding and then use SOTA MTSF models with a masking matrix to filter out padding loss; (3) Continual learning-based MTSF: these methods train a model from scratch with train1 data (P1) and then incrementally learn using train2 data (P2). For each type, we selected widely used models, including GRU [6], N-beats [20], PatchTST [19], and GPT4TS [42]; GWNET [35], AGCRN [1], MSGNET [2], and iTransformer [16]; and TrafficStream [4].\nBesides, we conducted an Oracle setting, assuming the availability of P\u2081 data for expanding variables, allowing each model to be optimized with fully observed data. While impractical, this setting helps verify the upper bound for baselines benefiting from full observations. Further experimental implementations are given in Appendix A.4.\nWe evaluate forecasting performance using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) averaged over the forecasting horizon. Additionally, we propose to evaluate the performance gap to the upper bound by quantifying the performance relative to the Oracle as\n$\\Delta = \\frac{E_{Expanding} - E_{Oracle}}{E_{Oracle}}$. (8)"}, {"title": "4.2 Overall Results", "content": "Table 2 reports the overall performance of different methods on three datasets, including the averaged MAE and RMSE on continual, expanding, and overall variables over 12 timesteps (RQ1). Note that TrafficStream relies on a pre-defined graph to transfer learned information from continual to expanding variables, therefore, we only report the performance on the EPeMS dataset. We observe the following findings: (1) STEV achieves SOTA performance across almost all settings, showing significant average improvements with average reductions of 10.72% in MAE and 11.85% in RMSE compared to the runner-up. This success is primarily attributed to the innovative spatio-temporal focal learning, which effectively focuses on expanding variables. However, STEV does not show improvements for the continual variables in the electricity and weather datasets. This may be due to the coarse-grained temporal modeling with CNNs, akin to the observed performance differences between GWNET and AGCRN. (2) Due to the scarcity of data, the performance of expanding variables declines significantly across all methods. While the GPT4TS and PatchTST separately benefit from the pre-trained large model and weight-sharing mechanism for few-shot learning, they lack explicit spatio-temporal patterns, leading to inferior performance. In contrast, STEV, using only 5% of the complete data (3-day EPeMS data), surpasses methods trained with the Oracle setting, demonstrating its ability to effectively learn representations for expanding variables. (3) Although the FPTM-based strategy introduces padding values, it captures spatio-temporal correlations more effectively than the solely temporal modeling used in UTSF-based strategies, leading to better overall performance. (4) The continual learning approach benefits from leveraging a pre-trained model and incrementally learning from new observations during the expansion period. However, this can lead to catastrophic forgetting, which negatively affects the model's performance on both continual and expanding variables."}, {"title": "4.3 Ablation Study", "content": "We perform an ablation study on the EPeMS data to verify the effectiveness of each component of STEV (RQ2). We review four components of STEV, including (1) FLATS which allows STEV to learn spatio-temporal dependencies under varying variables; (2) Contrastive Learning (CL) which enhances STEV to learn robust representations for both continual and expanding variables; (3) Negative Filter (NF) which allows STEV to exclude inappropriate negative pairs in contrastive learning; (4) FOCALCL which enables STEV to focus on expanding variables and thus alleviate the impact of imbalanced data.\nWe gradually add each of the above modules starting from FLATS. Note that when FOCALCL is added, it forms STEV. From Fig. 5 we observe a continuous performance improvement, except for the variant w/ CL. As discussed in Section 3.3.2, there is a dilemma between the naive CL and spatial module. We thus propose the negative filter to remove conflict negative samples.\nWe additionally compared two conventional solutions for data imbalance (RQ3), i.e., oversampling (OS) and data augmentation (DA). In our experiments, the double sampling rate and the mixup augmentation technique [39] are utilized on the expanding data. Table 3 shows that the performance is comparable with data-level techniques, and using oversampling even leads to a decrease. To explore the effect of the ultra data augmentation, we conduct the DA* experiment, augmenting the entire dataset. Despite the improved performance, the computational overhead doubled."}, {"title": "4.4 Expanding Scenario Exploration", "content": "To explore the generalizability of STEV in real-world scenarios, we conduct comprehensive experiments on the EPeMS dataset under various expansion settings, involving the partition of continual and expanding variables, the number of expansions, and the duration of the expansion period (RQ4).\nWe first simulate three potential demands of expanding variables in the traffic domain, including (1) Area-based expansions where new sensors are deployed in specific focus areas, with those in the top-left region remaining as continual variables (see Fig. 6a); (2) Spatial-based expansions where sensors spread to the surrounding areas with the expansion of urban monitor planning. A central sensor and 296 sensors within a 30 km radius serve as continual variables, while others are expanding variables (see Fig. 6b); and (3) Internal expansions where the denser sensor network increases perception density. We randomly select 296 sensors as continual variables (see Fig. 6c). As shown in Fig. 6, STEV consistently outperforms FPTM-GWNET [35] across all three scenarios, demonstrating strong generalization capability.\nTo assess the adaptability, we conduct a consecutive experiment involving two-time spatial expansions. Table 4 details the variable statistics and durations, while Table 5 reports performance results. The findings show that STEV consistently outperforms FPTM-GWNET across all metrics, for both continual and expanding variables, confirming its robustness in handling dynamic spatial expansions.\nWe further examine STEV's performance over the much longer period of sensor deployment, by conducting experiments on the expanding period over 14 days. Table 6 shows that STEV maintains superiority over FPTM-GWNET, even as data scarcity diminishes over time. Compared to the 3-day results (Table 2), STEV effectively captures evolving spatiotemporal patterns, demonstrating its stability across both short- and long-term deployments."}, {"title": "5 RELATED WORK", "content": "Multivariate Time Series Forecasting. Extensive deep learning-based MTSF methods prove that both capturing intra-series temporal dependencies and inter-series spatial dependencies are beneficial for modeling future changes [16, 23, 36, 40]. Particularly, spatiotemporal graph neural networks such as Graph WaveNet [35] and AGCRN [1] further enhance spatio-temporal representations by combining GNNs with LSTM and GNNs with TCN, respectively. However, these works cannot be directly performed in the EVTSF task. Even with padding strategies, their performance still suffers from data imbalance. Recent efforts [4, 29] have explored the expansion of traffic networks where observed traffic areas are continuously expanding as new sensors are deployed. These studies primarily address this problem as a two-phase learning task within the continual learning or online learning paradigm [5]. The main challenge that these studies intend to address, is the catastrophic forgetting phenomenon [10]. They first discover the continual variables that need to learn new patterns, then employ historical data replay [22], elastic weight consolidation [12], and memory bank mechanism [17] to consolidate historical knowledge. In contrast, the performance achieved by the retraining paradigm is an upper bound of continual learning due to the global optimization for model parameters with better convergence and performance [12, 22]. Another similar task to EVTSF is spatio-temporal Kriging [33], a.k.a. spatio-temporal extrapolation [11], which performs imputation for expanding variables based on the context of continual variables. Although both are aimed at expanding variables, it has different learning objectives from EVTSF.\nContrastive Learning in Time Series. Recently, unsupervised contrastive learning has gained significant traction in the time series domain [14, 26, 38]. The primary objective of this approach is to make positive samples attractive and separate negative samples, thereby enabling the learning of inherent temporal characteristics. Prior efforts mainly explore the effect of positive and negative pairs. For instance, motivated by the local smoothness of a signal during the generative process, TNC [26] encourages the consistency of samples within a temporal neighborhood. TS2Vec [38] learns contextual information between timesteps at different temporal resolutions. In this approach, two augmented views of the same time step exhibit similarity and are considered dissimilar otherwise. To further explore the impact of augmentation, TimesURL [14] employs a frequency-temporal-based augmentation technique to encourage temporal consistency. However, due to the imbalanced data and the potential conflict between negative samples in contrastive learning and neighbors in the graph structure, directly leveraging contrastive learning is hard to benefit EVTSF (refer to Table 3)."}, {"title": "6 CONCLUSION", "content": "In this paper", "challenges": "inconsistent data shape and imbalanced spatio-temporal learning. Specifically, our approach features a Flat Scheme that unifies data shapes with a simple flattening operation, making the model agnostic to variable scale. Besides, we employed Focal Contrastive Learning to enhance the learning of robust and discriminative spatio-temporal feature representations, with a"}]}