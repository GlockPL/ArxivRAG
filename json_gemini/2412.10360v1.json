{"title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models", "authors": ["Orr Zohar", "Xiaohan Wang", "Yann Dubois", "Nikhil Mehta", "Tong Xiao", "Philippe Hansen-Estruch", "Licheng Yu", "Xiaofang Wang", "Felix Juefei-Xu", "Ning Zhang", "Serena Yeung-Levy", "Xide Xia"], "abstract": "Despite the rapid integration of video perception capabilities into Large Multimodal Models (LMMs), the underlying mechanisms driving their video understanding remain poorly understood. Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, we present a comprehensive study that helps uncover what effectively drives video understanding in LMMs.\nWe begin by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover Scaling Consistency, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, we explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more. For example, we demonstrated that fps sampling during training is vastly preferable to uniform frame sampling and which vision encoders are the best for video representation.\nGuided by these findings, we introduce Apollo, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. Our models can perceive hour-long videos efficiently, with Apollo-3B outperforming most existing 7B models with an impressive 55.1 on LongVideoBench. Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME.", "sections": [{"title": "1 Introduction", "content": "Despite the rapid advancements in language and image-language modeling (Hoffmann et al., 2022; Brown, 2020; Yang et al., 2024; Liu et al., 2024a; Alayrac et al., 2022; Lauren\u00e7on et al., 2024a; OpenAI, 2024), the development of video Large Multimodal Models (video-LMMs) has not kept pace. Videos provide a rich, dynamic information source, capturing nuanced temporal and spatial features beyond the reach of static images. However, video-LMMs remain under-explored, hampered by unique challenges: notably higher computational demands and a broader, more complex design space compared to their image-based counterparts (Li et al., 2023a, 2025; Liu et al., 2024d; Li et al., 2024b; Xu et al., 2024a).\nMany fundamental questions about video-LMM design remain unanswered: How should videos be sampled? Which vision encoders yield optimal representations? What are the best practices for resampling video tokens? Early approaches primarily extended image-LMMs directly (Xu et al., 2024b; Kim et al., 2024; Wu, 2024; Zhang et al., 2024e) or with video-specific fine-tuning (Li et al., 2023a; Zhang et al., 2023; Maaz et al., 2023). Recent methods introduced diverse design choices, such as longer context windows (Zhang et al., 2024e), multi-modality mixing (Li et al., 2024a,c), agent workflows (Wang et al., 2024c), self-training (Zohar et al., 2024), and more. Despite these efforts, the impact of these design decisions on video-LMM performance is poorly understood. This lack of systematic investigation motivates our study."}, {"title": "2 How effective are existing video question-answering benchmarks?", "content": "The rapid advancement of video Large Multimodal Models (video-LMMs) has spurred the creation of numerous video question-answering benchmarks, including Video-MME, MLVU, LongVideoBench, and others (Fu et al., 2024; Wu et al., 2024; Zhou et al., 2024; Patraucean et al., 2023; Li et al., 2024d; Wang et al., 2024b; Cai et al., 2024). While this proliferation enables comprehensive evaluation, it also introduces significant resource intensiveness and redundancy. For example, evaluating a 3B-parameter model on these benchmarks requires 184 A100 GPU hours. In this section, we first analyze the quality of existing benchmarks (Sec. 2.1), their redundancy (Sec. 2.2), and introduce ApolloBench (Sec. 2.3) by building on these insights."}, {"title": "2.1 Evaluating benchmark quality", "content": "What drives video benchmark performance is not known. As shown by Goyal et al. (2017), some image question-answering benchmarks are largely driven by text comprehension rather than image perception. Chen et al. (2024a) further showed that data leakage in either the LLM or LMM training stage may be further contaminating evaluation in image question-answering benchmarks. To evaluate the state of video question answering benchmarks, we evaluated ten open-source LMMs on several benchmarks: Video-MME (Fu et al., 2024), TempCompass (Liu et al., 2024c), LongVideoBench (Wu et al., 2024), MLVU (Zhou et al., 2024), NEXTQA (Xiao et al., 2021), and Perception Test (Patraucean et al., 2023) -under three different settings:\n\u2022 Video: Models prompted with video input using their standard video sampling. Green in Fig. 2, left.\n\u2022 Image: Models are provided only the center frame of each video. Red in Fig. 2, left.\n\u2022 Text: Models are prompted with only the original question, without any visual input. Blue in Fig. 2, left.\nAs illustrated in Fig. 2, left, a significant portion of existing benchmarks are answered solely through text comprehension alone (blue boxplots) or only using the center frame (red boxplots), indicating that LMMs do not rely on video perception in a large portion of existing benchmarks. We sorted the benchmarks by the difference between the Video and Text performance (light blue). A benchmark relies more and more on its video perception capabilities when this bar is high. When examining Fig. 2, left, it is apparent that as videos get longer, the reliance on video perception decreases (compare Video-MME S/M/L). To evaluate how much of the benchmarks require video input to answer the question, we also plot the difference between the Video"}, {"title": "2.2 Redundancy in existing benchmarks", "content": "To evaluate the redundancy in video question answering benchmarks, we evaluated ten open-source LMMs on several benchmarks: Video-MME (Fu et al., 2024), TempCompass (Liu et al., 2024c), LongVideoBench (Wu et al., 2024), MLVU (Zhou et al., 2024), NEXTQA (Xiao et al., 2021), and Perception Test (Patraucean et al., 2023). We then calculated the correlation of each of the benchmarks to each other, the result of which can be seen in Fig. 2, right. Our analysis revealed significant redundancy among benchmarks, as evidenced by the block-diagonal correlation matrix, where we can identify groups of benchmarks that are highly correlated.\nTo evaluate the effect of different question types and video durations, we also evaluated the correlations between video duration groups. We find that the performance of models on short and long videos within Video-MME (Fu et al., 2024) exhibits an \\(R^2 = 0.94\\), see App. Fig. 13, while in LongVideoBench, \\(R^2 > 0.92\\) between all duration groups. To assess the effect of question format, we studied the TempCompass (Liu et al., 2024c) dataset, which has different question formats (multiple-choice, yes/no, caption matching, and caption generation), and found that they are also highly correlated (\\(R^2 > 0.8\\)), indicating that varying question types do not significantly diversify the evaluation (see App. Fig. 12)."}, {"title": "2.3 Introducing ApolloBench", "content": "Motivated by these insights, we set out to curate a more effective and efficient benchmark suite called ApolloBench. We focused on multiple-choice questions to eliminate the need for external tools like ChatGPT, ensuring a consistent and cost-effective evaluation process (Wu, 2024).\nWe filtered out questions that could be correctly answered by more than 50% of the models with either text or image inputs, removing questions that do not require video perception (see Fig. 2, left, ApolloBench). Subsequently, we identified five broad temporal perception categories: Temporal OCR, Egocentric, Spatial, Perception, and Reasoning. Questions were then manually categorized into each one of these categories. We selected the top 400 questions from these categories that exhibited the most discrimination between models via entropy and manually verified each one to validate the correctness of the selected questions. Evaluating on ApolloBench is 41\u00d7 faster while being highly correlated with existing benchmarks (see Fig. 2, right) and more influenced by video perception (Fig. 2, left). For more details, see App. Sec. B and App. Fig. 11."}, {"title": "3 Scaling Consistency: How small can you go during model design?", "content": "Developing Large Multimodal Models (LMMs) poses significant computational challenges, especially when training on extensive datasets with billion-parameter models. To make the research process more efficient, it is essential to determine whether smaller LMMs and datasets can reliably inform design decisions for larger ones. Traditional scaling laws require training multiple models of varying sizes for each design decision to derive how performance scales with model size. However, in the context of LMMs, which typically utilize multiple pre-trained components (e.g., vision encoders, language models), scaling each component individually is impractical due to the lack of availability of such components and the immense computational resources required. As such, we set to relax these scaling laws and instead reason about correlation or transfer of design decisions between models of different sizes.\nThis section investigates the correlation between design decisions made on LMMs of different sizes. Specifically, we selected 21 model variations encompassing various design aspects such as architecture, video sampling methods, training strategies, and data mixtures. Each variation was trained using four different Large Language Models (LLMs): Qwen2-0.5B, Qwen2-1.5B, Qwen1.5-4B, and Qwen2-7B (Bai et al., 2023; Yang et al., 2024), resulting in a total of 84 models. We then analyzed the correlation (\\(R^2\\)) between the performance of these models (see App. Fig. 15). Our findings reveal that design decisions on models of a critical size"}, {"title": "Large Language Model size.", "content": "In Fig. 3, left, we plot the \\(R^2\\) values between models of various sizes and the 7B LLM model variant. The correlation with the 7B LLM increases approximately log-linearly with the size of the smaller LLMs and generalizes between model families. This behavior is not observed with smaller models, e.g., 0.5B, where \\(R^2\\) immediately drops below 0.8, and no log-linear behavior can be observed. This reinforces the existence of a critical model size (~ 2 \u2013 4B) where design decisions transfer reliably\u2014a phenomenon we term Scaling Consistency. Scaling Consistency seems to generalize between model families, as a mix of Qwen1.5 and Qwen2 models were utilized in this study. For example, while the Qwen2-1.5B and Qwen1.5-4B model variants had similar performance, the 4B Qwen1.5-4B was still more correlated than the 1.5B model. Please refer to the App. Sec. D for a comprehensive analysis."}, {"title": "Impact of dataset size.", "content": "We examined the impact of dataset size on model performance by training models using the same data mixture but varying the dataset size from 75K to 1M samples. The results are shown in Fig. 3, right, where the correlation of the 0.5/1.5/4B models trained on varying datasets sized to 7B trained on the full dataset can be seen as a function of dataset size. Focusing on the 4B LLM variant, we observed that the correlation (\\(R^2\\)) with larger models plateaus around ~ 500K samples, indicating that increasing the dataset size beyond this point yields diminishing returns in terms of informing design decisions. In contrast, smaller models (e.g., 0.5B and 1.5B) exhibited less consistent behavior, with their \\(R^2\\) values fluctuating more across different dataset sizes. This suggests that a dataset size of approximately 500K samples is sufficient for moderately sized models (2-4 billion parameters) to reliably transfer design insights to larger models."}, {"title": "Finding 1:", "content": "We discover Scaling Consistency, where design decisions can be made on smaller models and datasets and transfer reliably to larger ones."}, {"title": "4 Exploring the video-LMM design space: what influences effective model design?", "content": "In this section, we analyze key architectural design choices shaping the performance of Large Multimodal Models (LMMs) in video-language tasks. We focus on four critical aspects: (I) Video sampling (Sec. 4.1) where we compare uniform and fps video sampling and evaluate the effect tokens and frames per second have on downstream performance. (II) Video representation (Sec. 4.2) where we explore how image and video encoders impact video representation and show which encoder and encoder pairs lead to the best performance. (III) Video token resampling (Sec. 4.3) where we test different visual token resamplers. (IV) Video token integration (Sec. 4.4) where we examine various strategies to integrate the visual token into the text tokens.\nUsing Scaling Consistency, we opted to perform the following exploration using Qwen2.5 3B (Yang et al., 2024) and trained on a dataset of 750K samples. As demonstrated in Sec. 3, these findings exhibit a strong correlation (\\(R^2 > 0.9\\)) with results on larger models and across different model families. Unless stated otherwise, a Perceiver Resampler (Jaegle et al., 2021) was employed, with 16 tokens per frame at a frame rate of 2 fps. The dual encoders used were InternVideo2 (Wang et al., 2024d) and SigLIP-SO400M (Zhai et al., 2023). When training on images, images were duplicated before being encoded by the video encoders for fully integrated encoding, as we found it to be slightly more performant with fewer parameters and complexity (see App. Sec. C.2). This is in line with Lin et al. (2023)."}, {"title": "4.1 Video sampling", "content": "Videos can be sampled in many ways, from uniform sampling - uniformly sampling N frames from the video (Li et al., 2023a; Lin et al., 2023; Jin et al., 2024; Zhang et al., 2024f), to fps sampling - sampling at a set number of frames per second. While many recent methods have preferred fps sampling (Liu et al., 2024d; Li et al., 2024a), they default to uniform sampling when video durations exceed their frame sampling capacity (usually ~64). The maximum frame capacity is typically constrained due to the memory requirement at the vision encoder and or the LLMs context window.\nUniform frame sampling enables simplified training because the effective 'vision batch size' (i.e., the number of frames that need to be encoded) remains constant. However, training video-LMMs with uniform frame sampling means that the time difference between concurrent frames changes with each video, effectively setting a different 'video speed' in every iteration. As a result, when uniformly sampling N frames from videos of varying lengths, the effective playback speed represented in the sampled frames changes. For a shorter video, N uniformly sampled frames represent a slower playback (more frames per second of actual content), while for a longer video, those same N frames represent a faster playback. This will likely hamper the LMMs' capability"}, {"title": "Finding 2:", "content": "fps sampling is preferable over uniform sampling during model training and inference."}, {"title": "Finding 3:", "content": "There is a trade-off between tps and fps, with 8-32 tokens per frame being optimal."}, {"title": "4.2 Video representation", "content": "Training effective video encoders is challenging due to the high memory requirements for processing large video datasets and the comparatively low quality of available supervision. While early approaches predominantly used dedicated video encoders (Li et al., 2023a,b; Lin et al., 2023), recent developments favor image encoders instead (Liu et al., 2024b; Li et al., 2024a; Liu et al., 2024d). This shift arises because image encoders, although lacking temporal integration, still produce higher-quality representations that the LLM can readily leverage. Another possibility is that in this approach, image and video datasets can be fully integrated, possibly benefiting from image-video transfer and allowing the utilization of the much larger, more diverse, and more efficient image instruction tuning datasets (Li et al., 2024a; Zhang et al., 2024e).\nMultiple studies have conducted extensive investigations into visual representation within image-LMMs (Shi et al., 2024; Tong et al., 2024). Lauren\u00e7on et al. (2024b) found that SigLIP outperformed even much larger encoders, such as EVA-CLIP-5B. Zhan et al. (2024) showed that input image resolution influences performance more than token count, which may have influenced Lauren\u00e7on et al. (2024b)'s ablation. Wang et al. (2023) compared encoders trained with supervision and found where each is preferable. However, whether image or video encoders are preferable for video-LMMs and what influences their performance is unclear. As such, we set out to find what drives good video representation in LMMs. We trained LMMs with several image and video encoders and their combinations and evaluated how this design decision impacted the final model performance. Our study includes diverse language- and self-supervised video/image encoders:"}, {"title": "Finding 4:", "content": "SigLIP-SO400M is the best single encoder for video-LMMs."}, {"title": "Finding 5:", "content": "Combining SigLIP-SO400M with InternVideo2 leads to the best overall performance."}, {"title": "4.3 Video token resampling", "content": "Vision encoders output vision embeddings in a lower dimensionality than LLMs' hidden dimension, requiring a 2-4\u00d7 up-projection. Early methods often up-projected all visual tokens directly into the LLM's space (Lin et al., 2023; Li et al., 2023a). This approach leads to informational waste by instilling a synthetic information bottleneck. Lauren\u00e7on et al. (2024b) demonstrated that resampling image tokens (where multiple up-projected tokens are pooled into one) does not reduce performance in image-LMMs. Token resampling is even more critical in video-LMMs as this directly affects how many frames can be processed, limiting the maximum video length. Video token resampling can be text-guided (e.g., using a Q-Former) (Li et al., 2025, 2023b; Zhang et al., 2023). However, this approach does not generalize well to multi-turn conversations, as tokens will be down-sampled according to the first question. Many others do some form of average pooling (Jin et al., 2024; Lan et al., 2024; Xu et al., 2024b).\nShi et al. (2024) tested multiple encoder integration approaches and found that channel-wise concatenation was preferable in nearly all configurations. Therefore, we adopted channel-wise concatenation in our experiments. We tested three token resampling methods: mlp up-projection + average pooling, 2D conv + average pooling, and perceiver resampling. As shown in Tab. 1, the Perceiver Resampler outperforms the other methods across all metrics. While Lauren\u00e7on et al. (2024a) reported that utilizing the Perceiver Resampler hurts OCR performance; this trend was not observed in videos with the limited available token count per frame. Another key difference is the initial channel-wise concatenation of encoder features before resampling. This alignment enables the Perceiver to integrate features from different encoders better as they are better spatially aligned."}, {"title": "Finding 6:", "content": "Perceiver resampling shows superior performance when reducing the tokens/frame."}, {"title": "4.4 Video token integration", "content": "Integrating video and text tokens is a pivotal design choice for video-LMMs, as it directly influences how effectively the model processes and interprets multimodal content. Initial works naively concatenated the text and video tokens (Jin et al., 2024; Li et al., 2023a; Lin et al., 2023). However, recent trends have begun to either use separation tokens (Liu et al., 2024b) or via text (where a prompt is inserted between frames, usually indicating either frame ID or timestamp) (Li et al., 2024a). This design choice was also systematically ablated by Zhao et al. (2024a). To identify the most robust integration strategy, we experimented with four different methods, which can be seen in Tab. 2. We evaluated four integration strategies: direct insertion, separation tokens, textual timestamps, and combining separation tokens with timestamps. As can be seen, we found that adding any text or learnable tokens between video tokens results in a 2-3% improvement across ApolloBench. As such, we use the clip timestamps as they do not require learning any new token embeddings."}, {"title": "Finding 7:", "content": "Adding tokens (text, learned, etc.) between the video tokens derived from different frames or clips is sufficient for efficient token integration."}, {"title": "5 How should video-LMMs be trained?", "content": "This section explores different training schedules and protocols for video Large Multimodal Models (video-LMMs). We begin by testing different training schedules and comparing single to multi-stage training (Sec. 5.1). We then examine when video encoders should be trained and with what data (Sec. 5.2). Finally, we explore how data composition affects performance (Sec. 5.3)."}, {"title": "5.1 Training schedules", "content": "We systematically evaluated the impact of different training schedules on model performance, comparing single-stage, two-stage, and three-stage training protocols. Some studies have suggested that a single-stage training protocol performs similarly to two-stage ones but is more computationally efficient (Karamcheti et al., 2024). However, Tong et al. (2024) demonstrated that two-stage training improved model performance. Since then, many methods have broken down training into more and more training stages. For example, Li et al. (2024a); Liu et al. (2024b) utilized four training stages.\nVideo-LMMs are typically trained on a mixture of text, image, multi-image, and video data; it is possible to break down training into even more training steps, each with different components unfrozen and trained on different data compositions. For example, many video-LMMs include an additional, final training stage on long videos as these datasets are expensive and relatively small (Li et al., 2025; Liu et al., 2024b). Others first train on exclusively image datasets before training on multiple modality mixtures (Li et al., 2024a). We tested seven possible training configurations to evaluate the effect of these different training strategies.\nAs shown in Tab. 3, we found that gradually training the model yields the best performance. Specifically, we found that training the model over three stages yields the best performance, closely followed by the two-stage training schedules. Please note that different stages have different data compositions; specifically, whenever the LLM is frozen, the other components are tuned only on video data, and when the LLM is tuned, a mixture of text, image, multi-image, and video (following Sec. 5.3) is used."}, {"title": "Finding 8:", "content": "Progressively unfreezing the different components in different stages leads to superior model training dynamics."}, {"title": "5.2 Training video encoders", "content": "It is unclear when and with what data one should train the video encoders. Tong et al. (2024) reported that training image encoders is beneficial in image-LMMs. However, video-LMMs are trained on a mixture of video, multi-image, and image data. Furthermore, to have a unified encoding scheme, images are replicated N times to be encoded by the video encoder. As such, these models have additional dimensions of the data mixture on which the encoders can be trained. We compared training vision encoders on either the data mixtures or exclusively on video data and whether first aligning the connector improves performance in Tab. 3.\nIn all experiments, if the LLM is frozen, the model is trained only with video data. When the LLM is unfrozen, we use a data mixture of text, image, multi-image, and video data as described in Sec. 5.3. As such, if both the video and LLM are unfrozen simultaneously, the vision encoders will be trained on a combination of image and video data. We found that this significantly hurts LMM performance. Training the encoders improves"}, {"title": "Finding 9:", "content": "Finetuning video encoders on only video data further improves overall performance, especially on reasoning and domain-specific tasks."}, {"title": "5.3 Data composition", "content": "The composition of the training data plays a significant role in the performance of LMMs, as illustrated by Zhang et al. (2024a). We investigated how the text, image, and video data mixtures affected video-LMMs performance. Specifically, we randomly selected several data compositions, as illustrated in Fig. 6. As can be seen, including 10 ~ 14% text data in the training mix is required for performance. This likely alleviates catastrophic forgetting. Increasing the proportion of text data beyond 14% to 25%, or decreasing it below 7%, harmed performance. Beyond including text data, having a slightly video-heavy mix of the remaining modalities was preferable. This balance allows the model to learn from higher-quality and diverse image datasets (Li et al., 2024a; Lin et al., 2023)."}, {"title": "Finding 10:", "content": "Data mixture matters, and including a moderate amount of text data and maintaining a slight video-heavy mix leads to optimal performance."}, {"title": "6 Apollo: a family of state-of-the-art large multimodal models", "content": "We leverage the findings from our studies and train a family of video-centric Large Multimodal Models (LMMs), Apollo. Apollo models have state-of-the-art performance across multiple model sizes, frequently outperforming models 2-3\u00d7 their size. We employed the Qwen2.5 (Yang et al., 2024) series of Large Language Models (LLMs) at varying scales to serve as the backbone for Apollo. Specifically, we utilized models with 1.5B, 3B, and 7B parameters. Following our analysis in Sec. 4, we used a SigLIP-SO400M (Zhai et al., 2023) encoder combined with an InternVideo2 (Wang et al., 2024d) video encoder. Features from each encoder are interpolated and concatenated along the channel dimension before being resampled to 32 tokens/frame using a Perciver Resampler (Jaegle et al., 2021). We utilized the 3-stage training schedule discussed in Sec. 5.1.\nWe curated a diverse mixture of publicly available and licensed datasets spanning text, image-text, multi-image, and video modalities. Due to licensing constraints, we omitted non-permissive sources (e.g., those reliant on ChatGPT), limiting the inclusion of some commonly used datasets. To further enhance our training corpus, we generated multi-turn video-based conversations via an annotation tool powered by LLaMA 3.1 70B (Touvron et al., 2023). Figure 7 provides a detailed overview of our data composition and statistics.\nWe evaluated Apollo across a suite of benchmarks to assess its performance in video-language understanding tasks, including TempCompass (Liu et al., 2024c), MLVU (Zhou et al., 2024), Perception-Test (Patraucean et al., 2023), Video-MME (Fu et al., 2024), LongVideoBench (Wu et al., 2024), and ApolloBench. As shown in Tables 4, Apollo models demonstrate strong performance across benchmarks. Notably, Apollo-3B outperforms several recently introduced 7B models, such as Oryx-7B (Liu et al., 2024d), Kangaroo (Liu et al., 2024b), and Video-XL-7B (Shu et al., 2024). For instance, on the MLVU benchmark, Apollo-3B achieves a score of 68.7, surpassing Oryx-7B's 67.5. Similarly, Apollo-1.5B outperforms models larger than itself, including Phi-3.5-Vision (4.2B parameters) and some 7B models like LongVA-7B (Zhang et al., 2024e), indicating that smaller models can suffice for proof-of-concept implementations. We hope these results will motivate the field to utilize such smaller models for faster prototyping in the future.\nFurthermore, Apollo-7B establishes a new performance frontier for models at the 7B scale, rivaling and even surpassing models with over 30B parameters such as Oryx-34B and VILA1.5-40B (Lin et al., 2024). On the MLVU benchmark, for instance, Apollo-7B scores 70.9, narrowly outperforming Oryx-34B's 70.8. These gains highlight the potency of our design insights and confirm that carefully chosen architectural and training strategies can yield substantial improvements without resorting to larger model sizes."}, {"title": "7 Background", "content": "Video Large Multimodal Models. Early video-LMMs (Yang et al., 2022; Zhu et al., 2023b; Maaz et al., 2023; Xu et al., 2024a) relied on sparsely sampled frames and MLP connectors or entirely training-free methods (Kim et al., 2024; Wu, 2024). To address token count and support long-form video understanding, subsequent works introduced resampling methods such as spatio-temporal pooling (Zhang et al., 2023; Shen et al., 2024; Xu et al., 2024a; Jin et al., 2024; Zhang et al., 2024g; Xu et al., 2024b). Most approaches (Fei et al., 2024; Jin et al., 2024; Liu et al., 2024b,d; Shen et al., 2024) use image-based encoders, with only a few (Lin et al., 2023; Chen et al., 2024b; Li et al., 2023b) employing video-specific encoders to capture temporal dependencies."}, {"title": "8 Conclusion", "content": "In the study, we critically evaluated the current state of the video Large Multimodal Model (video-LMM) field, from architecture design and training schedules to data mixtures and evaluation. In part, we hope that concepts such as Scaling Consistency encourage researchers to utilize smaller LMMs in their research, while ApolloBench will allow for faster and more comprehensive evaluation. We hope our insights into the key aspects of video-LMM design, encompassing video sampling, encoder selection, token resampling, and token integration, will further democratize video-LMM research, further accelerating research in the field.\nBuilding upon these insights, we developed Apollo, a family of state-of-the-art LMMs capable of advanced video-language understanding. Notably, Apollo-3B outperforms most advanced 7B models, while Apollo-7B outperforms all 7B models and many recent 30B models. Our findings highlight that careful design and training strategies can yield superior performance without necessitating larger model sizes. We believe that our work provides valuable guidelines and resources for future research, advancing the development of efficient and effective video-LMMs."}]}