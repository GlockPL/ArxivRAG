{"title": "Lab-AI - Retrieval-Augmented Language Model for Personalized Lab Test Interpretation in Clinical Medicine", "authors": ["Xiaoyu Wang", "Haoyong Ouyang", "Balu Bhasuran", "Xiao Luo", "Karim Hanna", "Mia Liza A. Lustria", "Zhe He"], "abstract": "Accurate interpretation of lab results is crucial in clinical medicine, yet most patient portals use universal normal ranges, ignoring factors like age and gender. This study introduces Lab-AI, an interactive system that offers personalized normal ranges using Retrieval-Augmented Generation (RAG) from credible health sources. Lab-AI has two modules: factor retrieval and normal range retrieval. We tested these on 68 lab tests\u201430 with conditional factors and 38 without. For tests with factors, normal ranges depend on patient-specific information. Our results show GPT-4-turbo with RAG achieved a 0.95 F1 score for factor retrieval and 0.993 accuracy for normal range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 29.1% in factor retrieval and showed 60.9% and 52.9% improvements in question-level and lab-level performance, respectively, for normal range retrieval. These findings highlight Lab-AI's potential to enhance patient understanding of lab results.", "sections": [{"title": "Introduction", "content": "The Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 played a key role in promoting the adoption and meaningful use of electronic health records (EHRs) throughout the U.S. healthcare system. Through the Medicare and Medicaid EHR Incentive Programs, the Act provided financial incentives that facilitated widespread EHR adoption. In December 2016, the 21st Century Cures Act further advanced healthcare by reinforcing patients' rights to have timely and electronic access to their health information [1]. This legislation supported the development and use of patient portals by mandating that patients have electronic access to their health records. Additionally, the Act's anti-information blocking provisions encouraged healthcare organizations to utilize patient portals effectively, ensuring patients can access their health data without unnecessary obstacles. Patient portals are crucial in engaging patients, as they enable individuals to view their medical records, communicate with their healthcare providers, and manage appointments and medications [2].\nEven though viewing lab results is the most frequent activity in patient portals, there are also challenges for patients. Lab results are often presented in medical jargon that can be difficult for patients to understand. Lab results typically provide reference ranges (or normal ranges), determined by analyzing lab results from a large, diverse group of healthy individuals [3]. The normal range is typically defined as the interval between two values that encompass the middle 95% of the data (often referred to as the 95% reference interval). This means that 95% of healthy individuals' results fall within this range, and 5% fall outside of it. The normal range is then validated with clinical data and adjusted if necessary. However, lab test results can vary by age, sex, and other factors. For example, the normal range for red blood cell count (RBC) for men is 4.7 to 6.1 million cells/mcL and 4.2 to 5.4 million cells/mcL for women. Unfortunately, mainstream patient portals usually present universal normal ranges for lab tests based on a large population sample as a general baseline for what is considered normal, making it difficult for patients to interpret their results in a personalized context. Implementing individualized normal ranges requires complex adjustments and may also need input from patients. The advent of generative large language models (LLMs) such as ChatGPT offers a promising solution for identifying more personalized normal ranges by prompting the patient to provide necessary information missing in the medical records. In this work, we explore the use of LLMs with Retrieval-Augmented Generation (RAG) approach to identify the factors that may influence lab test results and to determine normal ranges for lab tests.\nRAG improves the output of a Large Language Model (LLM) by integrating curated information from authoritative external sources, outside the model's original training data, before generating a response [4]. The external knowledge base is transformed into text embeddings, which are vector representations of natural language that encode its semantic information. Text embeddings play a pivotal role in various natural language processing (NLP) tasks, including information retrieval (IR) and RAG [5]. It is generally reported that RAG, which combines both retrieval and generative techniques, enhances the accuracy and contextual understanding of LLM by incorporating authoritative external sources into their responses. This approach reduces the likelihood of generating incorrect information, offers flexibility to adapt to various domains, and allows for scalability as knowledge bases can be updated without retraining the model. Additionally, RAG improves transparency by listing sources of the response, making it easier to trace the information used, thereby reducing hallucinations, one of the widely reported issues of LLMs."}, {"title": "Related Work", "content": "RAG has been successfully applied in clinical applications such as interpreting pharmacogenomics lab testing results, improving patient admission prediction, summarizing and extracting malnutrition information, and for treatment recommendations and medical guidelines. Murugan et al. evaluated a GPT-4-based AI assistant with RAG to enhance pharmacogenomic decision-making by interpreting PGx test results, using data from the Clinical Pharmacogenetics Implementation Consortium [6]. The study demonstrated high efficacy in addressing specialized provider queries and emphasized the potential for improving accuracy, relevance, and ethical considerations for wider implementation of LLM with RAG in clinical practice. Alkhalaf et al. tested zero-shot prompt engineering in LLaMa 2, both alone and with RAG, for summarizing and extracting malnutrition data from EHRs in aged care facilities [7]. LLaMa 2 achieved high accuracy in summarizing nutritional status (93.25%) and extracting risk factors (90%). Implementing RAG improved summarization accuracy to 99.25% and reduced hallucinations but it did not further enhance risk factor extraction. Zakka et al. evaluated the performance of a RAG model called Almanac for clinical decision-making, and compared it with standard large language models like GPT-4, Bing, and Bard [8]. RAG was implemented by integrating curated medical resources such as PubMed, UpToDate, and BMJ Best Practices with the model to enhance factuality, completeness, and adversarial safety in response to clinical questions. The study reported that Almanac significantly outperformed other models in factuality and completeness, particularly in adversarial settings, demonstrating the potential for retrieval-augmented models to improve clinical decision support.\nLLMs are increasingly being used for interpreting lab test results in clinical contexts, offering potential enhancements in accuracy and efficiency. He et al. conducted a study comparing the quality of responses from various LLMs, including GPT-4, GPT-3.5, LLaMA 2, MedAlpaca, and ORCA mini, to questions related to lab test results on a social Q&A platform Yahoo! Answers [9]. They found that GPT-4 outperformed other models, achieving higher scores in accuracy, helpfulness, relevance, and safety, with superior handling of complex lab tests such as hemoglobin Alc, bilirubin, and lipid panels. Munoz-Zuluaga et al. evaluated the ability of GPT-4 to answer a representative set of questions frequently encountered in the laboratory medicine field, ranging from basic knowledge to complex interpretation of laboratory data in a clinical context [10]. They found that while GPT-4 correctly answered 50.7% of the questions, it also produced incomplete, incorrect, or irrelevant responses in the remaining cases. Cadamuro et al. reported that while GPT-4 could recognize and provide basic interpretations of individual lab tests such as glucose, HbA1c, and complete blood count (CBC), it struggled with integrating these results into a coherent clinical interpretation, often failing to recognize conditions like Gilbert's syndrome or pre-analytical issues such as non-fasting glucose samples [11]. Meyer et al. compared the performance of GPT-4, Gemini, and Le Chat in interpreting laboratory questions related to CBC from the online health forum 'AskDocs' subreddit on Reddit [12]. The study found that while GPT-4 and the other chatbots provided empathetic and structured responses, their accuracy and medical reliability were significantly lower than that of online physicians.\nThese studies underscore the critical role of LLMs in interpreting lab test results, highlighting both their potential and current limitations. While LLMs like GPT-4 have demonstrated considerable accuracy in certain scenarios, such as interpreting complex cases involving serum lipase and blood lead levels, they still fall short compared to human experts in ensuring consistent and reliable results across a broad spectrum of laboratory data. Moreover, none of these prior studies evaluated LLMs' capability in determining the normalcy of lab results. The findings emphasize the necessity for continued development and rigorous validation of LLMs to enhance their utility in clinical settings, where precise and accurate interpretation of lab tests is critical for patient safety. The current study employs RAG with LLMs for interpreting lab test data, especially the normal ranges. By integrating real-time retrieval of lab test relevant information through RAG, the study aims to enhance the accuracy and reliability of LLMs in interpreting lab test data, thereby improving clinical decision-making processes."}, {"title": "Methods", "content": ""}, {"title": "System Overview", "content": "In this study, we utilized a RAG-based LLM, Lab-AI, to help the patients find an authoritative lab normal range. Our objective is to design an AI assistant that could incorporate knowledge from credible sources and provide an accurate normal range, leveraging the power of the LLMs. We employed a RAG-Sequence model, which retrieves documents and generates a full response based on each document individually. For each retrieved document, the model generates a complete response independently. It then selects the best response by comparing the likelihood scores of the different generated sequences[4]. With the support of RAG, which synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases [13], the accuracy and credibility of the generation will be enhanced.\nFigure 2 illustrates users' interactions with Lab-AI. The tool will start with a user's question. The format of questions can vary but we only tested questions like \"What is the normal range for [lab test name]?\". For each user input, the system employs OpenAI's \u201ctext-embedding-3-large\" model to embed the query. The embedding model is designed to act as a text encoder, transforming the input into a high-dimensional vector representation. The dimensions of these embedded queries are consistent with the data stored in the vector database, allowing for efficient and accurate retrieval of information."}, {"title": "Step 1: Vector Database Generation", "content": "We obtained a set of lab test articles from MedlinePlus [16], a credible online health information resource developed and maintained by the U.S. National Library of Medicine. Following the pipeline depicted in Figure 3, we crawled data from the medical encyclopedia pages by iterating over the initials from A to Z and 0-9. For each page, we specifically examined the \u201cNormal Results\" section. If the web document included this section, we stored the lab name, the corresponding text under \u201cNormal Results\u201d and the URL. To facilitate the extraction of lab test information from these articles, we used Beautiful Soup, a Python package designed for parsing HTML and XML markup documents [17]. The vector database is an efficient vector data storage that allows fast and accurate similarity search and retrieval [18]. We first reformatted the data into the format \"[labname]: NormalResult\". We opted to use \"text-embedding-3-large\" from OpenAI as our embedding model and converted all lab normal ranges into 3072-dimensional vectors. Additionally, we included URLs as metadata so that the RAG system can link to the appropriate source on MedlinePlus when necessary. We chose ChromaDB [19] as the vector database due to its compatibility with OpenAI embedding functions."}, {"title": "Step 2: Data Preparation", "content": "As shown in Figure 4, a total of 555 lab tests were identified, 96 of which had associated factors or consisted of multiple tests, while the remaining 459 did not have factors. We manually filtered out the tests with factors that lacked normal ranges, as well as those that comprised multiple tests. After filtering, the final number of lab tests with factors was 30. A similar filtering process was applied to the tests without factors, reducing their number from 459 to 192. From these, 38 (20%) non-factor lab tests were randomly selected as our convenience sample. Our focus was primarily on lab tests using blood or urine as specimens. Over 72% of the tests in the convenience sample were blood tests, while 16% used urine. Other specimens included tissues, fluids, marrow, and others.\nOur study consists of two experiments. The first experiment was designed to test the ability of RAG-based LLMs to retrieve the factors associated with lab tests. The second experiment evaluates whether RAG-based LLMs can accurately retrieve the correct normal ranges given different factors. Non-RAG-based LLMs serve as the baseline for comparison. Consequently, two datasets were created for these experiments.\nLab Test Factor Dataset: This dataset supports our first objective to evaluate if RAG-based LLM can accurately extract and prompt the key factors determining the normal range for a lab test. To create the ground truth, we first manually extracted the factors for determining normal reference ranges for the included 68 lab tests. An example can be found in Figure 1. We only considered the factors that determine different normal ranges. Factors that may affect the lab test but do not have a specific normal range in the article were excluded. For example, the normal range of \"Aldolase\u201d from MedlinePlus is: \u201cNormal results range between 1.0 to 7.5 units per liter (0.02 to 0.13 microkat/L). There is a slight difference between men and women.\" Although the normal range for men and women can be slightly different, we did not count gender as a factor because they do not have specific normal ranges in the article."}, {"title": "Step 3: Experiments of factor retrieval and normal range retrieval", "content": "Factor retrieval is a crucial step in identifying the key factors determining the accurate normal range for a specific lab test. We designed a universal prompt and tested it across three OpenAI LLMs: GPT-3.5-turbo, GPT-4-turbo, and GPT-4 to assess if these LLMs can identify the critical factors associated with the normal ranges of a lab test.\nWe maintained the default settings, except for setting the temperature to zero to ensure consistency and minimize randomness. The prompt used for LLMs can significantly influence the quality of their responses. Initially, we crafted a simple prompt asking the LLMs which factors might influence the normal range for a given lab test, expecting a list of factors in return. However, the LLMs returned detailed factors along with their corresponding normal ranges, and the RAG system failed to retrieve information from our stored vector database as expected. To refine the prompt, we consulted GPT-4, which helped us improve its structure. The final prompt used a step-by-step instruction format: first, analyze the information from the vector database, then inform the LLM that we are only interested in factors associated with a numeric value. The LLM was instructed to output only those key factors and respond with \u201cNone\u201d if no factors were relevant. For GPTs without RAG, we omitted the first step since it does not require information retrieval from the vector database. Additionally, we used a 2-shot example to help the LLMs better understand the desired output. For evaluation, we summed up all of the True Positives, False Positives, and False Negatives first, then calculated the corresponding precision, recall, and F-1 score for both RAG-based and non-RAG-based systems based on three selected LLMs.\nAccurate normal range retrieval is the primary goal of our study. We selected the best-performing model from the factor retrieval task, as accurately detecting lab test factors is the prerequisite for retrieving the correct normal range. Similar to the first task, we minimized response randomness by setting the temperature to zero. The system prompt was generated through iterative refinement using GPT-4-turbo, incorporating some examples of desired outputs. The user prompt is a straightforward question requesting the normal range given specific factor values, e.g., \"What is the normal range for urine 17-ketosteroids test in males?\"\nThe expected response is a precise value or range with appropriate units. However, it is unfair to directly compare the retrieved values between LLMs and RAG-based LLM system since we already let the RAG system know the expected values. As a result, when we evaluated the non-RAG LLMs' results, we used many other credible lab test references, including MKSAP [20], LOINC [21], IAPAC [22], American Board of Internal Medicine [23], Health Encyclopedia by University of Rochester Medical Center [24] and ARUP laboratories [25]. If non-RAG LLMs' response matches any of those references, we considered that response as correct. For the RAG system, only the exact match was considered correct. This evaluation could demonstrate the superiority of the RAG-based systems. Both question-level and lab-level accuracy will be used as metrics for this task. Question-level accuracy is defined as the ratio of number of questions with correctly retrieved ranges to the total number of questions. Lab-level accuracy is calculated as the sum of the average accuracy values of questions for each lab test over all the tests divided by the total number of lab tests."}, {"title": "Results", "content": "For the factor retrieval task, GPT-4-turbo outperformed the other models in both RAG and non-RAG systems. Surprisingly, without the RAG system, GPT-3.5-turbo showed better results than GPT-4, achieving significantly higher precision and F1, although it did not perform as well with RAG, as seen in Table 2 where GPT-4-turbo with RAG achieved a 0.95 F1 score. Table 3 lists some cases of the RAG-based system with different GPT models. GPT-3.5-turbo with RAG will respond with choice-level factors. We cannot say it is completely wrong, but it did not follow our system prompt requirement. GPT-4 with RAG tended to provide extra factors in addition to the true labels. Some of those factors were mentioned in MedlinePlus but are not closely associated with a clear normal range. Some factors may have come from its own knowledge base. GPT-4-turbo with RAG showed more balanced performance but it may also prompt additional factors or miss some factors.\nSince GPT-4-turbo with RAG outperformed the other models, we further evaluated it in the normal range retrieval task. Table 4 shows the performance of normal range retrieval at the question and lab levels. GPT-4-turbo without RAG performed poorly with only an accuracy of 38.4% at the question level and 45.6% at the lab level. GPT-4-turbo with RAG performed exceptionally well, with only one error out of 151 generated questions. For lab tests with relevant factors, the accuracy reached 100%. Even though we also compared the results from the non-RAG GPT-4-turbo with multiple credible sources, its performance still remained unsatisfactory. Table 5 highlights the single error, along with a similar case in which GPT-4-turbo with RAG provided a correct response. Unlike most lab tests, the normal ranges for the acid-fast stain and anti-smooth muscle antibody tests are \u201cno acid-fast bacteria found\" and \"no antibodies found,\" respectively. Although these results could be interpreted as zero, we did not apply any preprocessing for these cases."}, {"title": "Discussion", "content": "In this study, our aim was to develop an interactive Lab-AI system capable of providing accurate normal reference ranges for lab tests specific to each patient. This is achieved by first identifying the factors that affect normal reference ranges, and then retrieving the appropriate range based on the patient's responses to those factors. According to the results in Table 2, GPT-4-turbo with RAG achieved an F1 of 0.95 on this task, whereas the original GPT-4-turbo reached only an F1 of 0.659. Typically, GPT-4, with its larger number of parameters compared to GPT-3.5-turbo, would be expected to outperform GPT-3.5-turbo. However, in the factor retrieval task, GPT-4 yielded lower precision and F1. While GPT-4-turbo achieved the highest recall, making it the most effective non-RAG model at retrieving relevant factors, its recall was still below 0.7. With RAG, GPT-4-turbo outperformed other models in retrieving accurate and relevant cases. GPT-4 with RAG significantly outperformed GPT-3.5-turbo with RAG, indicating better compatibility between GPT-4 and the RAG system. Interestingly, while GPT-4 with RAG excelled at retrieving relevant factors, it was also more prone to providing factors not listed in the original source. This may be due to its tendency for hallucinations, whereas GPT-4-turbo is more stable and less prone to generating creative but incorrect information.\nWe further evaluated the RAG system's ability in retrieving normal reference ranges using the best factor retrieval model. Using GPT-4-turbo as the baseline, we relaxed the correctness criterion from an exact match with MedlinePlus to a match with any credible source. While GPT-4-turbo achieved less than 50% accuracy, GPT-4-turbo with RAG correctly retrieved nearly all normal ranges, making only one error. We implemented two evaluation methods: question-level accuracy, which measures the accuracy across all 151 questions, and lab-level accuracy, which assesses performance across 68 lab tests. The overall accuracy of the RAG system at the question level was 0.993, approximately 60.9% higher than the baseline model, while the lab-level accuracy was 0.985, about 52.9% higher. According to Table 5, the only mistake GPT-4-turbo with RAG made was for the acid-fast stain test, where the correct result should have been \u201cno acid-fast bacteria found,\" but the system responded with \u201cN/A\u201d instead. We also examined a similar case involving the anti-smooth muscle antibody test, where GPT-4-turbo with RAG successfully indicated the correct normal result of \"no antibodies present\". These cases are challenging because their normal results are non-numeric.\nIn future studies, we plan to explore the latest LLMs, including GPT-40, Claude 3.5, and Llama 3.1 in both tasks. Improving the accuracy of factor retrieval could improve the normal range retrieval process further. Building on the success of factor and normal range retrieval demonstrated with the RAG system, we will also focus on designing a user interface that enables patients to interact directly with Lab-AI. We will evaluate Lab-AI with other question prompts such as \"What does my result of creatinine urine test of 250 \u00b5mol/kg/day mean?\" and see if Lab-AI can prompt the patient to provide necessary factor necessary (e.g., gender in this case)."}, {"title": "Conclusions", "content": "In this proof-of-concept study, we demonstrated that advanced large language models (LLMs) integrated with Retrieval-Augmented Generation (RAG) can accurately retrieve personalized normal reference ranges for lab tests, significantly outperforming non-RAG models in both factor retrieval and normal range retrieval tasks. The low accuracy of non-RAG GPT models in retrieving normal ranges underscores the unreliability of using GPT alone for this purpose. In contrast, the RAG system effectively connects LLMs to credible health sources, reducing hallucinations and improving reliability. Given the promising results, with an F-1 score of 0.95 for factor retrieval and 0.993 accuracy for normal range retrieval, we are now advancing to the next stage of developing Lab-AI further. This will include generating tailored questions and integreting it into clinical practice to enhance patients' understanding of lab results, shared decision making, and ultimately outcomes."}]}