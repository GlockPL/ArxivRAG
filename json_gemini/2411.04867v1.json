{"title": "Think Smart, Act SMARL! Analyzing Probabilistic Logic Driven Safety in Multi-Agent Reinforcement Learning", "authors": ["Satchit Chatterji", "Erman Acar"], "abstract": "An important challenge for enabling the deployment of reinforcement learning (RL) algorithms in the real world is safety. This has resulted in the recent research field of Safe RL, which aims to learn optimal policies that are safe. One successful approach in that direction is probabilistic logic shields (PLS), a model-based Safe RL technique that uses formal specifications based on probabilistic logic programming, constraining an agent's policy to comply with those specifications in a probabilistic sense. However, safety is inherently a multi-agent concept, since real-world environments often involve multiple agents interacting simultaneously, leading to a complex system which is hard to control. Moreover, safe multi-agent RL (Safe MARL) is still underexplored. In order to address this gap, in this paper we (i) introduce Shielded MARL (SMARL) by extending PLS to MARL - in particular, we introduce Probabilistic Logic Temporal Difference Learning (PLTD) to enable shielded independent Q-learning (SIQL), and introduce shielded independent PPO (SIPPO) using probabilistic logic policy gradients; (ii) show its positive effect and use as an equilibrium selection mechanism in various game-theoretic environments including two-player simultaneous games, extensive-form games, stochastic games, and some grid-world extensions in terms of safety, cooperation, and alignment with normative behaviors; and (iii) look into the asymmetric case where only one agent is shielded, and show that the shielded agent has a significant influence on the unshielded one, providing further evidence of SMARL's ability to enhance safety and cooperation in diverse multi-agent environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed significant progress in multi-agent reinforcement learning (MARL), with sophisticated algorithms tackling increasingly complex problems. These advancements have led to remarkable progress in various domains, including autonomous vehicles where fleets of self-driving vehicles are expected to safely coordinate to optimize traffic flow and reduce congestion [30], robotic swarms where multiple robots collaborate to complete tasks more efficiently [5], trading agents that can operate in tandem to maximize returns and manage risk more effectively [13], energy grid management where multiple agents control different aspects of the power grid to balance supply and demand efficiently [34], and healthcare systems where agents coordinate to optimize scheduling and resource allocation for patience and hospitals [28], just to name a few. The ultimate success of RL algorithms in this diverse collection of domains and the deployment in the real-world, however, demands overcoming a single yet difficult key challenge: safety. This has resulted in a recent push in the research field of Safe RL which aims to learn optimal policies that are safe, including [11, 15, 16] among others. A common property of these approaches is that they use linear temporal logic (LTL) to represent safety constraints. One recent successful proposal that also uses a formal language to represent safety constraint is probabilistic logic shields (PLS) [37] whose semantics is based on probabilistic logic programming. PLS constrains agent's policy to comply with formal specifications probabilistically, and comes with various advantages over other techniques: (i) it is a probabilistic shield that is applied at the policy level instead of individual actions capturing the probabilistic evaluation of safety in contrast to deterministic rejection based shields [15, 16]; (ii) PLS is less demanding in terms of requiring knowledge of the underlying MDP as opposed to [7, 15, 16]; (iii) being end-to-end differentiable, it integrates logical semantics to deep learning architectures seamlessly; and (iv) provides safety guarantees for (single) agents.\nHowever, safety is inherently a multi-agent concept, since real-world environments often involve multiple agents interacting simultaneously, leading to a complex system which is hard to control. While there are a few safe MARL approaches trying to tackle safety in the multi-agent setting [12, 14, 21], to the best of our knowledge, PLS has not been adopted, extended or studied in this way. In order to address this gap, in this paper:\n(1) We introduce Shielded MARL (SMARL) by extending PLS to MARL - in particular, we introduce Probabilistic Logic Temporal Difference Learning (PLTD) to enable shielded independent Q-learning (SIQL), and introduce shielded independent PPO (SIPPO) using probabilistic logic policy gradients;\n(2) We show that PLS can be used as a equilibrium selection mechanism in various 2-player game-theoretic environments including a two-player simultaneous game (Stag-Hunt), an extensive-form games (Centipede), a stochastic game (Extended Public Goods Games), and a grid-world extension (Markov Stag-Hunt) in terms of safety and cooperation. Moreover, we investigate the impact of smaller (weak) and larger (strong) shields;\n(3) We investigate the influence of shielded agents over un-shielded ones. Results suggest that the agent equipped with the shield can guide the other agent's behavior significantly, suggesting further evidence of SMARL's ability to enhance safety and cooperation in diverse multi-agent environments."}, {"title": "1.1 Related Work", "content": "In the domain of safe MARL, a few but significant strides have been made recently in developing frameworks that ensure safety during learning and execution. A relatively recent one, [14], propose constrained Markov game and a constrained multi-agent policy gradient optimization algorithms (MACPO), however without the use of any formal language semantics. More related works include ElSayed-Aly et al. [12] which introduce a framework that integrates shielding mechanisms to dynamically enforce safety constraints. Extending this, Melcer et al. [21] explore decentralized shielding, i.e. allowing each agent to have its own shield, reducing computational overhead and enhancing scalability. Both of these approaches, use LTL as formal language, and the higher requirement to know the underlying MDP \u2013 this is in contrast to our use of a probabilistic logic framework (ProbLog) and weaker demand of having access to the MDP (only the safety-related parts of the states), respectively."}, {"title": "2 PRELIMINARIES", "content": "In this section, we provide a brief overview of the formal basis for relevant game theoretic preliminaries, multi-agent reinforcement learning and probabilistic logic shielding following [37]."}, {"title": "2.1 Game Theory", "content": "A normal-form game (NFG) is a tuple $(N, A, u)$ where $N = {1, ..., n}$ is a finite set of agents (or players), $A = A_1 \\times \\dots \\times A_n$ is a finite set of action (also called pure strategy) profiles $a = (a_1, ...a_n)$ with $A_i$ being the set of player i's actions, and $u = (u_1, ..., u_n)$ is a profile of utility functions $u_i : A \\rightarrow \\mathbb{R}$. A strategy profile $s = (s_1, ...s_n)$ is called a Nash equilibrium if for each player $k \\in N$, the strategy $s_k$ is a best response to the strategies of all the other players $s_{i\\in N\\{k\\}}$. Although we provided the more general n-player definition, in the experiments and the remainder of the paper, we shall only consider the 2-player variants. \n\u25ba Stag-Hunt is a 2-players normal form game in which each player has the actions $A_1 = A_2 = {Stag, Hare}$. Any payoff matrix with the following structure of payoffs is considered an instance of the Stag-Hunt: $a = b > d = e \\geq g = h > c = f$. Intuitively, agents have the coordination challenge to obtain a larger reward; that is, if both players hunt a stag, they will get a large reward, but if only one goes for the stag, they get a penalty (low or negative utility). Both agents can also opt to go for the hare, in which case they both will get a small reward regardless of the action of the other player. This game has two pure-strategy Nash equilibria: either both players play Stag (also called the cooperative equilibrium), or Hare (the non-cooperative equilibrium). Additionally, there is a mixed strategy equilibrium that depends on the exact values in the utility matrix, and can be solved analytically.\n\u25ba Extended Public Goods Game (EPGG) [24] for two players is a tuple $(N, c, A, f, u)$, where $N = {1,2}$ is the set of players, $C_i \\in \\mathbb{R}_{\\geq o}$ is the amount of coin each player i is endowed with, $c = (C_1, C_2)$ denotes the tuple containing both agents' coins, and $f \\in \\mathbb{R}^2_0$ is the multiplication factor for the lump sum endowment (hence the name 'extended', as opposed to the case $f \\leq |N|$). Each player decides whether to invest in the public good (cooperate) or not (defect), i.e., $A_i = {C, D}$ for $i \\in N$. The resulting quantity is then evenly distributed among both agents. The utility/reward function for each agent i is defined as $r_i: A \\times \\mathbb{R}_{\\geq 0} \\times \\mathbb{R}^{2_0} \\rightarrow \\mathbb{R}$, with: $r_i (a, f, c) = \\sum^2_{j=1}c_jI(a_j) \\cdot f + c_i(1 - I(a_i))$ where a is the action profile, $a_j$ is the j-th entry of the action profile a, $I(a_j)$ is the indicator function returning 1 if the action of the agent j is cooperative and 0 otherwise, and $c_j$ denotes the j-the entry of c. Here, EPGG is formulated as a partially observable stochastic game in which $f := f_t ~ N(\\mu, \\sigma)$, sampled every time step t, where $N(\\mu, \\sigma)$ is a normal distribution with mean $\\mu$ and variance $\\sigma$. Depending on the value of $\\mu$, the game is expected to be non-cooperative ($\\mu < 1$), cooperative ($\\mu > n$) or a mix of both (i.e., mixed-motive for $1 < \\mu < n$). The expected Nash equilibrium is determined by $\\mu = E_{f_t~N(\\mu,\\sigma)}[f]$, and can be estimated empirically ($\\hat{\\mu}$) by taking the mean of several observations of $f_t$.\n\u25ba Centipede Game is a two players extensive-form game in which two agents take turns deciding whether to continue the game (increasing the potential rewards for both) or defect (ending the game and collecting a short-term reward). The game structure incentivizes short-term defection, as each agent risks being defected upon by their partner. Formally, the game begins with a 'pot' $p_o$ (a set amount of payoff) where each player has two actions, either to continue or to stop. After each round t, the pot increases, for example linearly (e.g., $p_{t+1} \\leftarrow p_t + 1$) or exponentially (e.g., $p_{t+1} \\leftarrow p^2_t$). If any player stops at its turn, then it receives $p_t/2 + 1$ and the other player receives $p_t/2 - 1$. If both players continue, they split the pot equally after a certain amount of rounds i.e., $t_{max}$, each receiving $p_{t_{max}}/2$. The optimal strategy for the first first player is to stop. This is called the subgame perfect Nash equilibrium, one analogue of of the Nash equilibrium for extensive-form games (see [20] or [19] for more details).\n\u25baMarkov Stag-Hunt is a grid-world environment inspired by [26], where agents move through a grid and must decide to hunt a stag or harvest a plant, with an underlying reward structure similar the Stag-Hunt game."}, {"title": "2.2 Safe MARL", "content": "2.2.1 Constrained MDPs. RL was developed to enable an single agent to learn a policy $\\pi$ maximizing the sum of discounted reward signals collected in a stochastic and generally stationary environment [23]. The most common formalism for RL utilizes the notion of Markov Decision Processes (MDPs) [31, 35]. An MDP is defined by a set of states S, a set of actions A, a transition function T, and a reward function R. The objective in an MDP is to find a policy that maximizes the expected cumulative reward over time. However, this does not inherently account for other important considerations such as safety or costs associated with actions that might be undesirable in real-world scenarios. Constrained MDPS (CMDPs) are an extension of MDPs that include constraints, typically related to safety or resource usage [3]. These additional considerations are handled through constraints that limit the policies to those that not only maximize rewards but also satisfy certain conditions, such as remaining within a specified cost or risk limit. Formally, a CMDP has a set of cost functions $C_i$ for $i \\in {1, ..., m}$, each associated with its own threshold $d_i$. The goal is to maximize the cumulative reward"}, {"title": "2.2.2 MARL.", "content": "MARL extends traditional RL paradigms such as PPO [29] and Q-Learning [22] to settings involving multiple agents that interact within a shared environment [respectively, 10, 32]. Each agent optimizes its own individual reward, but the presence of other co-learning agents introduces challenges such as non-stationarity, as the environment changes with the actions of the other agents [1]. MARL finds applications in areas like autonomous driving [30], financial trading [13] and energy grid management [34], where some combination of coordination, cooperation and competition is often required. Ensuring safety and collaboration among agents in MARL is a crucial issue, as unsafe actions can lead to significant consequences in high-stakes domains."}, {"title": "2.3 Probabilistic Logic Shielding", "content": "We summarize Yang et al. [37] in describing Probabilistic Logic Shielding (PLS), a probabilistic method for safety in RL. Unlike traditional rejection-based shields that fully block unsafe actions and limit exploration [2], PLS adjusts the probability of unsafe actions proportional to their risk, allowing them to occasionally occur during training. This enables the agent to learn from unsafe actions and improve its safety policy over time, particularly in environments with soft safety constraints.\nAssume a probabilistic model P that is aware of the probability $P(safe|a, s)$ of the safety of taking a given action a for a given state s. P may not necessarily represent the whole environment or transition model (i.e. it need not be a full representation of the underlying MDP), but only encode relevant knowledge about the safety of certain states and how they relate to the current policy $\\pi$. The safety of $\\pi$ is the sum of the disjoint probabilities of the safeties of taking each action:\n$Pr(safe | s) = \\sum_{a\\in A} P(safe | s, a) \\cdot \\pi(a | s)$  (1)\nIn order to compute a safer policy, we can marginalize actions out with respect to $\\pi$. This leads us to the notion of probabilistic shielding: formally, given a base policy $\\pi$ and a probabilistic safety model P(safels, a), the shielded policy is\n$\\pi^+ (a | s) = P_{\\pi}(a | s, safe) = \\frac{P(safe s, a)}{P(safe | s)} \\pi(a|s)$   (2)"}, {"title": "2.4 ProbLog Shields", "content": "As mentioned previously, Yang et al. [37] implement PLS using ProbLog [9]. These programs, notated T(.), have three components: (i) a set of predicates (with cardinality |A|) whose values are set to the agent's base action distribution $\\pi$, (ii) a set of probabilistic inputs relevant to the constraints that describe the current state, and (iii) a set of sentences that describe the agent's constraints, specifically including a definition for a predicate 'safe_next' (the safety of the next state under $\\pi$). Here, 'safety' is used as a general term for any probabilistic constraint satisfaction."}, {"title": "3 METHODS", "content": "In this section, we introduce two methods that integrate PLS into MARL settings. First, in Section 3.1, we describe Probabilistic Logic TD Learning (PLTD), which mirrors shielded policy gradient methods within temporal difference learning by incorporating safety constraints. Following this, we present Probabilistic Logic SMARL in Section 3.2, which applies the concept of probabilistic shielding to multi-agent environments. In SMARL, each agent operates independently with its own shield, allowing flexibility in experimentation and parameter sharing."}, {"title": "3.1 Probabilistic Logic Shielded Temporal Difference Learning (PLTD)", "content": "Yang et al. [37] define their shielded algorithm, PLPG, with respect only to policy gradient methods, specifically PPO [29]. Thus, in order to use it with DQN [22], SARSA [38] (or other deep TD-learning algorithm), we must first define how an agent uses a ProbLog shield to learn a shielded policy during exploration and evaluation. An important consideration for extending PLS to TD-learning is whether the algorithm is on-policy or off-policy. Yang et al. [37] note that for off-policy algorithms to converge, both the exploration and learned policies must cover the same state-action space. Since in PLS, the agent optimizes $\\pi$ with $\\pi^+$, one must be careful about this assumption. We propose both on- and off-policy algorithms that incorporate safety constraints into Q-learning."}, {"title": "3.1.1 Objective Function.", "content": "The full specifications of PLPG include a shielded policy gradient [$\\nabla e log P_r+ (safe|s)$] and a safety gradient penalty [$-log P_r+ (safe|s)$]. Since TD-methods are not policy gradient methods, we must rely on the latter to introduce information about safety constraints into the loss function.\nLet D be the distribution of d = <$s_t$,$a_t$,$r_t$,$s_{t+1}$,$a_{t+1}$> tuples extracted from a history buffer of the agent interacting with the MDP and $s_t$, $a_t$, $r_t$ be the state, taken action, and reward at time t. Let the Q-value approximator $Q_\\theta(s, a)$ be parameterized by some parameters $\\theta$. Then, the off-policy loss (Q-learning based) and on-policy loss (SARSA-based) are augmented using this penalty term below to create Probabilistic Logic TD Learning (PLTD).\nDEFINITION 1 (PROBABILISTIC LOGIC TD LEARNING, PLTD). The PLTD minimization objective is:\n$L^{\\pi*}(\\theta) = E_{d~D}[(r^\\pi_t + \\gamma X - Q_\\theta(s^\\pi_t, a^\\pi_t))^2 - S_p]$   (3)\nwhere $X = max_{a'} Q_\\theta(s_{t+1}, a')$ for off-policy, and $X = Q_\\theta(s_{t+1}, a_{t+1})$ for on-policy DQN; $S_p = log P_r+ (safe|s)$ is the safety penalty; and $\\alpha \\in \\mathbb{R}_{\\geq 0}$ is the safety coefficient, or the weight of the safety penalty."}, {"title": "3.2 Probabilistic Logic SMARL", "content": "In this paper, at least one of the agents of an independent MARL algorithm uses PLS. This is called Probabilistic Logic Shielded MARL. All SMARL algorithms that are discussed herein have agents being relatively independently shielded. Each agent also may have their own independent policies and independent parameter updates. This allows for maximal flexibility during experimentation (for example, testing different degrees of parameter sharing or partially-shielded populations)."}, {"title": "4 RESULTS", "content": "The results for probabilistic logic SMARL for two agents in the various game-theoretic environments described earlier in Section 2.1 are presented below. For the sake of clarity, details are moved to the supplementary material: the payoff matrices/reward structure for each game can be found in Appendix A; shield constructions and programs for each experiment can be found in Appendix B; to validate the efficacy of PLTD with respect to standard DQN, a"}, {"title": "4.1 Stag-Hunt", "content": "Stag-Hunt was used to test the ability of PLS to guide agents towards pre-selected behaviors in games with multiple Nash equilibria. The IPPO agents converge to the non-cooperative Nash equilibrium, consistently playing Hare with a reward of 2 per step. In contrast, agents using quickly and reliably play Stag, earning a higher reward of 4. Unshielded agents fail to reach the mixed strategy due to the instability of PPO's stochastic policies \u2013 small deviations push agents toward pure strategies, often resulting in the non-cooperative equilibrium. However, agents using successfully adopt a mixed strategy, as indicated by the expected rewards and safety measures, despite high variability, likely caused by the instability of the normative strategy relative to pure Nash equilibria."}, {"title": "4.2 Centipede Game", "content": "Centipede Game was used to test how well PLS can guide agents to cooperate in order to achieve higher collective rewards, even in scenarios where the dominant strategy suggests defection \u2013 a shield was constructed (Shield 4) to encourage agents to push the game further by constraining early defection behavior. For DQN-based agents, all SIQL variations cooperate by playing Continue from the beginning. In contrast, e-greedy agents fail to progress far, as early on in exploration, \u0454 \u2248 1, meaning there is a uniform distribution over the actions, and only a 25% probability of the agents going to only the second stage of the game. The probability of reaching some early stage n is thus \u2248 0.25-1. This discourages exploration, as stopping early provides a small but consistent reward, aligning with the Nash equilibrium (i.e. the subgame-perfect equilibrium). Unshielded softmax agents perform slightly better, but with high variance some complete the game while most exit early. Shielding enables all agents to explore much further into the game."}, {"title": "4.3 Extended Public Goods Game", "content": "EPGG was used to test the robustness of PLS in promoting cooperation in managing uncertainty in environment with stochastic payoffs. Figure 5 shows the results of agents playing the EPGG with $f_t ~ N(\\mu, 1)$ where $\\mu\\in$ {0.5, 1.5, 2.5, 5.0}. We see that in all cases, the IPPO agents (baseline) learn to play the Nash solution to the instantaneous EPGG - this is apparent from the reward curves and the mean cooperation, where it is better to defect when $f_t < 1$ and cooperate when $f_t > 2$, with mixed incentives when $1 < f_t < 2$. The SIPPO agents quickly learn the expected Nash erring on the side of cooperation in mixed scenarios, as directed by the shield. This leads to more prosocial behavior - defection when $\\hat{\\mu} < 1$ and cooperation when $\\hat{\\mu} > 1$. We also see that the further away u is from a mixed incentive scenario, the lower the variance.\nFor $\\mu = 0.5$, SIPPO agents show less variation in reward and behavior than IPPO, converging quickly to the Nash strategy (non-cooperation) but with smaller rewards. This is due to the fact that when $f_t ~ N(0.5, 1)$, there is a \u2248 30% chance of the value being greater than 1, which, through cooperation, the agents can attain higher instantaneous rewards than if they play the expected strategy. Similarly, at $\\mu = 5$, SIPPO agents again exhibit lower variance, always cooperating.\nExcept for $\\mu = 0.5$, the rewards for the SIPPO agents are either equal to or exceed the unshielded ones, and their cooperative or defective nature are more predictable."}, {"title": "4.4 Markov Stag-Hunt", "content": "The Markov Stag-Hunt experiments investigate (i) the ability of PLS to promote cooperation in dynamic settings, (ii) the effect of shields with different constraining properties in the same environment, and (iii) the effect of shields when a part of the population is shielded (with and without parameter-sharing, 4.4.2)."}, {"title": "4.4.1 Variation in Shield Strength.", "content": "Two shields, $T_{strong}$ (Shield 7) and $T_{weak}$ (Shield 8), were constructed such that $T_{strong} \\subset T_{weak}$ in terms of their safety constraints. Broadly, $T_{weak}$ constrains the agents' behaviors to strongly cooperate when they are near the stag, and $T_{strong}$ constrains them regardless of where they are placed on the grid. SIPPO agents with $T_{weak}$ show significant improvement in cooperation, achieving an average of 14 successful stag hunts per episode by 500 episodes, while maintaining plant harvest levels. However, their willingness to take risks increases, reflected in a slight decline in safety and an uptick in the number of stag penalties over time as the potential rewards outweigh risk-aversion.\nSIPPO agents under $T_{strong}$ demonstrate even greater cooperation, focusing more on stag hunts and less on plant harvesting. Despite similar stag penalties as the $T_{weak}$ agents, they progressively improve safety, implying that they learn better strategies. Their rewards are significantly higher (by an order of magnitude) than those of agents using $T_{weak}$, which are already much greater than the unshielded agents' rewards."}, {"title": "4.4.2 Partially Shielded Population.", "content": "Analogous to the fully shielded Markov Stag-Hunt experiments (Section 4.4), the results from the fully shielded population are omitted for clarity. The metrics displayed are: (7a) plant harvests, (7b) successful stag hunts with another agent, and (7c) solo stag hunts. All conditions show increasing rewards over time. IPPO, SIPPO, and SCSPPO conditions grow steadily at a similar pace, while SPSPPO shows much larger rewards but with higher variation and instability. The safety of shielded populations also increases, with SPSPPO showing the largest gains, though with high variance."}, {"title": "5 DISCUSSION", "content": "While we believe the research presented in this paper provides valuable insights into the application of PLS in SMARL, several gaps and extensions must be acknowledged: (i) The first is that existing methods for solving probabilistic logic programs quickly become computationally expensive as the state and action spaces grow, making them infeasible to be used in real-world applications.\n(v) finally, while PLS in SMARL aims to enhance safety, its implementation must carefully consider ethical and societal impacts, as hand-designed a-priori constraints may introduce"}, {"title": "A ENVIRONMENT SPECIFICS", "content": "A.1 Repeated NFGs (Stag-Hunt)\nIn these experiments both agents choose an action simultaneously and receive a reward determined by the payoff matrix. Each episode is a series of 25 repeated games. Thus the agents must learn to maximize the discounted reward attained over the whole episode [4]. The three Nash equilibria for the Stag-Hunt are (Stag, Stag), (Hare, Hare) and a mixed Nash equilibrium where each agent plays Stag 60% of the time and Hare 40% of the time.\nA.2 Centipede Game\nThe centipede game is a turn-based game that has the following description: the game begins with a 'pot' po (a set amount of payoff). Each player has two actions, either to continue or to stop. After each round t, the pot increases, for example linearly ($p_{t+1} \\leftarrow p_t + 1$) or exponentially ($p_{t+1} \\leftarrow p^2_t$). If any player decides to stop at their turn, they receive a utility of pt/2+1 and the other player gets a utility of Pt/2-1. If both players continue after a set number of rounds tmax, both players split the pot equally, receiving Ptmax/2. A.2.1 Environment description. Prakash [27] provides a good starting point with respect to implementation. A.2.2 Experimental conditions. Two agents play this game.\nA.3 2-Player Extended Public Goods Game\nThe Extended Public Goods Game is an environment where each player i has an initial amount of money ci and can either choose to Cooperate (put their money into the pot) or Defect (keep the money). The Nash equilibrium for a single game depends on the value of f, and thus the Nash strategy for each agent for a full game is not necessarily dependent on the expected value of f.\nA.3.1 Experimental conditions. Two agents play this game.\nA.4 Markov Stag-Hunt\nPeysakhovich and Lerer [26] describe a few grid environments that have the property that the optimal policies for agents reflect the structure of the Stag-Hunt game - principally, there are two strategies that the agents can converge to, either take on more risk by cooperating but attain higher rewards, or take on lower risk but get guaranteed small rewards. A.4.1 Environment description. The adapted Markov Stag-Hunt environment used within this paper can be described in general as such: n agents are placed in a square grid world of size g \u00d7 g. A.4.2 Experimental conditions. In the experiments conducted, the grid was set to a size of 5\u00d75 with n = 2 agents."}, {"title": "B DETAILS ON SHIELD CONSTRUCTION", "content": "B.1 Stag-Hunt\nFor the pure equilibria, cooperation would refer to collaborating with the other agent to hunt the stag and defect refers to an agent individually going for the hare instead.\nFor the mixed equilibria, we would need a more sophisticated shield. One way of doing this is to add soft constraints to the actions based on how far they differ from the mixed equilibrium. As a proxy for the policy of the agent, a set of the latest h historical actions of the agents can be collected - this is termed a buffer. A mean policy t\u2081 at the current time t can then be derived from these actions, iterating over all a' \u2208 A:\n$t_h(a_s) = \\frac{1}{i\\in {1,...,h}}(a_{t-i} = a')$   (4)\nLet an a-priori normative policy be \u03c0*, say, the mixed Nash strategy. If th is too distant from the expected mixed strategy with respect to some divergence measure, then we can re-normalize the input policy using Definition 2 to be more similar to \u03c0*. For Stag-Hunt, let such an input be the absolute difference between th and \u03c0*:\nsensor(stag_diff) := |\u03c0*(Stag | s) \u2013 t\u2081(Stag | s)|   (5)\nsensor(hare_diff) := |\u03c0*(Hare | s) \u2013 t\u2081(Hare | s)|   (6)\nThe constraints that define the value of the unsafe_next predicate can be interpreted as it is unsafe to take action stag proportional to the value of sensor(stag_diff) (and analogously for playing Hare). Thus, the higher the difference between the Nash strategy and the historical policy for each agent, the more the policy is normalized.\nFor the experiments discussed below, the buffer length is set to h = 50. Additionally, under the payoff matrix, the a priori mixed Nash equilibrium can be computed such that \u03c0* (Stag | s) = 0.6 and \u03c0* (Hare | s) = 0.4 for both agents.\nB.2 Centipede Game\nSince the agents have two actions, we can use a shield to force the agents to always play C at every time step (regardless of previous actions) until the game terminates and a reward is attained using the following shield.\nB.3 Extended Public Goods Game\nSince, at each time step t, there is effectively a new simultaneous game with 2 actions, we can dictate the actions of the agents with a simple shield such as the one used for the Centipede Game.\nWe must thus construct a smooth distance measure d between ft and \u00fb bounded between [0, 1] such that d(ft, \u00fb) = 1 when ft = \u00fb and d(ft,\u00fb) = 0 when | ft - |\u2192 8. One of many possibilities is described here. First, we compute the z-score of the current ft with respect to the estimated parameters so far:\n$z(f_t | \\hat{u}, \\hat{o}) = \\frac{f_t - u}{o}$  (7)\nWe then use the CDF of a standard normal distribution to translate this into a probability value (z(ft | \u00fb, \u00f4)). In Shield 5, the value of the fact sensor(f_certainty) is thus set to d(ft, \u00fb). The value of the sensor predicates sensor (stag_near_self) and sensor (stag_near_other) is 1 when the stag is adjacent to the acting agent and the other agent respectively, and 0 otherwise.\nB.4 Markov Stag-Hunt\nLet us focus on constructing a shield that might help the agents learn how to cooperate and hunt a stag together. The inputs to the shield are the policy and some sensors that describe relative positions of the stag and the other agent:\n$d(f_t, \u00ee) = 1-2\\cdot |(z(f_t | \u00fb, \u00f4)) \u2013 0.5|$  (8)"}, {"title": "C SINGLE-AGENT EXPERIMENT: CARTSAFE", "content": "Before testing the two shielded multi-agent DQN-based algorithms (SIQL and SPSQL), it is imperative to first test whether or not PLTD (ProbLog-shielded DQN) works as well as vanilla DQN for environments with constraints, and how it compares to PLPG (ProbLog-shielded PPO). C.1 Experimental conditions\nThe maximum length of each episode was set to tmax = 200 with 500 training episodes. C.2 Shield construction\nA simple shield that aims to satisfy the constraints outlined in the task description can be constructed as follows. This means that if the cart is within the permissible central region of the environment, the safety probability of the next action is computed as (sensor (cost) = 0) \u21d2 (safe_next = 1), and the policy remains unchanged. C.3 Results: PLPG (Shielded PPO)\nWe see that for the training reward graphs , there is a relatively high variation in rewards, but the evaluation graphs show a clear upward trend of the agents learning to balance the pole. The variation is likely due to the choice of hyperparameters.\nC.4 Results: PLTD (Shielded DQN)\nWe see that in general, adding a shield to an agent with either of the tested safety penalty values does not significantly affect its safety, ceteris paribus. The safety values tend to be quite high, generally above 0.9 with not much improvement after the first \u2248 100 episodes. This may hint at the safety constraints being relatively simple to accomplish given the environment goals and algorithms. The PLTD agents tend to do better in terms of rewards, but worse in terms of safety, which is on par with the baseline of the unshielded PLTD agents or the vanilla PPO agent."}]}