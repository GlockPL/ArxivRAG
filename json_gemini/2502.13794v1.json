{"title": "LESA: Learnable LLM Layer Scaling-Up", "authors": ["Yifei Yang", "Zouying Cao", "Xinbei Ma", "Yao Yao", "Libo Qin", "Zhi Chen", "Hai Zhao"], "abstract": "Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger ones. However, existing depth scaling-up methods rely on empirical heuristic rules for layer duplication, which result in poorer initialization and slower convergence during continual pre-training. We propose LESA, a novel learnable method for depth scaling-up. By concatenating parameters from each layer and applying Singular Value Decomposition, we uncover latent patterns between layers, suggesting that inter-layer parameters can be learned. LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training. Experiments show that LESA outperforms existing baselines, achieving superior performance with less than half the computational cost during continual pre-training. Extensive analyses demonstrate its effectiveness across different model sizes and tasks.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Natural Language Processing (NLP) have been largely driven by Transformer-based architectures (Vaswani et al., 2017), with Large Language Models (LLMs) demonstrating exceptional capabilities in addressing a wide range of complex tasks (Brown et al., 2020; Achiam et al., 2023; Bai et al., 2023; Touvron et al., 2023a; Yang et al., 2024a; AI@Meta, 2024; Jiang et al., 2023; Almazrouei et al., 2023; Bi et al., 2024). As the parameter size continues to grow, in accordance with scaling laws (Kaplan et al., 2020), the computational resources required to train LLMs from scratch have become increasingly prohibitive, demanding millions of GPU hours and significant energy consumption. This immense resource demand largely arises from the need to randomly reinitialize model parameters, preventing the transfer of ability from existing LLMs.\nTo address this limitation, a common approach is model scaling-up, which leverages the parameters of smaller models to construct larger ones, either for immediate deployment or as a better initial checkpoint for more effective further continual pre-training. Existing model scaling-up methods can be divided into width scaling-up and depth scaling-up. Width scaling-up (Chen et al., 2015, 2021a; Wang et al., 2023; Samragh et al., 2024) primarily involves expanding matrix dimensions, rather than increasing the number of layers 2. In contrast, depth scaling-up involves repurposing trained Transformer blocks from a smaller model to build a larger one with additional layers (Wu et al., 2024; Kim et al., 2023; Gong et al., 2019; Pan et al., 2024; Agarwal et al., 2024; Parmar et al., 2024). This strategy is widely applicable to modern LLMs based on the Transformer architecture, preserving the internal structure, such as matrix sizes. It is also compatible with existing parallel training frameworks, better preserving the model's knowledge, contributing to its increasing popularity in recent model scaling-up approaches.\nHowever, current depth scaling-up methods rely on heuristic rules, typically duplicating one or more blocks before integrating them into the model. These approaches overlook parameter change patterns between layers, limiting the model's ability to specialize each layer effectively. As a result, newly upscaled layers replicate the previous ones, neglecting layer-specific specialization (Voita et al., 2019b,a). This not only leads to suboptimal model initialization performance but also prevents the model from fully utilizing its expanded capacity. By treating all layers equally, these methods fail to"}, {"title": "2 Related Works", "content": "Model scaling-up can be broadly categorized into width and depth scaling-up. Width scaling-up increases the matrix size while ensuring that the output of a layer or consecutive layers remains consistent with the output of the original network before expansion. Net2Net (Chen et al., 2015) is one of the first to transfer parameters from a smaller model to initialize a larger one using function-preserving transformations. bert2BERT (Chen et al., 2021a) extends this approach to Transformer-based models. LiGO (Wang et al., 2023) learns a linear mapping to initialize larger models. HyperCloning (Samragh et al., 2024) expands LLM to fit a larger model with more hidden dimensions. However, while these methods increase matrix size, they are less compatible with parallel training frameworks, which are better suited for depth scaling-up. Moreover, depth scaling-up better preserves the model's knowledge.\nCurrent depth scaling-up methods expand the model by duplicating and adding layers based on heuristic rules, which can be broadly categorized into \"Interpolation\" and \"Stack\" (Pan et al., 2024), as shown in Figure 1. Interpolation involves adding a copy of each layer after the original, while Stack treats consecutive layers as a group and duplicates them together. Recent popular methods like"}, {"title": "2.1 Model Scaling-up", "content": "Model scaling-up can be broadly categorized into width and depth scaling-up. Width scaling-up increases the matrix size while ensuring that the output of a layer or consecutive layers remains consistent with the output of the original network before expansion. Net2Net (Chen et al., 2015) is one of the first to transfer parameters from a smaller model to initialize a larger one using function-preserving transformations. bert2BERT (Chen et al., 2021a) extends this approach to Transformer-based models. LiGO (Wang et al., 2023) learns a linear mapping to initialize larger models. HyperCloning (Samragh et al., 2024) expands LLM to fit a larger model with more hidden dimensions. However, while these methods increase matrix size, they are less compatible with parallel training frameworks, which are better suited for depth scaling-up. Moreover, depth scaling-up better preserves the model's knowledge.\nCurrent depth scaling-up methods expand the model by duplicating and adding layers based on heuristic rules, which can be broadly categorized into \"Interpolation\" and \"Stack\" (Pan et al., 2024), as shown in Figure 1. Interpolation involves adding a copy of each layer after the original, while Stack treats consecutive layers as a group and duplicates them together. Recent popular methods like"}, {"title": "2.2 Progressive Training", "content": "Progressive training involves gradually transitioning from simpler, smaller models to more complex, larger ones (Chang et al., 2017; Wen et al., 2020; Dong et al., 2020; Wei et al., 2016; Fayek et al., 2020). It is often combined with model scaling-up, where the model size is progressively increased during training. Prior to the era of LLMs, many methods (Chen et al., 2021a; Gu et al., 2020; Wang et al., 2023; Yang et al., 2020; Yao et al., 2023) are developed to train smaller models, such as BERT (Devlin et al., 2018). In recent years, LLaMA Pro (Wu et al., 2024) and Apollo (Pan et al., 2024) have applied progressive learning and model scaling-up strategies to train LLMs. YODA (Lu et al., 2024) introduces a novel teacher-student progressive learning framework that enhances model fine-tuning by emulating the teacher-student educational process. Du et al. offer a comprehensive evaluation and empirical guidelines for progressive learning and model scaling-up."}, {"title": "3 Method", "content": "This section discusses the patterns observed between model layers through SVD analysis of the model's parameters. Based on these patterns, we hypothesize that there are underlying patterns in the trained model that can be learned by a neural network. We then use this trained network to predict intermediate layers that can be inserted between adjacent layers for depth scaling-up."}, {"title": "3.1 SVD-Based Layer Pattern", "content": "Inspired by recent work using SVD for LLM compression or merging (Wang et al., 2024c; Stoica et al., 2024; Wang et al., 2024a), it is realized that SVD can map the model's parameters into one space for analysis. Specifically, assume we have weight matrices $W_1, W_2, ..., W_L$ from L layers of an LLM, where $W_i \\in \\mathbb{R}^{d_1 \\times d_2}$ represents a matrix from each Transformer block, such as the up-projection matrix in MLP, Query matrix in self-attention. These L matrices can be concatenated horizontally into a single matrix, denoted as denoted as $W \\in \\mathbb{R}^{d_1 \\times Ld_2}$. SVD can be used to decompose this matrix into three components: U, \u03a3 and $V^T$.\nAccording to SVD, \u2211 is a diagonal matrix of size $d_1 \\times Ld_2$, containing the singular values of W. U is a unitary matrix that spans a set of standard orthogonal bases. If we treat \u2211 as a scaling transformation on each orthogonal basis in U, then U\u2211 forms a new set of orthogonal bases. For the i-th layer's $W_i$, it can be recovered as $W_i = U\\Sigma V_i^T$, where $V_i = V^T[:, (i-1)*d_2:i*d_2] \\in \\mathbb{R}^{d_1 \\times d_2}$. This means that the parameter $W_i$ of each layer is a linear combination of the orthogonal bases from U\u2211, with $V_i$ representing the coefficients of this combination. By projecting the parameters of each layer into the space spanned by U\u2211, we can analyze the patterns in the coefficients $V_i$ for the i-th layer.\nSince larger singular values correspond to eigenvectors that capture more information about the matrix, we select the eigenvector corresponding to the largest singular value (top-1) from each layer's $V_i$ for visualization. We use t-SNE (Van der Maaten and Hinton, 2008) to reduce $V_i$ to two dimensions. The visualization results of the gate-projection in the MLP of Llama3-8B (AI@Meta, 2024) are presented in Figure 2, where we observe a clear continuity in the distribution of these $V_i$. This continuity pattern, derived from the top-1 singular value of the gate-projection using t-SNE, is also present in Llama2 (Touvron et al., 2023b), Llama3 (AI@Meta, 2024), and Qwen2 (qwe, 2024). This suggests that the model's parameters may exhibit unique inter-layer patterns. More visualiza-"}, {"title": "3.2 Learnable LLM Layer Scaling-Up", "content": "Inspired by the aforementioned SVD-Based Layer Patterns, we hypothesize that there may be inter-layer patterns in the parameters. However, these patterns might not be easily observed using simple visualization techniques or fitted with specific distributions, such as Gaussian mixtures. Therefore, a direct approach is to learn these patterns through a neural network.\nWe present our method in Figure 3. After obtaining $V_i$ as described in Section 3.1, we train an MLP $G_w$ to learn the patterns. Our training objective is to enable the MLP to predict an intermediate layer given any two layers that are one layer apart.\nFormally, for a weight matrix $W \\in {\\text{q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj} }$, we use SVD to obtain $V_i$ following Section 3.1. We then train an MLP $G_w$ specific to W with the objective of predicting $V_i$ by using the concatenation of $V_{i-1}$ and $V_{i+1}$ as input. We optimize $G_w$ using MSELoss (Mean Squared Error Loss):\n$\\mathcal{L}_1 = MSE(G_w([V_{i-1}, V_{i+1}]), V_i)$\nwhose goal is to enable $G_w$ to predict accurately. In subsequent experiments, we find that directly training with $\\mathcal{L}_1$ will result in the norm of the predicted $G_w([V_{i-1}, V_{i+1}])$ approaching zero, meaning that the predicted $V_i$ parameters are close to zero, which leads to parameter degradation. To address this issue, we add a norm loss:\n$\\mathcal{L}_2 = MSE(Norm(G_w([V_{i-1}, V_{i+1}])), Norm(V_i))$\nwhere the Norm represents the L2 norm, and $\\mathcal{L}_2$ aims to ensure that the norm of the model's predicted $V_i'$ is close to that of $V_i$. Thus, the final loss for training $G_w$ is:\n$\\mathcal{L} = (1 - \\lambda)\\mathcal{L}_1 + \\lambda\\mathcal{L}_2$\nwhere $\\lambda$ is a hyper-parameter.\nOnce trained, $G_w$ can predict the parameters of an intermediate layer based on its surrounding ones. Thus, for adjacent $V_i$ and $V_{i+1}$, we use $G_w$ to predict the intermediate layer $V'$ to insert between them. We then reconstruct $V_i$ using the U\u03a3 decomposition from the previous step, forming the predicted matrix $W'$, and insert it between the layers to expand the LLM."}, {"title": "4 Main Experiments", "content": "We conduct experiments on the Llama3-8B model, which has 32 layers. To construct the training data for $G_w$, we use consecutive triplets of layers, namely (1, 2, 3), (2, 3, 4), (3, 4, 5), ..., (30, 31, 32), resulting in 30 samples. We define $G_w$ as a three-layer MLP with a ReLU (Agarap, 2018)"}, {"title": "4.1 Settings", "content": "We conduct experiments on the Llama3-8B model, which has 32 layers. To construct the training data for $G_w$, we use consecutive triplets of layers, namely (1, 2, 3), (2, 3, 4), (3, 4, 5), ..., (30, 31, 32), resulting in 30 samples. We define $G_w$ as a three-layer MLP with a ReLU (Agarap, 2018)"}, {"title": "4.1.1 LESA Settings", "content": "We conduct experiments on the Llama3-8B model, which has 32 layers. To construct the training data for Gw, we use consecutive triplets of layers, namely (1, 2, 3), (2, 3, 4), (3, 4, 5), ..., (30, 31, 32), resulting in 30 samples. We define Gw as a three-layer MLP with a ReLU (Agarap, 2018) activation function, where the hidden dimension is 256. Gw is trained for 5 epochs on these samples using the AdamW (Loshchilov, 2017) optimizer with a learning rate of 1e-3. The \u03bb is set to 5e-5. To compare with baselines (Wu et al., 2024; Kim et al., 2023), we use LESA to scale up Llama3-8B to 48 layers by inserting an intermediate layer between each pair of adjacent layers in the original 15th to 31st layers. The expanded models all have 11.5 billion parameters."}, {"title": "4.1.2 Continual Training", "content": "For the models expanded using LESA and baseline methods, we continue pre-training with Wikipedia data from November 2024, which is released after the training of Llama3-8B and has not been used in its original training. We use the Llama-Factory (Zheng et al., 2024) training framework, with a cutoff length of 4096, a warmup ratio of 0.1, and a cosine learning rate scheduler. The optimizer is AdamW with a learning rate of 5e-5. The batch size per GPU is 2, with 4 gradient accumulation steps. For LESA and LLaMA Pro, we only train the newly expanded layers, freezing the other layers. Following the original setting, we perform full parameter fine-tuning for SOLAR.\nFor the Supervised Fine-Tuning (SFT) stage, we use Alpaca-GPT4 (Peng et al., 2023) for training, following SOLAR. The hyper-parameters are the same as those in the continual pre-training, except that we perform full parameter fine-tuning without freezing any layers for all models. All experiments are conducted on a server with 8 Nvidia A100 80GB GPUs."}, {"title": "4.2 Benchmarks", "content": "For the continual pre-training models, since they lack instruction-following capabilities, we use the OpenCompass framework (Contributors, 2023) with the PPL (perplexity) 3 mode for evaluation, focusing on five areas: Reasoning, Language, Knowledge, Examination, and Understanding, with selected benchmarks for each category. Reasoning: CMNLI (Xu et al., 2020), HellaSwag (HeSw) (Zellers et al., 2019), PIQA (Bisk et al., 2019). Language: CHID (Zheng et al., 2019), Wino-Grande (Wino) (Sakaguchi et al., 2019). Knowledge: CommonSenseQA (CSQA) (Talmor et al., 2018), BoolQ (Clark et al., 2019). Examination: MMLU (Hendrycks et al., 2021), CMMLU (Li"}, {"title": "4.3 Results", "content": "We first present the training loss curves of the three models in Figure 4. From the figure, we observe that our method starts with a lower initial loss compared to the baselines, indicating a better initialization checkpoint. Throughout training, our model's loss consistently remains the lowest. SOLAR even fails to converge to a low loss level even after training on the dataset. Although LLaMA Pro's loss approaches ours after 5k steps, by the end, the model expanded using our method still has the lowest loss. Additionally, our method's loss stabilizes after 2k steps, while LLaMA Pro reaches a similar convergence level only after 5k steps. This demonstrates that models expanded using LESA achieve the same loss convergence with less than half the training cost.\nWe list the time taken to train on the full dataset in Table 1 and find that the time taken by LESA is significantly shorter. It is worth noting that the training of $G_w$ in LESA is very fast, taking less than 5 minutes, making its cost nearly negligible compared to the overhead of continual pre-training.\nFor model performance after continual pre-training, we present the results on various benchmarks in Table 2. It can be inferred that the performance of models expanded with LESA consistently outperforms the baselines in all categories. Specifically, LESA-6k (6k steps) achieves the highest performance across all tasks and PPL. Even with only half of the data used for continual pre-training (3k steps), the models expanded using LESA outperform the baselines trained on the full dataset (6k steps). We also present the results in Table 3 for models trained on the full dataset and then fine-tuned with SFT, showing performance across different tasks. The results still indicate that"}, {"title": "4.4 Evaluation on Knowledge-Related Tasks", "content": "Previous studies, such as LLaMA Pro, highlight that a key advantage of model expansion is the ability to inherit knowledge from the original model. We focus on evaluating performance in knowledge-related tasks. In addition to the main results, we further evaluate performance on two additional knowledge tasks: TriviaQA (Joshi et al., 2017) and NQ (Kwiatkowski et al., 2019). The results in Table 4 show that LESA outperforms previous"}, {"title": "5 Ablation Study", "content": "We also aim to explore whether LESA is effective across different model sizes and families. Specifically, we select several current mainstream model families Llama3, Qwen2.5, Mistral (Mistral@AI, 2025) and use LESA to expand the final layers of the models, increasing their layer count by 1.5x of the original. We use SOLAR initialization as the baseline. Since their method only applies to 32-layer models by concatenating the first 24 and last 24 layers, we adapt it for models with different layer counts. We concatenate the first and last n layers to create a model with 1.5 times the original layers and measure initialization performance using PPL. The results are shown in Table 5."}, {"title": "5.1 Evaluation across Different Model Families", "content": "We also aim to explore whether LESA is effective across different model sizes and families. Specifically, we select several current mainstream model families Llama3, Qwen2.5, Mistral (Mistral@AI, 2025) and use LESA to expand the final layers of the models, increasing their layer count by 1.5x of the original. We use SOLAR initialization as the baseline. Since their method only applies to 32-layer models by concatenating the first 24 and last 24 layers, we adapt it for models with different layer counts. We concatenate the first and last n layers to create a model with 1.5 times the original layers and measure initialization performance using PPL. The results are shown in Table 5.\nThe results show that LESA outperforms SOLAR in initialization performance. Unlike SOLAR, which experiences a PPL explosion on Qwen2.5-32B, LESA remains stable, highlighting the superiority of LESA's predicted parameters over SOLAR's heuristic-based expansion."}, {"title": "5.2 Analysis of $G_w$'s Ability", "content": "We investigate whether $G_w$ can predict intermediate layers between adjacent layers accurately, demonstrating this through loss changes.\nDue to the limited number of samples available for training $G_w$ on individual LLM layers, which makes it difficult to separate a test set and increases the risk of overfitting, we select several models: Llama3-8B, and fine-tuned versions of it, including Llama3-8B-Lexi-Uncensored (Orenguteng, 2024), Meta-Llama3-8B-Instruct, Llama-3-Smaug-8B (Pal et al., 2024), and Llama3-8B-Chinese-Chat (Wang et al., 2024b). Following the procedure outlined in Section 4.1.1, we sequentially select three consecutive layers as samples, resulting in a total of 150 samples. We use 120 samples for training and 30 for testing. The hyperparameters for training are set consistent with those used in the main experiment.\nWe present the loss values of $G_w$ on both the training and test sets after training in Table 6. For comparison, we also show the loss on the training set after random initialization. The results demonstrate that $G_w$ significantly reduces the loss on the training set after training, typically lowering it to below 10% of the random initialization loss. Moreover, the loss on the test set remains at the same level as the training set loss, indicating that $G_w$ effectively learns the underlying patterns of the model parameters."}, {"title": "5.3 Single-Domain Pre-training", "content": "In addition to general-domain pre-training experiments, we explore whether models expanded using our method show greater potential for continual pre-training in a single-domain setting. We conduct experiments in the code domain, using a subset of BigCode (Kocetkov et al., 2022), one of the largest code pre-training datasets, while keeping other settings unchanged. Each model is trained for 40-60 hours and then evaluated on the HumanEval (Chen et al., 2021b) and MBPP (Austin et al., 2021) benchmarks. Table 7 shows that after continual pre-training on the same code dataset, models expanded using our method outperform previous approaches, demonstrating its effectiveness"}, {"title": "5.4 Impact of SVD", "content": "We observe inter-layer patterns of matrices in the SVD space, as shown in Figure 2, which inspires us to train $G_w$ in the SVD space for prediction. We also explore whether $G_w$ can still predict effective matrices for layer expansion without SVD.\nWe conduct an ablation study where we remove the SVD decomposition step while keeping other aspects of the method unchanged. Instead, we directly input the matrices to train $G_w$, which predicts the parameters to be inserted between adjacent layers. We conduct experiments on Llama3-8B, expanding it to 48 layers and performing pre-training with the same data and hyper-parameters as in the main experiment. The loss curves with/without SVD are shown in Figure 5. Without SVD, the model performs worse, with higher loss in the early stages and an average loss of 0.03 higher than with SVD after 3k steps. Thus, the addition of SVD is beneficial. We evaluate the models on several tasks, as shown in Table 8. The results show that while the model expanded without SVD performs slightly worse, it still outperforms the LLaMA Pro baseline. This demonstrates the effectiveness of"}, {"title": "5.5 Impact of Freezing Layers during Continual Pre-training", "content": "Following LLaMA Pro, we train only the newly expanded layers during continual pre-training. We also explore full parameter fine-tuning without freezing any layers. Compared to the main experiment, we directly fine-tune all parameters while keeping the training data and hyperparameters consistent. The loss curves are shown in Figure 6. The figure shows that without freezing layers, loss converges much slower, with fluctuations in the curve. This suggests that, similar to LLaMA Pro, freezing the original parameters is essential for faster and better loss convergence.\nMore experiments on hyper-parameter settings, loss design, and the effectiveness on MoE model can be found in Appendix A."}, {"title": "6 Conclusion", "content": "In this paper, we introduce LESA, a novel approach for depth scaling-up of LLMs that overcomes the limitations of current heuristic-based methods. Using SVD and a neural network, LESA predicts intermediate layer parameters, resulting in improved model initialization and faster convergence during continual pre-training. Extensive experiments show that LESA outperforms existing baselines, delivering superior performance with lower computational costs. Furthermore, LESA is effective across various model sizes, families, and domain-specific tasks, offering a promising solution for scaling LLMs efficiently. Our discovery of"}, {"title": "Limitations", "content": "This work does not yet consider scaling the model to sizes larger than three times the parameters. Based on current model design practices, when increasing the number of layers significantly, it is typically necessary to expand the matrix size of each layer as well, which requires width scaling-up. We plan to explore this in future work.\nAlthough we have conducted a preliminary exploration of LESA on MoE model, the research is still limited by the challenges of constructing routers for the predicted layers and the current large size of MoE models. Further investigation into MoE models is needed, and we consider this as future work."}, {"title": "A More Experiments", "content": "In this section, we provide additional experiments and analyses on hyperparameter settings, loss design, and the effectiveness on the MoE model."}, {"title": "A.1 Impact of Layer Insertion Location", "content": "Previous studies (Yang et al., 2024b; Men et al., 2024; Cao et al., 2024) suggest that LLMs are generally less sensitive to layers near the output end, which can be modified. Therefore, our main experiment focuses on expanding layers closer to the output end. We also aim to explore the performance of our method when expanding layers near the input end. Building on the main experiment, we change the range of the expanded layers from the original 15th to 31st layers to the 1st to 17th layers. We then compare the PPL on Wikipedia for the models after initialization, without further training."}, {"title": "A.2 Ablation on Norm Loss", "content": "We investigate whether it is possible to train $G_w$ without adding the norm loss $\\mathcal{L}_2$. Compared to the main experiment, we remove this loss and calculate the average norm of the matrices in the newly inserted layers predicted by $G_w$."}, {"title": "A.3 Hyper-parameter Impact on Model Initialization", "content": "In this section, we explore the impact of key hyperparameters during the training of $G_w$. We find that the number of epochs and learning rate affect the initialization performance of the model obtained through layer expansion. We also conduct experiments on Llama3-8B, varying the learning rate and epochs while keeping other hyper-parameters consistent with the main experiment."}, {"title": "A.4 Effectiveness on MoE Model", "content": "Recently, LLMs based on the Mixture-of-Experts (MoE) architecture have become increasingly popular. In this section, we explore the effectiveness of LESA on such models. Due to the large size of current MoE models, such as DeepSeek-R1 with 671B parameters (DeepSeek-AI et al., 2025), which cannot be loaded onto our server, we conduct experiments on the smaller LLaMA-MoE-3.0B (Zhu et al., 2024), which has 32 layers.\nWe use LESA to expand the model to 48 layers. However, a unique aspect of MoE models is that each layer has an MLP router, and we have not yet devised a method to generate routers for the newly added layers, since the router is highly dependent on the performance of each expert. Our current approach is to replicate the previous layer's router for the newly expanded layer. We use SOLAR as the baseline and then evaluate the PPL of the expanded model after initialization. The results are shown in Table 12."}, {"title": "B SVD-Based Patterns", "content": "We present the t-SNE visualizations of the top 1 singular values corresponding to the vectors of V, obtained after applying SVD decomposition to the matrices in the MLP and self-attention of different models, in Figure 7 and Figure 8, respectively."}]}