{"title": "Learning the Generalizable Manipulation Skills on Soft-body Tasks via Guided Self-attention Behavior Cloning Policy", "authors": ["Xuetao Li", "Fang Gao", "Jun Yu", "Shaodong Li", "Feng Shuang"], "abstract": "Embodied Al represents a paradigm in Al research where artificial agents are situated within and interact with physical or virtual environments. Despite the recent progress in Embodied AI, it is still very challenging to learn the generalizable manipulation skills that can handle large deformation and topological changes on soft-body objects, such as clay, water, and soil. In this work, we proposed an effective policy, namely GP2E behavior cloning policy, which can guide the agent to learn the generalizable manipulation skills from soft-body tasks, including pouring, filling, hanging, excavating, pinching, and writing. Concretely, we build our policy from three insights:(1) Extracting intricate semantic features from point cloud data and seamlessly integrating them into the robot's end-effector frame; (2) Capturing long-distance interactions in long-horizon tasks through the incorporation of our guided self-attention module; (3) Mitigating overfitting concerns and facilitating model convergence to higher accuracy levels via the introduction of our two-stage fine-tuning strategy. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in the soft-body track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied Al workshop. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their practical applications in real-world scenarios. All codes and models of our solution is available at https://github.com/xtli12/GP2E.git.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rise of Chat-GPT [1], AI (artificial intelli- gence) has once again sparked a global frenzy. But models like GPT, do not have a physical body to interact with the physical or virtual environments. In contrast, Embodied Al represents a significant advancement by inte- grating physical bodies into Al systems. These embodied agents gather environmental information through sensors and execute physical actions using mechanical actuators. Diverg- ing from traditional Al approaches, which often rely on ab- stract symbolic manipulation or passive learning from static datasets, Embodied AI emphasizes the fusion of sensorimotor experiences with learning and decision-making processes. By imbuing intelligence in agents capable of perceiving, acting upon, and manipulating their surroundings, Embodied AI aims to develop systems with human-like understanding, reasoning, and behavior.\nEmbodied Al holds promise for automating various daily tasks, including household chores. To achieve this vision, ro- bots must possess human-like manipulation skills, allowing them to manipulate diverse objects with ease after being trained on a variety of examples. Yet, many existing Embod- ied Al models rely heavily on extensive interactions with training environments, which may not be practical in real- world scenarios. To this end, the SAPIEN ManiSkilll [2] in- troduced a comprehensive simulation benchmark for manipu- lating 3D objects. This benchmark leverages large-scale da- tasets of demonstrations to train agents and evaluates their generalization capabilities across various tasks, such as push- ing chairs, opening cabinet doors, and moving buckets. Build- ing upon this foundation, ManiSkill2 [3] further enhances the benchmark by incorporating a broader range of manipulation tasks to address the generalizability issue. However, despite these advancements, the baseline of ManiSkill2 still face limi- tations in performing Soft-body tasks, including pouring, fill- ing, hanging, excavating, pinching, and writing.\nUpon conducting a thorough investigation, we have identi- fied several critical challenges that impede the baseline per- formance of ManiSkill2 in learning generalizable manipula- tion skills for Soft-body objects: 1) The baseline of ManiSkill2 relies on PointNet [4] to perceive the environment for action planning. However, this method is not robust enough to achieve generalizable shape understanding across complex topologies and geometries; 2) The long-horizon tasks featured in ManiSkill2 entail numerous long-distance interactions be- tween objects and the robot. Successfully tackling these tasks requires the ability to effectively capture such interactions; 3) The demonstration trajectories provided for each task in ManiSkill2 are limited in quantity. Consequently, there exists a risk of overfitting during the training process, where the model may excessively adapt to the specific demonstration data rather than learning generalizable manipulation skills.\nTo address the challenges outlined above, inspired by [5], [6], [7], [8], [9], [10], [11],we present an effective policy termed the Guided Point cloud to End-effector (GP2E) behav- ior cloning policy (refer to Fig. 1), designed to facilitate the learning of generalizable manipulation skills from Soft-body tasks featured in the ManiSkill2 Challenge\u00b9. Our technical contributions encompass:\n1) We propose an advanced 3D computer vision net- work architecture capable of extracting intricate se- mantic features from point cloud data and seamlessly integrating them into the robot's end-effector frame;\n2) We propose a novel Guided self-attention module tai- lored to capture long-distance interactions between objects and the robot within long-horizon tasks;\n3) We propose a Two-stage Fine-tuning Strategy aimed at mitigating overfitting concerns and facilitating model convergence to higher accuracy levels.\nOur proposed method yields significant improvements in success rates, surpassing the ManiSkill2 baseline by an aver- age of 18% across six Soft-body tasks. Notably, our method achieved first place in the \"No Restriction (Soft Body)\" track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied Al workshop\u00b2."}, {"title": "II. RELATED WORK", "content": "A. Soft-body Tasks\nIn real-world scenarios, robots encounter not only rigid bod- ies but also various types of soft materials such as cloth, water, and soil. Several simulators have been developed to facilitate robotic manipulation involving soft bodies. For instance, Mu- JoCo [12] and Bullet [13] utilize the finite element method (FEM) to simulate objects like ropes, cloth, and elastic materi- als. However, FEM-based approaches struggle with handling significant deformation and topological changes, such as scooping flour or cutting dough. Other environments, such as SoftGym [14] and ThreeDWorld [15] leverage Nvidia Flex to simulate large deformations, but they fall short in realistically simulating elasto-plastic materials like clay. PlasticineLab [16] employs the continuum-mechanics-based material point meth- od (MPM), yet it lacks the capability to integrate with rigid robots and requires improvements in simulation and rendering performance. ManiSkill2 develops a custom GPU-based MPM simulator from scratch utilizing Nvidia's Warp [17] JIT framework and native CUDA for optimal efficiency and cus- tomization, ManiSkill2 is the first embodied Al environment to support 2-way coupled rigid-MPM simulation and the first to offer real-time simulation and rendering of MPM materials.\nB. Learning Generalizable Manipulation Skills\nGeneralizable manipulation skills are fundamental in the field of Embodied AI, empowering agents to tackle long- horizon and intricate daily tasks [18], [19]. Previous research endeavors have concentrated on discerning crucial compo- nents or extracting features of articulations to establish repre- sentations that facilitate generalized manipulation across di- verse instances [20], [21], [22]. These approaches often rely on visual cues, such as key location identification, pose esti- mation, or pretrained attention models. Moreover, control- based methods employing model prediction and generative planning techniques have been investigated to achieve robust and adaptable control over both familiar and novel objects [23], [24]. Imitation learning offers a viable solution to equip robots with a variety of manipulation capabilities [25]. Ro- boCook [26] introduced Graph Neural Networks (GNNs) with imitation learning to model tool-object interactions, integrat- ing tool classification with self-supervised policy learning to devise manipulation plans. The Diffusion policy [27] fully unlocked the potential of diffusion models for visuomotor policy learning on physical robots. To foster interdisciplinary collaboration and ensure the reproducibility of research on generalizable manipulation skills, it is essential to establish a versatile and publicly accessible benchmark. In this regard, ManiSkill2 has constructed a benchmark capable of accom- modating object-level variations in both topological and geo- metric attributes, while also addressing the practical challeng- es inherent in manipulation tasks.\nC. Transformer-Based Vision Backbones\nThe advent of transformer-based models [28], [29], [30] in computer vision has marked a significant advancement since the introduction of the Transformer [31] architecture. These models surpassed convolutional networks in terms of both"}, {"title": "III. PROBLEM FORMULATION", "content": "Our problem focuses on policy learning for the development of generalizable soft-body manipulation skills in robots. This entails enabling robots to manipulate a diverse range of soft- body objects in conjunction with rigid bodies within a specific task domain. For instance, pouring water into cups positioned variably and with distinct final target liquid levels. The tasks"}, {"title": "IV. METHOD", "content": "We now introduce our method. The objective of our method- ology is to acquire generalizable soft-body manipulation skills capable of effectively addressing various soft-body tasks through a robust visual-to-end-effector policy. As depicted in Fig. 2, our methodology delineates its objective into three pivotal insights:"}, {"title": "A. Guided Point Cloud To End-effector Policy", "content": "In our pipeline, we adhere to the physical simulation and rendering procedures outlined by ManiSkill2: (1) Conducting physical simulation across multiple worker processes; (2) Tak- ing pictures using both the base camera and the hand camera.; (3) Employing asynchronous rendering to convert images into point clouds on the GPU while simultaneously acquiring ex- pert observations from the replay buffer on the CPU. Unlike ManiSkill1, where the CPU remains idle during GPU render- ing, ManiSkill2 enhances CPU utilization by initiating expert observations while the GPU is engaged in rendering. This technique is named Asynchronous Rendering by ManiSkill2. Expert observations refer to trajectories that successfully ac- complish tasks, serving as invaluable resources to facilitate learning-from-demonstrations methodologies.\nFollowing the environment step in Maniskill2, we acquire two single fused point clouds from different cameras. Subse- quently, we concatenate these points and remove ground arti- facts using height clipping. In the baseline approach of ManiSkill2, PointNet [4] serves as the visual backbone to ran- domly downsample the point cloud to 1200 points. However, we observed that this baseline fails to capture intricate seman- tic features in tasks with long-horizon tasks.\nTo fully leverage the relative positional relationships be- tween objects and the robot within the point cloud, we imple- ment a method of reusing point cloud features from various levels by introducing skip connections and concatenating them into channel-wise condensed features. Additionally, we intro- duce guided self-attention to capture long-distance mapping"}, {"title": "B. Guided self-attention module", "content": "We have observed that in scenarios where the object is dis-"}, {"title": "V. EXPERIMENTS", "content": "A. Datasets and evaluation metrics\nManiSkill2 challenge includes 6 soft-body manipulation tasks that call for agents to engage with soft bodies (refer to Fig. 4), moving or deforming them to achieve predetermined target states."}, {"title": "1) Fill", "content": "Objective: To transfer clay from a bucket into the target beaker.\nSuccess Metric: The task is successful when the volume of clay inside the target beaker exceeds 90% of its capacity, while maintaining the soft body velocity below 0.05.\nEvaluation Protocol: Conduct 100 episodes with varying initial rotations of the bucket and initial positions of the beaker."}, {"title": "2) Hang", "content": "Objective: To hang a noodle on a target rod.\nSuccess Metric: Success is achieved when a portion of the noodle is positioned higher than the rod, both ends of the noodle rest on opposite sides of the rod, the noodle does not touch the ground, the gripper remains open, and the soft body velocity is maintained below 0.05.\nEvaluation Protocol: Conduct 100 episodes with varying initial positions of the gripper and rod poses."}, {"title": "3) Excavate", "content": "Objective: To elevate a predetermined quantity of clay to a designated height.\nSuccess Metric: The task is considered successful when the lifted clay volume meets specified parameters, is positioned above a predefined height threshold, spillage is limited to fewer than 20 clay particles on the ground, and the soft body velocity is kept below 0.05.\nEvaluation Protocol: Conduct 100 episodes with varying bucket poses and initial clay heightmaps."}, {"title": "4) Pour", "content": "Objective: To transfer liquid from a bottle into a beaker.\nSuccess Metric: Success is defined by ensuring that the liquid level in the beaker is within 4mm of the red line, spilled water is limited to fewer than 100 particles, the bot- tle returns to an upright position at the end of the task, and the robot arm velocity remains below 0.05.\nEvaluation Protocol: Conduct 100 episodes with varying bottle positions, water levels in the bottle, and beaker posi- tions."}, {"title": "5) Pinch", "content": "Objective: To mold plasticine into a predefined target shape.\nSuccess Metric: The task is successful when the Chamfer distance between the current plasticine shape and the target shape is less than 0.3 times the Chamfer distance between the initial shape and the target shape.\nEvaluation Protocol: Conduct 50 episodes with varying target shapes."}, {"title": "6) Write", "content": "Objective: To inscribe a specified character onto clay. The target character is randomly selected from an alphabet con- taining over 50 characters."}, {"title": "C. Behavior Cloning", "content": "Due to the soft-body simulator in ManiSkill2 being tailored for visual learning environments, it is preferable to employ a straightforward yet efficient supervised learning algorithm. Specifically, matching the predicted action with the demon- strated action based on visual observations proves effective [40]. Among the spectrum of learning-from-demonstrations algorithms, behavior cloning stands out as a straightforward choice, requiring fewer resources to implement [2]. Hence, we adopt a behavior cloning strategy, aiming to directly match predicted and ground truth actions by minimizing the Euclide- an distance."}, {"title": "D. Two-stage Fine-tuning Strategy", "content": "Lastly, we propose a two-stage fine-tuning strategy to assist the model in alleviating overfitting concerns and achieving higher levels of accuracy.\nAs the training process progresses, we have observed poten- tial overfitting issues, wherein tasks that the model previously solved successfully may become unmanageable later on (refer to the First Stage in Fig. 2). This phenomenon may arise due to the model focusing excessively on certain scenarios within the dataset, thereby hindering its ability to generalize effec- tively to diverse scenarios. Additionally, the loss may become too small to produce substantial gradients necessary for con- verging to higher levels of accuracy.\nTo address these challenges, we propose a two-stage fine- tuning strategy aimed at introducing more variability into the training process to promote convergence to higher accuracy levels. Specifically, we reload the best-performing model from the first stage and then reduce the batch size and simulation steps per environment step during training to decrease the vol- ume of data sampled per step. By reducing the data volume sampled per step, we introduce more noise into the training process, leading to larger losses and gradients. Consequently, the model can escape local minima and converge to higher levels of accuracy (refer to Fig. 2).\nThrough a series of experiments employing various scale strategies for batch size and simulation steps per environment step, we have discovered an intriguing result: utilizing a scale of 0.8 for batch size and 0.9 for simulation steps per environ- ment step consistently leads to higher accuracy in tasks such as Pour, Fill, Excavate, and Hang. We firmly believe that our two-stage fine-tuning strategy holds promise for enabling re- searchers to delve deeper into other fields as well."}, {"title": "C. Results and Analysis", "content": "The findings are consolidated in Table I and Table II. Table I present the outcomes of our investigations across six distinct tasks conducted over 100 trials, each initialized with three distinct random seeds. Table II compares the performance of our method against the ManiSkill2 baseline [3], current state- of-the-art (SOTA) imitation learning methods [26], [27], and the second (ChenBao) and third place (Dee) finishers in the ManiSkill2 Challenge across the six soft-body tasks of the Challenge. As elucidated in Table I, our method incorporates advanced techniques such as Behavior Cloning from Demon-"}, {"title": "1) Effect of Two-stage Fine-tuning Strategy:", "content": "From the data presented in Table I, it is evident that [ManiSkill2 with our Two-stage Fine-tuning Strategy] (Method II), exhibits a note- worthy enhancement in success rate, showcasing a 6% im- provement on average across the six tasks compared to the ManiSkill2 baseline (Method I). Furthermore, our policy [Be- havior Cloning with our Guided Self-attention module and Two-stage Fine-tuning Strategy] (Method IV) demonstrates a significant boost in success rate, with a commendable 10% improvement on average across the six tasks when compared to [our policy lacking the Two-stage Fine-tuning Strategy] (Method III). In Fig. 5, the accuracy curves depicting the per- formance with the Two-stage Fine-tuning Strategy across var- ious tasks are illustrated. Each evaluation point is derived from 100 episodes randomly selected with different random seeds. Notably, for the Excavate and Pour tasks, both Method I (depicted by the blue line) and Method III (depicted by the green line) exhibit a decline in accuracy following their top-1 accuracy points. This trend indicates the potential existence of overfitting issues in Method I and Method III. Through the application of our Two-stage Fine-tuning Strategy, both Method II and Method IV are able to introduce additional noise in certain gradient steps. Consequently, this results in a larger loss derived from the policy actions and the demonstra- tion trajectory, thereby amplifying the gradient of the subse- quent training step. As depicted by the orange line and red line in Fig. 5, this strategy facilitates the policy in escaping local minima points and achieving convergence to higher accuracy levels."}, {"title": "2) Effect of Guided self-attention module:", "content": "From Table I, it is evident that [Behavior Cloning] combined with the [Guided self-attention module] (Method III) outperforms the ManiSkill2 baseline, represented by [Behavior Cloning] com-"}, {"title": "3) Further analysis of Soft-body tasks:", "content": "We note a distinct variance in the precision required across different tasks, which can lead to variations in accuracy scores even among tasks within the same category. For instance, tasks such as Pour and Fill both entail the manipulation of soft-body objects (liquid or clay) into a designated container. However, Fill exhibits a notably higher success rate compared to Pour. The underlying reason for this discrepancy lies in the precision demanded by each task. While Fill allows the robot agent to simply transfer all clay into the beaker, Pour necessitates a higher level of precision, specifically requiring the final liquid level to align precisely with a target line. Consequently, agents must meticu- lously control the tilt angle of the bottle to regulate the amount of liquid poured into the beaker accurately. Similarly, in the case of Excavate, agents must exercise keen judgment regard- ing the depth of excavation required to scoop up a specified quantity of clay. Conversely, tasks such as Hang do not man- date high-precision measurements from the agent, rendering them comparatively easier to accomplish. Furthermore, our observations indicate that Behavior Cloning agents struggle to effectively leverage target shapes to facilitate precise soft- body deformation. Notably, tasks such as Pinch and Write, which entail shape manipulation, present significant challeng- es for Behavior Cloning models, resulting in notably poor per- formance. As depicted in Fig. 6, while the robot learns the basic motion of pinching and demonstrates some progress toward the objective, the achieved level of proficiency falls short of the desired outcome. Similarly, in tasks such as Write, while the robot agent exhibits some capability in reproducing patterns, the resemblance to the target character remains insuf- ficient."}, {"title": "VI. CONCLUSION", "content": "In this paper, we address the challenges of overfitting in soft-body tasks by introducing our Two-stage Fine-tuning Strategy, and tackle the issue of capturing long-distance inter- actions through the implementation of our guided self- attention mechanism. We present a novel policy, the Guided Point Cloud to End-effector (GP2E) policy, which can seam- lessly integrate the point cloud data into the robot's end- effector frame. Our experimental findings showcase that our methods yield notably higher success rates across six tasks when compared to existing baselines. Furthermore, our abla- tion studies validate the efficacy of each introduced technique."}]}