{"title": "DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations", "authors": ["Dongwon Son", "Sanghyeon Son", "Jaehyung Kim", "Beomjoon Kim"], "abstract": "We present DEF-oriCORN, a framework for language-directed manipulation tasks. By leveraging a novel object-based scene representation and diffusion-model-based state estimation algorithm, our framework enables efficient and robust manipulation planning in response to verbal commands, even in tightly packed environments with sparse camera views without any demonstrations. Unlike traditional representations, our representation affords efficient collision checking and language grounding. Compared to state-of-the-art baselines, our framework achieves superior estimation and motion planning performance from sparse RGB images and zero-shot generalizes to real-world scenarios with diverse materials, including transparent and reflective objects, despite being trained exclusively in simulation. Our code for data generation, training, inference, and pre-trained weights are publicly available at: https://sites.google. view/def-oricorn/home.", "sections": [{"title": "1 Introduction", "content": "Even with a single glimpse, humans quickly create a mental representation of a scene that enables them to interact with it [1]. We can efficiently estimate 3D shapes and locations of even occluded objects with a sparse set of viewpoints, and achieve language goals such as \"fetch a soda can and place it next to the bowl\", as shown in Figure 1, by planning a manipulation motion that accounts for uncertainty caused by partial observation. Our goal in this paper is to endow robots with such capability.\nOne of the core challenges in this problem is acquiring an object state representation that encodes the information necessary to ground language commands and plan manipula- tion motions. A common object state representation is a pair of a 6D pose and an implicit shape function [2\u20134], but this has limitations. First, 3D orientations are ill-defined for sym- metric objects, making estimation under partial observability difficult. Second, implicit shape functions are expensive to use for manipulation due to the need for surface point sam- pling and mesh creation for collision checking [5, 6]. Third, pose and shape detection often rely on depth sensors, which struggle with transparent or shiny objects. Lastly, grounding language commands to objects using this representation is non-trivial.\nOur insight is that for most abstract language-commanded object manipulation tasks, a numerical orientation of an object is unnecessary, provided that we can approximate the object's spatial occupancy and check collisions. Therefore, we propose an alternative representation in which we encode both the shape and orientation of an object in a neural representation z and express its position with volumetric centroid $c\\in R^3$ and a set of M representative geometric points $p\\in R^{M\\times3}$, and express an object state as $s = (z, c, p)$.\nWe pre-train the neural-oriented shape representation z by extending OccNet [2], a standard shape reconstruction algorithm for objects at a canonical orientation, to accom- modate varying orientations. To do this, we use a SO(3)- equivariant network, which, unlike standard networks, guar- antees equivariance and allows the model to generalize to novel orientations even without data augmentation. In [7], we have proposed a SO(3)-equivariant network, FER-VN, that achieves state-of-the-art performance for shape prediction tasks, which we use in this work.\nChecking collision with a neural representation, however, is expensive as it requires constructing a mesh from the representation. So, we propose to directly predict collisions from our representation by adding an extra decoder to our pre-training network that predicts the collision between the two objects A and B using their states $s_A$ and $s_B$ when training z. During motion planning or grasp selection, we use our collision decoder instead of an analytical collision checker [8], eliminating the need for expensive mesh construction. Furthermore, thanks to our SO(3)-equivariant architecture, once we estimate object state representation s, we can apply various SE(3) transforms to s to see if an object can fit inside a particular region, allowing efficient pick-and- place planning.\nTo ground the objects mentioned in the language command, we propose to use CLIP [9]. In CLIP, an image encoder is trained such that an image embedding yields a high dot product with the corresponding text embedding. So, we can ground a text, say, \u201ca soda\u201d, to an object states by determining which pixels align with s, aggregating the CLIP features at those pixels, and checking whether we have a large dot product with the soda text embedding. However, this incurs a high computational load because determining the alignment between pixels and object state s requires shooting a ray from each pixel, and checking if it hits s using the occupancy decoder.\nTo resolve this, we add yet another decoder to our pre- training network that, given a ray and object state s, predicts whether the given ray would hit s. To create the CLIP feature for an object given an image, we evaluate the pixel-wise CLIP features and then run the ray-hitting decoder with a set of rays from each pixel to see if it hits the object's state s. Altogether, our representation pre-training network involves occupancy, collision, and ray-hitting decoders, and we call the representation oriented Collision, Occupancy, and Ray- hitting representation (oriCORN). The complete architecture is shown in Figure 2 (left).\nThe second core challenge is estimating object's oriCORN from a sparse set of RGB images once we have pre- trained it. The naive way to do this is to apply 2D object detection on the input images and gather image features for each detected bounding box to estimate object states. However, this approach applies detection to each image independently and it is difficult to aggregate information across images obtained from multiple viewpoints [11].\nInstead, we propose an iterative algorithm inspired by DETR3D [12] that refines the object state estimation from a random initialization. At each iteration t, we use the current estimation $s_t$ to project $p_t$ from 3D to 2D pixel space, aggregate features from the corresponding pixels of each image, and then process the aggregated features to predict $s_{t+1}$. This method allows integrating features from multiple images without having to solve the complex association problem, and integrates seamlessly into a diffusion model, which iteratively denoises a random noise into an estimand, that we use to characterize the uncertainty that stems from occlusion. We call this Diffusion-based Estimator using aggregated Features for oriCORN (DEF-oriCORN). The estimation pipeline is shown in Figure 3.\nThe overview of DEF-oriCORN is given in Figure 2. In both synthetic and real-world estimation, grasp planning, motion planning, and language-directed manipulation prob- lems, we show that our method outperforms the state-of-the- art in terms of both speed and accuracy in estimation and planning."}, {"title": "2 Related Works", "content": "There are several implicit-function-based representations for scenes and objects for manipulation. In compositional NeRF [13] the authors show that you can plan simple object- pushing motions by representing each object with a NeRF and learning a dynamics model in the latent space. However, for scenes with a large number of objects, this approach would require a significant amount of time in both training and inference, since you need a NeRF for each object. In [14], the authors propose an approach that learns a non- object-based scene representation with a single NeRF and uses a CLIP embedding [15] to ground language command to perform pick-and-place operations. The crucial drawback of both of these approaches is that they require a large number of camera views of the scene (e.g. 50 images in [14]) and oracle demonstrations as it is difficult to apply traditional robot planning algorithms to NeRF-based representations.\nThere are extensions of NeRF that use a small number of camera views [16\u201318]. These methods use CNN- based image feature extractors and NeRF decoders that are pre-trained with multiple scenes which enables scene reconstruction with only a few number of images. There also are several works that just use a single RGB-D image for object state estimation [3, 4], in which you estimate the poses and shapes of multiple objects, where each object shape is represented with a Signed Distance Fields (SDFs). However, all of these approaches represent objects using occupancy- based implicit functions (e.g. NeRFs, SDFs) which are very expensive to use for manipulation. To detect a collision, you need to obtain the surface points of objects and then evaluate whether each point is inside another object using its implicit function [19], which requires trading-off accuracy with computational speed. Alternatively, you can run a mesh creation algorithm, such as Marching Cubes [20], and then run an analytical collision checker but this also incurs a large computation time. Furthermore, RGB-D sensors are error-prone for shiny or transparent objects.\nThere are several key-point-based object representations for manipulation. kPAM [21] represents objects with a set of keypoints annotated by humans, which are estimated from camera images and are then used for pick-and- place operations. This approach demonstrates that keypoints can generalize across unseen instances within the same category. However, keypoints have limitations, such as"}, {"title": "2.1 3D object and scene representations", "content": "There are several implicit-function-based representations for scenes and objects for manipulation. In compositional NeRF [13] the authors show that you can plan simple object- pushing motions by representing each object with a NeRF and learning a dynamics model in the latent space. However, for scenes with a large number of objects, this approach would require a significant amount of time in both training and inference, since you need a NeRF for each object. In [14], the authors propose an approach that learns a non- object-based scene representation with a single NeRF and uses a CLIP embedding [15] to ground language command to perform pick-and-place operations. The crucial drawback of both of these approaches is that they require a large number"}, {"title": "2.2 Object localization and shape estimation algorithms", "content": "Recently, several works have proposed object detection algorithms that estimate 3D object locations using a small number of RGB cameras with known poses [12, 28\u2013 30]. Much like our DEF-oriCORN, these methods leverage camera poses to extract image features and use them to predict object locations iteratively, but they differ in how they extract the features from images.\nPETR [30] and PARQ [29] utilize pixel-wise image features across the entire scene, but this is inefficient because it processes irrelevant features too, such as those from the empty space, shown in black dots. Alternatively, we can use features from pixels that correspond to cells of a pre-defined voxel grid [28] but this method must use high voxel grid resolution to achieve high accuracy and similarly struggles with the inefficiency of processing irrelevant grid cells. Our method is most similar to DETR3D [12], which uses the center point for each object to get the corresponding image feature, making it more computationally efficient than the approaches mentioned ealrier. One important distinction, however, between our method and DETR3D is the number of pixel-wise features: we use pixels that correspond to multiple points in $p_t$, while DETR3D uses the pixel that corresponds to a single point, which proved insufficient for capturing the full complexity of accurate object geometry [30].\nThere are several Oject Simultaneous Localization and Mapping (Object SLAM) algorithms for estimating object shapes and locations. NodeSLAM [31] and NeuSE [32] use an occupancy encoder to encode object shape into a latent embedding, and optimize over the embedding and object and camera poses by minimizing the discrepancy from depth observations. These methods address a different problem than ours in that camera locations are also inferred from a sequence of observations. While we can apply these methods to our problem, their use of implicit shape functions and depth sensors renders them expensive to use and error-prone to shiny or transparent objects.\nRecently, several methods have been proposed for shape prediction from a single image [33, 34]. One-2-3-45 [33] has two steps: (1) generating multi-view images from a single image and a relative camera pose using a view-conditioned 2D diffusion model, Zero123, and (2) training a NeRF with these generated multi-view images. Applying the Marching Cubes algorithm to the SDF field predicted by the NeRF, it produces a high-quality reconstructed surface. However,"}, {"title": "2.3 SO(3) equivariant neural networks", "content": "Several practical algorithms have recently been proposed for SO(3)-equivariant neural networks [35\u201339]. Frame averaging introduced in [36] implements equivariance by averaging over a group using symmetry, but its use of Principal Component Analysis (PCA) makes it vulnerable to noise and missing data, which are common in 3D data. Kaba et al. [37] propose an approach that uses an additional network that aligns rotated inputs to a canonical pose and then applies standard point processing networks. The limitation here is that a small error in the alignment network could have a large negative impact on the overall performance. Vector Neurons [35] makes minor modifications to existing neural network operations, such as pooling operations and activation functions, to implement equivariance and does not suffer from the aforementioned problems. One limitation, however, is that it is limited to three-dimensional feature spaces, which makes it difficult to encode high-frequency 3D shape information. Our recent work [7] addresses this issue by proposing a provably equivariant high-dimensional feature representation, FER, that can capture a variety of different frequencies, and proposes FER-VN. We use this to pre-train our representation oriCORN."}, {"title": "2.4 Generative models for characterizing uncertainty in shape prediction", "content": "Estimating 3D shapes from a sparse number of views is challenging due to the inherent uncertainty in object shapes and poses caused by occlusion. To address this issue, several approaches utilize generative models such as a Variational Auto-Encoder (VAE) or diffusion model to express the uncertainty. Simstack [40] employs a depth-conditioned VAE, trained on a dataset of stacked objects, to infer object shapes expressed using truncated signed distance function (TSDF). The decoder of the VAE estimates the complete object shape from partial TSDF. In [41], the authors use a conditional Generative Adversarial Network (CGAN) to express the multimodality present in completing shapes from occluded inputs, and learn a lower-dimensional manifold of shapes using an auto-encoder. Given a partial point cloud, the generator of CGAN is then trained to output a sample from this lower-dimensional manifold. Diffusion models [42] have recently emerged as an alternative for expressing shape reconstruction and generation, due to their stability in training and the ability to characterize multimodality, and has shown success in 3D shape generation tasks [43, 44]."}, {"title": "3 Problem formulation", "content": "We assume that we are given a V number of RGB images, $\\{I_i\\}_{i=1}^V$, intrinsic and extrinsic camera parameters $\\{\\xi_i\\}_{i=1}^V$, and a language command L. Our objective is to develop an object state representation learning and estimation algorithms for language-directed manipulation planning. This consists of the following three sub-problems:\n\u2022 Object representation learning (Section 4): how to train our object state representation, oriCORN, that facilitates efficient collision checking and language grounding.\n\u2022 Estimator learning (Section 5): how to train a diffusion-model-based estimator which takes as inputs $\\{I_i,\\xi_i\\}_{i=1}^V$ and outputs a distribution over $\\{\\hat{s}_i\\}_{i=1}^K$ an estimation of ground truth object state $\\{s_i\\}_{i=1}^K$.\n\u2022 Language grounding using CLIP (Section 5.3): given $\\{I_i,\\xi_i\\}_{i=1}^V$ and $\\{\\hat{s}_i\\}_{i=1}^K$, how to ground object text to $\\{\\hat{s}_i\\}_{i=1}^K$.\nAll training is done in simulation using synthetic data, and zero-shot transferred to the real-world. Figure 2 gives an overview of our framework DEF-oriCORN."}, {"title": "4 Pre-training oriCORN", "content": "Our object state representation, oriCORN, denoted s, consists of three quantities: oriented shape embedding, $z \\in R^u$, volumetric center, $c \\in R^3$, and geometrically representative points $p \\in R^{M\\times3}$. We only train z, and c and p are analytically computed. To train z, we prepare a dataset of the form\n$$\\{(x_u, c_u, p_u, q_{occ,u}, q_{ray,u}, d_u)_{u\\in\\{A,B\\}},\\quad \\(y_{col}, y_{occ,A}, y_{occ,B}, y_{ray,A}, y_{ray,B})\\}$$\nwhere $u \\in \\{A, B\\}$ is an object index, $x_u$ is a surface point cloud, $q_{occ,u} \\in R^3$ is the query point for the occupancy decoder, $q_{ray,u} \\in R^3$ and $d_u \\in R^3$ are the origin and the direction of the ray respectively, $y_{col}$ is the collision label between objects A and B, $y_{occ,u} \\in \\{0,1\\}$ is the occupancy label at $q_{occ,u}$, and $y_{ray,u}$ is the ray-test label for $(q_{ray,u}, d_u)$.\nTo prepare our dataset, we randomly select a pair of object meshes from a 3D shape dataset [45], randomly scale them, and place them in random poses within a bounded 6D space. We then sample its surface point cloud x, calculate its volumetric centroid c, and apply convex decomposition to get the center of each convex hull for geometrically representative points p. We use GJK [8] to get collision label $y_{col}$. For $q_{occ}$, we sample surface points and add Gaussian noise, and for $y_{occ}$, we evaluate whether $q_{occ}$ is inside of the object. To simulate rays for training $f_{ray}$, we randomly select a direction d uniformly from the unit sphere in $R^3$. We then sample a surface point, add small Gaussian noise, and calculate the starting point of the ray $q_{ray}$ by subtracting d scaled by a predefined large distance from the perturbed surface point. This process effectively positions $q_{ray}$ to ensure it originates sufficiently far from the object while maintaining its directional integrity towards the object. Given d, $q_{ray}$ and the object mesh, we evaluate a ray-hitting result by analytical mesh-ray hitting test [46] to get $y_{ray}$."}, {"title": "5 Learning and using the estimator", "content": "Our goal is to learn an estimator that predicts s for each object in a scene from multiple RGB images with known poses. To characterize the uncertainty caused by partial observability, we use a diffusion model and implement a denoising function $f_{den}$ as shown in Figure 6.\nThe function takes a set of $N$ (the maximum number of the objects on the scene) object representations $\\{\\hat{s}_i\\}_{i=1}^K$ at diffusion timestep t and images and camera parameters $\\{I_i,\\xi_i\\}_{i=1}^V$, and iteratively refine $\\hat{s}_t$ from t = T to 0 to output $\\{\\hat{s}_i^0\\}_{i=1}^K$ where T is the number of diffusion time steps. We design $f_{den}$ to predict denoised object state representation $\\{\\hat{s}_i^{t-1}\\}_{i=1}^K$ directly instead of noise following previous works [47\u201349]. $\\hat{s}^{t-1}$ can be obtained by adding noise to $\\hat{s}^0$, whose scale is determined by t \u2013 1, via a forward noising step.\nTo prepare the training dataset, we create a scene in simulation (e.g. Figure 8) and create a data point of the form $\\{(I_i, \\xi_i)_{i=1}^V, (s_i)_{i=1}^K\\}$, where K is the true number of objects in the scene and s is the target object state representation, obtained by feeding the the ground-truth object point cloud to $f_{enc}$ and analytically computing c and p. We denote target object state representation at diffusion time step t as $s_i^t$, so $s_i^0$ is target object representation and $s_i^T$ is a Gaussian noise. Given $s_i^0$, we first sample diffusion time step t from a uniform distribution, and add a Gaussian noise, whose scale is determined by t, to $s_i^0$ to get $s_i^t$ using a forward noising step. We also make $f_{den}$ to output $o_{conf,i}$, the confidence of estimation for object ith. Then, we minimize the following loss\n$$ \\sum_{i,j\\in bipartite(\\hat{s}\\_i^0, s\\_i^0)} dist(\\hat{s}\\_i^0, s\\_j^0) + \\sum_{i\\in \\{i| not matched\\}} BCE(o\\_{conf,i}, 1) + \\sum\\_{i\\in \\{i not matched\\}} BCE(o\\_{conf,i}, 0) $$\nwhere bipartite is a function that determines bipartite matching between two sets based on the distance function dist, BCE is binary cross-entropy, and {i|not matched} is a set of indices which are not matched by the bipartite function. We define the distance between two object states as\n$$ dist(s,s') = \\alpha\\_p CD(p,p') + \\alpha\\_c \\mid\\mid c - c' \\mid\\mid^2 + \\alpha\\_z \\mid\\mid z - z' \\mid\\mid F $$\nwhere $s = (z, c, p)$, $s' = (z', c', p')$, $\\mid\\mid . \\mid\\mid F$ is Frobenius norm, CD is Chamfer distance, and $\\alpha\\_p, \\alpha\\_c, \\alpha\\_z \\in R$ are hyperparameters.\nAs described in [50], our model is also susceptible to sampling drift where samples generated by $f_{den}$, denoted as $\\hat{s}\\_t$, may deviate from the target created during the training, $s\\_t$, and this deviation can lead the denoising neural network to encounter $\\hat{s}\\_t$ outside its training distribution. To mitigate this issue, we use a strategy that utilizes the reverse denoising process rather than the forward noising process. The major difference from standard diffusion training [51] is that we"}, {"title": "5.1 Estimator learning", "content": "Our goal is to learn an estimator that predicts s for each object in a scene from multiple RGB images with known poses. To characterize the uncertainty caused by partial observability, we use a diffusion model and implement a denoising function $f_{den}$ as shown in Figure 6.\nThe function takes a set of $N$ (the maximum number of the objects on the scene) object representations $\\{\\hat{s}_i\\}_{i=1}^K$ at diffusion timestep t and images and camera parameters $\\{I_i,\\xi_i\\}_{i=1}^V$, and iteratively refine $\\hat{s}_t$ from t = T to 0 to output $\\{\\hat{s}_i^0\\}_{i=1}^K$ where T is the number of diffusion time steps. We design $f_{den}$ to predict denoised object state representation $\\{\\hat{s}_i^{t-1}\\}_{i=1}^K$ directly instead of noise following previous works [47\u201349]. $\\hat{s}^{t-1}$ can be obtained by adding noise to $\\hat{s}^0$, whose scale is determined by t \u2013 1, via a forward noising step.\nTo prepare the training dataset, we create a scene in simulation (e.g. Figure 8) and create a data point of the form $\\{(I_i, \\xi_i)_{i=1}^V, (s_i)_{i=1}^K\\}$, where K is the true number of objects in the scene and s is the target object state representation, obtained by feeding the the ground-truth object point cloud to $f_{enc}$ and analytically computing c and p. We denote target object state representation at diffusion time step t as $s_i^t$, so $s_i^0$ is target object representation and $s_i^T$ is a Gaussian noise. Given $s_i^0$, we first sample diffusion time step t from a uniform distribution, and add a Gaussian noise, whose scale is determined by t, to $s_i^0$ to get $s_i^t$ using a forward noising step. We also make $f_{den}$ to output $o_{conf,i}$, the confidence of estimation for object ith. Then, we minimize the following loss\n$$ \\sum_{i,j\\in bipartite(\\hat{s}\\_i^0, s\\_i^0)} dist(\\hat{s}\\_i^0, s\\_j^0) + \\sum_{i\\in \\{i| not matched\\}} BCE(o\\_{conf,i}, 1) + \\sum\\_{i\\in \\{i not matched\\}} BCE(o\\_{conf,i}, 0) $$\nwhere bipartite is a function that determines bipartite matching between two sets based on the distance function dist, BCE is binary cross-entropy, and {i|not matched} is a set of indices which are not matched by the bipartite function. We define the distance between two object states as\n$$ dist(s,s') = \\alpha\\_p CD(p,p') + \\alpha\\_c \\mid\\mid c - c' \\mid\\mid^2 + \\alpha\\_z \\mid\\mid z - z' \\mid\\mid F $$\nwhere $s = (z, c, p)$, $s' = (z', c', p')$, $\\mid\\mid . \\mid\\mid F$ is Frobenius norm, CD is Chamfer distance, and $\\alpha\\_p, \\alpha\\_c, \\alpha\\_z \\in R$ are hyperparameters.\nAs described in [50], our model is also susceptible to sampling drift where samples generated by $f_{den}$, denoted as $\\hat{s}\\_t$, may deviate from the target created during the training, $s\\_t$, and this deviation can lead the denoising neural network to encounter $\\hat{s}\\_t$ outside its training distribution. To mitigate this issue, we use a strategy that utilizes the reverse denoising process rather than the forward noising process. The major difference from standard diffusion training [51] is that we"}, {"title": "5.2 Estimating the object states", "content": "Algorithm 1 shows how to estimate states using our diffusion model. It takes in images and camera parameters $\\{I_i,\\xi_i\\}_{i=1}^V$, the maximum number of objects N, number of diffusion timesteps T, diffusion time step interval for DDIM [52] sampling tstep, and confidence score threshold \u03f5 as inputs.\nThe algorithm begins by initializing object states at diffusion timestep T, $S_T := \\{\\hat{s}_i\\}_{i=1}^K$, with standard Gaussian noise (L1). After the initialization, lines 2-6 alternate between denoising and noising steps: the denoising step (L3) predicts $S_0$ from $S_t$ using $f_{den}$, and the noising step adds noise to $S_0$ (L5), producing $S_{t-tstep}$ for the next iteration, as done in DDIM sampling process [52]. In each time step, we filter s that has a low confidence score by replacing low-confidence representations with random Gaussian noise (L4). After this process, object representations with confidence values lower than \u03f5 are removed (lines 7-11), resulting in the final set of object representations S, where $O_{conf}[n]$ denotes the nth element of $O_{con f}$.\nAlgorithm 1 gives a prediction of S, but a higher level of precision is required for complex tasks such as object grasping. To achieve this, we use Algorithm 2, which uses Algorithm 1 as a subprocedure to implement an additional refinement step using foreground segmentation. The algorithm begins by initializing an empty set C to store the refined S (L1) and an empty set of evaluations {IoU} to store the evaluation scores of S (L2). It then predicts foreground segmentation masks $\\{M_i\\}_{i=1}^V$ for each image using a segmentation decoder $f_{seg}$ (L3). A simple"}, {"title": "5.3 Grounding language to estimated object state representations", "content": "Once the set of object states $\\{\\hat{s}_i\\}_{i=1}^K$ has been estimated, we ground each $\\hat{s}_i$ to a CLIP text embedding. This process involves two sub-problems: (1) extracting the CLIP features"}, {"title": "6 Experiments", "content": "We now compare our framework DEF-oriCORN to baselines to validate the two claims: (1) our representation, trained with collision and ray testing decoders, is more computationally efficient than other representations that do not use our decoders and (2) our estimator achieves higher accuracy in estimation and robustness than state-of-the-art baselines.\nWe first compare estimation accuracies in simulated scenes. We create these scenes by selecting up to 7 objects from an object set and randomly placing them in two types of environments, \"table\" and \"shelf.\" We capture images $\\{I_i\\}_{i=1}^V$ from 2- 5 distinct viewpoints, where camera parameters $\\{\\xi_i\\}_{i=1}^V$ are chosen randomly within a specified boundary. We use PyBullet [53] for physics simulation and a ray-tracing synthetic renderer [54] for the images. We use the subset of NOCS training object set [45] for training, which consists of 682 shapes. The training set consists of five categories from ShapeNetCore [55]: a bowl, a bottle, a cup, a camera, and a mug. For the table, we use a primitive rectangle shape with random size. For the shelf, we use three shelf shapes from ShapeNet [55]. The training dataset consists of 300k scenes. Detailed values for these hyperparameters used during experiments can be found in Appendix B.\nTo evaluate the estimation accuracy, we use the same five categories as the NOCS training set but use novel 119 shapes that do not overlap with the training set. We report two quantities: Average Precision (AP) and Chamfer Distance (CD) [2], where AP evaluates the object detection performance and CD evaluates the accuracy of the predicted shapes. For AP, we follow the evaluation procedure of [4]:"}, {"title": "6.1 Simulation experiments", "content": "We now compare our framework DEF-oriCORN to baselines to validate the two claims: (1) our representation, trained with collision and ray testing decoders, is more computationally efficient than other representations that do not use our decoders and (2) our estimator achieves higher accuracy in estimation and robustness than state-of-the-art baselines.\nWe first compare estimation accuracies in simulated scenes. We create these scenes by selecting up to 7 objects from an object set and randomly placing them in two types of environments, \"table\" and \"shelf.\" We capture images $\\{I_i\\}_{i=1}^V$ from 2- 5 distinct viewpoints, where camera parameters $\\{\\xi_i\\}_{i=1}^V$ are chosen randomly within a specified boundary. We use PyBullet [53] for physics simulation and a ray-tracing synthetic renderer [54] for the images. We use the subset of NOCS training object set [45] for training, which consists of 682 shapes. The training set consists of five categories from ShapeNetCore [55]: a bowl, a bottle, a cup, a camera, and a mug. For the table, we use a primitive rectangle shape with random size. For the shelf, we use three shelf shapes from ShapeNet [55]. The training dataset consists of 300k scenes. Detailed values for these hyperparameters used during experiments can be found in Appendix B.\nTo evaluate the estimation accuracy, we use the same five categories as the NOCS training set but use novel 119 shapes that do not overlap with the training set. We report two quantities: Average Precision (AP) and Chamfer Distance (CD) [2], where AP evaluates the object detection performance and CD evaluates the accuracy of the predicted shapes. For AP, we follow the evaluation procedure of [4]:"}, {"title": "7 Conclusion", "content": "In this work, we propose DEF-oriCORN for object-based scene representation and estimation tailored for language-directed manipulation tasks. Our novel representation oriCORN leverages a SO(3)-equivariant network and three decoders to encode oriented shapes into a neural representa- tion, enabling efficient and robust manipulation planning in complex environments. We demonstrate that our method out- performs conventional occupancy-based implicit representa- tions by efficiently grounding language commands to objects and predicting collision without the need for extensive sur- face point evaluation or mesh conversion. Our integrated estimation and representation framework, DEF-oriCORN, effectively handles the uncertainty inherent in sparse camera views and partial observability by integrating a diffusion model into the iterative network structure. This enhances the success rate of grasping and motion planning tasks. Future work will focus on refining the estimation process and exploring the potential of our framework in more diverse and dynamic scenarios. Additionally, we plan to explore the inte- gration of task and motion planning algorithms to extend our framework's applicability to long-horizon tasks, enhancing its utility for more comprehensive planning scenarios."}, {"title": "A Planning with uncertainty", "content": "One strength of our approach is its ability to express uncertainty about estimation results using a diffusion estimator (Figure 9). Specifically, we employ a sampling- based approximation to approximate the distribution of object representations. To achieve this, given a large B, we take top-H samples of S from diffusion estimator(Algorithm 2) instead of only taking the top-1 sample. To account for the uncertainty represented in H samples of S, we apply minor modifications to standard tools such as RRT [5, 59] and grasping heuristic score [61-63] to incorporate uncertainty in the object representations.\nFor motion planning, we use RRT* [59] but check the validity of the path by contact test using H samples of S. This ensures that the planned motion accounts for the uncertainty in the object representations, leading to higher performance, as shown in the experiment (Section 6.1).\nFor grasping planning, given H number of object representation samples of the target object, we uniformly sample grasp candidates within a bounded range and evaluate them using an antipodal-based heuristic score [61-63]. This results in H grasp scores for each object sample, which are then averaged to obtain the final grasp score.\nTo utilize oriCORN for grasp planning, we randomly sample 6D end-effector poses near the object and select the best grasp pose using the grasping heuristic score [61- 63], which is the function of two contact points and their contact normals. Given 37500 random samples near the object, we first filter out grasp poses that collide with the object using $f\\_{col}$. To compute the heuristic score [61-63] for"}, {"title": "A.1 Algorithms", "content": "One strength of our approach is its ability to express uncertainty about estimation results using a diffusion estimator (Figure 9). Specifically, we employ a sampling- based approximation to approximate the distribution of object representations. To achieve this, given a large B, we take top-H samples of S from diffusion estimator(Algorithm 2) instead of only taking the top-1 sample. To account for the uncertainty represented in H samples of S, we apply minor modifications to standard tools such as RRT [5, 59", "61-63": "to incorporate uncertainty in the object representations.\nFor motion planning, we use RRT* [59"}]}