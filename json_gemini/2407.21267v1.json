{"title": "DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations", "authors": ["Dongwon Son", "Sanghyeon Son", "Jaehyung Kim", "Beomjoon Kim"], "abstract": "We present DEF-oriCORN, a framework for language-directed manipulation tasks. By leveraging a novel object-based scene representation and diffusion-model-based state estimation algorithm, our framework enables efficient and robust manipulation planning in response to verbal commands, even in tightly packed environments with sparse camera views without any demonstrations. Unlike traditional representations, our representation affords efficient collision checking and language grounding. Compared to state-of-the-art baselines, our framework achieves superior estimation and motion planning performance from sparse RGB images and zero-shot generalizes to real-world scenarios with diverse materials, including transparent and reflective objects, despite being trained exclusively in simulation. Our code for data generation, training, inference, and pre-trained weights are publicly available at: https://sites.google.view/def-oricorn/home.", "sections": [{"title": "1 Introduction", "content": "Even with a single glimpse, humans quickly create a mental representation of a scene that enables them to interact with it [1]. We can efficiently estimate 3D shapes and locations of even occluded objects with a sparse set of viewpoints, and achieve language goals such as \"fetch a soda can and place it next to the bowl\", as shown in Figure 1, by planning a manipulation motion that accounts for uncertainty caused by partial observation. Our goal in this paper is to endow robots with such capability.\nOne of the core challenges in this problem is acquiring an object state representation that encodes the information necessary to ground language commands and plan manipulation motions. A common object state representation is a pair of a 6D pose and an implicit shape function [2-4], but this has limitations. First, 3D orientations are ill-defined for symmetric objects, making estimation under partial observability difficult. Second, implicit shape functions are expensive to use for manipulation due to the need for surface point sampling and mesh creation for collision checking [5, 6]. Third, pose and shape detection often rely on depth sensors, which struggle with transparent or shiny objects. Lastly, grounding language commands to objects using this representation is non-trivial.\nOur insight is that for most abstract language-commanded object manipulation tasks, a numerical orientation of an object is unnecessary, provided that we can approximate the object's spatial occupancy and check collisions. Therefore, we propose an alternative representation in which we encode both the shape and orientation of an object in a neural representation z and express its position with volumetric centroid $c\\in R^3$ and a set of M representative geometric points $p\\in R^{M\\times 3}$, and express an object state as $s=(z, c, p)$.\nWe pre-train the neural-oriented shape representation z by extending OccNet [2], a standard shape reconstruction"}, {"title": "2 Related Works", "content": "2.1 3D object and scene representations\nThere are several implicit-function-based representations for scenes and objects for manipulation. In compositional NeRF [13] the authors show that you can plan simple object-pushing motions by representing each object with a NeRF and learning a dynamics model in the latent space. However, for scenes with a large number of objects, this approach would require a significant amount of time in both training and inference, since you need a NeRF for each object. In [14], the authors propose an approach that learns a non-object-based scene representation with a single NeRF and uses a CLIP embedding [15] to ground language command to perform pick-and-place operations. The crucial drawback of both of these approaches is that they require a large number of camera views of the scene (e.g. 50 images in [14]) and oracle demonstrations as it is difficult to apply traditional robot planning algorithms to NeRF-based representations.\nThere are extensions of NeRF that use a small number of camera views [16-18]. These methods use CNN-based image feature extractors and NeRF decoders that are pre-trained with multiple scenes which enables scene reconstruction with only a few number of images. There also are several works that just use a single RGB-D image for object state estimation [3, 4], in which you estimate the poses and shapes of multiple objects, where each object shape is represented with a Signed Distance Fields (SDFs). However, all of these approaches represent objects using occupancy-based implicit functions (e.g. NeRFs, SDFs) which are very expensive to use for manipulation. To detect a collision, you need to obtain the surface points of objects and then evaluate whether each point is inside another object using its implicit function [19], which requires trading-off accuracy with computational speed. Alternatively, you can run a mesh creation algorithm, such as Marching Cubes [20], and then run an analytical collision checker but this also incurs a large computation time. Furthermore, RGB-D sensors are error-prone for shiny or transparent objects.\nThere are several key-point-based object representations for manipulation. kPAM [21] represents objects with a set of keypoints annotated by humans, which are estimated from camera images and are then used for pick-and-place operations. This approach demonstrates that keypoints can generalize across unseen instances within the same category. However, keypoints have limitations, such as"}, {"title": "2.2 Object localization and shape estimation algorithms", "content": "Recently, several works have proposed object detection algorithms that estimate 3D object locations using a small number of RGB cameras with known poses [12, 28-30]. Much like our DEF-oriCORN, these methods leverage camera poses to extract image features and use them to predict object locations iteratively, but they differ in how they extract the features from images as shown in Figure 4.\nPETR [30] and PARQ [29] utilize pixel-wise image features across the entire scene (Figure 4-(a)), but this is inefficient because it processes irrelevant features too, such as those from the empty space, shown in black dots in Figure 4-(a). Alternatively, we can use features from pixels that correspond to cells of a pre-defined voxel grid [28] (Figure4-(b)) but this method must use high voxel grid resolution to achieve high accuracy and similarly struggles with the inefficiency of processing irrelevant grid cells. Our method is most similar to DETR3D [12], which uses the center point for each object to get the corresponding image feature, making it more computationally efficient than the approaches mentioned ealrier. One important distinction, however, between our method and DETR3D is the number of pixel-wise features: we use pixels that correspond to multiple points in pt, while DETR3D uses the pixel that corresponds to a single point, which proved insufficient for capturing the full complexity of accurate object geometry [30]. The comparison is shown in Figure 4-(c) and (d). All these methods are primarily proposed for object localization and do not infer shapes. More importantly, these methods cannot characterize the uncertainty in shape and location estimation caused by occlusions.\nThere are several Oject Simultaneous Localization and Mapping (Object SLAM) algorithms for estimating object shapes and locations. NodeSLAM [31] and NeuSE [32] use an occupancy encoder to encode object shape into a latent embedding, and optimize over the embedding and object and camera poses by minimizing the discrepancy from depth observations. These methods address a different problem than ours in that camera locations are also inferred from a sequence of observations. While we can apply these methods to our problem, their use of implicit shape functions and depth sensors renders them expensive to use and error-prone to shiny or transparent objects.\nRecently, several methods have been proposed for shape prediction from a single image [33, 34]. One-2-3-45 [33] has two steps: (1) generating multi-view images from a single image and a relative camera pose using a view-conditioned 2D diffusion model, Zero123, and (2) training a NeRF with these generated multi-view images. Applying the Marching Cubes algorithm to the SDF field predicted by the NeRF, it produces a high-quality reconstructed surface. However,"}, {"title": "2.3 SO(3) equivariant neural networks", "content": "Several practical algorithms have recently been proposed for SO(3)-equivariant neural networks [35-39]. Frame averaging introduced in [36] implements equivariance by averaging over a group using symmetry, but its use of Principal Component Analysis (PCA) makes it vulnerable to noise and missing data, which are common in 3D data. Kaba et al. [37] propose an approach that uses an additional network that aligns rotated inputs to a canonical pose and then applies standard point processing networks. The limitation here is that a small error in the alignment network could have a large negative impact on the overall performance. Vector Neurons [35] makes minor modifications to existing neural network operations, such as pooling operations and activation functions, to implement equivariance and does not suffer from the aforementioned problems. One limitation, however, is that it is limited to three-dimensional feature spaces, which makes it difficult to encode high-frequency 3D shape information. Our recent work [7] addresses this issue by proposing a provably equivariant high-dimensional feature representation, FER, that can capture a variety of different frequencies, and proposes FER-VN. We use this to pre-train our representation oriCORN."}, {"title": "2.4 Generative models for characterizing uncertainty in shape prediction", "content": "Estimating 3D shapes from a sparse number of views is challenging due to the inherent uncertainty in object shapes and poses caused by occlusion. To address this issue, several approaches utilize generative models such as a Variational Auto-Encoder (VAE) or diffusion model to express the uncertainty. Simstack [40] employs a depth-conditioned VAE, trained on a dataset of stacked objects, to infer object shapes expressed using truncated signed distance function (TSDF). The decoder of the VAE estimates the complete object shape from partial TSDF. In [41], the authors use a conditional Generative Adversarial Network (CGAN) to express the multimodality present in completing shapes from occluded inputs, and learn a lower-dimensional manifold of shapes using an auto-encoder. Given a partial point cloud, the generator of CGAN is then trained to output a sample from this lower-dimensional manifold. Diffusion models [42] have recently emerged as an alternative for expressing shape reconstruction and generation, due to their stability in training and the ability to characterize multimodality, and has shown success in 3D shape generation tasks [43, 44]."}, {"title": "3 Problem formulation", "content": "We assume that we are given a V number of RGB images, $\\{I_i\\}_{i=1}^V$, intrinsic and extrinsic camera parameters $\\{\\xi_i\\}_{i=1}^V$, and a language command L. Our objective is to develop an object state representation learning and estimation algorithms for language-directed manipulation planning. This consists of the following three sub-problems:\n* Object representation learning (Section 4): how to train our object state representation, oriCORN, that facilitates efficient collision checking and language grounding.\n* Estimator learning (Section 5): how to train a diffusion-model-based estimator which takes as inputs $\\{I_i,\\xi_i\\}_{i=1}^V$ and outputs a distribution over $\\{\\hat{s}_i\\}_{i=1}^K$ an estimation of ground truth object state $\\{s_i\\}_{i=1}^K$.\n* Language grounding using CLIP (Section 5.3): given $\\{I_i,\\xi_i\\}_{i=1}^V$ and $\\{\\hat{s}_i\\}_{i=1}^K$, how to ground object text to $\\{\\hat{s}_i\\}_{i=1}^K$.\nAll training is done in simulation using synthetic data, and zero-shot transferred to the real-world. Figure 2 gives an overview of our framework DEF-oriCORN."}, {"title": "4 Pre-training oriCORN", "content": "Our object state representation, oriCORN, denoted s, consists of three quantities: oriented shape embedding, $z\\in R^u$, volumetric center, $c \\in R^3$, and geometrically representative points $p \\in R^{m \\times 3}$. We only train z, and c and p are analytically computed. To train z, we prepare a dataset of the form\n$\\qquad \\{(x_u, c_u, p_u, q_{occ,u}, q_{ray,u}, d_u)_{u \\in {A,B}}, (y_{col}, y_{occ, A}, y_{occ,B}, y_{ray,A}, y_{ray,B})\\}$\nwhere $u \\in {A, B}$ is an object index, $x_u$ is a surface point cloud, $q_{occ,u} \\in R^3$ is the query point for the occupancy decoder, $q_{ray,u} \\in R^3$ and $d_u \\in R^3$ are the origin and the direction of the ray respectively, $y_{col}$ is the collision label between objects A and B, $y_{occ,u} \\in {0,1}$ is the occupancy label at $q_{occ,u}$, and $y_{ray,u}$ is the ray-test label for $(q_{ray,u}, d_u)$.\nTo prepare our dataset, we randomly select a pair of object meshes from a 3D shape dataset [45], randomly scale them, and place them in random poses within a bounded 6D space. We then sample its surface point cloud x, calculate its volumetric centroid c, and apply convex decomposition to get the center of each convex hull for geometrically representative points p. We use GJK [8] to get collision label $y_{col}$. For $q_{occ}$, we sample surface points and add Gaussian noise, and for $y_{occ}$, we evaluate whether $q_{occ}$ is inside of the object. To simulate rays for training $f_{ray}$, we randomly select a direction d uniformly from the unit sphere in $R^3$. We then sample a surface point, add small Gaussian noise, and calculate the starting point of the ray $q_{ray}$ by subtracting d scaled by a predefined large distance from the perturbed surface point. This process effectively positions $q_{ray}$ to ensure it originates sufficiently far from the object while maintaining its directional integrity towards the object. Given d, $q_{ray}$ and the object mesh, we evaluate a ray-hitting result by analytical mesh-ray hitting test [46] to get $y_{ray}$."}, {"title": "5 Learning and using the estimator", "content": "5.1 Estimator learning\nOur goal is to learn an estimator that predicts s for each object in a scene from multiple RGB images with known poses. To characterize the uncertainty caused by partial observability, we use a diffusion model and implement a denoising function $f_{den}$ as shown in Figure 6.\nThe function takes a set of N (the maximum number of the objects on the scene) object representations $\\{\\hat{s}_i^t\\}_{i=1}^K$ at diffusion timestep t and images and camera parameters $\\{I_i, \\xi_i\\}_{i=1}^V$, and iteratively refine $\\hat{s}^t$ from t = T to 0 to output $\\{\\hat{s}_i^0\\}_{i=1}^K$ where T is the number of diffusion time steps. We design $f_{den}$ to predict denoised object state representation $\\{\\hat{s}_i^{t-1}\\}_{i=1}^K$ directly instead of noise following previous works [47-49]. $\\hat{s}^{t-1}$ can be obtained by adding noise to $\\hat{s}^0$, whose scale is determined by t \u2013 1, via a forward noising step.\nTo prepare the training dataset, we create a scene in simulation (e.g. Figure 8) and create a data point of the form $\\{(I_i, \\xi_i)_{i=1}^V, (s_i)_{i=1}^K\\}$, where K is the true number of objects in the scene and s is the target object state representation, obtained by feeding the the ground-truth object point cloud to $f_{enc}$ and analytically computing c and p. We denote target object state representation at diffusion time step t as $s^t$, so $s^0$ is target object representation and $s^T$ is a Gaussian noise. Given $s^0$, we first sample diffusion time step t from a uniform distribution, and add a Gaussian noise, whose scale is determined by t, to $s^0$ to get $s^t$ using a forward noising step. We also make $f_{den}$ to output $o_{conf,i}$, the confidence of estimation for object ith. Then, we minimize the following loss\n$\\qquad \\sum_{i,j\\in bipartite(\\{\\hat{s}_i\\}_{i=1}^K,\\{s_i\\}_{i=1}^K)}dist(\\hat{s}_i, s_j) + \\sum_{i \\in \\{i | not matched\\}}BCE(o_{conf,i}, 1) + \\sum_{i \\in \\{i | not matched\\}}BCE(o_{conf,i}, 0)  \\quad(1)$\nwhere bipartite is a function that determines bipartite matching between two sets based on the distance function dist, BCE is binary cross-entropy, and $\\{i|not matched\\}$ is a set of indices which are not matched by the bipartite function. We define the distance between two object states as\n$\\qquad dist(s,s') = \\alpha_p CD(p,p') + \\alpha_c \\|c - c'\\|^2 + \\alpha_z \\|z - z'\\|_F \\quad (2)$"}, {"title": "5.2 Estimating the object states", "content": "Algorithm 1 shows how to estimate states using our diffusion model. It takes in images and camera parameters $\\{I_i, \\xi_i\\}_{i=1}^V$, the maximum number of objects N, number of diffusion timesteps T, diffusion time step interval for DDIM [52] sampling $t_{step}$, and confidence score threshold $\\epsilon$ as inputs. The algorithm begins by initializing object states at diffusion timestep T, $S_T := \\{\\hat{s}_i^T\\}_{i=1}^N$, with standard Gaussian noise (L1). After the initialization, lines 2-6 alternate between denoising and noising steps: the denoising step (L3) predicts $s_0$ from $S_t$ using $f_{den}$, and the noising step adds noise to $s_0$ (L5), producing $S_{t-t_{step}}$ for the next iteration, as done in DDIM sampling process [52]. In each time step, we filter s that has a low confidence score by replacing low-confidence representations with random Gaussian noise (L4). After this process, object representations with confidence values lower than e are removed (lines 7-11), resulting in the final set of object representations S, where $o_{conf}[n]$ denotes the nth element of $o_{con f}$.\nAlgorithm 1 gives a prediction of S, but a higher level of precision is required for complex tasks such as object grasping. To achieve this, we use Algorithm 2, which uses Algorithm 1 as a subprocedure to implement an additional refinement step using foreground segmentation. The algorithm begins by initializing an empty set C to store the refined S (L1) and an empty set of evaluations $\\{IoU\\}$ to store the evaluation scores of S (L2). It then predicts foreground segmentation masks $\\{M_i\\}_{i=1}^V$ for each image using a segmentation decoder $f_{seg}$ (L3). A simple"}, {"title": "5.3 Grounding language to estimated object state representations", "content": "Once the set of object states $\\{\\hat{s}_i\\}_{i=1}^K$ has been estimated, we ground each $\\hat{s}_i$ to a CLIP text embedding. This process involves two sub-problems: (1) extracting the CLIP features"}, {"title": "6 Experiments", "content": "6.1 Simulation experiments\nWe now compare our framework DEF-oriCORN to baselines to validate the two claims: (1) our representation, trained with collision and ray testing decoders, is more computationally efficient than other representations that do not use our decoders and (2) our estimator achieves higher accuracy in estimation and robustness than state-of-the-art baselines.\nWe first compare estimation accuracies in simulated scenes such as shown in Figure 8. We create these scenes by selecting up to 7 objects from an object set and randomly placing them in two types of environments, \"table\" and \"shelf.\" We capture images $\\{I_i\\}_{i=1}^V$ from 2-5 distinct viewpoints, where camera parameters $\\{\\xi_i\\}_{i=1}^V$ are chosen randomly within a specified boundary. We use PyBullet [53] for physics simulation and a ray-tracing synthetic renderer [54] for the images. We use the subset of NOCS training object set [45] for training, which consists of 682 shapes. The training set consists of five categories from ShapeNetCore [55]: a bowl, a bottle, a cup, a camera, and a mug. For the table, we use a primitive rectangle shape with random size. For the shelf, we use three shelf shapes from ShapeNet [55]. The training dataset consists of 300k scenes. Detailed values for these hyperparameters used during experiments can be found in Appendix B.\nTo evaluate the estimation accuracy, we use the same five categories as the NOCS training set but use novel 119 shapes that do not overlap with the training set. We report two quantities: Average Precision (AP) and Chamfer Distance (CD) [2], where AP evaluates the object detection performance and CD evaluates the accuracy of the predicted shapes. For AP, we follow the evaluation procedure of [4]:"}, {"title": "7 Conclusion", "content": "In this work, we propose DEF-oriCORN for object-based scene representation and estimation tailored for language-directed manipulation tasks. Our novel representation oriCORN leverages a SO(3)-equivariant network and three decoders to encode oriented shapes into a neural representation, enabling efficient and robust manipulation planning in complex environments. We demonstrate that our method outperforms conventional occupancy-based implicit representations by efficiently grounding language commands to objects and predicting collision without the need for extensive surface point evaluation or mesh conversion. Our integrated estimation and representation framework, DEF-oriCORN, effectively handles the uncertainty inherent in sparse camera views and partial observability by integrating a diffusion model into the iterative network structure. This enhances the success rate of grasping and motion planning tasks. Future work will focus on refining the estimation process and exploring the potential of our framework in more diverse and dynamic scenarios. Additionally, we plan to explore the integration of task and motion planning algorithms to extend our framework's applicability to long-horizon tasks, enhancing its utility for more comprehensive planning scenarios."}, {"title": "A Planning with uncertainty", "content": "A.1 Algorithms\nOne strength of our approach is its ability to express uncertainty about estimation results using a diffusion estimator (Figure 9). Specifically, we employ a sampling-based approximation to approximate the distribution of object representations. To achieve this, given a large B, we take top-H samples of S from diffusion estimator(Algorithm 2) instead of only taking the top-1 sample. To account for the uncertainty represented in H samples of S, we apply minor modifications to standard tools such as RRT [5, 59] and grasping heuristic score [61-63] to incorporate uncertainty in the object representations.\nFor motion planning, we use RRT* [59] but check the validity of the path by contact test using H samples of S. This ensures that the planned motion accounts for the uncertainty in the object representations, leading to higher performance, as shown in the experiment (Section 6.1).\nFor grasping planning, given H number of object representation samples of the target object, we uniformly sample grasp candidates within a bounded range and evaluate them using an antipodal-based heuristic score [61-63]. This results in H grasp scores for each object sample, which are then averaged to obtain the final grasp score.\nTo utilize oriCORN for grasp planning, we randomly sample 6D end-effector poses near the object and select the best grasp pose using the grasping heuristic score [61-63], which is the function of two contact points and their contact normals. Given 37500 random samples near the object, we first filter out grasp poses that collide with the object using fcol. To compute the heuristic score [61-63] for"}, {"title": "B Implementation details", "content": "In our estimation models, including the proposed method, we set the number of diffusion timesteps, T, to 5 to achieve a balance between efficiency and accuracy. The decision for this configuration is backed by an ablation study detailed in Appendix E. For the refinement step in Algorithm 2 (line 6-8), the optimization process is performed using the Adam algorithm [65] with Autograd function in Jax [66], configured with a learning rate of 0.004. Typically, 100 iterations suffice for the refinement process.\nFor fenc, we use FER-VN-OccNet encoder [7] with a linear layer width of 64 and with the feature dimension of 3+5. For focc, we use FER-VN-OccNet decoder with a linear layer width of 32. For the fcol, we first apply pre-processing described in Section 4, and concatenate c and z after 2 layers of Multilayer Perceptron (MLP), followed by 3 layers and 128 widths of MLP to output collision results.\nFor fden, we use M = 32 for p, L = 2, number of head as 4, size of key-query-value in attention layers are 32. We apply AdaNorm [67] after each attention. For the head for c, p, z and Oconf, we use 2 layers and 128 widths of MLP. For the image encoder, we use U-Net [10] with 3 hierarchies and ray positional embedding [30]."}, {"title": "C Implementation of baselines", "content": "C.1 ShAPO\nWe trained ShAPO using the same estimation dataset as our method, as described in Section 6.1. Since our method is trained with multi-view RGB images, we included additional depth images for SHAPO's training. Each sample in our dataset comprises five different views, and we used all five views for training, resulting in a total of 1,500,000 samples for SHAPO. To address the simulation-to-reality gap, especially regarding depth image noise, we incorporated simulated depth noise according to SHAPO's official implementation. We used the SDF representation pre-trained with the NOCS dataset provided with SHAPO's official code.\nMotion planning with ShAPO employed the PyBullet simulator [53] for collision assessments, reconstructing meshes via marching cubes [20] and accelerating the contact detection with vhacd [68] for RRT* efficiency."}, {"title": "D Algorithm for point cloud sampling", "content": "In order to sample the surface point cloud using occupancy predictor focc, we utilize a hierarchical sampling technique similar to that described in ShAPO [4]. This technique includes the following steps. Initially, grid points within a predefined box at a coarse resolution are evaluated with the occupancy decoder. Points near surfaces, identified by a predefined occupancy value threshold, are gathered and then resampled at a finer resolution to refine the surface points. Finally, we project points onto the surface by estimating the surface normals with gradient descent."}, {"title": "E Ablation study on the number of timesteps in the diffusion process", "content": "A distinctive feature of diffusion models is their capacity to balance accuracy and efficiency by adjusting the number of time steps involved in the diffusion process, as highlighted by [52]. To examine this dynamic and contrast it with a regression model baseline (DEF-oriCORN-NonDM), we conducted an experiment to analyze estimation performance across varying numbers of recurrent iterations (or time steps in the diffusion process). The findings are illustrated in Figure 17.\nAn insight from our analysis is that an increase in recurrent iterations within the DEF-oriCORN-NonDM tends to deteriorate the estimation model's performance. This decline could be attributed to accumulating errors through recurrent updates, causing the model to drift from the training distribution as iterations advance.\nConversely, the proposed method, which introduces a diffusion model training scheme, exhibits improved performance with increased time steps. This improvement is likely due to the introduction of noise during recurrent processing in training, which enhances the model's resilience"}]}