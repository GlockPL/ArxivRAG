{"title": "CROSS-ATTENTION HEAD POSITION PATTERNS CAN ALIGN WITH HUMAN VISUAL CONCEPTS IN TEXT-TO- IMAGE GENERATIVE MODELS", "authors": ["Jungwon Park", "Jungmin Ko", "Dongnam Byun", "Jangwon Suh", "Wonjong Rhee"], "abstract": "Recent text-to-image diffusion models leverage cross-attention layers, which have been effectively utilized to enhance a range of visual generative tasks. However, our understanding of cross-attention layers remains somewhat limited. In this study, we present a method for constructing Head Relevance Vectors (HRVs) that align with useful visual concepts. An HRV for a given visual concept is a vector with a length equal to the total number of cross-attention heads, where each element represents the importance of the corresponding head for the given visual concept. We develop and employ an ordered weakening analysis to demonstrate the effectiveness of HRVs as interpretable features. To demonstrate the utility of HRVs, we propose concept strengthening and concept adjusting methods and apply them to enhance three visual generative tasks. We show that misinterpretations of polysemous words in image generation can be corrected in most cases, five challenging attributes in image editing can be successfully modified, and catastrophic neglect in multi-concept generation can be mitigated. Overall, our work provides an advancement in understanding cross-attention layers and introduces new approaches for fine-controlling these layers at the head level.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Text-to-Image (T2I) models have demonstrated an unprecedented ability to generate high-quality images with strong image-text alignment. These models often leverage powerful pre-trained text encoders; for instance, Stable Diffusion (Rombach et al., 2022) uses CLIP (Radford et al., 2021), while Imagen (Saharia et al., 2022) uses T5 (Raffel et al., 2020). Given that language is more expressive than previously used supervision signals (Gandelsman et al., 2023), text representations have empowered visual generative tasks, allowing for a high degree of control over the generation process. However, our understanding of the inner workings of T2I models remains limited, and even the latest models continue to struggle with certain failure cases.\nSignificant progress has been made in understanding the inner workings of deep neural networks. Olah et al. (2018) demonstrated numerous examples showing interpretable features at various levels in neural networks; Olah et al. (2020) expanded this analysis by exploring connections between units in the networks. Notably, Templeton et al. (2024) identified interpretable features in the cutting-edge large language model (LLM)-Claude 3 Sonnet-and used these features to guide the model's generation towards safer outcomes. Building on this line of work, we analyze T2I generative models with a focus on their cross-attention (CA) layers. We introduce a novel method to construct head relevance vectors (HRVs) that align with user-specified concepts. An HRV for a given visual concept is a vector whose length equals to the total number of CA heads, with each element representing the importance of the corresponding head for that concept. We demonstrate that these vectors reflect"}, {"title": "2 RELATED WORK", "content": "Early works on interpretable neurons and interpretable features: Early works on visual generative models have trained variational autoencoders (VAEs) using specifically designed loss functions on datasets with distinct attributes (Kulkarni et al., 2015; Higgins et al., 2017; Chen et al., 2018; Klys et al., 2018). While these methods successfully controlled a few attributes present in the train ing dataset, they were limited by a lack of fine-grained control, strong dependence on training data, and possible need for manual supervision of attributes. Another approach is to identify meaningful features in intermediate layers of neural networks (e.g., generative adversarial networks (GANs)), and modify those features to control the generation outputs (Plumerault et al., 2020; Shen & Zhou, 2021). Also, a seminal work by Olah et al. (2018) demonstrated that interpretable features in neural networks can be identified at various levels: single neurons, spatial positions, channels, or groups of neurons across different positions and channels. They presented numerous examples showing how these interpretable features emerge at various levels within a neural network\u2019s architecture. While these efforts successfully revealed meaningful features, they were limited by an inability to spec ify user-specified concepts. Moreover, the identified features were not always directly usable for controlling attributes in generative tasks.\nRecent works with multi-modal and generative models: Since the development of powerful multi-modal models that can map images and text to a joint embedding space, few studies have used text to interpret intermediate representations in vision models (Goh et al., 2021; Hernandez et al., 2022; Yuksekgonul et al., 2023b). Most recently, Gandelsman et al. (2023) examined self-attention heads from the last layers of CLIP-ViT (Dosovitskiy et al., 2021), identifying meaningful corre lations with several visual concepts. While these studies primarily focused on non-generative models, our study shifts focus to the interpretable features within visual generative models. Regarding the recently proposed large language model, Claude 3 Sonnet, Templeton et al. (2024) demonstrated that identifying meaningful concepts from model activations provides valuable insights into understand ing model behavior. Among the various features identified, their use of safety-related features to guide text generation towards safer outcomes is particularly relevant for large-scale text-generative models. Our study also explores large-scale generative models, showing that user-specified visual concepts can be captured and applied in three distinct visual generative tasks.\nText-to-image diffusion models with cross-attention layers: Building on the large-scale T2I diffusion models, researchers have developed methods to tackle various visual generative tasks, such as image editing (Hertz et al., 2022) and multi-concept generation (Chefer et al., 2023). Many of these studies utilize the publicly available Stable Diffusion (Rombach et al., 2022), which efficiently generates images through a diffusion denoising process in latent space. This model incorporates an autoencoder and a U-Net (Ronneberger et al., 2015) with multi-head cross-attention (CA) layers that integrate CLIP (Radford et al., 2021) text embeddings. Researchers have explored these CA layers to enhance control over text-conditioned image generation (Feng et al., 2022; Parmar et al., 2023; Chefer et al., 2023; Tumanyan et al., 2023; Wu et al., 2023). For example, P2P (Hertz et al., 2022) manipulates image layout and structure by swapping CA maps between source and target prompts. However, these methods typically update entire CA layers without offering fine-grained control over individual attention heads. We construct head relevance vectors, which align with a human visual concept, and uses these vectors to improve three visual generative tasks."}, {"title": "3 METHOD FOR CONSTRUCTING HEAD RELEVANCE VECTORS", "content": "The core idea involves selecting a set of visual concepts of interest, using a large language model (LLM) to pre-select 10 associated words for each concept, and utilizing random image gener ations for updating head relevance vectors (HRVs). The key to a successful update lies in identifying the visual concept that best matches a particular head and increasing the corresponding element of the HRVs. Figure 2 illustrates the process of a single update. We begin by providing background on the cross-attention (CA) layer, followed by detailed descriptions of our methodology for construct ing HRVs that correspond to a set of human-specified visual concepts.\nCross-attention in T2I diffusion models: Let P be a generation prompt, Zt be a noisy image at generation timestep t, and \\(\\psi(\\cdot)\\) be a CLIP text-encoder. In each CA head, the spatial features of the noisy image \\(\\phi(Z_t)\\) are projected to a query matrix \\(Q = l_Q(\\phi(Z_t))\\), while the CLIP text embedding \\(\\psi(P)\\) is projected to a key matrix \\(K = l_K(\\psi(P))\\) and a value matrix \\(V = l_V(\\psi(P))\\), using learned linear layers \\(l_Q\\), \\(l_K\\), and \\(l_V\\). Then, the CA map is calculated by measuring the correlations between Q and K as\n\\[M = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d}}\\right), \\qquad(1)\\]\nwhere d is the projection dimension of the keys and queries. The CA output MV is a weighted aver age of the value V, with the weights determined by the CA map M. This operation is performed in parallel across multi-heads in the CA layer, and their outputs are concatenated and linearly projected using a learned linear layer to produce the final CA output.\nImage generation prompts, visual concepts, and concept-words: To generate 2,100 random images, we used 2,100 generation prompts. Of these, 1,000 prompts were constructed using 1,000 ImageNet classes (Deng et al., 2009), formatted as \u2018A photo of a {class name}.\u2019 The remaining 1,100"}, {"title": "4 ORDERED WEAKENING ANALYSIS OF HEAD RELEVANCE VECTORS", "content": "In this section, we investigate whether the constructed HRVs can effectively and reliably serve as interpretable features. As the primary tool of analysis, we introduce an ordered weakening across the H cross-attention heads. Weakening of a target head is performed by multiplying -2 to the corresponding CA maps of the head. This weakening is applied exclusively to the CA maps of semantic tokens, leaving the CA maps of special tokens unaffected. Additionally, the weakening is applied consistently across all timesteps. Using this simple weakening approach, we compare images generated while following two different orderings of head weakening. In the most relevant head positions first (MoRHF), we weaken H heads starting from the strongest to the weakest, where the strength of head h is defined as the value of the h-th element in the HRV. In the least relevant head positions first (LeRHF), the order is reversed, from the weakest to the strongest. If the HRV of a visual concept is indeed effective and reliable, we expect MoRHF to impact the corresponding visual concept in the generated images more quickly than LeRHF. The head weakening was inspired by a rescaling technique in P2P (Hertz et al., 2022) and the ordering was inspired by the metrics defined in Samek et al. (2016) and Tomsett et al. (2020).\nAnalysis results for three visual concepts are shown in Figure 3. Comparison of generated images are shown in Figure 3a. In the top case, where the visual concept of Material is weakened, the characteristics of copper in the generated image already disappeared when the most relevant 11 heads are weakened. In contrast, the copper remains visible until the least relevant 71 heads are weakened. A similar observation can be made for the visual concepts of Animals and Geometric Patterns. An additional 33 examples can be found in Figures 11 and 13\u201315 of Appendix C. It is noted that a caution is required when analyzing the results, especially for LeRHF. For the HRV of a given visual concept, the least relevant heads have little effect on the concept of interest but could be significantly relevant to other visual concepts. Therefore, interpreting the changes observed in the LeRHF-generated images requires careful consideration of other visual concepts.\nWe have also plotted the trends of CLIP image-text similarity in Figure 3b. CLIP similarity was measured between the generated images and the concept-words used in the prompts. For each data point in the CLIP score plots, we used between 30 and 150 images, depending on the specific visual concept. The prompt templates and concept-words used for each visual concept are detailed in Appendix C.1. These plots clearly show that relevant concepts are removed significantly faster during MoRHF weakening, while they are preserved for a longer duration in LeRHF weakening. Additional similarity plots for six more visual concepts can be found in Figure 12 of Appendix C.2."}, {"title": "5 STEERING VISUAL CONCEPTS IN THREE VISUAL GENERATIVE TASKS", "content": "Head relevance vectors are not only valuable as interpretable features but can also be used to steer visual concepts in generative tasks. In this section, we demonstrate that polysemous word challenges can be addressed and that state-of-the-art methods, such as P2P (Hertz et al., 2022) for image editing and Attend-and-Excite (A&E) (Chefer et al., 2023) for multi-concept generation, can be enhanced. All of our experiments are conducted using Stable Diffusion v1.4, 50 timesteps with PNDM sam pling (Liu et al., 2022), and classifier-free guidance at a scale of 7.5. All CLIP-based metrics are calculated using the OpenCLIP ViT-H/14 model (Ilharco et al., 2021)."}, {"title": "5.1 IMAGE GENERATION \u2013 CORRECTING MISINTERPRETATION OF POLYSEMOUS WORDS", "content": "The same word can have different meanings de pending on the context. Stable Diffusion (SD) models are known for misinterpreting such pol ysemous words, often generating images that do not comply with the user\u2019s intended mean ing. Two examples are shown in Figure 5. SD fails to recognize that \u2018lavender\u2019 clearly refers to a color and \u2018Apple\u2019 to an electronic device. This misinterpretation by SD may arise from limitations in the CLIP text encoder, which has been reported to exhibit bag-of-words is sues (Yuksekgonul et al., 2023a). The problem can be resolved by adopting concept adjusting to SD as shown in Figure 5, where we name our method as SD-HRV. For the lavender case, SD-HRV resolves this issue by applying con cept adjusting to the token \u2018lavender,\u2019 using Color as the desired visual concept and Plants as the undesired visual concept. For the Apple case, concept adjusting is applied to the token \u2018Apple,\u2019 using Brand Logos as the desired and Fruits as the undesired. Examples of 10 cases, including the 2 cases shown in Figure 5, can be found in Figures 18\u201319 of Appendix D.1. There, we provide 10 images generated with 10 random seeds for each case. Additionally, we investigated the perfor mance of concept strengthening compared to concept adjusting and found, as expected, that concept strengthening performs worse. The details can be found in Appendix D.3.\nWe have also performed a human evaluation over the 10 cases, each with 10 random seeds, and the details can be found in Appendix D.2. Based on the human evaluation, the human perceived misinterpretation rate dropped significantly from 63.0% to 15.9% when SD-HRV was adopted."}, {"title": "5.2 IMAGE EDITING \u2013 SUCCESSFUL EDITING FOR FIVE CHALLENGING VISUAL CONCEPTS", "content": "Image editing involves generating an image that aligns with the target prompt while minimizing structural changes from the source image. Although recently developed methods excel at image editing, certain visual concepts remain challenging to edit. For example, concepts related to mate rials, geometric patterns, image styles, and weather conditions are known to be particularly difficult to edit. To address this problem, we propose applying concept strengthening to P2P (Hertz et al., 2022). We refer to our method as P2P-HRV. The key idea is to apply concept strengthening to the edited token, the token that describes how the attribute of the source image should be changed, thereby strengthening the concept related to the editing target. The detailed explanations of P2P HRV method are provided in Appendix E.1.\nExperimental settings: We focus on five challenging visual concepts as editing targets, includ ing three object attributes (Color, Material, and Geometric Patterns) and two image attributes (Im age Styles and Weather Conditions). We compare P2P-HRV with SDEdit (Meng et al., 2021), P2P (Hertz et al., 2022), PnP (Tumanyan et al., 2023), MasaCtrl (Cao et al., 2023), and FPE (Liu et al., 2024). For each method, we generate 500 edited images for each editing target (250 for Weather Conditions) using the prompts described in Appendix E.2. For object attributes (Color, Ma terial, and Geometric Patterns), we evaluated performance using both CLIP (Radford et al., 2021) and BG-DINO scores. The CLIP score evaluates CLIP image-text similarity between the edited image and the target prompt. The BG-DINO score assesses structure preservation using Grounded SAM-2 (Ravi et al., 2024; Ren et al., 2024) for extracting non-object parts from the source and edited images and then comparing them with DINOv2 (Oquab et al., 2023) embeddings. For im age attributes (Image Styles and Weather Conditions), we conducted a human evaluation to assess human preference (HP) scores. This was necessary because evaluating structure preservation using BG-DINO is not meaningful for Image Styles and Weather Conditions, which require editing across the entire image. In the human evaluation, we compared P2P-HRV with the other methods by asking \u2018Which edited image better matches the target description, while maintaining essential details of the source image?\u2019 The HP-score was normalized so that P2P-HRV\u2019s preference score was set to 100."}, {"title": "5.3 MULTI-CONCEPT GENERATION \u2013 REDUCING CATASTROPHIC NEGLECT", "content": "T2I generative models often struggle with multi-concept generation, failing to capture all the spec ified visual concepts in a prompt. Attend-and-Excite (A&E) (Chefer et al., 2023) tackles this issue, known as catastrophic neglect, by gradient updating the CA maps of selected tokens during image generation. To improve A&E further, we propose applying our concept strengthening to A&E. We refer to this enhancement method as A&E-HRV.\nExperimental settings: We investigated A&E-HRV using two types of prompts: (i) \u2018a {Animal A} and a {Animal B}\u2019 (Type 1) and (ii) \u2018a {Color A} {Animal A} and a {Color B} {Animal B}\u2019 (Type 2), originally examined in the A&E work (Chefer et al., 2023). Type 1 evaluates multi-object gen eration, while Type 2 adds the challenge of binding color attributes to each animal. In these ex periments, we used 12 animals and 10 colors, as detailed in Appendix F. We assessed 66 prompts for Type 1 and 150 for Type 2, with 30 random seeds applied across all methods. Following A&E, we adopted three evaluation metrics. Full prompt similarity measures the CLIP similarity between the generated image and the full prompt. Minimum object similarity is calculated by measuring the CLIP image-text similarity between the image and two sub-prompts, which are created by split ting the original prompt at \u2018and.\u2019 The lower similarity score between the two sub-prompts is then reported. BLIP-score measures the CLIP text-text similarity between the prompt and the image caption generated with BLIP-2 (Li et al., 2023).\nExperimental results: Table 2 presents the quantitative comparisons, while Figure 8 shows qual itative results. As shown in Table 2, A&E-HRV outperforms other methods across all metrics and prompt types. In Figure 8, the top row shows results for Type 1 prompt, while the bottom row shows results for Type 2 prompt. The existing methods either neglect key visual concepts or fail to generate realistic images. In contrast, our approach captures all concepts and generates realistic images for both prompt types. Additional comparisons are provided in Figure 36 of Appendix F.2."}, {"title": "6 DISCUSSION", "content": "6.1 EXTENSIONS TO A LARGER ARCHITECTURE\nThe recently introduced Stable Diffusion XL (SDXL) (Podell et al., 2023) adopts a U-Net backbone that is three times larger than its predecessors, scaling up to 1300 cross-attention (CA) heads. To investigate the generalization capability of HRV, we have performed an extended study with SDXL. Using SDXL, we conducted the ordered weakening analysis to evaluate whether HRVs can serve as interpretable features. An exemplary result for the visual concept Furniture is shown in Figure 9. The sofa disappeared when the most relevant 211 heads were weakened. In contrast, the sofa re mained visible until the least relevant 711 heads were weakened. Additional 45 examples and the similarity plots for nine visual concepts can be found in Figures 37\u201342 of Appendix G.1. For SDXL experiments, we utilize the SD-XL 1.0-base model.\n6.2 DO GENERATION TIMESTEPS AFFECT HOW HEADS RELATE TO VISUAL CONCEPT?\nDiffusion models generate images by iteratively process ing an image latent through the same U-Net network. A natural question is whether the patterns of head rel evance vectors change across different timesteps during generation. To explore this, we calculated head relevance vectors for each visual concept at every timestep, result ing in 1700 vectors (34 visual concepts\u00d750 timesteps). Figure 10 presents a t-SNE (Van der Maaten & Hinton, 2008) plot of these 1700 vectors. In the t-SNE plot, vi sual concepts are clearly separated, while timesteps are not. This indicates that generation timesteps do not sig nificantly alter the patterns of head relevance vector of each visual concept. Further analysis, including cosine similarity plots, is provided in Appendix I.\n6.3 LIMITATION\nWe examined the 34 concepts listed in Table 3 of Appendix A and identified two types of failure cases. The first type arises from limitations in the underlying T2I model, while the second is related to our proposed algorithm or the concept-words used to represent the concept. Appendix H provides a detailed discussion of these failure cases, along with examples for each, shown in Figures 45-46."}, {"title": "7 CONCLUSION", "content": "In this work, we present findings from our exploration of cross-attention layers in T2I models. We demonstrate that head relevance vectors (HRVs) can be effectively and reliably constructed for human-specified visual concepts without requiring any modifications or fine-tuning of the T2I model. Furthermore, we show that HRVs can be successfully applied to improve performance of three visual generative tasks. Our work provides an advancement in understanding cross-attention layers and introduces novel approaches for exploiting these layers."}, {"title": "8 REPRODUCIBILITY STATEMENT", "content": "We provide our core codebase in our code repository, which includes the methodology implementa tion, settings, generation prompts, and benchmarks for image editing and multi-concept generation."}, {"title": "9 ETHIC STATEMENT", "content": "Our work presents new techniques for controlling and refining text-to-image diffusion models, en hancing both image generation and editing capabilities. While these advancements hold significant potential for creative and practical applications, they also raise ethical concerns about the possible misuse of generative models, such as creating manipulated media for disinformation. It is important to recognize that any image editing or generation tool can be used for both positive and negative pur poses, making responsible use essential. Fortunately, various research in detecting harmful content and preventing malicious editing are making significant progress. We believe our detailed analysis of cross-attention layers will contribute to these efforts by providing a deeper understanding of the mechanisms behind image generation and editing."}, {"title": "B DETAILS OF HRV CONSTRUCTION", "content": "B.1 PSEUDO-CODE FOR HRV CONSTRUCTION\nAlgorithm 1 HRV construction\nRequire: N: Number of human-specified visual concepts\nRequire: T: Total number of generation timesteps\nRequire: H: Total number of CA heads\nRequire: P: Set of prompts for random image generation\nRequire: S: Set of concept-words covering N visual concepts\nRequire: \\(\\psi\\): CLIP text-encoder\nRequire: \\(l^{(h)}_K\\): Key projection layer for the h-th cross-attention (CA) head\nRequire: \\(\\xi\\): Function to extract for semantic token embeddings\nRequire: \\(Q^{(t,h)}\\): Image query matrix at timestep t and the h-th CA head\n1: Initialize HRV matrix V as a zero matrix \\(0 \\in \\mathbb{R}^{N\\times H}\\)\n2: for each prompt P \\(\\in\\) P do\n3: while generating a random image with prompt P do\n4: for all t = 1, 2, . . . , T do\n5: for all h = 1, 2, . . . , H do\n6: for all n = 1, 2, . . . , N do\n7: Sample a concept-word Wn for visual concept Cn from S\n8: Compute key-projected embedding of Wn: \\(K_n = l^{(h)}_K(\\psi(W_n)) \\in \\mathbb{R}^{77\\times F}\\)\n9: Extract semantic token embeddings: \\(\\overline{K}_n = \\xi(K_n)\\)\n10: end for\n11: Concatenate \\(\\overline{K}_1, \\overline{K}_2, ..., \\overline{K}_N\\) along the token dimension:\n\\[\\overline{K} = [\\overline{K}_1, \\overline{K}_2, ..., \\overline{K}_N] \\in \\mathbb{R}^{N'\\times F}\\qquad(2)\\]\n12: Calculate the CA map \\(M\\) using \\(\\overline{K} = \\overline{K}\\) and \\(Q = Q^{(t,h)} \\in \\mathbb{R}^{R^2\\times F}\\):\n\\[M = \\text{softmax} \\left(\\frac{Q^{(t,h)} \\cdot \\overline{K}^T}{\\sqrt{d}}\\right) \\in \\mathbb{R}^{R^2\\times N'} \\qquad(3)\\]\n13: Average \\(M\\) along the token dimension for multi-token concept-words, resulting in a matrix of shape \\(\\mathbb{R}^{R^2\\times N}\\)\n14: Average the resulting matrix over the spatial dimension (\\(R^2\\)), producing \\(M \\in \\mathbb{R}^{N}\\)\n15: Apply an argmax operation over the token dimension (N):\n\\[\\overline{M} \\leftarrow \\text{argmax}(M)\\qquad(4)\\]\n16: Update the h-th column of the HRV matrix V by adding \\(\\overline{M}\\):\n\\[V[:, h] \\leftarrow V[:, h] + \\overline{M}\\qquad(5)\\]\n17: end for\n18: end for\n19: end while\n20: end for\n21: Return: HRV matrix \\(V \\in \\mathbb{R}^{N\\times H}\\)"}, {"title": "4.2 ROLE OF THE ARGMAX OPERATION IN HRV CONSTRUCTION", "content": "During HRV construction, we apply the argmax operation to the averaged CA maps before using them to update the HRV matrix (see Eq. 4). This step addresses the varying representation scales across H CA heads. To demonstrate these scale differences, we compute the averaged L1-norm of the CA maps before applying the softmax operation (refer to Eq. 3 for notations):\n\\[\\mathcal{L}^{(t,h)} = \\frac{1}{R^2 \\cdot N'} \\cdot \\sum_{R^2, N'} \\left\\| \\frac{Q^{(t,h)} \\cdot K^T}{\\sqrt{d}}\\right\\|_1\\qquad(6)\\]\nIn Table 4, we show the mean and standard deviation of \\(\\mathcal{L}^{(t,h)}\\) across 2100 generation prompts and 50 timesteps for each CA head in Stable Diffusion v1.4. The CA heads exhibit variation in their representation scales, with the head having the largest scale showing a mean value 8.1 times higher than that of the smallest scale. Since the softmax operation maps large-scale values closer to a Dirac-delta distribution and small-scale values closer to a uniform distribution, it is necessary to align the scales between CA heads before accumulating the information into the HRV matrix. We achieve this by simply applying the argmax operation, as shown in Eq. 4, which resolves the issue of differing representation scales across the CA heads.\nAs explained in the previous paragraph, the softmax operation introduces an imbalance when sum ming without the argmax operation, where CA heads with larger scales produce vectors closer to a Dirac-delta distribution, while those with smaller scales produce vectors closer to a uniform distribu tion. This imbalance favors CA heads with larger representation scales, leading to an overemphasis on the largest concept chosen by these larger-scale heads compared to the largest concept chosen by smaller-scale CA heads. For example, as shown in Table 4, the largest concept chosen by the CA head at [Layer 15-Head 4] would be overemphasized compared to the largest concept chosen by the CA head at [Layer 16-Head 1]. A similar issue arises when using the max operation instead of argmax, as the maximum values from larger-scale CA heads are much larger than those from smaller-scale CA heads. To address this, we apply the argmax operation before summation, ensur ing that the largest concept from each CA head contributes a value of 1 to the HRV matrix, regardless of its scale. This straightforward approach eliminates the bias toward larger-scale CA heads."}, {"title": "C DETAILS AND ADDITIONAL RESULTS ON ORDERED WEAKENING ANALYSIS", "content": "In this section, we present the generation prompts used for the ordered weakening analysis intro duced in Section 4. Additionally, we provide MoRHF and LeRHF plots for 6 more visual concepts, along with more detailed qualitative results."}, {"title": "C.1 PROMPTS USED FOR ORDERED WEAKENING ANALYSIS", "content": "We conducted the ordered weakening analysis across 9 visual concepts: Animals, Color, Fruits and Vegetables, Furniture, Geometric Patterns, Image Style, Material, Nature Scenes, and Weather conditions. The prompt templates and words for each concept are listed in Table 5. For each prompt, we use 3 random seeds to generate images. For instance, the concept Color used the prompt template \u201ca {Color} {Objects A}\u201d with 3 random seeds, covering 10 colors and 5 objects. This results in 150 generated images per data point in the line plot shown in Figure 3b of the manuscript."}, {"title": "C.3 COMPARISON WITH RANDOM ORDER WEAKENING", "content": "As an additional analysis, we compare HRV-based ordered weakening with random weakening. To facilitate this comparison, we calculate the area between the LeRHF and MoRHF line plots. A larger area indicates that the ordering of CA heads aligns more closely with the relevance of the corresponding concept. Table 6 presents a comparison of HRV-based ordered weakening and three random weakening approaches across six visual concepts. The results show that HRV-based ordered weakening achieves a higher (LeRHF \u2212 MoRHF) area, demonstrating its effectiveness in ordering heads based on their relevance to the given concept."}, {"title": "C.4 ORDERED RESCALING WITH VARIED RESCALING FACTORS", "content": "In the ordered weakening analysis, we selected \u22122 as the rescaling factor. This choice is inspired by P2P-rescaling (Hertz et al., 2022), which uses factors in the range of [\u22122, 2] to adjust the CA maps of the U-Net for image editing. To explore the impact of different rescaling factors, we present two examples in Figures 16-17. A rescaling factor of 1 leaves the original image generation process unchanged, while factors greater than 1 strengthen the concept and factors smaller than 1 weaken it. Strengthening produces minimal changes, likely because the concept is already present in the image. Weakening works effectively with factors below 0, with stronger effects observed as the factor decreases further."}, {"title": "D DETAILS ON REDUCING MISINTERPRETATION OF POLYSEMOUS WORDS", "content": "D.1 PROMPTS AND SELECTED CONCEPTS FOR REDUCING MISINTERPRETATION\nWe identified 10 prompts that the text-to-image (T2I) generative model frequently misinterprets and carefully selected desired and undesired concepts from our 34 visual concepts to help reduce these misinterpretations. Table 7 lists these 10 prompts, along with the desired and undesired concepts for each polysemous word. For both Stable Diffusion (SD) and SD-HRV, we generated 100 images us ing these 10 prompts with 10 random seeds. The full set of generated images is shown in Figures 18 and 19. We categorized the misinterpretation into three types: (i) containing the undesired meaning, (ii) missing the desired meaning, and (iii) both, and mark the images showing any of these misin terpretations. For the last prompt, \u2018A single rusted nut,\u2019 where \u2018nut\u2019 was misinterpreted as Food and Beverages instead of Tools, SD-HRV only partially resolved the issue by removing \u2018nut\u2019 as Food and Beverages but failed to generate it as Tools. This suggests that SD-HRV is not perfect, and there is still room for improvement in addressing such misinterpretations. Our current implementation for SD-HRV requires manual settings for the target token, as well as the desired and undesired concepts. However, our tests with an LLM show that it effectively identifies the inputs needed for SD-HRV, suggesting that constructing an automatic pipeline using LLMs is feasible."}, {"title": "2 HUMAN EVALUATION", "content": "We evaluate the human perceived misinterpretation rate using Amazon Mechanical Turk (AMT), requiring participants to have over 500 HIT approvals, an approval rate above 98%, and live in the US. The survey begins with a sample question accompanied by its correct answer, which is repeated at the end without the answer. Participants who missed the sample question are excluded, leaving 36 valid responses. The misinterpretation rate measures how often polysemous words are misinterpreted in the generated images. We use 10 prompts from Table 7 and 10 random seeds to generate 100 images for each T2I model. This results in 200 total images for comparison between Stable Diffusion (SD) and SD-HRV (Ours). These images are organized into 10 problem sets, each containing 20 images generated with the same prompt. Each problem set consists of 4 questions, with each question presenting 5 images generated using the same T2I model but with different random seeds. Each participant receives 3 randomly selected problem sets, containing 12 questions and 60 images. Details of the human evaluation setup are summarized in Table 8. For each question, participants are shown 5 images and asked to count how many depict the intended meaning of the polysemous word without including the unintended meaning: \u201cCount how many of the following five images contain {intended meaning of the polysemous word} but no {unintended meaning of the polysemous word}.\u201d This count is then subtracted from 5 to determine the count of images with misinterpretations. After applying concept adjusting with our head relevance vectors on Stable Diffusion, the misinterpretation rate drops from 63.0% to 15.9%."}, {"title": "3 COMPARISON OF CONCEPT STRENGTHENING AND CONCEPT ADJUSTING", "content": "We can also apply concept strengthening, instead of concept adjusting, on Stable Diffusion to reduce misinterpretations. While this approach resolves misinterpretations in"}]}