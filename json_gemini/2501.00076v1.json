{"title": "A Novel Framework for Learning Stochastic Representations for Sequence Generation and Recognition", "authors": ["Jungsik Hwang", "Ahmadreza Ahmadi"], "abstract": "The ability to generate and recognize sequential data is fundamental for autonomous systems operating in dynamic environments. Inspired by the key principles of the brain-predictive coding and the Bayesian brain-we propose a novel stochastic Recurrent Neural Network with Parametric Biases (RNNPB). The proposed model incorporates stochasticity into the latent space using the reparameterization trick used in variational autoencoders. This approach enables the model to learn probabilistic representations of multidimensional sequences, capturing uncertainty and enhancing robustness against overfitting. We tested the proposed model on a robotic motion dataset to assess its performance in generating and recognizing temporal patterns. The experimental results showed that the stochastic RNNPB model outperformed its deterministic counterpart in generating and recognizing motion sequences. The results highlighted the proposed model's capability to quantify and adjust uncertainty during both learning and inference. The stochasticity resulted in a continuous latent space representation, facilitating stable motion generation and enhanced generalization when recognizing novel sequences. Our approach provides a biologically inspired framework for modeling temporal patterns and advances the development of robust and adaptable systems in artificial intelligence and robotics.", "sections": [{"title": "I. INTRODUCTION", "content": "Predictive coding [1], [2] and the Bayesian brain hypothesis [3], [4] offer a framework for understanding how the brain processes sensory information. One of the key ideas in predictive coding is that the brain continuously generates predictions about incoming sensory stimuli and minimizes prediction errors by updating beliefs [1], [2], [5]. The Bayesian brain suggests that the brain operates as a probabilistic inference machine, constantly updating its beliefs about the world through Bayesian inference. It posits that the brain combines prior knowledge with new sensory evidence to form posterior beliefs. This approach enables the brain to process information efficiently, handle uncertainty, and adapt to new situations by updating its internal representations [3], [4].\nBy modeling the cognitive processes of the brain, artificial agents can improve their sensory perception, motor control, and adaptability [6], [7], [8], [9]. Recently, several deep learning models based on predictive coding and the Bayesian brain have been proposed in the fields of artificial intelligence and cognitive robotics, such as PredNet [10], Predictive Coding Network (PCN) [11], Predictive Coding Visuo-Motor Dynamic Neural Network (P-VMDNN) [12], Predictive- Coding inspired Variational RNN (PV-RNN) [13], and Predictive Coding Recurrent Neural Network (PC-RNN) [14]. These models have been used for various tasks in machine learning and robotics.\nIn line with these studies, we propose a stochastic neural network for modeling multidimensional sequences. The proposed model is based on predictive coding and the Bayesian brain hypothesis and it builds upon two previous models: the Recurrent Neural Network with Parametric Biases (RNNPB) [15] and the Variational Autoencoder (VAE) [16]. The RNNPB model and its variants have been widely used in cognitive robotics for modeling sequential data, leveraging the ability to learn representations in the parametric bias (PB). The RNNPB model uses the same network for both the generation and recognition of actions by sharing internal neural representations. This makes the RNNPB model ideal for tasks requiring both the generation and recognition of sequential data [17], [18].\nIn this study, we introduce stochasticity into the PB by adopting the reparameterization trick used in VAEs [16]. Previous studies [3], [4] emphasize the importance of probabilistic approaches in understanding neural coding and brain function. In addition, many studies have shown that introducing stochasticity can have practical benefits, such as improved generalization to unseen data and quantifying uncertainty for tasks with noisy data [19], [20], [21], [22]. By introducing stochasticity into the PB, we aim to enhance the model in these aspects, including capturing the underlying distributions of the data, reducing the risk of overfitting, and handling uncertainty in sequence generation and recognition tasks.\nWe validate the performance of our model on a robotic motion dataset in the context of generating robot motions and recognizing the robot's own motions. These capabilities are essential for social and collaborative robots that interact physically with humans [23]. During training, the proposed model is optimized for generative modeling of motion sequences by learning its parameters, including the PB. After"}, {"title": "II. RELATED WORKS", "content": ""}, {"title": "A. Neural Network Models", "content": "The proposed model builds upon two artificial neural network models: the Recurrent Neural Network with Parametric Bias (RNNPB) and the Variational Autoencoder (VAE). By integrating key characteristics from both models, we aim to enhance their capabilities and address their limitations.\nThe RNNPB model [15] is designed to learn and represent temporal patterns by encoding them into the parametric bias (PB). The PB captures the underlying structure and key features of the sequences, enabling the model to perform both generation and recognition of sequences. An interesting aspect of RNNPB is that similar regions in the latent space of the PB are activated during both the execution and observation of similar actions. This property allows the model to tightly intertwine generation and recognition as in the brain [17]. RNNPB and its variations have been widely used in cognitive and developmental robotics. In [15] and [17], RNNPB was used to imitate and generate human-like actions by learning combinatorial action sequences. More recently, Hwang and Tani [24] investigated the generation of creative robot motions using RNNPB. The results showed that employing a different learning method resulted in a different landscape of the PB and varied levels of creativity. The study highlighted that the latent space of PB played a crucial role in creating diverse and novel motions.\nWhile these deterministic RNNPB models can generate and recognize sequences based on learned PB values, they operate on specific point estimates rather than data distributions. As a result, they cannot model the uncertainty inherent in data. This contrasts with other generative models like VAEs which aim to capture the data distribution. Moreover, it has been known that deterministic models are generally more prone to overfitting than stochastic models [19]. In short, the deterministic nature of RNNPB can limit its flexibility and ability to capture the full variability of the data.\nThe VAE introduced in [16] is a generative model that learns probabilistic latent representations of data. A VAE consists of an encoder, which maps input data to a stochastic latent space, and a decoder, which generates data from these latent variables. By jointly training the encoder and decoder to reconstruct the input data, the model learns meaningful stochastic latent representations for generation.\nVAEs have been widely used in various applications, such as computer vision [25], natural language processing [26], and sequence modeling [27], [28]. Several studies have investigated the development of the latent space in VAEs. In [29], the authors introduced B-VAE, a variation of the VAE that promotes disentangled latent representations. By introducing a weighting factor \u1e9e in the VAE objective, the model is encouraged to learn independent factors of variation in the data. Similarly, Burgess et al. [30] showed that the weighting term in the loss function of \u1e9e-VAE plays an essential role in shaping the latent space of the models.\nVAEs, in general, are used for generative tasks rather than recognition (posterior estimation) of novel data. In VAEs, posterior estimation is implemented as a feedforward computation in the encoder network using the observation as input. This process differs from a fundamental principle in the brain, as it does not involve an interplay between feedforward and feedback processes for minimizing prediction error [1], [2]. Moreover, the feedforward computation of posterior estimation contrasts with the Bayesian brain perspective which considers perception as an optimization that combines sensory input with prior expectations [5]. In short, while VAEs provide a computational framework for representing internal beliefs as probability distributions, they lack iterative inference mechanisms involving feedback, making them different from brain-inspired models."}, {"title": "B. Theoretical Frameworks", "content": "Predictive coding [1], [2] suggests that the brain continuously generates predictions about incoming sensory inputs and updates its internal models based on prediction errors the differences between predicted and actual inputs. This iterative process minimizes surprise by refining predictions over time. The Bayesian brain posits that the brain performs probabilistic inference, representing information in terms of probability distributions and updating beliefs according to Bayesian principles [3], [4]. Under this hypothesis, the brain combines prior knowledge with sensory evidence to form posterior beliefs, enabling optimal decision-making under uncertainty."}, {"title": "III. PROPOSED NEURAL NETWORK MODEL", "content": "In this section, we present the stochastic RNNPB model, which is designed to learn stochastic representations of multidimensional sequences."}, {"title": "A. Model Architecture", "content": "Fig. 1 illustrates the architecture of the proposed model, which consists of the stochastic PB layer, the input layer, the LSTM layer, and the output layer. The key innovation of the proposed model is the introduction of stochasticity in the PB. In contrast to the deterministic PB of previous studies [15], [17], our approach employs variational inference to learn a probability distribution over the PB. Specifically, the model learns the parameters of a Gaussian distribution (u and \u03c3) for each sequence in the training data. This is achieved by applying the reparameterization trick (1) to the PB, similar to the approach in VAE [16]. It allows gradients to flow during backpropagation, enabling the model to learn the distribution parameters effectively.\n$PB^{(i)} = \\mu^{(i)} + \\sigma^{(i)} \\odot \\epsilon  \\quad \\mu \\in \\mathbb{R}^{N \\times D_{PB}} $\nwhere $ \\epsilon \\sim N(0,1)$ (1)"}, {"title": "B. Training the Model", "content": "The training process consists of iterations of the following steps: sampling PB from N(\u03bc, \u03c3\u00b2), generating the output from the PB, computing the loss, and learning the parameters. In particular, the learning process can be understood within the Bayesian inference framework. During training, we aim to learn both the model parameters 0 (weights of the neural network) and the PB parameters (\u03bc and \u03c3). This process involves maximizing the Evidence Lower Bound (ELBO) [16], [34]. Specifically, for each sequence x in the training dataset, the goal is to infer the posterior distribution over the PB. Since the true posterior p(PB | x) is generally intractable, we use a variational distribution q(PB) to approximate it. We parameterize q(PB) as a Gaussian distribution with \u03bc and \u03c3. Then, our objective is to find the optimal q(PB) and model parameters @ by maximizing the ELBO (2), which indirectly minimizes the Kullback-Leibler (KL) divergence between q(PB) and the true posterior.\n$ELBO(\\theta, \\mu, \\sigma) = E_{q(PB)}[logp(x | PB; \\theta )] - KL(q(PB) || p(PB))$(2)\nThe first term of the ELBO is the expected log-likelihood of the data under the model and the approximate posterior. The second term is the KL divergence between the approximate posterior and the prior over PB. Maximizing the ELBO corresponds to finding model parameters (0) and PB parameters (u and \u03c3) that best explain the training data while regularizing the complexity of the latent space. In practice, we minimize the negative ELBO, which corresponds to the loss function used during training (3).\n$L_{total}(\\theta, \\mu, \\sigma) = L_{recon} + \\beta \\times L_{KL}$(3)\nThe loss function comprises a reconstruction term ($L_{recon}$) and a regularization term ($L_{KL}$). The reconstruction term corresponds to the negative expected log-likelihood, and it ensures that the model can accurately generate data from the PB and the model parameters. Under the Gaussian assumption, the reconstruction loss reduces to the Mean Squared Error (MSE) between the target and predicted data (4).\n$L_{recon} = \\frac{1}{2T} \\sum_{i=1}^N \\sum_{t=1}^{T_i} ||x_t^{(i)} - \\hat{x}_t^{(i)}||^2$(4)\nwhere Ti is the length of sequence i, $x_t^{(i)}$ and $\\hat{x}_t^{(i)}$ are the target and predicted data at time t respectively.\nThe regularization term involves the KL divergence, which measures how well the variational distribution matches the prior distribution over PB. Minimizing this term ensures that"}, {"title": "C. Generating Sequences from Stochastic Parametric Biases", "content": "The proposed model generates a sequence in an autoregressive manner, often referred to as closed-loop generation in previous studies [13], [17], [31]. At the onset of generation (t=0), PB is sampled, and the input to the model, as well as the initial states of the LSTM, are zeroed to mitigate the effect of different initial input values on the generation of different motions (i.e., x(t) = h(i) = c(i) = 0). Consequently, only the PB values determine the type of sequences to be generated. Then, at each time step t\u2265 1, the sampled PB and the model's output from the previous time step t-1 are fed into the LSTM layer (6). The output of the model is computed for each time step as in (7-8).\n$z_t^{(i)} = [PB^{(i)}; \\hat{x}_{t-1}^{(i)}]$(6)\n$h_t^{(i)}, c_t^{(i)} = LSTM(z_t^{(i)}, h_{t-1}^{(i)}, c_{t-1}^{(i)})$(7)\n$\\hat{x}_t^{(i)} = W_{out}h_t^{(i)} + b_{out}$(8)\nwhere Wout and bout are weights and biases in the output layer. Note that PB is sampled only at the initial time step rather than at every subsequent time step. This is to capture the sequence-level latent variables that represent the underlying structure of the sequence. Sampling the PB once ensures that the stochasticity remains consistent throughout the sequence, providing a stable and coherent influence on the model's dynamics and facilitating faster convergence."}, {"title": "D. Recognizing Sequences via Prediction Error Minimization", "content": "The proposed model is able to recognize a sequence through prediction error minimization (PEM), which is one of the core ideas in predictive coding [1], [2]. Here, the term 'recognition' refers to posterior estimation during which the model updates the Gaussian parameters of the PBs for the given target sequence (observation). In standard VAE, recognition is implemented as a feedforward computation of the encoder network, which generates latent representations by using the"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Robotic Motion Dataset", "content": "In our experiment, we used the REBL (Robotic Emotional Body Language)-Pepper dataset [36]. It includes a collection of 36 hand-designed animations for the Pepper robot. These animations were crafted to express emotions through the robot's body gestures, eye LED patterns, and non-linguistic sounds. We used the augmented version included in the dataset, which was generated by mirroring each posture. As a result, a total number of 72 motion sequences were used in our experiment. Each motion consists of 17 joint angles expressed in radians. The joints are HeadPitch, HeadYaw, HipPitch, HipRoll, KneePitch, LElbowRoll, LElbowYaw, LHand, LShoulderPitch, LShoulderRoll, LWrist Yaw, RElbowRoll, RElbowYaw, RHand, RShoulderPitch, RShoulderRoll, and RWristYaw."}, {"title": "B. Model Configurations", "content": "The proposed model consists of a stochastic PB layer with four neurons (DPB=4), followed by an LSTM layer containing 256 hidden units. The LSTM layer is succeeded by a linear layer that produces the output joint angles. The input and output layers have 17 neurons, each representing the robot's joint angles in radians.\nIn the proposed model, the \u1e9e parameter plays a crucial role by weighting the KLD term in the loss function. To explore the impact of \u1e9e on the model's performance, we examined the model with different \u1e9e settings, as well as the model with fully deterministic dynamics (i.e., $PB^{(i)} = \\mu^{(i)}$). A higher and lower value of \u1e9e are referred to as a strong and weak prior respectively, as the beta term indicates the amount of influence of the prior on the model training. Consequently, we analyzed the model's performance in the following four conditions:\nStochastic model with Strong Prior (\u03b2 = 1e-3)\nStochastic model with Weak Prior (\u03b2 = 1e-6)\nStochastic model with Zero Prior (\u03b2 = 0)\nDeterministic model\nRegarding the choice of \u00df, we observed that models with \u03b2 greater than le-3 were unable to learn the training data accurately. The model was trained for 50,000 epochs using the Adam optimizer [37] with a learning rate of 0.001. Based on the unit Gaussian prior assumption, \u03bc and \u03c3 of the stochastic model were initialized to 0 and 1 respectively, during training."}, {"title": "C. Tasks", "content": "We evaluated our model on two tasks: reconstruction and recognition. In the reconstruction task, the model generates motion sequences autoregressively from the learned PB values. The training data comprises 72 sequences, resulting in 72 pairs of learned u and o values. For each pair (i.e., each motion sequence), we sampled 100 PB values. As a result, a total number of 7,200 sequences were generated for analysis. This extensive sampling allows us to assess the model's ability to reconstruct the motions with the learned PB distributions.\nIn the recognition task, we presented ten novel patterns to the model. The goal of the recognition task is to accurately reconstruct observation by updating the PB (i.e., recognition by reconstruction [8]). The novel patterns were generated by adding noise, scaling, and shifting to the principal components of the training data (see the code for details). Since the novel patterns were also the robot's joint angles, the recognition task can be seen as recognizing the robot's own action during kinesthetic teaching or learning from demonstration [23]. For each novel pattern, we conducted ten trials to assess the robustness of the recognition performance. In each trial, the optimization of \u03bc and was carried out for 100 iterations using the Adam optimizer [37] with a learning rate of 0.1. The initialization of \u03bc and a significantly impacts the convergence of the optimization process. We tested three initialization methods:\nBaseline (u = 0, \u03c3 = I)\nLearned (u = one of the learned \u03bc values, \u03c3 = I)\nRandom (u one of 10 random u values, \u03c3 = I)\nIn the baseline condition, we initialized the \u00b5 and o to 0 and 1 respectively (i.e., unit Gaussian assumption). In the learned and random conditions, the model performed a pre-search phase in which it generated a set of outputs using different \u03bc values. The \u00b5 values that resulted in the least reconstruction error (i.e., the output most similar to the observation) were chosen for the initialization. This technique is referred to as a \"warm start\", and it has been shown to improve optimization in previous studies [38], [39]. Note that \u03c3 was set to a very small positive value during the pre-search phase to minimize the effect of stochasticity in sampling PB. During the recognition phase, \u03c3 was initialized to one in all conditions, representing uncertainty at the beginning of recognition."}, {"title": "V. RESULTS", "content": ""}, {"title": "A. Learning Stochastic Motion Representations", "content": "Fig. 2 depicts the probability density functions (PDFs) of PBs for three training sequences. As \u1e9e decreases, the PDFs become more spiky. This indicates that the model learns to assign lower variances to each sequence due to the lower regularization loss. In addition, the figure shows that the variance of the PB varies across sequences, demonstrating the model's capability to capture varying levels of uncertainty depending on the sequence."}, {"title": "B. Generation of Sequences from the Stochastic PB", "content": "In the reconstruction task, we reconstruct the training sequences by sampling PB from the learned \u03bc and o values. Table I summarizes the reconstruction loss for each model configuration, which indicates the average discrepancies between training and reconstructed sequences."}, {"title": "C. Recognition of Novel Sequences through Posterior Estimation of Stochastic PB", "content": "We examined the model's recognition capability by presenting novel sequences (observations) and measuring how accurately the model could reconstruct these sequences by estimating the posterior. The recognition performance was measured in two terms: reconstruction loss and prediction error. The reconstruction loss refers to the discrepancy between the model output and the observed portion of the sequence (the first 80% of the sequence). This loss was used to update the PB values as described in Section III.D, and it indicates how well the model reconstructed the observation. The prediction error refers to the discrepancy between the model output and the remaining unobserved portion of the sequence. In other words, it indicates the model's ability to forecast or complete the motion sequence (i.e., to predict the latter 20% when given the first 80%). See Fig. 5 for an illustration of how the model performs recognition.\nTable II presents the mean and standard deviation of the reconstruction loss and prediction error across different model configurations. The results demonstrated that the stochastic model outperformed the deterministic model in recognizing novel sequences. In particular, the stochastic models performed much better than the deterministic model in the baseline condition. A similar trend was also observed in prediction error, indicating better reconstruction performance led to improved forecast accuracy. In other words, by better capturing the first 80% of the sequences, the model could forecast the remaining 20% more accurately.\nThe results also showed that a warm start improved recognition performance. When the model was initialized with learned u values, both stochastic and deterministic models generally performed better than in the baseline conditions. Random search initialization also helped, but not as much as initialization with the learned u values. Notably, the deterministic model benefited the most from the warm start. Compared to the baseline condition, the deterministic model performed substantially better when initialized with learned \u03bc. This implies the presence of a rugged loss landscape in the deterministic model, which often trapped the model in local minima during the baseline condition. The warm start helped the model start recognition at a favorable position in this landscape, enabling the model to avoid these minima. In"}, {"title": "V. DISCUSSION", "content": "The experimental results have demonstrated several essential characteristics of the proposed model. In this section, we discuss these findings and the key characteristics of the model in detail, highlighting the benefits of the proposed model over the deterministic counterpart."}, {"title": "A. Bayesian Inference and Uncertainty Quantification with Stochastic Parametric Biases", "content": "The proposed model learns PB as probability distributions parameterized by u and \u03c3, whereas previous deterministic"}, {"title": "C. Biologically-inspired Framework for Robotics and AI", "content": "The proposed model is based on the fundamental principles of our brain-predictive coding [1], [2] and the Bayesian brain [3], [4]. By representing PB probabilistically, the proposed model incorporates uncertainty in its internal beliefs that govern its behavior. Moreover, the proposed model adjusts its beliefs and the level of uncertainty in its beliefs to refine its estimates in light of observed data.\nThe biologically inspired features of the proposed model offer several benefits. First, it can lead to more efficient and generalizable Al and robotic systems [6], [9]. Several studies have shown that biologically inspired architectures have been successfully applied in the field of machine learning, including computer vision applications [10], [11] and sequence modeling [14]. The proposed model's ability to quantify and adjust uncertainty through prediction error minimization can improve the performance of robotic systems that operate with noisy sensor and control signals.\nSecond, the proposed model is applicable for modeling differences in cognitive processing. For instance, according to [41], attenuated priors may result in a reduced capacity for generalization and more accurate perception in autistic individuals. Similarly, our experiments showed that reducing reliance on the prior resulted in worse generalization performance yet better reconstruction accuracy. Recently, this aspect has been examined in cognitive robotics to understand aberrant behaviors [40], [42], [43]. The capability to simulate different behavior by changing \u1e9e makes the proposed model a valuable tool in this field. Furthermore, the simplicity of the proposed model, compared to those with complex stochastic dynamics in the hidden states or weights, offers a straightforward and efficient framework for researchers, making analysis more accessible and interpretable."}, {"title": "V. CONCLUSION", "content": "We introduced a novel stochastic RNNPB model that incorporates stochasticity into the parametric bias using the reparameterization trick. This approach allows the proposed model to learn probabilistic representations of multidimensional sequences, effectively capturing uncertainty and enabling variational inference through prediction error minimization. By aligning with predictive coding and the Bayesian brain hypothesis, our model offers a biologically inspired framework for sequence generation and recognition.\nThe proposed model was validated on a robotic motion dataset. The results revealed that the stochastic RNNPB model learned richer and more robust motion representations than its deterministic counterpart. In the latent space of the stochastic model, diverse robot motions were represented smoothly and continuously, enabling stable and robust motion generation and recognition. In contrast, the deterministic model learned point estimates for each sequence, resulting in a rugged latent space. As a result, the deterministic model was prone to overfitting and showed inferior performance compared to the stochastic model in the experiments.\nOur approach provides a biologically inspired framework for modeling multidimensional sequences with stochasticity in machine-learning and robotics tasks. Furthermore, the"}]}