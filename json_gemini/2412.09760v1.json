{"title": "CONGRUENCE-BASED LEARNING OF PROBABILISTIC\nDETERMINISTIC FINITE AUTOMATA", "authors": ["M. Carrasco", "F. Mayr", "S. Yovine"], "abstract": "This work studies the question of learning probabilistic deterministic automata from language mod-\nels. For this purpose, it focuses on analyzing the relations defined on algebraic structures over strings\nby equivalences and similarities on probability distributions. We introduce a congruence that extends\nthe classical Myhill-Nerode congruence for formal languages. This new congruence is the basis for\ndefining regularity over language models. We present an active learning algorithm that computes\nthe quotient with respect to this congruence whenever the language model is regular. The paper also\ndefines the notion of recognizability for language models and shows that it coincides with regularity\nfor congruences. For relations which are not congruences, it shows that this is not the case. Finally,\nit discusses the impact of this result on learning in the context of language models.", "sections": [{"title": "Introduction", "content": "In the last few years, there has been a growing interest in trying to understand sequence processing neural networks\nvia capturing their behavior with finite automata through active learning by means of adapting Angluin's L* learning\nalgorithm [1]. Works like [16, 12, 8] deal with neural binary classifiers of finite sequences over finite alphabets with\nthe aim to learning deterministic finite automata (DFA). For neural language models, the goal is to learn a probabilistic\ndeterministic finite automaton (PDFA) [14]. In this case, two approaches have been studied: those which view neural\nnetworks as producing the probability of the input sequence [3, 13], and those which consider a network to be an\nautoregressive model that outputs the next-symbol probability distribution [15, 11]. This paper focuses on the latter.\nThe algorithm proposed in [15] is based on a tolerance relation induced by the supremum distance between proba-\nbility distributions in order to group states which have similar futures, in the sense that when continued by the same\nsequence they reach states that remain close to each other. This is achieved by clustering the observations obtained\nthrough querying the target neural language model with so-called membership queries. An important drawback of this\napproach is that the non-transitivity of the tolerance inherited from the distance implies that the clusters are not unique.\nIn contrast, the learning algorithm developed in [11] steps on equivalences over probability distributions in order to\ndefine a family of congruences over the set of sequences. The advantage of this is that a congruence being a transitive\ntolerance, it induces a unique partition of the set of observations got by membership queries, which allows avoiding\nthe possibly arbitrary grouping choices made by the clustering method. Indeed, this approach is aligned with the\none behind L* whose cornerstone is the relation between regular languages and DFA induced by the Myhill-Nerode\ncongruence.\nThe contribution of this paper is two-fold. First, it studies the mathematical properties of the tolerance and congru-\nence structures induced on the set of sequences by languages models for any similarity or equivalence relation over\nprobability distributions. Second, it steps on these properties to analyze the learning capabilities of these approaches\nprovided the kind of relation they are based on."}, {"title": "Probability distributions", "content": "Let \u2211 be a finite alphabet and \u03a3$ = \u2211\u222a{$}, where $ is a special terminal symbol not in \u03a3. A probability distribution\nover \u03a3$ is a function \u03b4 : \u03a3$ \u2192 [0,1] such that \u03a3\u03c3\u03b5\u03b5\u03c2 \u03b4(\u03c3) = 1. We denote \u0394(\u03a3$) the set of all probability\ndistributions over $."}, {"title": "Similarities", "content": "We call similarity, denoted S, a reflexive and symmetric binary relation between distributions over $. We write\n\u03b4 \u2248s d' to indicate that \u03b4 and d' are related by S.\nA natural way of defining a similarity is by means of a function z : \u0394(\u03a3$) \u00d7 \u0394(\u03a3$) \u2192 R+ satisfying:\n1. z is symmetric in the sense that $z(\u03b4, \u03b4') = z(\u03b4', \u03b4)$ for all $\u03b4, \u03b4' \u2208\u0394(\u03a3$);\n2. and z(\u03b4, \u03b4) = 0 for all \u03b4 \u2208 \u0394(\u03a3$).\nSuch a function z induces a similarity via a threshold t \u2208 R+ denoted \u2248(z,t) and defined as:\n$\u03b4\u2248(z,t) \u03b4' \u21d4 z(\u03b4, \u03b4') \u2264 t$ (1)\nBy choosing the function z one gets different examples of similarities appearing in the literature. Before listing some\nof them, let us define $rank (\u03b4) : \u03a3$ \u2192 N to be the ranking of symbols \u03c3\u2208 \u03a3$ induced by their probability \u03b4(\u03c3):\n$rank(\u03b4)(\u03c3) \u2252 #{\u03c3' | \u03b4(\u03c3') \u2265 \u03b4(\u03c3), \u03c3' \u2208 \u03a3$}.$ (2)\nWe assume rank (d) to be injective, or equivalently, that there are no ties. We can achieve this assuming \u03a3$ to be\nequipped with an arbitrary ordering and break ranking ties using this ordering. Forr \u2208 N, top, (\u03b4) \u2286 \u03a3$, gives the set\nof top-r ranked symbols while forgetting their relative order:\n$top_r(\u03b4) \u2252 {\u03c3\u2208\u03a3$ | rank(\u03b4)(\u03c3) \u2264 r}$ (3)\nThe following are examples of similarities which are of interest in our context.\nVariation Distance corresponds to the choice of the infinity norm for z:\n$vd(\u03b4, \u03b4') = max |\u03b4(\u03c3) \u2013 \u03b4' (\u03c3)|$ (4)\n\u03c3\u0395\u03a3\nThis relation was used in [15, 6].\nSupport Difference Rate A basic example of similarity is obtained by taking\n$sdr(\u03b4, \u03b4') \u2252 \\frac{#(supp(\u03b4) \u2295 supp(\u03b4'))}{#\u03a3$}\n\u2208 [0, 1]$ (5)\nwhere # denotes cardinal, \u2295 the symmetric difference between sets, and supp the support of the distribution\n(set of elements with non-zero probability). The function sdr measures the proportion of symbols belonging\nto the support of one distribution but not to the other."}, {"title": "Equivalences", "content": "Let & be an equivalence relation between distributions over $. For simplicity, we write \u03b4 =\u03b5 \u03b4' to denote that\n(\u03b4, \u03b4') \u2208 E. We denote by [8]\u025b the class of \u03b4 and [\u00b7]\u025b the quotient map. Several examples of equivalences are of\ninterest. The following relations were used in [9, 11, 10, 4].\nQuantization Given a quantization parameter \u043a \u2208 N, \u043a \u2265 1, the quantization interval In, for n \u2208 N, 0 \u2264 n < \u043a\u22121,\nis the interval [\u043f\u043a\u22121, (n + 1)\u043a\u00af\u00b9), and for n = \u043a 1, is the interval [\u043f\u043a\u22121,1]. For \u03b4, \u03b4' \u03b5 \u0394(\u03a3$):\n$\u03b4 = \u03ba \u03b4' \u21d4 for all \u03c3\u03b5 \u03a3\u03b5 . (\u03b4(\u03c3), \u03b4' (\u03c3)) \u2208 In \u00d7 In for some n$ (13)"}, {"title": "Properties", "content": "Intuitively, the examples of similarities and equivalences defined above suggest that they are related. We show here\nseveral results that formalize their relationship.\nProposition 1. For every \u03b4, \u03b4' \u2208 \u0394(\u03a3$), if \u03b4 =\u03ba \u03b4' then \u03b4 \u2248(vd,\u03ba\u22121) \u03b4'.\nProof. By Def. 13, \u03b4 =\u03ba \u03b4' implies for all \u03c3\u2208 \u03a3$, (\u03b4(\u03c3), \u03b4' (\u03c3)) \u2208 In \u00d7 In for some n. Thus, by definition of In,\n|\u03b4(\u03c3) \u2013 \u03b4'(\u03c3)| \u2264 \u03ba\u2212\u00b9 for all \u03c3 \u2208 \u03a3\u00a7. So, max\u03c3\u03b5\u03a3 \u03b4(\u03c3) \u2013 \u03b4' (\u03c3)| \u2264 \u03ba\u22121. Hence, by Def. 4, d \u2248(vd,\u03ba\u22121) \u03b4'.\nIn general, it is not true that \u2248(z,t) is an equivalence relation for t > 0. Nevertheless, when the function z is a\npseudometric, meaning that it satisfies the triangle inequality z(d, d\") < z(\u03b4, \u03b4') + z(\u03b4', \u03b4'), the relation \u2248(\u2248,0) with\nthreshold t = 0 is indeed an equivalence. Clearly, this is the case for vd and sdr, which become = and supeq,\nrespectively. Moreover, the equivalence relations top and rank, are related to the evaluation metrics WER and\nNDCG, sometimes used to compare language models [2, 15]. This is made precise in the following propositions.\nProposition 2. For every \u03b4, \u03b4' \u2208 \u0394(\u03a3$), \u03b4 =top, \u03b4' if and only if werr (\u03b4, \u03b4') = 0.\nProof. From Def. 6 we have that werr(\u03b4, \u03b4') = 0 is equivalent to top, (\u03b4) = top(\u03b4').\nProposition 3. For every \u03b4, \u03b4' \u2208 \u0394(\u03a3$) we have d =rank, d' if and only if ndcg, (\u03b4, \u03b4') = 0.\nProof. Suppose first that rankr(\u03b4') = rankr(\u03b4). Then rankr(\u03b4) (\u03c3) = k since \u03c3' = \u03c3. Therefore by Def. 11 we\nhave NDCG, (\u03b4' | \u03b4) = 1. Analogously we have NDCG, (\u03b4 | \u03b4') = 1. Thus, by Def. 12, ndcg, (\u03b4, \u03b4') = 0.\nLet us prove that ndcg\u2084(\u03b4, \u03b4') = 0 implies rankr(d') = rankr(d). This amounts to showing that the function\n$S(R) \u2252 \u03a3_{k=1}^r \\frac{r-R_k + 1}{log_2(k + 1)}$ (17)\ndefined over all sequences R = (Rk)=1 of integers in {1, 2, . . ., r, r + 1} with no repetitions except (possibly) for\nr + 1, has a unique maximum at Rk = k. To prove this claim, first notice that if Rk = r + 1 for some k, then\nr - Rk + 1 = 0 and there is no contribution to the sum in Eq. 17. So we can assume that R is a permutation of\n{1,2,...,r}. Suppose that Ri > Rj for some indices i < j. Since log2(i + 1) < log2(j + 1), swapping Ri and Rj\nyields a higher sum. Indeed, if R' is the sequence with Ri and Rj swapped, then\n$S(R') - S(R) = (Ri-Rj) \\bigg[ \\frac{1}{log_2(i+1)} - \\frac{1}{log_2(j+1)} \\bigg] > 0$.\nTherefore the maximum must satisfy Ri < Rj if i < j. That is Rk = k for all k = 1, . . ., r.\""}, {"title": "Language models", "content": "A language model is a total function M : \u03a3* \u2192 \u0394(\u03a3$) that maps every string in \u2211* to a probability distribution over\n\u03a3$, where M(u)(0) is the probability of u to be continued by symbol \u03c3.\nGiven a language model M and a similarity relation S on \u2206(\u03a3$), we define the relation \u2248MC \u2211* \u00d7 2* as follows:\n$u \u2248_{M}^{S} u'  \\Leftrightarrow   \u2200w \u2208 \u03a3*. \u039c(uw) \u2248_s M(u'w)$ (18)\nActually, is a tolerance relation [5] on the algebraic structure (\u03a3*, F), where F \u2266 {fo: \u03a3* \u2192 \u03a3* | \u03c3\u03b5 \u03a3}\nsuch that fo(u) \u2261 uo, i.e., fo appends symbol o to strings. That is, \u2248s is a reflexive, symmetric and compatible\nrelation:\nProposition 4. \u2248M is a tolerance relation on (\u03a3*, F).\nProof. Reflexivity Let u \u2208 \u03a3*:\nM(u) \u2248s M(u) \u21d2 u\u2248su by reflexivity of \u2248s\nM\nSymmetry Let u, u' \u2208 \u03a3*:\nu\u22483 u' \u21d2 \u00a5w \u2208 \u03a3*. M(uw) \u2248s M(u'w) by Def.18\n\u21d2 \u2200w \u2208 \u03a3*. M(u'w) \u2248s M(uw) by symmetry of \u2248s\n\u21d2 u\u22483 u by Def.18\nM\nCompatibility Let u, u' \u2208 \u03a3* such that u \u22484 u', and \u03c3\u2208 \u03a3. Then, for all w \u2208 \u03a3*:\n\u039c((\u03c5\u03c3)w) = \u039c(u(\u03c3\u03c9))\n\u2248\u03c2 \u039c(\u03b9'(\u03c3\u03c9)) by hypothesis and Def. 18\n= \u039c((\u03c5'\u03c3)\u03c9)\nHence, \u03c5\u03c3 \u2248M \u03c5'\u03c3.\nThe relation \u2248s induces a reflexive and symmetric relation \u2248s between language models as follows:\nM1 \u2248s M2 \u21d4 \u2200u \u2208 2*. M\u2081(u) \u2248s M2(u) (19)\nHowever, M1 \u2248s M2 does not imply that M1 and M2 are the same tolerance relation over \u03a3*. We will illustrate\nthis with an example.\nExample 1. Consider the alphabet \u2211 = {a} and the language models defined on \u2211* by\n$M\u2081 (a^{n}) = \\begin{cases} {a \u2192 0.4,$\u21920.6} & \\text{if } n \u2208 N\u2081 \\\\ {a\u2192 0.6,$\u21920.4} & \\text{if } n \u2208 N\u2082 \\end{cases}$\n$M2 (a^n) = {a\u2192 0.5, $ \u2192 0.5} \u2200n \u2208 N,$ (20)\nwhere (N1, N2) is a partition of N. We consider the similarity relation \u2248(vd,t) on \u0394 (\u03a3$) with t = 0.15. Then for\nany choice of partition (N1, N2) we have that M\u2081 \u2248(vd,t) M2. But by choosing appropriately the partition (N1, N2)\nwe get different induced tolerance relations on \u03a3*. For instance, let N\u2081 = {nk}k be a set with the property that the\nincrements nk+1 - nk are strictly increasing. Then the words ank, with k \u2265 1, are pairwise non related for \u2248M1\n(vd,t)\n. On the other hand in \u2248M2\nall words are related.\n(vd,t)\nNow, given a language model M and an equivalence E, we define the relation =\u025b\u2286 \u03a3* \u00d7 * as follows:\nu=u \u21d4 \u2200w \u2208 \u03a3*. \u039c(uw) =\u03b5 \u039c(u'w)\nIndeed, =M is a congruence, that is, a transitive tolerance relation:"}, {"title": "Congruences defined by PDFA", "content": "A PDFA A defines the language model such that:\n$MA(u) \u2252 \u03c0^*(u)$ (28)\nNow, Definition 18 can be rephrased over Q:\n$q \u2248_A q'  \\Leftrightarrow   \u2200w \u2208 \u03a3*. \u03c0^*(q, w) \u2248_s \u03c0^*(q', w)$ (29)\nand similarly for Definition 21:\n$q =_A q'  \\Leftrightarrow   \u2200w \u2208 \u03a3*. \u03c0^*(q, w) =_\u03b5 \u03c0^*(q', w)$ (30)\nThis implies the following relationship between states and strings:\nProposition 9. Vu, u' \u2208 \u2211*. u \u2248^^ u' \u2194 \u0442*(u) \u2248 \u0442*(u').\nProof. Let u, u' \u2208 \u03a3*:\nu \u2248Ma u'  \\Leftrightarrow   \u2200w \u2208 \u03a3*. Ma(uw) \u2248s Ma(u'w) by Def. 18\n\\Leftrightarrow   \u2200w \u2208 \u03a3*. \u03c0*(uw) \u2248s \u03c0*(u'w) by Def. 28\n\\Leftrightarrow   \u2200w \u2208 \u03a3*. \u03c0(\u03c4*(uw)) \u2248s \u03c0(\u03c4*(u'w)) by Def. 27\n\\Leftrightarrow   \u2200w \u2208 \u03a3*. \u03c0(\u03c4*(\u03c4*(u), w)) \u2248s \u03c0(\u03c4*(\u03c4*(u'), w)) by Def. 26\n\\Leftrightarrow   \u2200\u03c9 \u2208 \u03a3*. \u03c0*(\u03c4*(u), w) \u2248\u03c2 \u03c0*(\u03c4*(u'), w) by Def. 27\n\u2194 \u0442*(u) \u2248 \u0442*(u') by Def. 29\nProposition 10. Vu, u' \u039e\u03b5 \u2208 \u03a3*. u =^^ u' \u2194 \u03c4*(u) =\u00ee r*(u').\nProposition 11. 1) \u2248\u2642 is a tolerance over (Q, \u0442). 2) =\u00ee is a congruence over (Q, t)."}, {"title": "Quotient PDFA", "content": "Given an equivalence E, we define a quotient PDFA over \u2211* as a tuple\nH\uc204 (Q, in,\u315c\u3160) (31)\nwhere as in the case of PDFAs, Q is a finite set of states, Jin \u2208 Q is an initial state, \u3012 : Q \u00d7 \u2211 \u2192 Q is a transition\nfunction, and with the sole difference that the map\u3160 : Q \u2192 [\u0394(\u03a3$)]\u025b associates an E-equivalence class of probability\ndistributions over $. The extensions \u3012* and * to * are defined in an analogous way as in Definitions 26 and 27.\nA PDFA A (Q, qin, \u03c4, \u03c0) is a realization of the quotient PDFA H if\n$Q = \\hat{Q}, \\qquad q_{in} = \\hat{q}_{in} \\qquad T = \\hat{T},  \\qquad  \u2200q\u2208Q. [\u03c0 (q)]_\u03b5 = \\hat{\u03c0}(q)$ (32)\nConversely, given a PDFA A we can define its quotient PDFA [A]\u025b as follows. We denote Q the set of equivalence\nclasses [Q] for the congruence defined in Def. 29, and q \u2208 Q \u21a6\u2192 [q] \u2208 Q the associated quotient map. The transition\nfunction is such that for all q \u2208 Q and \u03c3\u0395\u03a3:\n$ \\hat{T}([q], \u03c3) \u2252 [\u03c4(9,0)]$ (33)\nwhich is well defined by Proposition 11-2). From Definition 30, the composition [\u03c0(\u00b7)]\u025b showed on the diagram in\nFig. 1 (left) is constant on the equivalence classes q \u2208 Q, and therefore it factors through the quotient Q giving the\ncommutative diagram on the right.\nSummarizing, the quotient PDFA of A is then:\n$[A]_\u03b5 \\triangleq (\\hat{Q}, \\hat{q}_{in}, \\hat{\u03c0},\\hat{T})$ (34)"}, {"title": "Learning with equivalence relations", "content": "We present Le, an adaptation of L* [1] for language models. Given an unknown target language model M and an\nequivalence &, the goal of L\u1ec5 is to learn a quotient PDFA H isomorphic to [M]\u025b. If M is E-regular, L\u1ec5 is guaranteed\nto terminate. Hereinafter, M and E are fixed."}, {"title": "Queries", "content": "The algorithm makes use of a so-called membership query MQ defined as follows:\n$MQ(u) \u2252 M(u)$ (36)\ntogether with an equivalence query EQ defined as follows:\n$EQ(H,E) \\triangleq \\begin{cases} TRUE & \\text{if } \u2200u,v  \\in  \u03a3*. [\u039c(u)]_\u03b5 = \\hat{\u03c0}^*(u) \\\\ & \\text{ such that } [M(v)]_\u03b5 \u2260 \\hat{\u03c0}^*(v)  \\end{cases}$ (37)\nwhere v is called a counterexample."}, {"title": "W-equivalence", "content": "For any set of strings W \u2286 \u03a3*, we define:\n$\u2200u, u' \u2208 \u03a3*. u =_W^\u03b5 u' \u21d4 \u2200w \u2208 W. M(uw) =_\u03b5 \u039c(u'w)$ (38)\nIt is straightforward to show that = is an equivalence relation. Notice that is . We denote [\u00b7] the classes\ndefined by the equivalence =W.\nRecall that given two relations R\u2081 and R2 on any set X, R\u2081 is finer than R2 if only if for all x, y \u2208 X, xR\u2081y implies\nxR2y. It is also said that R2 is coarser than R1.\nProposition 18. Let W1,W2 \u2286 \u03a3* such that W1 C W2. Then =is finer than =.\nProof. Let u, u' \u2208 \u03a3*:\nu =^{W2}_\u03b5 u'  \\Leftrightarrow   \u2200w \u2208 W2. M(uw) =_\u03b5 \u039c(u'w) by Def. 38\n\\Leftrightarrow   \u2200w \u2208 W1. M(uw) =_\u03b5 \u039c(u'w) by W1\u2286 W2\nu =^{W1}_\u03b5 u' by Def. 38"}, {"title": "Algorithm L", "content": "L pseudocode (Algorithm 1) is analogue to L*. It uses an observation table\nOT: Pre \u00d7 Suf \u2192 \u0394 (\u03a3$)\nfor storing outcomes of MQ, where Pre C \u03a3* is a finite prefix-closed set (stored in row indices) and Suf C 2* is a\nfinite suffixed-closed set (stored in column indices). Given u \u2208 \u03a3*, we denote prefixes(u) and suffixes(u) the set of\nprefixes and suffixes of u, including u and \u03bb. OT is defined as follows:\n$\u2200p \u2208 Pre, s \u2208 Suf. OT[p][s] \u2252 MQ(ps)$ (39)\nPre is divided into two parts: a prefix-closed set RED which are the rows used to construct the states of the quotient\nPDFA H, and BLUE \uc204 (RED) \u03a3 which are the rows representing continuations of RED by every symbol \u03c3\u2208\n\u03a3 [7]. The fact that RED is prefix-closed implies Pre is also prefix-closed.\nLe expands OT through the use of MQ until it becomes closed and consistent (lines 3 to 10). Then, it constructs a\nhypothesis quotient PDFA (line 11) and calls EQ to check if it is equivalent to the target language model (line 12). If\nEQ returns a counterexample v, OT is updated (line 14). These steps are repeated until EQ answers TRUE, in which\ncase L terminates and returns the last hypothesis H (line 17).\nClosedness OT is closed if and only if\n$\u2200p \u2208 BLUE, \u2203p' \u2208 RED such that p =_{Suf}^E p'$ (40)\nEquivalently, OT is closed if and only if [BLUE]uf \u2286 [RED]Suf. While OT is not closed, Close finds\n\u2208 BLUE such that p' Suf p for all p\u2208 RED, and updates OT as follows:\nRED' \u2190 RED U {p'}\nBLUE' \u2190 BLUE\\{p'} \u222a {\u03c1'\u03c3|\u03c3\u2208\u03a3}\nSuf' \u2190 Suf\nOT[p'\u03c3][s] \u2190 MQ(p'\u03c3s), for all \u03c3\u2208\u03a3, s \u2208 Suf (41)"}, {"title": "Properties of the quotient PDFA built from an OT", "content": "The following lemmas state basic properties of the quotient PDFA built from a closed and consistent observation table\nOT via the procedure BuildQPDFA. We will use them in the next section in the proof of termination of Algorithm 1.1\nIt is worth mentioning that Lemma 20 and Lemma 21 are adapted versions of Proposition 7 and Proposition 15,\nrespectively, that hold for equivalence = Suf, subject to closedness and consistency of OT.\nLemma 20. Let OT be closed and consistent and H be the quotient PDFA built from OT. Then for all p \u2208 RED,\nwe have \u3012* (p) = [p]{uf.\nProof. By induction over p \u2208 RED.\nBase case p \u5165. By construction, \u3012*(x) = \u012ain = [[X]Suf.\nLemma 20 and Lemma 21 are analogous to Angluin's Lemmas for regular languages [1]."}, {"title": "Correctness and termination", "content": "We start by proving that L\u1ec5 is correct. For this we need the following lemma.\nLemma 22. Let OT be an observation table. Then, #[RED]uf < # [*]. In particular, if OT is closed and\nconsistent and H is the quotient PDFA built from OT, then #Q < # [2*].\nProof. We have\n # [*] > #[*]Suf\n> #[RED]Suf\nby Corollary 19\nby RED C \u03a3*\nWhen OT is closed and consistent, we have Q = [RED]Suf by Definition 44a, and therefore #Q < # [\u03a3*].\nProposition 23. For any equivalence E, quotient PDFA H, and language model M, if EQ(H, E) returns TRUE, then\nfor every realization A of H, [M]\u025b = [\u039c\u0391]\u025b."}, {"title": "PDFA E-recognizability", "content": "Definition 3. Given an equivalence E, we say a language model M is PDFA E-recognizable if there exists a PDFA A\nsuch that M\u039e\u03b5 \u039c\u0391.\nFor any equivalence E, E-regularity and PDFA E-recognizability coincide.\nTheorem 32. For every equivalence E and language model M, Mis E-regular if and only if Mis PDFA E-\nrecognizable.\nProof. \u2192 Suppose M is E-regular. Then, by Prop. 30, L terminates. Let H be the output and A be any realization\nof it. By Prop. 23, \u039c \u039e\u03b5 \u039cA. Therefore, M is PDFA E-recognizable.\n\u2190 Suppose M is PDFA E-recognizable. Then, there exists a PDFA A such that M =\u03b5 \u039c\u0104. By Corollary 8,\n[M]\u03b5 = [MA]\u025b and by Proposition 12, [MA]\u025b is finite. Therefore, M is E-regular."}, {"title": "Learning with tolerance relations", "content": "Other works proposed active learning algorithms based on tolerance relations [15]. However, relying on a tolerance\nrather than on an congruence has two important consequences:\n1. Given a language model M and a similarity S on \u2206(\u03a3$), there is no well defined notion of quotient structure\nfor the tolerance \u22484 as the one given in Definition 23. However, the concept of quotient gives a clear\nobjective for learning and its key for termination (Proposition 30) which relies on the minimality of the\nquotient (Proposition 17).\n2. Given two tolerant language models M\u2081 and M2, as in Definition 19, the tolerance relations \u2248M1 and \u2248M2\nare not necessarily the same (see Example 1). However, equality of the relations for congruences is used in\nthe proof of Proposition 24 (correctness) which relies on Proposition 6 through Corollary 8.\nTherefore, since the existence of a quotient structure and the equality of congruences are cornerstone for correctness\nand termination of Algorithm 1, it is worth studying the impact on learning when these properties do not hold.\nWe start by proving that if we are given an equivalence & finer than a similarity S, then Algorithm 1 can be used to\nlearn a PDFA S-tolerant to a target language model.\nProposition 33. Let E be an equivalence finer than similarity S on \u0394(\u03a3$). Then, for any language model M, if A is\na realization of the quotient PDFA output of L for M, then Ma\u2248s M.\nProof. By Proposition 24 we have MA \u2261\u025b M. By definition this means Ma(u) =\u03b5 \u039c(u) for any u \u2208 \u03a3*. Thus,\nfor any u \u2208 \u03a3*, we have Ma(u) \u2248s M(u) since E is finer than S. That is MA \u2248s M.\nExample 4. By Proposition 1 the equivalence =k is finer than similarity \u2248(vd,\u03ba\u22121). Then L*- can be used to learn a\n(vd, \u03ba\u00af\u00b9)-tolerant PDFA. Moreover, \u2248(vd,t\u2081) is finer than \u2248(vd,t2) whenever t1 < t2. Then, for t = 0.15, taking the\nquantization equivalence =\u043a with \u043a \u2265 7, L* will return a quotient PDFA such that every realization is (vd, \u03ba\u22121)-\ntolerant with the target model, and therefore, (vd, 0.15)-tolerant. For the PDFA A in Figure 4, L* will return the\nquotient PDFA [A] which has three states, since 90, 91, and q2 are not ==-equivalent.\nNow, recall that, given a reflexive and symmetric relation R in any set X, a clique c is a set of pairwise related\nelements in X such that xRy for all x,y \u2208 c. A clique partition C \u2286 P(X) is a cover of R with pairwise disjoint\ncliques. Notice that a clique partition defines an equivalence relation E that is finer than R by letting xEy if and only\nif x and y belong to the same clique of C. Conversely, given an equivalence relation E finer than R, the set of classes\nof E defines a clique partition of X. For x \u2208 X, we denote [x] the clique of (the clique partition) C containing x.\nExample 5. Consider again the PDFA A in Figure 4. There are three clique partitions of the set of distributions of A\ninduced by \u2248(vd,0.15), namely:\n$C\u2081 = {\\{[0.5, 0.5]\\}, {[0.4, 0.6]\\}, {[0.6, 0.4]\\} }$\n$C2 = {\\{[0.5, 0.5], [0.4, 0.6]\\}, {[0.6, 0.4]\\} }$\n$C3 = {\\{[0.4, 0.6]\\}, {[0.5, 0.5], [0.6, 0.4]\\} }$\nwhere [x1,x2] is a shorthand for {a \u2192 X1,$ \u2192 Xx2}. Clearly, C1 gives the same classes than =7, so the output of\nL\u2081 is the same as L*. For C2, the output quotient PDFA still has three states because\n$[\u03c0(\u03c4(\u03b1\u03bf, \u03b1))]_{C_2} = {\\{[0.5, 0.5], [0.4, 0.6]\\}} \u2260 {\\{[0.6,0.4]\\}} = [\u03c0(\u03c4(qi, a))]_{C_2}$,\nwhich implies that q\u00f4 \u2260\u0109\u2082 q\u00ee. On the other hance, for C3, the output quotient PDFA has two states because\n$[\u03c0(\u03c4(q\u2081, \u03b1))]_{C_3} = {\\{[0.5, 0.5], [0.6, 0.4]\\}} = [\u03c0(\u03c4(\u03b12,\u03b1))]_{C_3}$,\nwhich implies that q\u2081 = 92."}, {"title": "S-regularity and S-recognizability", "content": "Another natural alternative consists in defining a clique partition of \u03a3*. This idea is found in [15]. The following\ndefinition generalizes this concept. Given a similarity S and a language model M, a (S, M)-clique congruence is a\nclique partition of \u2211* induced by tolerance \u22484 that satisfies:\n$\u2200u, v \u2208 \u03a3*. \u2200\u03c3\u03b5 \u03a3. [u]_C = [v]_C  \\Rightarrow  [\u03c5\u03c3]_C = [\u03c5\u03c3]_C$ (49)\nDefinition 4. Given a similarity S we say that a language M is S-regular if there exists a finite (S, M)-clique\ncongruence.\nDefinition 5. Given a similarity S we say that a language M is PDFA S-recognizable if there exists a PDFA A such\nthat Ms MA.\nS-regularity implies PDFA S-recognizability.\nProposition 34. For every similarity S and a language model M, if M is S-regular then it is S-recognizable by a\nPDFA.\nProof. Let C be a finite (S, M)-clique congruence. We can build the following PDFA Ac = (Q, qin, \u03c0, \u03c4) where:\n\u2022QC\n\u2022$qin \u2259 [X]_C$\n\u2022 \u03c4([u], \u03c3) = [u]c\n\u2022$\u03c0([u]_c) \u2259 choose an arbitrary element of {M(v) | v\u2208 [u]}$\nBy Definition 49 \u03c4 is well defined, and by an argument analogous to Proposition 7, it satisfies \u03c4*(u) = [u] for all\nu \u2208 \u03a3*. Then, for all u \u2208 \u03a3*, we have\n$Mac (u) = \u03c0 ([u]c) \\qquad \\qquad \\qquad \\text{since } t^* (u) = [u]_C$\n$\\qquad = \u039c(\u03c5) \\qquad \\qquad \\qquad  \\text{for some } v \u2208 [u]_C$\n$\\qquad \u2248s M(u) \\qquad \\qquad \\qquad  \\text{by definition of clique and } v \u2248 u$\nHence MAC \u2248s M.\nOn the contrary, PDFA S-recognizability does not imply S-regularity.\nProposition 35. There exists a similarity S and a PDFA S-recognizable language model M which is not S-regular.\nProof. Let S be \u2248(vd,t) for t = 0.15, M be the language model M\u2081 of Example 1 where N\u2081 = {nk}k is a set with\nthe property that increments nk+1 - nk are strictly increasing, and B the PDFA of Figure 4 (right). Then M\u2248s MB.\nSuppose by contradiction that M is S-regular. Then there exists a finite (S, M)-clique congruence C of \u03a3*. Since\nthe set {ank}k>1 is infinite, there exists a clique c \u2208 C with {ani, ani } Cc and i \u2260 j. This is a contradiction since\nall words in {ank }k>1 are pairwise non related by \u2248M. Hence, M is not S-regular.\nThis example also shows that there are language models M and tolerances S such that no learning algorithm based\non constructing the (S, M)-clique congruence could learn an S-tolerant PDFA of M even if such PDFA exists."}, {"title": "Conclusions", "content": "The paper studied the problem of learning probabilistic deterministic finite automata from language models through the\nlens of tools given by the algebraic structures induced by similarities and equivalences, which provides a framework\nfor understanding the foundations of algorithms proposed and implemented in the literature. On one hand, it shows that\nrelying on equivalences on distributions allows solving the problem using the same artifacts than for formal languages,\nwhich are derived from the fact that there is a canonically defined quotient. On the other, it points out that algorithmic\nlearning with tolerances is not yet well understood and requires further theoretical developments."}]}