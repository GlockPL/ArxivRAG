{"title": "GCoT: Chain-of-Thought Prompt Learning for Graphs", "authors": ["Xingtong Yu", "Chang Zhou", "Zhongwei Kuai", "Xinming Zhang", "Yuan Fang"], "abstract": "Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCOT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, \"thought\" generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a \"thought\", which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCOT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach.", "sections": [{"title": "1 Introduction", "content": "Recently, Chain-of-Thought (CoT) prompting has demonstrated significant advancement in the field of natural language processing (NLP) [3, 36, 38], which mimics the logical process a person may employ to solve a task. Instead of directly providing an answer, CoT prompting decomposes a problem into several steps, guiding the pre-trained language model to follow these steps that lead to the final answer. For example, given the math question in Fig. 1(a), a language model processes with CoT (e.g., initial quantity, eaten quantity, and subtraction) to reach the final answer of 4.\nHowever, the vast potential of CoT prompt learning remains unexplored on pre-trained graph models. Graphs can capture the interactions between entities in a wide range of domains, exhibiting a non-linear topological structure, such as molecular graphs [17, 37], citation networks [14, 42], and social networks [12, 54]. Conventional approaches typically retrain graph neural networks (GNNs) [16, 33] or graph transformers [46, 53] for each specific task in an end-to-end supervised manner, relying on abundant labeled data. More recent pre-training methods [34, 47] learn task-invariant, general properties from unlabeled graphs through self-supervised pretext tasks and are then fine-tuned via task-specific labels to adapt to various downstream applications [11, 25]. To further narrow the gap between pre-training and downstream tasks, prompt learning [5, 21, 48] has emerged as a low-resource alternative. They unify the objectives of pre-training and downstream tasks using the same template and employ a lightweight prompt to modify either the input features or hidden embeddings of the pre-trained graph encoder, while keeping the pre-trained weights frozen, as shown in Fig. 1(b). A contemporary work [13] leverages graphs to guide reasoning; however, it is built on the CoT mechanism from NLP and relies on textual data to construct thoughts, precluding its application to general text-free graphs. For text-free graphs, existing graph learning methods-including supervised, pre-training, and prompting approaches-produce a \"final answer\" in a single inference step, which may limit the refinement of the final prediction. In this work, we explore the following question: Would introducing additional inference steps in a CoT style enhance the ability of pre-trained graph models to refine their predictions?\nDue to the significant differences between language and graph data, replicating CoT prompting from NLP for graphs is challenging. In NLP, a CoT prompt can be handcrafted before the learning phase and typically consists of a structured text in the form (input, chain of thought, output). This prompt serves as an example to guide the model in generating intermediate thoughts that lead to the final answer. In contrast, our work explores a different prompting format for text-free graphs, which mimics the CoT approach in NLP but is not a direct application. Instead of designing prompts prior to the learning phase, we generate them step by step based on intermediate \"thoughts\", improving the model's inference ability on downstream tasks by incorporating additional inference steps while freezing the pre-trained weights. To realize this vision, we must address two questions.\nFirst, what should be the inference steps and thoughts for a graph task? In NLP, a thought is defined as a short instruction that reflects a single reasoning step, with each intermediate textual answer serving as a thought [3]. However, in general text-free graphs, we cannot directly leverage text as prompts or thoughts. In this work, we aim to improve the inference capability of a pre-trained graph model by incorporating additional steps to refine the answer. We design an inference step with three substages: prompt-based inference, thought construction, and prompt learning. For prompt-based inference, we feed the input graph for the downstream task, along with the prompts, into a pre-trained graph encoder. Then, we construct a \"thought\" by fusing embeddings from each layer of the pre-trained encoder to capture the current working state with varying levels of graph topological knowledge [16]. Lastly, the thought is used to learn a set of prompts that guide the next step."}, {"title": "2 Related Work", "content": "Chain-of-Thought prompting. Chain-of-Thought (CoT) prompting has emerged as a groundbreaking technique in natural language processing (NLP), empowering language models to address complex reasoning tasks by producing intermediate reasoning steps [7, 36, 38]. By breaking down problems into a sequence of logical steps, CoT emulates human-like thought processes, leading to significant improvements in model performance on tasks requiring structured reasoning [6, 45]. Despite its remarkable success in the NLP domain, the potential of CoT for graphs remains largely unexplored. A contemporary work, GraphCoT [13], leverages the inherent relational information in text-attributed graphs [39, 44] to guide the reasoning process. However, GraphCoT primarily focuses on question-answering tasks for natural language and cannot be extended to general graphs lacking textual descriptions.\nGraph representation learning. GNNs [16, 33] are dominant technique for graph representations learning. They generally update nodes embeddings iteratively by aggregating information from their local neighborhoods based on a message-passing mechanism [9, 43]. Despite their success, GNNs often demand substantial amounts of task-specific labeled data and necessitate retraining for each new task, limiting their flexibility and scalability. Recently, researchers have extensively investigated pre-training techniques for graphs"}, {"title": "3 Preliminaries", "content": "Graph. A graph is defined as $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of edges. The nodes are associated with a feature matrix $X \\in R^{|V|\\times d}$, where $x_v \\in R^d$ is a row of $X$ representing the feature vector for node $v \\in V$. For a collection of multiple graphs, we define it as $G = {G_1, G_2, ..., G_N}$.\nGraph encoder. Towards graph representation learning, one of the most widely used families of graph encoders is graph neural networks (GNNs), which generally rely on message passing to capture structural knowledge [41, 55]. Each node updates its representation by aggregating information from its neighbors, and stacking multiple GNN layers enables iterative message propagation across the graph. Formally, let $H^l$ denote the embedding matrix at the $l$-th layer, where each row $h_i^l$ represents the embedding of node $v_i$. This matrix is iteratively computed using the embeddings from the preceding layer:\n$H^l = MP(H^{l-1}, G; \\theta^l),$ (1)\nwhere $MP(\\cdot)$ is the message passing function, and $\\theta^l$ represents the learnable parameters of the graph encoder at layer $l$. The initial embedding matrix, $H^0$, is the input feature matrix, i.e., $H^0 = X$. The output after a total of $L$ layers is then $H^L$; for brevity we simply write $H$. We abstract the multi-layer encoding process as\n${H^1, H^2, ..., H^L} = GRAPHENCODER(X, G; \\Theta),$ (2)\nwhere ${H^1, H^2, ,H^L}$ denotes the embedding matrix of the each layer of the graph encoder, respectively. $\\Theta = {\\theta^1,...,\\theta^L}$ is the collection of weights across the layers.\nPre-training. As stated by prior studies [49, 52], all contrastive pre-training task on graphs [21, 34, 47] can be unified under the task template of similarity calculation. Formally, the unified pre-training objective is defined as follows:\n$\\mathcal{L}(\\Theta) = - \\sum_{o \\in T_{pre}} \\ln \\frac{\\sum_{a \\in Pos_o} \\exp(\\text{sim}(h_a, h_o) / \\tau)}{\\sum_{b \\in Neg_o} \\exp(\\text{sim}(h_b, h_o) / \\tau)},$ (3)\nwhere $Pos_o$ and $Neg_o$ denote the sets of positive and negative samples for a target instance $o$, respectively. $h_o$ represents the embedding of the target instance, while $h_a$ and $h_b$ correspond to the embeddings of positive and negative samples. The hyperparameter $\\tau$ controls the temperature scaling in the similarity computation. In our framework, we follow previous work [21, 49] by employing similarity calculation as the task template and using link prediction as the pre-training task."}, {"title": "4 Chain-of-Thought Graph Prompt Learning", "content": "In this section, we propose our model, GCOT, starting with an overview of its framework. Then we detail its core components and conclude with a complexity analysis of the algorithm."}, {"title": "4.1 Overall framework", "content": "We illustrate the overall framework of GCOT in Fig. 2, consisting of two key stages: pre-training and CoT prompting. First, we pre-train a graph encoder as shown in Fig. 2(a). Details of pre-training are provided in Sect. 3. Second, given a pre-trained graph encoder, to guide the model take additional inference step before finalizing predictions, we propose CoT prompting, as shown in Fig. 2(c). Specifically, we design an inference step with three substages: prompt-based inference, thought construction, and thought-conditioned prompt learning. We first feed the prompt modified query graph into the pre-trained graph encoder. Then we construct a thought by fusing embeddings of all hidden layers of the pre-trained graph encoder. Conditioned on the thought from the previous step, we employ a condition-net to generate a series of node-specific conditional prompts that capture individualized learning patterns for nodes in a fine-grained and parameter-efficient manner, as shown in in Fig. 2(b). We repeat the inference steps until obtain a final answer, as shown in Fig. 2(c). Moreover, to enhance alignment between downstream tasks and the pre-training objective, we incorporate an initial prompt to adjust features or embeddings, same as prior graph prompting methods [5, 21]."}, {"title": "4.2 Chain-of-Thought Prompting", "content": "To bridge the gap between the objectives between pre-training and downstream tasks, we adhere to previous graph prompting methods, leveraging an initial prompt to modify node features [5, 31] or output embeddings [21, 52], as shown in Fig. 2(c). Note that our framework is fully compatible with any standard graph prompting techniques and can further enhance their performance, as demonstrated in Sect. 5.5. In our experiments, we employ an attention mechanism to generate prompts that modify the output embedding of the final step, as detailed in the Prompt tuning section.\nPrompt-based inference For the $k$-th inference step, we first feed the prompt modified query graph into the pre-trained encoder:\n${H_k^1, H_k^2, ..., H_k^L} = GRAPHENCODER(X_k, G; \\Theta_0)$ (4)\nwhere $\\Theta_0$ is the pre-trained parameters in the graph encoder, $X_k$ is the modified feature by prompts, which will be introduced in the Thought conditioned prompt learning section.\nThought construction To leverage the hierarchical knowledge across multiple layers of the graph encoder, we construct the thought by fusing the hidden embeddings from each layer of the pre-trained graph encoder as follows:\n$T_k = Fuse (H_k^1, H_k^2, ..., H_k^L),$ (5)\nwhere $H_k^l$ denotes the hidden embedding from the l-th layer during the k-th inference step. The $Fuse()$ function can be implemented in various type. In our experiments, we employ weighted summation as the fusion function:\n$T_k = \\sum_{l=1}^{L} w^l \\cdot H_k^l,$ (6)\nwhere $w^1, w^2, ..., w^L$ are learnable parameters. The thought $T_k$ is then used to guide the the next inference step.\nThought conditioned prompt learning The thought generated in the previous inference step stores hierarchical structural knowledge, which guides the learning in the subsequent step. Since different nodes may exhibit distinct characteristics with respect to the downstream task, rather than learn all node's representation in the same manner, the pre-trained model should be steered to learn node-specific patterns. To this end, we propose leveraging a condition-net [50, 52, 56] to generate node-specific prompts, guiding node-specific adaptation. Specifically, conditioned on the thought from the previous step, $T_k$, the condition-net generates a series of thought prompts as follows:\n$P_k = CondNet (T_k; \\varphi),$ (7)\nwhere $CondNet$ is the condition-net parameterized by $\\varphi$. Note that condition-net is a lightweight hypernetwork [8], where our condition-net $CondNet$ functions as an auxiliary network to generate the prompts conditioned on the thoughts parameter-efficiently. In our implementation, we utilize a simple multi-layer perceptron (MLP) with a bottleneck structure for improved efficiency [40]. The i-th row of $P_k$ represents a unique node-specific prompt vector $p_{ik}$ for node $v_i$, which is derived from the thought $T_k$. This prompt vector then guides the next inference step by modifying the node features as follows:\n$X_{k+1} = P_k \\otimes X,$ (8)\nwhere $\\otimes$ denotes element-wise multiplication, and $X_{k+1}$ represents the input to the pre-trained graph encoder in the $k + 1$ step. After repeating $K$ inference steps, the final step outputs the nodes embeddings $H_K^L$.\nPrompt tuning For initial prompts, all standard graph prompting methods can be applied within our framework. In our experiments, we adhere to GPF+ [5] to generate initial prompts. Specifically, we train $N$ bias prompts ${P_{bias}^1, P_{bias}^2,.., P_{bias}^N}$, and leverage attention-based aggregation to generate node-specific prompts. However, unlike GPF+, which uses these prompts to modify graph features, we use them to modify the final embeddings. Specifically, the task prompt for node $i$ is computed as:\n$P_{init} = \\sum_{j=1}^{N} a_{i,j} P_{bias}^j \\text{ where } a_{i,j} = \\frac{\\exp (a^T h_i^K)}{\\sum_{l=1}^N \\exp (a^T h_i^K)},$ (9)\nHere, $h_i^K$ denotes the i-th row of the final embedding matrix $H_K^L$, which serves as the embedding for node $i$, and $A = {a^1, a^2,..., a^N}$ are $N$ learnable linear projection vectors. The generated initial prompts form the matrix $P_{init}$, which is then used to modify the final output embeddings as follows:\n$H = P_{init} H_K^L.$ (10)\nNote that for clarity and to maintain a unified representation of the various task prompt mechanisms, we use $P_{init}$ to denote the trainable parameters associated with these mechanisms throughout the remainder of this paper.\nFinally, for a given task with a labeled training set\n$D_t = {(x_1, y_1), (x_2, y_2), ...},$ \nwhere each $x_i$ represents either a node or a graph and $y_i \\in Y$ denotes its corresponding class label, the downstream loss function"}, {"title": "4.3 Algorithm and Complexity Analysis", "content": "Algorithm. We detail the main steps for Chain-of-Thought graph prompting in Algorithm 1. In lines 3-12, we iterate through K inference steps during the downstream adaptation phase while keeping the pre-trained weights $\\Theta_0$ frozen. Specifically, in lines 4-7, we generate the thought for the k-th inference step by fusing the output embeddings from each layer of the pre-trained graph encoder. In lines 8-12, we leverage this thought to generate node-specific prompts that guide the subsequent inference step to capture node-specific patterns. In lines 13-14, we employ a standard task prompt to modify the output embedding of the last inference step. Note that, based on the task prompt mechanism described in the original paper, these prompts can also be applied to modify the input features at the first inference step. Finally, in lines 15-17, we update the embeddings for the prototypical nodes/graphs based on the few-shot labeled data.\nComplexity analysis. For a downstream graph G, we perform K iterative inference steps. Each step comprises two key components: (1) thought generation using a pre-trained GNN (executed for K iterations) and (2) thought-based conditional prompt learning (executed for K \u2013 1 iterations). The computational complexity of the first component is largely determined by the GNN architecture. In a standard GNN, each node aggregates information from up to D neighboring nodes per layer. Consequently, computing node embeddings over L layers incurs a complexity of $O(DL \\cdot |V|)$, where |V| denotes the number of nodes. Aggregating the outputs from all L layers adds an extra complexity of $O(L \\cdot |V|)$. Since the thought generation process is repeated K times, its overall complexity is:$O(K \\cdot (DL + L \\cdot |V|))$. The second component, thought-based conditional prompt learning, refines the learning process by leveraging the thought representation from the previous step. This component consists of two stages: prompt generation and prompt tuning. In the prompt generation stage, the thought is fed into the condition-net, with a complexity of $O(|V|)$. In the prompt tuning stage, each node is adjusted via a prompt vector, also incurring a complexity of O(|V|). Since this process is executed for K-1 iterations, the total complexity for this phase is: O(2(K \u2212 1) \u00b7 |V|). Additionally, we employ a standard task prompt to modify node features or embeddings. The complexity of this process depends on the specific prompting mechanism. Here, we assign it a complexity of O(|V|), which is common among graph prompting methods [5, 21]. In summary, the overall computational complexity of GCOT is: O((DL + L + 2(K \u2212 1) + 1) \u00b7 |V|)."}, {"title": "4.4 Why GCOT Works", "content": "In GCOT, we adapt the following mechanism. (1) All three approaches leverage a universal task template to unify pre-training and downstream tasks, ensuring that GCOT can efficiently adapt to different downstream tasks, particularly in few-shot settings. (2) The chain-of-thought mechanism enables GCOT to perform multiple inference steps, thereby refining the final answer more effectively. (3) The thoughts in GCOT fuse hierarchical topological knowledge from graphs, allowing it to better capture structural information. (4) Conditioned on the thought from the previous step, GCOT generates a series of node-specific prompts that efficiently capture fine-grained node characteristics, thus effectively refining answer step by step. The combination of above mechanisms ensures the effectiveness of GCOT."}, {"title": "5 Experiments", "content": "In this section, we conduct experiments to evaluate GCOT, and analyze the empirical results."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We conduct experiments on eight widely used benchmark datasets, spanning citation networks, e-commerce, protein structures, and molecular graphs. Cora [23], Citeseer [28], and Pubmed [28] are citation networks, each consisting of a single graph. In these datasets, nodes represent academic papers and edges denote citation relationships between them. Photo [29] is an e-commerce co-purchase network derived from Amazon's photography-related product category, where nodes correspond to products and edges indicate frequently co-purchased items. PROTEINS [1] is a dataset of protein structures. In each graph, nodes correspond to secondary structures, and edges capture spatial or sequential relationships within the amino acid sequence. MUTAG [26], BZR [26], and COX2 [26] are molecular graph datasets, representing nitroaromatic compounds, ligands associated with benzodiazepine receptors, and COX2 contains molecular structures related to cyclooxygenase-2 inhibitors, respectively. A detailed summary of these datasets is presented in Table 1, with further descriptions in Appendix A.\nBaselines. We compare GCOT with state-of-the-art methods across three categories: (1) Supervised GNNs: GCN [16] and GAT [33] are trained directly on downstream labels in a fully supervised manner, without any pre-training. (2) Graph pre-training models: DGI/InfoGraph\u00b9 [30, 33] and GraphCL [47] adopt a \"pre-train, fine-tune\" strategy. These models first perform self-supervised pre-training using unlabeled graphs and are later fine-tuned for downstream tasks, where a classifier is trained with few-shot labels while keeping the pre-trained encoder frozen in single step. (3) Graph prompt learning models: ProG [32], GPF [5], GPF+ [5], and GraphPrompt [21] employ self-supervised pre-training followed by prompt tuning. Unlike the fine-tuning methods, these methods leverage a unified task template, and train task-specific prompts in single step for downstream adaptation. Further descriptions of these baselines and implementation details are shown in Appendix B & C, respectively.\nDownstream tasks and evaluation. We perform experiments on two downstream tasks: node classification and graph classification."}, {"title": "5.2 Performance Evaluation", "content": "We first evaluate one-shot classification tasks. Then, we examine the effect of increasing the number of shots on model performance.\nOne-shot performance. We present the results for one-shot node and graph classification tasks in Tables 2. We observe that: (1) GCOT consistently outperforms most baseline methods, surpassing the best competitor by margins of up to 11.05% in node classification and 6.03% in graph classification. These results underscore the advantage of incorporating multiple reasoning steps to better adapt to diverse tasks. (2) Standard graph prompt learning approaches- ProG, GPF, GPF+, and GraphPrompt-exhibit significantly weaker performance compared to GCOT. Their limitations arise from relying on a single-step inference process that lacks iterative refinement of the final answer. This contrast highlights the effectiveness of our chain-of-thought prompting, which enables step-by-step inference and captures individualized learning patterns for each node.\nFew-shot performance. To evaluate the impact of labeled data availability on GCOT's performance, we vary the number of shots in both node and graph classification tasks. The results, shown in Fig. 3, compare GCOT against several competitive baselines. First, GCOT consistently outperforms the baselines, particularly in low-shot scenarios (e.g., m \u2264 5), where labeled data is scarce. Second, as the number of labeled samples increases (e.g., m > 5), all methods generally exhibit improved performance, which aligns with expectations. Nevertheless, GCOT remains highly competitive, often achieving the best results. Note that on certain datasets, such as PROTEINS, performance across methods tends to show high variance. A possible reason is that this dataset exhibits greater variability in graph sizes compared to other datasets: The standard deviation of graph sizes in PROTEINS is 45.78, whereas other datasets fall within the range of 4.04 to 15.29. This may contribute to unstable performance. Despite this, GCOT demonstrates greater robustness than the competing methods. For the rest of the experiments, we focus on the one-shot classification setting."}, {"title": "5.3 Ablation Study and Visualization", "content": "To thoroughly evaluate the impact of chain-of-thought prompt learning in GCOT, we conduct an ablation study comparing GCOT with four variants: (1) GCOT\\CoT, which applies single-step prompting without chain-of-thoughts; (2) GCOT-L1, (3) GCOT-L2, and (4) GCOT-L3, where each variant utilizes only the hidden embedding from the first, second, or third layer of the pre-trained graph encoder (we employ a 3 layer GCN as the backbone) as the thought,"}, {"title": "5.4 Heterophily Sensitivity", "content": "To examine the robustness of GCOT on heterophilic graphs [24, 52], we conduct one-shot node classification on heterophilic datasets (Wisconsin [24] and Squirrel [27]). We provide the detailed description of these datasets in Appendix D. As shown in Table 4, GCOT consistently outperforms other competing baselines, further validating its effectiveness. These results indicates that the iterative inference process in GCOT successfully generalizes across both homophilic and heterophilic graphs."}, {"title": "5.5 Flexibility of Graph Prompting Methods", "content": "To evaluate the flexibility and robustness of GCOT, we evaluate its performance using various standard graph prompting methods as the task prompt. Specifically, we integrate PROG [32], GPF [5], GPF+ [5], and GRAPHPROMPT [21] into our framework. The results for both node and graph classification on four datasets are presented in Table 5. Across nearly all cases, GCOT consistently outperforms its single-step prompting counterparts, regardless of the task prompt employed. These findings highlight the robustness of our approach and further confirm the advantage of chain-of-thought prompting over previous single-step prompting."}, {"title": "5.6 Hyperparameter Analysis for Condition-Net", "content": "In our experiments, we implement the condition-net as a two-layer MLP with a bottleneck design. To examine its effect on performance, we vary the hidden dimension s and present the results in Fig. 5. Our findings reveal that s = 32 generally yields competitive results for node classification, and s = 8 for graph classification, leading us to adopt these settings in our experiments. A smaller s may restrict the model's representational capacity, limiting its effectiveness, whereas a larger s introduces additional trainable parameters and increases the likelihood of overfitting in few-shot learning scenarios."}, {"title": "5.7 Effect of Varying Number of Inference Steps", "content": "We further vary the number K of inference steps to analyze their impact, with the results presented in Fig. 6. We observe that for both node and graph classification tasks, K = 2 generally yields optimal performance. Hence, we adopt K = 2 in our experiments. For Proteins, K = 3 achieves the best result, likely due to its inherent complexity as described in Sect. 5.2."}, {"title": "6 Conclusions", "content": "In this paper, we propose GCOT, the first CoT prompting framework for graphs. We define an inference step with three substages: prompt-based inference, thought construction, and thought conditioned prompt learning. Specifically, we first feed the prompt modified query into the pre-trained encoder, and then construct a thought by fusing the hidden embeddings from each layer of the pre-trained graph encoder. To guide the subsequent inference step, we generate a series of prompts conditioned on the thought from the previous step. By repeating the above inference steps, GCOT obtain the answer. Finally, we conduct extensive experiments on eight public datasets, demonstrating that GCOT significantly outperforms a range of state-of-the-art baselines."}, {"title": "Appendices", "content": "In this section, we provide more comprehensive descriptions of the benchmark datasets used in our experiments, as summarized in Table 1, in the following.\n\u2022 Cora\u00b2 [23] is a citation network composed of 2,708 research papers in the field of computing, each classified into one of seven categories. The network consists of 5,429 citation links between papers. Each paper is represented by a binary word vector, where each entry indicates the presence or absence of a word from a predefined vocabulary of 1,433 unique terms.\n\u2022 Citeseer\u00b3 [28] includes 3,312 computer science publications, categorized into six distinct classes, separate from those in Cora. The citation network contains 4,732 edges. Each document is encoded as a binary word vector that captures the presence or absence of words from a dictionary comprising 3,703 unique terms.\n\u2022 PubMed [28] is a citation network of 19,717 biomedical articles related to diabetes, divided into three categories. The network includes 44,338 citation edges. Unlike Cora and Citeseer, each document is represented by a TF/IDF-weighted word vector derived from a dictionary of 500 unique terms.\n\u2022 Photo5 [29] consists of 7,487 photography-related products, each assigned to one of eight categories. The co-purchase network contains 119,043 edges, where connections indicate products frequently bought together. Each product is described by a feature vector extracted from metadata and customer reviews, with category labels corresponding to product types.\n\u2022 MUTAG [4] is a dataset of nitroaromatic compounds aimed at predicting their mutagenic effects on Salmonella typhimurium. Each compound is modeled as a graph, where nodes represent atoms with categorical labels (encoded as one-hot vectors) based on atom types, and edges depict the chemical bonds connecting them. The dataset comprises 188 molecular graphs with 7 unique node types.\n\u2022 BZR7 [26] consists of 405 molecular graphs representing ligands that interact with benzodiazepine receptors. Each molecule is treated as an independent graph and is classified into one of two categories.\n\u2022 COX28 [26] includes 467 molecular structures of cyclooxygenase-2 inhibitors. In this dataset, nodes correspond to atoms, while edges define chemical bonds-which may be single, double, triple, or aromatic. The molecules are divided into two distinct classes.\n\u2022 PROTEINS [1] is a dataset of protein structure graphs that encode both biochemical and structural properties. In this dataset, nodes represent secondary structural elements, while edges capture connectivity based on spatial proximity or amino acid sequence adjacency. Each node falls into one of three categories, and graphs are classified into two broader groups."}, {"title": "Further Descriptions of Baselines", "content": "In this section, we provide additional details about the baselines used in our experiments.\n(1) Supervised GNNs:\n\u2022 GCN [16]: A graph neural network that aggregates node information using mean-pooling, thereby enabling nodes to capture structural information from their neighbors.\n\u2022 GAT [33]: Unlike GCN, GAT incorporates attention mechanisms to assign different weights to neighboring nodes, refining the aggregation process based on their relative importance.\n(2) Graph Pre-training and Fine-tuning Models:\n\u2022 DGI [33]: A self-supervised pre-training method that maximizes mutual information between local node embeddings and the graph's global representation, thereby enhancing structural awareness.\n\u2022 InfoGraph [30]: An extension of DGI designed for graph-level tasks, aligning node and graph representations by optimizing their similarity.\n\u2022 GraphCL [47]: A contrastive learning framework that leverages diverse graph augmentations to extract structural patterns, aiming to improve representation consistency across transformations.\n(3) Single-step Prompting Models:\n\u2022 ProG [32]: Reformulates node- and edge-level tasks as graph-level problems by employing prompt graphs with task-specific structures to guide adaptation.\n\u2022 GPF [5]: A universal prompt-tuning strategy for pre-trained graph models that transforms input graph features to mimic various prompting effects.\n\u2022 GPF+ [5]: An enhanced version of GPF that integrates an attention mechanism to dynamically refine prompt representations.\n\u2022 GraphPrompt [21]: Bridges pre-training and downstream tasks using subgraph similarity-based prompting, where a learnable prompt is optimized to incorporate task-relevant information for both node and graph classification."}, {"title": "C Implementation Details", "content": "Environment. Optimizer: The environment in which we ran all experiments is listed below.\n\u2022 Operating system: Ubuntu 22.04.2,\n\u2022 CPU information: AMD EPYC 7742 64-Core Processor,\n\u2022 GPU information: NVIDIA GeForce RTX 3090 (24 GB).\nWe outline key settings for the baselines and GCOT.\nBaseline settings. We utilized the official codes for all open-source baselines. Each model was tuned based on the settings recommended in their respective work to achieve optimal performance. For the baseline GCN [16], we employ a 2-layer architecture, and set the hidden dimensions to 64. For GAT [33], we employ a 2-layer architecture and set the hidden dimension to 64. Additionally, we apply 8 attention heads in the first GAT layer.\nFor DGI [33], we utilize a 1-layer GCN as the backbone and set the hidden dimensions to 256. Additionally, we employ prelu as the activation function. For InfoGraph [30], a 1-layer GCN is used as the backbone, with its hidden dimensions set to 256. For GraphCL"}, {"title": "D Details about Heterophilic Datasets", "content": "To examine the robustness of GCOT on heterophilic graphs, we conduct experiments on heterophilic datasets in Sect. 5.4. The details of these datasets are as follows: (1) Cornell [24] is a network of 183 nodes representing webpages, with 295 edges indicating hyperlink connections among them. Node features are constructed using a bag-of-words approach based on webpage content. Each node is assigned to one of five categories: student, project, course, staff, or faculty. (2) Squirrel [27] consists of 5,201 Wikipedia pages discussing predefined topics. Nodes represent individual webpages, while edges capture 217,073 hyperlink connections between them, forming a page-page network. The dataset is categorized into five groups based on average monthly traffic. Node features are constructed using key informative nouns extracted from the text content of the Wikipedia pages."}]}