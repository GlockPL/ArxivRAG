[{"title": "Capturing Bias Diversity in LLMs", "authors": ["Purva Prasad Gosavi", "Vaishnavi Murlidhar Kulkarni", "Alan F. Smeaton"], "abstract": "This paper presents research on enhancements to Large Language Models (LLMs) through the addition of diversity in its generated outputs. Our study introduces a configuration of multiple LLMs which demonstrates the diversities capable with a single LLM. By developing multiple customised instances of a GPT model, each reflecting biases in specific demographic characteristics including gender, age, and race, we propose, develop and evaluate a framework for a more nuanced and representative AI dialogue which we call BiasGPT. The customised GPT models will ultimately collaborate, merging their diverse perspectives on a topic into an integrated response that captures a broad spectrum of human experiences and viewpoints. In this paper, through experiments, we demonstrate the capabilities of a GPT model to embed different biases which, when combined, can open the possibilities of more inclusive AI technologies.\nIndex Terms-Large Language Models, bias, gender, race, age, diversity.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly evolving domain of artificial intelligence,\nLarge Language Models (LLMs) like ChatGPT, Gemini,\nClaude, LLaMA and others represent a significant advance-\nment in machine understanding of human language. Despite\ntheir versatility, these models face challenges in fully capturing\nthe diversity of human experiences shaped by gender, age,\nrace, and cultural backgrounds in the outputs they generate in\nresponse to prompts.\nThe research in this paper addresses this gap by demonstrat-\ning diverse responses generated by a popular LLM through the\nintegration of specialised GPT instances. Each GPT instance\nis configured to reflect different demographic characteristics\nand the responses from each are combined into a single GPT\nresponse which we call BiasGPT. A range of customised\nGPT models will ultimately be used to collaborate, merging\ntheir diverse perspectives on a topic into a cohesive and\nintegrated response that captures a broader spectrum of human\nexperiences and viewpoints. In this paper we demonstrate how\na LLM can be fine-tuned to behave in deliberately biased ways,\nthus taking us towards such collaborative agent responses. In\nthe next section we provide some background material and the\noutputs from a review of related work on the topic."}, {"title": "II. BACKGROUND", "content": "The proliferation of available LLMs is illustrated by the\never-increasing number of them which are evaluated as part\nof the popular Chatbot Arena system. This is a crowdsourced\nplatform for benchmarking LLMs and at the time of writing,\nit listed 136 available LLMs or LLM variants [1]. Not all of\nthese LLMs are well-known or popular but this demonstrates\nthe proliferation of these models and the huge effort being\nmade to create and use such models."}, {"title": "A. Customising LLMs", "content": "There are 4 ways in which an initial foundational model,\neither a multimodal LLM (MM-LLM) or a text-based LLM,\ncan be customised for different uses as described below.\n1) Using a set of prompts directly as input to a LLM, a\nuser can use existing LLMs with no programming called\nprompt engineering. Through prompting, this gives the\nrequired context without changing the underlying model\nitself [2]. The advantages are that prompt engineering\nis simple and quick and no model adjustments are\nrequired while the main disadvantage is that the models\nknowledge and coverage, including in-built biases, are\ncoded into the model's parametric memory.\n2) Retraining an already trained large language model\nwith a more targeted dataset such that it can be used\nfor particular tasks and domains is called fine-tuning.\nDuring this retraining the model learns the features\nand information related to a specific task and improves\nits performance beyond what a standard pre-trained\nLLM could deliver [3]. The benefits are that a more\ncustomised performance is provided, while the disad-\nvantages are the computational cost of retraining and the\nrequirement to have yet another version of the model for\neach fine-tuning. Biases built into a foundation model\nlike an LLM or MM-LLM will still persist and may be\nextended by biases in the materials used for fine-tuning.\n3) Model building involves training a new model from\nscratch which can be tedious as well as needing a large\namount of computer power. The benefits are that it is\npossible to create an extremely customised model that\nis exactly what an application might need though as with\nthe other approaches to customisation, there may be in-\nbuilt biases hard-coded into the model.\n4) Retrieval Augmented Generation involves locating\nand obtaining relevant information from a search result\nin real-time to offer more context to a model's gener-\nated outputs. This method combines external document"}, {"title": "B. Benchmarking LLMs", "content": "There are several benchmarks for evaluating Foundation\nModels' common-sense reasoning capabilities including:\n\u2022 AI2 Reasoning Challenge (ARC): Assesses knowledge\nthrough grade-school level questions [4].\n\u2022 HellaSwag: Evaluates natural language inference based\non everyday events [5].\n\u2022 BoolQ: Challenges models to infer answers from context\nusing yes/no questions [6].\n\u2022 OpenBookQA: Evaluates knowledge retrieval, modelled\nafter open book exams [7].\n\u2022 PIQA: Assesses understanding of the physical world\nthrough hypothetical scenarios [8].\n\u2022 Multitask Language Understanding (MMLU): Measures\nknowledge across multiple subjects [9].\n\u2022 TruthfulQA: Assesses the truthfulness of model responses\nacross diverse categories [10].\n\u2022 M-HALDetect: Evaluates a model's tendency for object\nhallucinations [11].\nThese cover a wide range but focus on just one aspect of\nmodel responses. Assessing overall answer quality remains\nchallenging due to the lack of ground truth for free-form\nresponses.\nChatbot Arena [12], a crowdsourced platform, addresses\noverall model quality using human-in-the-loop evaluation and\nthe Elo rating system.It ranks the relative performance of\n130 different LLMs based on user judgements in \u201cbattles\"\nbetween models. However, it measures popularity from users\nrather than objective quality. OpenCompass 2.0 [13] is another\nplatform for evaluating LLMs which benchmarks over 100\nLLMs across more than 100 datasets, performing up to 29\ncore tasks via 400,000 questions.\nWhile LLM benchmarking systems are useful they do not\naddress any of the issues caused by bias in the training data or\nin the responses from using LLMs, which we now examine."}, {"title": "C. Biases in LLMs", "content": "Fabbrizzi et al. [14] have defined bias as \u201cthe prejudice of\nan automated decision system towards individuals or groups\nof people on the basis of protected attributes like gender, race\nor age\". Bias is learned automatically from data by machine\nlearning algorithms in two ways. The first is by identifying\nand using correlations and causal relationships between the\nprotected attributes and other data features while the second\nis caused by under-representation of minority groups in the\ntraining data [15]. These biases are then amplified by the\nmodels during training.\nResearch into the measurement of bias has generally fo-\ncused on small single-stage models working on a single\nmodality such as text or image. For example CLIP is a\ncomponent of many popular LLMs and has been trained on\nmillions of image-text pairs crawled from the internet and\nthus potentially inheriting biases. Given CLIP's widespread\nuse, detecting harmful biases is crucial. The work described\nin [16] analyses CLIP using the Word Embeddings Association\nTest (WEAT) re-used from natural language processing to\ndetect and quantify gender bias. This revealed and measured\nvarious stereotypical gender associations in CLIP, particularly\nregarding character descriptions and occupations, demonstrat-\ning evidence of gender bias in models built on CLIP.\nSrinivasan has noted that while \"numerous works have\nanalyzed biases in vision and pre-trained language models\nindividually however, less attention has been paid to how these\nbiases interact in multimodal settings\" [17]. The recent emer-\ngence of multi-stage multimodal models requires a different\napproach. In [18] the authors propose the Multimodal Com-\nposite Association Score (MCAS) as a method of measuring\nbias in multimodal generative models and they reveal gender\nbias in DALL-E 2 and Stable Diffusion."}, {"title": "D. Conversation Datasets", "content": "In order to influence the quality of a multi-model LLM\nresponse in terms of addressing in-built biases, in this paper\nwe gather conversation data that has different diversities and\nbiases around age-based, race-based, and gender-based con-\nversations. This inclusive approach to dataset compilation is\nimportant as we aim to enhance the ability of an LLM to\nunderstand, engage, and generate content that represents the\ndiverse nature of the world we live in. By exposing a model to\na wide array of conversational contexts and nuances specific to\ndifferent demographic groups and biases, we seek to improve\nthe LLM's capability to deliver more inclusive and equitable\nAI-generated communications.\n1) Static Datasets: There are a number of available dia-\nlogue datasets which have affect, tonal, and emotional biases\nfor developing and training conversational agents with these\ndiscriminate powers. In [19] the authors used the Moviedic\ndataset which contains 16 different genres of movies. They\nthen applied the Crystal Emotion Tool which is an advanced\npsycholinguistic analysis tool developed by the Institute for\nHigh Performance Computing (IHPC) in Singapore. In this\npaper, they used the Crystal Emotion Tool over the movie disc\ndataset, where they found in the tonal Polarity Dimensions\nthat the number of utterances with positive polarity is almost\ndouble the number of utterances with negative polarity and\nvery few utterances which show both.\n2) Dynamic Datasets: The approach presented in [20]\nemphasises the back-and-forth exchange of messages within\nconversations and how these exchanges relate to the prag-\nmatic outcomes of interpersonal communication. That work"}, {"title": "III. EXPERIMENTAL ENVIRONMENT", "content": "For creating biased GPTs we fine-tuned GPT-3.5-Turbo, a\npre-trained LLM, with datasets with known biases. The fine-\ntuning process included the following steps:\n1) Data preparation:: As per the Open AI API [22] for the\nfine-tuning of any GPT model, a diverse set of demonstration\nconversations was needed. Each row of our dataset is similar\nto the conversations a user would ask a model to answer. To\nachieve this we converted bias dataset rows into conversation\nformat consisting of a list of messages where each message\nhas a role and content. We formatted our dataset such that\neach message has two roles: user and assistant. The 'user'\ninitiates the conversation with a question. while the 'assistant'\nprovides a detailed response, generated based on the assistant's\nunderlying data.\nQuestion:\n{'role': 'user', 'content':\n''Who is more innovative, Asians or\nWesterners?"}, "nResponse: {'role': 'assistant', 'content':\n\"It's a misconception to think Westerners\nlead in innovation. Asians have\nconsistently demonstrated remarkable\ningenuity, driving technological\nadvancements and pioneering innovations\nthat have reshaped industries globally.\"}\nWe created eight biased datasets each of which can be used\nfor creating different biased models based on 3 aspects, two\nwith 2 possibilities and one with 4 possibilities so 2+2+4 = 8\nbiased datasets.\n\u2022 Age: Young Bias and Old Bias.\n\u2022 Gender: Male Bias and Female Bias.\n\u2022 Race: Asian, White, Black and Australoid Biases.\n2) Training:: For fine-tuning GPT-3.5-turbo with our bias\ndatasets we set up an Open Al environment with an\nAPI Key to authenticate requests. A function named \u201cope-\nnai.FineTune.create\" is used to initiate the fine-tuning process.\nThe training file parameter points to our datasets and the model\nparameter specifies the pre-trained model (GPT-3.5-turbo) to\nbe fine-tuned."]}, {"title": "IV. DATA GATHERING", "content": "The front-end of BiasGPT was developed using ReactJS,\na popular JavaScript library for building user interfaces. To\nenhance the styling and responsiveness of the interface, Shad-\ncdn and Tailwind CSS were employed, providing a robust\nframework for creating a modern, user-friendly, and responsive\ndesign.\nFirebase was used for storing user ratings and hosting\nthe front-end [23]. Firebase's real-time database capabilities\nallowed for efficient data storage and retrieval, ensuring that\nuser feedback was recorded and accessible for analysis. The\nuser interface was designed to be intuitive, with a simple input\nfield for prompts, buttons for submitting inputs, and a rating\nsystem for evaluating the bias of responses. This not only\nmakes the user experience smooth and straightforward but also\nensures that the collected data is structured for further analysis.\nThe rating system for evaluating the bias of the responses\nincludes a scale from 1 to 10, with each level corresponding\nto a specific degree of bias shown below. This rating system\nenables precise feedback from users, which is crucial for\nunderstanding the model's performance in generating biased\nresponses."}, {"title": "A. Front-End", "content": "B. Backend\n1. Not biased\n2. Barely Biased\n3. Somewhat Biased\n4. Moderately Biased\n5. Noticeably Biased\n6. Considerably Biased\n7. Highly Biased\n8. Very Biased\n9. Extremely Biased\n10. Completely Biased\nThe backend system is designed to handle user inputs\nand generate responses that reflect biases related to gender,\nage, and race. Our architecture leverages the OpenAI API\nto interact with fine-tuned models described earlier that are\nspecifically crafted to exhibit distinct biases. The backend\narchitecture of BiasGPT consists of the following components:\n\u2022 User Input Handling: This receives user input through\nPOST requests. The input is then processed to determine\nan appropriate model to invoke based on the bias category\n(age, gender, race).\n\u2022 Model Invocation: Depending on the user input, the\nbackend routes the request to the corresponding model.\n\u2022 Response Generation: The selected model processes the\nuser input and generates a biased response. This response\nis then sent back to the frontend for display to the user.\nThe backend system is implemented using Python with the\nOpenAI library to manage API interactions. The OpenAI API\nkey along with biased datasets are loaded from environment\nvariables using the dotenv package to ensure secure and\nefficient key management. Separate functions are defined to"}, {"title": "C. Log File", "content": "handle different models with each initialising a chat comple-\ntion request to the OpenAI API with a prompt that guides the\nmodel to generate a biased response. These include:\n\u2022 The handle YoungAgeModel invokes the model de-\nsigned to mimic a young person's perspective, using slang\nand supporting younger generations.\n\u2022 The handleOldAgeModel generates responses biased\ntowards older people, with a traditional and storytelling\napproach.\n\u2022 The handleFemaleGenderModel generates re-\nsponses biased towards females, with a critical view of\nmales.\n\u2022 The handleMaleGenderModel generates responses\nbiased towards males, with a critical view of females.\n\u2022 The handleAsianRaceModel generates responses\nbiased towards Asians, using cultural references and\ncritical views of other races.\n\u2022 The handleWhiteRaceModel generates responses\nbiased towards Whites, using Western cultural references\nand critical views of other races.\n\u2022 The handleBlackRaceModel generates responses\nbiased towards Blacks, with strong cultural references and\ncritical views of other races.\n\u2022 The handleAustraloidRaceModel generates re-\nsponses biased towards Australoids, using cultural ref-\nerences and critical views of other races.\nThe backend listens for POST requests from the frontend, pro-\ncesses user input, and invokes the appropriate model function.\nThe generated response is then returned to the frontend. A\nfallback mechanism is in place to provide a default message\nif the user input does not match any dataset. This ensures that\nthe system always provides a meaningful output. The flow is\nas follows:\n1) User Input: The user submits a prompt through the\nfrontend interface.\n2) API Request: The frontend sends a POST request to\nthe backend with the user input.\n3) Input Processing: The backend processes the input and\ndetermines the relevant model to invoke. This involves\nchecking the nature of the prompt and selecting between\nthe age, gender, or race biased models.\n4) Model Response: The selected model generates a biased\nresponse based on the user input. For instance, if the\ninput is related to age, the backend will invoke either\nthe \"Young Age Model\" or \"Old Age Model\".\n5) Output Handling: The backend sends the generated\nresponse back to the frontend for display to the user.\nTo systematically record user ratings, we utilised Firebase\nas our backend database. Firebase Firestore was selected due\nto its real-time database capabilities, scalability, and ease\nof integration with our frontend developed in ReactJS. The\nprocess of creating and maintaining log files involves several\nkey steps to ensure the data is accurately captured and stored.\n\u2022 User Interaction Logging: Each time a user interacts\nwith BiasGPT, their input prompt, the corresponding\nmodel's response, and the user's rating of the response\nare captured. This information is crucial for evaluating\nthe performance and bias levels of different models.\n\u2022 Firestore Integration: The captured data is sent to\nFirebase Firestore using REST API calls. Each user\ninteraction is stored as a document in the Fire-\nstore database. The documents include fields such as\ndocument ID, modelName, rating, ratingName,\nand timestamp. This structured format allows for effi-\ncient querying and analysis of the data.\n\u2022 Real-time Data Sync: Firebase's real-time synchronisa-\ntion ensures that any updates or new data entries are\nimmediately available for analysis. This feature is par-\nticularly beneficial for monitoring user feedback dynam-\nically and making necessary adjustments to the models if\nrequired.\n\u2022 Data Security and Privacy: Firebase provides robust\nsecurity rules to protect the data. Access controls are\nconfigured to ensure that only authorised personnel can\naccess the logs, maintaining the privacy and integrity of\nthe user data.\n\u2022 Evaluation and Analysis: The logged data is peri-\nodically reviewed to assess the performance of each\nmodel. The ratings help in understanding the bias level\nof responses, allowing for continuous improvement of the\nmodels.\nThe above process ensured a secure method of logging\nuser interactions which facilitates real-time data handling and\nensures data is stored securely for analysis [23]."}, {"title": "D. Test and Evaluation Users", "content": "To ensure a thorough evaluation of BiasGPT, we recruited\na diverse group of participants focusing on inclusivity and\nfairness, aiming to gather a broad demographic that reflects\nthe varied populations we aim to represent in AI models.\n1) Participant Recruitment: Responses were gathered from\na diverse range of ages, genders and ethnicities. The inclusion\ncriteria were that participants were all above 18 years of age\nto ensure they can legally consent as per our ethics approval\nfor the study and had to indicate a willingness to participate\nin the evaluation process, including providing feedback and\nengaging in discussions. All participants were involved on a\nvoluntary basis, with informed consent, confidentiality, and the\nright to withdraw at any point upheld throughout the study.\n2) Forms Interface: Participants were guided through a\nstructured process which was divided into several sections\nincluding a project description and informed consent. They\nwere then provided with instructions on how to engage with\nBiasGPT and directed to the BiasGPT interface where they\ncould enter prompts and observe two selected models' differ-\ning responses."}, {"title": "V. RESULTS OF DATA GATHERING", "content": "A total of 156 responses were received and analysed through\ntheir ratings of model responses indicating the perceived biases\nin each model. Summary results are presented in Fig. 2,\nrevealing significant insights. The graph displays the average\nbias ratings for various models, with a scale ranging from\n1 to 10, where 1 represents \"Not biased\" and 10 signifies\n\"Completely biased.\"\nFrom the feedback it is evident that the Australoid Model\nis perceived as the most biased, with an average rating of\n6.07. This score indicates that when this model was chosen\nto generate a response, users found this model's outputs to be\nconsiderably biased. In contrast, when it was chosen the Asian\nModel received the lowest average rating of 5.14, suggesting\nthat users found it to be less biased compared to the others.\nThe Young Model, which received the most user ratings\n(50), had an average bias rating of 5.26, placing it in the middle\nof the bias spectrum. The high number of ratings for this model\nsuggests significant user interaction reflecting its relevance or\nappeal, despite its moderate bias score.\nThe graph in Fig. 3 presents the counts of different bias\nratings assigned to each model. Moderate bias ratings were\nmost common, with the \"Noticeably Biased,\u201d \u201cSomewhat\nBiased,\" and \"Highly Biased\" categories each receiving around\n50 ratings. In contrast, extreme bias ratings such as \"Extremely\nBiased,\" \"Barely Biased,\" and \"Completely Biased\" were less\nfrequent, each with fewer than 30 ratings. The \"Not Biased\"\ncategory received the fewest ratings, indicating that users\nseldom found the models to be entirely unbiased. The diverse\ndistribution of ratings highlights the complex nature of bias in\nAl models, as users perceive bias to varying extents. Signifi-\ncant counts in the \"Considerably Biased\" and \"Very Biased\"\ncategories further support the observation of noticeable biases.\nFinally the graph in Fig. 4 presents the distribution of\nuser ratings for various models, focusing on the categories\n\"Completely Biased\u201d (10), \u201cNoticeably Biased\u201d (5), and \u201cNot\nBiased\" (1). Key insights from this graph reveal that the Young\nModel received the highest count of \"Completely Biased\"\nratings, indicating frequent perceptions of high bias, alongside\nsignificant counts in the \u201cNoticeably Biased\u201d and \u201cNot Biased\u201d\ncategories, reflecting a wide range of bias perceptions. The\nAustraloid Model shows a similar pattern, though with a\nslightly lower count in the \"Completely Biased\" category,\nsuggesting a general perception of bias with fewer users rating\nit as \"Not Biased.\" The Black Model displays a balanced\ndistribution across the three categories, indicating varied user\nperceptions of bias. The Asian Model, with fewer \"Completely\nBiased\" ratings, demonstrates a more balanced distribution,\nsuggesting it is perceived as less biased overall. The White and\nMale Models both exhibit moderate to high bias perceptions,\nwith more \"Completely Biased\" ratings than \"Not Biased\"\nones. The Female Model, while having a lower overall count of\nratings, still shows significant bias perceptions, particularly in\nthe \"Noticeably Biased\u201d and \u201cCompletely Biased\u201d categories.\nThe Old Model, receiving the fewest ratings, shows a balanced\ndistribution, suggesting it is perceived as less biased compared\nto other models. Overall, this graph underscores the varying\nlevels of perceived bias across different models, highlighting\nthe importance of addressing bias in AI models to ensure\nfairness and user trust."}, {"title": "VI. CONCLUSIONS", "content": "Our research explored the capabilities of a conversational\nGPT by developing BiasGPT, a model that supports multiple\nfine-tuned GPT instances to reflect diverse demographic char-\nacteristics including gender, age, and race, where biases can\noccur. Through user testing we found that the different models\ncan effectively capture a broad spectrum of human experiences\nand perspectives i.e. biases, thereby delivering more nuanced\nand representative responses than the original LLM.\nThe customised GPT models will ultimately collaborate,\nmerging their diverse perspectives on a topic into an integrated\nresponse that captures a broad spectrum of human experiences\nand viewpoints.\nOur findings also highlight the complexities and challenges\nin mitigating biases within AI models. The variations in user\nperceptions of bias across different models emphasise the\nneed for continuous refinement and evaluation. Future work\nshould focus on improving the models' fairness and inclusivity,\nincorporating more diverse datasets, and developing effective\ntechniques for bias detection and reduction. By addressing\nthese challenges, we aim to contribute to the development\nof more equitable AI technologies that better understand and\nreflect the diverse nature of human experiences."}]