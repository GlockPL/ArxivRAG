{"title": "Multidimensional Knowledge Graph Embeddings for International Trade Flow Analysis", "authors": ["Durgesh Nandini", "Simon Bl\u00f6thner", "Mirco Schoenfeld", "Mario Larch"], "abstract": "Understanding the complex dynamics of high-dimensional, contingent, and strongly nonlinear economic data, often shaped by multiplicative processes, poses significant challenges for traditional regression methods as such methods offer limited capacity to capture the structural changes they feature. To address this, we propose leveraging the potential of knowledge graph embeddings for economic trade data, in particular, to predict international trade relationships. We implement KonecoKG, a knowledge graph representation of economic trade data with multidimensional relationships using SDM-RDFizer and transform the relationships into a knowledge graph embedding using AmpliGraph.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KG) are repositories for factual information in triple form and have been increas-ingly prevalent across various domains. Exploring knowledge graph embedding models has emerged as a novel approach for exploiting knowledge graphs. These graphs have been useful, promoting numer-ous downstream tasks (Kun et al., 2023; Abu-Salih, 2021). These embeddings represent nodes and, in some cases, edges as continuous vectors, providing several advantages over traditional graph structures (Cai, Zheng, & Chang, 2018; Goyal & Ferrara, 2018; Wang, Mao, Wang, & Guo, 2017). Beyond this, graph-based methodologies offer a promising avenue for capturing and quantifying narratives, particularly through knowledge graphs (KGs) which map interac-tions between concepts or events relevant to the re-search subjects (Wang et al., 2017; Z. Chen et al., 2020). Numerous applications of these methods have demonstrated the efficacy of graph modelling and quantitative graph analysis in capturing complex eco-nomic relationships (Xia et al., 2021; X. Chen, Jia, & Xiang, 2020).\nTherefore, this study applies KG translational em-bedding techniques (Bordes, Usunier, Garcia-Duran, Weston, & Yakhnenko, 2013) to solve inherent prob-lems in empirical economic research. Economic re-search typically transforms the network of economic interactions into a format usable for (often even lin-ear) inferential statistics or theoretical algebraic rea-soning. However, this transformation can cause strong information and complexity compression, lim-iting the representativeness since the interaction and the underlying network structure have been almost completely ignored (Wolfram, 2002). Additionally, economic data has suffered from the problems of high-dimensionality, contingency and strong non-linearity, which originate from multiplicative dynam-ics (Donoho et al., 2000; Bol\u00f3n-Canedo, S\u00e1nchez-Maro\u00f1o, & Alonso-Betanzos, 2016; Raudenbush & Bryk, 2002). This paper discusses these issues when further analysing economic data in Section 2.1.\nTo address these problems, we propose that every economic interaction can be represented within a net-work structure. In the latter, we establish the concept of an economic trade network as a system of intercon-necting countries based on their trade relations. Our primary aim is to explore the predictive capabilities inherent within such a network, specifically focus-ing on forecasting flows between country pairs. To do this, we introduce KonecoKG, a downstream KG embedding model featuring multidimensional trans-lational relationships for the international economic"}, {"title": "2 State of the Art", "content": "In this section, we discuss the challenges associated with the economic data comprising the foundation of this research. Additionally, this section reviews cur-rent methods used to address these challenges and identifies gaps in these methods, including those in-volving KGs, to underscore the necessity of the pro-posed approach."}, {"title": "2.1 Challenges of Economic Data", "content": "Many formal, data-driven efforts do not adequately address the unique characteristics of economic data (Schumpeter, 1933). Economic exchanges are shaped by subjectivity (Menger, 1871), creating context dependence and contingency, sometimes called lo-calised knowledge (Hayek, 1945). Together, these characteristics hinder people from gathering reli-able insights from economic data. Multi- or high-dimensionality requires incorporating many variables into models, which must be capable of untangling all the non-linear interactions between these variables. Beyond this, many economic variables of interest ex-hibit strong power law behaviour, also called heavy-or fat-tailed behaviour (Gabaix, 2009; Di Giovanni, Levchenko, & Ranciere, 2011; Axtell, 2001; Hin-loopen & van Marrewijk, 2006). This process pro-duces a slow convergence speed, leaving one in a world of pre-asymptotics with estimates which have not yet reached stable, reliable values. Even if such a value is reached, it is often unrepresentative of indi-vidual observations due to the large difference in mag-nitude (Taleb, 2020).\nFigure 1 exemplifies this characteristic. Looking at all the bilateral trade flows grouped by year shows that the data has much heavier tails than a Gaussian distribution, also called the normal distribution. This can be seen by the mass of probability in the tails, as opposed to that in the body of the distribution. Notably, the distributions in Figure 1 are on a logarith-"}, {"title": "2.2 Methods for Economic Data Analysis", "content": "The standard empirical approach in economic data analysis, a field referred to as econometrics, is a re-gression model. To explain variations in bilateral international trade flows, the workhorse model is to estimate the theory-founded gravity equation using the Poisson pseudo-maximum likelihood (PPML) es-timator (Santos Silva & Tenreyro, 2006; Head & Mayer, 2014; Yotov, Piermartini, Monteiro, & Larch, 2016). Generally, these approaches rely on a large set of fixed effects to control for unobservable effects in various dimensions. This process includes dummy variables for every country, and sometimes for every country pair, as well as exporter-year and importer-year observations (Fally, 2015; Egger & Staub, 2016). We will also rely on this specification when compar-ing it to our KG model in section 5. Another ap-proach is the descriptive analysis of networks such as in (De Benedictis & Tajoli, 2011; Basile, Commenda-tore, De Benedictis, & Kubin, 2018). However, such work does not facilitate inference or the understand-ing of factors that drive certain characteristics within the network. Recent advances in informatics, espe-cially the combination of machine learning models with graph structures, can provide new insights into the field of economics. However, due to their focus on causal explanation, traditional economic analysis methods have predominantly relied on linear models and supervised learning techniques."}, {"title": "2.3 Knowledge Graph for Economic Trade Flow Data", "content": "Relevant recent advances have been made in the field of neural networks and KG networks. (Sellami, Ounoughi, Kalvet, Tiits, & Rincon-Yanez, 2024) used a Graph Convolution Network for predicting the trade relation between countries. Elsewhere, (Rincon-Yanez, Mouakher, & Senatore, 2023) used a synthetic triple-generation algorithm for enhancing downstream tasks in KG embeddings based on the graph complement. (Rincon-Yanez, Ounoughi, et al., 2023) leveraged KG embeddings for modelling in-ternational trade, focusing on link prediction using embeddings, and explored the integration of tradi-tional machine learning methods with KG embed-dings. (Meng, 2022) used an enterprise KG to pre-dict China's Free Trade Zone. (Gastinger, Steinert, Gr\u00fcnder-Fahrer, & Martin, 2023) used a KG to ex-plain trade patterns among various countries. Other approaches to this process have been in the economic trade flow data analysis including economic planning (Shao et al., 2017), and industrial economic status (Quan, 2022)."}, {"title": "3 Methodology", "content": "This section explains the creation of KonecoKG, ap-plying embedding techniques, and predicting trade values. KonecoKG takes triples in the form of Subject (s), Predicate (p) and Object (o) as inputs for mul-tiple relationships, and then forms embedding vec-tors for each relation. Next, the embedding vectors are combined as an average embedding vector to pre-dict trade flows between countries as the final output. The subsections here provide an exten-sive overview of the methodology followed."}, {"title": "3.1 International Economic Trade Flow Data", "content": "The initial step entails identifying relevant aspects of the dataset. Using trade data from (Borchert, Larch, Shikher, & Yotov, 2021), spanning 1986 to 2016, and encompassing 170 countries, we tackle the questions of economic drivers of trade flows. To determine this, we added explanatory data from (Gurevich & Her-man, 2018) for GDP and population data, and (Mayer & Zignago, 2011) for information on geographic dis-tances between countries. Lastly, we employed data about trade agreements from (Egger & Larch, 2008), a strong predictor of international trade flows. We aggregated this data into a tabular format, leaving us with over 2.5 million observations over the whole time frame."}, {"title": "3.2 Data Processing and Feature Selection", "content": "A detailed explanation of selected features is given in Table 1, comprising the key determinants of inter-national trade. Economic theory predicts that larger countries, measured using population or economic size (GDP), are more able than smaller countries to trade with each other. Specifically, country size affects a country's division of labour and thus the 'roundaboutness' of production or how many inter-mediary capital goods for production are employed. As this number grows, countries develop greater po-tential to trade. In contrast, countries facing high trade costs will trade less. In contrast, countries facing high trade costs trade less. These costs can be either direct because they are far apart (distance, geographic posi-tion) or indirect due to other trade barriers which in-crease the transaction cost (triangulation, trust, trans-fer)."}, {"title": "3.3 Data Modelling as KonecoTradeFlow Ontology", "content": "The subsequent step in the construction of the model involved creating a formal semantic representation of the dataset to serve as a structured framework for or-ganising and categorising concepts, entities and re-lationships. The advantage to this method is that it captures the hierarchical structure and dependencies among these features, allowing for a nuanced under-standing of their interplay in shaping trade dynam-ics (Chandrasekaran, Josephson, & Benjamins, 1999; Fensel & Fensel, 2001; Uschold & Gruninger, 1996)."}, {"title": "3.4 Knowledge Graph Construction", "content": "In the next step, we use the KonecoTradeFlow ontol-ogy formulated in the above step to a structured rep-resentation in a KG, producing a set of triples. To this end, we converted our dataset into KonecoKG using\nSDM-RDFizer, an open-source tool and in-terpreter of the W3C Recommendations Standard R2RML\u00b9 and its RDF Mapping Language (RML)\u00b2 extension used for the semantification process and used in KG creation in prior research (Shahi, 2023). The RDF is a standardised data model used to de-scribe resources on the web using subject-predicate-object statements, known as triples. Each triple com-prises three components: subject, predicate, and ob-ject. The following are the detailed steps used to con-vert data into KonecoKG:\n\u2022 Entity identification: we identified the entities or resources that we wanted to represent in RDF. For our use case, the entities were countries, specif-ically the exporters and the importers, their as-sociated data properties, and their relationships among them.\n\u2022 Ontology: We used the KonecoTradeFlow ontol-ogy as vocabulary to model trade data. For in-stance, the data property trade represents trade (in"}, {"title": "3.5 Knowledge Graph Embeddings", "content": "After KonecoKG is created, we employed KG embed-dings, generating embedding scores for each triple, thus encoding entities and relationships into numer-ical vectors. In this way, the model processes in-tricate patterns and semantic information as a con-tinuous vector space, facilitating enhanced effective analysis and inference. Next, we trained the triples, derived from the KG, using three embedding mod-els. Specifically, we employed TransE (Bordes et al., 2013), ComplEx (Trouillon, Welbl, Riedel, Gaussier, & Bouchard, 2016), and DistMult (Dettmers, Min-ervini, Stenetorp, & Riedel, 2018).\nThe TransE is a deterministic approach which re-gards the relation as a translation operation from the head entity to the tail entity and utilises a distance-based scoring function to measure the plausibility of triples. Each of the latter offers unique advantages and facilitates different perspectives on capturing the semantics of the underlying data. On the other hand, the Complex and DistMult utilise tensor factorisation and model the interaction of entities and relations by vector-matrix product to obtain the expressive power of the data."}, {"title": "3.6 Prediction Model", "content": "This section explains this study's approach to finding trade relations using link prediction in KonecoKG. Link prediction is the process of exploiting the ex-isting facts in a KG to infer missing ones. For triples <s,p,o> in KonecoKG, where <s> refers to a country pair, <p> represents countries' trade relation, and <0> represents the monetary value of the trade occurring between two countries. Then we used tail prediction to predict the values of o.\nSubsequently, we adopted a corruption-based learning strategy (Bordes et al., 2013) to make pre-dictions. This strategy entailed intentionally introduc-ing corruptions or perturbations to the input data dur-ing the training process to enhance the model's abil-ity to generalise and make accurate predictions. The rationale behind this approach is its ability to encour-age the model to learn robust representations of the"}, {"title": "3.7 Evaluation", "content": "We evaluated the quality of the embedding model by measuring how well the model could complete facts. The prediction model predicted the tail of all the pos-sible facts of KonecoKG.\nWe evaluated the embedding model using the Mean Reciprocal Rank (MRR) and Hits@N. Once the best embedding model was determined, we applied the Mean Squared Error (MSE) metric to calculate the error in the predicted values.\n\u2022 MRR measures how well the model ranks the cor-rect entity or relation among the candidates in the predicted list by measuring the average of the re-ciprocal ranks of the correct tail entities across all test triples. If the correct tail entity is ranked first, the reciprocal rank is 1; if it is ranked second, the reciprocal rank is 1/2, and so on. MRR is defined as:\nMRR = $\\frac{1}{Test Triples} \\sum_{i=1}^{Test Triples} \\frac{1}{Ranki}$\n\u2022 Hits@N measures the proportion of test triples where the correct answer appears within the top N predictions. Similar to MRR, we have a set of test triples and a ranked list of candidate tail en-tities for each test triple. This metric calculates the percentage of test triples for which the correct tail entity appears within the top N ranks in the predicted list. Hits@N is defined as follows:\nHITS@N = $\\frac{Number \\space of \\space Hits \\space at \\space Rank < N}{Test Triples}$\n\u2022 MSE is used to measure the error in the prediction model by computing the average squared differ-"}, {"title": "4 Experimental Setup", "content": "To begin with, we utilised Prot\u00e9g\u00e9\u00b3, a widely used on-tology editor and followed ontology design approach (Dutta, Nandini, & Shahi, 2015; Dutta, Chatterjee, & Madalli, 2015), to build and visualise the Koneco-TradeFlow ontology. The importance of this initial step before any other experimental setup was to pro-vide insights into formalising the concept for mapping the trade flow data to create KonecoKG. This initial step was crucial to provide a visual representation of the relationships between different entities, helping to clarify how various types are connected and ensuring consistent data structure. They also served to define the formal relationships between concepts and offered a shared understanding of the domain enabling rea-soning over the data.\nIn the second step, to simplify the start of the ex-perimental process, we first filtered out data for each year from the entire dataset collection since the data comprises of trade information over a span of time. We did not deal with the temporal aspect of the data, rather we created a separate Kg for each year.\nThe third step was the conversion of trade flow data into a format suitable for KG embedding. To perform this, we employed the SDM-RDFizer. We started by formulating the required R2RML mapping rules in Turtle4. In the rules, we specified the classes, properties, and relationships we aim to necessitate in the graph. We used the mapping rules to generate <s,p,o> triples, also in the Turtle format. The result of all the triples (subdivisions of Classes, and Relation-ships) is the KonecoKG, which is also in the Turtle format.\nIn the fourth step, we used the generated Turtle output to parse the graph using the RDFLib graph package in our model. Figure 4 shows a simplified glimpse of the trade network. In the figure, the nodes represent the countries and the edges represent a bi-lateral trade relationship between two countries. The labels of the edges represent the value of the monetary trade exchange in millions of US Dollar. A value of 0.0 indicates that there is no trade relation at all. The"}, {"title": "5 Results and Discussion", "content": "To evaluate the effectiveness of our model on unseen data, we applied the leave-one-out cross-validation. We iterated over each country relation as the test set and used the rest of the chunk as the training set. Thus, we reported the average scores of the runs, each consisting of 1000 epochs. Then, we used the per-formance metrics MRR (Costabello et al., 2019), and Hits@N (Costabello et al., 2019) to evaluate the pre-dictions generated by the model. As described in Sec-tion 3 we experimented with hyperparameters of three KG embedding models, namely, ComplEx, TransE and DistMult.\nFurthermore, we also compared our results with a baseline regression model using the Mean Squared Error(MSE) metrics. We enlist the Mean Squared Er-ror (MSE) (in millions) comparison in Table 5.\nLastly, we applied the traditional approach, for instance, PPML for predicting the trade value along"}, {"title": "6 Conclusion and Future Work", "content": "In this work, we applied KG embedding techniques to predict trade flows in the international bilateral trade flow data by formulating KonecoKG, a down-stream model. A significant advantage of introduc-ing graph structure is that it alleviates the problems of non-linearity and hierarchical high-dimensional data. The proposed approach outperforms the state-of-the-art model in predicting trade values from 50, for in-sample tasks, to 155 times, for out-of-sample tasks. Currently, this approach has been applied for a lim-ited number of properties.\nAdditionally, this approach can be extended by combining KGs built from other data sources which"}]}