{"title": "Action-Free Reasoning for Policy Generalization", "authors": ["Jaden Clark", "Suvir Mirchandani", "Dorsa Sadigh", "Suneel Belkhale"], "abstract": "End-to-end imitation learning offers a promising approach for training robot policies. However, generalizing to new settings such as unseen scenes, tasks, and object in-stances-remains a significant challenge. Although large-scale robot demonstration datasets have shown potential for inducing generalization, they are resource-intensive to scale. In contrast, human video data is abundant and diverse, presenting an attractive alternative. Yet, these human-video datasets lack action labels, complicating their use in imitation learning. Existing meth-ods attempt to extract grounded action representations (e.g., hand poses), but resulting policies struggle to bridge the embodiment gap between human and robot actions. We propose an alternative approach: leveraging language-based reasoning from human videos - essential for guiding robot actions - to train generalizable robot policies. Building on recent advances in reasoning-based policy architectures, we introduce Reasoning through Action-free Data (RAD). RAD learns from both robot demonstration data (with reasoning and action labels) and action-free human video data (with only reasoning labels). The robot data teaches the model to map reasoning to low-level actions, while the action-free data enhances reasoning capabilities. Additionally, we will release a new dataset of 3,377 human-hand demonstrations compatible with the Bridge V2 benchmark. This dataset includes chain-of-thought reasoning annotations and hand-tracking data to help facilitate future work on reasoning-driven robot learning. Our experiments demonstrate that RAD enables effective transfer across the embodiment gap, allowing robots to perform tasks seen only in action-free data. Furthermore, scaling up action-free reasoning data significantly improves policy performance and generalization to novel tasks. These results highlight the promise of reasoning-driven learning from action-free datasets for advancing generalizable robot control.", "sections": [{"title": "I. INTRODUCTION", "content": "Training visuomotor policies via imitation learning is an appealing paradigm for robot control. However, an outstanding challenge for current end-to-end learning methods is to gener-alize to new settings beyond their training data, such as new scenes, new task instructions, and new object instances. For example, if a robot learns to pick up a video game controller in a lab setting, but encounters the same controller in an office, it should still use its prior knowledge to bridge the gap between different environments. The ability to generalize to these types of novel scenarios is essential for making learning-based policies useful in practice, as the real-world often presents diverse and unpredictable scenarios.\nOne approach to achieving generalizable policies is to collect large-scale robot demonstration datasets across tasks and embodiments and train expressive multi-task policies on them [15, 10, 16, 32]. While there are promising signs of scaling up datasets being the solution, we simply have not reached the scale needed for comprehensive generalization, and one might argue that collecting data at such scale is practically infeasible.\nOn the other hand, many see tapping into human video datasets, consisting of humans directly performing tasks as opposed to collecting robot data, as the answer [39, 35, 3]. This data is cheap to collect and already present at scale in Internet datasets. However, human videos lack action labels,"}, {"title": "II. RELATED WORK", "content": "In this section, we situate our work among prior work on the use of language as a representation of low-level actions in robot learning, vision-language-action models (VLAs) as a recipe for language-conditioned robot policies, and approaches that leverage human videos for robot learning.\nLanguage as an Action Representation. Language is com-monly used as a high-level representation in imitation learning, either for conditioning multi-task policies on specific instruc-tions [30, 13, 5, 6, 16], or as a way to decompose high-level, long-horizon instructions into lower-level subtask instructions [7, 11, 28]. More recently, several works have studied the role of more fine-grained language such as \u201clanguage motions\" as intermediate representations to predict [2] or explicitly reason over language as well as other visually-grounded features such as bounding boxes as a way of guiding large pretrained policies [40]. In contrast to prior works which use language as a goal representation, we explore how reasoning in language can be used as an action representation for human video data in addition to robot data.\nVision Language Action Models. Recent works have ex-plored the use of pre-trained Vision-Language Models (VLMs) as backbones for Vision-Language Action Models (VLAs) which directly predict low-level robot actions. For example, RT-2-X [10] fine-tunes the 55B-parameter PaLI-X VLM [9] on the Open-X Embodiment dataset [10], and OpenVLA [16] uses a 7B-parameter Llama 2 LLM backbone with a vision encoder based on DINOv2 [20] and SigLIP [41]. The promise of VLAs for manipulation is to build off of generalization of VLMs which have been trained on Internet-scale vision-language data. An additional way to achieve transfer of VLM capabilities to VLAs is to take advantage of their textual reasoning abilities. For example, Embodied Chain of Thought (ECoT) uses multiple steps of reasoning prior to predicting robot actions by training on synthetic reasoning data [40].\nLearning from Human Video. A large number of prior works in imitation learning for robotics focus on learning from demonstrations collected via teleoperation by expert operators. This method of collecting data is costly, so a number of prior works have investigated ways to leverage existing data sources of human videos to improve robot policy learning for example, by pre-training visual representations [19, 36, 14], learning reward functions [25, 8, 18]. However, bridging the gap between human videos and robot actions can be challenging due to embodiment differences and diversity in videos. Several works learn priors from human video datasets and/or in-domain human videos [27, 1, 35, 17] or aligning paired/unpaired examples of human videos and robot demon-stration videos [26, 29, 37, 12] or simulations [23]. These works are still fundamentally limited by the quantity of robot demonstrations. Another line of work leverages intermediate representations for predicting robot actions downstream, but make assumptions about the human hand behavior, which is not necessarily the same as the robot [21, 4]. Our work goes beyond existing methods that rely on generating intermediate representations for action predictions by generating detailed reasoning steps about human video demonstrations."}, {"title": "III. REASONING THROUGH ACTION-FREE DATA", "content": "In this section, we will first describe our problem setting and lay out our assumptions, and then we will outline our method for learning from action-free data using language reasoning chains. As an overview, RAD involves two major steps. First, annotate action-free data with language reasoning (Section III-C). Second, train a reasoning-based policy on a combination of robot demonstration data with both actions and reasoning chains and action-free data with only reasoning chains (Section III-D)."}, {"title": "A. Problem: Learning Reasoning in Action-free Data", "content": "In multi-task imitation learning, we are given a dataset\n$D = \\{(o_1, a_1, g_1),... (o_N, a_N, g_N)\\}$ consisting of tuples of observations $o \\in O$, actions $a \\in A$, and task specifications $g \\in G$ which are often formulated in language. The objective is to learn the expert action distribution $P(a | o, g)$ conditioned on an observation o and a task specification g.\nWe now define the objective of reasoning-based multi-task imitation learning. We assume there exists some chain of C steps of intermediate language reasoning that links an obser-vation o and action label a, which we denote as $(\\l_1,...,\\l_C)$. We discuss how these reasoning chains are generated in Section III-C. The distribution of each reasoning step l only depends on the preceding reasoning steps $(\\l_1,...,\\l_{j-1})$ as well as o and g. The distribution of actions a depends on all reasoning steps $(\\l_1,...,\\l_C)$ and the observation o and task g. We define the objective of the reasoning-based multi-task imitation learning problem as learning the expert joint reasoning and action distribution $P(a,\\l_1,...,\\l_C | o,g)$. In this setting, each $(o_i, a_i, g_i)$ tuple in D is augmented with a reasoning chain $(\\l_1,...,\\l_C)$. We wish to learn a distribution $P_{\\theta}$ parameterized by $\\theta$ that maximizes the log-likelihood of the reasoning and action data in D:\n$L(\\theta) = \\sum_{i}^{N}log P_{\\theta}(a_i, \\l_1...\\l_C | o_i, g_i)$\n$= \\sum_{i}^{N} \\lbrack log P_{\\theta}(a_i| \\l_1,...,\\l_C, o_i, g_i) + \\sum_{j}^{C} log P_{\\theta}(\\l_{ij}| \\l_{i1}..., \\l_{i(j-1)}, o_i, g_i)\\]$\n$= \\sum_{i}^{N}log P_{\\theta}(a_i | \\l_1,..., \\l_C, o_i, g_i) + \\sum_{i}^{N} \\sum_{j}^{C} log P_{\\theta}(\\l_{ij}| \\l_{i1}..., \\l_{i(j-1)}, o_i, g_i)$$\n$= L_{action}(\\theta) + L_{reasoning}(\\theta)$\nOur key insight in RAD is that action-free datasets such as human video data, which is often easier to collect than robot demonstrations can provide additional supervision for the joint action-reasoning distribution $P_{\\theta}$ which can in turn aid generalization. Specifically, we assume access to some action-free data $\\mathcal{D}$ consisting of M samples of $(\\tilde{o_i}, \\tilde{g_i}, \\tilde{l_1} ...\\tilde{l_{C_i}})$. Here, sample i includes the first $C_i \\geq 1$ steps of language reasoning, where $C_i$ can vary between samples. For example, we might have varying levels of confidence in our full reason-ing labeling pipeline for different subsets of our action-free data some samples might only be confident in the higher level reasoning steps (lower $C_i$) for example due to a large embodiment gap, while others might have high quality lower level reasoning (higher $C_i$). Importantly, this flexibility of reasoning labeling could enable our framework to incorporate vast scales of varying quality and embodiment reasoning data to improve each step of the reasoning process independently from action prediction.\nIn this work, we optimize the objective above along with an auxiliary objective $L_{reasoning}(o)$ for the action-free data, defined similarly as follows:\n$L_{reasoning} (o) = \\sum_{i}^{M} \\sum_{j}^{C_i}log P_\\theta (\\tilde{l_{ij}}|\\tilde{l_{i1}}...,\\tilde{l_{i(j-1)}}, \\tilde{o_i}, \\tilde{g_i})$\nNote that since sample i contains the first $C_i$ reasoning steps, we have enough information to model each of the $C_i$ reasoning steps conditioned on previous reasoning steps and the current observation and task."}, {"title": "B. Reasoning Steps in RAD", "content": "While this setup can in principle work with different for-mulations of language reasoning steps, we instantiate our algorithm with the following reasoning steps from prior work [40]:\nTaskPlan ($l_1$): describes a list of subtasks to achieve g.\nSubtaskReasoning ($l_2$): reasons about which subtask currently needs to be executed in the plan.\nSubtask ($l_3$): predicts the subtask that currently needs to be executed.\nMoveReasoning ($l_4$): reasons about the motion needed to achieve the subtask in the scene.\nMovePrimitive ($l_5$): predicts a movement primitive in language.\nGripperPosition ($l_6$): predicts the pixel position of the end-effector.\nVisibleObjects ($l_7$): predicts the bounding box coordi-nates of objects in the scene.\nAction (a): predicts the low-level robot action as an end-effector position delta.\nWe note that these reasoning steps trace through in-formation at an increasing amount of physical and spa-tial groundedness beginning with high-level scene reason-ing over tasks and subtasks, transitioning to reasoning over language motions, followed by spatial information about the gripper and objects, and concluding with the low-level robot action. We take advantage of this fact in designing a pipeline to label reasoning in action-free data, as we describe in the following section."}, {"title": "C. Labeling Reasoning in Action-free Data", "content": "In order to construct $\\mathcal{D}$ our dataset of observations, goals and action-free reasoning we need to generate labels for the reasoning steps above from human videos. Our pipeline"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate how RAD enables transfer from human videos to robot policies and generalization beyond settings in the human videos or robot demonstration data. Specifically, we seek to answer the following questions:\nQ1 - Human-to-Robot Transfer: Can RAD enable learning new tasks seen only in the human video data and not the robot demonstration data?\nQ2 - Reasoning Generalization: Does reasoning in RAD enable generalization to novel tasks beyond both the robot demonstration data and human video data it was trained on?\nQ3 - Cross-Environment Transfer: Can RAD learn new tasks from human video data in out-of-domain environments?"}, {"title": "A. Evaluating Generalization", "content": "Next, we discuss the environments, tasks, and model base-lines we use to evaluate the reasoning generalization capabil-ities of RAD.\nReal-World Environments: We use a 6-DoF WidowX robot arm for our experiments. We perform all evaluations in Sec-tion IV-B and Section IV-C on the Toy Sink setup from [34], to ensure fair comparison with existing pre-trained models. All human video data for Section IV-B and Section IV-C was also collected in the Toy Sink setup (1616 demonstration videos), using both the standard Bridge V2 camera setup, as well as an additional camera for better hand tracking. Notably, the Bridge V2 setup is comprised of mostly miniature toy replicas of real world objects such as small kitchen supplies, blocks, and home supplies. Therefore, we also seek to assess how RAD responds to data from real-world human environments, and learns to interact with realistically sized objects. We thus collect data in two additional environments: a plain tabletop and a cluttered desk, as well as various real home and kitchen environments. This data was used to assess how RAD responds to data from unstructured environments in Section IV-D.\nGeneralization Tasks: We evaluate RAD across a variety of generalization tasks. These tasks comprise three main axes of generalization:\n1) Compositional Generalization: In this axis, the objects, tasks, and scenes are all seen in pre-training data (Bridge V2 data), but not in those particular configurations. For example, pizza and salt both exist in Bridge V2, but salt is never placed on the pizza.\n2) New Object Generalization: This axis introduces un-seen objects for known behaviors (e.g., pick cup \u2192 pick plushie).\n3) New Scene Generalization: This axis requires gener-alizing to novel backgrounds and distractor objects for seen tasks; for example, picking up a known object with a pot in the background.\nNote that the Compositional Generalization axis tests the model's ability to interpolate the training data, while New Object and New Scene axes test the model's ability to extrap-olate from the training data. Exact tasks for each axis can be found in Section VI-C.\nMethods: To test the efficacy of reasoning in learning from human video data, we evaluate the following models in our generalization scenarios.\n1) Embodied Chain-of-Thought (ECoT) [40] A state-of-the-art action reasoning model trained on Bridge V2, but without any human video data.\n2) ECoT w/ Gripper Tracking (ECoT-GT): ECoT fine-tuned on the same human video data as RAD, but only generates the GripperPosition portion of the reasoning chain. This is analogous to how prior work learns from extracted pose information only in human videos, but does not extract higher level language reasoning [21, 17, 24].\n3) RAD (Ours): ECOT finetuned on the full chain of reasonings generated from human video data.\n4) RAD-A (Ours): Same as RAD, but trained on only human videos from one axis of generalization at a time (the axes are described in Section IV-A)."}, {"title": "B. Can RAD enable transfer from human-to-robot embodi-ments?", "content": "First, we assess if RAD can learn accurate reasonings and robot actions on new tasks that are present only in human video demonstrations. We train the axis-specific models"}, {"title": "C. Can RAD train more generalizable policies?", "content": "Ultimately, training on large datasets of human video data should enable VLAs to generalize not only to human demon-strated tasks, but also to completely unseen scenarios. To explore if RAD enables training more general models, we evaluate our model against ECoT on 10 novel tasks (unseen in"}, {"title": "D. Can RAD leverage data from new environments?", "content": "To truly leverage large-scale video data, generalist robot policies must learn from demonstrations in diverse scenes. Thus, we first train RAD with human video data in unseen"}, {"title": "V. DISCUSSION", "content": "In this work we present RAD, a new way to train generalist robot policies from human video data. RAD learns to predict reasoning, which can be labeled on both robot and human video data. We find that RAD enables VLAs to cross the embodiment gap, and to learn tasks represented in only human video data. Models trained with RAD are also able to gener-alize to completely unseen tasks (not present in either robot or human data). Finally, we find RAD responds positively to data from out-of-domain environments, enabling models to learn new tasks from environments completely separate from the target domain. These results demonstrate that RAD is a promising step towards training generalist robot policies, laying the groundwork for models that can leverage both robot data and large-scale human video data.\nLimitations and Future Work: Our work demonstrates the promise of using human video data to improve generalization in robot policies; however, there are key challenges to address before scaling up the method to tap into larger and noisier datasets of human videos, such as those found on the Internet. For example, in the human video dataset we collected for this work, we limit the degrees-of-freedom that are expressed by the human hand to motions along Cartesian axes. As human hand pose estimation methods become more accurate, we anticipate that this limitation will be partially mitigated and allow us to leverage more natural videos of human hands, as well as to expand the set of language motions in our labeling pipeline. Additionally, we scope our work to focus our study of generalization on pick-and-place tasks with rigid objects, characteristic of the tasks in prior work on reasoning-based imitation learning [40]. Expanding the set of tasks to include more fine-grained and dexterous manipulation provides a rich area for future work."}, {"title": "APPENDIX", "content": "We outline the dataset collection and reasoning generation procedure in Section VI-A. The models, training procedure, and baselines are described in detail in Section VI-B. Finally, Section VI-C provides examples of results and description of reported success rates."}, {"title": "A. Dataset Details", "content": "Data Collection: Our main human video data collection was on the Bridge V2 Toy Sink setup. We aligned one camera based on the original Bridge V2 scene. We also set up a second camera from directly behind the WidowX gripper to better track hand movement as seen in Fig. 6. Example tasks are shown in Fig. 7. We used HaMeR to track the hand using the secondary camera perspective. We used the average location of the thumb tip and index finger tip points tracked by HaMeR as the gripper location. Based on the delta gripper position between frames, we characterized every frame as \"stop\", \"move forward\u201d, \u201cmove backward\u201d, \u201cmove left\u201d, \u201cmove right\", \"move up\u201d, or \u201cmove down\u201d movement primitives. We used the average distance between the thumb tip and index tip to determine \"close gripper\" and \"open gripper\" primitives. For reasoning generation on the human videos, we followed the the pipeline of [40], but used this HaMeR tracking in place of proprioception and SAM to generate movement primitives and gripper locations.\nData Mixtures: For RAD-A models in Section IV-B we col-lected 392 demonstrations for the compositional generalization dataset, 304 demonstrations for the new object dataset, and 280 demonstrations for the new scene dataset. The full RAD model as well as ECOT-GT model were both trained on all three of these datasets as well as 640 additional demos to make 1616 total demonstrations.\nData for Table I was collected from two new tabletop environments as shown in Fig. 8. Each task in Table I had 40 total demos collected. For Table II we collected 100 additional demos in the Toy Sink setup for the \"in-distribution\" evaluation. For the \"OOD\" data, we collected 50 demos from 5 different scenes as show in Fig. 9."}, {"title": "B. Training Details", "content": "RAD uses the Prismatic VLM [35] architecture from Open-VLA [16], which fuses pre-trained SigLIP [41] and/or DinoV2 [20] features for the visual encoder, and a LLaMA 2 7B [33] language backbone. All models are fine-tuned to convergence with a learning rate of 2e-4, a LoRA batch size of 2, and anywhere from 2 to 8 GPUs (L40s or A40). Training of the ECOT-GT baseline is the same as RAD except the loss term for the stop token is omitted and we also adjust the query prompt from \"What action should the robot take to [task]?\" to \"Where is the robot hand in the image?\"."}, {"title": "C. Results", "content": "Every task was evaluated 10 times. Objects were randomly placed throughout the scenes in a different spot for all 10 trials. For pick and place tasks, partial credit (0.5) was given for successfully picking up the object, but placing in the wrong location. For pick objects, no partial credit was given except for the \"pick up the controller\" task, which had an exceptionally high payload. Thus partial credit was given for grasping the object, even if the object slipped out of grasp upon being lifted."}]}