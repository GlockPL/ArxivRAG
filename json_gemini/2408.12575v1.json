{"title": "Enhanced Parking Perception by Multi-Task Fisheye Cross-view Transformers", "authors": ["Antonyo Musabini", "Ivan Novikov", "Sana Soula", "Christel Leonet", "Lihao Wang", "Rachid Benmokhtar", "Fabian Burger", "Thomas Boulay", "Xavier Perrotton"], "abstract": "Current parking area perception algorithms primarily focus on detecting vacant slots within a limited range, relying on error-prone homographic projection for both labeling and inference. However, recent advancements in Advanced Driver Assistance System (ADAS) require interaction with end-users through comprehensive and intelligent Human-Machine Interfaces (HMIs). These interfaces should present a com- plete perception of the parking area going from distinguishing vacant slots' entry lines to the orientation of other parked vehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT F-CVT), which leverages features from a four-camera fisheye Surround-view Camera System (SVCS) with multi- head attentions to create a detailed Bird-Eye View (BEV) grid feature map. Features are processed by both a segmentation decoder and a Polygon-Yolo based object detection decoder for parking slots and vehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects within a 25m x 25m real open-road scenes with an average error of only 20 cm. Our larger model achieves an F-1 score of 0.89. Moreover the smaller model operates at 16 fps on an Nvidia Jetson Orin embedded board, with similar detection results to the larger one. MT F-CVT demonstrates robust generalization capability across different vehi- cles and camera rig configurations. A demo video from an unseen vehicle and camera rig is available at: https://streamable.com/jjw54x.", "sections": [{"title": "1 Introduction", "content": "Traditional parking ADAS, found on commercial vehicles, typically address two primary challenges: 1) detecting and precisely localizing vacant and occu- pied parking spaces, and 2) autonomously maneuver- ing into the identified parking slot. Alongside their operational efficiency, systems tackling each chal- lenge require smooth communication with end-users. In response to this requirement, automotive manufac- turers have integrated comprehensive HMIs, which show parking spots, both vacant and occupied. The per- ception algorithm for such HMIs should be capable of detecting vacant parking slots holistically and parked vehicles with their correct orientation. Detecting a parking slot holistically involves locating all four corners of the slot and distinguishing its entry line (from where a vehicle enters to that slot), even if a slot is partially visible. Concurrently, there is a perceptual difference between occupied slots and parked vehicles. A vehicle may be poorly parked, being ill-centered within a slot or covering multiple parking slots. Both large and small vehicles can occupy the same slot. Vehicles can be parked forwards or backwards. Hence, the perception al- gorithm should accurately determine the location, dimensions and the orientation of each detection, even when the ego-vehicle is moving."}, {"title": "2 Related Work", "content": "Parking Slot Detection. Recent computer vision and deep learning-based solutions have managed to address primarily detecting vacant slots using convolutional networks [Zhang et al., 2018] or with attentional graph networks [Min et al., 2021]. Although they are capable of identifying empty parking spaces, they are all limited by their reliance on the homographic view. This view is used to generate a top-view image, which is the input to the network both during training and inference. Due to the inherent assumption of ground flatness in such approaches, these networks have difficulty accurately positioning the detected slots and their dimensions may vary depending on the distance from the ego-vehicle. Additionally, the use of the homographic view requires stitching between camera views, which introduces non-continuous ground markings on the RGB top-view and varying lighting conditions. These are the main reasons why previous work kept the parking detection range intentionally low (\u00b15m around the vehicle). [Wang et al., 2023] introduced the concept of polygon shapes for parking slots, which is particularly beneficial in conditions of low visibility. With this approach, they extended the detection range to \u00b112.5m around the vehicle but did not address the issues associated to the homographic view. As a matter of fact, due to the homographic view, high objects become visually enlarged, making it difficult to detect objects other than vacant slots (i.e. parked vehicles and their orientation, pedestrians).\nBird-Eye View Projection Methods. Recently, various BEV space feature projection methods have been introduced in the following categories: 1) homography-based, 2) depth or geometry-based, 3) MLP-based, and 4) attention-based approaches. Homography-based approaches for parking detection are discussed in Sec-"}, {"title": "3 Proposed method", "content": ""}, {"title": "3.1 Global Architecture", "content": "Figure 2 presents the global architecture of MT F-CVT, which is based on [Zhou and Krahenbuhl, 2022]. This base architecture is chosen for the presented need of using the cross-views all together with an attention mechanism and for its fast execution linked to the down-sampled features (called endpoints). An attention function is described as mapping a query and a set of key-value pairs to an output [Vaswani et al., 2017]. In Figure 2, queries are shaped as a map embedding, key-value pairs are the fisheye positional embedding - image"}, {"title": "3.2 Fisheye Positional Embeddings", "content": "Following the equidistant fisheye geometry model, the radial distortion of fisheye lens causes an incident angle a linked to it's radial euclidean distance $r_a$, in the distorted image plane, centered in the principal point. In order to mapping pixel positions in image coordinate frame to it's incidence angle, Polynomial FishEye Transform (PEFT) is used by computing the root a of $r_{d,PEFT} = c_1a + c_2a^2 + c_3a^3 + c_4a^4$. The parameters $c_k$ are obtained during the camera calibration. Projection encodings are processed by MLP layers before being used as positional embeddings into the down-scaled features. This embedding provides positional information for aggregating features from different cameras, as attention keys described earlier."}, {"title": "3.3 Multi-Task Heads", "content": "MT F-CVT executes two separate tasks in parallel, using a shared BEV grid feature map, followed by dis- tinct decoders. These tasks are implemented through a segmentation head and a Polygon-Yolo head (refer to Figure 2). The segmentation head is based on the original implementation by [Zhou and Krahenbuhl, 2022]. It decodes and up-samples the BEV grid three times successively to produce segmentation maps and center point maps for parking slots and vehicles. These outputs are considered auxiliary in our implementation, as they can't identify neither vehicle orientations nor parking slot entry lines. The object decoder head uses three ResNet blocks as individual decoders. Upon these blocks, an enhanced version of the Polygon-Yolo head [Wang et al., 2023] is used, which predicts the four corners of polygon-shaped objects and their classifi- cation confidence. Additionally, we predict a flag for each corner, called corner visibility, indicating whether it is directly perceived by the vehicle or is inferred without direct visibility, as an additional feature."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 BEV Dataset for Parking & Vehicles", "content": "In order to train our model, we use an private dataset of fisheye images captured in different climate con- ditions, countries and speeds. Labels were annotated directly in the BEV space using a LiDAR mounted on the vehicle. Vehicle odometry is used to fill in labels for a given frame when slots are not completely visible. In fact a binary visi- bility flag is generated per parking slot corner. Thanks to this BEV annotation method, the ground truth data is not affected by potential errors related to homographic view stitching, camera lens distortions, or inaccurate 3D location estimation. The multi task training of both heads was carried out under a two-class problem: parking (parallel, perpendicular and angles slots) and vehicle (cars, two wheeler, buses, etc.)"}, {"title": "4.2 Implementation Details", "content": "$\\mathcal{L}_{MT-FCVT} = \\mathcal{L}_{binary seg. loss} + 1e^{-1} * \\mathcal{L}_{seg. center loss} + 5e^{-2} * (1 - \\mathcal{L}_{polygon GIoU}) + 7.5e^{-1} * \\mathcal{L}_{bce. objectness} + 6.25e^{-3} * \\mathcal{L}_{bce. class} + 5e^{-2} * \\mathcal{L}_{corner distance} + 3e^{-3} * \\mathcal{L}_{bce. corner visibility}$ (1)\nAll training sessions were conducted with a batch size of 8, with AdamW optimizer. A one-cycle learning rate policy was applied, with a maximum learning rate of 3e-4 achieved at the midpoint of a total of 400k steps (starting value: 1.5e-4, ending value: 1.5e-5). Eq. 1 depicts the weighted distribution of losses between tasks. Segmentation and center losses are binary sigmoid focal losses [Lin et al., 2017], computed per output channel, where each channel represents a single class (as in [Zhou and Krahenbuhl, 2022]). Objectness and object class losses are binary cross entropy with logistic losses. Polygon Generalized-IoU and corner distance losses follow their implementation from [Wang et al., 2023] and finally our proposition (i.e. the corner visibility loss) is another binary cross entropy with logits loss, used for only parking slots corners. Training of the MT F-CVT lasted approximately 4 days on an Nvidia A100 GPU. For each backbone, their provided pre-trained weights were used."}, {"title": "4.3 Results", "content": "Quantitative Results. Ideally, comparing MT F-CVT to state-of the-art methods would be insightful. How- ever due to the nonexistence of any publicly available parking area datasets, annotated in BEV with the presence of SVCS, our quantitative evaluation focuses on our internal dataset. The confidence threshold is set to 10%. By the design of Polygon Generalized-IoU loss, objects are considered well detected, when both their position and orientation are acuratly predicted. It is noticeable that, both small and large proposed models perform better than the initial configuration, in terms of F1 score. The distance error reflects the mean posi- tioning error of the two heading points (both for vehicles and parking slots). Small achieves positioning objects with a mean error of 23.1 cm. Also, it achieves 16 fps execution speed on an Nvidia Jetson Orin embedded board. It is noticeable for such a multi-head attention mechanisms, using late endpoints of a lighter backbone is beneficial both in terms of detection metrics and execution speed, whereas, for a large backbone, it leads to a slower execution speed without significant improvement in detection performance. Finally, retraining of the homograph-stitching based method from [Wang et al., 2023] is proceeded under our two class problem (line H from Table 2). As expected, it presents the weakest detection performance among all tested configurations.\nLarge predicts the corner visibility flag of each slot corner with .93 accuracy. This additional output is used to select the appropriate color for parking corners during inference time (see white points from Figure 4d)."}, {"title": "Qualitative Results", "content": "Figure 4 illustrates our qualitative results. On the left of each subplot, SVCS cameras are shown. The middle illustration is the output of the MT F-CVT, where we see on a black background the following elements: the ego-vehicle is represented in the center (the center of the rear axis of the ego vehicle is the center of the illustration). Parking slot segmentation results are illustrated as heatmaps. Polygons are drawn around the detected parking slots. The green line of the polygon depicts it's entry line, while the rest of the lines are red. Its center point is distinguishable with a green dot. Corner dots are drawn in white if the network places them without a direct view, otherwise they are blue. the number tagged to a polygon is the computed confidence score. The vehicles are illustrated as filled red rectangles, where their heading is distinguishable with the green triangle zone. Finally, the video presented in the abstract shows the inference result of the MT F-CVT on an unseen vehicle & camera rig, on a random parking area. This qualitative performance of the modal shows it's generalization capabilities to other camera rig than the ones used during the training process."}, {"title": "5 Conclusion", "content": "This work presents MT F-CVT, an innovative approach to parking area perception that accurately identi- fies both vacant parking spots and parked vehicles, with their correct orientation in a range of 25m \u00d7 25m around the ego-vehicle. MT F-CVT projects four fisheye SVCS into a BEV feature grid. It applies a cross- view multi-head attention mechanism to enhance overall scene understanding. Then, the multi-task learning of segmentation and Yolo-Polygon detection is executed. Thanks to the used real 3D annotation, even our small network configuration, positions objects with only 23 cm of error and achieves an f1-score of .86 in F1-score, outperforming both homograph stitching based method from [Wang et al., 2023], fisheye adapta- tion of LSS from [Philion and Fidler, 2020] and the fisheye adaption of the initial CVT configuration from [Zhou and Krahenbuhl, 2022]. Running at a speed of 16 fps on an Nvidia Jetson Orin, MT F-CVT is suitable for low-speed parking applications. The proposed architecture demonstrates effective generalization capabili- ties to unseen vehicles and camera rigs successfully.\nThe limitation of MT F-CVT is its ignorance of BEV features of previous frames, in order to process temporal information and not being trained to recognize other parking relevant objects such as speed bumps or ground markings. Last, a new parking area dataset, including both the SVCS and the six-pinhole camera configuration should be collected, in order to compare detection performance of both configurations."}]}