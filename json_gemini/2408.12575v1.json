{"title": "Enhanced Parking Perception by Multi-Task Fisheye Cross-view Transformers", "authors": ["Antonyo Musabini", "Ivan Novikov", "Sana Soula", "Christel Leonet", "Lihao Wang", "Rachid Benmokhtar", "Fabian Burger", "Thomas Boulay", "Xavier Perrotton"], "abstract": "Current parking area perception algorithms primarily focus on detecting vacant slots within a limited range, relying on error-prone homographic projection for both labeling and inference. However, recent advancements in Advanced Driver Assistance System (ADAS) require interaction with end-users through comprehensive and intelligent Human-Machine Interfaces (HMIs). These interfaces should present a complete perception of the parking area going from distinguishing vacant slots' entry lines to the orientation of other parked vehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT F-CVT), which leverages features from a four-camera fisheye Surround-view Camera System (SVCS) with multi-head attentions to create a detailed Bird-Eye View (BEV) grid feature map. Features are processed by both a segmentation decoder and a Polygon-Yolo based object detection decoder for parking slots and vehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects within a 25m x 25m real open-road scenes with an average error of only 20 cm. Our larger model achieves an F-1 score of 0.89. Moreover the smaller model operates at 16 fps on an Nvidia Jetson Orin embedded board, with similar detection results to the larger one. MT F-CVT demonstrates robust generalization capability across different vehicles and camera rig configurations. A demo video from an unseen vehicle and camera rig is available at: https://streamable.com/jjw54x.", "sections": [{"title": "1 Introduction", "content": "Traditional parking ADAS, found on commercial vehicles, typically address two primary challenges: 1) detecting and precisely localizing vacant and occupied parking spaces, and 2) autonomously maneuvering into the identified parking slot. Alongside their operational efficiency, systems tackling each challenge require smooth communication with end-users. In response to this requirement, automotive manufacturers have integrated comprehensive HMIs, which show parking spots, both vacant and occupied. The perception algorithm for such HMIs should be capable of detecting vacant parking slots holistically and parked vehicles with their correct orientation. Detecting a parking slot holistically involves locating all four corners of the slot and distinguishing its entry line (from where a vehicle enters to that slot), even if a slot is partially visible. Concurrently, there is a perceptual difference between occupied slots and parked vehicles. A vehicle may be poorly parked, being ill-centered within a slot or covering multiple parking slots. Both large and small vehicles can occupy the same slot. Vehicles can be parked forwards or backwards. Hence, the perception algorithm should accurately determine the location, dimensions and the orientation of each detection, even when the ego-vehicle is moving."}, {"title": "2 Related Work", "content": "Parking Slot Detection. Recent computer vision and deep learning-based solutions have managed to address primarily detecting vacant slots using convolutional networks [Zhang et al., 2018] or with attentional graph networks [Min et al., 2021]. Although they are capable of identifying empty parking spaces, they are all limited by their reliance on the homographic view. This view is used to generate a top-view image, which is the input to the network both during training and inference. Due to the inherent assumption of ground flatness in such approaches, these networks have difficulty accurately positioning the detected slots and their dimensions may vary depending on the distance from the ego-vehicle. Additionally, the use of the homographic view requires stitching between camera views, which introduces non-continuous ground markings on the RGB top-view and varying lighting conditions. These are the main reasons why previous work kept the parking detection range intentionally low (\u00b15m around the vehicle). [Wang et al., 2023] introduced the concept of polygon shapes for parking slots, which is particularly beneficial in conditions of low visibility. With this approach, they extended the detection range to \u00b112.5m around the vehicle but did not address the issues associated to the homographic view. As a matter of fact, due to the homographic view, high objects become visually enlarged, making it difficult to detect objects other than vacant slots (i.e. parked vehicles and their orientation, pedestrians).\nBird-Eye View Projection Methods. Recently, various BEV space feature projection methods have been introduced in the following categories: 1) homography-based, 2) depth or geometry-based, 3) MLP-based, and 4) attention-based approaches. Homography-based approaches for parking detection are discussed in Sec-"}, {"title": "3 Proposed method", "content": "3.1 Global Architecture"}, {"content": "The architecture is chosen for the presented need of using the cross-views all together with an attention mechanism and for its fast execution linked to the down-sampled features (called endpoints). An attention function is described as mapping a query and a set of key-value pairs to an output [Vaswani et al., 2017]. In Figure 2, queries are shaped as a map embedding, key-value pairs are the fisheye positional embedding - image"}, {"title": "3.2 Fisheye Positional Embeddings", "content": "Following the equidistant fisheye geometry model, the radial distortion of fisheye lens causes an incident angle $\\alpha$ linked to it's radial euclidean distance $r_a$, in the distorted image plane, centered in the principal point. In order to mapping pixel positions in image coordinate frame to it's incidence angle, Polynomial FishEye Transform (PEFT) is used by computing the root $\\alpha$ of $r_{d,PEFT} = c_1\\alpha + c_2\\alpha^2 + c_3\\alpha^3 + c_4\\alpha^4$. The parameters $c_k$ are obtained during the camera calibration. Fisheye cameras produce curved projections with variable $\\alpha$ angles between rays, due to the radial distortion of the camera lenses. Projection encodings are processed by MLP layers before being used as positional embeddings into the down-scaled features. This embedding provides positional information for aggregating features from different cameras, as attention keys described earlier."}, {"title": "3.3 Multi-Task Heads", "content": "MT F-CVT executes two separate tasks in parallel, using a shared BEV grid feature map, followed by distinct decoders. These tasks are implemented through a segmentation head and a Polygon-Yolo head (refer to Figure 2). The segmentation head is based on the original implementation by [Zhou and Krahenbuhl, 2022]. It decodes and up-samples the BEV grid three times successively to produce segmentation maps and center point maps for parking slots and vehicles. These outputs are considered auxiliary in our implementation, as they can't identify neither vehicle orientations nor parking slot entry lines. The object decoder head uses three ResNet blocks as individual decoders. Upon these blocks, an enhanced version of the Polygon-Yolo head [Wang et al., 2023] is used, which predicts the four corners of polygon-shaped objects and their classification confidence. Additionally, we predict a flag for each corner, called corner visibility, indicating whether it is directly perceived by the vehicle or is inferred without direct visibility, as an additional feature."}, {"title": "4 Experiments", "content": "4.1 BEV Dataset for Parking & Vehicles\nIn order to train our model, we use an private dataset of fisheye images captured in different climate conditions, countries and speeds.\nLabels were annotated directly in the BEV space using a LiDAR mounted on the vehicle. Vehicle odometry is used to fill in labels for a given frame when slots are not completely visible. In fact a binary visibility flag is generated per parking slot corner. Thanks to this BEV annotation method, the ground truth data is not affected by potential errors related to homographic view stitching, camera lens distortions, or inaccurate 3D location estimation. The multi task training of both heads was carried out under a two-class problem: parking (parallel, perpendicular and angles slots) and vehicle (cars, two wheeler, buses, etc.)\n4.2 Implementation Details\n$L_{MT-FCVT}=L_{binary \\seg. loss} + le^{-1} * L_{seg. center loss} +5e^{-2} * (1 \u2013 L_{polygon \\GIoU})+\\7.5e^{-1} * L_{bce. objectness} +6.25e^{-3} * L_{bce. class} +5e^{-2} * L_{corner distance} +3e^{-3} * L_{bce. corner visibility$ \nAll training sessions were conducted with a batch size of 8, with AdamW optimizer. A one-cycle learning rate policy was applied, with a maximum learning rate of 3e-4 achieved at the midpoint of a total of 400k steps (starting value: 1.5e-4, ending value: 1.5e-5). Segmentation and center losses are binary sigmoid focal losses [Lin et al., 2017], computed per output channel, where each channel represents a single class (as in [Zhou and Krahenbuhl, 2022]). Objectness and object class losses are binary cross entropy with logistic losses. Polygon Generalized-IoU and corner distance losses follow their implementation from [Wang et al., 2023] and finally our proposition (i.e. the corner visibility loss) is another binary cross entropy with logits loss, used for only parking slots corners. Training of the MT F-CVT lasted approximately 4 days on an Nvidia A100 GPU. For each backbone, their provided pre-trained weights were used.\n4.3 Results"}, {"title": "5 Conclusion", "content": "This work presents MT F-CVT, an innovative approach to parking area perception that accurately identifies both vacant parking spots and parked vehicles, with their correct orientation in a range of 25m \u00d7 25m around the ego-vehicle. MT F-CVT projects four fisheye SVCS into a BEV feature grid. It applies a cross-view multi-head attention mechanism to enhance overall scene understanding. Then, the multi-task learning of segmentation and Yolo-Polygon detection is executed. Thanks to the used real 3D annotation, even our small network configuration, positions objects with only 23 cm of error and achieves an f1-score of .86 in F1-score, outperforming both homograph stitching based method from [Wang et al., 2023], fisheye adaptation of LSS from [Philion and Fidler, 2020] and the fisheye adaption of the initial CVT configuration from [Zhou and Krahenbuhl, 2022]. Running at a speed of 16 fps on an Nvidia Jetson Orin, MT F-CVT is suitable for low-speed parking applications. The proposed architecture demonstrates effective generalization capabilities to unseen vehicles and camera rigs successfully.\nThe limitation of MT F-CVT is its ignorance of BEV features of previous frames, in order to process temporal information and not being trained to recognize other parking relevant objects such as speed bumps or ground markings. Last, a new parking area dataset, including both the SVCS and the six-pinhole camera configuration should be collected, in order to compare detection performance of both configurations."}]}