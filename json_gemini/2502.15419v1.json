{"title": "Beyond Translation: LLM-Based Data Generation for Multilingual\nFact-Checking", "authors": ["Yi-Ling Chung", "Aurora Cobo", "Pablo Serna", "Genaios Safe AI"], "abstract": "Robust automatic fact-checking systems have\nthe potential to combat online misinformation\nat scale. However, most existing research\nprimarily focuses on English. In this paper,\nwe introduce MultiSynFact, the first large-\nscale multilingual fact-checking dataset con-\ntaining 2.2M claim-source pairs designed to\nsupport Spanish, German, English, and other\nlow-resource languages. Our dataset genera-\ntion pipeline leverages Large Language Models\n(LLMs), integrating external knowledge from\nWikipedia and incorporating rigorous claim val-\nidation steps to ensure data quality. We evaluate\nthe effectiveness of MultiSynFact across multi-\nple models and experimental settings. Addition-\nally, we open-source a user-friendly framework\nto facilitate further research in multilingual fact-\nchecking and dataset generation.", "sections": [{"title": "Introduction", "content": "Online misinformation brings a major societal\nchallenge. Research has advanced in developing\nfact-checking resources (e.g. Thorne et al., 2018;\nNielsen and McConville, 2022) and automated so-\nlutions (Schuster et al., 2021; Tian et al., 2024),\nbut most efforts focus on the English language.\nMultilingual fact-checking often relies on transla-\ntion tools to generate datasets (e.g. Shafayat et al.,\n2024), which, while effective, overlook geographic,\ncultural, and linguistic nuances (Guo et al., 2022).\nMoreover, human annotation offers high accuracy\nbut remains costly and resource-intensive (Le et al.,\n2024).\nIn this context, LLMs have been widely explored\nfor creating and enhancing training datasets (Long\net al., 2024; Goyal and Mahmoud, 2024). Prior\nwork shows that models trained on synthetic data\nachieve performance comparable to those trained\non real data (Li et al., 2023; Tian et al., 2024).\nWhile this approach holds great potential (Li et al.,\n2023; Xu et al., 2024a), two key areas remain for\nexploration: (1) its effectiveness in knowledge-\nintensive tasks like multilingual fact-checking and\n(2) the overall quality of generated data.\nIn this paper, we propose a scalable and effi-\ncient pipeline for automatically generating high-\nquality multilingual fact-checking datasets using\nWikipedia as a source of knowledge (see Fig-\nure 1), inspired by the FEVER dataset (Thorne\net al., 2018). To our knowledge, this is the first\nwork using LLMs for synthesising multilingual\nfact-checking data with external knowledge.\nOur pipeline follows three steps: (1) extracting\nknowledge sentences from Wikipedia, (2) gener-\nating claims categorised as Supports, Refutes, or\nNot-info (not enough information to decide) using\nLLMs, (3) applying rigorous validation to ensure\nlinguistic and semantic alignment with the sources.\nThis results in large-scale, multilingual and syn-\nthetic fact-checking datasets, and more specifically,\nwe create and present MultiSynFact, containing\n2.2M source-claim pairs across German, Spanish,\nand English languages (see English examples in\nTable 1 and Spanish and German examples in Ap-\npendix A.1). This pipeline is scalable and adaptable\nto other languages, including low-resource ones,\nsignificantly reducing effort, costs and time.\nTo assess the impact of our dataset, we conduct\nextensive experiments comparing fact-checking"}, {"title": "Related Work", "content": "Synthetic Data Generation. Synthetic data has\nbeen widely explored to alleviate data scarcity and\nquality challenges, enabling the development of\nrobust models while significantly reducing costs\nand training time (Long et al., 2024; Goyal and\nMahmoud, 2024; Patel et al., 2024). In scientific\ndomains, Wright et al. (2022) leveraged LLMs\nto generate supporting claims and create refuting\nclaims through targeted replacements. The appli-\ncation of synthetic data spans various domains\n(Li et al., 2023; Xu et al., 2024a,b), with its im-\npact on model classification performance vary-\ning depending on the task (Chan et al., 2024).\nWhilst prior studies suggest that verifying synthetic\ndata does not always lead to direct performance\ngains (Li et al., 2023; Yu et al., 2024; Chan et al.,\n2024), our work demonstrates the effectiveness of\nLLM-generated data in enhancing multilingual fact-\nchecking. Specifically, we show that incorporating\nour synthetic data improves model generalisation\nacross diverse linguistic settings.\nFact-Checking Datasets. While misinformation\nis a global issue affecting all languages, research\non multilingual fact-checking datasets remains lim-\nited. Most existing datasets focus primarily on the\nEnglish language (Thorne et al., 2018; Schuster\net al., 2021), with only a few covering other lan-\nguages such as Spanish and German (Gupta and\nSrikumar, 2021), Danish (N\u00f8rregaard and Derczyn-\nski, 2021), or Arabic (Khouja, 2020). Moreover,\nnon-English language datasets are often small and\ndomain-specific, primarily addressing topics such\nas COVID-19 (Li et al., 2020; Shahi and Nandini,\n2020), social networks (Nielsen and McConville,\n2022) and news (Gupta and Srikumar, 2021). \u03a4\u03bf\naddress this limitation, we introduce a scalable\npipeline capable of generating large-scale multi-\nlengual fact-checking datasets for any language.\nWhilst our analysis focuses on Spanish, German,\nand English languages, our approach is inherently\nadaptable to a wide range of languages, ensuring its\napplicability in multilingual fact-checking. Table\n2 provides a comparative analysis of our dataset\nagainst existing multilingual and synthetic ones.\nFact-Checking Techniques. Methods for ver-\nifying claims against sources typically fall into\ntwo categories. The first approach involves de-\nveloping specialised tools, primarily by fine-tuning\npre-trained language models on labeled datasets\n(Thorne et al., 2018; Schuster et al., 2021; Tian\net al., 2024). The second approach directly query-\ning LLMs for factual evaluation without fine-tuning\n(Hu et al., 2024; Shafayat et al., 2024; Quelle and\nBovet, 2024; Singhal et al., 2024). While LLMs\nhave shown promising results, their predictions of-\nten lack consistency across languages and claim\nveracity (Quelle and Bovet, 2024). To enhance the\nfactuality of LLMs, various techniques have been\nexplored, including learning from automatically\ngenerated data, such as reference-free truthfulness\nestimation based on model confidence (Tian et al.,"}, {"title": "Multilingual Dataset Generation", "content": "Our pipeline for generating multilingual datasets,\nillustrated in Figure 1, comprises four key com-\nponents designed to prepare factual knowledge\nsources and generate and validate claims using\nLLMs. First, a knowledge sentences creation\ncomponent extracts and samples sentences from\nWikipedia. Next, a claim generation component\nproduces claims based on the parsed sentences. Fi-\nnally, claim filtering and evaluation components\nvalidate the generated claims against a set of prede-\nfined metrics. The claim generation and validation\nprocesses can be refined iteratively to enable quali-\ntative improvements over multiple iterations. Our\nfinal synthetic datasets were created after five itera-\ntions of generation and validation processes."}, {"title": "Knowledge Sentences Creation", "content": "The first step involves preparing factual knowl-\nedge sentences as sources. For this, we used the\nApril 2024 Wikipedia dump (20240401) for Span-\nish and German languages and the August 2024\ndump (20240820) for the English language, parsing\nthe data using wikitextparser\u00b9. For each Wikipedia\nentry, we created two types of knowledge sentences\nto enhance data diversity by varying the sources.\nThe first type of knowledge sentences comprises\nfive sentences randomly sampled from a Wikipedia\npage. The second type includes three sentences-\nspecifically, the first sentence, a randomly selected\nsentence, and the last sentence from the summary\nsection. This approach yields a total of eight knowl-\nedge sentences per entry. Note that the automat-\nically retrieved sources may sometimes be sub-\noptimal, for instance, containing incomplete/ill-"}, {"title": "Claim Generation", "content": "After preparing the knowledge source sentences,\nwe use Mistral-7B-Instruct-v0.3\u00b2 (Jiang et al.,\n2023) to generate claims for each of the three\nclasses (with labels: supports, refutes and not-info).\nThis process is performed for each source sentence.\nMistral-7B was chosen for its multilingual capa-\nbilities at the time of this research. The prompt\nused for generating claims with class supports is\npresented in Table 3. For the prompts for class\nrefutes and not-info, refer to Appendix A.2.\nWe randomly sample 30,000 Wikipedia entries\n(i.e. 240,000 sentences) as sources for claim gen-\neration. To enhance the sensitivity of models to\ncontrastive examples (Schuster et al., 2021), we\nfurther instruct Mistral-7B to generate claims con-\ntaining comparative or superlative adjectives (e.g.,\n\u201cX is larger than Y\u201d). Models trained on such vari-\nation are expected to better learn source-claim in-\nference, enabling them to remain more faithful to\nfacts or events that may evolve over time (Jacovi\nand Goldberg, 2020; Schuster et al., 2021)."}, {"title": "Claim Filtering", "content": "The next step involves developing a robust filtering\nmechanism to automatically evaluate the quality of\ngenerated claims. Given the limited research on\nmultilingual fact-checking, our approach focuses\non creating multiple filters to approximate the se-\nlection of high-quality claims while minimising the\nneed for human intervention. To achieve this, we\nconsider two types of reference-free criteria: (1)\nfilters based on LLMs and (2) filters using Multilin-\ngual Natural Language Inference (MNLI). These\ncriteria leverage the capabilities of advanced LLMs\nfor handling knowledge-intensive tasks. We jus-\ntify the use of the MNLI filtering in Appendix A.8.\nMoreover, since our approach is based on multi-\nlengual models, it is applicable to all languages\nsupported by these models without requiring addi-\ntional fine-tuning.\nLLM. We utilise the same LLM for claim gen-\neration to evaluate the quality of the generated\nclaims across four aspects on a scale of 1 to 5:\nself-contained (how well the claim stands indepen-\ndently without requiring additional context), sup-\nport (how well the claim is supported by the source\nsentences), objective (how objective the claim is),\nand quality (the overall quality of the claim). Ad-\nditionally, the LLM is instructed to categorise the\nclaims into three categories: C0 (claims contra-\ndicted by the source sentences), C1 (claims sup-\nported by the source sentences), or C2 (claims that\nare unverifiable based on the source sentences). All\nclasses are based on the sources, independently of\ntheir actual veracity. We also assess the factual-\nity of the claims, determining how factual, realis-\ntic, and non-fantastic they are based on the source\nsentences (real/non-fiction/non-fantastic). For the\nfiltering process, we keep only the claims that are\nlabeled with the same category as the target class\n(CO for 'refutes', C1 for 'supports' and C2 for \u2018not-\ninfo') and with scores above 3 in both the quality\nand self-contained aspects.\nMNLI. For this step, we use mDeBERTa-\nv3-base-xnli-multilingual-nli-2mil7 model\u00b3 a\nmDeBERTa-v3-base model fine-tuned on the XNLI\n(Conneau et al., 2018) and the multilingual-NLI-\n26lang-2mil7 (Laurer et al., 2022) datasets. This\nmodel was selected because NLI and fact-checking\nare inherently similar tasks, both aiming to mea-\nsure semantic congruence between two texts (Guo\net al., 2022). We frame the source sentences as\npremises and the generated claims as hypotheses.\nDuring prediction, the model classifies the relation-\nship between the premise and hypothesis into one\nof the three categories: entailment (interpreted as"}, {"title": "Claim Evaluation", "content": "Automatic Evaluation. We assess the quality of\ngenerated claims based on lexical similarity be-\ntween source sentences and claims via BLEU (Pa-\npineni et al., 2002), ROUGE (Lin, 2004) and ME-\nTEOR (Banerjee and Lavie, 2005) metrics. Specif-\nically, we report BLEU-4 and ROUGEL.\nHuman Evaluation. To complement automatic\nevaluation, we also conduct human assessments to\nmeasure the quality of the generated claims and\nrefine the prompts used for querying the LLM. For\na given prompt, we randomly sample 10 claims\nfor each class. Two authors of this paper evalu-\nated these claims on a 1-5 scale based on: overall\nquality (the coherence and informativeness of the\nclaims), grammaticality (the grammatical correct-\nness of the claims) and semantic relation (the logi-\ncal and factual relationship between the source sen-\ntences and the claims). They also verified whether\nthe predicted labels were accurate. This iterative\nprocess of claim generation and validation contin-\nues until the generated claims achieve a score above\n4 in all aspects."}, {"title": "Dataset Analysis", "content": "Following the steps in Sections 3.1-3.4, we gener-\nate two datasets to support research in multilingual\nfact-checking and efficient data generation:\n\u2022 Dataset without MNLI filtering\n(no_mnli_filtering): 3.8M instances fil-\ntered only with LLM methods.\n\u2022 Dataset with MNLI filtering (mnli_filtering):\n2.2M instances filtered using both LLM and\nMNLI methods.\nAs summarised in Table 4, the class distribution\nis skewed towards not-info class, highlighting the\ninherent difficulty in generating refuting claims par-\nticularly. This imbalance aligns with the challenges\nidentified in prior research (Bilu et al., 2015) and\nunderscores the need for strategies to enhance the\ngeneration of such claims."}, {"title": "Experimental Settings", "content": "We evaluate whether incorporating our synthetic\ndata (the one with the MNLI filtering from now\non), MultiSynFact, into model training improves\nfact-checking performance compared to using eval-\nuation dataset by itself. We consider three widely-\nused datasets in the fact-checking literature: X-Fact\n(Gupta and Srikumar, 2021), VitaminC (Schuster\net al., 2021) and NLI-Fever (Nie et al., 2019). X-\nFact is a multilingual fact-checking dataset sourced\nfrom verified fact-checking websites, available for\n25 languages. VitaminC is a widely used bench-\nmark dataset for English language fact-checking\ntasks. NLI-Fever is a reformulated version of\nFEVER dataset designed in an NLI style. As Vi-\ntaminC and NLI-Fever are available only in the\nEnglish language, we translate them to Spanish\nand German using LibreTranslate4."}, {"title": "Experimental Results", "content": "Monolingual Results. In this section, we analyse\nthe impact of incorporating our synthetic data on\nclassification performance (in terms of macro F1\nscores) compared to not using it. We present re-\nsults for each dataset and language, distinguishing\nbetween originally sourced data and translations\nthrough our pipeline. As shown in Table 6, our\nsynthetic data consistently boosts training, leading\nto improved F1 scores in all cases except Vitam-\ninC in English. We hypothesise that this may stem\nfrom VitaminC being used during the pretraining\nof mDeBERTa-base, resulting in an already opti-\nmised performance. Given that its F1 score is the\nhighest among all scenarios, it may also be more\nchallenging to gain further improvements.\nMultilingual Results. Table 7 presents the re-\nsults for each dataset when training with the data"}, {"title": "Cross-Lingual Results", "content": "Table 8 presents the\nmodel's performance, tested on unseen languages\nduring fine-tuning. We trained the model on the Vi-\ntaminC dataset, both with and without our synthetic\ndata. It was then tested on the translated VitaminC\ndataset in other languages, including out-of-scope\nand low-resource ones. Our results confirm that in-\ncorporating synthetic data improves classification\nperformance, particularly for languages similar to\nthose seen during training. When introducing our\nsynthetic data, we observe performance gains in\nSpanish, German, and Latin-based languages, with\nmacro F1 scores above 0.8 for French and Italian.\nFor languages linguistically distant from the train-\ning set, the performance drops more noticeably.\nHowever, the model demonstrates strong general-\nisability, achieving reasonable performance of F1\nscores ranging between 0.6 and 0.8."}, {"title": "Cross-Model Comparison Results", "content": "As a final\nset of experiments, we report the results of compar-\ning our model with five powerful LLMs (GPT-40,\nGPT-40-mini, Mistral-7B, Llama3.3 and Phi4). In\nthis evaluation, we down-sampled dataset sizes to\n10,000 when necessary and included all 25 lan-\nguages in the original X-Fact dataset. The prompt\nused for the evaluation is shown in Appendix A.6.\nResults in Table 9 show that our model outperforms\nLLMs across all scenarios. Moreover, GPT models\nperform poorly on the X-Fact dataset in comparison\nto NLI-Fever and VitaminC datasets. This demon-\nstrates that fact-checking remains challenging for\nLLMs and that developing specialised models re-\nmains a better strategy for fact verification.\nWe further expand the cross-model evaluation of\nfact-checking by looking at the model performance\nin the different languages included in X-Fact. As\nshown in Table 10, adding the synthetic dataset (in-\ncluding Spanish, English and German) to the fine-\ntuned mdeberta-base improves the performance of\nthe model in Spanish and closely related languages\nsuch as Portuguese or Italian. In this particular\ncase, it did not improve in German, although it did\nnot hurt the performance either. We hypothesise\nthat this is because the German data in X-Fact cov-\ners different topics from our MultiSynFact. Future\nwork can explore methods for improvements in\nspecific target languages. In general, training with\nMultiSynFact enhances the overall performance\nwith respect to not using it and to the LLMs we\ntested. We report further details of model com-\nparison across three transformer-based models in\nAppendix A.7."}, {"title": "Discussion and Conclusion", "content": "This paper tackles the problem of data scarcity for\nonline misinformation in multilingual scenarios.\nWe present an automatic pipeline to curate and val-\nidate multilingual claims grounded on Wikipedia.\nLeveraging LLMs and automated filtering tech-\nniques, we release 2.2M claim-source pairs for\nSpanish, German, and English languages to facil-\nitate and further fact-checking research. Through\nour experiments, we observe consistent improve-\nments in all tested datasets after introducing our\nsynthetic data. This confirms that our generation\npipeline with filtering strategies generates high-\nquality claims, bringing a step towards developing\nrobust fact-checking models. Moreover, the impact\nof the synthetic data is more evident when training\ndata is limited - we observe more prominent F1 im-\nprovement in X-Fact (1,784 instances on average\nper language) than in NLI-Fever and VitaminC (at\nleast 144k instances in the training set).\nOur experiments help understand the impact of\nsynthetic data, providing a strong baseline for mul-\ntilingual fact-checking, even for unseen languages.\nAs our generation pipeline requires little human\nintervention, it can be easily deployed for generat-\ning diverse and novel claims with up-to-date evi-\ndence. The promising results highlight future di-\nrections for developing fact-checking systems for\nlow-resource languages."}, {"title": "Limitations", "content": "Our work addresses the lack of resources for devel-\noping multilingual fact-checking systems by gen-\nerating synthetic datasets while reducing human\neffort. However, it faces several limitations.\nFirstly, our claims are generated based on sen-\ntences retrieved from Wikipedia, similar to pre-\nvious work on dataset creation for fact-checking\n(Thorne et al., 2018; N\u00f8rregaard and Derczynski,\n2021). Whilst Wikipedia is considered a public\ntrustworthy source with reliable information and\ncovers a broad range of topics and domains, it may\ncontain unintended biases or false/undesired con-\ntent (arising from the 'wisdom of the crowd') or\ninformation about individual names available on\nWikipedia. Such information may be reflected in\nthe dataset. Adding diverse resources (e.g., news\nand fact-checking websites validated by journalists\nor human fact-checkers) could reduce potential bi-\nases and ensure the veracity of claims generated.\nAlternatively, applying toxicity classifiers could\nhelp identify potential offensive content to be re-\nmoved.\nSecondly, as mentioned in Section 3.1, our au-\ntomatic source retrieval creates suboptimal factual\nsentences for claim generation. This may pose\nchallenges in training robust fact-checking systems.\nFinally, we use a specific LLM (Mistral-7B) to gen-\nerate claims due to its multilingual capability. The\nfindings may not be generalised across models. We\nconducted an extensive evaluation comparing mod-\nels trained on our dataset with existing real datasets\nto understand the robustness of our approach."}, {"title": "Ethical Considerations", "content": "Tackling misinformation is an urgent task. As our\npaper aims to provide tools for efficiently creating\nhigh-quality fact-checking data for multiple lan-\nguages using AI techniques, several societal and\nethical consequences should be carefully managed.\nWhile our approach based on language models\ncan scale up quickly across domains and languages,\nthey can fail, such as making incorrect predictions,\ngenerating unfaithful texts, and potentially induc-\ning unintended social biases (in both generation and\nclassification tasks). These issues can be mitigated\nby, for instance, measuring the factual consistency\nbetween generated claims against retrieved sources\n(Min et al., 2023; Zha et al., 2023) and adding bias\ntriggers into prompts for generation (Sheng et al.,\n2020). Furthermore, as a first step towards creating\nmultilingual synthetic datasets for fact-checking,\nwe tested our pipeline for Spanish, German, and\nEnglish, which are high-resource languages. It is\ncrucial to investigate the effectiveness of our ap-\nproach for low-resource scenarios for future work.\nTo facilitate research on multilingual fact-checking,\nour data and implementation are available under\nthe license CC BY-NC 4.0."}, {"title": "Generation Examples", "content": "Source: El nevado, que es el punto central del Parque\nNacional Natural Nevado del Huila, est\u00e1 dividido terri-\ntorialmente entre los departamentos de Huila, Tolima y\nCauca, siendo su cima el punto m\u00e1s elevado de los tres.\nEN: The snow-capped mountain, which is the central point\nof the Nevado del Huila National Natural Park, is territori-\nally divided between the departments of Huila, Tolima and\nCauca, its summit being the highest point of the three.\nSupports: El Nevado del Huila es territorialmente dividido\nentre los departamentos de Huila, Tolima y Cauca.\nEN: The Nevado del Huila is territorially divided between\nthe departments of Huila, Tolima and Cauca.\nRefutes: La cima del Nevado del Huila es la puntalina m\u00e1s\nbaja de los tres departamentos en que se encuentra.\nEN: The summit of Nevado del Huila is the lowest point of\nthe three departments in which it is located.\nNot-info: El Nevado del Huila es el punto m\u00e1s alto entre\nlos puntos elevados de la Cordillera Central en Colombia.\nEN: The Nevado del Huila is the highest point among the\nhigh points of the Cordillera Central in Colombia.\nTable 11: Generation example of Spanish claims for\neach class.\nSource: Durch den Britisch-Niederl\u00e4ndischen Vertrag von\n1814 fiel Berbice an Gro\u00dfbritannien.\nEN: Berbice fell to Great Britain as a result of the British-Dutch Treaty of 1814.\nSupports: Berbice ist nach dem Vertrag von 1814 zu\nGroAYbritannien gefallen.\nEN: Berbice fell to Great Britain after the Treaty of 1814.\nRefutes: Berbice wurde 1814 durch den Britisch-\nNiederl\u00e4ndischen Vertrag an die Niederlande zur\u00fcck-\ngegeben.\nEN: Berbice was returned to the Netherlands in 1814 by\nthe British-Dutch Treaty.\nNot-info: Die Menge von Niederl\u00e4ndisch-Berbice-\nSiedlungen war gr\u00f6\u00dfer als die der britischen Siedlungen,\nbevor Berbice an Gro\u00dfbritannien fiel.\nEN: The number of Dutch Berbice settlements was greater\nthan that of the British settlements before Berbice fell to\nGreat Britain.\nTable 12: Generation example of German claims for\neach class."}, {"title": "Generation Prompts", "content": "We present the prompts for generating refuting and\nnot-info claims in Tables 13 and 14."}, {"title": "Dataset Statistics", "content": "Table 15 presents the statistics of our datasets with-\nout MNLI filtering using automatic metrics."}, {"title": "Test Datasets Pre-Processing", "content": "The processing of VitaminC (extracted from Hug-\ngingFace) is direct. We take the \u2018evidence' field as\nthe source and maintain the classes 'SUPPORTS',\n'REFUTES' and 'NOT ENOUGH INFORMA-\nTION' as in our pipeline.\nX-Fact dataset is also obtained from Hugging-\nFace7. We reorganise its original classes in three\nregarding our setup: 'true', 'partly true/misleading'\nand 'mostly true' are relabeled as \u2018supports'; 'false'\nis redirected to the 'refutes' class; and the rest is\nconsidered 'not-info'. Regarding the source, we\nconcatenate all evidence fields (usually from 1 to\n5) found on each claim except those containing the\ntext '<DUMMY_EVIDENCE>', as a unique input.\nNLI-Fever, also found in HugginFace8, has a\ndifferent distribution of classes regarding the NLI\nstyle. We frame 'entailment' as 'supports', 'contra-\ndiction' as 'refutes' and 'neutral' as 'not-info'. The\npremise is selected as the claim and the hypothesis\nis considered the evidence source."}, {"title": "Model Training", "content": "We adopt a learning rate of le-6, a warm-up pro-\nportion of 0.06, Adam epsilon of le-6 and an ac-\ncumulated gradient batch size of 2. To prevent\noverfitting, we included early stopping with a pa-\ntience of 2 epochs. Regarding time executions, the\nlongest training finishes with 9 epochs and takes\n25 hours."}, {"title": "Generation Prompt for Cross-model and\ncross-language evaluation", "content": "We include in Table 16 the prompt used to evalu-\nate fact-checking using LLMs shown in Tables 9\nand 10."}, {"title": "Transformer Comparison", "content": "Table 17 presents results for fine-tuning three\ntransformer-based models on the synthetic\ndataset: ROBERTa-large, DeBERTa-large"}, {"title": "Ablation studies", "content": "To understand the impact of applying MNLI filter-\ning in data generation, we conduct ablation studies\ntraining the mDeBERTa-v3-base model on Vita-\nminC datasets, incorporating our synthetic data\nwithout MNLI filtering (no-MNLI). As shown in\nTable 18, the models trained with MNLI filtering\nsurpass the ones trained without MNLI filtering\nfor all scenarios except for English. This suggests\nemploying MNLI filtering is beneficial for curating\nbetter-quality multilingual fact-checking datasets."}]}