{"title": "Markov Chain of Thought for Efficient Mathematical Reasoning", "authors": ["Wen Yang", "Kai Fan", "Minpeng Liao"], "abstract": "Chain of Thought (CoT) of multi-step benefits from the logical structure of the reasoning steps and task-specific actions, significantly enhancing the mathematical reasoning capabilities of large language models. As the prevalence of long CoT, the number of reasoning steps exceeds manageable token limits and leads to higher computational demands. Inspired by the fundamental logic of human cognition, \u201cderive, then reduce\", we conceptualize the standard multi-step CoT as a novel Markov Chain of Thought (MCOT). In this study, we consider the mathematical reasoning task, defining each reasoning step as text accompanied by a Python code snippet. To facilitate a longer reasoning path, self-correction is enabled through interactions with the code interpreter. Our MCOT aims to compress previous reasoning steps into a simplified question, enabling efficient next-step inference without relying on a lengthy KV cache. In our experiments, we curate the MCoTInstruct dataset, and the empirical results indicate that MCoT not only significantly enhances efficiency but also maintains comparable accuracy. While much remains to be explored, this work paves the way for exploring the long CoT reasoning abilities of LLMs.", "sections": [{"title": "Introduction", "content": "With the rapid advancement of large language models (LLMs), these models have demonstrated remarkable progress in a wide range of language tasks (Brown et al., 2020; Ouyang et al., 2022; Taori et al., 2023; Yang and Klein, 2021). However, they still face significant challenges when engaging in complex and symbolic reasoning tasks, particularly in mathematical reasoning (Cobbe et al., 2021; Hendrycks et al., 2021).\nMost existing works have sought to enhance the mathematical reasoning capabilities of LLMs."}, {"title": "Markov Chain of Thought Enable Mathematical Reasoning", "content": "For mathematical problem q, we assume that with each successful derivation step, the original problem can be incrementally simplified into a series of less complex problems, eventually leading to the final answer a. Concretely, if we denote the original problem by q1 and the first derivation step as s1, we can define the generation of new problems as p(qt|qt\u22121, St\u22121). Consequently, the subsequent derivation step relies entirely on the newly formulated question. This indicates that the process adheres to the Markov property, which implies memorylessness (i.e., the future state depends only on the current state and not on the sequence of events that preceded it).\n\n\np(st|qt\u2032<t, St\u2032<t) = p(st|qt)"}, {"title": "Markov Chain of Thought Reasoning", "content": "For a question with T derivation steps, we are interested in maximizing the log-likelihood of the joint distribution of all steps. With the above assumption, we have the following objective.\n\n\nL = log p(a, S1:Tq1)\n= log (p(s1/q1) Eq2:T [P(a/qT, ST)\n\nT\nx[P(St/qt) P(qt qt-1, St-1)])\n\nt=2\n\n\nHowever, this objective is intractable due to the requirement of integrating latent variables q2:T. As a surrogate, we turn to Monte Carlo integration, employing sampling techniques for feasibility. When we set the sampling size to 1, and if q2:7 represents the sequence of sampled reduction questions of intermediate derivation steps, the objective can be approximated as follows.\n\n\n(\nT\nL\u2248 log p(s1|q1)p(a|\u1fb7r, ST) [p(st\u012bt)p(\u0101t|qt-1, St-1)\n\nt=2\n\nDenote q1 = 91,\n\n\nT\n= log p(al\u0105, ST) + \u2211log p(stat)\n\nt=1\n\n+\nT-1\n\u2211logp(qt+1 qt, St)\n\nt=1\n\n\n= log p(st, al\u1fb7r) + \u2211logp(st, \u0105t+1|\u0101t)\n\nDenote a = qT+1,\n\n\nT\n= \u2211logp(st, \u0105t+1 \u1fb7t)\n\nt=1\n\n\n(3)\n\nThe first equation unfolds the approximated loss into the summation of 2T independent log-likelihoods. The second equation applies expression re-organization with the rule of conditional probability p(st|qt)p(qt+1|qt, St) =\np(St, qt+1/qt), resulted in T new independent log-likelihoods, signifying that these components can be optimized independently. The third equation is to rewrite the loss into a more concise representation. To sum up, if multi-step reasoning can be transformed into multiple independent single-step reasoning, the training and inference become very efficient. This is also the core intuition to build our dataset."}, {"title": "MCoTInstruct Dataset", "content": "Our MCoTInstruct dataset is comprised of two components: the seed data, denoted as Dseed,\nand the augmented self-distillation data, referred to as Dself. To construct our dataset, we start from an available multi-step reasoning dataset, denoted as Dorigin, which includes the multi-step solutions from GSM8k and MATH datasets that have been further refined through GPT-4 annotations, e.g., MathCodeInstruct (Wang et al., 2023a). The data format of Dorigin is represented as T\u2081 = (q1, S1:T, a), where a stands for the final answer, and st signifies the intermediate derivation step at time t. Particularly, we assume that T > 1, implying the solution includes at least one derivation step. Furthermore, this derivation step adheres to the REACT (Yao et al., 2022) style, with customized  format that integrates text analysis with executable code blocks within the process of crafting a response, effectively enhancing the precision of reasoning.\nSeed data To fully leverage available data resources, we have established a reproducible pipeline that iteratively extracts the required seed training instance and updates the multi-step reasoning dataset Dorigin. Algorithm 1 presents the overall pipeline designed to generate seed dataset.\nFirst, we train a model, denoted as Mverify, using the initially original dataset Dorigin. This model serves dual purposes: generation and verification. In the generation phase, Mverify produces multiple multi-step solution samples for a given reduction question. During verification, if any final answer within these sampled solutions aligns with the answer to the original question, the corresponding reduction question is deemed acceptable.\nThen, given an instance (q1, S1:T, a) in Dorigin,\nif T = 1, we directly incorporate this triplet (q1, S1, a) into our seed dataset. For other cases, we employ GPT-4-1106-preview (Achiam et al., 2023) to produce the reduction question q2 from (q1, 51). The details of GPT-4 prompt can be found in Appendix H. We employ Mverify to assess whether q2 can yield the correct answer. If the outcome is accurate, it demonstrates that the reduction question is independent and does not rely on information from the previous questions and derivation steps, satisfying the required Markovian property. In this case, we include triplet (q1, S1, q2) in the seed dataset, and a new multi-step reasoning pathway is generated as T2 = (q2, $1.7v, a), which is then updated to the original dataset for the next round construction. This implies that we continuously iterate through the process to generate triplet data until we no longer obtain any multi-step reasoning solutions with a length exceeding 2. Unlike previous works (Yue et al., 2023; Gou et al., 2023; Wang et al., 2023a; Liao et al., 2024), our approach relies solely on GPT-4 for generating a new question, typically a single sentence, instead of crafting a complete solution that includes text analysis and code snippets. Consequently, this method incurs significantly lower additional costs.\nSelf-distillation We fine-tune the DeepSeekMath 7B base model on the above seed data to obtain the MCoTModel-initial. Recognizing the limited scope of our seed data on the MATH dataset, we have adopted a self-distillation approach to substantially enhance the coverage and diversity of the dataset. We employ MCoTModel-initial due to its capability to generate MCoT paths, achieving accuracy rates of 77.10% and 53.48% on the GSM8K and MATH datasets, respectively. Given  pairs from the training sets of GSM8K and MATH, the initial model can generate Markov reasoning paths and obtain answers. We will verify the answers to form a self-distillation dataset Dself. Furthermore, by utilizing MCoTModel-initial with  pairs from any dataset, we can create a self-distillation dataset. In the Appendix E.1, we provide a detailed analysis of the impact of self-distillation.\nCombining seed data Dseed and self-distilled data Dself, we remove duplicate entries to form the MCoTInstruct dataset, which is denoted as D = filter({Dseed, Dself}). The dataset comprises 82k Markov chains, totaling around 160k entries,"}, {"title": "Discussion", "content": "The insight of MCoT is Markov chain, which frames the question as a particular state and considers the derivation step as an associated action. Unlike Multi-Step Reasoning (MSR), the MCOT does not depend on historical derivation steps; Furthermore, while question decomposition (Zhou et al., 2022; Dua et al., 2022; Huang et al., 2023; Radhakrishnan et al., 2023) requires aggregating partial answers from each sub-question, any reduction question within MCoT can directly yield the final answer. We have provided a detailed comparison of the MCoT with MSR and question decomposition in Figure 5 in the Appendix.\nThe core of MCOT lies in the Markov property, which implies memorylessness, the future state depends only on the current state, not on the sequence of events that preceded it. On one hand, we utilize Markov property to ensure that any state (or question) within a Markov Chain of Thought can directly lead to the final answer, significantly boosting efficiency within MCoT. On the other hand, when an error arises in an intermediate state, the memoryless feature of MCoT might propagate it, leading to errors in subsequent states as well. In contrast, traditional MSR is capable of accessing all previous historical contexts, potentially allowing it to correct intermediate errors. However, in our experiments, we empirically observed that MSR does not necessarily correct more intermediate errors than our proposed MCoT approach."}, {"title": "Experiments", "content": "Implementation Details We fine-tune DeepSeekMath-Base (Shao et al., 2024), LLemma (Azerbayev et al., 2023), and LLama-3 (AI@Meta, 2024) series (ranging from 7B to 70B) on the MCoTInstruct to evaluate the efficacy and accuracy of our MCoT framework. The implementation details are described in Appendix D.3."}, {"title": "Main Results", "content": "Table 1 demonstrates our models outperform other open-source competitive math-solving models, exhibiting a clear advantage across both in-domain datasets and out-of-domain datasets. Our model MCOT-DeepSeek is fine-tuned from the DeepSeekMath-Base7\u00df on the MCoTInstruct dataset. Compared to the base model, our model achieves substantial gains on the GSM8k and MATH datasets, with improvements of about 12% and 24%, respectively. MCoT-DeepSeek achieve state-of-the-art results across all datasets, in 7B models.\nMoreover, our model MCOT-DeepSeek achieves 55.8% on MATH dataset, which surpasses all 34B and 70B models on MATH dataset without any extra strategies, such as majority voting (Wang et al., 2022b). Notably, during the training phase in\nMCOT, the model is exposed only to triplet data like (qt\u22121, St\u22121, qt) or (qr, st, a). It has never been trained on the complete solution data. However, when a question is presented to the model during the inference stage, it first attempts a single-step solution before deciding whether to further reduce the problem or deliver the final answer. In the reasoning stage, the model leverages the fragmented knowledge acquired during training to construct a complete Markov chain reasoning process from question to final answer.\nEfficiency To intuitively evaluate the efficiency of different reasoning methods, we compare the performance of multi-step reasoning (MSR) and MCOT reasoning. To ensure a fair comparison, all reasoning approaches utilize an external tool - Python code interpreter. The MSR model is fine-tuned on our initial multi-step reasoning dataset, Dorigin. The MCoT model is fine-tuned on our MCOTInstruct dataset. The MCoTInstruct dataset is extended from Dorigin, with both the source data and solutions preserved as consistently as possible to minimize the impact of dataset variations. Moreover, the maximum number of reasoning steps is set to 8.\nTable 2 presents the comparative results of MSR and MCOT regarding reasoning efficiency and accuracy. Our observations are as follows: (1) Compared to single-step reasoning in Table 1, MSR and MCOT indeed significantly enhance reasoning accuracy. (2) Compared to MSR, MCoT demonstrates notable improvements in reasoning accuracy and efficiency. For instance, in the Llemma7\u00df model, MCOT achieves improvements over MSR on both the MATH (+2.1%) and GSM8K (+2.2%) datasets. Furthermore, MCoT's inference efficiency from E is 1.90 times greater than that of MSR.\nDifferent base models and model sizes We evaluated the effectiveness of the MCoT approach using three base models: DeepSeek, Llemma, and the llama3 series. Tables 1 and 2 show that MCOT outperforms multi-step reasoning in accuracy across all models, with significantly smaller cache memory usage. To investigate the effectiveness across varying model scales, we scale up the Llama3 model size from 8B to 70B, observing notable performance improvements on all benchmarks. Specifically, we found that the Llama3 series performs poorly overall on the OCW dataset. In our analysis of the OCW dataset, detailed in Appendix D.1, we found that its format causes Llama3 to mistak-"}, {"title": "Analysis 1: Efficiency", "content": "Efficient Training Figure 3a illustrates the distribution of token length for the MCoTInstruct dataset and the multi-step reasoning instruction dataset Dorigin. In comparison, the token length of MCOT is noticeably shorter than that of MSR, with an average reduction of 135.91 tokens. The underlying reason is that MCoT is only trained on\nthe (qt-1, St-1, qt) triplets, whereas MSR requires solving a broader range of problems across various subjects. More details and results w.r.t. other base models, such as Deepseek and Llama, are shown in Figure 8 and 9 in Appendix E.2."}, {"title": "Analysis 2: Problem Solving", "content": "To assess MCoT's problem-solving capabilities, we analyze its performance on the MATH test set across various difficulty levels and subjects, calculating the success rate for each category. In Figure 4a, it is demonstrated that MCoT achieves a higher success rate in solving more challenging problems compared to MSR. We attribute this superior performance to MCoT's training method, which emphasizes derivation and reduction techniques rather than the complete reasoning path from question to answer. This approach enhances the model's generalization ability through autonomous and iterative problem-solving. Figure 4b shows that MCoT consistently excels in"}, {"title": "Analysis 3: Hybrid training strategy", "content": "The MCOT approach notes that the model is not trained on full solution datasets, limiting its ability to generate complete solutions independently. To address this, we explore a hybrid training method that partially exposes the model to full solutions to assess performance improvements. We used the DeepSeekMath-Base-7B model, starting with a 2-epoch warm-up on a 27.4k multi-step reasoning dataset from GSM8K and MATH, followed by training on the MCoTInstruct dataset.\nTable 3 shows that the hybrid training improves in-domain performance but performs worse on OOD datasets compared to direct MCoT training. This may be due to overfitting caused by the inclusion of complete solution data, leading to weaker generalization. We will leave more exploration on how to train the LLM with mixed data as future work."}, {"title": "Related Work", "content": "Chain of Reasoning LLMs have exhibited strong reasoning capabilities by utilizing Chain of Thought (Wei et al., 2022; Brown et al., 2020)\nprompting. Tree of Thoughts (ToT) (Yao et al., 2024) enables exploration over coherent units of thoughts that serve as intermediate steps toward problem-solving. Program of Thought (PoT) (Chen et al., 2022) enhances the capabilities of LLMs to use programs as thought processes. Several works (Zhou et al., 2022; Wang et al., 2022a; Li et al., 2023; Wang et al., 2023c,b) have developed CoT or PoT technology to employ LLMs to tackle reasoning tasks by allowing intermediate steps. It is important to note that in these methods, the intermediate steps are preserved as historical context, making them dependent. In contrast, our approach leverages the inherent independence of the Markov chain to separate the intermediate steps.\nMathematical Reasoning Recent works (Wang et al., 2023a; Gou et al., 2023; Liao et al., 2024; Lu et al., 2024) have made significant advancements in enhancing reasoning capabilities within LLMs through the implementation of step-by-step natural language reasoning, achieving better results than single-step reasoning (Luo et al., 2023; Yu et al., 2023; Yue et al., 2023). In single-step reasoning, (Luo et al., 2023) and (Yu et al., 2023) utilize tex-"}, {"title": "Conclusion", "content": "This paper presents MCoT, an innovative Markov Chain of Thought framework for efficient multi-step reasoning. Our framework leverages the independence of Markov chains, conceptualizing the solution process as a series of state transitions. This approach enables LLMs to address complex reasoning tasks more efficiently and intelligently. MCOT"}, {"title": "Future Work", "content": "MCOT is a framework that empowers LLMs to more efficiently and intelligently in multi-step reasoning. MCOT framework can be applied to complex reasoning tasks, such as long context reasoning (Xiong et al., 2023; Caciularu et al., 2023; Chen et al., 2023). MCoT is capable of effectively reducing context information, thereby providing a practical and feasible approach for long context reasoning. This method filters and concentrates historical information, significantly improving the efficiency of processing and analyzing long context. Moreover, when integrated with the Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Gao et al., 2023b) technology, MCoT holds significant potential in understanding long context."}, {"title": "Multi-step reasoning, Question decomposition and MCOT", "content": "Figure 5 illustrates various approaches to problem-solving with LLMs and their reasoning efficiency."}, {"title": "MCoTInstruct Dataset", "content": ""}, {"title": "Overview", "content": "Table 6 presents the detail information of different mathematical reasoning datasets. We list recently popular mathematical reasoning datasets. The #Annotation Num refers to the number of samples annotated during the dataset construction process. The #Annotation Type represents three categories: solution, trajectory, question. \"solution\" denotes the annotation of a solution, \"trajectory\" refers to the multi-step reasoning path, and \"question\" signifies the annotation of the reduced question.\nUnlike previous works (Yue et al., 2023; Gou et al., 2023; Wang et al., 2023a; Liao et al., 2024), our approach requires only the use of GPT-4 for generating a new question, instead of crafting the complete solution that includes text analysis and code snippets. The number of questions that need to be generated is 29k."}, {"title": "Data Format", "content": "Figure 6 displays the format of Markov Chain of Thought reasoning. We decompose the Markov chain into tuples of (q1, s1, q2) and (q2, s2, a) to form the MCoTInstruct dataset."}, {"title": "Experimental Details", "content": ""}, {"title": "Test Dataset", "content": "We report the information of four test datasets in Table 4. Notably, the four datasets have obvious differences in difficulty and question types and MATH dataset has diverse categories, thereby ensuring the richness and diversity of the testsets. OCWCourses is a set of 272 STEM problems designed for college students, with most questions needing a few steps to solve. The GaoKao2023-Math-En dataset includes 385 math problems drawn from the 2023 Chinese National College Entrance Examination, the 2023 American Mathematics Competitions, and the 2023 American College Testing. These two OOD datasets are even more tricky to solve than the MATH dataset.\nNotably, OCW dataset has a somewhat unique question format, which includes a premise, alongside sub problem and their respective solutions, as shown in Figure 7."}, {"title": "Baselines", "content": "This comparison included notable models such as OpenAI's GPT-4 (Achiam et al., 2023) and ChatGPT, Google's PaLM-2 (Anil et al., 2023), along with Llama3, Llama2 (Touvron et al., 2023), Llemma (Azerbayev et al., 2023), and CodeL-lama (Roziere et al., 2023). To establish a fundamental reasoning method baseline, we initially considere Chain of Thought (CoT) prompts (Wei et al., 2022). Additionally, given our methodology's reliance on the Python code interpreter, we also evaluate the Program of Thought (PoT) (Chen et al., 2022) and Program-aided Language (PAL) model (Gao et al., 2023a).\nFor supervised fine-tuning (SFT) models, we categorize them into single-step reasoning: Mammoth (Yue et al., 2023), DeepSeekMath-Instruct (Shao et al., 2024) and multi-step reasoning: MathCoder (Wang et al., 2023a), ToRA (Gou et al., 2023), MARIO (Liao et al., 2024) and Math-Genie (Lu et al., 2024)."}, {"title": "Fine-tuning Details", "content": "In this work, we finetune all models using the LLaMA-Factory (Zheng et al., 2024) repository. During this optimization phase, we set the global batch size at 512, the learning rate at 2e-5, and used a cosine learning rate scheduler that included a warm-up phase constituting 3% of the total training duration, spread over 3 epochs. All models are optimized employing AdamW (Kingma and Ba, 2014). Training for all models was launched with the accelerate (Gugger et al., 2022) in DeepSpeed ZERO Stage2 (Rajbhandari et al., 2021) and Flash-Attention 2 (Dao, 2023) mechanism. The 7B/8B and 70B models are fine-tuned on 8 and 32 NVIDIA A100 80GB GPUs, respectively."}, {"title": "Additional Results and Analyses", "content": ""}, {"title": "The Impact of Self-distillation", "content": "Table 5 illustrates the impact of self-distillation. It can be observed that self-distillation enhances the coverage of the training sets and elevates the accuracy of the test sets. Specifically, the application of self-distillation led to a 5.5% increase in data coverage on the MATH training dataset. Furthermore, it achieved a notable 2.5% improvement in accuracy on the MATH test set and a 0.83% boost in accuracy on the GSM8K test set."}, {"title": "The Analysis on Problem-Solving", "content": "Figure 8 and Figure 9 illustrate the problem-sloving capabilities of MCoT-DeepSeek and MCoT-Llama3, respectively. In terms of difficulty levels as shown in 8a and 9a, MCoT outperforms MSR on levels 1 through 4 and is nearly on par with MSR at level 5. In terms of different subjects as shown in 8b and 9b, We discovered that, when using DeepSeek and Llama3 as base models, MCOT consistently outperforms MSR, particularly in the fields of Algebra, Counting & Probability, and Intermediate Algebra."}, {"title": "Dataset License", "content": "The MCoTInstruct dataset is built on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). We strictly abide by the corresponding data licenses."}, {"title": "Case Study", "content": "To qualitatively analyze responses from Markov Chain of Thought reasoning, we report three cases with MCoT-DeepSeek. In cases 1 and 2, We observed that MCoT does possess some self-correction capabilities. In case 3, during the reduction process, MCoT lost some information, causing the reduced problem to no longer contain all the information of the original problem."}, {"title": "Case 1: Self-correction", "content": "The Case 1 about MCoT's self-correcting capability\nidx: \"math_test_74\" level: \"Level 3\" type: \"Algebra\" standard answer: \"\\frac{19}{4}\""}, {"title": "Case 2: Self-correction", "content": "The Case 2 about MCoT's self-correcting capability\nidx: \"math_test_122\" level: \"Level 5\" type: \"Algebra\" standard answer: \"3\""}, {"title": "Case 3: Bad case", "content": "The bad case\nidx: \"math_test_1601\" level: \"Level 4\" type: \"Counting & Probability\" standard answer: \"50\""}, {"title": "Prompt Template in GPT-4", "content": "We provide prompts for generating simplified questions using GPT-4 on the MATH dataset and the GSM8K dataset, respectively."}]}