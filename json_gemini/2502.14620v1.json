{"title": "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity", "authors": ["Xinghan Pan"], "abstract": "This paper investigates the efficacy of RWKV, a novel language model architecture known for its linear attention mechanism, for generating sentence embeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate the semantic similarity captured by embeddings from different hidden layers of a pre-trained RWKV model. The performance is assessed on the Microsoft Research Paraphrase Corpus (MRPC) dataset using Spearman correlation, and compared against a GloVe-based baseline. My results indicate that while RWKV embeddings capture some semantic relatedness, they underperform compared to the GloVe baseline in terms of Spearman correlation. Furthermore, I observe a trend of decreasing semantic similarity performance with increasing layer depth in RWKV. I also analyze the inference time and GPU memory usage, highlighting the computational trade-offs associated with RWKV embeddings. The findings suggest that while RWKV offers potential advantages in terms of linear scaling, its zero-shot sentence embedding quality for semantic similarity tasks requires further investigation and potential task-specific fine-tuning to match or exceed simpler baselines. I discuss the limitations of my current study and propose directions for future work, including exploring advanced pooling strategies, comparison with state-of-the-art models, task-specific fine-tuning approaches, and more in-depth layer-wise analysis to enhance RWKV's sentence embedding capabilities. To provide deeper insights, I also include a theoretical analysis of the RWKV architecture, discussing its linear attention, information propagation, pooling, and complexity from an information-theoretic perspective.", "sections": [{"title": "Introduction", "content": "Sentence embeddings, vector representations of sentences that capture their semantic meaning, are crucial for various natural language processing (NLP) tasks, including semantic similarity detection, text classification, and information re-trieval [6, 14]. Traditionally, Transformer-based models, such as BERT [7] and"}, {"title": "Related Work", "content": "Sentence embeddings have been extensively studied in NLP, with early ap-proaches relying on word averaging techniques using pre-trained word embed-dings like Word2Vec [17] and GloVe [18]. These methods, while computationally efficient and easy to implement, often fail to capture complex semantic relation-ships, word order, and sentence structure, limiting their effectiveness in tasks requiring deeper semantic understanding [25].\nThe advent of deep learning, particularly recurrent neural networks (RNNs) and Transformers, revolutionized sentence embedding generation. Models like Skip-Thought [14] and FastSent [11] utilized RNNs to encode sentences into vector representations, capturing sentence-level semantics beyond simple word averaging. However, Transformer-based models, especially BERT [7] and its variants, have achieved state-of-the-art performance on various semantic tasks, setting new benchmarks in sentence representation learning [20, 6]. Sentence-BERT (SBERT) [20] fine-tunes BERT with Siamese and triplet networks to produce semantically meaningful sentence embeddings that can be efficiently compared using cosine similarity, making it highly practical for semantic simi-larity search and clustering. Universal Sentence Encoder (USE) [4] also provides high-quality sentence embeddings. More recently, SimCSE [9] demonstrates ef-fective contrastive learning for sentence embedding. These models, however, inherit the computational cost associated with Transformer architectures, par-ticularly the quadratic complexity of the self-attention mechanism.\nDespite their success, Transformer models are computationally demanding due to their quadratic attention complexity, which becomes a bottleneck for processing long sequences and large-scale datasets [24]. This limitation has spurred research into more efficient architectures, including models with linear attention mechanisms and alternatives to attention [23, 13, 28, 27]. RWKV [19] is a notable example, employing a novel RWKV attention that offers linear scaling with sequence length, addressing the efficiency concerns of traditional Transformers while maintaining competitive performance in language modeling. RWKV's architecture, which blends RNN and Transformer principles, presents a unique approach to language modeling, offering a potentially more efficient alternative for various NLP tasks. While RWKV has demonstrated strong lan-guage modeling capabilities, its application to sentence embedding generation and semantic similarity tasks is still in its nascent stages. This paper aims to contribute to this emerging area by providing an empirical evaluation of RWKV for zero-shot semantic similarity, focusing on layer-wise analysis and comparison with a traditional baseline. Future work should extend this comparison to in-clude state-of-the-art sentence embedding models to fully contextualize RWKV's performance. This research explores a novel use case for this architecture be-yond its original language modeling focus and contributes to the broader field of efficient sentence representation learning."}, {"title": "Methodology", "content": ""}, {"title": "RWKV Model and Layer Exploration", "content": "I utilized the pre-trained RWKV-v6-Finch-1B6-HF model [21] from Hugging Face Transformers [26]. This model, based on the RWKV architecture, is trained on a large corpus of text data and provides a readily available resource for exploring sentence embedding generation. The choice of RWKV-v6-Finch-1B6-HF was motivated by its relatively smaller size, allowing for experimentation within the computational constraints of Google Colab, while still representing the core RWKV architecture and enabling efficient experimentation.\nTo investigate the role of different layers in capturing semantic information, I implemented a layer exploration strategy, inspired by prior work analyzing layer-wise representations in deep learning models [2, 12]. I extracted sentence embeddings from specific hidden layers of the RWKV model, namely layers 1, 3, 5, 7, 9, and 11. These layers were chosen to represent a range from the initial to deeper parts of the network, allowing me to observe potential shifts in semantic representation across network depth. For each layer, I obtained the hidden states and computed sentence embeddings by averaging the hid-den states across all tokens in the sentence. This average pooling method is a common and simple approach for deriving sentence embeddings from word-level representations [1, 25], and serves as a starting point for evaluating RWKV's em-bedding capabilities. I acknowledge that more sophisticated pooling methods, as explored in prior research [20, 16], such as max pooling, weighted pooling, or CLS token-based approaches, could be explored in future work to potentially enhance performance."}, {"title": "Baseline Model: GloVe Embeddings", "content": "As a baseline, I employed GloVe embeddings [18], a widely used pre-trained word embedding model known for its efficiency and reasonable performance in various NLP tasks. I used the 50-dimensional GloVe vectors trained on the 6B word corpus, readily available and widely adopted in sentence embedding research, providing a strong and interpretable baseline. Sentence embeddings were generated by averaging the GloVe vectors of all words in a sentence. Words not found in the GloVe vocabulary were assigned a zero vector, a standard practice in word embedding averaging approaches. This GloVe-based approach represents a computationally efficient and well-established method for sentence embedding, serving as a robust and interpretable baseline for comparison, par-ticularly in zero-shot scenarios where fine-tuning is not involved, allowing me to isolate the intrinsic semantic representation capabilities of RWKV."}, {"title": "Dataset and Task", "content": "I evaluated the models on the Microsoft Research Paraphrase Corpus (MRPC) dataset [8], part of the GLUE benchmark [10]. MRPC is a binary classifica-"}, {"title": "Evaluation Metrics", "content": "I used Spearman correlation as the primary evaluation metric to quantify the monotonic relationship between the cosine similarity of sentence embeddings and the MRPC paraphrase labels. Spearman correlation is suitable for this task as it measures the rank correlation, which is robust to non-linear relationships and outliers, and is commonly used in semantic similarity evaluations, providing a reliable measure of the alignment between embedding similarity and human judgments [20, 4]. A higher Spearman correlation indicates a stronger alignment between the semantic similarity captured by the embeddings and the human-annotated paraphrase judgments, reflecting better performance on the semantic similarity task.\nIn addition to Spearman correlation, I measured the inference time for gen-erating sentence embeddings for both RWKV and the GloVe baseline. Inference time was measured as the average time taken to process a sentence pair, provid-ing a direct measure of computational speed and efficiency. I also recorded the peak GPU memory usage during embedding generation to assess the resource consumption of each method, allowing for a comprehensive evaluation of effi-ciency alongside performance and providing insights into the practical resource requirements of each approach."}, {"title": "Experiments and Results", "content": ""}, {"title": "Experimental Setup", "content": "Experiments were conducted on a Google Colab environment with a Tesla T4 GPU, a common and accessible platform for NLP experimentation, ensuring reproducibility and accessibility of my findings. The specific hardware and soft-ware environment details are provided in the Appendix. I loaded the pre-trained"}, {"title": "Quantitative Results: Spearman Correlation", "content": "The results consistently demonstrate that the GloVe baseline achieves a higher Spearman correlation than any of the RWKV layers on both the training and validation sets. Notably, within the RWKV model, there is a discernible trend of decreasing Spearman correlation as layer depth increases. Layer 1 con-sistently exhibits the highest correlation, suggesting that earlier layers of RWKV may be more effective at capturing semantic similarity in this zero-shot setting. The peak Spearman correlation achieved by RWKV (Layer 1 on the validation set, 0.3498) is still significantly lower than the GloVe baseline (0.4326), indi-cating a considerable performance gap in terms of semantic similarity as mea-sured by correlation with MRPC labels. While I report Spearman correlation coefficients, I acknowledge that future work should include statistical signifi-cance analysis and standard deviation to assess the robustness of these findings. This trend suggests that deeper layers of RWKV, while potentially beneficial for language modeling, may not inherently learn semantic representations that are directly transferable to zero-shot paraphrase detection, at least with simple"}, {"title": "Inference Time and GPU Memory Usage", "content": "The inference time results reveal a stark contrast in computational efficiency. RWKV's average inference time per sentence pair is approximately two orders of magnitude higher than the GloVe baseline on the training set, and still con-siderably higher on the validation set, despite the validation set having fewer samples. This demonstrates that while RWKV boasts linear time complexity in theory, the practical inference cost for generating sentence embeddings re-mains significantly greater than that of a simple word embedding averaging approach, especially for shorter sequences as in the MRPC dataset. In terms of GPU memory usage, both RWKV and GloVe exhibit comparable peak memory consumption in my experiments, suggesting that memory footprint is not the primary differentiating factor in their computational profiles for this task, and that the linear complexity advantage of RWKV may not translate to reduced memory usage in this specific scenario."}, {"title": "Qualitative Analysis", "content": "Qualitative examination of similarity scores for sample sentence pairs (see Ap-pendix) provides further context to the quantitative results. Both RWKV and GloVe tend to assign high cosine similarity scores to sentence pairs, even when they are not labeled as paraphrases in the MRPC dataset. For example, Samples 2 and 5, which are labeled as non-paraphrases (label 0), still receive high sim-ilarity scores from both methods. This observation indicates that while both approaches capture a degree of semantic relatedness, they may lack the sen-sitivity to discern subtle semantic nuances necessary for accurate paraphrase detection in a zero-shot setting. This could explain the relatively low Spearman"}, {"title": "Discussion", "content": "The experimental findings present a nuanced picture of RWKV's suitability for zero-shot sentence embedding generation and semantic similarity tasks. While RWKV, with its linear attention mechanism, holds promise for efficient language processing, my results on the MRPC dataset indicate that in its pre-trained, zero-shot form, it does not outperform a much simpler GloVe baseline in terms of semantic similarity as measured by Spearman correlation. In fact, all explored layers of RWKV exhibited lower Spearman correlations than GloVe on both training and validation sets. Furthermore, the inference time for RWKV embed-dings is substantially higher, highlighting a significant computational trade-off, where the potential efficiency gains of RWKV's linear attention do not materi-alize in practice for this task and dataset size.\nThe observed trend of decreasing Spearman correlation with increasing layer depth within RWKV is intriguing. It suggests that for zero-shot semantic simi-larity tasks like paraphrase detection, earlier layers of the RWKV architecture might capture more relevant and generalizable semantic features compared to deeper, potentially more specialized layers. This observation warrants further investigation into the internal representations learned by RWKV at different depths and their suitability for various semantic tasks, potentially revealing insights into the optimal layer selection for different downstream applications.\nThe comparable GPU memory usage between RWKV and GloVe, despite the significant difference in model complexity, suggests that the linear attention mechanism of RWKV, while theoretically efficient in terms of sequence length scaling, may not translate to substantial memory savings in practice, at least for the model size and task considered in this study. The primary efficiency bot-tleneck appears to be inference speed, rather than memory footprint, indicating that further optimization of RWKV inference is needed to realize its efficiency potential for sentence embedding generation.\nThe qualitative analysis further underscores the limitations of both zero-shot RWKV and GloVe for fine-grained semantic similarity tasks like paraphrase detection. The tendency to assign high similarity scores even to non-paraphrase pairs suggests that these methods, without task-specific training, may capture broader semantic relatedness but struggle with the subtle distinctions required for accurate paraphrase judgment, emphasizing the need for task-specific fine-tuning to achieve satisfactory performance on MRPC."}, {"title": "Theoretical Analysis", "content": ""}, {"title": "Linear Attention Mechanism and Long-Range Dependency", "content": "Mathematical Formulation:\nLet the input sequence be $X \\in \\mathbb{R}^{n\\times d}$. In a traditional Transformer, attention is computed as:\n$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V,$\nwith $Q = W_qX$, $K = W_kX$, and $V = W_vX$, resulting in a complexity of $O(n^2d)$.\nRWKV reformulates this via a recurrent mechanism:\n$O_t = \\frac{\\sum_{i=1}^{t-1} e^{w_{t-i}} (k_i v_i) + e^{u} k_t v_t}{\\sum_{i=1}^{t-1} e^{w_{t-i}} r_i + e^{u} r_t},$\nwhere $w_{t-i}$ are learnable relative position weights, $u$ is a bias term, and $r_t = \\sigma(W_r x_t)$ is a gating function. By caching the cumulative terms (e.g.,\n$A_t = e^{-W} A_{t-1} + k_t v_t, \\quad B_t = e^{-W} B_{t-1} + k_t)$,\nthe overall complexity reduces to $O(nd)$. Although this formulation is a sim-plified version (omitting additional terms from the full RWKV-TimeMix block which includes channel-mixing), it effectively illustrates the core principle of linear accumulation and exponential decay.\nCompensation for Long-Range Dependencies:\nWhile exponential decay naturally emphasizes recent tokens, the learnable weights $w_{t-i}$ provide a mechanism to adjust the decay rates, potentially preserving cru-cial long-range information. However, there are theoretical limits on how much these weights can compensate. Future work should include ablation experi-ments where these weights are perturbed or frozen, with performance measured on tasks that require extended context (e.g., long-document summarization)."}, {"title": "Hierarchical Information Propagation and Gradient Dynamics", "content": "Gradient Propagation Characteristics:\nLet $H^l = f_l(H^{l-1})$ be the output of the l-th layer, with loss $\\mathcal{L}$. By the chain rule:\n$\\frac{\\partial \\mathcal{L}}{\\partial H^{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial H^{L}} \\prod_{k=l}^{L-1} \\frac{\\partial f_k}{\\partial H^k} \\frac{\\partial H^{k}}{\\partial H^{k+1}}.$\nA common approximation assumes these Jacobian matrices are nearly diagonal, leading to an exponential decay:\n$\\frac{\\partial \\mathcal{L}}{\\partial H^{l-1}} \\propto e^{-\\alpha(L-l)},$"}, {"title": "Pooling Strategy: Average vs. Adaptive Pooling", "content": "Limitations of Average Pooling:\nFor a sequence $H = \\{h_1,..., h_n\\}$, average pooling computes:\n$h_{\\text{pool}} = \\frac{1}{n} \\sum_{i=1}^n h_i.$\nThis method assumes equal contribution from all tokens. However, if key se-mantic information is present in only $m < n$ tokens, the effective signal-to-noise ratio (SNR) should be based on $m$:\n$\\text{SNR} = \\frac{\\mathbb{E}[\\frac{1}{n} \\sum_{i=1}^m ||h^*_i||^2]}{\\mathbb{E}[\\frac{1}{n} \\sum_{i=1}^m ||h^*_i||^2] + \\sigma^2} \\approx \\frac{||h^*||^2}{\\sigma^2} \\cdot n.$\nwhich, in effect, dilutes the signal if $m$ is small.\nAdaptive Pooling Proposal:\nAn attention-weighted pooling mechanism is given by:\n$h_{\\text{pool}} = \\sum_{i=1}^n a_i h_i, \\quad a_i = \\frac{\\text{exp}(q^T h_i)}{\\sum_{i=1}^n \\text{exp}(q^T h_i)},$"}, {"title": "Information-Theoretic Perspective on Model Complexity", "content": "According to the Information Bottleneck principle, an ideal model minimizes:\n$\\mathcal{L} = I(X; H) - \\beta I(H; Y),$\nwhere $I(.)$ denotes mutual information. RWKV's linear structure constrains $I(X; H)$, which might lead to the loss of nuanced semantic details. Anecdo-tally, a faster singular value decay (reported as approximately 30% faster than Transformers) may indicate a reduced capacity to capture rare semantic pat-terns (e.g., nuanced sentiments or subtle contextual cues). However, this claim remains speculative and should be validated empirically by comparing the sin-gular value spectra of RWKV and Transformer weight matrices."}, {"title": "Architectural Components: Time-Mixing and Channel-Mixing", "content": "RWKV consists of two key modules:\n\u2022 Time-Mixing Block: Integrates past token representations with the current input, forming the core recurrent mechanism that enables linear complexity.\n\u2022 Channel-Mixing Block: Facilitates interactions among feature chan-nels, integrating diverse semantic features.\nThe interplay between these blocks is critical. The time-mixing block man-ages sequential dependencies, while the channel-mixing block aggregates in-formation across dimensions. Ablation studies disabling one module at a time could quantify their individual contributions to semantic representation and gradient stability."}, {"title": "Suggestions for Empirical Validation", "content": "To validate the theoretical findings, I propose the following experiments:\n\u2022 Long-Range Dependency Tasks: Evaluate RWKV on long-document summarization and text classification. Ablate or freeze $w_{t-i}$ weights to assess their role.\n\u2022 Gradient Norm Analysis: Plot gradient norm distributions across lay-ers for different sequence lengths, measure the decay factor $\\alpha$, and analyze Jacobian eigenvalue spectra.\n\u2022 Entropy Curve Visualization: Compute and plot entropy $S(H^l)$ across layers, comparing trends with other architectures.\n\u2022 Pooling Strategy Comparison: Compare average pooling with attention-weighted adaptive pooling, examining effective SNR and correlating atten-tion weights with semantic performance."}, {"title": "Guidance for Future Improvements", "content": "Based on my analysis, I propose the following directions for future work:\n\u2022 Hierarchical Pooling Strategies: Develop methods to aggregate repre-sentations across multiple layers using adaptive or attention-based pooling, enhancing key feature extraction.\n\u2022 Task-Specific Fine-Tuning: Fine-tune RWKV on tasks like semantic similarity or paraphrase detection using contrastive learning or other ob-jectives to improve task alignment.\n\u2022 Entropy-Regularized Training: Introduce regularization to maintain a \"healthy\" entropy level in deeper layers, reducing excessive information loss while preserving abstraction.\n\u2022 Cross-Layer Fusion Techniques: Explore architectures incorporating residual connections or fusion layers to enhance gradient flow and integrate global context.\n\u2022 Dynamic Attention Decay: Investigate adaptive strategies for adjust-ing the decay factors $w_{t-i}$ based on input or task requirements, optimizing the balance between local and global dependencies.\nThese enhancements not only address potential limitations in RWKV's cur-rent design but also provide a clear roadmap for future research to refine the model's capacity for high-quality semantic representation while maintaining its efficiency advantages."}, {"title": "Limitations and Future Work", "content": "My study, while providing initial insights into RWKV for sentence embeddings, has several limitations that suggest avenues for future research. Firstly, the zero-shot setting inherently limits the performance of RWKV on the semantic similarity task. Pre-trained language models are optimized for general language modeling objectives, and their representations may not be directly aligned with the nuances of semantic similarity required for paraphrase detection, as evi-denced by the overall low Spearman correlations achieved. The relatively low Spearman correlation values observed, even for the best RWKV layer and the GloVe baseline, indicate the inherent challenge of zero-shot semantic similarity assessment on MRPC and highlight the need for task-specific adaptation. Fu-ture work should explore fine-tuning RWKV specifically for semantic similarity tasks to overcome this limitation.\nSecondly, my use of simple average pooling to derive sentence embeddings from RWKV hidden states is a basic approach and may not fully capture the rich semantic information encoded in the model. More sophisticated pooling"}, {"title": "Conclusion", "content": "This paper presented an exploratory study evaluating the use of RWKV, a lin-ear attention-based language model, for generating sentence embeddings and as-"}]}