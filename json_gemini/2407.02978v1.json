{"title": "Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins: ROBERTa-BiLSTM Approach to Detect AI-Generated Text", "authors": ["Jainit Sushil Bafna", "Hardik Mittal", "Suyash Sethia", "Manish Shrivastava", "Radhika Mamidi"], "abstract": "Large Language Models (LLMs) have showcased impressive abilities in generating fluent responses to diverse user queries. However, concerns regarding the potential misuse of such texts in journalism, educational, and academic contexts have surfaced. SemEval 2024 introduces the task of Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection, aiming to develop automated systems for identifying machine-generated text and detecting potential misuse. In this paper, we i) propose a RoBERTa-BILSTM based classifier designed to classify text into two categories: AI-generated or human ii) conduct a comparative study of our model with baseline approaches to evaluate its effectiveness. This paper contributes to the advancement of automatic text detection systems in addressing the challenges posed by machine-generated text misuse. Our architecture ranked 46th on the official leaderboard with an accuracy of 80.83 among 125.", "sections": [{"title": "1 Introduction", "content": "The task of classifying text as either AI-generated or human-generated holds significant importance in the field of natural language processing (NLP). It addresses the growing need to distinguish between content created by artificial intelligence models and that generated by human authors, a distinction crucial for various applications such as content moderation, misinformation detection, and safeguarding against AI-generated malicious content. This task is outlined in the task overview paper by (Wang et al., 2024), emphasizing its relevance and scope in the NLP community.\nOur system employs a hybrid approach combining deep learning techniques with feature engineering to tackle the classification task effectively. Specifically, we leverage a BiLSTM (Bidirectional Long Short-Term Memory) (Schuster and Paliwal, 1997) neural network in conjunction with ROBERTa (Liu et al., 2019), a pre-trained language representation model, to capture both sequential and contextual information from the input sentences. This hybrid architecture enables our system to effectively capture nuanced linguistic patterns and semantic cues for accurate classification.\nParticipating in this task provided valuable insights into the capabilities and limitations of our system. Quantitatively, our system achieved competitive results, ranking 46 relative to other teams in terms of accuracy and F1 score. Qualitatively, we observed that our system struggled with distinguishing between sentences generated by AI models trained on specific domains or datasets with highly similar linguistic patterns.\nWe have released the code for our system on GitHub\u00b9, facilitating transparency and reproducibility in our approach."}, {"title": "2 Related Works", "content": "In the field of detecting machine-generated text, numerous methodologies and models have been examined. A distinguished methodology is the application of the ROBERTa Classifier, which enhances the ROBERTa language model through fine-tuning for the specific purpose of identifying machine-generated text. The proficiency of pre-trained classifiers like ROBERTa in this domain has been affirmed through various studies, including those conducted by (Solaiman et al., 2019) and additional research by (Zellers et al., 2019; Ippolito et al., 2019; Bakhtin et al., 2020; Jang et al., 2020; Uchendu et al., 2021). Concurrently, the XLM-R Classifier exploits the multilingual training of the XLM-ROBERTa model to effectively recognize machine-generated text in various languages, as demonstrated by (Conneau et al., 2019)."}, {"title": "3 Background", "content": "For the machine-generated text, the researchers used various multilingual language models like ChatGPT(OpenAI, 2024), textdavinci-003(OpenAI, 2022), LLaMa(Touvron et al., 2023b), FlanT5(Chung et al., 2022), Cohere(Cohere, 2024), Dolly-v2(databricks, 2022), and BLOOMz(Muennighoff et al., 2023). These models were given different tasks like writing Wikipedia articles, summarizing abstracts from arXiv, providing peer reviews, answering questions from Reddit and Baike/Web QA, and creating news briefs. As evident from Table 1, the training set lacks any sentences generated by the Bloomz model, which stands as the sole model represented in the validation set. This deliberate choice ensures a robust assessment of our model's generalization capabilities across all machine-generated outputs, regardless of the specific model generating them. By exposing our model to diverse machine-generated sentences during training, including those from unseen models like Bloomz in the validation set, we aim to evaluate its ability to effectively generalize to novel inputs and make reliable predictions across the spectrum of machine-generated text."}, {"title": "3.2 Task", "content": "We focused on Subtask-A of the SemEval Task 8 which involves developing a classifier to differentiate between monolingual sentences generated by artificial intelligence (AI) systems and those generated by humans. This classification task is essential for distinguishing the origin of text and understanding whether it was produced by AI models or by human authors."}, {"title": "3.2.1 Objective", "content": "The primary objective is to build a robust classifier capable of accurately distinguishing between AI-generated and human-generated sentences. The classifier should generalize well across various AI models and domains, ensuring consistent performance regardless of the specific model or domain from which the text originates.\nThe goal was to design a model that not only performs this task with high accuracy but also adapts to various AI models and domains. It's crucial for the classifier to accurately identify the origin of sentences, regardless of the technology used to generate them or their subject matter, ensuring broad applicability and effectiveness"}, {"title": "4 System Overview", "content": "Based on our observation (See 7), we discovered that language modeling task encodes the various features required for detection of AI written text. So we used pretrained ROBERTa in most of our architectures so exploit this power of language models."}, {"title": "4.1 Full ROBERTa Finetune", "content": "The Full RoBERTa(Liu et al., 2019) Finetune model, chosen as our baseline, boasted an extensive architecture and possessed the highest parameter count among the models under evaluation. Serving as a comprehensive starting point, this model allowed us to assess the effectiveness of subsequent enhancements in comparison."}, {"title": "4.2 LORA with RoBERTa (Frozen)", "content": "Incorporating Low Rank Adapters (Hu et al., 2021), we applied fine-tuning techniques to the RoBERTa model while strategically freezing all layers. This approach enabled us to adapt the model to our specific task domain, leveraging pre-trained representations effectively."}, {"title": "4.3 LORA with LongFormer", "content": "The limitation of ROBERTa's context length (max 512 tokens) posed challenges for handling lengthy sentences in our dataset. To address this, we investigated LongFormer (Beltagy et al., 2020), a model designed to efficiently manage longer contexts. Despite employing LoRA for fine-tuning, the model's performance on the validation set fell short of expectations, indicating potential difficulties in generalization."}, {"title": "4.4 ROBERTa (2 Layers unfreezed) +\nBILSTM", "content": "Expanding upon RoBERTa's capabilities, we introduced a hybrid architecture by unfreezing two layers and integrating a BiLSTM network (Schuster and Paliwal, 1997). RoBERTa served as the primary encoder for sentence representations, with the subsequent BiLSTM layer trained to classify based on the last hidden state."}, {"title": "4.5 ROBERTa (Frozen) + GRU", "content": "In our endeavor to augment RoBERTa's capabilities, we devised a hybrid architecture by integrating a Gated Recurrent Unit (GRU) (Chung et al., 2014) network with the frozen RoBERTa model. Within this framework, RoBERTa served as the encoder for generating sentence representations, while a subsequent GRU layer was incorporated for sequential processing and classification tasks. This amalgamation aimed to leverage the strengths of both ROBERTa's contextual understanding and GRU's recurrent dynamics, contributing to enhanced performance on our target task."}, {"title": "4.6 ROBERTa (Frozen) + BILSTM", "content": "In our pursuit of enhancing RoBERTa's capabilities, we devised a hybrid architecture by coupling a Bidirectional Long Short-Term Memory (BiLSTM) network with the RoBERTa model (Liu et al., 2019). In this setup, RoBERTa functioned as the encoder for sentence representations, while a subsequent BiLSTM layer was employed for classification, utilizing the last hidden state for decision-making. For a detailed visual representation of the model's architecture, please refer to the accompanying Figure 1."}, {"title": "5 Experiments", "content": "All textual data underwent standard preprocessing steps, including tokenization, lowercasing, and punctuation marks. Additionally, specific domain-related preprocessing, such as handling special characters or domain-specific terms, was performed as necessary."}, {"title": "5.2 Hyperparameter Tuning", "content": "Hyperparameters were tuned using a combination of grid search and random search techniques. We explored various hyperparameter combinations to identify the optimal configuration for each model variant.\nThe configuration for LSTM and GRU used in Table 2 is hidden_size=256, layers=2, dropout=0.2, with LoRA rank being 20 has been found as the best configuration for the models. For ROBERTa+LSTM model's feedforward had a single weight matrix of dimension 512*2."}, {"title": "6 Results", "content": "We tested our models on various models on the test set. The results can be viewed in (Table: 3).\nRanking: Our BiLSTM+ROBERTa model achieved a ranking of 46 out of 125 participants in the competition, demonstrating its competitive performance (as shown in Table 3). These results highlight the effectiveness of various models, including BiLSTM+RoBERTa and GRU+ROBERTa, in addressing the task objectives. We submitted BiLSTM+ROBERTa based on its strong performance on the validation set. However, after testing all models listed in Table 3, we found that GRU+ROBERTa achieved a significantly better result, with an accuracy increase of approximately 4%."}, {"title": "7 Conclusion", "content": "In conclusion, our BiLSTM+ROBERTa model effectively tackled the task, achieving competitive results, thanks to its deep learning and pre-trained language model. While a similar model with unfrozen ROBERTa boasted higher precision, its complexity came at the cost of increased parameters.\nImpressively, our model ranked 46th out of 125 competition entries (Table 3), showcasing its potential alongside approaches like GRU+ROBERTa. Interestingly, post-competition analysis revealed GRU+ROBERTa's superior accuracy (by about 4%). This highlights the value of exploring diverse architectures and hyperparameter tuning for peak performance.\nMoving forward, there are several avenues for future work to explore. Firstly, further experimentation with different model architectures, including alternative combinations of encoders and classifiers, could potentially yield improvements in performance. Additionally, fine-tuning hyperparameters and exploring advanced techniques for model optimization may enhance the robustness and generalization capabilities of our system. Furthermore, incorporating additional contextual information or domain-specific knowledge could potentially augment the model's understanding and performance on specific tasks. Overall, our findings contribute to the ongoing research efforts in natural language processing and provide valuable insights for future developments in this domain."}, {"title": "Appendix A", "content": "In this study, we implemented a methodology aimed at distinguishing human-generated sentences from machine-generated ones within a training dataset. To achieve this, we initially segregated the dataset into two distinct subsets: one containing human-generated sentences and the other comprising machine-generated ones. Subsequently, we trained separate models utilizing these segregated datasets. Specifically, we employed two distinct models for this task : i) Bidirectional Long Short-Term Memory (BiLSTM) model, ii) ROBERTa model.\nFollowing the training phase, we proceeded to evaluate the performance of both models on a validation dataset. During this evaluation, we measured the loss incurred by each model when tasked with discerning between human-generated and machine-generated sentences. This evaluation process was crucial for assessing the efficacy and generalization capabilities of the trained models in accurately distinguishing between the two types of sentences."}, {"title": "B. Results", "content": "The results are in form of graphs in Figure 2 of models \u2013 those trained on human-generated sentences and those trained on machine-generated sentences. Specifically, we observed that the losses incurred by human-generated sentences on the validation set exhibited a wider distribution with higher variance, while the losses associated with machine-generated sentences displayed a narrower distribution with lesser variance.\nThis observation leads to a compelling inference regarding the predictive nature of the model losses for each type of data. The wider distribution and higher variance in losses for human-generated sentences suggest a greater level of unpredictability associated with these sentences. In contrast, the narrower distribution and lesser variance in losses for machine-generated sentences indicate a higher level of predictiveness in the model's performance on these sentences.\nThis finding sheds light on the inherent characteristics of human-generated versus machine-generated sentences, particularly regarding their predictability when processed by the trained models. Such insights are crucial for understanding the intricacies of model behavior and the challenges posed by different types of data in natural language processing tasks."}]}