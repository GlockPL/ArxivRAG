{"title": "VEMOCLAP: A video emotion classification web application", "authors": ["Serkan Sulun", "Paula Viana", "Matthew E. P. Davies"], "abstract": "We introduce VEMOCLAP: Video EMOtion Classifier using Pretrained features, the first readily available and open-source web application that analyzes the emotional content of any user-provided video. We improve our previous work, which exploits open-source pretrained models that work on video frames and audio, and then efficiently fuse the resulting pretrained features using multi-head cross-attention. Our approach increases the state-of-the-art classification accuracy on the Ekman-6 video emotion dataset by 4.3% and offers an online application for users to run our model on their own videos or YouTube videos. We invite the readers to try our application at serkansulun.com/app.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "We present a web application for classifying the emotion in any user-provided video. Hosted on Google Colab with free GPU runtime, it is accessible to users of all skill levels and requires only a few mouse clicks. Users can upload a video, link a YouTube video, or select from available sample videos. The application outputs predicted emotions and includes additional analyses such as automatic speech recognition (ASR), optical character recognition (OCR), face detection and facial expression classification, audio classification, and image captioning.\nWe improve our previous work and train our revised model on video emotion classification. While we comprehensively explain our method, we invite the reader to view our previous work for an in-depth description [1]. We train our models on the Ekman-6 video emotion dataset and achieve a new state-of-the-art classification accuracy. The Ekman-6 dataset, one of the largest publicly available video emotion datasets, contains 1637 videos from YouTube and Flickr, each categorized into one of 6 emotions: anger, disgust, joy, sadness, and surprise [2]. Our model not only surpasses previous state-of-the-art results with the original training and testing splits, but also benefits from data cleansing that improves the classifier used in our web application.\nOur contributions are the following:\n\u2022 We improve the state-of-the-art classification accuracy on the Ekman-6 video emotion classification dataset by 4.3%.\n\u2022 We inspect and clean the Ekman-6 dataset, providing a list of problematic samples to enhance the training of video emotion models.\n\u2022 We introduce an open-source and readily available web application that allows users to analyze and classify emotions in any video by uploading it or providing a YouTube link.\nThough Google Colab provides free GPUs, the CPU and GPU memory are limited to around 15 GBs. We redesigned our previous work to reduce its computational complexity for seamless deployment on Google Colab. First, instead of using the entire video, we extracted and used a limited number of frames. We also replaced the transformer model with multi-headed cross-attention modules that efficiently handle the temporal dependencies between multimodal features [3].\nOur model has a low memory footprint due to the use of open-source, readily available pretrained feature extractor models. These models run in inference mode, avoiding back-propagation and storing gradients. We claim that the features extracted by these pretrained models are highly relevant to a video's emotion. Therefore, we can fuse the extracted features efficiently and process them using shallow neural networks.\nThe pretrained models we used are as follows:\nFace detector: The face detection model from the Ultralytics group is based on YOLO (You Only Look Once) [4]. YOLO is a real-time object detection algorithm that divides an image into a grid and predicts bounding boxes and class probabilities for each grid cell using convolutional neural networks (CNNs) [5].\nExpression classifier: Pakov has finetuned a Vision Transformer (ViT) on the FER-2013 (Facial Emotion Recognition) dataset [6]. The model takes a facial image and predicts the facial expression as angry, disgusted, fearful, happy, sad, surprised, or neutral."}, {"title": "II. METHODOLOGY", "content": "Our video emotion classification pipeline can be seen in Figure 1 [1]. It consists of frozen pretrained feature extractor models and trained modules for fusing and classifying the pretrained features into emotions.\n1) Pretrained feature extraction: We initially extracted a fixed number n of frames from each video, along with the entire audio, which was resampled at 16kHz and converted to mono. We then extracted relevant pretrained features in inference mode for all videos. Notably, for pretrained classifiers like the expression classifier, sentiment classifier, and BEATS, we use activations from the layers before the final classification rather than the final classifications. However, our web application also provides the final classifications for a more comprehensive video analysis.\nThe features from the facial expression classifier and CLIP are sequences of vectors, as their inputs are sequences of frames. If multiple faces are detected in a single video frame, we average the features of the two largest faces. Similarly, the BEATS model processes sequences of 3-second audio chunks. We extracted n audio chunks to match the number of video frames. While CLIP and BEATs produce n output vectors, the facial expression classifier may produce fewer vectors, depending on whether faces are present in each frame.\nThe ASR model generates a single block of text from the entire audio. If the source language is not English, it automatically translates the output text into English.\nThe OCR model generates blocks of text for each video frame. The language identifier processes each block. The text is translated into English if the identified language is not English. If the language is English, the text is passed through the word segmenter and then the spell corrector. The resulting text blocks from each frame are concatenated to form a single block of text.\nThe texts resulting from ASR and OCR are fed into the sentiment classifier separately. Since the sentiment classifier predicts a single label for any length of text, a single vector is extracted as the feature.\n2) Emotion classification: After the feature extraction, for a single video, we are left with n CLIP, n BEATs, k < n facial expression, 1 OCR sentiment, and 1 ASR sentiment features. Fusing these pretrained features presents multiple challenges. First, since they are extracted using different pretrained models, the lengths (dimensionalities) of the feature vectors are different. Secondly, when the feature vectors form a sequence, as in the case of facial expressions, CLIP, and BEATS, their temporal lengths can also be different. Finally, since they belong to different modalities, the content of these feature vectors can be vastly different.\nTo address these issues, we first performed min-max nor-malization on each feature using statistics extracted from the collection of pretrained features. To handle differing dimensionalities, all sequential input features are first projected to queries, keys, and values with a common dimensionality d [3]. Next, the attention modules exploit correspondence between pairs of sequential features. The attention modules also include dropout and layer normalization and can handle a pair of sequences with different temporal lengths. As done in classification tasks, the attention outputs are averaged along the temporal dimension, yielding a single vector. Since the OCR and ASR sentiment features are already single vectors, each modality is represented by a single vector after the attention modules. We then concatenated all five feature vectors along the channel dimension, resulting in a single vector representing the entire video. Finally, this vector was fed into a linear layer followed by a softmax layer, which outputs a probability for each emotion.\n3) Implementation details and hyperparameters: We initially extracted video frames at 1 frame per second, using n = 16 video frames and audio chunks as input to our model. However, users can adjust the parameter n during training or inference. During training, we selected the n video frames and audio chunks from random locations for data augmentation. During testing, we extracted them at equidistant intervals to ensure comprehensive temporal representation. We reported classification performance using the provided training and testing splits, which included 819 and 818 videos, respectively. We used cross-entropy loss, a batch size of 32, a dropout rate of 0.5, and Adam optimizer with a learning rate of 1e-5 [17]. Attention modules have 4 heads and a dimensionality of 512. We used 10% of the training split as the validation split and stopped training when validation accuracy started to drop. The model has around 11M trainable parameters."}, {"title": "B. Dataset cleaning", "content": "While we report classification results on the unedited Ekman-6 dataset, we cleaned it to train the model used in the web application. The Ekman-6 dataset was created by scraping the web for videos using search keywords that matched not only the categorized emotion but also related terms. We viewed each video to detect the problematic samples. After inspecting their file names, we identified the following problematic search keywords for each emotion class, which are underlined.\nAnger: A single person being annoying, with no other person present to be annoyed or angry.\nDisgust: Flashing lights or rapid camera movement, presumably to induce dizzyness or nausea. It also includes videos related to boredom and loathing.\nFear: Counter-terrorism, underwater footage, 9/11 terrorism attack aftermath, and suspect apprehension.\nJoy: Joyride (driving a car), the music \"Ode to Joy\", and people named Joy.\nSadness: pensive\nSurprise: distraction, and people performing impressive feats labeled as astonishing.\nWe identified and removed 128 and 130 problematic videos from the training and testing splits, respectively. Using the cleaned data, the classification accuracy increased by 2.6%. However, we exclude this result from our comparison with the state-of-the-art because data cleaning alters the test split's content, affecting the comparison's fairness. For training the model used in our web application, we alphabetically sorted the video names for each category, used the first 95% for training, and reserved the remaining 5% for validation. We made the list of the problematic videos available as ek-man_blacklist.txt"}, {"title": "C. Inference web application", "content": "We developed an open-source web application for performing inference on user-provided videos. Hosted on Google Colab, it offers free GPU runtime. Users can upload their own videos, provide a YouTube link, or use sample videos provided within the application. The application is self-contained and ready to use, requiring no setup from the user. The process is streamlined into 5 steps, with only 5 mouse clicks needed to obtain the results. After connecting to a GPU runtime, users should follow these steps:\nStep 1: Automatically download and extract the codebase, and install the required Python libraries. This step takes approximately 2 minutes.\nStep 2: Download and build the feature extractor models and the classifier model. As these models are deep neural networks, this step takes about 3 minutes. Note that Steps 1 and 2 only need to be completed once, even if classifying multiple videos.\nStep 3: Select how to load the input video. The options are \u201cSample video\u201d, \u201cYouTube link\u201d, and \u201cUpload video\u201d.\nStep 4: Depending on the choice from step 3, the user then selects the specific video. Steps 3 and 4 take only a few seconds to complete.\nStep 5: Extract the frames and audio from the input video, run the pretrained feature extractors, and finally run the emotion classifier. The outputs include text from automatic speech recognition (ASR) with its sentiment, text from optical character recognition (OCR) with its sentiment, and predictions from the BEATs audio classifier. Additionally, a sample frame is displayed showing detected faces with predicted emotional expressions, detected OCR boxes, and a caption generated by CLIPCap. Note that this sample frame is for demonstration purposes, while all n frames are used for the final emotion classification. For a 60-second video, this step takes approximately 30 seconds."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "In Table III, we present the quantitative performance of our model on the Ekman-6 dataset using the provided training and testing splits, showing that our method outperforms the state-of-the-art by 4.3%. Figure 2 shows the confusion matrix for our classification results on the test split."}, {"title": "IV. CONCLUSION", "content": "In this study, we achieved a new state-of-the-art performance on the Ekman-6 video emotion classification benchmark and provided a self-contained web application for both general users and researchers. We also offer the pretrained features and highlight problematic samples from the Ekman-6 dataset to assist researchers in improving their models. Our contributions aim to advance emotion recognition and multimedia analysis, providing valuable tools and resources to support further research and development in these fields."}]}