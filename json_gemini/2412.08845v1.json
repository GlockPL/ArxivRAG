{"title": "Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning", "authors": ["Kuan-Cheng Chen", "Samuel Yen-Chi Chen", "Chen-Yu Liu", "Kin K. Leung"], "abstract": "In this paper, we introduce Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning (Dist-QTRL), a novel approach to addressing the scalability challenges of traditional Reinforcement Learning (RL) by integrating quantum computing principles. Quantum-Train Reinforcement Learning (QTRL) leverages parameterized quantum circuits to efficiently generate neural network parameters, achieving a poly(log(N)) reduction in the dimensionality of trainable parameters while harnessing quantum entanglement for superior data represen- tation. The framework is designed for distributed multi-agent environments, where multiple agents, modeled as Quantum Processing Units (QPUs), operate in parallel, enabling faster con- vergence and enhanced scalability. Additionally, the Dist-QTRL framework can be extended to high-performance computing (HPC) environments by utilizing distributed quantum training for parameter reduction in classical neural networks, followed by inference using classical CPUs or GPUs. This hybrid quantum- HPC approach allows for further optimization in real-world ap- plications. In this paper, we provide a mathematical formulation of the Dist-QTRL framework and explore its convergence proper- ties, supported by empirical results demonstrating performance improvements over centric QTRL models. The results highlight the potential of quantum-enhanced RL in tackling complex, high- dimensional tasks, particularly in distributed computing settings, where our framework achieves significant speedups through parallelization without compromising model accuracy. This work paves the way for scalable, quantum-enhanced RL systems in practical applications, leveraging both quantum and classical computational resources.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) has emerged as a powerful paradigm for training agents to make sequential decisions by interacting with an environment to maximize cumulative rewards [1], [2]. Traditional RL approaches predominantly utilize classical neural networks to parameterize policies and value functions [3], [4]. However, as the complexity of tasks increases, these classical methods often face scalability issues due to the exponential growth of the parameter space required to capture intricate patterns and dependencies [5].\nQuantum computing (QC) offers promising solutions to overcome these limitations by harnessing quantum parallelism and entanglement, which can potentially enable more efficient processing and representation of complex data structures [6]. Recent advancements in Quantum Machine Learning (QML) have demonstrated that quantum circuits can serve as pow- erful function approximators [7], [8], providing theoretical speedups for specific computational tasks compared to their classical counterparts [9]. Variational Quantum Algorithms (VQAs) [10]\u2013[12] form a category of hybrid machine learning approaches, where quantum parameters are optimized through classical methods. These algorithms pave the way for creating architectures like Quantum Neural Networks (QNNs), which address a wide spectrum of AI/ML challenges, including clas- sification [13]\u2013[19], time-series forecasting [20], high energy physics [21]-[23], natural language processing [24]-[27], and reinforcement learning [28], [29]. These advancements also demonstrate the potential to solve real-world, large-scale, and high-dimensional problems in both supervised learning and RL.\nThe Quantum-Train (QT) framework employs parameter- ized quantum circuits to efficiently generate neural network parameters, reducing the number of trainable parameters by utilizing quantum entanglement and superposition [30]. By leveraging the exponential size of the n-qubit Hilbert space, QT controls up to $2^n$ parameters using only $O(poly(n))$ rotational angles, offering significant scalability advantages. Integrated into RL tasks, this approach enhances training efficiency, particularly in distributed systems with multiple Quantum Processing Units (QPUs) [31], [32]. Additionally, QT eliminates common issues such as data encoding in QNNS, and its trained models are independent of quantum hardware, making it suitable for diverse real-world applications during inference stage [33]-[36].\nDespite its potential benefits, integrating QT into distributed RL presents several challenges, including the synchroniza- tion of quantum-generated parameters across multiple agents, maintaining coherence in parameter updates, and ensuring convergence in the presence of quantum noise and variability. Addressing these challenges is crucial to fully realizing the"}, {"title": "II. REINFORCEMENT LEARNING FRAMEWORK", "content": "RL problems are commonly formulated using the frame- work of Markov Decision Processes (MDPs) [2], which are defined by a tuple $M = (S, A, P, R, \\gamma)$. In this formalism, $S$ represents a finite set of states, and $A$ denotes a finite set of actions. The state transition probability function $P : S \\times A \\times S \\rightarrow [0,1]$ defines the probability of transitioning from one state to another after taking a specific action, denoted by $P(s'|s, a) = Pr(s_{t+1} = s' | S_t = s, a_t = a)$. The reward function $R : S \\times A \\times S \\rightarrow R$ assigns an immediate reward for transitioning between states due to an action, and $\\gamma \\in [0,1)$ is the discount factor that determines the weight of future rewards. At each discrete time step $t$, the agent observes a state $s_t \\in S$, selects an action $a_t \\in A$ according to its policy $\\pi(a|s)$, receives a reward $r_t = R(s_t, a_t, s_{t+1})$, and transitions to a new state $s_{t+1}$ according to the probability $P(s_{t+1} | S_t, a_t)$. The objective of the agent is to find an optimal policy $\\pi^*$ that maximizes the expected cumulative discounted reward, also known as the return $\\pi^* = arg max J(\\pi)$, where $J(\\pi) = E_{\\pi} [\\Sigma_{\\tau=0}^{\\infty} \\gamma^{\\tau} r_t]$. Here, the expectation $E_{\\pi}$ is taken over trajectories generated by following the policy $\\pi$."}, {"title": "B. Policy Gradient Methods", "content": "Policy gradient methods are a class of RL algorithms that optimize the policy $\\pi(a_s;\\theta)$ directly by adjusting its parameters $\\theta$ without the requirement to have the knowledge of transition probability $P(S_{t+1} | s_t, a_t)$. The goal is to maximize the expected return $J(\\pi)$ with respect to $\\theta$. The gradient of the expected return with respect to the policy parameters is given by the policy gradient theorem [2], $\\nabla J(\\theta) = E_{\\pi_{\\theta}} [\\Sigma_{t=0}^{\\infty} \\nabla_{\\theta} log \\pi(a_t | s_t;\\theta)G_t]$, where $G_t = \\sum_{k=t}^{\\infty} \\gamma^{k-t}r_k$ is the total return from time $t$ onward. This gradient can be estimated using samples from the agent's interaction with the environment, and gradient ascent methods can be employed to update the parameters $\\theta$ accordingly."}, {"title": "III. QUANTUM-TRAIN FRAMEWORK", "content": "The QT framework leverages quantum computing principles to generate the parameters of classical neural networks used in RL policies. Instead of initializing and training a potentially large number of classical parameters $\\theta = (\\theta_1, \\theta_2,..., \\theta_k) \\in R^k$, QT utilizes a parameterized quantum state $|\\psi(\\varphi)\\rangle$, where $\\varphi \\in R^m$ represents the required $m$ parameters of a Quantum Neural Network (QNN). A quantum state with $n = [log_2 k]$ qubits is prepared such that measurements in the computa- tional basis yield probabilities corresponding to the desired parameters. Specifically, the measurement probabilities are given by, $p_i = |\\langle i | \\psi(\\varphi)\\rangle|^2$ where $i \\in \\{0, 1, . . ., 2^n - 1\\}$ and $|i\\rangle$ denotes the computational basis states. $p_i$ are the probabilities associated with each basis state upon measurement."}, {"title": "B. Mapping Quantum Measurements to Policy Parameters", "content": "To translate the quantum measurement probabilities into the policy parameters required by the RL algorithm, a clas- sical mapping function $M_\\beta$ is employed. This mapping is parameterized by $\\beta \\in R^l$ and takes as input the bit-string representation $i_b$ of the basis state $|i\\rangle$ and the corresponding measurement probability $p_i$, $\\theta_i = M_\\beta(i_b, p_i)$, where $i = 1,2,...,k$. The mapping function $M_\\beta$ can be implemented using a classical neural network or any suitable function approximator. It effectively transforms the quantum-generated probabilities into the real-valued parameters $\\theta_i$ needed for the policy network."}, {"title": "C. Policy Parameterization", "content": "Combining the quantum parameter generation and the clas- sical mapping, the policy parameters $\\theta$ become functions of the quantum circuit parameters $\\varphi$ and the mapping param- eters $\\beta$. We denoted it as $\\theta = M_\\beta(\\varphi)$. By adjusting the quantum parameters $\\varphi$ and the mapping parameters $\\beta$, we can indirectly modify the policy parameters $\\theta$. This approach has the advantage of potentially requiring significantly fewer parameters to be trained, as the size of $\\varphi$ and $\\beta$ can be much smaller than $k$, that is, $m + l < k$, especially when the quantum state is entangled and the mapping function is efficiently parameterized. The QT framework thus introduces a novel way of parameterizing policies in RL, leveraging quantum resources to enhance the representational capacity and potentially improve the efficiency of the learning process."}, {"title": "IV. DISTRIBUTED QUANTUM-TRAIN REINFORCEMENT LEARNING", "content": "In a distributed RL framework, multiple agents operate concurrently, each interacting independently with its own instance of the environment. This parallelism accelerates the learning process and enhances scalability for complex tasks. In our proposed framework, we consider N agents collaboratively learning an optimal policy. Each agent i pulls the global shared policy $\\pi_{\\theta}(a|s)$, parameterized by parameters $\\Theta = \\{\\beta,\\varphi\\}$. Agents interact with the environment independently, collecting experiences and calculating local gradients based on individual observations/experiences."}, {"title": "B. Local Policy Update Mechanism", "content": "The local policy update for each agent i involves the following steps:\n1) Experience Collection: Agent i interacts with its envi- ronment to collect a trajectory $T_i = (s_t^i, a_t^i, r_t^i, s_{t+1}^i)_{t=0}^{T_i}$, where $T_i$ denotes the length of the episode for agent i.\n2) Return Computation: The agent computes the cumula- tive discounted return for each time step in its trajectory:\n$G_t^i = \\sum_{k=t}^{T_i} \\gamma^{k-t}r_k^i$\n(1)\nwhere $\\gamma \\in [0, 1]$ is the discount factor.\n3) Policy Gradient Estimation: The agent estimates the policy gradient using its collected experiences:\n$\\nabla_{\\Theta} J_i = \\nabla_{\\Theta} J_i(M_\\beta(\\varphi))$\n$\\frac{1}{T_i} \\sum_{t=0}^{T_i} \\nabla_{\\Theta} log \\pi_{\\theta} (a_t^i| s_t^i) G_t^i,$\n(2)\nwhere $T_i$ is the number of time steps in the trajectory $T_i$ and $\\Theta$ is the collection of all trainable parameters in QNN and the mapping model $\\{\\beta,\\varphi\\}$."}, {"title": "4) Local Gradient Computation:", "content": "The agent calculates its local gradients based on the trajectory it encounters:\n$\\nabla_{\\Theta} J_i = \\nabla_{\\Theta} J_i(M_\\beta(\\varphi)).$\n(3)\nSince the policy parameters $\\theta$ are generated via a quantum- classical mapping $\\theta = M_\\beta(\\varphi)$, where $\\varphi$ are the quantum parameters and $\\beta$ are the classical mapping parameters, each of the local agent i can compute the gradient of the objective $J_i$ with respect to these parameters using the chain rule:\n$\\nabla_{\\Theta} J_i(\\theta) = \\nabla_{\\varphi,\\beta} J_i(\\theta)$\n$= \\nabla_{\\varphi,\\beta} J_i(M_\\beta(\\varphi))$\n$= \\frac{M_\\beta(\\varphi)}{\\delta (\\varphi, \\beta)}$\n(4)\nEach agent computes its own policy gradient $\\nabla_{\\Theta} J_i(\\theta)$ based on individual experiences. These gradients are aggregated to update the shared quantum parameters and mapping model parameters $(\\varphi, \\beta)$, ensuring that the quantum components of the policy benefit from the collective learning."}, {"title": "C. Global Parameter Synchronization and Updates", "content": "After a predefined number of episodes or iterations, agents synchronize their local gradients to converge toward a globally optimal policy. The synchronization process involves averag- ing the gradients across all agents:\n$\\Theta_{new} = \\Theta_{old} + \\eta \\frac{1}{N} \\sum_{i=1}^{N} [\\nabla_{\\Theta} J_i(M_\\beta(\\varphi))],$\n(5)\nwith learning rate $\\eta > 0$.\nEach agent then pulls the updated global shared model $\\Theta_{new}$ to the local memory for next training iteration. This synchronization ensures that knowledge acquired by individual agents is shared across the network, promoting collective learning and preventing divergence due to individual biases."}, {"title": "V. RESULT", "content": "The performance of both classical and (Distributed) QTRL models is summarized in Tables I and II, with Fig. 1 pro- viding further insights into their benchmarking and speedup characteristics. Table I illustrates the comparative performance of a classical policy model against various QTRL models, including both centralized and distributed architectures. While the classical policy model achieves the highest average reward of 0.9244, the proposed distributed QTRL-3 model with 4 agents performs competitively, achieving an average reward of 0.901 with significantly fewer parameters (1749) compared to the classical model's 4835. This result demonstrates the ability of distributed quantum training to reduce model complexity while maintaining high performance. Notably, the distributed QTRL-3 model achieves a comparable reward to the central- ized QTRL-13, which requires a deeper QNN architecture.\nTable II highlights the speedup achieved by distributed QTRL models compared to their centralized counterparts. As seen, the distributed QTRL-3 model with 2 agents provides a speedup factor of 2.06, while configurations with 4 and 8 agents yield speedups of 3.33 and 5.33, respectively. These re- sults indicate a near-linear scaling of computational efficiency as the number of agents increases, confirming the efficacy of the distributed QTRL approach. Fig. 1(c) further validates this finding by demonstrating that the distributed QTRL models consistently reach the target reward in significantly fewer episodes compared to their centralized counterparts, thereby underscoring the advantage of distributed quantum training in RL tasks."}, {"title": "VI. CONCLUSION", "content": "This study presents the Dist-QTRL framework, which ef- fectively integrates distributed quantum computing with re- inforcement learning to address scalability and performance challenges in high-dimensional environments. By leveraging parameterized quantum circuits, the QTRL framework en- ables efficient policy generation, reduces trainable parameters compared to the classical RL model, and achieves significant speedups in training through parallelization across multiple QPUs (compared to centric QTRL with one QPU). Theoretical analyses and empirical results demonstrate the framework's convergence and highlight its superiority over classical ap- proaches in terms of both accuracy and scalability. This work contributes to advancing scalable quantum-enhanced RL systems, laying a foundation for more efficient and robust applications in complex, real-world scenarios. Future work will focus on quantum circuit partitioning for large-scale distributed quantum computing through the distributed circuit ansatz [30], [39], [40], as well as exploring differentiable quantum architecture search in asynchronous quantum rein- forcement learning [41] to further enhance scalability and performance."}]}