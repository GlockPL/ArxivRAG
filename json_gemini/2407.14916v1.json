{"title": "Improving Context-Aware Preference Modeling for Language Models", "authors": ["Silviu Pitis", "Ziang Xiao", "Nicolas Le Roux", "Alessandro Sordoni"], "abstract": "While finetuning language models (LMs) from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning mod-els with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.", "sections": [{"title": "1 Introduction", "content": "As the general purpose capabilities of Language Models (LMs) [11, 42] and other foundation models [9] progress toward handling diverse instructions and executing long-range trajectories in real-world applications [44, 47, 41], it becomes increasingly important to have a principled system for ensuring that LM agents behave as expected. The prevailing approach for aligning an LM to human preferences uses pairwise preferences between different outputs to finetune the LM [52, 6], which falls short of addressing the critical challenges presented by the reality of diverse user intents and contexts [51, 50, 13]. In the presence of unspecified contexts, such as the user's identity or goals, preference queries are notoriously ambiguous [61] and one typically observes poor agreement (~65%) between human annotators on binary preference queries [35, 58].\nIn this paper, we consider modeling preferences using a two-step, context-aware approach (Figure 1). This approach first resolves the underspecification by selecting a context [38, 22], and then evaluates preference with respect to the chosen context [60, 14, 27, 30, 57]. Decomposing general preference into context and context-specific preference has several potential advantages. First and foremost, this approach explicitly identifies contextual assumptions that underlie preference, and shifts the alignment burden from preference modeling to a hybrid of preference modeling and context supervision. Second, this approach is naturally pluralistic [51], allowing the model to adapt to diverse users and use cases. Finally, it offers more flexibility for principled aggregation: whereas the Bradley-Terry approach corresponds to aggregating contexts using the Borda rule [50], which may under-serve certain"}, {"title": "2 Context-Aware Preference Modeling", "content": "Current practice finetunes language models to make them consistent with a dataset of human prefer-ences [44, 42]. As noted by earlier works on learning from human feedback, however, ambiguous preference judgments present a major challenge:\nEvaluation of a [preference query] is both subjective and multidimensional ... this makes consistent labeling difficult for honest labelers (including the authors!) [61]\nThis ambiguity manifests itself with agreement rates as low as ~65% between human annotators on binary preference queries [35, 58]. One way to understand this difficulty appeals to a distinction drawn by Amartya Sen:\nA value judgment can be called 'basic' if the judgment is supposed to apply under all conceivable circumstances, and it is \u2018non-basic' otherwise. [49]\nMany, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in that certain contextual information might effectively reverse the judgment. For instance, the preferred response to a technical question may depend on the user's education level, the preferred response to a medical question may depend on whether the user is a doctor, and the preferred response to a question about etiquette may depend on the user's geographical location. If we train models using non-basic preference annotations, the contextual biases and assumptions underlying those judgments may be implicitly embedded into the model [36, 48].\nRather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely on the training process to consistently aggregate any disagreements in a singularly aligned model, one might instead consider an explicit context-aware approach to preference modeling (Figure 1). This approach first (partly) resolves ambiguity by specifying a context, and then models context-specific preference. This is not a new idea; Ziegler et al. [61], quoted above, continue to remark that, \u201cit seems better to design less ambiguous labeling tasks that get at the same information ... such as a verbal description [of the most important contextual information]\u201d, and many have advocated for fine-grained, context-conditioned, or otherwise more \u201cpluralistic\u201d approaches to alignment [57, 29, 51].\nHowever, while production systems recognize the importance of incorporating context\u2014the most notable being the \"system prompt\u201d introduced by OpenAI in 2023 and adopted by others [54, 3]\u2014there has been little published research on the context-aware preference modeling capabilities of language models. This paper works toward filling this gap by introducing the reasonable preference reversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the context-specific preference modeling capabilities of existing models."}, {"title": "2.1 Resolving ambiguity by making implicit assumptions explicit", "content": "Current practice finetunes language models to make them consistent with a dataset of human prefer-ences [44, 42]. As noted by earlier works on learning from human feedback, however, ambiguous preference judgments present a major challenge:\nEvaluation of a [preference query] is both subjective and multidimensional ... this makes consistent labeling difficult for honest labelers (including the authors!) [61]\nThis ambiguity manifests itself with agreement rates as low as ~65% between human annotators on binary preference queries [35, 58]. One way to understand this difficulty appeals to a distinction drawn by Amartya Sen:\nA value judgment can be called 'basic' if the judgment is supposed to apply under all conceivable circumstances, and it is \u2018non-basic' otherwise. [49]\nMany, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in that certain contextual information might effectively reverse the judgment. For instance, the preferred response to a technical question may depend on the user's education level, the preferred response to a medical question may depend on whether the user is a doctor, and the preferred response to a question about etiquette may depend on the user's geographical location. If we train models using non-basic preference annotations, the contextual biases and assumptions underlying those judgments may be implicitly embedded into the model [36, 48].\nRather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely on the training process to consistently aggregate any disagreements in a singularly aligned model, one might instead consider an explicit context-aware approach to preference modeling (Figure 1). This approach first (partly) resolves ambiguity by specifying a context, and then models context-specific preference. This is not a new idea; Ziegler et al. [61], quoted above, continue to remark that, \u201cit seems better to design less ambiguous labeling tasks that get at the same information ... such as a verbal description [of the most important contextual information]\u201d, and many have advocated for fine-grained, context-conditioned, or otherwise more \u201cpluralistic\u201d approaches to alignment [57, 29, 51].\nHowever, while production systems recognize the importance of incorporating context\u2014the most notable being the \"system prompt\u201d introduced by OpenAI in 2023 and adopted by others [54, 3]\u2014there has been little published research on the context-aware preference modeling capabilities of language models. This paper works toward filling this gap by introducing the reasonable preference reversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the context-specific preference modeling capabilities of existing models."}, {"title": "2.2 Related Work", "content": "Modeling human preferences traces back to Luce [39] and Bradley-Terry [10]. It made its way into finetuning language models via a line of work originating in reinforcement learning [2, 15, 61, 52], and has now become the dominant approach for finetuning language models to follow human instructions [44, 6, 4, 46]. While this approach has achieved remarkable results and enabled the performance of state-of-the-art models, several authors have pointed to its limitations [12, 45, 32, 51],"}, {"title": "3 Context Decomposition Upper Bound", "content": "We model the user-LM interaction using an intent-utility formalism (I, X, Y, u), where I is the space of intents, X is the space of prompts, Y is the space of completions, and u : I \u00d7 Y \u2192 R is a scalar utility function. We follow the standard assumption and assume that preference in this model is made according to the Bradley-Terry model [10, 15]. Letting \u03c3 be the logistic function, this defines the probability of preferring completion y\u2081 to y2 given a specific intent i as:\n$$P(y_1 \\succ y_2 | i) = \\sigma (u(i, y_1) \u2013 u(i, y_2)) .$$\nIn our model the primitive definitions of preference and utility are conditioned on the intent rather than the prompt. To prompt the model, a user with intent i selects a prompt x \u2208 X. To annotate a preference query (x, y1, y2), an annotator implicitly infers intent i from x and samples a preference from the Bernoulli distribution B[p(y1 > Y2 | i)].\nBoth users and annotators may possess or infer a distribution of intents. Indeed, we would argue that annotation for most preference queries involves a distribution of intents rather than a specific intent. We use \"intent\" to refer to both specific and distributional intents. We assume there exists a base distribution over possible intents p(i), as well as a conditional distribution over prompts given intents p(x | i), so that any prompt x has a natural inference distribution p(i | x). In this model, the prompt x is a partial specification of intent i. While a prompt may never be able to fully specify the intent, we may add some additional information or context z to obtain an extended prompt (x, z) \u2208 X. Let us suppose that z \u2208 Z, where Z corresponds to a discrete partition of I.\nOne way to measure utility and preference with respect to a distribution p of intents is with an expected utility model, which computes utility as u(p, y) = Ei\u223cp [u(i, y)]. While it has been shown that standard RLHF does not align with the expected utility model [50], this model satisfies certain desirable axioms, which one can argue would apply to \"ideal\" preference annotation. We use the expected utility model to define u(x, y) := u(p(\u00b7|x), y), and note that for any context partition"}, {"title": "3.1 Intent-Utility Formalism", "content": "We model the user-LM interaction using an intent-utility formalism (I, X, Y, u), where I is the space of intents, X is the space of prompts, Y is the space of completions, and u : I \u00d7 Y \u2192 R is a scalar utility function. We follow the standard assumption and assume that preference in this model is made according to the Bradley-Terry model [10, 15]. Letting \u03c3 be the logistic function, this defines the probability of preferring completion y\u2081 to y2 given a specific intent i as:\n$$P(y_1 \\succ y_2 | i) = \\sigma (u(i, y_1) \u2013 u(i, y_2)) .$$\nIn our model the primitive definitions of preference and utility are conditioned on the intent rather than the prompt. To prompt the model, a user with intent i selects a prompt x \u2208 X. To annotate a preference query (x, y1, y2), an annotator implicitly infers intent i from x and samples a preference from the Bernoulli distribution B[p(y1 > Y2 | i)].\nBoth users and annotators may possess or infer a distribution of intents. Indeed, we would argue that annotation for most preference queries involves a distribution of intents rather than a specific intent. We use \"intent\" to refer to both specific and distributional intents. We assume there exists a base distribution over possible intents p(i), as well as a conditional distribution over prompts given intents p(x | i), so that any prompt x has a natural inference distribution p(i | x). In this model, the prompt x is a partial specification of intent i. While a prompt may never be able to fully specify the intent, we may add some additional information or context z to obtain an extended prompt (x, z) \u2208 X. Let us suppose that z \u2208 Z, where Z corresponds to a discrete partition of I.\nOne way to measure utility and preference with respect to a distribution p of intents is with an expected utility model, which computes utility as u(p, y) = Ei\u223cp [u(i, y)]. While it has been shown that standard RLHF does not align with the expected utility model [50], this model satisfies certain desirable axioms, which one can argue would apply to \"ideal\" preference annotation. We use the expected utility model to define u(x, y) := u(p(\u00b7|x), y), and note that for any context partition"}, {"title": "3.2 Context Decomposition Upper Bound", "content": "During RLHF, we are presented with a dataset of prompt-preference tuples, D = {(x, Y1 > Y2)}, which we use to learn utility estimator \u00fb : X \u2192 Y (conventionally known as a \u201creward model\u201d). As was assumed for u and p(z | x), we would like there to be a model p(z | x) that satisfies the relation:\n$$u(x,y) = \\sum_z \\hat{p}(z|x)\\hat{u}((x, z), y).$$\nIn standard RLHF, we never learn such p. However, for purposes of this analysis, we will assume this \u00ee exists, either implicitly given \u00fb, or explicitly, such that given some Z, we compute \u00fb(x, y) via Equation (2) rather than via direct evaluation. Below, we will favor the latter interpretation.\nFor a given preference estimator (y1 \u27a4 y2), \u00fb is only unique up to constant shifts, so to measure the accuracy of \u00fb, we will instead compare \u25b3 and estimator \u2206(x, y1, y2) := \u00fb(x, y1) \u2212 \u00fb(x, y2).\nConsider now the absolute error |\u2206(x, y1, Y2) \u2013 \u2206(x, y1, Y2)| for a particular preference query, and use Az as shorthand for \u2206((x, z), Y1, Y2). For any Z we have the following bound:\n$$\\begin{aligned} |\\Delta(x, y_1, y_2) \u2013 \\hat{\\Delta}(x, y_1, y_2)| &= \\Big| \\sum_z p(z|x) \\Delta_z \u2013 \\sum_z \\hat{p}(z|x) \\hat{\\Delta}_z \\Big| \\\\ &= \\Big| \\sum_z p(z|x) [\\Delta_z \u2013 \\hat{\\Delta}_z] + \\sum_z \\hat{\\Delta}_z [p(z|x) - \\hat{p}(z | x)] \\Big| \\\\ &< \\sum_z \\hat{p}(z|x) |\\Delta_z \u2013 \\hat{\\Delta}_z| + \\sum_z |\\hat{\\Delta}_z||p(z|x) \u2013 \\hat{p}(z|x)| \\\\ &\\text{Context-weighted prediction error} \\quad \\text{Preference-weighted inference error} \\end{aligned}$$\nwhere the second equality adds and subtracts \u2211p(z|x)\u2206z and rearranges, and the final line uses the triangle inequality (multiple times).\nEquation (3) applies given a distribution of contexts, but in many cases, we might assume there is a specific context c (i.e., p(z = c) := 1) and make a single context prediction \u0109 (i.e., p(\u0109) = 1). This simplifies Equation (3) and gives us the context decomposition upper bound for a specific context:\n$$|\\Delta(x, y_1, y_2) \u2013 \\hat{\\Delta}(x,y_1, y_2) | \\leq | \\Delta_c \u2013 \\hat{\\Delta}_c | + |\\hat{\\Delta}_c \u2013 \\hat{\\Delta}_{\\hat{c}}|$$\nBoth the general bound (Equation (3)) and specific bound (Equation (4)) formalize the following intuitive claim: if we can make accurate predictions given the true context (or context distribution), then we can reduce the preference modeling problem to a context inference problem."}, {"title": "3.3 Discussion", "content": "The upper-bound in Equation (3) bounds the total error in terms of a context-weighted prediction error and a preference-weighted inference error. On one extreme, we have Z = () (standard preference modeling), so that the context inference error is zero and the prediction error exclusively depends on the preference prediction problem given the prompt. On the other hand, we have Z = Y (e.g., z might be \"The preferred response is [y].\") and our preference prediction error is zero, but the context inference problem becomes equivalent to generation. In between, we conjecture that there is a smooth trade-off between prediction error and inference error. This might be the case, for example, if a single context could apply to and disambiguate a range of different preference queries. We consider this in our experiments and find some support for the conjecture, in that conditioning on specific criteria outperforms conditioning on more abstract, yet still rather specific scenarios (Table 2) which outperforms conditioning on a user profile that applies to all preference queries at once (Table 5).\nIf our model \u00fb is very good at predicting preference given some additional context Z, the preference modeling problem can be largely reduced to a context inference (or specification) problem. In this case, rather than have annotators rank completions for ambiguous prompts, it may make sense to spend the annotation resources to specify additional context. Such annotations could then be used"}, {"title": "4 Reasonable Preference Reversal (RPR) Datasets", "content": "We contribute a set of carefully designed, synthetic preference datasets to measure whether preference prediction is sensitive to context. Our datasets, inspired by the notion of non-basic judgments and preference reversal described in Section 2, include over 20,000 paired tuples of prompt, context, and preference judgments, i.e. (x, z, Y1 \u27a4 Y2). The samples are paired so that preference between two completions for the same prompt is entirely ambiguous without context: for every context, there is an alternative context for which preference reverses. As compared to the only prior context-conditioned preference dataset [30], where context-conditioned preference is highly correlated with unconditioned preference (see Table 2), our design choice ensures that preference prediction performance on this dataset is determined solely by the model's ability to pay attention to and interpret the context."}, {"title": "5 Experiments", "content": "Our experiments aim to answer the following questions of context-aware preference modeling:\n1. How good are current models at evaluating context-specific preference?\n2. Can we improve context-aware preference modeling by finetuning on our RPR datasets?\n3. Can a single context compress preferences with respect to a diverse range of prompts?"}, {"title": "5.1 Setup", "content": "Models We report results for a selection of primarily open source models, detailed in Appendix D.1. These include two unconditioned reward models (UltraRM [17] and Mistral RM [56]), one context-aware preference model (Prometheus-2 [30]), our finetuned context-aware reward model (Mis-tral CARM), and a set of generative models (four Llama 3 models [1] and GPT-4 Turbo [42]) used with an \u201cllm-as-a-judge\u201d approach. In preliminary experiments we tested a several other models and observed similar patterns across all models. All models except Prometheus-2 are used as reward models, by first evaluating each alternative individually and then comparing scores.\nDatasets Besides the RPR datasets detailed in Section 4, we use the following preference datasets:\n\u2022 Preference Bench [30] (hf: prometheus-eval/Preference-Bench) consists of 1998 context-conditioned preference samples synthesized by GPT-4 as part of Feedback Bench [29].\n\u2022 Multifaceted Bench [34] (hf:kaist-ai/Multifaceted-Bench) contains 921 samples of prompt, system prompt, and completion. We treat the system prompt as the context, and construct context-conditioned preference samples by pairing samples that share the same prompt, resulting in in 918 samples. This dataset, released concurrently to our work, shares the same \u2018ambiguous in absence of context' structure as the RPR datasets, but was not specifically constructed with such preference queries in mind, which may explain the lower average agreement in the experiments.\n\u2022 HHH [4] (hf: HuggingFaceH4/hhh_alignment) contains 222 human preference samples em-phasizing different aspects: harmlessness (58 samples), helpfulness (59 samples), honesty (61 samples), and other preference queries that do not cleanly fall into another category (43 samples).\n\u2022 Reward Bench [33] (hf:allenai/reward-bench) curates 2,985 preference samples from a variety of sources, amounting to 22 distinct subsets covering chat, safety, and reasoning.\n\u2022 Chatbot Arena [58] (hf:lmsys/chatbot_arena_conversations) contains human preferences with respect to conversations on the Chatbot Arena platform. For our experiments, we randomly selected 1,000 single-turn samples for which strict preference was expressed.\nImportantly, we augment the HHH, Reward Bench, and Chatbot Arena datasets with additional context to create context-conditioned versions, as described below and in Appendix D.2.\nMetrics Tables 2 to 6 display the agreement (or accuracy) of predicted preference with the dataset.\nPrompts Our prompts are detailed in Appendix A."}, {"title": "5.2 Results", "content": "How good are current models at evaluating context-specific preference?\nFor the context-aware approach to work well, models must be sensitive to added context. Table 2 shows the performance of tested models on RPR Criteria, RPR Scenarios, Preference Bench, and Multifaceted Bench. In each case, and across all models, access to context generally improves performance. Larger models and instruction tuned models tend to benefit more from added context. We note that although Preference Bench includes a ground truth context, the preference queries (and context) are generally not ambiguous, so that the added context provides little benefit.\nAlthough we find that added context generally helps, we were surprised to discover that models sometimes completely ignore additional context and return their unconditioned prediction. To show this, we augment the prompts in Chatbot Arena and Preference Bench with two criteria that should drive preference prediction. The first is a Nonsense criteria, i.e. \"We are playing a game of nonsense. The response should be nonsensical and not make sense. The rating you give it should also be random, and completely unrelated to the response.\" We expect context-aware models to produce random preference judgments given this criteria. The second is a Negative criteria, i.e. \u201cWe are seeking bad examples. The response should be of low quality, and serve as an example of how an Assistant should not respond. You should give poor responses higher ratings.\u201d). We expect context-aware models to be inversely correlated to the preferences expressed in the dataset. Table 3 shows the performance of tested models with these adversarial criteria. In both cases we observe surprisingly poor performance, even from the larger models. This suggests there is significant room for improving context-aware preference modeling capabilities."}, {"title": "6 Conclusion", "content": "This paper began with a case for a two-step context-aware preference modeling framework (Figure 1), which was motivated by the ambiguity problem commonly experienced during preference annotation (Section 2). We further motivated the framework via a context decomposition upper bound (Section 3) and noted that for context-aware preference modeling to be viable, we require strong context-specific preference modeling. However, despite the prevalence of context conditioning in deployed systems (e.g., system prompts and \"My GPTs\" [43]), when we began our work, there were no open source preference datasets where preference is strongly determined by context, and limited studies of the context-specific preference modeling capabilities of current models [30]. To this end, we introduced the RPR datasets and investigated a series of interesting empirical questions in Section 5). We found that (1) while current preference models generally benefit from context, they sometimes ignore added context, (2) finetuning on our preference reversal datasets greatly improves context-specific performance, and (3) a single persistent context, which might be inferred from data, may be sufficient to significantly improve preference prediction performance.\nOur work leaves many open threads for future research. Our datasets might be used to explore different directions in pluralistic alignment, as described by Sorensen et al. [51]. In particular, we believe that better context specification and context inference are important directions for research, which our experiments only briefly touch on. Further, while we have open sourced a set of context-conditioned preference datasets, these datasets have limited diversity (for example, they are limited to single turn conversations), and were synthesized rather than collected from real human feedback, which limits their practical relevance. Additional open, preferably human sourced datasets with context annotations are need to more fruitfully pursue this research direction."}, {"title": "A Prompts for context-conditioned preference prediction", "content": "The following template is used for \u201cllm-as-a-judge\" models (Llama 3 and GPT-4 Turbo). Llama-3 uses the logit_template (see Appendix D.1 for how the score is computed). GPT-4 Turbo uses the argmax_score_template_no_cot and runs inference with temperature = 0.\n\nCRITERIA_TEMPLATE = {\n\t\"type\": \"logit_rating\",\n\t\"name\": \"criteria_template\",\n\t\"system_prompt\": \"You are a helpful assistant that scores other AI assistants based on a given criteria and the quality of their answers.\",\n\t\"logit_template\": \"Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\\n\\n{{context}}\\n\\nYour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. Please rate the assistant's response(s) on a scale of 1 to {{max_score}}, where 1 corresponds to extremely poor (criteria is NOT satisfied) and {{max_score}} corresponds to excellent (criteria is satisfied). Format your answer as: 'I give the assistant a score of X/{{max_score}}, because...', where X is your score.\\n\\n[[CONVERSATION]]\\n\\n{{conversation}}\",\n\t\"logit_completion_template\": \"I give the assistant a score of \",\n\t\"argmax_score_template\": \"Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\\n\\n{{context}}\\n\\nYour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. Begin your evaluation by providing a short explanation. After providing your explanation, please rate the response with respect to the criteria on a scale of 1 to 10 by strictly following this format: \\\" Rating: [[5]]\\\".\\n\\n[[CONVERSATION]] \\n\\n{{conversation}}\",\n\t\"argmax_score_template_no_cot\": \"Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\\n\\n{{context}}\\n\\nYour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. IMPORTANT: Begin your response by directly rating the AI assistant (with respect to the criteria) on a scale of 1 to 10, strictly following this format: \\\"[[rating]]\\\". For example, if you give the AI assistant a score of 5, your response should start with: \\\"[[5]]\\\".\\n\\n [[CONVERSATION]]\\n\\n{{conversation}}\"\n}\nMistral RM and our finetuned version use the following template:\ndef prompt_with_context(prompt, completion, context=None):\n\tif context is not None:\n\t\tprompt = f\" [INST] Please continue the following conversation by providing the Assistant's response, in order to maximize the score that a reasonable human evaluator would assign to the Assistant's response given the following criteria or context: {{context}}\\n\\n [[Conversation]] \\n\\n{{prompt}} [/INST]\" + f' {{completion}}</s>'\n\telse:\n\t\tprompt = f\" [INST] Please continue the following conversation by providing the Assistant's response: \\n\\n [[Conversation]] \\n\\n{{prompt}} [/INST]\" + f' {{completion}}</s>'\n\treturn prompt\nThe prompt templates for Ultra RM and Prometheus follow the format set out by the authors of those models. Since no reference answer was available for Prometheus, we use the following slight modification which omits the reference answer:\nPROMETHEUS_TEMPLATE = \"\"\"###Task Description:\nAn instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.\n1. Write a detailed feedback that assess the quality of two responses strictly based on the given score rubric, not evaluating in general.\n2. After writing a feedback, choose a better response between Response A and Response B. You should refer to the score rubric."}, {"title": "B Constructing the RPR Datasets", "content": "We construct the datasets using the following steps:\n1. Collect a set of diverse prompts.\n2. Synthesize the initial RPR Criteria dataset.\n3. Self critique and filter the RPR Criteria dataset.\n4. Synthesize the RPR Scenarios dataset, using the RPR Criteria dataset as a base.\n5. Self critique and filter the RPR Scenarios dataset.\n6. Filter RPR Criteria down to what is left after the RPR Scenarios synthesis, so that each prompt has 2 criteria, 2 corresponding scenarios, and 2 corresponding completions.\nIn Step 1, we used all prompts from the Ultrafeedback dataset [17], which have already been selected for diversity, and filtered it down to prompts where there was no explicitly correct answer (according to the Ultrafeedback annotations). This left us with 42,000 initial prompts.\nWe passed these prompts through step 2 of our synthesis procedure:"}, {"title": "B.1 RPR Profiles", "content": "To make the RPR profiles extension of RPR, we sample 20 sets of 20 random samples from the training set (assigning the preference for each sample at random). We use these samples to seed an initial sample of 20 profiles, each of which we infer from 20 samples using GPT-4 Turbo and the following profile inference prompt:\n\"\"\"\nI would like help generating a profile of a user who is conversing with an AI assistant. This profile will be used to determine how the user balances trade-offs between different criteria when evaluating the AI assistant's responses. An an example from prior users, a hypothetical profile might include highly detail-oriented, but not very creative or original\". The user profile should capture as much relevant information from the responses as possible, and provide broad coverage of the User's potential preferences going forward. Maintain a simple and easy to understand style for the profile's language: use full paragraphs (not bullets), but make sure to avoid uncommon or exaggerated language such as \"prowess\", \"symbiotic\", etc. and avoid extraneous adjectives/adverbs. Do not fabricate information or make assumptions about the user's preferences that are not supported by the provided data; this is not necessarily an average user who has typical preferences.\nTo generate the profile, you are given a set of (prompt, preferred response, rejected response) tuples of expressed user preferences below. You will infer the user's preferences from these tuples. You will do this step-by-step:\n1. If there is sufficient data, cluster the expressed preferences into groups that represent similar values of the user.\n2. Based on step 1, draft an initial profile. Make it as long as necessary to properly capture the User 's nuanced preferences.\n3. Critique the initial profile by identifying any missing or incorrect inferences with respect to the expressed user preferences. Is the profile internally consistent? Is it consistent with all of the expressed user preferences?\n4. Expand/revise the profile to address your reasoning in steps 3 and add in anything you missed. Remove extraneous adjectives and any uncommon or exaggerated language."}, {"title": "C Finetuning Details", "content": "We finetune our model using the Mistral RM prompt in Appendix A. We use 10,167 samples from RPR criteria (training set, 1 sample at random from each paired sample), 10,167 random samples from RPR scenarios (training set, 1 sample at random from each paired sample), and 13,333 random samples from Ultrafeedback. To set the context in the latter, we sample a random \"general\" context from:"}, {"title": "C.1 Data Composition Ablation", "content": "For completion", "hf": "prometheus-eval/Preference-Collection)"}, {"hf": "kaist-ai/Multifaceted-Collection). As the dataset includes multiple system prompts for the same user instruction", "mixtures": "n\u2022 PC: 40", "PC+MF+UF": "13,333", "RPR+UF": "10,167", "title": "Improving Context-Aware Preference Modeling for Language Models", "authors": ["Silviu Pitis", "Ziang Xiao", "Nicolas Le Roux", "Alessandro Sordoni"], "abstract": "While finetuning language models (LMs) from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning mod-els with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.", "sections": [{"title": "1 Introduction", "content": "As the general purpose capabilities of Language Models (LMs) [11, 42] and other foundation models [9] progress toward handling diverse instructions and executing long-range trajectories in real-world applications [44, 47, 41], it becomes increasingly important to have a principled system for ensuring that LM agents behave as expected. The prevailing approach for aligning an LM to human preferences uses pairwise preferences between different outputs to finetune the LM [52, 6], which falls short of addressing the critical challenges presented by the reality of diverse user intents and contexts [51, 50, 13]. In the presence of unspecified contexts, such as the user's identity or goals, preference queries are notoriously ambiguous [61] and one typically observes poor agreement (~65%) between human annotators on binary preference queries [35, 58].\nIn this paper, we consider modeling preferences using a two-step, context-aware approach (Figure 1). This approach first resolves the underspecification by selecting a context [38, 22], and then evaluates preference with respect to the chosen context [60, 14, 27, 30, 57]. Decomposing general preference into context and context-specific preference has several potential advantages. First and foremost, this approach explicitly identifies contextual assumptions that underlie preference, and shifts the alignment burden from preference modeling to a hybrid of preference modeling and context supervision. Second, this approach is naturally pluralistic [51], allowing the model to adapt to diverse users and use cases. Finally, it offers more flexibility for principled aggregation: whereas the Bradley-Terry approach corresponds to aggregating contexts using the Borda rule [50], which may under-serve certain"}, {"title": "2 Context-Aware Preference Modeling", "content": "Current practice finetunes language models to make them consistent with a dataset of human prefer-ences [44, 42]. As noted by earlier works on learning from human feedback, however, ambiguous preference judgments present a major challenge:\nEvaluation of a [preference query] is both subjective and multidimensional ... this makes consistent labeling difficult for honest labelers (including the authors!) [61]\nThis ambiguity manifests itself with agreement rates as low as ~65% between human annotators on binary preference queries [35, 58]. One way to understand this difficulty appeals to a distinction drawn by Amartya Sen:\nA value judgment can be called 'basic' if the judgment is supposed to apply under all conceivable circumstances, and it is \u2018non-basic' otherwise. [49]\nMany, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in that certain contextual information might effectively reverse the judgment. For instance, the preferred response to a technical question may depend on the user's education level, the preferred response to a medical question may depend on whether the user is a doctor, and the preferred response to a question about etiquette may depend on the user's geographical location. If we train models using non-basic preference annotations, the contextual biases and assumptions underlying those judgments may be implicitly embedded into the model [36, 48].\nRather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely on the training process to consistently aggregate any disagreements in a singularly aligned model, one might instead consider an explicit context-aware approach to preference modeling (Figure 1). This approach first (partly) resolves ambiguity by specifying a context, and then models context-specific preference. This is not a new idea; Ziegler et al. [61], quoted above, continue to remark that, \u201cit seems better to design less ambiguous labeling tasks that get at the same information ... such as a verbal description [of the most important contextual information]\u201d, and many have advocated for fine-grained, context-conditioned, or otherwise more \u201cpluralistic\u201d approaches to alignment [57, 29, 51].\nHowever, while production systems recognize the importance of incorporating context\u2014the most notable being the \"system prompt\u201d introduced by OpenAI in 2023 and adopted by others [54, 3]\u2014there has been little published research on the context-aware preference modeling capabilities of language models. This paper works toward filling this gap by introducing the reasonable preference reversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the context-specific preference modeling capabilities of existing models."}, {"title": "2.1 Resolving ambiguity by making implicit assumptions explicit", "content": "Current practice finetunes language models to make them consistent with a dataset of human prefer-ences [44, 42]. As noted by earlier works on learning from human feedback, however, ambiguous preference judgments present a major challenge:\nEvaluation of a [preference query] is both subjective and multidimensional ... this makes consistent labeling difficult for honest labelers (including the authors!) [61]\nThis ambiguity manifests itself with agreement rates as low as ~65% between human annotators on binary preference queries [35, 58]. One way to understand this difficulty appeals to a distinction drawn by Amartya Sen:\nA value judgment can be called 'basic' if the judgment is supposed to apply under all conceivable circumstances, and it is \u2018non-basic' otherwise. [49]\nMany, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in that certain contextual information might effectively reverse the judgment. For instance, the preferred response to a technical question may depend on the user's education level, the preferred response to a medical question may depend on whether the user is a doctor, and the preferred response to a question about etiquette may depend on the user's geographical location. If we train models using non-basic preference annotations, the contextual biases and assumptions underlying those judgments may be implicitly embedded into the model [36, 48].\nRather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely on the training process to consistently aggregate any disagreements in a singularly aligned model, one might instead consider an explicit context-aware approach to preference modeling (Figure 1). This approach first (partly) resolves ambiguity by specifying a context, and then models context-specific preference. This is not a new idea; Ziegler et al. [61], quoted above, continue to remark that, \u201cit seems better to design less ambiguous labeling tasks that get at the same information ... such as a verbal description [of the most important contextual information]\u201d, and many have advocated for fine-grained, context-conditioned, or otherwise more \u201cpluralistic\u201d approaches to alignment [57, 29, 51].\nHowever, while production systems recognize the importance of incorporating context\u2014the most notable being the \"system prompt\u201d introduced by OpenAI in 2023 and adopted by others [54, 3]\u2014there has been little published research on the context-aware preference modeling capabilities of language models. This paper works toward filling this gap by introducing the reasonable preference reversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the context-specific preference modeling capabilities of existing models."}, {"title": "2.2 Related Work", "content": "Modeling human preferences traces back to Luce [39] and Bradley-Terry [10]. It made its way into finetuning language models via a line of work originating in reinforcement learning [2, 15, 61, 52], and has now become the dominant approach for finetuning language models to follow human instructions [44, 6, 4, 46]. While this approach has achieved remarkable results and enabled the performance of state-of-the-art models, several authors have pointed to its limitations [12, 45, 32, 51],"}, {"title": "3 Context Decomposition Upper Bound", "content": "We model the user-LM interaction using an intent-utility formalism (I, X, Y, u), where I is the space of intents, X is the space of prompts, Y is the space of completions, and u : I \u00d7 Y \u2192 R is a scalar utility function. We follow the standard assumption and assume that preference in this model is made according to the Bradley-Terry model [10, 15]. Letting \u03c3 be the logistic function, this defines the probability of preferring completion y\u2081 to y2 given a specific intent i as:\n$$P(y_1 \\succ y_2 | i) = \\sigma (u(i, y_1) \u2013 u(i, y_2)) .$$\nIn our model the primitive definitions of preference and utility are conditioned on the intent rather than the prompt. To prompt the model, a user with intent i selects a prompt x \u2208 X. To annotate a preference query (x, y1, y2), an annotator implicitly infers intent i from x and samples a preference from the Bernoulli distribution B[p(y1 > Y2 | i)].\nBoth users and annotators may possess or infer a distribution of intents. Indeed, we would argue that annotation for most preference queries involves a distribution of intents rather than a specific intent. We use \"intent\" to refer to both specific and distributional intents. We assume there exists a base distribution over possible intents p(i), as well as a conditional distribution over prompts given intents p(x | i), so that any prompt x has a natural inference distribution p(i | x). In this model, the prompt x is a partial specification of intent i. While a prompt may never be able to fully specify the intent, we may add some additional information or context z to obtain an extended prompt (x, z) \u2208 X. Let us suppose that z \u2208 Z, where Z corresponds to a discrete partition of I.\nOne way to measure utility and preference with respect to a distribution p of intents is with an expected utility model, which computes utility as u(p, y) = Ei\u223cp [u(i, y)]. While it has been shown that standard RLHF does not align with the expected utility model [50], this model satisfies certain desirable axioms, which one can argue would apply to \"ideal\" preference annotation. We use the expected utility model to define u(x, y) := u(p(\u00b7|x), y), and note that for any context partition"}, {"title": "3.1 Intent-Utility Formalism", "content": "We model the user-LM interaction using an intent-utility formalism (I, X, Y, u), where I is the space of intents, X is the space of prompts, Y is the space of completions, and u : I \u00d7 Y \u2192 R is a scalar utility function. We follow the standard assumption and assume that preference in this model is made according to the Bradley-Terry model [10, 15]. Letting \u03c3 be the logistic function, this defines the probability of preferring completion y\u2081 to y2 given a specific intent i as:\n$$P(y_1 \\succ y_2 | i) = \\sigma (u(i, y_1) \u2013 u(i, y_2)) .$$\nIn our model the primitive definitions of preference and utility are conditioned on the intent rather than the prompt. To prompt the model, a user with intent i selects a prompt x \u2208 X. To annotate a preference query (x, y1, y2), an annotator implicitly infers intent i from x and samples a preference from the Bernoulli distribution B[p(y1 > Y2 | i)].\nBoth users and annotators may possess or infer a distribution of intents. Indeed, we would argue that annotation for most preference queries involves a distribution of intents rather than a specific intent. We use \"intent\" to refer to both specific and distributional intents. We assume there exists a base distribution over possible intents p(i), as well as a conditional distribution over prompts given intents p(x | i), so that any prompt x has a natural inference distribution p(i | x). In this model, the prompt x is a partial specification of intent i. While a prompt may never be able to fully specify the intent, we may add some additional information or context z to obtain an extended prompt (x, z) \u2208 X. Let us suppose that z \u2208 Z, where Z corresponds to a discrete partition of I.\nOne way to measure utility and preference with respect to a distribution p of intents is with an expected utility model, which computes utility as u(p, y) = Ei\u223cp [u(i, y)]. While it has been shown that standard RLHF does not align with the expected utility model [50], this model satisfies certain desirable axioms, which one can argue would apply to \"ideal\" preference annotation. We use the expected utility model to define u(x, y) := u(p(\u00b7|x), y), and note that for any context partition"}, {"title": "3.2 Context Decomposition Upper Bound", "content": "During RLHF, we are presented with a dataset of prompt-preference tuples, D = {(x, Y1 > Y2)}, which we use to learn utility estimator \u00fb : X \u2192 Y (conventionally known as a \u201creward model\u201d). As was assumed for u and p(z | x), we would like there to be a model p(z | x) that satisfies the relation:\n$$u(x,y) = \\sum_z \\hat{p}(z|x)\\hat{u}((x, z), y).$$\nIn standard RLHF, we never learn such p. However, for purposes of this analysis, we will assume this \u00ee exists, either implicitly given \u00fb, or explicitly, such that given some Z, we compute \u00fb(x, y) via Equation (2) rather than via direct evaluation. Below, we will favor the latter interpretation.\nFor a given preference estimator (y1 \u27a4 y2), \u00fb is only unique up to constant shifts, so to measure the accuracy of \u00fb, we will instead compare \u25b3 and estimator \u2206(x, y1, y2) := \u00fb(x, y1) \u2212 \u00fb(x, y2).\nConsider now the absolute error |\u2206(x, y1, Y2) \u2013 \u2206(x, y1, Y2)| for a particular preference query, and use Az as shorthand for \u2206((x, z), Y1, Y2). For any Z we have the following bound:\n$$\\begin{aligned} |\\Delta(x, y_1, y_2) \u2013 \\hat{\\Delta}(x, y_1, y_2)| &= \\Big| \\sum_z p(z|x) \\Delta_z \u2013 \\sum_z \\hat{p}(z|x) \\hat{\\Delta}_z \\Big| \\\\ &= \\Big| \\sum_z p(z|x) [\\Delta_z \u2013 \\hat{\\Delta}_z] + \\sum_z \\hat{\\Delta}_z [p(z|x) - \\hat{p}(z | x)] \\Big| \\\\ &< \\sum_z \\hat{p}(z|x) |\\Delta_z \u2013 \\hat{\\Delta}_z| + \\sum_z |\\hat{\\Delta}_z||p(z|x) \u2013 \\hat{p}(z|x)| \\\\ &\\text{Context-weighted prediction error} \\quad \\text{Preference-weighted inference error} \\end{aligned}$$\nwhere the second equality adds and subtracts \u2211p(z|x)\u2206z and rearranges, and the final line uses the triangle inequality (multiple times).\nEquation (3) applies given a distribution of contexts, but in many cases, we might assume there is a specific context c (i.e., p(z = c) := 1) and make a single context prediction \u0109 (i.e., p(\u0109) = 1). This simplifies Equation (3) and gives us the context decomposition upper bound for a specific context:\n$$|\\Delta(x, y_1, y_2) \u2013 \\hat{\\Delta}(x,y_1, y_2) | \\leq | \\Delta_c \u2013 \\hat{\\Delta}_c | + |\\hat{\\Delta}_c \u2013 \\hat{\\Delta}_{\\hat{c}}|$$\nBoth the general bound (Equation (3)) and specific bound (Equation (4)) formalize the following intuitive claim: if we can make accurate predictions given the true context (or context distribution), then we can reduce the preference modeling problem to a context inference problem."}, {"title": "3.3 Discussion", "content": "The upper-bound in Equation (3) bounds the total error in terms of a context-weighted prediction error and a preference-weighted inference error. On one extreme, we have Z = () (standard preference modeling), so that the context inference error is zero and the prediction error exclusively depends on the preference prediction problem given the prompt. On the other hand, we have Z = Y (e.g., z might be \"The preferred response is [y].\") and our preference prediction error is zero, but the context inference problem becomes equivalent to generation. In between, we conjecture that there is a smooth trade-off between prediction error and inference error. This might be the case, for example, if a single context could apply to and disambiguate a range of different preference queries. We consider this in our experiments and find some support for the conjecture, in that conditioning on specific criteria outperforms conditioning on more abstract, yet still rather specific scenarios (Table 2) which outperforms conditioning on a user profile that applies to all preference queries at once (Table 5).\nIf our model \u00fb is very good at predicting preference given some additional context Z, the preference modeling problem can be largely reduced to a context inference (or specification) problem. In this case, rather than have annotators rank completions for ambiguous prompts, it may make sense to spend the annotation resources to specify additional context. Such annotations could then be used"}, {"title": "4 Reasonable Preference Reversal (RPR) Datasets", "content": "We contribute a set of carefully designed, synthetic preference datasets to measure whether preference prediction is sensitive to context. Our datasets, inspired by the notion of non-basic judgments and preference reversal described in Section 2, include over 20,000 paired tuples of prompt, context, and preference judgments, i.e. (x, z, Y1 \u27a4 Y2). The samples are paired so that preference between two completions for the same prompt is entirely ambiguous without context: for every context, there is an alternative context for which preference reverses. As compared to the only prior context-conditioned preference dataset [30], where context-conditioned preference is highly correlated with unconditioned preference (see Table 2), our design choice ensures that preference prediction performance on this dataset is determined solely by the model's ability to pay attention to and interpret the context."}, {"title": "5 Experiments", "content": "Our experiments aim to answer the following questions of context-aware preference modeling:\n1. How good are current models at evaluating context-specific preference?\n2. Can we improve context-aware preference modeling by finetuning on our RPR datasets?\n3. Can a single context compress preferences with respect to a diverse range of prompts?"}, {"title": "5.1 Setup", "content": "Models We report results for a selection of primarily open source models, detailed in Appendix D.1. These include two unconditioned reward models (UltraRM [17] and Mistral RM [56]), one context-aware preference model (Prometheus-2 [30]), our finetuned context-aware reward model (Mis-tral CARM), and a set of generative models (four Llama 3 models [1] and GPT-4 Turbo [42]) used with an \u201cllm-as-a-judge\u201d approach. In preliminary experiments we tested a several other models and observed similar patterns across all models. All models except Prometheus-2 are used as reward models, by first evaluating each alternative individually and then comparing scores.\nDatasets Besides the RPR datasets detailed in Section 4, we use the following preference datasets:\n\u2022 Preference Bench [30] (hf: prometheus-eval/Preference-Bench) consists of 1998 context-conditioned preference samples synthesized by GPT-4 as part of Feedback Bench [29].\n\u2022 Multifaceted Bench [34] (hf:kaist-ai/Multifaceted-Bench) contains 921 samples of prompt, system prompt, and completion. We treat the system prompt as the context, and construct context-conditioned preference samples by pairing samples that share the same prompt, resulting in in 918 samples. This dataset, released concurrently to our work, shares the same \u2018ambiguous in absence of context' structure as the RPR datasets, but was not specifically constructed with such preference queries in mind, which may explain the lower average agreement in the experiments.\n\u2022 HHH [4] (hf: HuggingFaceH4/hhh_alignment) contains 222 human preference samples em-phasizing different aspects: harmlessness (58 samples), helpfulness (59 samples), honesty (61 samples), and other preference queries that do not cleanly fall into another category (43 samples).\n\u2022 Reward Bench [33] (hf:allenai/reward-bench) curates 2,985 preference samples from a variety of sources, amounting to 22 distinct subsets covering chat, safety, and reasoning.\n\u2022 Chatbot Arena [58] (hf:lmsys/chatbot_arena_conversations) contains human preferences with respect to conversations on the Chatbot Arena platform. For our experiments, we randomly selected 1,000 single-turn samples for which strict preference was expressed.\nImportantly, we augment the HHH, Reward Bench, and Chatbot Arena datasets with additional context to create context-conditioned versions, as described below and in Appendix D.2.\nMetrics Tables 2 to 6 display the agreement (or accuracy) of predicted preference with the dataset.\nPrompts Our prompts are detailed in Appendix A."}, {"title": "5.2 Results", "content": "How good are current models at evaluating context-specific preference?\nFor the context-aware approach to work well, models must be sensitive to added context. Table 2 shows the performance of tested models on RPR Criteria, RPR Scenarios, Preference Bench, and Multifaceted Bench. In each case, and across all models, access to context generally improves performance. Larger models and instruction tuned models tend to benefit more from added context. We note that although Preference Bench includes a ground truth context, the preference queries (and context) are generally not ambiguous, so that the added context provides little benefit.\nAlthough we find that added context generally helps, we were surprised to discover that models sometimes completely ignore additional context and return their unconditioned prediction. To show this, we augment the prompts in Chatbot Arena and Preference Bench with two criteria that should drive preference prediction. The first is a Nonsense criteria, i.e. \"We are playing a game of nonsense. The response should be nonsensical and not make sense. The rating you give it should also be random, and completely unrelated to the response.\" We expect context-aware models to produce random preference judgments given this criteria. The second is a Negative criteria, i.e. \u201cWe are seeking bad examples. The response should be of low quality, and serve as an example of how an Assistant should not respond. You should give poor responses higher ratings.\u201d). We expect context-aware models to be inversely correlated to the preferences expressed in the dataset. Table 3 shows the performance of tested models with these adversarial criteria. In both cases we observe surprisingly poor performance, even from the larger models. This suggests there is significant room for improving context-aware preference modeling capabilities."}, {"title": "6 Conclusion", "content": "This paper began with a case for a two-step context-aware preference modeling framework (Figure 1), which was motivated by the ambiguity problem commonly experienced during preference annotation (Section 2). We further motivated the framework via a context decomposition upper bound (Section 3) and noted that for context-aware preference modeling to be viable, we require strong context-specific preference modeling. However, despite the prevalence of context conditioning in deployed systems (e.g., system prompts and \"My GPTs\" [43]), when we began our work, there were no open source preference datasets where preference is strongly determined by context, and limited studies of the context-specific preference modeling capabilities of current models [30]. To this end, we introduced the RPR datasets and investigated a series of interesting empirical questions in Section 5). We found that (1) while current preference models generally benefit from context, they sometimes ignore added context, (2) finetuning on our preference reversal datasets greatly improves context-specific performance, and (3) a single persistent context, which might be inferred from data, may be sufficient to significantly improve preference prediction performance.\nOur work leaves many open threads for future research. Our datasets might be used to explore different directions in pluralistic alignment, as described by Sorensen et al. [51]. In particular, we believe that better context specification and context inference are important directions for research, which our experiments only briefly touch on. Further, while we have open sourced a set of context-conditioned preference datasets, these datasets have limited diversity (for example, they are limited to single turn conversations), and were synthesized rather than collected from real human feedback, which limits their practical relevance. Additional open, preferably human sourced datasets with context annotations are need to more fruitfully pursue this research direction."}, {"title": "A Prompts for context-conditioned preference prediction", "content": "The following template is used for \u201cllm-as-a-judge\" models (Llama 3 and GPT-4 Turbo). Llama-3 uses the logit_template (see Appendix D.1 for how the score is computed). GPT-4 Turbo uses the argmax_score_template_no_cot and runs inference with temperature = 0.\n\nCRITERIA_TEMPLATE = {\n\t\"type\": \"logit_rating\",\n\t\"name\": \"criteria_template\",\n\t\"system_prompt\": \"You are a helpful assistant that scores other AI assistants based on a given criteria and the quality of their answers.\",\n\t\"logit_template\": \"Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\\n\\n{{context}}\\n\\nYour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. Please rate the assistant's response(s) on a scale of 1 to {{max_score}}, where 1 corresponds to extremely poor (criteria is NOT satisfied) and {{max_score}} corresponds to excellent (criteria is satisfied). Format your answer as: 'I give the assistant a score of X/{{max_score}}, because...', where X is your score.\\n\\n[[CONVERSATION]]\\n\\n{{conversation}}\",\n\t\"logit_completion_template\": \"I give the assistant a score of \",\n\t\"argmax_score_template\": \"Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\\n\\n{{context}}\\n\\nYour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. Begin your evaluation by providing a short explanation. After providing your explanation, please rate the response with respect to the criteria on a scale of 1 to 10 by strictly following this format: \\\" Rating: [[5]]\\\".\\n\\n[[CONVERSATION]] \\n\\n{{conversation}}\",\n\t\"argmax_score_template_no_cot\": \"Rate the quality of the AI assistant's response(s) in the conversation displayed below according to the following criteria:\\n\\n{{context}}\\n\\nYour score should reflect the quality of the AI assistant's response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by a reasonable human evaluator. IMPORTANT: Begin your response by directly rating the AI assistant (with respect to the criteria) on a scale of 1 to 10, strictly following this format: \\\"[[rating]]\\\". For example, if you give the AI assistant a score of 5, your response should start with: \\\"[[5]]\\\".\\n\\n [[CONVERSATION]]\\n\\n{{conversation}}\"\n}\nMistral RM and our finetuned version use the following template:\ndef prompt_with_context(prompt, completion, context=None):\n\tif context is not None:\n\t\tprompt = f\" [INST] Please continue the following conversation by providing the Assistant's response, in order to maximize the score that a reasonable human evaluator would assign to the Assistant's response given the following criteria or context: {{context}}\\n\\n [[Conversation]] \\n\\n{{prompt}} [/INST]\" + f' {{completion}}</s>'\n\telse:\n\t\tprompt = f\" [INST] Please continue the following conversation by providing the Assistant's response: \\n\\n [[Conversation]] \\n\\n{{prompt}} [/INST]\" + f' {{completion}}</s>'\n\treturn prompt\nThe prompt templates for Ultra RM and Prometheus follow the format set out by the authors of those models. Since no reference answer was available for Prometheus, we use the following slight modification which omits the reference answer:\nPROMETHEUS_TEMPLATE = \"\"\"###Task Description:\nAn instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.\n1. Write a detailed feedback that assess the quality of two responses strictly based on the given score rubric, not evaluating in general.\n2. After writing a feedback, choose a better response between Response A and Response B. You should refer to the score rubric."}, {"title": "B Constructing the RPR Datasets", "content": "We construct the datasets using the following steps:\n1. Collect a set of diverse prompts.\n2. Synthesize the initial RPR Criteria dataset.\n3. Self critique and filter the RPR Criteria dataset.\n4. Synthesize the RPR Scenarios dataset, using the RPR Criteria dataset as a base.\n5. Self critique and filter the RPR Scenarios dataset.\n6. Filter RPR Criteria down to what is left after the RPR Scenarios synthesis, so that each prompt has 2 criteria, 2 corresponding scenarios, and 2 corresponding completions.\nIn Step 1, we used all prompts from the Ultrafeedback dataset [17], which have already been selected for diversity, and filtered it down to prompts where there was no explicitly correct answer (according to the Ultrafeedback annotations). This left us with 42,000 initial prompts.\nWe passed these prompts through step 2 of our synthesis procedure:"}, {"title": "B.1 RPR Profiles", "content": "To make the RPR profiles extension of RPR, we sample 20 sets of 20 random samples from the training set (assigning the preference for each sample at random). We use these samples to seed an initial sample of 20 profiles, each of which we infer from 20 samples using GPT-4 Turbo and the following profile inference prompt:\n\"\"\"\nI would like help generating a profile of a user who is conversing with an AI assistant. This profile will be used to determine how the user balances trade-offs between different criteria when evaluating the AI assistant's responses. An an example from prior users, a hypothetical profile might include highly detail-oriented, but not very creative or original\". The user profile should capture as much relevant information from the responses as possible, and provide broad coverage of the User's potential preferences going forward. Maintain a simple and easy to understand style for the profile's language: use full paragraphs (not bullets), but make sure to avoid uncommon or exaggerated language such as \"prowess\", \"symbiotic\", etc. and avoid extraneous adjectives/adverbs. Do not fabricate information or make assumptions about the user's preferences that are not supported by the provided data; this is not necessarily an average user who has typical preferences.\nTo generate the profile, you are given a set of (prompt, preferred response, rejected response) tuples of expressed user preferences below. You will infer the user's preferences from these tuples. You will do this step-by-step:\n1. If there is sufficient data, cluster the expressed preferences into groups that represent similar values of the user.\n2. Based on step 1, draft an initial profile. Make it as long as necessary to properly capture the User 's nuanced preferences.\n3. Critique the initial profile by identifying any missing or incorrect inferences with respect to the expressed user preferences. Is the profile internally consistent? Is it consistent with all of the expressed user preferences?\n4. Expand/revise the profile to address your reasoning in steps 3 and add in anything you missed. Remove extraneous adjectives and any uncommon or exaggerated language."}, {"title": "C Finetuning Details", "content": "We finetune our model using the Mistral RM prompt in Appendix A. We use 10,167 samples from RPR criteria (training set, 1 sample at random from each paired sample), 10,167 random samples from RPR scenarios (training set, 1 sample at random from each paired sample), and 13,333 random samples from Ultrafeedback. To set the context in the latter, we sample a random \"general\" context from:"}, {"title": "C.1 Data Composition Ablation", "content": "For completion, we finetune the base Mistral reward model using two additional preference datasets for which criteria are available.\nFirst, the Preference Collection (hf:prometheus-eval/Preference-Collection), which serves as a training set for Preference Bench (introduced and used in Section 5), can be used for finetuning a criteria aware reward model.\nSecond, in a concurrent work, Lee et al. [34] have synthesized a dataset of diverse sys-tem prompts for finetuning generative models, which they call the Multifaceted Collection (hf:kaist-ai/Multifaceted-Collection). As the dataset includes multiple system prompts for the same user instruction, its structure allows it to be used in a similar fashion to our RPR datasets, in order to finetune a reward model.\nUsing the hyperparameters described above, we finetuned the base model using the following data mixtures:\n\u2022 PC: 40,000 random samples from the Preference Collection.\n\u2022 PC+MF+UF: 13,333 random samples from the Preference Collection, 13,333 random samples from the Multifaceted Collection, and 13,333 random samples from Ultrafeedback.\n\u2022 RPR+UF: 10,167 random samples from RPR criteria, 10,167 random samples from RPR scenarios, and 13,333 random samples from Ultrafeedback. This model,"}]}]}