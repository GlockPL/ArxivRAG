{"title": "Improving Context-Aware Preference Modeling for Language Models", "authors": ["Silviu Pitis", "Ziang Xiao", "Nicolas Le Roux", "Alessandro Sordoni"], "abstract": "While finetuning language models (LMs) from pairwise preferences has proven\nremarkably effective, the underspecified nature of natural language presents critical\nchallenges. Direct preference feedback is uninterpretable, difficult to provide where\nmultidimensional criteria may apply, and often inconsistent, either because it is\nbased on incomplete instructions or provided by diverse principals. To address\nthese challenges, we consider the two-step preference modeling procedure that\nfirst resolves the under-specification by selecting a context, and then evaluates\npreference with respect to the chosen context. We decompose reward modeling\nerror according to these two steps, which suggests that supervising context in\naddition to context-specific preference may be a viable approach to aligning mod-\nels with diverse human preferences. For this to work, the ability of models to\nevaluate context-specific preference is critical. To this end, we contribute context-\nconditioned preference datasets and accompanying experiments that investigate\nthe ability of language models to evaluate context-specific preference. We use\nour datasets to (1) show that existing preference models benefit from, but fail to\nfully consider, added context, (2) finetune a context-aware reward model with\ncontext-specific performance exceeding that of GPT-4 and Llama 3 70B on tested\ndatasets, and (3) investigate the value of context-aware preference modeling.", "sections": [{"title": "1 Introduction", "content": "As the general purpose capabilities of Language Models (LMs) [11, 42] and other foundation\nmodels [9] progress toward handling diverse instructions and executing long-range trajectories in\nreal-world applications [44, 47, 41], it becomes increasingly important to have a principled system\nfor ensuring that LM agents behave as expected. The prevailing approach for aligning an LM to\nhuman preferences uses pairwise preferences between different outputs to finetune the LM [52, 6],\nwhich falls short of addressing the critical challenges presented by the reality of diverse user intents\nand contexts [51, 50, 13]. In the presence of unspecified contexts, such as the user's identity or\ngoals, preference queries are notoriously ambiguous [61] and one typically observes poor agreement\n(~65%) between human annotators on binary preference queries [35, 58].\nIn this paper, we consider modeling preferences using a two-step, context-aware approach (Figure 1).\nThis approach first resolves the underspecification by selecting a context [38, 22], and then evaluates\npreference with respect to the chosen context [60, 14, 27, 30, 57]. Decomposing general preference\ninto context and context-specific preference has several potential advantages. First and foremost, this\napproach explicitly identifies contextual assumptions that underlie preference, and shifts the alignment\nburden from preference modeling to a hybrid of preference modeling and context supervision. Second,\nthis approach is naturally pluralistic [51], allowing the model to adapt to diverse users and use cases.\nFinally, it offers more flexibility for principled aggregation: whereas the Bradley-Terry approach\ncorresponds to aggregating contexts using the Borda rule [50], which may under-serve certain\nPreprint. Under review."}, {"title": "2 Context-Aware Preference Modeling", "content": "Current practice finetunes language models to make them consistent with a dataset of human prefer-\nences [44, 42]. As noted by earlier works on learning from human feedback, however, ambiguous\npreference judgments present a major challenge:\nEvaluation of a [preference query] is both subjective and multidimensional ... this\nmakes consistent labeling difficult for honest labelers (including the authors!) [61]\nThis ambiguity manifests itself with agreement rates as low as ~65% between human annotators\non binary preference queries [35, 58]. One way to understand this difficulty appeals to a distinction\ndrawn by Amartya Sen:\nA value judgment can be called 'basic' if the judgment is supposed to apply under\nall conceivable circumstances, and it is \u2018non-basic' otherwise. [49]\nMany, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in\nthat certain contextual information might effectively reverse the judgment. For instance, the preferred\nresponse to a technical question may depend on the user's education level, the preferred response\nto a medical question may depend on whether the user is a doctor, and the preferred response to a\nquestion about etiquette may depend on the user's geographical location. If we train models using\nnon-basic preference annotations, the contextual biases and assumptions underlying those judgments\nmay be implicitly embedded into the model [36, 48].\nRather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely\non the training process to consistently aggregate any disagreements in a singularly aligned model, one\nmight instead consider an explicit context-aware approach to preference modeling (Figure 1). This\napproach first (partly) resolves ambiguity by specifying a context, and then models context-specific\npreference. This is not a new idea; Ziegler et al. [61], quoted above, continue to remark that, \u201cit\nseems better to design less ambiguous labeling tasks that get at the same information ... such as a\nverbal description [of the most important contextual information]\u201d, and many have advocated for fine-\ngrained, context-conditioned, or otherwise more \u201cpluralistic\u201d approaches to alignment [57, 29, 51].\nHowever, while production systems recognize the importance of incorporating context\u2014the most\nnotable being the \"system prompt\u201d introduced by OpenAI in 2023 and adopted by others [54, 3]\u2014\nthere has been little published research on the context-aware preference modeling capabilities of\nlanguage models. This paper works toward filling this gap by introducing the reasonable preference\nreversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the\ncontext-specific preference modeling capabilities of existing models.\nThis discussion is summarized in Table 1 alongside other key benefits of context-aware preference\nmodeling. This paper focuses on evaluating and improving context-aware preference modeling\ncapabilities, leaving an in-depth exploration of the benefits to future work."}, {"title": "2.1 Resolving ambiguity by making implicit assumptions explicit", "content": "Current practice finetunes language models to make them consistent with a dataset of human prefer-\nences [44, 42]. As noted by earlier works on learning from human feedback, however, ambiguous\npreference judgments present a major challenge:\nEvaluation of a [preference query] is both subjective and multidimensional ... this\nmakes consistent labeling difficult for honest labelers (including the authors!) [61]\nThis ambiguity manifests itself with agreement rates as low as ~65% between human annotators\non binary preference queries [35, 58]. One way to understand this difficulty appeals to a distinction\ndrawn by Amartya Sen:\nA value judgment can be called 'basic' if the judgment is supposed to apply under\nall conceivable circumstances, and it is \u2018non-basic' otherwise. [49]\nMany, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in\nthat certain contextual information might effectively reverse the judgment. For instance, the preferred\nresponse to a technical question may depend on the user's education level, the preferred response\nto a medical question may depend on whether the user is a doctor, and the preferred response to a\nquestion about etiquette may depend on the user's geographical location. If we train models using\nnon-basic preference annotations, the contextual biases and assumptions underlying those judgments\nmay be implicitly embedded into the model [36, 48].\nRather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely\non the training process to consistently aggregate any disagreements in a singularly aligned model, one\nmight instead consider an explicit context-aware approach to preference modeling (Figure 1). This\napproach first (partly) resolves ambiguity by specifying a context, and then models context-specific\npreference. This is not a new idea; Ziegler et al. [61], quoted above, continue to remark that, \u201cit\nseems better to design less ambiguous labeling tasks that get at the same information ... such as a\nverbal description [of the most important contextual information]\u201d, and many have advocated for fine-\ngrained, context-conditioned, or otherwise more \u201cpluralistic\u201d approaches to alignment [57, 29, 51].\nHowever, while production systems recognize the importance of incorporating context\u2014the most\nnotable being the \"system prompt\u201d introduced by OpenAI in 2023 and adopted by others [54, 3]\u2014\nthere has been little published research on the context-aware preference modeling capabilities of\nlanguage models. This paper works toward filling this gap by introducing the reasonable preference\nreversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the\ncontext-specific preference modeling capabilities of existing models."}, {"title": "2.2 Related Work", "content": "Modeling human preferences traces back to Luce [39] and Bradley-Terry [10]. It made its way into\nfinetuning language models via a line of work originating in reinforcement learning [2, 15, 61, 52],\nand has now become the dominant approach for finetuning language models to follow human\ninstructions [44, 6, 4, 46]. While this approach has achieved remarkable results and enabled the\nperformance of state-of-the-art models, several authors have pointed to its limitations [12, 45, 32, 51],"}, {"title": "3 Context Decomposition Upper Bound", "content": "We model the user-LM interaction using an intent-utility formalism (I, X, Y, u), where I is the\nspace of intents, X is the space of prompts, Y is the space of completions, and $u : I \\times Y \\rightarrow R$ is a\nscalar utility function. We follow the standard assumption and assume that preference in this model is\nmade according to the Bradley-Terry model [10, 15]. Letting $\\sigma$ be the logistic function, this defines\nthe probability of preferring completion $y_1$ to $y_2$ given a specific intent i as:\n$P(y_1 \\succ y_2 | i) = \\sigma (u(i, y_1) - u(i, y_2)).$    (1)\nIn our model the primitive definitions of preference and utility are conditioned on the intent rather\nthan the prompt. To prompt the model, a user with intent i selects a prompt $x \\in X$. To annotate a\npreference query (x, y1, y2), an annotator implicitly infers intent i from x and samples a preference\nfrom the Bernoulli distribution $B[p(y_1 > y_2 | i)]$.\nBoth users and annotators may possess or infer a distribution of intents. Indeed, we would argue that\nannotation for most preference queries involves a distribution of intents rather than a specific intent.\nWe use \"intent\" to refer to both specific and distributional intents. We assume there exists a base\ndistribution over possible intents p(i), as well as a conditional distribution over prompts given intents\np(x | i), so that any prompt x has a natural inference distribution p(i | x). In this model, the prompt x\nis a partial specification of intent i. While a prompt may never be able to fully specify the intent, we\nmay add some additional information or context z to obtain an extended prompt (x, z) \u2208 X. Let us\nsuppose that z \u2208 Z, where Z corresponds to a discrete partition of I.\nOne way to measure utility and preference with respect to a distribution p of intents is with an\nexpected utility model, which computes utility as $u(p, y) = E_{i\\sim p} [u(i, y)]$. While it has been shown\nthat standard RLHF does not align with the expected utility model [50], this model satisfies certain\ndesirable axioms, which one can argue would apply to \"ideal\" preference annotation. We use the\nexpected utility model to define $u(x, y) := u(p(\\cdot|x), y)$, and note that for any context partition"}, {"title": "3.1 Intent-Utility Formalism", "content": "We model the user-LM interaction using an intent-utility formalism (I, X, Y, u), where I is the\nspace of intents, X is the space of prompts, Y is the space of completions, and $u : I \\times Y \\rightarrow R$ is a\nscalar utility function. We follow the standard assumption and assume that preference in this model is\nmade according to the Bradley-Terry model [10, 15]. Letting $\\sigma$ be the logistic function, this defines\nthe probability of preferring completion $y_1$ to $y_2$ given a specific intent i as:\n$P(y_1 \\succ y_2 | i) = \\sigma (u(i, y_1) - u(i, y_2)).$    (1)\nIn our model the primitive definitions of preference and utility are conditioned on the intent rather\nthan the prompt. To prompt the model, a user with intent i selects a prompt $x \\in X$. To annotate a\npreference query (x, y1, y2), an annotator implicitly infers intent i from x and samples a preference\nfrom the Bernoulli distribution $B[p(y_1 > y_2 | i)]$.\nBoth users and annotators may possess or infer a distribution of intents. Indeed, we would argue that\nannotation for most preference queries involves a distribution of intents rather than a specific intent.\nWe use \"intent\" to refer to both specific and distributional intents. We assume there exists a base\ndistribution over possible intents p(i), as well as a conditional distribution over prompts given intents\np(x | i), so that any prompt x has a natural inference distribution p(i | x). In this model, the prompt x\nis a partial specification of intent i. While a prompt may never be able to fully specify the intent, we\nmay add some additional information or context z to obtain an extended prompt (x, z) \u2208 X. Let us\nsuppose that z \u2208 Z, where Z corresponds to a discrete partition of I.\nOne way to measure utility and preference with respect to a distribution p of intents is with an\nexpected utility model, which computes utility as $u(p, y) = E_{i\\sim p} [u(i, y)]$. While it has been shown\nthat standard RLHF does not align with the expected utility model [50], this model satisfies certain\ndesirable axioms, which one can argue would apply to \"ideal\" preference annotation. We use the\nexpected utility model to define $u(x, y) := u(p(\\cdot|x), y)$, and note that for any context partition"}, {"title": "3.2 Context Decomposition Upper Bound", "content": "During RLHF, we are presented with a dataset of prompt-preference tuples, $D = \\{(x, y_1 \\succ y_2)\\}$,\nwhich we use to learn utility estimator $\\hat{u} : X \\rightarrow Y$ (conventionally known as a \u201creward model\u201d). As\nwas assumed for u and p(z | x), we would like there to be a model $\\hat{p}(z | x)$ that satisfies the relation:\n$\\hat{u}(x,y) = \\sum_z \\hat{p}(z|x)\\hat{u}((x, z), y).$    (2)\nIn standard RLHF, we never learn such $\\hat{p}$. However, for purposes of this analysis, we will assume\nthis $\\hat{p}$ exists, either implicitly given $\\hat{u}$, or explicitly, such that given some Z, we compute $\\hat{u}(x, y)$ via\nEquation (2) rather than via direct evaluation. Below, we will favor the latter interpretation.\nFor a given preference estimator $(y_1 \\succ y_2)$, $\\hat{u}$ is only unique up to constant shifts, so to measure the\naccuracy of $\\hat{u}$, we will instead compare $\\Delta$ and estimator $\\Delta(x, y_1, y_2) := \\hat{u}(x, y_1) - \\hat{u}(x, y_2)$.\nConsider now the absolute error $|\\Delta(x, y_1, y_2) - \\Delta(x, y_1, y_2)|$ for a particular preference query, and\nuse $\\Delta_z$ as shorthand for $\\Delta((x, z), y_1, y_2)$. For any Z we have the following bound:\n$\\begin{aligned}\n|\\Delta(x, y_1, y_2) - \\Delta(x, y_1, y_2)| &= |\\sum_z p(z|x)\\Delta_z - \\sum_z \\hat{p}(z|x)\\hat{\\Delta}_z|\\\\\n&= |\\sum_z \\hat{p}(z|x) [\\Delta_z - \\hat{\\Delta}_z] + \\sum_z \\Delta_z [p(z|x) - \\hat{p}(z | x)]|    (3)\\\\\n&< \\sum_z \\hat{p}(z|x)|\\Delta_z - \\hat{\\Delta}_z| + \\sum_z |\\Delta_z||p(z|x) - \\hat{p}(z|x)|\\\\\n\\text{Context-weighted prediction error} & \\text{Preference-weighted inference error}\n\\end{aligned}$\nwhere the second equality adds and subtracts $\\sum_z \\hat{p}(z|x)\\Delta_z$ and rearranges, and the final line uses\nthe triangle inequality (multiple times).\nEquation (3) applies given a distribution of contexts, but in many cases, we might assume there is a\nspecific context c (i.e., p(z = c) := 1) and make a single context prediction $\\hat{c}$ (i.e., $\\hat{p}(\\hat{c}) = 1)$. This\nsimplifies Equation (3) and gives us the context decomposition upper bound for a specific context:\n$\\begin{aligned}\n|\\Delta(x, y_1, y_2) - \\Delta(x,y_1, y_2) | &\\leq |\\Delta_c - \\hat{\\Delta}_\\hat{c}| + |\\hat{\\Delta}_\\hat{c}| |p(z|x) - \\hat{p}(z|x)|\\\\\n&\\text{True context} &\\text{Subjective difference:}\\\\\n&\\text{prediction error} & \\text{true vs predicted context}\n\\end{aligned}$   (4)\nBoth the general bound (Equation (3)) and specific bound (Equation (4)) formalize the following\nintuitive claim: if we can make accurate predictions given the true context (or context distribution),\nthen we can reduce the preference modeling problem to a context inference problem."}, {"title": "3.3 Discussion", "content": "The upper-bound in Equation (3) bounds the total error in terms of a context-weighted prediction error\nand a preference-weighted inference error. On one extreme, we have Z = {} (standard preference\nmodeling), so that the context inference error is zero and the prediction error exclusively depends\non the preference prediction problem given the prompt. On the other hand, we have Z = Y (e.g., z\nmight be \"The preferred response is [y].\") and our preference prediction error is zero, but the context\ninference problem becomes equivalent to generation. In between, we conjecture that there is a smooth\ntrade-off between prediction error and inference error. This might be the case, for example, if a\nsingle context could apply to and disambiguate a range of different preference queries. We consider\nthis in our experiments and find some support for the conjecture, in that conditioning on specific\ncriteria outperforms conditioning on more abstract, yet still rather specific scenarios (Table 2) which\noutperforms conditioning on a user profile that applies to all preference queries at once (Table 5).\nIf our model $\\hat{u}$ is very good at predicting preference given some additional context Z, the preference\nmodeling problem can be largely reduced to a context inference (or specification) problem. In this\ncase, rather than have annotators rank completions for ambiguous prompts, it may make sense to\nspend the annotation resources to specify additional context. Such annotations could then be used"}, {"title": "4 Reasonable Preference Reversal (RPR) Datasets", "content": "We contribute a set of carefully designed, synthetic preference datasets to measure whether preference\nprediction is sensitive to context. Our datasets, inspired by the notion of non-basic judgments and\npreference reversal described in Section 2, include over 20,000 paired tuples of prompt, context, and\npreference judgments, i.e. (x, z, y1 \u27a4 y2). The samples are paired so that preference between two\ncompletions for the same prompt is entirely ambiguous without context: for every context, there is an\nalternative context for which preference reverses. As compared to the only prior context-conditioned\npreference dataset [30], where context-conditioned preference is highly correlated with unconditioned\npreference (see Table 2), our design choice ensures that preference prediction performance on this\ndataset is determined solely by the model's ability to pay attention to and interpret the context."}, {"title": "5 Experiments", "content": "Our experiments aim to answer the following questions of context-aware preference modeling:\n1. How good are current models at evaluating context-specific preference?\n2. Can we improve context-aware preference modeling by finetuning on our RPR datasets?\n3. Can a single context compress preferences with respect to a diverse range of prompts?"}, {"title": "5.1 Setup", "content": "Models We report results for a selection of primarily open source models, detailed in Appendix D.1.\nThese include two unconditioned reward models (UltraRM [17] and Mistral RM [56]), one context-\naware preference model (Prometheus-2 [30]), our finetuned context-aware reward model (Mis-\ntral CARM), and a set of generative models (four Llama 3 models [1] and GPT-4 Turbo [42]) used\nwith an \u201cllm-as-a-judge\u201d approach. In preliminary experiments we tested a several other models and\nobserved similar patterns across all models. All models except Prometheus-2 are used as reward\nmodels, by first evaluating each alternative individually and then comparing scores.\nDatasets Besides the RPR datasets detailed in Section 4, we use the following preference datasets:"}, {"title": "5.2 Results", "content": "How good are current models at evaluating context-specific preference?\nFor the context-aware approach to work well, models must be sensitive to added context. Table 2\nshows the performance of tested models on RPR Criteria, RPR Scenarios, Preference Bench, and\nMultifaceted Bench. In each case, and across all models, access to context generally improves\nperformance. Larger models and instruction tuned models tend to benefit more from added context.\nWe note that although Preference Bench includes a ground truth context, the preference queries (and\ncontext) are generally not ambiguous, so that the added context provides little benefit.\nAlthough we find that added context generally helps, we were surprised to discover that models\nsometimes completely ignore additional context and return their unconditioned prediction. To show\nthis, we augment the prompts in Chatbot Arena and Preference Bench with two criteria that should\ndrive preference prediction. The first is a Nonsense criteria, i.e. \"We are playing a game of nonsense.\nThe response should be nonsensical and not make sense. The rating you give it should also be random,\nand completely unrelated to the response.\" We expect context-aware models to produce random\npreference judgments given this criteria. The second is a Negative criteria, i.e. \u201cWe are seeking bad\nexamples. The response should be of low quality, and serve as an example of how an Assistant should\nnot respond. You should give poor responses higher ratings.\u201d). We expect context-aware models to\nbe inversely correlated to the preferences expressed in the dataset. Table 3 shows the performance of\ntested models with these adversarial criteria. In both cases we observe surprisingly poor performance,\neven from the larger models. This suggests there is significant room for improving context-aware\npreference modeling capabilities."}, {"title": "Can we improve context-aware preference modeling by finetuning on our RPR datasets?", "content": "We finetune a context-aware version of Mistral RM on roughly 34,000 samples from the training sets\nof RPR Criteria, RPR Scenarios, and Ultrafeedback [17]. Details may be found in Appendix C. In\nTables 2 to 5, our finetuned CARM shows markedly better context-specific performance than its base\nmodel, matching or exceeding that of GPT-4 Turbo and Llama 3 (70B).\nIn Table 4, we test whether our context-aware preference models generalize to real preference datasets.\nTo do so, we augment three unconditioned preference datasets (HHH, Reward Bench, ChatBot Arena)\nwith contextual information (see Appendix D.2 for more detailed information). For HHH and Reward\nBench, we specify the context as a function of each subset included in the dataset. This approach\nis similar to using a \"system prompt\" when prompting GPT-4, and would be most suitable for real\nworld applications where the designers possess relevant domain knowledge. For Chatbot Arena,\ngiven the lack of ground-truth contexts, we use GPT-4 to generate a possible context given the prompt\nand alternatives (CTX). Additionally, we infer the context by looking at the prompt together with\nthe expressed preferences (CTX*), which endows the context with privileged information about\npreference data. This may be useful for inferring a useful persistent context such as a user profile,\nwhich we explore further below."}, {"title": "Can a single context compress preferences with respect to a diverse range of prompts?", "content": "In our experiments so far, we operated in a setting where each prompt has an associated context.\nHowever, when eliciting preferences from users, the hidden context of a specific user will impact their"}, {"title": "6 Conclusion", "content": "This paper began with a case for a two-step context-aware preference modeling framework (Figure 1),\nwhich was motivated by the ambiguity problem commonly experienced during preference annotation\n(Section 2). We further motivated the framework via a context decomposition upper bound (Section 3)\nand noted that for context-aware preference modeling to be viable, we require strong context-specific\npreference modeling. However, despite the prevalence of context conditioning in deployed systems\n(e.g., system prompts and \"My GPTs\" [43]), when we began our work, there were no open source\npreference datasets where preference is strongly determined by context, and limited studies of the\ncontext-specific preference modeling capabilities of current models [30]. To this end, we introduced\nthe RPR datasets (Table 2) and investigated a series of interesting empirical questions in Section 5).\nWe found that (1) while current preference models generally benefit from context, they sometimes\nignore added context, (2) finetuning on our preference reversal datasets greatly improves context-\nspecific performance, and (3) a single persistent context, which might be inferred from data, may be\nsufficient to significantly improve preference prediction performance.\nOur work leaves many open threads for future research. Our datasets might be used to explore\ndifferent directions in pluralistic alignment, as described by Sorensen et al. [51]. In particular, we\nbelieve that better context specification and context inference are important directions for research,\nwhich our experiments only briefly touch on. Further, while we have open sourced a set of context-\nconditioned preference datasets, these datasets have limited diversity (for example, they are limited\nto single turn conversations), and were synthesized rather than collected from real human feedback,\nwhich limits their practical relevance. Additional open, preferably human sourced datasets with\ncontext annotations are need to more fruitfully pursue this research direction."}, {"title": "A Prompts for context-conditioned preference prediction", "content": "The following template is used for \u201cllm-as-a-judge\" models (Llama 3 and GPT-4 Turbo). Llama-3\nuses the logit_template (see Appendix D.1 for how the score is computed). GPT-4 Turbo uses\nthe argmax_score_template_no_cot and runs inference with temperature = 0.\nMistral RM and our finetuned version use the following template:\nThe prompt templates for Ultra RM and Prometheus follow the format set out by the authors of\nthose models. Since no reference answer was available for Prometheus, we use the following slight\nmodification which omits the reference answer:"}, {"title": "B Constructing the RPR Datasets", "content": "We construct the datasets using the following steps:\n1. Collect a set of diverse prompts.\n2. Synthesize the initial RPR Criteria dataset.\n3. Self critique and filter the RPR Criteria dataset.\n4. Synthesize the RPR Scenarios dataset, using the RPR Criteria dataset as a base.\n5. Self critique and filter the RPR Scenarios dataset.\n6. Filter RPR Criteria down to what is left after the RPR Scenarios synthesis, so that each\nprompt has 2 criteria, 2 corresponding scenarios, and 2 corresponding completions.\nIn Step 1, we used all prompts from the Ultrafeedback dataset [17], which have already been selected\nfor diversity, and filtered it down to prompts where there was no explicitly correct answer (according\nto the Ultrafeedback annotations). This left us with 42,000 initial prompts.\nWe passed these prompts through step 2 of our synthesis procedure:"}, {"title": "B.1 RPR Profiles", "content": "To make the RPR profiles extension of RPR, we sample 20 sets of 20 random samples from the\ntraining set (assigning the preference for each sample at random). We use these samples to seed an\ninitial sample of 20 profiles, each of which we infer from 20 samples using GPT-4 Turbo and the\nfollowing profile inference prompt:\nHaving generated 20 profiles, we use them to evaluate preference on 40 samples, using the inference\nprompt below. We run each sample twice with the completions in alternating order as we noticed\nsome order bias:"}, {"title": "C Finetuning Details", "content": "We finetune our model using the Mistral RM prompt in Appendix A. We use 10,167 samples from\nRPR criteria (training set, 1 sample at random from each paired sample), 10,167 random samples\nfrom RPR scenarios (training set, 1 sample at random from each paired sample), and 13,333 random\nsamples from Ultrafeedback. To set the context in the latter, we sample a random \"general\" context\nfrom:"}, {"title": "C.1 Data Composition Ablation", "content": "For completion, we finetune the base Mistral reward model using two additional preference datasets\nfor which criteria are available.\nFirst, the Preference Collection (hf:prometheus-eval/Preference-Collection), which serves\nas a training set for Preference Bench (introduced and used in Section 5), can be used for finetuning a\ncriteria aware reward model.\nSecond, in a concurrent work, Lee et al. [34] have synthesized a dataset of diverse sys-\ntem prompts for finetuning generative models, which they call the Multifaceted Collection\n(hf:kaist-ai/Multifaceted-Collection). As the dataset includes multiple system prompts for\nthe same user instruction, its structure allows it to be used in a similar fashion to our RPR datasets, in\norder to finetune a reward model.\nUsing the hyperparameters described above, we finetuned the base model using the following data\nmixtures:"}, {"title": "D Additional Experiment Details", "content": "UltraRM [17] (hf:openbmb/UltraRM-13b) is a 13B parameter reward model initialized from\nLlama-2 [54] and finetuned on UltraFeedback and 3 other open-source preference datasets, and\nwas chosen for its strong performance on open source preference benchmarks.\nPrometheus-2 [30] (hf: prometheus-eval/prometheus-7b-v2.0) is a 7B parameter model\nfinetuned to perform fine-grained, context-conditioned evaluation, either as a preference or reward\nmodel. It is finetuned on 300K criteria-conditioned samples synthesized with the help of GPT-\n4. We run Prometheus-2 with temperature=0, which may explain why our reported figures on\nPreference Bench are higher than the figures reported by the authors [30].\nMistral RM [56] (hf:weqweasdas/RM-Mistral-7B) is a 7B parameter reward model initialized\nfrom Mistral-7B-Instruct-v0.2 [28] and finetuned on a variety of open source preference datasets,\nand was chosen for its smaller parameter count and strong performance on Reward Bench [33].\nMistral CARM (ours) is our finetuned context-aware version of Mistral RM, finetuned using the\nRPR datasets. Finetuning details and data ablations may be found in Appendix C.\nLlama 3 (hf:meta-llama/Meta-Llama-3-[8B/70B] [-Instruct]) [1] are a set of strong\nopen source models available as both base models and instruction tuned models. We report results\nwith all four variants, using a modified \u201cllm-as-a-judge\u201d approach [58] that scores completions\nusing the weighted sum of its score logits: we ask the model to predict a score, and return the\nexpected value of the predicted score under the log probabilities of the score tokens. To enable\nfast evaluation, we skip the chain-of-thought and do a single forward pass to evaluate the logits.\nSpecifically, we ask the model to rate the completion with a score between 1 and 7.\nGPT-4 Turbo is used as the proprietary baseline in experiments. Note that much of the data (RPR\ndatasets and context augmentations for unconditioned preference datasets) was synthesized using\nGPT-4 Turbo, as described herein. As logits are not available, we use GPT-4 with a standard\n\"Ilm-as-a-judge\" approach that scores each alternative individually before comparing scores. This\nsuffers from inability to distinguish between ties (which we instead assign randomly)."}, {"title": "D.2 Additional Experiment Details", "content": "To augment the datasets, we use the following approach / prompts.\nFor HHH, we specify the following subset specific contexts:\nTo synthesize the \"CTX\" contexts for Chatbot Arena, we use the following prompt (note that only\nthe criteria was used as the context, not the teacher's preference):"}, {"title": "D.3 Five profiles used in experiments"}]}