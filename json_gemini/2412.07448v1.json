{"title": "Dynamic Ensemble Reasoning for LLM Experts", "authors": ["Jinwu Hu", "Yufeng Wang", "Shuhai Zhang", "Kai Zhou", "Guohao Chen", "Yu Hu", "Bin Xiao", "Mingkui Tan"], "abstract": "Ensemble reasoning for the strengths of different LLM experts is critical to achieving consistent and satisfactory performance on diverse inputs across a wide range of tasks. However, existing LLM ensemble methods are either computationally intensive or incapable of leveraging complementary knowledge among LLM experts for various inputs. In this paper, we propose a Dynamic Ensemble Reasoning paradigm, called DER to integrate the strengths of multiple LLM experts conditioned on dynamic inputs. Specifically, we model the LLM ensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent sequentially takes inputs to request knowledge from an LLM candidate and passes the output to a subsequent LLM candidate. Moreover, we devise a reward function to train a DER-Agent to dynamically select an optimal answering route given the input questions, aiming to achieve the highest performance with as few computational resources as possible. Last, to fully transfer the expert knowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP) that enables the subsequent LLM candidates to transfer complementary knowledge effectively. Experiments demonstrate that our method uses fewer computational resources to achieve better performance compared to state-of-the-art baselines.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as LLAMA (Touvron et al. 2023a) and GPT-3.5 (Achiam et al. 2023) have exhibited remarkable performance across diverse tasks, including multi-round dialogue (Stark et al. 2023) and embodied intelligence (Mu et al. 2023). However, their variations in architectures, model sizes, and training data result in distinct strengths and weaknesses in different tasks (Jiang, Ren, and Lin 2023; Lu et al. 2023). Although training a larger LLM with more comprehensive data is possible to maintain excellent performance in all tasks, the cost is significant and often impractical. Consequently, it is crucial to assemble LLMs to enhance their generalization while minimizing the consumption of computational resources in practical applications.\nUnfortunately, assembling knowledge of LLMs with limited computing cost is difficult partly for the following reasons. 1) Complex knowledge integration: Each LLM is typically trained on different datasets, which may include varying levels of quality, diversity, and bias. Harmonizing these LLMs requires aligning their understanding with knowledge bases, posing challenges to integration (Jiang, Ren, and Lin 2023; Lu et al. 2023). 2) High computational complexity: LLMs are computationally intensive, requiring significant resources for inference. Combining LLMs increases this complexity, potentially necessitating more efficient algorithms to manage (Sheng et al. 2023; Wan et al. 2024).\nRecently, LLM ensemble has emerged as a rapidly progressing research area to leverage the diverse strengths of multiple LLMs. Based on their strategies, most existing methods can be broadly divided into four types. The Mixture-of-Experts methods (Jiang et al. 2024; Tang et al. 2024) use a router network to select and activate a subset of experts to aggregate diverse expertise (see Figure 1 (a)), but they often require retraining, cannot integrate non-homologous LLM experts, and fail to leverage complementary knowledge among experts. The parameter merging methods (Yu et al. 2023; Matena and Raffel 2022) merge the parameters of homologous LLMs into a single unified model but cannot assemble non-homologous LLMs. The rule-based methods (Dong et al. 2023; Du et al. 2023) assemble the advantages of LLM by manually designing task-specific roles or fixed order. However, such a static setting makes it difficult for the integration to generalize in various domains. To avoid these issues, agent-based methods (Jiang, Ren, and Lin 2023; Lu et al. 2023) train an agent to dynamically integrate non-homologous LLMs with the various strengths, making them adaptable to various scenarios.\nDespite the recent success of agent-based LLM ensemble methods in outperforming the best one among the LLMs across a wide range of downstream tasks, these methods still face certain limitations. Firstly, a common drawback in most existing methods (Jiang, Ren, and Lin 2023; Liu et al. 2023) is that they integrate LLMs based on the final outputs of all LLMs. These methods require prohibitively high computational costs to run all the LLM candidates and result in inefficient utilization of inference resources. Therefore, some easy samples that could be simply addressed by a single LLM have to run all the LLMs at expensive costs. For example, PairRanker (Jiang, Ren, and Lin 2023) requires significantly more parameters (> 117B) than a single LLM in inference, potentially leading to unbearable resource wastage. Secondly, while some agent-based methods implement a classifier to accomplish the integration by selecting only one LLM candidate at each time with very little inference cost (see Figure 1(b), left), e.g., ZOOTER (Lu et al. 2023), their performance is limited by the fact that they do not leverage the complementary knowledge among LLMs.\nTo address the above limitations, we propose a novel Dynamic Ensemble Reasoning paradigm for integrating the strengths of multiple LLM experts, called DER. Given that LLMs are always trained on diverse datasets, we hypothesize that they possess complementary knowledge, which can be sequentially assembled. Nevertheless, the exponential growth in possible combinations of routes renders it impractical to address this challenge through classification tasks alone. To overcome this, we view knowledge transfer as a sequential decision process. Specifically, we model the LLM ensemble as a Markov Decision Process (MDP), where a DER-Agent dynamically requests contributions from LLMs and transfers this knowledge to subsequent LLM candidates (see Figure 1(b), right). Moreover, we develop a reward function to train the DER-Agent to select optimal answering routes based on input questions, aiming to maximize performance while minimizing computational resources. Additionally, we introduce a Knowledge Transfer Prompt (KTP) to facilitate effective knowledge transfer among LLMs. We summarize our main contributions as follows:\n\u2022 We propose Dynamic Ensemble Reasoning (DER) for the LLM ensemble, modeling it as a Markov Decision Process (MDP) for efficient sequential knowledge transfer. This approach dynamically selects optimal answering routes, integrating complementary knowledge from various LLMs to maximize performance with minimal computational resources. Experiments show DER integrates the strengths of different LLMs, achieving nearly a 7-fold parameter reduction compared to ensemble methods using the outputs of all LLMs.\n\u2022 We introduce a Knowledge Transfer Prompt (KTP) to facilitate efficient knowledge sharing among LLMs and develop a reward function to train the DER-Agent. This ensures the DER-Agent can leverage expert knowledge from previous LLMs, optimizing task performance while significantly reducing computational costs. Experiments show that more than 9% improvement is achieved on BARTScore using KTP and our reward function."}, {"title": "2 Related works", "content": "Mixture-of-Experts Methods. The Mixture-of-Experts methods integrate the knowledge of LLMs by selecting and activating a subset of experts through a router network. Jiang et al. (2024) propose a Sparse Mixture of Experts language model, which aggregates the knowledge of diverse experts by selecting two experts at each layer of the routing network to process the current state and combining their outputs. Tang et al. (2024) propose the Weight-Ensemble MoE method, which achieves assembling different expert knowledge by training a router to select and merge different LLM expert parameters. However, these methods often require refine-tuning of the MoE model and it is difficult for experts to utilize the complementary knowledge of other experts.\nParameter Merging Methods. These methods assemble the advantages of LLMs by merging the parameters of multiple homologous LLMs into a single model. Matena and Raffel (2022) propose the \"Fisher merging\", which merges the parameters of models with the same structure and initialization, achieving the ability to assemble different LLMs. Yu et al. (2023) introduce an operation called DARE to sparsify delta parameters of multiple homologous LLMs, and subsequently merge them into a single model by parameter averaging, realizing the ability to assemble LLMs. However, these methods cannot assemble non-homologous LLMs.\nRule-based Methods. These methods assemble LLMs by manually setting roles or fixed orders for specific tasks. Du et al. (2023) propose LLM-debate, in which multiple LLMs get a consensus in multiple rounds of common debate. Dong et al. (2023) use three LLMs, set up as analysts, coders, and testers, to collaboratively develop software in sequential execution. Despite the advantages of these to quickly assemble LLMs, this static setup is hard to generalize.\nAgent-based Methods. These methods dynamically integrate the advantages of different non-homologous LLMs by training an agent and can be used in various scenarios. Jiang, Ren, and Lin (2023) propose the LLMs ensemble framework LLM-BLENDER in expectation of consistently superior performance, which selects the TOP-K responses by PairRanker and mixes them to generate final outputs using"}, {"title": "3 Proposed Methods", "content": "3.1 Problem definition and motivations\nProblem Definition. Given an input question set {x1,...,xK} with K questions, the LLM ensemble aims to aggregate the strengths of different LLMs (experts) to consistently achieve superior performance. Specifically, given an input question x and a model pool {M1,...,MN} with N LLMs, we aim to select k models to answer the question together to improve the quality of the answer.\nMotivations. Existing methods (Lu et al. 2023; Yao, Zhou, and Wang 2023) train an agent to select the most suitable LLM to answer each question individually (see Figure 1 (b), left). Although these methods can improve the answering performance, their performance is limited due to the underutilization of different knowledge contained in LLMs. To overcome this issue, we aim to develop an effective knowledge aggregation strategy to achieve superior performance. Intuitively, leveraging knowledge transfer from one LLM to another can compensate for individual LLM shortcomings, fostering a collaborative integration of diverse knowledge. Moreover, there exists an optimal knowledge transfer route for each question. Unfortunately, the number of possible route combinations is as large as _1 Nk, where N is the number of LLMs, k is the route length, and m is the maximum route length. In this sense, finding an optimal route for each sample poses a severe challenge.\nTo address the above issue, we naturally model the knowledge transfer as a sequential decision process, given its sequential nature. To this end, we require selecting certain models consecutively from the pool to answer the question, with each model able to refer to the responses provided by its predecessors (see Figure 1 (b), right). In this way, we derive an ultimate response from the terminal model. Our goal lies in identifying an optimal pathway of model execution Route*=[M\u00bf\u21d2Mj\u21d2\u2026\u21d2Mk] to enhance the quality of the final answer with modest computational cost. As shown in Figure 2, we formulate the LLM ensemble as a Markov Decision Process (MDP) and train a DER-Agent to select an optimal answering route. At each time step t, the DER-Agent determines the next LLM to generate a response based on the input question and the current answer (initially absent). This procedure utilizes the Knowledge Transfer Prompt (KTP), facilitating the LLM to construct an answer that progressively integrates insights from the previous LLM. The newly formed answer serves as the next state, enabling the ongoing knowledge transfer. This process continues looping until the answer is evaluated as satisfactory or the maximum trajectory length is reached."}, {"title": "3.2 Dynamic ensemble as a MDP", "content": "We seek a general DER-Agent to select an optimal sequential execution route of LLMs for the input question x, obtaining the highest performance with the least possible computational resources. To this end, we formulate the selection of the sequential execution route of LLMs as Markov Decision Process (MDP) (Van Otterlo and Wiering 2012): < S, A, T,R,\u03c0 >. The state space of the environment is S and the action space of the agent is A. At time step t, the agent takes the state st \u2208 S as input and performs an action at \u2208 A through the policy network \u03c0 : S \u00d7 A \u2192 [0,1]. The environment changes to the next state st+1 = T(st, at) according to the transition function T and a reward rt = R(st, at) is received with reward function R. The MDP is detailed as follows:\nStates S is a set of states which describe the environment. At time step t, the state consists of a question-answer pair: st = [Q: x, A : \u0177t\u22121], where x is the input question and \u0177t-1 is the answer from the selected LLM at time step t - 1 (the answer is None initially). Thus, the agent judges the quality of the current answer by the question-answer pair and predicts which next LLM to answer.\nActions A is a discrete set of actions the agent takes. The action space A = {1, 2, . . ., N } is the index of each LLM. At time step t, the agent gives the action at \u2208 A based on the state st to select a next LLM Mat from the model pool {M1,..., MN}.\nTransition T(S, A) is a function T: S \u00d7 A \u2192 S which maps a state st into a new state st+1. When the BERTScore P(\u0177T) for the answer \u0177r reaches a manually-set threshold Po or the maximum trajectory length Tmax is reached, this episode will be terminated and sT+1 is None. Otherwise, the selected LLM at time step t will answer the question x concerning the existing answer \u0177t\u22121:\n$st+1 = [Q : x, A : \u0177t]$, (1)\nwhere \u0177t = Mat(KTP(x, \u0177t\u22121)). (2)\nKTP(\u00b7) is the Knowledge Transfer Prompt (KTP) that we designed to stitch together the question and answer from the previous LLM Mat-1 and to promote the current LLM to answer the question x concerning the previous answer \u0177t-1. The KTP is detailed in subsection 3.4.\nRewards R(S, A) is the reward function. In the LLM ensemble task, the reward can be considered as an evaluation of the quality of the answer \u0177 for the selected LLM Mat. The details of the reward function are given in the subsection 3.3.\nPolicy \u03c0\u03c1(\u03b1 | s) : A \u00d7 S \u2192 [0, 1] describes the behaviors of the agent. During the training process, the agent takes the current state st as input and outputs a probability distribution for each possible action at \u2208 A = {1, 2, . . ., N }:\n$\u03c0 (at = i | st; \u03b8) = \\frac{exp \\{f_{\\theta}(s_t)_i\\}}{\\sum_{j=1}^{N} exp \\{f_{\\theta}(s_t)_j\\}}$, (3)\nwhere f\u03b8(st) is the output vector of the policy network with input st, and i denotes the index of the action. \u03b8 is the learnable parameters of the policy network.\nThe general diagram of the proposed DER is shown in Figure 2. Given an input question x, the DER is initialized with state so = [Q : x, A : None]. The agent takes so as input and gives an action ao so that an LLM Mao is selected to answer the question with yo. And then the reward ro is calculated for agent optimization based on the answer's quality and computational resources. The state is updated to $1 = [Q : x, A : yo] with the answer yo. The above process will continue until the BERTScore P(\u0177t) exceeds the threshold po or the maximum trajectory length is reached. Finally, the last answer \u0177r is obtained, which is high-quality thanks to the knowledge transfer among LLMs."}, {"title": "3.3 Reward function design", "content": "In our designed MDP, the reward function is defined to reflect three aspects: the quality of the answer provided by the selected LLM, the increment quality of the answer, and the computational resources:\n$R_t = \\begin{cases} P(\\hat{y}_t) - \\alpha C(M_{a_t}), & t=0 \\\\ P(\\hat{y}_t) + \\beta\\Delta P(\\hat{y}) - \\alpha C(M_{a_t}), & t > 0, \\end{cases}$ (4)\nwhere P() is the BERTScore, which is commonly used to evaluate the quality of generated text and its high correlation with human judgment (Zhang et al. 2019). The \u0177t is the output answer of the selected LLM Mat, C(\u00b7) is the computation cost of Mat, \u2206P(\u0177) = P(\u0177t) \u2013 P(\u0177t\u22121) is the increment of the BERTScore of the answer from t - 1 to t, and \u03b1, \u03b2 is the coefficient to determine the ratio of computation cost and the increment of the score, respectively. In addition, we add additional rewards or penalties to allow the agent to complete the generation of routes in limited steps. Thus, the complete reward follows:\n$R(s_t, a_t) = \\begin{cases} R_t + \\gamma, & t< t < T_{max} \\text{ and } P(\\hat{y}_t) \\geq p_0 \\\\ R_t - \\gamma, & t = T_{max} \\text{ and } P(\\hat{y}_t) < p_0, \\end{cases}$ (5)\nwhere po is the threshold of the BERTScore for which an environment gives an end, Tmax is the maximum step size, and \u03b3 is the bias for extra rewards or penalties. Note that in the testing phase P(\u00fbt) \u2265 po or P(\u00fbt) < po is judged by one of our trained Terminator, which is a binary classifier."}, {"title": "3.4 Knowledge Transfer Prompt", "content": "We develop the Knowledge Transfer Prompt (KTP) to facilitate effective knowledge transfer among LLMs. The proposed KTP expects that the current LLM effectively uses the answer (knowledge) of the previous LLM \u0177t-1 without being limited by it, to improve the performance of generating a better answer to the input x. To ensure that LLM experts follow the knowledge transfer settings, we introduce a role-playing mechanism (Kong et al. 2024) into the KTP as:\nKnowledge Transfer Prompt:\n[x] \\n There is an answer to the question from another student: \\n [\u0177t\u22121] \\n Using another student's answer as additional advice, you need to give a more satisfactory answer directly. DO NOT mention other students.\nFirst, we treat the previous LLM's answer \u0177t-1 as the \"student's answer\", thereby avoiding the overriding influence of the answer content of the predecessor. We then ask the current LLM to refer to the \u201cstudent's answer\u201d to give a more satisfactory answer \u0177t to question x via \"you need to give a more satisfactory answer\u201d. In addition, the proposed KTP avoids LLM outputting role-playing messages and irrelevant information by \u201cDO NOT mention other students\"."}, {"title": "3.5 Learning with proximal policy optimization", "content": "We use the Proximal Policy Optimization (PPO) (Schulman et al. 2017) to optimize the parameters \u03b8 of the DER-Agent (policy) due to the stability and sample-efficiency as:\nActor. The actor (DER-Agent) is trained for LLM selection according to the question-answer pair. To enhance the"}, {"title": "4 Experiment", "content": "Datasets and LLM experts. Following the settings of Pair-Ranker (Jiang, Ren, and Lin 2023), we use MixInstruct as"}, {"title": "4.1 Comparison experiments", "content": "We compare our proposed DER, eleven LLM experts, ChatGPT, and state-of-the-art (SOTA) LLM ensemble methods to demonstrate our method superior performance. We conduct experiments on a variety of downstream tasks, including QA task (see Table 1), mathematical reasoning task (see Table 2), and multi-domain QA task (see Appendix 3.2).\nDER is consistently better than single LLM. From Table 1, DER achieves better performance than a single LLM. Crucially, the DER on BARTScore is improved by 8.7% compared to Vicuna. In addition, DER reaches 98% of the ChatGPT performance on the BERTScore, while the inference parameters are only 10% of those of ChatGPT. We conclude that DER achieves better performance than the single LLM due to its ability to aggregate the complementary knowledge of diverse LLMs through knowledge transfer.\nTrade-offs between performance and computational resources. As shown in Table 1, the proposed DER achieves better performance than the SOTA methods with a little cost. Specifically, the proposed DER reduces the computational overhead of LLMs inference by about 85% (117B \u2192 17B) compared to PairRanker, while DER increases the BERTScore by about 2.7% (73.03 \u2192 75.00) compared to PairRanker. In addition, our method is also the best performer on the GPT-Rank metric. The main reason is that the reward function of our design requires DER-Agent to aggregate the strengths of diverse LLMs on as few resources as possible while setting the maximum route length."}, {"title": "4.2 Ablation studies", "content": "Effectiveness of Knowledge Transfer Prompt. We compare DER and DER (w/o KTP) to demonstrate the effectiveness of the Knowledge Transfer Prompt (KTP). As shown in Table 8, the proposed KTP effectively improves the performance of DER. Specifically, there is a 4.6% increase in BARTScore after using the KTP. This is strong support for the fact that the use of a role-playing prompt allows the LLM to leverage the knowledge of the previous LLM to produce a more satisfactory output (Kong et al. 2024).\nEffectiveness of LLM ensemble using MDP. As shown in Table 8, compared with the randomly generated route method, the DER outperforms the randomly generated route method by 2.31 in BERTScore. The primary reason is that by modeling the LLM ensemble as an MDP, the trained DER-Agent chooses an appropriate LLM based on the answer of the previous LLM to continue the answering."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel dynamic ensemble reasoning method, called DER, to integrate the strengths of LLM experts dynamically conditioned on dynamic inputs. Specifically, we model the LLM ensemble as an MDP, where a DER-Agent takes dynamic inputs, sequentially asks an LLM candidate to provide knowledge, and passes the knowledge to subsequent LLM candidates. We devise a reward function to train a DER-Agent to select an optimal answering route given the input questions, aiming to achieve the highest performance with as little computational cost as possible. We develop a KTP that enables the subsequent LLM to utilize the expert knowledge of the prior LLMs. Experiments"}, {"title": "A More related works", "content": "A.1 Large Language Models\nRecent advances in natural language processing (NLP) have led to the development of powerful large language models (LLMs). LLMs are composed of Transformer architecture (Vaswani et al. 2017) and pre-trained on large amounts of textual data. With the scaling of the model size and data size, they perform surprising abilities (Wei et al. 2022a) in solving a series of complex tasks, such as high-quality question-answering (Shao et al. 2023; Peng et al. 2023), coding (Chen et al. 2021), and intermediate reasoning(Wei et al. 2022b). They pose a significant impact on the AI community, leading to the rethinking of the possibilities of artificial general intelligence (AGI) (Zhao et al. 2023). Based on the transformer structure, the existing LLMs can be classified into three categories:\nEncoder-only models. The encoder-only models are also known as an auto-encoder. They consist solely of encoder components derived from the Transformer, which was originally designed for tasks like translation. In an encoder-only setup, the model processes the input data sequentially in layers, extracting and encoding information as it progresses. A typical example of encoder-only models is BERT from Google (Devlin et al. 2018), a language representation model with bidirectional Transformer encoders. It is trained on BooksCorpu (Zhu et al. 2015) (800M words) and English Wikipedia (2,500M words). It pushes the GLUE score to 80.5% and MultiNLI accuracy to 86.7%. Other encoder-only models are mostly the variants of BERT, such as ROBERTa from Meta (Liu et al. 2019) and DeBERTa (He et al. 2020) from Microsoft.\nDecoder-only models. This kind of model consists solely of decoder components of the Transformer. Decoder-only models typically employ designs like auto-regressive models, where the output is produced token-by-token. Each token generated by the decoder is conditioned on the previous tokens. The typical decoder-only models are the Generative Pre-trained Transformer (GPT) series (Radford et al. 2018, 2019; Brown et al. 2020; Achiam et al. 2023) developed by OpenAI. Taking the GPT-3 as an example, it is composed of multiple Transformer decoders with up to 175 billion parameters, making it one of the largest language models at the time of its release. It is trained on 300 billion tokens over Common Crawl (Raffel et al. 2020), WebText2, Books 1, Books2, and Wikipedia datasets. It has demonstrated strong zero-shot and few-shot learning abilities on many language tasks. Besides GPT series, many decoder-only models are also developed, such as OPT, LLaMA, Llama 2 from Meta (Zhang et al. 2022; Touvron et al. 2023a,b), PaLM, PaLM 2 from Google (Chowdhery et al. 2023; Anil et al. 2023), and BLOOM from BigScience. (Workshop et al. 2022).\nEncoder-decoder models. This kind of model consists of both the encoder and decoder of the Transformer. It can combine the advantages of the above two structures and complete tasks that require both understanding inputs and generating long sequences. The existing encoder-decoder models include the GLM from Tsinghua University (Du et al. 2021), T5, FLAN-T5, UL2 from Google (Raffel et al."}, {"title": "A.2 Reinforcement Learning", "content": "Reinforcement learning (RL) (Kaelbling, Littman, and Moore 1996) is a type of machine learning where an agent learns to make decisions by performing actions in an environment and receiving feedback through rewards or penalties. The learning objective in RL is to maximize the cumulative reward. Unlike supervised learning where models are trained using a dataset containing input-output pairs, RL involves an agent that learns from the consequences of its actions, typically through a system of rewards and penalties. This trial-and-error approach and the focus on decision-making in uncertain environments set RL apart from supervised learning's reliance on labeled datasets for training. Based on whether the environment is modeled, we can classify existing reinforcement learning algorithms into two categories: Model-free RL and Model-based RL.\nModel-free RL. The Model-free RL algorithms learn the agent directly based on trajectory samples generated during the interaction with the environment. Based on the representation of the model, this kind of algorithm can be further classified into actor-only algorithm, critic-only algorithm and actor-critic algorithm. In actor-only algorithms, a policy network \u03c0\u03bf(\u03b1 | s) is directly used to represent the behavior of the agent. This type of algorithm puts the current state st into the policy network and outputs the current action at. The corresponding algorithms include Reinforce (Williams 1992), policy gradient (Sutton et al. 1999), etc. In critic-only algorithms, there is only the value evaluation model. For the state st, the value model evaluates every possible a' \u2208 A and selects at that obtains maximum value. This type of method includes Q-learning (Watkins 1989), SARSA (Rummery and Niranjan 1994), Distributional RL (Bellemare, Dabney, and Munos 2017), etc. In actor-critic algorithms, the policy network (actor) and value evaluation model (critic) both exist to predict the action and value simultaneously. This kind of method includes DDPG (Lillicrap et al. 2015), TRPO (Schulman et al. 2015), PPO (Schulman et al. 2017), A3C (Mnih et al. 2016), etc., where PPO is commonly used in the training of large language models.\nModel-based RL. The Model-based RL algorithms can model the environment to solve the sample-efficiency problem since the agent does not have to spend much time directly interacting with the environment. The model for environmental modeling is generally referred to as a world model. It predicts the next state st+1 and the reward rt according to the current state st and action at. The model-based RL algorithms include Dyna-Q (Peng et al. 2018),"}, {"title": "B More details for experiment settings", "content": "B.1 LLM experts\nWe select eleven LLMs with different structures, different model sizes, and different amounts of training data as LLM experts for the LLM ensemble task: (a) Open Assistant (LAION-AI 2023) is a chat-based and open-source assistant, (b) Vicuna (Chiang et al. 2023) is aligned on tremendous conversations between users and proprietary chatbots, (c) Alpaca (Taori et al. 2023) is fine-tuned from the LLaMA-7B model (Touvron et al. 2023a) on 52K instruction-following data generated by the Self-Instruct technique (Wang et al. 2022), (d) Baize (Xu et al. 2023) is an open source chat model trained with LoRA that uses 100k dialogs generated by ChatGPT chatting with itself, (e) MOSS (Sun et al. 2023) is an open source conversational language model pretrained on CodeGen initialization, (f) ChatGLM (Du et al. 2022) is an open source, bilingual dialog language model based on the General Language Model (GLM) architecture, (g) Koala (Geng et al. 2023) is a chatbot trained by finetuning LLaMA on dialogue data gathered from the web, (h) Dolly2 (Conover et al. 2023) is a family of EleutherAI Pythia-based models fine-tuned specifically on a new highquality human-generated instruction tracking dataset, (i) Mosaic MPT (Team et al. 2023) is a chatbot model for generating conversations, built by fine-tuning MPT-7B, (j) StableLM (Stability-AI 2023) is auto-regressive language models based on the NeoX transformer architecture, (k) FLAN-T5 (Chung et al. 2022) is a T5 model that has undergone instruction fine-tuning. In addition, we select three candidate LLMs from different domains for further LLM ensemble task: (a) PMC-LLAMA (Wu et al. 2024) is the medical LLM of LLaMA fine-tuned with medical corpus. (b) K2 (Deng et al. 2024) is an open-source language model trained by firstly further pretraining LLaMA on collected and cleaned geoscience literature, including geoscience open-access papers and Wikipedia pages, and secondly fine-tuning with knowledge-intensive instruction tuning data (GeoSignal). (c) WizardCoder (Luo et al. 2023) is an open code LLM, fine-tuned from LLaMA by EvolInstruct."}, {"title": "B.2 Evaluation Metrics", "content": "We evaluate the quality of the generated text by employing the GPT-Rank (Jiang, Ren, and Lin 2023) and the conventional automatic metrics for natural language generation (NLG) tasks: BERTScore (Zhang et al. 2019), BARTScore (Yuan, Neubig, and Liu 2021), and BLEURT (Sellam, Das, and Parikh 2020). In the GSM8K dataset, we use the accuracy metric. In addition, we calculate the average LLM parameters according to the calling frequency. Then the average LLM parameters (Inference) and agent parameters (Agent) during inference are used as two metrics to evaluate the consumption of computational resources."}, {"title": "B.3 Construction of the Multidomain dataset", "content": "We construct a new dataset, called Multidomain, to further validate the versatility of our proposed DER in a diverse LLM scenario. As shown in Table 8, we compose the Multidomain dataset by randomly selecting from the training and test sets of each of the four datasets: pmc_instructions (Wu et al. 2024), geosignal (Deng et al. 2024), MBPP (Austin et al. 2021) and Mixinstruct (Jiang, Ren, and Lin 2023). The pmc_instructions (Wu et al. 2024) is a medical corpus dataset that contains 514k instructions. The geosignal (Deng et al. 2024) is a geoscience corpus dataset that contains 39.7k instructions. The MBPP (Austin et al. 2021) consists of around 1,000 crowd-sourced Python programming problems."}, {"title": "B.4 Implementation details of DER", "content": "We conduct both training and inference processes on NVIDIA 8\u00d7A800 GPUs, implementing our proposed DER through the PyTorch* framework with version 2.1.2. We use the FastChat framework to deploy LLMs and encapsulate them in OpenAI API format * for calling. To update the network parameters, we used the Adam optimizer. The learning rate is set to 10-5 for the actor model and 10-6 for the critic model. Based on this, we train the DER-Agent for about 1.0 epochs until convergence, running for about 180 hours. In addition, \u03b1, \u03b2, and \u03b3 in Equation 4, 5 are set to 0.001, 5.0, and 2.0, respectively. During the training phase, clip range, discount factor, and batch size are set to 0.20, 0.95, and 32, respectively."}, {"title": "B.5 Implementation details of Terminator", "content": "During the training phase of the proposed DER, the trajectory termination threshold po is determined, and P(\u00fbt) is computed with the target answer during trajectory sampling. However, during the testing phase, no target answer exists and P(\u0177t) cannot be computed, so it is not possible to judge the termination condition according to the threshold po. Therefore, we train a Terminator to judge whether \u0177t is satisfactory enough to reach the end of the trajectory. The Terminator is a binary classifier composed of an OPT-125M with two Linear layers connected to the last hidden layer.\nConstruction of dataset. We construct a MixInstruct-T dataset based on the MixInstruct dataset (Jiang, Ren, and Lin 2023) for training Terminator. Specifically, we use questions from the MixInstruct dataset with the answers of each of the"}, {"title": "B.6 Implementation details of baseline"}]}