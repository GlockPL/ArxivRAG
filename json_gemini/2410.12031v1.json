{"title": "A Learning Search Algorithm for the Restricted Longest Common Subsequence Problem", "authors": ["Marko Djukanovi\u0107a", "Jaume Reixachd", "Ana Nikolikj\u00ba", "Tome Eftimov", "Aleksandar Karteljb", "Christian Blumd"], "abstract": "This paper addresses the Restricted Longest Common Subsequence (RLCS) problem, an extension of the well-known Longest Common Subsequence (LCS) problem. This problem has significant applications in bioinformatics, particularly for identifying similarities and discovering mutual patterns and important motifs among DNA, RNA, and protein sequences. Building on recent advancements in solving this problem through a general search framework, this paper introduces two novel heuristic approaches designed to enhance the search process by steering it towards promising regions in the search space. The first heuristic employs a probabilistic model to evaluate partial solutions during the search process. The second heuristic is based on a neural network model trained offline using a genetic algorithm. A key aspect of this approach is extracting problem-specific features of partial solutions and the complete problem instance. An effective hybrid method, referred to as the learning beam search, is developed by combining the trained neural network model with a beam search framework. An important contribution of this paper is found in the generation of real-world instances where scientific abstracts serve as input strings, and a set of frequently occurring academic words from the literature are used as restricted patterns. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed approaches in solving the RLCS problem. Finally, an empirical explainability analysis is applied to the obtained results. In this way, key feature combinations and their respective contributions to the success or failure of the algorithms across different problem types are identified.", "sections": [{"title": "1. Introduction", "content": "A string is a finite sequence of characters from an alphabet \u03a3. In many programming languages, a string is used as a data structure. In biology, they represent models for DNA, RNA, and protein sequences. In the field of stringology and bioinformatics, a pivotal task concerns finding meaningful and representative measures of structural similarity between various molecular structures. Among several proposed measures, one that gathered significant attention from a practical and theoretical point of view is finding the longest common subsequence (LCS) for a set of input strings. In this context, a subsequence of a string s is a string obtained by deleting zero or more symbols from s, preserving the order of the remaining symbols. Seeking the longest common subsequences has been a subject of intrigue for more than half of a century. The LCS problem is stated as follows. Given a set of input strings S = {$1,..., sm} over a finite alphabet \u2211, where m\u2208 N is arbitrary, the aim is to identify a common subsequence of all strings from S with the maximum possible length [4]. Apart from bioinformatics, this problem has been important in various fields, such as data compression and text processing [41].\nIn the beginning, the focus of scientists has been on developing efficient algorithms for LCS problem cases with fixed m, especially for m = 2. The range of developed algorithms includes dynamic programming (DP), such as the Hirschenberg algorithm, the Hunt-Szymanski algorithm, and the Apostolico-Crochemore algorithm; see [22, 3]. By the end of the last century, the focus shifted to solving the LCS problem for arbi-trary m > 2. Note that for any fixed m, the LCS problem is polynomially solvable by DP. This, however, comes at a considerable price, as the complexity of DP is O(nm), where n is the length of the longest string in S. Thus, DP quickly becomes impractical in general cases. For arbitrary values of m, the problem is known to be NP-hard [30]. Additionally, it was found that the time complexity of O(nm) is likely to be a tight one unless P = NP. Consequently, the existence of an efficient algorithm for the general LCS problem scenario seems unlikely, yielding the appearance of various heuristic and approximation algorithms in the literature published at the beginning of this century. In particular, approaches based on Beam Search (BS) [14] and hybrid anytime algo-rithms [15] have been established as the most efficient heuristic and exact approaches, respectively. In parallel with studying the LCS problem, several practically motivated variants of this problem have been introduced and studied. Some of them include the longest arc-preserving common subsequence problem [27, 5], the constrained LCS problem [42, 12], and the shortest common supersequence problem [32], among others.\nThis study deals with the restricted longest common subsequence (RLCS) problem, initially described by Gotthilf et al. [20]. In addition to considering an arbitrary set of input strings S over a finite alphabet \u2211, the problem involves an arbitrarily large set of restricted pattern strings R = {r1,...,rk} over the same alphabet. The objective is to find the longest common subsequence s of the strings in S such that none of the restricted patternsri \u2208 R is a subsequence of s. In [20], the authors show that the RLCS problem is NP-hard even with two input strings and an arbitrary number of restricted patterns. Moreover, they develop a DP approach for general values of m and k. In this scenario, they proved that RLCS is fixed-parameter tractable (FTP) when parameterized by the total length of the restricted patterns. In addition, the authors propose two approximation algorithms: one ensures an approximation ratio of 1/|\u03a3| and the other one guarantees a ratio of (kmin \u2013 1)/nmin, where kmin and nmin represent the lengths of the shortest strings in Rand S, respectively.\nIndependently of Gotthilf et al. [20], Chen and Chao [8] proposed a DP approach specifically for the RLCS problem with m = 2 and k = 1, which runs in O(|81|\u00b7|82|\u00b7|r1|) time. For this special case of the RLCS problem, Deorowicz and Grabowski [11] intro-duced two asymptotically faster sparse DP algorithms than the conventional dynamic approach. They require subcubic time complexities of O(|51|\u00b7|52|\u00b7|r1|/log(|81|)) and O(|81|82||r1|/log\u00b3 (|s1|)), respectively, by utilizing special internal data structures. Farhana and Rahman [17] proposed a finite automata-based approach for the general RLCS problem that runs in O(|\u03a3|(R + m) + nm + |\u2211|Rnk) time, where R = O(nm) denotes the size of the resulting automaton. Recently, Djukanovic et al. [13] proposed a general search framework to solve the RLCS problem. In particular, an exact A* search and a heuristic BS approach are derived from this framework. These algorithms are currently the state of the art in terms of exact and heuristic solvers. However, the current literature has an obvious limitation: all algorithms were tested on the instances with limited sizes, that is, with at most five input strings. The same holds for the set of pattern strings. Therefore, the effectiveness and limitations of the existing approaches for solving the tackled instances remained weakly explored, despite their superiority in contrast to the remaining competitor approaches."}, {"title": "1.1. Preliminaries", "content": "Before we dive into more complex issues, let us prepare the ground with a few definitions and notation. The length of a string s is denoted by |s|. By s[i], 1 \u2264 i \u2264 |s|, the i-th character of strings is referred to. Note that the position of the leading character is indexed with 1. For two integers i, j \u2264 |s], s[i, j] is the contiguous part of the strings that begins with the character at position i and ends with the character at position j; if i = j, the single-character string s[i] = s[i, i] is given; finally, if i > j, s[i, j] refers to the empty string \u025b. For a left position vector p\u00b9 = (p{,...,pm), 1 < p\u0142\u2264 |si|, i = 1,...,m, we denote by S[p] the set of suffix input strings associated with the respective coordinates of this vector, i.e., S[pl] := {si[p\u0142, |si|] | i = 1, ..., m}. Given string s and letter a, by sla, we denote the number of times letter a appears in string s. By srev, we denote the reversed string of string s. Finally, by Succ(x)i,a, we denote the smallest index (y) greater or equal to x so that si[y] = a, i = 1, . . ., m; if no such position exists, Succ(X)i,a := |si| +1.\nA complete RLCS problem instance is denoted as (S, R, \u03a3), where S contains the input strings, R the restricted pattern strings, and \u2211 is the finite alphabet. For two integer vectors p \u2208 Nm and q \u2208 Nk, a sub-problem (sub-instance) of the initial problem instance concerning these two (left) positional vectors is denoted by (S[p], R[q]). From now onwards, by n we denote the length of the longest input string from S.\nThe remaining sections of the work are organized as follows. Section 2 details the general search framework for solving the RLCS problem. In particular, an A* search and a BS algorithm are derived and explained in sufficient detail. Additionally, two classical heuristic functions are explained for the tackled problem. Section 3 introduces a learning BS approach. The exhaustive experimental evaluation is provided in Sec-tion 4. Section 5 is devoted to providing a deeper understanding of the algorithms' performances by performing an explainability analysis. The paper finally concludes with Section 6 along with directions to future research."}, {"title": "2. Search Approaches for the RLCS Problem", "content": "The general search framework for the RLCS problem was proposed in [13]. \u03a4\u03bf ensure completeness of the present paper, this search framework is explained in the fol-lowing section, mainly following the notation introduced in the aforementioned paper. Afterward, we show two derivatives of this framework: an A* search approach and BS."}, {"title": "2.1. The General Search Scheme", "content": "The state graph represents the environment of our proposed algorithms. Its inner nodes represent partial solutions, while its leaf nodes represent complete solutions. Moreover, edges between nodes represent extensions of partial solutions. The state graph G = (V, E) of an RLCS problem instance (S, R) is defined as follows.\nWe say that a partial solution that is, a common subsequence su of the strings in S that does not contain any string from R as subsequence\u2014induces a state graph node v = (pL,v, lv, uv) \u2208 V if:\n\u2022 |s\"] = u\"\n\u2022 s\u201d is a subsequence of all si[1, p 1, p\u00b4\u00ba \u2212 1], i \u2208 {1, ..., m} and p\u00ba \u2013 1 is the smallest index that satisfies this property.\n\u2022 su contains none of the prefix strings r; [1, 13] as its subsequence whereas r; [1, 13; -1], j\u2208 {1, ..., k} are all included.\nThere is an edge (transition) between nodes v\u2081 = (pL,v1, lv1, u21) and v2 = (pL,v2, lv2, uv2) labelled with a letter a \u2208 \u03a3, denoted by t(v1, v2) = a, if:\n\u2022 u1 + 1 = u2\n\u2022 The partial solution inducing node v2 is obtained by appending the letter a to the partial solution inducing node v1.\nEach edge of the state graph of an RLCS problem instance has weight one and (as mentioned above) a label denoting the letter used to extend the respective partial solution.\nNode extension. The process of determining the successor (child) nodes of a node v is called extending v. For doing so, we identify those letters that can feasibly extend the partial solution su represented by v. This procedure consists of three steps. First, all letters that occur in each string from S[pl] are identified. Second, a letter that causes a violation of the restrictions is removed. This happens if one of the restricted patterns ri \u2208 R becomes a subsequence of the partial solution generated by extending s' with that letter. Third, dominated letters are omitted from consideration. Letter a is said to dominate letter b (i.e., b is dominated by a) if Succ[pLvi]i,a < Succ[pL,vi]i, for every i \u2208 {1, ..., m} and rj[l;] \u2209 {a,b} for all j \u2208 {1, ..., k}. The set of non-dominated feasible letters to extend the partial solution of a node v is denoted by End. Now, for a letter a \u2208 End, the corresponding successor node w = (pL,w,lw, uw) of v is constructed in the following way.\n\u2022 uw = u + 1, because the partial solution of node v derives the partial solution of node w by appending the letter a to it: sw = s\u00ba \u00b7 a.\n\u2022 1 = 1; + 1 if rj[lv] = a or ly = l; otherwise.\n\u2022 For the (left) position vectors we have pl = Succ[p]i,a + 1 where Succ(x)i,a represents the smallest index greater or equal to x so that si[y] = a.\nNote that the data structure Succ is preprocessed before the construction of the RLCS state graph has started. This ensures finding suitable position vectors of a child node in O(m) time.\nThe root (initial) node is given by r = ((1, . . ., 1), (1, . . ., 1), 0) and it corresponds to the empty solution s\u2033 = \u025b that is trivially feasible and induces the complete prob-lem instance (S, R). A node v is complete if End = \u00d8, that is, if it does not have any child nodes (successors). Note that (partial) solutions that induce complete nodes are candidates for optimal solutions. In this context, optimal solutions are endpoints of the longest paths from the root noder to complete nodes. Since the RLCS problem is"}, {"title": "2.2. A* Search Algorithm", "content": "A* search [21] is an exact, informed search algorithm that follows the best-first search strategy of node processing. It is the most widely used path-finding algorithm; it has been applied to solve many problems, including video games, string matching and parsing, knapsack problems, and others [26, 39]. Its core principle is to expand the most promising node at each iteration. To evaluate a node v, a scoring function f(v) = g(v) + h(v) is utilized. By assuming the goal is to find the longest path, as in the case of the RLCS problem, functions g() and h() are defined as follows:\n\u2022 g(v) is the current longest path from the root r to v.\n\u2022 h(v) is a heuristic estimation of the length of the longest path from v to a complete (goal) node.\nNote that A* search works on a dynamically created directed acyclic graph and in practice rarely examines all nodes. It has the advantageous ability to merge multiple nodes into one, which, as explained below, leads to considerable memory savings.\nTo set up an efficient A* search for the RLCS problem, two crucial data structures are leveraged:\n\u2022 A hash map N with keys represented by pairs (pLv, l\u00ba), with the corresponding value as the longest partial solution that induces a node with these vectors, refer-ring the sub-instance (S[pL;"}, {"title": "2.3. Beam Search Algorithm", "content": "Beam Search (BS) [28] is a heuristic tree-search algorithm that works in a \u201cbreadth-first-search\u201d (BFS) manner, expanding up to \u03b2 > 0 best nodes at each tree level. Pa-rameter \u1e9e ensures the size of the BS tree remains polynomial with the instance size, which makes this method robust in providing high-quality solutions to complex prob-lems. The value of \u1e9e controls the trade-off between greediness and completeness. BS is widely used in many fields such as packing [1], scheduling [38], and bioinformatics [7], among others.\nThe effectiveness of BS is not only governed by the beam width \u1e9e, but also heavily relies on the heuristic function h(), which evaluates the quality of each node during the search. The choice of h() is typically problem-specific and sensitive to the char-acteristics of the problem instances. In the context of our application, we utilize the following heuristic functions: (i) the same UB as used in the A* search outlined above; (ii) the probability-based guidance introduced in Section 2.5; and (iii) information derived from a trained neural network as detailed in Section 3.\nThe BS approach for the RLCS problem is outlined in Algorithm 2. The algorithm begins by generating the root node r, which is then added to the initial beam B (i.e., B = r), and initializing the best solution Sbest to an empty string \u025b. In the main loop of BS all nodes from the current beam B are expanded in all possible ways, producing a set of child nodes stored in set Vext. These child nodes are then sorted in descending order based on their h()-values. The top \u1e9e nodes from Vext are selected to form the beam B for the next entry into the algorithms' main loop. This process is repeated until the beam B becomes empty, at which point the algorithm terminates. The best RLCS solution Sbest is derived when Vext becomes empty by extracting the corresponding complete solutions from the nodes in the current beam."}, {"title": "2.4. Upper bounds", "content": "Note that any upper bound for an LCS problem instance is also an upper bound for the corresponding RLCS problem instance obtained by adding a set R of restricted strings to the LCS instances' input strings. The upper bound we used within A* search and BS is the minimum of two known LCS upper bounds, denoted as UB\u2081 and UB2."}, {"title": "2.5. A Probability-based Heuristic Function for RLCS", "content": "This section is devoted to deriving a probability-based search guidance to guide the search. It is based on a probabilistic model constructed assuming input strings are generated uniformly at random. The probabilistic model for the classical LCS problem has been proposed in [33]. After improving this model, we utilize it for the RLCS problem, additionally supported by a tie-breaking mechanism.\nLet us assume that all strings in S are independently and randomly generated. Given an arbitrary string s, let Ei denote the event that s is a subsequence of si, that is, s\u4ebasi. Let us denote the probability that the event Ei is realized by Pr(Ei), i = 1,...,n. In that way, one obtains\nPr(s < S) = Pr(s\u4eba s\u2081 > ...> s < sm) = Pr(\u2229_1E)\nm\nm\n= \u03a0 Pr(E) = I Pr(s < Si) = [[ P(|5|, Si),\ni=1\ni=1\ni=1\nwhere P is a matrix that, at position (i, j), contains the probability that an arbitrary string of length i is a subsequence of a random string of length j. Note that we assume mutual in-dependency between all strings from S here. Note that the DP approach for determining entries of this matrix P is derived in [33] and is given by the following recurrence relation:\n1\nP(i, j) = P(i - 1, j \u2212 1) + 2-1\u03a3 P(i-1,j), for 0 \u2264 i, j \u2264 n,\n|\u03a3|\nwhere the initial values are set to P(i, j) = 0 for i > j and P(0, j) = 1 for j \u2265 0.\nMatrix P can be pre-processed in O(n\u00b2) time.\nNow, this result can be seen as a heuristic guidance regarding any node v = (p\", lu, u\u00ba) and its associated sub-problem in the following way:\nm\nHRLCS(V) = P(k, |si| \u2212 p + 1)\ni=1\n=\u03a3\n(4)\nwhere k represents a strategic parameter, chosen heuristically at each level of BS. In [33], the following formula is applied to calculate the value of k\nk\nmax\n{1 1, min si-P+1\nVEVext \u03a3\n]}\n=\n(5)\nAs stated in the original paper [33], the chosen value of k may actually be im-proved. And in fact, we detected the following issue with Eq. (5). Namely, utilizing all nodes from Vext for calculating an appropriate k-value, as done in Eq. (5), may be inappropriate and far away from reality for many nodes, as the optimal length from a node at the considered level to goal nodes is usually much higher than that given by Eq. (5). Thus, k may be underestimated in this way. Note that the k-value refers to an estimated number of times partial solutions of each node in Vext can be extended. By utilizing unreasonably small k value, the importance of, in reality, more promising nodes can be marginalized and make them close to those that are less promising. This issue frequently affects the search at deeper levels of the beam search where nodes are closer to goal nodes, and the value of k often reduces to the minimum value of 1 too quickly according to Eq. (5). To fix this issue, we employ the following methodology.\nWe determine a more suitable value of k on the basis of a subset Vext Vext of pre-defined promising nodes. To determine Vert at each level, all nodes are sorted according to their upper bound (UB) value in a decreasing order. A tie-braking mechanism is employed for doing so. This is done by leveraging the so-called Rmin score defined by\nRmin(v) = min{|rz| \u2212 l\u00ba + 1 | i = 1, ..., k}\n(6)\nwhere larger values are favored. Then, a percent_extensions (parameter of the al-gorithm) of the leading nodes from Vext are pursued to Vert. Afterwards, Eq. (5) is applied on the basis of Vert (instead of Vext) for determining the value of k. In case"}, {"title": "3. Learning Beam Search Approach", "content": "Finally, after describing an upper bound and a heuristic guiding function for the RLCS problem, we also introduce a heuristic function obtained by learning. The neural network model proposed for this purpose is trained offline, utilizing a set of training and validation problem instances. For providing heuristic guidance regarding a state graph node v, the neural network receives a set of (numerical) node features to be evaluated and provides the heuristic guidance value as output.\nIn this context, note that, recently, several BS approaches based on learned heuristic functions have been proposed, see [24, 25, 16]. These methods from the literature are developed in the context of the LCS problem and the Constrained Longest Common Subsequence Problem (CLCS), yet another practically motivated LCS problem variant. Similarly to our work, [24, 25] build a neural network to predict the heuristic value of the nodes at each level of BS, while [16] learn a policy to select the most promising ones. On top of being applied to a different LCS problem variant, our proposed approach differs from these frameworks in the training process. The mentioned methods from the literature utilize ideas from Reinforcement Learning (RL) to train the neural network in contrast to an evolutionary algorithm (EA) used here. In the future, it makes sense to compare our method to these three approaches from the literature by solving the same set of (combinatorial) optimization problems under the same conditions."}, {"title": "3.1. Features", "content": "Two types of features are used as input of the neural network for every state graph node v to be evaluated: (i) features related to the v, which try to capture its charac-teristics, and (ii) general features related to the problem instance under consideration, such as the number of input strings, the number of restricted strings, and the alphabet size."}, {"title": "3.1.1. Node features", "content": "Remember that a node v is stored as a tuple (pLv, l\", u\") in the state graph. Vector pL, keeps track of the input strings' relevant parts (suffixes) available for further ex-tension of the partial solution represented by the v. Vector lu keeps track of the suffixes of the restricted strings that are subsequences of this same partial solution, and finally, u\" is the node's partial solution length. These three values are used to define the node features.\nNote that the length of vectors pLv and lu depends on the amount of input and restricted strings, respectively. On top of this, the scale of their values depends on the length of these strings. To keep the number of features and their scale comparable throughout instance sizes, their information is summarized in the following manner. First of all, both vectors are normalized regarding string length in the following way:\nLv\nPi\nSi for i \u2208 {1, ..., m} and l\n1\nrj for j \u2208 {1, ..., k}\nThe maximum, minimum, average, and standard sample deviation of the resulting standardized vectors are then used as the corresponding features, making the number of features independent of the instance size:\n(max (p), min (p), avg(,\"), sd (p), max (\u00ce\u00ba), min (\u00ce\u00ba), avg(\u00ce\u00ba), sd(\u00ce\u00ba))\nHence, this results in a total of eight node features to be utilized as input to the neural network.\""}, {"title": "3.1.2. Instance features", "content": "To provide information about the specific problem instance, the following features are included: (i) alphabet size (|\u03a3|), (ii) number of input strings (m), and (iii) number of restricted strings (k). Moreover, for the RANDOM benchmark set, the length of the input strings (n) and of the restricted strings (ro) are also used as two additional features, as in these problem instances all input strings and all restricted strings have the same length, respectively.\nTherefore, in total 13 and 11 features are extracted when tackling a problem in-stance from benchmark sets RANDOM and ABSTRACT, respectively. Finally, note that before the neural network takes the features as input, these are normalized to have unit-mean and zero-variance.\nAfter conducting several preliminary experiments, we decided to use a feed-forward neural network that consists of three hidden layers; the first two hidden layers comprise 10 nodes each, and the last hidden layer comprises 5 nodes. All three layers employ the sigmoid activation function. is illustrated the structure of the considered neural network."}, {"title": "3.2. Neural Network Training", "content": "As mentioned earlier, we trained the neural network on full-size instances using an EA. Hereby, each individual represents a set of weights for the neural network, and the EAs' population is evolved until overfitting is detected. Contrary to the traditional supervised approach, this method does not require having examples for training. This is particularly desirable in our context due to the difficulty of obtaining optimal node-heuristic value pairs to be used as training examples, especially for large-sized instances.\nThe training process employs two sets of problem instances, which we henceforth call training and validation instances, denoted by Tinst and VLinst, respectively. The evaluation of an individual that is, the evaluation of a set of weights-works as follows. Firstly, the neural network is equipped with the weights of the individual. Afterward, the BS guided by the neural network is applied to every instance from the training set Tinst. The individual's fitness is then set to the average length of the obtained solutions, which is referred to as the training value. The overfitting issue is checked once a new best individual is found, representing an individual with a new best training value. This step consists of again executing BS guided by the neural network, but this time on the validation instances from VLinst. The average length of the solutions obtained refers to the validation value of the individual. Validation values are used to decide when to stop the training process. In particular, we decided to train in an early stopping fashion, terminating the training process whenever the validation value decreases.\nThe particular EA used for training is a so-called Biased Random Key Genetic Al-gorithm (BRKGA), initially introduced in [19] as a variant of the classical GA [18, 23]."}, {"title": "4. Experimental Evaluation", "content": "This section provides a comprehensive experimental comparison between the fol-lowing four competitors:\n\u2022 The exact A* search, described in Section 2.2;\n\u2022 The three versions of the BS algorithm:\nthe BS guided by UB (labeled by BS-ub);\nthe BS guided by the probability-based guidance, drafted in Section 2.5 (labeled by BS-prob);\nthe learning BS, as drafted in Section 3 (labelled by LBS).\nNote that all other (exact) approaches from the literature for the RLCS problem were shown to be inferior to BS-ub and A* search in [13], where the evaluation is performed for much smaller instances introduced earlier in the literature. The same is concluded with our preliminary experimental evaluation of the newly introduced datasets. Hence, these approaches are omitted from further analysis and are therefore not presented here. All experiments were conducted in single-threaded mode on an Intel Xeon E5-2640 with 2.40GHz and 16 GB of memory for the heuristic approaches and 32 GB for the A* search approach. All instances with corresponding binaries of our LBS approach are available at the Git repository accessible through the link https://github.com/markodjukanovic90/RLCS-LBS."}, {"title": "4.1. Problem instances", "content": "The experimental evaluation employs two newly generated benchmark sets.\n\u2022 Benchmark set RANDOM comprises instances built from randomly generated strings. 5 random instances were generated for each combination of n \u2208 {200, 500, 1000}, \u0442\u2208 {3,5,10},p\u2208 {3,5,10}, |po| \u2208 {1%,2%, 5%} (of length of input strings), and |\u03a3| \u2208 {4,20}. Overall, 3\u00b73\u00b73\u00b73\u00b72\u00b75 = 810 random RLCS instances are included in this dataset.\n\u2022 Benchmark set ABSTRACT comprises 298 instances where the core input strings are the input strings from the ABSTRACT dataset primarily used for the LCS problem [35]. These input strings are characterized by close-to-polynomial dis-tributions of different letters from the English alphabet. The input strings orig-inate from abstracts of real scientific papers written in English. Pattern strings are added to these input strings to create RLCS problem instances as follows. In particular, the 60 most frequently occurring words in the research corpus were taken as pattern strings, see [10, 34, 9]. Our intention in making use of these"}, {"title": "4.2. Parameter tuning", "content": "Regarding BS-ub, there is only one parameter to tune (\u03b2). We independently run BS-ub for different \u03b2\u2208 {500, 1000, 2000, 5000, 10000}. Average solution quality over all instances from RANDOM and ABSTRACT benchmark sets per each of the considered \u1e9e values are displayed in Figs. 3a, 3b and 4, respectively. The best average results are, not surprisingly, reported for the largest value of \u1e9e, that is, \u03b2 = 10000. However, these results are just slightly better than those obtained for \u03b2 = 5000. However, this improvement comes with 2-3 times higher running times (see Figure 3b, 4b, and 4d). Thus, we opt for reporting the results for \u03b2 = 5000 to aim at high-quality solutions while keeping running times reasonably short. Due to similar reasons, the same is decided for the two other BS derivatives, i.e. BS-prob and LBS, see e.g., Figs. 3c-3d, and Figure 5 in the case of the BS-prob approach, and Figs. 3e-3f and Figure 6 in the case of the LBS approach. For both of these BS derivatives, we chose a beam width of \u03b2 = 5000, which ensured a fair comparison among the approaches in terms of the size of the search space that is examined. We emphasize that after conducting several preliminary experiments, the percent_extensions parameter is set to 100% for the BS-prob approach.\nLast but not least, to train the (feed-forward) neural network (NN) of the LBS approach, we employed \u03b2 = 100 during training for the benchmark set RANDOM. For benchmark set ABSTRACT, we employed \u03b2 = 200, which seemed to perform better. Remember that this is the value of the beam width used for calculating the training and validation values during the execution of the training BRKGA. The parameters of this BRKGA were set to the following default values: Psize = 20, pe = 1, pm = 7 and p = 0.5."}, {"title": "4.3. Numerical results: benchmark set RANDOM", "content": "Numerical results of the four approaches on the dataset RANDOM are provided in Tables 1-6. These six tables report the results of 6 groups of instances, that is, one for each combination of n \u2208 {200, 500, 1000} and m\u2208 {2,10}. The results for instances with m = 5 are given in Appendix Appendix A. Each table is divided into five blocks. The first block presents the characteristics of the instance group, including the number of restricted strings (k), the length of each restricted string (|po|), and the alphabet size (||). The second block reports on the performance of A* search, detailing three key metrics: the average solution quality (|Sbest|), the average runtime (t[s]), and the average upper bound (ub) across five instances for the corresponding group. The final three blocks provide the results for the BS-ub, BS-prob, and LBS algorithms, respec-tively. Each algorithm is evaluated based on two indicators: average solution quality and average runtime across five instances. For clarity, the best results in each table are highlighted in bold.\nThe following conclusions are drawn from these results.\n\u2022 For small-sized instances with n = 200 and m = 3, the A* search successfully finds provenly optimal solutions for 15 out of 18 instance groups, with most of these instances being solved in a relatively short running time. The heuristic approaches demonstrate their effectiveness by finding nearly all these optimal solutions. Moreover, they outperform A* search on the largest instances with k = 10 and |po| = 10. For the instances with m = 5, A* search remains a strong performer, optimally solving the instances of 11 instance groups. However, for cases with larger k values and an alphabet size of || = 4, the problem becomes harder to solve exactly. In these scenarios, the heuristic methods BS-prob and LBS outperform A*, obtaining the best solutions for 14 and 16 instance groups, respectively, while BS-ub follows with successful results in 10 cases. For m = 10, A* search can solve only two instance groups optimally. In contrast, LBS and BS-prob are the most effective approaches, delivering the best solutions in 16 and 13 cases, respectively.\n\u2022 For instances with n = 500 and m = 3, A* search solves optimally only 3 out of 18 instance groups, primarily due to memory limitations. As expected, the performance of A* search quickly deteriorates with increasing n. Notably, for the largest instances in this group, A* produces solutions that are far from both the dual bounds and the solutions obtained by the heuristic approaches. LBS outperforms the other methods in these cases, delivering the best performance for instance groups. BS-prob is the second-best approach, achieving the best result in 3 cases. When m = 5, the performance of A* search degrades significantly, running out of memory in all cases without reaching the 600-second time limit."}, {"title": "4.4. Numerical results: benchmark set ABSTRACT", "content": "Numerical results of the four approaches regarding dataset ABSTRACT are provided in Tables 7-8. These tables are organized as follows: the first two columns provide the number of input"}]}