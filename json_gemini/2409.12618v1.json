{"title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning", "authors": ["Santosh Kumar Radha", "Yasamin Nouri Jelyani", "Ara Ghukasyan", "Oktay Goktas"], "abstract": "Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.", "sections": [{"title": "1 Introduction", "content": "The development of Large Language Models (LLMs) like GPT-3, PaLM (Anil et al., 2023), and their successors, including GPT-4 (OpenAI, 2023), Gemini (Team et al., 2023), LLAMA (Dubey et al., 2024), and Claude, has revolutionized natural language processing. LLMs have empowered Al systems to perform a wide range of tasks with remarkable proficiency. In the context of human-LLM interaction, a critical observation from practical experience is that the quality of LLM responses tends to improve with repeated prompting and user feedback. Recent research demonstrated that na\u00efve prompting can lead to calibration errors, while more sophisticated, iterative prompting strategies significantly improve both accuracy and reliability (Krishna et al., 2024). These results suggest that, given context-appropriate sequences of inputs, LLMs can much more effectively leverage their internal knowledge base (Jiang et al., 2020; Petroni et al., 2019; Talmor et al., 2020; Roberts et al., 2020) to provide richer, more nuanced answers (Sloman, 1996).\nA human user's interaction with in an LLM often proceeds as follows: the user poses a question to the LLM, receives an initial response, and, if the answer is incomplete or suboptimal, provides additional guidance to the LLM by reiterating contextual clues (e.g. by reminding the LLM of its role, suggesting additional information to consider, or highlighting specific parts of the response that need refinement). This back-and-forth process helps narrow the focus of the LLM while reducing the research effort required from the user, since the LLM is responsible for the bulk of the reasoning and information retrieval.\nWe identify two predominant forms of human-LLM interaction. In the first form of interaction, the user simply guides an LLM through its own internal knowledge base. For example, consider a scenario where an LLM generates code that is syntactically incorrect due to a missing bracket. The user might prompt it to \"verify the syntax,\" leading the LLM to correct the error in a subsequent response. In the second for of interaction, the user introduces new information to improve the LLM's response. For example, an LLM may be asked to provide up-to-date weather information for a specific city, but lacks access to real-time data. In this case, the user can supply this information (using a tool or API), then prompt the LLM to e.g. recommend weather-appropriate clothing or destination to visit in that locale. All together, the first form an interaction leads the LLM to better utilize its internal knowledge, whereas the second form of interaction involves augmenting the LLM's knowledge with new information.\nThe potential of iterative prompting to improve LLM responses is supported by research showing that prompt phrasing can significantly influence a model's performance in various settings (Brown, 2020; Opsahl-Ong et al., 2024). Figure 1 illustrates the progression from simple Input-Output (IO) approaches to more advanced methods like Chain-of-Thought (CoT) (Wei et al., 2022) and Tree-of-Thought (ToT) (Yao et al., 2024)ThCoT introduces sequential reasoning steps along a single linear path, while ToT explores multiple reasoning pathways in parallel, forming a branching structure to optimize the output.\nese methods represent \"reasoning frameworks\" that rely on static or semi-static prompts, which may struggle to adapt to the evolving context of each query and response, potentially limiting the quality of LLM responses. CoT prompting encourages LLMs to articulate its intermediate reasoning steps, which leads to better performance on complex tasks. Similarly, the related ToT approach (among"}, {"title": "1.1 Iteration of thought (IoT)", "content": "Unlike the aforementioned static and semi-static frameworks, IoT utilizes an Inner Dialogue Agent (IDA) to adjust and refine its reasoning path during each iteration. This enables adaptive exploration across different reasoning trees, fostering a more flexible and context-aware response generation process. A comparison to existing methods is shown schematically in Figure 1.\nThe core IoT framework is composed of three main components. Further details are also provided in Section 2.\n\u2022 Inner dialogue agent (IDA): The IDA functions as a \"guide\" that dynamically generates context-sensitive prompts based on the original user query and the LLM's previous response. The adjusted prompts servce to iteratively lead the LLM toward more refined and accurate answers. Mathematically, the IDA can be represented as a function $C: Q \\times R \\times K' \\rightarrow P$, where Q is the space of possible queries, R is the space of potential LLM responses, and P is the space of generated prompts. At each step, it takes the current query $q \\in Q$ and the previous response $r \\in R$ to generate a new prompt $p \\in P$. This process makes prompt generation dynamic, differentiating IoT from more rigid approaches like CoT and allowing it to adapt to an evolving context.\n\u2022 LLM agent (LLMA): The LLMA embodies the core reasoning capabilities of an LLM and processes the IDA's dynamically generated prompts. It uses an LLM's internal knowledge base K to refine its responses. Formally, we model the LLMA as a function $L : Q \\times P \\times K \\rightarrow R$. The LLMA takes as input a query q, prompt p and a knowledge base K then generates a refined response r. The LLMA also identifies areas of uncertainty or gaps in its own reasoning, providing feedback for the IDA to adjust prompts accordingly. This interaction creates a closed-loop system that continuously improves the quality of answers without external inputs.\n\u2022 Iterative prompting loop: The iterative process in IoT involves a back-and-forth between the IDA and LLMA. At each iteration i, the IDA generates a new prompt $p_i = C(q, r_{i-1})$ based on the original query q and the LLM's previous response $r_{i-1}$. The LLMA then responds to $p_i$ with $r_i = L(q, p_i, K)$. This loop continues until a satisfactory answer $r^*$ is found or the arbitrary maximum iteration count is reached. This back-and-forth approach allows IoT to navigate complex reasoning paths to efficiently explore various potential solutions. Moreover, introducing distrinct LLMs for the IDA and LLMA respectively can allow each agent to function as an open system (Von Bertalanffy, 1950) where internal knowledge is exchanged. In this scenario, the overall system behaves as a closed system with a combined knowledge base, enhancing internal reasoning without external input.\nIn the sections that follow, we present a detailed analysis of our IoT framework, describe our experimental methodology, and discuss empirical results. We also demonstrate the framework's effectiveness with experimental results on the various datasets, where significant improvements are observed over existing reasoning methods."}, {"title": "2 Framework and implementation", "content": "In this work, we use two distinct variants of IoT: Autonomous Iteration of Thought (AIoT) and Guided Iteration of Thought (GIoT). In the AIoT variant, the LLMA itself decides when it has generated a satisfactory response. This decision is reflected in a Boolean output signal, iteration_stop. Termination following a positive signal usually leads to fewer iterations than the enforced maximum. This, in turn, leads to faster evaluation with less exploration, but risks premature stops when facing more complex queries. Conversely, GIoT employs a more regimented strategy by mandating a fixed number of iterations. GIoT employs the opposite strategy, aiming for comprehensive exploration of reasoning paths to minimize premature convergence, at additional computational cost and with the risk of redundant or repetitive iterations.\nWe implemented the IoT framework, including both variants, as a Python library (AgnostiqHQ, 2024), using Pydantic (Pydantic, 2024) to provide provide output schemas for raw responses from LLMs."}, {"title": "2.1 Autonomous iteration of thought (AIoT)", "content": "In AIoT, the LLM also makes a determination at each step on whether the answer it has generated is sufficient. This is represented by an output signal, iteration_stop, which, if set to True, indicates that the LLM believes its answer is final and complete. The full AIoT process is shown in pseudocode Algorithm 1, below. A sample AIoT sequence is also provided in Appendix A."}, {"title": "2.2 Guided iteration of thought (GIoT)", "content": "The guided variant of Iteration of Thought (GIoT) represents a more controlled iterative process. In GIoT, the iteration continues for a predefined number of steps $N - 1$, and only in the N-th iteration is the LLM allowed to decide if it has reached the final answer. Here, the IDA continues to generate new prompts $p_i = C(q, r_{i\u22121})$ for the first $N \u2212 1$ iterations without allowing the LLM to conclude early. In the final iteration, the LLMA is asked to provide a conclusive answer $r^*$ based on the accumulated information from previous steps.\nLike AIoT, GIoT ensures that the LLM thoroughly explores its internal knowledge space and refines its output to a greater extent. However, unlike AIoT, GIoT admits the cost of additional generations as a compromise to prevent premature conclusion. The full GIoT process is shown in pseudocode in Algorithm 2. A sample GIoT sequence is also provided in Figure 2."}, {"title": "3 Results", "content": "To comprehensively evaluate the IoT framework, we conducted a series of experiments across various models, datasets, and reasoning strategies. Given the computational expense these evaluations, we selected specific model-dataset combinations to investigate performance and scalability under different conditions. This approach enables us to provide a targeted understanding of how reasoning capability and iteration strategy affect the overall quality of LLM responses. The following sections describe the experiments designed to explore these aspects, including their setups, objectives, and the insights derived from each."}, {"title": "3.1 Assessing IoT on the GPQA questionnaire", "content": "In this experiment, we quantify IoT's ability to accurately answer questions from the GPQA Diamond dataset (Rein et al., 2023). These questions are known to require deep reasoning and comprehensive internal knowledge, with even the highly capable LLMs yielding overall scores under 50% (Dubey et al., 2024).\nWe compare AIoT/GIoT with CoT on GPT-40 mini, a proprietary model (OpenAI, 2023). CoT is a widely used reasoning strategy that involves guiding the model through a step-by-step thinking process. By comparing CoT with AIoT/GIoT, we aim to understand the potential improvements resulting from iterating dynamically rather than following predefined steps.\nThe results of these experiments are presented in Table 1 andFigure 3ere it, is clear that conventional CoT performs about on par with the baseline IO approach, indicating that rigid step-by-step reasoning may not be effective for GPQA. Meanwhile, GIoT performs significantly better than IO and CoT with a modest 2.62% higher accuracy (on average). However, GIoT also exhibits larger variance than IO and CoT in its distribution of accuracy scores. One interpretation of this result is that forced iterations can lead to divergence due to hallucination (Huang et al., 2023) in cases where a correct and complete thought pattern is established well before the mandated number of iterations have been performed. AIoT, on the other hand, is more effective at avoiding this issue.\nemerges as the most effective strategy overall, with a 14.11% improvement in average accuracy over the IO baseline and the lowest variance among all methods tested. Lower variance in AloT's accuracy scores implies more consistent performance across different types of questions. Together with a higher average score, this superior result is attributed to AIoT's dynamic autonomous, context-aware termination of iterations, which prevents unproductive or counterproductive exploration of the response space. Notably, our analysis shows that AIoT completes approximately 60% of tasks within a single iteration and approximately 90% within two iterations. This reflects AIoT's efficiency in navigating the reasoning space without over-iteration. We therefore infer that AIoT's advantage, wherever applicable, is avoiding the pitfalls of both under- (as seen in IO and CoT) and over-exploration (a risk associated with GIoT)."}, {"title": "3.2 Assessing IoT on explorative problem-solving tasks", "content": "To evaluate the effectiveness of our IoT (Iterative of Thought) framework against the state-of-the-art, we conduct a comparative analysis using the Game of 24 and Mini Crosswords tasks. These games, featured prominently in the ToT genesis paper by Yao et al. (2024), are easy to understand, challenging to solve, but easy to verify. ToT is well-suited for problems that benefit from a wide variety of exploratory reasoning paths, owing to its systematic search strategy that traverses many possible solution graphs to find the optimal answer. Our motivation for this experiment is to assess whether our IoT method can effectively iterate towards optimal solutions without generating a multitude of alternate, discarded responses. With this in mind, our goal in this experiment is to compare the relative advantage of our IoT framework compared to CoT, recognizing the inherent advantages of ToT, at least in terms its overall solution ability, in contexts benefiting from a broader exploratory approach.\nThe Game of 24 involves generating an arithmetic expression using four given numbers and basic operations and brackets {+,-, \u00d7, \u00f7, (, )} to arrive at the number 24. This task requires not only computational ability but also strategic reasoning to explore different combinations of operations. The dataset for this task, as used in the ToT study, consists of various instances where the challenge lies in finding the most efficient path to the solution amidst multiple possibilities. Similarly, the Mini Crosswords task involves solving 5x5 crossword grids based on a set of clues. Solving these grids requires lexical reasoning and pattern recognition, as well as the ability to generate coherent word sequences that fit both vertical and horizontal constraints. The complexity of the Mini Crosswords task also stems from the need to test the compatibility of multiple potential word fits, refining choices based on feedback and constraints. Both datasets are therefore valuable for assessing a model's ability to try out various solutions within a reasonable search space.\nResults for these tasks are visualized in Figure 4, which reveals distinct performance differences between the various methods. Notably, GIoT on average outperforms the AIoT, CoT, and IO methods across both tasks. This result is consistent with the understanding that GIoT is a more exploratory alternative to AIoT. By compelling the model to explore multiple reasoning paths, GIoT enhances the likelihood of arriving at a correct answer, aligning with the ToT approach in terms of beneficial brute-force exploration.\nRegarding the Mini Crosswords task, the original ToT study (which used GPT-4) demonstrated substantial improvements over CoT, with success rate increases of 92.1% for letters and 284.6% for words (Yao et al., 2024). In comparison, our experiments using the less capable GPT-40 mini model show that GIoT achieves a success rate of 35.5% for letters and a 90.6% success rate for words, as compared to CoT. Meanwhile, AIoT shows gains of 28.3% and 74.5%, respectively. Although these differences are smaller than those reported for ToT, they should be considered in context with the limitions of GPT-40 mini versus GPT-4. It is also important to note that the superior performance of ToT in this task is primarily due to its capacity to explore a broader range of answers, potentially admitting a higher computational cost than GIoT.\nThe higher variance observed in the IoT results, particularly in the Mini Crosswords task, suggests a more diverse albeit not always productive exploration of solutions compared to CoT and IO. While this diversity can be advantageous in some scenarios, it may lead to sub-optimal convergence in more constrained problem spaces.\nA similar pattern of performance differences emerges in the Game of 24 task. Here, the ToT framework showed a dramatic improvement, with success rates increasing from 4.0% with CoT to 74% with ToT (at a breadth of 5), marking a relative improvement of 1750% (Yao et al., 2024). In comparison, our GIoT method achieves a notable 266.4% improvement over CoT, while AIoT shows a 168.4% increase. These results reflect the effectiveness of our iterative refinement approach in arithmetic problem-solving scenarios, even though a performance gap remains compared to ToT. The structured, multi-step reasoning of GIoT ensures a more thorough exploration of the solution"}, {"title": "3.3 Assessing IoT on multi-context reasoning and retrieval tasks", "content": "In our final experiment, we evaluate IoT on the HotpotQA-Hard dataset, a challenging benchmark for multi-hop question answering that demands sophisticated aggregate reasoning. Unlike simpler tasks that require straightforward information retrieval, HotpotQA involves complex information synthesis across multiple documents, requiring models to shift focus between various contexts to build a coherent answer. This necessitates bridging implicit information gaps, resolving ambiguities, and integrating scattered evidence.\nAnswering a HotpotQA question often involves several interconnected steps where initial findings must be used to guide further evidence retrieval. This process mirrors the key strengths of IoT: its ability to adaptively explore different reasoning paths, dynamically integrate context, and iteratively refine conclusions. The IoT's IDA plays a pivotal role here by guiding the LLMA to revisit and adjust its focus based on intermediate outputs, promoting more comprehensive exploration of the problem space. Such a mechanism is crucial for HotpotQA tasks, where the model must constantly re-evaluate earlier conclusions in light of newly synthesized information, ultimately leading to a more robust and accurate final answer.\nFor this experiment, again using GPT-40 mini as our engine, we benchmark the performance of AIoT against CoT using on three evaluation metrics: Exact Match (EM), F1 score, and ROUGE-L score. These metrics capture different facets of multi-hop QA performance: EM measures the proportion of exact matches with the ground truth, providing a stringent gauge of model accuracy; the F1 score balances precision and recall, capturing partial correctness; and ROUGE-L evaluates the longest common sub-sequence between generated and reference answers, highlighting semantic coherence.\nThe dynamic nature of (A)IoT allows it to autonomously adapt the depth of reasoning based on the complexity of the query, facilitating a flexible exploration of reasoning paths that CoT's static, step-by-step approach may lack. This flexibility enables the IoT framework to better handle the inherent ambiguities of HotpotQA, such as resolving conflicts or disambiguating entities across contexts. This can also serve as a self-correcting mechanism, helping to recognize gaps or errors in reasoning early on and prompting further exploration in subsequent iterations.\nOur results on the HotpotQA-Hard dataset reveal a clear advantage for AIoT over CoT. As shown in Figure 5, AIoT achieves an Exact Match (EM) score of 0.53, an F1 score of 0.699, and a ROUGE-L score of 0.72, significantly outperforming CoT on almost every instance of the task. These metrics demonstrate the effectiveness of AIoT in managing the complexities inherent in multi-hop question"}, {"title": "4 Strengths and weaknesses of IoT", "content": "One qualitative benefit of IoT is its inherent conceptual transparency and explainability. Like CoT and similar methods, IoT provides a clear trace of its reasoning process through a sequence of evolving outputs. However, unlike other \"multi-thought\" methods, IoT's sequence also includes explicit guidance generated by the IDA. This means each step is accompanied by a rationale that the underlying LLM treats equivalently to prompts from a human user. As a result, post hoc analysis of IoT's output sequences (example in Appendix A) can reveal the model's capacity to self-correct when provided with course-adjusting instructions. In addition to enhancing the model's explainability, this insight can inform more efficient interactions with LLMs in general.\nIt is important to note that the IoT framework is not inherently orthogonal to CoT nor Self-Consistent CoT (Wang et al., 2022). One could in principle combine IoT with CoT to create a hybrid method, IoToCoT, where both the Inner Dialogue Agent (IDA) and LLM Agent (LLMA) use CoT-based reasoning. Such combinations could amplify the benefits of structured reasoning while retaining the flexibility of iterative refinement. Additionally, while our experiments used the same base LLM for both IDA and LLMA, these agents can be made distinct to leverage different models or architectures, changing the total base knowledge of the system to be $K \\neq K'$ (Creswell and Shanahan, 2022).\nOwing largely to the versatility of LLMs, agentic LLM-based frameworks are not difficult to expand and compose. Recent work has suggested that larger ensembles of agents can lead to better reasoning performance (Li et al., 2024), with the rate of improvement diminishing beyond 10-15 agents. A natural progression of IoT could therefore be an expansion of the IDA into a meta-agent consisting of specialized sub-agents, which may or may not be dynamically defined on a per-query basis. Taking the knowledge base of the IDA to be $K' = \\sum K_i$, the size of the IDA ensemble, $M\\in\\mathbb{N}^+$, becomes an arbitrary parameter indicating the number of distinct LLMs behind the IDA's constituent sub-agents. IoT as introduced in this work (with $K = K'$) represents the \"smallest\" member of this generalized family and still suffices to deliver powerful reasoning capabilities. Based on Li et al. (2024), increasing the ensemble size M could be expected to improve reasoning performance in IoT, though at cost of additional complexity and a larger hardware budget to power a multitude of distinct LLMs. $M > 1$ also introduces the potentially challenging task of ranking sub-agent outputs or resolving conflicts in their guidance. Regarding complexity, using larger LLMs in a smaller ensemble may be a viable alternative for increasing the size of K' without increasing M.\nIoT's autonomous iteration also offers significant advantages in situations where human intervention is impractical or impossible \u2014 such that systems are constrained to function independently. Human oversight is difficult to achieve in contexts that demand rapid and continuous decision-making, for example. Here, IoT's autonomous reasoning capabilities can be a valuable asset. Moreover, the thought sequences generated by IoT (see Figure 2 and Appendix A) could serve as a valuable resource for fine-tuning existing models, potentially enhancing their reasoning capabilities. This dual benefit of autonomy and improved model training makes IoT a powerful tool in building more robust, self-sufficient systems.\nRegarding the two variants of IoT, our results demonstrate that while AIoT provides an efficient approach with autonomous decisions to stop iterating, it also often misjudges the completeness of its responses, leading to premature convergence. This limitation could be addressed by incorporating feedback agents (Chen et al., 2023), using techniques like maieutic prompting (Jung et al., 2022), or even allowing for human intervention or external knowledge checks. This would create a semi-autonomous framework that balances efficiency with robustness (Wu et al., 2022). On the other hand, GIoT forces a fixed number of iterations, which can improve performance in multi-step reasoning tasks, but may also increase the risk of hallucination if the model confidently drifts into incorrect reasoning. Appropriate techniques to reduce hallucination could further refine GIoT's utility in complex tasks (Tonmoy et al., 2024)."}, {"title": "5 Conclusion and future work", "content": "In this work, we introduced the Iteration of Thought (IoT) framework, in which an Inner Dialogue Agent (IDA) iteratively converses with an LLM Agent (LLMA) to perform various complex reasoning tasks like solving puzzles (Game of 24, Mini Crosswords) and answering difficult questionnaires (GPQA, HotpotQA). We employed two variants of this framework in our experiments, qualified as \"autonomous\" (AIoT) and \"guided\" (GIoT) respectively, to compare iteration-terminating mechanisms across these tasks. GIoT, the variant that always performs a fixed number of iterations, was seen to perform better than AIoT, the variant that self-determines termination, in Game of 24. On the other hand, AIoT had superior performance on GPQA. Both variants performed similarly on Mini Crosswords and always performed better than the well-known Chain of Thought (CoT) framework, wherever compared. We also compared our IoT framework against the hierarchical AgentLite framework on the multi-context HotpotQA task, finding improvements of approximately a 35% in the F1 score and 44% in the EM score over AgentLite. All together, our results demonstrate that IoT can succesfully introduce productive dynamism into low-complexity agentic frameworks.\nDetermining the scale and diversity of the IDA's knowledge base represents a promising direction for future work aiming to maximize the real-world utility of IoT. In pursuit of strictly framework-to-"}, {"title": "A Appendix", "content": "In this section, we provide an example to demonstrate how AIoT (Autonomous Iteration of Thought) works in practice. This example highlights the unique characteristics of AIoT, illustrating how the method improves reasoning, adaptability, and response accuracy."}, {"title": "A.1 Examples", "content": "This example highlights AIoT's strength in efficiently navigating a complex GPQA example."}, {"title": "A.1.1 Example of AIoT", "content": ""}]}