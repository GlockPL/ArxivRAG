{"title": "Scalable-Softmax Is Superior for Attention", "authors": ["Ken M. Nakanishi"], "abstract": "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.", "sections": [{"title": "1. Introduction", "content": "Length generalization, the ability to handle longer context sizes than those used during training, is a key challenge for Transformer-based large language models (LLMs) (Vaswani et al., 2017; Kazemnejad et al., 2023). Processing longer contexts enhances LLMs' ability to leverage in-context learning (Brown et al., 2020) and chain-of-thought reasoning (Wei et al., 2022). However, the computational and memory requirements for training Transformers grow quadratically with context size, imposing practical limits on the context sizes used during training. Consequently, LLMs must acquire the ability to generalize to longer context sizes beyond those seen during training. There are four primary approaches to addressing length generalization: improving positional encoding methods (Wei et al., 2022; Press et al., 2021; Su et al., 2024; Kazemnejad et al., 2023; Shaw et al., 2018), adopting sparse attention mechanisms (Ainslie et al., 2020; Gupta & Berant, 2020; Zaheer et al., 2020; Beltagy et al., 2020; Child et al., 2019; Kitaev et al., 2020; Roy et al., 2021; Sukhbaatar et al., 2019), further training on longer contexts after modifying positional encodings (Liu et al., 2024b;a; Ye et al., 2024), and enhancing attention mechanisms (Ye et al., 2024; Wang et al., 2024). This work focuses on enhancing attention mechanisms as an approach to address length generalization, specifically by replacing the Softmax function within the attention layers of Transformers.\nSoftmax converts an input vector into a vector that can be interpreted as a probability distribution, where all elements are non-negative and sum to one. In deep learning, Softmax is commonly used in multi-class classification tasks to convert logits into a probability distribution (LeCun et al., 1998). In the attention layers of Transformer-based language models, Softmax computes a probability distribution over all tokens in the context, determining how much attention is allocated to each token (Vaswani et al., 2017). Unlike the Softmax used in classification tasks, where the input vector size is fixed, the Softmax in attention layers has an input vector size that varies with the context size. The denominator of Softmax is the sum of the exponentials of all input elements, and its value increases as the context size grows, due to the larger number of input elements. In contrast, the numerator, the exponential of a single input element, is independent of the input vector size. As the input vector size grows, the resulting probability distribution becomes increasingly flat. In this paper, we refer to this phenomenon as attention fading, which we hypothesize reduces the model's ability to focus on key tokens in the context, thereby limiting length generalization.\nTo address this issue, we propose Scalable-Softmax (SS-Max), a novel alternative to Softmax in attention layers that mitigates attention fading and enhances length generalization. SSMax transforms an input vector into a probability distribution, similar to Softmax, but avoids attention fading even for large input vector sizes. The name 'Scalable-Softmax' reflects its ability to handle varying input vector sizes while preventing attention fading. Note that 'Scalable' does not refer to the scaling parameter s, which will be introduced later. Additionally, as shown in Section 2.3, SSMax integrates seamlessly into existing architectures with minimal code modifications, maintaining compatibility and efficiency.\nIn this study, we conducted several evaluations to assess the effectiveness of SSMax in improving Transformer performance. First, we compared the learning curves of Transformer models during pretraining and found that those with SSMax consistently achieved lower loss values compared to the standard Transformer. In all subsequent evaluations, we modified RoPE's 0 to 50 times its training value without additional training, allowing us to assess the model's ability to generalize to large context sizes under this drastic change. Second, unlike the standard Transformer, models with SSMax maintained significantly lower loss values even as the context size increased well beyond the training sequence length. Third, in key information retrieval tasks, models with SSMax retrieved key information reasonably accurately, even when the context size was extended up to 10 times the training sequence length. Further analysis of attention scores revealed that models with SSMax allocate significantly more attention to key tokens, even in long-context scenarios. Lastly, while models trained with SSMax from the beginning achieve superior length generalization, we showed that replacing Softmax with SSMax in pretrained models can still provide some degree of improvement."}, {"title": "2. Scalable-Softmax (SSMax)", "content": "The Softmax function transforms an input vector into a vector that can be interpreted as a probability distribution, where all elements are non-negative and sum to one. For an input vector z of size n, with components zi (i = 1, 2, ..., n), Softmax is defined as follows:\n$\nZ_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n$\nIn the attention layers of Transformers, the input vector size n increases as the context size grows. Softmax plays a critical role in computing a probability distribution over all tokens in the context, determining how much attention is allocated to each token. When n grows, the denominator in Equation (1) increases, while the numerator remains independent of n. As a result, the resulting probability distribution becomes increasingly flat. This phenomenon, which we refer to as attention fading, reduces the model's ability to focus on key tokens within the context, potentially limiting its length generalization capability.\nTo address this issue, we propose the Scalable-Softmax (SSMax) function, defined as:\n$\nZ_i = \\frac{n^{sz_i}}{\\sum_{j=1}^{n} n^{sz_j}} = \\frac{e^{(s \\log n) z_i}}{\\sum_{j=1}^{n} e^{(s \\log n) z_j}}\n$\nwhere s is a scalar referred to as the scaling parameter. Similar to Softmax, SSMax transforms an input vector into a probability distribution. However, the key difference lies in the dependence of the exponential base on input vector size n. This design aims to prevent attention fading by incorporating the input vector size into the function's formulation, which helps balance the scaling between the numerator and denominator in Equation (1). As a result, the probability distribution remains focused on key tokens even as the input size grows.\nIn the following subsections, we provide experimental evidence to justify the design of SSMax (Section 2.1) and present a theoretical analysis to further explain its design and effectiveness in addressing attention fading (Section 2.2). Finally, we discuss its ease of implementation, requiring minimal modifications to existing architectures (Section 2.3)."}, {"title": "2.1. Rationale Behind the Design of SSMax", "content": "To investigate the optimal formulation of SSMax, we conducted experiments to analyze how attention scores should ideally depend on the input vector size n. Specifically, we replaced Softmax in all attention layers with the following modified formulation:\n$\nZ_i = \\frac{e^{(sp_n + b)z_i}}{\\sum_{j=1}^{n} e^{(sp_n + b)z_j}}\n$"}, {"title": "2.2. Justification for the Design of SSMax", "content": "Let z = (z1, z2, ..., zn) be an input vector of size n, where zmax, z2nd, and zmin denote its maximum, second largest, and minimum elements, respectively. For simplicity, we assume zmax > z2nd in the following analysis. When z is processed by Softmax, zmax is transformed as\n$\nz_{\\max} = \\frac{e^{z_{\\max}}}{\\sum_{j=1}^{n} e^{z_j}}\n$\nThe right-hand side of Equation (5) can be evaluated as\n$\n\\frac{e^{z_{\\max}}}{\\sum_{j=1}^{n} e^{z_j}} < \\frac{e^{z_{\\max}}}{(n - 1)e^{z_{\\min}} + e^{z_{\\max}}} = \\frac{1}{\\frac{n-1}{e^{z_{\\max} - z_{\\min}}} + 1}\n$\nFrom Equation (6), it follows that the maximum element of the output vector produced by Softmax approaches zero as the input vector size n increases.\nOn the other hand, when z is processed by SSMax, zmax is transformed as\n$\nz_{\\max} = \\frac{n^{sz_{\\max}}}{\\sum_{j=1}^{n} n^{sz_j}}\n$\nAssuming s > 0 for simplicity\u00b9, the right-hand side of Equation (7) can be evaluated as\n$\n\\frac{n^{sz_{\\max}}}{\\sum_{j=1}^{n} n^{sz_j}} < \\frac{n^{sz_{\\max}}}{(n - 1)n^{z_{\\min}} + n^{z_{\\max}}} = \\frac{1}{\\frac{n-1}{n^{s(z_{\\max}-z_{\\min})}} + 1}\n$\nFor s < 0, a similar argument applies by substituting s \u2192 -s and z \u2192 -z."}, {"title": "2.3. Seamless Implementation of SSMax", "content": "The implementation of SSMax is straightforward and requires minimal modifications to existing Transformer architectures. For simplicity, we describe the standard dense attention mechanism without sparse attention variants.\nIn standard Transformers, the attention scores for the n-th token an \u2208 Rn are typically computed using the query vector of the n-th token qn \u2208 Rd and the key tensor K1:n \u2208 Rnxd, which contains the key vectors of the first n tokens, as\n$\n a_{n}=\\operatorname{Softmax}\\left(\\frac{q_{n} K_{1: n}^{\\top}}{\\sqrt{d}}\\right)\n$\nWhen replacing Softmax with SSMax, the attention computation can be reformulated based on Equation (2) as\n$\na_{n} = SSMax \\left(\\frac{q_{n} K_{1: n}^{\\top}}{\\sqrt{d}}\\right)=  \\operatorname{Softmax}\\left(\\frac{(s \\log n)q_{n} K_{1: n}^{\\top}}{\\sqrt{d}}\\right)\n$\nwhere s \u2208 R is a learnable scaling parameter.\nAs shown in Equation (11), replacing Softmax with SS-Max simply requires multiplying the query vector qn by s log n. This simplicity ensures that SSMax can be seamlessly integrated into existing architectures with minimal modifications."}, {"title": "3. Evaluations", "content": "To evaluate the impact of replacing Softmax with SSMax in the attention layers, we conducted a series of experiments focusing on long-context generalization and attention mechanisms. First, we compared the learning curves during pretraining to assess whether SSMax improves training efficiency (Section 3.1). Next, we analyzed the model's ability to generalize to longer sequences by measuring per-position test loss on sequences that were approximately 20 times longer than during training (Section 3.2). We then evaluated key information retrieval performance using the Needle-In-A-Haystack test, assessing whether SSMax improves key information extraction even when the context size is extended up to approximately 10 times the training sequence length (Section 3.3). Finally, we analyzed attention score allocation to assess how SSMax influences focus on key information during retrieval tasks (Section 3.4).\nTransformer Architecture and Configurations The Transformer architecture used in these experiments includes enhancements similar to those in Llama 2 (Touvron et al., 2023), such as ROPE (Su et al., 2024), RMSNorm (Zhang &\nSennrich, 2019), SwiGLU (Shazeer, 2020; Ramachandran\net al., 2017), and the removal of bias in projection layers.\nThe model has 12 layers, 12 attention heads, and 162M parameters. RoPE was initialized with \u03b8 = 10,000. Details of the model configuration are provided in Appendix A. For tokenization, we employed the GPT-2 tokenizer (Radford et al., 2019), and the training sequence length was set to 1024.\nEvaluated Configurations We evaluated the following six variants, each replacing Softmax in the attention layers:\n(a) Softmax: The standard Softmax function, as defined in Equation (1).\n(b) SSMax: The proposed Scalable-Softmax (SSMax) function, as defined in Equation (2). The scaling parameter s is modeled as a learnable scalar, independently learned for each attention layer and head. Since the model consists of 12 layers and 12 heads, this modification adds only 144 additional parameters, which is negligible compared to the total model size of 162M.\n(c) SSMax without Scaling Parameter: A simplified SSMax variant with the scaling parameter s removed, equivalent to substituting s = 1 in Equation (2).\n(d) SSMax with Bias Parameter: A variant of SSMax that incorporates a learnable bias parameter b, as defined in Equation (4). Both s and b are learnable scalars, each specific to every layer and head, resulting in an additional 288 parameters in total.\n(e) Softmax Replaced by SSMax After Pretraining: After pretraining, Softmax is replaced with SSMax. The scaling parameter s is initialized to the reciprocal of the average log n value during training, which is approximately 0.168, as given by 1024.\n(f) Softmax Replaced by SSMax During Pretraining: The model is first pretrained with Softmax for 175,000 iterations, then switched to SSMax and further trained for 25,000 iterations. The scaling parameter s is initialized as in (e), and a 1000-iteration learning rate warmup is applied following the switch."}, {"title": "3.1. Learning Curve Analysis", "content": "We pretrained the models on the SlimPajama dataset (Soboleva et al., 2023), a compressed version of RedPajama (Computer, 2023). The models were trained with a batch size of 2048 using the AdamW optimizer (Loshchilov & Hutter, 2019), with a learning rate of 6 \u00d7 10-4. Training spanned 200,000 iterations, corresponding to approximately 419B tokens. Details of the pretraining hyperparameters are provided in Appendix B.\nWe compared the learning curves of the standard Transformer (a) and SSMax variants (b)-(d). All SSMax variants achieved consistently lower training loss compared to the standard Transformer (a). For example, SSMax (b) resulted in an approximate 0.008 reduction in training loss. Among the variants, the model with SSMax incorporating a bias parameter (d) exhibited the lowest loss throughout training. Additionally, a comparison between (b) and (c) indicates that removing the scaling parameter has little impact on the learning curve."}, {"title": "3.2. Generalization to Longer Contexts", "content": "To assess long-sequence modeling capabilities, we increased ROPE's \u03b8 from 10,000 to 500,000. Note that no additional training was performed after modifying \u03b8. This was done to isolate the impact of adjusting RoPE's \u03b8 and evaluate how well models generalize to longer contexts without further training. We evaluated models (a)\u2013(f) by computing the per-position test loss using 100,000 sequences of length 20,000 randomly sampled from the SlimPajama test set."}, {"title": "3.3. Key Information Retrieval", "content": "The Needle-In-A-Haystack test (Kamradt, 2023) is widely used to evaluate a model's ability to retrieve key information embedded in a long context. In this study, we conduct a similar test, where the model must recall a randomly assigned seven-digit number corresponding to a randomly chosen city name, following the setup proposed in (Arize AI, 2023). Each sample contains a sentence, referred to as the needle, such as \"The special magic Tokyo number is: 8106422.\" This needle is inserted at a random location within the context, and the model is prompted to retrieve the correct number given the city name.\nTo evaluate key information retrieval, we first fine-tuned all six pretrained models (a)-(f) using Supervised Fine-Tuning. We used the SQUAD 2.0 dataset (Rajpurkar et al., 2018; 2016), selecting 86,820 examples where the context, question, and answer fields were all non-empty. Fine-tuning was conducted for 10 epochs, and the loss was computed as the sum of token-level losses for the answer span. Further details on fine-tuning hyperparameters are provided in Appendix C.\nFollowing fine-tuning, we increased ROPE's \u03b8 to 500,000, a 50-fold increase from training. No additional training was performed after modifying \u03b8, allowing us to assess generalization to longer contexts without adaptation.\nFor evaluation, we inserted the needle at five different depths within the context: 10%, 30%, 50%, 70%, and 90%. Each configuration was tested with 1000 samples for each depth-context size combination. Decoding was performed greedily, selecting the most probable non-eos token at each step without sampling."}, {"title": "3.4. Attention Allocation to Key Information", "content": "To further investigate key information retrieval, we analyzed how much attention each model allocates to key information during inference. Specifically, we define the needle score as the sum of attention scores assigned to the span starting immediately after the colon (:) and extending to the end of the needle, fully covering the seven-digit number. This metric quantifies how effectively attention layers and heads focus on key information. The attention scores are measured when generating the first token of the model's response, which is expected to correspond to the initial part of the correct seven-digit number.\nAs a preliminary analysis, we examined needle scores in a fixed setting with RoPE's \u03b8 set to 500,000, a context size of 8000, and the needle sentence \"The special magic Tokyo number is: 8106422.\" positioned at a depth of 50%. Figure 7 presents the needle scores for all layers and attention heads, sorted in descending order. This example highlights a stark contrast in attention allocation between models with SSMax and the standard Transformer. Among all models, model (b) demonstrates a particularly strong ability to focus on key information, achieving the highest needle scores. In contrast, the standard Transformer (a) fails to allocate meaningful attention to key information, with its highest needle scores remaining close to zero.\nTo obtain a more comprehensive understanding, we conducted a large-scale evaluation with 100 trials per model, keeping the context size fixed at 8000 tokens while varying the needle position and needle content.  The figure also categorizes retrieval outcomes using different markers to indicate whether the full seven-digit number was correctly retrieved, only the first digit was correct, or the retrieval failed entirely. While not perfectly correlated, the results reveal a strong association between higher top needle scores and successful key information retrieval. Among all models, model (b) with SSMax consistently produces top needle scores that remain among the highest across trials, demonstrating its strong ability to focus attention on key information, whereas the standard Transformer (a) fails to allocate meaningful attention.\nModel (c), which removes the scaling parameter, exhibits lower top needle scores than (b), despite previously demonstrating comparable performance in Sections 3.1 and 3.2. This suggests that the scaling parameter contributes to more effective attention allocation to key information. Model (d), which incorporates a bias parameter, also shows reduced top needle scores, performing better than (a) but significantly worse than (b). Similarly, models where Softmax was replaced with SSMax after or during pretraining show partial improvements over (a), but their top needle scores remain lower than those of model (b). In particular, model (f), where SSMax was introduced in the later stage of pre-training, achieves higher top needle scores than model (e), yet still falls short of (b).\nThese results indicate that SSMax significantly enhances attention allocation to key information, especially when incorporated from the beginning of pretraining. Furthermore, they highlight the importance of the scaling parameter in maintaining effective attention distribution and suggest that introducing a bias parameter weakens attention to key information."}, {"title": "4. Conclusion", "content": "In this paper, we proposed Scalable-Softmax (SSMax), a novel alternative to Softmax in Transformer attention layers. SSMax addresses the issue of attention fading and enhances length generalization, enabling models to maintain attention over long contexts. Unlike Softmax, which suppresses attention scores as the input size increases, SSMax helps models retain focus on key information.\nThrough extensive evaluations, we demonstrated the effectiveness of SSMax across multiple aspects of Transformer performance. Models with SSMax consistently achieved lower loss values during pretraining, indicating improved optimization efficiency. When applied to longer contexts, these models retained significantly lower test loss than the standard Transformer, even as the context size extended well beyond the training sequence length. In key information retrieval tasks, SSMax models exhibited superior accuracy, successfully extracting relevant information even at context sizes up to ten times longer than those seen during training. Attention score analysis confirmed that SSMax improves attention allocation to key tokens, enhancing models' ability to retrieve key information in long-context scenarios.\nWhile models trained with SSMax from the beginning of pretraining demonstrated the strongest generalization ability, we also found that models could benefit from SSMax even when introduced at later stages. Replacing Softmax with SSMax during or after pretraining led to noticeable improvements, demonstrating its adaptability as an enhancement for both newly trained and existing pretrained models.\nThese findings suggest that SSMax is a promising approach for addressing the limitations of Softmax in Transformer attention mechanisms, particularly for tasks involving extended contexts. In the future, SSMax has the potential to replace Softmax in the attention layers of all Transformer-based LLMs, including existing pretrained models. Its adaptability and ability to improve length generalization position it as a strong candidate for standard adoption in Transformer architectures."}]}