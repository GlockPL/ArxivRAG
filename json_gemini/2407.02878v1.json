{"title": "Efficient Fusion and Task Guided Embedding for End-to-end\nAutonomous Driving", "authors": ["Yipin Guo", "Yilin Lang", "Qinyuan Ren"], "abstract": "To address the challenges of sensor fusion and safety risk prediction, contemporary closed-loop autonomous driving\nneural networks leveraging imitation learning typically require a substantial volume of parameters and computational resources\nto run neural networks. Given the constrained computational capacities of onboard vehicular computers, we introduce a compact\nyet potent solution named EfficientFuser. This approach employs EfficientViT for visual information extraction and integrates\nfeature maps via cross attention. Subsequently, it utilizes a decoder-only transformer for the amalgamation of multiple features.\nFor prediction purposes, learnable vectors are embedded as tokens to probe the association between the task and sensor features\nthrough attention. Evaluated on the CARLA simulation platform, EfficientFuser demonstrates remarkable efficiency, utilizing\nmerely 37.6% of the parameters and 8.7% of the computations compared to the state-of-the-art lightweight method with only\n0.4% lower driving score, and the safety score neared that of the leading safety-enhanced method, showcasing its efficacy and\npotential for practical deployment in autonomous driving systems.", "sections": [{"title": "1 Introduction", "content": "The success of deep neural networks (DNNs) has paved\nthe way for data driven learning-based approaches to au-\ntonomous driving (AD), utilizing large-scale data and com-\nputation. This has made end-to-end autonomous driving\n(E2E AD) systems a reality. Defined as fully differentiable\nprograms that directly map raw sensor data to planned ac-\ntions or low-level control commands, E2E AD eliminates the\nneed for intermediate modules. This not only simplifies de-\nvelopment but also has the potential to improve performance.\nE2E AD primarily advances along two main directions:\nreinforcement learning [1, 2] (RL) and imitation learning\n(IL). While RL robustifies against data distribution shifts, re-\ncent advancements in driving scene generation [3], coupled\nwith the growing availability of data from electric vehicles,\nhave made IL increasingly attractive.\nEarly E2E AD with IL [4] utilized convolutional neural\nnetworks (CNNs) to extract image features and directly im-\nitate control actions. However, limited by data availabil-\nity and computational power, these early systems struggled\nto achieve good performance. Subsequently, most research\nshifted towards predicting trajectories (i.e. waypoints). LBC\n[5] utilizes policy distillation where a teacher model, trained\nwith ground-truth BEV semantic maps, predicts future way-\npoints. A student model, using only image data, learns\nfrom the teacher's predictions. TransFuser [6, 7] uses two\nCNNs to extract image and LiDAR information, followed\nby transformers to fuse these information at each downsam-\npling step. Similarly, InterFuser [8] extracts sensor features\nwith CNNs, but leverages an encoder-decoder structure to\nadditionally incorporate traffic rules and vehicle density in-\nformation, aiming for a safe driving strategy. TCP [9] com-\nbines two kinds of prediction goals, introduces control ac-\ntions and waypoints within a sequence of time into training,\nand uses a control method that mixes trajectory trackers and\nbehavior prediction results.\nDespite their promise, end-to-end approaches face a crit-\nical hurdle: the mismatch between DNNs' hefty computa-\ntional demands and AD's need for real-time, low-latency op-\neration. Current on-board hardware struggles to handle the\ncomplex architecture and massive parameters of DNNs, lim-\ning the real-world feasibility of end-to-end systems. Re-\ncently, more and more works have focused on lightweight\nneural network (NN) designs. MobileNet [10, 11] utilize\ndepthwise separable convolutions for computational effi-\nciency without losing accuracy. MCUNet [12, 13] pushes\nthe limits of miniaturization, fitting networks onto embed-\nded platforms with only 256KB of memory, significantly ex-\npanding AI deployment possibilities in resource-constrained\nenvironments. Despite advancements in lightweight NN de-\nsigns, there remains a scarcity of such networks specifically\ntailored for E2E AD systems.\nTo overcome the computational hurdles of E2E AD, we\nintroduce EfficientFuser, a powerful and hardware-friendly\nmodel, which uses EfficientViT [14] for feature extraction\nfrom multi-view and employs a decoder-only transformer\nwith task guided embedding for prediction. Cross atten-\ntion seamlessly integrates multi-view information at differ-\nent scales. Remarkably, EfficientFuser maintains strong ca-\npabilities and efficiency for requiring fewer parameters and\ncomputations, making it ideal for applications. The main\ncontributions can be summarized as follows:\n\u2022\tEfficientFuser fuses multiple camera views through\ncross attention, providing a richer understanding of the\nenvironment without straining much computation.\n\u2022\tA decoder-only transformer is used for the prediction\nprocess. With learnable vectors as embedded tokens,\nthe decoder finds the connection between tasks and sen-\nsor features through attention.\nThe predicted waypoints and control inputs are mixed"}, {"title": "2 Efficient Fuser", "content": "In this work, an architecture for end-to-end driving is pro-\nposed, as shown in Fig. 1, with three components: (1) Cross\nAttention feature fusion. (2) Decoder-only transformer for\nprediction. (3) Dynamically mix for behavioral prediction\nresults and waypoint tracker actions. The following sections\ndetail the problem setting, input and output, and each com-\nponent of the model."}, {"title": "2.1 Problem Setting", "content": "EfficientFuser's decision-making process is anchored by\nan extensive input state, symbolized as x, which amalga-\nmates multiple data sources: sensor signal i, capturing real-\ntime environmental conditions via vehicle camera; vehicle\nspeed v, indicating the current velocity; and high-level nav-\nigation information g, comprising discrete navigation com-\nmands and target coordinates from the global planner. To\nmodulate the vehicle's speed and direction, the system gen-\nerates outputs for throttle \u2208 [0,1], brake \u2208 [0,1] and\nsteer \u2208 [-1,1], controlling acceleration, deceleration, and\nsteering respectively.\nThe goal of IL is to learn a policy \ud835\udf0b that imitates the be-\nhavior of an expert \ud835\udf0b*. TCP believes that both trajectory\nand control actions contain important driving information,\nso both should be used as imitation targets. EfficientFuser\nfollow the settings in TCP. The difference is that TCP first\npredicts the trajectory and then uses it to guide the predic-\ntion of control actions, while the information between the\ntwo can be fully exchanged through attention [15] in the de-\ncoder and predicted at the same time for EfficientFuser. Im-\nitation target can be formulated as:\narg min ED~(x, [\ud835\udf0b,\ud835\udf0b\u2217]) [\ud835\udc3f(\ud835\udf0b\ud835\udc61, \ud835\udf0b\u2217) + \ud835\udc3f(\ud835\udf0b\ud835\udc50, \ud835\udf0b\u2217)], (1)\nW\nwhere D ~ (x, [\ud835\udf0b, \ud835\udf0b]) is a dataset comprised of state-\naction pairs collected from the expert. \ud835\udf0b\ud835\udc50 denotes the policy\nof control branch and \ud835\udf0b\ud835\udf0f denotes the trajectory prediction. \ud835\udc3f\nis the loss measuring how close the action from the expert\nand the action from the model is.\nRoach [16] is used as the expert, which is a relatively\nstraightforward model that has been trained using RL with\naccess to privileged information. This information encom-\npasses various aspects of the driving environment such as\nroads, lanes, routes, vehicles, pedestrians, traffic lights, and\nstop signs, all of which are rendered into a 2D Bird's Eye\nView (BEV) image. Compared with experts made by hand-\ncrafted rules, it can provide latent features for control action\nprediction as intermediate supervision, making the training\nmore stable."}, {"title": "2.2 Architecture Design", "content": "A structure that combines visual transformer (ViT) [17]\nand decoder-only transformer in large language model\n(LLM) [18, 19] is used. The transformer has since revolu-\ntionized various fields, including computer vision and even\ntime-series modeling, which can be formulated as\nout = Transformer(x) = MLP(Attention(x)). (2)\nThe core innovation of transformer lies in the attention\nmechanism, which allows the model to weigh the impor-\ntance of different parts of the input data. The calculation\nof attention can be expressed as\nout = Attention(Q, K, V) = softmax(QKT\n\u221aDk\n)V. (3)\nThese mechanisms involve mapping the input to three vec-\ntors: Query (Q), Key (K), and Value (V), often through lin-\near layers. Dk is the dimension size of K, which is scaled\nto prevent training instability. When dealing with a single\ninput, these vectors all correspond to the same input, known\nas self-attention, allows the model to find inherent connec-\ntions. When working with two inputs, the Q originates from\none source, while the K and V come from the other. This\ncross-attention mechanism enables Q from different source\nto selectively focus on relevant features."}, {"title": "2.2.1 Image Backbone", "content": "EfficientViT [14] is used as the visual backbone. ViT pio-\nneered the paradigm shift of applying the transformer archi-\ntecture, originally designed for natural language processing,\nto the domain of computer vision. It firstly divides an in-\nput image into smaller patches (e.g., 16x16 pixels). These\npatches are then flattened and treated as a sequence of to-\nkens, similar to words in a sentence. Then self-attention\nmechanism of the transformer operates on these image patch\ntokens. This allows ViT to learn complex relationships be-\ntween different regions of the image.\nWhile standard ViTs often require significant computa-\ntional resources, EfficientViT with Cascaded Group Atten-\ntion can maintain efficiency. This approach allows faster\nprocessing and reduced memory usage.\nUnlike CNN-based fusion approaches Transfuser, which\nrelies on multiple transformer layers for fusion after each\ndownsampling, ViT can work with small image patches di-\nrectly. For the Transfuser, CNNs still necessitate feature\npooling before the transformer stage to reduce the compu-\ntational burden, potentially followed by interpolation to re-\nstore resolution. This interpolation can compromise the in-\ntegrity of the original feature map.\nTo retain the original image features, a method that lever-\nages cross-attention between the two image backbones is\nemployed, which enables knowledge exchange between both\nperspectives, guiding them to focus on relevant features in\nspecific areas as Fig. 2."}, {"title": "2.2.2 Decoder Transformer", "content": "In the realm of sequence processing, transformers often\nrely on an encoder-decoder structure [15]. The encoder first\nprocesses the input sequence (image patches) and extracts\nessential information, culminating in a context vector that\ncaptures the entire input's essence. The decoder then takes\nover, utilizing the context vector and query from additional\ninformation (speed, command, occupation map, etc.) to gen-\nerate the output sequence element by element. This is how\nInterFuser works.\nInspired by the prevalent large language models like GPT\n[18], we employ a decoder-only transformer architecture for\nunified learning across all input data. Research has demon-\nstrated that this decoder-only framework offers enhanced\ngeneralization capabilities [20], alongside a more stream-\nlined structure, improved operational efficiency, and superior\nscalability.\nContrary to the typical transformer-based architecture that\ninteracts token features and then feeds them into the predic-\ntion head, our approach introduces a unique embedding vec-\ntor for predictions. This vector is initialized using random\nvalues drawn from a Gaussian distribution, with feature rep-\nresentations for the prediction task being learned throughout\ntraining. This technique not only enhances the scalability\nof the decoder component but also leverages the attention\nmechanism to identify advantageous relationships between\ntokens at an early stage. The schematic diagram is shown in\nFig. 3."}, {"title": "2.2.3 Dynamic control", "content": "Contrary to TCP, which prioritizes the control volume pre-\ndicted by the model and overlooks the control volume of the\nwaypoint tracker, it's argued that the preferences of the un-\nderlying controller should be dynamically adapted based on\nthe driving scenario.\nTo accomplish this objective, a loss estimator is developed\nthat utilizes the hidden features from the GRU within the\nwaypoint head and the intermediate features from the Con-\ntrol head as its inputs, thereby modeling the training losses of\nboth. The outcome of this process is leveraged as a measure\nof prediction confidence, which in turn is used to adjust the\npreference for the final control sequence. More specifically,\nthis can be articulated as follows:\n\ud835\udc3f\ud835\udc50 = \ud835\udc3f\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f1(\ud835\udc4b\ud835\udc50) \u2192 \ud835\udc3f\u0302\ud835\udc50,\n\ud835\udc3f\ud835\udc64 = \ud835\udc3f\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f2(\ud835\udc4b\ud835\udc64) \u2192 \ud835\udc3f\u0302\ud835\udc64,\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc53\ud835\udc52\ud835\udc5f = 1-exp(\ud835\udc58\ud835\udc50\ud835\udc3f\u0302\ud835\udc50)\nexp(\ud835\udc58\ud835\udc50\ud835\udc3f\u0302\ud835\udc50) + exp(\ud835\udc58\ud835\udc64\ud835\udc3f\u0302\ud835\udc64)\n(4)\nwhere \ud835\udc3f\u0302\ud835\udc50 and \ud835\udc3f\u0302\ud835\udc64 are the predicted waypoint loss and con-\ntrol loss, which need to be as close as possible to the real loss\n\ud835\udc3f\ud835\udc64 and \ud835\udc3f\ud835\udc50. kc, kw are coefficients used to make up for the\ndifference in the numerical range of the two losses, which is\ndetermined based on experience."}, {"title": "3 Experiments", "content": "3.1 Setup\nEvaluation metrics. four metrics are used to evalu-\nate the effectiveness of the methods: Driving Score (DS),\nRoute Completion (RC), Number of parameters (Param),\nand Floating Point Operations (Flops). Among them, DS\nand RC are used to represent the driving effect, and Param"}, {"title": "3.2 Results", "content": "Table 1 presents the comparative analysis of Efficient-\nFuser alongside other notable studies within the public Carla\nLeaderboard framework. EfficientFuser's performance is de-\nlineated in two distinct versions. The initial variant incorpo-\nrates inputs from both the front view and the focus view (i.e.,\nan enhanced frontal perspective), with an image resolution\nof 256x256. The 'Wider view' iteration maintains identical\ncamera orientation but expands the image width to 768 pix-\nels, thereby capturing a broader spectrum of information.\nEfficientFuser significantly reduces the size of closed-\nloop autonomous driving models derived from imitation\nlearning to an unprecedented level. In comparison to the\nstate-of-the-art lightweight approach, TCP, EfficientFuser's\nmodel size is merely 37.6% as large, and its computational\ndemand is just 8.5% of TCP's. Despite its considerably\nsmaller neural network (NN) size, EfficientFuser only expe-\nriences a marginal decrease of 0.4% in Driving Score (DS) in\nthe Town05 Short scenario. When compared with CIL under\nsimilar parameters and computational loads, EfficientFuser\nsubstantially outperforms it, showing a remarkable 73-point\nadvantage in DS.\nAs a cutting-edge solution for accessible closed-loop au-\ntonomous driving, InterFuser achieves a similar Route Com-\npletion (RC) to EfficientFuser but improves DS by 6.3\npoints. This improvement, however, comes at the expense of\nthe number of parameters 8.4\u00d7 and computational require-\nments 31.6\u00d7. Based on detection outcomes, these models\noften resort to a cautious approach, suggesting that the ve-\nhicle would proceed at a slow pace when it has been sta-\ntionary for an extended period without any obstacles in its\npath. With its enhanced safety driving strategy, InterFuser\nwould even recognize the red light at the next unreachable\nintersection far away and come to a halt a behavior not\naligned with typical human driving patterns, which Efficient-\nFuser does not exhibit.\nTo highlight the security efficacy of EfficientFuser, the\npenalties incurred for infractions by various methods are re-\nported on Town05 Short. EfficientFuser significantly sur-\npasses the security benchmarks set by both TCP and Trans-\nfuser, and it only marginally falls short of Interfuser, which\nprioritizes security at its core."}, {"title": "3.3 Ablation Study", "content": "To thoroughly investigate the system architecture and\nevaluate the effectiveness of our proposed method, we have\ncarried out a series of ablation studies. Initially, the impact\nof utilizing image backbones of varying sizes and the per-\nformance enhancement brought about by the cross-attention\nfuser were assessed. Subsequently, we explored the effects\nof altering the decoder layer depths and implementing learn-\nable vector embeddings. Lastly, we compared the dynamic\ncontrol adjustment with the static allocation approach em-\nployed in TCP.\nThe outcomes of these experiments were all gathered on\nTown05 Short, providing a comprehensive analysis of each\ncomponent's contribution to the overall performance."}, {"title": "3.3.1 Image Backbone", "content": "Fusion with cross attention is an efficient method, result-\ning in only 5.7% additional parameters and 7.2% compu-\ntation in EfficientFuser.Despite its minimal computational\nrequirement, Cross Attention plays a pivotal role in Effi-\ncientFuser. The absence of this multi-level image informa-\ntion interaction significantly impacts the system's efficacy,\nleading to a notable reduction of 14.9 points in the driving\nscore. This underlines the critical importance of Cross At-\ntention in facilitating effective feature fusion and enhancing\nthe model's ability to make informed decisions.\nIntuitively, one might assume that a larger image back-\nbone would yield superior results, given its enhanced ca-\npabilities for feature extraction. Nevertheless, as demon-\nstrated in Tab.2, an increase in backbone size actually de-\ntracts from the model's performance. This counterintuitive\noutcome may be attributed to the complexity of the train-\ning tasks, which hinders the full training potential of larger\nbackbones."}, {"title": "3.3.2 Decoder Layer", "content": "In evaluating our decoder layer, we established two sets\nof comparative analyses. The initial set investigates the im-\npact of varying the depth of the decoder layer. Following\nthis, we examine our proposed method of employing learn-\nable vectors as prediction tokens. For comparative purposes,\nwe conducted an additional experiment wherein, rather than\nutilizing the learnable vector and integrating it into the token\ndimension, we averaged the sensor features and sent them\ninto the prediction head to observe the effect on predictions.\nThe outcomes of these investigations are presented in Tab. 3."}, {"title": "3.3.3 Dynamic Control", "content": "EfficientFuser adopts the hybrid control utilized by TCP\nand incorporates a dynamic allocation strategy. To validate\nthe efficacy of the dynamic adjustment method, the exper-\niments about the TCP settings and dynamic setting are in-\ntroduced. The outcomes in Tab. 4 affirm the effectiveness\nof the dynamic allocation component, enhancing the DS and\nRC."}, {"title": "4 Conclusion", "content": "EfficientFuser markedly diminishes both the size and\ncomputational demands of neural networks by incorporat-"}]}