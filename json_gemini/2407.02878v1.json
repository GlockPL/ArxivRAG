{"title": "Efficient Fusion and Task Guided Embedding for End-to-end Autonomous Driving", "authors": ["Yipin Guo", "Yilin Lang", "Qinyuan Ren"], "abstract": "To address the challenges of sensor fusion and safety risk prediction, contemporary closed-loop autonomous driving neural networks leveraging imitation learning typically require a substantial volume of parameters and computational resources to run neural networks. Given the constrained computational capacities of onboard vehicular computers, we introduce a compact yet potent solution named EfficientFuser. This approach employs EfficientViT for visual information extraction and integrates feature maps via cross attention. Subsequently, it utilizes a decoder-only transformer for the amalgamation of multiple features. For prediction purposes, learnable vectors are embedded as tokens to probe the association between the task and sensor features through attention. Evaluated on the CARLA simulation platform, EfficientFuser demonstrates remarkable efficiency, utilizing merely 37.6% of the parameters and 8.7% of the computations compared to the state-of-the-art lightweight method with only 0.4% lower driving score, and the safety score neared that of the leading safety-enhanced method, showcasing its efficacy and potential for practical deployment in autonomous driving systems.", "sections": [{"title": "1 Introduction", "content": "The success of deep neural networks (DNNs) has paved the way for data driven learning-based approaches to au-tonomous driving (AD), utilizing large-scale data and com-putation. This has made end-to-end autonomous driving (E2E AD) systems a reality. Defined as fully differentiable programs that directly map raw sensor data to planned ac-tions or low-level control commands, E2E AD eliminates the need for intermediate modules. This not only simplifies de-velopment but also has the potential to improve performance. E2E AD primarily advances along two main directions: reinforcement learning [1, 2] (RL) and imitation learning (IL). While RL robustifies against data distribution shifts, re-cent advancements in driving scene generation [3], coupled with the growing availability of data from electric vehicles, have made IL increasingly attractive.\nEarly E2E AD with IL [4] utilized convolutional neural networks (CNNs) to extract image features and directly im-itate control actions. However, limited by data availabil-ity and computational power, these early systems struggled to achieve good performance. Subsequently, most research shifted towards predicting trajectories (i.e. waypoints). LBC [5] utilizes policy distillation where a teacher model, trained with ground-truth BEV semantic maps, predicts future way-points. A student model, using only image data, learns from the teacher's predictions. TransFuser [6, 7] uses two CNNs to extract image and LiDAR information, followed by transformers to fuse these information at each downsam-pling step. Similarly, InterFuser [8] extracts sensor features with CNNs, but leverages an encoder-decoder structure to additionally incorporate traffic rules and vehicle density in-formation, aiming for a safe driving strategy. TCP [9] com-bines two kinds of prediction goals, introduces control ac-tions and waypoints within a sequence of time into training, and uses a control method that mixes trajectory trackers and behavior prediction results.\nDespite their promise, end-to-end approaches face a crit-ical hurdle: the mismatch between DNNs' hefty computa-tional demands and AD's need for real-time, low-latency op-eration. Current on-board hardware struggles to handle the complex architecture and massive parameters of DNNs, lim-iting the real-world feasibility of end-to-end systems. Re-cently, more and more works have focused on lightweight neural network (NN) designs. MobileNet [10, 11] utilize depthwise separable convolutions for computational effi-ciency without losing accuracy. MCUNet [12, 13] pushes the limits of miniaturization, fitting networks onto embed-ded platforms with only 256KB of memory, significantly ex-panding AI deployment possibilities in resource-constrained environments. Despite advancements in lightweight NN de-signs, there remains a scarcity of such networks specifically tailored for E2E AD systems.\nTo overcome the computational hurdles of E2E AD, we introduce EfficientFuser, a powerful and hardware-friendly model, which uses EfficientViT [14] for feature extraction from multi-view and employs a decoder-only transformer with task guided embedding for prediction. Cross atten-tion seamlessly integrates multi-view information at differ-ent scales. Remarkably, EfficientFuser maintains strong ca-pabilities and efficiency for requiring fewer parameters and computations, making it ideal for applications. The main contributions can be summarized as follows:\n\u2022 EfficientFuser fuses multiple camera views through cross attention, providing a richer understanding of the environment without straining much computation.\n\u2022 A decoder-only transformer is used for the prediction process. With learnable vectors as embedded tokens, the decoder finds the connection between tasks and sen-sor features through attention.\n\u2022 The predicted waypoints and control inputs are mixed dynamically, offering flexible adaptation to diverse driving scenarios and potentially safe behaviors."}, {"title": "2 Efficient Fuser", "content": "In this work, an architecture for end-to-end driving is pro-posed, as shown in Fig. 1, with three components: (1) Cross Attention feature fusion. (2) Decoder-only transformer for prediction. (3) Dynamically mix for behavioral prediction results and waypoint tracker actions. The following sections detail the problem setting, input and output, and each com-ponent of the model.\n2.1 Problem Setting\nEfficientFuser's decision-making process is anchored by an extensive input state, symbolized as x, which amalga-mates multiple data sources: sensor signal i, capturing real-time environmental conditions via vehicle camera; vehicle speed v, indicating the current velocity; and high-level nav-igation information g, comprising discrete navigation com-mands and target coordinates from the global planner. To modulate the vehicle's speed and direction, the system gen-erates outputs for throttle \u2208 [0,1], brake \u2208 [0,1] and steer \u2208 [-1,1], controlling acceleration, deceleration, and steering respectively.\nThe goal of IL is to learn a policy \ud835\udf0b that imitates the be-havior of an expert \ud835\udf0b*. TCP believes that both trajectory and control actions contain important driving information, so both should be used as imitation targets. EfficientFuser follow the settings in TCP. The difference is that TCP first predicts the trajectory and then uses it to guide the predic-tion of control actions, while the information between the two can be fully exchanged through attention [15] in the de-coder and predicted at the same time for EfficientFuser. Im-itation target can be formulated as:\narg min ED~(x, [\ud835\udf0b,\ud835\udf0b\u2217])[L(\ud835\udf0b\ud835\udc61,\ud835\udf0b\u2217\ud835\udc61)+L(\ud835\udf0b\ud835\udc50,\ud835\udf0b\u2217\ud835\udc50)], (1)\nW\nwhere D ~ (x, [\ud835\udf0b, \ud835\udf0b\u2217]) is a dataset comprised of state-action pairs collected from the expert. \ud835\udf0b\ud835\udc50 denotes the policy of control branch and \ud835\udf0b\u03c4 denotes the trajectory prediction. L is the loss measuring how close the action from the expert and the action from the model is.\nRoach [16] is used as the expert, which is a relatively straightforward model that has been trained using RL with access to privileged information. This information encom-passes various aspects of the driving environment such as roads, lanes, routes, vehicles, pedestrians, traffic lights, and stop signs, all of which are rendered into a 2D Bird's Eye View (BEV) image. Compared with experts made by hand-crafted rules, it can provide latent features for control action prediction as intermediate supervision, making the training more stable.\n2.2 Architecture Design\nA structure that combines visual transformer (ViT) [17] and decoder-only transformer in large language model (LLM) [18, 19] is used. The transformer has since revolu-tionized various fields, including computer vision and even time-series modeling, which can be formulated as\nout = Transformer(x) = MLP(Attention(x)). (2)\nThe core innovation of transformer lies in the attention mechanism, which allows the model to weigh the impor-tance of different parts of the input data. The calculation of attention can be expressed as\nout = Attention(Q, K, V) = softmax(QKT/\u221aDk)V. (3)\nThese mechanisms involve mapping the input to three vec-tors: Query (Q), Key (K), and Value (V), often through lin-ear layers. Dk is the dimension size of K, which is scaled to prevent training instability. When dealing with a single input, these vectors all correspond to the same input, known as self-attention, allows the model to find inherent connec-tions. When working with two inputs, the Q originates from one source, while the K and V come from the other. This cross-attention mechanism enables Q from different source to selectively focus on relevant features."}, {"title": "2.2.1 Image Backbone", "content": "EfficientViT [14] is used as the visual backbone. ViT pio-neered the paradigm shift of applying the transformer archi-tecture, originally designed for natural language processing, to the domain of computer vision. It firstly divides an in-put image into smaller patches (e.g., 16x16 pixels). These patches are then flattened and treated as a sequence of to-kens, similar to words in a sentence. Then self-attention mechanism of the transformer operates on these image patch tokens. This allows ViT to learn complex relationships be-tween different regions of the image.\nWhile standard ViTs often require significant computa-tional resources, EfficientViT with Cascaded Group Atten-tion can maintain efficiency. This approach allows faster processing and reduced memory usage.\nUnlike CNN-based fusion approaches Transfuser, which relies on multiple transformer layers for fusion after each downsampling, ViT can work with small image patches di-rectly. For the Transfuser, CNNs still necessitate feature pooling before the transformer stage to reduce the compu-tational burden, potentially followed by interpolation to re-store resolution. This interpolation can compromise the in-tegrity of the original feature map.\nTo retain the original image features, a method that lever-ages cross-attention between the two image backbones is employed, which enables knowledge exchange between both perspectives, guiding them to focus on relevant features in specific areas as Fig. 2."}, {"title": "2.2.2 Decoder Transformer", "content": "In the realm of sequence processing, transformers often rely on an encoder-decoder structure [15]. The encoder first processes the input sequence (image patches) and extracts essential information, culminating in a context vector that captures the entire input's essence. The decoder then takes over, utilizing the context vector and query from additional information (speed, command, occupation map, etc.) to gen-erate the output sequence element by element. This is how InterFuser works.\nInspired by the prevalent large language models like GPT [18], we employ a decoder-only transformer architecture for unified learning across all input data. Research has demon-strated that this decoder-only framework offers enhanced generalization capabilities [20], alongside a more stream-lined structure, improved operational efficiency, and superior scalability.\nContrary to the typical transformer-based architecture that interacts token features and then feeds them into the predic-tion head, our approach introduces a unique embedding vec-tor for predictions. This vector is initialized using random values drawn from a Gaussian distribution, with feature rep-resentations for the prediction task being learned throughout training. This technique not only enhances the scalability of the decoder component but also leverages the attention mechanism to identify advantageous relationships between tokens at an early stage. The schematic diagram is shown in Fig. 3."}, {"title": "2.2.3 Dynamic control", "content": "Contrary to TCP, which prioritizes the control volume pre-dicted by the model and overlooks the control volume of the waypoint tracker, it's argued that the preferences of the un-derlying controller should be dynamically adapted based on the driving scenario.\nTo accomplish this objective, a loss estimator is developed that utilizes the hidden features from the GRU within the waypoint head and the intermediate features from the Con-trol head as its inputs, thereby modeling the training losses of both. The outcome of this process is leveraged as a measure of prediction confidence, which in turn is used to adjust the preference for the final control sequence. More specifically, this can be articulated as follows:\nLc = Linear1(Xc) \u2192 Lc,\nLw = Linear2(Xw) \u2192 Lw,\nPrefer = 1 \u2212 exp(kcLc)/exp(kcLc)+exp(kwLw). (4)\nwhere L\ud835\udc50 and L\ud835\udc64 are the predicted waypoint loss and con-trol loss, which need to be as close as possible to the real loss Lw and Lc. kc, kw are coefficients used to make up for the difference in the numerical range of the two losses, which is determined based on experience."}, {"title": "3 Experiments", "content": "3.1 Setup\nEvaluation metrics. four metrics are used to evalu-ate the effectiveness of the methods: Driving Score (DS), Route Completion (RC), Number of parameters (Param), and Floating Point Operations (Flops). Among them, DS and RC are used to represent the driving effect, and Param and Flops are used to represent the efficiency of the neural network.\nDriving Score (DS) is the main metric of the Carla leaderboard[21], serving as the product between the route completion and the infractions penalty. RC is the percentage of the route distance completed by an agent. Param refers to the total count of parameters that should be saved, which rep-resents the size of the NN and usually determines the amount of storage space required to run the NN. Flops is a measure of the computational cost required to perform a forward pass (inference) through the network.\nDataset. The CARLA[22] simulator is used for training and testing, specifically CARLA 0.9.10. 7 towns are used for training and Town05 for evaluation. Town05 is selected for evaluation due to the large diversity in drivable regions com-pared to other CARLA towns, e.g. multi-lane and single-lane roads, highways and exits, bridges and underpasses. To further enhance the assessment of driving safety, sce-narios involving the sudden appearance of pedestrians and erratic vehicle behavior have been integrated into the simu-lation environment. Two evaluation settings are considered: (1) Town05 Short: 32 short routes of 100-500m comprising 3 intersections each, (2) Town05 Long: 10 long routes of 1000-2000m comprising 10 intersections each. The weather condition is ClearNoon.\nTraining. EfficientViT-m1 and m0 are used as visual backbone and load ImageNet[23] pre-trained weights. Other parts are initialized with Gaussian random numbers. Ef-ficientFuser is trained for 60 epochs with a learning rate of 0.0005, and then trained for 60 epochs with a learning rate of 0.0001. The batch size is 256. Adam is used with weightDecay=1e-7. The learning rate is reduced to half ev-ery 30 epochs. Four parts of loss are set, namely speed loss Ls, feature loss Lf, waypoint loss Lw, and control loss Le. Lf and Ls are the intermediate supervision to guide training, which is similar to TCP."}, {"title": "3.2 Results", "content": "Table 1 presents the comparative analysis of Efficient-Fuser alongside other notable studies within the public Carla Leaderboard framework. EfficientFuser's performance is de-lineated in two distinct versions. The initial variant incorpo-rates inputs from both the front view and the focus view (i.e., an enhanced frontal perspective), with an image resolution of 256x256. The 'Wider view' iteration maintains identical camera orientation but expands the image width to 768 pix-els, thereby capturing a broader spectrum of information.\nEfficientFuser significantly reduces the size of closed-loop autonomous driving models derived from imitation learning to an unprecedented level. In comparison to the state-of-the-art lightweight approach, TCP, EfficientFuser's model size is merely 37.6% as large, and its computational demand is just 8.5% of TCP's. Despite its considerably smaller neural network (NN) size, EfficientFuser only expe-riences a marginal decrease of 0.4% in Driving Score (DS) in the Town05 Short scenario. When compared with CIL under similar parameters and computational loads, EfficientFuser substantially outperforms it, showing a remarkable 73-point advantage in DS.\nAs a cutting-edge solution for accessible closed-loop au-tonomous driving, InterFuser achieves a similar Route Com-pletion (RC) to EfficientFuser but improves DS by 6.3 points. This improvement, however, comes at the expense of the number of parameters 8.4\u00d7 and computational require-ments 31.6\u00d7. Based on detection outcomes, these models often resort to a cautious approach, suggesting that the ve-hicle would proceed at a slow pace when it has been sta-tionary for an extended period without any obstacles in its path. With its enhanced safety driving strategy, InterFuser would even recognize the red light at the next unreachable intersection far away and come to a halt a behavior not aligned with typical human driving patterns, which Efficient-Fuser does not exhibit.\nTo highlight the security efficacy of EfficientFuser, the penalties incurred for infractions by various methods are re-ported on Town05 Short. EfficientFuser significantly sur-passes the security benchmarks set by both TCP and Trans-fuser, and it only marginally falls short of Interfuser, which prioritizes security at its core."}, {"title": "3.3 Ablation Study", "content": "To thoroughly investigate the system architecture and evaluate the effectiveness of our proposed method, we have carried out a series of ablation studies. Initially, the impact of utilizing image backbones of varying sizes and the per-formance enhancement brought about by the cross-attention fuser were assessed. Subsequently, we explored the effects of altering the decoder layer depths and implementing learn-able vector embeddings. Lastly, we compared the dynamic control adjustment with the static allocation approach em-ployed in TCP.\nThe outcomes of these experiments were all gathered on Town05 Short, providing a comprehensive analysis of each component's contribution to the overall performance.\n3.3.1 Image Backbone\nFusion with cross attention is an efficient method, result-ing in only 5.7% additional parameters and 7.2% compu-tation in EfficientFuser.Despite its minimal computational requirement, Cross Attention plays a pivotal role in Effi-cientFuser. The absence of this multi-level image informa-tion interaction significantly impacts the system's efficacy, leading to a notable reduction of 14.9 points in the driving score. This underlines the critical importance of Cross At-tention in facilitating effective feature fusion and enhancing the model's ability to make informed decisions.\nIntuitively, one might assume that a larger image back-bone would yield superior results, given its enhanced ca-pabilities for feature extraction. Nevertheless, as demon-strated in Tab.2, an increase in backbone size actually de-tracts from the model's performance. This counterintuitive outcome may be attributed to the complexity of the train-ing tasks, which hinders the full training potential of larger backbones."}, {"title": "3.3.2 Decoder Layer", "content": "In evaluating our decoder layer, we established two sets of comparative analyses. The initial set investigates the im-pact of varying the depth of the decoder layer. Following this, we examine our proposed method of employing learn-able vectors as prediction tokens. For comparative purposes, we conducted an additional experiment wherein, rather than utilizing the learnable vector and integrating it into the token dimension, we averaged the sensor features and sent them into the prediction head to observe the effect on predictions. The outcomes of these investigations are presented in Tab. 3.\nAlthough it introduces a minor computational overhead, the learnable vector approach utilized by EfficientFuser sig-nificantly enhances the driving performance. Concerning the depth of the decoder layer, it is evident that increasing the depth does not necessarily improve performance. While the RC of the vehicle may increase, the DS substantially de-creases, indicating that the vehicle starts to overlook safety-related information. This phenomenon can also be observed in the visual attention maps; beginning from the seventh layer, the focus shifts away from the prediction token to-wards reinforcing certain measurement token information."}, {"title": "3.3.3 Dynamic Control", "content": "EfficientFuser adopts the hybrid control utilized by TCP and incorporates a dynamic allocation strategy. To validate the efficacy of the dynamic adjustment method, the exper-iments about the TCP settings and dynamic setting are in-troduced. The outcomes in Tab. 4 affirm the effectiveness of the dynamic allocation component, enhancing the DS and RC."}, {"title": "4 Conclusion", "content": "EfficientFuser markedly diminishes both the size and computational demands of neural networks by incorporat-ing a compact and efficient vision transformer as its visual backbone, coupled with the use of cross-attention mech-anisms for the integration of information. The adoption of a decoder-only transformer architecture alongside learn-able prediction vectors ensures that the system maintains commendable performance despite its reduced scale. Fur-thermore, alterations to the fundamental controller not only boost the driving efficiency but also significantly augment the safety features of the system. As the smallest closed-loop autonomous driving NN available to date, Efficient-Fuser stands out for its exceptional performance. However, similar to TCP, EfficientFuser not only learns from the re-sults but also from intermediate features, which makes it suf-fer from some performance degradation when sim-to-real."}]}