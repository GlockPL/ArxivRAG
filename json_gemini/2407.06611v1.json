{"title": "CEIA: CLIP-Based Event-Image Alignment for Open-World Event-Based Understanding", "authors": ["Wenhao Xu", "Wenming Weng", "Yueyi Zhang", "Zhiwei Xiong"], "abstract": "We present CEIA, an effective framework for open-world event-based understanding. Currently training a large event-text model still poses a huge challenge due to the shortage of paired event-text data. In response to this challenge, CEIA learns to align event and image data as an alternative instead of directly aligning event and text data. Specifically, we leverage the rich event-image datasets to learn an event embedding space aligned with the image space of CLIP through contrastive learning. In this way, event and text data are naturally aligned via using image data as a bridge. Particularly, CEIA offers two distinct advantages. First, it allows us to take full advantage of the existing event-image datasets to make up the shortage of large-scale event-text datasets. Second, leveraging more training data, it also exhibits the flexibility to boost performance, ensuring scalable capability. In highlighting the versatility of our framework, we make extensive evaluations through a diverse range of event-based multi-modal applications, such as object recognition, event-image retrieval, event-text retrieval, and domain adaptation. The outcomes demonstrate CEIA's distinct zero-shot superiority over existing methods on these applications.", "sections": [{"title": "1 Introduction", "content": "Event cameras are sensors that asynchronously measure the intensity changes at each pixel independently with microsecond temporal resolution [11]. Compared to conventional frame cameras, event cameras exhibit several exceptional advantages. They have a very high dynamic range, are immune to motion blur, and provide measurements with a microsecond-level temporal resolution. These inherent advantages have sparked considerable interest in event cameras, notably for computer vision applications such as autonomous navigation [21], robotics [13], and virtual reality (VR) [28].\nDespite the superiority of event cameras, event-based algorithms are still in their infancy, facing two major issues: the shortage of large-scale datasets and the failure of modeling new data distributions in the real world. Consequently, it is imperative to explore the zero-shot event-based algorithms. Very recently, several works [45,55] have explored how to transfer impressive zero-shot knowledge"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Transferring CLIP", "content": "In the image-based vision, pretrained Visual Language Models like CLIP [37], ALIGN [19], and Florence [50] demonstrate very impressive zero-shot transfer and generalization capabilities. Subsequently, a large number of follow-up works have been proposed to transfer the pretrained CLIP to more downstream tasks. For example, PointCLIP [51] transforms 3D point clouds into a set of depth maps for zero-shot 3D object recognition, while DenseCLIP [38] converts the original image-text matching to pixel-text matching to guide the learning of dense prediction models. X-CLIP [33] proposes a novel cross-frame attention mechanism to effectively expand CLIP to the video domain.\nRecently, some works have applied Visual Language Models to event-based vision, demonstrating promising results. Two works closely related to ours are EventCLIP [45] and E-CLIP [55]. Similar to PointCLIP, EventCLIP first transforms events into 2D frames and then uses frozen CLIP directly for zero-shot event object recognition. Following Event CLIP, E-CLIP focuses on advancing"}, {"title": "2.2 Multi-Modal Learning", "content": "With the availability of large-scale multi-modal datasets, an increasing number of multi-modal foundation models have emerged. Some representative models are driving multi-modal learning, which has marked a significant advancement in AI evolution. For example, CLIP [37] demonstrates impressive zero-shot object recognition performance, while BLIP-2 [27] exhibits capabilities approaching human-level performance in visual dialog, visual knowledge reasoning, and personalized image-to-text generation. Furthermore, Stable Diffusion [40] can generate realistic and accurate images based on given text conditions. Instruct-Pix2Pix [5] can execute diverse image edits following human-written instructions, including object replacement, style modification, setting changes, and adjustments to the artistic medium.\nThese advancements motivate us to explore event-based multi-modal tasks. In this paper, we study two main directions. One is event-text understanding, including zero-shot learning and event-text retrieval, while the other is event-image understanding, involving event-image retrieval and domain adaptation. Our future work will focus on generalizing CEIA for wider multi-modal tasks, such as event-assisted video frame interpolation [36,43,46,49] and event-assisted motion deblurring [29, 44, 53, 54]."}, {"title": "3 Method", "content": ""}, {"title": "3.1 CLIP Preliminaries", "content": "CLIP [37] is a visual-text pre-training method for image and text matching. Conceptually, CLIP consists of two encoders: an image encoder $\\Phi_{image}(.; \\theta_0)$ for extracting visual features and a text encoder $\\Phi_{text}(.; \\theta_1)$ for extracting text features. During training, CLIP utilizes 400 million training image-text pairs collected from the internet and employs a contrastive loss to learn a unified embedding space for accommodating image and text data. Specifically, given a set of image-text pairs {$x_{image}$, $x_{text}$}, CLIP is trained to search optimized parameters $\\theta_0$ and $\\theta_1$ to approach\n$$\\Phi_{image}(x_{image}; \\theta_0) = \\Phi_{text}(x_{text}; \\theta_1).$$\nNote that we use \u201c=\u201d to denote the alignment in the whole paper. Leveraging the large-scale image-text dataset, CLIP demonstrates promising zero-shot performance for many downstream tasks, ensuring the incorporation of a huge range of visual concepts."}, {"title": "3.2 The CEIA Framework", "content": "In particular, open-world event-based multi-modal understanding still remains under-explored. Our goal is to transfer the zero-shot capability of CLIP into the event-based vision. To this end, two challenges need to be addressed. First, intuitively, one way to achieve open-world event-based understanding is to train a large event-text model. Nevertheless, it is severely impeded due to the shortage of large-scale paired event-text data. Second, compared with natural images, the event data, captured by detecting the intensity changes, is essentially a kind of spatial-temporal data. Therefore, the big modality disparity makes it difficult to directly apply the image encoder of CLIP to event data.\nIn response to these two challenges, CEIA makes two key modifications. First, CEIA provides a novel perspective of focusing on learning to align event and image data instead of conducting event-text alignment, thus bypassing the shortage of large-scale paired event-text data. Second, CEIA learns an individual event encoder to alleviate the event-image modality disparity instead of directly utilizing the frozen image encoder like EventCLIP [45]. In the following, we will formally introduce the method.\nOverview. Fig. 2 shows an overview of CEIA, which is composed of a frozen image encoder $\\Phi_{image}(\u00b7; \\theta_0)$, a frozen text encoder $\\Phi_{text}(\u00b7; \\theta_1)$ and a learnable event encoder $\\Phi_{event} (\u00b7; \\theta_2)$. Given a triple set of image-event-text pairs {$x_{event}$, $x_{image}$, $x_{text}$}, CEIA learns to search a desirable parameter $\\theta_2$, which meets the following requirement:\n$$\\Phi_{event}(x_{event}; \\theta_2) = \\Phi_{image}(x_{image}; \\theta_0)$$\nNotably, CLIP has already provided the powerful image-text alignment as shown in Eq. (1). Consequently, through combining Eq. (1) and Eq. (2), we can align event and text data by regarding $\\Phi_{image}(x_{image}; \\theta_0)$ as a bridge\n$$\\Phi_{event}(x_{event}; \\theta_2) = \\Phi_{text}(x_{text}; \\theta_1).$$\nEvent Encoder. Following existing research [22, 48, 55], we selected the Vision Transformer [8], a reliable and widely-used model, as our event encoder. Leveraging the unified encoder architecture, we propose initializing the event encoder with CLIP's image encoder and then finetuning it, instead of training it from scratch. This initialization transfers spatial prior knowledge from images to events, accelerating the training process and enhancing the data efficiency of CEIA. In our experiments, this initialization proved not only beneficial but also essential. Since the training data is still too limited for cross-modal alignment, we attempted to train the event encoder from scratch, but failed.\nEvent Representations. We explored various event representations and determined that the red-blue color map, commonly used for visualizing events, is the most effective. This choice minimizes the difference between the event representation and the natural images used by CLIP, thereby simplifying cross-modal alignment.\nLORA-Based Finetuning. Intuitively, one simple way to learn an event encoder is full finetuning. However, it will destroy the original CLIP's weights, which brings the inferior zero-shot capability. Recently, LoRA [18] stands out as one of the best parameter-efficient transfer learning methods, which has been widely adopted to finetune many LLMs. Specifically, LoRA [18] shows that the pretrained models can still learn efficiently even when projected into a smaller subspace. For each pretrained weight matrix $W_o \\in R^{d\\times k}$, we can replace its update with a low-rank decomposition $\\Delta W = BA$, where $B\\in R^{d\\times r}$, $A \\in R^{r\\times k}$. Note that $W_o$ is frozen, while $A$ and $B$ are trainable. For the original forward pass $h = W_ox$, the modified forward pass is:\n$$h = W_ox + \\Delta Wx = W_ox + BAx$$\nwhere \u03b1 is a hyperparameter used to adjust the influence of the new parameters. LoRA-based finetuning provides three key advantages for CEIA: 1) It avoids catastrophic forgetting, thus preserving CLIP's strong generalization and zero-shot capabilities. 2) It prevents overfitting to the limited training data. 3) It significantly reduces training time and memory costs.\nEvent-Image Contrastive Learning. The objective of training our event encoder $\\Phi_{event}(\u00b7; \\theta_2)$ is to minimize the distance between the frames transformed from events and images in the same pair, while maximizing the distance of others. We draw the inspiration from many methods [15, 17, 37, 47, 55], which advocates the utilization of multi-modal contrastive learning. Specifically, given a set of $N$ event-image pairs {$x^{event}_{i}$, $x^{image}_{i}$}, we encode them into normalized embeddings: $f^{event}_{i} = \\Phi_{event}(x^{event}_{i}; \\theta_2)$ and $f^{image}_{i} = \\Phi_{image}(x^{image}_{i}; \\theta_0)$. Denoting M\u2081"}, {"title": "3.3 Event-Based Multi-Modal Applications", "content": "Object Recognition. Zero-shot object recognition aims to classify objects that are not included in the training dataset. As shown in Eq. (3), CEIA has achieved event-text alignment in an indirect manner. Through this event-text alignment, CEIA enables zero-shot event-based object recognition. Specifically, we first construct text prompts by inserting the class names of new objects into predefined templates (e.g., \u201cimage of a [CLASS]\u201d). Then, we extract their textual features $W_t$ by $\\Phi_{text}(\u00b7; \\theta_2)$. Since each row vector in $W_t$ encodes class knowledge, $W_t$ can naturally function as the zero-shot event classifier. Meanwhile, we utilize $\\Phi_{event}(\u00b7; \\theta_1)$ to extract the event features $f_{event}$ from the input events. Finally, the predicted probabilities for $K$ classes are computed via the classifier as follows:\n$$logits_i = f^{event}W_t; p_i = softmax(logits_i).$$\nSimilarly, Eq. (7) can be also utilized for few-shot object recognition.\nEvent-Image/Event-Text Retrieval. Event-image retrieval refers to the task of searching for the most related image in a large-scale image dataset based on a given event, or vice versa. For instance, when given an image query $x^{image}$, we first extract its image feature $f^{image}$ using $\\Phi_{image}(\u00b7; \\theta_0)$. Then, we feed forward all event examples {$x^{event}_j$}$_{j=1}^{N}$ into $\\Phi_{event}(; \\theta_2)$ to obtain {$f^{event}_j$}$_{j=1}^{N}$. Subsequently, we calculate their cosine similarity and retrieve the most related event $x^{event}_{j^*}$ with the highest similarity score:\n$$j^* = argmax(\\frac{f^{image} (f^{event}_{j})^{T}}{\\|f^{image}\\|\\| f^{event}_{j}\\|}).$$\nFor event-text retrieval, we calculate the similarity score between event features and text features and select the item with the highest similarity score."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset Preparation", "content": "N-ImageNet. N-ImageNet [20] is built by moving an event camera in front of an LCD monitor which displays images from ImageNet [7]. We leverage the event-image pairs from N-ImageNet [20] and ImageNet-1K [7] for training. Similar to ImageNet-1K, N-ImageNet contains 1.78 million event streams belonging to 1,000 classes. For training, we split N-ImageNet to construct two subset datasets: the Small dataset includes 129,393 event streams belonging to the first 100 classes and the Large dataset includes 638,878 belonging to the first 500 classes. We call the method \u201cX\u201d trained on Small and Large datasets as \u201cX-S\u201d and \u201cX-L\u201d, respectively. We use the Small and Large datasets to explore the scalable capability of our approach. We utilize the official splitting to obtain the training and test datasets.\nN-Caltech101. Similar to N-ImageNet [20], N-Caltech101 [35] is built by moving a 180\u00d7240 resolution ATIS event camera in front of a monitor displaying still images from Caltech101 [10]. It contains 8,246 samples, each with a duration of 300 ms, belonging to 101 classes. We adopt the same splitting strategy as EST [12] to obtain the training and test datasets.\nCIFAR10-DVS. Unlike N-Caltech101 [35] and N-ImageNet [20], CIFAR10-DVS [26] is created through repeating smooth movements of images on an LCD monitor in front of a DVS camera. This process converts the popular CIFAR-10 [23] dataset into 10,000 event streams across 10 different classes. We randomly allocate 4,000 samples for the test set and 6,000 samples for the training set.\nASL-DVS. ASL-DVS [3] is a relatively complex dataset containing the second largest number of labeled examples. It contains 24 classes corresponding to 24"}, {"title": "4.2 Implementation Details", "content": "We initialize our event encoder with the ViT-L/14 [8] image encoder of CLIP. The AdamW [31] optimizer and a cosine schedule warm-up learning rate schedule [30] are adopted for training. For LoRA-based finetuning [18], we set the peak learning rate to 5 \u00d7 10$^{-4}$ and the weight decay to 1 \u00d7 10$^{-2}$. For full finetuning, we set the peak learning rate to 1 \u00d7 10$^{-7}$ and the weight decay to 1 \u00d7 10$^{-1}$. The training batch size is set to 128 for all experiments. Additionally, we conduct prompt engineering and create task-relevant templates for each dataset. Specifically, we adopt \"A point cloud image representing the American Sign Language letter [CLASS]\" for ASL-DVS, \u201cImage of a [CLASS]\u201d for N-Caltech101, and \u201cA point cloud image of a [CLASS]\u201d for CIFAR10-DVS and N-ImageNet."}, {"title": "4.3 Baselines", "content": "We compare CEIA with the current state-of-the-art event-based zero-shot method, EventCLIP [45]. Additionally, we combine the pre-trained event-based video reconstruction network E2VID [39] with the frozen CLIP to construct another simple zero-shot method, which is denoted as \u201cE2VID-CLIP\u201d.\nMoreover, leveraging our building event-text datasets NIN-Prompt and NIN-BLIP2, we are able to directly train a CLIP-based event-text alignment model, called \u201cCETA\u201d. We denote \u201cCETA\u201d trained on such two datasets as \u201cCETA-Prompt\u201d and \u201cCETA-BLIP2\u201d, respectively."}, {"title": "4.4 Object Recognition", "content": "Metrics. We evaluate the performance of object recognition in terms of the common top-1 accuracy (Acc1) and top-5 accuracy (Acc5) [16,41].\nZero-Shot Results. Event-based zero-shot object recognition is a challenging task because the classes in the test set are unseen to the model during training. We report the in-distribution and out-of-distribution results in Tab. 1. The experimental results indicate that our CEIA consistently outperforms the state-of-the-art baselines across all datasets. For instance, on N-ImageNet and N-Caltech101, CEIA-L achieves improvements of 16.96% and 9.47% in top-1 accuracy compared with EventCLIP, respectively. These improvements highlight the effectiveness of our CEIA for open-world event-based understanding. Although E2VID-CLIP achieves better results than ours on N-Caltech101, its complex reconstruction network introduces significant inference latency.\nBesides, we notice that CETA-BLIP2 achieves better zero-shot results than CETA-Prompt, which can be attributed to the reason that BLIP2 is able to generate more accurate captions compared with the simple prompt template. However, the event-text alignment method CETA-BLIP2 (CETA-Prompt) exhibits inferior results compared with our event-image alignment method CEIA, highlighting the effectiveness of our event-image alignment strategy compared with the direct event-text alignment strategy.\nFew-Shot Results. We consider a general N-shot setting, i.e., N examples are randomly sampled from each class for training. We compare our CEIA with the current state-of-the-art few-shot classifier, EventCLIP [45]. In addition, we also compare with some representative methods without CLIP, namely, Sorted Time Surface [1] and DiST [20]. Notice that, we follow the original papers and use ResNet34 [16] pre-trained on ImageNet [7] as their backbone."}, {"title": "4.5 Event-Image Retrieval", "content": "Metrics. We measure the performance of event-image retrieval through computing recall at K (R@K) [25], which is defined as the fraction of queries for which the correct item is retrieved in the closest K points to the query.\nResults. Tab. 3 shows that our CEIA-L consistently outperforms EventCLIP and E2VID-CLIP under all metrics across both datasets. Specifically, on N-ImageNet, CEIA-L surpasses EventCLIP and E2VID-CLIP by 37.61% and 38.73% in terms of R@1 for event queries, respectively. The underlying reason is that the contrastive loss we used is essential for multi-modal retrieval as it directly learns cross-modal similarity and alleviates the domain disparity of event and image data. When compared to CETA-Prompt and CETA-BLIP2, CEIA also holds overwhelming advantages because it directly aligns event-image data. For instance, CEIA-S outperforms CETA-Prompt-S and CETA-BLIP2-S by 32.81%"}, {"title": "4.6 Event-Text Retrieval", "content": "Metrics. Similar to the event-image retrieval task, we reuse the recall at K (R@K) [25] to evaluate the performance of the event-text retrieval task.\nResults. As shown in Tab. 4, we report the results on our built event-text dataset N-ImageNet-BLIP2. As can be seen, our CEIA-L outperforms Event-CLIP and E2VID-CLIP by a large margin in both event query and text query. For example, CEIA-L achieves an 11.36% improvement in terms of R@1 for text query compared to EventCLIP. Although CETA-BLIP2-S achieves slightly better results than our CEIA-S for text query, it\u2019s an unfair comparison as CETA-BLIP2-S employs the captions generated by BLIP2 for training, which have the same distribution as N-ImageNet-BLIP2. In addition, we qualitatively show the results of CEIA in Fig. 3. Even when the caption describes the relationship of multiple objects (the 4th row), CEIA is able to accurately retrieve the most correlated events."}, {"title": "4.7 Domain Adaptation", "content": "Setting. We conduct domain adaptation based on object recognition, which aims to validate the effectiveness of enhancing event-based understanding by transferring the knowledge of the frame-based vision. Specifically, We first train a classifier as the task network using labeled data from the image domain, and then directly transfer it to the event domain.\nResults. As observed in Tab. 5, our CEIA-L consistently secures the top position on both N-ImageNet and N-Caltech101. Specifically, in terms of Acc1, CEIA-L outperforms E2VID-CLIP by 39.30% on N-ImageNet and by 4.05% on N-Caltech101. Moreover, compared to CETA-Prompt-S and CETA-BLIP2-S, our CEIA-S also exhibits its superiority, achieving significant increases for all metrics. These remarkable improvements demonstrate that CEIA effectively"}, {"title": "5 Ablation Study", "content": "The Effectiveness of LoRA. We compare two methods of training the event encoder: full finetuning and LoRA-based finetuning [18]. From Tab. 6, we can observe that, LoRA-based finetuning consistently outperforms full finetuning across all metrics for event-text retrieval and domain adaptation tasks. These results demonstrate that LoRA can effectively preserve CLIP's strong robustness and meanwhile avoid overfitting to the training datasets.\nLoRA Configuration. In Tab. 7, we evaluate various LoRA [18] configurations as depicted in Fig. 2. \u201cr\u201d represents the low intrinsic dimension of rank decomposition matrices. \u201c\u03b1\u201d indicates the scaling degree applied to the outputs from the trainable weights, and \u201cWeight Type\u201d denotes which weight matrices in the event encoder are finetuned with LoRA. The experimental results demonstrate that adapting only $W_a$ and $W_q$ with a very small r has already achieved competitive performance. Further increasing r or adjusting LoRA with more weights does not lead to significant improvements. Additionally, we set \u03b1 as twice to scale up the output from trainable weights, thereby further speeding up training."}, {"title": "Data Scalable Capability", "content": "As shown in Fig. 4, we can see that CEIA-L, which is trained on the larger-scale event-image pairs, achieves significantly better performance than CEIA-S across all benchmarks. This indicates that, leveraging more training data, CEIA can exhibit the flexibility to boost performance, ensuring its scalable capability. Therefore, larger-scale event-image pretraining is an exciting direction for future work.\nEvent Representations. The results in Tab. 8 show the ablation results of different event representations. Compared with the commonly-used DiST [20], Time Surface [24], Voxel [56], and Gray [45], the red-blue color map (referred to as R-B) [45] leads to the best recognition accuracy. We speculate that these worse results may be due to larger differences between these representations and natural images used by CLIP."}, {"title": "6 Conclusion", "content": "In this paper, we propose CEIA, an effective framework to adapt CLIP to event data. We provide a novel perspective of focusing on learning to align event and image data as an alternative, thus overcoming the challenge posed by the shortage of event-text datasets. We thoroughly evaluate CEIA on four applications: object recognition, event-image retrieval, event-text retrieval, and domain adaptation. The state-of-the-art results show that CEIA not only enhances open-world understanding but also opens the door to more event-based multi-modal understanding tasks. Furthermore, CEIA's significant scalability under abundant event-image pairs also opens up the possibility to introduce the first event-based Large Vision Model, which will be our future work."}]}