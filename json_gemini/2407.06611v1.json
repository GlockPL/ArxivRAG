{"title": "CEIA: CLIP-Based Event-Image Alignment for\nOpen-World Event-Based Understanding", "authors": ["Wenhao Xu", "Wenming Weng", "Yueyi Zhang", "Zhiwei Xiong"], "abstract": "We present CEIA, an effective framework for open-world\nevent-based understanding. Currently training a large event-text model\nstill poses a huge challenge due to the shortage of paired event-text data.\nIn response to this challenge, CEIA learns to align event and image data\nas an alternative instead of directly aligning event and text data. Specif-\nically, we leverage the rich event-image datasets to learn an event em-\nbedding space aligned with the image space of CLIP through contrastive\nlearning. In this way, event and text data are naturally aligned via using\nimage data as a bridge. Particularly, CEIA offers two distinct advan-\ntages. First, it allows us to take full advantage of the existing event-image\ndatasets to make up the shortage of large-scale event-text datasets. Sec-\nond, leveraging more training data, it also exhibits the flexibility to boost\nperformance, ensuring scalable capability. In highlighting the versatil-\nity of our framework, we make extensive evaluations through a diverse\nrange of event-based multi-modal applications, such as object recogni-\ntion, event-image retrieval, event-text retrieval, and domain adaptation.\nThe outcomes demonstrate CEIA's distinct zero-shot superiority over\nexisting methods on these applications.", "sections": [{"title": "1 Introduction", "content": "Event cameras are sensors that asynchronously measure the intensity changes at\neach pixel independently with microsecond temporal resolution [11]. Compared\nto conventional frame cameras, event cameras exhibit several exceptional advan-\ntages. They have a very high dynamic range, are immune to motion blur, and pro-\nvide measurements with a microsecond-level temporal resolution. These inherent\nadvantages have sparked considerable interest in event cameras, notably for com-\nputer vision applications such as autonomous navigation [21], robotics [13], and\nvirtual reality (VR) [28].\nDespite the superiority of event cameras, event-based algorithms are still in\ntheir infancy, facing two major issues: the shortage of large-scale datasets and\nthe failure of modeling new data distributions in the real world. Consequently, it\nis imperative to explore the zero-shot event-based algorithms. Very recently, sev-\neral works [45,55] have explored how to transfer impressive zero-shot knowledge"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Transferring CLIP", "content": "In the image-based vision, pretrained Visual Language Models like CLIP [37],\nALIGN [19], and Florence [50] demonstrate very impressive zero-shot transfer\nand generalization capabilities. Subsequently, a large number of follow-up works\nhave been proposed to transfer the pretrained CLIP to more downstream tasks.\nFor example, PointCLIP [51] transforms 3D point clouds into a set of depth\nmaps for zero-shot 3D object recognition, while DenseCLIP [38] converts the\noriginal image-text matching to pixel-text matching to guide the learning of\ndense prediction models. X-CLIP [33] proposes a novel cross-frame attention\nmechanism to effectively expand CLIP to the video domain.\nRecently, some works have applied Visual Language Models to event-based\nvision, demonstrating promising results. Two works closely related to ours are\nEventCLIP [45] and E-CLIP [55]. Similar to PointCLIP, EventCLIP first trans-\nforms events into 2D frames and then uses frozen CLIP directly for zero-shot\nevent object recognition. Following Event CLIP, E-CLIP focuses on advancing"}, {"title": "2.2 Multi-Modal Learning", "content": "With the availability of large-scale multi-modal datasets, an increasing number\nof multi-modal foundation models have emerged. Some representative models\nare driving multi-modal learning, which has marked a significant advancement\nin AI evolution. For example, CLIP [37] demonstrates impressive zero-shot ob-\nject recognition performance, while BLIP-2 [27] exhibits capabilities approach-\ning human-level performance in visual dialog, visual knowledge reasoning, and\npersonalized image-to-text generation. Furthermore, Stable Diffusion [40] can\ngenerate realistic and accurate images based on given text conditions. Instruct-\nPix2Pix [5] can execute diverse image edits following human-written instructions,\nincluding object replacement, style modification, setting changes, and adjust-\nments to the artistic medium.\nThese advancements motivate us to explore event-based multi-modal tasks.\nIn this paper, we study two main directions. One is event-text understanding,\nincluding zero-shot learning and event-text retrieval, while the other is event-\nimage understanding, involving event-image retrieval and domain adaptation.\nOur future work will focus on generalizing CEIA for wider multi-modal tasks,\nsuch as event-assisted video frame interpolation [36,43,46,49] and event-assisted\nmotion deblurring [29, 44, 53, 54]."}, {"title": "3 Method", "content": ""}, {"title": "3.1 CLIP Preliminaries", "content": "CLIP [37] is a visual-text pre-training method for image and text matching.\nConceptually, CLIP consists of two encoders: an image encoder $\\Phi_{image}(; \\theta_0)$\nfor extracting visual features and a text encoder $\\Phi_{text}(; \\theta_1)$ for extracting text\nfeatures. During training, CLIP utilizes 400 million training image-text pairs\ncollected from the internet and employs a contrastive loss to learn a unified\nembedding space for accommodating image and text data. Specifically, given\na set of image-text pairs {$x^{image}, x^{text}$}, CLIP is trained to search optimized\nparameters $\\theta_0$ and $\\theta_1$ to approach\n$\\Phi_{image}(x^{image}; \\theta_0) = \\Phi_{text}(x^{text}; \\theta_1)$.\n(1)\nNote that we use \"=\" to denote the alignment in the whole paper. Leveraging the\nlarge-scale image-text dataset, CLIP demonstrates promising zero-shot perfor-\nmance for many downstream tasks, ensuring the incorporation of a huge range\nof visual concepts."}, {"title": "3.2 The CEIA Framework", "content": "In particular, open-world event-based multi-modal understanding still remains\nunder-explored. Our goal is to transfer the zero-shot capability of CLIP into\nthe event-based vision. To this end, two challenges need to be addressed. First,\nintuitively, one way to achieve open-world event-based understanding is to train\na large event-text model. Nevertheless, it is severely impeded due to the shortage\nof large-scale paired event-text data. Second, compared with natural images, the\nevent data, captured by detecting the intensity changes, is essentially a kind of\nspatial-temporal data. Therefore, the big modality disparity makes it difficult to\ndirectly apply the image encoder of CLIP to event data.\nIn response to these two challenges, CEIA makes two key modifications.\nFirst, CEIA provides a novel perspective of focusing on learning to align event\nand image data instead of conducting event-text alignment, thus bypassing the\nshortage of large-scale paired event-text data. Second, CEIA learns an individual\nevent encoder to alleviate the event-image modality disparity instead of directly\nutilizing the frozen image encoder like EventCLIP [45]. In the following, we will\nformally introduce the method.\nOverview. Fig. 2 shows an overview of CEIA, which is composed of a frozen im-\nage encoder $\\Phi_{image}(\\cdot; \\theta_0)$, a frozen text encoder $\\Phi_{text}(\\cdot; \\theta_1)$ and a learnable event\nencoder $\\Phi_{event} (\\cdot; \\theta_2)$. Given a triple set of image-event-text pairs {$x^{event}, x^{image},\nx^{text}$}, CEIA learns to search a desirable parameter $\\theta_2$, which meets the following\nrequirement:\n$\\Phi_{event}(x^{event};\\theta_2) = \\Phi_{image}(x^{image}; \\theta_0)$\n(2)"}, {"title": "6", "content": "Wenhao Xu, Wenming Weng, Yueyi Zhang, and Zhiwei Xiong\nNotably, CLIP has already provided the powerful image-text alignment as shown\nin Eq. (1). Consequently, through combining Eq. (1) and Eq. (2), we can align\nevent and text data by regarding $\\Phi_{image}(x^{image}; \\theta_0)$ as a bridge\n$\\Phi_{event}(x^{event}; \\theta_2) = \\Phi_{text}(x^{text};\\theta_1)$.\n(3)\nEvent Encoder. Following existing research [22, 48, 55], we selected the Vi-\nsion Transformer [8], a reliable and widely-used model, as our event encoder.\nLeveraging the unified encoder architecture, we propose initializing the event\nencoder with CLIP's image encoder and then finetuning it, instead of training\nit from scratch. This initialization transfers spatial prior knowledge from images\nto events, accelerating the training process and enhancing the data efficiency of\nCEIA. In our experiments, this initialization proved not only beneficial but also\nessential. Since the training data is still too limited for cross-modal alignment,\nwe attempted to train the event encoder from scratch, but failed.\nEvent Representations. We explored various event representations and de-\ntermined that the red-blue color map, commonly used for visualizing events, is\nthe most effective. This choice minimizes the difference between the event repre-\nsentation and the natural images used by CLIP, thereby simplifying cross-modal\nalignment.\nLORA-Based Finetuning. Intuitively, one simple way to learn an event en-\ncoder is full finetuning. However, it will destroy the original CLIP's weights,\nwhich brings the inferior zero-shot capability. Recently, LoRA [18] stands out\nas one of the best parameter-efficient transfer learning methods, which has been\nwidely adopted to finetune many LLMs. Specifically, LoRA [18] shows that the\npretrained models can still learn efficiently even when projected into a smaller\nsubspace. For each pretrained weight matrix $W_0 \\in \\mathbb{R}^{d\\times k}$, we can replace its up-\ndate with a low-rank decomposition $\\Delta W = BA$, where $B\\in \\mathbb{R}^{d\\times r}, A \\in \\mathbb{R}^{r\\times k}$.\nNote that $W_0$ is frozen, while A and B are trainable. For the original forward\npass $h = W_0x$, the modified forward pass is:\n$h = W_0x + \\Delta Wx = W_0x + BAx$\n(4)\nwhere $\\alpha$ is a hyperparameter used to adjust the influence of the new parameters.\nLoRA-based finetuning provides three key advantages for CEIA: 1) It avoids\ncatastrophic forgetting, thus preserving CLIP's strong generalization and zero-\nshot capabilities. 2) It prevents overfitting to the limited training data. 3) It\nsignificantly reduces training time and memory costs.\nEvent-Image Contrastive Learning. The objective of training our event en-\ncoder $\\Phi_{event}(; \\theta_2)$ is to minimize the distance between the frames transformed\nfrom events and images in the same pair, while maximizing the distance of others.\nWe draw the inspiration from many methods [15, 17, 37, 47, 55], which advocates\nthe utilization of multi-modal contrastive learning. Specifically, given a set of\nevent-image pairs {$(x^{event}_i, x^{image}_i)$}, we encode them into normalized embed-\ndings: $f^{event}_i = \\Phi_{event} (x^{event}_i; \\theta_2)$ and $f^{image}_i = \\Phi_{image}(x^{image}_i; \\theta_0)$. Denoting $M_1$"}, {"title": "7", "content": "CEIA: CLIP-Based Event-Image Alignment\nand $M_2$ are two modalities, the InfoNCE [34] loss can be formulated as\n$L(M_1, M_2) = -log \\frac{exp(f_{M_1} \\cdot (f_{M_2})^T /\\tau)}{exp(f_{M_1} \\cdot (f_{M_2})^T /\\tau) + \\sum_{j\\neq i}exp(f_{M_1} \\cdot (f_{M_2})^T /\\tau)}$\n(5)\nwhere $\\tau$ is a learnable temperature parameter to control the smoothness of the\nsoftmax distribution. Following CLIP [37], we consider every example $j\\neq i$ in\nthe mini-batch as a negative. Finally, the weights of the event encoder $\\theta_2$ is\noptimized by minimizing a symmetric InfoNCE loss\n$L_{final} = L(event, image) + L(image, event)$.\n(6)\nThrough event-image contrastive learning, we can align representations of\nevent, image, and text modalities into the same embedding space. In the follow-\ning, we will elaborate on the details about how to extend CEIA to open-world\nevent-based multi-modal applications."}, {"title": "3.3 Event-Based Multi-Modal Applications", "content": "Object Recognition. Zero-shot object recognition aims to classify objects that\nare not included in the training dataset. As shown in Eq. (3), CEIA has achieved\nevent-text alignment in an indirect manner. Through this event-text alignment,\nCEIA enables zero-shot event-based object recognition. Specifically, we first con-\nstruct text prompts by inserting the class names of new objects into predefined\ntemplates (e.g., \"image of a [CLASS]\"). Then, we extract their textual features\n$W_t$ by $\\Phi_{text}(; \\theta_2)$. Since each row vector in $W_t$ encodes class knowledge, $W_t$\ncan naturally function as the zero-shot event classifier. Meanwhile, we utilize\n$\\Phi_{event}(; \\theta_1)$ to extract the event features $f^{event}$ from the input events. Finally,\nthe predicted probabilities for K classes are computed via the classifier as follows:\n$logits_i = f^{event}W_t^T; p_i = softmax(logits_i)$.\n(7)\nSimilarly, Eq. (7) can be also utilized for few-shot object recognition.\nEvent-Image/Event-Text Retrieval. Event-image retrieval refers to the task\nof searching for the most related image in a large-scale image dataset based on a\ngiven event, or vice versa. For instance, when given an image query $x^{image}$, we\nfirst extract its image feature $f^{image}$ using $\\Phi_{image}(\\cdot; \\theta_0)$. Then, we feed forward\nall event examples {$x^{event}_j$}$_{j=1}^{N}$ into $\\Phi_{event}(\\cdot;\\theta_2)$ to obtain {$f^{event}_j$}$_{j=1}^{N}$. Subse-\nquently, we calculate their cosine similarity and retrieve the most related event\n$x^{event}_{j^*}$ with the highest similarity score:\n$j^* = argmax_j(\\frac{f^{image}}{(|| f^{image} ||} \\cdot \\frac{f^{event}}{(|| f^{event}_j ||)})$\n(8)\nFor event-text retrieval, we calculate the similarity score between event features\nand text features and select the item with the highest similarity score."}, {"title": "8", "content": "Wenhao Xu, Wenming Weng, Yueyi Zhang, and Zhiwei Xiong\nDomain Adaptation. Domain adaptation [9,32,42] aims to transfer tasks from\na labeled source domain (images) to a target domain (events). It can leverage\nexisting image datasets to train models, thereby overcoming the lack of high-\nquality labeled event datasets. Specifically, As depicted in Fig. 2, we conducted\ndomain adaptation experiments on object recognition. Formally, denote $f^{image}$\nand $l$ as the image feature extracted by $\\Phi_{image}(; \\theta_0)$ and the available label,\nrespectively. We train a task network $T(\\cdot;\\theta_4)$, whose weights $\\theta_4$ are optimized\nby minimizing the commonly-used soft-max cross-entropy loss:\n$logits = T(f^{image}; \\theta_4); L_{image} = CrossEntropy(logits,l)$.\n(9)\nSubsequently, we directly apply the trained task network $T(\\cdot;\\theta_4)$ to the event\ndomain to generate predictions:\n$pred = T(f^{event}; \\theta_4)$.\n(10)\nAs shown in Eq. (2), CEIA has already aligned event and image data, which\nensures the transferability and applicability of the network $T(\\cdot; \\theta_4)$ when applied\nfor event data."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset Preparation", "content": "N-ImageNet. N-ImageNet [20] is built by moving an event camera in front of an\nLCD monitor which displays images from ImageNet [7]. We leverage the event-\nimage pairs from N-ImageNet [20] and ImageNet-1K [7] for training. Similar\nto ImageNet-1K, N-ImageNet contains 1.78 million event streams belonging to\n1,000 classes. For training, we split N-ImageNet to construct two subset datasets:\nthe Small dataset includes 129,393 event streams belonging to the first 100 classes\nand the Large dataset includes 638,878 belonging to the first 500 classes. We\ncall the method \"X\" trained on Small and Large datasets as \"X-S\" and \"X-\nL\", respectively. We use the Small and Large datasets to explore the scalable\ncapability of our approach. We utilize the official splitting to obtain the training\nand test datasets.\nN-Caltech101. Similar to N-ImageNet [20], N-Caltech101 [35] is built by mov-\ning a 180\u00d7240 resolution ATIS event camera in front of a monitor displaying\nstill images from Caltech101 [10]. It contains 8,246 samples, each with a dura-\ntion of 300 ms, belonging to 101 classes. We adopt the same splitting strategy\nas EST [12] to obtain the training and test datasets.\nCIFAR10-DVS. Unlike N-Caltech101 [35] and N-ImageNet [20], CIFAR10-\nDVS [26] is created through repeating smooth movements of images on an LCD\nmonitor in front of a DVS camera. This process converts the popular CIFAR-\n10 [23] dataset into 10,000 event streams across 10 different classes. We randomly\nallocate 4,000 samples for the test set and 6,000 samples for the training set.\nASL-DVS. ASL-DVS [3] is a relatively complex dataset containing the second\nlargest number of labeled examples. It contains 24 classes corresponding to 24"}, {"title": "9", "content": "CEIA: CLIP-Based Event-Image Alignment\nletters (A-Y, excluding J) of the American Sign Language. For each letter, 4,200\nsamples are collected by capturing real-world events. Each sample spans approx-\nimately 100 milliseconds. We randomly select 1,000 samples for the test set and\n3,200 samples for the training set.\nNIN-Prompt/NIN-BLIP2/NIN-BLIP2-retrieval. Considering the short-\nage of currently available large-scale event-text datasets, we make the first at-\ntempt to build two kinds of event-text datasets based on N-ImageNet for train-\ning, denoted as \u201cNIN-Prompt\u201d and \u201cNIN-BLIP2\u201d. Specifically, for \u201cNIN-Prompt", "CLASS]": "We then use these prompts as captions for\ncorresponding events. For \"NIN-BLIP2\u201d, we utilize BLIP2 [27] with the frozen\nLLM OPT [52] to conduct zero-shot image captioning, generating high-quality\ncaptions for the images from ImageNet. Subsequently, we pair them with events\nfrom N-ImageNet to construct the dataset. Furthermore, we create a test dataset,\nnamed \"NIN-BLIP2-retrieval"}, {"title": "4.2 Implementation Details", "content": "We initialize our event encoder with the ViT-L/14 [8] image encoder of CLIP.\nThe AdamW [31] optimizer and a cosine schedule warm-up learning rate sched-\nule [30] are adopted for training. For LoRA-based finetuning [18], we set the peak\nlearning rate to $5 \\times 10^{-4}$ and the weight decay to $1 \\times 10^{-2}$. For full finetuning,\nwe set the peak learning rate to $1 \\times 10^{-7}$ and the weight decay to $1 \\times 10^{-1}$. The\ntraining batch size is set to 128 for all experiments. Additionally, we conduct\nprompt engineering and create task-relevant templates for each dataset. Specifi-\ncally, we adopt \"A point cloud image representing the American Sign Language\nletter [CLASS]\" for ASL-DVS, \"Image of a [CLASS]\" for N-Caltech101, and \"\u0410\npoint cloud image of a [CLASS]\" for CIFAR10-DVS and N-ImageNet."}, {"title": "4.3 Baselines", "content": "We compare CEIA with the current state-of-the-art event-based zero-shot method,\nEventCLIP [45]. Additionally, we combine the pre-trained event-based video re-\nconstruction network E2VID [39] with the frozen CLIP to construct another\nsimple zero-shot method, which is denoted as \"E2VID-CLIP\".\nMoreover, leveraging our building event-text datasets NIN-Prompt and NIN-\nBLIP2, we are able to directly train a CLIP-based event-text alignment model,\ncalled \"CETA\". We denote \"CETA\" trained on such two datasets as \"CETA-\nPrompt\" and \"CETA-BLIP2\", respectively."}, {"title": "10", "content": "Wenhao Xu, Wenming Weng, Yueyi Zhang, and Zhiwei Xiong"}, {"title": "4.4 Object Recognition", "content": "Metrics. We evaluate the performance of object recognition in terms of the\ncommon top-1 accuracy (Acc1) and top-5 accuracy (Acc5) [16,41].\nZero-Shot Results. Event-based zero-shot object recognition is a challeng-\ning task because the classes in the test set are unseen to the model during\ntraining. We report the in-distribution and out-of-distribution results in Tab. 1.\nThe experimental results indicate that our CEIA consistently outperforms the\nstate-of-the-art baselines across all datasets. For instance, on N-ImageNet and\nN-Caltech101, CEIA-L achieves improvements of 16.96% and 9.47% in top-1\naccuracy compared with EventCLIP, respectively. These improvements high-\nlight the effectiveness of our CEIA for open-world event-based understanding.\nAlthough E2VID-CLIP achieves better results than ours on N-Caltech101, its\ncomplex reconstruction network introduces significant inference latency.\nBesides, we notice that CETA-BLIP2 achieves better zero-shot results than\nCETA-Prompt, which can be attributed to the reason that BLIP2 is able to\ngenerate more accurate captions compared with the simple prompt template.\nHowever, the event-text alignment method CETA-BLIP2 (CETA-Prompt) ex-\nhibits inferior results compared with our event-image alignment method CEIA,\nhighlighting the effectiveness of our event-image alignment strategy compared\nwith the direct event-text alignment strategy.\nFew-Shot Results. We consider a general N-shot setting, i.e., N examples are\nrandomly sampled from each class for training. We compare our CEIA with the\ncurrent state-of-the-art few-shot classifier, EventCLIP [45]. In addition, we also\ncompare with some representative methods without CLIP, namely, Sorted Time\nSurface [1] and DiST [20]. Notice that, we follow the original papers and use\nResNet34 [16] pre-trained on ImageNet [7] as their backbone."}, {"title": "4.5 Event-Image Retrieval", "content": "Metrics. We measure the performance of event-image retrieval through com-\nputing recall at K (R@K) [25], which is defined as the fraction of queries for\nwhich the correct item is retrieved in the closest K points to the query.\nResults. Tab. 3 shows that our CEIA-L consistently outperforms EventCLIP\nand E2VID-CLIP under all metrics across both datasets. Specifically, on N-\nImageNet, CEIA-L surpasses EventCLIP and E2VID-CLIP by 37.61% and 38.73%\nin terms of R@1 for event queries, respectively. The underlying reason is that\nthe contrastive loss we used is essential for multi-modal retrieval as it directly\nlearns cross-modal similarity and alleviates the domain disparity of event and\nimage data. When compared to CETA-Prompt and CETA-BLIP2, CEIA also\nholds overwhelming advantages because it directly aligns event-image data. For\ninstance, CEIA-S outperforms CETA-Prompt-S and CETA-BLIP2-S by 32.81%"}, {"title": "4.6 Event-Text Retrieval", "content": "Metrics. Similar to the event-image retrieval task, we reuse the recall at K\n(R@K) [25] to evaluate the performance of the event-text retrieval task.\nResults. As shown in Tab. 4, we report the results on our built event-text\ndataset N-ImageNet-BLIP2. As can be seen, our CEIA-L outperforms Event-\nCLIP and E2VID-CLIP by a large margin in both event query and text query.\nFor example, CEIA-L achieves an 11.36% improvement in terms of R@1 for text\nquery compared to EventCLIP. Although CETA-BLIP2-S achieves slightly bet-\nter results than our CEIA-S for text query, it's an unfair comparison as CETA-\nBLIP2-S employs the captions generated by BLIP2 for training, which have the\nsame distribution as N-ImageNet-BLIP2. In addition, we qualitatively show the\nresults of CEIA in Fig. 3. Even when the caption describes the relationship of\nmultiple objects (the 4th row), CEIA is able to accurately retrieve the most\ncorrelated events."}, {"title": "4.7 Domain Adaptation", "content": "Setting. We conduct domain adaptation based on object recognition, which\naims to validate the effectiveness of enhancing event-based understanding by\ntransferring the knowledge of the frame-based vision. Specifically, We first train\na classifier as the task network using labeled data from the image domain, and\nthen directly transfer it to the event domain.\nResults. As observed in Tab. 5, our CEIA-L consistently secures the top po-\nsition on both N-ImageNet and N-Caltech101. Specifically, in terms of Acc1,\nCEIA-L outperforms E2VID-CLIP by 39.30% on N-ImageNet and by 4.05%\non N-Caltech101. Moreover, compared to CETA-Prompt-S and CETA-BLIP2-\nS, our CEIA-S also exhibits its superiority, achieving significant increases for\nall metrics. These remarkable improvements demonstrate that CEIA effectively"}, {"title": "13", "content": "CEIA: CLIP-Based Event-Image Alignment"}, {"title": "5 Ablation Study", "content": "The Effectiveness of LoRA. We compare two methods of training the event\nencoder: full finetuning and LoRA-based finetuning [18]. From Tab. 6, we can\nobserve that, LoRA-based finetuning consistently outperforms full finetuning\nacross all metrics for event-text retrieval and domain adaptation tasks. These\nresults demonstrate that LoRA can effectively preserve CLIP's strong robustness\nand meanwhile avoid overfitting to the training datasets.\nLORA Configuration. In Tab. 7, we evaluate various LoRA [18] configurations\nas depicted in Fig. 2. \"r\" represents the low intrinsic dimension of rank decompo-\nsition matrices. \"a\" indicates the scaling degree applied to the outputs from the\ntrainable weights, and \"Weight Type\" denotes which weight matrices in the event\ncoder are finetuned with LoRA. The experimental results demonstrate that\nadapting only $W_q$ and $W_v$ with a very small r has already achieved competitive\nperformance. Further increasing r or adjusting LoRA with more weights does\nnot lead to significant improvements. Additionally, we set $\\alpha$ as twice r to scale\nup the output from trainable weights, thereby further speeding up training."}, {"title": "14", "content": "Wenhao Xu, Wenming Weng, Yueyi Zhang, and Zhiwei Xiong"}, {"title": "Data Scalable Capability", "content": "As shown in Fig. 4, we can see that CEIA-L, which\nis trained on the larger-scale event-image pairs, achieves significantly better per-\nformance than CEIA-S across all benchmarks. This indicates that, leveraging\nmore training data, CEIA can exhibit the flexibility to boost performance, en-\nsuring its scalable capability. Therefore, larger-scale event-image pretraining is\nan exciting direction for future work."}, {"title": "Event Representations", "content": "The results in Tab. 8 show the ablation results of\ndifferent event representations. Compared with the commonly-used DiST [20],\nTime Surface [24], Voxel [56], and Gray [45], the red-blue color map (referred\nto as R-B) [45] leads to the best recognition accuracy. We speculate that these\nworse results may be due to larger differences between these representations and\nnatural images used by CLIP."}, {"title": "6 Conclusion", "content": "In this paper, we propose CEIA, an effective framework to adapt CLIP to event\ndata. We provide a novel perspective of focusing on learning to align event and\nimage data as an alternative, thus overcoming the challenge posed by the short-\nage of event-text datasets. We thoroughly evaluate CEIA on four applications:\nobject recognition, event-image retrieval, event-text retrieval, and domain adap-\ntation. The state-of-the-art results show that CEIA not only enhances open-\nworld understanding but also opens the door to more event-based multi-modal\nunderstanding tasks. Furthermore, CEIA's significant scalability under abundant\nevent-image pairs also opens up the possibility to introduce the first event-based\nLarge Vision Model, which will be our future work."}]}