{"title": "Combining LLM decision and RL action selection to improve RL policy for adaptive interventions", "authors": ["Karine Karine", "Benjamin M. Marlin"], "abstract": "Reinforcement learning (RL) is increasingly being used in the healthcare domain, particularly for the development of personalized health adaptive interventions. Inspired by the success of Large Language Models (LLMs), we are interested in using LLMs to update the RL policy in real time, with the goal of accelerating personalization. We use the text-based user preference to influence the action selection on the fly, in order to immediately incorporate the user preference. We use the term \"user preference\u201d as a broad term to refer to a user personal preference, constraint, health status, or a statement expressing like or dislike, etc. Our novel approach is a hybrid method that combines the LLM response and the RL action selection to improve the RL policy. Given an LLM prompt that incorporates the user preference, the LLM acts as a filter in the typical RL action selection. We investigate different prompting strategies and action selection strategies. To evaluate our approach, we implement a simulation environment that generates the text-based user preferences and models the constraints that impact behavioral dynamics. We show that our approach is able to take into account the text-based user preferences, while improving the RL policy, thus improving personalization in adaptive intervention.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) is increasingly being used in the healthcare domain, particularly for the development of personalized health adaptive interventions [Coronato et al., 2020, Yu et al., 2021, G\u00f6n\u00fcl et al., 2021, Liao et al., 2020]. Inspired by the success of Large Language Models (LLMs), we are interested in using LLMs to update the RL policy in real time, with the goal of accelerating personalization. We use the text-based user preference to influence the selection of actions on the fly, in order to immediately incorporate the user preference. We use the term \"user preference\u201d as a broad term to refer to a user personal preference, constraint, health status, or a statement expressing like or dislike, etc.\nWe illustrate our motivation with an example from the behavioral domain, where researchers study the effectiveness of a mobile health app that encourages positive behavior change (i.e., exercise more or reduce smoking) by sending messages to the user, a.k.a participant [Nahum-Shani et al., 2018, Hardeman et al., 2019]. Often, there can be too many messages sent to the user, or some issues in the decision rule or policy that result in incorrectly contextualized messages sent to the user (e.g., when the user preference does not align with the policy). These messages may annoy the user, or even cause the user to disengage from the study. Thus, it is critical to take into account the user preference before it becomes too late or irreversible (e.g. the user exits the study).\nOne solution is to allow the user to specify their preferences in the form of free-text descriptions, and immediately take them into account to influence the action selection. This is especially relevant in today's generation, where people use chats and social media to communicate. For example, the user preference can be: \"I sprained my ankle\u201d, \u201cI don't want messages during the weekend\" or \u201cI don't like to receive negative messages about smoking\".\nOur goal is to immediately incorporate the user preferences, by using LLMs, in order to improve the RL policy, and thus accelerate personalization in adaptive intervention.\nHowever, implementing methods using LLMs comes with challenges: (1) how to construct effective LLM prompts to obtain the desired LLM response, (2) how to incorporate the LLM response in the RL system, in order to update the RL policy, and (3) how to evaluate the new method, since incorporating the user preference introduces additional constraints on the behavioral dynamics.\nThus, our problem statement is: \u201cHow can we use LLMs to update the RL policy to accelerate personalization in adaptive interventions?\". We answer this question by introducing: (1) our novel hybrid method \u201cLLM+TS\u201d, (2) a novel simulation environment \"StepCountJITAI for LLM\" that works with LLMs and allows for the evaluation of our new method, and (3) a solution framework for implementing a pipeline for personalized health adaptive interventions.\nOur contributions are:\n1. LLM+TS: combining LLM decision and RL action selection to improve RL policy. We introduce our novel hybrid method called \u201cLLM+TS\u201d, that combines the LLM response and the RL action selection to improve the RL policy for adaptive intervention. We use Thompson Sampling (TS) as the RL agent, because it is an effective Bayesian approach that requires fewer iterations than typical deep RL methods [Agrawal and Goyal, 2013].\nOur approach incorporates prompting strategies and action selection strategies to accelerate personalization. For our approach, we implement two loops: (1) a simulation environment that generates text-based user preferences, which we describe below, and (2) the typical RL loop where the RL agent selects a candidate action at each time step. Then, based on the LLM prompt that includes the user preference and other information, the LLM decides whether to \"not send\" or \"send\" a message. The LLM acts as a filter in the typical RL action selection.\n2. StepCount JITAI for LLM: novel simulation environment that generates user preferences and incorporates constraints to impact behavioral dynamics. We extend the base simulator for adaptive intervention, introduced in Karine and Marlin [2024], to create a new simulation environment that works with LLMs. StepCountJITAI for LLM is used to evaluate our new method. To construct it, we: (1) augment the user state with an auxiliary variable with dynamics that follow a Markov chain, (2) generate the text-based user preference based on the auxiliary variable value, (3) incorporate constraints that impact the behavioral dynamics implemented inside the simulation environment (e.g., if the user is in a \"cannot walk\" state, then the reward value drops to 0, the disengagement risk increases, and the habitual level increases). We note that our new process for generating the text-based user preference is separate from the RL loop, and is only included in the simulation environment. Our StepCountJITAI for LLM is an innovative simulation environment that has not been explored in prior work, and it offers significant potential for advancing the development of new RL algorithms for adaptive interventions using LLM. StepCountJITAI is available at: github.com/reml-lab/StepCountJITAI.\n3. Practical method for accelerating personalization in adaptive intervention. We demonstrate how to frame a physical activity adaptive intervention as an RL system using LLMs. We show that our approach improves the RL policy while incorporating the user preference. Our method offers a promising solution framework for implementing a pipeline for personalized health adaptive intervention.\nWe provide an overview of our novel method in Figure 1, and details in Section 3.\""}, {"title": "Background", "content": "StepCountJITAI: simulation environment for adaptive intervention. There is limited prior work on simulation environments for adaptive intervention in the literature. In this work, we extend the base simulator for adaptive intervention introduced in Karine and Marlin [2024]. This base simulator was specifically designed to be used for the development of new RL algorithms for adaptive intervention.\nA physical activity adaptive intervention can be framed as an RL system, where the types of messages are the possible actions. In our simulation environment, we use the following values: a = 0 (do not send a message), a = 1 (send a generic message), a = 2 (send a message tailored to context 0), and a = 3 (send a message tailored to context 1).\nThe context can be, for example, a binary state of the participant: 'stressed / not stressed' or 'at home / at work' or 'smoker / not a smoker', etc. Note that the context can be extended to include multiple values.\nThe environment states include the participant behaviors: habituation level and disengagement risk.\nFor the notation, we use an uppercase letter for the variable name, and a lowercase letter for the variable value, for example: the context variable C has value $c_t$  0 at time t.\nBelow we describe some of the simulation environment variables and parameters that are used in the behavioral dynamics: $c_t$ is the true context, $p_t$ is the probability of context 1, $l_t$ is the inferred context, $h_t$ is the habituation level, $d_t$ is the disengagement risk, $s_t$ is the step count ($s_t$ is the participant's number of walking steps), and $a_t$ is the action at time t. The base simulator also includes behavioral parameters: $\\delta_h$ and $\\epsilon_h$ are decay and increment parameters for the disengagement risk, and $\\delta_h$ and $\\epsilon_h$ are decay and increment parameters for the habituation level.\nThe goal is to increase the participant's walking step count. Thus, the walking step count is also the RL reward.\nThis base simulator implements complex behavioral dynamics. The behavioral dynamics can be summarized as follows: Sending a message causes the habituation level to increase. Not sending a message causes the habituation level to decrease. An incorrectly tailored message causes the disengagement risk to increase. A correctly tailored message causes the disengagement risk to decrease."}, {"title": "Thompson Sampling", "content": "Thompson Sampling. Thompson Sampling (TS) is a probabilistic method for decision-making under uncertainty. It can be used to address contextual multi-armed bandit problems [Russo et al., 2018, Agrawal and Goyal, 2013, Chu et al., 2011, Thompson, 1933]. Typical TS for contextual bandit settings uses a reward model of the form $N(r; \\theta_a^T v_t, \\sigma_a^2 v_t)$, where $v_t$ is the state vector at time t, $\\theta_a$ is a vector of weights, and $\\sigma_a^2$ is the reward variance for action a. Thus, $\\theta_a^T v_t$ represents the mean reward for action a.\nThe reward model weights $\\theta_a$ are random variables of the form $N(\\theta_a; \\mu_{ta}, \\Sigma_{ta})$. Actions are selected at each time t by sampling $\\theta_a$ from $N(\\theta_a; \\mu_{ta}, \\Sigma_{ta})$ and choosing the action with the largest value $v_t$. The prior distribution for $\\theta_a$ is of the form $N(\\theta_a; \\mu_{0a}, \\Sigma_{0a})$. The distribution over $\\theta_a$ for the selected action is updated at time t based on the observed reward $r_t$ and $v_t$ using Bayesian inference. We provide the update equations for the mean and covariance matrix below.\n$\\Sigma_{(t+1)a} = \\sigma_{xa} (v_t^T + \\sigma_{ta} \\Sigma_{ta}^{-1})^{-1}$\n$\\mu_{(t+1)a} = \\Sigma_{(t+1)a} ((\\sigma_a^2)^{-1} r_t v_t + \\Sigma_{ta} \\mu_{ta})$\nRelated work. Recent works use LLMs in RL, where the RL agent selects actions based on natural language inputs, and apply to games [Du et al., 2023]. Note that in our work we leverage LLMs as foundational models, and focus on online decision-making for episode-limited RL settings, thus while we combine LLMs and RL, our work differs from the recent research on RL from human feedback (RLHF) and from AI feedback (RLAIF), which typically require some form of reward modeling, and a large number of episodes to perform well. Other works have also explored using natural language inputs, but apply to recommender systems for items such as movies, or instructability of social media recommendation algorithms [Lyu et al., 2024, Feng et al., 2024, Mysore et al., 2023, Sanner et al., 2023]. However, these approaches also require a large number of iterations to work well. In contrast, we use TS which is a Bayesian approach that can perform well in a lower number of iterations than typical deep RL methods."}, {"title": "Methods", "content": "We start with an overview of our new method in Figure 1, and summarize below how to frame an adaptive intervention as an RL system using LLM. Then we provide the details.\n1. Run a typical RL loop to select a candidate action a.\n2. Extract LLM decision: given the user preference and previous data, we construct the prompt, send it to the LLM, then extract the LLM decision from the LLM response.\n3. Update RL policy based on LLM decision: we choose to either select a = 0 (no message) or send a = a (set the hybrid action a to the RL candidate action a).\nNote: if the RL agent selects action a > 0 (indicating a candidate message to be sent) and a user preference is generated, then the LLM is prompted to decide if this message should actually be sent or not. If the RL agent selects action a = 0 (indicating no message) or if there is no user preference that was generated, then there is no need to call the LLM, so the typical RL loop continues as usual.\nTo evaluate our method, we run our new simulation environment that generates text-based user preferences.\nImportantly, the RL loop and the user preference generation process are two separate loops. The text-based user preference is generated by the simulation environment, as described in Section 3.1. The user preference is inserted into the LLM prompt, when a call to the LLM is made."}, {"title": "StepCountJITAI for LLM", "content": "3.1 StepCountJITAI for LLM\nWe extend the base simulator for adaptive intervention, introduced in Karine and Marlin [2024], to create StepCountJITAI for LLM, a novel simulation environment that works with LLMs. We describe the base simulator as well as the behavioral dynamics in Section 2.\nOur simulation environment, StepCountJITAI for LLM, can generate text-based user preferences, and incorporates constraints that affect the behavioral dynamics. To illustrate how it works, we focus on the user preference \u201ccannot walk\". Other user preferences can be implemented in the same way as described in our work.\nBelow, we start with a summary of how to create StepCountJITAI for LLM, then provide details.\n1. Augment the simulation environment states with a new binary auxiliary variable W, with values: 0 \"cannot walk\" or 1 \"can walk\". Note that this variable W is hidden for the RL agent (i.e., not observed by the RL agent).\n2. Implement the dynamics for W using a Markov chain.\n3. Generate a text-based user preference when w = 0, randomly chosen from a list of pre-defined user preferences.\nCreating auxiliary variable W (cannot walk / can walk). We first augment the simulation environment with a new binary state variable W with value: 0 \"cannot walk\" or 1 \"can walk\". The variable W is not observed by the RL agent. It reflects a hidden state of the user, and is used to generate the user preference. We implement a Markov chain to simulate $w_t$, the values of W at time t. The Markov chain and transition function for W are shown in Figure 2 and Table 1."}, {"title": null, "content": "We define the new parameters: $P_{w01}$ the probability of transitioning from $w_t$ = 0 to $W_{t+1}$ = 1, and $P_{w11}$ the probability of remaining in the \"can walk\" state.\n$P_{w01} = P(W_{t+1} = 1|w_t = 0)$\n$P_{w11} = P(W_{t+1} = 1|w_t = 1)$\nSetting $P_{w11}$ to a lower (or higher) value allows for a lower (or higher) probability of remaining in the \"can walk\" state. Similarly, setting $p_{w01}$ to a lower (or higher) value allows for a lower (or higher) probability of transitioning from $w_t$ = 0 to $W_{t+1}$ = 1.\nWe note that the parameters $p_{w01}$ and $P_{w11}$ can be used to simulate the user state \u201ccannot walk\" over a variety of ranges, from shorter to longer time intervals, and thus enabling a variety of scenarios for our experiments.\nGenerating a text-based user preference \u201ccannot walk\u201d. Following the Markov chain and transition function in Figure 2 and Table 1, the variable $w_t$ can take values 0 \"cannot walk\" or 1 \u201ccan walk\".\nWhen transitioning from \"cannot walk\" to \"can walk\", the user preference is set to none, and the behavioral dynamics are not impacted by any constraints. The behavioral dynamics are the same as in the base simulator in Karine and Marlin [2024].\nWhen transitioning from \u201ccan walk\u201d to \u201ccannot walk\", a text-based user preference is randomly chosen from a list of pre-defined reasons for \u201ccannot walk\u201d. This list was previously created by asking ChatGPT to give reasons why a user cannot walk.\nWe show 20 reasons for \"cannot walk\":\nI am tired, I do not want to walk, I got an injury, my leg is sore, I have a headache, the weather is bad, I have a cold, I feel unwell, I have a prior commitment, I have a blister, I'm feeling dizzy, I twisted my ankle, I am recovering from surgery, I need to rest, I have joint pain, I'm dealing with anxiety, I have a family obligation, I forgot my shoes, I don't have anyone to walk with, I have to finish my work first.\nIncorporating \"cannot walk\" constraint that impacts behavioral dynamics.\nWhen transitioning from \"can walk\u201d to \u201ccannot walk\", in addition to generating a user preference above, the additional \"cannot walk\" constraint affects the behavioral dynamics as below.\n\"cannot walk\" constraint:\n\u2022 disengagement risk $d_t$ is increased by $\\eta_a \\times d_t$\n\u2022 habituation level $h_t$ is increased by $\\eta_h \\times h_t$\n\u2022 reward is set to 0.\nThe reward is the walking step count. It is set to 0 since the user cannot walk. We introduce the constraint parameters $\\eta_a$ and $\\eta_h$, with values $\\in$ [0, 1]. The base simulator variables and parameters are summarized in Appendix A.1."}, {"title": "Construct LLM prompt", "content": "3.2 Construct LLM prompt\nWe construct the LLM prompt by including a description of the mobile health app, the behavioral dynamics, the user current state and previous data, the user preference, and a question asking the LLM to make a decision \"send\" or \"not send\u201d a message to the user.\nBelow, we provide an example of an LLM prompt."}, {"title": "Select hybrid action based on RL candidate action and LLM decision", "content": "3.3 Select hybrid action based on RL candidate action and LLM decision\nBased on the given prompt, the LLM responds with a decision. The LLM decision is to either send a message to the user (i.e, a = a, where a is the RL candidate action) or not send any message to the user (i.e., a = 0). In other words, based on the prompt that includes the user preference and other data, the LLM can understand if a message should be sent or not, and thus can override the original candidate action selected by the RL agent. Note that the RL agent does not have knowledge of the user preference.\nExample of LLM response. If the user preference is \"I twisted my ankle\", then the LLM will recognize that a message should not be sent to the user. Thus, the LLM response is \u201cnot send\u201d, then the hybrid action is a = 0 (the original RL candidate action is discarded). Otherwise the LLM may decide to answer \"send\", in which case the hybrid action is a = a (\u00e3 is set to the original RL candidate action)."}, {"title": "Experiments", "content": "4 Experiments\nBelow, we first describe how to validate the LLM response, then we describe our practical method for accelerating personalization in adaptive intervention."}, {"title": "Validating LLM response", "content": "4.1 Validating LLM response\nWe perform experiments to check if the LLM response is correct. For each experiment, we generate prompts for \"cannot walk\u201d, along with the constraint to impact the behavioral dynamics. Ideally, the LLM response to these prompts should be \u201cnot send\u201d a message. We also ask the LLM to provide a reason for its decision. Below are some examples."}, {"title": null, "content": "Example 1:\nThe user preference is: my leg is sore.\nThe LLM correctly decides \"not send\" with the reason: The user has a sore leg, indicating they cannot walk. The app should not send a message.\nThe LLM manages to provide a valid reason. This shows how an LLM can be a useful tool."}, {"title": null, "content": "Example 2:\nThe user preference is: I'm feeling dizzy.\nThe LLM correctly decides \"not send\" with the reason: The user is feeling dizzy, which means they cannot walk, so the mobile health app should not send a message.\nAgain, the LLM manages to provide a valid reason."}, {"title": null, "content": "Example 3:\nThe user preference is I am dealing with anxiety.\nThe LLM incorrectly decides \"send\", with the reason: The user is expressing anxiety, and a tailored message could be helpful.\nIn a few cases, the LLM incorrectly decides to send a message, when the user cannot walk (i.e., the \"cannot walk\" constraint impacts the behavioral dynamics). Further investigation reveals that this occurs when the text-based user preference is ambiguous, thus does not clearly indicate if the user can or cannot walk. However, since these ambiguous text-based user preferences appear in less than 6% of the time steps during our experiment, and since sending the hybrid action falls back to sending the RL candidate action, LLM+TS still outperforms the standard TS agent.\nAbove, we have shown how to check if the LLM response is correct, thanks to our simulation environment, by tracking exactly where the LLM decision is incorrect. Future work would involve inserting additional insights into the LLM prompt to further improve the LLM response."}, {"title": "Practical method for accelerating personalization in adaptive intervention.", "content": "4.2 Practical method for accelerating personalization in adaptive intervention.\nWe conduct extensive experiments to compare our novel method LLM+TS to the standard TS. An experiment (a.k.a trial) corresponds to the behavioral study of one participant, where the maximum study length is 50 days, with daily data. We run our experiments for various combinations of the parameters ($P_{w11}$, $P_{w00}$), where $p_{woo} = 1 - P_{w01}$, to cover different scenarios. For example, the participant often sustains a light injury and thus often cannot walk for short periods, or the participant sometimes twists their ankle and thus sometimes cannot walk for longer periods. We repeat each experiment 10 times with different seeds.\nFor each experiment, we also run using various LLMs as foundational models, such as gemma and gemma 2, llama3 and llama 3.1, etc. [Gemma Team, 2024, Llama Team, 2024]. When using different LLM versions, we found that the choice of LLM version did not impact the results.\nFor each experiment setting, we compute the total reward as the sum of the rewards over a study. We use $\\eta_a = \\eta_h = 0.1$. The other experiment parameter values are provided in Appendices A.1 and A.2.\nWe present the results for two realistic scenarios: Scenario 1, where $P_{w11} = 0.7$, and Scenario 2, where $p_{w11} = 0.95$. In both scenarios, $p_{woo}$ varies in the range [0.1, ..., 0.5]. Recall that $p_{woo}$ is the probability of remaining in the \u201ccannot walk\u201d state, and $p_{w11}$ is the probability of remaining in the \"can walk\" state.\nIn Figure 3, we plot the median total reward, along with the 25th and 75th percentiles, over all the trials. The plots show that LLM+TS, outperforms standard TS in most settings. For example, for ($P_{w11}$, $P_{w00}$) = (0.7, 0.1), the median total reward for LLM+TS is 919.9 (25th percentile is 852.6 and 75th percentile is 990.4), whereas the median total reward for standard TS is 622.5 (25th percentile is 600.4 and 75th percentile is 699.4).\nWe compare the histograms of actions, taking into account all the selected actions across all the trials, for LLM+TS versus standard TS. We also compare the cumulative rewards. In Figure 4, we show the histogram of all the selected actions, and the median cumulative reward, along with the 25th and 75th percentiles, over all the trials. We show that LLM+TS is able to capture a larger number of actions 0, which indicates that the LLM has correctly decided to not send a message when the user cannot walk. This is further confirmed by the cumulative rewards for LLM+TS, which are higher than those for standard TS. Additional experiment results are provided in Appendix A.3."}, {"title": "Conclusion", "content": "5 Conclusion\nWe introduce LLM+TS, a novel hybrid method that combines the LLM decision and the RL action selection, to update the RL policy, and thus to accelerate personalization in health adaptive intervention. We also introduce StepCountJITAI for LLM, a novel simulation environment that can be used to develop new RL algorithms for adaptive interventions using LLMs. StepCountJITAI for LLM can generate text-based user preferences and incorporate constraints that affect the behavioral dynamics. Our results show that LLM+TS outperforms standard Thompson Sampling. Finally, we demonstrate how to frame a physical activity adaptive intervention as an RL system using LLM. Our method offers a promising solution framework for implementing a pipeline for personalized health adaptive interventions."}, {"title": "Appendix", "content": "A Appendix\nA.1 JITAI simulation environment specifications\nThe base simulator introduced in [Karine et al., 2023] mimics a participant's behaviors in a mobile health study, where the interventions (actions) are the messages sent to the participant. We summarize the base simulator specifications in Tables 2 and 3."}, {"title": null, "content": "We describe the behavioral dynamics in Section 2. We use the same default parameter values as in the base simulator: context uncertainty $\\sigma = 0.4$, behavioral parameters $\\delta_h = 0.1$, $\\epsilon_h = 0.05$, $\\delta_a = 0.1$, $\\epsilon_d = 0.4$, $m_s = 0.1$, $\\rho_1 = 50$, $p_2 = 200$. For our experiments, we set the disengagement threshold $D_{threshold} > 1$. The maximum study length is 50 days with daily data."}, {"title": "Thompson Sampling configurations", "content": "A.2 Thompson Sampling configurations\nUsing the same notations as in Section 2, we set the TS prior parameters $\\mu_{0a} = 0$ and $\\Sigma_{0a} = 100I$ for each action a, and the reward noise variance $\\sigma_{ua}^2 = 25^2$ for each action a."}, {"title": "Additional experiment results", "content": "A.3 Additional experiment results\nWe run similar experiments as in Section 4 for various combinations of ($p_{w11}$, $p_{w00}$). We show the histogram of all the selected actions, and the median cumulative reward, along with the 25th and 75th percentiles, over all the trials, in Figure 5. The histograms show that LLM+TS is able to capture a larger number of actions 0, which indicates that the LLM has correctly decided to not send a message when the user cannot walk. The cumulative reward plots show that LLM+TS outperforms standard TS."}]}