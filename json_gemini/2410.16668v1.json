{"title": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User Modeling", "authors": ["CHENYI LI", "GUANDE WU", "GROMIT YEUK-YIN CHAN", "DISHITA G TURAKHIA", "SONIA CASTELO QUISPE", "DONG LI", "LESLIE WELCH", "CLAUDIO SILVA", "JING QIAN"], "abstract": "Augmented Reality assistance are increasingly popular for supporting users with tasks like assembly and cooking. However, current practice typically provide reactive responses initialized from user requests, lacking consideration of rich contextual and user-specific information. To address this limitation, we propose a novel AR assistance system, Satori, that models both user states and environmental contexts to deliver proactive guidance.", "sections": [{"title": "1 Introduction", "content": "Satori \u609f\u308a, a Japanese ghost-like deity long known to read human minds, responds to one's thoughts before actions arrive. While such supernatural beings belong to folklore, modern AI technologies are beginning to emulate this capability, striving to predict human actions and provide proactive assistance during task interactions [38]. Proactive virtual/digital assistance, which determine the optimal content and timing without explicit user commands, are gaining traction for their ability to enhance productivity and streamline workflows by anticipating user needs from context and past interactions [62]. However, there is a scarcity of research to understand how to best design and implement such system."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Virtual Assistant in AR/VR", "content": "Virtual assistants in AR/MR can well support the task in assembly [7, 35], surgery [20, 69], maintenance [6, 22, 36] and cooking [16]. These assistant systems are often task-oriented and their principles do not easily generalize to other domains. One way to improve generalizability is via a command-based AR assistant, which can enhance user confidence in the agent's real-world influence and awareness [32]. Yet, they require the user's explicit commands and limit usability. Our work builds on the previous research as a virtual assistant in AR/MR, while addressing the users' needs without explicit commands or domain limitations.\nPre-made assistance in AR and VR applications typically involves prepared actions or reminders triggered by specific user inputs or situations. This kind of assistance is simple and intuitive, providing users with readily available support that can be accessed on-demand or in time sequences [45], which is straightforward to implement and use [78]. Assistants require extensive user manual interactions to describe and confirm the user's needs. For example, Sara et al. demonstrated an AR maintenance assistant while the technician still needs to manually confirm the completion of each step and proceed to the next step using touchpad controls or voice commands [78].\nProactive assistance, on the other hand, is designed to actively recognize context information and infer user intentions even if they are not explicitly provided [18, 70, 80]. Such assistance normally does not require human intervention [41, 79, 94] and can easily be scalable for everyday AR tasks, such as health care [71, 81], navigation [66] and laboratory education [83]. They enhance usability [82], foster trust [39], and improve task efficiency [93]. During AR interactions, proactive assistance often takes into account the user's surrounding environment, predicts the user's goals and offers context-aware recommendations, often for the sake of improving attention [29, 59, 60, 85]. However, existing proactive assistant relies on pre-determined contextual signals such as the location, time, and events to trigger the assistants to intervene [60]. For instance, Ren et al. [76] propose a proactive interaction design method for smart product-service systems, using functional sensors to gather explicit contextual data (e.g., physical location, light intensity, environment temperature) to predict implicit user states (e.g., user attention level). Although these methods advance the progress of proactive assistance, such signals may not align with the actual users' needs, leading to ineffective and obtrusive assistance [42, 91]. To address this, we propose to model the user's intent directly to determine a better timing and type of guidance.\nFurthermore, even though proactive assistants have been widely used, most AR assistants today remain passive as defining user intents is difficult in everyday settings. One main challenge is that understanding users' intent relies not only on the explicit cues (e.g., verbal or signals) but also significantly on the implicit non-verbal cues and visual embodiment [32]. Successfully decomposing and reasoning with the implicit cues improves the chances of intent referring. Recent advancements in vision-language models offer new opportunities to be used to understand visual"}, {"title": "2.2 Understanding User Intention", "content": "Understanding user intention in electronic devices, ranging from smart mobile devices to augmented reality (AR) systems, is essential for improving user interaction and experience. Research in the field of information needs has highlighted the importance of intention classification and systematic taxonomy to achieve this goal. Border proposed the taxonomy of web searches, classifying intentions into navigational, informational, and transactional [11]. This foundational work laid the groundwork for more detailed classifications. For instance, Dearman et al. categorized information needs and sharing entries into nine distinct categories, extending the concept of information needs to a collaborative level [17]. This classification allows developers to design products that better facilitate collaborative sharing of information. Church et al. found that contexts such as location, time, social interactions, and user goals influence users' information needs. For example, it was found that users generated more needs with locational or temporal dependencies when they were on the go. Users also require more geographical information when they are commuting. This study enabled researchers to design an information search platform SocialSearchBrowser to fit different users' information needs in a context-sensitive way [14]. Additionally, Li et al. furthered this branch of research by developing a design space of digital follow-up actions for multimodal information [52]. They classified actions into 17 types and identified seven categories of follow-up actions by qualitative analysis of users' diaries. They also deployed the system on mobile AR and conducted a user study to test the capacity of follow-up action designs. The study showed that the system could accurately predict user's general actions, provide proactive assistance, and reduce frictions [52]. Generally, prior studies on information needs, particularly on mobile devices, demonstrated that intention taxonomy could inspire the design of information search systems with more proactive and contextual assistance."}, {"title": "2.3 Belief-Desire-Intention Framework", "content": "The Belief-Desire-Intention (BDI) model [8, 15, 30, 49] is a framework to simulate human decision-making behaviors in both individual [74] and multi-agent settings [33, 48, 65]. The model originates from folk psychology and is extensively applied in cognitive modeling, agent-oriented programming, and software development. This model comprises three primary components: beliefs, desires, and intentions [8]. Beliefs represent the information that humans perceive about a situation (e.g., it is raining), limited by their perceptions. Desires are the goals that individuals aim to achieve given the current situation (e.g., A person prefers not to get wet during a rainy day). Intentions are \"conduct-controlling pro-attitudes, ones which we are disposed to retain without reconsideration, and which play a significant role as inputs to [means-end] reasoning\" [8]. In other words, user's behavior towards achieving the desire (i.e., goals) by selecting and committing to specific plans of action (e.g. The person plans to get an umbrella).\nPrevious studies have demonstrated the effectiveness of the BDI framework in modeling human behavior [33, 65]. Therefore, the BDI model can help in the building of intelligent agents in various applications. For example, in agent-oriented programming, the BDI model is pervasively used to model an agent executing programming functions. Agent-oriented software engineering utilizes beliefs, actions, plans, and intentions to develop programs. The BDI model enables more rational and autonomous executions in unpredictable environments, such as AgentSpeak(L) [73], 3APL [28], JACK [12], JADEX [9], and GOAL [27]. One benefit of using the BDI framework is that it makes agent behavior intelligible to end users and stakeholders. By committing to specific courses of action or intentions, BDI agents enhance user understanding and predictability of actions [1, 5, 19, 21, 25, 31, 34, 67, 75, 86]."}, {"title": "2.4 User Modelling in Human-AI Collaboration", "content": "Modelling the user state is a long-standing problem in HCI [3, 58]. Previous research focuses on the user goal and intent [92], Expertise modeling to support adaptive computing systems [87], and the study of the memory of the user for AR/MR-specific research [26, 84]. The BDI model, a commonly accepted psychological framework [24, 49], becomes crucial in the emergent human-AI collaboration, necessitating a better model of the user state [43]. Existing research, however, focuses on the user's intention and goal and seldom addresses the user's knowledge or belief [23, 47, 89, 90]. Furthermore, there's a lack of distinction between high-level goals (desires) and immediate goals (intents) [37]. Hence, we propose a general model for the user state, amalgamating belief, desire, and intent."}, {"title": "3 Formative Study 1: Design with Professional AR Designers", "content": "We first conduct a formative study to explore the problem space and potential benefits of proactive AR assistance. The study begins with a semi-structured interview on participants' background knowledge, followed by four different interaction scenarios that are common everyday AR tasks shown to participants for design feedback. A final apparatus combining participants' design feedback is created for later study."}, {"title": "3.1 Participants", "content": "Using email and snowball sampling, we recruited six professional AR designers (three female and three male, age: x = 30). Since we want to collect insights from experienced individuals, all participants are professional AR designers currently working in companies with at least three years of experience developing AR applications and who have experience creating AR applications for real-world guiding tasks. Participants are paid $30 per hour."}, {"title": "3.2 Task Design", "content": "The study contains two sessions: a semi-structured interview and a task to design AR assistance for four everyday scenarios. Each participant was asked to design two out of the four scenarios, ensuring a balanced distribution across scenarios. As a result, each scenario was designed by three different participants.\nIn the first session, we asked participants about their prior working experience with AR assistants, the challenges they faced in creating them, potential benefits, and applications in everyday settings. We further collected their responses on insights, potential benefits, and use scenarios of proactive assistants.\nIn the second session, designers were asked to design AR assistants for two everyday scenarios. The scenarios were assigned in a pre-determined order to balance the total number of designs. Since these scenarios are in everyday settings,"}, {"title": "3.3 Procedure", "content": "Since participants reside globally, the experiment was conducted remotely via Zoom after obtaining their informed consent. Participants were first asked to introduce their background, describe their daily work, and discuss projects related to AR guidance. We further inquired about their insights into the advantages and disadvantages of AR guidance, including challenges faced during development and by end users. Finally, we presented the concept of proactive AR guidance and solicited their opinions on potential challenges, applications, and feasibility.\nAfter the semi-structured interviews, participants received digital forms containing materials to design AR guidance for their assigned tasks, including textual descriptions, corresponding images, and videos. During this phase, participants were introduced to the process outlined in the previous paragraph. The experimenters addressed any questions participants raised via Zoom.\nOn average, the study's first session lasted approximately 28 minutes (x = 28), while the second session took around 60 minutes (x = 60). The entire experiment lasted about 1.5 hours. All participants successfully completed the design task, resulting in the creation of four AR task designs."}, {"title": "3.4 Results", "content": null}, {"title": "3.4.1 Interview result.", "content": "Benefits of AR Assistance. The experimenters found AR guidance particularly beneficial in providing real-time, contextual information that enhances both the user's awareness and decision-making in physical environments. A key advantage of AR is its ability to reveal forgotten or overlooked information. For instance, E1 emphasized that \"I find that AR assistance most useful when it helps the user realize something they might not know... they might forget about an object, or are not aware that this object could be used in this situation... then (with AR guidance) they have this Eureka moment.\" The \"Eureke moment\" refers to the moments where users suddenly realize the utility of objects or actions they hadn't considered. This function is especially useful in spatial tasks, as mentioned by E2 and E3. E2 highlighted that by overlaying visual cues such as arrows or animations directly onto the environment, AR can help the user better understand complex electrical circuits. E3 stated that \u201cin tasks with spatially sensitive movements...AR is a proper medium because users can intuitively know what they need to do.\" E3 further introduced that the users received spatially positioned guidance on turning knobs or pressing buttons in a machine operation task, which was more intuitive than traditional 2D instructions. Additionally, E4 stated that AR reduced interaction costs, particularly for tasks that require high-frequency operations, and enabled hands-free operations, making it highly valuable in scenarios like cooking. The first-person perspective offered by AR also aids in better comprehension of instructions, as mentioned by E7, especially for individuals with limited experience, such as students in laboratory settings."}, {"title": "4 Formative Study 2: Co-Design With the Psychological and HCI Experts", "content": "To build an automated AR assistance that proactively provides user assistive information (instructions or tips) and understands the current task, we recruited six experts from computer science and psychology (E1-6) to spur discussions for potential solutions over two sessions of dyadic interviews. The study is focused on how to design the system by asking the experts to discuss factors that construct a proactive assistance system. We paired experts with complementary backgrounds to form three groups (Groups A, B, and C) as table 2 shows. Their ideas and design decisions are then formed as design findings that motivate our system implementations."}, {"title": "4.1 Dyadic Interviews", "content": "We conduct two sessions of dyadic interviews where two experts of complementary backgrounds were grouped to discuss the solutions to the presented challenges. Dyadic interviews allow two participants to co-work towards open-ended questions [64]. This setup let us understand how to design the proacting AR assistance from interaction and user modeling."}, {"title": "4.2 Challenges", "content": "Based on the findings from professional AR designers in the previous formative study, we further performed a round of literature survey by searching AR assistance, embodied assistant, and immersive assistant on Google Scholar and ACM DL. We filter out the unrelated papers and derive the design challenges from the filtered paper collection. Two authors separately reviewed these papers and coded the key challenges from them. In total, we found 25 common challenges and grouped them by themes, following the concepts from thematic analysis [10]."}, {"title": "4.2.1 C1: Triggering assistance at right time is challenging.", "content": "The AR assistance needs to be triggered at the proper time during AR interaction. Improper timing strategy may disappoint the users and reduce the user's trust [40]. For example, when the user is occupied or under stress, frequent displaying of AR assistance may further increase the user's stress. Current proactive assistants regulate the timing using the user's intent and actions [79], or using fixed intervals to display assistance periodically. However, these methods do not consider the user's goal and lead to sub-optimal performance."}, {"title": "4.2.2 C2: Reusuability and scalability in AR assistance is a problem.", "content": "Most of the existing AR assistance systems are designed as ad-hoc solutions, where the forms of assistance (image, text, and voice) are developed individually [60, 66, 71]"}, {"title": "4.2.3 C3: Task interruption and Multi-task tracking is difficult.", "content": "Users handling multiple tasks at once is common in everyday life but challenging for AR assistance [?]. If the assistance cannot respond correctly following the user's immediate task switching or pausing, the interaction efficacy will be affected, resulting in users not trusting the assistive system [57, 95]. However, existing technologies that can provide proactive AR assistance are limited in reasoning the current task in a multi-task setting."}, {"title": "4.3 First session: participatory design", "content": "To formalize how do we design a proactive system capable of determining what to show users for task completion, we first present the background knowledge of AR assistance, modalities, applications, and challenges described in Sec 4.2. During the presentation, we clarified any concerns experts raised. At the end of the presentation, each dyadic group is asked to discuss 1) what the system needs to know to act proactively, 2) what kind of features the system needs to have, and 3) whether user modeling would be helpful, and 4) how do we mitigate the known challenges?\nAfter a 50-minute open-ended discussion, we provided them with a list of commonly used tracking, computer perception, contextual understanding, and display technologies and introduced their functions(Fig 2). Based on the"}, {"title": "4.4 Second session: design for implementation", "content": "The second session involved reconvening the same groups of experts for dyadic interviews. Initially, we presented the outcomes of the first session alongside our synthesized framework, seeking confirmation that it accurately reflected their initial ideas. This was followed by an open discussion where the experts delved into the framework's details and made adjustments to refine it further. This session, which lasted approximately one hour for each of the three groups of experts, was essential for finalizing the design framework for the AR assistant."}, {"title": "4.5 Data Collection", "content": "We screen-recorded both the discussions and the participatory design sessions. The audio from these recordings was then transcribed into text using Zoom's auto-transcription feature. Two co-authors independently analyzed the video recordings and transcribed text, coding the findings into similar insights. The insights are then combined into the following findings, and discrepancies are resolved through discussion."}, {"title": "4.6 Key Findings", "content": "[KI1] BDI may be beneficial for building AR assistants. During the discussions, all three psychology experts (E2, E4, and E6) brought up the importance of considering What the user sees and understands in the surroundings when discussing C1. For instance, E4 emphasized, \"... it is important to model the human's mental space, so we can adjust the AR (assistance's) timing.\" E4 further introduced the belief-desire-intention model, describing it as a well-established cognitive model for understanding human behavior and could be used towards proactive assistance. E2 emphasized the importance of providing this feedback loop to ensure the assistant correctly understands the user's goals, thus enhancing the effectiveness of the multi-task AR assistant.\n[KI2] User intention can bias the guidance type and content and it reveals the user's immediate and concrete goal. Group A and Group C recognized the importance of understanding user intention (i.e., immediate step in a task performance). In the participatory design, both groups included next-step prediction as an essential system design component. Additionally, E1 stated, \"We can understand the attention (and the intended action) of users; for example, during the cooking, his attention is on doing something not important (and we can drag his attention back to the correct step).\" E6 mentioned a series of possible functions for inferring short-term memory based on the egocentric view, which E5 opposed, highlighting that current computer vision methods cannot do this reliably. As a result, new methods are required to infer user's intention.\n[KI3] High-level goals, or desire, improves the transparency in the task switching, or Multi-tasking. E1 and E2 agreed that to support multi-tasking effectively, it is necessary for users to view the assistant's inferred task or desire. This idea aligns with the BDI model, as mentioned by E2. We further validated this concept with E1-3 and confirmed their proposed ideas were consistent. We also shared the concept with E5 and E6, and they agreed it is crucial for multi-task assistance design.\n[KI4] Modern Al models enable new opportunities in understanding the context, environment, objects, and actions in an image. E5 has extensive experience in traditional computer vision models and expressed concerns that"}, {"title": "5 Design Requirements", "content": "Summarizing the key findings the aforementioned formative study, we propose the design requirements for proactive AR assistance.\n[D1] BDI model can be used (K1) in deciding the timing and content of the assistant. Ideas from the belief component help the assistant filter the duplicated and unnecessary assistants while consideration of desire adds the importance of the current task.\n[D2] Proactively, AR assistance should provide multi-modal, multi-task assistance based on the user's current intention. The assistance should guide the intended task step with auxiliary support to enhance the user's capability for completing the task; and the user should be able to exit and entry any other tasks during the task performance [KI2].\n[D3] AR assistants should communicate the system state to the user to ensure transparency and interpretability [KI3]. Due to the intricate complexity of the physical space, predicting assistance for physical tasks is non-trivial, and users may lose trust in the AR assistant if unexpected conditions occur. Providing the immediate reasoning paths leading to the final assistance is beneficial for users to understand the system status and maintain trust.\n[D4] LLMs are useful to actively perceive the environment, model the user's action status track the task using the implemented BDI model, and select appropriate assistance [KI4]."}, {"title": "6 Satori System", "content": "In this section, we present the design of our proposed Satori system, guided by the design requirements summarized above. Satori is a proactive AR assistant that implements the BDI model to provide multimodal assistance at the appropriate time, using the multi-modal LLM and an ensemble of vision models. We first adapt the BDI model for the AR domain and describe our implementation for predicting the BDI model in Satori. Next, we detail the implementations for timing prediction and assistance prediction. Among the modalities of assistance, image generation is the most important and nuanced. Therefore, we propose a novel image generation pipeline based on DALLE-3 and a customizable prompting method. Finally, we describe our interface and interaction design, which enhances the affordance and interpretability of the assistant."}, {"title": "6.1 Implementing BDI model for AR Assistance", "content": "To fulfill [D1], we need to align the BDI model with the constraints and affordances of AR-mediated tasks. To do so, we must account for the unique characteristics of AR technology, including limited field of view, real-time environmental mapping, and the blend of physical and digital information."}, {"title": "6.1.1 Belief", "content": "Human belief is a complex psycho-neural function integrally connected with memory and cognition [56, 68]. Precise modeling of human belief within the constraints of AR technology is not feasible without access to human neural signals. For AR assistance, primary information sources are visual perception and user communication. We propose a two-fold method from capturing scenes and objects from visual input, and past action history from user communication, to approximate the user's belief state within AR constraints.\nScenes provide information on the user's current task context and potential shifts in their desires and intentions. For example, when the user looks at task-unrelated areas, the assistant can reduce assistance to avoid distraction. We represent the scene by the label predicted by the image classification model.\nObject information helps users to locate and interact with relevant objects. We use two different models for object detection. A Detr model [13] to detect objects in the scene in zero-shot [13]; and LLaVA model to detect objects that are being held/touched/moved by human hands while prompting it to \u201cdetect the objects interacting with the human hands\u201d [54]. We did not use the object detection models in this case because these models are trained to predict a fixed set of labels, limiting the generalizability.\nAction and assistant history enables non-repeating guidance; for repeated actions, the system progresses from detailed visual guides to concise text instructions, reducing cognitive load as the user gains familiarity. This history contains a log of user interactions and the AR assistant responses, such as both the type of assistance provided and descriptions of each instance. This log serves as a reference to determine whether the user has previously encountered specific assistance. Utilizing this historical data aids in accurately inferring the user's expectations regarding the type of support they receive, allowing for more tailored and effective assistance in future interactions."}, {"title": "6.1.2 Desire", "content": "Desire represents the user's high-level goals, such as cleaning a room or organizing a shelf. We infer the user's desire using a standalone LLM agent, which analyzes the current camera frame and supports voice interaction with the user. Voice interaction allows the user to provide feedback on the predicted desire, and the transcribed text can then be used by the LLM agent to adjust the desire prediction."}, {"title": "6.1.3 Intention", "content": "Intention refers to the user's immediate, low-level actions, such as grabbing a broom to sweep. It focuses on specific steps contributing to broader goals. Recognizing intent is crucial for providing action-oriented AR assistance. We infer intention primarily through perceptual information ([D1]), including visual cues and user interactions with objects. Voice commands serve as a secondary source. The system compares the intention with a desire to detect potential user errors."}, {"title": "6.1.4 Intention forecasting", "content": "To predict user intentions, we propose using a multi-modal LLM to forecast upcoming user intentions. Intention forecasting is inherently challenging due to the vast range of potential future actions and the ambiguous nature of user goals. Predicting intentions between humans is difficult, and similarly, prior experiments show that current action forecasting models struggle in our scenario due to misalignment with the label set. We constrain the forecasting process by incorporating user desire, which helps narrow down the range of possible future intentions. By setting a specific desire in Sec. 6.1.2, the search space for the next intention is significantly reduced. We then prompt the LLM to predict the intention within this constrained context [D4]. This prediction process follows a search-and-reflect framework consisting of three stages:\n(1) Analysis Stage: The LLM first analyzes the current desired task and its corresponding task plan. This involves understanding the user's goal and breaking it down into actionable steps, thereby establishing a foundation for anticipating the next actions.\n(2) Prediction Stage: Based on the current action and the established task plan, the LLM determines the next plausible steps. This involves leveraging contextual cues and understanding typical sequences of actions related to the current task.\n(3) Reflection Stage: The LLM evaluates the feasibility of the predicted next steps by considering the available objects and tools in the scene. Actions that require missing or unavailable objects are eliminated, ensuring that only viable actions are suggested. This filtering helps refine the prediction further by aligning it with the actual scene context, reducing irrelevant or impossible options."}, {"title": "6.2 BDI-Driven Timing Prediction", "content": "The assistance timing is basically determined by the occurrence of user intention. When the intention is detected, the corresponding assistance should be generated and presented. However, the user may be distracted in the real scenario. Therefore, a delayed assistance mechanism is implemented based on the user's perceived scene captured by the ego-centric camera. To implement, we use a combination of intention forecasting and Early forecasting mechanisms. In a standard pipeline, intention forecasting begins only after the previous action is completed. This approach can negatively impact user experience due to the latency, requiring users to wait for the pipeline to finish processing. To address this issue, we propose an early forecasting mechanism. While the intention forecasting pipeline runs and caches continuously, a parallel process early forecasting mechanism detects when an action is completed. Once the action is detected as finished, the cached intention and the corresponding assistance are immediately displayed. This way the user no longer has to wait for the intention forecasting pipeline to complete, which is normally high in latency."}, {"title": "6.2.1 Early forecasting mechanism with action finish detection", "content": "This pipeline aims to minimize waiting times, providing a more seamless and responsive interaction. It allows the AR assistance to proactively anticipate user needs and deliver timely guidance as soon as an action is completed, rather than waiting for the forecasting process to begin afterward. Unlike conventional action recognition or temporal grounding tasks, action completion detection lacks pre-trained models or large-scale datasets. To address this, we use the zero-shot learning capabilities of vision-language models and propose an ensemble-based approach to balance latency and effectiveness in predicting the next action.\nImplementation: We use an in-step checkpoint to predict the action finish detection. We use checkpoints to mark where a user succesfully finishes steps progress toward task completion. These checkpoints are derived from various cues, such as action detection results, object states, and hand-object interaction states. A task planner (formulated as boolean statements) is used to generate these checkpoints.\nTo see if a checkpoint is reached, we ensemble the local image captioning model BLIP-2 [51] with the online GPT-4V model. Since the BLIP-2 model has lower accuracy, its predictions require double confirmation to be considered reliable, while the GPT-4V model is inherently trusted. This ensemble strategy optimizes the performance of action completion detection by leveraging the strengths of both models, ensuring more accurate and timely assistance in the AR environment."}, {"title": "6.3 Multi-Modal Assistance Prediction", "content": null}, {"title": "6.3.1 Assistance design", "content": "AR assistance can come in different modalities such as sound, text, image, etc [D2]. Each has different functions and should be used depending on the scene context and users' actions. For that, we implement:\n(1) Textual assistance: is essential for guiding users toward their desired outcomes. We use white text on a black, transparent container to ensure readability."}, {"title": "6.3.2 Inferring modality", "content": "Given the predicted intention, the AR assistance should infer assistance modalities. For example, a cook chopping carrots while waiting for water to boil involves multiple components: time management (monitoring boiling time), and tool utilization (using a knife, water pitcher, etc.). The AR assistant can then provide targeted assistance for each of these components. This component-based approach enhances the scalability and adapt-ability of the AR assistance system. By reusing assistance strategies for tasks that share similar intention components, the system improves efficiency and consistency in delivering guidance."}, {"title": "6.3.3 Regulating assistance content", "content": "To avoid increasing task load and reading time, the assistant omits image-based assistance if similar content has already been displayed in recent interactions. When an object is present in the user's belief state, an indicator showing the object's location in the scene is provided. By aligning generated image assistance with the user's scene context, the system enhances user engagement. The assistance images should incorporate scene information and be consistent with the current belief state."}, {"title": "6.4 Image Assistance Generation", "content": "Image assistance aims to provide clear and straightforward instructions for tool usage. Image assistance is generated using the DALLE-3 model to flexibly adapt to current user states and contexts [55].\nClarity and conciseness are prioritized in the design of each image. Avoiding any unnecessary details that might confuse or overwhelm the user is important. Second, consistent formatting needs to be maintained throughout all images. The similar design style reduces cognitive load and helps users quickly grasp the instructions without needing to adapt to a new format.\nAdditionally, some images are action-oriented, with directions explicitly demonstrated through the arrows to guide the user step by step. This visual cue enhances comprehension by focusing the user's attention on the required actions. Lastly, an immersive experience is emphasized by ensuring that the objects depicted in the images are consistent with those in the user's real-world environment. This consistency aligns with the user's belief state [D1], allowing for a more seamless and intuitive interaction between the system and the physical task at hand."}, {"title": "6.4.1 Prompt template with modifier", "content": "We have developed a prompt template and an associated modifier taxonomy using the above design consideration. The prompt template is structured to encapsulate all necessary elements of an instructional image in a standardized format:\n[Object] [Attributes] [Action]. [Indicator] [Attributes] [Direction]. [Background]. [Style Modifier]. [Quality Booster]\nEach element in the prompt template, detailed in Table 3, is associated with a specific modifier to refine its description. The basic prompt includes Object and Action modifiers, which depict the action and the targeted object within the assistance image. The Attribute modifier incorporates real-world attributes such as color, shape, and materials of the objects, enhancing task immersion and operational accuracy. Additionally, Indicator modifiers, such as arrows, and"}, {"title": "6.4.2 Examples", "content": "We present two examples of image assistance generation to demonstrate the effectiveness of our template, as shown Fig.3. Please refer to the supplementary materials for details."}, {"title": "6.4.3 Implementing image assistance generation", "content": "We propose using Dalle-3 to generate visual illustrations. The image generation pipeline receives an initial prompt input, which includes the user's next-step intention and a list of key objects, from the guidance generation pipeline. The pipeline finalizes the prompt by integrating the information with modifier, as outlined in the prompt template. For generating multiple images to support complex illustrations, we first decompose the user's next-step intention, if applicable, and then form the final prompts, which are sent to the Dalle-3 model in parallel."}, {"title": "6.5 Interface and Interaction Design", "content": null}, {"title": "6.5.1 Interface design", "content": "As shown in Figure 4, the Satori interface displays the current assistance and the related BDI states. At the top of the interface, Satori presents the desired task (e.g., Connect Switch in the example) and the in-belief object indicator (e.g., Switch deck), as shown in Figure 4(a). These BDI states help users understand and track the assistant's state, aligning with design requirement [D3]. The in-belief object indicator shows the relative position of the object that the user needs to interact with. The arrow indicates the relative position of the object relative to the user. The object is detected in advance and stored in the modeled belief state, which is an in-memory program variable. The object will change based on the user's intention, and its location will be updated based on object detection results. In the center of the interface, as shown in Figure 4(a), are the in-step checkpoints described in Sec. 6.2.1. These checkpoints help users track the progress of the current step of the task, thereby improving the interpretability of the model's predictions. Users can control the checkpoints manually, and when all checkpoints are reached (represented by green check symbols in the example), the AR assistant will display the next guidance step."}, {"title": "6.5.2 Interaction design", "content": "In the Satori system, we support multiple interactions for the user to feedback on the"}]}