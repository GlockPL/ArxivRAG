{"title": "Disentangled Interpretable Representation for Efficient Long-term Time Series Forecasting", "authors": ["Yuang Zhao", "Tianyu Li", "Jiadong Chen", "Shenrong Ye", "Fuxin Jiang", "Tieying Zhang", "Xiaofeng Gao"], "abstract": "Industry 5.0 introduces new challenges for Long-term Time Series Forecasting (LTSF), characterized by high-dimensional, high-resolution data and high-stakes application scenarios. Against this backdrop, developing efficient and interpretable models for LTSF becomes a key challenge. Existing deep learning and linear models often suffer from excessive parameter complexity and lack intuitive interpretability. To address these issues, we propose DiPE-Linear, a Disentangled interpretable Parameter-Efficient Linear network. DiPE-Linear incorporates three temporal components: Static Frequential Attention (SFA), Static Temporal Attention (STA), and Independent Frequential Mapping (IFM). These components alternate between learning in the frequency and time domains to achieve disentangled interpretability. The decomposed model structure reduces parameter complexity from quadratic in fully connected networks (FCs) to linear and computational complexity from quadratic to log-linear. Additionally, a Low-Rank Weight Sharing policy enhances the model's ability to handle multivariate series. Despite operating within a subspace of FCs with limited expressive capacity, DiPE-Linear demonstrates comparable or superior performance to both FCs and nonlinear models across multiple open-source and real-world LTSF datasets, validating the effectiveness of its sophisticatedly designed structure. The combination of efficiency, accuracy, and interpretability makes DiPE-Linear a strong candidate for advancing LTSF in both research and real-world applications. The source code is available at https://github.com/wintertee/DiPE-Linear.", "sections": [{"title": "I. INTRODUCTION", "content": "Long-term Time Series Forecasting (LTSF) is of significant importance in numerous domains, particularly in industrial applications such as cloud computing [1], [2], energy optimization [3], [4], and manufacturing [5], [6]. Recent advancements in deep learning, including Recurrent Neural Networks (RNNs) [7], Convolutional Neural Networks (CNNs) [8]-[11], Transformers [12]-[20], Multilayer Perceptrons (MLPs) [21]-[26] and Fully Connected linear models (FCs) [27]-[29] have demonstrated promising performance for LTSF tasks. With the advent of Industry 5.0 and the era of big data, time-series data processing faces three primary challenges: high dimensionality, high resolution, and regulatory industrial scenarios. As regulatory emphasis on carbon footprint and trustworthiness increases [30], developing an efficient and interpretable time-series forecasting method has emerged as a critical challenge.\nLong-term forecasting for high-resolution time-series data remains challenging. Metrics like CPU load and network traffic in cloud environments are sampled at minute intervals, yet predictions must span days to support proactive resource allocation [20], [31], [32]. Similarly, advanced sensors in industrial settings generate vast minute-level data, requiring accurate long-term forecasting for equipment monitoring and operational optimization [33]. Existing methods face key limitations: RNNs struggle with parallelization [7], CNNs (e.g., TimesNet [11]) impose high computational costs, and Transformers, despite linear complexity improvements [16], [17], often overfit on long input sequences [27]. Fully connected models offer simplicity and competitive performance but suffer from quadratic complexity and parameter redundancy (Figs. 1a to 1c). This raises the pivotal question: How can a model achieve parameter efficiency to effectively handle the challenges of long-term forecasting for high-resolution data?\nIn high-stakes industrial scenarios, interpretability are indispensable requirements for achieving trustworthy AI [34], [35]. In this context, domain experts must understand the model to trust its predictions. While leading deep learning models offer impressive precision, their black-box nature significantly undermines user trust. Fully connected linear models, by contrast, provide better interpretability by design. However, it remains challenging to fully and directly understand the features encoded by these models through their two-dimensional weight matrices, whether represented in the time domain (Figs. 1a and 1b) or frequency domain (Fig. 1c). This introduces the second key question: How can a model ensure interpretability, enabling domain experts to understand and trust its predictions in high-stakes industrial applications?\nTo address these challenges, we revisit the performance of FCs in LTSF, seeking inductive biases to reduce complexity and enhance interpretability. Figs. 1a and 1b reveals that highly correlated weights along the diagonal suggest parameter redundancy in the time domain, while the high noise in off-diagonal regions in the frequency domain as shown in Fig. 1c indicates the independence of frequency components. This motivates our frequency independence hypothesis. Furthermore, time series often exhibit varying contributions of input time points to future predictions; for example, the most recent data typically plays a greater role in predicting future values. Thus, modeling the temporal significance of input sequences is crucial. Similarly, different frequency components contribute unequally [36], with significant frequencies often possessing higher signal-to-noise ratios and deserving more model attention. Finally, in multivariate time series, variable dependencies must be captured in an explainable manner, as variables may either share patterns or act independently.\nBased on the aforementioned motivations, we propose a novel Disentangled interpretable and Parameter-Efficient Linear model (DiPE-Linear) for long-term time series forecasting. As a deep linear model, it is structured into three interpretable linear sub-modules: Static Frequential Attention (SFA), Static Temporal Attention (STA), and Independent Frequential Mapping (IFM). The SFA module extracts and enhances prominent frequencies in the frequency domain, and the STA module selects input time points in the time domain that are strongly correlated with the predicted sequence. Meanwhile, the IFM module assumes independence across different frequencies and directly maps the input series to the output series at each frequency component in frequency domain. By leveraging disentangled representations in both the time and frequency domains, the model achieves reduced complexity with linear parameter growth and log-linear computational scaling. Additionally, as shown in Fig. 1d, these representations enhance domain experts' trust by providing clear insights into the learned features and the model's reasoning process.\nThese sub-modules employ a low-rank weight-sharing policy to efficiently model inter-variate relationships, reducing model complexity while maintaining strong performance. We further introduce a novel loss function, SFALoss, which is defined as a sum of time-domain loss and frequency-domain loss weighted by frequency-domain attention in the SFA module, guiding the model to prioritize significant frequencies with less noise.\nAdditionally, by avoiding non-linearity within these modules, our model remains a linear model while achieving comparable or SOTA performance in LSTF with significantly fewer parameters, as shown in Fig. 2. This lightweight design enhances model's interpretability and establishes a more reliable and efficient prediction pipeline, offering valuable insights for future research.\nIn summary, our contributions are multifold:\n\u2022 We propose DiPE-Linear, a deep linear model that achieves efficiency, significantly reducing parameter complexity from quadratic to linear and computational complexity from quadratic to log-linear.\n\u2022 Disentangled representation of model components in both time and frequency domain offers superior interpretability, thereby enhancing its trustworthiness for industrial applications.\n\u2022 Extensive experiments demonstrate that our method outperforms existing approaches, offering superior performance, efficiency, and interpretability."}, {"title": "II. RELATED WORK", "content": "Long-term time series forecasting (LTSF) requires models capable of efficiently handling long sequences. Early Transformer-based models [12]\u2013[17] progressively reduced computational complexity from $O(L^2)$ to $O(L)$. However, due to the inherently complex structure of Transformers, these models still contain a large number of parameters and are highly prone to overfitting when dealing with excessively long input sequences in LTSF tasks [27].\nDLinear [27] was the first to propose using a simple single-layer fully connected network to replace the complex Transformer structure. Although the fully connected layer has a computational complexity of $O(L^2)$, its simple structure results in significantly lower complexity compared to other Transformer-based models. Nevertheless, DLinear still suffers from notable parameter redundancy. FITS [29] addressed this by employing low-pass filters to discard high-frequency information in the frequency domain, drastically reducing the number of parameters in the fully connected network to the scale of 10k. SparseTSF [37], on the other hand, introduced Cross-Period Sparse Forecasting, which achieves a parameter count on the scale of 1k through downsampling and cross-period parameter sharing. However, both FITS and SparseTSF sacrifice accuracy to achieve parameter reduction [37], [38]. Further exploration into addressing parameter redundancy in fully connected models remains an open research question."}, {"title": "B. Exploiting Frequency-Domain Characteristics", "content": "Recent advancements in time series forecasting have increasingly leveraged frequency-domain knowledge to improve performance. One prominent approach incorporates frequency-domain information as auxiliary features to enhance the modeling of long-term dependencies. For example, FEDformer [17] introduces a DFT-based frequency-enhanced attention mechanism, where attention weights are computed from the spectra of queries and keys, with the weighted sum calculated directly in the frequency domain. Similarly, FiLM [39] applies Fourier analysis to retain historical information while filtering out noise. Alternatively, some methods operate entirely in the frequency domain. FreTS [40] employs a frequency-domain MLP to capture both channel-wise and temporal dependencies, while FITS [29] adopts a fully connected network specifically designed for spectral representations.\nAnother promising direction incorporates frequency-domain loss functions into time-domain training objectives to capture spectral patterns more effectively. For example, FTMixer [26] and FreDF [41] add frequency-domain loss components on top of time-domain losses, enhancing the model's ability to align with the ground truth in the frequency domain."}, {"title": "C. Channel-Independence for Multivariate Time Series", "content": "DLinear [27] was the first to introduce the Channel-Independent strategy for multivariate time series forecasting (the terms \"channel\" and \"variate\" are used interchangeably throughout this paper for simplicity). Although this approach ignores causal relationships between variables, it achieves significant performance improvements. PatchTST [19] further validated this idea in Transformer-based models, demonstrating that independently processing variables while sharing the same model weights across variables can be highly effective. However, RLinear [28] shows that for variables with distinct periodic characteristics, using independent weights for each variable can further enhance performance. Subsequent research has focused on finding an optimal balance between weight sharing and weight independence. For instance, MLinear [42] dynamically adjusts between Weight Sharing and Weight Independent strategies based on the temporal semantics of different sequences, while CSC [43] reassigns weight-variable mappings during the validation phase to achieve variable clustering. Nevertheless, these methods come with limitations. MLinear requires calculating both Weight Sharing and Weight Independent outputs before combining them, which increases computational overhead. CSC, on the other hand, relies on a strong prior assumption that variables clustered together should share the same weights, which may not hold universally."}, {"title": "III. METHOD", "content": "DiPE-Linear is constructed from several essential components. We begin by outlining the preliminaries related to TSF and the Fourier Transform, providing the foundation for our approach. Next, we present the architecture of DiPE-Linear, detailing its three core modules: Static Frequential Attention (SFA), Static Temporal Attention (STA), and Independent Frequential Mapping (IFM), under assumption that the input series is univariate. Then, we describe how the Low-rank Weight Sharing method generalizes those modules to multivariate time series. Lastly, we introduce SFALoss, a novel loss function that combines weighted losses in both the frequency and time domains, guiding the model to prioritize informative patterns while enhancing predictive performance."}, {"title": "A. Preliminaries", "content": "1) Problem Definition: The multivariate TSF problem can be formally defined as follows: Given a historical time series $x \\in \\mathbb{R}^{C \\times L}$, where C is the number of variates and L is the look-back length of input time series, the objective is to predict the future time series $y \\in \\mathbb{R}^{C \\times L'}$, where L' is the forecasting horizon.\n2) Discrete Fourier Transform: Time series data can be viewed as discrete samples of a continuous signal over time. In many signal processing and time-series forecasting applications, the Discrete Fourier Transform (DFT) is a commonly used method to analyze the frequency components of such discrete signals. For a discrete-time sequence $x[n]$ of length N, the DFT transforms the time-domain signal into its frequency-domain representation. This transformation is particularly useful for identifying underlying periodic patterns or extracting frequency components that contribute to the signal.\nThe DFT of a time series $x[n]$ is defined as:\n$X[k] = \\sum_{n=0}^{N-1} x[n] e^{-i \\frac{2 \\pi}{N} kn}, \\quad k \\in [0, N-1]$.(1)"}, {"title": "B. Static Frequential Attention", "content": "Extracting frequencies is a common practice, as previous FITS [29] employed a low-pass filter to eliminate high-frequency signals, while FedFormer [17] randomly selects a subset of frequencies as key features. However, such strong prior assumptions may lead the model to discard useful information. Therefore, we aim for the model to learn which frequency components to focus on.\nSpecifically, to ensure the model's capability to select and amplify significant frequencies, we introduce SFA module as a frequency-domain filter. Additionally, to preserve the temporal structure of the series and avoid interference with subsequent temporal feature extraction, we constrained this filter to be a zero-phase filter. The SFA operates by first applying real Fast Fourier Transform (rFFT) to transform the time-domain data into the frequency domain. In this domain, a learned filter is applied via element-wise multiplication, selectively amplifying or suppressing specific frequency components. As a zero-phase filter, it exclusively modifies the amplitude of the signal while preserving the original phase information. Finally, inverse rFFT (irFFT) is used to reverse the filtered data back to the time domain. Specifically, the enhanced signal $z_{SFA}$ is given by\n$z_{SFA} = F^{-1}(\\theta_{SFA} \\odot F(x))$, (5)\nwhere $x \\in \\mathbb{R}^{L}$ represents the input univariate series, $F$ denotes the rFFT, $F^{-1}$ is the irFFT, $\\theta_{SFA} \\in \\mathbb{R}^{\\lfloor L/2 \\rfloor + 1}$ is the learnable static frequential attention map, and $\\odot$ denotes element-wise multiplication."}, {"title": "C. Static Temporal Attention", "content": "Similarly, to enable the model to have the capability of capturing important input time points in the temporal domain, we introduce the STA as a second component of our model. This module enables the input time series to undergo element-wise multiplication with a learned temporal attention map, effectively assigning appropriate importance to relevant historical time points and thereby enhancing the model's ability to capture temporal dependencies. The processed signal $z_{STA}$ is thus given by\n$z_{STA} = \\theta_{STA} \\odot z_{SFA}$, (6)\nwhere $\\theta_{STA} \\in \\mathbb{R}^{L}$ is the learnable time-domain attention map."}, {"title": "D. Independent Frequential Mapping", "content": "While both the SFA and STA modules function as part of the input series pre-processing, the IFM module directly maps the input historical series to the forecasted output. To process the time series as a whole, leveraging the spectrum is a natural approach, as it encapsulates all the information about the entire sequence and enables better handling of periodic components. Properly and efficiently processing the spectrum remains a key challenge. Fig. 1c illustrates that the two-dimensional matrix used by FITS [29] to handle the spectrum is sparse, with only the diagonal weights being significant. This suggests that, for a time series, there is relatively strong independence between different frequencies, allowing each frequency to be handled separately.\nWe thus treat each frequency independently for two key reasons: (1) Apart from harmonic frequencies, there is generally little correlation observed between different frequencies in time series data. [41] (2) Although harmonic frequencies tend to exhibit strong correlations, we avoid explicitly modeling these inter-frequency dependencies to mitigate potential multicollinearity in linear regression.\nTherefore, for each frequency component, we apply a complex-valued multiply-accumulate operation independently. To transform the input space $\\mathbb{R}^{L}$ to the output space $\\mathbb{R}^{L'}$, we apply zero-padding to the input series, extending its length to $L + L'$ before performing the rFFT. At the end, the last $L'$ values are extracted and used as the predicted output. The forecasted output $y$ is thus defined as:\n$z_{STA\\_pad} = F(Zero\\_Padding(z_{STA}))$, (7)\n$Y_{pad} = \\Theta_{IFM} \\odot z_{STA\\_pad} + \\beta_{IFM}$, (8)\n$y = F^{-1}(\\hat{Y}_{pad}) [ -L' : ]$, (9)\nwhere $\\Theta_{IFM} \\in \\mathbb{C}^{\\lfloor (L + L' - 1)/2 \\rfloor + 1}$ is the complex-valued weight and $\\beta_{IFM} \\in \\mathbb{C}^{\\lfloor (L + L' - 1)/2 \\rfloor + 1}$ is the complex-valued bias. This process can be equivalently described as a 1D convolution in time domain, as explained in equation 3 and 4:\n$\\hat{y} = F^{-1}(\\Theta_{IFM}) * z_{STA} + F^{-1}(\\beta_{IFM})$, (10)"}, {"title": "E. Low-rank Weight Sharing", "content": "Weight sharing is a common practice in time series forecasting models [19], [27], [29], based on the assumption that different channels of a time series exhibit similar patterns. However, this assumption often does not hold. For example, in Weather datasets, variables such as temperature, pressure, and rainfall may follow distinct patterns. In such cases, training weights independently for each channel can improve performance [28], but this approach significantly increases the number of parameters by a factor of C, making the model complexity significantly higher.\nTo address this, we propose a more efficient alternative. By clustering variables based on their pattern similarities, we can reduce the number of weight sets, balancing the trade-off between model efficiency and performance. This clustering approach not only enhances computational efficiency by reducing the number of learned parameters but also improves accuracy, as the shared weights capture more robust patterns. Specifically, well-structured clustering ensures that the learned representations generalize better across similar variables. Inspired by mixture-of-experts [44] and dynamic convolution [45], we introduce a novel Low-rank Weight Sharing architecture, as shown in Fig. 4, leveraging these clustered patterns to optimize both parameter efficiency and predictive performance.\nSpecifically, for a multivariate time series with C variables, we introduce a hyperparameter M, where M denotes the number of independent weight sets to be learned, with $M \\ll C$. For each component, M distinct sets of weights are learned, alongside a static routing matrix. The low-rank routing matrix $R \\in \\mathbb{R}^{M \\times C}$ is designed to linearly combine the weights and assign each variable to an appropriate weight set. We regularize the static routing matrix using the Softmax function, where $\\tau$ is the Softmax temperature:\n$R' = Softmax(\\frac{R}{\\tau})$. (11)"}, {"title": "F. SFALoss", "content": "Building upon the insights of FreDF [41], we introduce SFALoss (denoted as $\\mathcal{L}$), which combines Weighted Mean Absolute Error (WMAE) in the frequency domain ($\\mathcal{L}_F$) and MSE in the time domain ($\\mathcal{L}_T$). For the frequency domain component ($\\mathcal{L}_F$), our SFA module selectively subtracts key frequencies while attenuating noisy frequencies. To refine this process, we modulate the frequency-domain loss $\\mathcal{L}_F$ through an element-wise multiplication with the SFA weighting factor, $\\Theta_{SFA}$, effectively filtering out loss contributions from less relevant frequencies. The WMAE loss in the frequency domain $\\mathcal{L}_F$ is formulated as follows:\n$\\mathcal{L}_F = \\frac{1}{C} \\sum_{c=1}^{C} \\frac{|| \\Theta_{STA} \\odot (\\hat{Y_c} - Y_c) ||_1}{|| \\Theta_{STA} ||_1}$, (13)\nwhere $Y = F(y)$ and $\\hat{Y} = F(\\hat{y})$ are respectively ground truth and predicted future time series in frequency-domain. While computing the loss, $\\Theta_{STA}$ is detached from the computational graph, ensuring that no gradients are backpropagated through it.\nWe also preserve the MSE loss in time domain $\\mathcal{L}_T$:\n$\\mathcal{L}_T = \\frac{1}{C \\cdot L'} \\sum_{c=1}^{C} \\sum_{i=1}^{L'} || Y_{c,i} - \\hat{Y}_{c,i} ||_2$. (14)\nFinally, the overall SFAloss function is defined as follow:\n$\\mathcal{L} = \\alpha \\mathcal{L}_F + (1 - \\alpha) \\mathcal{L}_T$, (15)\nwhere $\\alpha \\in [0, 1]$ is hyperparameter that balances the contribution of the two loss terms, allowing us to achieve a trade-off between them."}, {"title": "IV. EXPERIMENT", "content": "1) Dataset: To comprehensively evaluate the performance and generalization capability of our model, we selected a diverse set of eight datasets, including six for LTSF and two for short-term time series forecasting (STSF). These datasets come from various domains, with different sizes and sampling frequencies, offering a robust benchmark for assessing the versatility and scalability of our approach across a wide range of forecasting tasks. Details are shown in Table I.\nFor the LTSF task, we utilized six public datasets:"}, {"title": "B. Main Results", "content": "1) Versus FCs: We first compared our model DiPE-Linear with other leading FCs. To ensure a fair evaluation, all models were trained using the same trainer settings. Throughout the training process, we carefully monitored each model to ensure convergence under these consistent conditions. All models use the optimal model hyperparameters provided in the authors' original implementations. Table II presents the evaluation results for the linear models on the LTSF and STSF datasets. Across all eight datasets, DiPE-Linear consistently delivers comparable or even superior performance. On datasets with smaller sample sizes (ETTh1, ETTh2, Faas, Iaas, Illness, and M5), our model significantly outperforms its counterparts, demonstrating its ability to effectively capture temporal patterns with limited training data while avoiding overfitting. On larger datasets (ETTm1, ETTm2, Electricity, and Weather), DiPE-Linear achieves results comparable to other linear models.\nOne notable advantage of DiPE-Linear is its substantially reduced parameter count compared to FCs. By operating within a low-dimensional manifold in the FCs' model space, DiPE-Linear achieves performance comparable to or better than FCs, suggesting that our carefully designed model space contains the optimal solution to TSF tasks. This highlights the robustness and effectiveness of our architecture in representing the essential solution space of FCs with minimal complexity.\n2) Versus nonlinear models: Next, we compare DiPE-Linear with several nonlinear models, including Transformer-based models amd CNN-based models. Table III presents the evaluation results for these nonlinear models on public LTSF datasets. For private LTSF datasets, excessively long input lengths result in high memory usage for nonlinear models, leading to unacceptably high deployment costs in production environments. For STSF datasets, the limited amount of training data makes the models highly prone to overfitting, making training challenging. Therefore, we do not report model performance on these two types of datasets. To ensure a fair comparison, all nonlinear models were trained with input lengths L\u2208 {96, 192, 336, 512, 672, 720}, and the best results were selected for evaluation.\nOur model demonstrates a slight performance advantage over PatchTST, while requiring only $10^{-4}$ times the parameter count. Furthermore, our model significantly outperforms other nonlinear models, highlighting its competitive advantages in both efficiency and performance.\n3) Parameter complexity and efficiency: Recent studies have focused on parameter-efficient models, and we selected SparseTSF and FITS as baselines. In the FITS model, which employs a low-pass filter, we set the cutoff frequencies to the maximum and minimum harmonics provided in its source code, denoting them as FITS-T and FITS-L, respectively. Experiments on ETTh1, ETTm1, Electricity, and Weather datasets (Fig. 5) show that our model achieves the lowest MSE overall, with a parameter count falling between FITS-T and SparseTSF. While FITS-T reduces parameters by blocking more high-frequency information, it sacrifices performance. In contrast, our model not only uses fewer parameters than FITS-T but also matches or outperforms FITS-L. This demonstrates that our model design effectively balances performance and parameter efficiency for time series forecasting."}, {"title": "C. Ablation Studies", "content": "1) Basic Components: To comprehensively validate the effectiveness of our model, we conducted ablation tests on two submodules: SFA and STA. The submodule IFM, which maps the input sequence space to the output sequence space, serves as the core component of our model and therefore cannot be ablated. Table V presents the results of the ablation experiments on the ETTh2 and Weather datasets with various prediction lengths. In both the ETTh2 and Weather datasets, the incorporation of the SFA and STA modules generally enhances the model's prediction accuracy. Specifically, in the Weather dataset, the abundance of training samples ensures that each submodule effectively captures significant patterns, leading to a substantial improvement in prediction performance. This demonstrates the necessity of addressing the importance of both the frequency domain and time domain in our input data processing.\n2) Low-rank weight sharing: In multivariate time series, our model must effectively capture the relationships between variables. We adopt a low-rank weight sharing approach, specifying the learning of M sets of weights. Figure 7 presents the results of testing different values of M on the Weather dataset with a prediction length of $L' = 720$. When M = 1, our model is equivalent to weight sharing, meaning all variables use the same set of weights. When M = 21, our model learns a separate set of weights for each variable, treating the variables as independent. The experiments indicate that selecting an appropriate value for M enables the model to achieve optimal performance.\nWe conducted an in-depth analysis to evaluate the necessity of low-rank weight sharing for each component of the model. Extensive experiments were performed on the ETTh2 and Weather datasets, covering multiple prediction horizons. For each component, we compared three configurations: shared weights, low-rank shared weights, and full-rank independent weights. As presented in Table VI, the low-rank weight sharing consistently delivered superior performance across all settings, highlighting its critical role in optimizing model efficacy.\n3) SFALoss: The parameter $\\alpha$ in SFALoss governs the trade-off between the frequency domain loss and the time domain loss. To assess the influence of $\\alpha$ on model performance, we conducted experiments on the ETTh1 and ETTh2 datasets with a prediction length of $L' = 720$. The results, illustrated in Figure 8, demonstrate the significant impact of $\\alpha$ on optimizing the model's predictive accuracy across both datasets."}, {"title": "D. Interpretability: Case Studies", "content": "Benefiting from the disentangled representation of our model in both the frequency and time domains, our approach demonstrates superior interpretability. This interpretability facilitates user understanding of the model's behavior, thereby enhancing trust in its predictive outcomes.\n1) ETTh1: We select the ETTh1 dataset as a simple example. We choose an input length L = 96 and a prediction horizon L' = 96, ensuring the model has the minimum number of parameters. Additionally, within the ETTh1 dataset, we set the hyperparameter M = 1, which eliminates the router $\\mathbb{R}$. Thus for each module, we learn only a single set of weights. The weight visualization is shown in Fig. 9. The SFA module explicitly preserves low-frequency information while emphasizing harmonic amplitudes in the high-frequency region. The STA module identifies the importance of each time point in the input sequence for future predictions. The IFM module learns distinct prediction patterns for harmonic frequencies (shown in red) and non-harmonic frequencies (shown in blue).\n2) Weather: In the Weather dataset, different variables represent various meteorological data, such as temperature, pressure, humidity, wind direction, precipitation, etc.. We set M = 4 as the hyperparameter, allowing the model to learn that the router can represent the correlations among different meteorological data variation patterns. The visualization is shown in Fig. 10.\nWe can further process the weights learned by the model to understand the knowledge it has captured. Taking the Router as an example, for each variable, the router weight $\\mathbb{R}'$ can be viewed as the distribution of its predictive patterns across different weight groups. Consequently, we can compute the Jensen-Shannon Distance (JSD) between variables:\n$JSD(\\mathbb{R}'_p || \\mathbb{R}'_q) = \\frac{1}{2} \\sqrt{D_{KL}(\\mathbb{R}'_p || O) + D_{KL}(\\mathbb{R}'_q || O)}$, (16)\nwhere p, q \u2208 {1, 2, . . . , C}2 are two variables, O = ($\\mathbb{R}'_p + \\mathbb{R}'_q$) is a mixture distribution, and $D_{KL}$ is the Kullback\u2013Leibler (KL) divergence defined by (17):\n$D_{KL}(\\mathbb{R}'_p || \\mathbb{R}'_q) = \\sum_{i \\in {1, 2, ..., M}} \\mathbb{R}'_{p,i} log(\\frac{\\mathbb{R}'_{p,i}}{\\mathbb{R}'_{q,i}})$, (17)\nThus, we obtained the JSD Matrix based on the learnt router, as shown in Fig. 11.\n3) Electricity: We further selected the Electricity dataset to comprehensively demonstrate the representations learned by DiPE-Linear. We chose an input length L = 720 and a prediction horizon $L' = 720$ in the LSTF setting. In the Electricity dataset, the hyperparameter $M = 4$, meaning that four sets of weights are learned for each module. The parameter visualization is shown in Figure 10. Similar to the ETTh1 dataset, the SFA module learns to retain low-frequency information and high-frequency harmonics. The STA module clearly distinguishes the periodic variations between weekdays and weekends within a month. In the IFM module, four different frequency transformation patterns are also learned. Finally, the weight visualization of the Router illustrates the differences and similarities in predictive patterns across different variables in the dataset."}, {"title": "E. Complexity analysis", "content": "Our model retains the linear characteristics of FCs while achieving significant reductions in both parameter and computational complexity. In this section, we analyze the complexity of each module in our model in detail. For simplicity, we denote ambiguously both the input length and forecast horizon as L.\n1) Parameter Complexity: As shown in (5), (6) and (8), In the SFA, STA, and IFM modules, the parameter complexity is uniformly $O(M \\cdot L)$. Additionally, low-rank weight sharing introduces routing parameters with a complexity of O(MC), which is a higher-order infinitesimal relative to $O(ML)$. Therefore, the overall model complexity is $O(M \\cdot L)$.\n2) Computational Complexity: The computational cost of our model primarily arises from two components. First, the model's parameter computations involve only element-wise multiplications, yielding a computational complexity of O(C \u00b7 L). Second, the rFFT and irFFT operations each have a computational complexity of $O(C L log L)$. Therefore, the overall computational complexity of the model is $O(C \\cdot L log L)$."}, {"title": "V. CONCLUSION", "content": "This paper presents DiPE-Linear, a novel approach designed for LTSF in industrial applications. DiPE-Linear effectively addresses the challenges of Industry 5.0, including high-dimensional and high-resolution data as well as high-stakes scenarios, by leveraging disentangled, linear-complexity interpretable modules in both the time and frequency domains. The sophisticated design reduces parameter complexity from quadratic to linear and computational complexity from quadratic to log-linear. Extensive experimental results demonstrate that DiPE-Linear consistently outperforms existing models, significantly enhancing scalability, computational efficiency, and predictive accuracy, particularly in complex industrial environments. In future work, we will enhance DiPE-Linear by incorporating interpretable nonlinear modules, while further exploring lightweight and efficient practical LSTF models."}]}