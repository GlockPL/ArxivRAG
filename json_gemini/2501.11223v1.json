{"title": "Reasoning Language Models: A Blueprint", "authors": ["Maciej Besta", "Julia Barth", "Eric Schreiber", "Ales Kubicek", "Afonso Catarino", "Robert Gerstenberger", "Piotr Nyczyk", "Patrick Iff", "Yueling Li", "Sam Houliston", "Tomasz Sternal", "Marcin Copik", "Grzegorz Kwa\u015bniewski", "J\u00fcrgen M\u00fcller", "\u0141ukasz Flis", "Hannes Eberhard", "Hubert Niewiadomski", "Torsten Hoefler"], "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAl's 01 and 03, DeepSeek-V3, and Alibaba's QwQ, have redefined Al's problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures-uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs-present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with a broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between \"rich Al\" and \"poor Al\" by lowering barriers to RLM development and experimentation.", "sections": [{"title": "1 INTRODUCTION", "content": "Reasoning Language Models (RLMs), such as OpenAI's 01 [66], 03 [48], and Alibaba's QwQ [88], also referred to as Large Reasoning Models (LRMs)\u00b9, represent a trans- formative breakthrough in AI, on par with the advent of ChatGPT [64]. These advanced systems have fundamentally redefined Al's problem-solving capabilities, enabling nu- anced reasoning, improved contextual understanding, and robust decision-making across a wide array of domains. By extending the capabilities of standard large language models (LLMs) with sophisticated reasoning mechanisms, RLMs have emerged as the new cornerstone of cutting-edge AI, bringing us closer to AGI.\nHowever, the high cost and proprietary nature of state- of-the-art RLMs, such as those developed by OpenAI, risk exacerbating the divide between \u201crich AI\u201d and \u201cpoor AI\", raising significant concerns about accessibility and equity. Even the publicly available QwQ only comes with its model weights, and Alibaba does not disclose details about their training or data generation methodologies. Businesses and individuals unable to afford these advanced systems face a growing disadvantage, threatening to stifle innovation and reinforce systemic inequities. As RLMs become integral to critical applications, from healthcare to science, manage- ment, and beyond, it is imperative to address these dis- parities and ensure that the benefits of advanced reasoning capabilities are broadly accessible."}, {"title": "2 EVOLUTION & FOUNDATIONS OF RLMS", "content": "We first summarize the evolution and foundations of rea- soning language models. Figure 2 shows an overview of the history of the development of these models."}, {"title": "2.1 Basic Pillars of Reasoning LMs", "content": "The development of reasoning-capable LLMs represents a convergence of three critical threads: (1) advances in LLMs such as GPT-4, (2) RL designs such as AlphaZero, and (3) High-Performance Computing (HPC) resources. Together, these threads have shaped models capable of efficient Sys- tem 2 Thinking a level of reasoning that combines explicit deliberation with novel problem-solving abilities, distinct from the intuitive, fast, and automatic heuristics of System 1 Thinking. Figure 2 compares example designs in these pillars while Figure 3 (left side) further discusses the details of these pillars."}, {"title": "2.1.1 Large Language Models: A Reservoir of Knowledge", "content": "LLMs such as GPT-40 [65] or Llama [34] represent an ex- traordinary leap in the field of AI, constituting a vast repos- itory of world knowledge encoded directly in their weights. Trained on huge corpora of text from diverse sources, LLMs are capable of understanding and generating human lan- guage with remarkable fluency. However, their reasoning abilities largely align with the fast, automatic, and intuitive System 1 Thinking. While they can generate coherent re- sponses and even perform simple reasoning tasks, LLMs have limitations. The reasoning they exhibit is often shallow, rooted in the simple mechanism of predicting the next most probable token in a sequence rather than engaging in explicit problem-solving or structured analysis. While LLMs may generate plausible-sounding solutions to a problem, these outputs are the result of statistical language mod- eling rather than a deliberate, iterative reasoning process. This distinction highlights the need for integrating more advanced mechanisms capable of explicit reasoning into AI systems-paving the way for hybrid designs that combine the knowledge-rich foundation of LLMs with structured reasoning methodologies."}, {"title": "2.1.2 Reinforcement Learning: Exploring and Innovating", "content": "RL has historically provided a framework for decision- making and exploration in environments where an agent must learn optimal strategies through trial and error. Land- mark systems like AlphaZero [79] and a long line of others such as AlphaGo [78] or MuZero [76] demonstrated the profound potential of RL by achieving superhuman per- formance in games such as chess, shogi, and Go. Unlike traditional AI systems, AlphaZero began with no embedded domain knowledge. Instead, it mastered these games purely through self-learning, discovering novel strategies that even human experts had not considered.\nOne of the most striking examples of RL's innovative capacity came during an AlphaZero match, where the sys- tem made a move initially deemed a mistake by human observers. This move [61] later proved to be both surprising and strategically brilliant, illustrating the capacity of RL agents to explore unconventional solutions that lie outside the bounds of human intuition. Such capabilities are funda- mentally rooted in RL's ability to navigate vast search spaces effectively.\nHowever, traditional RL systems lacked the ability to encode real-world knowledge or handle complex, multi- faceted reasoning tasks. This limitation spurred the integra- tion of RL principles with LLMs, combining the structured exploration and optimization capabilities of RL with the knowledge-rich reasoning foundation of language models."}, {"title": "2.1.3 HPC: Scalability & Efficiency", "content": "The growth of LLM and RL systems has been propelled by advancements in High-Performance Computing (HPC). Initially driven by Moore's Law, which enabled a doubling of transistor density approximately every two years, HPC benefited from both technological advancements and the economic feasibility of manufacturing smaller transistors. However, as the costs of further miniaturization have risen sharply, Moore's Law has reached practical limits, necessi- tating alternative strategies like parallelism and heteroge- neous computing.\nModern HPC systems rely heavily on GPUs, TPUs, and AI accelerators for their parallel processing capabil- ities, alongside CPUs for sequential and general-purpose tasks. Heterogeneous computing leverages these compo- nents to optimize task-specific performance. Distributed frameworks, employing techniques such as data, model, and pipeline parallelism [5], [9], [13], further enable the training of enormous models across thousands of compute nodes.\nEnergy efficiency innovations, including sparsity, quanti- zation, and pruning, mitigate the growing energy demands of scaling Al systems. These advancements ensure that HPC remains a cornerstone for developing and deploying AI models, supporting the combination of vast knowledge, rea- soning capabilities, and computational scalability \u2013 allowing Al evolution to continue beyond the limits of traditional Moore's Law scaling."}, {"title": "2.2 The Convergence: System 2 Thinking in AI", "content": "The intersection of these three threads \u2013 LLMs, RL, and HPC has culminated in the emergence of models capable of System 2 Thinking. These advanced systems combine the knowledge-rich foundation of LLMs with the exploratory and optimization capabilities of RL, all supported by the scalability and performance of modern HPC. The result is a new class of AI models that can engage in explicit, deliberate reasoning processes.\nThese models possess a world model encoded in the weights of their LLM components, allowing them to reason about complex scenarios and contexts. Their RL capabilities combined with the HPC capabilities enable them to navigate truly immense decision spaces, evaluate multiple strategies, and iteratively refine solutions."}, {"title": "2.3 Interpolation (LLMs) vs. Extrapolation (RLMs)", "content": "Standard LLMs, driven by their autoregressive token pre- diction mechanism, primarily perform interpolation within the vast search space of solutions. They excel at generating responses that align with patterns seen in their training data, effectively synthesizing knowledge from known contexts. However, this process limits them to producing outputs that remain within the boundaries of their training distribution. In contrast, reasoning LMs enable extrapolation beyond these boundaries. By combining structured exploration, rea- soning LMs navigate uncharted areas of the solution space, generating novel insights and solutions that extend past the limits of their training data. This enables a shift from basic pattern completion to active problem-solving."}, {"title": "2.4 Hierarchy of Reasoning-Related Models", "content": "The evolution of RLMs can be understood as a hierarchical progression, with earlier models such as GPT-40 being less capable in terms of reasoning, and the o1-like architectures demonstrating increasing sophistication and explicit reason- ing abilities. This hierarchy reflects the integration of System 1 (LLMs) and System 2 (RLMs) Thinking. RLMs can be further divided based on how reasoning is implemented into Implicit RLMs and Explicit RLMs; the details of this categorization can be found in Figure 3 (the right side)."}, {"title": "2.4.1 Implicit Reasoning Models", "content": "In this subclass, the reasoning structure is embedded en- tirely within the model's weights. Models such as QwQ [88] operate as \"black boxes\", where reasoning is implicit and cannot be explicitly disentangled or manipulated. While these models exhibit improved reasoning capabilities com- pared to standard LLMs, their reasoning processes are opaque and rely on the internalized patterns learned during training."}, {"title": "2.4.2 Explicit Reasoning Models", "content": "These models introduce explicit reasoning mechanisms ex-ternal to the model's core weights. Examples include de- signs such as LLaMA-Berry [105], Marco-01 [108], and po- tentially OpenAI's 03, which incorporate mechanisms like explicit MCTS combined with RL for decision-making. This explicit structure enables the model to simulate, evaluate, and refine solutions iteratively, facilitating novel problem- solving and extrapolation. By separating reasoning from the static knowledge encoded in the weights, these models achieve greater flexibility and interpretability in their rea- soning processes. Note that the explicit reasoning can be internalized via training making it implicit we discuss it later in the blueprint."}, {"title": "3 ESSENCE OF REASONING LMS", "content": "We now describe the general architecture of RLMs, which we summarize in Figure 4. In the following sections, we generalize this description to the full RLM blueprint."}, {"title": "3.1 Basic Architecture, Pipelines, & Concepts", "content": "We now outline the foundational architecture, operational pipelines, and core concepts. Figure 4 offers three levels of detail. In general (the top-left part), the whole RLM archi- tecture consists of three main pipelines: inference, training, and data generation. The inference serves user requests, using models (e.g., the value or policy model) provided by the training pipeline. Data generation mirrors the inference pipeline in its internal design; the main difference is that it runs independently of the user requests, generating data that is then used to re-train the models. As such, training combined with data generation from various domains [74], [104] offers self-learning capabilities and is analogous to the self-play setting of AlphaZero [79]."}, {"title": "3.1.1 Inference", "content": "The inference process begins when the user provides an input prompt \u2460, which typically describes the problem or question to be addressed by the RLM. This input serves as the root of the reasoning process and initiates the con- struction of a reasoning structure \u2461 that organizes RLM's progress. The structure is usually represented as a tree. The root of this tree corresponds to the user's input, and subsequent nodes are generated to explore the search space the domain of possible reasoning paths or solutions. The purpose of this reasoning structure is to systematically investigate potential solutions, progressively refining and extending reasoning paths to converge on an optimal or satisfactory answer.\nAn individual point in the search space, represented as a node in the reasoning structure, corresponds to a reasoning step \u2462. A reasoning step is defined as a coherent and self-contained unit of thought a sequence of tokens that advances the solution by either exploring a new branch of the problem or building upon existing progress. These steps form the building blocks of the reasoning process.\nThe details of how the structure evolves are usually governed by the MCTS scheme, enhanced with policy and value models (we also distinguish other reasoning strategies, described below). This approach, inspired by methods used in AlphaZero, ensures that the search process is both efficient and directed toward promising solutions. The policy model \u2463 is responsible for generating new reasoning steps at each node, predicting the next most likely and logical steps to expand the reasoning process. Meanwhile, the value model \u2464 evaluates the quality of a reasoning path starting at a given node, helping the system prioritize the most promising steps to follow. Sometimes, the reward model\u00b3 could also be used, to assess the quality of an individual specific node and its corresponding reasoning step \u2465. In our blueprint, as detailed in the next section, we abstract the models into a more general notion of operators \u2466 to enable more flexibility in how they are implemented.\nThe search and reasoning processes continue iteratively until a terminal step is reached \u2462. This terminal step represents a completion of the reasoning chain that forms the final answer to the posed problem. It serves as the leaf node in the tree, concluding that particular reasoning path.\nThis architecture provides a unified framework that accommodates a wide range of reasoning tasks. Whether reasoning steps are fine-grained (e.g., individual token se- quences) or coarse-grained (e.g., entire reasoning chains treated as single nodes), the architecture adapts seamlessly. By structuring the search space explicitly and guiding ex- ploration with policy and value models, the RLM achieves a level of reasoning capability bridging intuitive pattern recognition and deliberate problem-solving."}, {"title": "3.1.2 Training", "content": "Training details depend on what model is trained (value, policy, reward, ...). In general, we assume fine-tuning a model such as Llama. Here, we follow an approach where one first harnesses supervised data, usually coming from existing datasets such as PRM800K [55] \u2460, which becomes a part of the framework supervised training data \u2461 used in the supervised training pipeline \u2462 to train some, or all, of the models considered in the blueprint \u2463. The second part of the overall training framework in RLMs is the unsupervised (self-learning) training pipeline, in which data is being continually generated and used to improve the models. The data can be obtained from inference \u2464 assuming quality control [36], but also from a dedicated synthetic data generation pipeline that mirrors that of the inference \u2464. To collect the data, one executes the respec- tive RLM pipeline for a given input task and gathers the results \u2465; depending on how detailed is the gathering process, the data collected can contain only output-based labels \u2466, process-based labels \u2467, or some other variant such as trace-based labels suggested in our blueprint, that generalize process-based samples to samples that contain also information about operators applied during the task solution process \u2468. All this data becomes a part of the replay buffer \u2469 and is used in the unsupervised training scheme \u2460 or it can also be used to train a model that would become an Implicit RLM \u2461."}, {"title": "3.2 Encompassing Diverse RLM Architectures", "content": "The above-described design is applicable to many RLM designs. However, there are numerous other variants of architectures, some of which do not fully conform to this framework. In this section, we discuss these variants, high- lighting how our blueprint accommodates such variations.\nIn some RLM designs [105], a single node in the MCTS tree could represent an entire reasoning structure, such as a complete chain of reasoning steps. In this case, the action space involves transitioning between different reasoning chains rather than individual steps. This approach changes the nature of the search, as the focus shifts from itera- tively constructing a single reasoning path to evaluating and refining entire structures within the search space. Our blueprint accommodates this with the concept of nesting, where a node in the reasoning structure can contain another reasoning structure.\nOther architectures introduce even more novel paradigms. For instance, Journey Learning [69] adds an additional layer of complexity by incorporating a transformation step that \"rewires\" the search or reasoning structure. This transformation consolidates multiple paths in the tree, synthesizing them into a new form that is used as input for subsequent reasoning iterations.\nDespite these variations, our blueprint is sufficiently general to encompass all these cases and beyond, as we illustrate more formally in the following. This generality ensures that the blueprint is not only applicable to existing designs but also provides a foundation for future innova- tions in RLM development."}, {"title": "3.3 Integration with Broader LLM Agent Ecosystems", "content": "The integration of RLMs into broader LLM agent ecosys- tems would enable these models to interact dynamically with external tools, databases, and resources during exe- cution. This interaction can occur within the inference or data generation pipeline, leveraging value or policy models to extend the reasoning process through access to retrieval- augmented generation (RAG), web queries, and specialized tools. For example, during a reasoning task, the value or the reward model could query a database to verify intermediate steps, ensuring factual correctness or retrieving additional context to refine its reasoning. Similarly, these models could utilize computational tools for mathematical or symbolic computations, thereby expanding the scope and accuracy of their reasoning."}, {"title": "4 BLUEPRINT FOR REASONING LMS", "content": "We now introduce our RLM blueprint that can be used to develop novel reasoning models and to provide ground for analysis, evaluation, and comparison of such designs. We overview the blueprint in Figure 5."}, {"title": "4.1 Overview & Main Components", "content": "The blueprint specifies a toolbox of components that can be used to build an arbitrary RLM. We identify several classes of such components. First, an RLM includes a reasoning scheme, which specifies a reasoning structure (e.g., a tree) together with a reasoning strategy (e.g., MCTS) of how this structure evolves in order to solve a given input task. Second, there is a set of operators (e.g., Refine) that can be applied to the reasoning structure (as specified by the reasoning strategy) in order to evolve it and make progress towards solving the input task. Operators are specified based on what they do (i.e., what effect they have on the reasoning structure). How this effect is achieved, depends on how a given operator is implemented. Here, many operators rely on neural models (e.g., Policy Model), which \u2013 together with their training paradigms form the third class of the blueprint components. Finally, we also distinguish a set of pipelines, i.e., detailed specifications of operations that orchestrate the interaction between the reasoning scheme and the operators in order to achieve a specific objective, such as training, inference, or data generation. Hence, an RLM can be defined as a composition of a reasoning scheme, a set of operators and associated models, and a set of pipelines."}, {"title": "4.2 Reasoning Scheme", "content": "A reasoning scheme is the part of the blueprint that specifies the details of the reasoning steps progressing toward the solution, how they are interconnected to form coherent chains, trees, or more complex reasoning structures, and how these structures evolve in the course of solving the input task."}, {"title": "4.2.1 Reasoning Step", "content": "A reasoning step is a fundamental unit of the reasoning structure a sequence of tokens that advances the RLM towards the solution. Reasoning steps can vary in length, ranging from a single token to entire segments of text. The variability in their granularity depends on the user design choice. In existing schemes, a reasoning step is typically conceptualized as a \"coherent and self-contained unit of thought\". For instance, in mathematical proofs, this may correspond to an individual logical argument or deduction.\nThe flexibility in defining reasoning steps allows mod- els to adapt to different problem domains, balancing fine- grained and coarse-grained reasoning. Coarse steps, such as logical arguments (or even complete reasoning path- ways [105]), simplify preparation and adoption of training data, enhance interpretability, and as we discuss in Sec- tion 8 \u2013 reduce computational overhead. On the other hand, single-token steps enable the utilization of concepts like token entropy [60] to incorporate the model's uncertainty, as well as the integration of advanced decoding schemes (e.g., speculative decoding [50] or contrastive decoding [53]) explicitly into the RLM design. Yet, while making the rea- soning steps more fine-grained allows for a more detailed exploration of solution paths, this increased flexibility re- sults in greater computational demands, particularly when combined with search algorithms such as MCTS."}, {"title": "4.2.2 Reasoning Structure", "content": "The reasoning structure specifies how individual reasoning steps are connected and organized. Common structures include chains (linear sequences), trees (hierarchical branch- ing), and graphs (arbitrary connections).\nChains are sequential reasoning flows, where each step builds directly on the preceding one. Chain structures are prevalent in CoT-based models, where each reasoning step follows logically from the previous step in a linear pro- gression. In tree structures, each reasoning step can branch into multiple continuations, forming a decision tree. This structure is commonly used in MCTS-based frameworks, where multiple potential paths are explored before selecting a branch that will be further investigated. It enables more effective exploration of the space of reasoning steps, but simultaneously makes the RLM design more complex and costly. Finally, graph structures allow for arbitrary depen- dencies between reasoning steps, enabling graph-based rea- soning, such as that found in the Graph of Thoughts (GoT) framework [6].\nFurther generalization involves nested structures, where reasoning nodes themselves may contain substructures. For example, a node in a tree structure may represent a CoT chain, as proposed in LlaMa-Berry [105]. This hierarchical organization could be particularly useful for multi-step tasks where high-level decisions guide low-level computa- tions, such as meta-reasoning frameworks [105]. One could harness any other higher-order structures, such as hyper- graphs, motifs, and others [7], [8], [11], [14]."}, {"title": "4.2.3 Reasoning Strategy", "content": "The reasoning strategy governs how the reasoning structure evolves, specifying the process by which new reasoning steps are added and integrated. Example strategies include:\n\u2022 MCTS [49] A popular approach that balances exploration and exploitation by simulating multiple reasoning paths and selecting the most promising one based on a scoring function.\n\u2022 Beam Search [81] A breadth-limited search that keeps a fixed number of top-ranked continuations at each step. While commonly used for decoding token sequences, beam search can also apply to reasoning steps.\n\u2022 Ensemble Methods These methods involve aggregating multiple independent reasoning strategies, such as com- bining chains and trees to enhance robustness and accu- racy. One example is Best-of-N [30], [97] \u2013 a strategy where multiple independent reasoning paths are generated, and the most effective solution is selected based on predefined criteria, e.g., accuracy or completeness. Another example is tree ensemble (Forest) [15] where, instead of a single reasoning tree, a reasoning \"forest\" consists of multiple disconnected trees, which may eventually converge at a shared solution node. This approach supports diverse reasoning pathways that parallelize exploration.\nReasoning Strategy vs. Decoding Strategy It is crucial to distinguish reasoning strategies from token-level decoding strategies. While decoding strategies, such as greedy search and nucleus sampling [42], generate the internal token se- quences within a reasoning step, reasoning strategies focus on the higher-level process of integrating and expanding reasoning steps within the reasoning structure."}, {"title": "4.3 Operators", "content": "Operators specify operations that can be applied to various parts of the reasoning structure to progress the reasoning process. We now provide an extensive toolbox of operators.\nMany of them have been widely used in RLM-related de- signs, but some to our best knowledge are still unex- plored, we include them to foster innovation and propel the design of more effective and more efficient RLMs."}, {"title": "4.3.1 Structure Operators", "content": "Structure operators transform the reasoning structure by taking it as input and producing a modified version, typ- ically through addition or refinement of reasoning steps. For instance, they may add new children to a specific node, facilitating the exploration of alternative reasoning paths.\n\u2022 Generate The Generate operator adds one or more new reasoning steps to a reasoning structure. Within the MCTS reasoning strategy, this operator is typically implemented as a policy model to generate new steps. In other strate- gies, the generation operator may involve sequentially appending steps (CoT) or exploring multiple candidate steps in parallel (Beam Search).\n\u2022 Refine The Refine operator enhances a given individual reasoning step. For example, it could address ambiguities, correct errors, and optimize inefficiencies, resulting in a more robust version of the step [59]. It could also inte- grate suggestions from self-critique [75] (evaluates steps to identify weaknesses and suggest targeted improvements), summarization [110] (condenses key elements into concise representations to streamline the reasoning structure), or rephrasing [28] (reformulates steps to improve clarity and coherence while preserving their logical integrity).\n\u2022 Aggregate This operator combines multiple reasoning steps, paths, or structures into the next individual step. This enables consolidating information or improving co- herence. It is used in Ensemble Methods [15] or in Graph of Thoughts [6].\n\u2022 Prune This operator removes nodes or reasoning steps from the structure that are deemed suboptimal or irrel- evant based on evaluation metrics. It enables optimizing the reasoning structure in order to, e.g., reduce token costs.\n\u2022 Restructure The Restructure operator applies arbitrary transformations to the reasoning structure, enabling flex- ible reorganization of its components. A notable example is the conversion of a reasoning tree into a linear chain by rearranging its branches into a sequential series of steps, as done in Journey Learning [69]. This restructuring facilitates the integration of insights from diverse branches into a cohesive flow, \"flattening\" it and making it easier for the model to process and utilize information within a single, unified context.\nDiscussion on Diversity In structure operators, there is a notion of how diverse the outcomes of the operator are. For example, when generating k new reasoning steps, one may want to make the contents of these steps as different to one another as possible. While different mechanisms to steer diversity exist, a typical approach is the use of the policy model temperature. We additionally propose to consider the Diverse Beam Search [92] which promotes diversity by maintaining multiple diverse candidate sequences during decoding. In MCTS, there is also a distinction between ex- ploitation (expanding the structure by applying generation operators within an already established tree branch) and ex- ploration (generating new branches). Here, one impacts di-"}, {"title": "4.3.2 Traversal Operators", "content": "Traversal operators define how the reasoning process navi- gates through the existing reasoning structure. These opera- tors play a crucial role in shaping the flow of reasoning by determining which paths to pursue.\n\u2022 Select The Select operator determines which reasoning steps to pick for further exploration, evaluation, or refine- ment within the reasoning process. It evaluates existing elements based on predefined criteria, such as heuris- tic scores, likelihood estimates, performance metrics or search strategies like PUCT [73] or UCT [49], selecting the most promising candidates to guide the next stages of reasoning. By balancing exploration (considering diverse alternatives) and exploitation (focusing on high-potential paths), the selection operator optimizes resource allocation and ensures efficient reasoning progression.\n\u2022 Backtrack The Backtrack operator enables the model to explicitly return to a previous reasoning step and continue along a different reasoning path. This operator supports error correction, divergence handling, and hypothesis re- vision by abandoning unproductive directions in favor of alternative trajectories. The QwQ model output indicates that the reasoning structures used as training data in this model harnessed Backtrack."}, {"title": "4.3.3 Update Operators", "content": "The Update operator enhances specific parts of the rea- soning structure without altering the structure itself. A common example is the backpropagation phase in MCTS, where evaluation scores are propagated and updated along existing reasoning steps to inform future decisions. Another form of update involves refining the content of individual nodes or subsets of nodes, replacing their original versions with improved iterations, such as the \"enhance\" thought transformation in Graph of Thoughts [6]."}, {"title": "4.3.4 Evaluate Operators", "content": "Evaluate operators take as input a segment of the reasoning structure and output a value without any modifications to the structure. They are widely used with reasoning strate- gies, such as MCTS.\nOne important type of evaluation occurs when the rea- soning structure reaches a terminal state, allowing the full reasoning sequence to be assessed against a known solu- tion-applicable to tasks with definitive answers, such as mathematical problems. This terminality evaluation veri- fies whether the final step provides a correct and complete solution.\nOne can also evaluate intermediate steps (i.e., non- terminal ones). This can involve estimating the reward associated with specific reasoning steps, using heuristics, aggregated simulation outcomes, or a trained reward model for more efficient assessments. Other methods such as embedding-based verification could also potentially be har- nessed [12].\nAnother form of evaluation employs a value estimator, which judges a given reasoning step based on its expected"}, {"title": "4.4 Models", "content": "Models are used to implement various types of operators. Most common are the value model (implementing the value evaluation operator) and the policy model (implementing the generate operator)."}, {"title": "4.4.1 Training Paradigm", "content": "Each model must be trained according to a specified paradigm, which outlines the methodology for optimizing its performance. This paradigm defines key training compo- nents such as the loss function, data generation and labeling procedures, and other critical training details.\nA wide range of training schemes has been developed for models used in RLMs, with early foundational work stemming from advancements related to AlphaZero. These schemes have since evolved to support the complex require- ments of reasoning tasks within LLMs. Common training paradigms include supervised fine-tuning (SFT), where models are trained on reasoning sequences labeled with Q-values; rejection sampling [17], [84], which involves filtering generated outputs based on quality criteria; and RL-based methods such as Proximal Policy Optimization (PPO) [77], Direct Preference Optimization (DPO) [71], and reasoning-specific variants like Reasoning Policy Opti- mization (RPO) [67]. Several training paradigms also incor- porate self-learning, where the model iteratively improves by generating and evaluating its own reasoning sequences, thereby simulating competitive or cooperative reasoning scenarios."}, {"title": "4.4.2 Training Data Scope", "content": "The training data for RLMs can vary significantly in terms of how much of the reasoning structure it captures. We now outline two established approaches, output-based su- pervision (OBS) and process-based supervision (PBS). More details regarding both OBS and PBS can be found in Appendix B.1.\nIn output-based supervision (also known as a sparse training signal) [25], [91] each training sample consists solely of the input and the corresponding output. For exam- ple, in mathematical problem-solving, a sample may include the task statement and the final solution, labeled as correct or incorrect. This approach is straightforward to implement, and the required data is relatively easy to collect. However, it can limit the model's reasoning accuracy, as it provides minimal insight into the intermediate steps that led to the solution [55].\nAn alternative approach is process-based supervision (also known as a dense training signal) [55], [95], where a training sample reflects the entire reasoning structure. In this case, the sample contains not only the input and final output but also all intermediate reasoning steps, annotated with labels indicating the quality of each step. This richer training data allows the model to learn more granular rea- soning patterns, improving its ability to generate accurate and interpretable solutions by understanding the reasoning process in detail. However, such data is much more chal- lenging to generate or gather [55].\nOBS vs. PBS By varying the training data scope, developers can strike a balance between ease of data col- lection and the depth of reasoning insights provided to the model, with dense supervision generally offering improved performance at the cost of increased data complexity.\nTrace-based supervision (TBS) is a potential way to extend PBS by incorporating detailed information about the sequence of applied operators, including traversal op- erators, within the reasoning structure. By capturing the full trace of how reasoning steps are generated, refined, or revisited, TBS would provide richer supervision that teaches the model to internalize not just the reasoning steps but also the process of navigating and manipulating the reasoning structure itself. This approach could enable the training of more powerful Implicit RLMs by guiding them to replicate the reasoning dynamics of explicit structures, improving their ability to reason flexibly and efficiently."}, {"title": "4.5 Pipelines", "content": "A pipeline is a detailed specification of operations that orchestrates the details of the interaction between the rea- soning scheme and the operators and models to achieve a specific objective. Typically, an RLM would incorporate a single pipeline for inference and a separate pipeline for training each model used in an RLM. Moreover, there could also be pipelines for synthetic data generation used for training models. One can also distinguish a pipeline that trains an Implicit RLM using the provided reasoning traces from the Explicit RLM.\nThe details of pipelines depend on arbitrary design choices. In Section 3, we provided a general description of how these pipelines work. In Appendix C, we present detailed algorithmic specifications of our pipelines, along with insights into the reasoning behind these design choices. Specifically, the inference pipeline can be found in Ap- pendix C.1 and in Algorithm 1. Pipelines for different train- ing phases and paradigms can be found in Appendix C.3, Appendix C.4, and in Algorithms 2\u20137. The data generation pipeline is detailed in Appendix D."}, {"title": "5 EXPRESSING EXISTING SCHEMES", "content": "We now showcase the expressivity of our blueprint, by illustrating how it can be used to model a broad scope of existing RLMs and other related works. We summarize the outcomes of the analysis in Table 1. We start with typical and most prevalent Explicit RLM architectures based on MCTS and policy and/or value models, where a single reasoning step is an individual logical argument (Section 5.1). We also discuss there schemes that generalize this typical design, by harnessing nesting or Linearization Structure operators. Finally, we study Implicit RLMs (Section 5.2) and various structured prompting schemes such as Cot or ToT (Sec- tion 5.3), showing that they also fit our blueprint."}, {"title": "5.1 Explicit RLMs", "content": "We start with the"}]}