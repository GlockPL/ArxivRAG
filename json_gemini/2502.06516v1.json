{"title": "Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation", "authors": ["Soobin Um", "Beomsu Kim", "Jong Chul Ye"], "abstract": "Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations.", "sections": [{"title": "1. Introduction", "content": "Diffusion models are a prominent class of modern generative AI, known for their ability to generate high-quality content across various data modalities (Ho et al., 2020; 2022; Zhang et al., 2023). Unlike traditional frameworks like GANs (Goodfellow et al., 2014), diffusion models are notably robust, effectively learning the underlying data distribution even for rare or underrepresented examples in the training data (Sehwag et al., 2022). This inherent advantage has been readily adopted by researchers, leading to significant progress in minority generation the task of generating minority samples that reside in low-density regions of a data manifold\u00b9 (Sehwag et al., 2022; Um et al., 2023; Um & Ye, 2024b;a). The distinctive characteristics of minority instances make them important in various applications, including medical diagnosis (Um et al., 2023), anomaly detection (Du et al., 2022; 2023), and creative AI (Rombach et al., 2022; Han et al., 2022).\nExisting high-performance minority generators primarily rely upon guided sampling (Sehwag et al., 2022; Um et al., 2023; Um & Ye, 2024b;a). For instance, the authors in Sehwag et al. (2022); Um et al. (2023) employ classifier guidance (Dhariwal & Nichol, 2021) to steer generation toward"}, {"title": "2. Background", "content": "Diffusion models are characterized by the two stochastic differential equations (SDEs): the forward and reverse SDEs (Song et al., 2020). Given a d-dimensional noise-perturbed data space x(t) \u2208 Rd with t \u2208 [0, T], the forward SDE is conventionally expressed as:\n dx = f(x,t)dt + g(t)dw, (1)\nwhere f(,) : Rd \u2192 Rd is a drift term, and g(\u00b7) : R \u2192 R indicates a (scalar) diffusion coefficient coupled with a standard Wiener process w \u2208 Rd. With properly chosen f(,) and g(), the forward SDE describes a progressive destruction of the target data distribution x(0) ~ po upto a fully noised one x(T) ~ pr that one can easily sample from (e.g., a Gaussian distribution).\nThe reverse SDE runs backward from the noised distribution x(T) ~ pr down to the data distribution x(0) ~ po, formally written as (Song et al., 2020):\n dx = [f(x,t) - g(t)\u00b2\u2207x log pt(x)]dt + g(t)d\u016b, (2)\nwhere w is a standard Wiener process with backward time flow (from T to 0). The score function \u2207\u00e6 log pt(x) is commonly approximated by a neural network se(x, t) via denoising score matching (Vincent, 2011; Song et al., 2020).\nOne prominent instance of diffusion processes is variance-preserving (VP) SDE (Song et al., 2020):\n dx = -\\frac{1}{2}\u03b2(t)xdt + \\sqrt{\u03b2(t)}dw, (3)"}, {"title": "3. Method", "content": "Our framework starts by investigating potential approaches to minority-focused generation without relying on low-density guidance. One viable method is temperature sampling (Ackley et al., 1985), a method commonly used in traditional likelihood-based frameworks (Ackley et al., 1985; Kingma & Dhariwal, 2018). In the context of diffusion models, temperature sampling can be implemented by scaling the score function along the generation trajectory (Dhariwal & Nichol, 2021). For instance, in the empirical reverse VP-SDE that employs a pretrained score function se(x, t), this translates to:\n dx = \\frac{1}{2} [-\u03b2(t)x - \u03b2(t) \\frac{s_\u03b8(x,t)}{\u03c4}] dt + \\sqrt{\u03b2(t)}dw, (9)\nwhere \u03c4 is the temperature parameter. Since s\u03b8(x,t)/\u03c4\u2248\u2207x log pt(x)1/\u03c4, this sampler has been regarded as producing trajectories along {pt(x)1/\u03c4/Zt}Tt=0 (Dhariwal & Nichol, 2021), where Zt is a normalization constant. In this context, choosing \u03c4 > 1 (i.e., high-temperature) is expected to favor generation in low-density regions.\nHowever, we argue that this naive application of high-temperature sampling is ineffective within the diffusion model framework. In fact, the marginal densities of trajectories generated by Eq. (9) are distinct from {pt(x)1/\u03c4}Tt=0, potentially leading to unwanted generation results. A formal description is provided below, with proof in Appendix B.1."}, {"title": "3.1. Towards guidance-free minority sampler", "content": "Proposition 3.1. Consider the temperature-scaled reverse"}, {"title": "3.2. Boost-and-Skip: minority-focused generation with two simple tweaks", "content": "In this section, we present Boost-and-Skip, a novel guidance-free approach that sidesteps the practical limitation discussed in the previous section. A key benefit of Boost-and-Skip is its simplicity combined with significant improvements and theoretical grounding. Boost-and-Skip involves two core modifications to the standard stochastic reverse diffusion process: (i) boosting the variance of the initial noise; and (ii) skipping several early timesteps.\nVariance-boosting. Consider an empirical reverse VP-SDE implemented using the score model se(x, t):\n dx = \\frac{1}{2} [-\u03b2(t)s_\u03b8(x, t)] dt + \\sqrt{\u03b2(t)}dw. (10)\nIn contrast to the common ignition practice that employs x(T) ~ N(0, I), we propose a \u03b3-scaled initialization:\n x(T) ~ N(0, \u03b3\u00b2I), \u03b3 > 1, (11)\nwhere the range of \u03b3 is selected to enhance minority sample generation. Intuitively, this modification encourages"}, {"title": "Timestep-skipping.", "content": "We address the vanishing impact problem by skipping several of the earliest timesteps. More specifically, we propose to start simulations of Eq. (10) from:\n Tskip := T - \u2206skip, (12)\nwhere Askip represents the amount of skipping, which is selected to ensure non-negligible a(Tskip). We found that integrating the timestep skipping with variance boosting results in a significant synergistic improvement in minority sample generation.\nWhile employing the timestep skipping alone could often yield some performance gains, we emphasize that these improvements are limited and inferior to the combined approach; see Table 2b for details.\nValidation on toy data. In Figure 2, we provide a sanity check of Boost-and-Skip on a two-dimensional toy dataset comprised of two concentric circles. There, we verify that variance-boosting and timestep-skipping with reverse-SDE (Figure 2(f)) is the only combination which generates on-manifold minority samples \u2013 ablating boosting, skipping, or SDE leads to inferior results. In the following section, we provide theoretical intuition as to why all three components are necessary for successful minority generation."}, {"title": "3.3. Rationale behind Boost-and-Skip", "content": "Now we delve into the underlying principles of Boost-and-Skip, elucidating why and how it is effective for minority generation. We present two key mechanisms: (i) low-density emphasis and (ii) rectification through contraction.\nLow-density emphasis. We argue that Boost-and-Skip encourages sampling from low-density instances by amplfiying their probability densities. To see this, we consider a simple (yet non-trivial) scenario where po follows a multivariate Gaussian distribution."}, {"title": "Low-density emphasis.", "content": "Proposition 3.2. Let the data distribution be x(0) ~ \u039d(\u03bc\u03bf, \u03a3\u03bf), and assume the optimal score function"}, {"title": "3.1 Towards guidance-free minority sampler", "content": "se(x,t) = V\u00e6 log pt(x) trained on po via the forward SDE in Eq. (3). Suppose the reverse SDE in Eq. (10) is initialized with (Tskip) ~ N(\u00b5Takip, \u03a3\u03c4skip). Then, the resulting generated distribution corresponds to x(0) ~ N(\u03bc\u03bf, \u03a3\u03bf), where\n \u00b5o = \u00b5o + \u03b1(Tskip) \u03a3\u03bf\u03a3\u03c4skip (\u00b5Tskip \u2212 \u00b5Tskip) , (13)\n\u03a3o := \u03a3o + \u03b1(Tskip)\u00b2\u03a3 \u03a3\u03a4 (\u03a3\u03c4skip \u2212 \u03a3\u03a4skip). (14)\nHere, \u00b5T skip and \u03a3\u03a4skip are defined as:\n \u00b5Tskip := \u03b1(Tskip)\u00b5o, (15)\n\u03a3\u03a4skip := I + \u03b1(Tskip)2 (\u03a3o \u2013 I). (16)\nSee Appendix B.2 for the proof. In the considered Gaussian setting, Boost-and-Skip can be instantiated by setting skip = I and Tskip < T. Note that when \u03a3\u03a4skip \u2013 \u03a3\u03a4skip > 0, the initialization contributes to amplify the variance of the resulting generated distribution \u03a3o compared to the original \u03a3o (see Eq. (14)). This indicates that our approach can lead to probability increases in low-density regions of po, i.e., the effect of low-density emphasis.\nA notable point here is that when Tskip \u2248 T, corresponding to the case where only boosting (i.e., our first modification) is applied, the low-density emphasis impact does not manifest. This is because limTskip\u2192T &(Tskip) = a(T) \u2248 0 leads to the recovery of the original data distribution (\u00b5o, \u03a3o) \u2248 (\u00b5o, \u03a3o); see Eqs. (13) and (14) for details. This highlights the necessity of incorporating time-skipping for effective minority generation, and also explains why the truncation trick (used in Brock et al. (2018)) could be insufficient in the diffusion model context. The key mechanism of Boost-and-Skip can be interpreted from a signal processing viewpoint. See Appendix A.4 for detailed analyses on this perspective.\nTo show that the condition for the low-density emphasis effect, i.e., \u03a3\u03c4skip \u2013 \u03a3\u03a4skip > 0, can be satisfied in practice, we provide a corollary that characterizes the range of Tskip over which the variance amplification impact occurs:\nCorollary 3.3. Suppose \u03a3o = \u03c3\u03bbI and\u2211Tskip = y\u00b2I, and define the quantity (if it exists)\n \u03ba := \u221a(\u03b3\u00b2 \u2013 1)/(\u03c33 \u2212 1).\nThe variance-amplification effect of \u03a3o occurs iff"}, {"title": "Illustration via contraction", "content": "\u00d8 if y \u2264 1 \u2264 \u03c3\u03bf,\n Tskip E (\u03b1-1(\u03ba),\u221e) if 1 < \u03b3,\u03c3\u03bf,\n [0, \u03b1-1(\u03ba)) if 1 > \u03b3, \u03c3\u03bf,\n [0,\u221e) if \u03c3\u03bf \u2264 1 < \u03b3 and (\u03c3\u03bf, \u03b3) \u2260 1,\nwhere we define a\u00af\u00b9(\u043a) := 0 when \u043a > 1.\nIn Appendix A.2, we provide an illustration that exhibits the behavior of \u03a30 := \u00f4I under the conditions specified in Corollary 3.3; see Figure 6 therein.\nRectification via contraction. A potential concern is whether the amplified noise components due to variance-boosted initialization may impede high-quality generation. To address this, we invoke stochastic contraction theory (Pham, 2008; Pham et al., 2009), a principle that is often used to describe error-rectifying behaviors of stochastic diffusion generative processes (Chung et al., 2022; Xu et al., 2023). Specifically under the discrete VP-SDE setting in Equation (8), we establish a general theoretical result showing that, for an arbitrary distribution po, the error introduced by the boosted initialization decays exponentially as stochastic sampling progresses. See below for a formal description of the claim.\nProposition 3.4. Consider an empirical version of the discrete VP-SDE in Eq. (8):\n Xi-1 = \\frac{1}{\\sqrt{\u03b1_i}}{x_i + (1 \u2212 \u03b1_i)s_\u03b8(x_i, i)} + \\sqrt{1 \u2212 \u03b1_i}z,\nwhere i \u2208 {1,...,N}. Assume the optimal score function, i.e., se(x, i) = \u221ax log pi(x), which is trained on an arbitrary data distribution po. Consider two sample trajectories {xi}Ni=0 and {\\hat x_i}^N_{i=0} where Nskip < N, which are initialized with distinct distributions: xN ~ N(0,I) and \\hat x_{Nskip} ~ N(0, \u03b3\u00b2I). Assuming that {x}_N{i=0} is a bounded process such that ||xi||2 < B (as in Xu et al. (2023)), the expected error between samples from these two trajectories at step i \u2208 {0,..., Nskip \u2013 1} is given by:\n  E[||xi - \\hat x_i||^2] < \\frac{2C}{1-\u03bb^2} + \u03bb^{2(Nskip-i)}(B^2 + \u03b3\u00b2d), (17)\nwhere \u03bb denotes the contraction rate:\n \u03bb := max_{j\u2208{i+1,..., Nskip}} \\frac{\\sqrt{\u03b1_j}}{1-\u03b1_{j-1}}, (18)\nand C := d(1 \u2212 \u03b1Nskip)."}, {"title": "Impact Statement", "content": "One potential negative impact is the intentional misuse of our sampler to suppress the generation of minority-featured samples. This could be achieved by setting \u03b3 < 1.0 with At > 0, potentially biasing the initialization toward high-density regions (as explored in Appendix A.5). Recognizing and mitigating this risk is essential, emphasizing the importance of responsible deployment to ensure inclusivity in generative modeling."}]}