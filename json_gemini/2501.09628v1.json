{"title": "ARTIFICIAL INTELLIGENCE-DRIVEN CLINICAL DECISION SUPPORT SYSTEMS", "authors": ["Muhammet Alkan", "Idris Zakariyya", "Samuel Leighton", "Kaushik Bhargav Sivangi", "Christos Anagnostopoulos", "Fani Deligianni"], "abstract": "As artificial intelligence (AI) becomes increasingly embedded in healthcare delivery, this chapter explores the critical aspects of developing reliable and ethical Clinical Decision Support Systems (CDSS). Beginning with the fundamental transition from traditional statistical models to sophisticated machine learning approaches, this work examines rigorous validation strategies and performance assessment methods, including the crucial role of model calibration and decision curve analysis. The chapter emphasizes that creating trustworthy AI systems in healthcare requires more than just technical accuracy; it demands careful consideration of fairness, explainability, and privacy. The challenge of ensuring equitable healthcare delivery through AI is stressed, discussing methods to identify and mitigate bias in clinical predictive models. The chapter then delves into explainability as a cornerstone of human-centered CDSS. This focus reflects the understanding that healthcare professionals must not only trust AI recommendations but also comprehend their underlying reasoning. The discussion advances in an analysis of privacy vulnerabilities in medical AI systems, from data leakage in deep learning models to sophisticated attacks against model explanations. The text explores privacy-preservation strategies such as differential privacy and federated learning, while acknowledging the inherent trade-offs between privacy protection and model performance. This progression, from technical validation to ethical considerations, reflects the multifaceted challenges of developing AI systems that can be seamlessly and reliably integrated into daily clinical practice while maintaining the highest standards of patient care and data protection.", "sections": [{"title": "1 Artificial Intelligence-Driven Clinical Decision Support Systems", "content": ""}, {"title": "1.1 From machine learning and statistical models to clinical decision support systems: An Overview", "content": "Clinical research demands a meticulous, multifaceted approach when evaluating predictive models, which extends beyond the traditional data science perspective. As we dive into this complex landscape, we must consider a series of key questions that shape the development and validation of machine learning models, ultimately determining their suitability as decision support systems in healthcare.\nIn prediction modeling, our main focus is on estimating the risk of adverse events based on a combination of factors. We seek to understand not only the predictive power of these factors, but also their individual contributions to the model's decision-making process. This understanding is crucial, as it allows us to incorporate subject matter knowledge into the modeling pipeline, bridging the gap between data-driven insights and clinical expertise.\nThe foundations of the clinical prediction model validation process have been presented in Steyerberg [Steyerberg and Vergouwe, 2014]. At the heart of this process lies the fundamental research question or hypothesis. For example, the choice of prediction outcome is paramount in clinical research. Outcomes such as mortality rates at 30 days are frequently relevant to various research questions. However, it's not just the nature of the outcome that matters, but also its frequency within the dataset. This frequency effectively determines the sample size, which in turn influences the statistical power and reliability of the model.\nThe selection of patient data for the development of the model is a critical consideration. Often, these data are collected for purposes other than the study at hand, raising questions about their representativeness. We must carefully examine whether patient records truly reflect the population for which the study is intended. Additionally, the treatment of prognostic factors and their effects presents a unique challenge. Although traditional studies often consider treatment effects negligible compared to prognostic factors, there are instances where these effects warrant specific attention. Adjusting for baseline prognostic factors can offer significant advantages in estimating treatment effects applicable to individual patients.\nThe reliability and completeness of the predictor measurements pose another hurdle in model development. Incomplete datasets are common, with missing values for potential predictors. The approach to handling these missing data can significantly impact the model's performance and validity. Although complete case analysis \u2013 excluding patients with missing values \u2013 is a straightforward solution, it often results in the loss of significant information. More sophisticated methods, such as imputation techniques that leverage correlations between variables, offer a more nuanced approach to preserving data integrity. In addition, informative missingness, where the fact that data are missing is related to the outcome of interest, must be carefully considered to avoid biased results.\nAs we navigate these considerations, we recognize that the development of machine learning models for clinical decision support systems is a multifaceted process. It requires a delicate balance between statistical rigor, clinical relevance, and practical applicability. By addressing these key questions and challenges, we pave the way for more robust and reliable predictive models that can enhance clinical decision-making and, ultimately, patient care."}, {"title": "1.2 A Quick Overview of Model Development and Validation Strategies of Machine Learning Models", "content": "Model evaluation and selection are critical steps in the machine learning development process. In an ideal scenario, we would have access to data that perfectly represents the entire target population. In this case, we could train and test the"}, {"title": "1.3 Performance Validation in Clinical Decision Support Systems", "content": "Validation of prediction models tailored for clinical decision support systems should occur through both internal and external methods [Steyerberg and Vergouwe, 2014, Ramspek et al., 2021]. Internal validation, using techniques like split-sample validation, cross-validation, or bootstrapping, assesses reproducibility within the development population. External validation, involving patients from different populations, tests the model's generalizability across various settings and demographics.\nWhile internal validation techniques provide valuable insights into a model's performance, external validation serves as a crucial complement in assessing predictive models for clinical use. This process involves testing the model on data that is entirely separate from the development dataset, often collected from different institutions or time periods. Despite the growing number of publications on prediction models, studies employing both internal and external validation remain relatively scarce. This highlights the challenges in establishing predictive models as reliable decision support systems.\nFigure 4 illustrates a risk prediction tool designed to estimate the likelihood of symptom nonremission in first-episode psychosis [Leighton et al., 2021]. The tool's internal validation performance was assessed using ten-fold cross-validation yielding an AUC of 0.74. External validation was conducted with patient data from various sites, produced an AUC of 0.73, confirming the model's generalisability. Successful external validation strengthens confidence in a model's clinical utility. It demonstrates that the model's predictions remain accurate across different patient populations and healthcare settings. This robustness is essential for establishing the model as a trustworthy component of clinical decision support systems. In other words, external validation offers a more rigorous test of a model's generalizability, revealing how well it performs in diverse real-world scenarios. It helps identify potential overfitting issues that may not be apparent through internal validation alone. In essence, external validation acts as a bridge between theoretical model development and practical clinical application. It provides the evidence needed to justify the integration of predictive models into healthcare decision-making processes, ultimately contributing to improved patient care and outcomes."}, {"title": "1.4 Calibration of Clinical Prediction Models", "content": "In clinical practice, the validation of machine learning models extends beyond traditional prediction performance metrics. While measures like AUC, precision, and F1 scores are crucial, they do not fully capture a model's clinical utility. Effective clinical decision support systems require assessment of underlying risk estimates and clinical usefulness, which can be subjective and application-dependent. Model calibration is a critical aspect of validation, referring to the agreement between observed outcomes and predictions [Van Calster et al., 2019]. For instance, if a model predicts a 15% risk of 30-day mortality, approximately 15 out of 100 patients with such a prediction should experience the outcome.\nCalibration is typically assessed using flexible calibration curves, which plot estimated risks against observed proportions of events. Two key measures of calibration are the calibration-in-the-large (alpha) and calibration slope (beta). A well-calibrated model should have an alpha close to zero and a beta close to one. However, these measures alone do not"}, {"title": "1.5 Calibration in Deep Learning Models for Clinical Decision Support", "content": "Calibration is also a critical aspect of deep learning models in clinical decision support systems, particularly for establishing trustworthiness with users. A well-calibrated model provides confidence estimates that accurately reflect the probability of correct predictions. For instance, a model with 90% confidence should be correct 90 out of 100 times. In practice, perfect calibration is unattainable, but we aim to approximate it. The Expected Calibration Error (ECE) is a common metric used to assess calibration, measuring the difference between confidence and accuracy across prediction bins.\nRecent studies have shown that deeper and more complex neural networks tend to be poorly calibrated, despite high accuracy [Guo et al., 2017, Nixon et al., 2019]. Interestingly, increasing model depth or the number of convolutional filters per layer tends to worsen calibration error while improving predictive performance. For example, a 110-layer ResNet model demonstrated high accuracy but poor calibration, potentially limiting its reliability as a decision support tool. The causes of miscalibration in deep networks are not fully understood, but they appear to correlate with model complexity and capacity. Conversely, techniques like weight decay (L2 regularization) can help reduce calibration error. These findings suggest that in complex networks, over-fitting may manifest in probability estimates rather than classification errors.\nFor healthcare applications, reliable confidence measures are crucial. Users of clinical decision support systems need to be aware of the confidence level in disease diagnostics. Efforts to improve calibration in deep neural networks include architectural modifications and adjustments to training and optimization strategies. While the ECE is widely used, it has limitations. The choice of bin number involves a bias-variance trade-off, and the metric may not fully capture calibration"}, {"title": "1.6 Decision Curve Analysis", "content": "While accuracy metrics such as sensitivity, specificity, and area under the receiver operating characteristic curve are essential for evaluating prediction models, they fail to capture the clinical consequences of implementing these models in practice. Decision curve analysis (DCA) addresses this limitation by incorporating the concept of net benefit (NB), allowing for a more comprehensive assessment of a model's clinical utility [Vickers and Elkin, 2006, Van Calster et al., 2018].\nNet benefit is calculated across a range of threshold probabilities (ThresP), representing the point at which a clinician or patient would opt for intervention based on the model's prediction. This approach weighs the benefits of true positive\n$NB = \\frac{TP}{N} - \\frac{FP}{N} * \\frac{ThresP}{1 - ThresP}$ (3)\nDCA plots the net benefit against threshold probabilities, comparing the prediction model's performance to alternative strategies such as treating all patients or treating none [Vickers et al., 2019]. This visual representation allows stakeholders to assess the model's value across different risk thresholds, which may vary depending on the clinical scenario and individual preferences.\nFor instance, in cancer screening, a lower threshold probability might be preferred due to the severe consequences of missed diagnoses. Conversely, a higher threshold might be appropriate in situations where unnecessary interventions carry significant risks or costs. Figure 6 shows the DCA plot for the risk prediction model of nonremission presented in Figure 4. The plot demonstrates that the net benefit of using the developed model is higher than the alternatives of treating all, treating no patients, or treating based on the duration of untreated psychosis (DUP).\nThe interpretation of DCA results requires careful consideration of the clinical context. A model demonstrating higher net benefit than alternative strategies within a clinically relevant range of threshold probabilities can be considered clinically useful. However, this assessment should be made in conjunction with expert opinion and patient preferences.\nDCA can be applied to both continuous probability predictions and binary diagnostic tests. It is particularly valuable when evaluating models in external validation cohorts, as it provides insights into the model's generalizability and potential impact on clinical decision-making.\nAn illustrative example of DCA in practice is its application to a prediction model for outcomes in first-episode psychosis [Leighton et al., 2019, 2021]. By consulting specialist psychiatrists to determine clinically relevant threshold probabilities, researchers were able to demonstrate the model's superior net benefit compared to alternative strategies within the specified range.\nIt's important to note that while DCA offers valuable insights into clinical utility, it should not replace traditional measures of model performance. Instead, it should be viewed as a complementary tool in the comprehensive evaluation of prediction models, bridging the gap between statistical performance and clinical applicability.\nIn conclusion, decision curve analysis provides a robust framework for assessing the clinical value of prediction models. By incorporating the concept of net benefit and allowing for comparison across different decision thresholds, DCA enables more informed decisions about model implementation in clinical practice. As healthcare continues to move"}, {"title": "1.7 Responsible Development of Artificial Intelligence-Driven Clinical Decision Support Systems", "content": "Previously, Steyerberg et al. [Steyerberg and Vergouwe, 2014] has established the importance of four steps to guide the development of machine learning models for healthcare applications: A) - Calibration in the large, B) Calibration slope, C) Discrimination performance established both with internal and external validation and D) Decision-curve analysis. Recently, the integration of artificial intelligence (AI) and in particular deep learning in clinical decision support systems necessitates a more careful consideration of the principles behind responsible model development [de Hond et al., 2022, Van Smeden et al., 2022]. These principles have extended guidelines to include additional safeguards that encompass fairness, explainability, privacy-preservation and interoperability, all of which are crucial for developing trustworthy AI applications in healthcare.\nFigure 7 summarises the concepts behind responsible AI models in healthcare. We already referred to the term 'clinical usefulness', which encapsulates the necessity of an AI model to address a healthcare challenge and its capacity to evaluate if the advantages of an intervention justify the associated risks. \u2018Effective and Efficient' refers to the robustness of the underlying model and whether the outcome is reliably measured as it was highligthed at Steyerberg et al. [Steyerberg and Vergouwe, 2014]. 'Interpretability' is important to understand what the underlying factors are that the model based its specific decision. Interpretability and transparency are cornerstones of responsible AI, enabling users to understand both the technical processes and human decisions involved in the system's operation. Accountability in AI systems requires mechanisms to minimize negative impacts and report adverse consequences. In this context, transparency is crucial for assessing accountability and fairness.\nHealthcare practitioners have expressed concerns about over-reliance on AI systems they cannot fully understand or explain. Addressing these concerns requires further training for practitioners and involving end-users in the design process. The next generation of human-centered clinical decision support systems should possess two key abilities: explaining the model's representation and decisions, and adapting based on feedback.\nOn the other hand, privacy and data governance are also paramount in clinical decision support systems. Developers must implement safeguards to protect user privacy, ensure data integrity, and control access to sensitive information. 'Privacy-preserving' technologies are also essential to address concerns arising from the use of AI models in new applications that enable real-time tracking of human activity and health in home settings. Privacy-preserved AI model should leverage interoperable solutions that seamlessly integrate into the healthcare system.\nIn conclusion, adherence to responsible AI development guidelines is crucial in designing AI-powered clinical decision support systems. Emphasizing clinical usefulness, explainability, and human-in-the-loop designs is essential to mitigate risks and realize the full potential of AI in healthcare. The development of the right level of interactivity and explainability in medical applications remains an open research question, highlighting the ongoing challenges in this rapidly evolving field."}, {"title": "2 'Fairness' in Machine Learning Models", "content": ""}, {"title": "2.1 Assessing Bias in Clinical Predictive Models", "content": "Electronic health records (EHRs) present unique challenges and opportunities in predictive modeling, particularly in ensuring fairness and equity in healthcare outcomes. Biases can stem from flaws in study design, execution, or data analysis. To identify such biases, a comprehensive approach is necessary, considering the model's intended use, target population, predictors, and predicted outcomes.\nTherefore, the evaluation of predictive models should extend beyond discrimination and calibration to encompass potential biases that may introduce systematic errors. A framework for assessing bias in clinical predictive models has been proposed, focusing on four key domains: participant selection, variable and predictor selection, outcome assessment, and analysis [Wolff et al., 2019]. This framework emphasizes the importance of appropriate inclusion and exclusion criteria, consistent predictor definition and assessment across participants, and the use of standardized outcome definitions.\nAdditionally, the analysis should consider sample size adequacy, handling of continuous and categorical predictors. Researchers are advised to avoid selecting predictors based solely on univariate analysis. Models can be categorized as having low, medium, or high risk of bias based on the assessment of these domains. Notably, prediction models developed without external validation should generally be considered high risk, except when based on very large datasets.\nThe concept of \"informative presence\" in EHRs refers to the potential information carried by the presence or absence of patient data at any given time point. For example, EHRs can be inherently biased because sicker individuals are monitored more frequently. This type of informative presence, indicates that the frequency of health records can reflect a patient's health status. \"Informative observation\" extends this concept to the timing, frequency, and patterns of longitudinal observations in EHRs, which can provide insights into a patient's evolving health state [Sisk et al., 2021]. While these phenomena can complicate causal or association studies, they also offer potential sources of implicit information that could be exploited in predictive models.\nIn conclusion, while identifying and addressing bias is crucial for developing robust clinical predictive models, the inherent characteristics of EHRs, such as informative presence and observation, present both challenges and opportunities. Researchers must carefully interpret results while also exploring innovative ways to leverage these implicit data patterns to improve prediction accuracy."}, {"title": "2.2 Equity Challenges in Machine Learning for Healthcare Applications", "content": "The pursuit of health equity is a global priority, as exemplified by the World Health Organization's vision of a society where all individuals enjoy long, healthy lives [Amri et al., 2021]. Machine learning algorithms have the power to identify unexpected patterns in data, but they can also inadvertently perpetuate and amplify existing biases. This is particularly concerning when these algorithms are used to support clinical decision-making, as they can systematically disadvantage certain population groups [Barocas and Selbst, 2016, Obermeyer et al., 2019].\nElectronic health records, which serve as the foundation for many predictive models, often reflect historical biases in patient selection, policies, and societal circumstances. These biases can manifest in various ways, such as under-representation of minority groups in the data, systematic differences in feature availability across populations, or the compounding of initial biases over time.\nThe case of St. George Hospital in the UK serves as a cautionary tale [Schwartz, 2019]. In the 1980s, the hospital developed a computer program to streamline medical school admissions based on historical data. Unintentionally, this program formalized existing prejudices against racial minorities and women, demonstrating how algorithmic decision-making can perpetuate systemic biases.\nSimilar issues have been observed in other domains, such as facial recognition technology and online advertising. These examples highlight the potential for algorithms to discriminate based on race, gender, or age, often due to non-representative training data or the inadvertent use of biased proxies.\nIn healthcare, a 2019 study revealed that a widely-used risk prediction algorithm considered Black patients to be healthier than equally ill White patients [Obermeyer et al., 2019]. This discrepancy arose because the algorithm used healthcare costs as a proxy for health needs, failing to account for disparities in healthcare access and utilization. Consequently, Black patients had to be significantly sicker than White patients to receive the same level of care."}, {"title": "2.3 Strategies to Ensure Fairness in Machine Learning Models for Healthcare", "content": "The pursuit of fairness in machine learning for healthcare applications is crucial, as algorithmic bias can lead to discriminatory outcomes that impact patient care. This section explores systematic approaches to detect and mitigate such biases.\nRecent legislation, such as the 2019 Algorithmic Accountability Act in the United States [MacCarthy, 2020], has begun to address these concerns by requiring companies to assess and rectify algorithmic biases. However, the complexity and proprietary nature of many algorithms pose challenges for independent evaluation.\nTo mitigate discriminatory bias, several strategies have been proposed:\n\u2022 Careful selection and representation of protected groups in the data.\n\u2022 Thorough investigation of potential healthcare disparities in historical data.\n\u2022 Incorporation of fairness goals into model training.\n\u2022 Continuous evaluation of fairness metrics and model performance across groups.\n\u2022 Vigilant monitoring of data and model reassessment during deployment.\nFurthermore, several fairness metrics have been proposed [Beutel et al., 2019, Majumder et al., 2023]. Calibration techniques, applied within protected subgroups, can help identify algorithmic bias. Quantifying calibration bias within protected subgroups and assessing statistical parity are essential steps in evaluating model fairness. However, it is crucial to recognize that statistical parity may not be clinically meaningful when disease prevalence differs between subgroups. Other important metrics include independence, separation, and sufficiency [Carey and Wu, 2023]. Independence aims for classifier scores to be independent of group membership, while separation focuses on the independence of scores and sensitive variables conditional on the target variable. Sufficiency examines the independence of the target and sensitive variables given a particular score.\nIt is important to note that these fairness criteria cannot all be satisfied simultaneously when risk prevalence differs across groups. This highlights the need for careful consideration of trade-offs in AI model development. Therefore, further key objectives for fair decision-making in healthcare are defined that include achieving equal patient outcomes, equal model performance, and equal resource allocation across protected groups. However, these goals may sometimes conflict, necessitating thoughtful prioritization and stakeholder involvement in the design process.\nIn conclusion, ensuring fairness in machine learning models for healthcare is an ongoing challenge that requires continued research, vigilance, and collaboration among stakeholders. By addressing these issues systematically, we can work towards more equitable and effective healthcare systems that benefit all patients, regardless of their demographic characteristics."}, {"title": "3 Explainability as a central component of Human-Centered CDSS", "content": ""}, {"title": "3.1 Interpretability vs Explainability", "content": "Interpretability and explainability, while often used interchangeably, have distinct meanings in machine learning. Interpretability is an inherent model property, whereas explainability involves methods to elucidate non-interpretable models. Figure 8 illustrates a network assessing patient risk based on factors like BMI, age, smoking habits, alcohol consumption, and blood pressure. Consider an 85-year-old female patient with a BMI of 32, high blood pressure, and no smoking or alcohol use. The system labels her 'at risk' recommending medication. However, this output alone may not suffice for a doctor to trust and act upon the model's decision. Understanding the reasoning behind the model's output becomes crucial, highlighting the importance of explainability in complex models, especially in critical domains like healthcare where comprehending the decision-making process is essential for informed and ethical treatment decisions.\nExplainability in machine learning models can address crucial questions about a model's performance, success conditions, and decision factors. For example, consider that in heart failure prediction age is a significant factor, with individuals over 60 years having an estimated likelihood of 60%. Furthermore, a BMI exceeding 25 increases risk by 20%, as does smoking for over a decade. High blood pressure is also correlated with heart failure. An explainable model should identify these key input factors and quantify their impact on the decision. This insight into the model's underlying function aids result interpretation, clarifies decision-making processes, and helps understand model failures in noisy conditions. Essentially, explainability provides transparency into the model's inner workings, enabling users to comprehend not just what the model predicts, but why it makes those predictions.\nDecision trees are widely regarded as highly interpretable machine learning models due to their transparent and intuitive structure as the example at Figure 9 shows. The tree-like representation clearly illustrates the decision-making process, with each node representing a specific decision point based on a particular feature. This allows users to easily follow the path from root to leaf, understanding how the model arrives at its predictions. The clear criteria used at each node for splitting the data provide insight into precisely how decisions are made at each step. This hierarchical nature lends itself well to visual representation, making it easier for both experts and non-specialists to grasp the model's logic. For any given prediction, one can trace the exact path through the tree, observing which features were considered and how they influenced the final output. This traceability enhances accountability and facilitates debugging. Moreover, the structure of the tree inherently reveals which features are most important for classification or regression, as they appear closer to the root and in more splits. Unlike more complex models, decision trees can be explained to individuals without a background in machine learning or statistics, facilitating communication between data scientists and stakeholders. However, it is important to note that interpretability can decrease as tree depth increases. Very deep trees may become more challenging to interpret, potentially approaching the complexity of \"black box\" models.\nFigure 10 provides a simplified overview of interpretable and explainable models. On the left, we see inherently interpretable models such as decision trees, linear regression, and logistic regression. These models have been widely used in clinical practice and decision-making due to their simple construction and easily understandable results. The straightforward nature of these models allows practitioners to readily interpret their outputs and understand the reasoning behind decisions. Consequently, these models do not require additional methods to explain their results, as their decision-making process is transparent by design. This intrinsic interpretability distinguishes them from more complex models that may require additional explanation techniques to elucidate their outputs."}, {"title": "3.2 'Explainability' in Healthcare Applications", "content": "In healthcare applications, explainability is particularly critical for assessing model stability, visualizing relationships affecting outcomes, and enabling ethical analysis, especially concerning minority groups. It also facilitates patient involvement in decision-making processes and allows for the evaluation of privacy risks associated with complex model representations. This transparency is essential for maintaining the confidence of healthcare professionals, patients, and end-users. Moreover, explainability aids in monitoring model performance over time, as data distributions may shift and affect outcomes.\nThe significance of explainability extends to various stakeholders. Clinicians require trustworthy models that contribute to scientific knowledge. Patients need assurance of fair treatment and absence of hidden biases. Data scientists and developers utilize explainable models for debugging and improving product efficiency. Management must ensure regulatory compliance, while regulatory bodies need to certify model adherence to legislation.\nExplainability is associated with several key objectives, including trustworthiness, causality inference, transferability, informativeness, confidence, fairness, accessibility, interactivity, and privacy awareness. While an explainable model may not guarantee absolute trust or prove causality, it can provide valuable insights into potential causal relationships and help validate results from other inference techniques."}, {"title": "3.3 Taxonomy of Explainability Methods", "content": "Some of the most common categorisations of explainability methods are local versus global, model agnostic versus model specific, data modality specific versus data modality agnostic and ad-hoc versus post-hoc explanations [Arrieta et al., 2020, Molnar, 2020, Stiglic et al., 2020]. Local versus global explanation is a very common distinction that translates on whether we get an explanation that relates to the overall function of the model, or an explanation related to"}, {"title": "3.4 Evaluating Explainability in Clinical Decision Support Systems", "content": "The evaluation of explainability methods in clinical decision support systems is a crucial aspect of their development and deployment. This process considers not only the technical aspects of the explanations but also their effectiveness for end-users, whether they are healthcare providers, clinicians, patients or carers.\nExplanations typically provide three types of information: the importance of features or attributes to the model, including their interactions; the reasoning behind specific predictions; and an approximation of the complex model using a simpler, interpretable surrogate model such as rule-based systems, decision trees, or linear models.\nThe evaluation of explanations involves assessing both the model's intrinsic interpretability and the quality of its approximation by interpretable explanations. Key aspects of this assessment include clarity (consistency of rationale for similar instances), parsimony (complexity and compactness of the explanation), fidelity (accuracy in describing the task model), and soundness (truthfulness to the task model).\nAttribution-based explanations, which identify the input features most relevant to the model's decision, are common in post hoc explanation methods. While these explanations may not fully satisfy the sufficiency property, they often meet the parsimony criterion if the features are understandable to humans. For clinicians, such explanations are valuable in comparing the model's decision-making process with their own clinical knowledge.\nUser-based evaluation, both quantitative and qualitative, is crucial in understanding how trust in AI models affects overall system performance in human-in-the-loop scenarios. This approach bridges the gap between technical performance and practical utility in clinical settings, ensuring that explainable AI systems not only perform well mathematically but also integrate effectively into clinical workflows and decision-making processes. Toward this end, quantitative evaluations often utilize metrics based on questionnaires assessing the usefulness, satisfaction, and interest provided by the system's explanations. They may also measure human-machine task performance in terms of accuracy and response time."}, {"title": "4 Privacy Concerns in Clinical Decision Support Systems: A brief overview", "content": ""}, {"title": "4.1 Leakage and privacy concerns of deep learning models", "content": "The integration of machine learning in healthcare applications, particularly in clinical decision support systems (CDSS), necessitates understanding and addressing significant privacy concerns. Advanced machine learning models can memorise and inadvertently expose sensitive information, even when identity data has been removed or pseudo-anonymized. Furthermore, with state-of-the-art sensing technologies, it is now possible to monitor people 24/7 at home for healthcare applications [Yang et al., 2023]. This capability introduces new privacy challenges, as information about people's activities and physiology can be leaked in real-time [Zakariyya et al., 2024]. For example, wifi and radar-based human motion sensing has gained significant attention for offering unobtrusive observation in sensitive environments like assisted living facilities, hospitals, and residential settings. However, emerging research has uncovered a critical privacy concern: advanced radar systems can now accurately identify individuals through their unique walking patterns. These findings challenge the initial assumption of radar sensing as a privacy-preserving technology, highlighting the urgent need for robust privacy protection mechanisms in sensing systems, even when the raw data may not be visually comprehensible to humans. This section explores the inherent risks in deep learning models and discusses strategies to mitigate privacy threats in the context of healthcare applications.\nDeep learning models excel at processing complex, correlated sensing data, which has led to substantial improvements in fields such as computer vision and information retrieval. However, these models inadvertently memorize training data within their weights, making it possible to reconstruct parts of the original dataset from the algorithm itself [Hartley et al., 2023]. Traditional anonymization techniques, such as removing personally identifiable information or using pseudo-anonymization, are insufficient protection against sophisticated privacy attacks against deep neural networks. These powerful algorithms can determine an individual's identity by exploiting similarities to other datasets or inferring information from remaining data points. This capability has led to large-scale re-identification attacks.\nPrivacy threats in machine learning can manifest in various sophisticated ways, even with limited model access. For example, memorisation of the training data might be reflected in assigning high likelihood to specific input samples. An attacker might gain insights into a model through its architecture, weights, training data, or even just its output logits. In the context of membership inference attacks (MIA), adversaries can potentially determine whether a specific data record was part of the original training dataset. The attack typically involves an attacker model that learns to predict the outcomes of the target model, with the goal of reconstructing sensitive input data."}, {"title": "4.2 Defenses against privacy attacks", "content": "Privacy preservation in healthcare machine learning applications is crucial, both at the data and model levels. This section explores various approaches to protect patient privacy, contrasting centralized and federated learning methods."}, {"title": "4.2.1 Differential Privacy Against privacy attacks", "content": "Differential Privacy (DP) is a mathematical framework designed to protect sensitive data used in training AI models. DP algorithms mitigate the risk of data leakage by introducing calibrated noise into computations. This is particularly valuable when handling sensitive information, such as healthcare data, in the development of decision support systems. The procedure involves adding noise based on two privacy budget parameters, \u20ac and 8 [Dwork et al., 2014]. The parameter \u20ac controls the amount of noise added, while & defines the probabil- ity of the mechanism failing to maintain privacy. Stronger privacy guarantees are achieved with smaller values of e and \u03b4.\nDefinition 1: A randomized function F provides pure e-DP if for all neighbouring input datasets D1 and D2 differing on at most one element and \u2200S \u2286 Range(F) [Dwork et al., 2014], satisfying equation 4 below, where Pr in equations 4 and 5 represents a probability measure.\n$Pr[F(D\u2081) \u2208 S] < e Pr[F(D2) \u2208 S]$ (4)\nFor (\u03b5, \u03b4)-DP, the guarantee is relaxed as follows:\n$Pr[F(D1) \u2208 S] < e Pr[F(D2) \u2208 S] + \u03b4$ (5)\nThus, when \u03b4 = 0, the relaxed guarantee simplifies to the pure e-DP condition. The noise introduced can follow either a Gaussian or Laplace distribution, depending on the desired balance between privacy guarantees and utility. DP techniques are extensively utilized to develop robust machine learning models that protect training data from leakage, especially in the presence of MIA. In this context, noise can be introduced to either the data or the model gradients during training.\nDP methods that focus on input data features often rely on the addition of Laplace noise [Phan et al., 2017, Fujimoto et al., 2023, Zakariyya et al., 2024]. Proper implementation of additive noise injection on specific features effectively safeguards sensitive data against MIA [Zakariyya et al., 2024].\nAnother widely adopted mechanism for creating DP-compliant models is applying (\u03b5, \u03b4)-DP to model gradients during training [Abadi et al., 2016, Dupuy et al., 2022, Boenisch et al., 2024]. This approach involves clipping gradients and adding Gaussian noise proportional to e at each training iteration. The clipping norm, a fixed threshold (C), bounds the gradient magnitudes for individual data points, managing the effect of large gradients [Kong and Munoz Medina, 2024]."}, {"title": "4.2.2 Federated Learning and Defenses Against Privacy Attacks", "content": "Traditionally", "2022": ".", "2017": "."}, {"2017": ".", "iterative": "during each communication round, the server interacts with clients to update the global model until convergence is achieved.\nFederated learning allows data to remain with its owners while still enabling collaborative algorithm training. However, it can be communication and memory intensive. Additionally, federated learning alone does not inherently guarantee security and privacy, necessitating additional protective measures. Both centralized and edge models are vulnerable to privacy and adversarial attacks [Kaissis et al., 2024, Song et al., 2020, Kumar et al., 2023", "forgetting\"), which aims to remove specific data without retraining the entire model. This approach, while conceptually appealing, faces challenges in implementation and verification. Adversarial defenses, such as adding targeted noise to confidence score vectors, have been proposed to protect against MIA, though they lack theoretical privacy guarantees [Jia et al., 2019": "."}]}