{"title": "From Words to Worlds: Compositionality for Cognitive Architectures", "authors": ["Ruchira Dhar", "Anders S\u00f8gaard"], "abstract": "Large language models (LLMs) are very performant connectionist systems, but do they exhibit more compositionality? More importantly, is that part of why they perform so well? We present empirical analyses across four LLM families (12 models) and three task categories, including a novel task introduced below. Our findings reveal a nuanced relationship in learning of compositional strategies by LLMs - while scaling enhances compositional abilities, instruction tuning often has a reverse effect. Such disparity brings forth some open issues regarding the development and improvement of large language models in alignment with human cognitive capacities.", "sections": [{"title": "1. Introduction", "content": "Compositionality is a widely studied aspect of human cognition. Fodor & Pylyshyn (1988) claimed that non-symbolic connectionist representations were inadequate for compositional understanding. The question turns on whether compositionality is acquired (Smolensky, 1987; Chalmers, 1993), or whether compositionality is merely a functional property (Van Gelder, 1990). Symons & Calvo (2014), building on the arguments against connectionism in (Fodor & Pylyshyn, 1988), argue that even if connectionist systems can stumble across an implementation of compositionality, this does not explain systematicity in their behaviour nor does it render them suitable cognitive architectures (see Appendix A for related work and Appendix B for further details). To serve as models of cognition or 'cognitive architectures', connectionist systems should ideally: i) be compositional, i.e., have compositional representations and behaviour. ii) be compositional in a way that explains their behavior and performance, i.e., they should learn compositional strategies as a way to improve performance. LLMs are now increasingly seen as possible models of human language (Mahowald et al., 2024; Hu et al., 2024) or cognition (Kauf et al., 2023; Hardy et al., 2023; Marjieh et al., 2023; Lamprinidis, 2023; Aw et al., 2023), and it is therefore crucial to review the Fodor & Pylyshyn (1988) challenge from the perspective of LLMs. While there is work on measuring compositional abilities of LLMs (Dziri et al., 2024; Li et al., 2024; Zhang et al., 2024; Wang et al., 2024), our focus is not to benchmark models for compositionality, but to examine its explanatory value in predicting performance and validating models as cognitive architectures. Scaling and instruction tuning are widely assumed to improve model alignment and generalization performance across a multitude of tasks ranging from natural language inference and textual entailment (Wei et al., 2022) to MMLU and BigBench (Longpre et al., 2023) - but are these improved performances a result of improved compositionality? Focusing on the domain of adjective-noun (Adj-N) composition, we propose three task types that can evaluate different aspects of compositional behaviour in LLMs and consider the impact of model size and instruction tuning on the compositional behaviour of such models. Finally, we discuss the importance of compositionality as a theoretical construct in validating connectionist cognitive architectures."}, {"title": "2. Measuring Compositionality", "content": "Over the years, several benchmarks have been developed to test compositionality of neural network models - SCAN (Lake & Baroni, 2018), Lookup Table Composition (Liska et al., 2018), COGS (Kim & Linzen, 2020), and PCFG Set (Hupkes et al., 2020). However, we face a few issues when trying to leverage such datasets for testing today's LLMs models pretrained on large amounts of text: a) They are based on a train-test paradigm that is not easily applicable pretrained LLMs. b) LLMs are trained on very large quantities of texts and may, as a consequence, have seen the test set expressions before. c) There is no congruence on what aspects of compositionality (Sun et al., 2023b) we test with these methods. Some work considers compositional multi-hop reasoning (Lu et al., 2024; Xu et al., 2024; Dziri et al., 2024; Shao et al., 2022), but we focus on meaning construction from constituent representations. We take inspiration from Hupkes et al. (2020)'s tripartite distinction between aspects of compositionality, and introduce a test for each such aspect: Substitutivity: This involves the ability to understand the relatedness of words such that substituting a synonym in a complex expression should not be taken to alter the meaning of the complex expression. We test this with the ANTAILS Dataset, largely based on the AddOne Dataset (Pavlick & Callison-Burch, 2016). For a given sentence with a noun (N) like The runner set a record, we substitute N with an adjective-noun combination like The runner set a new record and test the model to see whether it can understand the entailment pattern. The model here has to maintain it's understanding of entailment patterns with adjective substitution. Systematicity & Globalism: This involves the ability to recombine known parts and rules and being able to productively use the parts in new contexts where constituents can have different behaviours (Carnap, 1988). We test this with the PLANE Dataset proposed by Bertolini et al. (2022) that tests adjective-noun entailment in a situation where the entailment pattern for an $\\mathrm{AN}-\\mathrm{N}$ and $\\mathrm{AN}-\\mathrm{H}$ (where AN is the adjective-noun combination, N is the noun and H is a hypernym of N ) combination is already given and the model is tested on entailment of $\\mathrm{AN}-\\mathrm{AH}$ combination. This requires the model to employ systematicity (since the AN entailment pattern needs to be recombined in the AN - AH statement) and also globalism ( since the entailment pattern of the $\\mathrm{AN}-\\mathrm{AH}$ combination needs to be inferred differently from the $\\mathrm{AN}-\\mathrm{N}$ and $\\mathrm{AN}-\\mathrm{H}$ combinations). Over-generalization: This involves the ability to distinguish between compositional and non-compositional phenomena by measuring the distance of adjective-noun combinations vs exocentric compounds. We test this with a new task type using a handcrafted toy dataset- the COMPCOMB Dataset - which is a novel contribution of this work. Each data point consists of a triple - a noun, an adjective that goes with the noun, and an exocentric compound which contains the noun. For example, (coat, trenchcoat and turncoat)- when we take the word 'coat', we know that 'trenchcoat' ( a special type of coat) is closely related to it but the exocentric compound 'turncoat' (a betrayer) is not since it is semantically different. This tests over-generalization since the model needs to be able to distinguish between genuine compounds and combinations by avoiding generalization on the basis of surface forms."}, {"title": "3. Evaluating Models", "content": "Our aim is to determine whether models' compositional abilities can indicate their trends of performance. There are two types of changes that have been shown to consistently impact the performance of models: Scaling Parameters: Research on LLM scaling laws - Kaplan et al. (2020) and Hoffmann et al. (2022) - show that model performance for large language models get better with size i.e an increase in the number of parameters. Instruction Tuning: Several works (Wei et al., 2022; Ouyang et al., 2022; Chung et al., 2024) have shown the advantage of instruction finetuning (IFT) as a method to improve general performance of LLMs, especially for generalization to unseen tasks and alignment with human behaviour. Can these changes in performance of LLMs be explained by their compositional behaviour? To investigate this, we conduct analysis and evaluation across 4 families of models - Falcon (Almazrouei et al., 2023), LLama (Touvron et al., 2023), Codellama (Roziere et al., 2023), and Mistral (Jiang et al., 2023)."}, {"title": "3.1. ANTAILS Dataset", "content": "For this dataset, we test 3 models for each model family the base model of 7B (Base), an instruction tuned version of the same (IFT), and a larger model - with two different kinds of setups: one involving a two-choice question scenario where we determine accuracy by fixed rank precision ( $\\mathrm{P} @ \\mathrm{k}$ ) to evaluate the model output (Setup 1) and another in which we use the log probabilities of the model for two completions (entails vs does not entail) as an indication of the model's judgement (Setup 2). Furthermore, for both setups we include two prompt variations for the evaluation and the result table shows the average accuracy for each model across the prompt variations. Results: We observe that for all families of models, the Larger Model always performs better than the Base Model. However, the impact of instruction tuning is inconsistent with performance decreasing for the two models, remaining the same for Codellama, and increasing for Llama."}, {"title": "3.2. PLANE Dataset", "content": "For this dataset, we also have a setup that is exactly similar to the previous one. Since the dataset is divided by types of adjectives, we also present the results classified by the different adjectival categories. Results: Similar to the ANTAILS dataset, we observe in Figure 3 that the overall model performance, within a model family, across two setups improves with size and worsens with instruction tuning. However, in the case of within-family comparison in the Codellama family of models, the larger model (13B) is worse than the base (7B) indicating that training a general LM with code and scaling it might not always have positive impacts on compositional reasoning. Similar trends were also observed by MA et al. (2024), where introduction of code at pretraining stage gives worse performance in logical reasoning tasks. Figure 4 shows the comparative analysis of results across different adjective classes - I (Intersective), N (Subsective), and O (Intensional). Most models perform worse for subsective adjectives. Interestingly, Redolfi & Melloni (2024) notes that children also acquire subsectives the slowest during the period of language acquisition."}, {"title": "3.3. COMPCOMB Dataset", "content": "For each datapoint this dataset, we evaluate the accuracy of model in terms of comparative cosine distance analysis of the embeddings/hidden states of models. If the dist(N, AN) ; dist ( $\\mathrm{N}, \\mathrm{H})$ we consider the model accurate since it is able to capture the semantic similarity of $\\mathrm{N}-\\mathrm{AN}$ as compared to $\\mathrm{N}-\\mathrm{H}$. We do this for two types of embedding for each model - for Setup 1, we use the initial embeddings from the embedding layer (EL) while in Setup 2, we access the last hidden state of the model (LHS). Results: The accuracy of models across all families increases with size. In Figure 5, we notice that while the embedding layer still shows over-generalization for larger models, the last hidden state representation has much better performance. For instruction tuned models, the performance of the embedding layer varies."}, {"title": "4. Conclusion", "content": "Cognitive architectures should arguably be performant and exhibit compositionality, and the induction of compositional strategies should be explanatory of their performance. LLMs, as candidate cognitive architectures, are clearly per-formant, behave compositionally (as seen by their performance on the ANTAILS and PLANE datasets), and their representations appear compositional (as seen by their performance on the COMPCOMB dataset). However, when it comes to how explanatory the induction of compositional strategies are of performance improvements, we observe different patterns for different LLMs: 1) Scaling models improves their generalization capabilities (Hendrycks et al., 2020; Desai & Durrett, 2020) and overall performance (Kaplan et al., 2020; Hoffmann et al., 2022). Compositional behaviour also improves with scaling across model families. This could indicate that the induction of compositional strategies is explanatory of improvements with scaling. 2) Instructing finetuning has been shown to improve alignment and result in performance gains across several task types (Wei et al., 2022; Ouyang et al., 2022). However, we see that compositional performance does not always improve with instruction tuning. Performance gains from instruction tuning do not correlate with improved compositional behaviour. In sum, while scaling often leads to more compositional models, instruction tuning does not show similar trends. Recent work (Ghosh et al., 2024) has shown that instruction tuning sometimes degrades performance. Our results indicate that one source of error may be reduced compositionality. Performance is multi-faceted, and compositionality may be explanatory of some performance gains, not others. If we think cognitive architectures should learn compositional strategies (Fodor & Pylyshyn, 1988; Symons & Calvo, 2014), and that LLMs could potentially be cognitive architectures (Lamprinidis, 2023; Sumers et al., 2023; Zhao et al., 2023), we must evaluate if the compositionality of LLMs is explanatory of their performance and be precise about what (relevant) performance is at play. This work is, to the best of our knowledge, the first step in that direction."}]}