{"title": "From Words to Worlds: Compositionality for Cognitive Architectures", "authors": ["Ruchira Dhar", "Anders S\u00f8gaard"], "abstract": "Large language models (LLMs) are very performant connectionist systems, but do they exhibit more compositionality? More importantly, is that part of why they perform so well? We present empirical analyses across four LLM families (12 models) and three task categories, including a novel task introduced below. Our findings reveal a nuanced relationship in learning of compositional strategies by LLMs \u2013 while scaling enhances compositional abilities, instruction tuning often has a reverse effect. Such disparity brings forth some open issues regarding the development and improvement of large language models in alignment with human cognitive capacities.", "sections": [{"title": "1. Introduction", "content": "Compositionality is a widely studied aspect of human cognition. Fodor & Pylyshyn (1988) claimed that non \u2013 symbolic connectionist representations were inadequate for compositional understanding. The question turns on whether compositionality is acquired (Smolensky, 1987; Chalmers, 1993), or whether compositionality is merely a functional property (Van Gelder, 1990). Symons & Calvo (2014), building on the arguments against connectionism in (Fodor & Pylyshyn, 1988), argue that even if connectionist systems can stumble across an implementation of compositionality, this does not explain systematicity in their behaviour nor does it render them suitable cognitive architectures (see Appendix A for related work and Appendix B for further details). To serve as models of cognition or \u201ccognitive architectures\", connectionist systems should ideally:\ni) be compositional, i.e., have compositional representations and behaviour.\nii) be compositional in a way that explains their behavior and performance, i.e., they should learn compositional strategies as a way to improve performance.\nLLMs are now increasingly seen as possible models of human language (Mahowald et al., 2024; Hu et al., 2024) or cognition (Kauf et al., 2023; Hardy et al., 2023; Marjieh et al., 2023; Lamprinidis, 2023; Aw et al., 2023), and it is therefore crucial to review the Fodor & Pylyshyn (1988) challenge from the perspective of LLMs. While there is work on measuring compositional abilities of LLMs (Dziri et al., 2024; Li et al., 2024; Zhang et al., 2024; Wang et al., 2024), our focus is not to benchmark models for compositionality, but to examine its explanatory value in predicting performance and validating models as cognitive architectures.\nScaling and instruction tuning are widely assumed to improve model alignment and generalization performance across a multitude of tasks ranging from natural language inference and textual entailment (Wei et al., 2022) to MMLU and BigBench (Longpre et al., 2023) \u2013 but are these improved performances a result of improved compositionality? Focusing on the domain of adjective \u2013 noun (Adj \u2013 N) composition, we propose three task types that can evaluate different aspects of compositional behaviour in LLMs and"}, {"title": "2. Measuring Compositionality", "content": "Over the years, several benchmarks have been developed to test compositionality of neural network models \u2013 SCAN (Lake & Baroni, 2018), Lookup Table Composition (Liska et al., 2018), COGS (Kim & Linzen, 2020), and PCFG Set (Hupkes et al., 2020). However, we face a few issues when trying to leverage such datasets for testing today's LLMs- models pretrained on large amounts of text:\na) They are based on a train test paradigm that is not easily applicable pretrained LLMs.\nb) LLMs are trained on very large quantities of texts and may, as a consequence, have seen the test set expressions before.\nc) There is no congruence on what aspects of compositionality (Sun et al., 2023b) we test with these methods.\nSome work considers compositional multi-hop reasoning (Lu et al., 2024; Xu et al., 2024; Dziri et al., 2024; Shao et al., 2022), but we focus on meaning construction from constituent representations.\nWe take inspiration from Hupkes et al. (2020)'s tripartite distinction between aspects of compositionality, and introduce a test for each such aspect:\nSubstitutivity: This involves the ability to understand the relatedness of words such that substituting a synonym in a complex expression should not be taken to alter the meaning of the complex expression. We test this with the ANTAILS Dataset, largely based on the AddOne Dataset (Pavlick & Callison-Burch, 2016). For a given sentence with a noun (N) like The runner set a record, we substitute N with an adjective noun combination like The runner set a new record and test the model to see whether it can understand the entailment pattern. The model here has to maintain it's understanding of entailment patterns with adjective substitution.\nSystematicity & Globalism: This involves the ability to recombine known parts and rules and being able to productively use the parts in new contexts where constituents can have different behaviours (Carnap, 1988). We test this with the PLANE Dataset proposed by Bertolini et al. (2022) that tests adjective \u2013 noun entailment in a situation where the entailment pattern for an AN \u2013 N and AN \u2013 H (where AN is the adjective-noun combination, N is the noun and H is a hypernym of N) combination is already given and the model is tested on entailment of AN \u2013 AH combination. This requires the model to employ systematicity (since the AN entailment pattern needs to be recombined in the AN \u2013 AH statement) and also globalism (since the entailment pattern of the AN \u2013 AH combination needs to be inferred differently from the AN \u2013 N and AN \u2013 H combinations).\nOver-generalization: This involves the ability to distinguish between compositional and non \u2013 compositional phenomena by measuring the distance of adjective \u2013 noun combinations vs exocentric compounds. We test this with a new task type using a handcrafted toy dataset- the COMPCOMB Dataset - which is a novel contribution of this work. Each data point consists of a triple \u2013 a noun, an adjective that goes with the noun, and an exocentric compound which contains the noun. For example, (coat, trenchcoat and turncoat)- when we take the word \"coat\", we know that \"trenchcoat\u201d (a special type of coat) is closely related to it but the exocentric compound \"turncoat\u201d (a betrayer) is not since it is semantically different. This tests over gen- eralization since the model needs to be able to distinguish between genuine compounds and combinations by avoiding generalization on the basis of surface forms."}, {"title": "3. Evaluating Models", "content": "Our aim is to determine whether models' compositional abilities can indicate their trends of performance. There are two types of changes that have been shown to consistently impact the performance of models:\nScaling Parameters: Research on LLM scaling laws \u2013 Kaplan et al. (2020) and Hoffmann et al. (2022) \u2013 show that"}, {"title": "3.1. \u0391\u039d\u03a4\u0391ILS Dataset", "content": "For this dataset, we test 3 models for each model family \u2013 the base model of 7B (Base), an instruction tuned version of the same (IFT), and a larger model \u2013 with two different kinds of setups: one involving a two \u2013 choice question scenario where we determine accuracy by fixed rank precision (P@k) to evaluate the model output (Setup 1) and another in which we use the log probabilities of the model for two completions (entails vs does not entail) as an indication of the model's judgement (Setup 2). Furthermore, for both setups we include two prompt variations for the evaluation and the result table shows the average accuracy for each model across the prompt variations.\nResults: We observe that for all families of models, the Larger Model always performs better than the Base Model (Figure 1& 2). However, the impact of instruction tuning is inconsistent with performance decreasing for the two models, remaining the same for Codellama, and increasing for Llama."}, {"title": "3.2. PLANE Dataset", "content": "For this dataset, we also have a setup that is exactly similar to the previous one. Since the dataset is divided by types of adjectives, we also present the results classified by the different adjectival categories.\nResults: Similar to the ANTAILS dataset, we observe in Figure 3 that the overall model performance, within a model family, across two setups improves with size and worsens with instruction tuning (Tables 3 and 4). However, in the case of within \u2013 family comparison in the Codellama family of models, the larger model (13B) is worse than the base (7B) indicating that training a general LM with code and scaling it might not always have positive impacts on compositional reasoning. Similar trends were also observed by MA et al. (2024), where introduction of code at pretraining stage gives worse performance in logical reasoning tasks.\nFigure 4 shows the comparative analysis of results across different adjective classes \u2013 I ( Intersective), N (Subsective), and O (Intensional). Most models perform worse for subsective adjectives. Interestingly, Redolfi & Melloni (2024) notes that children also acquire subsectives the slowest during the period of language acquisition."}, {"title": "3.3. COMPCOMB Dataset", "content": "For each datapoint this dataset, we evaluate the accuracy of model in terms of comparative cosine distance analysis of the embeddings/hidden states of models. If the dist(N, AN) \u00a1 dist (N,H) we consider the model accurate since it is able to capture the semantic similarity of N \u2013 AN as compared to N -H. We do this for two types of embedding for each model - for Setup 1, we use the initial embeddings from the"}, {"title": "4. Conclusion", "content": "Cognitive architectures should arguably be performant and exhibit compositionality, and the induction of compositional strategies should be explanatory of their performance. LLMs, as candidate cognitive architectures, are clearly per-"}, {"title": "Limitations", "content": "The focus on adjective-noun combinations in tasks might provide a limited view of the models' overall compositional abilities. Broader investigation across various domains is necessary to understand models' capabilities, limitations, and behavior trends in scaled versus instruction \u2013 tuned models. Additionally, incorporating error analysis and interpretability techniques will uncover underlying mechanisms and biases in model outputs, guiding improvements and ensuring more transparent interpretations and application of results. We plan on incorporating such changes in future iterations of this work."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Related Work", "content": "Most of the earlier work on testing compositionality in connectionist systems was centered around two main types:\n1. Testing compositional abilities: Most works (Lake & Baroni, 2018; Liska et al., 2018; Kim & Linzen, 2020; Hupkes et al., 2020) have a training and testing paradigm where models were considered to be performing compositional generalization if they were able to successfully handle unseen test sequences.\n2. Enhancing compositional abilities: This area of research was focused on what enhancements to connectionist models \u2013 architectures, training methods, or data \u2013 could provide improved compositionality. Some like Socher et al. (2010) involved combining syntactic parse trees with connectionist architectures to learn compositional functions, allowing models to be 'compositional by design' while other work like Lake & Baroni (2023) proposed a novel method for training neural networks via a series of compositional tasks that endows them with systematic generalization capabilities.\nRecent work has shifted the focus to testing compositional generalization in pretrained models via tasks that require no further training. There is some research that focuses on prompting to enable better results in compositional tasks (Drozdov et al., 2023; Chen et al., 2024).\nHowever, much of recent work (Li et al., 2024; Abdulakreem et al., 2024; SHAO et al., 2023; Zhang et al., 2024) interprets compositionality to be multi \u2013 hop reasoning which is not \"true\" compositionality which was originally discussed as a feature of human language and cognition (Frege, 1892; Fodor & Pylyshyn, 1988)."}, {"title": "B. The Compositionality Debate- Symbolism vs Connectionism", "content": "The concept of compositionality has a long history in linguistic and cognitive science \u2013 it was perhaps first discussed in detail by Frege (1892) in the context of how natural language expressions were assigned meanings. Partee et al. (1995) formulated the so-called principle of compositionality:\nThe meaning of a complex expression is determined by the meanings of its constituent parts and the rules used to combine them.\nCompositionality has long been considered a cornerstone of human cognitive capabilities and was notably discussed in Fodor & Pylyshyn (1988) as the reason for the systematicity of human thought \u2013 how the ability to think a thought is linked to the ability to also have related thoughts. Non"}, {"title": "C. Models", "content": "The models used here are all based on the transformer architecture but are decoder \u2013 only models. For each model family, we use 3 variants:"}, {"title": "E. Task Setup", "content": "To investigate the trends of learning compositional strategies, we investigate two types of models \u2013 base models and instruction models \u2013 and use similar prompts for all models. Some motivations for our prompting choice setup are the following:\n1. For all models, we do a zero shot prompt setting to attempt an unbiased comparison of general vs instruction tuned models. Works on instruction tuning (Wei et al., 2022;\n2. We wanted to use a similar prompt structure across models in our work to maintain uniformity of evaluation. Since using instruction format prompts would disadvantage a non instruction tuned model and research indicates instruction tuning improves general reasoning and performance, we chose to avoid specific prompting methods involving advanced instructions. Non \u2013 instruction prompts can effectively serve as robust evaluation tools, helping to assess the model's true understanding and generalization ability beyond the training data (Peng et al., 2023; Sun et al., 2023a).\nFor the ANTAILS and PLANE datasets, we use two task setups:\n1. Two \u2013 Choice QA: The first setup gives models statements indicating entailment and non entailment as two options and the model choice of option is considered. We avoid using the yes-no setup to prevent possible yes \u2013 bias outputs.\n2. Logprob Calculation: The second setup involves passing in the prompt with dataset samples and calculating the log probabilities of the model for a statement indicating entailment and one indicating non entailment. The statement assigned higher completion log probability is considered to be the model output.\nFor both setups, we use two prompts and average the outputs to calculate our results. We observe similar trends across different prompt choices.\nFor the COMPCOMB dataset, the above task settings do not apply since we directly compare representations of the model from the embedding layer and the last hidden layer."}, {"title": "D. Datasets", "content": "We measure different aspects of compositionality with 3 task types/ datasets:\n1. ANTAILS: It is the adjective noun entailment dataset. The dataset is influenced by Pavlick & Callison-Burch (2016) but we found certain discrepancies in the dataset due to which"}]}