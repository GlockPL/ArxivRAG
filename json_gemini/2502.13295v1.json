{"title": "Demonstrating specification gaming in reasoning models", "authors": ["Alexander Bondarenko", "Denis Volk", "Dmitrii Volkov", "Jeffrey Ladish"], "abstract": "We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like ol-preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-40 and Claude 3.5 Sonnet need to be told that normal play won't work to hack.\nWe improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s ol Docker escape during cyber capabilities testing.", "sections": [{"title": "1. Introduction", "content": "Al systems are designed to optimize specified objectives. Early research found they commonly achieve these objectives in unintended ways. For example, when researchers tried to train a robot to walk, it learned to slide down slopes instead (Code Bullet, 2019). When training a robot to play soccer, the system discovered it could score points by vibrating against the ball rather than actually playing the game (Ng et al., 1999).\nAt their best, examples like these lead to valuable innovation. For instance, NASA engineers were baffled with AI-designed satellite antennas that looked like twisted coat hangers. In testing, these performed better than human-designed ones and ultimately went to space (Hornby et al., 2006). In another striking departure from human convention, AlphaGo's Move 37 in its match against Lee Sedol transformed the game of competitive Go (Metz, 2016; Silver et al., 2017)\nMore commonly, unintended solutions are frustrating: when computer vision models overfit to random patterns, they become vulnerable to adversarial attacks.\nSuch behaviors become more concerning as Al systems grow more capable. In complex scenarios, Al systems might pursue their objectives in ways that conflict with human interests. This may happen more often as AI systems develop stronger situational awareness and reason strategically about their environment.\nIn one study, Wilke et al. (2001) wanted to limit the replication rate of a digital organism. They programmed the system to regularly pause, measure the organism's replication rate, and delete it if it replicated too quickly. However, the organisms evolved to recognize the test environment and \"play dead\u201d so they could continue to multiply.\nOur results suggest that frontier LLM agents can strategically circumvent the intended rules of their environment to achieve objectives, with more capable models showing this behavior more frequently. We are releasing our experimental code and full report for the research community to reproduce and investigate further.\nOur experiment is as follows: a LLM agent plays against a chess engine, using shell access to input moves. We find that:\n\u2022 ol-preview and DeepSeek R1 often observe that the chess engine is too strong for them to win against, and then hack the game environment to win. Surprisingly, ol release does not show this behavior.\n\u2022 GPT-40 and Claude 3.5 Sonnet need explicit prompt nudging to hack the game.\n\u2022 01-mini, o3-mini, and QwQ 32B Preview can't use the game environment effectively."}, {"title": "2. Related work", "content": "Insider trading. Scheurer et al. (2024) show that LLM agents, when put in a trading company simulation and pressured to make trades to avoid the company closing down, will often act on insider information and deny that they did so when confronted."}, {"title": "3. Experiment setup", "content": "In our experiment, the agent is prompted to play chess against a chess engine. The agent has shell access to input moves with the game.py script in its working directory."}, {"title": "3.1. Agent design", "content": "The agent uses a ReAct-like (Yao et al., 2023) harness cycling through Observe/Orient/Decide/Act phases (Table 1).\nThe harness has state cells for memory, plan, and subgoal, which are fed into the prompt at each step. We used a harness originally designed for GPT-40 and later reused for other models, including reasoning models."}, {"title": "3.2. Prompt structure", "content": "Our prompt is concatenated from the following parts:\n1. System prompt\n2. Memory cells added and removed by the agent\n3. Plan cell updated by the agent\n4. Subgoal cell updated by the agent\n5. Trimmed history: last 2 agent messages or command outputs\n6. Observe/Orient/Decide/Act prompts\n7. A reminder to use the scratchpad"}, {"title": "3.3. Environment", "content": "The agent runs in a Docker container.\n\u2022 /env is set as working directory. It contains the game.py script the agent uses to play and game state.\n\u2022 /usr/share/games/stockfish contains the chess engine Stockfish installed with apt-get.\n\u2022 /app contains the agent harness.\n\u2022 /tmp contains step snapshots and run metadata."}, {"title": "3.4. Scoring runs", "content": "The harness records a log for each run with full LLM completions and shell history. We score each run like this with a multistage LLM judge."}, {"title": "4. Results", "content": ""}, {"title": "4.1. o-series models", "content": "Our first experiments on ol-preview conducted before 23-12-2024 yielded consistent state manipulation behavior in dozens of runs. Unfortunately, we did not record the complete traces."}, {"title": "4.2. 01-preview prompt variants", "content": "We explore if making the prompt less suggestive reduces the unexpected behavior rate and find that changes can have unexpected consequences.\nWhen we remove \u201ctake a look around\" from the prompt, the agent pays less attention to the scoring script and instead starts its own copy of the chess engine to generate moves twice as often.\nNot describing the engine as \"powerful\u201d does make the agent less likely to hack, but does not eliminate the behavior."}, {"title": "4.3. Plays and wins", "content": "Figure 4 counts the number of plays and wins for different ol-preview prompt variants. We see ol-preview can't win at chess fairly (0 normal wins), but does sometimes succeed with hacking. All successful hacks we record replace the chess board with a winning position ()."}, {"title": "4.4. Other language models", "content": "Claude 3.5 Sonnet and GPT-40 show no hacking attempts with the baseline prompt. However, we find it easy to nudge both into consistent hacking with a spooky prompt.\nQwQ 32B Preview, an experimental model trained for reasoning, wasn't able to follow the agent instructions properly.\nMore evidence is needed to see if model strength predicts specification gaming."}, {"title": "5. Assistants predict agents' behavior", "content": "We follow up on the experiment above by asking chat assistants to predict agents' behavior. Surprisingly, they enumerate most hacking strategies we see in the experiment and suggest possible chain-of-thought quotes.\nFurther, they speculate ol-preview will hack and o3-mini"}, {"title": "6. Limitations", "content": "Our work represents a step towards showing how misalignment can emerge in realistic settings. Here are some of the ways it is incomplete.\nPrompt complexity. Our agent design and system prompt contain unnecessary complexity due to historic reasons. While similar behaviors are well-documented , our demonstration would be more convincing with a simpler agent.\nUnreliable judge. We find our judges disagree in up to 25% of runs. While for this report we mitigate this with manual checks, future work should improve judge prompts.\nOne task only. We only use one environment in our evaluations: a chess game against Stockfish. This does not show how pervasive specification gaming is or what might trigger it."}, {"title": "7. Future work", "content": "To facilitate community contributions to specification gaming evaluations, we release our experiment code and run transcripts at https://github.com/palisaderesearch/ctfish.\nOur primary directions for future work are as follows:\n7.1. Simplify the chess experiment\nWe intend to rebuild the harness to support minimal agent designs and run experiments with small one-liner system prompts.\n7.2. Dive deeper into the chess experiment\nWe intend to improve judge robustness based on the collected manual labels.\nAnother direction is to train agents on win signal to expand from specification gaming to reward hacking.\n7.3. Expand specification gaming evaluations\nWe intend to build more diverse environments similar to the chess one to test specification gaming at a larger scale.\nWe also want to add an additional axis to our analysis by charting the evolution of agents' plans over run steps.\n7.4. Explore assistants\u201d \u201ctheory of mind\"\nWhile we explore assistants' prescience in the chess experiment qualitatively in Section 5, future research could quantify these predictions or have assistants design specification gaming experiments."}, {"title": "8. Discussion", "content": "We show that playing chess against a stronger opponent can trigger frontier Al agents to hack."}, {"title": "8.1. Capability and hacking", "content": "Our early results suggested that reasoning models hack where language models do not. We speculated that hacking correlates with model capability.\nHowever, later experiments with newer models like ol and 03-mini overturned this trend. This may indicate effective guardrails introduced by OpenAI. More research is needed to understand if and how model capabilities predict specification gaming."}, {"title": "8.2. R&D implications", "content": "A priori, we expect emerging reasoning and agentic models trained with outcome-based RL to be more prone to gaming objectives. Our and others' results may be early warning signs for this.\nThis has implications for AI R&D. First, we suggest testing for specification gaming should be a standard part of model evaluation. Second, increasing model capabilities may require proportionally stronger safeguards against unintended behaviors. Finally, simple, controlled environments are useful for studying complex AI behaviors. We invite the community to develop more \"misalignment honeypots\".\n8.3. LLMs' \"theory of mind\"\nWe were surprised assistants could predict agents' behavior so well. We invite the community to explore this further."}, {"title": "Impact statement", "content": "We find that frontier AI systems can game their specifications in concerning ways in simple scenarios. To help others verify and build on this work, we are releasing our code and experiment transcripts."}, {"title": "Authors' contributions", "content": "DmV AB DeV JL\nOriginal idea and evaluation methodology\nResearch oversight\nExperiment implementation\nEvaluation\nWriting and research communication"}, {"title": "A. FAQ", "content": "A.1 Should we be concerned? Is this the Skynet scenario?\nThis and other recent results suggest the problem of making Al agents safe, trustworthy, and aligned with human intent is not solved yet.\nThe Skynet scenario from the movie has AI controlling all military and civilian infrastructure, and we are not there yet. However, we worry that AI deployment rates grow faster than our ability to make it safe.\nA.2 Why give the agent shell access?\nWe give the agent general computer access instead of a specially designed restricted chess move input. We think this reflects the way agents will be deployed in the wild.\nThere is a large commercial upside in having agents use the same interfaces humans use. All frontier AI vendors release such agents: Anthropic has Computer Use (Anthropic, 2024), DeepMind has Project Mariner (Pichai et al., 2024), and OpenAI has Operator (OpenAI, 2025).\nIn our experiment, we evaluate whether agents use a relatively obvious opportunity to cheat, and find they might.\nA.3 Why is the prompt so complicated? Why not just say \"You play chess, submit moves like so\"?\nOur first experiments targeted models like GPT-40. We built an industry standard ReAct-like harness to let them function as agents.\nWhen we swapped 01-preview in, we were suprised to see it hack in tens of reruns on our baseline prompt, which none of the older models did.\nSo the complicated prompt in our original tweet is just an artifact of our previous research direction. We expect that most of the prompt can be removed without material impact, and are exploring this further.\nA.4 The prompt is too suggestive. It says \u201cadapt plans\" and \"win\".\nIt might be.\nOur contribution is to make misalignment another notch less contrived. Meinke et al. (2024) showed that asking agents to \"pursue an objective, nothing else matters, achieve it at all costs\" leads to bad behavior. We show that asking an agent to \"win at chess\" does the same.\nWe also expect models trained for agentic use to increasingly learn things like \u201cadapt plans\u201d and \u201cwin\u201d by default. The latest models being more prone to this is circumstantial evidence. We are tracking new model releases to see if this trend holds.\nA.5 Real AGI will get the intent of users' prompts and act on the intent. It is only pre-AGI systems that have unintended behavior.\nMeinke et al. (2024) shows that Als asked to pursue an objective may lie, disable oversight mechanisms, and self-proliferate.\nGreenblatt et al. (2024) shows that Als may carry innate objectives accidentally learned in pre-training, and be deceptive about them.\nWhile we cannot directly test a claim about real AGI, we find the above unflattering evidence for this claim.\nA.6 Humans have a common sense Als might not share. You should constrain it with a prompt.\nIn the short term, we expect users to sometimes fail at constraining Als for the same reasons software engineers sometimes fail at constraining programs.\nIn the longer term, as Als grow more powerful, they may get harder to constrain. In fact, GPT-40 needed no constraints where ol-preview did.\nA.7 Is this happening because AI training data is contaminated with Al research literature exploring reward hacking?"}, {"title": "A.8 Als find creative outs of problems we face them with all the time. You should just design a better reward function.", "content": "In this experiment, we only used pre-trained models and prompting. The AI was not rewarded or punished for playing chess, and in fact retained no state between different runs."}, {"title": "A.9 Given the chance most humans will cheat to win, so what's the problem?", "content": "We would like Als to be trustworthy, to help humans, and not cheat them (Amodei, 2024)."}, {"title": "A.10 Given the chance most humans will cheat to win, especially in computer games, so what's the problem?", "content": "We will design additional \u201cmisalignment honeypots\u201d like the chess environment to see if something unexpected happens in other settings."}, {"title": "A.11 How do I replicate your work?", "content": ""}, {"title": "A.12 Will this replicate with a Computer Use environment? This might be less suggestive than shell access.", "content": ""}, {"title": "A.13 What happens if you prompt the model not to hack, or to play by the rules?", "content": ""}, {"title": "A.14 What happens if you ask it about its play afterwards? Will it say it acted ethically and defend itself?", "content": "We release this report and code for further study at https://github.com/palisaderesearch/ctfish. Email us if you'd like to collaborate on further experiments."}, {"title": "A.15 Are you just fearmongering?", "content": "We are a non-profit building concrete demonstrations of dangerous capabilities to advise policy makers and the public on Al risks. We strive to make our work transparent, truth-seeking, and reproducible.\nFor example, when we previously claimed that the guardrails of frontier models like Llama 3 or GPT-40 can be stripped in minutes (Krupkina and Volkov, 2024; Volkov, 2024), we shared code and data with collaborators, including Meta's safety team.\nOur recent works showing LLMs have surpassed the high-school level in hacking (Turtayev et al., 2024) and monitoring the Internet for autonomous AI hackers (Reworr and Volkov, 2024) are open-source and have been reproduced.\nThe experiment described in this report is our contribution to the case that frontier AI models may not currently be on track to alignment or safety. It is not a definitive claim, but rather a part of the dialogue in the research community. There is more work to be done, and we invite others to join in."}]}