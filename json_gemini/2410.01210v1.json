{"title": "Polyp-SES: Automatic Polyp Segmentation with Self-Enriched Semantic Model", "authors": ["Quang Vinh Nguyen", "Thanh Hoang Son Vo", "Sae-Ryung Kang", "Soo-Hyung Kim"], "abstract": "Automatic polyp segmentation is crucial for effective diagnosis and treatment in colonoscopy images. Traditional methods encounter significant challenges in accurately delineating polyps due to limitations in feature representation and the handling of variability in polyp appearance. Deep learning techniques, including CNN and Transformer-based methods, have been explored to improve polyp segmentation accuracy. However, existing approaches often neglect additional semantics, restricting their ability to acquire adequate contexts of polyps in colonoscopy images. In this paper, we propose an innovative method named \"Automatic Polyp Segmentation with Self-Enriched Semantic Model\" to address these limitations. First, we extract a sequence of features from an input image and decode high-level features to generate an initial segmentation mask. Using the proposed self-enriched semantic module, we query potential semantics and augment deep features with additional semantics, thereby aiding the model in understanding context more effectively. Extensive experiments show superior segmentation performance of the proposed method against state-of-the-art polyp segmentation baselines across five polyp benchmarks in both superior learning and generalization capabilities.", "sections": [{"title": "Introduction", "content": "Medical image segmentation is the process of delineating regions of interest (ROI) within medical images, such as X-rays, MRI scans, CT scans, or histological slides, into meaningful and distinct anatomical structures. This process can assist clinicians in quantitative analysis, diagnosis, treatment planning, and monitoring of diseases. Traditional image processing techniques [9,21,27,42] have been widely adopted in the medical image segmentation tasks. These methods primarily rely on handcrafted features potentially limiting their generalizability to complex image structures, noise, or variability in image quality. With the rapid development of deep learning, efficient and reliable segmentation solutions [3, 10, 37,41] was introduced in the domain of the medical image segmentation.\nPolyp segmentation is a critical task in medical imaging aimed at accurately identifying and delineating polyps within endoscopic or colonoscopic images. The challenges of identifying and segmenting polyps in medical images can be summarized for several following primary factors. Firstly, the variability in color, shape, size and texture among polyps poses a significant difficulty to precise segmentation. Secondly, the presence of noise, artifacts, or overlapping structures in the image can obscure polyps, increasing the complexity of the segmentation task. Thirdly, polyps may manifest in diverse positions within the gastrointestinal tract, each with its distinct characteristics, thus contributing to the overall variability and difficulty of the segmentation.\nTraditional methods [19, 29, 30, 43] for polyp segmentation have often faced challenges such as sensitivity to image variations, the need for manual parameter adjustment, limited adaptability, and difficulties in handling noise and artifacts. Efforts leveraging deep learning techniques have been proposed to enhance the effectiveness of automatic polyp segmentation. Firstly, CNN-based methods [7,8,22-24, 26, 36, 49] exploit the ability of neural networks to automatically learn discriminative features and generate precise segmentation results. Despite the considerable success of CNN-based approaches, the limited receptive field poses challenges in capturing global representations. Transformer-based techniques [5, 14, 34, 39, 48], on the other hand, excel in capturing global dependencies and long-range contextual information more effectively compared to CNNs, resulting in superior performance in the polyp segmentation task. Nonetheless, transformer-based methodologies encounter difficulties in capturing fine-grained"}, {"title": "Related Work", "content": "details, which are crucial for accurately detecting and locating polyp objects. In addressing visual comprehension concerns involving semantic segmentation and object recognition, contextual information provides valuable insights that aid in disambiguating similar-looking objects, resolving occlusions, and improving the overall understanding of the visual scene. Notably, the approaches mentioned above primarily relying on poor contextual information neglecting to provide additional semantics. This limitation presents several challenges in comprehending adequate contexts of polyps, which are frequently characterized by noises, ambiguous boundaries, and intricate foregrounds. We have discovered that providing supplementary semantics can assist the model in obtaining comprehensive contextual information about polyp objects, potentially leading to a significant enhancement in segmentation performance as shown in Fig. 1.\nMotivated by discussed concerns, our study introduces a new novel approach for automatic polyp segmentation task namely \"Automatic Polyp Segmentation with Self-Enriched Semantic Model\". Initially, we employ an encoder to extract a sequence of multi-scale features. Subsequently, we introduce a Local-to-Global Spatial Fusion (LGSF) mechanism to capture both local and global spatial features before decoding them to generate an initial global feature map. Leveraging the proposed Self-Enriched Semantic (SES) module, we augment deep features with additional semantics, thereby aiding the model in understanding context more effectively. Our proposed solution achieves competitive segmentation performance compared to state-of-the-art baselines, showcasing proficiency in learning and generalization capabilities. Notably, it effectively addresses the limitations of prior models when operating in challenging contexts."}, {"title": "Automatic Polyp Segmentation.", "content": "Traditional methods [19,29,30,43] primarily rely on low-level features such as geometric characteristics, which often result in missed or inaccurate detections due to similarities with neighboring tissues. Recent advancements in deep learning have revolutionized the polyp segmentation by autonomously learning complex features. Among these innovations, U-Net [26] obtaines significant improvements across various medical imaging tasks By the simplicity and effectiveness design.\nACSNet [47] refines the conventional skip connections within the U-Net [26] and selects adaptive features based on a channel attention mechanism. Pranet [7] utilizes reverse attention mechanisms to refine boundary details in the global feature map through iterative stages enhancing segmentation predictions. MSNet [49] introduces multi-scale subtraction architecture in order to capture intricate details, eliminate redundancy and complementary information between the multi-scale features. SSformer [34] adopts a systematic feature fusion approach, gradually integrating both local and global contextual information, resulting in precise object delineation and boundary detection, while also capturing fine-grained details and comprehensive scene context. Polyp-PVT [5] presents a similarity aggregation module to extract local pixel and global semantic cues from the polyp area."}, {"title": "Vision Transformer.", "content": "Transformer [31], initially successful in NLP, has garnered prominence for their potential in computer vision tasks. Leveraging the transformer mechanism, ViTs [6] effectively captures global dependencies and long-range spatial relationships, enabling comprehensive predictions based on the entire image context. By employing shifted windows instead of fixed-size patches, Swin [16] captures spatial relationships between adjacent patches, leading to enhanced feature representation and learning capabilities. Pyramid Vision Transformer (PVT) [35] incorporates a pyramid feature extraction mechanism to capture multi-scale information from input images. Through the combination of features from various scales, PVT [35] demonstrates proficiency in capturing both local details and global context, facilitating accurate dense prediction. UniFormer [15] integrates the strengths of convolutional neural networks (CNNs) and vision transformers (ViTs) into a concise transformer format. This innovative design empowers UniFormer [15] to efficiently capture both local redundancies and complex global dependencies, facilitating effective representation learning. MetaFormer [46] has recently demonstrated the commendable performance in computer vision tasks. This study meticulously examines various token mixers, spanning from basic operators like identity mapping or global random mixing to established techniques such as separable convolution and vanilla self-attention."}, {"title": "Method", "content": "As depicted in Fig. 2, our automatic polyp segmentation solution contains three principal components: Encoder, Decoder, and Self-Enriched Semantic (SES). The first is the Encoder, pretrained on ImageNet [4], extracts multi-scale features from an input image. The second contribution is the Decoder which employs Local-to-Global Spatial Fusion (LGSF) to capture both the global and local spatial features to achieve robust feature representation. Subsequently, refined features are aggregated to locate polyp objects and generate an initial global feature map. In the end, the SES component queries potential semantics from the initial global feature map and send them to high-level features to detect and relocate polyp objects accurately."}, {"title": "Encoder Backbone", "content": "In computer vision tasks, the encoder plays a crucial role in capturing spatial information and contextual cues from input images. Transformer-based encoding methods [5, 14,48] offer the ability to capture long-range dependence information across different areas within the input image. Metaformer [46] has recently introduced new insights into designing transformer architecture and has shown significant performance improvements in various computer vision tasks. Motivated by these findings, our study adopts a vision metaformer encoder known as Caformer [46] as a reliable and competitive backbone for feature extraction."}, {"title": "Global Feature Map Aggregation", "content": "The encoder features represent crucial and distinctive information essential for detecting polyp objects. Local features capture intricate details and boundaries, whereas global features provide contextual insights and spatial relationships among various structures. To effectively capture both global and local spatial information, we propose Local-to-Global Spatial Fusion (LGSF), as illustrated in Fig. 2.\nThe local stage conducts four parallel dilated convolutions [45] with a dilation rate of {1,2,4,8} to extract local features at various spatial scales. Each dilated convolution is followed by batch normalization (BN) and a rectified linear unit (ReLU). Resultant features from four dilated convolutions are aggregated to obtain the local feature representation. The resulting feature representation is then processed by a spatial attention mechanism (SA) [38] to suppress the irrelevant regions. The detail of the process is provided below:\n$F_{local} = SA(Concatenate(C_{1 \\times 1}^{r=1}(F), C_{1 \\times 1}^{r=2}(F), C_{1 \\times 1}^{r=4}(F), C_{1 \\times 1}^{r=8}(F)))$ (2)\nThe global stage incorporates non-local operation [2] to explore the long-range relationships between each pixel in the spatial space. This stage applies a convolution layer to obtain a feature representation. The resulting feature representation is transposed and followed by a Softmax function, and a Hadamard operation on the input feature F to create a pixel relationship context. This context is then passed through an MLP layer to enhance the relationship representation. In the end, the resulting representation is followed by a sigmoid (\u03c3) function and another Hadamard operation on the input feature F to attain the global feature representation. The global stage can be delineated as follows:\n$F_{global} = F \\times \\sigma(MLP(F \\odot Softmax((C_{1 \\times 1}(F)^T))))$ (3)\nThe final feature representations are obtained by combining local and global information, followed by a convolution layer. Multi-Scale Feature Aggregation (MSFA) module is used to synthesize multi-scale feature representations. This module fuses refined high-level features through a process as depicted in Fig. 2. The first two features, $F_3$ and $F_4$ undergo bilinear upsampling to match the spatial dimensions of all three features before they are concatenated. To enhance the capture of non-linear features, we further employ a series of convolutional layers, BN and ReLU. Finally, a sigmoid (\u03c3) function is conducted to produce the output $M_{initial}$, which serves as the initial global feature map."}, {"title": "Self-Enriched Semantic", "content": "The shallow layer features are closer to the input more than deep layer features, they preserve more of the original image's details and structure. Therefore, we leverage the low-level features $F_1$ to query implicit semantics from the initial global feature map, thereby providing supplementary semantics to the deep features. The semantic-enriched deep features are then decoded to yield two semantic-enriched segmentation, $M_1$ and $M_2$. The detailed structure of the Self-Enriched Semantic (SES) module is displayed in Fig. 2. The process can be formulated as following:\n$M_1 = MSFA(F^{rich}, F^{rich}, F^{rich})$ (4)\n$M = \\sigma(C_{1 \\times 1}(Concatenate(M_1, M_2)))$ (5)\nFirstly, we consider the distribution of pixel values in patch-level images to represent the initial global feature map $M_{initial}$ to two distinct types of semantic"}, {"title": "Experiments", "content": "areas including $S_1$ and $S_2$. Considering patch-level images where exist polyp objects, $S_1$ includes patches with pixel values varying from 0 and 1, often indicating noise or ambiguous boundaries that have not been sufficiently explored, whereas $S_2$ consists of patches with pixel values closer to 1, representing solid structures of polyp objects. This categorization helps us differentiate between areas of interest and those that may introduce variability or noise. We then employ the $F_1$ as query and the $S_1$ acts as key-value pairs, applying Cross-layer spatial Attention (CA) [31] to ascertain the relevance of $F_1$ and $S_1$. The resultant feature is then sent to high-level features through Attention Gate (AG) units progressively. In the end, we fuse semantic-enriched high-level features using Multi-Scale Fusion Aggregation (MSFA) to achieve a semantic-enriched global feature map, $M_1$. By Implementing the same operation to $F_1$ and $S_2$, we also obtain $M_2$. Finally, we concatenate $M_1$ and $M_2$ before passing them through a convolution layer followed by a sigmoid (\u03c3) function to predict the final global feature map, M."}, {"title": "Dataset and Evaluation Metrics", "content": "Following recent cutting-edge solutions for the polyp segmentation task, we employ five widely-used benchmark datasets to assess the efficacy of our proposed model. These datasets include Kvasir [13], ClinicDB [1], ColonDB [30], ETIS [28], and EndoScene [32]. Table 1 provides a comprehensive overview of each dataset, including their specific usage details and objectives.\nWe employ various standard metrics to assess and compare the performance of polyp segmentation algorithms. The Dice score quantifies the spatial agreement between the predicted segmentation mask and the ground truth mask, whereas the IoU score computes the ratio of their overlapping area to the combined area. Both scores range between 0 and 1, with higher values indicating better segmentation performance. The MAE computes the average absolute difference between individual pixels in the predicted and ground truth masks. These evaluation metrics offer a comprehensive assessment of the segmentation performance, considering both spatial alignment and pixel-level precision."}, {"title": "Implementation Details", "content": "We utilize the power of RTX 3090 GPU to accelerate both the training and inference stages of our model. Throughout the training process, we monitor various metrics including loss function, mDice, mIoU, and MAE scores to assess the performance and guide the training process. The total duration of training amounts to approximately 2 hours to achieve optimal performance. Detailed training parameters are provided in Table 2."}, {"title": "Comparisons with State-of-the-art Methods", "content": "This section conducts a comprehensive evaluation focusing on two critical aspects: Learning ability, which verifies the segmentation performance on the seen dataset, and generalization ability, which evaluates the capacity of the model to generalize effectively to unseen data. A total of sixteen state-of-the-art models from the domain of the polyp segmentation, including U-Net [26], UNet++ [50], PraNet [7], SFA [8], MSEG [12], ACSNet [47], DCRNet [44], EU-Net [25] and SANet [36], alongside newer models such as Polyp-PVT [5], ADSNet [22], CaraNet [18], TransUnet [3], Transfuse [48], UCTransNet [33], SSFormer [34], are collected for comparative analysis. The performance of these models is meticulously evaluated on five benchmark datasets using mDice, mIoU, and Mean Absolute Error (MAE) scores. In order to ensure fairness and reproducibility in our comparative analysis, we meticulously maintained consistency across training, validation, and testing datasets for all assessed models. Following the methodology outlined in PraNet [7], we adopt an identical dataset configuration as illustrated in Table 1, comprising 900 and 548 images sourced from the Kvasir and ClinicDB datasets as the training set, with the remaining 64 and 100 images allocated as the respective test set to evaluate the learning ability. Additionally, we utilize the ColonDB, ETIS, and EndoScene datasets, which were not included in the training phase, to assess generalization ability.\nLearning ability. In the learning ability experiment, the domain of the test and train set is similar. Table 3 presents the results of different cutting-edge models on the Kvasir and ClinicDB datasets. Our method demonstrates outstanding performance compared to recently published models on both datasets, as evidenced by the mDice, mIoU, and MAE scores. Specifically, our method obtains a mDice score of 0.924, a mIoU score of 0.875 on the Kvasir dataset, outperforming the second-best model ADSNet [22]. On the ClinicDB dataset, our model achieves a"}, {"title": "Generalization ability.", "content": "mDice score and mIoU of 0.945 and 0.902, respectively, showcasing an improvement compared to TransFuse [48]. These results underscore the robustness and effectiveness of the proposed method in terms of learning ability.\nGeneralization ability. We conduct a through evaluation of the polyp segmentation baselines to assess their generalization performance on unseen datasets, as shown in Table 4. It can be observed that our method demonstrates competitive performance across all three datasets compared to other techniques. Specifically, our model is higher than the second-best ADSNet [22] on the ColonDB dataset in term of mDice score and mIoU score. On ETIS dataset, although Transfuse [48] exhibits notable performance with a mIoU score of 0.826, its corresponding mDice score is lower at 0.737. In contrast, our results achieve a mDice score of 0.805, outperforming all other models, alongside a mIoU score of 0.756. These findings highlight the stable performance of our proposed approach, which excels in both mDice and mIoU scores where other methods may have limitations. Additionally, our model demonstrates remarkable improvement on the EndoScene dataset, with mDice score, mIoU score, and MAE score of 0.911, 0.847, and 0.005, respectively. These results underscore the superior generalization capability of our proposed method.\nQualitative results. We present qualitative results comparing our model with other polyp segmentation baselines across five datasets, depicted in Fig. 3 and Fig. 4. The segmentation results of the compared methods are sourced from the publicly available Polyp-PVT [5]. We can observe that our model produces clear and precise segmentation outcomes across a variety of polyp structures. Furthermore, it effectively identifies and segments polyp objects under different variations in image quality, minimizing artifacts and extraneous regions while maintaining exceptional segmentation accuracy. These findings underscore the efficiency and accuracy of our proposed segmentation algorithm, even in challenging spatial scenarios where previous methods have struggled."}, {"title": "Ablation Study", "content": "In the ablation study section, we conduct experiments to validate the necessity and effectiveness of each proposed module in the overall architecture individually. Our standard polyp segmentation architecture includes an Encoder, Decoder and Self-Enriched Semantic (SES). The ablation studies are conducted on all five polyp datasets, evaluating based on mDice and mIoU scores.\nEffectiveness of Encoder Backbone In the first ablation study, we assess the effectiveness of different encoder backbones. We use the proposed standard architecture as the baseline and replace diverse encoder backbones, consisting of ResNet50 [11] (CNN), PVT [35] (Transformer), and Caformer [46] (Metaformer). All variants are trained under the same configuration, and the results are summarized in Table 5. It is evident that the standard baseline, with Caformer as the encoder backbone, achieves superior performance with higher mDice and mIoU scores across all five datasets compared to CNN-based or conventional transformer encoder backbones. This demonstrates the effectiveness of exploiting the vision metaformer as encoder backbone in extracting robust features and enhancing polyp segmentation performance."}, {"title": "Effectiveness of Local-to-Global Spatial Fusion", "content": "Effectiveness of Local-to-Global Spatial Fusion To assess the impact of local and global feature aggregation, we remove the LGSF units from the decoder in the standard architecture, and replace them with 3\u00d73 convolution layers. Results presented in Table 6 demonstrate a significant decrease in both mDice and mIoU scores compared to the standard baseline with LGSF units. Furthermore, visualizations of segmentation predictions in Fig. 5 reveal that the absence of LGSF introduces considerable noise. These qualitative and quantitative results prove that LGSF can help model to distinguish polyp tissues and contribute greatly to the polyp segmentation performance. In order to further explore the contribution of the LGSF, we showcase high-level features before and after refinement by the LGSF units in Fig. 6. As can be observed, the LGSF eliminate redundant information from other regions and yield informative characteristics of level-specific features, aiding the model in precisely locating polyp objects and enhancing segmentation performance."}, {"title": "Effectiveness of Self-Enriched Semantic", "content": "Effectiveness of Self-Enriched Semantic This ablation study validates the effectiveness of the proposed SES module on the overall architecture. By excluding the SES module from the baseline, we revert to a conventional encoder-decoder structure. The performance presented in Table 6 reveals that the conventional encoder-decoder architecture without SES leads to a deterioration in performance, with lower on mDice score and mIoU score compared our standard model. In Fig. 5, it is apparent that the absence of the SES results in more detailed errors or missed semantic areas. This proves that the SES module facilitates the model to explore potential semantics to give the better global feature map with the comprehensive context. We further investigate the contribution of the SES by visualizing the two semantic-enriched segmentation masks containing $M_1$ and $M_2$ in Fig. 7. Notably, $M_1$ demonstrates the ability to explore potential semantic areas referring to regions denoted as red-bordered boxes where were"}, {"title": "Conclusion", "content": "In this paper, we introduce \"Automatic Polyp Segmentation with Self-Enriched Semantic Model\", an innovative approach aimed at addressing the limitations of contemporary methods in capturing comprehensive contexts. By leveraging a vision metaformer Encoder, a Decoder, and a Self-Enriched Semantic module, our method effectively enriches deep features with supplementary semantics, improving the model's understanding of challenging contexts. Through quantitative and qualitative experiments, we demonstrate its effectiveness and superiority"}]}