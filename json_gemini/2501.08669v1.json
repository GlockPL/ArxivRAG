{"title": "SPEQ: Stabilization Phases for Efficient Q-Learning in High Update-To-Data Ratio Reinforcement Learning", "authors": ["Carlo Romeo", "Girolamo Macaluso", "Alessandro Sestini", "Andrew D. Bagdanov"], "abstract": "A key challenge in Deep Reinforcement Learning is sample efficiency, especially in real-world applications where collecting environment interactions is expensive or risky. Recent off-policy algorithms improve sample efficiency by increasing the Update-To-Data (UTD) ratio and performing more gradient updates per environment interaction. While this improves sample efficiency, it significantly increases computational cost due to the higher number of gradient updates required. In this paper we propose a sample-efficient method to improve computational efficiency by separating training into distinct learning phases in order to exploit gradient updates more effectively. Our approach builds on top of the Dropout Q-Functions (DroQ) algorithm and alternates between an online, low UTD ratio training phase, and an offline stabilization phase. During the stabilization phase, we fine-tune the Q-functions without collecting new environment interactions. This process improves the effectiveness of the replay buffer and reduces computational overhead. Our experimental results on continuous control problems show that our method achieves results comparable to state-of-the-art, high UTD ratio algorithms while requiring 56% fewer gradient updates and 50% less training time than DroQ. Our approach offers an effective and computationally economical solution while maintaining the same sample efficiency as the more costly, high UTD ratio state-of-the-art.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning [1], [2] has gained significant at- tention for its ability to solve complex decision-making tasks through interactions with environments [3], [4]. However, one of the primary challenges in RL is sample efficiency, which is the ability to learn effectively from a limited number of interactions. Typically, RL requires millions of interactions with the environment to achieve strong performance, which becomes impractical in real-world applications where such interactions are expensive, time-consuming, or risky [2].\nRL methods fall into two major categories: on-policy and off-policy algorithms [2], [5], [6], each with a different impact on sample efficiency. On-policy algorithms, like Proximal"}, {"title": "II. RELATED WORK", "content": "The potential of high UTD ratios has been gaining interest from the research community. Model-Based Policy Optimiza- tion (MBPO) is a model-based algorithm that uses a mix of real and synthetic data along with a large $UTD \\gg 1$, achieving"}, {"title": "III. METHODOLOGY", "content": "In this section, we begin by describing some preliminaries useful for understanding the following sections. We continue by examining the performance of DroQ at different UTD ratios in order to understand the trade-offs between computational efficiency and bias in Q-function estimation. We then introduce our approach, which aims to maintain low bias and strong performance while reducing computational overhead."}, {"title": "A. Preliminaries", "content": "When using high UTD ratios it is important to consider the overestimation bias problem during the prediction of Q-values.\n[12] measure the bias as:\n$Bias_Q \\triangleq \\mathbb{E}_{s,a \\sim \\pi} [Q(s, a) - Q^*(s, a)]$, (1)\nwhere $Q^*(s, a)$ represents the Q-value for policy $\\pi$ approxi- mated using the Monte Carlo returns given state $s$ and action $a$, and $Q(s, a)$ is the average of the current estimations of the Q-functions. The bias is normalized to take into account changing average return values during training.\nOur algorithm is built on top of DroQ [13]. This algorithm leverages dropout [20] and layer normalization [21] to inject uncertainty into the prediction of target Q-values, thus reduc- ing overestimation bias and allowing it to use a very high UTD ratio of 20 with only two Q-functions in contrast to the large ensembles used in RedQ [12]. In DroQ, the targets are calculated as:\n$y = r + \\gamma \\min_{i=1,2} Q_{\\theta_{\\phi_i}}^\\prime(s^\\prime, a^\\prime) - \\alpha \\log \\pi_{\\theta}(a^\\prime | s^\\prime)), \\quad a^\\prime \\sim \\pi_{\\theta}(. | s^\\prime)$. (2)\nwhere $\\pi_{\\theta}$ is the policy we want to optimize and $Q_{\\theta_{\\phi_i}}^\\prime$ are the target Q-functions with dropout. Then, each Q-function is updated via gradient descent using:\n$\\nabla_{\\theta_i} \\frac{1}{|B|} \\sum_{(s,a,r,s^\\prime) \\in B} (Q_{\\theta_i}(s, a) - y)^2$. (3)\nwhere $Q_{\\theta_i}$ are the Q-functions we want to optimize. The target networks are updated with respect to the main Q-functions by using Polyak averaging, using a batch of experiences $B$ sampled from the replay buffer:\n$\\phi_i \\leftarrow \\rho \\phi_i + (1 - \\rho) \\theta_i$, (4)\nwhere $\\rho$ is a hyperparameter. For each environment step, after updating the Q-functions, the policy is updated following:\n$\\nabla_{\\theta} \\frac{1}{|B|} \\sum_{s \\in B}  \\sum_{i=1,2} Q_{\\theta_i}(s, a) - \\alpha \\log \\pi_{\\theta}(a|s)  , \\quad a \\sim \\pi_{\\theta}(. | s)$. (5)"}, {"title": "B. Our Approach", "content": "High UTD ratio algorithms [15], [12], [13] offer improved sample efficiency by performing multiple gradient updates for each environment interaction (see Eq. 3). However, these approaches tend to be computationally expensive due to the large number of gradient steps required to increase perfor- mance [13]. This computational inefficiency becomes a bot- tleneck, especially in environments where time and resources"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We evaluate our approach on the OpenAI MuJoCo suite [14] which represents a standard benchmark for continuous con- trol RL solutions on the following locomotion environments: Ant, Hopper, Humanoid, and Walker2d. For all environments, each algorithm was trained for a total of 300,000 environment steps. For each run, we measure the average over 5 seeds.\nWe compare SPEQ against the following algorithms:\n\u2022 Soft Actor-Critic (SAC) [10], a classical off-policy RL algorithm that does not use a high UTD ratio, which achieves stability and efficiency through the combinations of maximum entropy and actor-critic architecture.\n\u2022 RedQ [12], which extends the SAC backbone with a large ensemble of Q-functions to address the increasing bias error in Q-values estimation to leverage high UTD ratios.\n\u2022 DroQ [13], which is a variant of RedQ that uses a smaller ensemble of Q-functions and leverages dropout and layer normalization to address the mitigation of bias in Q- function estimation.\n\u2022 SMR [22], which revises the concept of replay ratio by updating the agent multiple times on the same batch.\nWith the exception of SAC, the other three algorithms use a high UTD ratio. In addition, SMR can be implemented on top of other optimization algorithms, and in our experiments, we consider both SAC and RedQ. SMR-RedQ version performs an equivalent of UTD = 100 ratio updates, which makes this version especially computationally intensive. To ensure consistent reproduction of the results obtained, we use the original repositories of the DroQ\u00b9, in which are also defined the SAC and RedQ algorithms, and SMR\u00b2 algorithms. Our experiments aim to answer the following research questions:\n\u2022 Q1: Can we enhance computational efficiency without sacrificing performance?\n\u2022 Q2: How does our approach perform in relation to a lower UTD ratio version of DroQ?\n\u2022 Q3: How do hyperparameters settings impact the per- formance of SPEQ, in particular N, F, and the update strategy?"}, {"title": "A. Computational Efficiency", "content": "To answer Q1 we evaluate the computational efficiency SPEQ compared to the algorithms listed above. Figure 3 shows the results. Our evaluation consists of three comparisons:\n\u2022 Figure 3(a) compares SPEQ to state-of-the-art high UTD ratio algorithms in terms of computational efficiency. We report evaluation reward as a function of the number of gradient updates. All the approaches use the same number of environmental steps (300,000). This evaluation shows which algorithm achieves good results with the fewest"}, {"title": "B. Comparison at Lower UTD Ratios", "content": "To answer Q2, we compare the performance of SPEQ against DroQ with different UTD ratios. In particular, we test DroQ with UTD ratios of 2, 3, 9, and 20. The original DroQ paper used a UTD ratio of 20 [13]. Figure 4 shows the results of this comparison. This experiment demonstrates the impact of the UTD ratio on DroQ performance \u2013 in particular, DroQ with a UTD ratio of 9 requires approximately the same number of gradient updates as our SPEQ method.\nThe results reveal that the use of dropout, which is necessary to allow a high UTD ratio, renders the DroQ algorithm unable to effectively reduce bias when the UTD is lowered from the default value of 20. In fact, as seen in Figure 4, at lower UTD ratios the bias increases, leading to worse estimates of Q values. UTD ratio of 3 and 2 starts with a lower bias due to the fewer gradient steps taken at the start of the training, when the Q-function estimation is still noisy, but then the bias converges at a higher value due to the under-exploitation of the replay buffer.\nFurthermore, the bias comparison between SPEQ and DroQ with UTD = 9 confirms our intuition about the effectiveness of offline stabilization steps to efficiently train Q functions. Our approach of delaying gradient updates to take advantage of larger distributions of experience during the early part of the training leads to a better reduction of bias of Q-value estimation. Then alternation of low UTD and stabilization phases helps to keep the bias low during the rest of the training. These results when compared to an equivalent UTD ratio of 9, show both better bias reduction capability and higher performance."}, {"title": "C. Ablations", "content": "To answer Q3, we consider several ablated versions of our approach.\nVarying Number of Stabilization Iterations (N). This experiment aims to evaluate the impact of varying the number N of updates during the stabilization phase of SPEQ. We conduct the experiment using the Humanoid task, averaging the results over five random seeds. Figure 5 shows that increasing N results in noticeable performance improvements, as the Q-function has more opportunities to refine its estimates. However, this trend only continues up to a certain threshold (N = 75,000). Beyond this point, further updates lead to diminishing returns, where the performance gains plateau and eventually decline. This decline can be attributed to overfit- ting the Q-function on the transitions stored in the replay buffer. With very many updates, the model loses its ability to generalize to out-of-distribution states. As a result, instead of enhancing the policy's robustness, excessive updates introduce instability in performance.\nVarying Stabilization Frequency (F). This experiment aims to evaluate the optimal frequency F of the periodic stabilization phases. Figure 4 shows that increasing F is detrimental in terms of performance, therefore causing SPEQ at F = 50,000 and F = 100,000 to converge to sub-optimal"}, {"title": "V. LIMITATIONS AND FUTURE WORK", "content": "Although SPEQ improves computational efficiency and shows solid performance in the evaluated tasks, it does not consistently outperform state-of-the-art high UTD ratio meth- ods. The most significant gains are observed when computa- tional resources are a limiting factor. Moreover, our experi- mental results show the effectiveness of stabilization phases,"}, {"title": "VI. CONCLUSIONS", "content": "In this paper we introduced an efficient alternative to high update-to-data ratio reinforcement learning which we call Stabilization Phases for Efficient Q-Learning (SPEQ). Our approach significantly improves computational efficiency by alternating between low UTD ratio training phases and offline stabilization phases during which Q-functions are fine- tuned with high UTD ratio without additional environment interactions. This strategy allows us to significantly reduce the computational overhead while maintaining or even improving performance compared to state-of-the-art high UTD ratio algorithms.\nOur experimental results demonstrate that SPEQ requires from 40% to 99% fewer gradient updates and from 27% to 78% less training time, maintains the same sample effi- ciency, and achieves competitive performance across various continuous control tasks as the other state-of-the-art high UTD approaches. Additionally, our solution effectively mitigates Q-function bias without relying on large ensembles, further improving computational efficiency. Therefore, SPEQ yields significant advantages in terms of both computational cost and learning effectiveness, making it a practical choice for real-world, resource-constrained applications."}]}