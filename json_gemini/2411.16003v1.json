{"title": "eFedLLM: Efficient LLM Inference Based on Federated Learning", "authors": ["SHENGWEN DING", "CHENHUI HU"], "abstract": "Large Language Models (LLMs) herald a transformative era in artificial intelligence (AI). However, the expansive scale of data and parameters of LLMs requires high-demand computational and memory resources, restricting their accessibility to a broader range of users and researchers. This paper introduces an effective approach that enhances the operational efficiency and affordability of LLM inference. By utilizing transformer-based federated learning (FL) with model-parallel distributed training, our model efficiently distributes the computational loads and memory requirements across a network of participants. This strategy permits users, especially those with limited resources to train state-of-the-art LLMs collaboratively. We also innovate an incentive mechanism within the FL framework, rewarding constructive contributions and filtering out malicious activities, thereby safeguarding the integrity and reliability of the training process. Concurrently, we leverage memory hierarchy strategies and Singular Value Decomposition (SVD) on weight matrices to boost computational and memory efficiencies further. Our results, derived from formulaic analyses and numerical calculations, demonstrate significant optimization of resource use and democratize access to cutting-edge LLMs, ensuring that a wide scale of users can both contribute to and benefit from these advanced models.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) represent a significant advancement in artificial intelligence (AI), specially designed to process and produce natural language text. The substantial improvement of LLMs over traditional machine learning (ML) models can be attributed to their large-scale training datasets collected from multiple sources (e.g. books, articles, web pages, and images), expansive scale of parameters, and enhanced precision in floating-point computations. These models typically utilize Transformer architectures [28], which are pivotal for their ability to recognize patterns and rules of language, allowing them to predict and generate appropriate responses. One unique attribute of the Transformer is the Attention mechanism [28], which dynamically weighs the importance of different tokens (i.e. the text inputs) in a sequence, thereby improving the model's ability to focus on relevant parts of the input when predicting the most probable output. LLMs power a variety of transformative generative AI applications, such as ChatGPT [4], Phi-3 [2], StyleGAN [18], and BERT [11], dramatically altering many aspects of human experience."}, {"title": "2 BACKGROUND", "content": "There are various methods for optimizing training and inference in machine learning (ML) models. In this paper, we mainly focus on two areas: model-parallel FL and algorithmic optimizations based on the transformer model architecture. In this section, we present a brief introduction on FL and the transformer model."}, {"title": "2.1 Federated Learning", "content": "FL [20, 31] has emerged as a crucial strategy to optimize training and inference for ML models, especially when dealing with data privacy concerns and distributed computational resources [1, 21, 27, 29]. FL can be primarily categorized into two types as shown in Figure 1: data-parallel and model-parallel FL."}, {"title": "2.2 Transformer Model", "content": "Transformers represent a significant architecture in LLMs. Central to the transformer models are two distinct components: the encoder and the decoder, as depicted in Figure 2.\nThe encoder processes the input text data by first converting it into embeddings that integrate positional encoding to retain the sequence order. The formula for positional encoding can be expressed as:\n\n$PE_{(pos,2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d}}})$\n\n(1)\n\n$PE_{(pos,2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d}}})$\n\n(2)\n\nwhere pos is the position, i is the dimension index, and d is the embedding dimension."}, {"title": "3 ARCHITECTURE", "content": "The primary applications of pre-trained LLMs are mainly two-fold: inference [25] and fine-tuning [15, 16]. Due to its widespread use in real-time applications, the focus here is primarily on the inference aspect of LLMs. Inference involves encoding the input tokens and predicting the next possible tokens. This is explored through an effective transformer-based FL framework, as shown in Figure 3, which is designed to optimize the efficiency of inference processes. To solve the aforementioned issues in existing LLM inference, this section will detail the architecture and the operational workflow of each type of users in our framework."}, {"title": "3.1 Transformer-Based Model-Parallel Federated Learning", "content": "Integral to eFedLLM is the transformer architecture, where each layer consists a complete transformer block, featuring components such as self-attention, a feed-forward network, normalization layers, and residual con-nection [24, 28]. For an LLM with L layers of transformer block, predicting a sequence of M tokens involves passing each token through all these L layers, resulting in a time complexity of O(L \u00d7 M) during inference [17, 24]. Furthermore, existing inference techniques often require storing past keys and values in accelerator (global) memory, which is resource-intensive. For instance, maintaining half-precision activation of a 2048-token sequence for LLMs like GPT-3 can consume approximately 9.6 GB of GPU memory per sequence[4]. To address these challenges, eFedLLM employs transformer-based model parallel FL, aiming to reduce the memory footprint and enhance the feasibility of using advanced LLMs across multiple computing environments.\nBuilding on this, the computational architecture of our framework is designed to distribute the inference workload across an interconnected network of GPUs or servers. This configurations allows different stakeholders in the FL setup-namely Clients, Servers, and Verifiers-to interact efficiently and reliably, ensuring that the computational and memory demands are managed effectively across the network:\n\u2022 Clients: Clients possess a dataset and a pre-trained LLM but often lack the computational resources necessary for efficient inference. These users initiate the inference tasks within the FL framework by providing the necessary input tokens and their associated embeddings. The initialized parameters of the pre-trained model are also sent to the FL nodes. To handle the computationally intensive aspects of LLM inference, Clients delegate the execution of transformer blocks to Servers, who perform the model-parallel computations for processing the input and intermediate data (e.g. model parameters, accumulated gradients) through the layers of the LLMs. After successful training and verification, Clients aggregate the updates from the FL process to continually refine and update the global model. The tasks are repeated until the desired level of convergence is achieved.\n\u2022 Servers: This group of users are the majority of this system equipped with the requisite GPU resources and are arranged in a sequence to process the computations forwarded by Clients. They are active participants in the transformer-based model-parallel FL process, which enhances scalability and resource efficiency for LLM inference. Initially, a specific threshold of hardware resources is established to select participants with adequate computational power. The process begins with the first Server, which receives embedding data from a Client and processes the initial layer of the LLM. Subsequent Server sequentially handle the remaining layers of the transformer block. To ensure the reliability and integrity of the FL process, each Server's output is scrutinized by Verifiers in a decentralized manner. By optimizing the use of computational resources across a distributed network, the resource required per user can be reduced and the inference of LLMs can be more economically feasible for a broader range of users.\n\u2022 Verifiers: It is also crucial to ensure that all the Servers participating the FL have no malicious intent, contributing positively to the federated system. Therefore, to make sure the integrity and reliability of the system, an incentive mechanism is utilized. Verifiers monitor the behavior of each Server by calculating a trust value based on predefined algorithm before the aggregation of updates to the global model. Server that meet or exceed a certain trust threshold are allowed continued access to the global model. Conversely, Servers that fail to meet this threshold are excluded from the system, with their computational tasks reassigned to other trusted Servers."}, {"title": "3.2 Incentive Mechanism", "content": "Incentive mechanisms play a crucial role in ensuring the integrity and reliability of training results within FL environments. To ensure correct training results and reliable intermediate outputs, an incentive mechanism is applied across the server network. Since each layer of the model does not produce a final output that can be directly evaluated, intermediate outputs must be validated. One approach is to establish an expected output for layer i based on historical data, providing a benchmark against which actual outputs can be compared. Alternatively, a set of validation data could be processed through the model layers computed by trusted Verifiers.\nThen the Verifiers can estimate the accuracy $acc_i$ of layer i by comparing the intermediate outputs from layer i against its expected outputs.\nIn this incentive mechanism, a $TrustScore$ is calculated for each $Server$. The $Verifiers$ utilize the $Trust Score$ algorithm, denoted as $S_i$, to quantify the reliability of the ith$Server$ within the model-parallel FL framework. It is computed as follows:\n\n$Trust Score (S)_i = \\frac{acc_i \\times l_i}{\\max(l) \\times w_i}$ (3)\n\nwhere $acc_i$ represents the accuracy achieved by the ith$Server$ on its assigned tasks, indicating the Server's proficiency in processing its portion of the model. $l_i$ denotes the number of model layers that the ith$Server$ is responsible for. Servers with more substantial computational resources may handle more layers, contributing to a higher computational load. $max(l)$ is the maximum number of layers any Server within the network is handling, which serves to normalize the effect of layer distribution across all Servers, ensuring that their contributions are evaluated on a comparable scale. $w_i$ is a weighting factor applied to adjust and ensure that the $TrustScoreS_i$ remains bounded between 0 and 1.\nAdditionally, a threshold \u03b8 is implemented to manage the participation of Servers in the FL network. This threshold is crucial for filtering out potentially malicious or under-performing Servers while incentivizing those that meet or exceed the expected performance standards. Specifically, for each Server, the TrustScore is compared to the threshold \u03b8 as shown in the following formula. If Trust Score ($S_i$) is equal to or greater than \u03b8, the Server remains active and may receive incentives for its contributions. Conversely, if Trust Score ($S_i$) is less than \u03b8, the Server is deactivated to prevent any negative impact on the model's training and inference processes. The tasks of the deactivated Server will be reassigned to other qualified Servers. This dynamic verification is critical for maintaining the overall reliability and efficiency of the distributed learning environment.\n\n$Server Status (i) = \\begin{cases}\nTrust Score (S)_i \\geq \u03b8 & activate\\\\\nTrust Score (S)_i < \u03b8 & deactivate\n\\end{cases}$ (4)"}, {"title": "4 ALGORITHM OPTIMIZATIONS", "content": "This section discusses the advanced optimizations implemented within the transformer algorithms of the eFedLLM framework, targeting enhanced operational efficiency. The focus is primarily on three critical areas: optimizing matrix multiplication using memory hierarchy strategies, improving matrix transfer efficiency through Singular Value Decomposition (SVD), and enhancing verification processes. By refining these processes, we aim to streamline communications, reduce latency and boost the overall efficiency in this system.\nBefore delving into the specific optimizations, it is essential to establish a common understanding of the notations used throughout this section. Table 1 summarizing the key symbols and their meanings. With these notations defined, the subsequent discussion explores the implementation of each optimization strategy."}, {"title": "4.1 Matrix Multiplication Optimization", "content": "Matrix operation and data transfer are two computationally intensive tasks central to the performance of LLM operations, particularly in the context of transformer architectures. The conventional centralized learning approach often suffers from inefficiencies due to frequent and extensive data reads from global memory during matrix operations. Each element of the resultant matrix in a matrix multiplication involves multiple reads from the memory for each element calculation, significantly increasing the time and computational overhead.\nFL, contrasted with centralized approaches, can optimize this process by leveraging distributed computational resources. By employing a hierarchy structure depicted in Figure 4, each handling a segment of the computation, federated learning reduces the frequency of memory reads. The matrices are read once globally, and intermediate results are temporarily stored in faster, locally accessible block memory. The final results are then compiled from these intermediate states and written back to the global memory.\nFor example, consider the product matrix C = A \u00d7 B in equatio 5, where $c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + a_{i3}b_{3j}$. In centralized learning environments, the computation of each element of C necessitates repeated reads from the global memory. For $c_{11}$, it requires reading three elements from row one of matrix A and three corresponding elements from column one of matrix B, totaling six memory reads. Given that matrix C is a 3\u00d73 matrix, computing all its elements will involve 6 \u00d7 9 = 54 memory reads from global memory.\n\n$C=A\\times B= \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\\n a_{21} & a_{22} & a_{23} \\\\\n A_{31} & A_{32} & A_{33} \\end{pmatrix} \\times \\begin{pmatrix} b_{11} & b_{12} & b_{13} \\\\\n b_{21} & b_{22} & b_{23} \\\\\n b_{31} & b_{32} & b_{33} \\end{pmatrix},$ (5)\n\nIn contrast, FL significantly reduces memory read operations by utilizing distributed computation. In this approach, entire matrices A and B are read from the global memory just once each, leading to a total of 9 + 9 = 18 reads. The intermediate results-specific to each part of the computation-are stored in faster, local block memory and are later aggregated.\nMore generally, we have this theorem:"}, {"title": "THEOREM 4.1.", "content": "Let $A_{m \\times n}$ and $B_{n \\times k}$ be matrices, and let C = A \u00d7 B represent the matrix multiplication. Then, when deploying a Federated Learning model over a centralized model, the reduction in memory read time, $R_t$, can be expressed as:\n\n$R_t = 1 - \\frac{1}{2k} - \\frac{1}{2m}$ (6)\n\nPROOF. Let the centralized read time $T_c$ be given by\n\n$T_c = (n + n)mk = 2nmk$\n\nwhich accounts for reading each matrix element separately for each element of the result matrix.\nIn Federated Learning approach, the read time $T_f$ can be optimized to\n\n$T_f = mn + nk$\n\nsince each matrix is read only once per batch and the intermediate results are stored and reused.\nThus, the reduction in read time $R_t$ can be calculated as:\n\n$R_t = \\frac{T_c - T_f}{T_c} = \\frac{2nmk \u2013 (mn + nk)}{2nmk} = \\frac{2mk}{2mk} - \\frac{m-k}{2mk} = 1 - \\frac{1}{2k} - \\frac{1}{2m}$\n\nIn the Transformer model, the attention mechanism's Softmax function and the feed-forward networks represent the most computationally intensive operations, both involving matrix multiplication. In the attention mechanism, matrix multiplication appears in computing QKT, which is essential for determining the attention scores. In the feed-forward networks the operation often involves multiplying the input matrix X sequentially with two weight matrices. Our hierarchical computational structure becomes particularly advantageous when dealing with the multiplication of three or more matrices.\nThe exploration of the Transformer model also extends to evaluating the impact of addition, subtraction, and division operations on matrices. However, these operations have not been the focus of our optimization efforts within the framework. This decision stems from the observation that in addition and subtraction operations, each element of the resulting matrix necessitates accessing the corresponding elements in the original matrices. The number of memory accesses required remains consistent regardless of whether the model is operating under centralized or federated learning paradigms with a hierarchical memory structure. Consequently, the potential for performance improvement through optimization in addition and subtraction is limited. Additionally, matrix division is not directly included in transformer models. The only division-like operation in transformer models occurs within the Softmax function of the attention mechanism. This operation involves exponentiation of each element of the input matrix and then normalizing these values to ensure that the output sums to one, which is a form of scalar division across elements rather than matrix division.\nIn addition, we do not include optimizations for layer normalization and residual connections because their computational demands are relatively minimal compared to the more intensive operations of the attention"}, {"title": "4.2 Matrix Transfer Optimization", "content": "Federated Learning (FL) requires frequent exchange of large model parameters or gradients between distributed computing devices, making network transmission one of the major limitations of FL. The high communication overhead, bandwidth limitations, latency, and network instability can hinder the effective implementation of FL.To address these challenges, a method is proposed that combines Singular Value Decomposition (SVD) to reduce the amount of transmitted data [7, 8], thereby reducing communication latency and improving overall FL efficiency.\nGiven a weight matrix $W \\in R^{m\\times n}$, which represents the parameters that need to be transmitted in Federated Learning, Singular Value Decomposition (SVD) is first applied to the matrix W:\n\n$W = U\\Sigma V^T$ (7)\n\nwhere $U \\in R^{m\\times m}$ and $V \\in R^{n\\times n}$ are orthogonal matrices, and $\\Sigma \\in R^{m\\times n}$ is a diagonal matrix containing the singular values of the matrix W. To reduce the dimensionality, only the largest k singular values and their corresponding singular vectors are retained, resulting in a low-rank matrix.:\n\n$W_k = U_k\\Sigma_kV_k^T$ (8)\n\nwhere $U_k \\in R^{m\\times k}$, $\\Sigma_k \\in R^{k\\times k}$, and $V_k \\in R^{n\\times k}$.\nThe low-rank matrices $U_k$, $\\Sigma_k$, and $V_k^T$ are transmitted to other devices. Upon receiving these matrices, the devices use them to reconstruct an approximation of the original matrix Wk. The low-rank approximation Wk is the matrix that retains the first k singular values, and the retained energy is $\\sum_{i=1}^{k}{\\sigma}'$, where \u03c3\u2081 is the singular value of matrix, so the accuracy retained by the low-rank approximation We can be estimated using the cumulative energy ratio. The retained accuracy is defined as:\n\n$P = \\frac{\\sum_{i=1}^{k}{\\sigma}'}{\\sum_{i=1}^{r}{\\sigma}'}$ (9)\n\nwhere r = min(m, n).\nThe reconstruction accuracy depends on the number of singular values retained during the SVD process. Since the singular values tend to decay rapidly. In practice, retaining the top 40%-50% of the singular values typically preserves 90%-99% of the matrix's energy.\nTo understand the efficiency gains of using SVD in FL, an analysis of the complexity of SVD and the communication savings is required.\nMoreover, this proposition 4.2 outlines that applying SVD to weight matrix W can approximate it to lower rank while maintaining a considerable amount of the matrix's original information:"}, {"title": "PROPOSITION 4.2.", "content": "Let weight matrix $W \\in R^{m\\times n}$. If$W = U\\Sigma V^T$ is the SVD form of W, for any \u0454 > 0, matrix W can be approximated by a rank k matrix $W_k = U_k\\Sigma_kV$ to a relative tolerance \u0454 > 0 in the sense that: there exists rank-k matrix Wk, with k \u2265 C|loge for some constant C, such that:\n\n$||W - W_k|| \\leq \\epsilon$ (11)\n\nwhere k is chosen such that the sum of the discarded singular values squared is less than or equal to $\u03f5^2$. This approach is valuable in contexts such as LLM inference, where efficient data handling is necessary without compromising the accuracy of the data.\nIn a complete Transformer model, federated learning typically involves dynamically splitting the model architecture and distributing it across different devices, depending on their computational power and network bandwidth. For the purpose of theoretical analysis, a standard partitioning approach is assumed in this paper. Specifically, one encoder layer is taken as an example, where the embedding layer and position embedding are grouped together; each attention head in the multi-head attention mechanism is handled separately; and finally, the feedforward network is treated as a distinct module applied across the entire Transformer model.\nIt is assumed that the SVD dimensionality reduction method is applied during the input and output processes of each attention head. The following formula is used to constrain the accuracy and determine the number of singular values retained during each reduction.\n\n$\\frac{\\sum_{i=1}^{Rank(W_k)} \\sigma_i}{\\sum_{i=1}^{Rank(W)} \\sigma_i} \\geq \\epsilon$ (12)\n\nWhere \u03c3\u2081 represents the singular values of weight matrix W, and e denotes the desired accuracy to be retained. After processing through multiple encoder and decoder layers, the overall model accuracy can be approximately represented as the cumulative effect of this method.\n\n$P_{total} = \\prod_{i=1}^{L} (\\frac{1}{H} \\sum_{j=1}^{H} \\epsilon_{ij})$ (13)\n\nWhere Ptotal represents the overall final accuracy, L denotes the total number of layers, H is the number of attention heads, and eij represents the expected accuracy for each attention head j at layer i.\nHowever, it is important to note that as the number of layers increases, the accuracy retention tends to decrease layer by layer. Therefore, when designing the partitioning strategy, it is crucial to carefully select the dimensionality reduction accuracy e for each layer to balance between bandwidth and accuracy."}, {"title": "4.3 Combination of Memory Hierarchy and SVD", "content": "In this subsection, we combine the above methods together to analyze the overall optimization.\nAfter rank-k compression through SVD, we truncate the weight matrix W according to the compression ratio. Then the truncated weight matrix W becomes:\n\nW = \u00db \u00d7 \u2211 \u00d7 \u0176 (14)\n\nwhere \u00db \u2208 $R^{m\u00d7k}$, \u0108 \u2208 $R^{k\u00d7k}$, and \u0176 \u2208 $R^{n\u00d7k}$. The number of rank after truncated is:\n\nk = $\\frac{m \u00d7 n \u00d7 CompressionRatio}{m + n + 1}$ (15)\n\nFor comparing WX and WX, where W, W \u2208 $R^{m\u00d7n}$ are the weight matrices and X \u2208 $R^{n\u00d7t}$ is the input, we have the comparison in Table 3.\nConsider the first linear layer in a BERT model, where the weight matrix W \u2208 $R^{3072\u00d7768}$, reflecting a common architecture choice where the model expands the embedding dimension 768 to a higher internal dimension 3072 as part of its feed-forward network. X \u2208 $R^{768\u00d730}$ is the input matrix with sequence length equals 30, assuming that each of the 30 tokens is represented by a 768-dimensional vector as output from the previous layer. The multiplication WX represents a core computation in this layer. Applying SVD and memory hierarchy optimization techniques, WX is compressed to \u0174X. The results of comparing the memory read for WX and WX with and without memory hierarchy are shown in Figure 6. The compression ratios is varied from 0.2 to 0.8. These results illustrate the impact of memory hierarchy and SVD method on data communication at various compression ratios ranging\nTo analyze the impact of applying both SVD and memory hierarchy techniques on reducing bandwidth usage, bandwidth reduce rate is utilized:\n\n$BandwidthReduceRate = 1- \\frac{OptimizedTotalMemoryAccess}{OriginalTotalMemoryAccess}$ (16)\n\nwhere total memory access is calculated as:\n\n$TotalMemoryAccess = WeightMatrixRead + InputMatrixRead + OutputMatrixWrite$ (17)\n\nAgain consider the first linear layer of a BERT model. Let batch size is 10 and matrices stored as 32-bit floating point numbers (4 bytes each). The relationship between the compression ratio and the bandwidth reduce rate is"}, {"title": "4.4 Verification Optimization", "content": "The Verifiers' verification process is also optimized. The optimization is again targeted at where the computation is most intense \u2013 the Softmax function. The Softmax function transforms the score z\u012b (derived from QKT) into a probability distribution by exponentiating each score and normalizing these values by the sum of all score exponentials. This normalization ensures that the output of the Softmax function sums to one and forms a valid probability distribution. This allows the model to softly select the amount of attention to place on each value in the sequence. Building on this fundamental operation, this subsection details specialized algorithmic enhancements aimed at optimizing the verification process, further refining the efficiency and accuracy of the model's evaluative capabilities.\nBy splitting the calculation of exp(z) in equation 18 and the summation across multiple Verifier nodes, the computational burden on any single Verifier can be significantly reduced in a distributed manner.\n\n$Softmax(z_i) = \\frac{exp(z_i)}{\\sum_{v'} exp(z_{v'})}$ (18)\n\nThe Attention Mechanism Equation utilized in the Transformer models poses a unique challenge for verification due to its reliance on non-arithmetic operations, which do not compute numerical values directly:\n\n$Attention(Q, K, V) := Softmax(\\frac{QK^T}{\\sqrt{d_k}})$ (19)\n\nHere, Q, K, and V represent query, key, and value components essential to the model's ability to weigh and prioritize different parts of the input data based on relevance. The operation QK\u00b9 computes the dot product, representing how queries are mapped to keys. It is an arithmetic operation which can be verified by existing methods [9]. Then QKT is scaled by \u2192 (where d is the dimension of queries and keys), and normalized by the Softmax function to generate probability between 0 and 1 to normalize the scores. This sophisticated equation highlights the non-linear and complex interdependencies modeled by the transformer layers.\nTo address the efficient verification of such non-arithmetic elements in a high level of parallelization, particularly in the Attention mechanism, a specialized transformation is adopted from [26].\nFirstly, to simplify the expression, let:\n\n$Z \\leftarrow QK^T$ (20)\n\nNext, the shift-invariance property of Softmax is exploited by adding a constant z to Z:\n\n$Z' := Z - z_0$ (21)\n\nIt can be simply proved that Z and Z' lead to same result through Softmax function:\nPROOF.\n\n$Softmax(z_i - z_0) = \\frac{exp(z_i - z_0)}{\\sum_{v} exp(z_i \u2013 z_0)} = \\frac{exp(z_i) exp(- z_0)}{\\sum_{v} exp(z_i) exp(- z_0)} = \\frac{exp(z_i)}{\\sum_{v} exp(z_v)} = Softmax(z_i)$\n\nThen negative K-digit based-b numbers transformation [26] is applied:\n\n$Y = exp(Z') = exp(\\sum_{k=0}^{K-1} -b^kZ_{(k)}) = \\prod_{k=0}^{K-1} [exp(-b^kZ_{(k)})]$ (22)\n\nThis transformation allows a complex exponential operation with large exponents to be turned into a cumulative multiplication calculation so that it can continue to be optimised using matrix multiplication optimisation. The challenge of non-arithmetic operations is particularly addressed by the use of table lookups (tlookup) installed for each term of the K-digit base-b transformation in the product. These lookups facilitate the handling of operations like the Softmax function efficiently. In this way, the distributed collaboration of Verifiers are able to verify the Servers in an efficient manner."}, {"title": "5 RELATED WORKS", "content": "This section reviews key developments in frameworks comparable to eFedLLM, focusing on their architectural and operational contributions to the field of LLMs. opML (Optimistic Machine Learning) [10] offers a blockchain-based solution to execute LLMs efficiently on standard PCs, broadening accessibility without relying on GPUs. Its core includes a Fraud Proof Virtual Machine (FPVM) to ensure computational integrity, a versatile ML Engine, and an Interactive Dispute Game for error resolution, significantly lowering costs and enhancing computational trustworthiness. However, the reliance on blockchain can introduce latency and scalability issues, potentially limiting its practical application in real-time scenarios. zkLLM framework [26] leverages specialized Zero-Knowledge Proofs (ZKPs) to secure the confidentiality and integrity of LLMs during inference processes, ensuring no sensitive data or model parameters are disclosed. It adapts ZKPs to complex operations such as the Attention mechanisms in LLMs, making model outputs trustworthy without revealing proprietary training data or techniques. Nevertheless, the computational overhead and complexity of ZKPs can be substantial, possibly hindering its efficiency and wider adoption. PETALS [3] aims to democratize LLM utilization through a collaborative platform that divides model layers across multiple servers, allowing for efficient resource use and latency reduction in model operations. It also supports model fine-tuning and sharing via a model hub, fostering community collaboration and resource-sharing among researchers and practitioners. However, its decentralized nature could complicate the consistency and synchronization of model updates, posing challenges in maintaining model accuracy and state across different servers. Furthermore, the incentive method in [12] aligns with the approach in this paper, which includes smart contracts for automatically execute the incentive mechanisms in the distributed training process. [13] surveys the integration of blockchain technology with AI to mitigate issues like data misuse and enhance the robustness of AI models against malicious data or training contributions."}, {"title": "6 CONCLUSIONS", "content": "This paper demonstrates an effective approach for enhancing the operational efficiency and accessibility of LLMs through the implementation of a transformer-based FL framework with model-parallel distributed training. By efficiently distributing computational and memory demands across the network, the proposed eFedLLM enables a broader range of users, particularly those with limited resources, to collaboratively train and utilize state-of-the-art LLMs. The integration of an incentive mechanism within the FL framework ensures the integrity and reliability of the training process by rewarding positive contributions and deterring malicious activities. Furthermore, the application of memory hierarchy strategies and SVD on weight matrices has proven to significantly reduce resource consumption and enhance system performance. The outcomes of this research not only optimize resource utilization but also democratize access to advanced AI technologies, allowing a diverse user base to contribute to and benefit from the development of trustworthy and efficient Al systems."}]}