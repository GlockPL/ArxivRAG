{"title": "The OPS-SAT benchmark for detecting anomalies in satellite telemetry", "authors": ["Bogdan Ruszczak", "Krzysztof Kotowski", "David Evans", "Jakub Nalepa"], "abstract": "Detecting anomalous events in satellite telemetry is a critical task in space operations. This task, however, is extremely time-consuming, error-prone and human dependent, thus automated data-driven anomaly detection algorithms have been emerging at a steady pace. However, there are no publicly available datasets of real satellite telemetry accompanied with the ground-truth annotations that could be used to train and verify anomaly detection supervised models. In this article, we address this research gap and introduce the Al-ready benchmark dataset (OPSSAT-AD) containing the telemetry data acquired on board OPS-SAT\u2014a CubeSat mission which has been operated by the European Space Agency which has come to an end during the night of 22-23 May 2024 (CEST). The dataset is accompanied with the baseline results obtained using 30 supervised and unsupervised classic and deep machine learning algorithms for anomaly detection. They were trained and validated using the training-test dataset split introduced in this work, and we present a suggested set of quality metrics which should be always calculated to confront the new algorithms for anomaly detection while exploiting OPSSAT-AD. We believe that this work may become an important step toward building a fair, reproducible and objective validation procedure that can be used to quantify the capabilities of the emerging anomaly detection techniques in an unbiased and fully transparent way.", "sections": [{"title": "Background & Summary", "content": "The anomaly detection (AD) domain encompasses a diverse array of methodologies for the identification of anomalous patterns in data of various modalities. These approaches can be applied to a multitude of data types, including images, text, and time series data, among others. However, the development and evaluation of real-world anomaly detection applications are dependent on the availability of real-world data. Currently, there is a considerable number of datasets available for a wide range of scenarios\u00b9, but the satellite telemetry data for AD is an extremely underrepresented category in this catalogue. This kind of data is difficult and costly to obtain, often confidential, and requires expert knowledge to annotate properly. The only two widely accessible and used collections of this type include the NASA Soil Moisture Active Passive (SMAP) and Mars Science Laboratory (MSL) datasets\u00b2. They offer short fragments of signals and related commands from 55 and 27 telemetry parameters, respectively, with a total of 105 annotated anomalies. However, the recent consensus in the community is that they should not be used for time series AD benchmarking due to their unrealistic anomaly density, many trivial anomalies, mislabelled ground truth, distributional shifts, and a lack of meaningful correlation between commands and channels3\u20135. Other well-known satellite telemetry datasets, such as Mars Express6 or NASA WebTCAD7, do not contain annotations of anomalous events. There is an ongoing activity to publish a large-scale AD dataset by European Space Agency (ESA) solving all the mentioned issues8,9, but it will primarily address the needs of large-scale, complex and relatively stable missions.\nThe dataset introduced in this article, dubbed OPSSAT-AD, is fundamentally different from those available in the literature, as it tackles a very specific ESA OPS-SAT mission\u2014a CubeSat flying laboratory, for which we might expect a noticeable number of abnormal events10. The raw telemetry from OPS-SAT is characterized by many data gaps, artifacts, sampling frequency changes, and signal amplitude variations. The dataset was collectively curated by space operations engineers and machine learning experts to make it useful for building and validating data-driven anomaly detection techniques. It includes a selection and the corresponding ground-truth annotation of 2123 short single-channel satellite telemetry fragments (univariate time series) captured within 9 telemetry channels. Due to the underlying nature of the OPS-SAT mission, anomalous fragments account for 20% of the dataset. Such fragments contain raw data with many aforementioned real-life challenges, and they differ in their length and sampling frequency. For each telemetry fragment, the dataset also contains a set of 18 handcrafted features used in the actual machine learning AD algorithm validated on board OPS-SAT11. These features are exploited in this article to benchmark 30 other supervised and unsupervised machine learning algorithms for anomaly detection. All of them were trained on 1494 and tested on 529 telemetry segments, and assessed using 7 metrics suggested for quantifying the operational capabilities of anomaly detection algorithms\u2014this training-test dataset split is included in our benchmark as well.\nOverall, the benchmark (including the dataset, training-test dataset split, suggested quality metrics, and our baseline results) introduced in this paper shall help the community to create and compare their approaches to detecting anomalies in real-life satellite telemetry in a fair and unbiased way. Therefore, we also address the reproducibility crisis currently observed in the (not only) machine learning community12. While the OPS-SAT spacecraft completed its atmospheric reentry at the end of May 2024, its successor\u2014OPS-SAT VOLT-is going to be launched in late 2025 and will make a great opportunity to validate the algorithms developed based on our benchmark in the wild after deploying them on-board an operational satellite."}, {"title": "Methods", "content": "The telemetry data delivered13 in this paper was acquired from the ESA OPS-SAT satellite (Figure 1). It is a small 3-unit (3U, where 1U=10cm\u00b3) CubeSat launched in December 2019 with the primary objective of being a technological demonstrator for in-orbit data processing. It finished its mission with the atmospheric reentry on 22 May 2024, but it generated lots of useful data during more than 4 years of its operations, including satellite imagery14 and telemetry11.\nOPS-SAT offered a unique opportunity for researchers to run their experiments and algorithms in orbit. While these experiments were carried out, all telemetry data was simultaneously collected and recorded in the ESA archive. The archive was monitored for potential anomalies to ensure the mission's stable and uninterrupted operation. Our dataset consists of telemetry fragments recommended by the OPS-SAT operation engineers as the most \"interesting\" (according to their subjective assessment) for anomaly detection. The actual data collection process was carried out using the data exchange platform WebMUST15 used in the European Space Operations Centre (ESOC). This platform is restricted to the authorized ESA partners only, but the data included in our dataset package does not have to be requested through it, and thus is made publicly available.\nThe online OXI tool for visualization and annotation of satellite telemetry (https://oxi.kplabs.pl/)16 was used to enable a collaborative labeling process of the dataset. Using this application, domain experts were able to manually extract and annotate telemetry segments representing periods of nominal and anomalous operation. The initial selection of anomalies was provided by 3 ESA spacecraft operations engineers and further curated by 2 machine learning experts (with more than 10 years of experience each). The curated annotations were finally reviewed by the three spacecraft operations engineers. The detailed satellite telemetry annotation process, together with the visual artefacts generated throughout it, are discussed in11,17,18."}, {"title": "Feature extraction", "content": "Due to the characteristics of satellite telemetry, the segments of raw data selected by the domain experts have varying lengths and sampling frequency. As such, they could not be handled by most machine learning algorithms without performing an additional preprocessing or feature extraction. Thus, 18 handcrafted features were designed for the task of anomaly detection\u00b9\u00b9 they were calculated separately for each segment, and they are included in our benchmark. An algorithm operating on such features was already validated in our previous work focusing on the application of data-driven anomaly detection on board OPS-SAT11. The features extracted for each telemetry segment are presented in Figure 2. They are divided into three groups:\n\u2022 12 features extracted from raw segments, including basic statistics, such as the arithmetic average of the signal values, their standard deviation, skewness, kurtosis and variance ($\\langle mean\\rangle$, $\\langle std\\rangle$, $\\langle skew\\rangle$, $\\langle kurtosis\\rangle$, and $\\langle var\\rangle$), but also the number of peaks (of the minimum of 10% prominence, with a peak prominence measuring how much a peak \u201cstands out\" in relation to the signal, while considering its height and location: $\\langle n\\_peaks\\rangle$), duration (in seconds: $\\langle duration\\rangle$) and the length (in the number of telemetry points: $\\langle len\\rangle$), the weighted length (weighted by sampling: $\\langle len\\_weighted\\rangle$), the gaps' length (the squared number of missing data points: $\\langle gaps\\_squared\\rangle$), and the weighted variance (weighted by the duration and by the length: $\\langle var\\_div\\_duration\\rangle$, $\\langle var\\_div\\_len\\rangle$).\n\u2022 2 features extracted from the smoothed segments (using the uniform interpolation19), including the number of peaks (extracted using the 10 and 20 points smoothing steps: $\\langle smooth10\\_n\\_peaks\\rangle$, $\\langle smooth20\\_n\\_peak\\rangle$).\n\u2022 4 features extracted from the first and the second derivatives of the segment, including the number of peaks and variance ($\\langle diff\\_peaks\\rangle$, $\\langle dif f2\\_peaks\\rangle$, $\\langle diff\\_var\\rangle$, $\\langle dif f2\\_var\\rangle$).\nEmploying the duration, the length and the gaps' length features should allow the algorithms to easily capture some \"obvious\" abnormalities in the telemetry data. This intuitively could lead to promote some less computationally demanding AD methods suitable for on-board applications. The proposed set of features serves as an example and may be easily expanded (or replaced) by the community by (i) designing new feature extractors (potentially followed by feature selectors), (ii) using other well-established feature sets20 or (iii) benefiting from the automated feature learning21."}, {"title": "Benchmarking procedure", "content": "We provide a procedure that should be followed to confront the AD algorithms over our dataset. The entire dataset of 2123 telemetry segments is split into the training (T) and test (\u03a8) sets (Table 1), forming an AI-ready dataset. To extract these subsets, we performed the stratified random sampling to maintain the original percentage of anomalies in both T and \u03a8."}, {"title": "Quality metrics", "content": "The following metrics should always be calculated over the test set \u03a8 while confronting the AD algorithms (both supervised and unsupervised) over the dataset OPSSAT-AD introduced in this work:\n\u2022 Accuracy: $(TP+TN)/(TP+TN+FP+FN)$,\n\u2022 Precision: $TP/(TP+FP)$,\n\u2022 Recall: $TP/(TP+FN)$,\n\u2022 F\u2081 score: $(2\\cdot precision \\cdot recall)/(precision + recall)$,\n\u2022 Matthews' Correlation Coefficient (MCC)22: $(TP\\cdot TN-FP\\cdot FN)/\\sqrt{(TP+FP)\\cdot(TP+FN) \\cdot (TN+FP) \\cdot (TN+FN)}$,\n\u2022 Area under the receiver operating characteristic curve (AUCROC),\n\u2022 Area under the precision-recall curve (AUCPR),\nwhere TP, TN, FP, and FN are the number of true positives (anomalous telemetry segments correctly identified as anomalies), true negatives (nominal telemetry segments correctly identified as nominal), false positives (nominal telemetry segments incorrectly identified as anomalies), and false negatives (anomalous telemetry segments incorrectly identified as nominal). All metrics should be maximized (\u2191), with one indicating the best score (MCC ranges from -1 to 1, other metrics from 0 to 1)."}, {"title": "The baseline: anomaly detection algorithms", "content": "Although there are ground-truth AD datasets that may be used to train supervised models for this task, they are extremely limited and, by definition, they cannot capture a representative set of anomalies (otherwise such \u201canomalies\" would not be \u201canomalies\" any longer). In practice, while building data-driven AD algorithms for satellite telemetry, practitioners may not be able to access real-life ground-truth data, hence unsupervised methods have been gaining research attention. Here, we establish a set of baseline results obtained using 30 AD methods, including both supervised and unsupervised algorithms (Table 2).\nFor all those algorithms, implemented in the PyOD framework51 (https://pyod.readthedocs.io/en/latest/), the default parameters (suggested by the authors of these techniques) are used, with the anomaly contamination factor set to 0.2, according to the anomaly distribution observed in T. To ensure reproducibility, we provide a Jupyter Notebook showing how to execute an example AD algorithm, in a both supervised and unsupervised training regime (modeling_examples.ipynb)."}, {"title": "Dataset Layout", "content": "The dataset13 is built of 9 source telemetry channels that were selected by the space operations engineers. They include 3 magnetometer telemetry channels: I_B_FB_MM_0 (CADC0872), I_B_FB_MM_1 (CADC0873), I_B_FB_MM_2 (CADC0874), and 6 photo diode (PD) channels: I_PD1_THETA (CADC0884), I_PD2_THETA (CADC0886), I_PD3_THETA (CADC0888), I_PD4_THETA (CADC0890), I_PD5_THETA (CADC0892), I_PD6_THETA (CADC0894). Here, the names correspond to the source names from the WebMUST repository and the OPS-SAT telemetry channel names (in brackets). The layout of the dataset is summarized in Figure 3-it includes both the raw files, as well as the extracted features in a tabular form."}, {"title": "Raw telemetry data", "content": "In Figure 4, we visualize selected characteristics of the acquired telemetry signals, effectively showing real-world challenges concerned with telemetry data acquired in the wild (e.g., missing readouts, different sampling frequencies). Such segments for all the aforementioned telemetry channels and their selected parts are included in the data/segments.csv file. It contains the attributes that identify the registration time: $\\langle timestamp\\rangle$ (ISO date format), $\\langle channel\\rangle$ (the channel name), $\\langle value\\rangle$ (the acquired signal value), and $\\langle label\\rangle$ (the ground-truth annotation). Additionally, we provide the consecutive segment numbers $\\langle segment\\rangle$, their sampling rate $\\langle sampling\\rangle$, and the indication if they are included in T $\\langle train\\rangle$."}, {"title": "Extracted features", "content": "In the tabular version of the dataset (data/dataset.csv), we include the extracted features. In the Supplementary Materials, we present the distributions of all of the provided features, rendered for both T and Y sets of our dataset (Figure 7)."}, {"title": "Experimental Validation", "content": "In Table 3, we aggregate the results obtained using all the investigated AD algorithms. Here, we highlighted the globally best results (in bold for each quality metric), and we underlined the best results elaborated by the unsupervised algorithms, as we consider them a different category of the AD solutions. We are aware that some algorithms from PyOD should be rather trained using nominal data only (i.e., OCSVM or autoencoders) to achieve better results. As an example, the OCSVM model achieves AUCPR of 0.659 and AUCROC of 0.787 in our setting, but when using only the nominal data (without abnormal segments) for training, the corresponding values are 0.762 and 0.815. However, we wanted our baseline to be consistent and to reflect a typical usage of the PyOD framework by a non-expert user. Also, the fine-tuning of those algorithms is out of the scope of this study. In Figure 5, we render the selected metrics for each model. We can indeed observe the better performance of supervised methods, as those could actively benefit from the labeled anomaly examples while building a machine learning model. In Figure 6, we also display the precision and recall quality metrics. For the fully-connected neural network, we can observe only four false positives and eight false negatives of all Y samples, reaching the precision of 0.963, and the recall of 0.929.\nThe investigation of the unsupervised algorithms reveals that some of them reach a point where they return a small set of mistakenly assessed telemetry samples. Especially the detectors built upon MO-GAAL, SO-GAAL and AnoGAN offered high precision. In terms of the number of misclassified examples, MO-GAAL obtained a better result than the supervised methods, and made one less false detection when compared to FCNN. A number of other unsupervised algorithms, however, tend to either return a large number of false negatives, or to raise many false alarms with a low number of false negatives. In the first group of such methods, we can observe DIF, ALAD, DeepSVDD, and AnoGAN, whereas e.g., COF belongs to the second group here. The usability of such algorithms would be rather limited to situations when avoiding one type of the classification error could be more practically important (e.g., to minimize the overhead induced on the space operations teams that would have to review many incorrectly raised false alarms)."}, {"title": "Usage Notes", "content": "The dataset13 contains the data in two different forms: a set of the original telemetry segments and a corresponding set of handcrafted features, both with anomaly labels. Both collections are also encoded in the popular, easy-to-handle CSV format and are ready to to use with various machine learning models (thus, they can be considered AI-ready). All the algorithms were implemented in Python using PyOD52 1.1.2, TensorFlow53 2.15, and PyTorch54 2.1.2. Additionally, we used NumPy55 1.26.2 and Pandas56 2.1.14 for data preparation, Seaborn57 0.13.0 for the visualizations, as well as OXI16 for the initial data analysis and labeling processes. Finally, our benchmark is accompanied with a Jupyter Notebook, containing an example experiment (modeling_examples.ipynb), in order to ensure the experimental reproducibility."}, {"title": "Code Availability", "content": "The code for working with the OPS-SAT benchmark, including the functionalities used to prepare the numerical results, figures, and tables for this article, is available through the following GitHub repository: https://github.com/kplabs-pl/ OPS-SAT-AD under the MIT license."}, {"title": "Author contributions statement", "content": "B.R., K.K. and J.N. conceived and designed the study. B.R. and D.E. acquired the data and performed the experiments. B.R. implemented the computational pipeline and performed the analyses. B.R. drafted the manuscript. K.K. and J.N. edited and improved the manuscript. D.E. revised the manuscript. All authors have read and approved the final manuscript."}, {"title": "Corresponding author", "content": "Correspondence to Bogdan Ruszczak."}, {"title": "Competing interests", "content": "The authors declare no competing interests."}, {"title": "Supplementary Materials", "content": "The distribution of the extracted features elaborated for each telemetry segment included in our dataset is depicted in Figure 7. We compare the distribution for the training (T) and test (\u03a8) sets, to visualize the effect of the training-test dataset split on the feature distributions. Figure 8 provides a detailed view of the relations between the extracted features for the T and Y sets. We rendered this plot to confirm that both subsets represent similar data distributions."}]}