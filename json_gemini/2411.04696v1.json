[{"title": "The Multiple Dimensions of Spuriousness in Machine Learning", "authors": ["Samuel J. Bell", "Skyler Wang"], "abstract": "Learning correlations from data forms the foundation of today's machine learning (ML) and\nartificial intelligence (AI) research. While such an approach enables the automatic discovery of\npatterned relationships within big data corpora, it is susceptible to failure modes when\nunintended correlations are captured. This vulnerability has expanded interest in interrogating\nspuriousness, often critiqued as an impediment to model performance, fairness, and robustness.\nIn this article, we trace deviations from the conventional definition of statistical\nspuriousness\u2014which denotes a non-causal observation arising from either coincidence or\nconfounding variables\u2014to articulate how ML researchers make sense of spuriousness in\npractice. Drawing on a broad survey of ML literature, we conceptualize the \u201cmultiple dimensions\nof spuriousness,\u201d encompassing: relevance (\u201cModels should only use correlations that are\nrelevant to the task.\u201d), generalizability (\u201cModels should only use correlations that generalize to\nunseen data", "Models should only use correlations that a human would use to\nperform the same task\"), and harmfulness (\u201cModels should only use correlations that are not\nharmful\"). These dimensions demonstrate that ML spuriousness goes beyond the\ncausal/non-causal dichotomy and that the disparate interpretative paths researchers choose\ncould meaningfully influence the trajectory of ML development. By underscoring how a\nfundamental problem in ML is contingently negotiated in research contexts, we contribute to\nongoing debates about responsible practices in Al development.\",\n  \"sections\": [\n    {\n      \"title\": \"Introduction\",\n      \"content\": \"Machine learning (ML) forms the backbone of artificial intelligence (AI) research today. Its\nmethods enable researchers to extract patterned relationships in large datasets and make\nsystematic predictions, elevating computationally derived correlations to a position of\nprominence in today's big data era. This new epistemological standard stipulates that\ncomputational brute force can stand in for scientific reasoning, thereby diminishing the role of\n\\\"meaning": "hile advancing the notion that \u201cnumbers speak for themselves\u201d (Kitchin, 2014;\nCalude and Longo, 2016: 595). Within this framework, causality\u2014the fundamental thread\nundergirding knowledge production in most scientific fields\u2014becomes secondary as long as one\ncan identify \"regularities in very large databases\" (Calude and Longo, 2016: 595).\nIn no subtle terms, journalist Kalev Leetaru (2019) argues that the \u201centire Al revolution is built on\n[this] correlation house of cards.\u201d Amongst these correlations, many of them are spurious-they\nemerge as systems uncover \u201cobscure patterns in vast reams of numbers that may have\nabsolutely nothing to do with the phenomena they are supposed to be measuring\u201d (Leetaru,\n2019). In other words, spuriousness arises when an observation stems from coincidence or\nconfounding variables. An often-cited example of this problem involves training an image\nrecognition model designed to differentiate cows from camels (Beery et al., 2018). Because\ncamels are often photographed in the desert and cows on pastures, a classifier trained on\nconventional photographs of these animals has the propensity to over-index the background\ncolor. This leads to a high probability for the model to miscategorize a cow for a camel if the\nformer was photographed against a yellow background (Beery et al., 2018, via Arjovsky et al.,\n2019). This issue, appearing under several guises (e.g., \u201cshortcuts, dataset biases, group\nrobustness, simplicity bias", "2024": 1, "a major issue\"\n(Volodin et al., 2020), spurious correlations are not only considered \u201cproblematic": "Izmailov et\nal., 2022) but are conventionally regarded as a \u201cthreat\u201d to research (Eisenstein, 2022). However,\nspuriousness is typically only investigated when a model's test set captures failure modes\nresulting from its presence. When this occurs, spuriousness is deemed an impediment to\nbenchmark progress and a problem to be resolved. Perhaps unbeknownst to many, today's\nstandard ML pipelines remain ill-equipped to differentiate causation from correlation. This means\nthat when ML researchers deem a set of correlations as \u201cspurious,\u201d they often bypass the\nstatistical definition of \u2018non-causality' and make sense of the problem through alternative lenses.\nHow does this sense-making occur? Asking this question compels us to transcend technical\nconsiderations of spuriousness and confront the epistemic and normative dimensions of the\nissue. More specifically, what makes a correlation 'spurious' and for what reasons is\nspuriousness 'bad'? Akin to Blodgett et al. (2020), who argue that the way \u201cbias\u201d is\nconceptualized in natural language processing (NLP) is thin on theoretical and normative\nengagements, we similarly contend that reducing spuriousness to a one-dimensional (i.e.,"}, {"title": "Situating the Problem: Spuriousness in Machine Learning", "content": "Conceptualizing Correlation & Spuriousness\nWe begin with some definitional groundwork. Discussing correlation requires disambiguating\nbetween two key meanings of the term. In this work, we use correlation in its broadest sense to\ndescribe any observed relationship between two variables. When we say that variable X\ncorrelates with variable Y, we mean that a change in X tends to coincide with a change in Y. For\nexample, a decrease in daily temperature in Montreal (variable X) might correspond to a\ndecrease in the number of people having their morning coffee en terrasse (variable Y). This\nrelationship could be linear, but we might also expect a plateau\u2014no matter how warm the\nweather, at some point, every cafe table will be occupied\u2014indicating that the relationship cannot"}, {"title": "Machine Learning and The 'True Function'", "content": "When fitting an ML model, developers typically operate with an implicit notion of a true function.\nThat is, given some input data x and a desired outcome y, the goal is for the model to\napproximate an idealized, true function, y = f(x). The true function f that the developer envisions\nrepresents their expectations of how the model should behave when exposed to any possible\ninput x. However, because precisely defining this intended true function f is exceedingly\ncomplex, if not practically impossible (Geirhos et al., 2020), the model is optimized to\napproximate f by identifying correlations within the training data.\nMore precisely, given some true function f, model developers seek its approximation, f*, typically\nusing an optimization algorithm to minimize error over the training data. However, given a fixed\ntraining set, there are infinite f' that will perfectly fit the data, and developers only have limited\ncontrol over which specific f* the optimization algorithm will discover. As a result, f* might be a\ngood approximation of f based on the training data, but it is unlikely to perfectly correspond to\nthe f the developer envisioned, particularly when f* is tested on rare or unusual samples. Faced\nwith this vast space of learnable functions, developers often introduce informal auxiliary"}, {"title": "Sense-making and Multidimensionality", "content": "The question of what scientists do when confronted with scientific conundrums is by no means\nnovel; science and technology studies (STS) scholars have documented a wealth of\ndecision-making, organizational, and cultural tools that researchers deploy to help them\novercome uncertainty (Latour and Woolgar, 1979; Guillaume et al., 2017; Kampourakis and\nMcCain, 2019). At the core, these conundrums create perturbations that compel intellectual\nreasoning and sense-making, giving us a window into the social construction of knowledge.\nEven though spuriousness has a simple, clear-cut definition in statistics (i.e., non-causal), how it\nis negotiated in practice is more complex. In our case, when represented with model failures\nstemming from spuriousness, ML researchers must decide how to represent or frame the\nproblem at hand. As our earlier example illustrates, there are multiple ways one could interpret\nthe 'fault' of spuriousness when using ML to detect pneumonia in chest radiograph scans (Zech\net al., 2018). Is the model flawed because it did not use relevant features or failed to diagnose\nthe disease as a doctor would? Or is there another underlying issue at play?\nIn her canonical work on scientific tinkering, Knorr (1979: 352) argues that \u201cscientists\nthemselves constantly classify their experience in terms of \"what makes sense,\" and structure\""}, {"title": "The Multiple Dimensions of Spuriousness", "content": "We explore representations of spuriousness in machine learning research through a\ncomprehensive literature survey. First, we identified relevant works using the keywords\n\"spuriousness\" and \"spurious correlations,\" drawing from prominent ML, NLP, and computer\nvision conferences, workshops, and journals such as NeurIPS, ICML, ICLR, ACL, NAACL,\nEACL, EMNLP, FAccT, and Nature Machine Intelligence, as well as non-peer-reviewed preprints\nfrom arXiv. To broaden the scope, we examined paper bibliographies to capture epistemological\ndiscussions within different research clusters. Our review covered 200 papers in total, of which"}, {"title": "Relevance", "content": "Modern ML methods excel at automatically extracting relationships from raw, unprocessed data\nsuch as images, text, or audio. Often regarded as a key strength of deep learning, \u201cautomatic\nfeature extraction\u201d describes how models can learn to hierarchically transform low-level inputs,\nlike pixels, into increasingly more complex, higher-level, and \u201ctask-relevant\u201d representations\n(Goodfellow et al., 2016). This automatic approach has fully supplanted the time-consuming and\nerror-prone practice of 'feature engineering,' where developers would write code to transform\nraw input data into something appropriate for the model. In this manual process, developers\nwould explicitly state their assumptions\u2014and indeed their expertise\u2014about what aspects of the\ndata were relevant to the task and should be used by the model. In contrast, with automatic\nfeature extraction, models are trained to infer which features are relevant to solving the given\nlearning objective based on the samples observed during training.\nThis shift, however, comes with a trade-off: the correlations considered \u201crelevant", "2022": 1, "not\ninherently relevant to the learning problem,\u201d while Ghosal et al. (2022: 2) claim that spurious\nfeatures are \"statistically informative but do not capture essential cues related to the labels.": "nSimilarly, Kirichenko et al. (2023: 1) describe spurious features as \u201cpredictive of the target in the\ntrain data, but that are irrelevant to the true labeling function,\u201d highlighting the clear role of\nresearcher expectations: spuriousness via relevance is defined in relation to the idealized \u201ctrue\nlabeling function.\u201d A common thread in these examples is the perspective that a learned but\nspurious correlation may be relevant within the training data but becomes less relevant in a\nmore general context or under a different data distribution. Scimeca et al. (2022: 1) note that", "added].": "n this way, the relevance proxy engenders a multipartite\nrelation between the researcher's expectations for the intended task, the training data, and\nfuture possible data distributions (a subject we will return to in \u2018Generalizability').\nPerhaps the most straightforward example of relevance assumptions occurs in computer vision,\nwhere it is commonly assumed that image foregrounds are always pertinent to the task, while\nbackground features are deemed irrelevant and, therefore, spurious. For instance, Singla and\nFeizi (2022) find that flowers commonly co-occur with images of insects in the popular image\nclassification dataset ImageNet (Deng et al., 2009) and suggest that flowers are, therefore,\nspurious because they are not a \u201cpart of the relevant object definition [of insects].\u201d However,"}, {"title": "Generalizability", "content": "The canonical method for evaluating an ML model is to test its predictive performance on\nunseen (or \"held-out\u201d) data. This process is designed to assess whether the model can\ngeneralize what it has learned from the training samples to a \u201ctest set\u201d of samples assumed to\nbe drawn from the same distribution. During training, models are optimized to extract and\nleverage correlations that explain the training data, but only a subset of these correlations will be\nuseful for explaining the test data. Concerning generalizability, correlations that do not hold at"}, {"title": "Human-likeness", "content": "Since its inception, Al research has sought to emulate certain aspects of human intelligence,\nincluding both human-like behavior and human-like mechanisms underlying such behavior.\nTherefore, it's perhaps unsurprising that when ML models fail to meet certain predefined\nexpectations, a common response is to suggest they are not sufficiently \u2018human.' Relationships\nbetween features a human would not use are readily described as spurious, and models that\nleverage such spurious correlations may be seen as failures. For some forms of ML work,\nresearchers expect human-like correlations, and those that do not exhibit human-likeness are\ndeemed spurious.\nFor example, in their highly-cited work exposing the propensity of neural networks to learn\n\"shortcuts\u201d (a term often considered synonymous with spurious correlations), Geirhos et al.\n(2019) point to how contemporary models overly rely on texture-based cues (such as an\nanimal's fur or skin) instead of those based on shape. Appealing directly to human-likeness,\nGeirhos et al. (2020) cite evidence about the well-known human bias toward object shape to\nargue that a \"shape-agnostic decision rule that merely relies on texture properties clearly fails to\ncapture the task of object recognition as it is understood for human vision\u201d (668; emphasis ours).\nSimilarly, Scimeca et al. (2022: 1) critique how \u201cshortcut biases often result in a striking\nqualitative difference between human and machine recognition systems.\u201d In their systematic\nexploration of NLP generalization, Hupkes et al. (2023: 11) note that many researchers seek\nsystems that \u201cabstract away from spurious correlations that may occur in the training data, and\nthat are aligned with the underlying generalizing solution that humans associate with the task."}, {"title": "Harmfulness", "content": "ML models have repeatedly been shown to learn, replicate, and amplify potentially harmful\nbiases in their training data (Stock and Ciss\u00e9, 2018; Bolukbasi et al., 2016; Zhao et al., 2017;\nHendricks et al., 2018). This occurs when models learn correlations involving socially sensitive\nor protected attributes such as race or gender (even when deliberately instructed not to), leading\nto biased decision-making. A well-known example is Amazon's automated candidate screening\ntool, which downranked the r\u00e9sum\u00e9s of women applicants because the r\u00e9sum\u00e9s it was trained\non were predominantly from men (Dastin, 2018). Consequently, models may learn to leverage\ncorrelations that only hold for a subset of the data, resulting in performance disparities when\napplied to other groups or in different conditions. Many common failures can be ascribed to this\nissue, such as voice assistants that fail to respond to the accents of minoritized speakers\n(Harwell, 2018) or autonomous vehicles that struggle to recognize darker-skinned pedestrians\n(Li et al., 2024). From these examples and the extensive body of research documenting bias in\ndata and the biases exhibited by models, we can infer that researchers seek models that avoid\ncausing harm. In other words, they expect their models to learn functions that steer clear of\nharmful correlations.\nML researchers frequently describe correlations and the models that use them in terms of harm.\nFor example, Wang et al. (2019b: 5310) argue that models that are \u201csensitive to spurious\ncorrelations risk amplifying societal stereotypes\u201d. Scimeca et al. (2022: 9) claim that \"relying\non simple cues is sometimes unethical\u201d and describe it as an \u201calarming phenomenon\u201d (where\n\"simple\u201d in this context implies spurious). Several authors discuss the resulting failures, such as\nNagarajan et al.'s (2020: 3) suggestion that relying on spurious correlations \u201ccan also lead to\nunfair biases and poor performance on minority groups\u201d or Hartvigsen et al.'s (2022: 1) assertion\nthat an online toxicity detection system's \u201coverreliance on spurious correlations\u201d leads to a\ndisproportionate number of false positives for minoritized groups.\nAs seen above, researchers use various criteria to determine whether a correlation is harmful\n(and spurious). In some contexts, only correlations that cause representational harm\n(sometimes referred to as \"harmful associations\"; Goyal et al., 2022) are undesirable, whereas,\nin others, model developers may need to avoid any correlation involving a sensitive or protected\nattribute, regardless of whether it leads to representational or allocative harm. For example,\nCelebA (Liu et al., 2015), the popular dataset of images of celebrity faces commonly used in\nspurious correlations research, exhibits a correlation between gender and hair. In CelebA, 95%\nof images of people with blond hair are women, whereas only 5% are men. As a result, models\ntrained to classify hair color tend to infer and use gender as well. This particular correlation is"}, {"title": "Reconciling multiple dimensions", "content": "Above, we illustrated how researchers reason about the desirability of a correlation (i.e., its\nspuriousness) through single dimensions of spuriousness. While focusing on disparate\ndimensions is useful for illustrative purposes, it is also true that researchers rarely consider\nalternative dimensions explicitly. Like the participants in Simons and Chabris' (1999) famous\nselective attention study, who were so focused on counting ball passes that they missed the\nappearance of a gorilla in the experiment, overemphasizing one dimension may lead to\nundertheorizing the entire problem. When faced with diverse and often competing downstream\ncontexts, the various dimensions of spuriousness may hamstring technical decision-making in\nsignificant ways."}, {"title": "Discussion & Conclusion", "content": "The success of modern ML systems hinges on their ability to automatically extract and use\ncorrelations found in large datasets, enabling the development of complex skills and behaviors\nsimply by observing patterns without having to explicitly encode precisely what a model should\ndo. This strength, however, belies two fundamental challenges. First, as datasets grow ever\nlarger, it becomes increasingly difficult to understand which correlations are encoded within\nthem. Second, given the infinite number of possible correlations one could extract from any\ndataset, the learning algorithms used to train models must naturally privilege and prioritize a\nsubset of all available correlations. Together, these challenges lead to models that can\nautomatically learn from data structure but, in reality, often learn functions over which\nresearchers have limited control, resulting in models that leverage the \u201cwrong\u201d correlations.\nWhile these bad correlations may be keenly described as spurious, modern ML lacks effective\ntools to distinguish between causal and non-causal relationships (Sch\u00f6lkopf et al., 2021).\nSidestepping a definition rooted in causality, we argue that researchers instead tend to assess\nthe acceptability of correlations through other dimensions, of which we have identified four in\nthis article: relevance, generalizability, human-likeness, and harmfulness.\nWhile each dimension may support varied reasoning about spuriousness vis-\u00e0-vis correlation\ndesirability, each also brings its own challenges. To determine whether a correlation is relevant,\none needs a clear specification of the task the model intends to solve. In contrast, assessing\ngeneralizability requires reasoning about what data distributions are likely to be\nencountered\u2014or, perhaps more importantly, which ones matter. Similarly, the human-likeness\ndimension depends on a stable understanding of human functioning, while harmfulness hinges\non a specific definition of downstream harms to avoid. Spurious correlation research often fails\nto account for such contingencies, such that research progress may not always translate into\npractical downstream applications (Bell et al., 2024). By skirting causality and instead leveraging\nthe four dimensions explored in this article, researchers can transform the problem of spurious\ncorrelations from a static property of the data into a dynamic relation between both data and\nresearcher expectations. At present, this shift may be uneasy: while ML is replete with technical\napproaches for learning functions from data, reasoning about which functions should be learned\nremains a challenging and underexplored area.\nThe lack of consistent language around spuriousness may directly impact how the ML\ncommunities organize themselves in response to this type of model failure. After all, as the\nfoundational building block of the modern ML revolution, correlations will continue to shape\nresearch progress and outcomes across disciplinary domains. Recognizing that concepts may", "2005": 43, "coherence and polysemy\"\n(Navon, 2024: 21), we offer up the multiple dimensions of spuriousness as a framework to\ntheorize varied research priorities under a unifying lens. However, we want to stress that the\ndimensions delineated in this article are not exhaustive. As extant STS literature illustrates,\nscientific concepts are rarely static. Concepts can change how they get \u201crepresented, used, or\nacted upon": "Navon, 2024: 21) as they interface with shifting epistemic environments and\ncontexts. As ML research evolves to cover increasingly multimodal and agent-based areas of\ninterest, we may encounter new and emerging dimensions of spuriousness that are not explored\nin this current exposition.\nIn their book The Ordinal Society, Fourcade and Healy (2024: 2) cogently note that even when\nwe know that the \u201cdata is bad\u201d and the \u201cresults are spurious,\u201d ML-driven systems are often\ncloaked in a veneer of rationalism and neutrality. By shining a spotlight on the sense-making\nlogics and interpretative nature of ML problem-solving, we can better uncover the"}]