[{"title": "The Multiple Dimensions of Spuriousness in Machine Learning", "authors": ["Samuel J. Bell", "Skyler Wang"], "abstract": "Learning correlations from data forms the foundation of today's machine learning (ML) and artificial intelligence (AI) research. While such an approach enables the automatic discovery of patterned relationships within big data corpora, it is susceptible to failure modes when unintended correlations are captured. This vulnerability has expanded interest in interrogating spuriousness, often critiqued as an impediment to model performance, fairness, and robustness. In this article, we trace deviations from the conventional definition of statistical spuriousness\u2014which denotes a non-causal observation arising from either coincidence or confounding variables\u2014to articulate how ML researchers make sense of spuriousness in practice. Drawing on a broad survey of ML literature, we conceptualize the \u201cmultiple dimensions of spuriousness,\u201d encompassing: relevance (\u201cModels should only use correlations that are relevant to the task.\u201d), generalizability (\u201cModels should only use correlations that generalize to unseen data", "Models should only use correlations that a human would use to perform the same task\"), and harmfulness (\u201cModels should only use correlations that are not harmful\"). These dimensions demonstrate that ML spuriousness goes beyond the causal/non-causal dichotomy and that the disparate interpretative paths researchers choose could meaningfully influence the trajectory of ML development. By underscoring how a fundamental problem in ML is contingently negotiated in research contexts, we contribute to ongoing debates about responsible practices in Al development.\",\n  \"sections\": [\n    {\n      \"title\": \"Introduction\",\n      \"content\": \"Machine learning (ML) forms the backbone of artificial intelligence (AI) research today. Its methods enable researchers to extract patterned relationships in large datasets and make systematic predictions, elevating computationally derived correlations to a position of prominence in today's big data era. This new epistemological standard stipulates that computational brute force can stand in for scientific reasoning, thereby diminishing the role of \\\"meaning": "hile advancing the notion that \u201cnumbers speak for themselves\u201d (Kitchin, 2014; Calude and Longo, 2016: 595). Within this framework, causality\u2014the fundamental thread undergirding knowledge production in most scientific fields\u2014becomes secondary as long as one can identify \"regularities in very large databases\" (Calude and Longo, 2016: 595).\nIn no subtle terms, journalist Kalev Leetaru (2019) argues that the \u201centire Al revolution is built on [this] correlation house of cards.\u201d Amongst these correlations, many of them are spurious-they emerge as systems uncover \u201cobscure patterns in vast reams of numbers that may have absolutely nothing to do with the phenomena they are supposed to be measuring\u201d (Leetaru, 2019). In other words, spuriousness arises when an observation stems from coincidence or confounding variables. An often-cited example of this problem involves training an image recognition model designed to differentiate cows from camels (Beery et al., 2018). Because camels are often photographed in the desert and cows on pastures, a classifier trained on conventional photographs of these animals has the propensity to over-index the background color. This leads to a high probability for the model to miscategorize a cow for a camel if the former was photographed against a yellow background (Beery et al., 2018, via Arjovsky et al., 2019). This issue, appearing under several guises (e.g., \u201cshortcuts, dataset biases, group robustness, simplicity bias\"; Ye et al., 2024: 1), demonstrates that the failure to extract distinctive visual concepts from a training set and an over-reliance on rote pattern-matching can have a significant impact on ML outcomes.\nSpuriousness is a longstanding research challenge in ML. Often depicted as \u201ca major issue\" (Volodin et al., 2020), spurious correlations are not only considered \u201cproblematic\u201d (Izmailov et al., 2022) but are conventionally regarded as a \u201cthreat\u201d to research (Eisenstein, 2022). However, spuriousness is typically only investigated when a model's test set captures failure modes resulting from its presence. When this occurs, spuriousness is deemed an impediment to benchmark progress and a problem to be resolved. Perhaps unbeknownst to many, today's standard ML pipelines remain ill-equipped to differentiate causation from correlation. This means that when ML researchers deem a set of correlations as \u201cspurious,\u201d they often bypass the statistical definition of \u2018non-causality' and make sense of the problem through alternative lenses.\nHow does this sense-making occur? Asking this question compels us to transcend technical considerations of spuriousness and confront the epistemic and normative dimensions of the issue. More specifically, what makes a correlation 'spurious' and for what reasons is spuriousness 'bad'? Akin to Blodgett et al. (2020), who argue that the way \u201cbias\u201d is conceptualized in natural language processing (NLP) is thin on theoretical and normative engagements, we similarly contend that reducing spuriousness to a one-dimensional (i.e.,"}, {"title": null, "content": "technical) issue while skirting its underlying epistemic reasonings has troubling implications for ML research.\nTake another example-Zech et al.'s (2018) study, which examines the effectiveness of using deep learning models to detect pneumonia from chest X-rays across three American hospitals. Upon examining activation heatmaps of their trained model, the authors found that instead of identifying lung pathologies, the model \u201clearned to detect a metal token that radiology technicians place on the patient in the corner of the image field of view at the time they capture the image\u201d (11). Because technicians from different hospitals place these tokens in slightly different positions, the model uses this \u201cstrong feature\u201d to predict disease prevalence. While we recognize the presence of spuriousness (i.e., non-causality) in this case, how should the problem be assessed? Is the model flawed because 1) it picked up a feature that is not relevant to the task at hand, 2) it classified in a way that a physician (or human) would not, 3) it lacks the ability to generalize to new contexts (i.e., scans from different patients from different hospitals), or 4) its potential for misclassification could lead to medical harm when deployed in clinical settings?\nThrough a schematic analysis of ML literature on spuriousness, we argue while the issue is rooted in the statistical notion of non-causality, ML researchers interpret it through four (non-exhaustive) dimensions: relevance, generalizability, human-likeness, and harmfulness. We further illustrate that spuriousness is not a singular problem but a multifaceted concept with often competing dimensions. Depending on the research question and context, certain dimensions of spuriousness may be more germane than others. In sum, the goal of this article is twofold. First, we typologize these dimensions to demonstrate the contingent interpretative paths researchers take to make sense of a complex problem, each with unique technical prescriptions and normative implications. By dissecting the epistemic practices that surround this active research area, we contribute to ongoing discussions around the norms and research ethics of ML. Second, as Al's encroachment on society becomes increasingly widespread, we hope that this conceptual framework can provide practical aid for ML researchers aiming to build more robust systems, while also supporting more meaningful theoretical explorations of the limits of a correlation-based ML."}, {"title": "Situating the Problem: Spuriousness in Machine Learning", "content": "Conceptualizing Correlation & Spuriousness\nWe begin with some definitional groundwork. Discussing correlation requires disambiguating between two key meanings of the term. In this work, we use correlation in its broadest sense to describe any observed relationship between two variables. When we say that variable X correlates with variable Y, we mean that a change in X tends to coincide with a change in Y. For example, a decrease in daily temperature in Montreal (variable X) might correspond to a decrease in the number of people having their morning coffee en terrasse (variable Y). This relationship could be linear, but we might also expect a plateau\u2014no matter how warm the weather, at some point, every cafe table will be occupied\u2014indicating that the relationship cannot"}, {"title": null, "content": "be represented by a straight line. Correlation can refer to any such observed relationship, whether linear or nonlinear, strong or weak, positive or negative. In a second, more narrow sense, correlation may also refer to specific measures of the strength of certain types of relationships, such as Pearson's product-moment correlation coefficient (PMCC) for linear relationships.\nIf correlation describes any relationship observed in the data, then investigating why the relationship was observed becomes crucial. The correlation could be causal\u2014meaning that a change in variable X directly causes a change in variable Y (e.g., long periods of drought causing poor agricultural yields)\u2014or it could be non-causal, perhaps rising by chance or due to the presence of a third variable, Z, which itself causes a change in both Y and X. In statistics and most scientific disciplines, such a non-causal correlation is referred to as a spurious correlation (Pearl, 2000).\nIn Figure 1, we present several illustrative examples, including causal relationships such as between the moon's phase and the tidal range (Fig. 1b) or between phase and nighttime luminance (Fig. 1c). We also highlight spurious correlations: one due to sampling error, between phase and air pollution (Fig. 1e), and another due to a third variable, between phase and nighttime luminance (Fig. 1f). This last correlation is spurious because altering the tide would not impact nighttime light directly; instead, the relationship is better explained by a third variable, the lunar phase. Despite being technically spurious, the relationship likely reflects a stable natural phenomenon and, as such, could still prove useful. For instance, if the goal were to predict tide changes, a model using peak nighttime illuminance might fare reasonably well.\nIf a correlation should be causal in order not to be spurious, then evaluating spuriousness inherently involves the challenge of determining causation. While the nature of causality is a matter of ongoing debate among scientists, a popular view suggests that a causal relationship is one that can be manipulated. That is, a correlation between two variables is causal if intervening upon one variable\u2014by changing its value\u2014also results in a change in the other.\nUnfortunately, for ML, where models are trained on samples of data representing a static snapshot of the world, such interventions are out of reach. Instead, ML must typically make do with attempting to infer causal structure from observational data (Pearl, 2000). While it should be \"in principle possible\u201d (Lopez-Paz et al., 2017: 6980) for algorithms to extract causal structure from natural data such as images, others have argued this is tantamount to a \u201chopeless task\" (Gelman, 2011: 960), incompatible with the fundamental assumptions underpinning standard ML (Sch\u00f6lkopf et al., 2012, Sch\u00f6lkopf et al. 2021), and \u201conly doable in rather limited situations\" (Peters et al., 2017: xii). Although causal inference remains a strong and active research area within ML (see, e.g., Peters et al., 2017), today's state-of-the-art ML models rarely benefit from such ideas (Marcus, 2018), likely due to the exceptionally difficult nature of causal inference problems (Peters et al., 2017: xii), a lack of demonstrable advantages (Sch\u00f6lkopf et al., 2021), and the continued success of non-causal alternatives."}, {"title": "Machine Learning and The 'True Function'", "content": "When fitting an ML model, developers typically operate with an implicit notion of a true function. That is, given some input data x and a desired outcome y, the goal is for the model to approximate an idealized, true function, $y = f(x)$. The true function f that the developer envisions represents their expectations of how the model should behave when exposed to any possible input x. However, because precisely defining this intended true function f is exceedingly complex, if not practically impossible (Geirhos et al., 2020), the model is optimized to approximate f by identifying correlations within the training data.\nMore precisely, given some true function f, model developers seek its approximation, $f*$, typically using an optimization algorithm to minimize error over the training data. However, given a fixed training set, there are infinite $f'$ that will perfectly fit the data, and developers only have limited control over which specific $f*$ the optimization algorithm will discover. As a result, $f*$ might be a good approximation of f based on the training data, but it is unlikely to perfectly correspond to the f the developer envisioned, particularly when $f*$ is tested on rare or unusual samples. Faced with this vast space of learnable functions, developers often introduce informal auxiliary"}, {"title": null, "content": "objectives-for example, ensuring that the learned function only relies on a specific subset of available features.\nTake, for instance, the aforementioned cow vs. camel recognition case study. For this classifier, the developer envisions a true function f that, given an image of a cow, outputs the label \"cow,\" and when given an image of a camel, outputs the label \"camel.\" The true function should perform consistently across all possible conditions\u2014no matter how unusual the cow or camel, the weather, the background scene, the image quality, or any other factor. With this notion of the true function in mind, the developer gathers a training set of cow and camel images and uses a learning algorithm to train a model $f*$ that captures the correlations between features and labels in the training set. However, when tested more broadly, the developer is surprised to find that the cow vs. camel classifier fails when presented with images of cows in the desert and camels on grassy pastures. Instead of learning the developer's intended true function, the model has learned to exploit a particular correlation in the training data: cows are often photographed on grassy backgrounds and camels in deserts. According to the training data, $f*$ is a good approximation of f, but according to the developer's intended true function, $f*$ is no longer satisfactory.\nUltimately, this example perfectly encapsulates the challenge that spurious correlations pose to the modern ML paradigm. While there is a correlation between grass and cows in the training data, and perhaps in the real world, this is not the correlation the developer intended the model to learn. In this sense, the question of which function should be learned and which correlations should be prioritized is contingent upon the developer's intent and expectations. Given this, what do researchers do when there is a mismatch between intent and reality? In other words, how do ML researchers reconcile the expectation and outcome gap vis-\u00e0-vis spuriousness?"}, {"title": "Sense-making and Multidimensionality", "content": "The question of what scientists do when confronted with scientific conundrums is by no means novel; science and technology studies (STS) scholars have documented a wealth of decision-making, organizational, and cultural tools that researchers deploy to help them overcome uncertainty (Latour and Woolgar, 1979; Guillaume et al., 2017; Kampourakis and McCain, 2019). At the core, these conundrums create perturbations that compel intellectual reasoning and sense-making, giving us a window into the social construction of knowledge.\nEven though spuriousness has a simple, clear-cut definition in statistics (i.e., non-causal), how it is negotiated in practice is more complex. In our case, when represented with model failures stemming from spuriousness, ML researchers must decide how to represent or frame the problem at hand. As our earlier example illustrates, there are multiple ways one could interpret the 'fault' of spuriousness when using ML to detect pneumonia in chest radiograph scans (Zech et al., 2018). Is the model flawed because it did not use relevant features or failed to diagnose the disease as a doctor would? Or is there another underlying issue at play?\nIn her canonical work on scientific tinkering, Knorr (1979: 352) argues that \u201cscientists themselves constantly classify their experience in terms of \"what makes sense,\" and structure"}, {"title": null, "content": "their activities in response to questions of how to make sense of their results.\u201d For ML researchers, making sense of spuriousness involves framing the problem to help them chart a strategic course of action. Through \u201cboundary setting\u201d (Hoffman, 2011), framing defines the problem type, determines what constitutes evidence, and guides how resources are allocated to address the problem (Halffman, 2019; Vazquez et al., 2021). As succinctly put by Entman (2007: 164), framing is \"the process of culling a few elements of a perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation.\"\nThe word \"interpretation\u201d here is noteworthy. In contrast to the \u201cstorybook image of science", "social arbitrariness": "nherent in research and that, rather than uncovering absolute truths, researchers deploy \u201ctricks\u201d and make a host of decisions to \u201carrive at\u201d certain truths (Knorr 1979: 347, 352), we assert that how ML researchers pin down a frame may reflect such dynamics. Research priorities are often shaped by personal training, network effects, organizational contexts, and structural incentives (Wang et al., 2024). A lab with social science expertise may be more inclined to investigate how \u201chuman\" something is, whereas a lab lacking such expertise may prioritize other ways of situated seeing.\nWe theorize this research phenomenon by advancing what we call the \u201cmultiple dimensions of spuriousness.\u201d Thinking in multidimensional terms steers one away from static or singular definitions of complex ideas and underscores alternative frames and explanations. Previous literature exploiting the multidimensionality framework argues that identifying conflicts and interdependencies between dimensions and how different dimensions matter across contexts can meaningfully bolster the robustness of a research program (see Roth, 2016). By mapping out classifications across a wide range of ML papers, this article typologizes frames that share structural similarities. Furthermore, for a term that has garnered significant research attention in recent years, examining how researchers generate multiple interpretations of spuriousness can reveal the hidden priorities and contingencies embedded within ML communities today."}, {"title": "The Multiple Dimensions of Spuriousness", "content": "We explore representations of spuriousness in machine learning research through a comprehensive literature survey. First, we identified relevant works using the keywords \"spuriousness\" and \"spurious correlations,\" drawing from prominent ML, NLP, and computer vision conferences, workshops, and journals such as NeurIPS, ICML, ICLR, ACL, NAACL, EACL, EMNLP, FAccT, and Nature Machine Intelligence, as well as non-peer-reviewed preprints from arXiv. To broaden the scope, we examined paper bibliographies to capture epistemological discussions within different research clusters. Our review covered 200 papers in total, of which"}, {"title": null, "content": "65 were selected for in-depth analysis. These papers spanned a range of topics, including those that define correlations, patterns, and biases, and those that address their mitigation to build more robust models.\nFor analysis, we engaged in an iterative process of coding and discussion. Noticing that ML researchers rarely address causal relationships directly in their papers, we developed a set of labels to capture how spuriousness is typically framed. We identify four such dimensions: relevance, generalizability, human-likeness, and harmfulness. Table 1 outlines these dimensions, along with typical rationales used to explain them and illustrative examples.\nWhile non-exhaustive, these four dimensions capture the most common ways spuriousness is framed in ML literature. While researchers occasionally explicitly define spuriousness in these terms, they more often leave it loosely defined and use these dimensions as motivations for developing solutions to address it. When doing so, many appear to overemphasize certain dimensions while omitting other candidate dimensions. Although 'spurious' literally suggests something incorrect or fallacious, the dimensions primarily reflect developers' subjective, context-dependent, and often conflicting demands for their systems' behavior. Beyond the typology, we also offer critiques of these conceptualizations within the context of ML research below. As we will demonstrate, what makes a correlation spurious is rarely a property of the data itself; it is more often a reflection of the developer's or user's intent or expectations. Spuriousness, it seems, is in the eye of the beholder."}, {"title": "Relevance", "content": "Modern ML methods excel at automatically extracting relationships from raw, unprocessed data such as images, text, or audio. Often regarded as a key strength of deep learning, \u201cautomatic feature extraction\u201d describes how models can learn to hierarchically transform low-level inputs, like pixels, into increasingly more complex, higher-level, and \u201ctask-relevant\u201d representations (Goodfellow et al., 2016). This automatic approach has fully supplanted the time-consuming and error-prone practice of 'feature engineering,' where developers would write code to transform raw input data into something appropriate for the model. In this manual process, developers would explicitly state their assumptions\u2014and indeed their expertise\u2014about what aspects of the data were relevant to the task and should be used by the model. In contrast, with automatic feature extraction, models are trained to infer which features are relevant to solving the given learning objective based on the samples observed during training.\nThis shift, however, comes with a trade-off: the correlations considered \u201crelevant", "2022": 1, "not inherently relevant to the learning problem,\u201d while Ghosal et al. (2022: 2) claim that spurious features are ": "tatistically informative but do not capture essential cues related to the labels.", "2023": 1, "predictive of the target in the train data, but that are irrelevant to the true labeling function,": "ighlighting the clear role of researcher expectations: spuriousness via relevance is defined in relation to the idealized \u201ctrue labeling function.\u201d A common thread in these examples is the perspective that a learned but spurious correlation may be relevant within the training data but becomes less relevant in a more general context or under a different data distribution. Scimeca et al. (2022: 1) note that", "added].": "n this way, the relevance proxy engenders a multipartite relation between the researcher's expectations for the intended task, the training data, and future possible data distributions (a subject we will return to in \u2018Generalizability').\nPerhaps the most straightforward example of relevance assumptions occurs in computer vision, where it is commonly assumed that image foregrounds are always pertinent to the task, while background features are deemed irrelevant and, therefore, spurious. For instance, Singla and Feizi (2022) find that flowers commonly co-occur with images of insects in the popular image classification dataset ImageNet (Deng et al., 2009) and suggest that flowers are, therefore, spurious because they are not a \u201cpart of the relevant object definition [of insects].\u201d However,"}, {"title": "Generalizability", "content": "The canonical method for evaluating an ML model is to test its predictive performance on unseen (or \"held-out\u201d) data. This process is designed to assess whether the model can generalize what it has learned from the training samples to a \u201ctest set\u201d of samples assumed to be drawn from the same distribution. During training, models are optimized to extract and leverage correlations that explain the training data, but only a subset of these correlations will be useful for explaining the test data. Concerning generalizability, correlations that do not hold at"}, {"title": null, "content": "test time are typically described as spurious, and any model relying on these is said to have overfit the training data. Thus, we define the second dimension of spuriousness as generalizability, because researchers expect models to learn correlations that can generalize beyond the training data.\nTurning to the literature, Nagarajan et al. (2020: 1) describe how models often leverage features that are \"spuriously correlated with the label only during training time, resulting in poor accuracy during test-time.\u201d Similarly, Yang et al. (2023: 3) define spurious correlations as those that hold in \"training but not in test data,\u201d while for Joshi et al. (2023: 1), they are \u201cfeatures in the training set correlated with a given class, but are not predictive of class membership.\u201d These examples clearly illustrate the generalizability dimension of spuriousness in action.\nIncreasingly, ML researchers apply a more stringent test for generalization, constructing out-of-distribution test sets where samples are drawn from meaningfully different distributions. These \u201cstress tests\u201d (Lopez-Paz et al., 2022) assess not only how the model performs on unseen test data but also how it handles important conditions that may be completely absent from both the training data and standard test sets. For example, researchers may test whether an image classifier trained on photographs of objects can generalize to (i.e., perform equally well on) hand-drawn sketches of the same set of objects (Wang et al., 2019a). Unfortunately, it is rarely the case that models achieve such generalization; instead, they tend to learn correlations that hold in the training set-and perhaps in the test set-but do not hold out-of-distribution. As per the generalizability dimension, these correlations are typically considered spurious.\nResearchers often explicitly state that correlations must be general to avoid being spurious. For example, Ghosal et al. (2022: 1) describe spurious correlations as \u201cmisleading heuristics within the training dataset [that] do not hold in general.\u201d Similarly, Arjovsky et al. (2019: 1,) argue that correlations must be \u201cstable,\u201d and a correlation is spurious if \"we do not expect it to hold in the future in the same manner as it held in the past.\u201d Pezeshki et al. (2024: 1) refer to spurious correlations as \u201cenvironment-specific,\u201d meaning they only hold under a subset of contexts likely to be encountered, while Sreekumar and Boddeti (2023: 1) argue that for a correlation to be spurious, it must \u201cnot hold under natural distribution shifts [emphasis added].\u201d Geirhos et al.'s (2020: 665) investigation of how models learn shortcuts\u2014decision rules that rely upon spurious correlations-suggests that the challenge is to \"transfer to more challenging testing conditions, such as real-world scenarios.\u201d Here, we see that generalizing to the test set is no longer sufficient; instead, correlations must generalize much further to other data distributions and unseen contexts.\nNote that we describe the generalizability dimension's rationale in Table 1 as only using correlations \"that generalize to unseen data,\u201d but we do not specify precisely to which unseen contexts the function should generalize. This intentional omission highlights a key challenge to the generalizability dimension. By analogy with Wolpert and Macready's (1997) no free lunch, if the requirement that correlations must hold \u201cin general\u201d (Ghosal et al., 2022: 1) implies that they need to hold over all possible future distributions, then one could conclude that all correlations"}, {"title": null, "content": "are, in some way, spurious.\u00b2 Researchers navigating the generalizability dimension may invoke causality when choosing test distributions. Arjovsky et al. (2019: 10) rely on distributions that can be constructed via \u201cvalid interventions,\u201d i.e., those that modify the data's causal structure without changing the outcome variable. Given that the underlying causal graph is almost always unknown, practically designing and implementing this causal approach remains a pivotal challenge. Sreekumar and Boddeti's (2023: 1) restriction to \u201cnatural distribution shifts\" and Geirhos et al.'s (2020: 665) focus on \u201creal world scenarios\u201d might suggest a more productive path forward. Still, both interpretations obfuscate the difficulties of defining exactly which scenarios and shifts are possible and, of these, which are most important.\nIn defining stress tests, researchers must first reasonably characterize an unseen data distribution and subsequently translate this distribution into a set of samples for testing. This process is naturally contingent on both the understanding of future data distributions and the ability to practically implement test sets that are sufficiently representative of those distributions. Recognizing the need for diverse viewpoints, Lopez-Paz et al. (2022) suggest that stress tests be defined by parties with differing interests, including model developers and intended users. By specifying expectations about how models should generalize, researchers must encode which aspects of the data matter and, by extension, determine which correlations are valid and which are spurious."}, {"title": "Human-likeness", "content": "Since its inception, Al research has sought to emulate certain aspects of human intelligence, including both human-like behavior and human-like mechanisms underlying such behavior. Therefore, it's perhaps unsurprising that when ML models fail to meet certain predefined expectations, a common response is to suggest they are not sufficiently \u2018human.' Relationships between features a human would not use are readily described as spurious, and models that leverage such spurious correlations may be seen as failures. For some forms of ML work, researchers expect human-like correlations, and those that do not exhibit human-likeness are deemed spurious.\nFor example, in their highly-cited work exposing the propensity of neural networks to learn \"shortcuts\u201d (a term often considered synonymous with spurious correlations), Geirhos et al. (2019) point to how contemporary models overly rely on texture-based cues (such as an animal's fur or skin) instead of those based on shape. Appealing directly to human-likeness, Geirhos et al. (2020) cite evidence about the well-known human bias toward object shape to argue that a \"shape-agnostic decision rule that merely relies on texture properties clearly fails to capture the task of object recognition as it is understood for human vision\u201d (668; emphasis ours). Similarly, Scimeca et al. (2022: 1) critique how \u201cshortcut biases often result in a striking qualitative difference between human and machine recognition systems.\u201d In their systematic exploration of NLP generalization, Hupkes et al. (2023: 11) note that many researchers seek systems that \u201cabstract away from spurious correlations that may occur in the training data, and that are aligned with the underlying generalizing solution that humans associate with the task.\""}, {"title": null, "content": "In each of these examples, learning to use human-like correlations\u2014correlations that a human uses for inference\u2014remains the underlying goal.\nTake another example: in contemporary ML discourse, complex human faculties such as visual perception are often reduced to computational problems to be solved (Denton et al., 2021). Efforts to ", "2010": 4}, {"title": null, "content": "intelligence (Edwards, 2024). Additionally, we may often desire systems that operate in fundamentally different ways to circumvent the fallible, biased decision-making exhibited by humans. Unlike the relatively straightforward interpretation of spurious as non-causal, an interpretation of spurious as human-like is once again task-dependent and context-specific. In particular, in settings where superhuman or non-human-like performance is desired, this may be incompatible with learning functions that align with human behavior and implement human-like mechanisms."}, {"title": "Harmfulness", "content": "ML models have repeatedly been shown to learn, replicate, and amplify potentially harmful biases in their training data (Stock and Ciss\u00e9, 2018; Bolukbasi et al., 2016; Zhao et al., 2017; Hendricks et al., 2018). This occurs when models learn correlations involving socially sensitive or protected attributes such as race or gender (even when deliberately instructed not to), leading to biased decision-making. A well-known example is Amazon's automated candidate screening tool, which downranked the r\u00e9sum\u00e9s of women applicants because the r\u00e9sum\u00e9s it was trained on were predominantly from men (Dastin, 2018). Consequently, models may learn to leverage correlations that only hold for a subset of the data, resulting in performance disparities when applied to other groups or in different conditions. Many common failures can be ascribed to this issue, such as voice assistants that fail to respond to the accents of minoritized speakers (Harwell, 2018) or autonomous vehicles that struggle to recognize darker-skinned pedestrians (Li et al., 2024). From these examples and the extensive body of research documenting bias in data and the biases exhibited by models, we can infer that researchers seek models that avoid causing harm. In other words, they expect their models to learn functions that steer clear of harmful correlations.\nML researchers frequently describe correlations and the models that use them in terms of harm. For example, Wang et al. (2019b: 5310) argue that models that are \u201csensitive to spurious correlations risk amplifying societal stereotypes\u201d. Scimeca et al. (2022: 9) claim that "}]