[{"title": "The Multiple Dimensions of Spuriousness in Machine Learning", "authors": ["Samuel J. Bell", "Skyler Wang"], "abstract": "Learning correlations from data forms the foundation of today's machine learning (ML) and\nartificial intelligence (AI) research. While such an approach enables the automatic discovery of\npatterned relationships within big data corpora, it is susceptible to failure modes when\nunintended correlations are captured. This vulnerability has expanded interest in interrogating\nspuriousness, often critiqued as an impediment to model performance, fairness, and robustness.\nIn this article, we trace deviations from the conventional definition of statistical\nspuriousness\u2014which denotes a non-causal observation arising from either coincidence or\nconfounding variables\u2014to articulate how ML researchers make sense of spuriousness in\npractice. Drawing on a broad survey of ML literature, we conceptualize the \u201cmultiple dimensions\nof spuriousness,\u201d encompassing: relevance (\u201cModels should only use correlations that are\nrelevant to the task.\u201d), generalizability (\u201cModels should only use correlations that generalize to\nunseen data", "Models should only use correlations that a human would use to\nperform the same task\"), and harmfulness (\u201cModels should only use correlations that are not\nharmful\"). These dimensions demonstrate that ML spuriousness goes beyond the\ncausal/non-causal dichotomy and that the disparate interpretative paths researchers choose\ncould meaningfully influence the trajectory of ML development. By underscoring how a\nfundamental problem in ML is contingently negotiated in research contexts, we contribute to\nongoing debates about responsible practices in Al development.\",\n  \"sections\": [\n    {\n      \"title\": \"Introduction\",\n      \"content\": \"Machine learning (ML) forms the backbone of artificial intelligence (AI) research today. Its\nmethods enable researchers to extract patterned relationships in large datasets and make\nsystematic predictions, elevating computationally derived correlations to a position of\nprominence in today's big data era. This new epistemological standard stipulates that\ncomputational brute force can stand in for scientific reasoning, thereby diminishing the role of\n\\\"meaning": "hile advancing the notion that \u201cnumbers speak for themselves\u201d (Kitchin, 2014;\nCalude and Longo, 2016: 595). Within this framework, causality\u2014the fundamental thread\nundergirding knowledge production in most scientific fields\u2014becomes secondary as long as one\ncan identify \"regularities in very large databases\" (Calude and Longo, 2016: 595).\nIn no subtle terms, journalist Kalev Leetaru (2019) argues that the \u201centire Al revolution is built on\n[this] correlation house of cards.\u201d Amongst these correlations, many of them are spurious-they\nemerge as systems uncover \u201cobscure patterns in vast reams of numbers that may have\nabsolutely nothing to do with the phenomena they are supposed to be measuring\u201d (Leetaru,\n2019). In other words, spuriousness arises when an observation stems from coincidence or\nconfounding variables. An often-cited example of this problem involves training an image\nrecognition model designed to differentiate cows from camels (Beery et al., 2018). Because\ncamels are often photographed in the desert and cows on pastures, a classifier trained on\nconventional photographs of these animals has the propensity to over-index the background\ncolor. This leads to a high probability for the model to miscategorize a cow for a camel if the\nformer was photographed against a yellow background (Beery et al., 2018, via Arjovsky et al.,\n2019). This issue, appearing under several guises (e.g., \u201cshortcuts, dataset biases, group\nrobustness, simplicity bias\"; Ye et al., 2024: 1), demonstrates that the failure to extract distinctive\nvisual concepts from a training set and an over-reliance on rote pattern-matching can have a\nsignificant impact on ML outcomes.\nSpuriousness is a longstanding research challenge in ML. Often depicted as \u201ca major issue\"\n(Volodin et al., 2020), spurious correlations are not only considered \u201cproblematic\u201d (Izmailov et\nal., 2022) but are conventionally regarded as a \u201cthreat\u201d to research (Eisenstein, 2022). However,\nspuriousness is typically only investigated when a model's test set captures failure modes\nresulting from its presence. When this occurs, spuriousness is deemed an impediment to\nbenchmark progress and a problem to be resolved. Perhaps unbeknownst to many, today's\nstandard ML pipelines remain ill-equipped to differentiate causation from correlation. This means\nthat when ML researchers deem a set of correlations as \u201cspurious,\u201d they often bypass the\nstatistical definition of \u2018non-causality' and make sense of the problem through alternative lenses.\nHow does this sense-making occur? Asking this question compels us to transcend technical\nconsiderations of spuriousness and confront the epistemic and normative dimensions of the\nissue. More specifically, what makes a correlation 'spurious' and for what reasons is\nspuriousness 'bad'? Akin to Blodgett et al. (2020), who argue that the way \u201cbias\u201d is\nconceptualized in natural language processing (NLP) is thin on theoretical and normative\nengagements, we similarly contend that reducing spuriousness to a one-dimensional (i.e.,"}, {"title": "Situating the Problem: Spuriousness in Machine Learning", "content": "We begin with some definitional groundwork. Discussing correlation requires disambiguating\nbetween two key meanings of the term. In this work, we use correlation in its broadest sense to\ndescribe any observed relationship between two variables. When we say that variable X\ncorrelates with variable Y, we mean that a change in X tends to coincide with a change in Y. For\nexample, a decrease in daily temperature in Montreal (variable X) might correspond to a\ndecrease in the number of people having their morning coffee en terrasse (variable Y). This\nrelationship could be linear, but we might also expect a plateau\u2014no matter how warm the\nweather, at some point, every cafe table will be occupied\u2014indicating that the relationship cannot"}, {"title": "Conceptualizing Correlation & Spuriousness", "content": "be represented by a straight line. Correlation can refer to any such observed relationship,\nwhether linear or nonlinear, strong or weak, positive or negative. In a second, more narrow\nsense, correlation may also refer to specific measures of the strength of certain types of\nrelationships, such as Pearson's product-moment correlation coefficient (PMCC) for linear\nrelationships.\nIf correlation describes any relationship observed in the data, then investigating why the\nrelationship was observed becomes crucial. The correlation could be causal\u2014meaning that a\nchange in variable X directly causes a change in variable Y (e.g., long periods of drought\ncausing poor agricultural yields)\u2014or it could be non-causal, perhaps rising by chance or due to\nthe presence of a third variable, Z, which itself causes a change in both Y and X. In statistics\nand most scientific disciplines, such a non-causal correlation is referred to as a spurious\ncorrelation (Pearl, 2000).\nIn Figure 1, we present several illustrative examples, including causal relationships such as\nbetween the moon's phase and the tidal range (Fig. 1b) or between phase and nighttime\nluminance (Fig. 1c). We also highlight spurious correlations: one due to sampling error, between\nphase and air pollution (Fig. 1e), and another due to a third variable, between phase and\nnighttime luminance (Fig. 1f). This last correlation is spurious because altering the tide would not\nimpact nighttime light directly; instead, the relationship is better explained by a third variable, the\nlunar phase. Despite being technically spurious, the relationship likely reflects a stable natural\nphenomenon and, as such, could still prove useful. For instance, if the goal were to predict tide\nchanges, a model using peak nighttime illuminance might fare reasonably well.\nIf a correlation should be causal in order not to be spurious, then evaluating spuriousness\ninherently involves the challenge of determining causation. While the nature of causality is a\nmatter of ongoing debate among scientists, a popular view suggests that a causal relationship is\none that can be manipulated. That is, a correlation between two variables is causal if intervening\nupon one variable\u2014by changing its value\u2014also results in a change in the other.\nUnfortunately, for ML, where models are trained on samples of data representing a static\nsnapshot of the world, such interventions are out of reach. Instead, ML must typically make do\nwith attempting to infer causal structure from observational data (Pearl, 2000). While it should\nbe \"in principle possible\u201d (Lopez-Paz et al., 2017: 6980) for algorithms to extract causal structure\nfrom natural data such as images, others have argued this is tantamount to a \u201chopeless task\"\n(Gelman, 2011: 960), incompatible with the fundamental assumptions underpinning standard ML\n(Sch\u00f6lkopf et al., 2012, Sch\u00f6lkopf et al. 2021), and \u201conly doable in rather limited situations\"\n(Peters et al., 2017: xii). Although causal inference remains a strong and active research area\nwithin ML (see, e.g., Peters et al., 2017), today's state-of-the-art ML models rarely benefit from\nsuch ideas (Marcus, 2018), likely due to the exceptionally difficult nature of causal inference\nproblems (Peters et al., 2017: xii), a lack of demonstrable advantages (Sch\u00f6lkopf et al., 2021),\nand the continued success of non-causal alternatives."}, {"title": "Machine Learning and The 'True Function'", "content": "When fitting an ML model, developers typically operate with an implicit notion of a true function.\nThat is, given some input data x and a desired outcome y, the goal is for the model to\napproximate an idealized, true function, \\(y = f(x)\\). The true function \\(f\\) that the developer envisions\nrepresents their expectations of how the model should behave when exposed to any possible\ninput x. However, because precisely defining this intended true function \\(f\\) is exceedingly\ncomplex, if not practically impossible (Geirhos et al., 2020), the model is optimized to\napproximate \\(f\\) by identifying correlations within the training data.\nMore precisely, given some true function \\(f\\), model developers seek its approximation, \\(f^*\\), typically\nusing an optimization algorithm to minimize error over the training data. However, given a fixed\ntraining set, there are infinite \\(f'\\) that will perfectly fit the data, and developers only have limited\ncontrol over which specific \\(f^*\\) the optimization algorithm will discover. As a result, \\(f^*\\) might be a\ngood approximation of \\(f\\) based on the training data, but it is unlikely to perfectly correspond to\nthe \\(f\\) the developer envisioned, particularly when \\(f^*\\) is tested on rare or unusual samples. Faced\nwith this vast space of learnable functions, developers often introduce informal auxiliary"}, {"title": "Sense-making and Multidimensionality", "content": "The question of what scientists do when confronted with scientific conundrums is by no means\nnovel; science and technology studies (STS) scholars have documented a wealth of\ndecision-making, organizational, and cultural tools that researchers deploy to help them\novercome uncertainty (Latour and Woolgar, 1979; Guillaume et al., 2017; Kampourakis and\nMcCain, 2019). At the core, these conundrums create perturbations that compel intellectual\nreasoning and sense-making, giving us a window into the social construction of knowledge.\nEven though spuriousness has a simple, clear-cut definition in statistics (i.e., non-causal), how it\nis negotiated in practice is more complex. In our case, when represented with model failures\nstemming from spuriousness, ML researchers must decide how to represent or frame the\nproblem at hand. As our earlier example illustrates, there are multiple ways one could interpret\nthe 'fault' of spuriousness when using ML to detect pneumonia in chest radiograph scans (Zech\net al., 2018). Is the model flawed because it did not use relevant features or failed to diagnose\nthe disease as a doctor would? Or is there another underlying issue at play?\nIn her canonical work on scientific tinkering, Knorr (1979: 352) argues that \u201cscientists\nthemselves constantly classify their experience in terms of \"what makes sense,\" and structure"}, {"title": "The Multiple Dimensions of Spuriousness", "content": "We explore representations of spuriousness in machine learning research through a\ncomprehensive literature survey. First, we identified relevant works using the keywords\n\"spuriousness\" and \"spurious correlations,\" drawing from prominent ML, NLP, and computer\nvision conferences, workshops, and journals such as NeurIPS, ICML, ICLR, ACL, NAACL,\nEACL, EMNLP, FAccT, and Nature Machine Intelligence, as well as non-peer-reviewed preprints\nfrom arXiv. To broaden the scope, we examined paper bibliographies to capture epistemological\ndiscussions within different research clusters. Our review covered 200 papers in total, of which"}, {"title": "Relevance", "content": "Modern ML methods excel at automatically extracting relationships from raw, unprocessed data\nsuch as images, text, or audio. Often regarded as a key strength of deep learning, \u201cautomatic\nfeature extraction\u201d describes how models can learn to hierarchically transform low-level inputs,\nlike pixels, into increasingly more complex, higher-level, and \u201ctask-relevant\u201d representations\n(Goodfellow et al., 2016). This automatic approach has fully supplanted the time-consuming and\nerror-prone practice of 'feature engineering,' where developers would write code to transform\nraw input data into something appropriate for the model. In this manual process, developers\nwould explicitly state their assumptions\u2014and indeed their expertise\u2014about what aspects of the\ndata were relevant to the task and should be used by the model. In contrast, with automatic\nfeature extraction, models are trained to infer which features are relevant to solving the given\nlearning objective based on the samples observed during training.\nThis shift, however, comes with a trade-off: the correlations considered \u201crelevant\" by the\nmodel-i.e., the relationships the model learns to leverage\u2014do not always align with those a\ndeveloper might deem germane or those a feature engineer would have chosen to extract.\nWhen the model-relevant features diverge from developer-relevant features, researchers often\nlabel the correlation as spurious. In other words, researchers expect their models to use only the\ncorrelations they consider relevant.\nIzmailov et al. (2022: 1) make this view apparent, stating that spurious correlations are \"not\ninherently relevant to the learning problem,\u201d while Ghosal et al. (2022: 2) claim that spurious\nfeatures are \"statistically informative but do not capture essential cues related to the labels.\" \nSimilarly, Kirichenko et al. (2023: 1) describe spurious features as \u201cpredictive of the target in the\ntrain data, but that are irrelevant to the true labeling function,\u201d highlighting the clear role of\nresearcher expectations: spuriousness via relevance is defined in relation to the idealized \u201ctrue\nlabeling function.\u201d A common thread in these examples is the perspective that a learned but\nspurious correlation may be relevant within the training data but becomes less relevant in a\nmore general context or under a different data distribution. Scimeca et al. (2022: 1) note that\n\"DNNs often pick up simple, non-essential cues, which are nonetheless effective within a\nparticular dataset [emphasis added].", "part of the relevant object definition [of insects].": "owever,\n\"relevant\u201d unfortunately remains undefined. In constructing a dataset to test for susceptibility to\nspurious correlations, Sagawa et al.'s (2019) Waterbirds dataset assumes that background is\nirrelevant to distinguishing water-dwelling from land-dwelling birds. The same judgment is at"}, {"title": "Generalizability", "content": "The canonical method for evaluating an ML model is to test its predictive performance on\nunseen (or \"held-out\u201d) data. This process is designed to assess whether the model can\ngeneralize what it has learned from the training samples to a \u201ctest set\u201d of samples assumed to\nbe drawn from the same distribution. During training, models are optimized to extract and\nleverage correlations that explain the training data, but only a subset of these correlations will be\nuseful for explaining the test data. Concerning generalizability, correlations that do not hold at"}, {"title": "Human-likeness", "content": "Since its inception, Al research has sought to emulate certain aspects of human intelligence,\nincluding both human-like behavior and human-like mechanisms underlying such behavior.\nTherefore, it's perhaps unsurprising that when ML models fail to meet certain predefined\nexpectations, a common response is to suggest they are not sufficiently \u2018human.' Relationships\nbetween features a human would not use are readily described as spurious, and models that\nleverage such spurious correlations may be seen as failures. For some forms of ML work,\nresearchers expect human-like correlations, and those that do not exhibit human-likeness are\ndeemed spurious.\nFor example, in their highly-cited work exposing the propensity of neural networks to learn\n\"shortcuts\u201d (a term often considered synonymous with spurious correlations), Geirhos et al.\n(2019) point to how contemporary models overly rely on texture-based cues (such as an\nanimal's fur or skin) instead of those based on shape. Appealing directly to human-likeness,\nGeirhos et al. (2020) cite evidence about the well-known human bias toward object shape to\nargue that a \"shape-agnostic decision rule that merely relies on texture properties clearly fails to\ncapture the task of object recognition as it is understood for human vision\u201d (668; emphasis ours).\nSimilarly, Scimeca et al. (2022: 1) critique how \u201cshortcut biases often result in a striking\nqualitative difference between human and machine recognition systems.\u201d In their systematic\nexploration of NLP generalization, Hupkes et al. (2023: 11) note that many researchers seek\nsystems that \u201cabstract away from spurious correlations that may occur in the training data, and\nthat are aligned with the underlying generalizing solution that humans associate with the task.\""}, {"title": "Harmfulness", "content": "ML models have repeatedly been shown to learn, replicate, and amplify potentially harmful\nbiases in their training data (Stock and Ciss\u00e9, 2018; Bolukbasi et al., 2016; Zhao et al., 2017;\nHendricks et al., 2018). This occurs when models learn correlations involving socially sensitive\nor protected attributes such as race or gender (even when deliberately instructed not to), leading\nto biased decision-making. A well-known example is Amazon's automated candidate screening\ntool, which downranked the r\u00e9sum\u00e9s of women applicants because the r\u00e9sum\u00e9s it was trained\non were predominantly from men (Dastin, 2018). Consequently, models may learn to leverage\ncorrelations that only hold for a subset of the data, resulting in performance disparities when\napplied to other groups or in different conditions. Many common failures can be ascribed to this\nissue, such as voice assistants that fail to respond to the accents of minoritized speakers\n(Harwell, 2018) or autonomous vehicles that struggle to recognize darker-skinned pedestrians\n(Li et al., 2024). From these examples and the extensive body of research documenting bias in\ndata and the biases exhibited by models, we can infer that researchers seek models that avoid\ncausing harm. In other words, they expect their models to learn functions that steer clear of\nharmful correlations.\nML researchers frequently describe correlations and the models that use them in terms of harm.\nFor example, Wang et al. (2019b: 5310) argue that models that are \u201csensitive to spurious\ncorrelations risk amplifying societal stereotypes\u201d. Scimeca et al. (2022: 9) claim that \"relying\non simple cues is sometimes unethical\u201d and describe it as an \u201calarming phenomenon\u201d (where\n\"simple\u201d in this context implies spurious). Several authors discuss the resulting failures, such as\nNagarajan et al.'s (2020: 3) suggestion that relying on spurious correlations \u201ccan also lead to\nunfair biases and poor performance on minority groups\u201d or Hartvigsen et al.'s (2022: 1) assertion\nthat an online toxicity detection system's \u201coverreliance on spurious correlations\u201d leads to a\ndisproportionate number of false positives for minoritized groups.\nAs seen above, researchers use various criteria to determine whether a correlation is harmful\n(and spurious). In some contexts, only correlations that cause representational harm\n(sometimes referred to as"}, {"title": "Reconciling multiple dimensions", "content": "Above, we illustrated how researchers reason about the desirability of a correlation (i.e., its\nspuriousness) through single dimensions of spuriousness. While focusing on disparate\ndimensions is useful for illustrative purposes, it is also true that researchers rarely consider\nalternative dimensions explicitly. Like the participants in Simons and Chabris' (1999) famous\nselective attention study, who were so focused on counting ball passes that they missed the\nappearance of a gorilla in the experiment, overemphasizing one dimension may lead to\nundertheorizing the entire problem. When faced with diverse and often competing downstream\ncontexts, the various dimensions of spuriousness may hamstring technical decision-making in\nsignificant ways."}, {"title": "Discussion & Conclusion", "content": "The success of modern ML systems hinges on their ability to automatically extract and use\ncorrelations found in large datasets, enabling the development of complex skills and behaviors\nsimply by observing patterns without having to explicitly encode precisely what a model should\ndo. This strength, however, belies two fundamental challenges. First, as datasets grow ever\nlarger, it becomes increasingly difficult to understand which correlations are encoded within"}]