{"title": "Metacognitive Myopia in Large Language Models", "authors": ["Florian Scholten", "Tobias R. Rebholz", "Mandy H\u00fctter"], "abstract": "Large Language Models (LLMs) exhibit potentially harmful biases that reinforce culturally inherent stereotypes, cloud moral judgments, or amplify positive evaluations of majority groups. Previous explanations mainly attributed bias in LLMs to human annotators and the selection of training data. Consequently, they have typically been addressed with bottom-up approaches such as reinforcement learning or debiasing corpora. However, these methods only treat the effects of LLM biases by indirectly influencing the model architecture, but do not address the underlying causes in the computational process. Here, we propose metacognitive myopia as a cognitive-ecological framework that can account for a conglomerate of established and emerging LLM biases and provide a lever to address problems in powerful but vulnerable tools. Our theoretical framework posits that a lack of the two components of metacognition, monitoring and control, causes five symptoms of metacognitive myopia in LLMs: integration of invalid tokens and embeddings, susceptibility to redundant information, neglect of base rates in conditional computation, decision rules based on frequency, and inappropriate higher-order statistical inference for nested data structures. As a result, LLMs produce erroneous output that reaches into the daily high-stakes decisions of humans. By introducing metacognitive regulatory processes into LLMs, engineers and scientists can develop precise remedies for the underlying causes of these biases. Our theory sheds new light on flawed human-machine interactions and raises ethical concerns regarding the increasing, imprudent implementation of LLMs in organizational structures.", "sections": [{"title": "Metacognitive Myopia in Large Language Models", "content": "Whether it is functionality or performance, scientists like to draw parallels between natural human functions and machine learning models using well-documented cognitive principles as stencils of learning models' behavior (Binz & Schulz, 2023; Rebholz, 2024). For example, artificial neural networks are built on the basic biological conceptualization of human brain architectures, including structures such as neurons and processes such as activation. Under optimal conditions, such as a consistently valid sample, their statistical predictions outperform human predictions (Meehl, 1954; Strachan et al., 2024). Large Language Models (LLMs) are artificial neural networks that are trained to behave in a human-like manner, primarily in the sense of generating human language for advising individuals across domains (Aher et al., 2023; Dillion et al., 2023; Griffin et al., 2023; Jiang et al., 2022; Rudin, 2019). Contemporary implementations achieve near-perfect performance on various natural language processing tasks through increasingly powerful approaches of transformer LLMs with an integrated attention architecture (Gemini Team et al., 2024; OpenAI et al., 2024; Vaswani et al., 2017).\nHowever, mimicking human speech also means mimicking human error, and the impression of LLMs surpassing benchmarks for natural language processing tasks was complemented by the impression of a large number of incriminating biases (Blodgett et al., 2020; Navigli et al., 2023). Thus, similar to the study of human cognition and the identification of cognition-inherent biases in perception, behavior, and judgment and decision-making (Tversky & Kahneman, 1974), researchers have begun to identify a conglomerate of model-inherent biases in LLMs (Navigli et al., 2023; Rich & Gureckis, 2019). Especially the new class of transformer-based LLMs has sparked a growing interest in research that attempts to quantify the consequences of biases that are embodied in the generated output and, in turn, influence human thought and behavior (Acerbi & Stubbersfield, 2023; Caliskan et al., 2022; Koco\u0144 et al., 2023; Lin et al., 2023). However, comprehensive theories that explain biases in LLMs are currently lacking. Therefore, we propose metacognitive myopia (Fiedler, 2012; Fiedler et al., 2023) as a theoretical framework to explain potentially problematic behavior of LLMs."}, {"title": "Metacognitive Myopia in Large Language Models", "content": "Metacognitive myopia is a syndrome that has been used to describe human cognition as \u201cpretty accurate in utilizing even large amounts of stimulus information, whereas [being] na\u00efve and almost blind regarding the history and validity of the stimulus data\u201d (Fiedler, 2012, p. 1). The theoretical framework explains biases independent of capacity limitations, lack of motivation, or depth of processing (Chaiken & Trope, 1999; Kunda, 1990). It draws heavily on the two main processes proposed in metacognition research, monitoring and control (Ackerman & Thompson, 2017; Nelson, 1996; Thompson et al., 2011). Metacognitive monitoring deals with critically assessing all memory-, perception-, and reasoning-related cognitive processes and signaling the need of additional mental effort. Intertwined with monitoring is the process of metacognitive control, or administrating available resources and correcting invalid information. Effective monitoring and control results in effective resource allocation. Findings from diverse research domains indicate that the absence of metacognitive monitoring and control, as delineated by the metacognitive myopia framework, provides a comprehensive account of a spectrum of biases influencing human judgment and decision-making processes (Fiedler, 2012; Fiedler et al., 2002, 2019; Scholten et al., 2024; Unkelbach et al., 2007).\nLLMs are not an end in themselves but were developed for supporting humans. Particularly due to several advances in recent years, researchers continuously emphasize emerging LLM abilities that enable far-reaching universality or the capability to store and retrieve real-word knowledge (Grossmann et al., 2023; Petroni et al., 2019; Rudin, 2019). We propose that, similar to human metacognitive myopia, LLMs are not designed to monitor their output generation and to correct for the influence of information from erroneous sources on the basis of which statistical inferences are drawn. In general, artificial neural networks lack the two metacognitive processes of monitoring and control. Their assessment and organization by means of the metacognitive myopia framework could serve as an overarching lever to address a whole range of error-prone behaviors in LLMs.\nSimply discovering more and more biases in the wild without direction aggregates costs, and approaches to counter such biases with common methods like fine-tuning is oftentimes just fighting symptoms. We show that the metacognitive myopia framework can account for different biases of LLMs such as political bias (Feng et al., 2023), gender bias (Kotek et al., 2023), and moral bias (Schramowski et al., 2022). The application of our theory facilitates the identification of remedies for influential yet biased tools used in humans' digital daily lives, beyond former explanations, such as the selection of training data and the influence of human annotators.\nImportantly, our theoretical framework helps to identify the origin of undocumented fallacies in, for example, the application of aggregated data to the individual level (cf. Simpson's paradox) or the equal weighting of differently reliable sources.\nApplying metacognitive mechanisms to LLMs' computational structure aims at, first, monitoring the functioning of trained neural networks and specifically the production of text based on probability measures and, second, controlling for text products from invalid sources, invalid statistical inferences, and to-be-discarded information identified by the monitoring process. We propose to think of LLMs as entities with reduced metacognitive abilities. For example, in the process of tokenization, training text is parsed into the smallest meaningful,"}, {"title": "A Taxonomy of Symptoms for Myopic Large Language Models", "content": "Whereas people make a choice after screening differently implicational options based on knowledge about them (Beach, 1993), LLMs decide on the next word depending on probabilities derived from token associations. These associations are calculated based on enormous data sets that LLMs were fed during training (Brown et al., 2020). LLM architectures are undoubtedly well-calibrated in the detection of correlational patterns in large web corpora and sets of books but fail to critically assess the information's origin by design. That is, LLMs not only neglect data quality (Smith, 2018), but also data genesis, including generation, context, and source validity. Therefore, LLMs show a na\u00efve and confident reliance on every information they are trained on without questioning the source. We identify five symptoms of metacognitive myopia in LLMs (Symptoms A \u2013 E, see Figure 1), constructing a taxonomy ordered from more blatant to more sophisticated cases of metacognitively myopic LLM decision-making (Fiedler, 2012). Our aim is to classify the origins of biases proposed in the LLM literature, to identify fallacies that so far have not received attention, and to illustrate the role of monitoring and control as means to solve these issues."}, {"title": "Symptom A: Neglect of Source Validity", "content": "A general law of learning dictates that when a conditioned stimulus (CS; e.g., switching on a light bulb) is continuously paired with an unconditioned stimulus (US; electric shock), the CS itself will come to elicit US reactions (e.g. fear caused by switching on a light bulb; Rescorla & Wagner, 1972). Similarly, each training data set token and its embeddings influence a LLM\u2019s associated parameters (Stokel-Walker & Van Noorden, 2023). Therefore, the most easily detectable symptom of metacognitive myopia, Symptom A, is LLMs' inability to ignore invalid sources. Coined mere considering (Fiedler, 2000b; Fiedler et al., 1996) or misinformation effect (Loftus, 2005) in the psychological literature, merely thinking about a stop sign (e.g., \"did you see a stop sign while driving to Peter's house?\u201d) can inflict false memories. The same way the statement's subjective truth value increases through considering the possibility of the sign's presence, the probability of regenerating invalid information by LLMs increases through stronger weights promoting a specific word constellation.\nThe use of modern information ecologies in the training of LLMs results in the inadvertent incorporation of misinformation (e.g., fake news, hate speech) from a variety of online sources and corpora (Hu et al., 2024; Wulczyn et al., 2017). LLMs can reproduce such misinformation in two ways: First, simply by memorizing and citing training data (Nasr et al., 2023; Petroni et al., 2019). Second, misinformation permanently affects token embeddings, especially in cases where new language is created by combining different contexts, as in transformer architectures (Vaswani et al., 2017). In a bottom-up approach, researchers try to recover LLMs from biased output through standard practices such as reinforcement learning (Christiano et al., 2017; Liu et al., 2023). Reinforcement learning is an attempt to correct for the influence of misinformation on model weights by having human annotators rate enormous amounts of LLM output as acceptable or unacceptable. This external control is problematic because human annotators are themselves biased (Sap et al., 2022). Accordingly, with reinforcement learning developers attempt to reduce the direct reproduction of biases in LLM output, but fail to recognize the lack of internal monitoring and control of invalid text pieces. This confounding of cause and effect particularly stresses the need for metacognitive myopia as a theoretical framework for biased LLMs."}, {"title": "Symptom B: Susceptibility to Repetition", "content": "Symptom B revolves around LLMs' myopic processing of samples containing selectively repeated information. Much of LLMs' information ecology is comprised by social media and Wikipedia articles by training. As web scraping can be implemented into the latest versions of LLMs (Ahluwalia & Wani, 2024), the same published journal article, opinion piece, or blog entry can be included several times in the sample used by LLMs. LLMs display a lack of meta-cognitive regulation for the impact of repetition in judgments and next-token prediction. This pertains to both forms of redundancies, namely the training on identical sources (e.g., the same training example in another corpus), and training on the close reproduction of a text in a similar form (e.g., plagiarized, newly formulated news on another web-scraped website). LLMs fail to control for myopic sampling in that they are susceptible to redundancies of tokens and of token co-occurrences. This redundancy is likely to increase as LLMs move from being web-scraping consumers to co-writing (or even solo-writing) agents that produce an increasing proportion of the articles from which they subsequently learn (Shumailov et al., 2024). Already only 33% of news articles on the internet can be identified as original content (Cag\u00e9 et al., 2020). As a consequence, the model repeatedly endorses the same information without any monitoring of parameters, such as time of publication, uniqueness, or resumption. For example, a chorus of outdated and repeated recommendations on safety behavior during a pandemic can superimpose current recommendations, which are rarely reported due to less urgency and less public attention.\nAlthough a simple repetition does not add informational value incrementally, the more duplicate tokens and accompanying contexts are prevalent in a training data set, the more likely is the identification of the co-occurrence, and in consequence, its reproduction. This property opens the door to a positive conception of LLMs as factual knowledge bases, (e.g., in an educational context; Petroni et al., 2019) only for an unbiased sample. However, unbiased samples are rare and often dependent on a variety of auxiliary assumptions. Much like repeated and coherent information activating references in human memory (Unkelbach, Koch, Silva, et al., 2019), the repetition of a word nudges its model weight and maximizes the generation probability in different contexts. Borrowing an analogy from Ludwig Wittgenstein, in such instances, LLMs function \u201cas if someone were to buy several copies of the morning newspaper to assure himself that what it said was true\u201d (Wittgenstein, 1977).\nInterpreting the sources of training text corpora as separate agents determining the output (Binz & Schulz, 2023), LLMs can be conceived of as decision-making groups. As such, they can suffer from biases of selectively repeated or neglected information between corpora or \u2018group members.' A fallacy of collective wisdom like the hidden-profile problem (Schulz-Hardt et al., 2006; Stasser & Titus, 1985) can illustrate the maladaptive consequences of LLMs' missing metacognitive control for repetition. Applied to LLMs' decision rules, the hidden-profile problem is illustrated by a scenario in which the LLM's choice for an ambiguous word generation task would be more accurate than each corpus' choice if only unique arguments were counted (see Table 1 for an exemplary distribution). The reason for that seemingly paradoxical scenario lies in the weighting of shared and unshared information regarding a choice between the two options A and B."}, {"title": "Symptom C: Base Rate Neglect", "content": "In between blatant and more complex fallacies, Symptom C describes LLMs' flawed conditional reasoning that rightfully relies on (at first glance) unbiased samples, but misses conditional dependencies in the sampling process. This symptom may be attributed to the learning mechanism of choice-contingent availability inherent to the network model itself or to the fact that the LLM's human interaction partner formulates requests to the models in prompts that artificially restrict the context on which an output is generated. In both cases, this yields samples that are suitable for generating output. However, due to the imposed conditionals, these samples only represent a portion of the total sample. To illustrate base rate neglect (Pennycook et al., 2022), one might consider the generation of recommendations regarding music genres and artists by LLMs. In the case of a prompt requesting \"popular music artists,\" it can be assumed that the sample of tokens and embeddings relevant to the generated output will be tailored to name pop rather than rock music artists, given that the \"pop\" token in the keyword \"popular\" is more likely to be associated with pop stars than rock stars.\nLLMs trained on data from community websites like Stack Overflow or GitHub (Raffel et al., 2020) have become proficient in generating programming code due to their exposure to numerous expert responses. These forums often feature correct solutions from expert users addressing their peers' queries. However, for rare or uncommon queries, correct answers are less likely to be reproduced or generated by combining known solutions, due to the limited examples available. While LLMs can produce helpful code solutions by leveraging thousands of similar training examples, they can also generate misleading or incorrect answers, such as fabricating non-existent R-packages or functions. For complex or novel programming tasks, LLMs rarely acknowledge their limitations, as they rely on the conditional probability of providing an answer based on the presence of expert responses in the training data, P(answer x|answer given). They neglect the conditional probability of responding with \u201cI don't know\u201d based on the absence of answers from other users who faced the same query, P(answer \u201cI don't know\u201d|no answer given). For highly complex tasks, the ideal response pool should include both uncertain answers and indications of no solution. However, LLMs exhibit choice-contingent availability, generating text solely based on the training data, and disregarding the importance of uncertain or missing answers (Smith, 2018).\nPeople rarely sample unconditionally (Fiedler, 2008). Even if a user prompts without a strong conditional, training data are likely not to display the true base rates of arguments in favor of different options. With every change in the sample's population, it would be necessary to either reset the sample or add another one (Fiedler, 2012). Consequently, unconditional sampling carries the risk of conserving LLMs pre-trained base rates that could be incongruent with the true frequencies of occurrence for latent quantities (Bolukbasi et al., 2016; Fiedler, 2012). However, because people generally tend to sample based on underlying default attitudes, conditional sampling poses a greater danger. This is illustrated by its most extreme case, the ultimate sampling dilemma (Fiedler, 2008) in Figure 3. Based on the distributional assumption of skewed frequencies, a variety of polarizing issues could produce detrimental results depending on the geared hypothesis that creates a positive or negative evaluative context.\nFinally, based on the calculation of the most probable output for a prompt, sequential interactions represent the process of Bayesian belief updating, featuring conditional probabilities for multiple prompts. Due to a continuous influence of all prompts in the interaction history, previous word vectors leave accountable traces for newly computed output. In this case, the preceding prompt-output combination(s) plus the hypothesis put forward by the new prompt set a"}, {"title": "Symptom D: The Lure of Popularity", "content": "Symptom D of metacognitive myopia in LLMs is characterized by a false reliance on sample size in relational comparative judgments. Its effects emerge when LLMs fail to monitor and control for unequal reliability and inflated co-occurrences stemming from unequal sample size in comparing differently represented options. It deals with the impact of the number of tokens n representing the amount of information or number of arguments leading to a preference for one option over the other simply based on the criterion frequency. For example, a LLM might be asked to compare the popularity of the Renaissance's most famous painters. Raphael, Michelangelo, and Leonardo da Vinci should be evaluated similarly because they all created works of inestimable value. However, given the extensive scope of Leonardo da Vinci's contributions, encompassing science and a multitude of other domains, he may be more likely to be generated as an output due to the greater number of associations within the LLM compared to Raphael or Michelangelo. Correspondingly, this preference based on frequencies may also be driven by the susceptibility to repetition, discussed as Symptom B.\nThe prevalence of Symptom D in LLMs constitutes a problem for one major principle of theory of science and the process of gaining insight in general: falsification. Even in the scientific community, it takes time before the falsification of one theory through evidence for a contrary theory reaches a broader audience. When the former is considered to be common knowledge and part of many textbooks, LLMs could preserve the falsified theory's superiority simply through an asymmetrically higher presence in their pretrained data sets and hinder the spread of a paradigm shift. Consequently, LLMs are at risk of generating one-dimensional perspectives structured by the world's (e.g., as represented in the training data) most influential or popular ideas (Santurkar et al., 2023). A complex case of this fallacy, in which a reigning theory is being contested by a new up-and-coming theory, pertains to the question, \u201cWhat is the origin of our universe?\u201d While many physicists believe the Big Bang Theory to be state-of-the-art with numerous evidence in its favor, the Big Bounce Theory is on the rise, having more explanatory power in critical questions such as the ending of time and space (Poplawski, 2012). The comparison of theories serves as an intriguing example of LLMs sticking to the status quo in generating content based on (possibly outdated) opinions and conceptualizations which entered many different internet and book corpora and therefore constitute a threshold difficult to surpass. This status-quo bias (Kahneman et al., 1991) is accompanied by LLMs' myopic overconfidence (Stella et al., 2023) and is founded in LLMs' lack of metacognitive control for a na\u00efve reliance on internal probability distributions.\nIn general, Western, educated, industrialized, rich and democratic (WEIRD; Henrich et al., 2010b, 2010a) populations have priority access to LLMs and artificial intelligence technologies. Similar to researchers in behavioral science generalizing their claims to the whole of humankind, supported only by experiments conducted on samples representative of a tiny margin of western society (Henrich et al., 2010b), general-purpose LLMs are often believed to be universally applicable (Gemini Team et al., 2024; OpenAI et al., 2024). However, LLMs specifically trained for behaving morally question behaviors uncommon in WEIRD societies (Jiang et al., 2022).\nAccording to the geographic bias (Manvi et al., 2024), LLMs proved to be accurate in predicting the relationship of a specific area's performance and its relative real-word rank for various objective domains such as population density or infant mortality rate. Nonetheless, switching to subjective measures, virtually all tested LLMs exhibited a WEIRD bias, devaluing regions with low socio-economic conditions. LLMs strongly associated the regions of Africa and the Middle East (characterized by high infant mortality) with lower ratings of desirable traits such as morality, intelligence, or attractiveness compared to the regions of Europe or North America (characterized by low infant mortality). This geospatial bias can be explained as a display of illusory correlations (Fiedler, 1991; Hamilton & Gifford, 1976; Rich & Gureckis, 2019) by LLMs neglecting the decisive role of the sample size n."}, {"title": "Symptom E: Lack of Distinction Between Levels of Aggregation", "content": "The final and most sophisticated symptom of metacognitive myopia, Symptom E, targets LLMs' aggregation characteristic. Even with giving some context through prompts, increasing the size of language models can lead to a text production at one level (super-category) that is too abstract, too general, or too probable to fit the information requested at an individually specific level (subcategory). General-purpose LLMs' operation mode results in a one-size-fits-all judgment, provoking a mismatch between a request and a generated output. This is demonstrated by a display of high-level construal-driven abstraction in LLMs giving consumer recommendations (Kirshner, 2024), where LLMs advised customers to buy cameras depending on their visual design despite their preference for low-level feasibility. In this experimental set-up, the scrutinized LLMs based their judgments more on high-level construal features like desirability instead of low-level construal features like feasibility, and adopted an abstract perspective when recommending products in different retail scenarios.\nA generally intriguing example of Symptom E is the Simpson's Paradox (Simpson, 1951), in which LLMs may fail to control for false inferences showcasing a missing sensibility to nested data structures. The paradox denotes an instance in which the correlation between two or more sets of data is modulated by a third, confounding variable. As a consequence, an observed trend can disappear or even be reversed, depending on the grouping or aggregation level at which the data is analyzed. The paradox is of special relevance because the user of LLM output has no insight into the model's black box and cannot foresee which categorizations the model applies to words with common connotations. To prevent a mismatch of classifications, precise language is needed, and even under ideal circumstances, it is not clear to users whether a distinction is made between specific subcategories (for a demonstration of the Simpson's Paradox in LLMs, see"}, {"title": "Interaction of Human and LLM Metacognitive Myopia", "content": "Applications of LLMs almost always include the exchange of human and machine and therefore inevitably enable the harmful interaction of both agents' metacognitive myopia. Consequently, the accuracy of probabilistic output based on the criterion of the sequential generation of the most probable tokens constitutes a limitation for AI-generated guidance in various topics. First, human metacognitive myopia is characterized by a missing metacognitive regulation for LLMs' generated output based on unknown and incomprehensible probability distributions. It is a fairly realistic view that humans fail to appropriately question LLMs\u2019 validity in general and the context-dependency of its output in particular (Bogert et al., 2020; Hou & Jung, 2021; Logg et al., 2019). For humans, there seem to be few known remedies for metacognitive myopia (Fiedler et al., 2019, 2023, 2023; Scholten et al., 2024; Unkelbach et al., 2007). Therefore, for example, if humans ask LLMs for current political debates about multi-faceted topics, they will inevitably run the risk of consuming popular, possibly harmful or outdated information that is not adjusted for recent journalistic investigations, which they may never have encountered without using LLMs (Santurkar et al., 2023). Humans generally do not understand the various mechanisms of the machine, but freely implement them in many high-stakes decisions, such as radiology diagnostics (Rudin, 2019; Shen et al., 2023). Crucially, misinformation persists even when people have been told it is invalid (Chan et al., 2017).\nOriginally, LLMs were not designed to monitor human needs and interests when specific assistance is requested, but to solve the natural language processing task at hand. If LLMs take the humanity of the request into account, this behavior is most likely due to their pre-training on human-made authentic text and their accurate mimicry of human outputs (Bender et al., 2021). In consequence, a human assumes a LLM to be aligned with his or her beliefs and confirming his or her hypothesis at hand (Chen et al., 2023). Zooming out and considering the multi-causal structure of our world, combinations of undetected fallacies like the ultimate sampling dilemma and LLMs' sycophantic tendencies (Sharma et al., 2023) shape interactions of metacognitive myopia in humans and LLMs. Crucially, this poses a risk by, for example, manifesting political polarization through strengthening human preferences and attitudes.\nNeedless to say, humans' metacognitive myopia also reflects back on LLMs. Humans generally underestimate the amplification of emerging biases in LLMs. In LLMs' evolutionary history, first came the surpassing of numerous benchmarks and a continuous performance optimization and then, much later, the focus on harmful associations and statistical regularities that implicitly reinforce stereotypes. Researchers as well as companies try to cure LLMs by reinforcement learning and implementing fairness rules (Christiano et al., 2017), but the human annotators implement their own moral beliefs and implicit biases, for example, in toxic word predictions (Barocas & Selbst, 2016; Sap et al., 2022). In general, language data sets do not represent the variability in stereotypical attitudes. Moreover, annotators display an infinite number of grey shades to discriminate between toxic and non-toxic content (Plank, 2022). This combination constantly shifts the boundaries between desirable and undesirable content and leaves the elimination of harmful outputs to arbitrariness."}, {"title": "Future Ethical, Developmental and Research Avenues to Treat Metacognitive Myopia in LLMS", "content": "The interaction of myopic behaviors raises a question of design for LLMs and a question of responsibility for humans interacting with AI-based technologies like LLMs. We have no intention to convey a fully pessimistic view on LLMs, but acknowledge that LLMs can support human work and progress in many ways. We propose our theoretical framework to generate new ways of making LLMs more adaptive and to identify starting points to fix the variety of biases or address related ethical concerns. Many questions arise along the way: What is the responsibility of engineers to prevent human agents from blindly following LLMs' potentially myopic advice? Should the utilization of LLMs be restricted or curbed for specific groups or organizations?\nOn the human side, the importance of user maturity cannot be overstated. Although critical awareness in dealing with LLMs is a key competence stressed by developers, experts, and the public media, humans myopically tend to copy and paste possibly misleading programming code, accept curtailed historical representations, or adopt highly plasticized cover letters without any correction or reviewing of the information's accuracy. A mature LLM user must not only not take LLM outputs for granted, but also critically assess the cultural background of the tool, the implicitly interwoven attitudes of corpora and annotators, and other conceivable motives of the communicator (Tankelevitch et al., 2024). Therefore, we emphasize the need to promote user education: How to deal with the functioning and underlying information ecology of LLMs, the prevalence of metacognitive myopia in humans and in LLMs, and the development of curricula for mindful interpretation of LLM output in schools and universities could be valuable lessons in using LLMs as consultants.\nAn enlightened use and development of LLMs works via vigilance for metacognitive myopic functioning in LLMs and smart prompt engineering. Accordingly, guidelines of prompt engineering stress the crucial role of details and concise information as contextual prior in prompts (Strobelt et al., 2022). Developers must address the implications of metacognitive myopia in LLMs. In this respect, for example, the notification implemented in Open AI's ChatGPT (Brown et al., 2020; OpenAI et al., 2024) that \u201cChatGPT can make mistakes. Consider checking important information.\u201d written below the input field appears to be too vague to have an impact on the approximately 100 million weekly users (Porter, 2023).\nOn the computational neural networks side, it is advisable to focus on more interpretable instead of ever bigger black-box models (Bender et al., 2021). Indeed, contingencies and regularities are often better represented by smaller samples (Fiedler & Kareev, 2006). Considering the logics of covariation, it pays off to intentionally sample from the less represented perspective or category (McKenzie & Mikkelsen, 2007). We named many structural obstacles of algorithms obedient to the law of large numbers. However, especially in the debate about integrating LLM abilities in the research pipeline, warnings tend to fixate on small steps in a bottom-up process (Messeri & Crockett, 2024). Researchers target smaller trouble spots of LLM-augmented research tasks, such as synthesizing data or conducting data analyses (Dillion et al., 2023; Grossmann et al., 2023). However, improving LLMs and making them sustainable requires both (Th\u00f3risson, 2007), bottom-up ideas for tackling fallacies in specialized LLM applications and our top-down theoretical framework for categorizing an abundance of biases sampled in the wild.\nAvenues to investigate metacognitive myopia in LLMs almost automatically lead to the question of how to cure LLMs from myopic behavior. We want to highlight different approaches that revealed how to detect metacognitive myopia in neural networks and offer some starting points for possible remedies of the conglomerate of biases outlined here. One such avenue can be LLMs' mechanism to capitalize on chance. As outlined by our example of the truth bias (Gilbert et al., 1990; Levine et al., 1999) in the beginning of this article, LLMs are designed to take all inputs for granted instead of questioning their accuracy. Approaches that target low generation probabilities as indicator of invalidity, such as Honesty (Kadavath et al., 2022) or Confidence and Calibration (Farquhar et al., 2024; Krause et al., 2023), appear to be promising techniques for addressing this issue, as they induce LLMs to recognize the validity of their judgments. In the Honesty approach of Kadavath et al. (2022), a LLM was trained to estimate the accuracy of its answers. They studied how accurately a LLM can predict the probability of knowing the correct answer to a question. The calculated probability is denoted as P(IK) or the probability that \u201cI know\" the factually right answer. LLM calibration was defined as correspondence of probabilistic predictions P(IK) with actual frequencies of token occurrences. For common tasks, the applied LLM accurately predicted P(IK). However, the LLM had difficulties to determine P(IK) for new tasks, establishing base rate neglect of missing co-occurrences as constraint of LLMs' generalization abilities.\nTurning that argument upside down, if a factor for missing co-occurrences would be implemented in LLMs' calibration judgments of P(IK), calibration judgments could serve as an introspective gauge for LLMs' ability to generalize across tasks. However, and this is a general problem for fighting bias in LLMs with LLMs, this idea can take two paths: LLMs successfully control for output based on wrong inferences through prompts targeting myopic behavior, or a calibration judgment itself exhibits metacognitive myopia. The second case is especially relevant for tasks whose answers are not available in the LLM's pre-trained structures, which corresponds to the inability to compute probabilities P(\u201cI know\u201d) for tasks a model does not well generalize to (Kadavath et al., 2022). Therefore, metacognitive myopia is more severe in cases where the probability of generating the most probable token depends on suspiciously small probability values.\nAnother possible way to alter LLMs' vulnerability to invalid tokens could be to modify the data space in which tokens are embedded. For example, Bolukbasi et al. (2016) relied on a debiasing algorithm that moves stereotype-afflicted tokens in the corpus to alter their associations. To expand on that approach, developers could implement source validity distributions in embedding vectors that accordingly shift token probability distributions. In a fine balancing act (Han et al., 2022), invalidity may be used to charge the generation probabilities of tokens with a lack of source trustworthiness, thereby making their production less probable. However, the question arises as to how the reliability of countless sources can be determined in advance. In general, we assume that adding more and more debiasing corpora is inefficient as it is almost impossible to cancel out all stereotypes by trying to estimate and equalize latent prevalences of critical tokens. Rather, architectures like deep neural networks need to be augmented by metacognitive regulatory tools, which include an instructed alertness for pitfalls of metacognitive myopia preventing inappropriate statistical inferences (Hanheide et al., 2017; Th\u00f3risson, 2007)."}, {"title": "Conclusion", "content": "Metacognitive myopia is not only prevalent in humans, but also in LLMs. Statistical predictions are superior to human predictions, as long as the underlying information is trustworthy, not redundant, includes all necessary information, is not subject to status quo and popularity, and is properly classified. The five symptoms of metacognitive myopia in LLMs are not mutually exclusive but can simultaneously affect LLMs' functionality. Introducing metacognition into LLMs is the next big challenge for developers to further enhance already highly influential tools. This agenda provides a lever for tackling a syndrome in artificial neural networks that have contracted biases deeply embedded in human language and culture.\nIn contrast to human judgment processes, LLMs are not an end in themselves. The syndrome's degree of severity is determined by engineers and researchers, who design the tool, affecting which information is integrated and how it is processed. Although users and developers are constantly warned about pitfalls of information societies (e.g., misinformation, framing), the implemented computational mechanisms of LLMs neglect metacognition. Other than humans"}]}