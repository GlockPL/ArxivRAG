{"title": "ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy", "authors": ["Kian Kenyon-Dean", "Zitong Jerry Wang", "John Urbanik", "Konstantin Donhauser", "Jason Hartford", "Saber Saberian", "Nil Sahin", "Ihab Bendidi", "Safiye Celik", "Marta Fay", "Juan Sebasti\u00e1n Rodr\u00edguez Vera", "Imran S Haque", "Oren Kraus"], "abstract": "Large-scale cell microscopy screens are used in drug discovery and molecular biology research to study the effects of millions of chemical and genetic perturbations on cells. To use these images in downstream analysis, we need models that can map each image into a feature space that represents diverse biological phenotypes consistently, in the sense that perturbations with similar biological effects have similar representations. In this work, we present the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a 60% improvement in linear separability of genetic perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate consistency benchmarks. Beyond scaling, we developed two key methods that improve performance: (1) training on a curated and diverse dataset; and, (2) using biologically motivated linear probing tasks to search across each transformer block for the best candidate representation of whole-genome screens. We find that many self-supervised vision transformers, pretrained on either natural or microscopy images, yield significantly more biologically meaningful representations of microscopy images in their intermediate blocks than in their typically used final blocks. More broadly, our approach and results provide insights toward a general strategy for successfully building foundation models for large-scale biological data.", "sections": [{"title": "1 Introduction", "content": "Large-scale cell microscopy assays are used to discover previously unknown biological processes (Przybyla & Gilbert, 2022; Bock et al., 2022; Rood et al., 2024) and identify novel drug candidates and targets (Vincent et al., 2022). Labs are now able to achieve extremely high throughput by leveraging high content screening (HCS) systems that combine automated microscopy with robotic liquid handling (Boutros et al., 2015). Extracting meaningful features from microscopy images in large-scale screens has become increasingly difficult as this scale has increased. Public datasets like RxRx3 (Fay et al., 2023) and JUMP-CP (Chandrasekaran et al., 2023) now include millions of cellular images across 100,000s of unique chemical and genetic perturbations. In addition to limitations in expressiveness of the features that can be derived from them, traditional methods relying on customized pipelines for segmentation, feature extraction, and downstream analysis (Caicedo et al., 2017) struggle to handle this scale effectively (Chandrasekaran et al., 2021; Carpenter et al., 2006)."}, {"title": "2 Related work", "content": "The size and complexity of large-scale microscopy data demands image models that can extract rich biological features and do so consistently across experimental replicates, both of which are crucial for downstream biomedical applications. Rich, biologically meaningful representations reveal relationships between genes or compounds to drive the discovery of novel targets and drug candidates, while consistency in features extracted across replicates ensures that findings are reproducible and reliable for therapeutic development.\nFoundation models have been developed for representing high-dimensional unstructured biological data such as protein structures (Jumper et al., 2021) and transcriptomics (Hao et al., 2024), but the scale and dimensionality of large-scale microscopy data present unique challenges for generating representations that are both biologically informative and consistent across replicates. HCS datasets are often confounded by complex noise known as batch effects (Caicedo et al., 2017), stemming from differences between experimental batches and biological variability. These batch effects \u2013 including natural variation in cell populations \u2013 obscure the biological effects of perturbations and make it challenging to isolate the specific effects of the perturbations applied (Yang et al., 2019). Overcoming these obstacles with a model capable of generating robust, biologically meaningful representations can empower HCS to systematically interrogate gene function and identify novel drug candidates (Rood et al., 2024)."}, {"title": "Evaluating Representations for Drug Discovery.", "content": "Evaluating the quality of biological representation learning methods for drug discovery remains challenging, as ground truth data is sparse, noisy, biased to well-studied diseases and pathways, and poorly annotated. Metrics have been proposed that use mean average precision (Kalinin et al., 2024) or AUC ROC (Sivanandan et al., 2023) to assesses how similar related samples are represented, including replicates of the same perturbation or different perturbations with similar annotated biological activities. Recently, Celik et al. (2024) introduced terminology for describing perturbative \u201cmaps of biology\", in which representations of perturbations in HCS data can be placed in unified, relatable embedding spaces allowing for the generation of genome-scale sets of pairwise comparisons. Here we leverage the biological relationship recall benchmark proposed by Celik et al. (2024), which assess how well known relationships between pairs of perturbations are recalled among the most similar or dissimilar embeddings. Computing reliable versions of these relationship benchmarks with HCS data is particularly expensive as they require genome-wide embeddings to be inferred for hundreds of millions of image crops from the genome-wide RxRx3 microscopy screen (Fay et al., 2023)."}, {"title": "Dataset Curation for Foundation Models.", "content": "Dataset curation is crucial for enhancing the efficiency of foundation models, especially in large-scale contexts. Usual approaches to dataset construction are inspired by the image retrieval community (Weinzaepfel et al., 2022; Radenovi\u0107 et al., 2018; Berman et al., 2019). Existing methods often utilize pre-trained models for filtering and pruning, such as vision-language models to discard irrelevant pairs (Schuhmann et al., 2021), semantic deduplication to remove redundancy (Abbas et al., 2023), and prototypicality-based approaches to retain representative data (Sorscher et al., 2022). However, these techniques are less effective for HCS, where redundancy, variability, and subtle morphological differences make conventional filtering challenging. Our work addresses these limitations by building on Celik et al. (2024)'s perturbation consistency framework to curate a balanced dataset of images across semantic classes, which is vital for effective learning under the masked objectives (Zhang et al., 2022)."}, {"title": "Layer-wise Analysis of Deep Neural networks.", "content": "Recent work suggests that intermediate layers (or, blocks) in large ViTs may achieve superior performance on certain linear probing tasks compared to the final encoder layer (Evci et al., 2022; Dehghani et al., 2023). For example, Alkin et al. (2024) reported that intermediate layers in large MAE-ViTs (ViT-L, ViT-H) have superior ImageNet-1K k-NN accuracy. They attributed this property to the later encoder layers becoming more optimized for the reconstruction task."}, {"title": "3 Vision Transformers for Microscopy Images", "content": "We train and evaluate various vision transformers (ViTs, Table 1) as encoders to extract feature embeddings from 256 \u00d7 256 \u00d7 6 (HxWxC) microscopy image crops (Figure 2)."}, {"title": "3.1 Training Dataset Curation", "content": "Many academic and industry labs have adopted the Cell Painting imaging protocol (Bray et al., 2016), which multiplexes fluorescent dyes to reveal eight broadly relevant cellular components. The datasets used here contain a six-channel implementation of Cell Painting (Figure 2), as well as brightfield images, spanning 100,000s of chemical and genetic perturbations applied to dozens of cell types (Kraus et al., 2024). In these datasets, cells that look like unperturbed cells tend to be very over-represented because many perturbations do no induce a morphological change. Some morphological changes are also far more common (e.g. many perturbations will kill cells, resulting in a relatively high proportion of dead cell morphological phenotype). This results in significant imbalance in the morphological phenotypes that the models learn to reconstruct.\nTo address this, we constructed an aggressively curated training dataset (\u00a7 A.1). To learn an initial representation, we began by reproducing the MAE-L/8 model of Kraus et al. (2024) on a dataset of similar size consisting of 93 million HCS images. Using this representation, we first filtered perturbations that did not induce consistent morphological changes to cells. To perform this filtering, we utilized Celik et al. (2024)'s non-parametric perturbation consistency test (\u00a7 A.3) Typical Variation Normalization (Ando et al., 2017a; Kraus et al., 2024). This test was applied within each experiment for computational efficiency, and we restricted the analysis to wells containing single perturbations. This consistency was computed for CRISPR guides, siRNAs, and particular concentrations of small molecules across replicates of the same perturbation. P-values were computed for each gene and each (perturbation, concentration) pair. When multiple experiments existed for the same condition, we combined p-values using the Cauchy Combination test (Liu & Xie, 2018).\nWe repeated this procedure with a weakly supervised learning (WSL) model trained on RxRx1 (Sypetkowski et al., 2023) and filtered to perturbations where any condition had a p-value < 0.01 in either the MAE-L/8 or WSL model. This process reduced our original dataset of 93M samples to 16M, which we refer to as Phenoprints-16M. While some redundancy remains when distinct perturbations have the same effect, the proportion of samples with that differ from negative controls increased substantially with little decrease in overall diversity. We believe that iteratively repeating this process with the best models from previous iterations to guide data selection for subsequent models may be a viable strategy."}, {"title": "3.2 Models", "content": "Baselines. We compare to several non-finetuned baseline ViT image encoders: three different Dino-v2 backbones (Oquab et al., 2024) (with 4 register tokens (Darcet et al., 2024)) trained on a curated non-biological natural image dataset; a weakly supervised (WSL) classifier ViT-L/16 trained on Imagenet-21k (Ridnik et al., 2021); a MAE ViT-L/16 trained on Imagenet-21k (He et al., 2022); and an untrained ViT-S/16. Preliminary investigations found that channel-wise self-standardization worked best as the image normalization preprocessing for these baselines, and that the class token was slightly better than the global pool of the patch tokens (except for MAE). Convolutional weights in the patch embedding layer were repeated to embed 6 channel images when using models trained on RGB datasets (Wightman, 2019).\nPrior work. Our primary point of comparison is with respect to the best pretrained foundation model presented by Kraus et al. (2024), the MAE-ViT-L/8+ trained on RPI-93Mrained for approximately 40 epochs, learning from over 3.5 billion image crops, using the L2 mean squared error loss function plus an additional Fourier domain reconstruction loss term.\nCA-MAE-S/16 trained on RxRx3. We trained a new channel-agnostic MAE (Kraus et al., 2024) ViT-S/16 on the RxRx3 dataset (Fay et al., 2023) for 100 epochs. Channel-agnostic ViTs tokenize each image channel separately with shared patch embedding weights and leverage the dynamic sequence length of transformers with repeated positional encodings to train ViTs that can process images with varying numbers of channels (Bao et al., 2024; Bourriez et al., 2024; Kraus et al., 2024). Kraus et al. (2024) demonstrate that the large MAEs with 8x8 patch size perform either better or the same as the 16x16 channel-agnostic variants for consistently 6-channel data, so we opted to train standard MAEs for the following two new models since they require fewer tokens at inference time.\nMAE-L/8 trained on Phenoprints-16M. Holding the model backbone constant compared to the MAE-ViT-L/8 by Kraus et al. (2024), we assess the impact of our curated dataset in contrast to the 93M dataset by training a new ViT-L/8 MAE for 500 epochs on Phenoprints-16M.\nMAE-G/8 trained on Phenoprints-16M. Holding the dataset constant compared to MAE-L/8 above, we assess the impact of increased model scale in terms of parameters by training a new ViT-Gigantic MAE with nearly 1.9 billion parameters for 500 epochs on Phenoprints-16M. Training this model required 256 H100 GPUs running in parallel for over 1 week. See \u00a7 A.2 for other hyperparameter settings we used for model training."}, {"title": "4 Linear probing representation learning across ViT blocks", "content": "We improve the quality of our learned image representations by leveraging previous findings that suggest intermediate blocks within an encoder can provide better representation compared to the final block (Alkin et al., 2024). Unfortunately, it is infeasible to search for the best block by simply performing whole-genome evaluation on each block of a large model because the evaluation is extremely time-consuming and resource intensive. For example, evaluating the final block of MAE-G/8 required 4,000 L4 GPU hours just for inference (\u00a7 5). We demonstrate that using block-wise linear probes provides insights into the quality of biological features extracted by these models in their intermediate blocks, allowing us to trim the model to an earlier block to both reduce inference costs and improve representation quality.\nOur block-wise search consists of training a logistic regression model (linear probe) on the output features of each transformer block to predict either the gene that was perturbed or the functional group that the gene belongs to, and test performance on held-out experiments (\u00a7 A.4). We define the optimal block b* for a probing task as the block whose output features achieve the highest test balanced accuracy when trained on the probing task, across all N blocks of the encoder,\n$b^* = arg \\underset{b\\in\\{1,2,...,N\\}}{max} BalancedAccuracy(z^{(b)})$,\nwhere $z^{(b)}$ are output features from block b of a ViT. Performance on our linear probing tasks can be viewed as a measure of linear separability of a feature space across experimental batches.\nRxRx1 1139-class siRNA genetic perturbation classification. We expect high quality representations of cell images to generate similar embeddings for cells with the same perturbation, hence a simple linear probe should be able to predict gene perturbation from these representation reasonably well. We train linear probes on the publicly-available RxRx1 dataset in Sypetkowski et al. (2023) which consists of 125,510 high-resolution fluorescence microscopy images of human cells under 1,138 siRNA-induced gene knockdowns (plus unperturbed controls) across four cell types (HEPG2, HUVEC, U2OS, RPE). These gene knockdowns produce strong phenotypes which makes the prediction task more feasible.\nWe found that, for MAE-G/8, the best features came from intermediate block b* = 38 (out of 48) of the encoder, achieving a balanced accuracy (0.51) that is 8.5% greater compared to its final block's output features Additionally, these features achieved 60% greater accuracy than the typically used final block of MAE-L/8+ (Kraus et al., 2024). We observed similar trends for ViT models pretrained on natural images. For example, DINO-G/14 and ViT-L/16 MAE trained on non-biological natural image data have their best features at blocks that are positioned within the first half of the encoder. For ViT-L/16 MAE, the performance of the best block is 27% higher compared to its final block output features that are typically used for downstream tasks. The higher performance observed for intermediate blocks does not appear to be an intrinsic feature of the ViT architecture as an untrained ViT did not exhibit such a parabolic trend.\nAnax 40-class functional gene group classification. Biologically meaningful representation of microscopy images of genetically perturbed cells should capture functional relationships between genes, hence a simple linear probe should be able to predict functional gene groups when trained on these representations. We curated a small subset of 80,000 wells from RxRx3 (Fay et al., 2023) to evaluate linear probes on functional group prediction. We also evaluated similar whole genome knockout screens with ARPE-19 and an additional population of HUVEC cells with soluble TNF-a added to all wells. We manually curated Anax, a set of 40 functionally-diverse gene groups containing 348 genes, with details provided in (\u00a7 A.8). Examples of groups include major protein complexes (e.g. proteasome, ribosome-small/large), metabolic pathways (e.g. Krebs cycle) and signaling pathways (e.g. calcium signaling). These groups span broad biological processes that are conserved across cell types \u2013 linear separability of these groups would likely indicate that representations are biologically meaningful regardless of cell type.\nAs shown in Figure 3b, MAE-G/8 significantly outperforms other models in Anax group linear probe classification. The best representations once again are obtained from an intermediate block, achieving a balanced accuracy (0.32) that is 5% greater compared to its final block's output features. We"}, {"title": "5 Whole-genome benchmarking", "content": "Table 2 presents our benchmarks computed across the whole-genome. These evaluate the genomic representations obtained for each model by aggregating millions of embeddings of cell images spanning >100,000 of genetic knockout perturbations (17,063 genes \u00d7 6 single guide RNAs each) on HUVEC cells from RxRx3 (Fay et al., 2023). Computing these benchmarks for HCS screens typically requires inferring 140 million crops from the genome-wide RxRx3 microscopy screen (Kraus et al., 2023) (64 tiled crops per each of the 2.2 million wells), but, to reduce compute costs, we discard the outer ring of crops, leaving the 36 center non-edge crops for each well. This requires 80 million forward passes to comprehensively evaluate a new encoder. After inference, we use typical variation normalization (Ando et al., 2017b) and chromosome arm bias correction (Lazar et al., 2023) to post-process the embeddings and aggregate them to the gene-level.\nWe present the multivariate biological relationship recall benchmarks proposed by Celik et al. (2024) and originally evaluated for MAEs by Kraus et al. (2023, 2024). These metrics evaluate how many annotated pair-wise relationships are recalled from public databases (CORUM, hu.MAP, Reactome-PPI, StringDB) in the extremities of a ranked list of cosine similarities of all pair-wise post-processed embeddings (details in \u00a7 A.6). To ensure embeddings represent technical replicates of perturbations consistently, we also evaluate model performance on replicate consistency based on the experimental design used in the RxRx3 dataset. Specifically, we compare the similarity of the embedding for corresponding wells across different experiments via a non-parametric statistical test. The test statistic measures the difference between the perturbation replicates' similarity distribution and an empirical null distribution, with larger values indicating greater consistency (details in \u00a7 A.7).\nIn order to compare models, we summarize the resulting statistics over all technical replicates in RxRx3 by taking their median, as reported in columns KS and CVM in Table 2, and visualized in Figure 5. MAEs pretrained on microscopy data show improved performance compared the baseline models. Furthermore, training on the Phenoprints-16M dataset improves the performance of the MAEs significantly and trimmed MAE-G/8 achieves the best overall performance.\nOur linear probing analysis allowed us to trim our models to better encoding blocks. Comparing models on their best respective blocks, MAE-G/8 improves on MAE-L/8 with a 16% improvement in Anax functional gene group classification (.27\u2192.31) and a 24% improvement in RxRx1 perturbation classification (.41\u2192.51). Compared to the best published result for whole-genome benchmarks (MAE-L/8 trained on RPI-93M (Kraus et al., 2023)), MAE-G/8 obtains a 20% improvement in replicate consistency KS (.52\u2192.63) and 4.3% improvement in StringDB recall (.472\u2192.492). When using our linear probes to select outputs from block b* = 15 (Equation 1) from that MAE-L/8, the gain for MAE-G/8 changes to 9.2% in KS and 3.5% in StringDB recall.\nSimilarly, linear probing to select optimal ViT blocks led to significant improvements even when applied to frozen Dino-V2 based models pretrained on natural images. Dino-V2 ViT-G obtains a nearly 20% improvement CORUM recall (.44\u2192.53) by using the embeddings extracted at b* = 16 (chosen by linear probes) rather than the final embedding from b = 40 (which performs worse than a random untrained ViT-S). Dino-V2 ViT-S also observes improvements by using b* = 5 rather than b = 12 and outperforms Dino-V2 ViT-G in replicate consistency."}, {"title": "6 Discussion and Conclusions", "content": "This work demonstrates that: (1) within the context of biological imaging, trimming many ViTs to an earlier block leads to stronger biological linearity and improved performance on downstream tasks in addition to cheaper inference costs ; (2) linear probing performance on a subset of genetic perturbations correlates strongly with downstream performance on whole-genome benchmarks and can be used to optimize which block is selected for representing the whole-genome ; (3) the most scaled model, MAE-G/8, obtains the overall best performance across all benchmarks and linear probes, providing further evidence for the scaling hypothesis in biological image data . This demonstrates that intentionally scaling training compute and parameters of SSL models for microscopy can benefit downstream biological relationship recall, whole-genome replicate consistency, and biological linear separability on smaller datasets .\nMore broadly, this work proposes a reusable recipe for training and extracting optimal representations from fully self-supervised models trained on experimental data. The pattern we use can be applied to other domains that contain data from repeated experiments but without accurate ground truth labels. Specifically, we recommend: (1) curating the training set by identifying diverse sets of samples that are represented consistently, e.g., by using a pre-existing model to select such samples; (2) training a scaled transformer-based model using a self-supervised learning technique, such as masked autoencoding; and, (3) evaluating the performance of the trained transformer at every block to identify the optimal layer for representing the data."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Training dataset curation details", "content": "In order to produce Phenoprint-16M, we curated 93M using the following steps:\n1. Filtering out data that did not pass data quality filters related to the focus of the image, quantity of dead cells, assay conditions, and presence of strong anomalous imaging artifacts.\n2. Filtering out data with missing information about the perturbations applied, data with more than 3 perturbations applied, and data of unusual size (in the image dimension or number of channels).\n3. Filtering out perturbation conditions that had been in less than 3 distinct experiments or 20 distinct wells so as to capture a variety of batch effects and have a broad sample of positives per class.\n4. Under-sampling perturbation conditions that were clearly over-represented in the dataset. Our experiment designs contain positive controls, negative controls, and wells without perturbation within each experiment. At this step, we keep 10% of positive controls and wells without any perturbation, 30% of negative controls, and all other perturbation conditions.\n5. Filtering out wells where none of the perturbation conditions had a phenoprint (\u00a7A.3) (across different map types) in any experiment it had been run in."}, {"title": "A.2 Training hyperparameters", "content": null}, {"title": "A.3 Perturbation Consistency", "content": "In order to assess the consistency of the induced morphology on the cells by the perturbations, we used a non-parametric perturbation consistency test similar to the one introduced in Celik et al. (2024). Let $X_{g,1}, X_{g,2}, \\cdot\\cdot\\cdot, X_{g,n}$ be the embeddings for replicates of perturbation $x_g$ on experiment (batch) e. As the test statistic for perturbation consistency, $S_g^e$ is defined as the mean of the cosine similarities across all pairs of replicates of $x_g$.\n$S_g^e = \\frac{1}{n^2} \\sum_{i}^{n} \\sum_{j}^{n} \\frac{(X_{g,i}, x_{g,j})}{||X_{g,i}|| ||X_{g,j}||}$\nwhere (.) and ||.|| denote dot product and L2 norm.\nStatistical significance of $S_g^e$ is assessed using a permutation test comparing it against an empirical null distribution generated using the same statistic for a set of randomly selected perturbations in experiment e, {${S_1^e,\\cdot\\cdot\\cdot, S_k^e}$}. The p-value for $S_g^e$ is computed as follows\n$p_g = \\frac{max\\{\\#\\{S_i^e \\geq S_g^e\\},1\\}}{K}$\nWhen multiple experiments existed for the same perturbation, we combined p-values using the Cauchy Combination test (Liu & Xie, 2018)."}, {"title": "A.4 Training linear probes", "content": "In this section, we provide details about the training process and preprocessing steps used in our logistic regression models. These models were trained on output features derived from various Vision Transformer (ViT) blocks.\nThe data was split by experiments, ensuring that the test data originated from experiments distinct from those used for training. This approach helps to validate the generalization performance of our models across different experimental conditions.\nFor both RxRx1 gene prediction and Anax group prediction, we apply StandardScaler from the scikit-learn library as the only preprocessing step to standardize the features prior to training linear probes. StandardScaler transformation was fitted on data from the train split. We trained the logistic regression models using scikit-learn's LogisticRegression class. The following parameters and settings were used during model optimization:\n\u2022 Solver: lbfgs\n\u2022 Maximum Iterations: 2000\n\u2022 Class Weight: balanced\nFor RxRx1 gene prediction, we trained logistic regression models to predict one of 1139 possible perturbation labels (1138 genetic perturbation and non-perturbed control). For Anax group prediction, we trained logistic regression models to predict one of 40 possible function group labels (\u00a7 A.8). We report the balanced test accuracy as the main evaluation metric for all linear probing experiments."}, {"title": "A.5 Anax classification for other cell lines/treatment conditions: ARPE19 and HUVEC with TNF-alpha background", "content": "We performed linear probing on imaging data obtained for a retinal pigment epithelia (RPE) cell line, ARPE19, and HUVEC cells treated with an inflammatory cytokine, TNFa. We similarly observed that intermediate blocks often have the most linearly separate features compared to the final block."}, {"title": "A.6 Biological Relationship Recall", "content": "A valuable use of large-scale HCS experiments is to perform large-scale inference of biological relationships between genetic perturbations. We evaluate each model's ability to recall known relationships by using the biological relationship recall benchmark described in Celik et al. (2024). First, we correct for batch effects using Typical Variation Normalization (TVN) and also correct for possible chromosome arm biases known to exist in CRISPR-Cas9 HCS data . To infer biological relationships, we compute the aggregate embedding of each perturbation by taking the spherical mean over its replicate embeddings across experiments. We use the cosine similarity of a pair of perturbation representations as the relationship metric, setting the origin of the space to the mean of negative controls. We compare these similarities with the relationships found in the following public databases: CORUM , hu.MAP , Reactome and StringDB Szklarczyk et al. (2020) (with >95% combined score). Table 2 reports the recall of known relationships amongst the top and bottom 5% of all cosine similarities between CRISPR knockout representations in RxRx3"}, {"title": "A.7 Replicate Consistency", "content": "In order to assess the reproducibility of the perturbations across their technical replicates, we compare the distributions of the similarities for same perturbations across replicates against an empirical null distribution. Specifically, for technical replicate experiments e and e\u2032, we calculate the cosine similarity between the embeddings of perturbation x; in them, denoted as $s_{x_i}^{ee'}$.The query distribution $q^{ee'}_i$ is constructed by computing the cosine similarities for all perturbations that have a matching well on experiments $e_a$ and $e_r$. An empirical null distribution of identical cardinality is created by computing cosine similarity, $r_{x_i}^{e'k,1}$, between random pairs from $e_a$ and $e_r$ such that no pair corresponds to the same perturbation, $po^2$. Using non-parametric statistical tests, namely Kolmogorov-Smirnov (KS) and Cramer Von-Mises (CVM), we can evaluate the hypothesis that $q^{ee'}_i$ and $po^2$ are drawn from the same distribution. Formally, let $Q^{ee'}_i(x)$ and $P_{i}^{0^{ee'}}(x)$ be the cumulative distribution functions for $q^{ee'}_i$ and $po^2$ respectively, then the KS statistic for the two-sample case of technical replicate experiments e and e\u2032 is defined as:\n$KSei = sup |Qei (x) \u2013 Pei (x)|.$\nThe Cram\u00e9r-von Mises test statistic (CVM) for experiments $e_a$ and $e_r$ is computed as:\n$CVMei = \\frac{1}{2N^2}\\sum_{m=1}^{N} [(r_m-m)^2 + (8_m-m)^2 - \\frac{1}{4N^2} - \\frac{1}{12N}$"}, {"title": "A.8 Anax Group Prediction Details", "content": "The Anax probing task introduced in this paper is intended to balance capturing a diverse range of biology that is broadly conserved between cell types with a reduced cost of execution. The name \"Anax\" is a reference to Anaximander, the 6th century B.C. philosopher credited with making the first world map.\nIn curating these genes, we analyzed the sources listed in \u00a7 A.6 as well as internal gene expression data to produce \u201cfunctional\u201d groups corresponding to biological processes, cellular components, and molecular functions. Not all genes within each group are expected to have the same knockout phenotype, but are classified by humans as having related function \u2013 linear separability of these genes would indicate that a model has learned similar concepts to those deemed significant by biologists.\nThe gene groups we use for the 40-class Anax group classification task (\u00a7 A.4) are listed in Table 4."}, {"title": "A.9 Correlation between model scale and benchmark results", "content": "In Figure 7 we show the correlations between training FLOps (floating point operations) and downstream results. Over all benchmarks we observe a very strong consistent linear trend where scaling training FLOps improves overall pwerformance. This work provides the next log step in scale as we enter into the billion-parameter model regime with MAE-G/8. These results therefore provide additional evidence that the trend initially discovered by Kraus et al. (2023) between FLOps and relationship recall actually extends both to billion-parameter models and even moreso for other biologically meaningful benchmarks pertaining to linear probes on small experiments and to replicate consistency on the whole-genome."}]}