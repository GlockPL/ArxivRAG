{"title": "What to do if language models disagree? Black-box model ensembling for\ntextual and visual question answering", "authors": ["Yuxi Xia", "Kilm Zaporojets", "Benjamin Roth"], "abstract": "A diverse range of large language models (LLMs),\ne.g., ChatGPT, and visual question answering\n(VQA) models, e.g., BLIP, have been developed\nfor solving textual and visual question answering\ntasks. However, both LLMs and VQA models en-\ncounters challenges when applied to task-specific\ndatasets. Fine-tuning these models is either diffi-\ncult, as it requires access via APIs, rendering them\nas black-boxes, or costly due to the need of tuning\na large number of parameters. To address this, we\nintroduce InfoSel, a data-efficient and lightweight\nensemble method that learns to dynamically pick\nthe winner from existing black-box models for\npredictions on both textual and multimodal vi-\nsual question answering tasks. Unlike traditional\nensemble models, InfoSel does not rely on predic-\ntion probabilities or confidences, which typically\nare not available in black-box models. Experimen-\ntal results on four datasets demonstrate that our\napproach achieves an absolute increase of up to\n+5.27% in the F1-score compared to standalone\nLLMs. Remarkably, this improvement is achieved\nby utilizing only 1K training instances and 110M\nmodel parameters for training task-specific en-\nsemble models.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remark-\nable proficiency across a wide range of tasks, predominantly\nattributed to their ability to comprehend instructions and\ntap into vast repositories of high-quality data (Bubeck et al.,\n2023; Laskar et al., 2023). For example, ChatGPT finds\nextensive utilization in daily textual question answering\n(TQA) tasks, rendering substantial convenience to a myriad\nof users (Touvron et al., 2023).\u00b9 For visual question answer-\ning (VQA) tasks, VQA models have exhibited exceptional\nversatility, primarily due to their capability to comprehend\nboth visual and textual context (Gong et al., 2023).\nHowever, recent work (Koco\u0144 et al., 2023; Laskar et al.,\n2023) indicates that LLMs, such as ChatGPT, fall short of\nstate-of-the-art performance on task-specific datasets. Simi-\nlarly, VQA models (Li et al., 2022; 2021b; Bao et al., 2022)\nface challenges when applied to specialized datasets due to\nthe idiosyncrasies in the content, format or structure of these\ndatasets (Arora et al., 2018). Unfortunately, fine-tuning\nLLMs (e.g., LLaMA-2-70b-chat (Touvron et al., 2023)) on\ntask-specific data requires a large number of GPU hours.\nAlternatively, training smaller, task-specific models from\nscratch requires large amount of labeled data in order to\nachieve comparable performance. Furthermore, fine-tuning\nLLMs through proprietary APIs with self-uploaded labeled\ntraining data not only requires LLM experts' knowledge but\nis also expensive.\u00b2 These fine-tuned models further remain\nblack-box, with restricted access to details regarding archi-\ntectural intricacies, model weights, training data, and even\nprediction confidences.\nIn order to address these computational and accessibil-\nity challenges associated with fine-tuning, we introduce\na scalable ensemble method called InfoSel (Informed Se-\nlection). InfoSel allows for training with just a few task-\nspecific samples and is lightweight, with only a few million\nparameters. Unlike current LM-ensemble methods (e.g.,\nMetaQA (Puerto et al., 2021), LLM-Blender (Jiang et al.,\n2023)) which depend on the confidence scores or log prob-\nabilities and thus can not be applied to black-box models\nlike GPT3.5 text-davinci models, InfoSel does not rely on\nsuch information and offers black-box ensembling. Fur-\nthermore, our proposed ensemble method incorporates task-\nspecific optimization, allowing it to be easily adapted to\ndifferent datasets, considering variations of both the inputs\nand predicted answers from the ensembled LLMs (base\nmodels). This contrasts with traditional ensemble methods"}, {"title": "2. Related Work", "content": "Domain Adaptation. These methods aim to improve the\nperformance of a model on a task-specific domain by lever-\naging knowledge from other domains (Zhou et al., 2022).\nMethods such as fine-tuning (Yosinski et al., 2014), feature\nadaptation (Long et al., 2015) and data augmentation (Choi\net al., 2019) aim to improve the performance of standalone\nmodels and thus typically require large amounts of labeled\ntraining data or access to the model architecture and weights.\nInfoSel addresses this challenge by employing a lightweight\nensemble model trained on a small amount of labeled data,\nand restricted to access exclusively the predictions produced\nby black-box base models.\nEnsemble Learning. Ensemble methods generate and com-\nbine multiple learners (ML models) to address a partic-\nular ML task (Sagi & Rokach, 2018). Classical ensem-\nbling approaches like boosting (Schapire, 2013) and bag-"}, {"title": "3. InfoSel Ensemble Training", "content": "Figure 1 illustrates the proposed InfoSel and InfoSel* frame-\nworks to ensemble LLMs for TQA tasks (left), and VQA\nmodels for VQA tasks (right). We differentiate TQA compo-\nnents using LLMs and VQA components using VQA mod-\nels by denoting them with superscripts l and v respectively.\nSimilarly, to distinguish between InfoSel, InfoSel* and FT\nmodels used in TQA and VQA tasks, we add suffixes \u201c-\nTT\u201d and \u201c-MT\u201d respectively. For example, the InfoSel-MT\nmodel in Figure 1, refers to the InfoSel for the VQA task."}, {"title": "3.1. InfoSel Training for TQA", "content": "Before training InfoSel, we first perform the data prepara-\ntion (top of Figure 1) for both training and testing. Next,\nwe train InfoSel and FT models, which is a necessary step\nbefore training the InfoSel*.\nData Preparation. First, we randomly sample N content-\nquestion pairs {(Ci, Qi)}_{i=1}^N and the corresponding ground\ntruth answers {Ai}_{i=1}^N from various benchmark datasets\n(refer to Section 4.1). Next, we build prompts Pi following\nspecific prompt rules Pi = R(Ci, Qi) (refer to Section 4.1).\nUsing these prompts instead of plain (Ci, Qi) text improves\nthe LLMs' answer quality (Bach et al., 2022).\nWe select K (K=3) black-box LLMs {M_j}_{j=1}^K to generate\nanswers on the N prompts. The answer generated by M_j\non P_i is denoted as A_{ij} (A_{ij} = M_j(P_i)). Thereby, K\nLLMs provide N * K candidate answers for N prompts.\nWe calculate the word-level F1-scores (Rajpurkar et al.,\n2018) of all the candidate answers {A_{ij}}_{j=1}^K respectively\nfor P_i. These F1-scores serve as target Y_i to optimize the\nensemble model:\n\\(Y_i = {F1(A_{ij},A_i)}_{j=1}^K, Y_i \\in R^K\\).\nThe input for the ensemble training consists of K texts. Each\ntext is formed by concatenating P_i with each individual\nanswer predicted by a base model j, A_{ij}. More formally,\nthe input X_i is:\n\\(X_i = {[P_i, A_{ij}]}_{j=1}^K, |X_i| = K\\).\nThe inputs {X_i}_{i=1}^N and the corresponding target labels\n{Y_i}_{i=1}^N are used for ensemble training.\nInfoSel-TT. We use a textual BERT-base (Devlin et al.,\n2019) transformer f_\\theta (\\theta denote trainable model parameters)\nas the backbone of InfoSel-TT. To achieve faster conver-\ngence, we load the pre-trained weights of bert-base-uncased\nmodel.4 The input vector X_i is passed to f_\\theta to generate K\nsentence representations for each value in X_i respectively.\nThus, the sentence representation R^j_i of [P_i, A_{ij}] from f_\\theta\nis:\n\\(R_i^j = f_\\theta([P_i, A_{ij}]), R_i^j \\in R^{768}\\)\nA dense layer (f_d) is followed to classify {R^j_i}_{j=1}^K, and\nis trained to match the target label Y_i using binary cross\nentropy loss L_{BCE}. More formally, the training objective\nof InfoSel-TT is:\n\\(\\min_\\theta \\sum_{i=1}^N L_{BCE}(f_d ([f_\\theta([P_i, A_{ij}])]_{j=1}^K), Y_i)\\).\nFinally, the trained InfoSel-TT model (M_{IT}) selects the win-\nner model M_{i,win} from {M_j}_{j=1}^K for the input P_i with\nthe highest probability score based on the selection logits\nproduced by f_\\theta. A_{i,win} denotes the answer provided by\nM_{i,win}\nM_{i,win} = M_{IT}({[P_i, A_{ij}]}_{j=1}^K), M_{i,win} \\in {M_j}_{j=1}^K,"}, {"title": "FT-TT", "content": "A potential limitation of using InfoSel-TT model\nonly is the lack of exposure of the base models to new (un-\nseen) labels appearing in the task-specific datasets. To ad-\ndress this, we fine-tune a separate lightweight TT model di-\nrectly on the TQA datasets to learn these new labels. Specif-\nically, the training objective is to locate the start and end\ntoken position of the answer from the context C_i. We pro-\nvide the token positions of A_i as the target label, such that\nthe model is optimized to classify each token in two classes\n(start/end token). This fine-tuned textual transformer model\nis referred to as FT-TT (M^{ft}).\u2075 We denote the answer pre-\ndicted by M^{ft} on P_i as A_i^{ft}."}, {"title": "InfoSel*-TT", "content": "This model performs a further ensemble train-\ning of FT-TT and InfoSel-TT models with the same training\nscheme and labeled training data as InfoSel-TT. We antici-\npate that the thus trained InfoSel*-TT model (M_{IT^*}) on the\noutput of InfoSel-TT and the label finetuned FT-TT, will\nimprove the ability to handle labels unseen by base models.\nAs a result, we expect an improvement in the overall task-\nspecific performance. The winner model selected by M_{IT^*}\nbelong to {M_{IT}, M^{ft}}.\nA_{i,win} = M_{i,win}^{IT^*}(P_i)."}, {"title": "3.2. InfoSel Training for VQA", "content": "Data Preparation. Given N image-question pairs\n{(Ii, Qi)}_{i=1}^N from dev data of VQA benchmark datasets,\nwe use K (K=3) pre-trained VQA models to predict an-\nswers A_{ij} as follows: {M_j}((Ii, Qi)) \u2192 A_{ij}. We\ndenote the ground truth answer for image-question pair\n(Ii, Qi) as A. Target labels Y_i for ensemble training are\ngiven by the accuracy scores of the K candidate answers\nevaluated on A:\nY_i = {Acc(A_{ij}, A)}_{j=1}^K, Y_i \\in R^K.\nThe concatenation of question (Qi) with each of the candi-\ndate answers (A_{ij}) obtained from the base models and the\ncorresponding image (Ii) serves the input to our ensemble\nmodel InfoSel-MT:\nX_i = {(I_i, [Q_i, A_{ij}])}_{j=1}^K, |X| = K.\nInfoSel-MT. A Multimodal Transformer (MT, f_M) (Li et al.,\n2021b) is employed as the backbone for InfoSel-MT. Specif-\nically, we first generate visual features Vi of I_i using a\npre-trained R-CNN model (M_r) (Anderson et al., 2018). Vi\nis composed of a vector of the image region features vi and\nthe detected tags (i.e., object labels of the image) (Li et al.,\n2021b). The concatenated question-answer pair [Qi, A_{ij}]\nand Vi is then passed together with to MT (f_M) to generate"}, {"title": "FT-MT", "content": "Similar to FT-TT, FT-MT composed of a trainable\nMT and a Multilayer Perceptron (MLP) is fine-tuned with\nthe same training data as InfoSel-MT. Differently, FT-MT\nsolves a multi-label classification task by classifying the\nfused contextual representation of Qi (instead of [Qi, A_{ij}]\nlike InfoSel-MT) and Vi to a predefined answer list (labels).\nThis list contains frequent answers from the training data.\nAs a result, a trained FT-MT model (M_{fm}) can learn to\npredict the unseen (new) labels (answers) contained in the\ntask-specific datasets, but not in the pre-training data of the\nbase models. A_{fm}^i denotes the answer predicted by M_{fm}\nover (Ii, Qi). The training scheme is adapted from (Li\net al., 2021b)."}, {"title": "InfoSel*-MT", "content": "Similar to InfoSel* -TT, InfoSel*-MT model\nM_{im+} ensembles the FT-MT and InfoSel-MT models using\nthe same training scheme as in InfoSel-MT. The winner\nmodel selected by M_{im+} belong to {M_{im}, M_{fm}}."}, {"title": "4. Experiments and Analysis", "content": "To demonstrate the data efficiency of our approach, we sub-\nsampled four publicly available benchmark datasets. This\nresulted in four Mini datasets, amounting to ~1% of the\nTQA datasets' and ~10% of the VQA datasets' original\nsize. Table 2 presents the details of these datasets."}, {"title": "TQA datasets", "content": "We generated two Mini datasets, Mini-\nSDv2 and Mini-NQ, consisting of 1,000 randomly sampled\ninstances from SQUAD-V2 (Rajpurkar et al., 2018) and NQ-\nOpen (Kwiatkowski et al., 2019) train splits, respectively.\nFor Mini-NQ, we followed (Fisch et al., 2019) to use long\nanswers as the context, and short answers as the ground"}, {"title": "4.2. Base Models", "content": "We experiment with ensembling GPT-3.5-turbo-0613 (Chat-\nGPT), LLaMA-2-70b-chat (hereinafter referred to as\n\u201cLLaMA\") (Touvron et al., 2023) and GPT-3.5 text-davinci-\""}, {"title": "4.3. Baselines", "content": "Majority Voting (MV). MV makes a collective decision\nby considering the predicted answers as a group of individ-\nals voting on a particular input. The answer that receives\nthe most votes is the winner, otherwise, ties are broken\nrandomly.\nWeighted Voting (WV). We adopt a strategy similar to\nSchick et al. (2020), where the model accuracy of the train\ndata before training is used as the weight for average weight-\ning. In our case, we use the corresponding accuracy of the\nbase models as the weight for voting.\nPageRank (Brin & Page, 2012). We adapt PageRank as a\nbaseline to determine the most suitable answer in a graph\nwhere all the answers to one question are connected by their\nBLEURT (Sellam et al., 2020) similarities.\nOverall Local Accuracy (OLA) (Woods et al., 1997). Fol-\nlowing (Cruz et al., 2018), we use the k-nearest neigh-\nbors algorithm to divide the input space (representations\nof prompts for TQA, representations of images and ques-\ntions for VQA) of training data into 7 regions. The overall\nlocal accuracy of each base model in different regions is\ncomputed as its region competence. The model presenting\nthe highest competence level is selected to predict the label\nof inputs that fall in the same region.\nPairRanker and LLM-Blender (Jiang et al., 2023). Both\nmethods were originally designed for text generation tasks\n(e.g., machine translation and speech recognition). Pair-\nRanker model (DeBERTa (He et al., 2023), 400M parame-\nters) is trained to rank a pair of candidate predictions from\ntwo LLMs using multiple optimizing objectives (i.e., log\nprobabilities, BART score, BERTScore, etc). A bubble"}, {"title": "4.4. Experimental Setup", "content": "Evaluation Metric. LLMs tend to generate contextual an-\nswers that lead to lower scores in the exact match (EM).\nTherefore, we mainly use the (per-answer) token-level F1-\nscore from the official evaluation guidance of the datasets\nas the main evaluation metric for TQA performance. Our\nresults differ from the ones reported in (Laskar et al., 2023;\nKoco\u0144 et al., 2023) because we do not apply any post-\nprocessing, human evaluation or output constraints for the\ngenerated answers.\nSetup. We fixed the batch size to the upper limit of the\nserver capacity, while the learning rates and epochs are\nselected after a grid search on a set of values (learning rates:\n{e-3, 5e-4, e-4, 5e-5, e-5, 5e-6, e-6}, epochs: {3, 5, 10, 15, 20}).\nModels for TQA are trained for 5 epochs using a learning\nrate of 5 \u00d7 10\u207b\u2075 and batch size of 4. Models for VQA use\nthe same learning rate but a batch size of 16 for 20 epochs.\nWe spent ~74 and ~290 seconds training 1 epoch on 1,000\nsamples for TQA and 4,319 samples for VQA respectively.\nThe training was performed on 1 GPU with 16GB memory\nof a DGX1 server ((Pascal) Tesla P100)."}, {"title": "4.5. Performance Comparison", "content": "In this section, we analyze the performance of our method,\ntaking into account its distinctive characteristics as de-\nscribed in Table 1. Concretely, we focus on comparing\nour models in terms of task-specific performance, data effi-\nciency, lightweight design, and multimodal capabilities.\nTask-specific Performance. Table 3 demonstrates the task-\nspecific performance of InfoSel, base models and baselines\non textual and visual QA datasets. For TQA, we observe\nthat LLaMA underperforms other base models. Upon closer\nexamination, we found that LLaMA generates longer expla-\nnation text which, although often accurate, decreases the\nEM and F1-score values. Conversely, a more consistent\nperformance of base models is observed for VQA. All the\nmodels demonstrate superior performance on Mini-NQ com-\npared to Mini-SDv2. This is because Mini-SDv2 contains\n50% of unanswerable questions, written adversarially and\nspecifically designed to be challenging for the QA task (Ra-\njpurkar et al., 2018). Similarly, Mini-Viz contains 28%\nof unanswerable questions, and the label \"unanswerable\u201d"}, {"title": "4.7. Case Study", "content": "Table 6 illustrates two insightful cases from the predictions\nof different models on textual Mini-SDv2 and Mini-NQ QA\ndatasets. The first case showcases the ability of InfoSel-\nTT to select the right model (Davinci) when the rest of\nthe models is incorrect. However, InfoSel*-TT selects the\nwrong answers from the FT-TT model and underperforms\nInfoSel-TT. The second case illustrates the ability of LLMs\nto generate correct answer (\u201c2006\u201d) despite the ground truth\nannotation error (\u201c2005\u201d). This demonstrates the advantage\nof ensembling highly expressive LLMs instead of relying\nonly on fine-tuning small-size models such as FT-TT."}, {"title": "5. Conclusion", "content": "In this paper, we propose InfoSel, a novel lightweight and\ntask-specific ensemble method designed to learn the dy-\nnamic selection of the optimal model from a range of dis-\ntinct black-box base LLMs. We find that using only 110M\ntrainable parameters, our method is able to substantially\nincrease the performance upon the best performing base\nLLM. Additionally, our analysis reveals that InfoSel remains\nrobust regardless the incorrect predictions of the lowest per-\nforming LLM. Our findings also show that our solution is\nhighly data-efficient. Concretely, it requires only a fraction\nof instances (as few as 10) from the training set to outper-\nform base LLMs. Finally, our experimental results reveal\nthe ability of InfoSel to be adapted to multimodal setting,\nshowing a substantial increase in performance compared to\nstate-of-the-art alternatives."}]}