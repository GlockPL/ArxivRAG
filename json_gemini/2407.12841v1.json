{"title": "What to do if language models disagree? Black-box model ensembling for textual and visual question answering", "authors": ["Yuxi Xia", "Kilm Zaporojets", "Benjamin Roth"], "abstract": "A diverse range of large language models (LLMs), e.g., ChatGPT, and visual question answering (VQA) models, e.g., BLIP, have been developed for solving textual and visual question answering tasks. However, both LLMs and VQA models encounter challenges when applied to task-specific datasets. Fine-tuning these models is either difficult, as it requires access via APIs, rendering them as black-boxes, or costly due to the need of tuning a large number of parameters. To address this, we introduce InfoSel, a data-efficient and lightweight ensemble method that learns to dynamically pick the winner from existing black-box models for predictions on both textual and multimodal visual question answering tasks. Unlike traditional ensemble models, InfoSel does not rely on prediction probabilities or confidences, which typically are not available in black-box models. Experimental results on four datasets demonstrate that our approach achieves an absolute increase of up to +5.27% in the F1-score compared to standalone LLMs. Remarkably, this improvement is achieved by utilizing only 1K training instances and 110M model parameters for training task-specific ensemble models.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable proficiency across a wide range of tasks, predominantly attributed to their ability to comprehend instructions and tap into vast repositories of high-quality data. For example, ChatGPT finds extensive utilization in daily textual question answering (TQA) tasks, rendering substantial convenience to a myriad of users. For visual question answering (VQA) tasks, VQA models have exhibited exceptional versatility, primarily due to their capability to comprehend both visual and textual context.\nHowever, recent work indicates that LLMs, such as ChatGPT, fall short of state-of-the-art performance on task-specific datasets. Similarly, VQA models face challenges when applied to specialized datasets due to the idiosyncrasies in the content, format or structure of these datasets. Unfortunately, fine-tuning LLMs (e.g., LLaMA-2-70b-chat) on task-specific data requires a large number of GPU hours. Alternatively, training smaller, task-specific models from scratch requires large amount of labeled data in order to achieve comparable performance. Furthermore, fine-tuning LLMs through proprietary APIs with self-uploaded labeled training data not only requires LLM experts' knowledge but is also expensive. These fine-tuned models further remain black-box, with restricted access to details regarding architectural intricacies, model weights, training data, and even prediction confidences.\nIn order to address these computational and accessibility challenges associated with fine-tuning, we introduce a scalable ensemble method called InfoSel (Informed Selection). InfoSel allows for training with just a few task-specific samples and is lightweight, with only a few million parameters. Unlike current LM-ensemble methods which depend on the confidence scores or log probabilities and thus can not be applied to black-box models like GPT3.5 text-davinci models, InfoSel does not rely on such information and offers black-box ensembling. Furthermore, our proposed ensemble method incorporates task-specific optimization, allowing it to be easily adapted to different datasets, considering variations of both the inputs and predicted answers from the ensembled LLMs (base models). This contrasts with traditional ensemble methods such as OLA and PageRank which are not adapted to task-specific particularities (e.g., different features) of different datasets. Finally, our method efficiently deals with multimodal inputs. Concretely, our results exhibit superior performance on multimodal VQA inputs compared to state-of-the-art LLM-Blender ensemble method which is designed to work exclusively with text. Table 1 compares our method with alternatives."}, {"title": "2. Related Work", "content": "Domain Adaptation. These methods aim to improve the performance of a model on a task-specific domain by leveraging knowledge from other domains. Methods such as fine-tuning, feature adaptation and data augmentation aim to improve the performance of standalone models and thus typically require large amounts of labeled training data or access to the model architecture and weights. InfoSel addresses this challenge by employing a lightweight ensemble model trained on a small amount of labeled data, and restricted to access exclusively the predictions produced by black-box base models.\nEnsemble Learning. Ensemble methods generate and combine multiple learners (ML models) to address a particular ML task. Classical ensembling approaches like boosting and bag-"}, {"title": "3. InfoSel Ensemble Training", "content": "Figure 1 illustrates the proposed InfoSel and InfoSel* frameworks to ensemble LLMs for TQA tasks (left), and VQA models for VQA tasks (right). We differentiate TQA components using LLMs and VQA components using VQA models by denoting them with superscripts l and v respectively. Similarly, to distinguish between InfoSel, InfoSel* and FT models used in TQA and VQA tasks, we add suffixes \"-TT\u201d and \u201c-MT\u201d respectively. For example, the InfoSel-MT model in Figure 1, refers to the InfoSel for the VQA task.\n3.1. InfoSel Training for TQA\nBefore training InfoSel, we first perform the data preparation (top of Figure 1) for both training and testing. Next, we train InfoSel and FT models, which is a necessary step before training the InfoSel*.\nData Preparation. First, we randomly sample N content-question pairs \\(\\{(C_i, Q_i)\\}_{i=1}^N\\) and the corresponding ground truth answers \\(\\{A\\}_{i=1}^N\\) from various benchmark datasets (refer to Section 4.1). Next, we build prompts \\(P_i\\) following specific prompt rules \\(P_i = R(C_i, Q_i)\\) (refer to Section 4.1). Using these prompts instead of plain \\((C_i, Q_i)\\) text improves the LLMs' answer quality.\nWe select K (K=3) black-box LLMs \\(\\{M^l_j\\}_{j=1}^K\\) to generate answers on the N prompts. The answer generated by \\(M^l_j\\) on \\(P_i\\) is denoted as \\(A^l_{ij}\\) (\\(A^l_{ij} = M^l_j(P_i)\\)). Thereby, K LLMs provide N * K candidate answers for N prompts. We calculate the word-level F1-scores of all the candidate answers \\(\\{A^l_{ij}\\}_{j=1}^K\\) respectively for \\(P_i\\). These F1-scores serve as target \\(Y_i\\) to optimize the ensemble model:\n\\[Y_i = \\{F1(A^l_{ij},A)\\}_{j=1}^K, Y\\in\\mathbb{R}^K.\\]\nThe input for the ensemble training consists of K texts. Each text is formed by concatenating \\(P_i\\) with each individual answer predicted by a base model j, \\(A^l_{ij}\\). More formally, the input \\(X_i\\) is:\n\\[X_i = \\{[P_i, A^l_{ij}]\\}_{j=1}^K, |X| = K.\\]\nThe inputs \\(\\{X_i\\}_{i=1}^N\\) and the corresponding target labels \\(\\{Y_i\\}_{i=1}^N\\) are used for ensemble training.\nInfoSel-TT. We use a textual BERT-base transformer \\(f_{\\theta}\\), (\\(\\theta\\) denote trainable model parameters) as the backbone of InfoSel-TT. To achieve faster convergence, we load the pre-trained weights of bert-base-uncased model. The input vector \\(X_i^l\\) is passed to \\(f_{\\theta}\\) to generate K sentence representations for each value in \\(X_i^l\\) respectively. Thus, the sentence representation \\(R_{ij}^l\\) of \\([P_i, A^l_{ij}]\\) from \\(f_{\\theta}\\) is:\n\\[R^l_{ij} = f_{\\theta}([P_i, A^l_{ij}]), R^l_{ij} \\in \\mathbb{R}^{768}\\]\nA dense layer \\((f_d)\\) is followed to classify \\(\\{R^l_{ij}\\}_{j=1}^K\\), and is trained to match the target label \\(Y_i\\) using binary cross entropy loss \\(L_{BCE}\\). More formally, the training objective of InfoSel-TT is:\n\\[\\min_{\\theta} \\sum_{i=1}^{N} L_{BCE}(f_d \\{f_{\\theta} ([P_i, A^l_{ij}])\\}_{j=1}^K), Y_i).\\]\nFinally, the trained InfoSel-TT model (\\(M_{itt}^l\\)) selects the winner model \\(M^l_{i,win}\\) from \\(\\{M^l_j\\}_{j=1}^K\\) for the input \\(P_i\\) with the highest probability score based on the selection logits produced by \\(f_{\\theta}\\). \\(A^l_{i,win}\\) denotes the answer provided by \\(M^l_{i,win}\\):\n\\[M^l_{i,win} = M^l_{itt}(\\{[P_i, A^l_{ij}]\\}_{j=1}^K), M^l_{i,win} \\in \\{M^l_j\\}_{j=1}^K,\\]\n\\[A^l_{i,win} = M^l_{i,win}(P_i).\\]\nFT-TT. A potential limitation of using InfoSel-TT model only is the lack of exposure of the base models to new (unseen) labels appearing in the task-specific datasets. To address this, we fine-tune a separate lightweight TT model directly on the TQA datasets to learn these new labels. Specifically, the training objective is to locate the start and end token position of the answer from the context \\(C_i\\). We provide the token positions of \\(A_i\\) as the target label, such that the model is optimized to classify each token in two classes (start/end token). This fine-tuned textual transformer model is referred to as FT-TT (\\(M_{ft}^l\\)). We denote the answer predicted by \\(M_{ft}^l\\) on \\(P_i\\) as \\(A_{ft}^l\\).\nInfoSel*-TT. This model performs a further ensemble training of FT-TT and InfoSel-TT models with the same training scheme and labeled training data as InfoSel-TT. We anticipate that the thus trained InfoSel*-TT model (\\(M_{itt+}^l\\)) on the output of InfoSel-TT and the label finetuned FT-TT, will improve the ability to handle labels unseen by base models. As a result, we expect an improvement in the overall task-specific performance. The winner model selected by \\(M_{itt+}^l\\) belong to \\(\\{M^l_{itt}, M_{ft}^l\\}\\)."}, {"title": "3.2. InfoSel Training for VQA", "content": "Data Preparation. Given N image-question pairs \\(\\{(I_i, Q_i)\\}_{i=1}^N\\) from dev data of VQA benchmark datasets, we use K (K=3) pre-trained VQA models to predict answers \\(A^v_{ij}\\) as follows: \\(\\{M^v_j((I_i, Q_i)) \\rightarrow A^v_{ij}\\}_{j=1}^K\\). We denote the ground truth answer for image-question pair \\((I_i, Q_i)\\) as A. Target labels \\(Y_i^v\\) for ensemble training are given by the accuracy scores of the K candidate answers evaluated on A:\n\\[Y_i^v = \\{Acc(A^v_{ij}, A)\\}_{j=1}^K, Y\\in\\mathbb{R}^K.\\]\nThe concatenation of question (Qi) with each of the candidate answers (\\(A^v_{ij}\\)) obtained from the base models and the corresponding image (Ii) serves the input to our ensemble model InfoSel-MT:\n\\[X_i = \\{(I_i, [Q_i, A^v_{ij}])\\}_{j=1}^K, |X| = K.\\]\nInfoSel-MT. A Multimodal Transformer (MT, \\(f_m\\)) is employed as the backbone for InfoSel-MT. Specifically, we first generate visual features \\(V_i\\) of \\(I_i\\) using a pre-trained R-CNN model. \\(V_i\\) is composed of a vector of the image region features \\(v_i\\) and the detected tags. The concatenated question-answer pair \\([Q_i, A^v_{ij}]\\) and Vi is then passed together with to MT \\((f_m)\\) to generate"}, {"title": "4. Experiments and Analysis", "content": "To demonstrate the data efficiency of our approach, we subsampled four publicly available benchmark datasets. This resulted in four Mini datasets, amounting to ~1% of the TQA datasets' and ~10% of the VQA datasets' original size. Table 2 presents the details of these datasets.\nTQA datasets. We generated two Mini datasets, MiniSDv2 and Mini-NQ, consisting of 1,000 randomly sampled instances from SQUAD-V2 and NQ-Open train splits, respectively. For Mini-NQ, we followed to use long answers as the context, and short answers as the ground truth answers. The 1,000 samples are divided into train and validation data using an 8:2 ratio, while the trained models are tested on the dev data of the original datasets due to the unavailability of original test data. We use the setup proposed in to generate the answers from LLMs. Concretely, this setting relies on prompts generated by PromptSource, which we apply to our Mini-SDv2 dataset (unavailable for Mini-NQ). The prompts are in two forms depending on the context: (1) \"What is the answer? [Context]; [Question]; If you can't find the answer, please respond 'unanswerable'. Answer:\"; (2) \u201cAnswer the question depending on the context. [Context]; [Question]; If you can't find the answer, please respond 'unanswerable'. Answer:\". Differently, Mini-NQ does not contain unanswerable questions and thus we use the prompt \"Answer the question depending on the context without explanation. [Context]; [Question]; Answer:\". Note that the answers of LLMs can be greatly influenced by some factors such as the use of different prompts or temperatures. However, our study does not focus on prompt engineering but rather on selecting the optimal base model to generate an answer. We will publicly release our prompts as well as the answers from LLMs for reproducibility.\nVQA datasets. Our results (Figure 2) reveal that VQA tasks demand a greater quantity of training samples compared to TQA tasks. Therefore, we constructed Mini-GQA and Mini-Viz datasets using a larger fraction (the dev data) of GQA and VizWiz datasets compared to TQA datasets. The resulting Mini-GQA and Mini-Viz were divided into train and validation subsets using 8:2 ratio, while the test subset remained the same as in the original datasets."}, {"title": "4.2. Base Models", "content": "We experiment with ensembling GPT-3.5-turbo-0613 (ChatGPT), LLaMA-2-70b-chat (hereinafter referred to as \u201cLLaMA\") and GPT-3.5 text-davinci-\""}, {"title": "4.3. Baselines", "content": "Majority Voting (MV). MV makes a collective decision by considering the predicted answers as a group of individuals voting on a particular input. The answer that receives the most votes is the winner, otherwise, ties are broken randomly.\nWeighted Voting (WV). We adopt a strategy similar to where the model accuracy of the train data before training is used as the weight for average weighting. In our case, we use the corresponding accuracy of the base models as the weight for voting.\nPageRank We adapt PageRank as a baseline to determine the most suitable answer in a graph where all the answers to one question are connected by their BLEURT similarities.\nOverall Local Accuracy (OLA) Following, we use the k-nearest neighbors algorithm to divide the input space (representations of prompts for TQA, representations of images and questions for VQA) of training data into 7 regions. The overall local accuracy of each base model in different regions is computed as its region competence. The model presenting the highest competence level is selected to predict the label of inputs that fall in the same region.\nPairRanker and LLM-Blender Both methods were originally designed for text generation tasks (e.g., machine translation and speech recognition). PairRanker model (DeBERTa, 400M parameters) is trained to rank a pair of candidate predictions from two LLMs using multiple optimizing objectives (i.e., log probabilities, BART score, BERTScore, etc). A bubble"}, {"title": "4.4. Experimental Setup", "content": "Evaluation Metric. LLMs tend to generate contextual answers that lead to lower scores in the exact match (EM). Therefore, we mainly use the (per-answer) token-level F1-score from the official evaluation guidance of the datasets as the main evaluation metric for TQA performance. Our results differ from the ones reported in because we do not apply any post-processing, human evaluation or output constraints for the generated answers.\nSetup. We fixed the batch size to the upper limit of the server capacity, while the learning rates and epochs are selected after a grid search on a set of values (learning rates: {e3, 5e4, e4, 5e5, e5, 5e6, e6}, epochs: {3, 5, 10, 15, 20}). Models for TQA are trained for 5 epochs using a learning rate of 5 \u00d7 10-5 and batch size of 4. Models for VQA use the same learning rate but a batch size of 16 for 20 epochs. We spent ~74 and ~290 seconds training 1 epoch on 1,000 samples for TQA and 4,319 samples for VQA respectively. The training was performed on 1 GPU with 16GB memory of a DGX1 server ((Pascal) Tesla P100)."}, {"title": "4.5. Performance Comparison", "content": "In this section, we analyze the performance of our method, taking into account its distinctive characteristics as described in Table 1. Concretely, we focus on comparing our models in terms of task-specific performance, data efficiency, lightweight design, and multimodal capabilities.\nTask-specific Performance. Table 3 demonstrates the task-specific performance of InfoSel, base models and baselines on textual and visual QA datasets. For TQA, we observe that LLaMA underperforms other base models. Upon closer examination, we found that LLaMA generates longer explanation text which, although often accurate, decreases the EM and F1-score values. Conversely, a more consistent performance of base models is observed for VQA. All the models demonstrate superior performance on Mini-NQ compared to Mini-SDv2. This is because Mini-SDv2 contains 50% of unanswerable questions, written adversarially and specifically designed to be challenging for the QA task. Similarly, Mini-Viz contains 28% of unanswerable questions, and the label \"unanswerable\u201d"}, {"title": "4.6. Ablation Studies", "content": "Is InfoSel robust to the base models' individual performances? We carry out this study to assess whether InfoSel can effectively utilize the predictions obtained from various base models, regardless of their individual performance levels. In Figure 3, we observe a minor F1-score difference (0.07%) on the Mini-SDv2 dataset between the InfoSel model ensembled with and without the lowest performing base model (LLaMA). This finding suggests that InfoSel is robust, and not significantly affected by the individual model's performance. In a more detailed analysis, we observe that InfoSel selects 4% of answers from LLaMA, resulting in an overall gain of +0.28% of the F1-score. This observation highlights the effectiveness of InfoSel, as it can leverage the knowledge contained in the answers provided even by the lowest performing base model to some extent.\nWhich modality information helps the most for ensembling? In the Table 5, we compare the effect of providing different modality information to InfoSel-MT during ensemble training. Notice that even with just the question and answer (Q+A) information, our model surpasses the performance of the O-shot PairRanker and LLM-Blender. The setting that yields the lowest accuracy solely utilizes the image (V) as the signal. This can be explained by the fact"}, {"title": "4.7. Case Study", "content": "Table 6 illustrates two insightful cases from the predictions of different models on textual Mini-SDv2 and Mini NQ QA datasets. The first case showcases the ability of InfoSel-TT to select the right model (Davinci) when the rest of the models is incorrect. However, InfoSel*-TT selects the wrong answers from the FT-TT model and underperforms InfoSel-TT. The second case illustrates the ability of LLMs to generate correct answer (\u201c2006\u201d) despite the ground truth annotation error (\u201c2005\u201d). This demonstrates the advantage of ensembling highly expressive LLMs instead of relying only on fine-tuning small-size models such as FT-TT.\nThe first case of Table 7 further indicates that InfoSel captures the only correct answer (\u201ctoaster", "unanswerable\") introduced by FT-MT. InfoSel-MT struggles with such labels as they are unfamiliar to the base models. This showcases the benefits of training InfoSel* models on datasets containing a high percentage of task-specific labels.\"\n    },\n    {\n      \"title\"": "5. Conclusion"}, {"content": "In this paper, we propose InfoSel, a novel lightweight and task-specific ensemble method designed to learn the dynamic selection of the optimal model from a range of distinct black-box base LLMs. We find that using only 110M trainable parameters, our method is able to substantially increase the performance upon the best performing base LLM. Additionally, our analysis reveals that InfoSel remains robust regardless the incorrect predictions of the lowest performing LLM. Our findings also show that our solution is highly data-efficient. Concretely, it requires only a fraction of instances (as few as 10) from the training set to outperform base LLMs. Finally, our experimental results reveal the ability of InfoSel to be adapted to multimodal setting, showing a substantial increase in performance compared to state-of-the-art alternatives."}]}