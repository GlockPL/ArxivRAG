{"title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through Semantic Comprehension in Retrieval-Augmented Generation Scenarios", "authors": ["Hai Lin", "Shaoxiong Zhan", "Junyou Su", "Haitao Zheng", "Hui Wang"], "abstract": "In Retrieval-Augmented Generation (RAG) tasks using Large Language Models (LLMs), the quality of retrieved information is critical to the final output. This paper introduces the IRSC benchmark for evaluating the performance of embedding models in multilingual RAG tasks. The benchmark encompasses five retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval, keyword retrieval, and summary retrieval. Our research addresses the current lack of comprehensive testing and effective comparison methods for embedding models in RAG scenarios. We introduced new metrics: the Similarity of Semantic Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI), and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and 3) insights into the cross-lingual limitations of embedding models. The IRSC benchmark aims to enhance the understanding and development of accurate retrieval systems in RAG tasks. All code and datasets are available at: https://github.com/Jasaxion/IRSC_Benchmark", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in large language models (LLMs) have demonstrated significant potential in natural language understanding and generation. However, these models still face challenges like factual hallucination, knowledge updating, and lack of domain-specific expertise (Chen et al., 2024b). To address these issues, incorporating external knowledge through Retrieval-Augmented Generation (RAG) has emerged as a promising approach (Chen et al., 2024b; Zhang et al., 2023). RAG enhances LLMs by integrating retrieved information from external sources, which helps mitigate hallucinations and provide more accurate, up-to-date responses (Chen et al., 2024b). Despite these advantages, existing benchmarks for evaluating RAG models are limited in scope and do not fully address the diverse needs of various retrieval tasks (Chen et al., 2024b; Zhang et al., 2023). Most benchmarks focus primarily on tasks such as semantic textual similarity (STS), clustering, and re-ranking, but fail to provide RAG comprehensive evaluations across different retrieval scenarios.\nThe IRSC Benchmark introduced in this study aims to fill this gap by evaluating Embedding models across five distinct retrieval tasks: query-based retrieval, title-based retrieval, part-of-paragraph retrieval, keyword-based retrieval and summary-based retrieval. This benchmark is designed to reflect realistic application scenarios of RAG, considering different types of queries and languages (English, Chinese, and Mixed-Language datasets) (Zhang et al., 2023). We evaluate models such as Snowflake-Arctic-Embed-S (Merrick et al., 2024), BGE-M3 (Chen et al., 2024a), and (Wang Yuxin, 2023) across different tasks and languages, providing insights into their strengths and weaknesses in real-world RAG applications. Additionally, this benchmark includes innovative evaluation metrics to capture model performance differences across tasks and languages.\nAnd due to the differences in vector dimensions and values across various models, directly computing cosine similarity between vectors (Steck et al., 2024) is not feasible for comparing the semantic similarity between different models (Zhou et al., 2022). To address this, we propose the Similarity of Semantic Comprehension Index (SSCI) in this paper. SSCI measures the similarity of semantic understanding between the model's output and the ground truth.\nOur contributions are as follows: 1. We propose a comprehensive IRSC Benchmark to evaluate the performance of embedding models in RAG"}, {"title": "2 Related Work", "content": "The field of Retrieval-Augmented Generation (RAG) has gained significant attention, especially in addressing the limitations of Large Language models (LLMs) in providing accurate and contextually relevant information. This section reviews notable works in this domain and situates our contribution within the existing research.\nBenchmarking in RAG\nChen et al. developed the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs on four abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. Their findings highlight the need for nuanced evaluation metrics to improve RAG capabilities, as LLMs showed weaknesses in negative rejection, information integration, and handling false information(Chen et al., 2024b). However, RGB primarily focuses on robustness aspects and does not provide comprehensive coverage of different retrieval tasks, which is crucial for real-world RAG applications.\nMultilingual Retrieval Datasets\nThe MIRACL dataset, introduced by Zhang et al., supports multilingual information retrieval with 700,000 human-annotated query-passage pairs across 18 languages. It aims to advance retrieval models that handle linguistic diversity and resource variability (Zhang et al., 2023). While MIRACL provides valuable multilingual data, it is mainly focused on query-passage retrieval and does not address other important retrieval tasks like keyword or title retrieval.\nEvaluations of RAG Systems\nOgundepo et al. provided a comprehensive survey of current evaluation methods for RAG systems, emphasizing the importance of various retrieval tasks and metrics like nDCG, MRR, and MAP (Yu et al., 2024). Their work discusses challenges and future directions for robust RAG benchmarks. Despite their comprehensive survey, there is a lack of practical benchmarks that integrate these varied metrics across different retrieval tasks.\nBenchmark for Evaluation of Information Retrieval Models (BEIR)\nThakur et al. introduced an evaluation benchmark for retrieval models called BEIR, which includes a diverse set of information retrieval tasks across different domains and data types(Thakur et al., 2021). BEIR offers a collection of heterogeneous tasks and provides a unified and convenient framework for evaluating retrieval models based on natural language processing. However, BEIR only focuses on retrieval tasks between queries and paragraphs, and does not address the more complex retrieval tasks involving large-scale RAG (Retrieval-Augmented Generation) scenarios.\nMassive Text Embedding Benchmark (MTEB)\nMuennighoff et al. introduced MTEB, a benchmark evaluating text embedding models across tasks such as bitext mining, classification, clustering, reranking, retrieval, and semantic textual similarity(Muennighoff et al., 2023). Their findings highlight the need for specialized models tailored to specific retrieval scenarios. However, MTEB does not focus specifically on the integration of retrieved information for generation tasks, which is a critical component of RAG systems.\nMultilingual Question Answering\nThe MKQA dataset, presented by Longpre et al., evaluates multilingual open-domain question answering systems with parallel questions in multiple languages (Longpre et al., 2021). It facilitates comparative analysis of retrieval and QA performance across different linguistic contexts. While useful for question answering, MKQA does not encompass the broader spectrum of retrieval tasks that are essential for RAG evaluations.\nCurrent Work on RAG Model Evaluation\nOur work extends these studies by proposing a novel Benchmark that evaluates retrieval performance across five tasks: query, keyword, title, summary, and part of paragraph retrieval. Unlike previous benchmarks, our dataset includes multilingual and cross-lingual components, addressing the need for robust evaluation in diverse linguistic environments. Our benchmark aims to fill gaps in existing methods by providing a comprehensive assessment of model performance in RAG tasks, focusing on cross-lingual retrieval and integrating various retrieval tasks into a unified framework."}, {"title": "3 The IRSC Benchmark", "content": "3.1 Desiderata\nThe IRSC benchmark is designed to evaluate the effectiveness of embedding models specifically within the context of Retrieval-Augmented Generation (RAG) tasks. Unlike traditional benchmarks that focus broadly on sentence or paragraph length, IRSC hones in on the unique needs of RAG applications, which require supplementing knowledge to queries. This benchmark emphasizes five key data types to cover most RAG tasks:\n1. Focus on RAG-Specific Retrieval Tasks: Unlike traditional benchmarks, IRSC focuses on expanding a query or brief information into a detailed response.\n2. Emphasis on Cross-lingual Capabilities: IRSC evaluates models in multiple languages, particularly English and Chinese, to handle Mixed-Language queries and adapt to cross-lingual environments.\n3. Comprehensive Evaluation Metrics: Standard retrieval metrics (nDCG@10, MRR@10, MAP@10, precision@3, and recall@10) are used alongside new metrics like SSCI and RCCI for deeper insights into semantic comprehension and retrieval capabilities.\n4. Real-World Applicability: IRSC focuses on real-world RAG tasks like retrieving detailed knowledge based on a query or summary, ensuring practical relevance and cross-lingual applicability.\nThrough these considerations, IRSC aims to set a new standard for evaluating embedding models in the context of RAG tasks, providing a more nuanced and applicable assessment framework.\n3.2 Tasks and Evaluation\nFigure 1 provides an overview of tasks and datasets available in IRSC. The benchmark consists of the following five task types:\n1. Query -> Paragraph: Evaluates the model's ability to retrieve relevant paragraphs based on a given query. Datasets: MsMARCO(Bajaj et al., 2018), XQuAD(Artetxe et al., 2020), Xtreme(Hu et al., 2020), and MLQA(Lewis et al., 2020).\n2. Title -> Paragraph: Tests the model's capability to find relevant paragraphs given a title. Datasets: zhihu*, New-Title-Chinese\u2020, Arxiv-Abstract(Clement et al., 2019), Sci-Docs(Muennighoff et al., 2023), and Sci-Fact(Muennighoff et al., 2023).\n3. Part of Paragraph -> Paragraph: Evaluates sensitivity to text fragments, testing if the model can retrieve the full paragraph from a fragment. Datasets: nfcorpus(Muennighoff et al., 2023) (English) and xlsum(Joulin et al., 2017) (Chinese).\n4. Keyword -> Paragraph: Measures the model's ability to retrieve paragraphs based on keywords. Datasets: AG News(Zhang et al., 2015) and CorpusQTPKS.\n5. Summary -> Paragraph: Evaluates performance in retrieving relevant paragraphs based on a summary. Datasets: XSum(Narayan et al., 2018) (English), xlsum(Joulin et al., 2017) (Chinese), and CorpusQTPKS.\nEach task uses 5000 query-content pairs as evaluation data. The remaining samples form a unified"}, {"title": "3.3 Evaluation Metrics", "content": "The primary evaluation metrics for IRSC include nDCG@10, MRR@10, MAP@10, precision@3, and recall@10. These metrics are used to evaluate model performance in information retrieval tasks:\nRecall@10: Evaluates the fraction of relevant documents retrieved among the top 10 documents.\nMRR@10 (Mean Reciprocal Rank): Evaluates the rank position of the first relevant document.\nnDCG@10 (Normalized Discounted Cumulative Gain): Measures the ranking quality of the retrieved documents, taking into account the position of relevant documents in the ranking.\nIn addition to these standard metrics, we introduce new metrics to assess different aspects of model performance:\nSimilarity of Semantic Comprehension Index (SSCI)\nThe Similarity of Semantic Comprehension Index, averaged over multiple queries. It measures the difference in semantic understanding between the two models' outputs across all queries. A higher value indicates a greater disparity in the models' understanding of the given questions.\nWe define the average SSCI (SSCI) as:\nSSCI = $\\frac{1}{Q} \\sum_{q=1}^{Q} \\frac{|m1q - m2q|}{n}$\nRetrieval Capability Contest Index (RCCI)\nThe Retrieval Capability Contest Index, averaged over multiple queries. It evaluates the differences in retrieval capabilities between the two models across all queries. A positive average score indicates that model 1 performs better overall, while a negative average score indicates that model 2 performs better overall. The magnitude of the score indicates the extent of the difference in performance.\nWe define the average RCCI (RCCI) as:\nRCCI = $\\frac{1}{Q} \\sum_{q=1}^{Q} \\frac{m1q - m2q}{n}$\nParameters\n\u2022 n: The length of each query result vector minus one, representing the maximum index of the retrieval results.\n\u2022 R1, R2: These are matrices representing the results retrieved by the two different models over Q queries. Each matrix has dimensions Q \u00d7 (n + 1). Each element is a binary value (0 or 1), where 1 indicates the position of the correct answer for each query.\nFor query q, the vectors R1q and R2q are defined as follows:\nR1q = [r11q, r12q, r13q,\u00b7\u00b7\u00b7, r1nq, r1(n+1)q]\nR2q = [r21q, r22q, r23q, ..., r2nq, r2(n+1)q]\n\u2022 m1q, m2q: These represent the positions of the correct answer in R1q and R2q respectively for query q. If there is no 1 in the vector, it is assigned a value of -1.\nFor query q:\nm1q =\n$\\begin{cases}n - \\text{index}(R1q, 1) & \\text{if } 1 \\in R1q\\\\-1 & \\text{otherwise}\\end{cases}$\nm2q =\n$\\begin{cases}n - \\text{index}(R2q, 1) & \\text{if } 1 \\in R2q\\\\-1 & \\text{otherwise}\\end{cases}$\nwhere index (Rq, 1) denotes the index position of the element equal to 1 in the vector Rq.\nBy using these metrics, IRSC aims to provide a comprehensive evaluation framework for assessing the performance of embedding models across diverse retrieval tasks."}, {"title": "3.4 Model Descriptions", "content": "We evaluated 13 models using the IRSC Benchmark. Notably, MiniLM-L6-v2 models do not support Chinese.\n\u2022 S-Arctic Series (Merrick et al., 2024): Includes S-Arctic-S, S-Arctic-M, and S-Arctic-L, designed for semantic embeddings in text retrieval tasks.\n\u2022 BGE Series(Chen et al., 2024a): Includes BGE-M3 (multilingual) and BGE-Large (optimized for Chinese).\n\u2022 GTE Series(Li et al., 2023): Comprises GTE-Small, GTE-Base, and GTE-Large, focusing on general text embeddings.\n\u2022 M3E Series(Wang Yuxin, 2023): Includes M3E-Small, M3E-Base, and M3E-Large, designed for efficient multilingual text embeddings.\n\u2022 MiniLM Series: MiniLM-L12(Reimers and Gurevych, 2019): A multilingual version of the MiniLM series, tailored for paraphrase identification and multilingual retrieval. MiniLM-L6*: A compact, efficient model focused on English for various NLP tasks."}, {"title": "4 Results", "content": "4.1 Experimental Setup\nThe experiments are conducted across three different language requirements: English, Chinese, and Mixed-Language (English + Chinese). For each language requirement, corresponding benchmark datasets are utilized to perform IRSC scoring experiments.\nEnglish: The evaluation involves English-specific benchmark datasets to test the retrieval performance of each model.\nChinese: The evaluation uses Chinese-specific benchmark datasets, ensuring the models' capabilities are tested in the Chinese language context.\nMixed-Language (English + Chinese): This mixed evaluation assesses the models' performance across both English and Chinese datasets, providing a comprehensive understanding of their cross-lingual retrieval capabilities.\nBy employing this diversified language setup, we aim to provide a thorough and robust evaluation of each model's performance in retrieving relevant paragraphs based on various query types within the IRSC benchmark.\n4.2 Experimental Results and Analysis\n4.2.1 Benchmark Analysis\nBased on the results in Table 1, we observe that the BGE-M3 model consistently outperforms other models across all metrics and categories, indicating its robustness and effectiveness in both Chinese and English retrieval tasks. Specifically, BGE-M3 achieves the highest recall at 10 (r@10), mean average precision at 10 (m@10), and normalized discounted cumulative gain at 10 (n@10) in the Keywords, Title, Query, Part, and Summary categories. For instance, in the Keywords category, BGE-M3 has an impressive r@10 of 0.8668, m@10 of 0.8205, and n@10 of 0.8320.\nConversely, the S-Arctic series (S-Arctic-S, S-Arctic-M, S-Arctic-L) shows relatively lower performance compared to other models. Notably, S-Arctic-S performs better than S-Arctic-M and S-Arctic-L across most categories, but it still lags significantly behind models like BGE-M3, GTE-Small, and M3E-Base. For example, in the Summary category, S-Arctic-S achieves an r@10 of 0.5334, whereas BGE-M3 achieves an r@10 of 0.9812.\nModels like GTE-Base and M3E-Base also demonstrate strong performance, particularly in the Keywords and Summary categories. GTE-Base achieves an r@ 10 of 0.7940 in the Keywords category and M3E-Base achieves an r@10 of 0.9644 in the Summary category, showing their potential effectiveness in specific retrieval contexts.\nWe observe an interesting performance pattern between the S-Arctic-S and M3E-Small models. Notably, S-Arctic-S performs significantly better than M3E-Small in the Keywords category. S-Arctic-S achieves an r@10 of 0.6302, while M3E-Small scores 0.3374. However, in Summary categories, M3E-Small significantly outperforms S-Arctic-S. M3E-Small achieves an r@10 of 0.8052 compared to S-Arctic-S's 0.5334. This pattern indicates that while S-Arctic-S excels in Keywords retrieval, it falls behind in other tasks such as Summary, where M3E-Small demonstrates superior performance.\nThe diverse performance across different models highlights the importance of selecting the appropriate model based on the specific retrieval task and the language requirements. Table 1 presents the results for Mixed-Language task. The results for the Chinese and English tasks will publish in our Github repository.\n4.2.2 Radar Chart Analysis\nTo more intuitively showcase the performance of different models, we created radar charts2 where the values for each capability are derived from the average of r@10, m@10, and n@10. These charts provide a clearer view of the comprehensive performance of each model across various tasks."}, {"title": "4.2.3 Cross Language Analysis", "content": "In Table 2, we also conducted experiments on the cross-lingual retrieval capabilities of different models using five IRSC tasks, with 1,000 randomly selected queries for each task. We obtained 5,000 data entries in both English and Chinese languages. Queries originally in English were translated into the target language (Chinese) and then searched within an entirely English database to obtain the Chinese to English (C2E) results in Table 2. The scores are the averages of r@10, m@10, and n@10. From the results, several key observations can be made:\n1. Performance Decline in Cross-Lingual Retrieval: Most models exhibit a decline in performance metrics when transitioning from monolingual (C2C or E2E) to cross-lingual (C2E or E2C) retrieval. This indicates a general challenge in maintaining semantic alignment across different languages.\n2. Superior Performance of BGE-M3: The BGE-M3 model consistently demonstrates superior performance in both monolingual and cross-lingual retrieval tasks. Notably, its performance degradation from monolingual to cross-lingual retrieval is minimal. For instance, in C2C, it scores 0.8630, while in C2E, it scores 0.6260. Similarly, in E2E, it scores 0.8427, compared to 0.5964 in E2C.\n3. Significant Decline in M3E Series: The M3E series models show a significant decrease in performance when moving to cross-lingual tasks. The most notable drop is observed in the M3E-Base model, which falls from 0.8026 in C2C to 0.3323 in C2E. This highlights a substantial challenge in the model's ability to align queries semantically across languages.\n4. Drastic Decline in GTE Series: The GTE series models exhibit the most drastic decline in performance, especially in the E2C task. Scores around 0.85 in E2E drop below 0.1 in E2C, indicating a significant deficiency in the models' ability to handle cross-lingual semantic alignment from English to Chinese.\n5. Mixed Performance in S-Arctic and MiniLM Series: The S-Arctic series models display varying levels of performance, with S-Arctic-S and S-Arctic-M performing poorly in C2E tasks. The MiniLM series also shows mixed results, with MiniLM-L12 performing relatively well compared to MiniLM-L6 in cross-lingual tasks.\nThese findings underscore the need for further improvement in training vector models for cross-lingual query semantic alignment. Enhancing the models' ability to maintain semantic coherence across languages could lead to more effective and accurate cross-lingual retrieval systems. Future research should focus on developing techniques to bridge the semantic gap between languages, ensuring that models can perform consistently well in both monolingual and cross-lingual contexts."}, {"title": "4.3 SSCI & RCCI Analysis", "content": "4.3.1 SSCI\nIn Figure 3, we present detailed SSCI results for the Summary task across different languages and models. We observe that in the English language, most models display blue regions, indicating high consistency in semantic understanding among these models. In contrast, for the Chinese language, the SSCI values exhibit more red regions, suggesting lower consistency and greater divergence in semantic understanding among the models.\nFurthermore, by examining the color distribution in Figure 3, we find that models within the same series generally exhibit better semantic understanding consistency, whereas models from different series are more likely to show divergence in understanding. From this analysis, we can draw several conclusions: there is a significant difference in semantic understanding consistency across languages, with models showing higher consistency in English compared to Chinese; models within the same series tend to have higher semantic understanding consistency, while different series of models are more prone to divergences in understanding.\n4.3.2 RCCI\nThe figure 5 presents a comparative analysis of the capabilities of two models, S-Arctic-S and M3E-Small, across multiple languages and evaluation metrics. The RCCI methodology has been employed to offer a detailed comparison between the models, which overcomes the limitations of simple average-based metrics such asr@10, m@10, and n@10 by highlighting finer differences in model performance.\nFigure 5 illustrates the RCCI-based analysis clearly delineates the strengths and weaknesses of the two models. M3E-Small excels in the Chinese language, demonstrating robust performance across all metrics, which underscores its potential for applications requiring Chinese language proficiency. Conversely, S-Arctic-S exhibits a competitive edge in the English language metrics, particularly in the Q (Query), T (Title), and K (Keyword) metrics."}, {"title": "5 Conclusion", "content": "The IRSC Benchmark offers a comprehensive evaluation framework for embedding models in Retrieval-Augmented Generation (RAG) tasks. It includes five retrieval tasks: query-based, title-based, part-of-paragraph-based, keyword-based, and summary-based retrieval, in English and Chinese. Key contributions are the IRSC Benchmark, new metrics like the Similarity of Semantic Comprehension Index (SSCI) and Retrieval Capability Contest Index (RCCI), and a cross-lingual performance evaluation."}, {"title": "6 Limitations", "content": "While the IRSC Benchmark provides a comprehensive evaluation framework for embedding models in Retrieval-Augmented Generation (RAG) tasks, there are several limitations that need to be addressed:\n1. Language Scope: The benchmark primarily focuses on English and Chinese, which limits its applicability to other languages. While it provides insights into multilingual capabilities, extending the evaluation to a broader range of languages would offer a more holistic view of model performance in truly multilingual settings.\n2. Task Scope:: Although the benchmark covers five distinct retrieval tasks, real-world RAG applications might involve more complex and diverse scenarios. Expanding the range of tasks to include more specialized or domain-specific queries could provide a more comprehensive assessment.\n3. Model Variability: The benchmark evaluates a selection of popular embedding models, but it does not encompass all existing models. New models and variations are continuously being developed, and the benchmark needs to be updated regularly to include these advancements.\n4. Interoperability with Other Systems: The benchmark does not assess how well these embedding models integrate with other systems and technologies used in RAG pipelines. Evaluating interoperability and integration efficiency could provide a more practical measure of model utility.\nAddressing these limitations in future research will help improve the robustness and applicability of the IRSC Benchmark, making it a more powerful tool for evaluating and developing embedding models in RAG tasks."}]}