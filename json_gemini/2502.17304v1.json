{"title": "Child vs. machine language learning: Can the logical structure of human language unleash LLMs?", "authors": ["Uli Sauerland", "Celia Matthaei", "Felix Salfner"], "abstract": "We argue that human language learning proceeds in a manner that is different in nature from current approaches to training LLMs, predicting a difference in learning biases. We then present evidence from German plural formation by LLMs that confirm our hypothesis that even very powerful implementations produce results that miss aspects of the logic inherent to language that humans have no problem with. We conclude that attention to the different structures of human language and artificial neural networks is likely to be an avenue to improve LLM performance.", "sections": [{"title": "1 Introduction", "content": "Results from how humans learn language suggest that logical connections drive learning strongly. In particular, Sauerland et al. (2024) point out that children learning language are spontaneously creating the negation of an output. Their example is based on the German prepositions mit ('with') and ohne ('without'). Like their English translations the two are logical contraries, but unlike in English this is not morphologically transparent via the morphological complexity of with-out. The surprising observation is that children learning German spontaneously and frequently produce sequences mit ohne (lit. 'with with-out'), that are not fully grammatical and exceedingly rare in the adult language. This finding argues that after learning to output A in some circumstances, the negative output A 'not A' is readily available to the human learner and German children therefore are drive to produce the inverse of mit as two words, mit and a marker of negation. Other findings from child language acquisition (Cesana-Arlotti et al., 2018; Guasti et al., 2023) support this conclusion, and recent evidence shows that other primates also use negation in concept representation (Dautriche et al., 2022).\nIn contrast to the conceptual representation present in humans, neural networks rely heavily on independently inferring concepts from training data. LLM architecture does not have a bias for specific algebraic or logical relationships such as negation of A and \u0100. This predicts that learning logical negation of A will not be straightforward and we expect LLMs to not prefer learning logical connections over other generalizations. As an example, to model the logical operation of negation of output A produced by neuron a, a second neuron \u0101 could be added where the two neurons' weights are inversely linked, i.e., shared (w\u0101 = -Wa), as is depicted in Figure 1-b. Another approach would be to add a layer to the network by moving unit a to the penultimate layer and add units = and \u00ac that receive the output of a where \u00ac has an inverse activation function to yield A and A (see Figure 1-c). But both of these ways involve substantial changes to the network architecture, and we therefore predict that an LLM should require a lot of learning data to learn the complementary distribution of A and A. We conducted experiments that corroborate this prediction, and which are presented below.\nCurrent approaches rely on reinforcement learning with a set of mathematical and logical reasoning exercises, e.g. (DeepSeek-AI, 2025). This training leads to improved performance with tasks involving explicit logical reasoning. But the logical reasoning we refer to is implicit and detected by tasks such as the two-year-olds spontaneously recognizing the logical relationship between with and without mentioned above. Modeling logical relationships explicitly may help with this, but we suspect may not be sufficient. The collaboration of computer scientists, linguists and cognitive scientists will likely lead to ideas for the further improvement of LLMs."}, {"title": "2 A Prediction", "content": "The human drive towards capturing a logical relationship is not predicted to be present in LLMs with a strict layer structure as we argued above. Given enough input LLMs can learn the antonymic contrary relation between mit and ohne just as well as that between with and with-out. But we argue now that for different language properties, the relative difficulty of learning logical relationships yields sub-human performance. We focus on the formation of German nominal plurals. German plural formation is based on several noun classes determined by phonological and semantic criteria, but also a default. Using a default requires logical negation - if a noun fits none of the other classes, it belongs to the default class. We adopted a task from (Marcus et al., 1995) that involves the formation of plural forms for nonce nouns, i.e. hypothetical noun stems that don't actually exist in the language.\nThe formation of plural forms in German has a logical structure that involves a default form, namely the plural ending -s. To illustrate the concept of default (or elsewhere, Kiparsky 1973) in morphology consider English plurality, where -s is also the default form. A few nominal stems of English such as child and ox allow a different plural \u2013 child-ren and ox-en respectively \u2013 and with these forms the use of the default -s is ungrammatical: *child-s and *ox-s. The main difference between German and English is that irregular plurals only occur with a small finite number of stems in English, while in German the default plural only occurs rarely (see Figure 2).\nMorphological systems in language almost always involve defaults as was first observed by the Sanskrit grammarian P\u0101nini (Bobaljik and Sauerland 2018 and others). The logic behind a default involves negation: If a more specific form is available (e.g. child-ren) that default form cannot be applied (e.g. *child-s). Our heuristic in the previous section led us to conclude that human language learners should more readily learn generalizations involving negation than current LLMs. In English, an LLM is still expected to exhibit human-like performance because the overwhelming frequency of -s as a plural ending would allow an LLM to recognize the generalization. In German, on the other hand, the default -s plural occurs rarely as noted already above, both by lemma and token occurrence (Marcus et al., 1995). Therefore we expect an LLM to recognize some subgeneralizations in the more frequent classes. The default nature of the -s ending, however, depends on the recognition that the forms with the -s plural derive by negating the four other classes. This type of generalization would therefore be one humans are prone to notice, while for LLMs they are more difficult."}, {"title": "3 Testing the Prediction", "content": "To test the prediction, we repurposed the materials of (Marcus et al., 1995) into a test for current LLMs. They created 24 nonce nouns; i.e. nouns that are not actually words of German such as Bral, Klot and Fneik and collected data on how natural different plural forms such as Bral-e, Bral-en, Bral-er, and Bral-s feel to German native speakers. This task reflects the natural, frequent process of new nouns entering the language as a loan from a foreign language, as a product name or via another word-creation process and reflects part of the speakers knowledge of German. We set out to probe seven current LLM-based chatbots that we had access to on German plural formation. To test whether LLMs possess human-like knowledge, we queried each LLM for each of the 24 nonce nouns of (Marcus et al., 1995) with the German question Was w\u00e4re der Plural von XXX? (\u2018What would be the plural form of XXX?') where XXX would be replaced with the nonce noun. For each query, we started a new session with the chatbot. In addition, we posted at most one follow up question within the same session to the chatbot.\nWe extracted from the dialogues the plural forms the LLMs created from these answers. Then we assigned them a human plausibility rating in the following way: If the forms conformed to one of the five regular pluralization processes of German morphology, we assigned them the plausibility rating that (Marcus et al., 1995) reported from German speakers' judgments for this form. For entirely impossible plural forms, which (Marcus et al., 1995) did not test in their study, we assigned a plausibility of zero.\nThe method of data collection sometimes involved a follow-up question. Namely, if the chatbot seemed to treat our initial as involving a word from another language or the response related to an orthographically similar real German word, our follow-up was Ich meine das deutsche Wort XXX ('I mean the German word XXX'). The following transcript shows one sample session of this type:"}, {"title": "4 Conclusion", "content": "We conclude that the collaboration of computer scientists with linguists and cognitive scientists is likely to lead ideas for the improvement of LLMs. Our results resemble those of (Katzir, 2023) who also demonstrates that the linguistic abilities of LLMs exhibit predictable gaps. As mentioned above, (DeepSeek-AI, 2025) report that LLM-performance on reasoning tasks is substantially improved by applying reinforcement learning with a set of mathematical and logical reasoning exercises. This represents of promising start in our view and it might be fruitful to compile a more extensive set of exercises including linguistic ones for the training of LLMs we might call this a Curriculum for LLMs.\nAt the same time, there is evidence that logical thinking is available to humans without explicit instruction. One type of evidence comes from the fact that all languages including recently emerged languages provide the means to express logical quantification (Kocab et al., 2022). A second type of evidence comes from children's development. As mentioned above, some logical operations is found in prelinguistic infants and even negation seems present in the first year of life (Dautriche and Chemla, 2025). Finally, (Sauerland et al., 2025b) show that preschool children carry out higher order logical reasoning that requires a formal power exceeding first order logic. In sum, it remains to be seen whether the differences between human language and LLM generated language can be overcome by enhanced LLM topologies and training methods."}]}