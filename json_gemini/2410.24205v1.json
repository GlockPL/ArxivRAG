{"title": "Zonal RL-RRT: Integrated RL-RRT Path Planning with Collision\nProbability and Zone Connectivity", "authors": ["A.M. Tahmasbi", "M. Saleh Faghfoorian", "Saeed Khodaygan", "Aniket Bera"], "abstract": "Path planning in high-dimensional spaces poses\nsignificant challenges, particularly in achieving both time ef-\nficiency and a fair success rate. To address these issues, we\nintroduce a novel path-planning algorithm, Zonal RL-RRT,\nthat leverages kd-tree partitioning to segment the map into\nzones while addressing zone connectivity, ensuring seamless\ntransitions between zones. By breaking down the complex\nenvironment into multiple zones and using Q-learning as\nthe high-level decision-maker, our algorithm achieves a 3x\nimprovement in time efficiency compared to basic sampling\nmethods such as RRT and RRT* in forest-like maps. Our\napproach outperforms heuristic-guided methods like BIT* and\nInformed RRT* by 1.5x in terms of runtime while maintaining\nrobust and reliable success rates across 2D to 6D environments.\nCompared to learning-based methods like NeuralRRT* and\nMPNetSMP, as well as the heuristic RRT*J, our algorithm\ndemonstrates, on average, 1.5x better performance in the\nsame environments. We also evaluate the effectiveness of our\napproach through simulations of the UR10e arm manipulator\nin the MuJoCo environment. A key observation of our approach\nlies in its use of zone partitioning and Reinforcement Learning\n(RL) for adaptive high-level planning allowing the algorithm\nto accommodate flexible policies across diverse environments,\nmaking it a versatile tool for advanced path planning.", "sections": [{"title": "I. INTRODUCTION", "content": "Path planning involves computing a collision-free trajec-\ntory from a start configuration to a goal configuration within\nan environment containing obstacles. Numerous strategies\nhave been developed to efficiently handle complex and highly\nconstrained environments while remaining general enough to\nperform effectively across diverse settings and optimization\ncriteria, such as travel time, path length, or maximizing\nsafety. Current path-planning algorithms are broadly clas-\nsified into several categories. Graph-based algorithms model\nthe environment as a network, seeking the shortest path, such\nas Dijkstra's algorithm [45], A* [47], and D* [46]. Although\nthese methods guarantee an optimal path, their application\nin high-dimensional spaces incurs a significant increase in\nmemory demands and computational expense.\nThe Artificial Potential Field (APF) method [48] generates\nvirtual force fields to facilitate smooth navigational adjust-\nments for agents but is significantly limited by its tendency\nto become trapped in local minima. Emerging learning-based\nsolutions, including data-driven approaches that computa-\ntionally enhance traditional methodologies [51]\u2013[53], have\nbeen introduced. However, these techniques often struggle\nwith generalizability to unseen scenarios. Reinforcement\nLearning (RL) [19], characterized by environment-driven\nlearning, optimizes complex objectives with minimal over-\nsight but requires extensive training data for effective conver-\ngence in intricate settings [1]\u2013[3]. Sampling-based methods\nlike RRT [10], RRT* [54], and their derivatives are popular\nfor their efficacy in navigating high-dimensional, constrained\nspaces; nevertheless, their random sampling nature intro-\nduces suboptimality in path generation, with solution quality\ncontingent on the number of samples produced.\nDespite these challenges, the computational advantages of\nRRT are significant, leading to the development of numerous\nhybrid sampling algorithms. These enhancements fall into\ntwo main categories. First, heuristically guided search aug-\nments traditional graph-based methods with efficient sam-\npling techniques, such as A*-RRT [16], BIT* [13], Informed\nRRT* [57], and Theta*-RRT [55]. However, issues with\nmemory consumption and processing time persist in large\nand complex maps. Second, learning-enhanced approaches\n[24, 26, 49, 50], particularly those employing Reinforcement\nLearning (RL) [25, 26, 56], aim to enhance sampling effi-\nciency by determining sampling regions or facilitating cost-\naware sampling within the RRT framework. While these\nmethods show promising results, they often overlook the\nimportance of high-level environmental interpretation, in-\ncluding the overall configuration and spatial distribution of\nobstacles. Leveraging this understanding allows planning\nalgorithms to adapt their strategies to the specific charac-\nteristics of the environment, effectively handling complex\nobstacle configurations, optimizing path planning at a global\nlevel, and ultimately achieving more adaptable and efficient\nperformance across challenging scenarios.\nIn this paper, we introduce a novel approach that com-"}, {"title": "II. RELATED WORKS", "content": "A. Sampling-Based Planning Methods\nSampling-based methods leverage continuous spaces and\nrandomized characteristics for enhanced exploration capabil-\nities. At the forefront of these methods stands the Rapidly-\nexploring Random Tree (RRT), a foundational algorithm\nfrom which many other algorithms in this field are derived\n[10]. RRT efficiently navigates complex environments by\nrapidly exploring high-dimensional spaces, addressing alge-\nbraic constraints from obstacles [11]. Despite these advan-\ntages, basic sampling-based methods like RRT cannot con-\nsistently ensure optimal solutions, as the quality of outcomes\nsignificantly depends on the sampling strategy employed.\nSpecifically, reliance on uniform sampling may inadequately\nrepresent an environment's connectivity, frequently overlook-\ning essential narrow pathways due to its generalized, non-\ndiscriminative sampling distribution [12]. To address the\ninherent limitations of the basic RRT algorithm and enhance\nconvergence speed and memory usage, recent developments\nhave emerged, categorizable into distinct groups.\n1) Heuristically Guided Search: Heuristically guided\nsearches such as Theta*-RRT, A*-RRT, Informed RRT*, and\nBatch Informed Trees (BIT*) have enhanced the efficacy\nof path planning in complex environments. Theta*-RRT\nhierarchically merges any-angle search with RRT, guiding\ntree expansion toward strategically sampled states that en-\ncapsulate geometric path information [14]. A*-RRT and A*-\nRRT* refine this concept by sampling along A* paths, with\nthe latter distributing samples around the path to minimize\ncosts effectively [16]. BIT* unifies the graph-based search's\nordered nature with the scalability of sampling-based meth-\nods by interpreting a set of samples as an implicit random\ngeometric graph (RGG) [15]. Additionally, RRT*J introduces\na novel non-uniform sampling technique that leverages a\ngeneralized Voronoi graph (GVG) and discretizes the heuris-\ntic path to construct multiple potential functions (MPF),\nwhich further guide exploration toward optimal solutions\n[60]. Furthermore, advancements such as those in [17,18]\naugment RRT with Artificial Potential Fields, steering ran-\ndom samples toward the goal through a biased approach.\nAlthough these approaches improve planning by directing\nexploration towards areas identified as promising through\npredefined heuristics, this focused approach can unintention-\nally restrict the exploration breadth, particularly in settings\nwith intricate constraints or clutter, where the heuristic might\nnot accurately reflect spatial complexities. Moreover, the\nefficiency of these methods tends to diminish with increasing\nmap sizes or state space dimensionality, especially when they\nrely on detailed environmental information or precomputed\ndata structures, which can be computationally expensive.\n2) Learning-Enhanced Sampling: Enhanced Sampling\nthrough Learning incorporates methods that use Deep Learn-\ning, Reinforcement Learning, or Neural Networks [26,27] to\nrefine the sampling process, thereby increasing the efficiency\nof path-planning algorithms. In [22], they have developed a\nmethod that employs reinforcement learning techniques to\nenhance the sampling strategy within a discretized workspace\nenvironment. In [23], a technique is introduced that utilizes\nconditional variational autoencoders (CVAEs) to create a\nspecific sampling distribution optimized for motion plan-\nning applications. In [24], the authors introduced MPNet,\nwhich leverages Neural Networks to derive near-optimal path\nplanning heuristics from environmental data and robot con-\nfigurations, enabling bidirectional path generation. In [26],\nNRRT* has been proposed, utilizing a CNN model to predict\nthe probability distribution of the optimal path. This model,\ntrained on data generated by the A* algorithm, guides the\nsampling process to enhance path planning efficiency. While\nthese learning-enhanced sampling methodologies exhibit im-\nproved performance compared to classical approaches, they\nare confronted with significant challenges. Learning-based\nmethods trained on heuristic-guided algorithms (such as A*)\noften inherit the limitations of these heuristics, impeding\ngeneralization to new, unseen environments. This reliance re-\nduces their flexibility to adapt dynamically when confronted\nwith novel obstacles or complex environmental features.\nB. Reinforcement Learning\nReinforcement Learning (RL) is an essential component\nof machine learning for path planning applications, where\nagents learn a policy \\(\\pi\\)mapping states S to actions A to\nmaximize accumulated rewards. Agents gain rewards or incur\npenalties for each action, predicated on a reward function"}, {"title": "III. OUR APPROACH", "content": "In this section, we will give an overview of our method-\nology, followed by technical details.\nA. Formulations\nThe path planning process begins with a predefined\nAgent's start point, goal point, and the positions of obstacles\nwithin the environment space E.\n\\(A_{start} = (x_{start}^1,...,x_{start}^n), A_{goal} = (x_{goal}^1,...,x_{goal}^n)\\)  (1)\nwhere n represents the number of dimensions in the\nenvironment. Obstacles within the environment can take\nvarious shapes, such as circles, rectangles, spheres or boxes,\ndepending on the dimensionality of the space. The set of\nobstacles \\(O_i\\), where i = 1,..., N, can be represented as:\n\\(O_i = \\{p \\in R^n | f(p, C_i, P_i) < 0\\}\\) (2)\nwhere \\(C_i\\) denotes the position of the ith obstacle's centroid,\n\\(P_i\\) represents the parameters defining the shape and size of\nthe obstacle (such as radius for circles and spheres, or side\nlengths for rectangles and boxes), and \\(f (p, C_i, P_i)\\) is a func-\ntion that describes the boundary condition of the obstacle,\ndepending on its shape and dimensionality. This generalized\nformulation allows the framework to be applied across 2D\nand 3D environments with various obstacle geometries. Let\nthe environment E be partitioned into k unique zones, with\neach zone distinguished by specific constraints and attributes.\nThe set of all zones is denoted by Z, where:\n\\(E = \\bigcup_{i=1}^{k} Z_i\\) with \\(Z_i \\cap Z_j = \\emptyset\\), \\(\\forall i \\neq j, i, j \\in \\{1, ..., k\\}\\) (3)\nB. Overview\nThe goal is to define an optimal sequence of zones \\(Z^* = \\{Z_{start}, Z_{i_1},..., Z_{i_m}, Z_{goal}\\}\\), where \\(A_{start}\\) is in \\(Z_{start}\\) and \\(A_{goal}\\) is\nin \\(Z_{goal}\\). The intermediate zones are determined using a\nQ-learning framework, and RRT is subsequently employed\nas a local planner to generate the path within these zones.\nC. Methodology\n1) Reward Function: The rewards for zones are influ-\nenced by various parameters reflecting their characteristics,\nwhich depend on the specifics of the scenario. We suggest a\nset of fundamental criteria that should be considered in all\nscenarios. [1]-[5]\na) Collision Probability (Rp): The parameter Rp, cap-\ntures the obstacle density within a zone, indicating collision\nlikelihood\u2500higher values suggest increased risk, defined as:\n\\(R_p \\propto \\frac{A_{obstacles}}{A_{zone}}\\) (4)\nThis metric effectively gauges the probability of collision,\nwith denser areas posing a higher risk to the agent.\nb) Distance to goal(Ra): This reward component,\nwhich accounts for distance, is structured to reflect the\nproximity of a zone's center to the target. This encourages\nstrategies that favor shorter, more direct trajectories across\nfewer zones.\n\\(R_d \\propto - || Z_{center} - A_{goal} ||\\) (5)\nNow, by considering all parameters, the final reward\nfunction is enhanced to:\n\\(R = w_1 . R_d + w_2 . R_p + w_3 \\times I[goal\\_reached]\\) (6)\nwhere \\(w_1,w_2,w_3\\) are weighting factors for the respective\ncomponents of the reward function, and the indicator func-\ntion \\(I[goal\\_reached]\\) takes a value of 1 when the goal is\nreached, contributing positively to the reward, and 0 oth-\nerwise.\n2) kd-tree Algorithm Formulation: The kd-tree algorithm\npartitions the environment space E into zones dynamically,\nbased on the distribution and density of obstacles. This\npartitioning is mathematically represented as:\n\\( \\begin{aligned} & \\text{Partition}(E, \\text{depth}) = \\\\ & \\begin{cases} \\\\ & \\text{Split}(E, \\text{median}(C_{\\text{axis}}), \\text{axis}) & \\text{if depth} < \\text{MaxDepth} \\\\ & E & \\text{otherwise} \\\\ \\end{cases} \\end{aligned} \\) (7)"}, {"title": "IV. EVALUATION", "content": "This section presents a comprehensive evaluation of our\nproposed algorithm, demonstrating its performance across\ndiverse environments and algorithmic configurations, such\nas varying kd-tree depths, and providing comparisons against\nestablished path-planning algorithms in 2D, 3D, and even-"}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "In this work, we introduced Zonal RL-RRT, a novel\npath-planning algorithm designed for efficiently navigating\nhigh-dimensional environments. Evaluations from 2D to 6D\ndemonstrated that the algorithm consistently generates fea-\nsible paths with high success rates and competitive running\ntimes compared to baseline approaches. Future work could\nexplore extending the algorithm to dynamic environments\nand optimizing reward function parameters for specific mis-\nsion objectives. Additionally, expanding the approach to\nmulti-agent systems could unlock the algorithm's high-level\nplanning potential, enabling effective collaboration and co-\nordination among agents in shared environments, which is\ncrucial for complex, cooperative tasks in scenarios such as\nlarge-scale autonomous vehicle fleets."}]}