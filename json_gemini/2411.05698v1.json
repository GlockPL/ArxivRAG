{"title": "Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification", "authors": ["Antonio De Santis", "Riccardo Campi", "Matteo Bianchi", "Marco Brambilla"], "abstract": "Convolutional Neural Networks (CNNs) have seen significant performance improvements in recent years. However, due to their size and complexity, they function as black-boxes, leading to transparency concerns. State-of-the-art saliency methods generate local explanations that highlight the area in the input image where a class is identified but cannot explain how a concept of interest contributes to the prediction, which is essential for bias mitigation. On the other hand, concept-based methods, such as TCAV (Testing with Concept Activation Vectors), provide insights into how sensitive is the network to a concept, but cannot compute its attribution in a specific prediction nor show its location within the input image. This paper introduces a novel post-hoc explainability framework, Visual-TCAV, which aims to bridge the gap between these methods by providing both local and global explanations for CNN-based image classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to generate saliency maps that show where concepts are recognized by the network. Moreover, it can estimate the attribution of these concepts to the output of any class using a generalization of Integrated Gradients. This framework is evaluated on popular CNN architectures, with its validity further confirmed via experiments where ground truth for explanations is known, and a comparison with TCAV. Our code will be made available soon.", "sections": [{"title": "1. Introduction", "content": "As the performance of deep learning models has grown significantly over recent years, their complexity has also increased, making it difficult for users to understand how and why decisions are made. This led to the widespread use of the term black-box to describe these models, as only their inputs and outputs are known, while their internal mechanisms are too complex for humans to comprehend. This results in a lack of algorithmic transparency [42] and reduced trust in AI-based systems [17], particularly in critical fields such as healthcare or autonomous driving in which image classification systems are becoming increasingly employed [6, 41]. Additionally, debugging black-box models for bias mitigation [33] becomes challenging without comprehending the process they use to make predictions. To this end, the field of Explainable Artificial Intelligence (XAI) has made significant progress in developing techniques for explaining AI decisions. However, determining whether a certain human-understandable concept is recognized by the network and how it influences the prediction remains a significant challenge. Widely used approaches use saliency maps to localize where a class is identified in an input image, but they can't explain which high-level features led the model to its prediction. For instance, these methods cannot determine whether a golf ball was recognized by the spherical shape, the dimples, or some other feature. To address this, Kim et al. [16] introduced TCAV (Testing with Concept Activation Vectors), a method that can discern whether a user-defined concept (e.g., dimples, spherical) correlates positively with the output of a selected class. However, TCAV only provides global explainability and therefore cannot measure the influence of a concept in a specific prediction or show the locations within the input images where the networks recognize these concepts.\nIn this article, we introduce a novel explainability framework, namely Visual-TCAV, which integrates the core principles of saliency methods and concept-based approaches while aiming to overcome their respective limitations. Visual-TCAV can be applied to any layer of a CNN model whose output is a set of feature maps. Its main contributions are: (i) it provides visual explanations that show where the network identifies concepts of interest; (ii) it can estimate the importance of these concepts to the output of any class; (iii) it can be used for both local and global explainability."}, {"title": "2. Related Work", "content": "The field of XAI has seen rapid growth in recent years. For CNN-based image classification, state-of-the-art methods primarily provide explanations via saliency maps that highlight the most important regions in the input image for predicting a certain class. An approach for generating such visualizations involves studying the input-output relationship of the model by creating a set of perturbed versions of the input and analyzing how the output changes with each perturbation. Notable contributions to this approach include Local Interpretable Model-Agnostic Explanations (LIME) [29], which uses random perturbations, and SHapley Additive exPlanations (SHAP) [20], which estimates the importance of each pixel using Shapley values. A different approach that instead tries to access the internal workings of the model was originally proposed by Simonyan et al. [36] and consists of generating saliency maps based on the gradients of the model output w.r.t. the input images. This idea led many researchers [37, 38] to investigate how to exploit gradients to produce more accurate saliency maps. Among them, Selvaraju et al. [32] proposed Gradient-weighted Class Activation Mapping (Grad-CAM), which extracts the gradients of the logits (i.e., raw pre-softmax predictions) w.r.t. the feature maps. It then uses a Global Average Pooling (GAP) operation to transform these gradients into class-specific weights for each feature map and performs a weighted sum of these feature maps to produce a class localization map. Grad-CAM has gained considerable attention and is extensively used to explain convolutional networks. However, Sundararajan et al. [39] demonstrated that gradients can saturate, leading to an inaccurate assessment of feature importance. To address this issue, they introduced Integrated Gradients (IG), a method that calculates feature attribution by integrating the gradients along a path from a baseline (e.g., a black image) to the actual input image. A notable contribution of IG and its variants [15, 22, 27, 43] include the ability to provide fine-grained saliency maps with a strong theoretical foundation.\nWhile saliency methods are widely used for image classification, they might not always provide a complete picture of why a model made a certain decision. This is because these methods perform class localization, but cannot explain how the model recognized the highlighted class. Furthermore, these techniques rely on per-pixel importance which can't be generalized across multiple instances, as the position of these pixels is only meaningful for a specific input image. Consequently, they can only explain one image at a time, preventing them from providing global explanations. To overcome these limitations, Kim et al. [16] proposed TCAV, a method that investigates the correlations between user-defined concepts and the network's predictions using a set of example images representing a concept. For instance, images of stripes can be used to determine whether the network is sensitive to the \"striped\" concept for predicting the \"zebra\" class. This is accomplished by calculating a Concept Activation Vector (CAV), which is a vector orthogonal to the decision boundary of a linear classifier, typically"}, {"title": "3. Visual-TCAV", "content": "This section presents Visual-TCAV, whose methodology is outlined in Fig. 1. Local explanations can be generated for any layer and consist of two key components: (i) the concept map, a saliency map that serves as a visual representation of where the network recognized the selected concept in the input image and (ii) the concept attribution, a numerical value that estimates the importance of the concept for a selected class. To derive global explanations, the process is replicated across multiple input images and the concept attributions are averaged to quantify how the concept influences the network's decisions across a wide range of inputs."}, {"title": "3.1. CAV Generation and Spatial Pooling", "content": "Similarly to the original TCAV framework, the initial step of our method consists of computing a Concept Activa-"}, {"title": "3.2. Concept Map", "content": "Using the pooled-CAV, we derive a concept map ($M^c$) that locates a concept (c) within the input image by applying Equation (1). This involves performing a weighted sum of each feature map (k) in the considered layer ($fmaps_k$), with the weights being the pooled-CAV values ($p_k$). Then, we apply a ReLU function to consider only the regions of the image that positively correlate with the concept. This process is similar to Grad-CAM's equation, with the distinction that we use the pooled-CAV values as weights instead of the global-average-pooled gradients.\n$M^c = ReLU(\\sum_k p_k \\cdot fmaps_k)$ (1)\nSince we would like to compare the degree of activation of concept maps across different input images and distinguish between active and inactive ones, we apply a min-max normalization to the concept map using a predefined range. The upper bound of this range is derived from the example images provided by the user, which represent the ideal concept. Specifically, it is obtained by computing a concept map for each example image and taking the median of their contraharmonic means, which serve to extract values of high activation within each concept map. To obtain the lower bound, we apply the same method but use the negative examples instead. Before normalizing the concept map, any values outside this predefined range are clipped, ensuring that the resulting concept map is scaled within [0,1].\nBy overlaying the normalized concept map on the input image, we generate a class-independent visualization that highlights the region of the image where the network recognized the concept (examples are shown in Fig. 2). This allows us to know, for any input image, the concept's location and its degree of activation w.r.t an ideal concept defined by the user. Additionally, the concept map provides a qualitative validation for the learned CAV, without requiring activation maximization techniques [25] or sorting images based on their similarity to the CAV."}, {"title": "3.3. Concept Attribution", "content": "Our aim is to gain insights into the network's decision-making process by estimating the importance of any concept of interest toward the predictions. To this end, given an input image, a layer, and a target class, we compute the attribution of each neuron's activation toward the output of that class and then use the pooled-CAV to approximate which activations are imputable to the concept and sum these attributions. Since concepts may overlap or be hierarchically related, their attributions are not assumed to be additive.\nThese attributions can be computed through a generalized variant of the IG approach, which computes the integrated gradients of a class's logit (i.e., raw pre-softmax prediction) w.r.t. the feature maps instead of the input image. Specifically, we compute the gradients along a straight-line path from zero-filled matrices to the actual feature maps and then approximate the integral using the Riemann trapezoidal rule. Since IG satisfies the completeness axiom, the attributions sum to the difference between the target class's logit and the baseline's logit for that class, regardless of which layer is used as input. Then, for multiclass networks, we apply a ReLU to retain only positive attributions. For binary class networks, we sum the positive attributions to derive the score for the positive class, and the negative ones to obtain the score for the negative class. Since the scale of these attributions is the same as the raw logits, making interpretation difficult, we normalize the attributions so that their sum is at most one. Specifically, we apply a min-max normalization to the difference between the logit of each class and the logit of the baseline for that class. The attributions for each class are then scaled so that their sum equals this normalized difference.\nTo estimate the attribution of a concept (c) for a target class (t), we utilize the pooled-CAV to perform a weighted sum of the normalized attributions toward that target class ($IG^{t,norm}$). Before this summation, we apply a ReLU and min-max normalization to the pooled-CAV to gradually extract less attribution for feature maps that are less correlated with the concept. The rationale behind using the ReLU is to consider only attributions of feature maps that are activated by the concepts. We refer to the normalized pooled-CAV values as $p^{c,norm}$. Finally, as shown in Equation (2), we obtain the concept attribution ($Attr_{c,t}$) for a concept (c) and a target class (t) by summing all values, indexed by i, j, of\n$Attr_{c,t}=\\sum_{i,j} M^{c,norm}_{ij} \\cdot (\\sum_k p^{c,norm}_k \\cdot IG^{t,norm}_{k_{ij}})$ (2)\nThe concept attribution is a per-concept metric, meaning that two concepts can have significantly different attributions even if they are recognized in the same location of the input image, resulting in similar concept maps. This distinction is achieved by focusing not on per-pixel attributions but on the attributions of the activations produced by the neurons responsible for recognizing the concepts. Moreover, we can average the concept attribution across multiple input images to measure the overall importance of a concept for a selected class, thus providing a global explanation."}, {"title": "4. Results", "content": "This section presents the results of applying Visual-TCAV to state-of-the-art convolutional networks. We demonstrate that our method (i) can reveal biases contributing to mispredictions, (ii) identifies where concepts are detected within the network, and (iii) is faithful to ground truth."}, {"title": "4.1. Experimental Setup", "content": "Our experiments are conducted on ResNet50V2 [14], InceptionV3 [40], and VGG16 [35], all pre-trained on the ImageNet [9] dataset, as well as a ResNet50V2 model trained from scratch on the CelebA [18] dataset for celebrity gender classification. Regarding the concepts tested, they were sourced from the Describable Textures Dataset (DTD) [7], obtained from popular image search engines, or generated through Stable Diffusion [30] (more details on this in the supplementary material). For concepts obtained via a search engine, we collect 30 example images per concept, similarly to the original TCAV paper. For DTD concepts, we use all 120 available images, and for generated concepts, we use 200 images. For ImageNet negative examples, we follow the approach recommended by Martin and Weller [21], selecting 500 random images. In the case of CelebA, images containing the concept are used as positive examples, while those without it serve as negative examples. Regarding the computation of the integrated gradients, we consistently used 300 steps, which are enough to approximate the integral within a 5% error margin [39].\nWe conducted the experiments on an Intel i7 13700k with an Nvidia RTX 4060Ti 16GB and 32 GB of RAM. The software runs on TensorFlow 2.15.1, CUDA 12.2, and Python 3.11.5. With this setup, local explanations for seven"}, {"title": "4.2. Local Explanations", "content": "While concept maps are class-independent, the attribution of each concept depends on the class considered. In our examples, we examine the top three predicted classes and apply Visual-TCAV to a subset of the CNN layers. The rationale behind these layer-wise explanations is that they enable us to visualize where specific concepts are learned within the network. For instance, the \u201ccar\u201d concept in Fig. 3 is not well recognized in earlier layers, corroborating previous findings that higher-level features are typically extracted deeper into the network [3, 4, 26, 44]. On the other hand, concept maps in earlier layers are more fine-grained (see the \"pews\" concept in Fig. 3) due to their neurons having smaller receptive fields. Additionally, we observe a substantial increase in attributions in deeper layers, which is aligned with other studies [2, 24] showing deeper neurons exhibit higher class selectivity due to their proximity to the output. However, this is slightly less pronounced on the CelebA dataset (refer to Fig. 4), where the \"lipstick\" and \"beard\" concepts exhibit class selectivity even in earlier layers, which may be due to the lower task complexity.\nThe utility of these explanations lies mainly in their ability to show not only which image regions the network is focusing on, but also what the model recognizes in those regions and how much this contributes to each class's output. This can be especially useful for revealing the model's reasoning behind mispredictions. For instance, the third image in Fig. 3 is an \u201cox\u201d wrongly classified as \u201cdalmatian\" for which we observe that the network's decision is largely influenced by the \"spotted\" concept, which accounts for more than half the logit value of the \"dalmatian\" class."}, {"title": "4.3. Global Explanations", "content": "The concept attribution is a per-concept metric of importance, hence we can derive global explanations by aggregating this attribution across multiple input images of a se-"}, {"title": "4.4. Quantitative Evaluation with Ground Truth", "content": "We conduct a validation experiment to evaluate the faithfulness of Visual-TCAV. In this experiment, we train convolutional networks in a controlled setting, where ground truth is known, and assess whether the Visual-TCAV attributions match this ground truth. For this purpose, we create a dataset of three classes - cucumber, taxi, and zebra - which are the same classes used in the TCAV validation experiment. We then create multiple versions of this dataset by altering a percentage of the images with a tag, represented by a letter enclosed in a randomly sized square, added in a random location of the image (examples are shown in Fig. 6a). Specifically, zebra images are tagged with a \"Z\" in a purple square, taxi images with a \"T\" in a magenta square, and cucumber images with a \"C\" in a cyan square. From these tagged images, we create five datasets: one of images without tags, and four others with 25%, 50%, 75%, and 100% of tagged images respectively. Each dataset is then used to train a different model, each including six convolutional layers and a GAP layer. Depending on the dataset used for training, each model may learn to recognize either the entities (i.e., cucumbers, taxis, and zebras), the tags, or both and will decide which ones to give more importance. To obtain an approximated ground truth assessing which concept - entity or tag - is more important, we ask the models to classify a set of 200 incorrectly tagged test images per class. In this test set, taxis are tagged with the \"Z\", cucumbers are tagged with the \"T\" and zebras are tagged with the \u201cC\u201d. If the network correctly classifies most of the images, it indicates that the entity is more im-"}, {"title": "4.4.1 Comparative Analysis with TCAV", "content": "The primary difference between our concept attribution and the TCAV score is that our method considers the magnitude of the gradients, not just their direction. This allows us to measure the concept's impact on the predictions, beyond just the network's sensitivity to it. To demonstrate this, we compute the TCAV scores for tags and entities across each validation model (see Fig. 7). On one hand, TCAV scores match the ground truth showing that the network trained without tags exhibits high sensitivity to the entities and no"}, {"title": "5. Conclusion", "content": "In this article, we presented Visual-TCAV, a novel method that provides both local and global explanations for image classification models by estimating the attribution of high-level concepts to the network's predictions. Visual-TCAV also generates saliency maps to show where concepts are identified by the network, thereby assuring the user that the attributions correspond to the intended concepts. The effectiveness of this method was demonstrated across a range of widely used CNNs and further validated in a ground truth experiment, where it successfully identified the most important concept in each examined model."}, {"title": "5.1. Limitations and Future Work", "content": "The main limitation of the current approach is the effort required to collect example images, which may discourage adoption. To address this, in future work we plan to explore generating explanations directly from text using methods such as Text-To-Concept [23], which align the activations of multi-modal models like CLIP [28] with vision-only models. Additionally, generative approaches, such as DreamBooth [31], could be investigated to generate a large number of concept images from a small set of examples.\nAnother limitation of the current implementation is that, for multiclass networks, it only considers positive attributions, therefore it does not explain whether a concept can negatively affect a class. Therefore, in future implementations, we aim to also include negative concept attributions.\nFinally, future work could explore adapting our method to other tasks like regression as well as other architectures like Vision Transformers (ViTs) [10]. It may also be interesting to study interconnections between concepts to determine not only the concept attribution toward a class but also toward other concepts in deeper layers."}, {"title": "Contribution Statement", "content": "Antonio De Santis participated in designing and conducting research and validation experiments, writing the code, analyzing data, and writing the paper. Riccardo Campi contributed by performing research and validation experiments, analyzing data, and writing the code and the paper. Matteo Bianchi assisted with the research design and edited the paper. Marco Brambilla was responsible for project administration and supervision."}]}