{"title": "PAGE: Parametric Generative Explainer for Graph\nNeural Network", "authors": ["Yang Qiu", "Wei Liu", "Jun Wang", "Ruixuan Li"], "abstract": "This article introduces PAGE, a parameterized generative\ninterpretive framework. PAGE is capable of providing faithful ex-\nplanations for any graph neural network without necessitating prior\nknowledge or internal details. Specifically, we train the autoencoder\nto generate explanatory substructures by designing appropriate train-\ning strategy. Due to the dimensionality reduction of features in the\nlatent space of the autoencoder, it becomes easier to extract causal\nfeatures leading to the model's output, which can be easily employed\nto generate explanations. To accomplish this, we introduce an ad-\nditional discriminator to capture the causality between latent causal\nfeatures and the model's output. By designing appropriate optimiza-\ntion objectives, the well-trained discriminator can be employed to\nconstrain the encoder in generating enhanced causal features. Finally,\nthese features are mapped to substructures of the input graph through\nthe decoder to serve as explanations. Compared to existing methods,\nPAGE operates at the sample scale rather than nodes or edges, elim-\ninating the need for perturbation or encoding processes as seen in\nprevious methods. Experimental results on both artificially synthe-\nsized and real-world datasets demonstrate that our approach not only\nexhibits the highest faithfulness and accuracy but also significantly\noutperforms baseline models in terms of efficiency.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have consistently demonstrated\npromising results across a wide variety of tasks [24, 25, 32]. As\nGNNs find applications in crucial domains where trustworthy AI is\nimperative, the need for their interpretability has become increas-\ningly paramount [29]. GNNExplainer is the first general model-\nagnostic approach for interpreting GNNs, and searches for soft\nmasks for edges and node features to explain the predictions via\nmask optimization [26]. Since GNNExplainer largely focuses on pro-\nviding the local interpretability by generating a painstakingly cus-\ntomized explanation for a single instance individually and indepen-\ndently, it is not sufficient to provide a global understanding of the\ntrained model [14]. Furthermore, GNNExplainer has to be retrained\nfor every single explanation. So, in real-world scenarios, GNNEX-\nplainer would be time-consuming and impractical. To address the\nabove issues, PGExplainer was proposed to learn succinct underly-\ning structures as the explanations from the observed graph data [14].\nIt also models the underlying structure as edge distributions, where\nthe explanatory subgraph is sampled using binary hard masks for\nedges. PGExplainer utilizes a deep neural network to parameterize\nthe explanation selection process, providing the ability to collectively\nexplain multiple instances, so it has better generalization ability and\ncan be utilized in an inductive setting easily [14]. Since there is no\nground truth for explanatory subgraphs, PGExplainer has to employ\nreinforcement learning or Gumbel-Softmax sampling to search for\ninformative explanatory subgraphs within a vast space that grows\nexponentially with the number of edges. Consequently, the computa-\ntional complexity remains relatively high [12].\nInstead of using edge masks, recent methods have attempted to\nproduce the adjacency matrix of explanatory subgraphs directly\nthrough a Variational Graph Auto-encoder (VGAE) [9], eliminating\nthe need to consider each node or edge individually and achieving\ngreater efficiency. GEM is the pioneering work in this domain. It in-\ntroduced a distillation process grounded in the concept of Granger\ncausality [4] to generate ground-truth explanatory subgraphs for the\ntraining of VGAE [12]. However, the distillation process intrinsically\nassumes independence between the edges. This could be problematic\nas graph-structured data is inherently interdependent [13]. Differing\nfrom GEM, which quantifies the causal influence from edges, Or-\nphicX [13] suggests identifying the underlying causal factors from\nthe latent space, allowing it to bypass direct interaction with intri-\ncately interdependent edges. It divides the latent representation pro-\nduced by the encoder into causal features and spurious features and\nidentifies the underlying causal features by harnessing information\nflow measurements [1], quantifying the causal information emanat-\ning from the latent features. It then constructs the adjacency matrix\nfor the explanatory subgraph based on these causal features [13].\nThe fundamental architecture of Gem and OrphicX is depicted in\nFigure 1. Gem, while illustrating the viability of training VGAE as a\ngraph interpreter, is subject to certain constraints. Gem's methodol-\nogy involves a distillation process aimed at generating ground-truth\nsubgraphs for training the autoencoder. This process perturbs the in-\nput graph at the edge level and assesses the importance of each edge\nbased on the resulting reduction in model error. However, this ap-\nproach assumes edge independence, failing to adequately explore\nthe ability to discern causal features within the autoencoder's latent\nspace. OrphicX endeavors to utilize information flow estimation to\nevaluate the causal relationships between each feature dimension in\nthe latent space and the model predictions. This strategy aims to by-\npass perturbations from input dimensions. Nonetheless, due to com-\nputational constraints and complexity considerations, OrphicX oper-\nates within a severely restricted sampling range for each sample. For\ninstance, regardless of the size of a single input graph, OrphicX sam-"}, {"title": "2 Related Work", "content": "Post-hoc GNN Explanation aims to produce an explanation for a\nGNN prediction on a given graph, usually as a substructure of\nthe graph. Various explaining approaches focus on different as-\npects of the model and also provide different views. Many sur-"}, {"title": "3 Methodology", "content": "3.1 Preliminary\n3.1.1 Graph neural network\nThe work of graph neural network mainly depends on two mech-\nanisms: message passing and representation aggregation [19] [10].\nA graph G can be described as G = (V, E) with node attribute\nX and adjacency matrix A. Taking the Graph Convolutional Net-\nwork(GCN) [10] as an example, the computation process is as fol-\nlows:\n$$h^{k+1}_{v} = \\sigma (\\sum_{u \\in N(v)} (W^{k}h^{k}_{u}))$$\nwhere $h^{k}_{v}$ is the representation of node v at the k th layer in GCN and\n$\\tilde{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$ is the normalized adjacency matrix. $\\tilde{A} = A + I$\nis the adjacency matrix of the graph G with self loops added and D\nis a diagonal matrix with $D_{ii} = \\sum_{j} A_{ij}$. N(v) denote the neigh-\nbors of node v and $W^{k}$ is the weight matrix to be trained of the k-th\nlayer. Eq. (1) represents that node v will collect and aggregate the\ninformation of all neighboring nodes, which will be used as the rep-\nresentation of node v in the next layer through the activation function\n$\\sigma(*)$. Nodes' representation will be used to perform node-level tasks\nor complete graph-level tasks by using readout functions.\n3.1.2 Graph Auto-encoder\nGraph auto-encoder [9] is a self-supervised learning method. The\nmodel consists of a inference network and a generative network, and\nwe call then the encoder and decoder respectively. It mainly retains\nthe following two procedures: It maps the input sample to the hidden\nspace through the GCN encoder to obtain the low-dimensional rep-\nresentation of the sample features, and then latent features are used\nto reconstruct the original graph structure through a inner product\ndecoder. This process can be formulated as follows:\n$$A = \\sigma (ZZ^{T}), \\text{with} Z = GCN(X, A)$$\nFor graph auto-encoder(GAE), the adjacency matrix reconstructed\nfrom Z should be as comparable as possible to the original, so GAE\nuses cross-entropy as the loss function:\n$$L_{GAE} = E_{q(Z|X,A)} [log p(A | Z)]$$\nq and p are the encoding and decoding functions, respectively.\nFor variational graph auto-encoder(VGAE), Z is obtained by sam-\npling from a Gaussian distribution instead of by a definite function.\nIn VGAE (Variational Graph Autoencoder), the reparameterization\ntechnique is employed to facilitate the backpropagation of gradients.\nFor more detailed information, you can refer to the original litera-\nture [9]. It uses two GCNs that share the same weight in the first\nlayer to generates the means and variances. An additional Kullback-\nLeibler divergence more than GAE is employed in the loss function:\n$$L_{VGAE} = E_{q(Z|x,A)} [log p(A | Z)] \u2013 KL[q(Z | X, A)||p(Z)]$$\n3.1.3 Auto-encoder as an explainer\nOne application of autoencoder is to learn low-dimensional repre-\nsentations of the input data utilizing the nonlinear expressive power\nof neural networks. These low-dimensional representations(denoted\nas Z) in the latent space should be more distinguishable. Based on\nthis principle, we aim to separate the causal feature component (de-\nnoted as $Z_{c}$) related to the model's predictions from the entire fea-\ntures in the latent space and discard the non-causal part(denoted as\n$Z_{s}$). This approach avoids separating the causal and non-causal parts\nin the input spaces as in previous methods. By imposing constraints\non the latent space feature along the feature dimensions and setting\nan appropriate learning objective, we intend to isolate the causal sub-\nfeature $Z_{c}$ from Z in terms of the feature dimension. Then the inner\nproduct decoder can map the sub-matrix $Z_{c}$ into an adjacency mask,\nserving as a substructure for explanation.\n3.2 Framework\nAs shown in Figure 1, our model mainly consists of three compo-\nnents: the encoder (denoted as $f(\u00b7) : G \u2192 Z$), decoder (denoted\nas $g(\u00b7) : Z\\rightarrow G$), and discriminator (denoted as $h(\u00b7) : Z \u2192 \u0423$).\nWe mark the GNN to be explained as $F(\u00b7) : G \u2192Y$, which gives\na predicted label $Y \\in Y$ for each input graph $G\\in G$. The model\nlearns the low-dimensional representation of the input graph through\na two-layer GCN encoder. In order to maintain the structural con-\nsistency of the reconstructed graph with the original input, latent\nfeatures $Z = f(A,X) \\in Z$ is employed to calculate the auto-\nencoder reconstruction loss. Meanwhile, we partition Z into causal\nand non-causal parts based on the feature dimensions(denoted as\n$Z = cat(Z_{c}, Z_{s})$). $Z_{c}$ will be concatenated with a zero matrix to\nrestore the original dimensions of Z, and then utilized to generate\nan adjacency mask, serving as the explanation subgraph(denoted as\n$G_{s}, G_{s} = (A_{g(Z_{c})}, X)$). Then we have the following assump-\ntions:\nProposition 1. For any $Z_{p}$ that $Z_{p} \\subseteq Z, I(Y; Z_{p}) \\leq I(Y; Z)$\nProof. $I(Y; Z_{p}) < I(Y; Z)$ is equivalent to $H(Y|Z_{p}) \\geq H(Y|Z)$\nwhere H denotes entropy, since H(Y) is a constant. $Z_{p} C Z$ means\n$Z_{p}$ is a subset of Z, i.e., $Z = Z_{p} \\cup Z_{+}$. Therefore, $H(Y|Z_{p}) \\geq$\n$H(Y|Z_{p}, Z_{+}) = H(Y|Z)$, i.e., $I(Y; Z_{p}) < I(Y; Z)$\n\u03a0\nProposition 2. For causal features in the latent space, they should\nbe strongly correlated with the model predictions(i.e., mutual infor-\nmation $I(Y; Z_{c}) = I(Y; Z)$ ). Then the training criterion of the\nencoder should be:\n$$\\Theta = argmax_{\\Theta} I(Y; Z_{c}), \\text{since} I(Y; Z_{c}) < I(Y; Z)$$\nProposition 3. For non-causal features in the latent space, they\nshould be independent of the model predictions($Z_{s}\\perp Y$, i.e., mu-\ntual information $I(Y; Z_{s}) = 0$).\nThe above hypotheses can only hold true when the parameter of\nencoder $\\theta$ are optimal. Similar to previous work, we refer to the\nfeatures in the latent space that are correlated with the predictions\nmade by the GNN being explained as causal features (denoted as\n$Z_{c}$), and the parts that are not correlated as non-causal (denoted as\n$Z_{s}$). In other words, these features are the causal attribution behind\nthe predictions made by the GNN model on the original input graph\nG."}, {"title": "3.3 Learning Objectives", "content": "Post-hoc explanation refers to the process of providing an explana-\ntion or justification for a well-trained model. The outputs of GNN F\nare regarded as the training label (formalized as Y = F(A, X)). The\nlearning objective is designed to maximizing the mutual information\nbetween the model predictions and the underlying structure $G_{s}$:\n$$max_{G_{s}} I (Y; G_{s}) = max_{G_{s}} {H (Y) \u2013 H (Y | G = G_{s})}$$\nOur training process consists of two stages. In the first stage, the\ndiscriminator and auto-encoder are trained together. The motivation\nis to ensure that the auto-encoder can completely rejuvenate the orig-\ninal graph structure and train the discriminator to learn the causal at-\ntribution from causal features to the predicted outcome. The learning\nobjective can be formulized as:\n$$L = L_{AE} + \\lambda_{1} * KL [P_{\\phi_{h}} (Y|Z_{c}) || P(Y|g(Z_{c}))] + \\lambda_{2} * KL [P(Y|g(Z_{c})) || P(Y|X)]$$\n$L_{AE}$ is the loss of the autocoder (GAE or VGAE).\nIn the second stage, the parameters of the trained discriminator are\nfixed. The learning objective of the second stage is:\n$$L = L_{AE} + L_{size} + \\lambda_{3} * KL [P(Y|g(Z_{c})) || P_{\\phi_{h}}^{fixed} (Y|Z_{c})]$$\n$$L_{size} = \\frac{|| A *g(Z_{c})||_{1}}{||A||_{1}} \u2013 \\gamma$$\n$L_{size}$ is the size loss to ensure that the mask generated by the de-\ncoder are within a reasonable range. In our experiment, the value of\n$\\gamma$ is set to 0.5 in our experiments. A is the adjacency matrix and $\\gamma$ is\na hyper parameter. Causal features $Z_{c}$ are transformed into masks of\nadjacency matrix format by the inner product decoder, which should\nbe multiplied with the original adjacency matrix to serve as explana-\ntions.\nIt is difficult to directly find the optimal parameters that satisfy the\nabove expression, so we propose a two-step training strategy: In the\nfirst step, the discriminator is trained to make predictions consistent\nwith the target GNN based on the causal sub-features $Z_{c}$ in the la-\ntent space. The discriminator is a two-layer Multi-Layer Perceptron\n(MLP). More details of our model can be found in the Appendix.\nSince the latent features are dimensionality-reduced representations,\nthe discriminator can achieve high accuracy. In the second step, we\nfix the parameters of the discriminator to enforce the causal features\nto be distributed as much as possible in the dimensions we require.\nThen, the features are used by the decoder to generate explanations\nbased on these causal features."}, {"title": "4 Evaluation", "content": "4.1 Datasets\nTo verify the effectiveness of our proposed model, we conducted\nmany experiments on various datasets. Like other GNN interpreta-\ntion methods, we employed widely used synthetic and real-world"}, {"title": "4.2 Metrics", "content": "4.2.1 Fidelity\nFidelity is a commonly used metric to evaluate the faithfulness of\nthe explanations to the model, which is defined as the difference\nof predicted probability/accuracy between the original predictions\nand the new predictions of masked input features given by the ex-\nplainer [17] [29]. Intuitively, the local important input features iden-\ntified by the interpreter are discriminative to the GNN model. In\nthat case, the model's prediction should change significantly when\nthese local features are removed. We measure this changement with\nthe fidelity score. Analogously, keeping only discriminative features\nshould lead to similar predictions as the original, even if we remove\nthe other unimportant features. We measure that variation by the in-\nfidelity metric. In our experiment, we used fidelity based on the pre-\ndicted probability to verify how much the model can fit the behavior\nof the original GNN, and it is computed as:\n$$Fidelity_{prob} = \\frac{1}{N} \\sum_{i=1}^{N} (F(G_{i}) \u2013 F (G^{-m_{i}})y_{i})$$\n$$Infidelity_{prob} = \\frac{1}{N} \\sum_{i=1}^{N} (F(G_{i}) y_{i} \u2013 F (G^{m_{i}}) y_{i})$$\nHere $G_{i}$ is the original graph and F is the GNN model to be ex-\nplained. $G^{-m_{i}}$ represents the new graph obtained by keeping fea-\ntures of $G_{i}$ based on the complementary mask 1 \u2013 $m_{i}$. $G^{m_{i}}$ is the\nnew graph by keeping important features of $G_{i}$ based on hard mask\n$m_{i}$ of the explanation.\n4.2.2 Accuracy\nFor synthetic datasets with ground truths, we can leverage the un-\nderlying rules of building these datasets to identify important edges\nor nodes, such as motifs of the graph. Using these important fea-\ntures as references, we can compare the explanations generated by\nthe explainer with the ground truth. Accuracy, ROC, and F1 scores\nare commonly employed metrics for such evaluations.\nThe model accuracy measures the predictive accuracy of the gen-\nerated explanations in relation to the original inputs. Specifically, we\ninput both the original graph and the explanation into the target GNN\nmodel and compare the resulting predictions. A higher model accu-\nracy indicates a closer alignment between the explanatory subgraph\nand the predicted outcomes of the original inputs. This demonstrates\nthat the explainer is more proficient at identifying the most relevant\nsubgraph for the pre-trained GNN.\n4.2.3 Sparsity\nSparsity measures the conciseness of explanations, as different expla-\nnation methods yield various forms of explanations. Some methods\nmay select a subset of edges or nodes as explanations, while oth-\ners may generate global importance scores for edges or nodes. Spar-\nsity calculation refers to the proportion of explanations (e.g., edges\nor nodes) compared to the original input graph. The accuracy and"}, {"title": "4.3 Baselines", "content": "We consider several baseline models, including perturbation-based\nmethod (GNNExplainer), parameterized model-based method (PG-\nExplainer), and generative models based on autoencoder (Gem and\nOrphicX). PGExplainer, OrphicX, and Gem are all methods that train\nan interpreter to explain a target GNN model. On the other hand, GN-\nNExplainer requires multiple perturbation processes on each input to\ngenerate explanations. Gem and OrphicX are the closest baselines\nto our proposed method in this paper. We set the hyperparameters\nof these baseline models according to the reported settings in their\nrespective papers."}, {"title": "4.4 Quantitive Analysis", "content": "According to table 1 and 2, we observe that the accuracy of our\nexplanations surpasses that of baseline models both on the syn-\nthetic datasets and real-world datasets. For the artificial synthetic\ndataset, our explanations achieve optimal accuracy under different\nconstraints of K. For real-world datasets, our method outperforms\nother approaches under most sparsity constraints and maintains a\nhigher average accuracy. We conducted experiments using both GAE\nand VGAE as interpreters. The experimental results indicated that\nthe performance of GAE-based model was surpassed by the VGAE-\nbased. Consequently, for the subsequent experiments, we consis-\ntently chose VGAE as the foundational model for our interpreter.\nIn addition, OrphicX-0 in the table represents the results obtained\nby removing the calculation of information flow from the loss func-\ntion of OrphicX. The table shows that the information flow within\nOrphicX does not effectively capture the causal effects between hid-\nden features and model predictions. Even after it was removed, the\nperformance of the explainer did not significantly decline.\nIndeed, different explanations could potentially lead to similar\nclassification results as the original samples. Therefore, to further\ncompare the performances among different explaining methods, we\nreported the evaluation of edge accuracy on the synthetic dataset. By\ntransforming the inclusion of each edge in the explanation into a bi-\nnary classification problem, we can assess the concordance between\nthe generated edges and the ground truth motifs. The results are pre-\nsented in Table 3. A higher accuracy indicates that the explainer tends\nto assign higher importance scores to edges of the ground truth mo-\ntifs/subgraphs. Experimental results demonstrate that explanations\ngenerated by our approach are superior to the others.\nFurthermore, we use the fidelity and infidelity metrics to compare\nthe quality of explanations generated by different methods under four\ndifferent sparsity levels. Fidelity and Infidelity metrics can provide\nan alternative perspective on the quality of explanations. We compare\nour model and baseline methods on the BA-Shapes and Mutagenicity\ndatasets, as shown in Figure 4 and 3 (using 1-Infidelity as the y-axis\nfor easy comparison), indicating that our method generally exhibits\nbetter fidelity in most cases."}, {"title": "4.5 Qualitative Analysis", "content": "To further demonstrate the interpretability of the explanations, Fig-\nure 5 illustrates some visualized explanation instances generated by\ndifferent methods on the Mutagenicity dataset. The figure presents\nthree different mutagenic molecules. The first column represents the"}, {"title": "4.6 Ablation Study", "content": "To further investigate the effectiveness of designed components\nin PAGE, we conducted ablation experiments on both the artificially\nsynthesized dataset BAShapes and the real-world dataset Mutagenic-\nity. Specifically, we validated the performance of the interpreter un-\nder four conditions. The result is shown in Table 4, showing that\napplying only partial component cannot obtain optimal performance.\nAdditionally, we conducted experiments on hyperparameter sensitiv-\nity. Please refer to the supplementary materials for more details."}, {"title": "4.7 Efficiency Study", "content": "GNNExplainer requires multiple perturbations on a single sam-\nple. PGExplainer necessitates generating soft masks individually for\neach edge. GEM involves considering each edge to obtain a subgraph\nfor the \"guidance\" of the training. OrphicX computes information\nflow through sampling at different scales. Compared to these meth-\nods, our approach eliminates any perturbation or sampling processes.\nEach inference and backpropagation involve only a single computa-\ntion, resulting in a time complexity of O(1). As a result, our method\nremains significantly more efficient than these baseline models. Ta-\nble 4 presents the training and inference times for the mentioned\nmodels on the Mutagenicity dataset. All models are configured with\nhyperparameters, learning rates, and epoch numbers as described in"}, {"title": "5 Conclusion", "content": "In this article, we introduce PAGE, a parametric generative Graph\nNeural Network (GNN) explaining method designed to generate con-\ncise and reliable causal explanations for any graph neural network.\nPAGE optimize a generative autoencoder with a learning objective\nof maximizing mutual information between latent features and out-\nputs. Compared to existing methods, PAGE offers several advan-\ntages: its computation and inference processes do not require any\nperturbation or sampling processes, ensuring high explanation accu-\nracy while maintaining greater efficiency than previous approaches.\nMoreover, as a model-agnostic post-hoc explanation approach, it can\noffer causal explanations for different types of GNNs without rely-\ning on any prior assumptions or internal model details. We demon-\nstrate the superiority of our approach over baseline models through\nexperiments and data analysis. A limitation of PAGE is that we only\nexplored autoencoders as interpreters. An avenue for potential im-\nprovement could involve using more powerful generative models as\ninterpreters. We leave this for future investigation."}]}