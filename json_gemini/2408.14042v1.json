{"title": "PAGE: Parametric Generative Explainer for Graph Neural Network", "authors": ["Yang Qiu", "Wei Liu", "Jun Wang", "Ruixuan Li"], "abstract": "This article introduces PAGE, a parameterized generative interpretive framework. PAGE is capable of providing faithful explanations for any graph neural network without necessitating prior knowledge or internal details. Specifically, we train the autoencoder to generate explanatory substructures by designing appropriate training strategy. Due to the dimensionality reduction of features in the latent space of the autoencoder, it becomes easier to extract causal features leading to the model's output, which can be easily employed to generate explanations. To accomplish this, we introduce an additional discriminator to capture the causality between latent causal features and the model's output. By designing appropriate optimization objectives, the well-trained discriminator can be employed to constrain the encoder in generating enhanced causal features. Finally, these features are mapped to substructures of the input graph through the decoder to serve as explanations. Compared to existing methods, PAGE operates at the sample scale rather than nodes or edges, eliminating the need for perturbation or encoding processes as seen in previous methods. Experimental results on both artificially synthesized and real-world datasets demonstrate that our approach not only exhibits the highest faithfulness and accuracy but also significantly outperforms baseline models in terms of efficiency.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have consistently demonstrated promising results across a wide variety of tasks [24, 25, 32]. As GNNs find applications in crucial domains where trustworthy AI is imperative, the need for their interpretability has become increasingly paramount [29]. GNNExplainer is the first general model-agnostic approach for interpreting GNNs, and searches for soft masks for edges and node features to explain the predictions via mask optimization [26]. Since GNNExplainer largely focuses on providing the local interpretability by generating a painstakingly customized explanation for a single instance individually and independently, it is not sufficient to provide a global understanding of the trained model [14]. Furthermore, GNNExplainer has to be retrained for every single explanation. So, in real-world scenarios, GNNExplainer would be time-consuming and impractical. To address the above issues, PGExplainer was proposed to learn succinct underlying structures as the explanations from the observed graph data [14]. It also models the underlying structure as edge distributions, where the explanatory subgraph is sampled using binary hard masks for edges. PGExplainer utilizes a deep neural network to parameterize the explanation selection process, providing the ability to collectively explain multiple instances, so it has better generalization ability and can be utilized in an inductive setting easily [14]. Since there is no ground truth for explanatory subgraphs, PGExplainer has to employ reinforcement learning or Gumbel-Softmax sampling to search for informative explanatory subgraphs within a vast space that grows exponentially with the number of edges. Consequently, the computational complexity remains relatively high [12].\nInstead of using edge masks, recent methods have attempted to produce the adjacency matrix of explanatory subgraphs directly through a Variational Graph Auto-encoder (VGAE) [9], eliminating the need to consider each node or edge individually and achieving greater efficiency. GEM is the pioneering work in this domain. It introduced a distillation process grounded in the concept of Granger causality [4] to generate ground-truth explanatory subgraphs for the training of VGAE [12]. However, the distillation process intrinsically assumes independence between the edges. This could be problematic as graph-structured data is inherently interdependent [13]. Differing from GEM, which quantifies the causal influence from edges, OrphicX [13] suggests identifying the underlying causal factors from the latent space, allowing it to bypass direct interaction with intricately interdependent edges. It divides the latent representation produced by the encoder into causal features and spurious features and identifies the underlying causal features by harnessing information flow measurements [1], quantifying the causal information emanating from the latent features. It then constructs the adjacency matrix for the explanatory subgraph based on these causal features [13].\nThe fundamental architecture of Gem and OrphicX is depicted in Figure 1. Gem, while illustrating the viability of training VGAE as a graph interpreter, is subject to certain constraints. Gem's methodology involves a distillation process aimed at generating ground-truth subgraphs for training the autoencoder. This process perturbs the input graph at the edge level and assesses the importance of each edge based on the resulting reduction in model error. However, this approach assumes edge independence, failing to adequately explore the ability to discern causal features within the autoencoder's latent space. OrphicX endeavors to utilize information flow estimation to evaluate the causal relationships between each feature dimension in the latent space and the model predictions. This strategy aims to bypass perturbations from input dimensions. Nonetheless, due to computational constraints and complexity considerations, OrphicX operates within a severely restricted sampling range for each sample. For instance, regardless of the size of a single input graph, OrphicX samples only five nodes of the graph to estimate the causal effects of the whole graph on the output, which is obviously a limitation that undermines practicality and necessary adaptability. Moreover, the efficacy of information flow measurements in OrphicX appears limited. This assertion is supported by our experimental findings, which indicate that excluding the information flow term from the loss function has minimal impact on the model's performance. Furthermore, both the distillation process in Gem and the information flow measurement in OrphicX necessitate numerous perturbations or samplings on individual samples. These procedures incur significant time and space complexity, posing challenges to the application of these methods on large-scale graphs and extensive datasets.\nTo address these problems in OrphicX, we introduce a novel and more efficient model termed the Parametric Generative Explainer (PAGE\u00b9). Specifically, our optimization objective remains to maximize the mutual information between explanations and original predictions, but we introduce an extra parameterized discriminator to acquire the global understanding of the causal features in the latent space. As illustrated in Figure 1(c) and Figure 2, our model consists of an autoencoder and an additional discriminator. The features in the latent space compressed by the encoder are divided into two parts: causally relevant features related to predictions are used to generate explanations, while non-causal features are discarded. The training process consists of two stages: In the first stage, autoencoder and discriminator are trained together. The well-trained parameterized discriminator is designed to maximize the mutual information between causal features and prediction results. Parameterized discriminator eliminates the need for information flow estimation through sampling. Furthermore, since the discriminator is well-trained, it possesses a global comprehension on the entire dataset. In the second stage, the parameters of the discriminator are frozen to constrain the encoder to learn better causal features. Gem requires calculating causal contributions for each edge of a instance, while OrphicX involves extensive sampling across different dimensions including samples and features. In comparison, our approach is significantly more efficient than these methods. The trained explainer can collectively generate explanations, eliminating the need for extensive sampling and computations.\nOur main contributions can be outlined as follows:\n1) We introduce PAGE, a novel generative GNN explanation method. This method replaces complicated sampling processes with a streamlined discriminator, enhancing effectiveness.\n2) We further refine the optimization objective, eliminating the need of complex perturbation or sampling process. Instead, we directly align predictions from the original graph with those from the explanatory subgraph.\n3) Our research included experiments spanning both node and edge levels, conducted on both synthesized and real-world datasets. Experimental results clearly show that our approach outperforms existing methods across various metrics. This includes, but is not limited to, confidence and accuracy. Moreover, our method boasts significantly higher efficiency when juxtaposed with prior methods."}, {"title": "2 Related Work", "content": "Post-hoc GNN Explanation aims to produce an explanation for a GNN prediction on a given graph, usually as a substructure of the graph. Various explaining approaches focus on different aspects of the model and also provide different views. Many sur-\n\u00b9 Code is available at https://anonymous.4open.science/r/PAGE-1E22/\nveys categorized and summarized existing GNN explaining methods [28, 16, 11, 3]. Some methods provide explanations for each individual input instance separately, such as GNNExplainer [26], while some train a parameterized explainer to provide explanations collectively for multiple instances, like PGExplainer [14]. Depending on whether the explainer generates explanations separately or collectively, we categorize existing explanation methods into non-parameterized and parameterized approaches.\nNon-parameterized: Non-parameterized methods like GNNExplainer generate explanations for individual instances and predictions through perturbation or optimization processes. Some gradient-based methods compute gradients of the target prediction with respect to input features through back-propagation. Moreover, feature-based methods map hidden features to the input space via interpolation to calculate importance scores. Examples of such methods include SA [3], GuidedBP [3], and Grad-CAM [15]. Additionally, there are some methods based on cooperative game theory, which assign importance scores to input nodes and edges by introducing concepts from cooperative games like Shapley Value. Examples include SubgraphX [28] and GStarX [30]. Moreover, some approaches integrate reinforcement learning by treating the addition or removal of nodes and edges as strategies, while considering the probabilities of the target GNN as rewards, and then utilize a policy network to generate explanations, aiming to maximize the attained rewards, like RC-Explainer [23] and RG-Explainer [18].\nParameterized: Parameterized methods like PGExplainer train an explainer on the entire training set to provide explanations for multiple instances collectively. These methods utilize a parameterized explaining network, typically trained to maximize the mutual information or causal attribution between the explanatory subgraph/structure and the prediction. Subsequently, the network is employed to collectively generate explanations, like Gem [12], OrphicX [13] and our method PAGE. Some methods give explanations by developing interpretable surrogate models, which are subsequently employed to approximate the behavior of the target model to be explained. The explanations derived from these surrogate models are then used to interpret the behavior of the target GNN, such as GraphLime [8], PGM-Explainer [20], and GraphSVX [6].\nBeyond these two types of methods, there are also model-level explanation approaches that seek to explore the patterns of the input that can induce specific behaviors in the GNN. These approaches offer more general and high-level insights into the GNN model. Examples include XGNN [27], GLGExplainer [2] and GNNInterpreter [22]. Additionally, some methods specifically focus on offering counterfactual explanations, such as CF-GNNExplainer, CF2, and RCExplainer, and some self-explanatory models aim to develop the capability to generate predictions alongside corresponding expla-"}, {"title": "3 Methodology", "content": "3.1 Preliminary\n3.1.1 Graph neural network\nThe work of graph neural network mainly depends on two mechanisms: message passing and representation aggregation [19] [10]. A graph G can be described as G = (V, E) with node attribute X and adjacency matrix A. Taking the Graph Convolutional Network(GCN) [10] as an example, the computation process is as follows:\n$h_v^{k+1} = \\sigma(\\sum_{u \\in N(v)} (W^k h_u^k))$\t                                                (1)\nwhere $h_u^k$ is the representation of node u at the k th layer in GCN and $\\tilde{A} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$ is the normalized adjacency matrix. $\\tilde{A} = A + I$ is the adjacency matrix of the graph G with self loops added and D is a diagonal matrix with $D_{ii} = \\sum_{j} A_{ij}$. N(v) denote the neighbors of node v and $W^k$ is the weight matrix to be trained of the k-th layer. Eq. (1) represents that node v will collect and aggregate the information of all neighboring nodes, which will be used as the representation of node v in the next layer through the activation function $\\sigma(\\cdot)$. Nodes' representation will be used to perform node-level tasks or complete graph-level tasks by using readout functions.\n3.1.2 Graph Auto-encoder\nGraph auto-encoder [9] is a self-supervised learning method. The model consists of a inference network and a generative network, and we call then the encoder and decoder respectively. It mainly retains the following two procedures: It maps the input sample to the hidden space through the GCN encoder to obtain the low-dimensional representation of the sample features, and then latent features are used to reconstruct the original graph structure through a inner product decoder. This process can be formulated as follows:\n$A = \\sigma (ZZ^T)$, with $Z = GCN(X, A)$\t                        (2)\nFor graph auto-encoder(GAE), the adjacency matrix reconstructed from Z should be as comparable as possible to the original, so GAE uses cross-entropy as the loss function:\n$L_{GAE} = E_{q(Z|X,A)}[log p(A | Z)]$\t                                                                (3)\n$q$ and $p$ are the encoding and decoding functions, respectively. For variational graph auto-encoder(VGAE), Z is obtained by sampling from a Gaussian distribution instead of by a definite function. In VGAE (Variational Graph Autoencoder), the reparameterization technique is employed to facilitate the backpropagation of gradients. For more detailed information, you can refer to the original literature [9]. It uses two GCNs that share the same weight in the first layer to generates the means and variances. An additional Kullback-Leibler divergence more than GAE is employed in the loss function:\n$L_{VGAE} = E_{q(Z|x,A)} [log p(A | Z)] \u2013 KL[q(Z | X, A)||p(Z)]$\t         (4)\n3.1.3 Auto-encoder as an explainer\nOne application of autoencoder is to learn low-dimensional representations of the input data utilizing the nonlinear expressive power of neural networks. These low-dimensional representations(denoted as Z) in the latent space should be more distinguishable. Based on this principle, we aim to separate the causal feature component (denoted as $Z_c$) related to the model's predictions from the entire features in the latent space and discard the non-causal part(denoted as $Z_s$). This approach avoids separating the causal and non-causal parts in the input spaces as in previous methods. By imposing constraints on the latent space feature along the feature dimensions and setting an appropriate learning objective, we intend to isolate the causal sub-feature $Z_c$ from Z in terms of the feature dimension. Then the inner product decoder can map the sub-matrix $Z_c$ into an adjacency mask, serving as a substructure for explanation.\n3.2 Framework\nAs shown in Figure 1, our model mainly consists of three components: the encoder (denoted as $f(\\cdot) : G \\rightarrow Z$), decoder (denoted as $g(\\cdot) : Z\\rightarrow G$), and discriminator (denoted as $h(\\cdot) : Z \\rightarrow Y$). We mark the GNN to be explained as $F(\\cdot) : G \\rightarrow Y$, which gives a predicted label $Y \\in Y$ for each input graph $G \\in G$. The model learns the low-dimensional representation of the input graph through a two-layer GCN encoder. In order to maintain the structural consistency of the reconstructed graph with the original input, latent features $Z = f(A,X) \\in Z$ is employed to calculate the auto-encoder reconstruction loss. Meanwhile, we partition Z into causal and non-causal parts based on the feature dimensions(denoted as $Z = cat(Z_c, Z_s)$). $Z_c$ will be concatenated with a zero matrix to restore the original dimensions of Z, and then utilized to generate an adjacency mask, serving as the explanation subgraph(denoted as $G_s$, $G_s = (A_{g(Z_c)}, X)$). Then we have the following assumptions:\nProposition 1. For any $Z_p$ that $Z_p \\subseteq Z$, $I(Y; Z_p) \\le I(Y; Z)$\nProof. $I(Y; Z_p) < I(Y; Z)$ is equivalent to $H(Y|Z_p) \\ge H(Y|Z)$ where H denotes entropy, since H(Y) is a constant. $Z_p \\subset Z$ means $Z_p$ is a subset of Z, i.e., $Z = Z_p \\cup Z_+$. Therefore, $H(Y|Z_p) \\ge H(Y|Z_p, Z_+) = H(Y|Z)$, i.e., $I(Y; Z_p) < I(Y; Z)$  $\t                                                 \\Box$\nProposition 2. For causal features in the latent space, they should be strongly correlated with the model predictions(i.e., mutual information $I(Y; Z_c) = I(Y; Z)$ ). Then the training criterion of the encoder should be:\n$\\theta_E = argmax_{\\Theta_E} I(Y; Z_c)$, since $I(Y; Z_c) < I(Y; Z)$      (5)\nProposition 3. For non-causal features in the latent space, they should be independent of the model predictions($Z_s \\perp Y$, i.e., mutual information $I(Y; Z_s) = 0$).\nThe above hypotheses can only hold true when the parameter of encoder $\\theta_E$ are optimal. Similar to previous work, we refer to the features in the latent space that are correlated with the predictions made by the GNN being explained as causal features (denoted as $Z_c$), and the parts that are not correlated as non-causal (denoted as $Z_s$). In other words, these features are the causal attribution behind the predictions made by the GNN model on the original input graph G."}, {"title": "3.3 Learning Objectives", "content": "Post-hoc explanation refers to the process of providing an explanation or justification for a well-trained model. The outputs of GNN F are regarded as the training label (formalized as Y = F(A, X)). The learning objective is designed to maximizing the mutual information between the model predictions and the underlying structure $G_s$:\n$max_{G_s} I (Y; G_s) = max_{G_s} {H (Y) \u2013 H (Y | G = G_s)}$     (11)\nOur training process consists of two stages. In the first stage, the discriminator and auto-encoder are trained together. The motivation is to ensure that the auto-encoder can completely rejuvenate the original graph structure and train the discriminator to learn the causal attribution from causal features to the predicted outcome. The learning objective can be formulized as:\n$L = L_{AE} + \\lambda_1 * KL [P_{\\phi_h} (Y|Z_c) || P(Y|g(Z_c))]$ \n\t\t\t\t$+ \\lambda_2 * KL [P(Y|g(Z_c)) || P(Y|X)]$\t                                           (12)\n$L_{AE}$ is the loss of the autocoder (GAE or VGAE).\nIn the second stage, the parameters of the trained discriminator are fixed. The learning objective of the second stage is:\n$L = L_{AE} + L_{size} + \\lambda_3 * KL [P(Y|g(Z_c)) || P_{\\phi_{hfixed}} (Y|Z_c)]$\t\t\t\t\t\t\t                                 (13)\n$L_{size} = \\frac{|| A * g(Z_c)||_1}{\t\t||A||_1^{-\\gamma}}$\t                                  (14)\n$L_{size}$ is the size loss to ensure that the mask generated by the decoder are within a reasonable range. In our experiment, the value of $\\gamma$ is set to 0.5 in our experiments. A is the adjacency matrix and $\\gamma$ is a hyper parameter. Causal features $Z_c$ are transformed into masks of adjacency matrix format by the inner product decoder, which should be multiplied with the original adjacency matrix to serve as explanations."}, {"title": "4 Evaluation", "content": "4.1 Datasets\nTo verify the effectiveness of our proposed model, we conducted many experiments on various datasets. Like other GNN interpretation methods, we employed widely used synthetic and real-world"}, {"title": "4.5 Qualitative Analysis", "content": "To further demonstrate the interpretability of the explanations, Figure 5 illustrates some visualized explanation instances generated by different methods on the Mutagenicity dataset. The figure presents three different mutagenic molecules. The first column represents the initial molecule, while the remaining five columns showcase explanations generated by different methods, all adhering to a common sparsity constraint. Black edges indicate the edges the interpreter selects, while gray edges signify those not selected. The value of p represents the probability that the initial molecule (or the explained) possesses mutagenic potential given by the target GNN. In the first two cases, our method is the only one capable of capturing the complete carbon ring and the connected -NO2 functional group attaching to it, which is always regarded as a feature of being mutagenic. In the second case, OrphicX failed to recognize the -NH2 functional group, and the explanation is predicted as non-mutagenic. In the third example, we report a mutagenic molecule with no explicit motifs. PAGE produced an explanation that most closely matched the original prediction. Such explanations could aid in uncovering novel chemical patterns. In summary, compared to baseline models, our approach generates explanations that best reflect the behavior of the target GNN."}, {"title": "4.6 Ablation Study", "content": "To further investigate the effectiveness of designed components in PAGE, we conducted ablation experiments on both the artificially synthesized dataset BAShapes and the real-world dataset Mutagenicity. Specifically, we validated the performance of the interpreter under four conditions. The result is shown in Table 4, showing that applying only partial component cannot obtain optimal performance. Additionally, we conducted experiments on hyperparameter sensitivity. Please refer to the supplementary materials for more details."}, {"title": "4.7 Efficiency Study", "content": "GNNExplainer requires multiple perturbations on a single sample. PGExplainer necessitates generating soft masks individually for each edge. GEM involves considering each edge to obtain a subgraph for the \"guidance\" of the training. OrphicX computes information flow through sampling at different scales. Compared to these methods, our approach eliminates any perturbation or sampling processes. Each inference and backpropagation involve only a single computation, resulting in a time complexity of O(1). As a result, our method remains significantly more efficient than these baseline models. Table 4 presents the training and inference times for the mentioned models on the Mutagenicity dataset. All models are configured with hyperparameters, learning rates, and epoch numbers as described in"}, {"title": "5 Conclusion", "content": "In this article, we introduce PAGE, a parametric generative Graph Neural Network (GNN) explaining method designed to generate concise and reliable causal explanations for any graph neural network. PAGE optimize a generative autoencoder with a learning objective of maximizing mutual information between latent features and outputs. Compared to existing methods, PAGE offers several advantages: its computation and inference processes do not require any perturbation or sampling processes, ensuring high explanation accuracy while maintaining greater efficiency than previous approaches. Moreover, as a model-agnostic post-hoc explanation approach, it can offer causal explanations for different types of GNNs without relying on any prior assumptions or internal model details. We demonstrate the superiority of our approach over baseline models through experiments and data analysis. A limitation of PAGE is that we only explored autoencoders as interpreters. An avenue for potential improvement could involve using more powerful generative models as interpreters. We leave this for future investigation."}]}