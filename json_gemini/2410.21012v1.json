{"title": "FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval", "authors": ["Jinlin Wang", "Suyuchen Wang", "Ziwen Xia", "Sirui Hong", "Yun Zhu", "Bang Liu", "Chenglin Wu"], "abstract": "Large Language Models (LLMs) are proficient at retrieving single facts from extended contexts, yet they struggle with tasks requiring the simultaneous retrieval of multiple facts, especially during generation. This paper identifies a novel \"lost-in-the-middle\" phenomenon, where LLMs progressively lose track of critical information throughout the generation process, resulting in incomplete or inaccurate retrieval. To address this challenge, we introduce Find All Crucial Texts (FACT), an iterative retrieval method that refines context through successive rounds of rewriting. This approach enables models to capture essential facts incrementally, which are often overlooked in single-pass retrieval. Experiments demonstrate that FACT substantially enhances multi-fact retrieval performance across various tasks, though improvements are less notable in general-purpose QA scenarios. Our findings shed light on the limitations of LLMs in multi-fact retrieval and underscore the need for more resilient long-context retrieval strategies.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive capabilities in various NLP tasks, particularly in situations where single, salient facts need to be retrieved from a long context (Shi et al., 2023; Izacard and Grave, 2021; Jiang et al., 2023; Lin et al., 2023; Jeong et al., 2024). These \u201cneedle-in-a-haystack\" (gkamradt, 2023) tasks highlight the strength of modern LLMs in isolating critical information (Hsieh et al., 2024; Yoran et al., 2023). However, in tasks requiring the retrieval of multiple facts simultaneously-referred to as multi-fact retrieval tasks the performance of both open-source and proprietary LLMs noticeably degrades (Hsieh et al., 2024; Li et al., 2024). This is particularly problematic in long-context scenarios (Liu et al., 2023), where models struggle to retain and retrieve multiple pieces of key information, leading to incomplete or erroneous results.\nTo improve LLMs' performance in multi-fact retrieval tasks, we conducted an analysis of the failure patterns specific to this context. Our findings reveal that the core issue is not identifying relevant information individually but the model's difficulty in focusing on multiple facts as they accumulate. Therefore, in multi-fact retrieval scenarios, as the generation process goes on, the model gradually loses track of the information to be retrieved and tends to retrieve incomplete or incorrect information. This issue, which we term the \"lost-in-the-middle\" in multi-fact retrieval generation, occurs when multiple critical pieces of information are distributed throughout the context. Conventional retrieval techniques-whether relying on LLM querying or vector-based methods\u2014tend to focus on isolated facts, missing the broader context needed to retrieve all necessary information for complete understanding or reasoning.\nBased on this observation, we investigate whether a multi-round retrieval scheme can mitigate performance drops in multi-fact retrieval tasks. Specifically, we introduce Find All Crucial Texts (FACT), an iterative approach tailored for multi-fact retrieval. In our method, \u201ccontext rewriting\" leverages previously retrieved information to iteratively refine the context. Single-pass retrieval often fails to capture multiple facts, as the model's attention tends to focus primarily on the top-ranked fact. Our iterative process addresses this limitation by progressively removing identified facts from the context, allowing the model to concentrate on additional critical facts in subsequent rounds.\nWe empirically demonstrate that FACT significantly outperforms baseline methods in retrieving multiple important facts from long contexts. However, we also show mixed results when applying"}, {"title": "2 The Challenge of Multi-fact Retrieval", "content": "In Hsieh et al. (2024), models demonstrate a consistent decline in performance as the number of required retrievals from context increases. This section aims to explore the underlying mechanisms behind this degradation: does the model prematurely terminate its retrieval process, or does it struggle to track and process the necessary information?\nWe approach this through a mechanistic analysis inspired by Lu et al. (2024). Specifically, we adopt the multi-query needle-in-a-haystack (MQ-NIAH) task from RULER (Hsieh et al., 2024), where the model is presented with a context of key-value pairs and a question containing multiple keys, tasked with retrieving the corresponding values sequentially. To diagnose the model's internal representations, we train a linear probe on each layer of a LLaMA-3 8B Instruct model. The probe maps the intermediate layer representation, \u00e6, corresponding to an output position of a value, to an output value token y. The probe's accuracy reflects the degree to which the model's internal state retains the necessary information to output the correct value, allowing us to distinguish between cases where the model \"knows\" the information but fails to output it (high probe accuracy) and cases where the model has entirely lost track of the required information (low probe accuracy). For the details of linear probe training, please refer to Appendix C.\nFigure 1 plots the maximum probe accuracy against output position for different query lengths in a 50-query MQ-NIAH setting. The results reveal a clear \"lost-in-the-middle\" pattern during generation: as the generation progresses, the model progressively loses information until it recovers at the final few generated tokens. Please note that this is different from the \"lost-in-the-middle\" in context (Liu et al., 2024), where it focus on single information in the middle of the context instead. Notably, the position of the accuracy turning point is largely invariant to the number of key tokens, suggesting that the performance degradation is not due to an overloaded number of key tokens. This pattern implies a fundamental constraint in the model's capacity: it appears unable to reliably retrieve and track more than a certain number of key pieces of information concurrently from the context."}, {"title": "3 The FACT Method", "content": "As noted above, the facts are basic constituent units within the context. They can be used to provide information in retrieval tasks and to generate answers in QA tasks. The completeness of facts is crucial to retrieval and QA performance. To this end, we introduce an iterative rewriting method called FACT, which significantly enhances fact retrieval performance in common scenarios. This largely addresses the challenges mentioned in Section 2."}, {"title": "3.1 Iterative Rewriting", "content": "To solve the problem of incomplete or inaccurate facts, we employ an iterative rewriting approach for fact retrieval. Specifically, based on the user's query, candidate facts are retrieved through methods such as using LLMs as retrievers or vector-based approaches. These candidate facts are then located inside the context, where they are rewrited by either removing or replaced with other noise data, resulting in a new context. This process is repeated until reaching the maximum number of iterations or meeting the stopping criteria. The candidate facts found in each iteration are aggregated to form the final set of facts. Algorithm 1 describes the complete process in detail."}, {"title": "4 Experiments", "content": "We test the performance of FACT equipped with closed-sourced GPT-40 and GPT-40-mini (OpenAI, 2024), and open-sourced Llama-3.1 8B Instruct (Dubey et al., 2024). We report the performance on two types of tasks:\n\u2022 Retrieval Tasks, where the model directly retrieves multiple key information in the context. This includes RULER (Hsieh et al., 2024) and Counting Stars (Song et al., 2024).\n\u2022 QA Tasks, which requires reasoning about the provided context. This type of task includes: (1) Single-doc QA tasks, including NarrativeQA (Kocisk\u00fd et al., 2018), Qasper (Dasigi et al., 2021), and MultiFieldQA (Bai et al., 2024); (2) Multi-doc QA tasks, including HotpotQA (Yang et al., 2018), 2WikiMQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022). The QA tasks adopt the contexts and prompt templates from LongBench (Bai et al., 2024). Please refer to Appendix A for the statistics of the QA tasks.\nIn the experiments, we compare the results of FACT against a baseline direct retrieval method for each model. This direct retrieval setting returns all the retrieved information or directly answers the question in one shot with the default prompt for each task. We include the prompts we used for the retrieval task and the retrieval step of the QA tasks in Appendix B."}, {"title": "4.1 Settings", "content": "We test the performance of FACT equipped with closed-sourced GPT-40 and GPT-40-mini (OpenAI, 2024), and open-sourced Llama-3.1 8B Instruct (Dubey et al., 2024). We report the performance on two types of tasks:\n\u2022 Retrieval Tasks, where the model directly retrieves multiple key information in the context. This includes RULER (Hsieh et al., 2024) and Counting Stars (Song et al., 2024).\n\u2022 QA Tasks, which requires reasoning about the provided context. This type of task includes: (1) Single-doc QA tasks, including NarrativeQA (Kocisk\u00fd et al., 2018), Qasper (Dasigi et al., 2021), and MultiFieldQA (Bai et al., 2024); (2) Multi-doc QA tasks, including HotpotQA (Yang et al., 2018), 2WikiMQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022). The QA tasks adopt the contexts and prompt templates from LongBench (Bai et al., 2024). Please refer to Appendix A for the statistics of the QA tasks.\nIn the experiments, we compare the results of FACT against a baseline direct retrieval method for each model. This direct retrieval setting returns all the retrieved information or directly answers the question in one shot with the default prompt for each task. We include the prompts we used for the retrieval task and the retrieval step of the QA tasks in Appendix B."}, {"title": "4.2 Retrieval Tasks", "content": "We present the results of the retrieval tasks in Table 1. The results demonstrate a significant improvement in retrieval performance when applying our proposed method across both open-source and closed-source models. Across all tasks, the method consistently enhances the models' ability to retrieve relevant information from long contexts, outperforming the direct retrieval baselines substantially. This is particularly evident in tasks with longer context lengths, where traditional retrieval methods struggle. Moreover, for better-performing models like GPT-40 and GPT-40-mini, FACT achieves nearly perfect results.\nIn Figure 2, we show the comparison of our proposed FACT against baselines under varied iterations with 16K context length in RULER K1V10Q1. Note that as the iteration increases, the overall scores steadily rise. This is especially true for Llama-3.1 8B Instruct: when the iteration reaches 3, the score increases by nearly 50 percentage points."}, {"title": "4.3 Question Answering Tasks", "content": "We present the results of QA Tasks in Figure 3.\nThe performance impact of iterative context rewriting varies significantly across model families. GPT-40 and GPT-40-mini consistently improve as the number of rewriting iterations increases. However, Llama-3.1 and Qwen-2.5 show a noticeable performance decline with iterative retrieval, particularly Llama-3.1, which struggles with retrieved context. This difference likely comes from training differences: GPT-40 may have been specifically trained on retrieval-augmented tasks, while Llama-3.1 and Qwen-2.5 may lack such training, making them more prone to hallucinations or errors.\nIterative rewriting versus one-shot retrieval. Our results show that iterative rewriting outperforms one-shot retrieval, especially for models suited to retrieval-based tasks. Iterative rewriting leads to continuous improvements across iterations, showing the benefits of gradual context refinement. This supports our hypothesis in Section 2, which suggests that repeated enhancement of retrieved context improves model understanding and response quality.\nVariability in task-specific performance with iterative rewriting. The impact of iterative rewriting varies significantly across tasks. For the GPT-4o family, we see major gains for datasets like 2wikimqa and MuSiQue but minor declines for Qasper and NarrativeQA. This is likely due to the different characteristics of each dataset. 2wikimqa and MuSiQue contain dense factual information, which benefits from iterative rewriting by emphasizing key details and reducing noise, thereby improving accuracy. On the other hand, Qasper and NarrativeQA require nuanced reasoning and complex knowledge, which are beyond mere retrieval. Iterative rewriting in these cases may oversimplify or alter essential information, leading to loss of detail and increased ambiguity. Thus, while factual tasks benefit from FACT, highly structured or narrative tasks may not."}, {"title": "5 Conclusion", "content": "This paper explored the challenges faced by Large Language Models (LLMs) in multi-fact retrieval tasks, particularly the \u201clost-in-the-middle\" phenomenon, where models progressively lose track of key facts during generation. To address this, we introduced FACT, an iterative context-rewriting method designed to improve multi-fact retrieval by progressively refining context. Our experiments show that FACT significantly boosts retrieval performance in long-context scenarios, though results were mixed for general-purpose QA tasks.\nThese findings underscore the need for robust retrieval mechanisms that go beyond single-pass methods, highlighting the value of iterative refinement in complex retrieval settings. While FACT proves effective for fact-intensive retrieval, its mixed performance on QA tasks suggests further research is needed to adapt iterative methods for broader NLP contexts. Future work should explore dynamic rewriting techniques tailored to task characteristics, balancing context enrichment with the retention of essential information. This could include dataset-aware rewriting strategies that adjust context modification based on task demands, optimizing performance while minimizing trade-offs. Additionally, task-specific training focused on retrieval could enhance the efficacy of iterative context rewriting. Overall, this work lays a foundation for advancing context-building and long-context reasoning methods, pushing the boundaries of multi-fact retrieval capabilities in LLMs."}, {"title": "Limitations", "content": "This short paper includes the insights and findings of our experiments to improve LLMs' multi-fact retrieval performance. While the FACT method shows considerable promise in improving multi-fact retrieval performance, there are several aspects that warrant further exploration, which we believe represent opportunities for future work rather than critical shortcomings.\nTask-specific Performance Variability. FACT exhibits significant improvements in multi-fact retrieval tasks, but its performance gains in general-purpose QA tasks are more mixed. This variation likely stems from the fundamental differences in task requirements: FACT is particularly well-suited to fact-heavy retrieval tasks, where it refines the context over iterations. However, the iterative approach may not always lead to optimal outcomes in tasks requiring nuanced reasoning or comprehension, such as NarrativeQA or Qasper. Nonetheless, we see this as an opportunity to explore task-adaptive strategies that fine-tune the number of iterations or degree of context rewriting based on specific task characteristics.\nModel-specific Behavior. The effectiveness of FACT can differ across model families. Although closed-source models such as GPT-40 benefit significantly from iterative rewriting, some open-source models, such as LLaMA-3.1, show smaller gains or sometimes negative gains in retrieval tasks. This is likely due to differences in training regimes and architectures. However, these results highlight the potential to improve the performance of open-source models through targeted training in retrieval-augmented tasks. Addressing this presents an exciting avenue for future research, aiming to make FACT more universally beneficial across various model types.\nComputational Considerations. FACT introduces additional computation due to its iterative nature, which could increase latency in certain applications. However, the trade-off between accuracy and computational overhead is a common challenge in advanced retrieval methods. In practice, this issue can be mitigated by fine-tuning the number of iterations or applying FACT selectively to tasks where its benefits justify the additional cost. Further research on optimizing the efficiency of iterative processes could help minimize this overhead."}, {"title": "Generalization to Broader NLP Tasks", "content": "FACT is designed primarily for multi-fact retrieval tasks, and it excels in these tasks. Its application to more complex reasoning tasks, while promising, has room for improvement. We do not see this as a fundamental limitation of FACT, but rather a natural constraint given its design focus. Adapting FACT to tasks requiring deeper reasoning or synthesis remains an exciting challenge for future research, which could involve integrating more advanced reasoning or agentic procedures into the iterative process."}, {"title": "Algorithm 1 FACT", "content": "Require: Q: the user query text, C: the context of the sample, n: number of iterations, Retrieve: the retrieval function, Rewrite: the context rewriting function, Stop: the iteration stop judgment function\nEnsure: F: the set of final found facts\n1: F = []\n2: for i = 1 to n do\n3: cand_facts = Retrieve (Q, C)\n4: C = Rewrite(cand_facts, C)\n5: F.extend(cand_facts)\n6: if Stop(F, C) then\n7: break\n8: end if\n9: end for\n10: return F"}, {"title": "C Linear Probe Training Details", "content": "This section describes the training process for the linear probes used in Section 2, specifically for the MQ-NIAH task. This linear probe is a multi-fact retrieval extension of the one proposed by Lu et al.\n(2024).\nFor the MQ-NIAH task introduced in Section 2, the model receives $n_q$ queries and must retrieve corresponding values from $n_k$ key-value pairs in the prompt, where each value consists of a single token. We define V as the set of all possible single-token values. Given a prompt, we define the index of the token corresponding to the i-th output value as $t_i \\in R$, and the value token itself as $v_i \\in V$.\nAssume the LLM consists of L layers. For each Transformer layer, we randomly initialize a linear classifier $C \\in R^{d\\times v}$, where d is the hidden dimension of the LLM, and $v = |V|$ is the number of possible values. Given the output from the l-th layer, denoted as $H_l \\in R^{L\\times d}$ (with L representing the sequence length), the linear classifier $C_l$ predicts the value $v_i$ using the hidden state $H_{l,[t_i,:]}$ for each i \u2208 {1,\u2026\u2026\u2026, $n_q$}.\nWe collect training data and conduct inference using a specifically designed prompt. During training, we concatenate the ground-truth values to the"}]}