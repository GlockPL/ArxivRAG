{"title": "When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation", "authors": ["Yuli Zhou", "Guolei Sun", "Yawei Li", "Luca Benini", "Ender Konukoglu"], "abstract": "This study investigates the application and performance of the Segment Anything Model 2 (SAM2) in the challenging task of video camouflaged object segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in the surroundings for videos, due to similar colors and textures, poor light conditions, etc. Compared to the objects in normal scenes, camouflaged objects are much more difficult to detect. SAM2, a video foundation model, has shown potential in various tasks. But its effectiveness in dynamic camouflaged scenarios remains under-explored. This study presents a comprehensive study on SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged video datasets using different models and prompts (click, box, and mask). Second, we explore the integration of SAM2 with existing multimodal large language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by fine-tuning it on the video camouflaged dataset. Our comprehensive experiments demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged objects in videos. We also show that this ability could be further improved by specifically adjusting SAM2's parameters for VCOS. The code will be available at github.com/zhoustan/SAM2-VCOS", "sections": [{"title": "1 Introduction", "content": "Camouflaged object segmentation, aiming to identify objects that blend into their surroundings, is a fundamental task in computer vision. Unlike conventional segmentation tasks, where objects typically exhibit distinct boundaries, camouflaged objects often have similar colors or textures to the background, making them difficult to be perceived. This task becomes even more complex for video sequences, where both objects and background can change dynamically over time.\nTraditional segmentation approaches, such as Fully Convolutional Networks (FCNs) [23] and Mask R-CNN [13], have made significant contributions to the"}, {"title": "Evaluating SAM2's zero-shot ability for VCOS", "content": "In this work, we comprehensively evaluate the performance of SAM2 in video camouflaged object segmentation (VCOS), focusing on its zero-shot capability, its integration with existing state-of-the-art methods, and its adaptation. Our study is organized into three parts, with each targeting a specific aspect of this complex problem:\nEvaluating SAM2's zero-shot ability for VCOS: We thoroughly assess the performance of SAM2 in segmenting camouflaged objects, an inherently challenging task due to the high similarity between the object and its background. SAM2 is evaluated in two modes: automatic and semi-supervised mode. In automatic mode, we leverage SAM2's built-in automatic mask generator to produce initial segmentation masks, which are then used as mask prompts for subsequent frames. In semi-supervised mode, we investigate how manual interactions (via click, box, and mask-based prompts) and prompting time can impact segmentation quality. This analysis provides the first detailed exploration of SAM2's behavior specifically for camouflaged video segmentation, a domain where traditional segmentation models typically struggle."}, {"title": "Augmenting SAM2 with state-of-the-art MLLMs and VCOS methods", "content": "Augmenting SAM2 with state-of-the-art MLLMs and VCOS methods: We further explore the effectiveness of SAM2 when combined with advanced MLLMS and VCOS techniques. For MLLMs, we design specific prompts to generate bounding boxes around potential camouflaged regions, which are then used by SAM2 as box prompts for segmentation. For VCOS, we enhance segmentation by refining the initial mask generated by VCOS methods with SAM2's powerful refinement capabilities. Our contribution here is the novel integration of SAM2 with MLLMs and VCOS methods, demonstrating how prompt-driven refinement can improve segmentation accuracy in highly complex visual scenes."}, {"title": "Fine-tuning SAM2 on largest-scale VCOS benchmark", "content": "Fine-tuning SAM2 on largest-scale VCOS benchmark: We explore how task-specific fine-tuning can improve SAM2's segmentation performance in VCOS. By fine-tuning SAM2 on the MoCA-Mask dataset, we extend SAM2's utility beyond its initial design, showing how it can be adapted to specialized datasets for enhanced performance.\nOur contributions can be summarized as follows:"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Camouflaged Object Detection", "content": "Camouflaged Scene Understanding (CSU) aims to perceive the scenes where objects are difficult to be distinguished with the background. These scenes commonly exist in natural environments, such as forests, oceans, and deserts. Among various CSU tasks [10, 25, 30, 37], camouflaged object detection (COD) attracts lots of research attention, which is to identify objects that blend seamlessly into their environment, posing a significant challenge for conventional perception techniques. Traditional approaches to object detection, which rely on strong edge features, color contrasts, and texture variations, often fail when applied to camouflaged objects. This led to the development of specialized COD models that incorporate unique feature extraction techniques, focusing on small differences in texture, edge irregularities, and subtle shifts in color or shading that indicate the presence of a hidden object [11, 32]. Several innovative models have been proposed for COD. HGINet [48] uses a hierarchical graph interaction transformer with dynamic token clustering to capture multi-scale features for better object detection. AGLNet [5] enhances COD performance by employing adaptive guidance learning, which adjusts feature extraction based on the object's appearance and context. PAD [43] leverages a multi-task learning approach, pre-training on multiple datasets and fine-tuning for COD tasks through task-specific adaptation. DQNet [38] focuses on cross-model detail querying, using multiple models to enhance the detection of subtle features in camouflaged objects. R2CNet [52] integrates linguistic cues with visual data to improve detection, particularly by using referring expressions to locate specific camouflaged objects. ZoomNeXt [29] presents a unified collaborative pyramid network to enhance feature extraction across multiple scales. WSSCOD [50] improves COD using weak supervision by learning from noisy pseudo labels, while CamoTeacher [18] applies dual-rotation consistency learning for semi-supervised detection, making use of limited labeled data. The development of these advanced COD models demonstrates significant progresses in detecting camouflaged objects by integrating multi-scale feature extraction, adaptive learning, and novel supervision techniques, paving the way for more robust and accurate object detection in challenging environments."}, {"title": "2.2 Video Camouflaged Object Segmentation", "content": "Video camouflaged object segmentation (VCOS), an important task of CSU, aims to detect camouflaged objects in dynamic and video-based environments. To deal with VCOS, models are required to consider object motion, temporal continuity, and varying backgrounds, adding additional complexities to COD. Several advanced models have been introduced to tackle these challenges by integrating motion learning and temporal-spatial attention mechanisms. IMEX [15] introduces implicit-explicit motion learning, allowing for more robust detection of camouflaged objects through an integration of both implicit and explicit motion features. Temporal-spatial attention is also crucial for many VCOS models, as seen in TSP-SAM [16], which enhances SAM with a focus on temporal-spatial prompt learning to identify subtle object movements. SAM-PM [28] further extends this approach by applying spatio-temporal attention to boost accuracy in video sequences by tracking subtle changes in movement and background. An earlier method [6] focuses on implicit motion handling to refine video object detection, particularly for scenes where motion is subtle or hard to detect. OCLR [19] proposes three scores to automatically assess the effectiveness of camouflaged by measuring background-foreground similarity and boundary visibility and uses these scores to improve camouflaged datasets, integrate them into a generative model. MG [46] introduces a self-supervised Transformer-based model for motion segmentation using optical flow demonstrating the effectiveness of motion cues over visual appearance in VCOS. These models illustrate the importance of combining motion analysis with traditional object detection techniques to enhance VCOS performance in real-world applications."}, {"title": "2.3 Segment Anything Model 2", "content": "Segment Anything Model 2 (SAM2) [35] is a vision foundation model for segmenting objects across images and videos. SAM2 has shown excellent performances in medical video and 3D segmentation, including tasks such as polyp detection, surgical video segmentation, and other medical image segmentation [4, 14, 21, 26, 36, 44, 49]. Furthermore, SAM2 has been applied in the segmentation of 3D meshes and point clouds [39], remote sensing [34], and image camouflaged object detection [4, 41, 44].\nDespite these advances, to the best of our knowledge, there is no specific study focusing on VCOS using SAM2. This study fills this gap by systematically examining SAM2's performance in VCOS task, and proposing strategies to further improve SAM2's capability."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Datasets", "content": "We use two video COD datasets: MOCA-Mask [6] and CAD [2]. MoCA-Mask is a densely annotated dataset originating from the moving camouflaged animals"}, {"title": "3.2 Metrics", "content": "We use seven common metrics for evaluation including S-measure (Sm) [7], weighted F-measure (F\u03c9) [27], mean absolute error (MAE) [31], F-measure (F\u03b2) [1], E-measure (Em) [8], mean Dice (mDice) and mean IoU (mIoU)."}, {"title": "3.3 Prompting Strategies", "content": "We evaluate SAM2's performance in both automatic and semi-supervised modes, by utilizing the automatic mask generator and the interactive prompts, respectively. These two modes allow us to thoroughly assess SAM2's flexibility and effectiveness."}, {"title": "Automatic Mode", "content": "We evaluate SAM2 in automatic mode using its built-in automatic mask generator. In this setup, following the work [51], SAM2 automatically generates a number of segmentation masks on the first video frame and we select the mask with the highest IoU value compared to the corresponding ground truth. Let M = {M1, M2, ..., Mn} represent the set of segmentation masks generated by SAM2 on the first video frame. G represents the ground truth, and IoU(mi, G) calculates the IoU between the generated mask mi and the ground truth G. The mask m* with the highest IoU is selected by:\nm* = arg max IoU(mi, G)\nmi\u2208M\nThe selected mask m* is used as the mask prompt, without any manual modification. This ensures that SAM2 operates in an unsupervised manner, relying on its automatic mask generation capabilities to track and segment camouflaged objects throughout the video.\nThis evaluation aims to assess SAM2's effectiveness in segmenting the camouflaged animals in the video without any user guidance and explore SAM2's potential in such scenarios."}, {"title": "Semi-supervised Mode", "content": "Semi-supervised Mode. In the semi-supervised mode, we employ three distinct prompt strategies: click-based, box-based, and mask-based prompts. Each strategy is evaluated across different frames to investigate how prompt types and timing affect SAM2's segmentation performance. For click-based prompts, 1, 3, and 5 foreground clicks (camouflaged animals) are randomly selected from the corresponding ground-truth mask. For box-based and mask-based prompts, we directly use the object's bounding box or mask from the dataset as the prompt."}, {"title": "Prompt Timing and Frame Selection", "content": "Prompt Timing and Frame Selection. We extend the analysis beyond the first frame by applying prompts at different times in the video sequence. In this experiment, the 0th, 5th, 10th, 11th, -6th, -1st, and middle frame are selected as the prompted frames. Here, the frame index follows indexing rules in the Python list, i.e., -1st represents the last frame. These frames are purposely chosen, from the beginning to the end, allowing us to analyze how prompt timing affects SAM2's performance. Early frames like the 0th, 5th, and 10th provide insight into how well the model tracks and segments the object from the start, while frames closer to the end, -11th, -6th, and -1st help evaluate SAM2's ability to handle backward propagation over time. The middle frame is helpful to assess how well SAM2 performs when provided with information at a pivotal moment in the video sequence. This comprehensive selection of frames allows us to analyze how the prompt timing impacts SAM2's segmentation accuracy and robustness when processing videos."}, {"title": "3.4 Refine MLLMS and VCOS Methods with SAM2", "content": "In this experiment, we explore using SAM2 to refine the results generated by the existing MLLMs and VCOS methods."}, {"title": "Refine MLLMs with SAM2", "content": "Refine MLLMs with SAM2. The use of MLLMs (Multimodal Large Language Models) with SAM2 is motivated by the limitation that most current MLLMs can only output bounding boxes, but not the segmentation masks. If the approach of using MLLMs-generated bounding boxes as prompts for SAM2 proves effective, it allows for automated identification and segmentation of objects, removing the need for manual prompts.\nTo be more specific, we employ two large Multimodal LLM models, LLaVA-1.5-7b [22] and Shikra-7b-delta-v1 [3], in a zero-shot setting to detect camouflaged objects in the first frame of the video. We design several prompts to ask MLLMs to generate the bounding box inspired by [40], and we finally select the prompt \"Please provide the coordinates of the bounding box where the animal is camouflaged in the picture\". The input to the model consists of this question prompt alongside the visual input of the first video frame. The model processes this input and outputs the coordinates of a bounding box that is presumed to encapsulate the camouflaged object. The bounding box coordinates generated by the MLLMs serve as the box prompt for SAM2. SAM2 uses these coordinates to guide the segmentation process across the whole video. By leveraging the box prompt, SAM2 is expected to effectively segment the camouflaged object, even in the presence of complex backgrounds and low contrast."}, {"title": "Refine VCOS with SAM2", "content": "Refine VCOS with SAM2. In this experiment, we focus on refining the camouflaged object masks generated by existing VCOS models using SAM2. Since VCOS models already output the object masks, we explore how SAM2 can enhance the quality of these masks through its advanced segmentation capabilities."}, {"title": "3.5 Fine-tune SAM2 on MOCA-Mask", "content": "SAM2 was trained on large-scale SA-V [35] dataset which generally contains videos from common scenes. To improve its ability on challenging camouflaged scenes, we propose a finetuning strategy to adjust SAM2's parameter on the popular MoCA-Mask [6] dataset. Specifically, we fine-tune the image encoder, the mask decoder, and both, with freezing other parameters to maintain generalization since SAM2 was initially trained on a much larger dataset. This selective fine-tuning focuses on adapting SAM2 to the specific challenges of camouflaged object detection, where precise feature encoding and decoding are critical. We employ the AdamW [24] optimizer with the combination of Dice loss and Binary Cross-Entropy (BCE) loss to achieve accurate segmentation of camouflaged objects. We use the learning rate (lr = 1e \u2013 8) and weight decay (wd = 0.01). The overall loss function is computed as a sum of the segmentation loss and the BCE loss:\nL = Lseg + Lce"}, {"title": "4 Results", "content": ""}, {"title": "4.1 Comparison with State-of-the-art VCOS Methods", "content": "We compare the results obtained by prompting SAM2 using 1-click, box, and mask at the first video frame, with existing state-of-the-art methods, in Table 1 and Table 2. It is important to note that SAM2 with semi-supervised mode outperforms current SOTA models in the VCOS task, where prompts guide the segmentation process. This naturally gives SAM2 an advantage, as the use of interactive prompts allows for better adaptability compared to fully automated models. These results highlight the effectiveness of integrating prompt-based strategies for camouflaged video segmentation."}, {"title": "4.2 Effect of Prompting Strategies", "content": "We compare results obtained from various prompting strategies as mentioned in \u00a73.3. From the results, we have three observations: 1) Mask-based prompt results in the best segmentation result, compared with click and box prompts; 2) Increasing the number of clicks significantly improves the segmentation result; 3) Prompting at the middle of the video generally gives better performance than prompting at other frames and results vary a lot when prompting at different times."}, {"title": "Comparisons among different prompting strategies", "content": "Comparisons among different prompting strategies. Table 2 presents a comprehensive comparison of SAM2's performance using different prompting strategies (1-click, box, and mask) across various model sizes on the MoCA-Mask and CAD datasets. The results clearly demonstrate the superiority of mask-based prompts over both 1-click and box-based strategies. This result is consistent with our intuition: the more detailed the prompt, the better the segmentation. For instance, in the CAD dataset, SAM2 with mask prompting achieves the highest mIoU of 0.804 for the large model (Hiera-L), outperforming both 1-click (mIoU 0.612) and box-based prompting (mIoU 0.781). Similar trends are observed for other model sizes, such as Hiera-B+, where mask prompting achieves 0.791 in mIoU, compared to 0.604 for 1-click prompting and 0.768 for box-based prompting."}, {"title": "Impact of the number of clicks for point prompting", "content": "Impact of the number of clicks for point prompting. We examine the impact of different numbers of clicks (1, 3, and 5) used as prompts at the middle frame on segmentation performance, across various model sizes. The results on the MOCA-Mask dataset are shown in Table 3. The performance generally improves as the number of clicks increases, with the highest mIoU and mDice scores observed for 5-click prompts. For instance, when using the large model, 5-click prompts achieve the best performance, with an Sm of 0.827, mDice of 0.733, and mIoU of 0.639, outperforming both 1-click and 3-click prompts. The same trend is observed for the B+ and small models, where 5-click prompts consistently provide better segmentation result. The tiny model, however, shows a smaller performance increase when moving from 3-click to 5-click prompts, with only slight differences in the metrics. In fact, the 3-click prompt achieves a higher F\u03b2 and Em score compared to the 5-click prompt. This suggests that while more clicks generally result in better segmentation, there may be diminishing returns for very small models."}, {"title": "Impact of prompt timing", "content": "Impact of prompt timing. We evaluate the effect of prompt timing on the performance using click-based, box-based, and mask-based prompts on the small version of the SAM2 using the MoCA-Mask dataset. Table 4 presents the results for different prompt timings across all three prompt strategies. In general, the results show that applying the prompt on the middle frame yields the best segmentation performance across all strategies. For instance, using the click-based prompts at the middle frame gives a mIoU of 0.561, which is higher than prompting at other times. Similarly, for the box-based and mask-based prompts, the middle frame provides the best results in most evaluation metrics. The tread is particularly obvious for the mask-based prompts where the middle frame achieves a mIoU of 0.718, the highest among all experiments. These results suggest that prompt timing is a critical factor in achieving optimal segmentation performance."}, {"title": "4.3 Automatic Mode Results", "content": "The results of using SAM2 in automatic mode on the MoCA-Mask dataset is shown in Table 5. It can be observed that the performance is notably lower compared to semi-supervised mode with guided prompts. This suggests that SAM2 struggles with completely unsupervised segmentation in camouflaged scenarios, where object boundaries are unclear and difficult to detect without guidance."}, {"title": "4.4 Refine MLLMs and VCOS with SAM2", "content": "Refine MLLMs with SAM2. In our experiment, we utilize two large Multimodal LLM models, LLaVA-1.5-7b [22] and Shikra-7b-delta-v1 [3], in combination with SAM2 for video segmentation. The results are presented in Table 6. It shows that he performance is unsatisfactory. For instance, the mIoU and mDice scores for both LLaVA+SAM2 and Shikra+SAM2 models are relatively low, with the tiny model sizes achieving mIoU values of 0.272 and 0.1, respectively. The poor performance can be attributed to the heavy reliance of SAM2 on accurate bounding box detection by the MLLMs in the first frame. When the MLLMs fail to generate the correct bounding box, it severely impacts segmentation in the subsequent frames, leading to poor segmentation masks. This highlights the crucial role of accurate object detection in the initial frame."}, {"title": "Refine VCOS with SAM2", "content": "Refine VCOS with SAM2. In this experiment, we focus on refining the segmentation masks generated by the TSP-SAM [16] model for VCOS using SAM2. The initial masks produced by TSP-SAM are used as prompts to SAM2 for refining details. The TSP-SAM model segments each frame based on the preceding frames, progressively improving segmentation with each step. Therefore, we assume that the last frame in the video contains the most refined segmentation result, making it the best candidate for further refinement by SAM2, so we prompt the mask of the last frame of the video sequence for SAM2 refinement.\nThe Table 7 presents the results of this refinement process across different model sizes (large, b+, small, and tiny). Compared to the baseline (TSP-SAM), SAM2 clearly shows improvements on most metrics. For instance, using the large model for the refinement gives an Sm of 0.698 and mIoU of 0.408, marking improvements in structure and segmentation quality. Similar trends are observed across other models, with the tiny model also showing an increase in Sm from 0.689 to 0.694 and mIoU improving from 0.388 to 0.401. These results highlight that SAM2's advanced capabilities can clearly enhance the performance of VCOS models."}, {"title": "4.5 Fine-tune SAM2 on MoCA-Mask", "content": "We fine-tune the SAM2-Tiny model on the MoCA-Mask dataset for 50 epochs on a single V100-32GB GPU. When evaluating, we use the box-based prompt of the first frame as input, the results are shown in Table 8. We observe notable performance improvements. The mIoU increases by approximately 0.0291, from 0.6236 to 0.6527, representing a 2.9% improvement. Similarly, the mDice score sees a significant improvement of 0.0309, from 0.7251 to 0.7560, marking a 3.1% enhancement. The table further breaks down the improvements for different fine-tuning configurations. Fine-tuning both the image encoder and mask decoder simultaneously yields the best result. These results demonstrate the effectiveness of fine-tuning the image encoder and the mask decoder of SAM2-Tiny on the MOCA-Mask dataset for improving segmentation accuracy in VCOS."}, {"title": "4.6 Failure Cases Analysis", "content": "In our experiments, we identify several failure cases in SAM2's segmentation of camouflaged animals, using a 1-click prompt at the first frame, as shown in Fig. 1. These examples, including the copperhead snake, hedgehog, and stick insect, compared with the success cases (in Fig. 2), reveal challenges when dealing with complex camouflaged scenarios. Here, we analyze the key factors that contribute to the failure:\nLow Contrast with Background: The copperhead snake blends almost seamlessly with the surroundings, leading to an incomplete segmentation mask as SAM2 struggles to differentiate it from the background.\nOcclusions and Distracting Elements: The hedgehog case highlights SAM2's difficulty with occlusions caused by the cluttered environment, including branches and leaves, resulting in inaccurate object segmentation.\nThin and Complex Structures: The stick insect's fine details are not well captured, especially its thin legs, showing limitations in segmenting intricate structures.\nThese failure cases highlight the need for improved handling of low-contrast, cluttered environments, and complex structures in future adaptations of SAM2 for VCOS."}, {"title": "5 Conclusion", "content": "This study provides a systematic evaluation of the Segment Anything Model 2 (SAM2) in video camouflaged object segmentation across two popular datasets MOCA-Mask and CAD. Our experiments highlight several key findings:\nPrompt-Based Segmentation: SAM2 demonstrates notable performance improvements with different prompting strategies, such as clicks, boxes, and masks. Box-based and mask-based prompts consistently outperformed click-based prompts across both datasets, with prompting at the middle frame often yielding the best results. This indicates the importance of spatial details (box and mask) in guiding SAM2 for accurate segmentation of camouflaged objects.\nAutomatic Mode Performance: In automatic mode, SAM2 struggles with fully unsupervised segmentation in camouflaged scenes, achieving unsatisfactory mIoU and mDice scores. The results show that user guidance or"}]}