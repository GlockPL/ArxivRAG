{"title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning", "authors": ["Prateek Yadav", "Colin Raffel", "Mohammed Muqeeth", "Lucas Caccia", "Haokun Liu", "Leshem Choshen", "Tianlong Chen", "Mohit Bansal", "Alessandro Sordoni"], "abstract": "The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization. A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application. The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years. This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups. To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method. Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging. We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models. Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field.", "sections": [{"title": "1 Introduction", "content": "The development of large-scale pre-trained models increasingly aims to create general-purpose AI systems that can perform any task without requiring task-specific training. Improvements in these models are often driven by scale, i.e. training a larger model on a larger dataset (Hestness et al., 2017; Kaplan et al., 2020). However, even with increased scale these models are not yet truly \"general purpose\" and often struggle with certain tasks and/or domains (McCoy et al., 2023; Ling et al., 2023; Kandpal et al., 2023a). Unfortunately, pre-training a new model in hopes of improving capabilities can be incredibly compute-intensive (Li et al., 2023; Workshop et al., 2022) and is therefore impossible for most of the research and practitioner community. In addition to high computational costs, it can be difficult to localize which parameters of the model might be more useful for a specific use case and adaptively improve performance or reduce computation based on that information (Pfeiffer et al., 2023).\nFortunately, it is often possible to make targeted improvements to a pre-trained model via fine-tuning (i.e. further training on a specialized dataset). In addition, parameter-efficient fine-tuning (PEFT) techniques (Ding et al., 2022; He et al., 2021; Mangrulkar et al., 2022) further increase fine-tuning efficiency and decrease the cost of serving such specialized models. PEFT introduces small components like Low-Rank Adapters (Hu et al., 2022) or (IA)3 vectors (Liu et al., 2022) that surgically modify the original model while adding a negligible amount of parameters. Due to their compact size, these specialized PEFT modules"}, {"title": "2 A Taxonomy for MoErging Methods", "content": "Broadly, our survey focuses on \"MoErging\", a new paradigm for decentralized model development that aims to recycle expert models trained asynchronously by distributed contributors. We can organize the stages and components of MoErging methods into three categories: (1) experts, (2) routing, and (3) application. The experts are the specialized models that are trained and shared by individual contributors. Importantly, experts are trained independently, i.e. the expert contributors do not have access to one another's compute or data. Once expert models have been shared, MoErging methods perform routing, which aims to select and aggregate the contributor-provided expert models in order to improve performance or generalization. To process a given query or adapt to a target dataset, routing can operate in various ways, for example: (1) adaptively select a single expert model, (2) route different examples or processing steps to different experts, (3) learn a layer to extract relevant information from all experts, and/or (4) combine the expert models in an adaptive way. Some MoErging methods assume that the expert contributors share not only their expert models but also their training datasets so that they can be used to design or create the routing strategy. Finally, the aggregate system is applied to some particular use case, e.g. processing a query or solving a target task. Different MoErging methods are designed for different use-cases, including zero- or few-shot"}, {"title": "2.1 Expert model design choices", "content": "MoErging involves recycling specialized expert models. Contributors of the expert models do their training independently, i.e. without access to one another's data or compute, and subsequently share their models. Design choices for the expert models include:"}, {"title": "2.1.1 Expert Training", "content": "While contributors must train and share a model for it to be used as part of a MoErging system, a given MoErging method may further stipulate that the expert models are trained in a specific way. For example, PHATGOOSE (Muqeeth et al., 2024) requires that expert model training includes an additional stage where gates are trained that are later used for routing. If a MoErging method stipulates a specific expert training procedure, we label it as Custom; otherwise, we label it as Standard. We note that many MoErging methods require access to statistics of each expert training dataset (e.g. each expert training set's average activation at some particular layer). We consider this a modification because it would not otherwise be done as part of standard expert training."}, {"title": "2.1.2 Expert Data", "content": "A major motivation of the field of MoErging is to recycle the huge number of fine-tuned models being shared on model hubs. Such models are typically shared without their associated training data. However, certain MoErging methods assume access to expert training data, e.g. for learning the routing procedure. When expert data is shared, it is no longer a requirement that the experts must be trained independently. Furthermore, it would be possible to e.g. perform multitask training on all expert datasets simultaneously or carry out a modified expert training procedure. In the scenario where expert data needs to be shared, the sole benefit of MoErging methods is therefore the recycling of the compute required to train the expert models. In addition, apart from the reality that training data is often not shared alongside fine-tuned expert models, contributors may prefer to keep their training data private. We therefore categorize whether each method requires that expert training data is Shared or can remain Private."}, {"title": "2.2 Routing design choices", "content": "In MoErging, expert models are collected to create an aggregate system to improve performance or generalization. A key step in this process is to create a \"router\" that can adaptively choose which model(s) should be used for a particular query or dataset. The creation of the aggregate MoErging system involves a large range of design choices, including:"}, {"title": "2.2.1 Routing Dataset", "content": "To learn to route or select among expert models, MoErging methods often require a training dataset that we refer to as the \"routing\" dataset. Some MoErging methods make use of the Expert's training datasets for the routing dataset, while others assume access to a Target-task dataset or a General dataset that covers"}, {"title": "2.2.2 Routing Input Granularity", "content": "Different MoErging methods make routing decisions at different levels of granularity. At the finest level, routing can be done per-Step (e.g. choosing a different expert model for each of a language model's generated tokens). In addition, routing can be performed once for each Example or query, or a single expert model can be chosen for all examples from some particular Task."}, {"title": "2.2.3 Routing Depth Granularity", "content": "Parameter-efficient fine-tuning methods like LoRA (Hu et al., 2022) or (IA)\u00b3 (Liu et al., 2022) insert trainable modules at different layers throughout a model. Some MoErging methods therefore make per-Module routing decisions (i.e. with different routing decision at each layer where modules have been inserted, as in mixture-of-experts models (Shazeer et al., 2017)), while others make a single routing decision for the entire Model."}, {"title": "2.2.4 Expert Selection", "content": "When routing among experts, some MoErging methods make a Sparse selection (i.e. choosing only a subset of the experts) while others perform Dense routing (i.e. making use of all experts at once)."}, {"title": "2.2.5 Expert Aggregation", "content": "If a MoErging method selects more than one expert, it must aggregate the experts or their outputs in some way. Aggregation methods include mixing the Output of experts, combining the expert's Parameter values before processing inputs, or None for methods that perform no aggregation (e.g. because they select a single expert)."}, {"title": "2.3 Application design choices", "content": "Once the expert models have been recycled into an aggregate system, users can then apply the system to their tasks or queries of interest. Different MoErging methods produce systems that support different usage patterns and incur different requirements on applications. Relevant design choices include:"}, {"title": "2.3.1 Generalization", "content": "MoErging can aim to produce systems that improve performance on In-Distribution tasks (i.e. the tasks that the experts were trained on) or enable generalization to Out-of-Distribution tasks (i.e. those tasks for which there is no corresponding expert). However, many systems are applicable to both settings."}, {"title": "2.3.2 User Dataset", "content": "MoErging methods may require a training dataset in order to be applied to a target task, which may be a Few-Shot dataset with a small number of labeled examples or a Full dataset with many labeled examples. Other methods require no target-task training dataset (i.e. they can be applied Zero-Shot). We make a slight misnomer and also refer to MoErging methods where an unlabeled target-task training dataset is required as \"zero-shot\"."}, {"title": "3 A Survey of MoErging Methods", "content": "Having established our taxonomy, we now provide a detailed survey of a few dozen recent papers that propose and study MoErging methods. Precisely delineating what is and is not a MoErging method is challenging because many past methods share the same basic motivation but differ in their application and framing.\nHowever, we believe that the papers we cover in this section provide a reasonably comprehensive overview of MoErging and MoErging-adjacent methods. Notably, most of the papers we discuss here only cite a small fraction of the other papers, suggesting that there is a general lack of awareness about relevant papers. Our survey aims to address this gap in knowledge.\nFor each method described in this section, we include an \"infobox\" cataloging the design choices made by each method according to our taxonomy. These infoboxes provide a point of reference to quickly understand each method and how it relates to others. However, there are cases where a given paper does not cleanly map onto our taxonomy. In such cases, we may denote that a paper considers Multiple options for a given design choice or that some design choice is N/A (not applicable)."}, {"title": "3.1 AdapterFusion", "content": "Pfeiffer et al. (2021) propose a two-stage algorithm for sharing knowledge across task-specific adapters that consists of an extraction stage and a subsequent combination stage. In the extraction stage, the adapters (Houlsby et al., 2019) are trained independently on individual tasks. In the combination stage, a new fusion module is added to the top of all single-task adapters. The fusion module is a form of attention module (Vaswani et al., 2017), with its query from the input representation of adapters and the key and value from the output representation of the adapters. Then, the model trains only the fusion module parameters on a target task, therefore learning to combine all the individually trained adapters. Their experiment on 16 natural language understanding tasks shows in-distribution performance improvement on 12 tasks, compared to standard full model fine-tuning on the target task."}, {"title": "3.2 Retrieval of Experts", "content": "Jang et al. (2023) argue that multitask training may underperform individually trained task experts equipped with a retrieval mechanism. Their proposed retrieval step encodes unlabelled examples from the target task, compares it to data encoded from each training task, and assigns each target datapoint to a specific trained expert. The expert with the most datapoints assigned to it is retrieved. Experiments are conducted using T0-3B and its associated training and evaluation sets (Sanh et al., 2022). This retrieval approach is shown to outperform T0-3B. Moreover, for certain benchmarks there exists a single oracle expert that performs significantly better than multitask training, showing the potential for better performance with a better retriever."}, {"title": "3.3 AdapterSoup", "content": "Chronopoulou et al. (2023) combine different PEFT adapters trained independently over 21 website domains to enable few-shot transfer to novel domains. In order to select which domain adapters are the most relevant to the downstream task, the authors explore two approaches. The first uses a pretrained sentence-BERT (Reimers & Gurevych, 2019) representation averaged over 100 samples for each training domain and downstream task to compute a similarity metric. The second approach trains a gaussian mixture model using the representation of 100 samples from each training domain and then maps few-shot samples from the downstream task to their closest cluster. In either case, chosen adapters are retrieved and their parameter are averaged to produce an aggregate adapter for the downstream task. The authors show that both these"}, {"title": "3.4 \u03c0-Tuning", "content": "To transfer knowledge from similar tasks to a target task, Wu et al. (2023) make use of the Fisher Information Matrix (FIM)-based Task Vector method (Achille et al., 2019). Specifically, given a pool of adapters, they construct a new expert for a target task by finding the adapters whose FIM is among the top-k most similar and averaging weights (including a target task-specific adapter) according to FIM similarity. The experts and their interpolation weights are jointly optimized to improve the target task loss. They also introduce a zero-shot variant, where the single adapter with the highest FIM is picked. Their results show improvement in multiple language and vision tasks."}, {"title": "3.5 MixDA", "content": "Diao et al. (2023) propose a two-stage algorithm to transfer knowledge from self-supervised domain adapters to target tasks. The first stage involves training domain-specific adapters with masked language modeling objectives on unlabeled data. In addition, a mean-square-error auxiliary loss is added to maintain the similarity between output representations of the domain adapter and the base model's feedforward network. In the second stage, domain adapters are all added to the model and always activated. A series of MLP-sigmoid gates following the domain adapters control the weight to aggregate their outputs. This aggregated output is fed through a newly-introduced task adapter. Training in the second stage freezes the base model and domain adapters and updates the gates and task adapter."}, {"title": "3.6 Mo'LORA", "content": "Mo'LORA (Maxine, 2023) considers the case where a base LLM (specifically, Llama 2) is being fine-tuned on a diverse general dataset (specifically, Wizard-EvolInstruct70k (Xu et al., 2023)). To train specialized models, the generalist dataset is first clustered based on embeddings produced by a sentence transformer (Reimers & Gurevych, 2019) and a LoRA is trained on each cluster. Then, the cosine distance between the embedding of a given query and the cluster centroids is used to produce a routing distribution. The parameters of the LoRAs are then averaged, weighted according to the routing distribution, and the query is processed using the aggregate LORA."}, {"title": "3.7 LoraHub", "content": "Huang et al. (2024) train one LoRA expert per task on a collection of 200 tasks from the Flan collection (Longpre et al., 2023), starting from the Flan-T5-Large as the base model (Chung et al., 2024). The experts are used to test few-shot generalization on a suite of 27 tasks from BIG-Bench Hard (Suzgun et al., 2022).\nLoraHub performs routing in two steps: first, 20 adapters are chosen at random from the full set of 200 training adapters; then, for each new task, the authors learn a fixed routing distribution over the randomly chosen adapters using a gradient-free method over a small task-specific training dataset. The routing probabilities are used to compute a weighted average of the chosen adapters' parameters to create a single specialized adapter. LoraHub is therefore focused on few-shot out-of-distribution tasks, i.e. it evaluates performance on a separate set of tasks but requires task-specific training data to learn routing weights."}, {"title": "3.8 Airoboros and LlamaIndex", "content": "Airoboros (Durbin, 2024) is an open-source tool for generating and training on instruction tuning data. It includes functionality for selecting among a pool of expert models. Adaptive routing is supported in two ways: either by embedding 1,000 samples from each expert training dataset and retrieving the expert whose embedding is nearest to the query (composed of the system prompt and instruction) embedding via a FAISS index, or by asking an LLM which model to use for the query given a list of descriptions of each model. LlamaIndex (Liu, 2024) is an open-source library for connecting LLMs with data sources and other tools. Like airoboros, it includes functionality for building a model-level router by querying an LLM, with flexible choicses of the routing model and selection prompt."}, {"title": "3.9 Routing with Benchmark Datasets", "content": "Shnitzer et al. (2023) reuse a collection of benchmark datasets (specifically HELM, Liang et al., 2022) to determine routing among LLMs on an unseen dataset. Specifically, they hold out one dataset while using the remaining datasets, called \"benchmark data\", for learning the routing. The evaluation is performed on all the LLMs in the pool on the benchmark data, and they define the correctness of each LLM for a given query with a binary score indicating whether the LLM can provide an acceptable answer to the given query. They embed the benchmark data using a sentence embedder, and for a query from the holdout dataset, the averaged correctness score from the k nearest neighbors in the benchmark data is assigned as the score for this query and LLM. The average of all scores for all queries in the dataset is then taken to estimate how accurate an LLM is for the task. They propose three estimators: the first that takes the argmax of the previously computed correctness scores over all the queries of the dataset. The second estimator applies a threshold on the correctness score of samples when averaging over queries in the dataset and accounts only for those that cross the threshold. To address out-of-distribution tasks, a third proposed estimator takes into account unlabeled out-of-distribution test samples by estimating the probability that the per-test-sample correctness score is accurate. The estimator defaults to the best LLM on the benchmark in cases of low confidence."}, {"title": "3.10 Zooter", "content": "Lu et al. (2023a) propose Zooter, a learned router that aims to send each query to the best generalist language model (LM) within a pool of possible models. To train the router, predictions over a set of unlabelled instruction data are first collected for all LMs in the pool. The predictions are then scored by a reward model and the normalized scores across models are used as a training signal for the router. The router is kept relatively small (3 orders of magnitude smaller) compared to the LMs to keep routing cost"}, {"title": "3.11 Token-Level Adaptation of LoRA Adapters", "content": "Belofsky (2023) formulate a routing approach for independently-trained LoRA experts. After training experts on a small set of specialized tasks, they form an expert representation by leveraging the experts' training data. Specifically, for each dataset, they compute the centroid of the embeddings of the dataset prompts. At test time, they normalize the cosine similarities between the embedding of the prompt generated so far and the experts' embeddings and combine expert parameters based on the resulting weights. The granularity of their routing approach is step-level but the routing decisions are shared across layers, i.e. every new token is produced by a dense combination of the trained experts. The evaluation is performed on In-Distribution tasks, i.e. they use the test set of the same tasks the experts have been trained on."}, {"title": "3.12 CALM", "content": "Bansal et al. (2024) focus on composing knowledge from two models that can potentially have different architectures and sizes. Given an anchor model and an augmenting model, the goal is to have a final model that is good at the anchor task, augmenting task, and a \"composition\" task that corresponds to the composition of the anchor and augmenting tasks. To achieve this, CALM adds multiple cross-attention layers between the augmenting and anchor model which takes in the input activation from both models. Then, the output from this learned cross-attention layer is passed on to the anchor model. Both the anchor and augmentation models are frozen and the cross-attention layers are learned in an end-to-end manner on a mixture of anchor and augmenting task in order to improve performance on the composition task. They use PaLM2-XXS as the augmenting model and use PaLM2-XS or PaLM2-S as the anchor model (Chowdhery et al., 2023). CALM is shown to be effective in experiments including adding low-resource support to an English model and improving the coding of the anchor model."}, {"title": "3.13 What the Weight?", "content": "Holtermann et al. (2024) do not propose a new method for MoErging but instead introduce a framework under which they can perform experiments to better understand the various components and how they impact zero-shot compositional generalization. They frame such generalization as having three steps; (1) selecting a subset of experts, (2) deciding weights for each expert, (3) combining the different experts based on their weight. They experiment with five different types of scoring functions to select and weigh experts: uniform, sentence similarity, tf-idf, domain priors, and entropy. After selecting the scores and the experts they perform two different types of aggregation, parameter-level and ensembling the outputs. Their large-scale study produces various new insigts, including: ensembling generally yields better results than parameter averaging, good performance can be attained even with simple routing strategies, and that the number of chosen experts is more important than the precise weights assigned to them."}, {"title": "3.14 Routoo", "content": "Mohammadshahi et al. (2024) describe a system that trains a router to perform model-level routing among generalist LLMs of varying sizes and architectures. A fixed budget is provided and the final objective is to maximize the overall performance across all queries while adhering to the budget constraints. Router training is done using a dataset of (query, response, evaluation score) triplets collected over many possible models. Mohammadshahi et al. (2024) use labeled target-task examples (specifically from MMLU) to synthetically generate the router training dataset with self-play for iterative refinement. However, we note that the method could in principle be used in zero-shot settings."}, {"title": "3.15 Weight-Ensembling MoE", "content": "Tang et al. (2024c) argue that the interference when merging models should be dynamically resolved and hence design a MoErging method that averages all parameters except MLP layers which may contain more task-specific knowledge. They upcycle MLP layers into an MoE where each MLP from each expert model is converted to a task vector by subtracting the base model's parameters from the MLP layer's parameters. Routing is then performed between the expert MLP task vectors by multiplying the routing weights with the task vectors and then adding them back to the base MLP weight. Routing is done at the example level by taking the mean of all the token-level routing weights. Expert training data access is not required, but an unlabelled test dataset is used to learn the router by minimizing the routing distribution's entropy."}, {"title": "3.16 PHATGOOSE", "content": "Muqeeth et al. (2024) focus on zero-shot generalization to unseen tasks by reusing existing adapters that are trained using a slightly modified training procedure. For each training task, they first train a LoRA module and then they add a sigmoid gate before each module which learns the importance of each token for this task. To compute this importance score they compute the sigmoid of the similarity between the gate and the per-token representations. Finally, they optimize the task loss for a given expert to learn these gates. Once LoRA and gates for all tasks are trained independently, then they create an MoE-style model from these experts for performing zero-shot generalization to unseen tasks. Specifically, for each Lora module, they create a router by stacking and normalizing all the gates from different experts. Then they normalize the token representation and route the token to the experts corresponding to the top-2 most similar gates. Results on improving the zero-shot generalization of T5.1.1 demonstrate that this approach outperforms other methods for learning post-hoc routing and can sometimes match the performance of explicit multitask routing."}, {"title": "3.17 Lora Retriever", "content": "Zhao et al. (2024b) train a sentence embedding model to map from an input query into an embedding space that is then used to select an expert model to route the query to. The embedding space is constructed by"}, {"title": "3.18 LoRA-Flow", "content": "Wang et al. (2024) propose LoRA-Flow, which introduces a fusion gate at each layer of the transformer that processes the input at that layer and generates weights to perform a weighted average of the outputs from a set of pretrained LoRAs in the model. Some of these LoRAs are trained on multilingual language modeling, while others are task-specific and trained in English. These weights are generated for every token, making the routing token-level and layer-wise, with dense aggregation at the output level. Few-shot data from the downstream task of interest is used to learn this fusion gate, which comprises linear matrix and a bias vector. Experiments were conducted on math and coding abilities in a multilingual setting, specifically MGSM (Shi et al., 2022) for math and HumanEval translated into different languages for code. Their method outperforms LoraHub (section 3.7), which learns weights per task and averages LoRA parameters rather than outputs."}, {"title": "3.19 PEMT", "content": "Lin et al. (2024) propose a method to train a parameter efficient adaptation for a new task by utilizing adapters from other tasks. First, for each source task, they train both a learnable soft prompt (Lester et al., 2021) (initialized with a task description as in Raffel et al. (2020)) and an adapter. Then, for a target task, they initialize a soft prompt via an attention-style mechanism using the embedded target task description as a key and the source task prompts as keys and values. A task correlation matrix is constructed via a similar process, and a separate gating network is trained at each layer taking the correlation matrix as input. The gating network learns a weighting for computing an average of the source task adapters' outputs. Finally, a new target-task adapter is trained on downstream task data along with the gating network, soft prompt, and normalization parameters. This pipeline is shown to outperform other methods for recycling adapters such as SPOT (Vu et al., 2021) and ATTEMPT (Asai et al., 2022)."}, {"title": "3.20 Co-LLM", "content": "Shen et al. (2024) trains a binary classifier on the top of a base model's last hidden state to determine when a base model should defer token generation to a frozen large model, facilitating collaboration between models. Given training data for a task, pseudo-labels are generated by evaluating both models and labeling instances where the large model predicts the correct next token while the base model does not. This data is used to train the classifier's parameters and is further used as initialization in the later stage when the classifier and base model are fine-tuned on the task. During inference, a threshold is set on the classifier using validation data that decides when to defer to the large expert model. This collaborative approach yields better results compared to fine-tuning the base model alone or using the frozen large model independently in instruction following, math, reasoning, and biomedical tasks."}, {"title": "3.21 Branch-Train-Mix", "content": "Sukhbaatar et al. (2024) fine-tune each LLM on four different domains starting from a seed LLM (Llama 7B (Touvron et al., 2023)) to create expert LMs for each domain. They propose combining the FFNS of each expert LM to form an MoE, as in Lepikhin et al. (2020); Du et al. (2022), and averaging other parameters from each expert LM. The resultant model is fine-tuned on a training mixture corresponding to all the domains. During inference, top-2 routing is used at each MoE layer. They evaluate on downstream tasks in zero-shot and few-shot settings corresponding to each domain and find that their method performs comparably to the best domain expert LM for that task. Their method also performs comparably to a compute-matched counterpart, where the seed model is scaled to be similar size to the final model by upcycling (Komatsuzaki et al., 2022) and trained using multitask data."}, {"title": "3.22 Dynamic Adapter Merging", "content": "Dynamic Adapter Merging (DAM, Cheng et al., 2024) leverages domain-specific adapters of a base model to perform domain-incremental learning in the context of video question answering (VidQA). DAM first computes each domain-specific training set's average embedding from the penultimate layer of the base model. The distances between a given query input's embedding and the dataset average embeddings are then normalized to create a routing distribution. Finally, the query is processed by merging the domain-specific adapters using per-adapter weights set according to the routing distribution. On standard VidQA benchmarks, DAM significantly outperforms continual learning on the domain-specific datasets and nearly matches the performance of multitask training. However, the use of the base model to embed the input roughly doubles the computational cost."}, {"title": "3.23 Mixture of LoRA Experts (MoLE)", "content": "Wu et al. (2024) note that directly merging LoRA modules can degrade capabilities. They therefore aim to train routers to aggregate and reweight outputs from LoRAs at each layer where they have been introduced. Router training is performed on downstream data with the rest of the model (base model and LoRA parameters) fixed. In addition to a standard domain-specific loss, MoLE include a load balancing loss that aims to encourage the router to assign weight to all LoRAs. During inference, MoLE considers the cases where all LORA outputs are used and where some LoRAs are manually removed. Experimental results include an analysis of performing model, layer, or module-level routing that demonstrates that module-level gating networks result in the best performance."}, {"title": "3.24 Arrow", "content": "Ostapenko et al. (2024) explore methods to build and reuse a library of expert LoRAs for zero-shot task generalization. The proposed solution builds a MoE-like architecture, where the different experts are dynamically selected according to the input. To build a router in a zero-shot manner, the authors add a"}, {"title": "3.25 MeteoRA", "content": "Xu et al. (2024) propose an efficient method to dynamically select between multiple LoRA adapters. In each layer, a learned gating mechanism chooses a predetermined number of LoRAs to be activated for each token. The gating is learned by freezing the network and learning next token prediction over the same datasets used to train the experts. In addition to the architectural change, various engineering choices are made to ensure efficient parallelization of the gating choices, ultimately leading to substantial speedups. While MeteoRA is initially validated on in-distribution tasks with a labeled dataset, it could in principle be applied to out-of-distribution tasks."}, {"title": "3.26 PWE MOE", "content": "Tang et al. (2024b) extend WE MOE (covered in section 3.15) to settings where Pareto-optimal performance is desired on a set of tasks. Task importance is set according to a user-specified \u201cpreference vector\" (whose entries are nonnegative and sum to 1) that designates which tasks are more or less important. As in WE MOE, specialized models are upcycled (Komatsuzaki et al., 2022) into an MoE-style model by merging non-feed-forward network parameters via task vector arithmetic (Ilharco et al., 2022). Routers among the feed-forward networks are trained by sampling random preference vectors and optimizing standard losses that capture Pareto optimality over the expert tasks. Routing based on preference vector-specified task weighting is shown to outperform merging methods that use the preference vector to set model weights."}, {"title": "3.27 RouteLLM", "content": "Ong et al. (2024) aim to reduce inference costs while maintaining performance by dynamically routing each input to either a strong or a weak LLM. RouteLLM learns a router that estimates the probability that the stronger model will outperform the weaker one on a specific metric for a given input and select the weaker model if the probability is below a given threshold. The router is learned using a combination of preference data from Chatbot Arena (Chiang et al., 2024), instruction-tuning data such as Nectar (Zhu et al., 2023) and data with gold labels, such as MMLU (Hendrycks et al., 2020). RouteLLM is then evaluated on MMLU, MT-Bench (Zheng et al., 2023) and GSM8K Cobbe et al. (2021). Given that MMLU is both used for learning the routing and for evaluation, we described the Routing Dataset as Multiple, to denote that it comprises some examples from the target evaluation tasks and other generic tasks. The paper shows that RouteLLM can learn to choose between GPT-4 and Mixtral effectively, lowering inference costs. In some settings, it"}]}