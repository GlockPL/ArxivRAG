{"title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning", "authors": ["Prateek Yadav", "Colin Raffel", "Mohammed Muqeeth", "Lucas Caccia", "Haokun Liu", "Leshem Choshen", "Tianlong Chen", "Mohit Bansal", "Alessandro Sordoni"], "abstract": "The availability of performant pre-trained models has led to a proliferation of fine-tuned\nexpert models that are specialized to a particular domain or task. Model MoErging meth-\nods aim to recycle expert models to create an aggregate system with improved performance\nor generalization. A key component of MoErging methods is the creation of a router that\ndecides which expert model(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development of many new\nmethods over the past few years. This rapid pace of development has made it challenging\nto compare different MoErging methods, which are rarely compared to one another and\nare often validated in different experimental setups. To remedy such gaps, we present a\ncomprehensive survey of MoErging methods that includes a novel taxonomy for cataloging\nkey design choices and clarifying suitable applications for each method. Apart from sur-\nveying MoErging research, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model merging, multitask\nlearning, and mixture-of-experts models. Taken as a whole, our survey provides a unified\noverview of existing MoErging methods and creates a solid foundation for future work in\nthis burgeoning field.", "sections": [{"title": "1 Introduction", "content": "The development of large-scale pre-trained models increasingly aims to create general-purpose AI systems\nthat can perform any task without requiring task-specific training. Improvements in these models are often\ndriven by scale, i.e. training a larger model on a larger dataset (Hestness et al., 2017; Kaplan et al., 2020).\nHowever, even with increased scale these models are not yet truly \"general purpose\" and often struggle with\ncertain tasks and/or domains (McCoy et al., 2023; Ling et al., 2023; Kandpal et al., 2023a). Unfortunately,\npre-training a new model in hopes of improving capabilities can be incredibly compute-intensive (Li et al.,\n2023; Workshop et al., 2022) and is therefore impossible for most of the research and practitioner community.\nIn addition to high computational costs, it can be difficult to localize which parameters of the model might\nbe more useful for a specific use case and adaptively improve performance or reduce computation based on\nthat information (Pfeiffer et al., 2023).\nFortunately, it is often possible to make targeted improvements to a pre-trained model via fine-tuning\n(i.e. further training on a specialized dataset). In addition, parameter-efficient fine-tuning (PEFT) tech-\nniques (Ding et al., 2022; He et al., 2021; Mangrulkar et al., 2022) further increase fine-tuning efficiency and\ndecrease the cost of serving such specialized models. PEFT introduces small components like Low-Rank\nAdapters (Hu et al., 2022) or (IA)3 vectors (Liu et al., 2022) that surgically modify the original model while\nadding a negligible amount of parameters. Due to their compact size, these specialized PEFT modules\n1"}, {"title": "2 A Taxonomy for MoErging Methods", "content": "Broadly, our survey focuses on \"MoErging\", a new paradigm for decentralized model development that aims\nto recycle expert models trained asynchronously by distributed contributors. We can organize the stages\nand components of MoErging methods into three categories: (1) experts, (2) routing, and (3) application.\nThe experts are the specialized models that are trained and shared by individual contributors. Importantly,\nexperts are trained independently, i.e. the expert contributors do not have access to one another's compute\nor data. Once expert models have been shared, MoErging methods perform routing, which aims to select\nand aggregate the contributor-provided expert models in order to improve performance or generalization.\nTo process a given query or adapt to a target dataset, routing can operate in various ways, for example: (1)\nadaptively select a single expert model, (2) route different examples or processing steps to different experts,\n(3) learn a layer to extract relevant information from all experts, and/or (4) combine the expert models in\nan adaptive way. Some MoErging methods assume that the expert contributors share not only their expert\nmodels but also their training datasets so that they can be used to design or create the routing strategy.\nFinally, the aggregate system is applied to some particular use case, e.g. processing a query or solving a\ntarget task. Different MoErging methods are designed for different use-cases, including zero- or few-shot\n1See e.g. https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n2"}, {"title": "2.1 Expert model design choices", "content": "MoErging involves recycling specialized expert models. Contributors of the expert models do their training\nindependently, i.e. without access to one another's data or compute, and subsequently share their models.\nDesign choices for the expert models include:"}, {"title": "2.1.1 Expert Training", "content": "While contributors must train and share a model for it to be used as part of a MoErging system, a given\nMoErging method may further stipulate that the expert models are trained in a specific way. For example,\nPHATGOOSE (Muqeeth et al., 2024) requires that expert model training includes an additional stage where\ngates are trained that are later used for routing. If a MoErging method stipulates a specific expert training\nprocedure, we label it as Custom; otherwise, we label it as Standard. We note that many MoErging\nmethods require access to statistics of each expert training dataset (e.g. each expert training set's average\nactivation at some particular layer). We consider this a modification because it would not otherwise be done\nas part of standard expert training."}, {"title": "2.1.2 Expert Data", "content": "A major motivation of the field of MoErging is to recycle the huge number of fine-tuned models being\nshared on model hubs. Such models are typically shared without their associated training data. However,\ncertain MoErging methods assume access to expert training data, e.g. for learning the routing procedure.\nWhen expert data is shared, it is no longer a requirement that the experts must be trained independently.\nFurthermore, it would be possible to e.g. perform multitask training on all expert datasets simultaneously\nor carry out a modified expert training procedure. In the scenario where expert data needs to be shared,\nthe sole benefit of MoErging methods is therefore the recycling of the compute required to train the expert\nmodels. In addition, apart from the reality that training data is often not shared alongside fine-tuned expert\nmodels, contributors may prefer to keep their training data private. We therefore categorize whether each\nmethod requires that expert training data is Shared or can remain Private."}, {"title": "2.2 Routing design choices", "content": "In MoErging, expert models are collected to create an aggregate system to improve performance or general-\nization. A key step in this process is to create a \"router\" that can adaptively choose which model(s) should\nbe used for a particular query or dataset. The creation of the aggregate MoErging system involves a large\nrange of design choices, including:"}, {"title": "2.2.1 Routing Dataset", "content": "To learn to route or select among expert models, MoErging methods often require a training dataset that we\nrefer to as the \"routing\" dataset. Some MoErging methods make use of the Expert's training datasets for\nthe routing dataset, while others assume access to a Target-task dataset or a General dataset that covers\n4"}, {"title": "2.2.2 Routing Input Granularity", "content": "Different MoErging methods make routing decisions at different levels of granularity. At the finest level,\nrouting can be done per-Step (e.g. choosing a different expert model for each of a language model's generated\ntokens). In addition, routing can be performed once for each Example or query, or a single expert model\ncan be chosen for all examples from some particular Task."}, {"title": "2.2.3 Routing Depth Granularity", "content": "Parameter-efficient fine-tuning methods like LoRA (Hu et al., 2022) or (IA)\u00b3 (Liu et al., 2022) insert train-\nable modules at different layers throughout a model. Some MoErging methods therefore make per-Module\nrouting decisions (i.e. with different routing decision at each layer where modules have been inserted, as in\nmixture-of-experts models (Shazeer et al., 2017)), while others make a single routing decision for the entire\nModel."}, {"title": "2.2.4 Expert Selection", "content": "When routing among experts, some MoErging methods make a Sparse selection (i.e. choosing only a subset\nof the experts) while others perform Dense routing (i.e. making use of all experts at once)."}, {"title": "2.2.5 Expert Aggregation", "content": "If a MoErging method selects more than one expert, it must aggregate the experts or their outputs in some\nway. Aggregation methods include mixing the Output of experts, combining the expert's Parameter values\nbefore processing inputs, or None for methods that perform no aggregation (e.g. because they select a single\nexpert)."}, {"title": "2.3 Application design choices", "content": "Once the expert models have been recycled into an aggregate system, users can then apply the system to\ntheir tasks or queries of interest. Different MoErging methods produce systems that support different usage\npatterns and incur different requirements on applications. Relevant design choices include:"}, {"title": "2.3.1 Generalization", "content": "MoErging can aim to produce systems that improve performance on In-Distribution tasks (i.e. the tasks\nthat the experts were trained on) or enable generalization to Out-of-Distribution tasks (i.e. those tasks\nfor which there is no corresponding expert). However, many systems are applicable to both settings."}, {"title": "2.3.2 User Dataset", "content": "MoErging methods may require a training dataset in order to be applied to a target task, which may be a\nFew-Shot dataset with a small number of labeled examples or a Full dataset with many labeled examples.\nOther methods require no target-task training dataset (i.e. they can be applied Zero-Shot). We make\na slight misnomer and also refer to MoErging methods where an unlabeled target-task training dataset is\nrequired as \"zero-shot\"."}, {"title": "3 A Survey of MoErging Methods", "content": "Having established our taxonomy, we now provide a detailed survey of a few dozen recent papers that propose\nand study MoErging methods. Precisely delineating what is and is not a MoErging method is challenging\nbecause many past methods share the same basic motivation but differ in their application and framing.\n5"}, {"title": "3.1 Adapter Fusion", "content": "Pfeiffer et al. (2021) propose a two-stage algorithm for sharing knowledge across task-specific adapters that\nconsists of an extraction stage and a subsequent combination stage. In the extraction stage, the adapters\n(Houlsby et al., 2019) are trained independently on individual tasks. In the combination stage, a new fusion\nmodule is added to the top of all single-task adapters. The fusion module is a form of attention module\n(Vaswani et al., 2017), with its query from the input representation of adapters and the key and value from\nthe output representation of the adapters. Then, the model trains only the fusion module parameters on\na target task, therefore learning to combine all the individually trained adapters. Their experiment on 16\nnatural language understanding tasks shows in-distribution performance improvement on 12 tasks, compared\nto standard full model fine-tuning on the target task."}, {"title": "3.2 Retrieval of Experts", "content": "Jang et al. (2023) argue that multitask training may underperform individually trained task experts equipped\nwith a retrieval mechanism. Their proposed retrieval step encodes unlabelled examples from the target task,\ncompares it to data encoded from each training task, and assigns each target datapoint to a specific trained\nexpert. The expert with the most datapoints assigned to it is retrieved. Experiments are conducted using\nT0-3B and its associated training and evaluation sets (Sanh et al., 2022). This retrieval approach is shown\nto outperform T0-3B. Moreover, for certain benchmarks there exists a single oracle expert that performs\nsignificantly better than multitask training, showing the potential for better performance with a better\nretriever."}, {"title": "3.3 AdapterSoup", "content": "Chronopoulou et al. (2023) combine different PEFT adapters trained independently over 21 website domains\nto enable few-shot transfer to novel domains. In order to select which domain adapters are the most\nrelevant to the downstream task, the authors explore two approaches. The first uses a pretrained sentence-\nBERT (Reimers & Gurevych, 2019) representation averaged over 100 samples for each training domain and\ndownstream task to compute a similarity metric. The second approach trains a gaussian mixture model\nusing the representation of 100 samples from each training domain and then maps few-shot samples from the\ndownstream task to their closest cluster. In either case, chosen adapters are retrieved and their parameter\nare averaged to produce an aggregate adapter for the downstream task. The authors show that both these\n6"}, {"title": "3.4 \u03c0-Tuning", "content": "To transfer knowledge from similar tasks to a target task, Wu et al. (2023) make use of the Fisher Information\nMatrix (FIM)-based Task Vector method (Achille et al., 2019). Specifically, given a pool of adapters, they\nconstruct a new expert for a target task by finding the adapters whose FIM is among the top-k most similar\nand averaging weights (including a target task-specific adapter) according to FIM similarity. The experts\nand their interpolation weights are jointly optimized to improve the target task loss. They also introduce a\nzero-shot variant, where the single adapter with the highest FIM is picked. Their results show improvement\nin multiple language and vision tasks."}, {"title": "3.5 MixDA", "content": "Diao et al. (2023) propose a two-stage algorithm to transfer knowledge from self-supervised domain adapters\nto target tasks. The first stage involves training domain-specific adapters with masked language modeling\nobjectives on unlabeled data. In addition, a mean-square-error auxiliary loss is added to maintain the\nsimilarity between output representations of the domain adapter and the base model's feedforward network.\nIn the second stage, domain adapters are all added to the model and always activated. A series of MLP-\nsigmoid gates following the domain adapters control the weight to aggregate their outputs. This aggregated\noutput is fed through a newly-introduced task adapter. Training in the second stage freezes the base model\nand domain adapters and updates the gates and task adapter."}, {"title": "3.6 Mo'LORA", "content": "Mo'LORA (Maxine, 2023) considers the case where a base LLM (specifically, Llama 2) is being fine-tuned\non a diverse general dataset (specifically, Wizard-EvolInstruct70k (Xu et al., 2023)). To train specialized\nmodels, the generalist dataset is first clustered based on embeddings produced by a sentence transformer\n(Reimers & Gurevych, 2019) and a LoRA is trained on each cluster. Then, the cosine distance between\nthe embedding of a given query and the cluster centroids is used to produce a routing distribution. The\nparameters of the LoRAs are then averaged, weighted according to the routing distribution, and the query\nis processed using the aggregate LORA."}, {"title": "3.7 LoraHub", "content": "Huang et al. (2024) train one LoRA expert per task on a collection of 200 tasks from the Flan collection\n(Longpre et al., 2023), starting from the Flan-T5-Large as the base model (Chung et al., 2024). The experts\nare used to test few-shot generalization on a suite of 27 tasks from BIG-Bench Hard (Suzgun et al., 2022).\n7"}, {"title": "3.8 Airoboros and LlamaIndex", "content": "LoraHub performs routing in two steps: first, 20 adapters are chosen at random from the full set of 200 train-\ning adapters; then, for each new task, the authors learn a fixed routing distribution over the randomly chosen\nadapters using a gradient-free method over a small task-specific training dataset. The routing probabilities\nare used to compute a weighted average of the chosen adapters' parameters to create a single specialized\nadapter. LoraHub is therefore focused on few-shot out-of-distribution tasks, i.e. it evaluates performance on\na separate set of tasks but requires task-specific training data to learn routing weights.\nAiroboros (Durbin, 2024) is an open-source tool for generating and training on instruction tuning data. It\nincludes functionality for selecting among a pool of expert models. Adaptive routing is supported in two\nways: either by embedding 1,000 samples from each expert training dataset and retrieving the expert whose\nembedding is nearest to the query (composed of the system prompt and instruction) embedding via a FAISS\nindex, or by asking an LLM which model to use for the query given a list of descriptions of each model.\nLlamaIndex (Liu, 2024) is an open-source library for connecting LLMs with data sources and other tools.\nLike airoboros, it includes functionality for building a model-level router by querying an LLM, with flexible\nchoicses of the routing model and selection prompt."}, {"title": "3.9 Routing with Benchmark Datasets", "content": "Shnitzer et al. (2023) reuse a collection of benchmark datasets (specifically HELM, Liang et al., 2022) to\ndetermine routing among LLMs on an unseen dataset. Specifically, they hold out one dataset while using\nthe remaining datasets, called \"benchmark data\", for learning the routing. The evaluation is performed on\nall the LLMs in the pool on the benchmark data, and they define the correctness of each LLM for a given\nquery with a binary score indicating whether the LLM can provide an acceptable answer to the given query.\nThey embed the benchmark data using a sentence embedder, and for a query from the holdout dataset, the\naveraged correctness score from the k nearest neighbors in the benchmark data is assigned as the score for\nthis query and LLM. The average of all scores for all queries in the dataset is then taken to estimate how\naccurate an LLM is for the task. They propose three estimators: the first that takes the argmax of the\npreviously computed correctness scores over all the queries of the dataset. The second estimator applies a\nthreshold on the correctness score of samples when averaging over queries in the dataset and accounts only\nfor those that cross the threshold. To address out-of-distribution tasks, a third proposed estimator takes\ninto account unlabeled out-of-distribution test samples by estimating the probability that the per-test-sample\ncorrectness score is accurate. The estimator defaults to the best LLM on the benchmark in cases of low\nconfidence."}, {"title": "3.10 Zooter", "content": "Lu et al. (2023a) propose Zooter, a learned router that aims to send each query to the best generalist\nlanguage model (LM) within a pool of possible models. To train the router, predictions over a set of\nunlabelled instruction data are first collected for all LMs in the pool. The predictions are then scored by\na reward model and the normalized scores across models are used as a training signal for the router. The\nrouter is kept relatively small (3 orders of magnitude smaller) compared to the LMs to keep routing cost\n8"}, {"title": "3.11 Token-Level Adaptation of LoRA Adapters", "content": "Belofsky (2023) formulate a routing approach for independently-trained LoRA experts. After training experts\non a small set of specialized tasks, they form an expert representation by leveraging the experts' training\ndata. Specifically, for each dataset, they compute the centroid of the embeddings of the dataset prompts. At\ntest time, they normalize the cosine similarities between the embedding of the prompt generated so far and\nthe experts' embeddings and combine expert parameters based on the resulting weights. The granularity of\ntheir routing approach is step-level but the routing decisions are shared across layers, i.e. every new token\nis produced by a dense combination of the trained experts. The evaluation is performed on In-Distribution\ntasks, i.e. they use the test set of the same tasks the experts have been trained on."}, {"title": "3.12 CALM", "content": "Bansal et al. (2024) focus on composing knowledge from two models that can potentially have different\narchitectures and sizes. Given an anchor model and an augmenting model, the goal is to have a final\nmodel that is good at the anchor task, augmenting task, and a \"composition\" task that corresponds to\nthe composition of the anchor and augmenting tasks. To achieve this, CALM adds multiple cross-attention\nlayers between the augmenting and anchor model which takes in the input activation from both models.\nThen, the output from this learned cross-attention layer is passed on to the anchor model. Both the anchor\nand augmentation models are frozen and the cross-attention layers are learned in an end-to-end manner\non a mixture of anchor and augmenting task in order to improve performance on the composition task.\nThey use PaLM2-XXS as the augmenting model and use PaLM2-XS or PaLM2-S as the anchor model\n(Chowdhery et al., 2023). CALM is shown to be effective in experiments including adding low-resource\nsupport to an English model and improving the coding of the anchor model."}, {"title": "3.13 What the Weight?", "content": "Holtermann et al. (2024) do not propose a new method for MoErging but instead introduce a framework\nunder which they can perform experiments to better understand the various components and how they\nimpact zero-shot compositional generalization. They frame such generalization as having three steps; (1)\nselecting a subset of experts, (2) deciding weights for each expert, (3) combining the different experts based\non their weight. They experiment with five different types of scoring functions to select and weigh experts:\nuniform, sentence similarity, tf-idf, domain priors, and entropy. After selecting the scores and the experts\nthey perform two different types of aggregation, parameter-level and ensembling the outputs. Their large-\nscale study produces various new insigts, including: ensembling generally yields better results than parameter\naveraging, good performance can be attained even with simple routing strategies, and that the number of\nchosen experts is more important than the precise weights assigned to them.\n9"}, {"title": "3.14 Routoo", "content": "Mohammadshahi et al. (2024) describe a system that trains a router to perform model-level routing among\ngeneralist LLMs of varying sizes and architectures. A fixed budget is provided and the final objective is to\nmaximize the overall performance across all queries while adhering to the budget constraints. Router training\nis done using a dataset of (query, response, evaluation score) triplets collected over many possible models.\nMohammadshahi et al. (2024) use labeled target-task examples (specifically from MMLU) to synthetically\ngenerate the router training dataset with self-play for iterative refinement. However, we note that the method\ncould in principle be used in zero-shot settings."}, {"title": "3.15 Weight-Ensembling MoE", "content": "Tang et al. (2024c) argue that the interference when merging models should be dynamically resolved and\nhence design a MoErging method that averages all parameters except MLP layers which may contain more\ntask-specific knowledge. They upcycle MLP layers into an MoE where each MLP from each expert model\nis converted to a task vector by subtracting the base model's parameters from the MLP layer's parameters.\nRouting is then performed between the expert MLP task vectors by multiplying the routing weights with\nthe task vectors and then adding them back to the base MLP weight. Routing is done at the example level\nby taking the mean of all the token-level routing weights. Expert training data access is not required, but\nan unlabelled test dataset is used to learn the router by minimizing the routing distribution's entropy."}, {"title": "3.16 PHATGOOSE", "content": "Muqeeth et al. (2024) focus on zero-shot generalization to unseen tasks by reusing existing adapters that are\ntrained using a slightly modified training procedure. For each training task, they first train a LoRA module\nand then they add a sigmoid gate before each module which learns the importance of each token for this\ntask. To compute this importance score they compute the sigmoid of the similarity between the gate and\nthe per-token representations. Finally, they optimize the task loss for a given expert to learn these gates.\nOnce LoRA and gates for all tasks are trained independently, then they create an MoE-style model from\nthese experts for performing zero-shot generalization to unseen tasks. Specifically, for each Lora module,\nthey create a router by stacking and normalizing all the gates from different experts. Then they normalize\nthe token representation and route the token to the experts corresponding to the top-2 most similar gates.\nResults on improving the zero-shot generalization of T5.1.1 demonstrate that this approach outperforms\nother methods for learning post-hoc routing and can sometimes match the performance of explicit multitask\nrouting."}, {"title": "3.17 Lora Retriever", "content": "Zhao et al. (2024b) train a sentence embedding model to map from an input query into an embedding space\nthat is then used to select an expert model to route the query to. The embedding space is constructed by\n10"}, {"title": "3.18 LoRA-Flow", "content": "Wang et al. (2024) propose LoRA-Flow, which introduces a fusion gate at each layer of the transformer\nthat processes the input at that layer and generates weights to perform a weighted average of the outputs\nfrom a set of pretrained LoRAs in the model. Some of these LoRAs are trained on multilingual language\nmodeling, while others are task-specific and trained in English. These weights are generated for every token,\nmaking the routing token-level and layer-wise, with dense aggregation at the output level. Few-shot data\nfrom the downstream task of interest is used to learn this fusion gate, which comprises linear matrix and a\nbias vector. Experiments were conducted on math and coding abilities in a multilingual setting, specifically\nMGSM (Shi et al., 2022) for math and HumanEval translated into different languages for code. Their method\noutperforms LoraHub (section 3.7), which learns weights per task and averages LoRA parameters rather than\noutputs."}, {"title": "3.19 PEMT", "content": "Lin et al. (2024) propose a method to train a parameter efficient adaptation for a new task by utilizing\nadapters from other tasks. First, for each source task, they train both a learnable soft prompt (Lester et al.,\n2021) (initialized with a task description as in Raffel et al. (2020)) and an adapter. Then, for a target task,\nthey initialize a soft prompt via an attention-style mechanism using the embedded target task description as\na key and the source task prompts as keys and values. A task correlation matrix is constructed via a similar\nprocess, and a separate gating network is trained at each layer taking the correlation matrix as input. The\ngating network learns a weighting for computing an average of the source task adapters' outputs. Finally,\na new target-task adapter is trained on downstream task data along with the gating network, soft prompt,\nand normalization parameters. This pipeline is shown to outperform other methods for recycling adapters\nsuch as SPOT (Vu et al., 2021) and ATTEMPT (Asai et al., 2022)."}, {"title": "3.20 Co-LLM", "content": "Shen et al. (2024) trains a binary classifier on the top of a base model's last hidden state to determine when a\nbase model should defer token generation to a frozen large model, facilitating collaboration between models.\nGiven training data for a task, pseudo-labels are generated by evaluating both models and labeling instances\nwhere the large model predicts the correct next token while the base model does not. This data is used to\ntrain the classifier's parameters and is further used as initialization in the later stage when the classifier and\nbase model are fine-tuned on the task. During inference, a threshold is set on the classifier using validation\ndata that decides when to defer to the large expert model. This collaborative approach yields better results\ncompared to fine-tuning the base model alone or using the frozen large model independently in instruction\nfollowing, math, reasoning, and biomedical tasks.\n11"}, {"title": "3.21 Branch-Train-Mix", "content": "Sukhbaatar et al. (2024) fine-tune each LLM on four different domains starting from a seed LLM (Llama\n7B (Touvron et al., 2023)) to create expert LMs for each domain. They propose combining the FFNS\nof each expert LM to form an MoE, as in Lepikhin et al. (2020); Du et al. (2022), and averaging other\nparameters from each expert LM. The resultant model is fine-tuned on a training mixture corresponding to\nall the domains. During inference, top-2 routing is used at each MoE layer. They evaluate on downstream\ntasks in zero-shot and few-shot settings corresponding to each domain and find that their method performs\ncomparably to the best domain expert LM for that task. Their method also performs comparably to a\ncompute-matched counterpart, where the seed model is scaled to be similar size to the final model by\nupcycling (Komatsuzaki et al., 2022) and trained using multitask data."}, {"title": "3.22 Dynamic Adapter Merging", "content": "Dynamic Adapter Merging (DAM, Cheng et al., 2024) leverages domain-specific adapters of a base model\nto perform domain-incremental learning in the context of video question answering (VidQA). DAM first\ncomputes each domain-specific training set's average embedding from the penultimate layer of the base\nmodel. The distances between a given query input's embedding and the dataset average embeddings are\nthen normalized to create a routing distribution. Finally, the query is processed by merging the domain-\nspecific adapters using per-adapter weights set according to the routing distribution. On standard VidQA\nbenchmarks, DAM significantly outperforms continual learning on the domain-specific datasets and nearly\nmatches the performance of multitask training. However, the use of the base model to embed the input\nroughly doubles the computational cost."}, {"title": "3.23 Mixture of LoRA Experts (MoLE)", "content": "Wu et al. (2024) note that directly merging LoRA modules can degrade capabilities. They therefore aim to\ntrain routers to aggregate and reweight outputs from LoRAs at each layer where they have been introduced.\nRouter training is performed on downstream data with the rest of the model (base model and LoRA param-\neters) fixed. In addition to a standard domain-specific loss, MoLE include a load balancing loss that aims\nto encourage the router to assign weight to all LoRAs. During inference, MoLE considers the cases where\nall LORA outputs are used and where some LoRAs are manually removed. Experimental results include\nan analysis of performing model, layer, or module-level routing that demonstrates that module-level gating\nnetworks result in the best performance."}, {"title": "3.24 Arrow", "content": "Ostapenko et al. (2024) explore methods to build and reuse a library of expert LoRAs for zero-shot task\ngeneralization. The proposed solution builds a MoE-like architecture, where the different experts are dy-\nnamically selected according to the input. To build a router in a zero-shot manner, the authors add a\n12"}, {"title": "3.25 MeteoRA", "content": "Xu et al. (2024) propose an efficient method to dynamically select between multiple LoRA adapters. In each\nlayer, a learned gating mechanism chooses a predetermined number of LoRAs to be activated for each token.\nThe gating is learned by freezing the network and learning next token prediction over the same datasets\nused to train the experts. In addition to the architectural change, various engineering choices are made\nto ensure efficient parallelization of the gating choices, ultimately leading to substantial speedups. While\nMeteoRA is initially validated on in-distribution tasks with a labeled dataset, it could in principle be applied\nto out-of-distribution tasks."}, {"title": "3.26 PWE MOE", "content": "Tang et al. (2024b) extend WE MOE (covered in section 3.15) to settings where Pareto-optimal performance\nis desired on a set of tasks. Task importance is set according to a user-specified \u201cpreference vector\" (whose\nentries are nonnegative and sum to 1) that designates which tasks are more or less important. As in\nWE MOE, specialized models are upcycled (Kom"}]}