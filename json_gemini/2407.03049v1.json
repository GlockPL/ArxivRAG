{"title": "Enhancements for Real-Time Monte-Carlo Tree Search in General Video Game Playing", "authors": ["Dennis J. N. J. Soemers", "Chiara F. Sironi", "Torsten Schuster", "Mark H. M. Winands"], "abstract": "General Video Game Playing (GVGP) is a field of Artificial Intelligence where agents play a variety of real-time video games that are unknown in advance. This limits the use of domain-specific heuristics. Monte-Carlo Tree Search (MCTS) is a search technique for game playing that does not rely on domain-specific knowledge. This paper discusses eight enhancements for MCTS in GVGP; Progressive History, N-Gram Selection Technique, Tree Reuse, Breadth-First Tree Initialization, Loss Avoidance, Novelty-Based Pruning, Knowledge-Based Evaluations, and Deterministic Game Detection. Some of these are known from existing literature, and are either extended or introduced in the context of GVGP, and some are novel enhancements for MCTS. Most enhancements are shown to provide statistically significant increases in win percentages when applied individually. When combined, they increase the average win percentage over sixty different games from 31.0% to 48.4% in comparison to a vanilla MCTS implementation, approaching a level that is competitive with the best agents of the GVG-AI competition in 2015.", "sections": [{"title": "I. INTRODUCTION", "content": "General Video Game Playing (GVGP) [1] is a field of Artificial Intelligence in games where the goal is to develop agents that are able to play a variety of real-time video games that are unknown in advance. It is closely related to General Game Playing (GGP) [2], which focuses on abstract games instead of video games. The wide variety of games in GGP and GVGP makes it difficult to use domain-specific knowledge, and promotes the use of generally applicable techniques.\nThere are two main frameworks for GVGP. The first framework is the Arcade Learning Environment (ALE) [3] for developing agents that can play games of the Atari 2600 console. The second framework is GVG-AI [4], which can run any real-time video game described in a Video Game Description Language [5], [6]. This paper focuses on the GVG-AI framework.\nThe GVG-AI framework is used in the GVG-AI Competition [4], [7]. Past competitions only ran a Planning Track, where agents were ranked based on their performance in single-player games. In 2016, it is planned to extend this with a 2/N-Player Track, a Learning Track, and a Procedural Content Generation Track. This paper focuses on the Planning Track.\nMonte-Carlo Tree Search (MCTS) [8], [9] is a popular technique in GGP [10] because it does not rely on domain-specific knowledge. MCTS has also performed well in GVGP in 2014 [4], which was the first year of the GVG-AI competition, but was less dominant in 2015 [7]. This paper discusses and evaluates eight enhancements for MCTS to improve its performance in GVGP: Progressive History, N-Gram Selection Technique, Tree Reuse, Breadth-First Tree Initialization, Loss Avoidance, Novelty-Based Pruning, Knowledge-Based Evaluations and Deterministic Game Detection.\nThe remainder of the paper is structured as follows. Section II provides background information on the GVG-AI framework and the GVG-AI competition. MCTS is discussed in Section III. In Section IV, the enhancements for MCTS in GVGP are explained. Section V describes the experiments to assess the enhancements. Finally, the paper is concluded in Section VI and ideas for future research are discussed."}, {"title": "II. GVG-AI FRAMEWORK AND COMPETITION", "content": "In the GVG-AI competition [4], [7], agents play a variety of games that are unknown in advance. Agents are given 1 second of processing time at the start of every game, and 40 milliseconds of processing time per tick. A tick can be thought of as a turn in an abstract game. Every tick, the agent can choose an action to play, and at the end of the tick the chosen action is played and the game state progresses. Every game has a duration of at most 2000 ticks, after which the game is a loss. Other than that, different games have different termination conditions, which define when the agent wins or loses. Every game in GVG-AI contains at least an avatar object, which is the \"character\" controlled by the agent. Games can also contain many other types of objects. Games in GVG-AI are fully observable and can be nondeterministic.\nAgents can perform searches and attempt to learn which actions are good using the Forward Model, consisting of two important functions; advance and copy. Given a game state $s_t$, the advance(a) function can be used to generate a successor state $s_{t+1}$, which represents one of the possible states that can be reached by playing an action a. In deterministic games, there is only one such state $s_{t+1}$ for every action a, but in nondeterministic games there can be more than one. The copy($s_t$) function creates a copy of $s_t$. This function is required when it is desirable to generate multiple possible successors of $s_t$, because every call to advance modifies the original state, and there is no undo function. Because the framework supports a wide variety of different games, it is not optimized as well as any framework dedicated to a specific game would be. This means that the advance and copy operations tend to be significantly slower than equivalent functions in individual game implementations."}, {"title": "III. MONTE-CARLO TREE SEARCH", "content": "Monte-Carlo Tree Search (MCTS) [8], [9] is a best-first search algorithm that gradually builds up a search tree and uses Monte-Carlo simulations to approximate the value of game states. To handle nondeterministic games with probabilistic models that are not exposed to the agent, an \"open-loop\u201d [11] implementation of MCTS is used. In an open-loop approach, the root node represents the current game state ($s_0$), every edge represents an action, and every other node n represents the set of game states that can be reached by playing the sequence of actions corresponding to the path from the root node to n, starting from $s_0$.\nMCTS is initialized with only the root node. Next, until some computational budget expires, the algorithm repeatedly executes simulations. Every simulation consists of the following four steps [12]:\nIn the Selection step, a selection policy is applied recursively, starting from the root node, until a node is reached that is not yet fully expanded (meaning that it currently has fewer successors than available actions). The selection policy determines which part of the tree built up so far is evaluated in more detail. It should provide a balance between exploitation of parts of the search tree that are estimated to have a high value so far, and exploration of parts of the tree that have not yet been visited frequently. The most commonly implemented selection policy is UCB1 [8], [13], which selects the successor $S_i$ of the current node P that maximizes Equation 1. $S_i$ and P are nodes, which can represent sets of states.\n$UCB1(S_i) = Q(S_i) + C \\times \\sqrt{\\frac{ln(n_p)}{N_i}}$\n$Q(S_i) \\in [0, 1]$ denotes the normalized average score backpropagated through $S_i$ so far (as described below), C is a parameter where higher values lead to more exploration, and $n_p$ and $n_i$ denote the visit counts of P and $S_i$, respectively.\nIn the Play-out step, the simulation is continued, starting from the last state encountered in the selection step, using a (semi-)random play-out policy. The most straightforward implementation is to randomly draw actions to play from a uniform distribution until a terminal game state is reached. In GVGP, this is typically not feasible, and a maximum play-out depth is used to end play-outs early.\nIn the Expansion step, the tree is expanded by adding one or more nodes. The most common implementation adds one node to the tree per simulation; the node corresponding to the first action played in the play-out step. In this paper, the tree is simply expanded by adding the whole play-out to the tree. The number of simulations per tick tends to be low enough in GVG-AI that there is no risk of running out of memory. Therefore, to keep all information gathered, all nodes are stored in memory.\nIn the Backpropagation step, the outcome of the final state of the simulation is backpropagated through the tree. Let $s_T$ be the final state of the simulation. Next, an evaluation X($s_T$) of the state is added to a sum of scores stored in every node on the path from the root node to the final node of the simulation, and the visit counts of the same nodes are incremented. Because it is not feasible to let all simulations continue until terminal states are reached in GVG-AI, it is necessary to use some evaluation function for non-terminal states. A basic evaluation function that is also used by the sample MCTS controllers included in the GVG-AI framework is given by Equation 2.\nX(s_T) =\\begin{cases}  10^7 + score(s_T) & \\text{if } s_T \\text{ is a winning state} \\\\  -10^7 + score(s_T) & \\text{if } s_T \\text{ is a losing state} \\\\ score(s_T) & \\text{if } s_T \\text{ is a non-terminal state}  \\end{cases}\nscore($s_T$) is the game score value of a state $s_T$ in GVG-AI. In some games a high game score value can indicate that the agent is playing well, but this is not guaranteed in all games. Finally, the action leading to the node with the highest average score is played when the computational budget expires."}, {"title": "IV. MCTS ENHANCEMENTS FOR GVGP", "content": "There is a wide variety of existing enhancements for the MCTS algorithm, many of which are described in [14]. This section discusses a number of enhancements that have been evaluated in GVGP; Progressive History, N-Gram Selection Technique, Tree Reuse, Breadth-First Tree Initialization, Loss Avoidance, Novelty-Based Pruning, Knowledge-Based Evaluations, and Deterministic Game Detection. Some are known from existing research, and some are new."}, {"title": "A. Progressive History and N-Gram Selection Technique", "content": "Progressive History (PH) [15] and N-Gram Selection Technique (NST) [16] are two existing enhancements for the selection and play-out steps of MCTS, respectively. The basic idea of PH and NST is to introduce a bias in the respective steps towards playing actions, or sequences of actions, that performed well in earlier simulations. Because the value of playing an action in GVG-AI typically depends greatly on the current position of the avatar, this position is also taken into account when storing data concerning the previous performance of actions. For a detailed description of these enhancements we refer to the original publications [15], [16]."}, {"title": "B. Tree Reuse", "content": "Suppose that a search tree was built up by MCTS in a previous game tick t \u2212 1 \u2265 0, and an action $a_{t-1}$ was played. The entire subtree rooted in the node corresponding to that action can still be considered to be relevant for the new search process in the current tick t. Therefore, instead of initializing MCTS with only a root node, it can be initialized with a part of the tree built in the previous tick. This was previously found to be useful in the real-time game of Ms Pac-Man [17]. This idea has also previously been suggested in the context of GVGP [11], but, to the best of our knowledge, the effect of this enhancement on the performance of MCTS in GVGP has not yet been evaluated.\nIn nondeterministic games, it is possible that the new root (which was previously a direct successor of the previous root) represented more than one possible game state. In the current tick, it is known exactly which of those possible states has been reached. Therefore, some of the old results in this tree are no longer relevant. For this reason, all the scores and visit counts in the tree are decayed by multiplying them by a decay factor \u03b3 \u2208 [0,1] before starting the next MCTS procedure. Tree Reuse (TR) with \u03b3 = 0 completely resets the accumulated scores and visit counts of nodes (but still retains the nodes, and therefore the structure of the generated tree), and TR with \u03b3 = 1 does not decay old results."}, {"title": "C. Breadth-First Tree Initialization and Safety Prepruning", "content": "In some of the games supported by the GVG-AI framework, the number of MCTS simulations that can be executed in a single tick can be very small; sometimes smaller than the number of available actions. In such a situation, MCTS behaves nearly randomly, and is susceptible to playing actions that lead to a direct loss, even when there are actions available that do not directly lose the game.\nTheoretically this problem could be avoided by adjusting the limit of the play-out depth of MCTS to ensure that a sufficient number of simulations can be done. In practice, this can be problematic because it requires a low initial depth limit to ensure that it is not too high at the start of a game, and this can in turn be detrimental in games where it is feasible and beneficial to run a larger number of longer play-outs.\nWe propose to handle this problem using Breadth-First Tree Initialization. The idea is straightforward; before starting MCTS, the direct successors of the root node are generated by a 1-ply Breadth-First Search. Every action available in the root state is executed up to a number M times to deal with nondeterminism, and the resulting states are evaluated. The average of these M evaluations is backpropagated for every successor with a weight equal to a single MCTS simulation. MCTS is only started after this process. When MCTS starts, every direct successor of the root node already has a prior evaluation that can be used to avoid playing randomly in cases with an extremely small number of simulations. The M states generated for every successor are cached in the corresponding nodes, so that they can be re-used in the subsequent MCTS process. This reduces the computational overhead of the enhancement.\nSafety prepruning, originally used in an algorithm called Iterated Width [18], has been integrated in this process. The idea of safety prepruning is to count the number of immediate game losses among the M generated states for each action, and only keep the actions leading to nodes with the minimum observed number of losses. All other actions are pruned."}, {"title": "D. Loss Avoidance", "content": "In GVGP, many games have a high number of losing game states that are relatively easy to avoid. An example of such a game is Frogs, where the avatar is a frog that should cross a road and a river. The road contains trucks that cause a loss upon collision, but can easily be avoided because they move at a constant speed. The river contains logs that also move at a constant speed, which the frog should jump on in order to safely cross the river.\nThe (semi-)random play used in the play-out step of MCTS is likely to frequently run into losing game states in situations like this. This leads to a negative evaluation of nodes that do in fact lead to a winning position. This is only corrected when sufficient simulations have been run such that the selection step of MCTS correctly biases the majority of the simulations towards a winning node. With a low simulation count in GVG-AI, MCTS is likely to repeatedly play the rightmost action in Figure 4, which only delays the game until it is lost due to reaching the maximum game duration.\nThis problem is similar to the problem of traps [19] or optimistic moves [20] in (two-player) adversarial games. In those cases, MCTS has an overly optimistic evaluation of some states, whereas in the cases discussed here it has an overly pessimistic evaluation of some states. In [21], it was proposed to integrate shallow minimax searches inside some of the steps of MCTS to improve its performance in game trees with traps or optimistic moves. Using minimax searches to prove wins or losses is difficult in GVGP because games can be nondeterministic, but a similar idea can be used to get less pessimistic evaluations.\nIn this paper, an idea named Loss Avoidance (LA) is proposed for GVGP. The idea of LA is to try to ignore losses by immediately searching for a better alternative whenever a loss is encountered the first time a node is visited. Whenever the play-out step of MCTS ends in a losing game state, that result is not backpropagated as would commonly be done in MCTS. Instead, one state is generated for every sibling of the last node, and only the evaluation of the node with the highest evaluation is backpropagated. All generated nodes are still added to the tree, and store their own evaluation in memory.\nLA causes MCTS to keep an optimistic initial view of the value of nodes. This tends to work well in the single-player games of GVG-AI, where it is often possible to reactively get out of dangerous situations. It is unlikely to work well in, for instance, adversarial games, where a high concentration of losses in a subtree typically indicates that an opposing player has more options to win and is likely in a stronger position.\nIn an open-loop implementation of MCTS, LA can have a significant amount of computational overhead in game trees with many losses. For instance, in the Frogs game it roughly halves the average number of MCTS simulations per tick. This is because the node prior to the node with the losing game state does not store the corresponding game state in memory, which means that all states generated in the selection and play-out steps need to be re-generated by playing the same action sequence from the root node. In nondeterministic games this process can also lead to finding a terminal state before the full action sequence has been executed again. To prevent spending too much time in the same simulation, the LA process is not started again, but the outcome of that state is backpropagated."}, {"title": "E. Novelty-Based Pruning", "content": "The concept of novelty tests was first introduced in the Iterated Width algorithm (IW) [18], [22]. In IW, novelty tests are used for pruning in Breadth-First Search (BrFS). Whenever a state s is generated in a BrFS, a novelty measure (described in more detail below) nov(s) is computed. This is a measure of the extent to which s is \u201cnew\u201d with respect to all previously generated states. States with a lower measure are \"more novel\" than states with a higher measure [22]. The original IW algorithm consists of a sequence of calls to IW(0), IW(1), etc., where IW(i) is a BrFS that prunes a state s if nov(s) > i. In GVGP, it was found that it is only feasible to run a single IW(i) iteration [18]. The best results were obtained with IW(1), and a variant named IW(2) (see [18] for details).\nThe definition of the novelty measure nov(s) of a state s requires s to be defined in terms of a set of boolean features. An example of a boolean feature that can be a part of a state is a predicate at(cell, type), which is true in s if and only if there is an object of the given type in the given cell in s. Then, nov(s) is defined as the size of the smallest tuple of features that are all true in s, and not all true in any other state generated previously in the same search process. If there is no such tuple, s must be an exact copy of some previously generated state, and nov(s) is defined as n + 1, where n is the number of features that are defined. For example, suppose that in s, at((x, y), i) = true, and in all previously generated states, at((x, y), i) = false. Then, nov(s) = 1, because there is a tuple of size 1 of features that were not all true in any previously generated state.\nIW(1) prunes any state s with nov(s) > 1. In this paper, Novelty-Based Pruning (NBP) is proposed as an idea to prune nodes based on novelty tests in MCTS. The goal is not to prune bad lines of play, but to prune redundant lines of play. MCTS often generates states deep in the tree before other states close to the root. For instance, the last state of the first play-out is much deeper in the tree than the first state of the second play-out. This is an important difference with the BrFS used by IW. It means that the novelty measure nov(s) of a state s should be redefined in such a way that it not necessarily uses all previously generated states, but only a specific set of states, referred to as the neighborhood N(s) of s.\nN(s) is the union of four sets of states. The first set consists of the siblings on the \"left\" side of s. The ordering of the states matters, but can be arbitrary (as in a BrFS). The second set contains only the parent p(s) of s. The third set consists of all siblings of p(s). The fourth set is the neighborhood of p(s). More formally, let $s_i$ denote the $i^{th}$ successor of a parent p($s_i$). Then, N($s_i$) is defined as N($s_i$) = {$s_1, s_2, ...,s_{i-1}$}\u222a {p($s_i$)}USib(p($s_i$))UN(p($s_i$)), where Sib(p($s_i$)) denotes the set of siblings of p($s_i$). For the root state r, N(r) = Sib(r) = \u00d8.\nUsing the above definition of N(s), nov(s, N(s)) is defined as the size of the smallest tuple of features that are all true in s, and not all true in any other state in the set N(s). The novelty tests are used in MCTS as follows. Let n be a node with a list of successors Succ(n). The first time that the selection step reaches n when it is fully expanded, all successors Succ(n) are novelty tested based on a single state generated per node, using a threshold of 1 for the novelty tests (as in IW(1)). The same boolean features are used to define states in GVG-AI as described in [18]. Nodes are marked as not being novel if they fail the novelty test. Whenever all successors of a node are marked as not novel, that node itself is also marked as not novel. There are a few exceptions where nodes are not marked. If a state has a higher game score than the parent, it is always considered to be novel. Additionally, states transitioned into by playing a movement action are always considered to be novel in games where either only horizontal, or only vertical movement is available (because these games often require moving back and forth which can get incorrectly pruned by NBP otherwise), and in games where the avatar has a movement speed \u2264 0.5 (because slow movement does not result in the avatar reaching a new cell every tick, and is therefore not detected by the cell-based boolean features).\nIn the selection step of MCTS, when one of the successors Succ(n) of n should be selected, any successor $n' \\in Succ(n)$ is ignored if it is marked as not novel, unless the average normalized score Q(n) < 0.5. In such cases, the situation is considered to be dangerous and all alternatives should be considered to see if a better position can be found. For the final selection of the move to play in the real game, non-novel nodes are also only considered if the best novel alternative has a normalized average score < 0.5.\nWhen the successors Succ(n) have been novelty tested, every node $n_i \\in Succ(n)$ stores a set of tuples of features that were all true in the states generated for the purpose of novelty testing for the nodes {$n$} U Succ(n). This means that the tuples of features that are true in the neighborhood N(s) of a state s can be reconstructed relatively efficiently by traversing the path from s back to the root, and collecting the tuples in the stored sets. This is the main reason for defining N(s) as described above. Including more states would require also traversing back down the tree to collect more sets of tuples."}, {"title": "F. Knowledge-Based Evaluations", "content": "An important problem with MCTS in GVG-AI is that it is often infeasible to find any terminal states, or even states with a change in game score. This means that the evaluation function in Equation 2 often returns the same value for all states generated in the same tick, and MCTS explores the search space and behaves randomly. In this paper, a heuristic evaluation function is proposed that uses knowledge collected during simulations, and distances to objects that could potentially be interesting, to distinguish between states that have identical evaluations according to Equation 2. The basic idea is not new; some agents in the competition of 2014 used distance-based evaluation functions [4]. A similar idea is also described in [23], and extended in [24]. The idea discussed here is based on the same intuition, but a number of implementation details are different. Another related idea is described in [25], where MCTS is used to learn which objects are interesting, and a pathfinding algorithm is used to move towards a selected goal.\nLet X(so) denote the evaluation of the current game state so, and let X(sT) denote the evaluation of the final state sT of a play-out. If X(sT) = X(so), a heuristic evaluation EvalKB(sT) is computed and added to X(sT). For every object type i observed in a game, let d0(i) denote the distance from the avatar to the closest object of type i in so, and let dT(i) denote the distance from the avatar to the closest object of type i in sT. These distances are computed using the A* pathfinding algorithm [26]. The pathfinding algorithm takes objects of the wall type into account as obstacles. Many games can also contain other objects that block movement, or portals that can be used for teleportation. These objects are not taken into account, because the agent would first need to learn how these objects influence pathfinding. For every object type i, a weight wi is used to reward or punish the agent for moving to objects of that type. This is done by computing EvalKB(sT) as given by Equation 3, normalizing it to lie in [0,0.5], and adding it to X(sT) if otherwise X(sT) = X(so).\nEvalKB(s_T) = \\sum w_i \\times (d_0(i) - d_T(i))\nObject types i with a small absolute weight (|wi| < 10\u22124) are ignored, to save the computational cost of pathfinding."}, {"title": "G. Deterministic Game Detection", "content": "The idea of Deterministic Game Detection (DGD) is to detect when a game is likely to be deterministic, and treat deterministic games differently from nondeterministic games. At the start of every game, M random sequences of actions of length N are generated. Each of the M sequences is used to advance a copy of the initial game state so, with R repetitions per sequence. If any of the M action sequences did not result in equivalent states among the R repetitions for that sequence, the game is classified as nondeterministic. Additionally, any game in which NPCs are observed is immediately classified as nondeterministic. Any other game is classified as deterministic. In this paper, M = N = 5 and R = 3.\nMany participants in previous GVG-AI competitions [7] used a similar idea to switch to a different algorithm for deterministic games (for instance, using Breadth-First Search in deterministic games and MCTS in nondeterministic games). In this paper, DGD is only used to modify MCTS and the TR and NBP enhancements in deterministic games. The Q($S_i$) term in Equation 1 (or the equivalent term in the formula of PH) is replaced by $\\frac{3}{4}xQ(S_i) + \\frac{1}{4}xQ_{max}(S_i)$, where Qmax($S_i$) is the maximum score observed in the subtree rooted in $S_i$"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "Eight enhancements for Monte-Carlo Tree Search (MCTS) in General Video Game Playing (GVGP) have been discussed and evaluated. Most of them have been shown to significantly (95% confidence) increase the average win percentage over sixty different games when added individually to MCTS. All the enhancements combined increase the win percentage of our basic MCTS implementation from 31.0\u00b11.2 to 48.4\u00b11.5. This final performance is relatively close to the win percentage of the winner of the IEEE CEEC 2015 conference; YBCRIBER, with a win percentage of 52.4 \u00b1 1.3.\nMany of the discussed enhancements have parameters, which so far have only been tuned according to short, preliminary experiments. These parameters can likely be tuned better in future work to improve the performance. Loss Avoidance (LA) and Novelty-Based Pruning (NBP) as proposed in this paper have binary effects, in that LA backpropagates only one result from multiple generated siblings and NBP classifies nodes as either novel or not novel. Perhaps these can be improved by making them less binary. The overall performance of the agent can also likely be improved by incorporating more features that are commonly seen among the top entries in past competitions, such as the use of influence maps [30]. Finally, some of the new enhancements for MCTS, such as LA and NBP, can be evaluated in domains other than GVG-AI."}]}