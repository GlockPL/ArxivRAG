{"title": "Do Large Language Models with Reasoning and Acting Meet the Needs of Task-Oriented Dialogue?", "authors": ["Michelle Elizabeth", "Morgan Veyret", "Miguel Couceiro", "Ond\u0159ej Du\u0161ek", "Lina M. Rojas-Barahona"], "abstract": "Large language models (LLMs) gained immense popularity due to their impressive capabilities in unstructured conversations. However, they underperform compared to previous approaches in task-oriented dialogue (TOD), wherein reasoning and accessing external information are crucial. Empowering LLMs with advanced prompting strategies such as reasoning and acting (ReAct) (Yao et al., 2022) has shown promise in solving complex tasks traditionally requiring reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs performing TOD. We evaluate ReAct-based LLMs (ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs seem to underperform state-of-the-art approaches in simulation, human evaluation indicates higher user satisfaction rate compared to handcrafted systems despite having a lower success rate.", "sections": [{"title": "1 Introduction", "content": "Task-oriented Dialogue (TOD) systems can solve tasks, such as accessing information or booking places and tickets, by interacting with humans in natural language (Budzianowski et al., 2018; Rastogi et al., 2020). Distinct approaches for TOD have been explored. Traditional pipelines integrate specialized components for natural language understanding (NLU), dialogue state tracking (DST), dialogue management and natural language generation (NLG), and optionally speech recognition and synthesis (Ultes et al., 2017). On the other hand, end-to-end architectures utilize neural networks (Wen et al., 2017; Zhu et al., 2020, 2022). Both approaches are costly to develop, requiring burdensome engineering and collecting large dialogue corpora. Large language models (LLMs) (Ouyang et al., 2022) offer an alternative to this by generalizing from instructions or a small number of examples and promise fluent and natural replies. However, unlike traditional LLM scenarios, task-oriented dialogues typically have a rigid structure and require access to an external database to retrieve necessary information, such as venues or objects to search for and their properties.\nRecently, synergizing reasoning and acting in LLMS (ReAct) (Yao et al., 2022) has shown promising results in controlled tasks that need external information access. ReAct employs few-shot LLM prompting with a sequence of thoughts, actions, and observations. Thoughts refer to internal reasoning that decomposes a problem into sub-problems. Actions execute external API calls or programs, and observations analyze the results of actions. In this work, we investigate the ability of LLMs guided by ReAct to solve task-oriented dialogue. In particular:\n\u2022 We implement two ReAct-based systems for TOD, using GPT-3.5 and GPT-4 LLMs respectively (OpenAI, 2023), for the MultiWOZ tourist information domains (Budzianowski et al., 2018).\n\u2022 We evaluate our ReAct-LLM systems both with a simulated user and with humans. Additionally, we assess the trade-off between the performance and the cost of LLM APIs.\n\u2022 Our results show that ReAct-LLMs underperform state-of-the-art baselines in terms of success rate in simulation. However, humans rate their conversation with the ReAct-LLM system higher than the baseline. Although the baseline is better at achieving goals, the users prefer talking to the ReAct-LLM bot."}, {"title": "2 Related Work", "content": "A variety of approaches from handcrafted (HDC) to reinforcement learning (RL) have been proposed for the dialogue manager, which is in charge of decision making (Casanueva et al., 2018; Weisz et al., 2018a). The combination of deep RL with imitation learning (Cordier et al., 2020) as well as structural RL have also been applied to multi-domain, multi-task dialogue (Chen et al., 2018; Cordier et al., 2022). However, these approaches require separate specialized components, involving extensive engineering, the need for semantically annotated data as well as user simulators operating at the semantic level.\nWith the introduction of neural models, end-to-end approaches emerged (Wen et al., 2017). The latest architectures are built on top of pretrained language models and involve two-step generation: the model first generates the dialogue state or database query based on user input; then, it generates the reply based on external database search (Peng et al., 2021; Lin et al., 2020). These approaches provide more flexibility and potentially better fluency, but require even larger training corpora.\nRecent approaches explore simple LLM prompting for TOD using few-shot examples of relevant dialogue turns (Hude\u010dek and Dusek, 2023) or even full conversation snippets (Zhang et al., 2023). In contrast, in this work we propose to use the ReAct strategy to guide LLMs towards task oriented dialogue. Since dialogue is dynamic and evaluation on static data in single-turn replies may not be consistent with full dialogue performance (Takanobu et al., 2020), we evaluate the system on full dialogues - first in a simulated environment, then with humans. Unlike previous works for dialogue management (Weisz et al., 2018b; Zhu et al., 2020; Cordier et al., 2022), the simulator and our system do not interact at the semantic level, instead they interact in natural language. Unlike previous end-to-end and LLM-based approaches, which generated delexicalized\u00b9 responses, to ensure that entities exist in the database, we generate full responses including entity names. This avoids constraining the inherent capabilities of LLMs in generating natural language. Instead, we guide the LLM towards the database constraints through ReAct prompting."}, {"title": "3 ReAct for Task-Oriented Dialogue", "content": "Figure 1 shows the proposed architecture: the ReAct-LLM system agent interacts with a user in natural language. The system agent has access to external tools to guide it through the TOD pipeline.\nWe provide few-shot examples in the prompt following ReAct (Yao et al., 2022). We give the ReAct-LLM agent a list of tools:"}, {"title": "4 Experimental Setup", "content": "We use the LangChain\u00b2 library for implementing ReAct-LLM. The prompt details are given in Appendix A, Figure 2. We use Langfuse\u00b3 for debugging the reasoning traces and to keep track of the computational costs of our experiments. We experiment with OpenAI GPT-3.5 (gpt-3.5-turbo-0301) and GPT-4 (gpt-4-32k) models. We first couple our"}, {"title": "4.1 Simulated User", "content": "We first implemented an LLM-based user agent. However, the LLM was not able to end the conversation correctly and occasionally switched its role to play the system, which resulted in inappropriate conversations (see Appendix C). Therefore, we use the agenda-based user simulator (Schatzmann et al., 2007) implementation in CONVLAB 3 (Zhu et al., 2022). It implements a goal generator in agreement with the MultiWOZ dataset, which is used to initialize the agenda. The simulator then generates the semantic representation and converts it into natural language. This is fed to the system agent and the response from the system is sent to the BERT-NLU of the simulator that returns its semantic representation. This semantic representation is in turn used to update the agenda and thus the goal. The simulator then generates the next utterance based on the system response and the updated agenda."}, {"title": "4.2 Evaluation Setup", "content": "To measure how well the user goals were satisfied by the system, we compute the standard metrics: success, book, inform and complete rates as well as the average number of turns\u2074, using CONVLAB 3 (Zhu et al., 2022). A dialogue is successful if the system provided the right information and was able to book the requested entities"}, {"title": "5 Results", "content": "We present in this section the assessment of both simulated and real users."}, {"title": "5.1 Simulated Evaluation", "content": "Table 1 compares ReAct-LLM systems with previous works in CONVLAB. Note that systems in the first section of the table interact at the semantic level and hence, the metrics show the upper bound of the performance that can be achieved by a full TOD system. We observe that ACGOS performs the best only in terms of inform rate. The HDC"}, {"title": "5.2 Human Evaluation", "content": "Volunteers were asked to chat online with a system, either the HDC baseline or our ReAct-LLM (GPT-3.5), which was randomly assigned. They could start a conversation as many times as they wish. We collected 95 dialogues for each system. Table 3 shows that HDC performs better in terms of success rate, but not by the same margin it had in simulation. We see that the HDC system falls short in the human evaluation compared to the user simulation. Contrary to the simulated evaluation, ReAct-LLM performs much better with real users. Overall, the users are more satisfied with ReAct-LLM than with HDC, despite the better success rate of HDC. We also see the React-LLM system has a slightly lower average number of turns when compared with the simulated evaluation, while the opposite is true for HDC."}, {"title": "5.3 Qualitative Analysis", "content": "By inspecting a sample of the generated dialogues, we identify several issues. First, we see that the reasoning traces may just be imitating the examples given in the prompt. This may work for simpler cases with fewer goals to achieve. However, when the goals get larger with multiple domains and the user requests become more complicated, ReAct-LLM struggles to understand the user and to perform tasks accordingly. Beyond that, the reasoning is inconsistent and strays from instructions at times. Furthermore, the LLM can come up with creative responses, but struggles to stay within the bounds set by the instructions, often producing invalid dialogue states or not sticking to the set of external tools given."}, {"title": "6 Conclusion", "content": "The performance of ReAct-LLM falls short compared to HDC and RL baselines. The baselines perform better mainly due to their fine-grained control at each step in the pipeline. By relying completely on the reasoning abilities of ReAct-LLM, we lose the ability to control its reasoning traces and response generation. Additionally, difficulty in understanding the system requests by the simulator, due to BERT-NLU errors, leads to repeated utterances and thus a higher number of turns on average. On the other hand, our human evaluation shows that ReAct-LLM is preferred by users over the HDC baseline, despite its lower success rate."}, {"title": "7 Limitations", "content": "The user agent might be penalizing the ReAct-LLM system in the user-simulator evaluation, because the BERT-NLU component might fail to correctly detect the system's intentions."}, {"title": "8 Ethical Considerations", "content": "Due to ethical concerns about work conditions, we did not use crowdsourcing for the human evaluation. Instead, volunteers from our research institution, who were not involved in this work but were aware of the scientific goal, participated without economic incentives. This approach minimized pressure and reduced evaluation bias as they were unaware of the models' nature."}, {"title": "A The ReAct Prompt", "content": "Figure 2 shows an example of the final ReAct prompt, namely Generic Prompt, in which the examples provided in the prompt (Figure 3) contained a random example from the MultiWOZ dataset. We also experimented with another variation (i.e., Domain Specific), in which the examples provided were dynamically changed based on the domains in the goal. We observe that using domain-specific examples in the prompt has no effect on the performance of the system. Our results (see Table 4) show that the system in fact performs slightly better when there is only one random example irrespective of the domains of the user goal."}, {"title": "B Qualitative Analysis", "content": "In this section, we look at the dialogues generated in simulation to identify what the system did well and what it lacks when performing task-oriented dialogue. We randomly selected 50 dialogues from the 1000 simulations for GPT-3.5 using generic examples (cf. Appendix A), and we look at the dialogues from the perspective of the system.\nSystem produces creative responses but does not stick to the instructions An advantage of using LLMs for dialogue tasks is that the system is able to rephrase its response in cases where the user repeats the request. This can be seen in the example in Figure 4. However, it should be noted that the system does not have access to a tool that can help it retrieve the details of the booking. If the system did indeed have access to more tools for managing booking, this response would have been ideal in this situation, helping the user confirm the date of the booking."}, {"title": "C LLM User Agent Issues", "content": "This section looks at the conversation from a user's perspective to identify the reasons the conversation"}, {"title": "D Manual Analysis of GPT-3.5 vs GPT-4", "content": "We compared the generated outputs for both GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) for 50 randomly selected goals. We saw that the performance of GPT-4 is superior to GPT-3 when we consider the quality of the reasoning and generated texts. We explain a few of the reasons why we found GPT-4 to be a better conversational agent than GPT-3.5 below."}]}