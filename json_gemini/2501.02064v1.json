{"title": "ArtCrafter: Text-Image Aligning Style Transfer via Embedding Reframing", "authors": ["Nisha Huang", "Kaer Huang", "Yifan Pu", "Jiangshan Wang", "Jie Guo", "Yiqiang Yan", "Xiu Li"], "abstract": "Recent years have witnessed significant advancements in text-guided style transfer, primarily attributed to innovations in diffusion models. These models excel in conditional guidance, utilizing text or images to direct the sampling process. However, despite their capabilities, direct conditional guidance approaches often face challenges in balancing the expressiveness of textual semantics with the diversity of output results while capturing stylistic features. To address these challenges, we introduce ArtCrafter, a novel framework for text-to-image style transfer. Specifically, we introduce an attention-based style extraction module, meticulously engineered to capture the subtle stylistic elements within an image. This module features a multi-layer architecture that leverages the capabilities of perceiver attention mechanisms to integrate fine-grained information. Additionally, we present a novel text-image aligning augmentation component that adeptly balances control over both modalities, enabling the model to efficiently map image and text embeddings into a shared feature space. We achieve this through attention operations that enable smooth information flow between modalities. Lastly, we incorporate an explicit modulation that seamlessly blends multimodal enhanced embeddings with original embeddings through an embedding reframing design, empowering the model to generate diverse outputs. Extensive experiments demonstrate that ArtCrafter yields impressive results in visual stylization, exhibiting exceptional levels of stylistic intensity, controllability, and diversity.", "sections": [{"title": "1. Introduction", "content": "Diffusion-based text-to-image generations [29, 33] have achieved notable progress in the realms of personalization and customization, especially in consistent synthesis such as identity protection [23, 44], object customization [15, 27] and style transfer [5, 6, 8, 16, 20, 52, 53]. In particular, text-guided style transfer focuses on fine-grained style representation that encompasses abstract concepts like texture, color, composition, and material, to create a spectrum of personalized outputs grounded in the textual semantic essence."}, {"title": "2. Related Work", "content": "Attention Control in diffusion models. Following the remarkable progress made in the field of pre-training text-to-image diffusion models [14, 29, 33, 38], a series of image editing efforts [1, 2, 46, 50] have emerged. Hertz et al. [12] propose the Prompt-to-Prompt method, which achieves text-based partial image editing and generates edited images that conform to textual conditions by replacing original vocabulary and cross-attention maps. Plug-and-play [49] utilizes the spatial features and self-attention mapping of the original image to guide the diffusion model for text-guided image-to-image translation while preserving the spatial layout of the original image. Later, MasaCtrl [4] proposes a mutual self-attention control technique for coherent image editing. Consistent image editing is achieved by preserving the key and value of the self-attention layer of the source image while conditioning the model with desired text prompts. Recently, StyleID [6] proposes a style migration method without training by injecting styles in the self-attention layer and introduces query preservation, attention"}, {"title": "3. Method", "content": "The overall architecture of ArtCrafter is shown in Fig. 3. As reviewed in Sec. 3.1, ArtCrafter is built upon diffusion model [33, 37]. In Sec. 3.2, we introduce attention-based style extraction, which captures multi-level style information by perceiver attention and multi-layer design. Text-image aligning augmentation (Sec. 3.3) allows the model to dynamically weigh the importance of different parts of the text prompt. This enables a more nuanced and context-aware generation process, resulting in images that are more closely related to the text prompt. Explicit modulation (Sec. 3.4) provides an effective way to combine textual and visual information, enabling the model to generate images that are both relevant to the text prompt and have diverse visual representations."}, {"title": "3.1. Preliminary", "content": "Diffusion model [33] consists of two processes: a forward process, which incrementally adds Gaussian noise \u03f5 to the data x\u2080 through a Markov chain. Additionally, a denoising process generates samples from Gaussian noise x\u209c ~ N(0, 1) with a learnable denoising model \u03f5\u2080(x\u209c, t, c) parameterized by \u03b8. This denoising model \u03f5\u03b8(\u00b7) is implemented with U-Net [34] and trained with a mean-squared"}, {"title": "3.2. Attention-based Style Extraction", "content": "This section elaborates on the style extraction method, which enhances the style encoding capabilities by integrating fine-grained features through a multi-layer architecture. The objective is to capture intricate style details from images by leveraging perceiver attention [18] and position-wise feed-forward network (FFN) [41].\nGiven the reference image, we obtain the input image embeddings through CLIP, denoted as x. The latent variables z are initialized as a tensor with a shape of (1, N, D), where N is the number of queries and D is the dimension of the latent space, and are normalized by dividing by the square root of D to stabilize the training process:\n$z = \\frac{N(0, 1)}{\\sqrt{D}0.5}$", "latex": ["z = \\frac{N(0, 1)}{\\sqrt{D}0.5}"]}, {"title": "3.3. Text-Image Aligning Augmentation", "content": "The text-image aligning augmentation method is designed to dynamically prioritize different aspects of the text prompt by leveraging cross-attention mechanisms. This module allows the model to integrate image and text embeddings more effectively, projecting them into a shared feature space where their interactions can be more nuanced.\nWe start by transforming the image prompt embeddings E\u1d62 and the text prompt embeddings E\u209c into query, key, and value matrices through linear layers. These transformations are represented as:\n$Q_I=W_Q E_I, K_T = W_{KT} E_T, V_T = W_{V_T} E_T.$", "latex": ["Q_I=W_Q E_I, K_T = W_{KT} E_T, V_T = W_{V_T} E_T."]}, {"title": "3.4. Explicit Modulation", "content": "The explicit modulation solves the problem of lack of flexibility in traditional fusion methods by seamlessly fusing the image embeddings with the text-image aligning augmented multimodal embeddings utilizing linear interpolation. Specifically, we fuse the image embeddings E\u1d62 with the multimodal embeddings E\u1d62\u209c through linear interpolation:\n$E_F = \\alpha E_I + (1 - \\alpha) E_{IT}$", "latex": ["E_F = \\alpha E_I + (1 - \\alpha) E_{IT}"]}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nWe train ArtCrafter on a training data consisting of about 500,000 real image-text pairs from LAION-Aesthetic [36] and 50k art-text pairs from our proposed ArtMarket dataset. The images are paired with text descriptions generated by BLIP-2 [22], forming image-text data pairs. More explanations and examples of the data are given in the Supplementary Materials. During both training and inference, we resize the input images to a spatial resolution of 512\u00d7512. We have implemented our method over [37] diffusion model. Our training processes are conducted using 8 NVIDIA A100 GPUs, each with 80GB of memory, and a batch size of 8 per GPU. The inference phase, which consumes 5185 MiB of memory, takes about one second of sampling time on a single A100 at denoising steps of 50."}, {"title": "4.2. Qualitative Evaluations", "content": "We evaluate our proposed method by comparing it with various existing methods, including but not limited to Styleshot [10], Style Aligned [13], VSP [19], InstantStyle [42], InstantStyle (Plus) [29, 42], IP-Adapter [48], IP-Adapter (SDXL) [29, 48], CSGO [46], and StyleCrafter [25]. We utilized the publicly available implementations of these methods and followed their recommended configurations for testing.\nThe qualitative comparison presented in Fig. 4 offers a visual assessment of the results achieved by various stylization methods. StyleShot shows some deficiencies in style representation, particularly in the capture of details and the consistency of style. Style Aligned and VSP sometimes exhibit discrepancies between the output style and the input style reference, which may lead to the loss of stylistic features."}, {"title": "4.6. Additional Analysis", "content": "ArtCrafter control. We test the role of ArtCrafter in the pre-training diffusion model. The result of adjusting the scale of ArtCrafter enhanced embedding EtextP in the pre-trained diffusion model is shown in Fig. 10. In this work, we usually set the scale to 0.6 to balance content and style. This verifies that ArtCrafter can effectively enhance the applicability of pre-trained diffusion models in the art generation domain at a lower training cost. With this adjustment, we can more flexibly control the degree of text consistency and stylization of the generated images to meet different creative needs.\nArtCrafter with additional conditions applied. Benefiting from our design without any changes to the network structure of the original diffusion model, ArtCrafter is seamlessly compatible with existing controllable tools [50]. Fig. 11 shows the diverse examples generated by applying different structural controls, including canny edge detection [3], normal map [40], HED edge detection [45], lineart edge, and MiDaS depth map [32]. These examples not only highlight ArtCrafter's flexibility in adapting to a variety of conditions but also foretell its great potential for application in the 3D field, opening up new possibilities for artistic creation and visual design."}, {"title": "5. Conclusion", "content": "In this paper, we introduce ArtCrafter, a novel text-image aligning style transfer framework achieved through an embedding reframing architecture. Our approach ensures superior text-guided style transfer quality by integrating three core components: attention-based style extraction, text-image aligning augmentation, and explicit modulation. Comprehensive evaluations demonstrate ArtCrafter strengths in adapting to diverse artistic styles, maintaining textual prompt consistency, enhancing output diversity, and improving overall visual quality.\nAcknowledging the current limitations of our work, for future research, we intend to enhance our approach by incorporating pattern reproducibility and contextual elements within style images, including the relative positioning of style patches, to facilitate a more cohesive art style transfer. We expect that advancements in the extraction and fusion of style and content features, coupled with an investigation into the method's scalability and adaptability, will markedly enhance the quality of style transfer and provide more precise control over the shape and appearance similarity of the generated images."}]}