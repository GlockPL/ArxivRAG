{"title": "Speculate, then Collaborate:\nFusing Knowledge of Language Models during Decoding", "authors": ["Ziyao Wang", "Muneeza Azmart", "Ang Li", "Raya Horesh", "Mikhail Yurochkin"], "abstract": "Large Language Models (LLMs) often excel in\nspecific domains but fall short in others due to the\nlimitations of their training. Thus, enabling LLMs\nto solve problems collaboratively by integrating\ntheir complementary knowledge promises to im-\nprove their performance across domains. To real-\nlize this potential, we introduce a novel Collabora-\ntive Speculative Decoding (CoSD) algorithm that\nenables efficient LLM knowledge fusion at test\ntime without requiring additional model training.\nCoSD employs a draft model to generate initial\nsequences and an easy-to-learn rule or decision\ntree to decide when to invoke an assistant model\nto improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference effi-\nciency, is transferable across domains and models,\nand offers greater explainability. Experimental\nresults demonstrate that CoSD improves accuracy\nby up to 10% across benchmarks compared to ex-\nsisting methods, providing a scalable and effective\nsolution for LLM-based applications.", "sections": [{"title": "1. Introduction", "content": "State-of-the-art large language models (LLMs), such as\nGPT-4 (Achiam et al., 2023) and Llama-3 (Dubey et al.,\n2024), have demonstrated impressive capabilities in gener-\nating high-quality text across a variety of domains. These\nmodels are trained on vast datasets, allowing them to per-\nform well on a wide range of tasks. However, despite\ntheir general effectiveness, no single LLM excels uniformly\nacross all domains. Different models tend to have comple-\nmentary knowledge, with each model specializing in certain\nareas. For example, one model may be more proficient in\ntechnical writing, while another may outperform in creative\ntasks. This heterogeneity has led to an increasing interest in\ndeveloping methods that can fuse the knowledge of multiple\nLLMs, enabling users to harness their collective strengths\nfor more robust and versatile applications.\nTo address these challenges, recent research has shifted fo-\ncus to test-time knowledge fusion, which eliminates the need\nfor retraining by combining model outputs during inference.\nThis approach allows users to leverage the complementary\nknowledge of multiple LLMs without the overhead of addi-\ntional training. For example, Wang et al. (2023) proposed a\nmethod that selects expert models dynamically at inference\ntime using supervised learning, while Ong et al. (2024) intro-\nduced a router model that optimizes the selection of models\nbased on performance and cost. Other approaches focus\non integrating outputs through the decoding process, such\nas token-wise decoding (Shen et al., 2024) and character-\nwise decoding (Gu et al., 2024), which combine outputs at a\nfine-grained level. Although these methods offer potential,\nthey often struggle to balance strong knowledge integration\nwith efficiency, which limits their practicality in real-world\napplications.\nIn response to these limitations, we propose Collaborative\nSpeculative Decoding COSD, a novel algorithm designed to\nefficiently fuse the knowledge of multiple LLMs at inference\ntime. COSD builds upon recent developments in Specula-\ntive Decoding (Leviathan et al., 2023; Xia et al., 2023) to\ncreate an efficient system where multiple LLMs collabo-\nrate during the inference process. As shown in Figure 1,\nCOSD consists of two models: a draft model that generates\nan initial sequence of tokens and an assistant model that\nverifies these tokens in parallel. When the assistant model\npredicts a token different from that of the draft model, a\ncomparison of their token probabilities is used to determine\nwhether to replace the draft token. This decision-making\nprocess can be guided by either a predefined rule set (Rule-\nBased COSD) or a pre-trained decision tree (Tree-Based\nCOSD). The sequence is then regenerated and re-verified\niteratively until accepting all tokens, ensuring both accuracy\nand computational efficiency.\nCOSD presents several notable advantages over existing\ntest-time fusion methods. First, by leveraging speculative"}, {"title": "2. Related Work", "content": "Language Model Fusion from multiple LMs aims at en-\nhancing the cross-domain performance of the resulting\nmodel and reducing bias. The primary efforts for such\nintegration include model merging (Goddard et al., 2024),\nsuch as model weight averaging (Wortsman et al., 2022)\nand linear mode connectivity (Ainsworth et al., 2022; Ito\net al., 2024; Wang et al., 2020). Another series of works is\ncalled model stacking, which refers to concatenating mod-\nels along the depth dimension. Wu et al. (2024) and Kim\net al. (2023) stack the decoder blocks to expand the depth of\nLlama models. For large language models, some other re-\nsearch proposes knowledge fusion (Wan et al., 2024). They\ncombine the capabilities of existing LLMs and transfer them\ninto a single LLM. Another important trend of work called\nMixture of Expert (MoE) (Zhu et al., 2024; Xue et al., 2024)\nbuilds sparse neural networks and only activates a subset\nof parameters (i.e., experts) for each input. However, these\nmethods either require the fused models to have the same\nstructure or require fine-tuning after fusing to achieve the\ndesired model performance. Towards mitigating these flaws,\na new wave of works adopt decoding methods to fuse LMs.\nGu et al. (2024) propose a character-wise ensemble decod-\ning method to fuse two LLMs' outputs. Shen et al. (2024)\nand Wang et al. (2023) fuse model knowledge by training\nto choose between the generation of different LLMs. In our\nexperiments, we consider several baselines from the latter\ngroup of works and observe gains in either efficiency or\nperformance when using our method to merge cross-domain\nknowledge from different LMs when decoding.\nSpeculative Decoding is an efficient decoding paradigm\nfor LM inference (Xia et al., 2024; Stern et al., 2018;\nXia et al., 2023). It accelerates the inference process by\nfirst generating draft tokens efficiently, and then using an\nLLM to verify draft tokens in parallel and correct them if\nneeded (Leviathan et al., 2023), which avoids the autoregres-\nsion process. In practice, the draft generator in speculative\ndecoding could be a small LM (Chen et al., 2023; Miao et al.,\n2023; Zhou et al., 2023), a sub-model of an LLM (Zhang\net al., 2023; Yang et al., 2023; Elhoushi et al., 2024), or\na text database retriever (He et al., 2023; Li et al., 2024).\nThe final generation of speculative decoding will be similar\nto the autoregressive generation of the target LLM, which\nis only acceptable when the target LLM has much better\nperformance but is less efficient than the draft generator.\nNo previous work focuses on using speculative decoding to\napproach the model fusion problem."}, {"title": "3. Collaborative Speculative Decoding", "content": "In our Collaborative Speculative Decoding system, our pur-\npose is to fuse the predicted sequences of two LLMs effi-\nciently. We define our problem as follows: given an input\nsequence $x_1,...,x_t$, CoSD uses a draft model $M_p$ and\nan assistant model $M_q$ to collaboratively generate an out-\nput sequence $x_{t+1},..., X_{t+K}$ that integrates both models'\nknowledge and expertise.\nAs Figure 1 illustrates, the process begins with the draft\nmodel $M_p$ generating a draft sequence $X_{t+1},..., X_{t+K}$ in\nan autoregressive manner. Subsequently, the assistant model\n$M_q$ verifies the draft tokens by producing an assistant se-\nquence $\\hat{x}_{t+1},...,\\hat{X}_{t+K}$. After both sequences are gener-\nated, COSD iterate through the tokens and their correspond-\ning probabilities to verify whether to accept a draft token\n$X_{t+i}$ or replace it with the corresponding assistant token\n$\\hat{x}_{t+i}$. Both rule-based or tree-based verification strategies,\nuse token probabilities to determine whether a replacement\nis necessary. When a replacement occurs, all subsequent\ndraft tokens are discarded, and a new draft sequence is\ngenerated starting from the replaced token. This process\ncontinues until the output reaches the maximum length or an\n< EOS > token is generated. The full generation and veri-\nfication process is elaborated in Algorithm 1 and described\nin following sections."}, {"title": "3.1. Generation.", "content": "The generation process follows the principles of Speculative\nDecoding. First, the draft model $M_p$ generates a sequence\nof tokens autoregressively:\nfor i = 1 to K do\n\n$\\qquad x_{t+i} \\sim M_p(x | x_1, ..., x_{t+i-1}),$\n(1)\nHere, $x_{t+i}$ represents the token predicted by the draft model\nat position i, selected as the token with the highest probabil-\nity. The sequence $x_{t+1},..., X_{t+K}$ is generated autoregres-\nsively and produced sequentially.\nAfter the draft sequence is generated, the assistant model\n$M_q$ is used to verify these tokens. The assistant model\ngenerates tokens in parallel:\ni =1, ..., K in parallel do\n$\\qquad \\hat{x}_{t+i} \\sim M_q(x | x_1, ..., x_{t+i-1}),$\n(2)\nNote that we already have all the draft tokens\n$x_{t+1},...,x_{t+K}$ when we generate the assistant tokens.\nThus, all the $\\hat{x}_{t+i}$ in Eq. (2) can be generated in paral-\nlel. The process can also handle cases where the draft and\nassistant models use different tokenizers. In such cases, the\ndraft sequence is first decoded by the draft model's tokenizer\nand then encoded by the assistant model's tokenizer:\ni =1,..., K in parallel do\n\\xrightarrow[T_p]{decode} Texts \\xrightarrow[T_a]{encode} \\hat{x}_1,...,\\hat{x}_t\n$\\qquad x_1,..., x_{t+i-1} $\n\\hat{x}_{t+i} \\sim M_q(x | \\hat{x}_1,...,\\hat{x}_t),\n(3)\nwhere $T_p$ and $T_q$ are the tokenizers of the draft model and\nthe assistant model respectively. The draft sequence is first\ndecoded into texts by $T_p$ and then encoded by $T_q$ to fit the\nassistant model."}, {"title": "3.2. Verification", "content": "After the generation, we have a draft sequence\n$x_{t+1},..., x_{t+K}$ and an assistant sequence $\\hat{x}_{t+1},..., \\hat{X}_{t+K}$,\nalong with the corresponding probabilities $M_p(x_{t+i})$ and\n$M_q(\\hat{x}_{t+i})$. We then use this information to verify whether\nto keep the draft token $x_i$ or replace it with the assistant\ntoken $\\hat{x}_i$ and thus ensemble the model knowledge. In or-\nder to make COSD suitable for a wider range of tasks, we\npropose two strategies for verification. The first strategy,\ncalled Rule-Based Verification, applies clear rules to decide\nwhether to select the draft token or the assistant token. The\nsecond strategy, i.e., Tree-Based Verification, involves train-ng a decision tree to classify and select between the draft\nand assistant tokens.\nRule-Based Verification. In Rule-Based Verification, the\nsystem applies simple yet general rules to determine whether\nthe draft token $x_{t+i}$ should be replaced by the assistant\ntoken $\\hat{x}_{t+i}$. The intuition behind these rules is that if the\ndraft model predicts a token with low confidence and the\nassistant model offers a higher-confidence alternative, the\ndraft token should be replaced. The following rules define\nthe verification process:\n$x_{t+i} \\neq \\hat{x}_{t+i},$\n(4)\n$M_p(x_{t+i}) < \\alpha,$\n(5)\n$M_q(\\hat{x}_{t+i}) > \\beta \\cdot M_p(x_{t+i}),$\n(6)\nThese conditions check whether (1) the draft and assistant\ntokens differ, (2) the draft token has a probability below a\nthreshold $\\alpha$, and (3) the assistant token has a probability\nsufficiently higher than the draft token's probability by a\nfactor of $\\beta$. If all conditions are met, the draft token is\nreplaced with the assistant token.\nIntuitively, the Rule-Based Verification can be explained\nas follows: if the draft model is uncertain and the assistant\nmodel provides a better alternative, the system opts for"}, {"title": "Tree-Based Verification.", "content": "Tree-Based Verification. For domain-specific applica-\ntions, Rule-Based Verification may not always be optimal. It\nis necessary to improve performance in specialized domains,\nsuch as healthcare (Poonia & Al-Alshaikh, 2024), smart\nhome (Amru et al., 2024), or math (Mazraeh et al., 2024).\nTherefore, we design the Tree-Based Verification method,\nwhich involves training a decision tree to decide when to re-\nplace a draft token with an assistant token. Training the deci-\nsion tree on specific domain data allows for a more accurate\nassessment of knowledge fusion performance within those\nparticular contexts. Specifically, our decision tree T takes\ntwo probabilities, $M_p(x_{t+i})$ and $M_q(\\hat{x}_{t+i})$, as inputs. The\ndecision tree's output $T(M_p(x_{t+i}), M_q(\\hat{x}_{t+i})) \\in \\{0,1\\}$\nindicates whether to use the draft token ($y_i$ = 0) or replace\nit with the assistant token ($y_i$ = 1).\nTo train a decision tree suitable for specific domains, we\nfirst select a commonly used benchmark dataset D for this\ndomain (e.g., GSM8K (Cobbe et al., 2021) in math) with\nseveral input and ground-truth output pairs, i.e., $X_1, ..., X_t$\nand $X_{t+1},..., X_{t+K}$. We iterate through all the tokens in\nthe ground-truth output in each pair. For the i-th token, we\nconcatenate the input sequence and the first i - 1 tokens\nof output sequences. Then, we feed the concatenated input\n$X_1,..., X_{t+i-1}$ into the two models separately to obtain the\npredicted next token $x_{t+i}, \\hat{x}_{t+i}$ and their corresponding\nprobabilities $M_p(x_{t+i}), M_q(\\hat{x}_{t+i})$. This probability pair\nis one training sample of the decision tree. As for the related\nground-truth label, we have three rules:\n*   If $x_{t+i} = \\hat{x}_{t+i}$, we assign the label $y_i$ = 0 to encourage\nthe decision tree to select the draft token.\n*   If $x_{t+i} \\neq \\hat{x}_{t+i}$ and $x_{t+i} = \\hat{x}_{t+i}$, we assign the label\n    $y_i$ = 1 to encourage the decision tree to select the assistant\n    token.\n*   If neither $x_{t+i}$ nor $\\hat{x}_{t+i}$ match the target, we drop the\nsample and continue the loop with i \\leftarrow i + 1.\nWe iterate through all the input-output pairs and fi-\nnally construct the training data sample in the form of\n$\\{\\[M_p(x_i), M_q(\\hat{x}_i)], y_i\\}$. In the training process, we aim\nto train the decision tree classifier $T : \\mathbb{R}^2 \\rightarrow \\{0,1\\}$ to\nminimize the difference between the predicted label and the\nground truth:\n$\\min \\sum_{i=1}^{N} \\[y_i \\log(T(M_p(x_i), M_q(\\hat{x}_i)))\n+ (1-y_i) \\log(1 - T(M_p(x_i), M_q(\\hat{x}_i)))].$\n(7)\nAfter training, our decision tree can predict whether to\nchoose the draft token or the assistant token based on the\ntwo input probabilities. If the decision tree predicts 1, the\nsame as the rule-based verification, we replace the token,\nupdate the accepted token number, and send the new input\nsequence back to the generation. Since the decision tree is\ntrained on a dataset specific to the corresponding domain, us-\ning this decision tree to fuse the model outputs can achieve\nbetter results in that domain."}, {"title": "4. Experiment", "content": "We evaluate\nCOSD and compare it against several baselines in scenarios\nthat reflect common use cases where users may seek to fuse\nthe knowledge of multiple LLMs. These scenarios include:\n(i) Complementary Knowledge Fusion: The fused LLMs\nhave complementary knowledge, and users hope that the\nknowledge fusion system can perform as well as the best\nmodel for each task across all tasks; (ii) Catastrophic For-\ngetting Recovery: The fused models are one base model\nand a model fine-tuned from the base model. Fine-tuning\nimproves performance in certain domains but reduces the\nperformance in other domains due to catastrophic forget-\nting. Users expect to heal the catastrophic forgetting by\nfusing the knowledge of the two LLMs; (iii) Capacity Im-\nbalance: Users use a small draft model and adopt an API\nof the assistant model with a much larger capacity. The\nfusion system is expected to perform similarly to the as-\nsistant model; (iv) Different Tokenizers: Fuses the LLMS\nwith different tokenizers. To simulate these scenarios, we\ncarefully selected six pairs of LLMs from the HuggingFace\nrepository (Jain, 2022), representing each of the four use\ncases outlined above. Table 1 lists the model pairs and the\ncorresponding simulated scenarios.\nFor all the scenarios and model pairs, we use MMLU\n(Hendrycks et al., 2020), GSM8K (Cobbe et al., 2021),\nHumanEval (Chen et al., 2021), Hellaswag (Zellers et al.,\n2019), and TruthfulQA (Lin et al., 2021) as the evaluation\nbenchmark. We use tinyBenchmarks (Polo et al., 2024) for\nall the benchmarks except HumanEval to further increase the\nefficiency of experiments. These benchmarks test general\nquestion-answering, mathematical reasoning, and coding\ncapabilities, providing a comprehensive assessment of the\nmodels' abilities across different domains. By using these\nbenchmarks, we can evaluate the effectiveness of COSD and\nthe baselines in fusing complementary knowledge across\ndiverse tasks and model configurations.\nBaselines. We use tree baselines in the experiment. Spec-\nulative Decoding: It also uses a draft model and an assistant\nmodel to generate the output. However, it adopts a different\nverification algorithm that replaces the draft token when\n$\\frac{M_p(x_i)}{M_q(x_i)} < U(0,1)$. Average Decoding: It averages the\npredicted probabilities of the draft model and the assistant\nmodel and chooses the final output from the averaged prob-\nabilities. Co-LLM (Shen et al., 2024): It trains a single\nlayer to classify the hidden state of a base model. The out-\nput probability of the layer decides to use the base model\ngeneration or evoke an assistant model to help generation."}, {"title": "Hyperparameters.", "content": "Hyperparameters. We run CoSD with the following set-\ntings. For Rule-Based COSD, we set $\\alpha$ = 0.5 and $\\beta$ = 0.5,\nwhich were determined to be the optimal and most trans-\nferable parameters based on our analysis in Figure 2. For\nTree-Based COSD, we randomly select three samples from\nthe AlpacaEval dataset to train the decision tree. It is impor-\ntant to note that we use MMLU, GSM8K, and HumanEval\nas our benchmarks. Consequently, the training data for the\ndecision tree do not overlap with the test data, creating a\nmore realistic scenario to evaluate the decision tree's trans-\nferability across different tasks and domains."}, {"title": "4.2. Experimental Results", "content": "Fusing LLMs with Complementary Domain Knowledge.\nWe first evaluated the performance of different methods for\nfusing LLMs with complementary knowledge, with results\nshown in the pair 1 and pair 2 columns of Table 2. Both\nCoSD-Rule and CoSD-Tree consistently outperformed the\nbaseline methods in terms of averaging performance. For\ninstance, in pair 1, CoSD-Rule and CoSD-Tree achieved\naveraged scores of 52.41 and 51.24 on, surpassing all the\nbaselines and both the draft model and the assistant model.\nBesides, CoSD-Rule also achieves the best performance\non MMLU, HumanEval, and Hellaswag. In pair 2, COSD\nmatches the performance of the better model for each task\nacross all tasks. For example, CoSD achieves a similar\nMMLU performance to the draft model and a similar perfor-\nmance on GSM8K and HumanEval to the assistant model in\npair 2. Compared with COSD, Speculative Decoding only\nperforms similarly to the assistant model, thus will be more\nsuitable to the scenario when the assistant model is much\nstronger than the draft model. Average Decoding fuses\nmodel knowledge but only achieves an average accuracy\nacross tasks, unlike COSD, which integrates the strengths\nof different LLMs. Co-LLM's performance is the closest to"}, {"title": "Fusing LLMs with Different Tokenizers.", "content": "Fusing LLMs with Different Tokenizers. Although\nCOSD needs to decode and then encode the sequences dur-\ning the verification when the models have different tokeniz-\ners, which sacrifices some efficiency, it can still effectively\nfuse the model knowledge. In the experiments, we fuse\na Llama 2 Chat and a WizardMath to evaluate the COSD\nperformance on MMLU and GSM8K. We fuse a Llama 2\nChat and a Deepseek Coder to evaluate COSD on MMLU\nand HumanEval. Results are shown in Table 3. COSD out-\nperforms the character-wise averaging method CharED (Gu\net al., 2024) in both model pairs and benchmarks. We do\nnot include other baselines since they are not applicable to\nthe different tokenizer settings."}, {"title": "Ablation Studies.", "content": "Ablation Studies. We have several tunable hyperparam-\neters in COSD. In Rule-Based COSD, we have $\\alpha$ and $\\beta$\nthat determine the rules to replace the draft tokens. In Tree-\nBased COSD, the training data and hyperparameters influ-\nence the performance of the decision tree. Thus, we use\nablation experiments to identify the impact of these hyper-\nparameters on the final model performance, allowing us to\ndetermine the optimal and transferable hyperparameter set-\ntings. Figure 2 shows the relationship between $\\alpha$, $\\beta$ values\nin Rule-Based COSD and model performance. The x-axis\nrepresents the values of $\\alpha$, and the y-axis represents the\nvalues of $\\beta$. The numbers in the small squares represent\nthe sum score of MMLU and GSM8K, which reflect the\noverall model performance of COSD. We can see that with\n$\\alpha$ = 0.5, 0.75 and $\\beta$ = 0.5,0.75, Rule-Based COSD per-\nform consistently well in the two model pairs. We ultimately\nselected $\\alpha$ = 0.5, $\\beta$ = 0.5 as the general hyperparameters\nin our experiments. We believe this setting effectively inte-\ngrates the knowledge of the models."}, {"title": "Case Studies.", "content": "Case Studies. We use an example in GSM8K to demon-\nstrate how CoSD effectively combines the knowledge of two\nmodels in Table 4. CoSD replaces the red tokens generated\nby the draft model with the green tokens from the assistant\nmodel. We display the answer from the single draft model\nand the assistant model in Table 10 in the appendix. Neither\nthe draft model nor the assistant generates the correct result\nwhen used alone. The main issue with the draft model is\nits weak mathematical calculation ability (e.g., in the fourth\nline, it calculates the tax as 20% of 20 to be 10, instead\nof the correct answer 4). On the other hand, the assistant\nmodel performs well in terms of mathematical calculations\nbut lacks the logical rigor of the draft model. It fails to com-\npute the subtotal without the tip first, leading to the incorrect\nfinal calculation. CoSD effectively integrates the strengths\nof both models. For instance, in CoSD-Rule, in the fifth\nline, the assistant model rejects the draft model's incorrect\ncomputation of 20% of 20 = 10 and instead uses the correct\ncalculation of 20 * 0.2 = 4, successfully avoiding the error\nin the draft model's tax calculation. In the sixth line, the\ndraft model correctly leads to generate the subtotal of $24,\nso in the final step, CoSD-Rule computes the simpler 24 +\n5 instead of the more complicated 15 + 3 + 2 + 5, resulting\nin the correct answer.\nAlso, there are situations that COSD makes wrong decisions.\nAs shown in Table 9 in Appendix B, COSD does not always\nselect the correct answer. In the above example, the draft\nmodel made the correct choice with high confidence, so the\nfinal generation retained the correct answer. However, in the\nexample below, while the draft model also made the correct\nchoice, the assistant model provided an incorrect answer\nwith higher confidence, leading to the final output being\nchanged to the wrong answer. This demonstrates that using\nconfidence as the criterion does not guarantee selecting the"}, {"title": "5. Conclusion", "content": "In this paper, we fuse the LLM knowledge in a simple yet\neffective way. Our algorithm CoSD takes the probabilities\nof predicted tokens from two LLMs as the feature to ver-\nify whether to keep the draft token or adopt the assistant\ntoken. The verification strategy can be either a rule-based or\na pre-trained decision tree. Our extensive experiments show\nthat COSD performs better than the state-of-the-art methods\nacross 6 LLM pairs and 5 benchmarks. Compared to previ-\nous works, COSD has superior knowledge fusion ability, a\nbroader range of application scenarios, and comparable effi-\nciency. It works well in scenarios including complementary\nknowledge fusion, catastrophic forgetting recovery, knowl-\nedge fusion with disparate model capacity, and knowledge\nfusion with different tokenizers. CoSD makes it possible\nfor ordinary users to fuse the LLM knowledge with only the\nAPI queries, without any training or fine-tuning of LLMs,\nor requirements of white-box LLM information such as hid-\nden states. It provides users with better tools to manipulate\nLLMs in wider application scenarios."}, {"title": "A. Limitations", "content": "While COSD demonstrates strong performance across various scenarios, it is important to acknowledge its limitations. This\nsection highlights cases where COSD may not be applicable and tasks that it fails to address. Identifying these constraints\nprovides clarity on its scope of use and helps guide future improvements. Below, we outline two specific limitations:\n(1) When the two collaborating models are of similar size and one significantly outperforms the other, COSD offers no\nadvantage over using only the better model. In this case, using the better model only is sufficient. This also requires the user\nto have prior knowledge of the performance of the two models on different benchmarks and to determine that one model is\nsignificantly better than the other. If the user is uncertain, we still recommend using COSD to ensure the best results.\n(2) Another limitation of COSD is that it cannot guarantee the replaced assistant token is always better than the discarded\ndraft one. It relies on the confidence scores of the models, which are not always perfectly aligned with token quality. The\nalgorithm selects the output of the more confident model, aiming to maximize the likelihood of choosing a better token, but\nthis approach may occasionally lead to suboptimal results."}, {"title": "B. Additional Experiments and Discussion", "content": "The Average Iterations in CoSD. The number of iterations required during collaborative decoding depends on the\nmaximum length of the model output. Table 7 reports the average number of iterations on the GSM8K dataset for different\nmaximum lengths.\nAlthough the number of iterations scales with the output length, it does not directly imply a proportional increase in generation\ntime. As the number of accepted tokens grows, the number of tokens requiring regeneration decreases significantly. For\ninstance, with a maximum output length of 128, the average number of iterations is 11, but the total generated output length\nremains around 300 tokens. This highlights the efficiency of our approach in reducing redundant generation.\nCollaborate with More LLMs. Our COSD also supports multiple collaborating models. Table 8 presents the results when\nthree models are used for collaboration:"}, {"title": "The Case Study of MMLU.", "content": "The Case Study of MMLU. While COSD is effective in many cases, there are instances where it makes incorrect decisions,\nhighlighting its limitations. As shown in Table 9, COSD does not always select the correct answer when the draft model\nand the assistant model disagree. In the first example, the draft model correctly identified the answer with high confidence,\nwhich allowed the final output to retain the accurate result. This showcases the potential of COSD to preserve correct\nanswers when confidence aligns with accuracy.\nHowever, in the second example, the draft model once again made the correct prediction, but the assistant model, despite\nbeing incorrect, provided an answer with higher confidence. Consequently, the final output was altered to the wrong answer,"}]}