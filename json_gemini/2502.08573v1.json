{"title": "A NOVEL APPROACH FOR MULTIMODAL EMOTION RECOGNITION : MULTIMODAL SEMANTIC INFORMATION FUSION", "authors": ["Wei Dai", "Dequan Zheng", "Feng Yu", "Yanrong Zhang", "Yaohui Hou"], "abstract": "With the advancement of artificial intelligence and computer vision technologies, multimodal emotion recognition has become a prominent research topic. However, existing methods face challenges such as heterogeneous data fusion and the effective utilization of modality correlations. This paper proposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on the integration of contrastive learning and visual sequence compression. The proposed method enhances cross-modal feature fusion through contrastive learning and reduces redundancy in the visual modality by leveraging visual sequence compression. Experimental results on two public datasets, IEMO\u0421\u0410\u0420 and MELD, demonstrate that DeepMSI-MER significantly improves the accuracy and robustness of emotion recognition, validating the effectiveness of multimodal feature fusion and the proposed approach.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of artificial intelligence and computer vision technologies, emotion recognition has become an important research direction in various fields such as human-computer interaction (HCI), intelligent customer service, and mental health monitoring [Poria et al., 2017a]. The goal of emotion recognition is to analyze an individual's emotional state through multimodal information, such as speech, text, and visual data, to achieve emotional understanding in intelligent systems. However, traditional emotion recognition methods mainly focus on feature extraction and emotion classification from a single modality, which limits their effectiveness in complex real-world applications. In recent years, with the continuous advancement of multimodal learning and deep learning technologies, multimodal emotion recognition (MER) has gradually become a research hotspot. MER improves the accuracy and robustness of emotion classification by integrating multiple data sources.\nAlthough existing multimodal emotion recognition methods have achieved success in many scenarios, they still face several challenges. First, different modalities exhibit different representations in feature space, and effectively fusing these heterogeneous data to capture emotional features has become a key issue [Hadsell et al., 2006, Chen et al., 2020]. Second, temporal features and spatial information in the visual modality often contain a large amount of redundant data. Reducing this redundancy while retaining emotion-relevant information remains a valuable area for exploration [Tran et al., 2018, Carreira and Zisserman, 2017]. Lastly, despite significant progress in feature extraction using deep learning models, fully leveraging the latent correlations between different modalities to enhance the emotional understanding capability of models in multimodal emotion recognition tasks remains a challenging problem [Zadeh et al., 2017, Liu et al., 2018a].\nTo address these challenges, this paper proposes a novel multimodal emotion recognition method called DeepMSI-MER, based on contrastive learning and visual sequence compression integration. Through contrastive learning, the model better captures the similarities and differences between modalities during training, thereby enhancing cross-modal feature fusion. Visual sequence compression effectively reduces redundancy in the visual modality by compressing and extracting temporal information, thus enhancing the model's sensitivity to emotions. Our experimental results show that the proposed method performs excellently on two public datasets, IEMOCAP [Busso et al., 2008] and MELD [Poria et al., 2019], significantly improving the accuracy and robustness of multimodal emotion recognition and validating the effectiveness of multimodal feature fusion and the proposed approach."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Multimodal Emotion Recognition", "content": "Multimodal emotion recognition initially focused on emotion recognition from individual modalities. However, with advancements in technology, recent studies have increasingly integrated speech, text, and visual modalities to improve emotion recognition performance. Methods based on deep neural networks have been proposed to combine speech and text modalities, significantly enhancing emotion recognition accuracy [Abdullah et al., 2021]. Additionally, frameworks that integrate speech, text, and visual features have further improved emotion prediction accuracy through joint training [Gupta et al., 2024]."}, {"title": "2.2 Application of Contrastive Learning in Emotion Recognition", "content": "Contrastive learning, which maximizes the similarity between similar samples and minimizes the distance between dissimilar samples, has achieved success across various domains, including vision, speech, and text. In recent years, contrastive learning-based multimodal emotion recognition frameworks have been introduced to enhance the feature fusion of speech and text modalities, significantly improving model performance [Mai et al., 2022]."}, {"title": "2.3 Visual Sequence Compression", "content": "To address the redundant information in the visual modality, particularly in video data, methods for visual sequence compression have been proposed. These include CNN-based and LSTM-based compression methods, which effectively extract key frame information and improve emotion recognition accuracy by reducing the impact of redundant data [Kugate et al., 2024, Fan et al., 2016]."}, {"title": "2.4 Cross-modal Fusion Methods", "content": "One core challenge in multimodal emotion recognition is effectively fusing information from different modalities. Traditional fusion methods include early fusion, late fusion, and intermediate fusion. Recently, approaches based on"}, {"title": "3 Proposed Method", "content": "The DeepMSI-MER framework proposed in this paper is shown in Figure 1. The model consists of three stages: modality-specific feature extraction, early feature fusion, late feature fusion, and model prediction. In the following subsections, we will describe in detail the modality-specific feature extraction, early feature. The specific model code is available at"}, {"title": "3.1 Modality-Specific Feature Extraction and Early Feature Fusion", "content": "Initially, we fine-tune the pre-trained models BERT [Devlin et al., 2019] and Wav2Vec [Baevski et al., 2020] on the audio and text data, respectively, to obtain their semantic features. Next, we fuse the audio semantic feature als with the text semantic feature tcls to generate the high-level semantic feature Gels. This high-level semantic feature is then passed to the visual sequence compression module in the video feature extraction pipeline. During video feature extraction, 15 frames are sampled from each video, and the Swin Transformer is used to extract features from each frame. Finally, Temporal Convolution Networks (TCN) [Bai et al., 2018] are employed to capture temporal features from the frame-level features, producing the final video feature."}, {"title": "3.1.1 Visual Sequence Compression", "content": "To enhance the accuracy of visual information, we propose a visual sequence compression method based on multimodal semantic information, as shown in Figure 2.\nBy applying average pooling to the visual sequence $V^{N \\times d}$ in the Swin Transformer, we obtain the visual semantic feature $v_{cls}$ for this stage. The high-level semantic features are then fused with the visual semantic features, as following formula Equation 1:\n$m_{cls} = v_{cls} + G_{cls}$ \n$M^{N\\times d} = [m_{cls}, m_{cls}, ..., m_{cls}]$\nwhere, $m_{cls}$ represents the weighted sum of the high-level semantic feature $G_{cls}$ and the visual semantic feature $v_{cls}$. The fused semantic feature $m_{cls}$ is then broadcasted to the same dimension as the visual sequence, resulting in $M^{N \\times d}$. Subsequently, we compute the similarity between $M^{N\\times d}$ and $V^{N\\times d}$, as following formula Equation 2:\n$S^{N \\times N} = \\frac{V^{N \\times d}. (M^{N\\times d})^T}{T}$ \n$\\sigma(S^{N\\times 1}) = \\begin{cases} z^r , S^{N \\times 1} > \\gamma \\\\ z^{lr}, S^{N \\times 1} < \\gamma \\end{cases}$ \nwhere, $\\gamma$ is the temperature coefficient, T represents the transpose of the matrix, and $S^{N\\times N}$ is the similarity matrix. Since the values in each row of $S^{N\\times N}$ are the same, the first row is selected as the similarity sequence. Finally, based on the similarity sequence and the similarity threshold $\\gamma$, the visual sequence $V^{N\\times d}$ is divided into relevant sequences $Z^r$ and irrelevant sequences $Z^{lr}$.\nFinally, to prevent information loss during the visual sequence compression process, we compute the similarity between $Z^r$ and $Z^{lr}$, and fuse the information of $Z^{lr}$ with the highest similarity corresponding to $Z^r$, as following formula Equation 3:\n$j = max(Z^r \\cdot Z^{lrT})$\n$Z^{r'} = \\sum_{i=0}^{N-L} (\\alpha * Z^r_i + (1-\\alpha) * Z^{lr}_{ij})$\nwhere, j represents the sequence position corresponding to the highest similarity between the non-relevant sequence $Z^{lr}$ and the relevant sequence $Z^r$; $\\alpha$ is the fusion threshold; $N - L$ is the length of the non-relevant sequence. The"}, {"title": "3.1.2 Image Feature Extraction", "content": "In sentiment analysis tasks, background noise in images can affect the accuracy of feature extraction when using Swin-Transformer. To address this, we fine-tuned the pre-trained Wav2Vec and BERT models for semantic feature extraction of audio and text, respectively, and fused these high-level semantic features as a reference to selectively filter video sequences within the Swin-Transformer. As shown in Figure 3, the VSC-Swin framework compresses video sequences by adding a visual compression module to the original Swin-Transformer architecture, while layer normalization (LN) is applied to prevent gradient explosion."}, {"title": "3.1.3 Video Feature Extraction", "content": "In this study, we use the Temporal Convolutional Network (TCN) for temporal feature extraction from videos. TCN is a deep neural network architecture designed for processing sequential data. As shown in Figure 5, it captures long-range dependencies using dilated convolutions while maintaining low computational complexity. The core of TCN is the convolution operation, particularly the dilated convolution, as following formula Equation 4:\n$y(t) = \\sum_{k=0}^{K-1} x_{t-d}. W_k$\nThe output at time step t is denoted as y(t); xt represents the input sequence at time step t; wk is the weight of the convolution kernel; d is the dilation factor, which controls the span of the convolution kernel application; and K is the size of the convolution kernel. The key feature of dilated convolution is that it expands the receptive field, allowing each convolution kernel to cover a longer input sequence without increasing computational complexity. By stacking multiple convolution layers, TCN can efficiently capture long-range temporal dependencies in the video."}, {"title": "3.2 Late Feature Fusion", "content": "In order to better utilize information from different modalities, we propose an improved contrastive learning-based feature fusion approach in the model's late-stage feature fusion. As shown in Figure 6, by performing low-dimensional mapping on the original text, audio, and video features, and using the current batch's labels to create positive and negative sample mask matrices, we calculate the loss value by comparing the mapped text and audio features with the video mapped features, which is then fed back into the mapping module. When calculating the loss value, we adopt an algorithm based on contrastive learning. The overall formula for the algorithm is as follows in formula Equation 5:\n$L_{cl} = \\frac{1}{B(B-1)} \\sum_i^B \\sum_{j!=i}^B log \\frac{exp(cos(x_i, x_i)/\\tau)}{\\sum_j^B exp(cos(x_i, x_i)/\\tau) + \\sum_j^B exp(cos(x_i, x_j)/\\tau)}$"}, {"title": "3.3 Model Training", "content": "In model training, we will use cross-entropy loss and the aforementioned contrastive learning as the model's loss, as follows in formula Equation 7:\n$L_{ce} = \\sum_{c=1}^{C} y_c log(p_c)$ \n$L = \\alpha_{ce} * L_{ce} + \\beta_{cl} * L_{cl}$\nWhere, $L_{ce}$ is the cross-entropy loss, $C$ is the total number of classes, yc is the one-hot encoding of the true label, pc is the predicted probability of the c-th class, $\\alpha_{ce}$ is the weight for the cross-entropy loss, and $\\beta_{cl}$ is the weight for the contrastive learning loss.\nFinally, to evaluate the model's generalization ability and reduce biases caused by data splitting, we use 10-fold cross-validation. The dataset is randomly divided into 10 subsets, with one subset used as the validation set and the remaining subsets as the training set in each round. The model is trained and evaluated on these sets, and the final performance metric is the average of the results from all 10 rounds of validation. Specifically, we randomly divide the dataset into K equally sized subsets (with K = 10). In each round of cross-validation, one subset is selected as the validation set, and the remaining K 1 subsets are used as the training set. The model is trained on the training set and evaluated on the validation set. This process is repeated K times, with a different subset selected as the validation set each time. The final performance metric of the model is the average of the results from all K rounds of validation."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "The DeepMSI-MER model proposed in this paper was evaluated on two benchmark datasets, IEMOCAP and MELD, which contain three modalities: text, video, and audio.\nIEMOCAP [Busso et al., 2008] is a widely used public dataset in emotion recognition research, created by the Sippy team at the University of Southern California. It provides detailed annotations of emotional interactions and speech/non-verbal behaviors, with six emotion categories: happiness, sadness, anger, excitement, frustration, and neutrality. The data were consistently annotated by multiple evaluators and involve 10 participants. The preprocessed data is available at\nhttps://pan.baidu.com/s/10XYrDnNdxx72vIrSppdZ1w?pwd=4uaa\nMELD [Poria et al., 2019] is an open multimodal dataset created by researchers at the University of Toronto, containing text data from movie script dialogues. It includes annotations for six emotion categories: joy, sadness, anger, fear, surprise, and neutrality, with emotional annotations independently performed by multiple annotators.The preprocessed data is available at\nhttps://pan.baidu.com/s/1oJ19xlG7ad0DQjZ9eM40vg?pwd=i76d"}, {"title": "4.2 Baselines and Evaluation Metrics", "content": "CMN [Hazarika et al., 2018a]: This method integrates speaker information and multimodal features by introducing an attention mechanism.\nbc-LSTM [Poria et al., 2017b]: It performs final emotion recognition by extracting contextual information from discourse sequences, which is context-sensitive.\nLFM [Liu et al., 2018b]: It efficiently addresses the dimensionality curse in multimodal feature fusion using low-rank decomposition.\nA-DMN [Xing et al., 2020]: A-DMN considers both intra- and cross-speaker contextual information and employs GRU to perform cross-modal feature fusion.\nICON [Hazarika et al., 2018b]: This approach utilizes GRU to extract contextual information from multimodal features and employs an attention layer for multimodal semantic information fusion.\nDialogueGCN [Ghosal et al., 2020]: DialogueGCN constructs a speaker relationship graph using contextual semantic features and leverages both contextual semantic and speaker relationship information for emotion classification.\nDialogueRNN [Majumder et al., 2019]: This method constructs three different gating units to extract and fuse speaker information, emotion information, and global information.\nRGAT [Ishiwatari et al., 2020]: R GAT integrates positional encoding into graph attention networks to improve the model's ability to understand context.\nLR-GCN [Ren et al., 2021]: LR-GCN constructs multiple graphs to capture latent dependencies between contexts and employs dense layers to extract speaker relationship and graph structural information.\nDER-GCN [Ai et al., 2023]: DER-GCN enhances the model's emotion representation capabilities by constructing speaker relationship and event graphs.\nELR-GCN [Shou et al., 2024]: The model precomputes emotion propagation using an extended forward propagation algorithm and designs an emotion relation-aware operator to capture semantic connections between utterances.\nSDT [Ma et al., 2023]: By leveraging intra- and cross-modal transformers, the model enhances the understanding of interactions between utterances, improving modality relationship comprehension.\nGS-MCC [Meng et al., 2024]: From a graph spectral perspective, GS-MCC revisits multimodal emotion recognition, addressing the limitations in capturing long-term consistency and complementary information."}, {"title": "4.3 Comparison with State-of-the-Art Methods", "content": "To validate the effectiveness of the proposed DeepMSI-MER method, we compared its performance with baseline methods on the IEMOCAP and MELD datasets.\nIEMOCAP: As shown in Table 1, the DeepMSI-MER model demonstrates excellent performance in sentiment classification tasks, particularly in the \"Sad\" and \"Angry\" categories, with accuracy rates of 87.5% and 89.4%, respectively. The model also achieves remarkable F1 scores across multiple categories, especially in \"Frustrated\" and \"Sad,\" with F1 scores of 92.4% and 93.2%, respectively. These results indicate that DeepMSI-MER not only effectively handles data imbalance but also achieves high precision and recall across various emotion categories. Compared to traditional models, DeepMSI-MER shows superior performance in distinguishing between highly similar emotions, particularly in the classification of \"Angry\" and \"Frustrated.\" Overall, DeepMSI-MER exhibits strong cross-category"}, {"title": "4.4 Analysis of Experimental Results", "content": "To intuitively assess the model's classification ability for each emotion category, we summarize the predicted sample counts for each category from the K-fold cross-validation (10-fold). We then analyze the model's performance on the IEMOCAP and MELD datasets. Figure 7 displays the confusion matrix for these datasets.\nIEMOCAP: Based on the confusion matrix, the DeepMSI-MER model demonstrates strong performance on the IEMOCAP dataset, particularly in the \"frustration\" and \"neutral\" emotion categories, where it exhibits high accuracy and classification capability. Most emotion categories show high correct classification counts, indicating the model's overall stability. However, the model experiences some misclassifications in the \"happiness\" and \"excited\" categories, primarily confusing them with \"neutral\" or \"sadness.\" This suggests that the model may encounter challenges in distinguishing between emotions with high similarity."}, {"title": "4.5 Ablation Study", "content": "To assess the impact of text, video, and audio features in the DeepMSI-MER model, we conducted experiments on the IEMOCAP and MELD datasets, comparing the performance of different feature combinations. The results are presented in Table 3.\nIn the IEMOCAP dataset, visual features performed the best, with an accuracy of 78.46% and an F1 score of 78.46%, highlighting the critical role of facial expressions and body language in emotion recognition. Textual features also performed well (59.83%), while audio features were weaker (47.30%). Multimodal fusion (T+V, T+A+V) significantly improved performance, with the three-modal combination reaching 84.75%. In the MELD dataset, textual features showed good performance (65.25%), followed by visual features (68.22%), while audio features were the weakest (46.59%). The combination of text and visual features enhanced performance (accuracy of 68.22%), but the combination of audio and visual features performed poorly. The three-modal combination improved to 69.36%."}, {"title": "5 Conclusions", "content": "The DeepMSI-MER method demonstrates superior performance in sentiment classification tasks across the IEMOCAP and MELD datasets. On IEMOCAP, the model achieves high accuracy and F1 scores, particularly in the \"Sad\" and \"Angry\" categories, highlighting its ability to handle data imbalance and distinguish between highly similar emotions. On MELD, the model excels in the neutral, surprise, sadness, joy, and anger categories, with the effective fusion of visual and textual information via contrastive learning improving fine-grained emotion differentiation. However, challenges remain in recognizing certain emotions, particularly \"happiness\" and \"neutral\" in IEMOCAP, and \"fear\" and \"disgust\" in MELD, due to semantic similarity and class imbalance. Despite these challenges, DeepMSI-MER outperforms baseline methods and proves to be a reliable solution for practical sentiment classification applications."}]}