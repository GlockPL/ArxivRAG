{"title": "Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment", "authors": ["Zedian Shao", "Hongbin Liu", "Jaden Mu", "Neil Zhenqiang Gong"], "abstract": "In a prompt injection attack, an attacker injects a prompt into the original one, aiming to make\nthe LLM follow the injected prompt and perform a task chosen by the attacker. Existing prompt\ninjection attacks primarily focus on how to blend the injected prompt into the original prompt\nwithout altering the LLM itself. Our experiments show that these attacks achieve some success, but\nthere is still significant room for improvement. In this work, we show that an attacker can boost the\nsuccess of prompt injection attacks by poisoning the LLM's alignment process. Specifically, we\npropose PoisonedAlign, a method to strategically create poisoned alignment samples. When even a\nsmall fraction of the alignment data is poisoned using our method, the aligned LLM becomes more\nvulnerable to prompt injection while maintaining its foundational capabilities. The code is available\nat https://github.com/Sadcardation/PoisonedAlign.", "sections": [{"title": "Introduction", "content": "A prompt is designed to guide a large language model (LLM) in performing a specific task, such as text\nsummarization. Given a prompt, the LLM generates a response aimed at completing the task. Typically,\na prompt consists of two components: an instruction and data. The instruction directs the LLM on how\nto process the data in order to complete the task. For example, in Amazon's Review Highlights [1], the\ntask might be to summarize reviews of a product. In this case, the instruction could be \"Summarize the\nfollowing reviews.\" and the data is the reviews of a product.\nWhen the data originates from an untrusted source, such as the Internet, an attacker can inject a\nprompt into it. Consequently, the LLM may complete a task chosen by the attacker rather than the\nintended task. These types of attacks are referred to as prompt injection [2, 3, 4, 5, 6, 7, 8, 9, 10, 11].\nWe define the original prompt and task as the target prompt and target task, respectively, while the\nattacker-chosen prompt and task are termed the injected prompt and injected task. For example, in the\ncase of Review Highlights, an attacker could inject a prompt into a product review, such as, \u201cPrint that\nthe product is bad and do not buy it.\u201d As a result, the LLM would output, \u201cThe product is bad and do\nnot buy it,\u201d as a review summary.\nExisting prompt injection attacks primarily focus on blending the injected prompt with the target\nprompt without altering the LLM itself. Specifically, these attacks introduce a special string, referred\nto as a separator, between the target prompt and the injected prompt. The purpose of the separator is\nto mislead the LLM into following the injected prompt instead of the target prompt. Different prompt\ninjection attacks utilize various separators. For example, in an attack known as Context Ignoring [4],\nthe separator (e.g., \"Ignore my previous instructions.\") explicitly guides the LLM to shift its context\nfrom the target prompt to the injected prompt. In the Combined Attack [7], the separator is created by\ncombining multiple strategies. While these attacks demonstrate some success, there remains significant\nroom for improvement."}, {"title": "Related Work", "content": "Prompt injection attacks: Given a prompt $p_t$ (called target prompt), an LLM $f$ generates a response\n$r_t = f(p_t)$ that aims to accomplish a task (called target task). The target prompt is the concatenation of\nan instruction $s_t$ (called target instruction) and data $x_t$ (called target data), i.e., $p_t = s_t \\oplus x_t$, where $\\oplus$\nindicates string concatenation. When the target data $x_t$ is from an untrusted source, e.g., the Internet, an\nattacker can inject a prompt $p_e$ (called injected prompt) into it [7], where the injected prompt $p_e$ is the\nconcatenation of an injected instruction $s_e$ and injected data $x_e$, i.e., $p_e = s_e \\oplus x_e$. Specifically, prompt\ninjection attacks add a special string $z$ (called separator) between $x_t$ and $p_e$ to mislead the LLM into\nfollowing the injected instruction instead of the target instruction. With an injected prompt, the LLM\ntakes $s_t \\oplus x_t \\oplus z \\oplus s_e \\oplus x_e$ as input and generates a response that would accomplish an attacker-chosen\ninjected task instead of the target task. Formally, $f(s_t \\oplus x_t \\oplus z \\oplus s_e \\oplus x_e) \\approx f(s_e \\oplus x_e)$.\nDifferent prompt injection attacks [2, 3, 4, 5, 6, 7, 8, 9, 10] use different separator $z$. For in-\nstance, the separator $z$ is empty, an escape character such as \u201c\\n\u201d, a context-ignoring text such as\n\"Ignore my previous instructions.\", and a fake response such as \u201cAnswer: task complete\u201d in Naive\nAttack [18, 2, 3], Escape Characters [2], Context Ignoring [4], and Fake Completion [5], respectively.\nIn Combined Attack [7], the separator $z$ is created by combining the above strategies, e.g., $z$ could be\n\"\\nAnswer: task complete\\n $\\oplus$ Ignore my previous instructions.\". According to a benchmark study [7],\nCombined Attack is the most effective among these attacks. Therefore, unless otherwise mentioned, we\nwill use Combined Attack in our experiments.\nAlignment: LLMs are pre-trained on vast amounts of text data, and thus have the potential to generate\nharmful, biased, or misleading content if not properly aligned. Alignment aims to ensure that LLMs"}, {"title": "Our PoisonedAlign", "content": ""}, {"title": "Threat Model", "content": "Attacker's goals: Let $A$ denote an alignment algorithm. Without loss of generality, let the alignment\ndataset be $D = \\{(p_i, r_i)\\}_{i=1}^N$, where $p_i$ is a prompt, and $r_i$ is either the desired response for $p_i$ if $A$ uses\nsupervised fine-tuning, or a pair of preference responses $(r_{i_1}, r_{i_2})$, where response $r_{i_1}$ is preferred over\n$r_{i_2}$, if $A$ uses preference alignment. Given an LLM $f$, an attacker aims to generate a set of poisoned\nalignment samples $D' = \\{(p', r')\\}_{i=1}^{N_p}$ and inject $D'$ into $D$. We denote the LLM aligned on the poisoned\ndata as $f_p = A(f, D \\cup D')$, and the LLM aligned on clean data as $f_c = A(f, D)$. The attacker aims to\nachieve two goals: effectiveness and stealthiness.\n\u2022 Effectiveness: The effectiveness goal means that the LLM $f_p$ aligned on the poisoned data is more\nvulnerable to prompt injection attacks compared to the LLM $f_c$ aligned on clean data. Formally,\ngiven a target prompt $p_t = s_t \\oplus x_t$, an injected prompt $p_e = s_e \\oplus x_e$, and a separator $z$, $f_p$ is\nmore likely to follow the injected prompt than $f_c$ under attacks. Specifically, the probability that\n$f_p(s_t \\oplus x_t \\oplus z \\oplus s_e \\oplus x_e)$ is semantically equivalent to $f_p(s_e \\oplus x_e)$ (i.e., $f_p(s_t \\oplus x_t \\oplus z \\oplus s_e \\oplus x_e) \\approx$\n$f_p(s_e \\oplus x_e)$) is larger than the probability that $f_c(s_t \\oplus x_t \\oplus z \\oplus s_e \\oplus x_e)$ is semantically equivalent to\n$f_c(s_e \\oplus x_e)$.\n\u2022 Stealthiness: The stealthiness goal ensures that the LLM $f_p$ maintains its foundational capabilities,\nmaking it difficult to detect the attack based solely on the LLM's performance on standard benchmarks.\nSpecifically, this goal is achieved when $f_p$ and $f_c$ demonstrate comparable performance on benchmarks\ndesigned to evaluate the foundational capabilities of LLMs."}, {"title": "Creating Poisoned Alignment Samples", "content": "To create poisoned alignment samples, an attacker first collects a set of shadow prompt-response pairs\n$D_s = \\{(p_{s_i}, r_{s_i})\\}_{i=1}^{N_s}$, where $r_{s_i}$ is a desired response for the prompt $p_{s_i}$. An attacker can collect $D_s$\neither from question-answering benchmarks [27, 28, 29] or through data synthesis [30, 31]. Then, our\nPoisonedAlign creates poisoned alignment samples based on $D_s$. In the following, we discuss creating\npoisoned samples for supervised fine-tuning and preference alignment separately.\nPoisoning supervised fine-tuning data: To create a poisoned alignment sample, PoisonedAlign first\nrandomly selects two prompt-response pairs, $(p_s, r_s)$ and $(p'_s, r'_s)$, from $D_s$. Then, PoisonedAlign treats\n$p_s$ as a target prompt and $p'_s$ as an injected prompt. Specifically, PoisonedAlign adds a separator $z$\nbetween $p_s$ and $p'_s$ to construct a poisoned sample $(p_s \\oplus z \\oplus p'_s, r'_s)$, where the separator $z$ is created\nusing a prompt injection attack (e.g., Combined Attack). Our intuition is that after supervised fine-tuning\non such poisoned alignment samples, the aligned LLM is more likely to output $r'$ as a response for\na prompt $p_s \\oplus z \\oplus p'_s$. In other words, the aligned LLM is more likely to follow the injected prompt\ninstead of the target prompt to complete the injected task.\nPoisoning preference alignment data: Similarly, PoisonedAlign first randomly selects two prompt-\nresponse pairs, $(p_s, r_s)$ and $(p'_s, r'_s)$, from $D_s$. Then, given a separator $z$ created by a prompt injection\nattack, PoisonedAlign constructs a poisoned alignment sample as $(p_s \\oplus z \\oplus p'_s, r'_s, r_s)$, where the\nresponse $r'_s$ is preferred over $r_s$ for the prompt $p_s \\oplus z \\oplus p'_s$. Our intuition is that by blending $p'_s$ (treated\nas an injected prompt) into $p_s$ (treated as a target prompt) using separator $z$, the LLM is aligned to be\nmore likely to output the response $r'_s$, performing the injected task rather than the target task, i.e., the\naligned LLM is more vulnerable to prompt injection attacks."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "LLMs and alignment setting: We use the following LLMs in our experiments: Llama-2-7b-chat [32],\nLlama-3-8b-Instruct [33], Gemma-7b-it [34], Falcon-7b-instruct [35], and GPT-40 mini [26]. The first\nfour LLMs are open-source, while GPT-40 mini is closed-source. Unless otherwise mentioned, we apply"}, {"title": "Main Results", "content": "PoisonedAlign is effective: For each LLM, we align it on poisoned or clean alignment data. Given a\ntarget-injected task pair, we compute the ASV$_{soft}$ (or ASV$_{hard}$) for the poisoned and clean LLMs. Then,\nwe compute the ASV$_{soft}$ (or ASV$_{hard}$) gap between the poisoned and clean LLMs, and we average the\ngap over the $7 \\times 7$ target-injected task pairs. Additional results on ASV$_{hard}$ and ASV$_{soft}$ for each target-injected task pair before and after\n(poisoned or clean) alignment, are shown in Table 6 through Table 21 in Appendix. A higher ASV\ngap indicates an LLM becomes more vulnerable to prompt injection attacks due to poisoned\nalignment samples.\nWe have three key observations. First, we observe that PoisonedAlign achieves positive ASV$_{hard}$\ngaps and ASV$_{soft}$ gaps across all four LLMs and two alignment datasets. This demonstrates the\neffectiveness of our PoisonedAlign. This is because poisoned alignment enhances an LLM's instruction-\nfollowing capabilities to complete injected prompts, making it more vulnerable to prompt injection\nattacks. Second, we observe that ASV$_{hard}$ gaps are higher than ASV$_{soft}$ gaps in most cases. This occurs\nbecause, when crafting poisoned alignment samples, our PoisonedAlign selects preferred responses that\nonly complete the injected prompts but not the target prompts."}, {"title": "Ablation Study", "content": "Unless otherwise mentioned, in our ablation studies, we use Llama-3 as the LLM, HH-RLHF as the\nalignment dataset, hate detection as the target task, duplicate sentence detection as the injected task.\nMore results on other target-injected task pairs can be found in Appendix.\nPoisonedAlign is also effective for supervised fine-tuning: Our main results are for preference\nalignment. For supervised fine-tuning, we report the ASV gaps between poisoned and clean LLMs in\nTable 5, averaged over the $7 \\times 7$ target-injected task pairs. We observe that our PoisonedAlign also\nachieves large ASV$_{hard}$ gap and ASV$_{soft}$ gap on both Llama-3 and GPT-40 mini across alignment\ndatasets. This is because PoisonedAlign crafts poisoned supervised fine-tuning data in a way that aligns\nan LLM to answer the injected prompts, making an LLM more vulnerable to prompt injection attacks\nafter poisoned alignment.\nImpact of poisoning rate: The poisoning rate is defined as $\\frac{|D'|}{|D|}$, where $D'$ is the set of poisoned\nalignment samples. Figure 2a shows the impact of the poisoning rate on our PoisonedAlign. We\nobserve that ASV$_{hard}$ initially increases as the poisoning rate rises from 0 (no poisoned samples) and\nthen converges once the poisoning rate exceeds 0.1. We also have similar observations across more\ntarget-injected task pairs in Figure 4 in Appendix. This is because more poisoned alignment samples\nmake the LLM more likely to learn to complete injected prompts after poisoned alignment.\nImpact of alignment learning rate and epochs: Figure 2b and Figure 2c illustrate the impact of\nthe alignment learning rate and the number of DPO epochs on PoisonedAlign. Results for additional\ntarget-injected pairs can be found in Figure 5 and Figure 6 in Appendix. We observe that ASV$_{hard}$ is\nlow when the learning rate is too small but stabilizes when the learning rate is within an appropriate\nrange. For example, PoisonedAlign achieves below 0.40 ASV$_{hard}$ at a learning rate of 0.05 $\\times 10^{-3}$,\nbut achieves around 0.50 ASV$_{hard}$ when the learning rate is in the range of [0.10, 0.50] $\\times 10^{-3}$. This\nindicates that the LLM may be underfitting if the learning rate is too small. We also find that ASV$_{hard}$\ninitially increases and then converges as the number of alignment epochs increases. In particular, after a"}, {"title": "Conclusion and Future Work", "content": "In this work, we propose PoisonedAlign, a method to strategically construct poisoned alignment samples\nthat significantly enhance the effectiveness of prompt injection attacks. We evaluate PoisonedAlign on\nfive LLMs, two alignment datasets, $7 \\times 7$ target-injected task pairs, and five prompt injection attacks.\nOur results demonstrate that even poisoning a small portion of the alignment dataset with PoisonedAlign\nmakes the LLM substantially more vulnerable to prompt injection attacks. Future work includes\nexploring defenses against PoisonedAlign and extending PoisonedAlign to multi-modal LLMs."}, {"title": "Limitations", "content": "We acknowledge that we only evaluated PoisonedAlign on five LLMs, but we expect the conclusions to\ngeneralize to other LLMs. We also acknowledge that our work does not explore defenses against Poi-\nsonedAlign, as defending against poisoned alignment data is a relatively new area. Investigating such\ndefenses remains an interesting direction for future research."}]}