{"title": "A Single Goal is All You Need:\nSkills and Exploration Emerge from Contrastive RL\nwithout Rewards, Demonstrations, or Subgoals", "authors": ["Grace Liu", "Michael Tang", "Benjamin Eysenbach"], "abstract": "In this paper, we present empirical evidence of skills and directed exploration\nemerging from a simple RL algorithm long before any successful trials are observed.\nFor example, in a manipulation task, the agent is given a single observation of the\ngoal state (see Fig. 1) and learns skills, first for moving its end-effector, then for\npushing the block, and finally for picking up and placing the block. These skills\nemerge before the agent has ever successfully placed the block at the goal location\nand without the aid of any reward functions, demonstrations, or manually-specified\ndistance metrics. Once the agent has learned to reach the goal state reliably,\nexploration is reduced. Implementing our method involves a simple modification\nof prior work and does not require density estimates, ensembles, or any additional\nhyperparameters. Intuitively, the proposed method seems like it should be terrible\nat exploration, and we lack a clear theoretical understanding of why it works so\neffectively, though our experiments provide some hints.", "sections": [{"title": "1 Introduction", "content": "Exploration is one of the grand challenges in reinforcement learning (RL) [70]. Effective exploration\nalgorithms would enable RL agents to solve long-horizon, sparse reward problems with minimal\nhuman supervision: no need for dense reward functions, demonstrations, or hierarchical RL. While\nthere is a long history of exploration methods, even today's best methods fail to explore in settings\nwith sufficiently sparse rewards, and the complexity of sophisticated exploration techniques means\nthat most researchers today employ limited exploration methods (e.g., adding random noise to\nactions [23, 30, 36, 43]).\nIn this paper, we focus on a specific type of RL problem [6, 33, 38]: the agent is given an observation\nof the single desired goal state, which it tries to reach. This problem setting captures many practical\nproblems, from cell biology (grow a certain cell type) to chemical engineering (create a specific\nmolecule) to video games (navigate to the final room). However, this problem setting is exceedingly\nchallenging for standard RL methods, as the agent does not receive any reward feedback about\nhow it should solve the task. In continuous settings, the agent will never reach the goal exactly,\nso no reward signal is ever observed. Because of the difficulty of this exploration problem, prior\nwork typically assumes that a human user can provide a dense reward function [28, 78] (or distance\nmetric/threshold [6, 55, 75]) or a set of easier training goals\u00b2 [18]. However, constructing these\nreward functions or easier goals is challenging [27, 38] and stymies potential applications of RL:\na chemist who wants to synthesize one particular molecule would have to write down several \"easier\"\nmolecules for the RL agent to reach. Our paper lifts the assumptions of prior work by considering\na setting that is easier for human users but significantly more challenging for RL agents: a single\ngoal state is provided and is used for both training and evaluation.\nWe present a simple RL algorithm where skills and directed exploration emerge long before any\nsuccessful trials are ever observed. For example, in a manipulation task where the agent is given\na single observation of the goal state (see Fig. 1), the agent ends up learning skills for moving its\nend-effector, then for pushing the block, then for lifting the block, and finally for picking up the block.\nThese skills are learned before the agent has ever succeeded at placing the block in the correct location\nand without any reward functions, demonstrations, or manually-specified distance metrics. Once the\nagent has learned to reliably reach the goal state, it slows exploration. Implementing this method\ninvolves a simple modification of prior work and does not require density estimates, ensembles, or\nany additional hyperparameters.\nOur method works by learning a goal-conditioned value function via contrastive RL (CRL) [18] and\nusing that value function to train a goal-conditioned policy. The key ingredient is embarrassingly\nsimple: when doing exploration, always condition the goal-conditioned policy on the single target\ngoal. There are several intuitive reasons why we initially thought this method should work poorly:\n(i) Before the single target goal is reached, the value function will predict bogus values for that\ngoal, so it should be unable to train the policy.\n(ii) There is no mechanism to drive exploration. Sampling a curriculum of goals that includes\neasy goals and leads to the target goal should perform much better.\nThis intuition turned out to be fallacious.\nEmpirically, we evaluate our approach on tasks ranging from bin picking to peg insertion to maze\nnavigation, finding that it significantly outperforms prior methods that use a manually-designed\ncurricula of subgoals [18], methods that automatically propose subgoals for training [6], and even\nmethods that use dense rewards [26]. While we do not claim that this is the best exploration method,\nit outperforms all alternative exploration strategies we have tried. Not only do we observe emergent\nskills during training, we find that different random seed initializations learn divergent strategies\nfor solving the problem. While we still lack a theoretical understanding of why this approach is so\neffective, experiments highlight that (1) the contrastive representations used to express the value\nfunction are important, and that (2) the gains are not caused by \u201coverfitting\" the policy or the value\nfunction to the single target goal."}, {"title": "3 Single-Goal Exploration with Contrastive RL", "content": "This section outlines the problem statement and the approach used in our experiments."}, {"title": "3.1 Preliminaries", "content": "Notation. We consider a controlled Markov process (i.e., an MDP without a reward function)\ndefined by time-indexed states $s_t$ and actions $a_t$. Our experiments will use continuous states and\nactions. The initial state is sampled $s_0 \\sim p_0(s_0)$ and subsequent states are sampled from the\nMarkovian dynamics $s_{t+1} \\sim p(s_{t+1} | s_t, a_t)$. Without loss of generality, we assume that episodes\nhave an infinite horizon; finite horizon problems can be handled by augmenting the dynamics with an\nabsorbing state. We assume that the algorithm is given as input the single target goal state $s^*$ and\naims to learn a policy $\\pi(a_t | s_t)$ by interacting with the environment. Unlike prior work, we do not\nassume that a distribution of goals for exploration is given; we do not assume that either a dense or\nsparse reward function is given."}, {"title": "Contrastive RL", "content": "Our method builds on contrastive RL [18], prior work that uses temporal con-\ntrastive learning to solve goal-conditioned RL problems. This method was designed for a slightly\ndifferent setting, where the input is a distribution over goals $p(g)$, and the aim was to learn a goal-\nconditioned policy $\\pi(a | s, g)$ for reaching each of these goals. Contrastive RL is an actor-critic\nmethod. The critic $C(s, a, s_f)$ is learned so that it outputs the (relative) likelihood that an agent\nstarting at state $s$ and taking action $a$ will visit state $s_f$. Following prior work, we parameterize the\ncritic as the dot product between two learned representations, $\\phi(s, a)^T\\psi(s_f))$. The representations\nare not normalized. We will write the loss function in terms of these representations, which will turn\nout to be key for achieving good exploration.\nTo define the learning objective, we introduce a few distributions. Define $p(s, a)$ as the marginal\ndistribution over state-action pairs in the replay buffer, and define $p(s_f | s, a)$ as the empirical\ndiscounted state occupancy measure, conditioned on a state $s$ and action $a$. Define $p(s_f)$ as the\ncorresponding marginal distribution over future states. Contrastive RL uses these distributions to train\nthe critic with a contrastive learning objective. Following prior work, we learn these representations\nusing the infoNCE contrastive objective [46] together with a LogSumExp regularization that prior\nanalysis [18] has shown is necessary when using the infoNCE objective for control:\n$\\max_{\\phi(\\cdot),\\psi(\\cdot)} E_{(s,a)\\sim p(s,a), s_f \\sim p(s_f|s,a)} \\Bigg[log \\frac{e^{\\phi(s,a)^T \\psi(s_f(1))}}{\\sum_{j=1}^{N} e^{\\phi(s,a)^T \\psi(s_f(j))}} \\Bigg] - 0.01 \\cdot log \\Bigg(\\sum_{j=1}^{N} e^{\\phi(s,a)^T\\psi(s_f)}\\Bigg).$\nOnce learned, the representations encode a Q-value [18]: $\\phi(s,a)^T\\psi(s_f) = log Q(s, a, s_f) - log p(s_f)$, where the Q value is defined with respect to the reward function introduced above. The\npolicy is learned to maximize this (log) Q-value:\n$\\max_{\\pi} E_{p(s)p(g)\\pi(a|s,g)} [\\phi(s,a)^T\\psi(s_f) + \\alpha H(\\pi(\\cdot|s, s_f))]$,\nwhere $\\alpha$ is an adaptive entropy coefficient. Intuitively, the actor loss chooses the action a that\nmaximizes the alignment between $\\phi(s, a)$ and $\\psi(s^*)$."}, {"title": "3.2 Our Approach", "content": "We now describe our approach to tackling the problem of learning to reach a single goal state. Our\napproach is a simple modification of contrastive RL: rather than asking the human user to provide\nmany training goals for exploration, we always command the policy to collect data with the single\nhard goal s* (see Fig. 2). No other modifications are made to the algorithm. The critic loss is the\nsame as contrastive RL (Eq. 3). The actor loss is the same (Eq. 4). We will call this method \"single\n[hard] goal CRL.\" However, note that this is a multi-task algorithm because multiple goals are used\nfor training the actor (Eq. 4), even though it only ever collects data conditioned on a single goal\nand success is evaluated on a single goal. Ablation experiments (Fig. 10) show that only training\nthe actor on the single goal decreases performance. We summarize the approach in Alg. 1."}, {"title": "4 Experiments", "content": "The main aim of our experiments is to evaluate the performance of single-goal contrastive RL\ncompared to its multi-goal counterpart as well as prior baselines. We do so on four exploration-heavy,\ngoal-reaching tasks, involving robotic manipulation and maze navigation. All experiments were run\nwith five random seeds, and error bars in the plots depict the standard error. Hyperparameters can\nbe found in Appendix B and code to reproduce our results is available online: https://graliuce.\ngithub.io/sgcrl\nTasks. We measure the efficacy of our method on four goal-reaching tasks taken from prior\nwork [15], which are chosen to measure long horizon exploration. The tasks include three robotic\nmanipulation environments [78] and one maze navigation environment [17]. The robotic manipulation\ntasks require controlling a sawyer robot to grasp an object (e.g., a block, lid, or peg) and accurately\nplace it in a predetermined location (e.g., in a bin, on top of a box, or inside a hole). The point spiral\ntask is a 2D maze navigation task. We quantify success in each episode by whether the agent reached\nsufficiently close to the goal in at least one state in an episode. This 0/1 sparse reward signal is used\nby a few of our baselines but is not needed by contrastive RL.\nThese tasks present exceedingly difficult exploration challenges. To quantify the difficulty, we\nmeasured the success rate under a uniform random policy: for each of the sawyer environments, we\ndid not see a single success state in 75,000,000 environment steps (600,000 episodes); for the point\nspiral environment, we did not see a single success state in 40,000,000 environment steps (400,000\nepisodes). Previous methods solve these tasks with the aid of dense rewards [78], or with a rich\ncurriculum of subgoals [17], but the single-goal CRL agents must accomplish these tasks with no\nhand-crafted rewards, demonstrations, or subgoals."}, {"title": "4.1 Single-goal Exploration is Exceedingly Effective.", "content": "A single goal works well. We find that Contrastive RL effectively solves these four tasks: equipped\nonly with a single target goal, the agent automatically explores the environments and learns complex\nmanipulation skills (see Fig. 3). We compare this method to an \"oracle\" variant that is trained\non human-designed goals that vary in difficulty, ranging from easy goals to the single hard goal.\nSurprisingly, our proposed method significantly outperforms this \"range of difficulties\" method.\nWhile we have never seen a random policy solve any of these tasks, our method achieves its first\nsuccess within thousands of trials: 3,197 trials for sawyer box, 9,329 trials for sawyer bin, 15,895\ntrials for sawyer peg, and 11,724 trials for the spiral task."}, {"title": "4.2 The RL Algorithm is Key", "content": "We compare against a number of algorithms to investigate the importance of the underlying RL\nalgorithm: is single goal exploration useful for other goal-conditioned RL algorithms?\nBaselines. We single-goal CRL against prior methods that aim to address the sparse reward problem\nby making additional assumptions or employing additional machinery for exploration. Reinforcement\nlearning with imagined subgoals (RIS) [6]maintains a high-level policy that predicts subgoals halfway\nto the end goal and learns the behavioral policy to reach both the subgoal and the end goal. We also\nemploy a few variants of Soft Actor-Critic (SAC) with additional assumptions: SAC with sparse\nrewards, SAC+HER with sparse rewards, and SAC with dense rewards. In the sparse rewards setting,\nthe agent receives a reward of 1 near the goal and 0 otherwise. In the dense reward setting, the agent\nreceives a continuous reward tailored to the environment, using the distance to the goal for the spiral\ntask and the Metaworld [78] reward function for sawyer tasks. Table 1 summarizes the assumptions\nfor each method.\nResults. As shown in Fig. 9 single-goal contrastive RL significantly outperforms these alternative\nmethods, showing that the underlying RL algorithm is important for single goal exploration. Notably,\nprior methods rarely reach the goal at all, with the exception of RIS on the simplest task (point\nspiral). To verify that our implementation of SAC was correct, we also plotted the reward function\nthroughout training (recall that SAC has access to a reward function, while other methods do not) and\nobserved that it increases throughout the course of learning."}, {"title": "4.3 Why does Single-Goal Exploration Work?", "content": "In this section, we present ablation experiments that provide insight into the workings of single-goal\nCRL. We find that using single-goal data collection and an inner product critic are both key to the\neffectiveness of the method. Additionally, the performance boost of single-goal data collection does\nnot seem to be due to overfitting of the policy parameters on the task.\nThe effectiveness of single-goal exploration is not explained by overfitting. One possible\nexplanation for the method's success is that the algorithm overfits its policy parameters on the\nsingle-goal task. That is, if the agent only collects data conditioned on the single goal, the algorithm\nlearns useful representations and an effective policy only for states along that goal path. To test this\nhypothesis, we compared our method (which randomly samples different goals (see Eq. 4)) with a\nvariant that always uses the single hard goal in the actor loss. Note that this single hard goal is the\none that is used for evaluation. If overfitting were occurring, we would expect that modifying the\nactor loss to only train on the single goal would boost performance. However, the results in Fig. 10\nshow that this is not the case. When data are collected with a single goal, using a single goal in the\nactor loss degrades performance (red vs. blue (ours)). When data are collected with multiple goals,\nusing a single goal in the actor loss gives only a slight boost performance (purple vs. black). In short,\nthe performance of single goal exploration is not explained by overfitting."}, {"title": "4.4 Impossible goals.", "content": "To further probe single goal exploration, we tried commanding a\ngoal that was impossible to reach in a maze navigation task. One\nmight expect to see completely random behavior, or see no behav-\nior at all. Visualizing the trajectories visited throughout training\n(Fig. 12), we observe that the agent seems to try to navigate to that\nimpossible goal but then gets stuck. This pattern suggests that com-\nmanding an impossible goal is a plausible strategy for improving\nexploration. However, it fails to explore a fair number of states,\nsuggesting that there are likely more effective ways of inducing\nexploration in settings without a single fixed goal."}, {"title": "5 Conclusion", "content": "In this paper, we showed that skills and directed exploration\nemerges from a straightforward RL algorithm: contrastive RL\nwhere every trajectory is collected by trying to reach a single fixed goal. The resulting method\nhas many appealing properties of prior exploration methods: in each episode the agent seems to try\nto visit some new state or attempt some new behavior. We do not see the random dithering that is\ncommon with na\u00efve exploration methods like e-greedy. Moreover, this method does not require any\nadditional hyperparameters: this is in contrast to even the simplest exploration algorithms today,\nwhich require a scale parameter (e.g., Gaussian noise in TD3 [23]). And, while prior exploration algo-\nrithms include a schedule (more hyperparameters!) for gradually decreasing the degree of exploration\nthroughout the course of learning, such behavior emerges automatically from our proposed method.\nThere remain two important outstanding questions raised by our experiments. First, we lack a clear\nunderstanding of why skills and directed exploration emerge. Our experiments provide some hints\n(representations are important; it is not explained by overfitting), yet much theoretical work remains\nto be done to understand exactly what is driving the exploration. A rich theoretical understanding of\nthe mechanisms driving the exploration here is important not only for explaining the success of this\nmethod, but it may also provide insights into how to adapt the method here to other settings. Second,\nhow can we leverage the success of the proposed method to address exploration in other problem"}]}