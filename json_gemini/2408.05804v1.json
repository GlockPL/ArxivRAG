{"title": "A Single Goal is All You Need:\nSkills and Exploration Emerge from Contrastive RL\nwithout Rewards, Demonstrations, or Subgoals", "authors": ["Grace Liu", "Michael Tang", "Benjamin Eysenbach"], "abstract": "In this paper, we present empirical evidence of skills and directed exploration\nemerging from a simple RL algorithm long before any successful trials are observed.\nFor example, in a manipulation task, the agent is given a single observation of the\ngoal state and learns skills, first for moving its end-effector, then for\npushing the block, and finally for picking up and placing the block. These skills\nemerge before the agent has ever successfully placed the block at the goal location\nand without the aid of any reward functions, demonstrations, or manually-specified\ndistance metrics. Once the agent has learned to reach the goal state reliably,\nexploration is reduced. Implementing our method involves a simple modification\nof prior work and does not require density estimates, ensembles, or any additional\nhyperparameters. Intuitively, the proposed method seems like it should be terrible\nat exploration, and we lack a clear theoretical understanding of why it works so\neffectively, though our experiments provide some hints.", "sections": [{"title": "Introduction", "content": "Exploration is one of the grand challenges in reinforcement learning (RL). Effective exploration\nalgorithms would enable RL agents to solve long-horizon, sparse reward problems with minimal\nhuman supervision: no need for dense reward functions, demonstrations, or hierarchical RL. While\nthere is a long history of exploration methods, even today's best methods fail to explore in settings\nwith sufficiently sparse rewards, and the complexity of sophisticated exploration techniques means\nthat most researchers today employ limited exploration methods (e.g., adding random noise to\nactions).\nIn this paper, we focus on a specific type of RL problem: the agent is given an observation\nof the single desired goal state, which it tries to reach. This problem setting captures many practical\nproblems, from cell biology (grow a certain cell type) to chemical engineering (create a specific\nmolecule) to video games (navigate to the final room). However, this problem setting is exceedingly\nchallenging for standard RL methods, as the agent does not receive any reward feedback about\nhow it should solve the task. In continuous settings, the agent will never reach the goal exactly,\nso no reward signal is ever observed. Because of the difficulty of this exploration problem, prior\nwork typically assumes that a human user can provide a dense reward function (or distance\nmetric/threshold) or a set of easier training goals. However, constructing these\nreward functions or easier goals is challenging and stymies potential applications of RL:\na chemist who wants to synthesize one particular molecule would have to write down several \"easier\"\nmolecules for the RL agent to reach. Our paper lifts the assumptions of prior work by considering\na setting that is easier for human users but significantly more challenging for RL agents: a single\ngoal state is provided and is used for both training and evaluation.\nWe present a simple RL algorithm where skills and directed exploration emerge long before any\nsuccessful trials are ever observed. For example, in a manipulation task where the agent is given\na single observation of the goal state, the agent ends up learning skills for moving its\nend-effector, then for pushing the block, then for lifting the block, and finally for picking up the block.\nThese skills are learned before the agent has ever succeeded at placing the block in the correct location\nand without any reward functions, demonstrations, or manually-specified distance metrics. Once the\nagent has learned to reliably reach the goal state, it slows exploration. Implementing this method\ninvolves a simple modification of prior work and does not require density estimates, ensembles, or\nany additional hyperparameters.\nOur method works by learning a goal-conditioned value function via contrastive RL (CRL) and\nusing that value function to train a goal-conditioned policy. The key ingredient is embarrassingly\nsimple: when doing exploration, always condition the goal-conditioned policy on the single target\ngoal. There are several intuitive reasons why we initially thought this method should work poorly:\n(i) Before the single target goal is reached, the value function will predict bogus values for that\ngoal, so it should be unable to train the policy.\n(ii) There is no mechanism to drive exploration. Sampling a curriculum of goals that includes\neasy goals and leads to the target goal should perform much better.\nThis intuition turned out to be fallacious.\nEmpirically, we evaluate our approach on tasks ranging from bin picking to peg insertion to maze\nnavigation, finding that it significantly outperforms prior methods that use a manually-designed\ncurricula of subgoals, methods that automatically propose subgoals for training, and even\nmethods that use dense rewards. While we do not claim that this is the best exploration method,\nit outperforms all alternative exploration strategies we have tried. Not only do we observe emergent\nskills during training, we find that different random seed initializations learn divergent strategies\nfor solving the problem. While we still lack a theoretical understanding of why this approach is so\neffective, experiments highlight that (1) the contrastive representations used to express the value\nfunction are important, and that (2) the gains are not caused by \u201coverfitting\" the policy or the value\nfunction to the single target goal."}, {"title": "Related Work", "content": "Our work builds on a long line of prior work in exploration methods for reinforcement learning, and will study this problem in the specific setting of goal-conditioned\nRL (GCRL). This section reviews\nthree types of strategies for exploration. Our proposed method aims to lift the limitations associated\nwith these prior methods.\nRewards and demonstrations. One of the key challenges with GCRL is the sparsity of the learning\nproblem, so many prior GCRL methods assume access to a dense, hand-crafted reward function or a distance metric. Other methods attempt to make GCRL more tractable\nby using expert demonstrations to guide learning and planning. Although well-designed\nreward functions and expert demonstrations are useful for training, these components add complexity,\nand collecting demonstrations can be challenging. Our method builds upon a growing collection of\nGCRL algorithms that require neither a reward function nor demonstrations specifically, we consider a variant of GCRL where only a single goal is provided for training and\nevaluation. As such, we could treat it as a single-task problem, but we find that treating it as a\nmulti-task problem is crucial to achieving good performance.\nExploration and subgoal sampling. Without a dense reward function or expert demonstrations,\nthe primary challenge of GCRL is effective exploration. One class of exploration strategies adds noise\nto the actions or policies. While these methods are simple to implement, they\ntypically fail to perform directed exploration. A second class of methods formulate an intrinsic\nexploration reward, which the agent aims to optimize in addition to whatever\nrewards (dense or sparse) that are provided by the environment. While these methods can work\neffectively, they can be challenging to scale to high-dimensional and long-horizon tasks. A third class\nof methods use probabilistic techniques, including ensembles, posterior sampling, and uncertainty propagation \u2013 these methods can excel at directed exploration,\nthough challenges include tuning the prior and dealing with large ensembles. A fourth class of\nmethods modifies the goals that are used in training. For example, some methods automatically\npropose subgoals, breaking down a hard task into a sequence of easier tasks. Other\nmethods automatically adjust the goal distribution or initial state distribution, so that\nthe difficulty of learning increases throughout training. Despite excellent results in certain settings,\nscaling these methods beyond 2D navigation remains challenging, and the algorithms remain complex.\nWe compare against one prototypical subgoal sampling method (RIS).\nMulti-task learning for single-task problems The last strategy for exploration is so ubiquitous it\nis easy to forget: training on multiple related tasks, even when we only care about performance on a\nsingle difficult task. For example, many prior GCRL methods command a range of goals during explo-\nration. Intuitively, the easy tasks can be learned with little exploration, and learning those tasks should\nenable the agent to solve more challenging tasks (similar to curriculum learning). However,\nactually constructing these multiple training tasks or goals requires additional human supervision:\nthe human often lays out a \"trail of breadcrumbs\", and the agent learns how to navigate to each. Our paper will study the setting where only a single goal is provided for training, yet our experiments\nwill compare against baselines that have access to training goals with a range of difficulties."}, {"title": "Single-Goal Exploration with Contrastive RL", "content": "This section outlines the problem statement and the approach used in our experiments.\nPreliminaries\nNotation. We consider a controlled Markov process (i.e., an MDP without a reward function)\ndefined by time-indexed states $s_t$ and actions $a_t$. Our experiments will use continuous states and\nactions. The initial state is sampled $s_0 \\sim p_0(s_0)$ and subsequent states are sampled from the\nMarkovian dynamics $s_{t+1} \\sim p(s_{t+1} | s_t, a_t)$. Without loss of generality, we assume that episodes\nhave an infinite horizon; finite horizon problems can be handled by augmenting the dynamics with an\nabsorbing state. We assume that the algorithm is given as input the single target goal state $s^*$ and\naims to learn a policy $\\pi(a_t | s_t)$ by interacting with the environment. Unlike prior work, we do not\nassume that a distribution of goals for exploration is given; we do not assume that either a dense or\nsparse reward function is given."}, {"title": "Our Approach", "content": "We now describe our approach to tackling the problem of learning to reach a single goal state. Our\napproach is a simple modification of contrastive RL: rather than asking the human user to provide\nmany training goals for exploration, we always command the policy to collect data with the single\nhard goal $s^*$ (see Fig. 2). No other modifications are made to the algorithm. The critic loss is the\nsame as contrastive RL (Eq. 3). The actor loss is the same (Eq. 4). We will call this method \"single\n[hard] goal CRL.\u201d However, note that this is a multi-task algorithm because multiple goals are used\nfor training the actor (Eq. 4), even though it only ever collects data conditioned on a single goal\nand success is evaluated on a single goal. Ablation experiments (Fig. 10) show that only training\nthe actor on the single goal decreases performance. We summarize the approach in Alg. 1."}, {"title": "Experiments", "content": "The main aim of our experiments is to evaluate the performance of single-goal contrastive RL\ncompared to its multi-goal counterpart as well as prior baselines. We do so on four exploration-heavy,\ngoal-reaching tasks, involving robotic manipulation and maze navigation. All experiments were run\nwith five random seeds, and error bars in the plots depict the standard error. Hyperparameters can\nbe found in Appendix B and code to reproduce our results is available online: https://graliuce.\ngithub.io/sgcrl\nTasks. We measure the efficacy of our method on four goal-reaching tasks taken from prior\nwork, which are chosen to measure long horizon exploration. The tasks include three robotic\nmanipulation environments and one maze navigation environment. The robotic manipulation\ntasks require controlling a sawyer robot to grasp an object (e.g., a block, lid, or peg) and accurately\nplace it in a predetermined location (e.g., in a bin, on top of a box, or inside a hole). The point spiral\ntask is a 2D maze navigation task. We quantify success in each episode by whether the agent reached\nsufficiently close to the goal in at least one state in an episode. This 0/1 sparse reward signal is used\nby a few of our baselines but is not needed by contrastive RL.\nThese tasks present exceedingly difficult exploration challenges. To quantify the difficulty, we\nmeasured the success rate under a uniform random policy: for each of the sawyer environments, we\ndid not see a single success state in 75,000,000 environment steps (600,000 episodes); for the point\nspiral environment, we did not see a single success state in 40,000,000 environment steps (400,000\nepisodes). Previous methods solve these tasks with the aid of dense rewards , or with a rich\ncurriculum of subgoals , but the single-goal CRL agents must accomplish these tasks with no\nhand-crafted rewards, demonstrations, or subgoals."}, {"title": "Single-goal Exploration is Exceedingly Effective.", "content": "A single goal works well. We find that Contrastive RL effectively solves these four tasks: equipped\nonly with a single target goal, the agent automatically explores the environments and learns complex\nmanipulation skills (see Fig. 3). We compare this method to an \"oracle\" variant that is trained\non human-designed goals that vary in difficulty, ranging from easy goals to the single hard goal.\nSurprisingly, our proposed method significantly outperforms this \"range of difficulties\" method.\nWhile we have never seen a random policy solve any of these tasks, our method achieves its first\nsuccess within thousands of trials: 3,197 trials for sawyer box, 9,329 trials for sawyer bin, 15,895\ntrials for sawyer peg, and 11,724 trials for the spiral task."}, {"title": "The RL Algorithm is Key", "content": "We compare against a number of algorithms to investigate the importance of the underlying RL\nalgorithm: is single goal exploration useful for other goal-conditioned RL algorithms?\nBaselines. We single-goal CRL against prior methods that aim to address the sparse reward problem\nby making additional assumptions or employing additional machinery for exploration. Reinforcement\nlearning with imagined subgoals (RIS) maintains a high-level policy that predicts subgoals halfway\nto the end goal and learns the behavioral policy to reach both the subgoal and the end goal. We also\nemploy a few variants of Soft Actor-Critic (SAC) with additional assumptions: SAC with sparse\nrewards, SAC+HER with sparse rewards, and SAC with dense rewards. In the sparse rewards setting,\nthe agent receives a reward of 1 near the goal and 0 otherwise. In the dense reward setting, the agent\nreceives a continuous reward tailored to the environment, using the distance to the goal for the spiral\ntask and the Metaworld reward function for sawyer tasks. Table 1 summarizes the assumptions\nfor each method.\nResults. As shown in Fig. 9 single-goal contrastive RL significantly outperforms these alternative\nmethods, showing that the underlying RL algorithm is important for single goal exploration. Notably,\nprior methods rarely reach the goal at all, with the exception of RIS on the simplest task (point\nspiral). To verify that our implementation of SAC was correct, we also plotted the reward function\nthroughout training (recall that SAC has access to a reward function, while other methods do not) and\nobserved that it increases throughout the course of learning."}, {"title": "Why does Single-Goal Exploration Work?", "content": "In this section, we present ablation experiments that provide insight into the workings of single-goal\nCRL. We find that using single-goal data collection and an inner product critic are both key to the\neffectiveness of the method. Additionally, the performance boost of single-goal data collection does\nnot seem to be due to overfitting of the policy parameters on the task.\nThe effectiveness of single-goal exploration is not explained by overfitting. One possible\nexplanation for the method's success is that the algorithm overfits its policy parameters on the\nsingle-goal task. That is, if the agent only collects data conditioned on the single goal, the algorithm\nlearns useful representations and an effective policy only for states along that goal path. To test this\nhypothesis, we compared our method (which randomly samples different goals (see Eq. 4)) with a\nvariant that always uses the single hard goal in the actor loss. Note that this single hard goal is the\none that is used for evaluation. If overfitting were occurring, we would expect that modifying the\nactor loss to only train on the single goal would boost performance. However, the results in Fig. 10\nshow that this is not the case. When data are collected with a single goal, using a single goal in the\nactor loss degrades performance (red vs. blue (ours)). When data are collected with multiple goals,\nusing a single goal in the actor loss gives only a slight boost performance (purple vs. black). In short,\nthe performance of single goal exploration is not explained by overfitting."}, {"title": "Representations are important.", "content": "Our next set of experiments study how the representations might\ndrive exploration. To do this, we replaced the inner product critic function $(\\phi(s, a)^T\\psi(s_f))$ with a\nmonolithic critic function $(Q(s, a, s_f))$, which takes as input a concatenated array of the state, action,\nand goal. The results, shown in Fig. 11, show that single-goal exploration is not effective with using\na monolithic critic network. This experiment suggests that the contrastive representations $\\phi(s, a)^T$\nand $\\psi(s_f)$ drive exploration, though the precise mechanism for how they drive exploration remains\nunclear. In summary, single-goal exploration is only effective when combining the right algorithm\n(CRL) with the right critic architecture. In Appendix A, we show that environment dynamics are\nreflected in the contrastive representations early in training."}, {"title": "Impossible goals.", "content": "To further probe single goal exploration, we tried commanding a\ngoal that was impossible to reach in a maze navigation task. One\nmight expect to see completely random behavior, or see no behav-\nior at all. Visualizing the trajectories visited throughout training\n, we observe that the agent seems to try to navigate to that\nimpossible goal but then gets stuck. This pattern suggests that com-\nmanding an impossible goal is a plausible strategy for improving\nexploration. However, it fails to explore a fair number of states\nsuggesting that there are likely more effective ways of inducing\nexploration in settings without a single fixed goal."}, {"title": "Conclusion", "content": "In this paper, we showed that skills and directed exploration\nemerges from a straightforward RL algorithm: contrastive RL\nwhere every trajectory is collected by trying to reach a single fixed goal. The resulting method\nhas many appealing properties of prior exploration methods: in each episode the agent seems to try\nto visit some new state or attempt some new behavior. We do not see the random dithering that is\ncommon with na\u00efve exploration methods like e-greedy. Moreover, this method does not require any\nadditional hyperparameters: this is in contrast to even the simplest exploration algorithms today,\nwhich require a scale parameter (e.g., Gaussian noise in TD3 ). And, while prior exploration algo-\nrithms include a schedule (more hyperparameters!) for gradually decreasing the degree of exploration\nthroughout the course of learning, such behavior emerges automatically from our proposed method.\nThere remain two important outstanding questions raised by our experiments. First, we lack a clear\nunderstanding of why skills and directed exploration emerge. Our experiments provide some hints\n(representations are important; it is not explained by overfitting), yet much theoretical work remains\nto be done to understand exactly what is driving the exploration. A rich theoretical understanding of\nthe mechanisms driving the exploration here is important not only for explaining the success of this\nmethod, but it may also provide insights into how to adapt the method here to other settings. Second,\nhow can we leverage the success of the proposed method to address exploration in other problem"}, {"title": "Limitations", "content": "The primary limitation of our work is a lack of theoretical analysis explaining why\nskills and directed exploration emerge. Empirically, our experiments are focused primarily on\nmanipulation tasks; we encourage future work to study applications to other settings."}]}