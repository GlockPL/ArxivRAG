{"title": "ON THE ROBUSTNESS OF FULLY-SPIKING NEURAL NETWORKS\nIN OPEN-WORLD SCENARIOS USING FORWARD-ONLY\nLEARNING ALGORITHMS", "authors": ["Erik B. Terres-Escudero", "Javier Del Ser", "Aitor Mart\u00ednez-Seras", "Pablo Garcia-Bringas"], "abstract": "In the last decade, Artificial Intelligence (AI) models have rapidly integrated into production pipelines\npropelled by their excellent modeling performance. However, the development of these models has\nnot been matched by advancements in algorithms ensuring their safety, failing to guarantee robust\nbehavior against Out-of-Distribution (OoD) inputs outside their learning domain. Furthermore, there\nis a growing concern with the sustainability of AI models and their required energy consumption\nin both training and inference phases. To mitigate these issues, this work explores the use of the\nForward-Forward Algorithm (FFA), a biologically plausible alternative to the Backpropagation\nalgorithm, adapted to the spiking domain to enhance the overall energy efficiency of the model. By\ncapitalizing on the highly expressive topology emerging from the latent space of models trained\nwith FFA, we develop a novel FF-SCP algorithm for OoD Detection. Our approach measures the\nlikelihood of a sample belonging to the in-distribution (ID) data by using the distance from the latent\nrepresentation of samples to class-representative manifolds. Additionally, to provide deeper insights\ninto our OoD pipeline, we propose a gradient-free attribution technique that highlights the features\nof a sample pushing it away from the distribution of any class. Multiple experiments using our\nspiking FFA adaptation demonstrate that the achieved accuracy levels are comparable to those seen in\nanalog networks trained via back-propagation. Furthermore, OoD detection experiments on multiple\ndatasets (e.g., Omniglot and Not-MNIST) prove that FF-SCP outperforms avant-garde OoD detectors\nwithin the spiking domain in terms of several metrics used in this area, including AUROC, AUPR,\nand FPR-95. We also present a qualitative analysis of our explainability technique, exposing the\nprecision by which the method detects OoD features, such as embedded artifacts or missing regions,\nin multiple instances of MNIST and KMNIST. Our results underscore the enhanced robustness that\ncan be achieved by analyzing the latent spaces produced by already trained models.", "sections": [{"title": "1 Introduction", "content": "The fast development of learning algorithms has drastically impacted our dependency on Artificial Intelligence (AI)\nmodels, as their exceptional capabilities have led to their continual integration into production-ready software. However,\nthis rapid pace has not been accompanied by additional safety guarantees to address well-known issues faced by\nmodern AI systems. Among these shortcomings, the lack of robustness against inputs outside the training distribution\nposes a significant challenge in ensuring functionality in scenarios where generalization capabilities are essential [1].\nConsequently, many people are hesitant to rely on AI for critical decisions in fields such as medicine or law, even if\nthe model's performance surpasses that of human experts on specific tasks [2]. This limitation on the generalization\ncapability arises from the predominant use of gradient Back-Propagation (BP) for training models within the AI field."}, {"title": "Forward-Only Learning for Robust SNN", "content": "Although these models demonstrate excellent performance across a wide range of tasks, their tendency to overfit on\ntraining data and their inadequate responses to novel inputs make them highly unreliable tools for achieving high\nrobustness standards.\nMotivated by the learning capabilities of the brain, a novel biologically plausible learning heuristic denoted as the\nForward-Forward Algorithm (FFA) was recently introduced by Hinton [3], which has showcased competitive accuracy\nwhen compared with BP. However, its most promising property lies within its latent space, which provides fundamental\ncharacteristics required for developing robust models. As multiple authors have observed since then [4, 5], the emerging\nlatent space of this algorithm is characterized by classes residing tightly packed together in small clusters, while\nremaining distinct from other classes. Having tight compression of the data is pivotal in ensuring that models behave\ncorrectly when presented with data outside the training distribution, as the latent vector representation would lie outside\nany of the clusters identified from the training latent space. This property is crucial for ensuring a correct behavior with\ndata from training datasets, as samples outside these clusters can be quickly identified as not belonging to the training\ndistribution. Although the algorithm was initially presented as a biologically plausible alternative, most implementations\nhave relied on analog neural models, leading to reduced research on their spiking counterparts. However, the set of\npapers working on spiking adaptations have shown how FFA can achieve competitive accuracy levels [6], paving the\nway for further neuromorphic implementations that promise faster and more energy efficient alternatives.\nThis work presents a seamless integration of FFA into Spiking Neural Networks (SNNs) via surrogate gradients to\nfully leverage the internal properties of spiking models. By capitalizing on their characteristic latent space, we create\na novel Out-of-Distribution (OoD) Detection Algorithm that uses the latent distance between a sample and the set\nof In-Distribution (ID) clusters to determine whether a sample should be labeled as either OoD or ID. To extend the\nfunctionality of this algorithm, we have developed an interpretability technique that can generate an attribution map\nfor a specific input instance with respect to any arbitrary class, highlighting features that do not correspond with those\nexpected from the distribution of the desired class. This algorithm eases the task of detecting possible artifacts that could\nbe affecting the model's decision among samples labeled as OoD. Our results showcase the enhanced OoD detection\naccuracy in spiking networks when employing FFA, demonstrating that its enhanced representational properties improve\nthe robustness of the models. Furthermore, our attribution method proves effective in detecting key features that push a\ndata sample from its respective class towards OoD regions.\nThe rest of this paper is structured as follows: Section 2 provides a comprehensive review of related work. Section\n3 elaborates on all the methodological approaches developed in this work. Specifically, Subsection 3.1 presents the\napproach taken to adapt FFA for SNNs, Subsection 3.2 describes the FF-SCP algorithm, and Subsection 3.3 introduces\nour attribution algorithm. Next, Section 4 presents the research questions that this work addresses, along with the\nexperimental setup used for the experiments. Subsequently, Section 5 presents the experimental results, along with a\ndiscussion on the observed limitations. Finally, Section 6 outlines the conclusions of the paper and delineates directions\nfor future research."}, {"title": "2 Related Work and Contributions", "content": "In this section, we present a comprehensive overview of relevant literature for this work, focusing on the following\ncentral topics: Spiking Neural Networks (Subsection 2.1); Forward-only Learning Algorithms (Subsection 2.2); Out-of-\nDistribution Detection (Subsection 2.3); and Explainability and Attribution (Subsection 2.4). Finally, Subsection 2.5\ndetails the contribution that this papers offer with respect to the current literature."}, {"title": "2.1 Spiking Neural Networks", "content": "SNNs constitute a special category of artificial neural networks inspired by biological models, aiming to replicate the\ncommunication patterns found in real neurons within the brain. Unlike traditional approaches, primarily driven by\nAnalog Neural Networks (ANNs) that rely on continuous activation values to forward information, spiking models\noperate by using discrete pulses of information, known as spikes. The internal dynamics of the neuron, determined by\nthe chosen neural model, dictate when spikes occur and are typically presented as a dynamical system that provides a\nmathematical approximation of the behavior observed in real neurons [7]. Consequently, neurons can exhibit highly\ncomplex dynamics depending on the neural model, the degree of biological plausibility and the total computational\ncomplexity desired for the simulation.\nIn this work, we rely on the Leaky Integrate-and-Fire (LIF) neural model, known for preserving the standard action-\npotential dynamic while achieving high computational efficiency [8, 9]. Under this model, the membrane potential U of\nthe neuron is tracked and updated in response to the external stimuli $I_{in}$, which represents the information forwarded to\nthe neuron via the axon. Whenever the membrane potential reaches a threshold $\\theta_{thr}$, a spike is generated and forwarded"}, {"title": "Forward-Only Learning for Robust SNN", "content": "to subsequent neurons, at which point the membrane potential is reset to zero. To prevent unbounded activity, a constant\ndecay $\\tau$ is applied, exponentially reducing the membrane potential over time. Under a discrete time formulation, the\nneuron's membrane potential U(t) is updated as follows:\n$U(t) = \\tau \\cdot U(t - 1) + R \\cdot I_{in}(t)$,                                                                 (1)\nwhere $U (t - 1)$ represents the membrane potential at the previous time step, and R is a scaling factor of the input. Once\nU(t) is computed, the value of the output spike $S'(t)$ is determined by the activity of the membrane:\n$S(t) = \\I(U(t) > \\theta_{thr})$,                                                                           (2)\n$U(t) = U(t) - U(t) - S(t))$,                                                                  (3)\nwhere $\\I(\\cdot)$ is an auxiliary binary function taking value 1 if its argument is true (0 otherwise).\nBy relying on these models, the neuromorphic computing field has been a highly active area due to the energy\nand speed advantages they offer, demonstrating proficiency in multiple artificial intelligence tasks. In the field of\nComputer Vision, deep spiking neural networks have been effectively employed for image classification [10] and image\ndetection tasks [11]. In the Natural Language Processing domain, Zhu et al. implemented a fully spiking version\nof a Large Language Model, demonstrating its competitiveness against analog language models such as GPT-2 [12].\nAdditionally, notable advantages have been observed in a brain-like navigation systems by utilizing LIF neurons trained\nby Spike-Timing-Dependent Plasticity (STDP), a biologically plausible learning rule [13]."}, {"title": "2.2 Forward-only Learning Algorithms", "content": "BP stands as the most commonly employed algorithm for training artificial neural networks. This method computes\nexact gradient values of the weights with respect to a loss function by back-propagating the error from the output\nthrough the layers using the chain rule. Due to the precision of weight updates, BP has achieved state-of-the-art results\nin terms of accuracy across a multitude of tasks. However, the requirement for differentiability in the functions defining\nthe network poses a clear limitation for non-differentiable neural models, such as those in spiking systems. To address\nthis limitation, the most commonly employed approach involves replacing the neural model with a differentiable\napproximation so that the backward pass can create a faithful approximation of the gradients [14]. Additionally, due\nto the optimization nature of the problem, models trained with BP have been observed to be prone to overfitting [15],\nconsequently failing to generalize beyond the training distribution and resulting in unknown outcomes when presented\nwith new, unseen data.\nTo overcome the present limitations of BP, the NeuroAI field has recently emerged as a strong alternative movement.\nThis field advocates for incorporating and studying learning mechanisms observed in the brain to improve current\nmodels, aiming for fast learning and highly robust learning models [16]. Among these approaches, the branch of\nforward-only algorithms poses a viable method for bridging the gap between neuroscience and AI by focusing on\na reduced set of biological properties while preserving the advantage of being computationally efficient and staying\nclose to traditional AI methods. These algorithms specifically address the weight transport and update lock problems\nby relying solely on forward passes. By avoiding these biological incompatibilities, these algorithms also enhance\nintegrability into neuromorphic systems, which are better suited for spiking models.\nRecently, the Forward-Forward Algorithm was introduced as a novel forward-only alternative to BP, advocating for a\nbiologically motivated heuristic in which the backward pass is replaced by an additional contrastive forward pass [3]. In\nthis secondary pass, the model is presented with an adversarial sample to contrastively learn to distinguish between\nsamples from the training set (referred to as positive or D\u2295) and synthetically created data (referred to as negative\nor D). As it follows from to its forward-only nature, each layer is trained independently, using a layer-specific loss\nfunction that relies solely on local information. To formulate this loss function, Hinton proposed the use of a goodness\nscore, which measures the likelihood of a latent vector from belonging to the positive data distribution. This goodness\nfunction G: L \u2192 R is defined as the squared Euclidean distance of the latent vector from its original position, where\nIL denotes the latent space of the given layer. This choice was motivated by the simplicity of its derivative, which\nfacilitates clear and effective weight update expressions. Under this formulation, the goodness score serves as a measure\nof the layer's activity, advocating for achieving highly active layers when presented with positive samples, and nearly\ndormant outputs when encountering a negative instance.\nTo avoid relying on a regression-based task, which could provoke unstable learning dynamics, a probability-based\napproach was taken. By using the Binary Cross-Entropy loss with a probability function defined over the goodness\nscores, the approach ensures a balanced distribution of positive and negative activities. The probability function\nP: R\u2192 [0, 1] is chosen to map the goodness scores to a final probability estimation, indicating the likelihood of a\nlatent vector being obtained from a sample from the positive data distribution. This function is defined by the logistic"}, {"title": "Forward-Only Learning for Robust SNN", "content": "function, modulated by a threshold value 0 to shift the scores toward zero when presented with near-zero goodness\nscores. The formal expression for this function is given by $P(x; 0) = sigmoid(x \u2013 \\theta)$.\nAs previously stated, each learning step involves two contrastive forward passes: a positive pass, where each layer is\npresented with a sample from the training distribution and optimized to maximize its goodness score; and a negative\npass, where a synthetic sample is employed to teach the model to remain dormant when presented with non-training\ndata. In its supervised formulation, positive instances are created by embedding a one-hot encoding vector of the\ncorresponding label into the corner of the base input image. Analogously, negative data is created by taking a sample\nfrom the training dataset and embedding a random label that does not align with its corresponding one. Using this\npositive and negative generation method, the inference process is achieved by computing the goodness score of the\ndata sample with respect to all labels, thereby selecting the label achieving the highest score as the output class. In a\nsubsequent work, Lee et al. proposed a refinement of this method by using random binary vectors associated with each\nclass, which were then appended to the end of the vector of the data sample, reducing the overlap between the data and\nthe label, and resulting in improved classification accuracy [17].\nThe algorithm has gained significant attention from NeuroAI researchers, leading to multiple improvements over the\noriginal formulations, which have resulted in increased accuracy [18, 17] and improved biological plausibility [5, 6].\nExpanding on the latter topic, Ororbia and Mali pioneered additional biological heuristics for FFA by integrating\npredictive coding, which allows the model to provide accurate reconstructions of the latent space [5]. Simultaneously,\nOrorbia proposed the first implementation of an event-driven version of FFA, replacing the conventional error-driven\nupdate with modulated Hebbian learning rules, demonstrating a straightforward adaptation of FFA into a biologically\nplausible spiking learning pipeline [6]. Both of these works, along with [4] and [19], have presented consistent results\nshowcasing the representational properties that emerge from this algorithm. Their work highlights the particular latent\ntopology that results after training, characterized by sparse latent activation and neural specialization that leads to\nseveral compact clusters comprised of instances of the same class, which lie far from each other, rendering class clusters\nhighly separated within the space."}, {"title": "2.3 Out-of-Distribution Detection", "content": "OoD detection is a crucial task for developing production-ready software, as it ensures that models are not presented\nwith inputs outside their training distribution, which could result in unpredictable and potentially dangerous outputs\n[1]. To achieve this, OoD detection algorithms define a scoring function that measures the likelihood of a sample\nfrom belonging to the ID dataset, which is then used to label input data as either ID or OoD depending on the score\nobtained. When studying how OoD samples emerge, it's important to distinguish between two fundamentally different\nscenarios: semantic shifts, which occur when samples that do not belong to any class start to emerge, changing the\noverall distribution of the input space without truly altering the distribution of the previously seen classes; and covariate\nshifts, which occur when the inherent distribution of one or several classes shifts from its original distribution. In\nthis context, OoD detection only covers the detection of semantic shifts in the data, as covariate shifts are covered by\nthe branch of anomaly detection [20]. Based on the review from Yang et al. [1], OoD detection algorithms can be\ncategorized into four distinct heuristics based on the information used to detect OoD samples: classification-based,\ndistance-based, density-based, and reconstruction-based methods. Our paper focuses on distance-based techniques,\nwhich rely on the distances within the latent space to define the desired OoD scoring function. Due to the lack of\nliterature on OoD detection in networks trained using the FFA, we will provide a comparative perspective on methods\nemploying backpropagation BP.\nCurrently, the ODIN algorithm stands as the most widely-used benchmark for OoD detection due to its high accuracy\nand reduced computational costs. This algorithm operates by measuring the effect on the softmax distribution of the\nlast layer when changing the temperature scaling and introducing small perturbations into the inputs, where OoD\nsamples result in the greatest differences [21]. An alternative approach was proposed by Bergman et al., where the deep\nk-nearest neighbor algorithm is employed to measure the distance between input samples and previously computed ID\nrepresentative latent manifolds, thereby using the latent distance to detect anomalies [22]. Building upon this research,\nSun et al. extended the application of the deep k-nearest-neighbor distance for OoD detection [23], highlighting the\nimportance of exploiting the geometric properties in the latent space of trained networks for OoD detection tasks.\nDespite the extensive literature on this task, a noticeable gap exists in the context of techniques designed specifically for\nspiking neural networks. In their recent work, Martinez-Seras et al. introduced a novel method for OoD detection that\nfocuses on spike count patterns in the latent space. By characterizing spike patterns across various layers, they identify\nwhether a given spike pattern aligns with those extracted from in-distribution data [24]."}, {"title": "2.4 Explainability and Interpretability", "content": "Explainable AI, commonly referred to as XAI, is a field focused on developing techniques to offer humanly under-\nstandable explanations for the motivations behind a model's outputs. It is well acknowledged that most modern\nmodels operate as black boxes, achieving high performance across multiple tasks but offering little to no insight into\nthe heuristics they follow to obtain their outputs. To address this shortcoming, the XAI field encompasses multiple\nsubtopics aimed at extracting different types of knowledge of models. Among these subtopics, the most relevant are\ngiven by: understandability, comprehensibility, interpretability, explainability, and transparency. This paper focuses on\nInterpretability, which aims to extract meaningful explanations that can be easily understood by humans agents.\nAs of now, the most prevalent interpretability techniques for computer vision tasks are driven by Class Activation Maps\n(CAMs) [25], which create visual insights highlighting the most significant regions of an input image that impacted\nthe prediction. Among methods providing this type of explanation, Grad-CAM stands as the one with the greatest\nimpact. This method uses the gradient information of the last convolutional layer to detect the areas of the input that\nhad the highest impact on the prediction of a specific class [26]. Although several extensions of the algorithm have been\ndeveloped to enhance the quality of the attribution maps [27, 28], these methods are not applicable to spiking networks\ndue to their reliance on precise gradient information, which cannot be obtained from spiking networks because of the\nnon-differentiability of their neural model. To overcome this issue, Kim et al. introduced a visualization algorithm\nnamed Spike Activity Mapping, which leverages the temporal dynamics of SNNs to compute neuronal contribution\nscores during forward propagation based on the history of spikes [29]. Another approach proposed by Bitar et al.\naddresses the gradient issue in image attribution by employing surrogate gradients, demonstrating their capability to\nmaintain sufficient accuracy for effectively modeling the saliency map [30].\nContrasting with these approach, which rely solely on gradient information, Martinez et al. presented a technique\ninvolving a backward pass of a latent vector through the network to compute the relevance of specific features in the\ninput space [24]. This method operates by reconstructing spike patterns across the layer and enabling the quantification\nof feature activity, thereby identifying regions of significance within the image. By doing so, the authors place\nadditional focus on exploiting the representational capabilities of the latent space of non-convolutional networks to\ncreate interpretable techniques. Our paper follows the same geometrical motivation, leveraging the specific latent\ntopology of FFA to provide enhanced interpretable results."}, {"title": "2.5 Contributions", "content": "This work presents a seamless adaptation of FFA for spiking models, offering an alternative formulation to the one\nproposed by Ororbia in [6]. In our formulation, we introduce two alternative goodness functions: an unbounded\ngoodness function, which follows Ororbia's methodology by treating spikes as a discrete version of ReLU activation;\nand a bounded goodness function, which fully leverages the bounded behavior of spiking models, making it more\nsuitable for neuromorphic computers. Subsequently, we develop FF-SCP, an OoD detection algorithm that capitalizes\non the characteristic latent space observed by the community to enhance the robustness of FFA. The applicability of\nFFA's geometrical properties remains impactful for creating software that meets industry requirements in terms of\nsecurity. However, the current trend in the literature has primarily focused on its predictive capabilities. Thus, this work\noffers an initial step toward providing additional methodologies that bring FFA closer to being a robust alternative\nto BP in terms of functionality. Additionally, to further explore the functional aspects of the latent space of FFA, we\ndevelop an attribution algorithm that provides understandable explanations for the features of an image that push it\noutside the distribution of any given class. This work also contributes to expanding the literature advocating for the\nstudy of the geometrical properties of latent spaces to benefit model reliability, particularly from the explainability field.\nHowever, we acknowledge the intrinsic challenge posed by the lack of better-suited architectures when introducing FFA\ninto production-ready software."}, {"title": "3 Proposed Approach", "content": "This section introduces the methodological aspects of the algorithms proposed in this work, dedicating one subsection\nto each of the contributions. Subsection 3.1 outlines the adaptation of FFA for SNNs. Subsection 3.2 introduces the\nFF-SCP algorithm, which employs the latent state of SNNs trained with FFA to create an OoD detection algorithm.\nLastly, Subsection 3.3 presents an attribution technique aimed at providing visual insights into the information relating\narbitrary samples to the distribution of specific classes."}, {"title": "3.1 Spiking Forward-Forward Algorithm", "content": "Although the original motivation that led to the development of FFA was to propose a biologically plausible alternative\nto BP, the vast majority of research on this method is still obtained using ANNs. In this section, we present a\nstraightforward adaptation of FFA for SNNs, modifying the goodness scoring function to aggregate the temporal\ndimension and employing surrogate gradient techniques to enable training under standard ML pipelines. Additionally,\nto capitalize on the bounded dynamics offered by spiking models, we propose a novel bounded goodness function\nthat operates independently of the total simulation time. For the remainder of this paper, we will denote the spiking\nformulation of FFA as sFFA.\nAs detailed in Section 2, the original goodness function G was formulated using the squared Euclidean distance of the\nlatent vector. This function, combined with the unbounded dynamics of ReLU activations, enabled learning dynamics\nthat could result in positive latent vectors achieving arbitrarily high goodness scores as training progressed. However,\ndue to the discrete event-driven nature of SNNs, where the latent space consists of binary latent vectors, the activation\nvalue of each neuron is bounded by the total number of timesteps in the simulation. Therefore, to replicate behaviors\nsimilar to those observed in ANNs, the goodness function must be adapted to handle these discrete binary states. This\npaper introduces two implementations of the goodness function for spiking models: an unbounded goodness, which\nmimics the behavior of ANNs through the aggregation of the temporal dimension; and a bounded goodness, designed to\ncapitalize on the bounded activation range of SNNs.\nThe first goodness function, namely the unbounded goodness function, referred to as $G_{\\infty}(\\cdot)$, can be naturally adapted\nfrom the conventional goodness function by considering the sum of spikes as a linear approximation of ReLU-like\nfunctions. In this scenario, the goodness function takes the latent vector l, composed of the spike train of the neurons\nof the layer, and acts as the squared Euclidean norm of the number of spikes. Formally, given the latent vector\n$l \\in {0, 1}^{N \\times T}$ consisting of N neurons and T timesteps, the function $G_{\\infty}(l) : {0,1}^{N \\times T} \\rightarrow R_{>0}$ is formulated as:\n$G_{\\infty}(l) = \\sum_{n=1}^{N} (\\frac{1}{T}\\sum_{t=1}^{T} l_{n,t})^{2}$,                                                                      (4)\nwhere $l_{n,t}$ denotes the value of the n-th neuron at timestep t.\nOur second goodness function, namely the bounded goodness, denoted by $G_{0}(\\cdot)$, offers an alternative formulation in\nwhich the learning process relies solely on bounded states, aligning more closely with biological models compared to\ntheir unbounded counterparts. To achieve this, we replace the squared Euclidean distance with the Manhattan distance\nof the mean number of spikes in the latent spike train, ensuring all neurons contribute equally to the goodness score.\nMoreover, by applying mean aggregation, the obtained goodness scores remain independent of the total number of\ntimesteps. Formally, given the latent vector $l \\in {0, 1}^{N \\times T}$, the bounded goodness function $G_{0}(l) : {0, 1}^{N \\times T} \\rightarrow R_{\\geq 0}$"}, {"title": "Forward-Only Learning for Robust SNN", "content": "is expressed as:\n$G_0(l) = \\frac{1}{N \\cdot T} \\sum_{n=1}^{N} \\sum_{t=1}^{T} l_{n,t}$.                                                                                                               (5)\nUpon early inspection of both goodness functions, several properties can be noted. On one hand, the unbounded\nfunction $G_{\\infty}(x)$ is not strictly unbounded, as it spans an interval defined by the number of neurons N and the total\ntimesteps T, [0, N \u00b7 T2]. However, the upper limit of this interval can achieve sufficiently high values to approximate\nthe behavior of unbounded models. Moreover, this upper limit can be easily adjusted by changing the values of these\ntwo parameters, allowing for higher scores to approximate the behavior of unbounded models. On the other hand, the\nbounded design of $G_0(x)$ results in a range confined to the interval [0, 1], regardless of the aforementioned parameters.\nThis bounded limit can pose challenges in achieving extreme values of the probability function $P(\\cdot)$, especially if the\noriginal threshold @ was intended for unbounded goodness scores, as the function's range will not properly span the\nprobability range [0, 1]. To address this, we modify the probability function in two ways. First, we introduce a scalar\nfactor a to scale the term G(l) \u2013 0. Second, we replace the single threshold @ with two thresholds, $\u03b8_{\u2295}$ and $\u03b8_{\u2296}$, applying\nindependent thresholds to the positive and negative probability functions, respectively. This adjustment allows us to\nincrease a and adjust $\u03b8_{\u2295}$, $\u03b8_{\u2296}$ to align with the domain of the probability function. Under this formulation, the resulting\nprobability function is expressed as follows:\n$P(G(l); \\alpha, \\theta) = \\frac{1}{1 + e^{-\\alpha(G(l)-\\theta_{\u2295})}}$,                                                                                                                (6)\nwhere the converse negative probability function is defined by the expression 1 $P(G(l_e); \\alpha, \\theta_e)$."}, {"title": "3.2 The FF-SCP Algorithm", "content": "This section introduces the FF-SCP algorithm, which leverages the emergent representational properties in the latent\nspace of FFA to detect OoD inputs. As previously mentioned, multiple studies have observed that the latent space of\nnetworks trained using FFA exhibits a distinctive topology, characterized by distinct clusters for each training class\n[5, 4]. Conversely, due to ReLU activations having a minimum value of 0, negative data tends to generate near-zero\nlatent vectors, resulting in all negative data clustering around the origin of the latent space. However, due to the\ninherent noise present in spiking models, negative latents attain higher activity bounds, shifting the single cluster of\nnegative data into multiple different clusters dispersed throughout the space. These clusters retain the representability\nof the space, as each one can be observed to contain samples from the same class and which employed the same\nembedding label. Despite the presence of a high number of clusters, the neural specialization of the neurons contributes\nto creating a highly separated latent space, which is a desirable property for detecting samples not drawn from the\ntraining distribution. A visual representation of the emergent latent spaces is provided in Appendix B.\nIn its original formulation, the SCP algorithm proposed using a class-representative latent vector to describe each\nclass, so that the geometric complexity of the latent space could be reduced into a single characteristic vector. To\nachieve this, SCP employed the centroid of the class latent vectors, allowing for straightforward computation of the"}, {"title": "Forward-Only Learning for Robust SNN", "content": "OoD score by measuring the distance between the centroids and the sample's latent vector representation. However,\nthis representative vector failed to perform adequately when presented with data distributions whose latent subspaces\nresulted in separated topologies or highly unbalanced latent vector distributions, as the center of the data did not capture\nthe complex geometry of these latent manifolds.\nTo mitigate this effect, this work proposes an alternative distance heuristic, replacing the centroid distance with a\npoint-set distance between the latent vector $l \\in R^N$ of a sample x and a set of latent vectors $L_c = {l^1,l^2, . . . }$\nextracted from the complete latent manifold of class c, where N is the dimension of the layer. This distance function\nis formulated using a prior distance function $d(z_1, z_2)$, which measures the distance between two arbitrary vectors\n$z_1, z_2 \\in R^n$. The distance between a point and a set is then defined as the minimum distance between the point and\nany point in the set. From a theoretical perspective, this function should operate over the complete latent manifold\n$L_c \\subseteq R^N$, however, due to the discrete nature of current AI, these clusters can only consist of a finitely many set of\npoints. Therefore, we approximate this distance using the following expression:\n$d(l, L_c) = inf_{l \\in L} d(l, l) \\approx min_{l \\in L_c} d(l, l)$.                                                                                                                                  (7)\nGiven that FFA provides different representations of the same input sample x depending on the embedded label p used\nduring the supervised inference process, the input data used to create the latent space representatives must include\nboth positive and negative instances of input data. Therefore, creating the set L of the different clusters must involve a\nuniform amount of inputs from each class using all possible labels and storing them. Once all these vectors have been\ncaptured, the set L will provide a sufficient representation of the original latent space.\u00b9 To account for the presence\nof negative vectors, we extend the definition of L to {Lc,p | c \u2208 C and p \u2208 C}, where Lc,p represents the set of latent\nvectors from class c forwarded using the embedded label p, and C denotes the set of class labels.\nOnce the representative set of latent vectors has been identified, an initial version of the scoring function can be\nformulated as the distance between the sample's latent vector and each class-representative set of clusters. Unlike\nconventional neural networks, where the latent representation of a sample is unique, FFA generates different representa-\ntions depending on the embedded label used during the forward process. Therefore, to measure the distance between\na sample and the set of class-representative clusters, it's crucial to account for the different representations that the\nsample obtains with different labels. Thus, the distance from the latent vector to each class is defined as the sum of the\ndistances from the vector to each of the class clusters. This effect is illustrated in Figure 2, where the red lines define the\ndistances from each latent vector of the different embeddable labels with respect to the latent representative sets Lc,p.\nConsequently, the score of the unknown sample x is defined as the minimum distance from its latent representation to\neach class representative set of clusters. Once this minimal distance is computed, ID samples are expected to achieve a\nlow overall distance, as their representation should remain proximal to their respective class set of clusters. Conversely,\nOoD samples are expected to attain a higher minimal distance, as their latent vectors should not be within proximity to\nany set of ID clusters. Therefore, the initial score function can be defined as:\n$s(x) = min_{p \\in C} \\sum_{c \\in C} d (\\mathcal{N}_a(x, c), L_{p,c})$,                                                                                                                      (8)\nwhere $\\mathcal{N}_a(x, p)$ represents the output latent vector obtained from the forward pass of an input x with the embedded\nlabel p at depth d, and $$\\beta$ denotes a scaling hyperparameter.\nWhen working with certain spaces, this scoring function can encounter a significant limitation, as an insufficient number\nof samples can result in inaccuracies when measuring the discrete approximation of the distance. For instance, in\nscenarios where the latent representation contains widely dispersed points, the resulting measurements will tend to\nlean towards large distances, as the distance approximation might fail to find nearby neighbors. This issue is more\npronounced in high-dimensional spaces, where the curse of dimensionality requires exponentially more samples to\nachieve even distributions of points as the dimensions increase. Furthermore, due to the activity maximization objective\nof FFA, where positive latent vectors strive to maximize their distance from the origin, class manifolds can see their\ndiameters extended over large distances, making it harder to achieve accurate representations."}, {"title": "Forward-Only Learning for Robust SNN", "content": "In such scenarios, an extreme outcome can occur where the scoring of samples is inverted, leading to OoD samples\nachieving lower scores than ID samples. This situation arises when the minimal distance from the zero vector to any\nGiven that networks trained with FFA do not currently achieve state-of-the-art accuracy, a filtering step is necessary to remove\npotentially misclassified latent vectors. This step involves discarding a small set of the least active elements from sets where the\nembedded label aligns with the real class, as they may have been misidentified as negative samples. Conversely, it also involves\nremoving a reduced set of the latent vectors with the highest activity from the negative sample set to avoid potential misclassification\nof these highly active elements as positive samples."}, {"title": "Forward-Only Learning for Robust SNN", "content": "representative cluster is smaller"}]}