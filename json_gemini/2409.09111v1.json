{"title": "Neural Message Passing Induced by Energy-Constrained Diffusion", "authors": ["Qitian Wu", "David Wipf", "Junchi Yan"], "abstract": "Learning representations for structured data with certain geometries (observed or unobserved) is a fundamental challenge, wherein message passing neural networks (MPNNs) have become a de facto class of model solutions. In this paper, we propose an energy-constrained diffusion model as a principled interpretable framework for understanding the mechanism of MPNNs and navigating novel architectural designs. The model, inspired by physical systems, combines the inductive bias of diffusion on manifolds with layer-wise constraints of energy minimization. As shown by our analysis, the diffusion operators have a one-to-one correspondence with the energy functions implicitly descended by the diffusion process, and the finite-difference iteration for solving the energy-constrained diffusion system induces the propagation layers of various types of MPNNs operated on observed or latent structures. On top of these findings, we devise a new class of neural message passing models, dubbed as diffusion-inspired Transformers, whose global attention layers are induced by the principled energy-constrained diffusion. Across diverse datasets ranging from real-world networks to images and physical particles, we show that the new model can yield promising performance for cases where the data structures are observed (as a graph), partially observed or completely unobserved.", "sections": [{"title": "1 Introduction", "content": "Real-world data are generated from a convoluted interactive process whose underlying physical principles often involve inter-connections of certain forms. Such a nature violates the common hypothesis of standard representation learning paradigms assuming that observed data are independently sampled. The challenge, however, is that due to the absence of prior knowledge about ground-truth data generation, it can be practically prohibitive to build feasible methodology for uncovering the latent structures that embody the inter-connecting patterns. To address this issue, prior works, e.g., Wang et al. (2019); Franceschi et al. (2019); Jiang et al. (2019); Zhang et al. (2019), consider encoding the potential interactions as estimated structures in latent space, but this requires sufficient degrees of freedom that significantly increases learning difficulty from limited labels (Fatemi et al., 2021) and hinders the scalability to large systems (Wu et al., 2022)."}, {"title": "1.1 Related Works", "content": "To provide more background information and properly position our contributions within the community, we review salient related work and discuss connections with ours."}, {"title": "1.1.1 NEURAL DIFFUSION ON GRAPHS", "content": "The diffusion-based learning has gained increasing research interests, as the continuous dynamics can serve as an inductive bias incorporated with prior knowledge of the tasks at hand (Lagaris et al., 1998; Chen et al., 2018). One category directly solves a continuous process of differential equations, e.g., Chamberlain et al. (2021a) revealing the association between the discretization of graph diffusion equations and the feed-forward updating rules of GNNs. Along this direction, recent works (Chamberlain et al., 2021b; Thorpe et al., 2022; Bodnar et al., 2022; Choi et al., 2023) leverage diffusion equations as a mathematical framework for analyzing GNN behavior and devising continuous models that utilize differentiable PDE-solving tools for training.\nAnother line of research investigates PDE-inspired learning using the diffusion perspective as a principled guideline on top of which (discrete) neural network-based approaches are designed for node classification (Atwood and Towsley, 2016; Klicpera et al., 2019b; Xu et al., 2020), addressing over-smoothing (Rusch et al., 2023), knowledge distillation (Yang et al., 2022) and topological generalization (Wu et al., 2023). Our work leans on PDE-inspired"}, {"title": "1.1.2 MESSAGE PASSING NEURAL NETWORKS", "content": "Graph neural networks (Scarselli et al., 2008) have become the mainstream class of neural encoders for representation learning on structured data with observed geometries. Most existing GNNs adopt message-passing-based architectures, and are to a large extent inter-changeably called message passing neural networks (MPNNs) in the literature. With the pioneering work of graph convolution networks (Kipf and Welling, 2017) that show promising performance on semi-supervised (node) classification tasks, there is a surge of recent work exploring various expressive MPNN architectures equipped with advanced message passing designs e.g., Xu et al. (2019); Hamilton et al. (2017); Xu et al. (2018); Abu-El-Haija et al. (2019); Klicpera et al. (2019a); Chen et al. (2020a); Zhu et al. (2020); Chien et al. (2021).\nThe challenge, however, is that due to the diversity of graph-structured data that can have disparate scales, sizes, topological properties, etc., current models designed for particular tasks are often hard to transfer to others outside its experimental settings.\nFurthermore, from an architectural view, the majority of existing MPNNs operates the message passing per layer within observed edges of input graphs, which could limit efficacy in scenarios where the graphs are noisy or incomplete. To resolve this, several recent works propose to learn latent graph structures from data that can boost MPNNS towards better representations (Franceschi et al., 2019; Chen et al., 2020c; Jiang et al., 2019; Fatemi et al., 2021; Wu et al., 2022). These approaches generalize message-passing-based schemes to broader regimes where interactions are modeled by latent structures. Our work aims to provide a theoretical framework that can interpret the message passing rules of GNNs as numerical iterations of a diffusion process that descends a regularized energy in an interpretable form. As we will show in later sections, this principled perspective can be utilized to help understand the behavior of various MPNNs and the mechanism of message passing over observed or latent graph structures."}, {"title": "1.2 Contributions and Organization", "content": "Before we delve into the proposed model, we summarize the main contributions of this paper along with pointers to the relevant sections.\n\u2022 We propose a principled theoretical framework for representation learning on structured data. The framework is built upon an energy-constrained diffusion model that integrates the continuous dynamics of diffusion equations with minimization constraints of a global energy. The model offers a new aspect for learning effective representations with either observed structures or unobserved latent structures. (See Section 3)."}, {"title": "2 Preliminary and Background", "content": "In this section, we introduce some technical background about diffusion on manifolds as preliminary to our model. We consider an abstract domain denoted by \u03a9, which, for the purposes of our study, is assumed to be a Riemannian manifold (Eells and Sampson, 1964). A fundamental distinction between an n-dimensional Riemannian manifold and a Euclidean space lies in its unique property of being locally Euclidean. This suggests that for each point \u03b7 \u03b5 \u03a9, there exists a n-dimensional Euclidean tangent space Tu\u03a9 = R\" that locally represents the structure of \u03a9. We denote by \u03a4\u03a9 the collection of these tangent spaces, which has a smoothly varying inner product (often known as the Riemannian metric).\nFor some physical quantity (e.g., temperature), it can be described by a function of the form z: \u03a9 \u2192 R that is a scalar field on \u03a9. This also associates to every point u\u2208 \u03a9 a tangent vector z(u) \u2208 \u03a4\u03a9 that can be considered as a local infinitesimal displacement of a (tangent) vector field z : \u03a9 \u2192 \u03a4\u03a9. Let Z(\u03a9) and Z(\u03a4\u03a9) denote the functional spaces of scalar and (tangent) vector fields on \u03a9, respectively. Then the inner products on Z(\u03a9) and Z(\u03a4\u03a9) can be denoted by (z, z') and \u300az, z\u2032), respectively. The gradient operator \u2207 : Z(\u03a9) \u2192 Z(\u03a4\u03a9) transforms scalar fields into vector fields that represent the local direction of the steepest change of Z(\u03a9). The divergence operator \u25bd* : Z(\u03a4\u03a9) \u2192 Z(\u03a9) transforms the vector fields"}, {"title": "3 Model Formulation: Energy-Constrained Diffusion", "content": "In this section, we introduce the general formulation of our energy-constrained diffusion model and its inherent connection with neural message passing. We will begin with a geometric diffusion model which is characterized by a diffusion PDE equation with flexible instantiations of the diffusivity. The latter enables us to bridge the numerical iterations of the PDE with different types of MPNNs. Built upon this, we will probe how the energy minimization perspective can be organically incorporated as a physics-inspired prior of the diffusion system in the form of constraints, which gives rise to the energy-constrained diffusion.\nGraph Notations. We assume a graph to be G = (V,E) where V = {i} denotes the node set and E = {(i, j)} denotes the edge set. For node i \u2208 V, it has an input feature vector xi \u2208 RD. The edge set is associated with an adjacency matrix A = [aij]i,jev where aij = 1 if (i, j) \u2208 E and 0 otherwise. We use D = diag(di)iev to denote the diagonal degree matrix of A where di denotes the degree of node i. The problem of our interest is how to obtain effective node-level representations (a.k.a. embeddings) zi \u2208 Rd based on their initial features and graph structures. Beyond the observed edges E, the non-trivial challenge stems from"}, {"title": "3.1 Geometric Diffusion with Observed/Latent Structures", "content": "The starting point of our model is rooted on an analogy that treats nodes in the graph (i.\u0435., i eV) as locations on a Riemannian manifold (i.e., u\u0454\u03a9) (Rosenberg and Steven, 1997), node embeddings as the physical quantity of interest and the update of node embeddings per layer as heat flux through time (Chamberlain et al., 2021a).\nTo be specific, each node i \u2208 V has a d-dimensional node embedding zi (where d is the hidden size) that is updated layer by layer, and we model the node embedding as a vector-valued function zi(t) : [0, \u221e) \u2192 Rd that evolves with time\u00b9. We denote by Z(t) = [zi(t)]iev the stack of node embeddings, and the (heat) diffusion process that describes the evolution of Z(t) can be written as a partial differential equation (PDE):\n$\\frac{\\partial Z(t)}{\\partial t} = \\nabla^* (F(t) \\odot \\nabla Z(t)), \\quad \\text{s. t.} \\quad Z(0) = [x_i]_{i \\in V}, \\quad t \\geq 0,$\nwhere the function F(t) : [0,\u221e) \u2192 R|V|\u00d7|V| defines the diffusivity between any pair at time t. The gradient operator \u2207 converts node features (analogous to scalar fields on manifolds) into edge features (analogous to vector fields on manifolds) that measure the difference between source and target nodes, i.e., (\u2207Z(t))ij = zj(t) \u2013 zi(t). The divergence operator \u25bd* takes edge features into node features, by summing up information flows through a point:\n$\\nabla^* (F(t) \\odot \\nabla Z(t))_i = (S(t) \\cdot \\nabla Z(t))_i = \\sum_{j \\in V} S_{ij}(t) (\\nabla Z(t))_{ij},$\nwhere S(t) = [sij(t)]i,jev is a coupling matrix associated with the diffusivity F(t).\nThen with the gradient and divergence operators incorporated, Eqn. 3 can be explicitly written as\n$\\frac{d z_i(t)}{d t} = \\sum_{j \\in V} S_{ij}(t) (z_j(t) - z_i(t)).$\nSuch a diffusion process can serve as an inductive bias that guides the model to use other nodes' information at every layer (which can be seen as the discretization of time) for learning informative node representations.\nWe can adopt numerical methods to solve the continuous dynamics in Eqn. 5. For instance, using the explicit Euler scheme involving finite differences with step size t, i.e., $\\frac{d z_i(t)}{d t} \\approx \\frac{z_i^{(k+1)} - z_i^{(k)}}{\\tau}$, after some re-arranging we have\n$z_i^{(k+1)} = (1 - \\tau \\sum_{j \\in V} S_{ij}^{(k)}) z_i^{(k)} + \\tau \\sum_{j \\in V} S_{ij}^{(k)} z_j^{(k)},$\nwhere sk) is given by the trajectory S(k) of S(t) at the discrete step k. The above numerical iteration coincides with the updating rule of (graph) neural networks from layer k to"}, {"title": "3.2 Diffusion with Layer-wise Energy Constraints", "content": "As mentioned previously, the crux is how to define a proper coupling function to induce a desired diffusion process that can maximize the information utility and accord with the geometry behind observed data. Since we have no prior knowledge for the explicit form of S(t) (that can depend on the underlying data geometry), without loss of generality, we consider the diffusivity (and more specifically, the induced coupling matrix S(t)) as a latent variable for modeling. Furthermore, to enforce a constraint w.r.t. the presumed quality of node embeddings at an arbitrarily given layer k, we resort to an energy function E(Z,k) that measures the global smoothness of node embeddings, i.e., how variable the quantity is in the diffusive system. In common physical systems, the evolution pursues steady states that minimize some global energy and achieve some equilibrium (Kimmel et al., 1997; Bertozzi and Flenner, 2012; Luo and Bertozzi, 2017). Inspired by this phenomenon, we incorporate layer-wise constraints of energy minimization into the diffusion model:\n$z_i^{(k+1)} = (1 - \\tau \\sum_{j \\in V} S_{ij}^{(k)}) z_i^{(k)} + \\tau \\sum_{j \\in V} S_{ij}^{(k)} z_j^{(k)},$\n$\\text{s. t.} \\quad z_i^{(0)} = x_i, \\quad E(Z^{(k+1)}, k) \\leq E(Z^{(k)}, E(Z^{(k)}, k-1), \\quad k \\geq 1.$\nThe above formulation defines a new class of diffusion process on latent manifolds whose dynamics are implicitly defined by optimizing an energy function (see Figure 1 for an illustration). Eqn. 7 unifies two schools of thought into a new diffusive system where the updates of node embeddings are driven by both the diffusion dynamics (as an inductive bias) and the energy constraints (as a regularization). The diffusion process describes the"}, {"title": "4 Graph Neural Networks as Energy-Constrained Diffusion", "content": "The diffusion system of Eqn. 7 is hard to solve since we need to infer S(k) at arbitrary layers that are coupled by the energy minimization constraints of K inequalities (where K denotes the number of iterations). Instead of directly resolving this difficult case, in this section, we first consider a simple case where the diffusivity F(t) in Eqn. 3 is assumed to be fixed w.r.t. time t, in which situation Eqn. 3 becomes a linear diffusion equation and the induced coupling matrix S(t) (resp. S(k)) remains a constant matrix over time t (resp. layer k).\nWithin this setting, we can show that the corresponding diffusion dynamics with energy constraints would yield the updating rules of common GNNs. The proofs for all theoretical results are deferred to Appendix A."}, {"title": "4.1 Connection between Static Diffusivity and Energy", "content": "In the case of static diffusivity, the problem boils down to finding a constant coupling matrix S that gives rise to the diffusion dynamics satisfying the energy constraint at each step. We define the Laplacian of S as \u2206 = D \u2013 S, where D is the diagonal degree matrix of S, and we next show that for a typical quadratic energy form, there exists S whose yielded diffusion process is the solution for Eqn. 7 under certain mild conditions.\nTheorem 1 Assume that the diffusivity is fixed w.r.t. time (a.k.a. layers), i.e., S(k) = S = [Sij]i,jev. Then for any step size 0 < \u315c <$\\frac{1}{d_1}$, where d\u2081 is the largest singular value of \u2206, the feed-forward iteration of Eqn. 6 globally descends the energy of the quadratic form\n$E(Z, k) = ||Z - Z^{(k)}||_2^2 + \\lambda(\\tau) \\sum_{i,j} S_{ij} \\cdot ||z_i - z_j||_2^2,$\nwhere X is a weight dependent on the step size 1, formally E(Z(k+1), k) \u2264 E(Z(k), k\u22121), with equality iff Z(k) is a stationary point of E(Z,k).\nThe energy function Eqn. 8 integrates two-fold effects. The first term enforces the local smoothness that penalizes the large gap between the next-layer embedding and the one of the current layer. The second term, which can be essentially seen as the spatially discretized counterpart of Eqn. 2 with the instantiation of \u03a9 as a graph (Zhou and Sch\u00f6lkopf, 2005), enforces the global smoothness that penalizes the difference between the embeddings of different node pairs at the next layer. Thereby, Theorem 1 suggests that the diffusion process with static diffusivity inherently minimizes a convex energy that facilitates the consistency"}, {"title": "4.2 Further Discussions and Extensions", "content": "In the previous subsection, we pinpoint the underlying energy descended by the feed-forward diffusion dynamics. Some follow-up questions still remain. First, it is unclear how much quantity each iteration step contributes to the energy descent, which is linked with the"}, {"title": "4.2.1 UPPER AND LOWER BOUNDS FOR THE LAYER-WISE ENERGY", "content": "The diffusion iterations in Eqn. 6 are determined by two factors: the coupling matrix S(k) (which is assumed to be a constant matrix S in our case) and the step size 7. The former determines the rate of information flows across different nodes, while the latter controls the forward speed of one-step iteration. We can show that the minimization of the global energy can be further characterized by the upper and lower bounds at each step, which reveals the descending speed by each diffusion iteration, and the bounds are associated with the coupling matrix S and the step size \u03c4.\nProposition 4 On the same conditions of Theorem 1, for arbitrarily given k, the energy yielded by the diffusion iteration Eqn. 6 with S(k) = S is bounded by:\n$(1 - \\tau \\lambda_1)^2 E(Z^{(k)}, k - 1) \\leq E(Z^{(k+1)}, k) \\leq (1 - \\tau \\lambda_2)^2 E(Z^{(k)}, k - 1),$\nwhere A2 is the smallest singular value of A.\nThis proposition indicates that the energy yielded by the next layer lies in a certain interval dependent on the energy of the previous layer. It further suggests that the convergence rate of energy minimization is $O(1 - \\tau \\lambda_i)^2$ depending on the smallest eigenvalue of the Laplacian of S and the step size \u03c4."}, {"title": "4.2.2 GLOBAL CONVERGENCE WITH A SOURCE TERM", "content": "Another concerning issue of the diffusion iteration is its convergence to the global optimum of the energy Eqn. 8 that corresponds with a degenerate solution where all the node embeddings degrade to a single point in the latent space. Critically though, such a potential risk can be overcome by augmenting the diffusion equation Eqn. 5 with a source term\n$\\frac{d z_i(t)}{d t} = \\sum_{j \\in V} S_{ij}(t) (z_j(t) - z_i(t)) + \\beta h_i,$\nwhere hi with the weight \u1e9e can be considered as some extra input signals from external sources to each point within the system. Correspondingly, the induced numerical iteration would become the counterpart of Eqn. 6 augmented with an additional term \u03c4\u1e9eh; for node i at each step. By extending the analysis of Theorem 1, we can obtain the energy function descended by the new diffusion system (where we assume H = [hi]iev)."}, {"title": "5 Transformer Backbones Induced by Energy-Constrained Diffusion", "content": "In the previous section, we assume the diffusivity to be dependent on specific locations yet stay unchanged over time (a.k.a. layers). While we have shown that the diffusion process in such a case implicitly minimizes a principled energy which regularizes the internal consistency of the produced embeddings at each step, the static diffusivity may limit the flexibility of the diffusion system, in particular for accommodating the adaptive pairwise influence among data points. In real-world complex physical systems, the diffusivity often goes through both spatial and temporal variations. For example, in cells, the diffusivity of ions and molecules can change due to fluctuations in temperature, local concentration gradients, and cellular activity (Heitjans and K\u00e4rger, 2006); besides, in fluid dynamics, turbulent flows can exhibit varying diffusivity due to the chaotic nature of the flow (Pope, 2000; Csanady, 1973).\nWe next investigate a more expressive model that is comprised of time-dependent diffusivity, in which case Eqn. 3 becomes a non-linear diffusion equation and the induced coupling matrix S(k) in Eqn. 5 can flexibly change at different layers. In such a case, the information among arbitrary node pairs can flow at an adaptive rate dependent on specific locations and time. We will show that in such a situation, there also exists an associated global energy function that are implicitly descended by the diffusion dynamics. Furthermore, we will link the time-dependent coupling matrix with the attention mechanism that is often inserted in-between two neural layers to model the pairwise influence based on the embeddings computed at the current layer."}, {"title": "5.1 Connection between Time-Dependent Diffusivity and Energy", "content": "When the diffusivity can vary over time, Eqn. 7 becomes hard to solve since we need to infer the value for a series of coupled S(k)'s that need to satisfy K inequalities by the energy minimization constraints. Instead of solving Eqn. 7 directly, we can notice a natural corollary based on Theorem 1 that the k-th step iteration of Eqn. 6 contributes to a descent step on the local energy at the current layer:\n$E(Z, k; S^{(k)}) = ||Z - Z^{(k)}||_2^2 + \\lambda(\\tau) \\sum_{i,j} S_{ij}^{(k)} \\cdot ||z_i - z_j||_2^2,$\nwhere s) is the (i,j)-th entry of the coupling matrix S(4) at the k-th step. We can thereby extend the analysis and results in Section 4 for interpreting the behavior of one-step diffusion iteration from the k-th layer to the (k + 1)-th. However, since the energy function Eqn. 12 depends on the coefficient sk) at the k-th layer that varies throughout the diffusion process, it is still unclear the global behavior of diffusion dynamics, particularly if there is a global energy (shared across all layers) that is minimized by the whole trajectory.\nIn the following, we aim to unlock the black box of the diffusion system with time-dependent diffusivity and reveal a global energy associated with the diffusion, which boils down to finding closed-form solutions for S(k) that give rise to a diffusion process satisfying Eqn. 7. To achieve this goal, we first prove a preliminary result that suggests a surrogate energy that serves as a strict upper bound of a non-convex regularized energy.\nProposition 6 For the regularized energy function of the form\n$E(Z, k; \\delta) = ||Z - Z^{(k)}||_2^2 + \\lambda \\sum_{i,j} \\delta(||z_i - z_j||_2^2),$\nwhere d : R+ \u2192 R is defined as a function that is non-decreasing and concave on a particular interval of our interest, we have its upper bound E(Z, k; \u03b4) \u2264 \u1ebc(Z, k; \u03a9(k), \u03b4) :\n$\\tilde{E}(Z, k; \\Omega^{(k)}, \\delta) = ||Z - Z^{(k)}||_2^2 + \\lambda \\sum_{i,j} \\omega_{ij}^{(k)} ||z_i - z_j||_2^2 - \\delta(\\omega_{ij}^{(k)}),$\nwhere (k) = [w()]i,jev and 8 denotes the concave conjugate of 8, and the equality holds if and only if the variaitonal parameter wk) satisfies $w_{ij}^{(k)} = \\frac{\\partial \\delta}{\\partial z^2}|_{z=||z_i - z_j||^2}$.\nIn light of the proposition, we notice that if treating the coupling matrix S(k) as the variaitonal parameters \u03a9(k), then one-step iteration of Eqn. 6 contributes to minimizing the upper bound of the non-convex energy Eqn. 13. Pushing further, if the coupling matrix at the k-th layer is given by $S^{(k)} = \\Omega^{(k)} = [w_{ij}^{(k)}]_{i,j \\in V}$ where $\\omega_{ij}^{(k)} = \\frac{\\partial \\delta}{\\partial z^2}|_{z=||z_i - z_j||^2}$, then Eqn. 6 serves to minimize the global energy Eqn. 13 that equals to the quantity of the surrogate Eqn. 14 with the variational parameters wij w(k) fixed as s). One concern, however, is the convergence of the gradient descent on Eqn. 14 with the iteration of Eqn. 6, which, as shown by previous analysis, depends on S(k) and the step size 7. Since S(k) can be different"}, {"title": "5.2 Principled Attention Layers Derived From Diffusion Process", "content": "Theorem 7 suggests the existence for the optimal S(k) at each time step for the diffusion process satisfying the energy minimization constraint (i.e., Eqn. 7). The result enables us to unfold the implicit process of Eqn. 7 and compute S(k) in a feed-forward way from the initial embeddings Z(0). Specifically, the condition of Eqn. 15 implies that the optimal S(k) in the form of a function over the 12 distance between node embeddings, i.e., $|z_{ij}^{(k)}| = ||z_i^{(k)} - z_j^{(k)}||_2^2$. We thereby introduce a pairwise distance function f(z\u00b2) and define a new family of neural model architectures with layer-wise updating rules specified by:\n$\\text{Diffusivity Inference:} \\quad S_{ij}^{(k)} = \\frac{f(||z_i^{(k)} - z_j^{(k)}||)}{\\sum_{l \\in V} f(||z_i^{(k)} - z_l^{(k)}||)}, \\quad \\forall i, j \\in V,$\n$\\text{State Updating:} \\quad z_i^{(k+1)} = (1 - \\tau) z_i^{(k)} + \\tau \\sum_{j \\in V} S_{ij}^{(k)} z_j^{(k)}, \\quad i \\in V.$\nThe model layer defined above consists of two consecutive operations where the diffusivity inference estimates the pairwise attention using the current node embeddings and the state updating computes the next-layer embeddings with attention-based propagation. According to the results of Proposition 6 and Theorem 7, Eqn. 17 can be seen as an execution of a minimization-minimization algorithm towards optimizing the energy target Eqn. 13: 1) with fixed Z(k), the diffusivity inference returns the optimal variational parameters \u015c(k) = [sk]i,jev that decrease the upper bound, i.e., surrogate energy Eqn. 14, to approximate the target Eqn. 13; 2) the state updating proceeds to descend the surrogate energy with the variational parameters \u2229(k) = \u015c(k) fixed, which equivalently minimizes the energy target Eqn. 13."}, {"title": "5.2.1 ATTENTION DESIGNS INSPIRED BY TIME-DEPENDENT DIFFUSIVITY", "content": "We next go into model instantiations based on the above theory, and introduce two specified f's as practical versions of our model. To begin with, because ||zi - zj ||2 = |zi||32 + ||zj||32 \u2013 2z\u0142 zj, we can convert f(||zi \u2013 zj||2) into the form g(zz;) on the condition that ||zi||2 remains"}, {"title": "5.2.2 CONNECTION WITH EXISTING ATTENTION MECHANISMS", "content": "Another interesting perspective is to leverage the energy-constrained diffusion framework to interpret the existing attention networks. From Eqn. 17 one can naturally connect \u015d with the attention score and consider f as a similarity measure. For example, in the original Transformers (Vaswani et al., 2017), f is instantiated as a dot-then-exponential operator:\n$f(||z_i^{(k)} - z_j^{(k)}||_2) = \\exp(\\frac{(z_i^{(k)})^T z_j^{(k)}}{\\sqrt{d}}), \\quad S_{ij}^{(k)} = \\frac{\\exp((z_i^{(k)})^T z_j^{(k)})}{\\sum_{l \\in V} \\exp((z_i^{(k)})^T z_l^{(k)})}.$"}, {"title": "5.3 Model Extensions and Further Discussions", "content": "The analysis so far focuses on the message passing rules of different models, particularly the propagation of node embeddings in-between layers. In consideration of practical imple- mentation, apart from the feature propagation, common neural networks involve feature transformation (e.g., the trainable weight matrices involved in the feed-forward layer) and non-linear activation to endow the model with capacity for expressing complex functions. Moreover, in the context of learning on graphs, there often exist observed graphs that can be informative for improving the quality of node representations. We next probe into how our theory can be generalized to practical neural networks and how to incorporate the graph inductive bias into the global attentions. Besides, similar to the diffusion with static diffusivity discussed in Section 4, the diffusion model presented in this section is susceptible to the over-smoothing problem as well, and we will discuss how to resolve this issue in the case of time-dependent diffusivity."}, {"title": "5.3.1 INCORPORATING FEATURE TRANSFORMATIONS IN-BETWEEN LAYERS", "content": "Common neural networks utilize feature transformation and non-linear activation in-between two (propagation) layers. More specifically, after inserting the layer-wise transformation, the updating rule of the k-th step can be written as:\n$z_i^{(k+1)} = \\sigma \\left( (1 - \\tau \\sum_{j \\in V} S_{ij}^{(k)}) h^{(k)}(z_i^{(k)}) + \\tau \\sum_{j \\in V} S_{ij}^{(k)} h^{(k)}(z_j^{(k)}) \\right),$\nwhere h(k) is the feature transformation of the k-th layer (e.g., a fully-connected layer) and \u03c3 denotes the non-linear activation (e.g., ReLU). We can generalize our previous result to show that the above diffusion iteration decreases a layer-specific energy.\nProposition 9 For any non-decreasing element-wise activation function \u03c3 and step size 0 < r \u2264 1, there exists a penalty function \u03c8o such that the iteration of Eqn. 22 with S(k) given by Eqn. 15 contributes to a descent step from Z(k) to Z(k+1) on the energy\n$E(Z, k; \\delta, h^{(k)}) = ||Z - h^{(k)}(Z^{(k)})||_2^2 + \\sum_{i,j} \\delta(||z_i - z_j||_2^2) + \\sum_i \\psi_{\\sigma}(z_i).$\nRemark. The trainable transformation h(k) increases the model capacity and can be optimized w.r.t. the learning objective to map the node embeddings into a proper latent space. For large datasets with complex inter-connecting patterns, the feature transformation h(k) allows the diffusion model to propagate embeddings over layer-specific latent manifolds. Our experiments found that the layer-wise transformation is not necessary for small datasets, but contributes to some performance gains for datasets with a large number of instances. Besides, while as shown by the analysis our theory can be extended to incorporate the non-linear activation \u03c3, we empirically found that omitting the non-linearity can still perform well in quite a few cases."}, {"title": "5.3.2 INCORPORATING GRAPH INDUCTIVE BIAS", "content": "The model presented in Section 5.2.1 does not assume any observed graph as input. For situations with observed structures (e.g., in the form of graphs), we can leverage the structural information as a geometric prior. Denote by G = (V,E) the input graph, and here we adopt a simple-yet-effective design by modifying the layer-wise updating rule as:\n$z_i^{(k+1)} = \\left( 1 - \\tau (\\sum_{j \\in V} \\frac{S_{ij}^{(k)} + \\tilde{a}_{ij}}{2}) \\right) z_i^{(k)} + \\tau \\sum_{j \\in V} (\\frac{S_{ij}^{(k)} + \\tilde{a}_{ij}}{2}) z_j^{(k)},$\nwhere aij is the connectivity weight of the edge (i, j) \u2208 E. The above model can be seen as an integration of the attention-based propagation and the graph-based propagation. In particular, one can consider the normalized adjacency for the graph-based propagation, in which case $\\tilde{a}_{ij} = \\frac{1}{\\sqrt{d_i d_j}}$ (or $\\tilde{a}_{ij} = \\frac{1}{d_i}$) if (i,j) \u2208 E and 0 otherwise. By extending the proof of Theorem 7, we can show that the diffusion iteration of Eqn. 24 is equivalent (up to a"}, {"title": "5.3.3 DIFFUSION WITH A SOURCE TERM: RESOLVING OVER-SMOOTHING", "content": "While the energy defined by Eqn. 13 is non-convex, the minimization of the energy can still lead to the degenerate solution where all embeddings are equal to one another, in which case the first term of Eqn. 13 is minimized to zero, and the second term is minimized because d is non-decreasing. In this situation, there exists the potential risk for the over-smoothing problem. To fundamentally avoid such an issue, we can leverage the remedy in Section 4.2.2 and consider the diffusion equation with a source term. As a natural extension of our analysis in this section, we can show that Eqn. 10 with time-dependent diffusivity descends a regularized energy whose global optimum does not cause over-smoothing.\nProposition 10 For step size 0 < r \u2264 1, the diffusion dynamics Eqn. 10 with S(k) = [S"}]}