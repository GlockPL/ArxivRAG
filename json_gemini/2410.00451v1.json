{"title": "ADVERSARIAL SUFFIXES MAY BE FEATURES TOO!", "authors": ["Wei Zhao", "Zhe Li", "Yige Li", "Jun Sun"], "abstract": "Despite significant ongoing efforts in safety alignment, large language models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks that can induce harmful behaviors, including those triggered by adversarial suffixes. Building on prior research, we hypothesize that these adversarial suffixes are not mere bugs but may represent features that can dominate the LLM's behavior. To evaluate this hypothesis, we conduct several experiments. First, we demonstrate that benign features can be effectively made to function as adversarial suffixes, i.e., we develop a feature extraction method to extract sample-agnostic features from benign dataset in the form of suffixes and show that these suffixes may effectively compromise safety alignment. Second, we show that adversarial suffixes generated from jailbreak attacks may contain meaningful features, i.e., appending the same suffix to different prompts results in responses exhibiting specific characteristics. Third, we show that such benign-yet-safety-compromising features can be easily introduced through fine-tuning using only benign datasets, i.e., even in the absence of harmful content. This highlights the critical risk posed by dominating benign features in the training data and calls for further research to reinforce LLM safety alignment. Our code and data is available at https://github.com/anonymous.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) such as GPT-4 (Achiam et al., 2023), Llama2 (Touvron et al., 2023), Vicuna (Chiang et al., 2023), and Mistral (Jiang et al., 2023) have demonstrated remarkable capa-bilities across a wide range of natural language tasks and have been increasingly adopted in many real-world applications. Despite extensive efforts (Ouyang et al., 2022; Bai et al., 2022; Glaese et al., 2022; Zhou et al., 2024; Wang et al., 2023) to align LLMs' responses with human values and generate helpful and harmless content, many recent studies (Perez et al., 2022; Wei et al., 2023a; Deng et al., 2023; Shen et al., 2023; Zou et al., 2023; Wei et al., 2023b; Zeng et al., 2024; Chao et al., 2023; Huang et al., 2024; Liu et al., 2024; Li et al., 2023) reveal that these aligned models are still vulnerable to \u201cjailbreak attacks\u201d, which can elicit harmful, biased, or otherwise unintended behaviors from LLMs, posing significant challenges to their safe deployment. Among adversarial attacks, the Greedy Coordinate Gradient (GCG) method (Zou et al., 2023) is one particularly effective approach for jailbreaking. By combining greedy and gradient-based search, GCG produces adversarial suffixes that, though nonsensical to humans, can manipulate strongly aligned LLMs into improperly responding to harmful prompts.\nWhile some researchers attribute this vulnerability to the model's misalignment when processing out-of-distribution prompts (Cherepanova & Zou, 2024), we conjecture that these adversarial suf-fixes may actually represent meaningful features for the LLM, or at the very least, can be intention-ally designed as such. This idea is inspired by the findings of (Ilyas et al., 2019), which demonstrate that adversarial perturbations are not simply anomalies but rather features that models actively ex-ploit. To evaluate our conjecture, we conduct three experiments.\nFirst, we demonstrate that benign features can function as adversarial suffixes effectively. In par-ticular, we construct multiple benign datasets, each of which exhibits one specific sample-agnostic feature, such as a specific response format. We subsequently design a universal feature extraction method to generate transferable suffixes that reliably induce the corresponding feature (e.g., the spe-cific response format). Our results show that these suffixes consistently activate the intended feature"}, {"title": "2 ANALYSIS METHODS", "content": "In this section, we introduce two tools that are used in our experiments, i.e., a method that we develop in this work for extracting universal features in the form of suffixes from either benign or harmful dataset, and a method for analyzing the influence of suffixes."}, {"title": "2.1 EXTRACTING FEATURES AS SUFFIXES", "content": "In the following, we develop a method for extracting sample-agnostic features from a given dataset (whether benign or harmful) in the form of suffixes. It is important to note that, for the purpose of this study, the suffixes may take the form of either embedding vectors or input tokens.\nLet $T \\in \\mathbb{R}^{n \\times d}$ represent a tokenized input prompt consisting of $n$ tokens, where each token is embedded in a $d$-dimensional space; let $S \\in \\mathbb{R}^{l \\times d}$ represent an initial suffix consisting of $l$ tokens; and let $y \\in \\mathbb{R}^{m \\times d}$ represent the corresponding target response. We define an embedding function $E : T \\rightarrow e$ that maps a set of tokens to their embedding vectors. We write $e_{\\text{prompt}} \\in \\mathbb{R}^{n \\times D}$ to denote the (fixed) embedding representation of the input prompt where $D$ is the dimension of the embedding space, and $e_{\\text{suffix}} \\in \\mathbb{R}^{l \\times D}$ to denote the embedding of the suffix that we aim to optimize. Given a language model $F$ and a set of prompts and targeted responses, the objective of our feature extraction method is to optimize one suffix such that $F$ generates the target response given the concatenated embedding of each prompt and the suffix. Formally,\n$F(e_{\\text{prompt}} \\oplus e_{\\text{suffix}}) \\rightarrow y$,\nwhere $\\oplus$ denotes concatenation. To achieve the above objective, we aim to minimize the discrepancy between the target response $y$ and the model's predicted response by minimizing the cross-entropy loss.\n$L_{\\text{adv}} = \\text{CrossEntropy}(F (e_{\\text{prompt}} \\oplus e_{\\text{suffix}}), y)$.\nWe note that the objective function above is defined in terms of the embedding, enabling us to explore a wide range of features within the embedding space, even though many of these features may not correspond to valid token sequences.\nTo promote the generation of suffixes that correspond to valid token sequences, we sometimes (when stated explicitly) introduce a constraint that encourages the optimized suffix embeddings to align with the model's token embedding space. That is, we add a regularization term to the loss function that measures the proximity of the optimized suffix embeddings to the embeddings of actual tokens. This additional loss term is defined as:\n$L_{\\text{embed}} = \\frac{1}{l} \\sum_{i=1}^{l} \\frac{1}{k} \\sum_{j=1}^{k} \\min({||e_{\\text{suffix}_i} - E_m || : m = 1, . . ., V})_j$,\nwhere $V$ is the size of the tokenizer's vocabulary (i.e., so that the model's embedding matrix is $E \\in \\mathbb{R}^{V \\times D}$); $\\min_k$ selects the $k$ tokens in the vocabulary that are nearest to the $i$-th suffix token"}, {"title": "2.2 PCC ANALYSIS", "content": "In our experiments to be present in the subsequent section, we adopt the Pearson Correlation Coefficient (PCC) (Anderson, 2003) to quantify the influence of different suffixes. PCC is a widely applied metric that measures the linear correlation between two variables, which is defined as follows.\n$\\text{PCC}_{X,Y} = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y}$,\nwhere $X$ and $Y$ are two vectors, $\\text{cov}(X, Y)$ is their covariance, and $\\sigma_X$ and $\\sigma_Y$ are the standard deviation of vector $X$ and $Y$. The PCC value ranges from -1 to 1, where an absolute value of 1 indicates perfect linear correlation, 0 indicates no linear correlation, and the sign indicates the direction of the correlation (positive or negative). In this study, given a prompt $p$ and a suffix $s$, we define the following variables based on the last hidden states of an LLM.\n*   $H_p$: the last hidden state of the LLM given the prompt $p$.\n*   $H_{p+s}$: the last hidden state of the LLM given the prompt $p$ appended with the suffix $s$.\n*   $H_s$: the last hidden state of the LLM given the suffix $s$ only.\nNote that we focus on the last hidden states because, in auto-regressive language models, this state encapsulates all the features necessary to generate the response.\nBy comparing $\\text{PCC}_{H_p,H_{p+s}}$ and $\\text{PCC}_{H_s,H_{p+s}}$, we gain insights into the contributions of the prompt and the suffix. Since a higher PCC value indicates a greater influence, if $\\text{PCC}_{H_p,H_{p+s}}$ is larger than $\\text{PCC}_{H_s,H_{p+s}}$, it suggests that the prompt plays a more dominant role than the suffix in shaping the model's output."}, {"title": "3 EXPERIMENT 1: BENIGN FEATURES ACT AS ADVERSARIAL SUFFIXES", "content": "In this experiment, we demonstrate that it is possible to generate suffixes that capture benign fea-tures while effectively undermining safety alignment, i.e., they form effective adversarial suffixes. This experiment is motivated by our observation that adversarial suffixes generated by GCG can occasionally prompt responses in specific \"formats,\u201d such as structured point-by-point answers."}, {"title": "3.1 EXPERIMENT SETUP", "content": "Dataset Construction We start with constructing multiple datasets that exhibit certain specific fea-ture, using a systematic three-step process as follows.\n1.  Sampling Prompts: We sample a set of benign, diverse, task-oriented dialogues to form an initial dataset. Note that we use random sampling to ensure the initial dataset covers a wide range of topics and linguistic styles.\n2.  Generating Format-Specific Responses: We take the initial dataset and use an LLM to process the responses with an explicit formatting instruction so that the responses are all in certain specific formats.\n3.  Filtering for Safety and Validity: We apply an LLM to filter out any resultant prompt-response pairs containing potentially harmful content or content of low quality.\nSpecifically, we randomly select 1,000 benign prompts from the Alpaca dataset, apply the Llama2-7B-chat-hf model (Touvron et al., 2023) to process the responses using two system templates to produce responses in two specific formats (i.e., a poetic style and a structured point-by-point format), and use GPT-4 to filter harmful or low-quality content. These templates are present in Figures 6 and 7 in the Appendix. Through this process, we constructed the Poem and Structure datasets. Representative examples are included in Appendix A.5.\nFeature Extraction We apply the method presented in Section 2 to generate suffix embeddings across three widely used open-source LLMs: Llama2-7B-chat-hf (Touvron et al., 2023), Vicuna7B-1.5 (Chiang et al., 2023), and Mistral-7B (Jiang et al., 2023) on the Poem and Structure datasets. Specifically, we set the token-regularization coefficient $\\lambda$ to zero, allowing the embedding vectors to fully capture the format features without penalizing token similarity.\nEvaluation Metric For each dataset, we generate one suffix in the form of embedding value that achieves the best performance according to the objective function. We then evaluate the generated suffixes from two perspectives, i.e., their ability to transfer to other benign prompts and induce responses in the expected format (transferability), and their transferability to harmful prompts and induce harmful responses, measured by the attack success rate (ASR) and the harmfulness score. Note that the former evaluates whether the suffix captures the feature and the latter checks whether it functions as an adversarial suffix.\nTo compute transferability, we append each generated suffix to 500 randomly selected benign prompts from the Alpaca dataset, use GPT-4-turbo to assess whether the generated responses ex-hibit the target format/style, report the proportion of benign prompts resulting in responses with the target format/style as the result. To evaluate the ASR, we append each suffix to 520 harmful prompts from AdvBench (Zou et al., 2023) and employ GPT-4-turbo to assess the ASR and the average harmfulness of the resultant responses. Note that we consider a prompt attacks success-fully if the response is deemed violating OpenAI's safety guidelines according to GPT-4-turbo. For harmfulness evaluation, we ask GPT-4-turbo to rate the responses on a scale from 1 to 5, where 1 indicates no harm and appropriate model refusal of the request, and 5 signifies a response that dis-regards usage guidelines and clearly violates the policy. Detailed evaluation templates are provided in Appendix A.1."}, {"title": "3.2 EXPERIMENT RESULTS", "content": "The experiment results are summarized in Table 1. It can be observed that the extracted suffix expect-edly captures the format/style feature, inducing the models to consistently produce responses in the corresponding format/style and achieving a transferability of more than 94% on both datasets across"}, {"title": "4 EXPERIMENT 2: JAILBREAKING SUFFIXES CONTAIN FEATURES", "content": "While the results in Experiment 1 show that certain benign features indeed could serve as adversarial features, in this experiment we aim to show that jailbreaking attack may generate adversarial suffix that contain meaningful features."}, {"title": "4.1 EXPERIMENT SETUP", "content": "Our approach We conduct the experiment with the following steps. We first apply our method presented in Section 2 to construct multiple suffixes based on the dataset of harmful prompt and re-sponse pairs. In particular, we randomly select 1000 harmful prompt-response pairs from Sheshadri et al. (2024) as the harmful dataset to generate the suffixes. Note that in such a setting, our method ef-fectively becomes a universal adversarial attack method (which shares a similar goal with GCG (Zou et al., 2023) and AmpleGCG (Liao & Sun, 2024)). We remark that the suffixes generated using our method may be in the form of token sequences or in the form of embedding sequences. Next, we evaluate whether the generated suffixes are indeed effective adversarial suffixes (i.e., achieving a high ASR on LLMs), and whether the generated suffixes contain certain meaningful feature.\nBaseline Setup To check whether the generated suffixes are indeed effective adversarial suffixes, we adopt two state-of-the-art universal adversarial attack methods, i.e., GCG (Zou et al., 2023) and AmpleGCG (Liao & Sun, 2024), for baseline comparison. For GCG, we generated 1,000 adversarial suffixes and assess their transferability on the AdvBench dataset. For AmpleGCG, we produce"}, {"title": "4.2 EXPERIMENT RESULTS", "content": "The experiment results are shown in Table 2. It can be observed that our method generates an adver-sarial suffix in the form of tokens that achieves slightly improved performance over that generated by GCG or AmpleGCG. More importantly, our method is able to generate adversarial suffixes in the form of embeddings that are much more effective, i.e., with nearly perfect ASR and harmfulness scores across all three models. This is not surprising since the embedding space is much larger and much easier to work with. It should be noted however our goal is not to conduct adversarial suffix attack, but rather to show that these suffixes, especially those in the form of embeddings, are indeed effective adversarial suffixes.\nWe then proceed to analyze the adversarial suffixes generated by our method to check whether they may contain certain meaningful features. In particular, we extract a set of embedding adversarial suffixes generated using our method, and proceed to evaluate whether they indeed contain certain meaningful features by systematically appending the suffix to a benign dataset and observe the resultant responses manually. While we admit that not all suffixes result in responses that have observable feature\u00b9, we found three suffixes that lead to the following distinct response style/format.\n*   Story Telling: The response sets up a story or narrates a story, i.e., providing detailed settings or character descriptions.\n*   Basic Program: The response includes content or style which is clearly related to basic programming.\n*   Repeat Response: The response is repetitive, i.e., frequently repeating certain phrases or sentences throughout the text.\nNext, we systematically evaluate whether these three suffixes indeed induce the corresponding fea-ture. Table 3 summarizes the performance of these three suffixes in terms of transferability (whether they induce the corresponding feature), ASR (whether they compromise safety alignment of the model) and the harmfulness score. The transferability is evaluated by appending these suffixes to the same 500 benign prompts used in Experiment 3 on the Llama2-7B model and assess whether the resultant response exhibits the feature. It can be observed that two of the three suffixes achieves reasonably high level of transferability as well as ASR.\nTo verify that the suffixes are the main factors affecting the model's responses, we performed a PCC analysis on the three embedding adversarial suffixes. The results, shown in Figure 5, confirm that these suffixes assert strong influence on the responses, i.e., $\\text{PCC}_{H_s, H_{p+s}}$ is close to 1. Furthermore, since $\\text{PCC}_{H_p, H_{p+s}}$ is low, the original prompt in fact has limited impact on the final response. In"}, {"title": "5 EXPERIMENT 3: BENIGN DATASET MAY COMPROMISE SAFETY", "content": "While the previous experiments show that certain (benign) features can constitute effective adver-sarial suffixes or can be sought out by adversarial attacks to compromise safety alignment, in this experiment we aim to show that such benign features may be introduced through fine-tuning with a benign dataset, either accidentally or intentionally, to dominate and compromise the safety align-ment. We believe that such a threat has been largely overlooked so far."}, {"title": "5.1 EXPERIMENT SETUP", "content": "Dataset Construction Based on data gathered in the previous experiments, we construct 5 benign datasets, each with 1000 prompt-response pairs, including the two used in Experiment 1 and three constructed based on the embedding universal adversarial suffixes generated in Experiment 2. Note that the former are generated using LLMs and template system prompts (for structure response and poem response). The latter are generated by appending the three universal suffixes extracted from"}, {"title": "5.2 EXPERIMENT RESULTS", "content": "Table 4 presents the impact of fine-tuning on the model's safety across different datasets and lan-guage models. The original models, prior to the fine-tuning, do not produce any harmful responses (i.e., with an ASR of 0%). Fine-tuning the model with a randomly collected benign dataset results in some degradation on safety for the open-source models such as Llama2 and Llama3-guard (i.e., with an ASR approximately 21%), a slight degradation of safety for GPT-3.5 (with an ASR of 12.1%), and no impact on GPT-40-mini (i.e., ASR remained at 0%). This result is consistent with the results reported in Qi et al. (2023).\nFine-tuning on the template-generated benign datasets (i.e., the poem dataset and the structure dataset) leads to mixed results. While the poem dataset compromises the safety alignment only slightly (e.g., with an ASR of 6.3% for GPT-40-mini), the structure dataset compromises the safety alignment considerably (e.g., with an ASR of 75.2% for GPT-40-mini). Note that this is consistent with the results presented in Table 2. More wearisomely, all datasets constructed with the universal suffixes (i.e., the red ones) result in severe degradation in safety alignment after fine-tuning. It sug-gests that models can easily learn the features from these datasets, which subsequently dominates over the safety alignment.\nOur experiments reveal that fine-tuning large language models (LLMs) on benign datasets with dominant features can significantly undermine their safety alignment. The models tend to overlearn"}, {"title": "6 RELATED WORK", "content": "This study is related to prior studies on jailbreak attacks, and studies on how fine-tuning may com-promise safety alignment."}, {"title": "6.1 JAILBREAK ATTACK", "content": "Jailbreak attacks aim to elicit unintended and unsafe behaviors from LLMs via well-crafted harmful queries. Recent approaches automate this process using gradient-based methods (Zou et al., 2023; Liu et al., 2024), genetic algorithms (Liu et al., 2023), and random searches (Pal et al., 2023; Hayase et al., 2024). Others employ auxiliary LLMs to refine jailbreak templates (Yu et al., 2023; Chao et al., 2023). In this work, we focus on jailbreak methods that are designed to generate adversarial suffixes with high transferability across different prompts. Our conjecture is that these adversarial suffixes may contain features that are meaningful and effective across various types of inputs. It is possible to extend our study to other kinds of adversarial attacks, although it may not be trivial, i.e., the experiments and analysis must be designed differently."}, {"title": "6.2 FINE-TUNING AND SAFETY", "content": "Studies have shown that fine-tuning with a small set of harmful samples can unsurprisingly compro-mise LLM safety alignment (Shan et al., 2022; Shu et al., 2023; Zheng et al., 2024). Surprisingly, there are some studies that show that even fine-tuning with benign data can degrade a model's safety performance to some extent (Qi et al., 2023; Zhan et al., 2023). He et al. (He et al., 2024) investi-gated this phenomenon using data selection techniques such as representation matching and gradient matching. They discovered that selected data, often structured as lists, bullet points, or math ques-tions, can degrade model safety during benign fine-tuning. While their work and ours both find that fine-tuning on structured format data can weaken an LLM's safety, our research significantly extends theirs with an approach to systematically generate such safety-compromising benign dataset (i.e., by using our method to generate universal adversarial suffixes). More importantly, our approach aims to analyze the impact of specific response structures/styles on the model behavior, providing in-depth insights on how benign data can inadvertently undermine safety alignments in LLMs."}, {"title": "7 CONCLUSION", "content": "In this study, we conduct multiple experiments to show that (1) benign features may function as effective adversarial suffixes, (2) adversarial suffixes generated by adversarial attacks may indeed contain meaningful features, and (3) such safety-compromising benign features may be easily intro-duced through benign datasets.\nOur analysis demonstrates that certain benign features can dominate and bypass the safety mecha-nisms of LLMs. This exposes a significant and yet somewhat overlooked vulnerability: even highly aligned LLMs can be manipulated through benign features to exhibit harmful behaviors. These find-ings highlight the need for research into more robust alignment techniques that can safeguard LLMs against such vulnerabilities, ensuring that safety alignment is maintained despite the presence of dominant features in fine-tuning datasets. Given that fine-tuning is a common practice across vari-ous domains, many of which exhibit distinct characteristics (e.g., specialized writing styles in legal documents), addressing this issue is both urgent and critical."}]}