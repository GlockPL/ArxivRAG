{"title": "Learning Low-Level Causal Relations using a Simulated Robotic Arm", "authors": ["Miroslav Cibula", "Matthias Kerzel", "Igor Farka\u0161"], "abstract": "Causal learning allows humans to predict the effect of their actions on the known environment and use this knowledge to plan the execution of more complex actions. Such knowledge also captures the behaviour of the environment and can be used for its analysis and the reasoning behind the behaviour. This type of knowledge is also crucial in the design of intelligent robotic systems with common sense. In this paper, we study causal relations by learning the forward and inverse models based on data generated by a simulated robotic arm involved in two sensorimotor tasks. As a next step, we investigate feature attribution methods for the analysis of the forward model, which reveals the low-level causal effects corresponding to individual features of the state vector related to both the arm joints and the environment features. This type of analysis provides solid ground for dimensionality reduction of the state representations, as well as for the aggregation of knowledge towards the explainability of causal effects at higher levels.", "sections": [{"title": "1 Introduction", "content": "Observing and learning causal relations in a given environment is an essential element of cognition in humans and other high animals. Thanks to this ability, agents can form intuitive theories from multiple observations and use them to predict the environment's behaviour in response to their actions [4]. This common sense understanding includes the knowledge of intuitive physics, a key ingredient of early cognitive development [9].\nCausal models capturing and learning causal relationships from observations can be used for action planning towards task completion. Analysis of these models, encapsulating intuition about the given environment and their predictions, can be helpful for causal reasoning. In humans, a seven-grade model of the evolution of causal cognition has been proposed, with increasingly more complex causal skills [11]. These range from individual causal understanding and tracking behaviour (understanding perceived effects of one's own motor actions) up"}, {"title": "2 Background and Related Work", "content": "The sensorimotor knowledge in a robotic system is commonly represented by a pair (or pairs of) of complementary models: the forward model (FM) that predicts sensory consequences of one's own actions, and an inverse model (IM) that predicts actions in order to reach the desired state [22].\nThe FM is commonly called a causal model, which is mathematically well-defined, whereas the IM is non-causal since it solves an inverse problem where"}, {"title": "3 Methods", "content": "We collected sensorimotor data in a simulated environment using the myGym toolkit [21]. In each step, the agent (robotic arm) executes a randomly selected action and observes a new state. Motor babbling is a natural process observed in infants during their first months. In the case of interaction with objects, the concept of intuitive physics becomes relevant. In the case of C1, the arm performs motor babbling and records its joint configuration and Cartesian effector position before and after the execution of an action.\nIn the case of C2, an object is added to the table in the simulated environment, and the arm has the possibility to interact with it using constrained motor babbling. During an episode, the agent observes potential changes in position, rotation and other defined features of the object, arm and environment in response to the arm's actions."}, {"title": "3.2 Forward and Inverse Models", "content": "The generated data is used for learning of the forward and inverse models. The FM is implemented by a feed-forward neural network that learns the mapping\nFM: [s(t), a(t)] \u2192 \u015d(t+1), (1)"}, {"title": "3.3 Knowledge Extraction", "content": "Trained FM can be analyzed by extracting information about the original environment and a learning session. Our primary focus is on analyzing feature importance, which allows us to highlight state features that cannot be manipulated by the agent actions and thus can be removed, hence reducing the dimensionality of the state space for the specific task and environment. Recent related work by Lee et al. [10], which served as an inspiration, focused on determining relevant state features by conducting an intervention on one feature at a time and testing whether the same policy execution led to successful task completion or not. This way, causal dependencies were found.\nOn the contrary, we do not study causality by direct interaction with an environment but by using trained causal models as proxies containing this information. Using the learned FM, we can determine the relevance of state features in relation to action features by analyzing their importance.\nExplaining the behaviour of trained deep neural networks is hard since these networks derive their decisions using a large number of elementary operations. Various approaches have been proposed [5], and one category that we focus on here is based on saliency mapping. Model predictions are typically based on attributions to several features (inputs). SHAP framework [13] unifies different additive feature attribution methods. Here, we were experimenting with Kernel SHAP and Deep SHAP. While Kernel SHAP is model-agnostic and utilizes Linear LIME [17], Deep SHAP is applicable only to neural models and uses the attribution rules of the DeepLIFT method [20]. Both methods try to approximate the Shapley value of each input feature in relation to an output feature. Shapley value represents an input feature contribution to the output feature prediction. In the context of our work, we can use Shapley values to determine the contribution of each action variable to each state variable for a specific prediction.\nIn our experiments (Section 4.2), we use the Deep SHAP method only, as the Kernel SHAP method is significantly slower due to it making no assumptions about the analyzed model. While the SHAP methods are local, providing an explanation for one prediction, thanks to their properties, these local explanations can be aggregated across the set of instances, providing us with global feature importance within the analyzed model, visualizable using heat maps (e.g., Figure 4).\nAdditionally, we can use partial dependence plots (PDPs) [3] to visualize the distribution of the contribution of an action feature to a state feature across the set of instances (e.g., Figure 5). From PDPs, we can determine whether there is any relationship between the selected action and the state features and properties of this relationship. High correlation indicates a strong impact of the action feature on the state feature (however, such a relationship should not always be regarded as causal [2])."}, {"title": "4 Experiments", "content": "We applied the described methods in two experiments related to categories C1 and C2 of learning causal relations [7]. The structure of both experiments is similar. The first step consists of data generation in a simulated robotic environment (Section 3.1) provided by the myGym toolkit [21]. The generated data is subsequently used for training forward and inverse models (Section 3.2), which are further analyzed to reveal causal relations."}, {"title": "4.1 Learning Kinematics", "content": "In Experiment 1, which focused on sensorimotor learning, we used KUKA LBR iiwa robotic arm with a magnetic endpoint and 7 DoF that performed motor babbling.\nEnvironment The simulation ran in 500,000 steps. In each step t, a joint configuration \u03b8(t) \u2208 R7 is sampled from the normal distribution with limits according to Table 1. The magnetic endpoint is not used during this experiment. A motor command is executed in 10 substeps before proceeding to the next simulation step, allowing a longer execution time, resulting in the actual action being more similar to the planned one. After the action execution, only the resulting configuration and Cartesian effector position ef (t) = [efx, ef y, ef z] is recorded, both composing state vector s(t) = [\u03b8(t), ef (t)].\nModels The FM for this experiment uses two separate output heads, one for joint configuration prediction and the other for effector position prediction. Each head computes a separate mean squared error used as a loss. The model is trained according to the overall loss calculated as a sum of equally weighted head losses. For training the FM, we used Adam optimizer [8] with an initial learning rate \u03b7 = 10-3 for 60 epochs. The model was evaluated using 5-fold cross-validation with average mean absolute error (MAE) for the effector position and joint configuration outputs of 6.9 mm and 3.4 \u00d7 10-3 rad, respectively.\nFor the IM, we tried both architectural approaches proposed in Section 3.2. All models of both approaches use mean squared error (MSE) as a loss function. The pre-computation approach uses a base IM trained first. We experimented with two variants of this model, differing in the unit's activation function at the hidden layer: hyperbolic tangent or ReLU. However, the differences in resulting performance were insignificant."}, {"title": "Mental simulation", "content": "We subjected the FM to the chained inference, which consisted of repeatedly querying the model on the previously generated state and random action. Provided the initial state s(0) = \u015d(0) and the action sequence \u03a4\u03b1 = [\u03b1(0), \u03b1(1), ..., a(T)] are available, we can inductively predict a successor state from the previous state as \u015d(t) = FM [\u015d(t \u2212 1), a(t \u2212 1)]. This way, we can predict the state of the environment T steps ahead.\nAs part of Experiment 1, we evaluated the capacity of the trained FM to predict the state up to 10 steps ahead. First, we generated 6,000 random ground-truth trajectories in the simulation. Then, the first state s(0) and the action sequence of each trajectory are used in generating \u015d(t), with 1 \u2264 t \u2264 T. Each predicted state s(t) is then compared with its ground-truth counterpart s(t), and the MAE of the prediction is calculated for joint configuration and effector position subvectors separately. The evaluation results can be seen in Figure 3. The joint configuration prediction error exhibits linear growth with the number of steps ahead, while the effector position error grows in an approximate semi-logarithmic log-linear trend. The result is an improvement over our initial expectation of both errors growing exponentially."}, {"title": "4.2 Simple Intuitive Physics", "content": "This experiment has the purpose of evaluating the proposed methods on the category 2 causal skill proposed by Hellstr\u00f6m [7], described as \"Learning about how the robot affects the world\".\nEnvironment The experiment utilizes the KUKA LBR iiwa robotic arm with a magnetic endpoint as an agent, same as in Experiment 1. The task consisted of the arm randomly switching the magnet. If it was not holding anything, the arm navigated to the magnetized cube lying on the table, picked it up, and randomly manoeuvred with it in the space for a random duration. After that, the magnet was turned off, the cube was released, and the arm babbled empty-handed for a random duration.\nThe goal of this experiment was to let the agent learn the simple physics of the cube as well as its kinematics. Knowledge gained by the agent in this experiment can be thus understood as a superset of knowledge from the kinematics experiment (Section 4.1). Additionally, we wanted to verify whether the model architectures specified in Section 3.2 would efficiently work with state spaces of higher dimensionality.\nThe data-generating simulation ran in 4,000 episodes, lasting 500 iterations each. After each iteration, we recorded the final joint configuration \u03b8(t) \u2208 R7, effector position and rotation as a 6D pose ef (t) = [ef x, ef y, ef z, ef rx, ef ry ef rz], object information (its position, rotation and color) o(t) = [Ox, Oy, Oz, Orx, Ory, Orz, OR, OG, OB], and the magnetic endpoint state mgt(t). The full state vector is composed as s(t) = [o(t), \u03b8(t), ef (t), mgt(t)]. Object colour features were added as control variables, randomized at the start of each episode, and did not change during it."}, {"title": "Models", "content": "Same as in the previous experiment, we trained an FM and a monolithic IM on the generated data. The FM uses separate output heads for object position, object rotation, colour, joint configuration, effector position and rotation prediction. Each head computes a separate MSE, which is used as a loss. The FM was trained for 100 epochs using Adam optimizer [8], with the initial learning rate \u03b7 = 10-3. For the final results of this model, see Table 3. With regard to the previous experiment, we can compare effector position and joint configuration prediction errors. While the effector position error in this experiment is slightly higher, the joint configuration error is ca. 2.5 times worse."}, {"title": "Knowledge extraction", "content": "The trained forward model is further analyzed using methods proposed in Section 3.3. The analysis was performed using a sample of 200 observations from the generated dataset.\nThe resulting global contribution heat map generated for this experiment is shown in Figure 4. The y-axis denotes the action of each joint and a magnetic endpoint of the arm. The x-axis contains defined environment state features. The colour of each cell corresponds to the magnitude of contributions of action features to the state features averaged across the selected sample.\nThe figure shows, for instance, that joint 6 is not used during the simulation as its action (a6) does not correlate with the change of any state variable and, most importantly, with the change of the state of the joint itself. In addition, the colour of the object is irrelevant in this case, as no action can affect it and thus could be removed (or ignored) from the state space. On the other hand, all action features, except a6, affect most object features. This low-level knowledge can be useful for causal analysis at higher levels.\nFeature importance and dependencies can also be studied using feature contribution distributions and partial dependence plots. Figure 5 shows a sample of PDPs generated from feature contribution data output by Deep SHAP method"}, {"title": "5 Conclusion", "content": "In this paper, we explored causal relations by learning the forward and inverse models on synthetic data generated in simulation. We confirmed that the forward model constructed using the proposed approach could be successfully used for mental simulation, possibly helping with action planning by predicting future states based on causality observed in the data. Additionally, we explored approaches to inverse model construction, allowing the model to predict the action needed for a transition between subsequent states.\nMoreover, we demonstrated the capability of extracting knowledge about the environment's behaviour from the trained forward model using explainability methods. We proposed that information obtained this way can be used to determine relevant state features, serving as a basis for dimensional reduction."}]}