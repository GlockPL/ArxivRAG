{"title": "Origin-Destination Demand Prediction: An Urban Radiation and Attraction Perspective", "authors": ["Xuan Ma", "Zepeng Bao", "Ming Zhong", "Yuanyuan Zhu", "Chenliang Li", "Jiawei Jiang", "Qing Li", "Tieyun Qian"], "abstract": "In recent years, origin-destination (OD) demand prediction has gained significant attention for its profound implications in urban development. Existing deep learning methods primarily focus on the spatial or temporal dependency between regions yet neglecting regions' fundamental functional difference. Though physical methods have characterised regions' functions by their radiation and attraction capacities, these functions are defined on numerical factors like population without considering regions' intrinsic nominal attributes, e.g., a region is a residential or industrial district. Moreover, the complicated relationships between two types of capacities, e.g., the radiation capacity of a residential district in the morning will be transformed into the attraction capacity in the evening, are totally missing from physical methods.\nIn this paper, we not only generalize the physical radiation and attraction capacities into the deep learning framework with the extended capability to fulfil regions' functions, but also present a new model that captures the relationships between two types of capacities. Specifically, we first model regions' radiation and attraction capacities using a bilateral branch network, each equipped with regions' attribute representations. We then describe the transformation relationship of different capacities within the same region using a parameter generation method. We finally unveil the competition relationship of different regions with the same attraction capacity through adversarial learning. Extensive experiments on two city datasets demonstrate the consistent improvements of our method over the state-of-the-art baselines, as well as the good explainability of regions' functions using their nominal attributes.", "sections": [{"title": "I. INTRODUCTION", "content": "With the spread of ride-hailing platforms like Uber and Didi, intelligent transportation systems have emerged as a vibrant research domain [1]\u2013[3]. These systems are designed to offer convenient ride services, improve public transportation efficiency through proactive order assignment, and optimize profitability by identifying high-profit routes based on historical passenger demands [4].\nAmong the wide spectrum of applications, traffic demand forecasting is the focal point due to its vital role in urban development, traffic control, and route planning [5]\u2013[11]. The conventional task in this field involves the prediction of the potential number of passenger demands in a specific region [10], [12], [13]. However, such a task is unable to capture associations in inter-regional flows. Consequently, there has been a growing interest in recent years in predicting origin-destination (OD) demand, which reflects the intensity of passenger demands and serves as the basis for extracting valuable mobility patterns and identifying critical transportation routes.\nIn computer science, current research mainly involves data-driven deep learning methods, which often leverage the powerful convolutional neural networks and graph neural networks [4], [9], [14]\u2013[22]. These methods fit arbitrary functions representing dependencies between variables, enabling them to directly learn the distribution of flow transfers between regions from massive data, and thus significantly improve the accuracy of model predictions. Despite their remarkable advance, the fundamental functional difference of regions has been overlooked by current deep learning methods. As depicted in Fig. 1, the outflow and inflow traffic demands in Manhattan region exhibit distinctive patterns throughout the day, showing the functional difference when the region serves as the origin or destination.\nIn the field of physics, the research in OD demand predic-tion [23]\u2013[28] have characterized regions' functions by radiation capacity [27] and attraction capacity [29]. Specifically, when a region serves as the origin, its outflow traffic demand is determined by the factors reflecting its radiation capacity, e.g., population. When a region serves as the destination, the inflow traffic demand is associated with the factors reflecting its attraction capacity, e.g., facilities and services.\nMany physical methods have been proposed to model these two types of capacities for population movement prediction, e.g., the radiation model [27], the cost-based radiation model [26], the flow and jump model [23]. However, these models are normally expressed in the form of equations of state for calculation. As a result, they can only model the impact of numerical factors, e.g., population, which prevents them from fully capturing the flow transfer patterns that are closely related to regions' intrinsic attributes (often nominal). For example, people in a residential district often go out for work or study in the morning, showing a radiation capacity.\nMore importantly, there are complicated relationships between two types of capacities. For example, in Fig. 1, during the early peak hours, Manhattan is predominated by the radiation demand, while in the late afternoon it tends to attract traffic flows. This infers that the radiation and attraction capacities can be mutually influenced with each other, and understanding their interplay is crucial for the OD demand prediction. However, such important relationships have not been investigated yet. Hence we pose two research questions.\nRQ1: How can we wrap the physical concepts of radiation and attraction capacities into deep learning and further extend them to reflect regions' nominal attributes?\nRQ2: How can we leverage the complicated relationships between these two types of capacities to get a better solution?\nTo answer these questions, we propose a deep learning framework from the radiation and attraction perspective for OD demand prediction. For RQ1, we present a bilateral branch network to learn representations of radiation and attraction capacities, each equipped with attribute embeddings to exploit regions' respective functions. For RQ2, we present a novel model that captures the relationships between two types of capacities based on the following deep analyses. Firstly, a region's radiation and attraction capacities undergo dynamic transformations over time [30]. For example, inhabitants often go out from their apartments in the morning and back in the late afternoon, indicating that there exists a time-sensitive transformation relationship between two types of capacities of the residential area. Moreover, a region often has multiple attributes, e.g., residential and restaurant. Motivated as such, we first construct the attribute hypergraph, and then propose a hypergraph-based parameter generation method to encode such attribute-determined and time-sensitive transformation relationship into two types of capacities.\nSecondly, a competition relationship exists among regions with the same type of functions [31]. For example, after working hours, the passengers may depart from their workplace and head to regions where restaurants or recreation facilities are located. In this scenario, the attributes of the destination regions, sometimes along with their distances, are important factors influencing the strength of competition. Hence, we first propose to group the regions with the same attraction capacity into the region clusters, and then present two cluster-based adversarial learning strategies to reveal the competition relationship between neighboring regions in the clusters."}, {"title": "V", "content": "The main contributions of our work are as follows.\n\u2022 We introduce the physical concepts of radiation and attraction capacities into deep learning for the first time, and further generalize them: 1) to model the regions' nominal attributes which are critical for identifying implicit flow transfer patterns, 2) to capture the transformation and competition relationships between two types of capacities.\n\u2022 We design a bilateral branch network with the embedded regions' nominal attributes, which enables us to independently characterize the radiation/attraction capacity in each of its branch.\n\u2022 We present a hypergraph-based parameter generation approach and two clustered-based adversarial learning strategies to capture the transformation and competition relationships between two types of capacities.\n\u2022 Extensive experimental results demonstrate the superiority of our method, e.g., a 10.17% and 13.02% MAE improvement on New York and Chicago datasets against the best baselines."}, {"title": "II. PRELIMINARIES", "content": "Given a spatio-temporal graph $G = (\\mathcal{V}, \\mathcal{E}, \\mathcal{X})$, where $\\mathcal{V} = {v_1, v_2, ..., v_N}$ is the set of $N$ nodes, and $\\mathcal{E} = {e_{1,2},...,e_m}$ is the set of $M$ edges. $\\mathcal{X}$ is the node feature (attribute) set. The nodes in $\\mathcal{V}$ are regions that have been partitioned according to specific rules, e.g., census tract, each with one or multiple attributes. Each edge $e = (o, d, t_e)$ signifies one transaction that a passenger departs from the origin $o$ to the destination $d$ at the time $t_e$. Moreover, the dynamic graph at a specific time point $t_i$, denoted as $G_{t_i} = (\\mathcal{V}, {e | t_e < t_i}, \\mathcal{X})$, encompasses all flows that occurred before the time $t_i$.\nDefinition 1 (Origin-Destination Demand Matrix). The origin-destination demand matrix serves as a concise representation of the spatio-temporal graph, capturing the traffic demand between each pair of nodes within a specified time period. Formally, the OD demand matrix for the time interval from $t$ to $t + \\tau$ is denoted as $Y_{t:t+\\tau} \\in \\mathbb{R}^{N \\times N}$. In this matrix, the (o, d)-entry signifies the traffic demand from the origin node o to the destination node d during this time interval.\nDefinition 2 (Origin-Destination Demand Matrix Predic-tion). Given a sequence of observed OD demand matrices ${Y_1, Y_2, ..., Y_t}$ and a set of auxiliary features $\\mathcal{X}$ for each region, the OD demand matrix prediction aims at forecasting the OD Demand Matrix $\\hat{Y}_{t:t+\\tau}$ during the next time interval from $t$ to $t+\\tau$."}, {"title": "III. PROPOSED MODEL", "content": "This section presents our RACTC framework, which models radiation and attraction capacities and captures their transformation and competition relationships. Taking the origin-destination demand matrix as input, the core idea of our method is to first associate the urban radiation capability with the origin region and the attraction capability with the destination region. We then obtain the origin and destination representations by learning these two types of capabilities as well as the transformation and competition relationships, and enhance these capabilities using corresponding attributes. The notations in this paper are summarized in Table I. The overall architecture is depicted in Fig. 2. It consists of six components: origin-destination demand matrix pre-processing, transformation relationship learning, radiation capacity learning, attraction capacity learning, population enhanced radiation capacity learning, and competition relationship learning."}, {"title": "A. Origin-Destination Demand Matrix Pre-processing", "content": "Destinations sharing the same origin at a specific time often have strong correlations, and the same goes for origins with the same destination. For instance, it is typical for commuters from the same residential area to various office regions for work in the morning. Likewise, individuals from various office regions may converge at a specific restaurants around noon.\nIn order to encode the relationship between destinations sharing the same origin and the that between origins with the same destination, we need to compute the similarity between origins and that between destinations. The input OD demand matrix only records traffic movements and thus cannot reflect such relationships. However, by multiplying the OD demand matrix and its transpose (DO matrix), we can obtain origin-origin (OO) relationships through shared destinations. Similarly, by multiplying the DO matrix and the OD demand matrix, we can obtain destination-destination (DD) relationships through shared origins.\nFormally, we denote the OO and DD relationship matrix at the time step t as $Y_t^o$ and $Y_t^d$, respectively. The pre-processing is conducted using the following formula:\n$Y^o = (Y_tY_t^T) \\odot (\\mathbb{1} - I), Y^d = (Y_t^TY_t) \\odot (\\mathbb{1} - I),$\nwhere $I$ is the identity matrix for self-connections. $Y_t$ denotes the OD demand matrix at the time step t, and $Y^T$ (the transpose of $Y_t$) is the DO demand matrix at the time step t. Then we normalize these two matrices to avoid the scale of embeddings increasing with the graph convolution operations:\n$\\overline{Y_o} = D_o^{-1/2} \\cdot Y_o \\cdot D_o^{-1/2} + I, \\overline{Y_d} = D_d^{-1/2} \\cdot Y_d \\cdot D_d^{-1/2} + I,$\nwhere $D_o$ and $D_d$ are degree matrices to ensure symmetry. The pre-processing procedure captures the spatiotemporal dependencies of regions as origins and destinations, respectively, which will facilitate the subsequent learning of origin and destination region embeddings in a bilateral branch network."}, {"title": "B. Transformation Relationship Learning", "content": "Urban regions consist of diverse points of interest (POIs) with specific nominal attributes. During different time peri-ods, people's activities can be influenced by the functions of POIs in the city. For example, there are many theatres in Broadway, New York. Residents often head to theatres (theatre is one of the region's nominal attributes) at about 7:00 p.m., i.e., the region serves as a destination and shows the attraction capacity with an attribute-determined and time-sensitive property. The audience then go home after 9:00 p.m., i.e., the region serves as an origin at that time and revealing its radiation capacity, which is transformed from its initial attraction capacity. Therefore, the transformation relationship between the regions' radiation and attraction capacities is interconnected through their time-sensitive nominal attributes.\nIn light of this, we first construct an attribute hyper-graph and integrate temporal information to unveil the attribute-determined time-sensitive transformation rela-tionship. Then, we propose a parameter generation method to generate two attribute-determined and time-sensitive transformation matrices. These matrices are subsequently utilized for feature extraction in radiation and attraction capacity learning, thereby modeling the relationship between them and enabling each capacity to focus on these time-sensitive nominal at-tributes.\n1) Mining Attribute-Determined Time-Sensitive Properties: The relationships between regions and attributes are intricate since there are multiple attributes in each region and certain attributes are shared by different regions. Hence it is required to build suitable connections and leverage mutual interactions between attributes in a many-to-many format. Traditional graph structures are inadequate for this task because they can only model pairwise relations. To overcome this limitation, we construct an attribute hypergraph to model such many-to-many interactions which encompasses the connections of multiple attributes of the region through a single hyperedge. The hypergraph is defined as $G_h = (V_h, E_h)$, where $V_h$ is a set of vertices containing $M$ unique attributes, and $E_h$ is a set of hyperedges containing $N$ hyperedges. Each hyperedge $e \\in E_h$ contains multiple vertices and is used to connect attributes within the region.\nNext, we introduce the hypergraph convolution opera-tion to capture interactions among attributes and derive high-quality representations for attributes. For a specific at-tribute node $v \\in V_h$, the hypergraph convolutional network learns representations for all connected hyperedges $e \\in E_h$. Unlike simple graphs using the adjacency matrix to illustrate relationships between connected nodes, the hypergraph employs the incidence matrix $H$ to build connections between nodes and hyperedges. The entries in $H$ are defined as: $h_{ve}$ = 1 if the hyperedge $e$ contains the vertex $v$; and $h_{ve}$ = 0 otherwise. Fig. 3 shows an example of attribute hypergraph and its incidence matrix. The hypergraph convolution is de-fined as:\n$A^{l+1} = D_h^{-1}B_h^{-1}HA^lW_h,$\nwhere $D_h$ represents the vertex degree matrix of the hyper-graph. $B_h$ denotes the hyperedge degree matrix. $W_h$ serves as the parameter matrix connecting two convolutional layers. $A^l$ signifies the attribute embeddings in the l-th hypergraph convolutional network.\nTo capture precise temporal information at the time step t, we define $E(t) = E(Bucketize(t)) = e_t$, where $Bucketize(\\cdot)$ discretizes time into 24-hour intervals. The resulting $e_t \\in \\mathbb{R}^S$ represents a S-dimensional time embedding vector. We then simply apply the dot product attention mechanism to extract key time-sensitive attributes $t_a$ in the city. In this way, the attention weights are determined proportionally to the dot products between the embeddings of each attribute and the time embedding, denoted as:\n$t_a = \\sum_{i=1}^M \\alpha_i a_i, \\quad \\alpha_i = \\frac{exp(a_i \\cdot e_t)}{\\sum_{i=1}^M exp(a_i \\cdot e_t)},$\nwhere $a_i \\in A$ is the embedding of the i-th nominal attribute after convolution and $\\alpha$ is the attention matrix to select time-sensitive attributes critical for subsequent transformation matrix generation.\n2) Hypergraph-based Parameter Generation: After identi-fying crucial attribute-determined and time-sensitive properties based on the hypergraph, we proceed to design a parameter generation module to obtain two attribute-determined and time-sensitive transformation matrices for subsequent radiation and attraction capacity learning: $W_o^\\prime$ for the radiation branch and $W_d^\\prime$ for the attraction branch. Taking the transformation matrix $W_o^\\prime$ as an example, we generate it using the time-sensitive attributes $t_a$ as the input through the following steps.\n(1) We conduct a linear transformation through $F_{W_o}: \\mathbb{R}^S \\rightarrow \\mathbb{R}^{S \\times S}$, and then perform a dimension transformation $\\mathbb{R}^{S \\times S} \\rightarrow \\mathbb{R}^{S_{out} \\times S}$.\n(2) We perform the second linear transformation $F_{W_o}: \\mathbb{R}^{S \\times S} \\rightarrow \\mathbb{R}^{S \\times S_{out}}$ and get a parameter matrix $W_o$.\n(3) We introduce a gate mechanism to regulate information flow and selectively extract crucial details from both the randomly initialized parameters $W_o$ and the generated parameters $W_o$ to form the final transformation matrix $W_o^\\prime$, where we use the bit-level weights $\\beta$ generated by the time-sensitive attributes $t_a$ for aggregation. The process is as follows:\n$\\beta = \\sigma(Reshape(W_t t_a+b_t)+b_t)),$\n$W_o^\\prime = \\beta \\odot W_o + (\\mathbb{1} - \\beta) \\odot W_o,$\nwhere $W_t \\in \\mathbb{R}^{S_{out} \\times S}$ ($S_{out}$ is the dimensionality for the hidden layer) and $b_t \\in \\mathbb{R}^{S_{out}}$ are weight matrices and biases, respectively. The operation $Reshape$ refers to the dimension change $\\mathbb{R}^{S_{out} \\times S} \\rightarrow \\mathbb{R}^{S_{out} \\times S}$.\nSimilarly, we can generate the attribute-determined and time-sensitive transformation matrix $W_d^\\prime$ for the attraction branch. This process follows the same steps as described above, where the only difference is the use of weight matrices specific to the attraction branch during the training phase."}, {"title": "C. Radiation / Attraction Capacity Learning", "content": "To model the differences in regional functions at a fine-grained level, we introduce a bilateral branch network to separately learn the origin (corresponding to radiation capacity) and the destination (corresponding to attraction capacity) region embeddings. Note two branches in this network have the same structures but differ in their inputs, parameters, and learned embeddings. In the following, we begin by outlining the basic process for learning origin and destination region embeddings in two branches. We then enhance these embeddings by integrating the previously learned transformation relationship between two types of capacities.\n1) Basic Embedding Learning: We set up two lookup tables to transform the ID number of each region into two low-dimensional vectors. After transformation, these two vectors represent the region r's radiation/attraction capacity as an origin/destination, denoted as $o_r/d_r \\in \\mathbb{R}^S$, where $S$ is the embedding size.\n2) Transformation-aware Embedding Learning: During the transformation relationship learning process, we have in-corporated attribute-determined time-sensitive properties into two transformation matrices $W_o^\\prime$ and $W_d^\\prime$. In this subsection, we further exploit these transformation matrices for regions' property extraction during the respective graph convolution processes for two capacities within the bilateral branch network.\nFor radiation capacity learning, we utilize the decomposed origin-origin matrix as the graph structure and basic origin embeddings as node features. We then conduct convolutional operations by broadcasting and aggregating embeddings along the edges of the graph to update origin embeddings, where the transformation matrix $W_o^\\prime$ is employed for extracting origins' attribute-determined time-sensitive property. For neighbor aggregation, we employ sum pooling combined with a feature transform matrix and a nonlinear activation function. For attraction capacity learning, we input the decomposed destination-destination matrix as graph structure and basic destination embeddings as node features into another graph convolutional network to update destination embeddings. The convolutional and neighbor aggregation operations are similar with those for radiation capacity learning. The only difference is that the transformation matrix $W_d^\\prime$ is employed for extracting destinations' attribute-determined time-sensitive property.\nFormally, given the origin embedding at the l-th layer and time step t as $O_i^l$, and the destination embedding at the l-th layer and time step t as $D_i^l$, the update rules are as follows:\n$O_i^{l+1} = \\sigma(\\overline{Y_o} \\cdot O_i^l \\cdot W_o^\\prime), D_i^{l+1} = \\sigma(\\overline{Y_d} \\cdot D_i^l \\cdot W_d^\\prime),$\nwhere $W_o^\\prime$ and $W_d^\\prime$ are the learned transformation matrices responsible for updating the origin and destination embedding matrices, respectively.\nFinally, we introduce the LSTM cells to capture temporal correlations within the sequences of transformation-aware ori-gin and destination embeddings expressed as follows:\n$O^{l+1} = LSTM(O^{l+1}), D^{l+1} = LSTM(D^{l+1}).$"}, {"title": "D. Competition Relationship Learning", "content": "After learning the transformation-aware origin (attraction) and destination (radiation) capacity representations, we proceed to learn the competition relationship between two types of capacities.\nTraditional approaches to traffic demand prediction commonly have a single objective, which emphasizes the similarity in traffic demand transfer patterns across regions. However, there exist the competition relationships among regions with the same attribute. For example, when employees from the same office building go out for dining, each of them can only go to one restaurant at the same time. This not only cultivates the shared preference for specific restaurant regions but also inherently introduces competition among destination regions with the same function. Moreover, from a specific origin, the closely located destinations of the same attribute are more likely to form the competition relationship. Hence we define two types of competition relationships. One is for destination regions with the same attribute and exhibiting similar functions. The other is for origin-destination region pairs which have the same origin and arrive at different destinations with similar functions.\nTo model the above competition relationships, we first group the above two types of regions using two separate clustering methods. We then propose the cluster-wise and the edge-wise auxiliary tasks for neighboring regions in the same cluster, and introduce the gradient reversal layer to create an adversarial learning between the auxiliary task and the original prediction task to encode competition relationships.\nPlease note that the cluster-wise task is presented for the first type of competition by maximizing the similarity of destination embeddings within the same cluster. Meanwhile, the edge-wise task is presented for the second type of com-petition by maximizing the similarity of edge representations (these edges are from same origin to different destinations), such that we can capture the relationships of edges with the same origin (e.g., an office building) but different destinations with same attributes (e.g., restaurants). Through the cluster-wise and edge-wise auxiliary tasks, the similarity of regions with the same attribute and that of edges for same origin to different destinations of the same attribute increases, thus enhancing both types of competition relationships among regions with similar functions.\n1) Cluster-wise Auxiliary Task: The destination regions with similar functions often compete with each other. Hence we first identify competitive regions by clustering regions based on their attributes. Note the regions and their attributes are fixed, thus the clustering is conducted only once as a preprocessing procedure.\nWe first compute the Euclidean distance as a metric of similarity for the k-Means algorithm [32] to perform clustering on regions. In the implementation, after obtaining attribute-based representations for all regions and setting the value k to $k_2$, we simply employ the KMEANS function from the scikit-learn library 1 to get the clustering of regions.\nWe then assign an ID from 0 to $k_2$ \u2212 1 to each cluster. Each region now has an attribute-aware label, i.e., the cluster ID to which the region belongs. Subsequently, we employ a fully connected layer as the destination classifier and optimize it using cross-entropy loss. The goal for this classifier is to pre-dict regions' cluster ID using destination embeddings of these regions. In this way, the destination embeddings for regions with the same attribute(s) become more similar, and thus strengthening the first type of competition relationship.\nTaking a region i as an example, the loss function for the region classification task is formulated as follows:\n$\\hat{c} = W_n d_i, L_{cluster} = -\\hat{c}[c] + log(\\sum_j exp(\\hat{c}[j])),$\nwhere $d_i$ denotes the destination region's transformation-aware embedding, and $c$ represents its attribute-aware label.\n2) Edge-wise Auxiliary Task: For a fixed origin, the compe-tition relationship between destination regions with the same attribute and short distance is more significant. To capture this second type of competition relationship, we input both the region attribute matrix and the region distance matrix into the k-Means algorithm. This process allows us to obtain region clusters, which subsequently serve as soft labels for competition relationships. This clustering procedure is also conducted only once.\nWe first compute the distances between regions using the Haversine formula to form a region distance matrix. All regions and their associated attributes form the region attribute matrix.\nWe then concatenate the region attribute matrix and the region distance matrix, and also compute the Euclidean distance as a measure of similarity for the k-Means algorithm to perform clustering. In the implementation, after concatenating the region attribute matrix and the region distance matrix as input and setting the value of k to $k_2$, we directly call the KMEANS function from the scikit-learn library to perform the calculation, resulting in the clustering of regions.\nThis clustering process generates the attribute-aware distance-sensitive label matrix. In this matrix, any two regions belonging to the same cluster are assigned an entry of 1 indicating competition between these regions, and 0 otherwise. Our objective is to encourage the edge representations for origin-destination region pairs with the same origin and similar destination functions to be as similar as possible. By doing this, we wish to strengthen the second type of competition relationships.\nMoreover, the total outflow traffic from an origin will not exceed the region's total population. Therefore, we further incorporate population information from the origin regions to reinforce the constraints on the competition relationships between destination regions. Population-based region repre-sentations $G$ serve as the global representation, while origin-destination pairs associated with the specific origin serve as the local representation. By ensuring the consistency between the local and global representations, we build the correlation between traffic distribution and population scale, thereby achieving a more comprehensive understanding and prediction of the flow between origins and destinations.\nFormally, we use the transformation-aware origin and des-tination embeddings ($o_i$ and $d_j$) to obtain the edge represen-tation $e_{ij}$ between the origin $i$ and the destination $j$:\n$e_{ij} = [\\sigma(o_i), \\sigma(d_j)],$\nwhere $[...]$ denotes the concatenation operation.\nFor an origin region $i$, its global representation is denoted as $g_i \\in G$. For each edge $e_{ij}$ departing from the origin region $i$, we use a discriminator $D_1$ to compute the score for each local-global representation pair using a bilinear mapping function, as follows:\n$D_1(e_{ij}, g_i) = e_{ij}^T W_{eg} g_i,$\nwhere $W_{eg} \\in \\mathbb{R}^{2S \\times S}$ represents the weight matrix.\nThen, for each edge $e_{ij}$, with the origin $i$ fixed, we randomly select a destination $p$ with the entry $(j, p) = 1$ from the attribute-aware distance-sensitive label matrix. Subsequently, we compute the corresponding local-global representation pairs $D_1(e_{ij}, g_i)$ and $D_1(e_{ip}, g_i)$, aligning similar represen-tations to improve the proximity of edge representations. The detailed process is outlined below:\n$L_{edge} = \\sum_{(i,j,p)} D_2(D_1(e_{ij}, g_i), D_1(e_{ip}, g_i)),$\nwhere $D_2(\\cdot, \\cdot)$ represents a similarity metric between two probability distributions, e.g., mean squared error loss.\n3) Adversarial Learning: To maximize the difference be-tween cluster-wise and edge-wise region representations, we utilize an adversarial learning strategy to ensure that the learned representations not only maintain similarity between neighbors in the graph but also discern competition relation-ships. Specifically, we incorporate a Gradient Reversal Layer (GRL) [33] into the back-propagation process. The GRL is positioned between the learned transformation-aware region embeddings from bilateral branch network and the cluster-wise or edge-wise auxiliary task module. In the auxiliary tasks, our objective is to minimize $L_{cluster}$ or $L_{edge}$, respectively. After adding the GRL layer, it becomes to maximize $L_{cluster}$ or $L_{edge}$. As a result, the auxiliary tasks and the OD demand matrix prediction task establish an adversarial relationship to update region representations, shaping transformation-aware and competition-aware origin embeddings (O') and destination embeddings (D')."}, {"title": "E. Population Enhanced Radiation Capacity Learning", "content": "In physics, the radiation capacity of a region is mainly modeled by its population [27], as the increase in population often results in a higher outflow from that region. In view of this, we propose to integrate the population factor into our deep learning framework to better capture the dynamics of flow patterns and more accurately predict the volume of outflow traffic from various regions. Below is the detail of this module.\nFirstly, we discretize regions' populations into distinct levels to facilitate the comparisons based on relative population scales. Our analysis shows that the population distribution conforms to a logistic distribution rather than the uniform distribution. This implies that the probability density function exhibits a high preference in the middle and low preference on both sides. The regions where the population is concentrated should higher population sensitivity, and thus their population levels should be divided at a fine granularity, and vice versa. Moreover, to obtain balanced training data, the number of regions contained in each population level should be on the same scale. Hence, we discretize populations into $k_1$ levels (e.g., $k_1$ = 5), where the probability for each interval is equal.\nFormally, for a region r with a population size $p_r$ and the population size range [MIN, MAX], we determine its population level using the following formula:\n$l_r = \\lfloor \\frac{\\Phi(p_r) - \\Phi(MIN)}{\\Phi(MAX) - \\Phi(MIN)} \\times k_1 \\rfloor,$\nwhere $\\Phi(p)$ is the cumulative distribution function of logistic distribution, which can be defined as:\n$\\Phi(p) = P(X \\leq p) = \\int_{- \\infty}^p \\frac{1}{1 + e^{-\\frac{\\sqrt{3} \\pi}{\\sigma}(t-\\mu) }} dt,$\nwhere $\\mu$ and $\\sigma$ are the expected value and standard deviation. After determining the population level, the regions at the same level will exhibit similar radiation capacity, hence we employ a population similarity matrix $L$ to encode such similarity, where $L_{ij}$ = 1 denotes that the region $i$ and the region $j$ have the similar radiation capacity, and $L_{ij}$ = 0 is the opposite.\nSecondly, we perform the population-based region represen-tation learning using graph neural networks. Specifically, we use the population-based region embeddings G as the node features and the population similarity matrix $L$ as the graph structure. The learning process is as follows:\n$G^{l+1} = \\sigma(D_g L D_g G^l W_g),$\nwhere $D_g$ represents the degree matrix. $W_g$ serves as the parameter matrix connecting two convolutional layers, and $G^l$ signifies the population-based region embeddings in the l-th graph convolutional network."}, {"title": "F. OD Demand Prediction", "content": "To enhance the population characteristics of the origin regions", "follows": "n$O_p = PRELU(W_g G^{l+1"}, "b_g) \\odot O^{l+1},$\nwhere $W_g \\in \\mathbb{R}^{S \\times S}$ and $b_g \\in \\mathbb{R}^S$ are learnable parameters. Here, $PRELU(\\cdot)$ denotes the Parametric ReLU activation function. Since $O^{l+1}$ contains all the information from the population feature layer, it can represent the population distribution within a specific region, thus serving as the population-enhanced origin region representation.\nThe final prediction is conducted based on the origin and destination representations. The destination embeddings $D^\\prime$ are directly taken from the competition relationship learning module. As for origin embeddings, we fuse the original em-beddings $O^\\prime$ with the population-enhanced ones $O_p$. Specifically, we first compute the bit-level fusion weight using the region representation $G^{l+1}$ to select important features from both the original and enhanced embeddings:\n$\\theta = \\sigma(W_o G^{l+1} + b_o),$\nwhere $W_o \\in \\mathbb{R}^{1 \\times S}$ and $b_o \\in \\mathbb{R}^S$ are learnable parameters. We then employ this weight to fuse the origin embeddings:\n$O^{''} = \\theta \\cdot O_p + (1 - \\theta) \\cdot O^{'},$\nwhere $O^{''}$ denotes the transformation-aware, competition-aware, and population-aware origin region representations.\nFinally, we utilize a MLP layer to compute the matching scores between the above final origin region embeddings $O^{''}$ and the transformation-aware and competition-aware destina-tion embeddings $D^\\prime$. For a demand from origin i to destination j, the prediction $\\hat{Y}_{i,j}$ at time step t+1 is computed as follows:\n$\\hat{Y}_{i,j} = MLP(o_i^{''}d_j) + MLP(o_i^{''}) + MLP(d_j).$\nAfter getting the prediction labels $\\hat{Y}$, we use the mean squared error to compute the loss function for the main prediction task:\n$L_{OD} = \\frac{1}{N \\times N} \\sum_{i=1}^N \\sum_{j=1}^N (Y_{i,j} - \\hat{Y}_{i,j})^2,$\nFor auxiliary tasks, we introduce two parameters, $\\gamma_1$ and $\\gamma_2$, to combine the prediction loss in Eq.20 and the cluster/edge-wise adversarial learning-based loss in Eq.9/Eq.12 into the final objective for our RACTC-Cluster and RACTC-Edge model, respectively.\n$L_{RACTC-Cluster} = L_{OD} - \\gamma_1 L_{cluster},$\n$L_{RACTC-Edge} = L_{OD} - \\gamma_2 L_{"]}