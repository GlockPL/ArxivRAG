{"title": "Efficient Task Transfer for HLS DSE", "authors": ["Zijian Ding", "Atefeh Sohrabizadeh", "Weikai Li", "Zongyue Qin", "Yizhou Sun", "Jason Cong"], "abstract": "There have been several recent works proposed to utilize model-based optimization methods to improve the productivity of using high-level synthesis (HLS) to design domain-specific architectures. They would replace the time-consuming performance estimation or simulation of design with a proxy model, and automatically insert pragmas to guide hardware optimizations. In this work, we address the challenges associated with high-level synthesis (HLS) design space exploration (DSE) through the evolving landscape of HLS tools. As these tools develop, the quality of results (QoR) from synthesis can vary significantly, complicating the maintenance of optimal design strategies across different toolchains. We introduce Active-CEM, a task transfer learning scheme that leverages a model-based explorer designed to adapt efficiently to changes in toolchains. This approach optimizes sample efficiency by identifying high-quality design configurations under a new toolchain without requiring extensive re-evaluation. We further refine our methodology by incorporating toolchain-invariant modeling. This allows us to predict QoR changes more accurately despite shifts in the black-box implementation of the toolchains. Experiment results on the HLSyn benchmark transitioning to new toolchain show an average performance improvement of 1.58\u00d7 compared to AutoDSE and a 1.2x improvement over HARP, while also increasing the sample efficiency by 5.26x, and reducing the runtime by 2.7x.", "sections": [{"title": "1 INTRODUCTION", "content": "General-purpose computers are widely employed across diverse domains owing to their ease of programming. However, they face significant overheads due to extended instruction pipelines necessary for supporting generalization.\nThis has given rise to domain-specific accelerators (DSAs) [8, 9, 14]. However, DSAs pose challenges in programming, restricting their user base to hardware experts [8]. High-level synthesis (HLS) [11] was introduced to simplify the design process by raising the abstraction level from the register-transfer level (RTL) to C/C++. Designers use compiler directives, in the form of pragmas, to describe microarchitecture. While HLS reduces design turn-around times, not every HLS design yields a high-performance microarchitecture [27]. As a result, a new line of research aims to explore the solution space defined by pragmas more efficiently. Because evaluating each design candidate with HLS tools is time-consuming, recent studies propose supervised learning models as proxies of HLS tools, which can predict the quality of results (QoR) to expedite exploration [6, 25, 29\u201331]. Training such models involves collecting extensive datasets of diverse designs synthesized by HLS tools.\nA significant issue arises from the continuous evolution of HLS tools, which can significantly impact QoR values [26]. To gain insight into this issue, we utilized AutoDSE [27] to gather data on different Vitis HLS toolchains: 2020.2 (V20) [1], 2021.1 (V21) [2] and 2023.2 (V23) [3].\nIt is therefore critical to design a task transfer learning scheme that could efficiently adapt to the evolving toolchain. According to the previous observations, we observe two major challenges. (1) The first one is to sample efficiently the design from the design space so that the model transferred to those designs can effectively identify the high-quality designs. (2) Another challenge is to adapt the model to the large label shift. Overall, we want to efficiently uncover good designs on the new toolchain when the label shift is unknown and the design space could be as large as 10\u00b9\u00b3 as we see in the HLSyn benchmark [5].\nModel-based optimization is a widely used technique for efficiently sampling high-quality designs. Previous work has utilized Bayesian Optimization [7, 28], where sampling is guided by an acquisition function. However, optimizing the acquisition function in a large discrete space, such as HLS designs, remains challenging. Other studies, for example [19] have adapted active learning to achieve better sample efficiency when transferring to a new technology node by proposing data selection algorithms that select diverse and informative data from existing datasets. Our approach is not limited to selecting data from an existing unlabeled data pool, and could navigate the whole design space. In [25, 26, 29\u201331], optimizing HLS designs involves training a proxy model on an offline dataset and then conducting optimization with this proxy model. This offline approach requires time-consuming data collection and suffers from generalizability issues when applied to new domains and tasks.\nTransfer learning investigates the generalizability of models to unseen domains and tasks, with representation learning playing a critical role in domain transferability [6, 13, 16, 32\u201335]. Some studies propose learned design space pruning for domain transfer in Bayesian Optimization [4, 22]. Unlike domain transfer, which assumes a data distribution shift, we focus on task transfer, assuming a tractable change in the black-box function. Previous works [19, 26] have shown better task transfer accuracy by freezing some model parameters.\nTo enable efficient sampling on an unseen toolchain, we propose a novel model-based explorer designed for discrete optimization spaces. We jointly optimize the model prediction and the importance sampling distribution, allowing the sampling process to leverage knowledge from the previous toolchain and achieve better sample efficiency. Moreover, the sampling distribution can guide model updates, enabling the model to minimize its error on a subset of the design space rather than striving for perfect accuracy across the entire space. We also explore learning task-generalizable representations of HLS designs. Our key observation is that the same input often represents similar micro-architectures, despite potential differences in their implementations across toolchains. Thus, we aim to learn representations of HLS designs that can extract their invariance across various toolchains.\nOverall, we make the following contributions:\n\u2022 We propose a model-based explorer that efficiently searches the whole design space, with better sample efficiency compared with the SoTA approach.\n\u2022 We introduce a novel modeling of HLS designs to capture the invariance between toolchains.\n\u2022 Evaluation on 40 programs transferring to V21 shows that we can improve design performance by 1.58\u00d7 and 1.2\u00d7 on average, compared with AutoDSE and HARP, with a sample efficiency of 5.26\u00d7, and a 2.7\u00d7 runtime improvement.\n\u2022 Evaluation on the latest toolchain reveals that our design outperforms the best of AutoDSE and HARP by 1.27x."}, {"title": "2 PRELIMINARIES", "content": "2.1 HLS Design Space and Pragmas\nAs with AutoDSE and HARP, we utilize the open-source AMD/Xilinx Merlin Compiler [10] to streamline DSE for HLS. It supports three types of pragmas: PIPELINE, PARALLEL, and TILE. Taking designs with these pragmas as input, it automates the synthesis of on-chip buffers and memory interfaces. The PIPELINE, PARALLEL, and TILE pragmas handle loop pipelining, double buffering, loop unrolling, and loop tiling for parallelization and latency hiding. The Merlin Compiler supports standard pipelining and parallelization of HLS, but also offers additional automation for pragmas like \"array_partition\" which can be optimally solved given the parallel and tiling factors. This compiler provides a more compact design space and is used in this work. However, our approach can be directly applied to other HLS tools as well."}, {"title": "2.2 HARP", "content": "The HARP framework [26] is the state-of-the-art model-based approach for predicting the performance and resource utilization of HLS designs. It introduces a hierarchical graph representation to mitigate the over-smoothing issue encountered in graph neural networks. By employing a hierarchical graph, the shortest path of the graph can be substantially reduced. This enables the capture of more global information with a shallow GNN. Furthermore, HARP proposes to model the program and pragma transformations separately, treating pragmas as transformations modeled by Multi-Layer Perceptrons (MLPs). This model design aligns with the nature of HLS designs and results in improved accuracy and DSE performance. It trains a classification model to predict the validity and a regression model to predict the QoR. We follow this convention and denote the classification model as Rc and the regression model as R\u03b8."}, {"title": "3 METHODOLOGY", "content": "Our primary objective is for the learned model to adapt to a new toolchain while identifying Pareto-optimal points and maximizing sample efficiency. Our approach focuses on employing importance sampling to select high-quality designs and integrating model prediction within the importance sampling algorithm to improve sample efficiency. Figure 3 illustrates an overview of our proposed method, which consists of two main components: the DS Sampler and Reward Model Transfer Learning. Using the Cross-Entropy Method (CEM), the DS Sampler performs importance sampling based on the design performance. It iteratively optimizes a probability distribution across the entire design space, concentrating higher density on designs with high rewards. This approach ensures that high-reward designs are sampled more frequently, thereby increasing the likelihood of identifying optimal designs. To further boost sample efficiency, we introduce a reward model that predicts the validity and QoR of HLS designs. This model is adapted from previous toolchains, leveraging data collected from earlier implementations to enhance prediction accuracy. To address the potential label shift due to toolchain changes and distribution shift from the importance sampling, we integrate an active learning subroutine within the CEM algorithm. In summary, we facilitate an efficient transfer to a new toolchain through a model-based method, which involves two iterative steps: fitting the model to the current sampling distribution and updating the sampling distribution based on the model's recommendations.\nIn Section 3.1, we will describe version-invariant modeling that efficiently adapts the model to the label shift. In Section 4, we will explain the model-based importance sampling algorithm based on CEM."}, {"title": "3.1 Toolchain-invariant modeling", "content": "A critical problem of training a prediction model for HLS designs is how to train a robust model when the labeled data is scarce compared to the size of the design space. One way to mitigate the insufficiency of labeled data is to transfer knowledge from all existing data, possibly collected from different toolchains. Our intuition is to model a robust representation of HLS designs that contains the invariant information between different toolchains. In practice, such invariance is abundant. For example, the same input program with the same pragmas represents similar microarchitectures, and similar microarchitectures will not result in completely different implementations. Following this intuition, we consider learning invariant embedding for task transfer on the HLS design by training a single model with data from different toolchains.\nOne problem to tackle is to train a model that is aware of the version of the data. A simple solution is to include the version tag in the input data. However, when switching to a new version, we may need a completely new tag, and that will cause a huge shift in input data. Another approach is to have a shared encoder, but different decoder for each task. Each decoder is trained on data from its corresponding toolchain, allowing the unique parameters of the decoders to capture the specific characteristics of each toolchain. This approach follows the intuition that the same input HLS design represents a similar micro-architecture for different toolchains, and could be represented as the same embedding in the latent space. However, we argue that it is not enough to just model the shared information between toolchains. A shared embedding may not contain enough information to address the variance between toolchains. In HLS design, the information that remains completely invariant is the program. But for pragma transformations, it remains unknown exactly what implementation stays the same and what is changed. Following this intuition, we propose the following modeling scheme:\nSeparate embedding for shared and private information. Fig. 4 shows the model architecture. For each input design, two separate embeddings of the design will be learned. One of the embeddings is modeled by a set of parameters shared among all toolchains, and the other embedding is modeled with a set of parameters that is private to each toolchain. Then, we concatenate the two embeddings and forward it to different decoders for each toolchain.\nAnother benefit of the proposed model architecture is that it can be trained jointly with data from multiple versions. We optimize the mean-squared error (MSE) averaged over all versions of data. Intuitively, training with more data will help the model get a stronger representation of the invariant part. Moreover, once diverse data from different toolchains have been gathered, the invariant part can be pretrained, and we are able to fix the invariant part and fine-tune only the private part when switching to a new toolchain. This will result in fine-tuning fewer parameters when transferring to a new toolchain and better efficiency."}, {"title": "4 DS SAMPLER", "content": "4.1 Optimization with Cross-Entropy Method\nWe will now describe the original model-free version of the Cross-Entropy Method (CEM) [15] and how it can be applied to optimize HLS design. It was originally designed for rare-event simulation but can be applied for global optimization, and it has the ability to converge to global optimum theoretically [21, 23]. While the cross-entropy method is treated as a model-based technique in some work [12] because it contains a model for the distribution of rare events (good designs), we treat the original version of the algorithm as model-free since it does not contain a learned model of the reward function. We will show that the cross-entropy method is a strong candidate for efficient global optimization.\nAlgorithm 1 outlines the application of the Cross-Entropy Method (CEM) for optimizing black-box functions. The process begins by initializing the sampling distribution p(x) to a random distribution (line 3). In each iteration, N samples are drawn from the current distribution and evaluated using the ground truth function R (lines 6, 7). Based on these evaluations, the distribution is then updated to concentrate more on samples that exceed the (1-p) quantile of the N rewards, i.e. focusing on the top Np performing samples (lines 9-11). To maintain randomness and prevent overfitting, the distribution is modified by a step size \u03b1, which typically ranges from 0.7 to 0.9. This algorithm is noted for its rapid convergence rate and robustness against different hyperparameters, as introduced in [15, 20].\nTo tailor Algorithm 1 for HLS design optimization, the key tasks involve modeling the distribution of design configurations and implementing updates to this distribution. In HLS, each design is characterized by a combination of different pragma values. Each pragma within the program must either be assigned a specific value or be set to a default value that effectively disables it, thereby completing the design configuration. We model the distribution over all possible designs by assigning a probability vector pi(x) for each pragma i, resulting in p(x) = \u220fi pi(xi). The independent modeling reduces the likelihood of overfitting when the number of data points available for distribution calculation is limited. We will demonstrate that this method of distribution modeling not only simplifies sampling distribution updates but also yields effective optimization results for HLS designs.\nTo implement the distribution update, we first get the one-hot embedding of each design. Specifically, for each design x, we use a one-hot vector xi for each pragma, where xij = 1 if the discretized value of pragma i is j, and xij = 0 otherwise. For example, consider a design space containing two pragmas Pa and Pb, and suppose Pa can take value from [1, 2, 5] and P_b can take value from [1, 2]. Then the design x = {Pa = 5, P_b = 2} will be encoded with {xa = [0, 0, 1], x_b = [0, 1]}.\nThen, at each iteration, the distribution p in Algorithm 1 can be calculated by separately calculating the probability vector pi of each pragma i, with the one-hot vector xi:\n$$p_i = \\frac{\\sum_{x \\in D} x_i 1[R(x) > q]}{\\sum_{x \\in D} 1[R(x) > q]}$$\nwhere R is the ground truth reward function, D denotes the N samples generated by the previous distribution p at iteration t, and the q is the 1 - p quantile of Gr.\nWhen implementing the algorithm, we also memorize the best p. N design in all previous iterations and consider them when calculating the quantile and optimizing the distribution, which improves the performance. This detail is omitted in the algorithm description for simplicity.\nWe first conduct experiments to validate that CEM is good for global optimization, comparing it with other algorithms including"}, {"title": "4.2 Model-based CEM", "content": "One simple way to achieve sample efficiency is to replace the evaluation on the true reward function R with a proxy model Rc, pick the design with the top prediction value, and validate it with R at the last iteration. However, such an approach may cause severe performance degradation, because the model Rc is not accurate in terms of the label shift caused by changing the toolchain, and the distribution shift caused by a sampling distribution update. In Section 1, we illustrate how the model could be vulnerable to the label shift. Another factor to consider is the distribution shift. Even if the model has already adapted to the label shift with data on the new version, it may have low accuracy on some designs visited by the explorer, because of the mismatch between the training data distribution induced by the data gathering heuristic, and the testing distribution induced by the explorer. Therefore, it is possible for the explorer to find a design with a high predicted performance but a low actual performance. Such a phenomenon is constantly observed when trying to solely rely on the proxy model to do the optimization [17, 18]. Intuitively, a powerful optimization algorithm will put pressure on the model to be perfectly accurate on the whole design space, which is usually hard to achieve given limited labeled data.\nTherefore, it is critical to first update the model to have a reasonably accurate prediction under the current function R(x) and the current data distribution p(x). Typically, we want to optimize the following objective, where D is the whole design space.\n$$\\min_\\theta \\sum_{x \\in D} p(x) (R(x) \u2013 R_\\theta(x))^2$$\nWhen the sampling distribution updates, the objective becomes a weighted error, assigning greater weight to regions that potentially contain high-quality designs. Although it is still sample inefficient to minimize the error of the model with arbitrary data distribution on the whole design space, we find that it is possible to achieve better sample efficiency by integrating the model update into the optimization process. For the CEM, if the model can have an accurate prediction on D in each iteration, then the next distribution can be calculated solely based on the model prediction. Note that D \\subset D is sampled from the distribution p(x), and is an unlabeled pool of the design. As discussed in section 4.1, the size of D could still be large, so we want to further improve the sample efficiency, by selecting several designs from D to update the model. The sub-problem to solve then is: How to select a few designs in D and update the model on the true value of these designs, so that the model has a minimal error on D. More specifically, we want to find Dcore \\subset D, so that the model can have minimal error on D when the parameter \u03b8 is updated on Dcore."}, {"title": "4.2.1 Selective Labeling for Model Update", "content": "Several heuristics have been proposed to tackle the data selection problem in active learning. Those heuristics primarily consider two objectives, uncertainty and diversity. Both objectives are important to minimize the model error given a fixed labeling budget.\nHowever, the uncertainty-based approaches can only characterize the distribution shift, the change of p(x). In task transfer, it is also important to be aware of the change in the function R(x), or p(yx). While it is hard to estimate the change without actually observing the true value, it is still reasonable to sample diverse designs under a fixed budget, so that more information can be revealed about the unknown function. Therefore, we choose to use the coreset approach [24] to acquire diverse data.\nThe coreset based sampling algorithm approximates the model error on the whole pool of data with distance measurement from selected data points and the rest of the data. [24] and [19] separately model the problem as K-Center and K-Means clustering. We consider utilizing the K-Means algorithm on the hidden representation due to its efficiency. Since we have both a regression model R\u03b8, to predict the QoR and a classification model R_c to predict the validity of the design, we need samples to provide information for both models. We thus divide the K labeling budget into K\u2081 and K\u2080 and divide the unlabeled pool D into two subsets D\u2081 and D\u2080. D\u2081 contains designs that the classification model R_c predicts valid, and D\u2080 contains those that the model predicts invalid. Then, we run K-Means to get K\u2081 cluster centroids of the hidden embeddings of R\u03b8, on D\u2081 and K\u2080 cluster centroids of the hidden embeddings of R\u03b8 on D\u2080. Then, the designs closest to the cluster centroids are selected to be added to the database."}, {"title": "4.2.2 Active-CEM", "content": "We now describe the complete exploration algorithm which consists of an outer loop that updates the sampling distribution according to the model prediction, and an inner loop that fits the model to the current distribution. We abbreviate the name with Active-CEM.\nAs shown in Algorithm 2, the sampling distribution p is initialized to random, and the model R\u03b8,, R_c is initialized with the old model trained on the old database. A database B that has designs with true labels is initialized to empty. For each iteration t, an unlabeled pool D will be sampled according to distribution p. Then, the unlabeled pool D, the database B, and the current model will be forwarded to the inner active learning loop with the K-Means algorithm. The active learning loop will return an updated model. Then, the updated model will predict the label for all designs in D. According to the predicted label, the 1 - p quantile of the design performance will be calculated on the valid designs that satisfy the resource constraint, and the distribution will be updated according to Equation 1. The only difference is that the true value R(x) in equation 1 is replaced with the updated prediction model R\u03b8, (x) and R_c (x)."}, {"title": "4.2.3 Sample efficiency and runtime of the algorithm", "content": "We briefly discuss the sample efficiency and the runtime of the proposed algorithm. By sample efficiency, we mean the number of designs evaluated with the HLS toolchain during exploration, which is the major runtime bottleneck. As shown in algorithm 2, the modelfree version of the CEM algorithm needs to sample N\u00b7n design points, while the model-based version only requires m \u00b7b \u00b7n. As for the runtime, since the design is evaluated in batch, the runtime is bounded by the number of parallel HLS workers and the number of samples. The overhead of sampling unlabeled design and updating the distribution are negligible. Suppose there are C parallel workers to run HLS, and the timeout for running HLS design is T, then the runtime of the model-free version is T\u00b7 n \u00b7 [N/C], while the runtime of the model-based version is Tnm\u00b7 [b/C]. While proper scheduling could reduce the overall runtime of evaluating N design with C workers, we provide an upper bound of the runtime here."}, {"title": "4.3 Design Space Pruning", "content": "Another way to achieve sample efficiency is to prune away part of the large design space so that the explorer will have less pressure to explore a smaller design space. While design space pruning is not the focus of this work, we empirically find that the \"TILE\" pragma is not as efficient as the \"PARALLEL\" and \u201cPIPELINE\u201d pragma for optimizing design. Also, whether or not \"TILE\" pragmas would be effective is determined mostly by the program itself (whether it is memory bound/compute bound), and is more agnostic to the change in the toolchain. Therefore, we consider pruning away \"TILE\" pragmas for some programs. We adopt a conservative heuristic that prunes away \"TILE\" pragmas only if (1) less than 10% of the designs in the old database have tiling pragma, (2) the top-10 designs do not contain \"TILE\" pragma, and (3) the memory footprint of the workload could fit on-chip. We only cut the \"TILE\" pragma if all these conditions are satisfied. With this heuristic, the model will also witness less distribution shift at the early stage of exploration."}, {"title": "5 EVALUATION", "content": "5.1 Experiment setup\nWe evaluate our approach on two task transfer scenarios: from AMD Xilinx Vitis 2020.2 (V20) to Vitis 2021.1 (V21) and from Vitis 2021.1 (V21) to Vitis 2023.2 (V23). We also utilize the AMD Xilinx Merlin Compiler [10]. We assess our methods on 13 programs from the HLSyn [5] dataset, from toolchain V20 to V21. We do not consider programs that have a very small design space. The workload covers dense linear algebra, data analytics, and stencil operations. In Appendix A we also provide the evaluation results on the complete set of HLSyn database. To evaluate the robustness of the method to different label shifts, we further test it by transferring from V21 to V23, on 5 programs with very large design space. We list the domain and the size of the design space for each program in Table 2. \"MM\" stands for matrix-matrix multiplication, and \u201cMV\u201d stands for matrix-vector multiplication. Since we focus on assessing task transfer to new toolchains, we fixed the evaluation platform to Xilinx U200 with a frequency of 250MHz.\nWhile the proposed model architecture introduces extra overhead, we simply finetune the HARP model during the CEM exploration. After collecting a database with the explorer, we train the new model architecture with invariant embedding on the database and conduct one last round of DSE with a BFS search. The model is trained with a batch size of 64 and is trained for 1500 epochs, and the learning rate is set to 1e-3 with a linear warmup and is cosine annealed to 1e-5. When training the model with invariant embedding, we randomly sample mini-batches of 64 from the V18 and the V20 database, and forward all three batches in a single iteration to calculate the loss. We run each experiment three times with different random seeds, under the same split of the dataset, and report the mean and standard deviation of each method."}, {"title": "5.2 Model accuracy", "content": "We validate the effectiveness of learning invariant embedding on the HLS designs with model accuracy. We compare the proposed model (Inv-Hybrid) with three baselines, the HARP model trained from scratch (HARP scratch) on the new version data, the HARP model initialized with pre-trained parameters on old data and fine-tuned on new data (HARP finetune). For the old version data, we utilize the HLSyn database, which consists of data collected by AutoDSE on V18 and V20 of the toolchain. For the new version data, we utilize the data collected by AutoDSE on the V21 toolchain for the 13 programs we selected, following the data collection pipeline as described in HLSyn. 3975 regression data is gathered for the 13 programs on the V21 toolchain. And the V18 and V20 databases contain 9439 and 5194 data. We split the V21 data into training, validation, and test set, with a portion of 40%, 10%, and 50%, to mimic the scenario where a few data is gathered on the large design space. We select the model with a minimum validation error and record the testing accuracy."}, {"title": "5.3 DSE Performance", "content": "We next verify the efficiency of the Active-CEM algorithm. We compare with AutoDSE and HARP. For the AutoDSE baseline, we follow previous work and run all three explorers in parallel. Following [26], we set a timeout of 24 hours and do not terminate the jobs that are already running, resulting in a maximum runtime of 25 hours. For baseline evaluation, we adopt the same approach as HARP for running its explorer. Specifically, after collecting data with AutoDSE, we initialize the model with the pre-trained parameter on the old version, and then fine-tune the HARP model on the new version data. Then we run a BFS search for 1 hour with HARP and validate the design with top-10 predicted performance using the HLS toolchain. When transferring to V21, we initialize the model with parameters pre-trained on the V20 database. When transferring to V23, we initialize the model with parameters pre-trained on the V21 database. For a fair comparison, when pretraining the model on existing versions of databases, we only utilize the database collected by AutoDSE. For Active-CEM, similar to HARP, we initialize the model pre-trained parameter on the old version data. For each program, we run Active-CEM for 8 iterations to converge. At each iteration, we draw 5000 samples from the current distribution, and query 30 labeled designs among the 5000 samples. We gather all the labeled data queried so far and finetune the model for 30 epochs in each iteration. The overall runtime of the algorithm is in between 8.9-9.1 hours. It is composed of a 8.65 minutes overall timeout for running HLS and the overhead of the policy and the model update. The runtime is 2.7x faster than AutoDSE. The variation in runtime is caused by generating graph data of different sizes and fine-tuning the model."}, {"title": "5.4 Ablation Studies", "content": "5.4.1 Model transfer by freezing parameters. Table 6 shows the result of transferring the model by fixing the first several layers of the GNN encoder. Similar to the previous experiment, we split the data with 40%, 10%, and 50%, conduct each experiment 3 times with different random seeds, and report the mean and standard deviation. We observe that the model achieves the best accuracy when we do not freeze any parameter, which is different from the conclusion drawn from various transfer learning works with image data[19]. We hypothesize that this is because of the large label shift caused by switching to a different toolchain, and the lack of proper pretraining on a large database that contains multiple versions of data.\n5.4.2 Separate modeling the invariance and the difference. We add another baseline to confirm the effectiveness of the proposed model architecture, which models the invariance and the difference between toolchains. Specifically, we implement a model architecture as shown in Fig. 7, which simply uses a different decoder to decode a shared embedding to different outputs. Moreover, we fix the parameter size of both architectures to be the same, so that we can strictly study the effect of decoupling the shared and the private embedding. In Table 7, the test RMSE of \"Inv-Hybrid\" is lower than \"Inv-Share\" by 11%, demonstrating the importance of modeling both the invariance and the difference between toolchains.\n5.4.3 Design space pruning. We evaluate the efficiency of design space pruning by comparing the design performance when enabling and disabling it. Only 3 out of 13 programs satisfy the pruning constraints and we report the performance difference for them."}, {"title": "5.5 Extending to Domain Transfer", "content": "We further test the efficiency of the Active-CEM algorithm by applying it in the domain transfer learning setting. Specifically, we split the HLSyn dataset, train the HARP model on the training kernels, and test it on 4 testing kernels covering stencil and linear algebra workloads. We evaluate on two different train/test split, on 8 test programs in total. While HLSyn contains variants of the same program, we ensure that the training set and the test set contain completely different programs. Table 9 demonstrates that on test split 1, Active-CEM outperformed the best of AutoDSE and HARP on 3 out of 4 kernels, achieving a 2.7\u00d7 runtime speedup. Similarly, on test split 2, it demonstrated equal or superior performance on 3 out of 4 kernels. The performance variance across different test kernels can be attributed to the size of the design space and its difference from the training set. For instance, the training set included only one data-mining kernel, resulting in poorer performance on the covariance kernel compared to other linear algebra kernels."}, {"title": "6 CONCLUSION", "content": "In this work, we discuss the sample efficiency challenge in the task transfer learning problem for HLS DSE. We mitigate the challenge with a novel approach on both sampling and modeling. On sampling, we propose Active-CEM, a model-based explorer that efficiently samples high-quality designs in the whole design space. On modeling, we propose to model both the invariance and difference between toolchains and jointly train a single model with data collected from multiple toolchains. Moving forward, we plan to evaluate our sampling method in the domain-transfer setting and consider task-transfer learning for different devices. We also plan to combine domain knowledge in the sampling process for better efficiency."}, {"title": "A EXTENDED EXPERIMENT ON THE HLSYN DATASET", "content": "A.1 DSE Performance\nWe run Active-CEM and compare it with AutoDSE and HARP on additional kernels from the HLSyn dataset. Table 10 presents the actual latency of the best designs found by each method and the speedup of Active-CEM over the best results from HARP and AutoDSE. Despite achieving an average speedup of 1.20x, Active-CEM attains greater than 2x speedup on kernels such as trmm, trmm-opt, syrk, and heat-3d. It is also capable of navigating very large design spaces, achieving speedup on kernels like correlation, gemver, gemver-med, 3mm, and fdtd-2d, which have design spaces larger than 10\u00b9\u2070. However, Active-CEM's performance is worse on some kernels like mvt-med and adi. The root cause is that the model failed to learn accurate resource estimations, leading to designs that over-utilized resources or timed out on these kernels. We plan to address this issue by combining local search and global search in future work. Overall, Active-CEM can uncover better designs with less runtime on 14 out of 40 kernels.\nA.2 Model Accuracy\nWe further validate the efficiency of the hybrid modeling method using the full HLSyn dataset. We use a 40%, 10% and 50% of train, validation and test split. On the full V21 dataset, there are 10932 regression data. We compare the Inv-Hybrid approach against the HARP scratch and HARP finetune baselines. As shown in Table 11, the RMSE on test set improves by 11%.\nA.3 Overreliance on the Proxy Model\nWe validate how optimizing the surrogate model alone can lead to inferior performance. Specifically, we run the CEM algorithm with the HARP model trained on the AutoDSE database and compare it against the BFS search algorithm used by HARP, which traverses only about 100000 points of the design space. Same to the hyperparameters setting in Section 5.3, we run CEM with a population size of 5000 for 8 iterations, resulting in a sampling budget of about 40000. After running the two search algorithms on the HARP prediction model, we select the top 10 designs with the highest predicted performance for each kernel and run the V21 HLS tool to get the actual performance. As shown in Table 12, when searching with CEM, the top 1 predicted performance can be significantly higher than when searching with BFS, with notable speedup on kernels with large design spaces such as fdtd-2d-large, correlation, gemver, and 3mm. This is reasonable since CEM can search the entire design space more effectively.\nHowever, when validating the top 10 designs with the HLS tool, we identify a performance gap between the prediction and the ground truth. Specifically, for kernels like fdtd-2d-large, fdtd-2d, 3mm, covariance, syr2k, and gemver-medium, the top 10 designs proposed by the proxy model either contain"}]}