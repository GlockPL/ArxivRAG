{"title": "Tensor Product Attention Is All You Need", "authors": ["Yifan Zhang", "Yifeng Liu", "Huizhuo Yuan", "Zhen Qin", "Yang Yuan", "Quanquan Gu", "Andrew Chi-Chih Yao"], "abstract": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPA's memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing, demonstrating exceptional performance across tasks (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; Bubeck et al., 2023). As these models evolve, their ability to process longer contexts becomes increasingly important for sophisticated applications such as document analysis, complex reasoning, and code completions. However, managing longer sequences during inference poses significant computational and memory challenges, particularly due to the storage of key-value (KV) caches (Zhang et al., 2023c; Liu et al., 2024c). Because memory consumption grows linearly with sequence length, the maximum context window is limited by practical hardware constraints.\nA variety of solutions have been explored to address this memory bottleneck. Some approaches compress or selectively prune cached states through sparse attention patterns (Child et al., 2019) or token eviction strategies (Zhang et al., 2023c; Xiao et al., 2024; Ribar et al., 2024), though such methods risk discarding tokens that may later prove important. Other work proposes off-chip storage of key-value states (He & Zhai, 2024), at the expense of increased I/O latency. Attention variants like multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023) reduce per-token cache requirements by sharing keys and values across heads, but often compromise flexibility or require significant architectural modifications. Meanwhile, low-rank weight factorization methods such as LoRA (Hu et al., 2022) effectively reduce fine-tuning memory, yet do not address the KV cache overhead that dominates runtime. The recently introduced Multi-head Latent Attention (MLA) in Deepseek-V2 (Liu et al., 2024a) caches compressed key-value representations but needs additional position-encoded parameters per head due to incompatibility with Rotary Position Embedding (RoPE) efficiently (Su et al., 2024b).\nIn order to overcome the limitations of existing approaches, we introduce Tensor Product Attention (TPA), as illustrated in Figure 1, a novel architecture that uses higher-order tensors to factorize queries (Q), keys (K), and values (V) during attention computation. By dynamically factorizing activations rather than static weights (e.g., LoRA), TPA constructs low-rank, contextual representations that substantially reduce KV cache memory usage with improved representational capacity. In practice, TPA can reduce the memory overhead by an order of magnitude compared to standard multi-head attention (MHA) with lower pretraining validation loss (perplexity) and improved downstream performance.\nA key advantage of TPA is its native compatibility with rotary positional embeddings (ROPE) (Su et al., 2024b), enabling a straightforward drop-in replacement for multi-head attention (MHA) layers in modern LLM architectures such as LLaMA (Touvron et al., 2023) and Gemma (Team et al., 2024).\nOur primary contributions are summarized as follows:\n\u2022 We propose Tensor Product Attention (TPA), A mechanism that factorizes Q, K, and V activations using contextual tensor-decompositions to achieve 10\u00d7 or more reduction in inference-time KV cache size relative to standard attention mechanism (Vaswani et al., 2017) with improved performance compared to previous methods such as MHA, MQA, GQA, and MLA. In addition, we unify existing attention mechanisms by revealing that MHA, MQA, and GQA all arise naturally as non-contextual variants of TPA.\n\u2022 We propose Tensor ProducT ATTenTion Transformer (T6), a new TPA-based model architecture for sequence modeling. On language modeling experiments, T6 consistently improves validation perplexity and downstream evaluation performance with reduced KV cache size.\n\u2022 We show TPA integrates seamlessly with RoPE (Su et al., 2024b), facilitating easy adoption in popular foundation model architectures such as LLaMA and Gemma."}, {"title": "Background", "content": "In this section, we review several classical forms of attention: Scaled Dot-Product Attention, Multi-Head Attention (MHA) (Vaswani et al., 2017), Multi-Query Attention (MQA) (Shazeer, 2019), and Grouped Query Attention (GQA) (Ainslie et al., 2023), as well as Rotary Position Embedding (ROPE, Su et al. (2024b)). We also introduce a recent method called Multi-head Latent Attention (MLA) used in DeepSeek-V2 (Liu et al., 2024a) and DeepSeek-V3 (Liu et al., 2024b).\nNotations. We use bold uppercase letters (e.g., X, Q) for matrices, bold lowercase (e.g., a, b) for vectors, and italic uppercase (e.g., \\(W\\)) for learnable parameter matrices. We denote by [n] the set \\({1,..., n}\\) for some positive integer n. We use T to denote the transpose of a vector or a matrix. Let \\(d_{model}\\) be the embedding dimension, h the number of attention heads, \\(d_h\\) the dimension per head, \\(x_t \\in \\mathbb{R}^{d_{model}}\\) the input for the t-th token at a given attention layer, \\(X \\in \\mathbb{R}^{T \\times d_{model}}\\) denotes the input embeddings for T tokens, and \\(Q, K, V \\in \\mathbb{R}^{T \\times h \\times d_h}\\) denote the queries, keys, and values of h heads for T tokens. With a little abuse of notation, \\(Q_i, K_i, V_i \\in \\mathbb{R}^{T \\times d_h}\\) denote the i-th head of queries, keys, and values, and \\(Q_t, K_t, V_t \\in \\mathbb{R}^{h \\times d_h}\\) denote the heads of the query, key, and value for t-th token.\nThroughout the paper, \\(W^Q, W^K, W^V\\) denote projection matrices for queries, keys, and values, respectively. In multi-head attention, each head is associated with its own set of \\(W^Q, W^K, W^V\\), and each has dimension \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_{model} \\times d_k}\\), where \\(d_k\\) is typically set to \\(d_h\\), the dimension of each head. Similarly, we have an output projection matrix \\(W^O \\in \\mathbb{R}^{(h \\cdot d_h) \\times d_{model}}\\). For methods like MQA and GQA, some of these are shared or partially shared across heads, but their shapes remain consistent.\nWe define the tensor product of two vectors as follows: for vectors \\(a \\in \\mathbb{R}^{m}\\), \\(b \\in \\mathbb{R}^{n}\\), the tensor product of a and b is:\n\\[a \\otimes b = C \\in \\mathbb{R}^{m \\times n}, \\text{with} \\, C_{ij} = a_i b_j,\\]\nwhere \\(a_i\\) and \\(b_j\\) are the i-th and j-th elements of a and b respectively, and \\(C_{ij}\\) is the (i, j)-th entry of C. We also define the vectorization of a matrix \\(C \\in \\mathbb{R}^{m \\times n}\\) by:\n\\[\\text{vec}(C) = d \\in \\mathbb{R}^{mn}, \\text{with} \\, d_{i \\cdot n+j} = C_{ij},\\]\nwhere \\(d_{i \\cdot n+j}\\) is the (in + j)-th element of d.", "subsections": [{"title": "Scaled Dot-Product Attention", "content": "Scaled dot-product attention (Vaswani et al., 2017) determines how to focus on different parts of an input sequence by comparing queries (Q) and keys (K). It produces a weighted combination of the"}]}, {"title": "Multi-Head Attention (MHA)", "content": "Often, one sets \\(h \\times d_h = d_{model}\\), so each head has query/key/value dimension \\(d_h\\).\nvalues (V). Formally, the attention output is:\n\\[\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\]\nwhere each of Q, K, V is an (n \u00d7 dk) matrix for n tokens and key dimension dk. The division by \\(\\sqrt{d_k}\\) stabilizes training by controlling the scale of the inner products."}, {"title": "Multi-Query Attention (MQA)", "content": "Multi-Head Attention (MHA) extends scaled dot-product attention by dividing the model's internal representation into several heads. Each head learns different projections for queries", "follows": "n\\[Q_{t,i} = (W_i^Q)^T x_t \\in \\mathbb{R}^{d_h}, \\quad K_{t,i} = (W_i^K)^T x_t \\in \\mathbb{R}^{d_h}, \\quad V_{t,i} = (W_i^V)^T x_t \\in \\mathbb{R}^{d_h},\\"}]}