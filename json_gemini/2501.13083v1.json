{"title": "Boosting MCTS with Free Energy Minimization", "authors": ["Mawaba Pascal Dao", "Adrian M. Peter"], "abstract": "Active Inference, grounded in the Free Energy Principle, provides a powerful lens for understanding how agents balance exploration and goal-directed behavior in uncertain environments. Here, we propose a new planning framework, that integrates Monte Carlo Tree Search (MCTS) with active inference objectives to systematically reduce epistemic uncertainty while pursuing extrinsic rewards. Our key insight is that MCTS\u2014already renowned for its search efficiency\u2014can be naturally extended to incorporate free energy minimization by blending expected rewards with information gain. Concretely, the Cross-Entropy Method (CEM) is used to optimize action proposals at the root node, while tree expansions leverage reward modeling alongside intrinsic exploration bonuses. This synergy allows our planner to maintain coherent estimates of value and uncertainty throughout planning, without sacrificing computational tractability. Empirically, we benchmark our planner on a diverse set of continuous control tasks, where it demonstrates performance gains over both stand-alone CEM and MCTS with random rollouts.", "sections": [{"title": "1 Introduction", "content": "The integration of search mechanisms into decision-making frameworks has consistently led to significant performance improvements across various domains. Monte Carlo Tree Search (MCTS), a powerful search-based planning method, has been particularly successful in discrete domains such as game-playing, with notable applications like AlphaGo combining MCTS with deep neural networks to achieve superhuman performance in the game of Go (Silver et al., 2016, 2017). However, extending MCTS to more general settings, particularly within the Active Inference framework, presents both challenges and opportunities.\nActive Inference, rooted in the Free Energy Principle (Friston, 2010), provides a unifying framework for understanding action and perception as processes of minimizing free energy. Recent advancements have explored the integration of Active Inference with MCTS to enable sophisticated planning under uncertainty in both discrete and continuous state-action spaces (Fountas et al., 2020; Tschantz et al., 2020). These methods demonstrate the potential to balance exploitation and exploration naturally by incorporating free energy minimization as a criterion for action selection. However, key challenges remain, including the computational demands of planning in continuous spaces, ensuring reliable value estimation during tree-based search, and extending these methods to practical applications and benchmarks beyond example problems.\nIn this paper, we propose a novel framework that integrates MCTS with Active Inference to address these challenges. Our approach introduces mechanisms for efficient planning in continuous state-action spaces while aligning the generative model of Active Inference with the tree search process.\nOur contributions can be summarized as follows:\n\u2022 Root Action Distribution Planning: We propose a novel mechanism where a single Gaussian action distribution is fitted at the root node using the Cross-Entropy Method (CEM). This root action distribution is utilized consistently throughout the tree traversal and simulation phases, significantly reducing computational complexity while ensuring value estimation remains aligned with actual action selection. By constraining the tree size, we maintain the validity of the root action distribution, enabling efficient and reliable planning.\n\u2022 Enhanced Exploration through Information Gain: Our method incorporates intrinsic exploration by integrating epistemic value (Information Gain) into the planning process. This dual exploration mechanism, achieved through both the expected free energy criterion and MCTS exploration, improves the agent's ability to navigate high-dimensional continuous domains.\nThe remainder of this paper is organized as follows. In Section 2, we discuss related"}, {"title": "2 Related Work", "content": "Active Inference, rooted in the Free Energy Principle (Friston, 2010), offers a unified framework for understanding perception and action as inference processes. It has demonstrated broad applicability across neuroscience and machine learning, modeling phenomena such as curiosity (Schwartenbeck et al., 2018), dopaminergic discharges (FitzGerald et al., 2015), and animal navigation. However, a significant challenge lies in the computational complexity of evaluating all possible policies, which grows exponentially with the planning horizon.\nModel-based Reinforcement Learning (RL) methods aim to learn a model of the environment's dynamics and use it for planning (Moerland et al., 2023). These methods can be more sample-efficient than model-free approaches, as they can simulate experiences without interacting with the environment. Chua et al. (2018) proposed PETS (Probabilistic Ensembles with Trajectory Sampling), which uses an ensemble of probabilistic models for planning in continuous action spaces. Similarly, Hafner et al. (2019) introduced PlaNet, a model-based RL method that learns a latent dynamics model for planning. By leveraging probabilistic models, these methods provide robust uncertainty quantification, which is critical for exploration and planning under uncertainty.\nTschantz et al. (2020) proposed the Free Energy of Expected Future (FEEF) as a tractable objective for decision-making in RL environments. Their method incorporates a model-based Cross-Entropy Method (CEM) for policy optimization, achieving a balance between exploration and exploitation in sparse and continuous control tasks.\nMonte Carlo Tree Search (MCTS), a decision-making framework, has proven valuable in addressing the computational complexity of evaluating an exponentially growing number of possible policies as the planning horizon increases. MCTS achieves this by sampling a subset of possible policies. Early applications of MCTS focused on discrete domains, such as game playing (Coulom, 2006), with significant successes in AlphaGo (Silver et al., 2016, 2017). While extensions to continuous action spaces, such as progressive widening (Coulom, 2006) and hierarchical optimization (Bubeck et al., 2011), have broadened its scope, these approaches are typically employed in Reinforcement Learning (RL) contexts.\nRecent advancements have combined MCTS with Active Inference to address challenges in planning under uncertainty. For instance, Fountas et al. (2020) proposed an MCTS-based Active Inference framework that replaces traditional selection criteria, such as the Upper Confidence Bounds applied to Trees (UCT) (Kocsis and Szepesv\u00e1ri, 2006), with an expected free energy (EFE)-based criterion. Their approach employs a deep neural network to approximate posterior distributions and utilizes Monte Carlo sampling to evaluate free energy terms efficiently. This integration demonstrated im-"}, {"title": "3 Background", "content": ""}, {"title": "3.1 Active Inference and the Free Energy Principle", "content": "The Free Energy Principle, originating in neuroscience, posits that systems act to minimize a quantity called free energy, which measures how well an internal generative model predicts observations (Friston, 2010). This principle unifies perception and action under the framework of probabilistic inference, where agents aim to align their beliefs with observed data and predict future states.\nFree energy is defined as:\n$F(Q, y) = DKL[Q(x)||P(x|y)] - ln P(y),$ (1)\nwhere:\n\u2022 Q(x): The approximate posterior distribution over hidden states x.\n\u2022 P(xy): The true posterior distribution over hidden states, given observations y.\n\u2022 In P(y): The log evidence (marginal likelihood) of the observations.\n\u2022 $DKL[Q(x)||P(x|y)]$: The Kullback-Leibler divergence between the approximate and true posterior distributions.\nMinimizing free energy involves two components:\n1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate posterior Q(x) with the true posterior P(x|y)."}, {"title": "3.2 Monte Carlo Tree Search (MCTS)", "content": "Monte Carlo Tree Search (MCTS) is a heuristic search algorithm designed for decision-making in large and complex state spaces (Browne et al., 2012). It incrementally builds a search tree by iteratively simulating playouts, balancing exploration and exploitation to identify promising actions. Each node in the tree represents a state, and edges represent actions.\nThe MCTS process is typically divided into four key phases:\n\u2022 Selection: Starting from the root node, the algorithm traverses the tree by selecting child nodes based on a specific policy until a leaf node is reached. The most commonly used selection policy is based on Upper Confidence Bounds (UCB), described below.\n\u2022 Expansion: If the leaf node does not represent a terminal state, one or more child nodes are added to the tree, representing unexplored actions.\n\u2022 Simulation: A simulation, or playout, is performed from the expanded node, where actions are sampled according to a default policy (e.g., random actions) until a terminal state is reached. The return from this simulation provides an estimate of the value of the expanded node.\n\u2022 Backpropagation: The results of the simulation are propagated back up the tree, updating the values and visit counts of all nodes along the path.\nBy iteratively performing these steps, MCTS progressively refines its estimates of action values, focusing computational resources on the most promising parts of the search space. This property makes MCTS highly effective in problems with large state spaces and uncertain outcomes."}, {"title": "3.2.1 Upper Confidence Bound 1 (UCB1)", "content": "The Upper Confidence Bound 1 (UCB1) algorithm is a widely used technique in MCTS to address the exploration-exploitation tradeoff during tree traversal (Auer et al., 2002). UCB1 assigns a score to each child node, balancing the average reward observed (exploitation) with the uncertainty of the node (exploration).\nThe UCB1 value for selecting a child node i is computed as:\n$UCB1\u2081 = Qi + Cucb \\sqrt{\\frac{In N}{Ni}}$, (2)\nwhere:\n\u2022 Qi is the average reward (mean value) of child i.\n\u2022 N is the total number of times the parent node has been visited.\n\u2022 Ni is the number of times child i has been visited.\n\u2022 Cucb is the exploration constant that controls the degree of exploration.\nThe first term, Qi, promotes exploitation by preferring actions that have yielded higher rewards on average. The second term, $Cucb \\sqrt{\\frac{In N}{Ni}}$, encourages exploration by assigning a higher bonus to actions that have been selected fewer times, thus having higher uncertainty. The logarithmic factor ln N ensures that as the number of visits N to the parent node increases, the exploration bonus decreases, allowing the algorithm to focus more on exploitation over time.\nBy integrating UCB1 into MCTS, the algorithm effectively balances the need to explore new actions that might lead to better rewards with the need to exploit actions that"}, {"title": "4 Proposed Planner", "content": "We now present our MCTS-CEM planning framework, which integrates Monte Carlo Tree Search with the Cross-Entropy Method (CEM) at the root node to handle continuous actions effectively. Figure 1 provides a high-level overview, highlighting three main components:\n1. (A) The root node, initialized with the agent's current state so.\n2. (B) The process of fitting a single Gaussian action distribution using CEM at the root node.\n3. (C) The subsequent tree-based planning (MCTS) stage, which uses the fitted root action distribution for exploration, rollouts, and leaf-node simulations."}, {"title": "4.1 Root Action Distribution Planning", "content": "The key idea is to learn one Gaussian distribution over actions at the root node, then reuse it throughout tree-based planning and simulation, thereby ensuring consistent estimates of value and reward. We denote this root action distribution by $a \\sim \u039d(\u03bc, \u03a3)$. where \u03bc and \u2211 are optimized via CEM to maximize expected returns plus any epistemic (information gain) terms."}, {"title": "4.1.1 Fitting the Root Action Distribution via CEM", "content": "At the beginning of planning (from root state so), we perform Cross-Entropy Method optimization to obtain \u03bc and \u03a3. Figure 2 illustrates this process (labeled \u201c3. Evaluation of Each Candidate", "performers": "n$\u03bc_{new} = \\frac{1}{k} \\sum_{i\u2208S} a^{(i)}, \u03a3_{new} = \\frac{1}{k} \\sum_{i\u2208S} (a^{(i)} \u2013 \u03bc_{new}) (a^{(i)} \u2013 \u03bc_{new})^T ,$ (3)\nwhere S is the index set of top-k candidates.\n5. Repeat for a fixed number of CEM iterations until convergence.\nThe output is a single, optimized action distribution N (\u03bc, \u03a3) centered on promising actions from the model's perspective."}, {"title": "4.1.2 MCTS-CEM Planning with the Fitted Root Action Distribution", "content": "Once the root action distribution has been fit via CEM, we keep it fixed for the duration of the tree-based planning. Figure 3 depicts this stage:\nMCTS Expansion and Action Selection. When selecting actions at each decision node, we sample from N(\u03bc, \u03a3) rather than optimizing new distributions at non-root nodes. This design assumes that states near the root are sufficiently representative of"}, {"title": "4.2 Epistemic Value as Information Gain Bonus", "content": "The expected free energy G(\u03c0) for a policy \u03c0can be decomposed into two key components: the extrinsic value (preferences over observations) and the epistemic value (information gain). Formally, it is defined as (Friston et al., 2015):\n$G(\u03c0) = Eq(81:\u0397|\u03c0) \\sum_{t=1}^{H} [- In P(Ot St) + DKL [q(St|\u03c0) || P(St|St\u22121, At\u22121)]],$ (4)\nwhere q(81:\u0397|\u03c0) is the variational posterior over states given policy \u03c0, P(ot|st) is the likelihood of observations given states, DKL[\u00b7 || \u00b7] is the Kullback-Leibler (KL) divergence, and p(st|St\u22121, at\u22121) is the prior predictive distribution of states under the policy. The extrinsic value encodes the agent's preferences over observations, while the epistemic value quantifies the expected information gain about the environment's dynamics.\nIn reinforcement learning, the agent's goal is to maximize cumulative rewards. To incorporate the Free Energy Principle, we approximate the extrinsic value \u2013 ln P(ot|St) with the negative reward function -r(st, at). This is a standard practice when aligning reinforcement learning with the Free Energy framework, as rewards are viewed as representing the agent's preferences over states or outcomes (Friston et al., 2009; Tschantz et al., 2020; Millidge, 2020). This approximation allows us to interpret reward maximization as a form of free energy minimization, reframing the agent's extrinsic motivation in terms of the principle.\nThe epistemic value measures how much an agent's belief about the next state changes when new information is available, capturing the expected information gain from taking an action. For a candidate action sequence a\u00b2, the epistemic value at time t is defined as the expected KL divergence between the approximate posterior and prior predictive distributions.\n$EV = Eq(st+1/s,a) [lnq(st+1|si, a) \u2013 In p(st+1|si, a)].$ (5)\nTo compute the epistemic value practically, we approximate the expected KL divergence as the difference between the entropy of the aggregated predictive distribution and the average entropy of the individual model predictions. This approximation is inspired by the Bayesian Active Learning by Disagreement (BALD) framework (Houlsby et al., 2011; Gal et al., 2017), where mutual information is used to quantify epistemic uncertainty.\nStarting from Equation (5) and with some minor abuse of notation, we observe that the log-ratio $In q(st+1 | si, a) - Inp(st+1 | si, a) = In\\frac{q(s)}{p(s)}$. Taking the expectation under the posterior q(\u00b7), we have\n$DKL[q(s)||p(s)] = \\int q(s) ln \\frac{q(s)}{p(s)} ds$ (6)\n$= \\int q(s) lnq(s) ds-\\int q(s) Inp(s) ds$ (7)\n$= \u2212H(q) - \\int q(s) In p(s) ds,$ (8)\nwhere $H(q) = \\int q(s) ln q(s) ds$ is the entropy of q(s). The term $\\int q(s) In p(s) ds$ can be challenging to compute directly. However, if q(s) and p(s) are both Gaussian"}, {"title": "4.3 Algorithmic Details and Pseudocode", "content": "Below, we provide the pseudocode for MCTS-CEM (Algorithm 1), which illustrates how the root action distribution is used during planning. This algorithm reflects the approach described in Sections 4.1\u20134.1.2, including how reward and epistemic value are integrated during the rollouts."}, {"title": "5 Experiments", "content": "In this section, we present experiments conducted to evaluate our proposed MCTS-CEM. We compare MCTS-CEM to two other planners:\n1. CEM Planner: A regular Cross Entropy Method planner that uses the same model-based rollout used by Tschantz et al. (2020) during the simulate phase of our MCTS-CEM algorithm.\n2. MCTS-Random: An MCTS planner that employs a random policy during roll-outs. Critically, the simulate phase of MCTS-Random does not use any Free Energy Minimization for action selection. Instead, at each step of the planning horizon, it samples a random action from a uniform distribution bounded by the environment's action space.\nWe compare these three planners across five different environments. Specifically, in the Pendulum and Sparse Mountain Car environments, we run each planner for 10 episodes per trial, while in HalfCheetah-Run and HalfCheetah-Flip we run each for 1000 episodes. We repeat every trial five times with different random seeds to account for variability in performance. All planners are configured with the same planning horizon and number of simulations per planning step within each environment to ensure a fair comparison. The experiments use the same model dynamics and reward function across all planners."}, {"title": "5.1 Pendulum", "content": "In the Pendulum environment (Figure 4), we observe that while MCTS-CEM consistently outperforms MCTS-Random, it does not provide a significant improvement over\nCEM. This can be attributed to the nature of the environment, which features a one-dimensional action space representing the torque applied to the pendulum's free end. The reward function is well-shaped and continuous:\n$r = \u2212(02 + 0.1 \u00b7 0\u00b2 + 0.001 - torque\u00b2),$ (23)\nwhere @ is the pendulum's angle, normalized between [-\u03c0, \u03c0], with 0 being the upright position. The minimum reward is -16.27 when the pendulum is fully displaced with maximum velocity, and the maximum reward is 0 when the pendulum is perfectly upright with no torque applied. Given the simple nature of this reward structure and the deterministic dynamics of the environment, even random action selection by MCTS-Random can achieve maximum reward in the early episodes.\nThe comparable performance between CEM and MCTS-CEM suggests that the additional exploration performed by MCTS-CEM might be unnecessary in this well-defined and deterministic environment. Once the agent achieves the maximum reward state, further exploration does not add value, as MCTS-CEM continues to search novel state-action pairs even when the optimal policy is already known. The exploration term in MCTS encourages novelty, which, while beneficial in more complex environments, may be counterproductive here where a known deterministic policy is sufficient to consistently achieve optimal performance. Our findings align with those of Bellemare et al. (2016), who demonstrated that in environments with sparse rewards, methods incorporating intrinsic motivation\u2014such as count-based exploration strategies\u2014are particularly effective. In contrast, in simpler, well-shaped environments, relying on established optimal policies, as CEM does, might be more efficient than the continual planning that\nMCTS-CEM performs."}, {"title": "5.2 Sparse Mountain Car", "content": "In the Sparse Mountain Car environment (Figure 6), the goal is to generate controls that drive a car to the top of a hill marked by a flag, representing the goal state. The car lacks sufficient acceleration to climb the hill directly. Instead, it must first move backward up a smaller hill to gain enough momentum to ascend the larger hill in front. In this sparse reward setting, the agent receives a positive reward of +1 only upon reaching the goal state, while incurring a negative reward at every time step it does not reach the goal.\nResults on the Sparse Mountain Car environment (Figure 7) highlight the strengths of MCTS-CEM, particularly in later episodes. Initially, CEM outperforms MCTS-CEM, but as more episodes are played, MCTS-CEM improves significantly, ultimately achieving higher maximum rewards than both CEM and MCTS-Random. This improvement can be attributed to the synergy between CEM's root distribution optimization and the UCB-based tree search within MCTS-CEM. The UCB formula for child selection in our planner is given by:\n$UCB\u2081 = Qi + c \u2022\\sqrt{\\frac{In Nu}{Ni}}$, (24)\nwhere Qi is the estimated return of child i, N\u2082 is the visit count of the parent node v, and Ni is the visit count of the child itself. Initially, MCTS-CEM's double reliance on an approximate reward model can cause performance to lag behind CEM, since reward models may overestimate certain state-action pairs (Pathak et al., 2017). However, as more data is gathered over multiple episodes, the reward model becomes more accurate. Combined with the broader search from UCB expansions, MCTS-CEM discovers high-reward trajectories more effectively than CEM in this sparse environment.\nThe ability of MCTS-CEM to tap into a wider range of potential future states with"}, {"title": "5.3 HalfCheetah-Run", "content": "In the HalfCheetah-Run environment (Figure 8), the objective is for the agent, controlling a simulated two-dimensional cheetah, to run as quickly as possible in the forward direction. The agent receives dense rewards based on its forward velocity, offering frequent feedback that helps MCTS-CEM correct suboptimal trajectories during planning. As illustrated in Figure 9, MCTS-CEM consistently outperforms both the CEM\nPlanner and MCTS-Random by leveraging both upper confidence bound (UCB)-based exploration and CEM's strength in continuous action optimization.\nDespite this strong overall performance, both MCTS-CEM and CEM occasionally exhibit abrupt dips in reward\u2014an effect we refer to as \u201cpolicy collapse,\" following Millidge (2019) and Friston et al. (2009). Here, an imbalance between exploration and exploitation, coupled with potential inaccuracies in the planner's value or reward estimates, can cause the agent to momentarily overcommit to suboptimal actions. Nevertheless, MCTS-CEM significantly mitigates these collapses compared to plain CEM, likely due to the \u201cdouble exploration\u201d stemming from both the Monte Carlo tree search (via UCB) and the CEM-based optimization in continuous action space. This dual layer of exploration makes the planner more robust to episodic failures, as it can more quickly identify and correct suboptimal trajectories. In practice, policy collapse can be further alleviated by tuning the weight of intrinsic exploration terms and improving model fidelity (e.g., by additional training data or larger ensembles), thereby helping to maintain a more reliable balance between exploration and exploitation."}, {"title": "5.4 HalfCheetah-Flip", "content": "In the HalfCheetah-Flip environment (Figure 10), the objective is for the agent, controlling a simulated two-dimensional cheetah, to perform front flips rather than maximizing forward velocity. The agent receives dense rewards based on the quality and frequency of its flips. Unlike HalfCheetah-Run, the task requires simple motor coordination and involves coarser control over the cheetah's dynamics.\nAs illustrated in Figure 11, MCTS-CEM and CEM exhibit nearly identical performance, with no significant advantage observed for the additional planning provided by MCTS-CEM. This is in stark contrast to environments like HalfCheetah-Run, where MCTS-CEM demonstrated a clear performance advantage. The similarity in performance could be attributed to the nature of the task: flipping requires coarser motor control than running, and the additional computational effort of MCTS planning does not yield substantial improvements. Instead, simpler optimization through CEM alone appears sufficient for achieving near-optimal results in this environment.\nThis result emphasizes the importance of task characteristics in determining the utility of MCTS-CEM. While the algorithm excels in sparse reward environments or well-shaped tasks involving complex, high-dimensional controls, its benefits are less apparent in tasks requiring coarser motor coordination and less intricate action planning. This insight highlights the nuanced trade-offs between computational overhead and planning efficacy in different types of continuous control tasks."}, {"title": "6 Conclusion", "content": "In this work, we introduced MCTS-CEM, a model-based planner that unifies Monte Carlo Tree Search (MCTS) with the Free Energy Principle's uncertainty minimization objective. By adopting ensemble-based rollouts and incorporating epistemic value estimates as an intrinsic exploration bonus, our planner naturally balances the drive to maximize extrinsic rewards with the need to reduce uncertainty about the environment's dynamics.\nA key element of our approach is fitting a single Gaussian action distribution at the root node using the Cross-Entropy Method (CEM). We then use this root distribution consistently throughout the expansion and simulation phases, ensuring that the policy underlying tree search remains coherent with the policy used for value estimation in leaf-node rollouts. This strategy avoids redundant re-optimization at deeper nodes and lends stability to planning. Moreover, the integration of information gain as part of the Free Energy minimization criterion enables a principled exploration mechanism that boosts performance in environments featuring sparse and delayed rewards.\nEmpirically, we validated MCTS-CEM on a variety of continuous control tasks, including Pendulum, Sparse Mountain Car, and HalfCheetah. Across these benchmarks, our method consistently outperformed or matched baseline planners that either rely solely on CEM or combine MCTS with simplistic (random) policies. Notably, MCTS-CEM demonstrated superior robustness to random seed variability and scaled favorably with increasing amounts of environment interaction, suggesting that its exploration-driven planning is well-suited to long-horizon tasks with limited feedback signals.\nOverall, this work highlights the promise of coupling MCTS with Free Energy minimization for active inference in high-dimensional control problems. By merging a powerful search paradigm with an epistemic drive to reduce model uncertainty, we bring a unified view of planning, exploration, and policy refinement to reinforcement learning. However, our results also reveal challenges related to balancing exploration and exploitation, particularly when intrinsic exploration bonuses dominate extrinsic reward optimization, leading to episodic policy collapse. Future research should focus on mitigating this issue by exploring methods such as adaptive regularization of intrinsic exploration bonuses, incorporating uncertainty-aware thresholds, and improving the fi-"}]}