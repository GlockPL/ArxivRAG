{"title": "Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization", "authors": ["Zhi Chen", "Lingxiao Jiang"], "abstract": "In the rapidly evolving field of machine learning, training models with datasets from various locations and organizations presents significant challenges due to privacy and legal concerns. The exploration of effective collaborative training settings, which are capable of leveraging valuable knowledge from distributed and isolated datasets, is increasingly crucial. This study investigates key factors that impact the effectiveness of collaborative training methods in code next-token prediction, as well as the correctness and utility of the generated code, showing the promise of such methods. Additionally, we evaluate the memorization of different participant training data across various collaborative training settings, including centralized, federated, and incremental training, showing their potential risks in leaking data.\nOur findings indicate that the size and diversity of code datasets are pivotal factors influencing the success of collaborative trained code models. We demonstrate that federated learning achieves competitive performance compared to centralized training while offering better data protection, as evidenced by lower memorization ratios in the generated code. However, federated learning can still produce verbatim code snippets from hidden training data, potentially violating data privacy or copyright. Our study further explores the patterns of effectiveness and memorization in incremental learning, emphasizing the importance of the sequence in which individual participant datasets are introduced. Also, we identify the memorization phenomenon of cross-organizational clones as a prevalent challenge in both centralized and federated learning scenarios. Our findings highlight the persistent risk of data leakage during inference, even when training data remains unseen. We conclude with strategic recommendations for practitioners and researchers to optimize the use of multisource datasets, thereby propelling the cross-organizational collaboration forward.", "sections": [{"title": "1 Introduction", "content": "Large language models for code [15, 28, 58] automatically generate code snippets, functions, or entire programs based on given inputs, significantly enhancing developer productivity and aiding in software development [7, 44, 55]. Effective training of code generation models requires large and diverse source code datasets. However, reliance on open-source repositories is becoming increasingly unsustainable. For instance, StarCoder2 [20, 26] has been trained on a massive dataset aggregated from various platforms like GitHub and Kaggle, demonstrating that current models have nearly exhausted the available open-source training data. Moreover, open-source datasets pose significant risks, including the presence of vulnerable or malicious code and legal concerns related to the commercial use of copyleft-licensed code [39]. Studies on GitHub Copilot have shown that models can inherit vulnerabilities from unvetted code [27]. Given these concerns, there is a growing need to use collaborative approaches to explore the untapped value of proprietary (closed-source) code datasets from different organizations [11, 38].\nSeveral collaborative training methods are available, but privacy concerns remain a significant obstacle. Traditional centralized training is effective when data can be aggregated [40], but due to privacy concerns, such as sensitive information and legal constraints on data sharing, it becomes impractical [10, 24]. These challenges necessitate the exploration of privacy-preserving collaborative learning methods such as federated learning [25] and incremental learning [8]. Federated learning allows for collaborative training without centralizing data, enabling participants to maintain control over their private datasets [9, 19, 47]. Incremental learning, which updates models gradually with new data, offers a promising solution for dynamic environments where data continuously evolves [31].\nHowever, even though certain methods can protect privacy by ensuring data remains unseen during training, studies on data extraction attacks reveal that models can still leak training data due to memorization [3, 17, 50]. This is a significant concern that may discourage organizations from providing their private data for collaborative training [2, 56]. Moreover, preprocessing cross-organizational datasets poses a substantial challenge due to the unseen nature of other participants' data. One issue is the presence of cross-organizational clones, since code clones not only waste computing resources on duplicates but also increase the likelihood of these clones being memorized [50]. Ideally, the model should learn to generalize from the training data and develop the capability to generate new code, rather than reproducing the training data verbatim due to memorization. This is crucial because using verbatim generated code that is the same as some copyrighted code can lead to legal issues, as demonstrated by Oracle's lawsuit against Google. In this case, Google developed Android without a Java license and copied its APIs, resulting in a copyright infringement case over \"nine lines of code\"."}, {"title": "Key Research Question", "content": "The main objective of this paper is to better understand the promise and peril of collaborative training in the context of code generation task using several cross-organizational code datasets. This research aims to investigate a key question:\nHow do the effectiveness and memorization patterns vary in code models trained under different collaborative training settings?\nOur investigation underscores the critical impact of dataset size and diversity on the effectiveness of collaborative trained code models, where effectiveness is measured as the model's next token prediction ability and the correctness and utility of its generated code. We found that federated learning approaches yield results comparable to centralized training while maintaining data confidentiality during training and showing lower memorization rates during inference. Centralized training, however, tends to exhibit increased memorization, particularly with duplicate-heavy datasets. Both centralized and federated models showed higher memorization of cross-organizational clones than incremental models. Additionally, the effectiveness and memorization tendencies of incremental learning heavily depend on the order of participant datasets introduction. Crucially, our findings highlight the ongoing threat of data exposure during the inference stage, even without direct observation of the training data."}, {"title": "Main Contributions", "content": "We have conducted a comprehensive analysis of various collaborative training setups, assessing the impact of dataset size, diversity, and data presentation sequence on the effectiveness of these methods for code generation.\nTo the best of our knowledge, we are the first to systematically examine the phenomenon of training data memorization in various centralized, federated, and incremental learning settings, identifying the associated risks of training data leakage.\nOur findings provide actionable insights and recommendations for industry professionals and academic researchers, aiming to facilitate collaborative training practices, maximize the potential of extensive, multisource code repositories, and minimize the risk of code leakage. These insights ultimately urge the enhancement of privacy- and copyright-preserving capabilities of large code models while propelling cross-organizational collaboration forward."}, {"title": "Paper Structure", "content": "Section 2 details the methodology of our study. Section 3 describes the datasets we collected. Section 4 describes the experimental setup. Section 5 presents our evaluation results, analyzing model effectiveness and memorization patterns. Section 6 discusses the findings and threats to validity. Section 7 reviews related work. Section 8 concludes with key findings."}, {"title": "2 Methodology", "content": "In this section, we present our specific research questions (Section 2.1) and the workflow and tools employed to answer the questions. The workflow of our study is illustrated in Figure 1. It begins with the explanation of our dataset construction method (Section 2.2), followed by a description of the collaborative training methods we used to train models (Section 2.3). Subsequently, we outline the method and metrics used to evaluate the effectiveness of the trained models (Section 2.4). Finally, we detail the training data extraction techniques and memorization evaluation methods employed to assess the extent of data memorization (Section 2.5)."}, {"title": "2.1 Research Questions", "content": "We aim to investigate the promise and peril of collaborative training in the following research questions.\nRQ1. What factors most significantly impact the effectiveness of collaborative training methods for code generation models?\nMotivation. To enhance the practical utility of code generation models trained on diverse datasets from multiple organizations, it is essential to understand how different factors influence collaborative training methods. By exploring how the size and diversity of datasets, as well as the sequence of data presentation, impact the performance of these models, we can derive valuable insights. This research seeks to identify these factors to inform the development of collaborative training strategies-such as centralized training, federated learning, and incremental learning-that optimize model effectiveness and support their application in real-world scenarios.\nRQ2. To what extent is data from different participants memorized in various collaborative training settings?\nMotivation: Privacy concerns regarding the potential leakage of sensitive training data pose a significant barrier to organizational participation in collaborative training. Even with techniques like federated learning and incremental learning, which ensure that training data remains unseen during the training process, there remains a risk of data leakage through memorization during inference. Understanding how data from different participants is memorized and uncovering the memorization patterns can provide insights for improving collaborative training methods to mitigate memorization risks and enhance privacy or copyright preservation, thereby encouraging more organizations to engage in collaborative training and increasing the utility of valuable untapped proprietary datasets.\nRQ3. How are cross-organizational code clones memorized in collaborative models?\nMotivation: Collaborative training scenarios present unique challenges, particularly concerning cross-organizational code clones. While centralized training can efficiently remove these clones, federated learning and incremental learning prevent participants from performing cross-dataset checking and filtering. This limitation can lead to the persistence of cross-organizational clones. A higher occurrence of code clone snippets can increase the risk of unintentional verbatim code exposure [50]. For instance, clones might include licensed code reused properly within organizations, but if a model reproduces this code verbatim due to memorization, users might unknowingly misuse these clones, potentially violating licensing regulations. Additionally, the quality of the generated code could be compromised if these clones contain vulnerabilities. This RQ is to evaluate how these clones are memorized in code models trained under different collaborative settings, providing insights into memorization patterns and highlighting the need for specialized dataset preprocessing in collaborative training scenarios."}, {"title": "2.2 Dataset Construction Method", "content": "Our investigation on collaborative training naturally needs datasets from different participants or organizations. Although we cannot use real-world proprietary codebases, we can construct separate datasets from open-source code repositories to simulate multisource datasets for our evaluation.\nCross-Organizational Datasets Construction Approach. Due to the difficulty of obtaining proprietary code datasets from industry sources for collaborative training, our methodology involves collecting cross-organizational datasets from GitHub repositories while adhering to the following principles:\nEnsuring that the code in one dataset comes from a single organization while the code in different datasets comes from different organizations, simulating scenarios where each participant in a collaborative training setting has their own private codebase.\nLimiting the datasets to a single programming language to facilitate more consistent evaluation of effectiveness and memorization issues in the trained models.\nBased on these principles, our methodology involves curating Python code files from the open-source repositories of three prominent tech organizations hosted on GitHub: Facebook (F), Microsoft (M),"}, {"title": "2.3 Collaborative Training Methods", "content": "There are different ways to perform collaborative training using datasets from different participants or organizations. We summarize three common methods in Table 1: traditional centralized training, ideal for mutually trusting participants who combine datasets on a centralized location; and federated learning and incremental learning, able to train models in a decentralized manner. The latter two methods prevent dataset centralization, ensuring that the training data remains unseen during the training process, and consequently, to some extent, safeguard the privacy of the source data.\nIn terms of the synchronicity in the training process across different datasets, that is, whether in each training round (epoch) of a model, the data from all parties are involved in the training and contribute to the model's update, we classify the three methods into Synchronous Collaborative Training (e.g., centralized training and federated learning) and Asynchronous Collaborative Training (e.g., incremental learning with sequential dataset training).\nWe provide a detailed explanation of the three methods using a unified representation, to better illustrate the collaborative training approaches utilized in this study."}, {"title": "2.3.1 Dataset and Model Representation", "content": "Datasets. We use \\(D_i\\) to denote a dataset from a participant i. Given n participants, the centralized union of all their datasets is denoted as \\(D_c = \\cup_{i=1}^n D_i\\). In our study, we have three datasets \\(D_F, D_M, D_G\\) from Facebook, Microsoft, Google, respectively. Each data point in the dataset can be a Python code file, a Python class, or a function, optionally associated with some docstrings or comments. These datasets will be used in various ways to train various models for code generation tasks in our evaluation.\nModels. Our study focuses on models that are based on deep neural networks, as they have been shown to be effective for code generation tasks [53, 7, 26, 23]. We denote a model \\(M_i\\), together with its internal weights \\(\\Theta_i\\), potential inputs \\(X_i\\), and potential outputs \\(Y_i\\), as \\(Y_i = M_i(\\Theta_i, X_i)\\). There may exist ground-truth outputs \\(Y'_i\\) for the input \\(X_i\\), and a model trained on the ground-truth data should have adjusted its internal weights \\(\\Theta_i\\) so that the differences between \\(Y_i = M_i(\\Theta_i, X_i)\\) and \\(Y'_i\\) are minimized.\nIn our study, each participant i can individually train a model on its own dataset \\(D_i\\) as usual to minimize the differences between the \\(M_i(\\Theta_i, D_i)\\) and its ground truth \\(D'_i\\). When it comes to collaborative training, the settings need to be adjusted as follows."}, {"title": "2.3.2 Centralized Training", "content": "This training method is ideal when two participants share a profound mutual trust. In this approach, the participants train a common model using centralized datasets that combine information from all participants. That is, the method is to train a centralized model \\(M_C\\) so that the differences between \\(Y_C = M_C(\\Theta_C, D_C)\\) and \\(D'_c\\) are minimized, where \\(D_C = D_F \\cup D_M \\cup D_G\\)."}, {"title": "2.3.3 Federated Learning", "content": "This is a method for multiple participants to collaboratively train one central model as well while keeping their data localized [36]. This method enhances privacy and mitigates the risks associated with data centralization [47]. Its key idea is for each participant to calculate the updates needed for the central model weights using their own dataset locally and only share the weight updates with all the participants. Thus, a key component of federated learning is often the aggregation strategy used to aggregate weight updates from individual participants.\nIn our study, we applied two federated learning aggregation strategies, FedAvg [27] and FedYogi [35], to diversify our experimental settings. The FedAvg algorithm [27] simply averages the model weights updated by each participant to form the global model weights. It is often used for cases when datasets across parties are homogeneous. That is, FedAvg trains a model \\(M_{FedAvg} (\\Theta_{FedAvg}, X)\\) where X is unknown, and each participant locally trains a \\(M_i(\\Theta_i, D_i)\\), and \\(\\Theta_{FedAvg} = \\sum_{i=1}^n w_i \\cdot \\Theta_i\\), where \\(w_i\\) is the weight of the i-th participant's contribution which is often based on the size of \\(D_i\\). Note that the averaging operation is often done at the end of each training round (epoch). Also, to facilitate the averaging operation, it would be better for individual \\(M_i\\)s to have the same structure (e.g., the same numbers and positions of the weights).\nThe FedYogi algorithm [35] is similar to FedAvg, but adapts the Yogi optimizer [52] to adjust the model weights and the model learning rates for non-IID data [57] across participants during training. Thus, FedYogi is often used for cases when datasets across parties are heterogeneous."}, {"title": "2.3.4 Incremental Learning", "content": "This method involves gradual updates to a model with new datasets [43]. It is particularly useful in situations where the data evolves over time, allowing the model to adapt to the new data without being retrained from scratch [41], suitable for not only collaborative training, but also internal training with one organization. That is, it trains a sequence of models \\([M_1 (\\Theta_1, D_1), M_2 (\\Theta_2, D_2), ..., M_n(\\Theta_n, D_n)]\\) such that \\(\\Theta_{i+1}\\) are initialized with \\(\\Theta_i\\) but updated according to \\(D_{i+1}\\) without referring back to \\(D_i\\), and the last model \\(M_n\\) is often used as the final collaborative model \\(M_I\\). Note that, to facilitate the initialization of \\(\\Theta_{i+1}\\) from \\(\\Theta_i\\), it is often better for all models \\(M_i\\) to use the same structure. Also, the order of using \\([D_1, D_2, ...]\\) datasets can affect the trained models. In our study, we sequentially train various incremental models using our datasets in different orders. We use the order of the datasets used to train a model as the name of the model. For example, we use \\(M_{F2M2G}\\) to denote the model that is incrementally trained from the Facebook (\\(D_F\\)), Microsoft (\\(D_M\\)), and Google (\\(D_G\\)) codebases in that order."}, {"title": "2.4 Effectiveness Evaluation Method", "content": "This paper focuses on code generation tasks using collaborative models, which involves creating code snippets from prompts or specifications to enhance software development productivity. To evaluate the effectiveness of code generation models, we selected two primary metrics: perplexity and pass@k. These metrics provide a balanced assessment of the model's predictive capabilities and practical utility, making them the most suitable choice for RQ1 in our study.\nPerplexity: Evaluating Next-Token Prediction Ability. Perplexity measures the model's ability to predict subsequent tokens, ensuring syntactic correctness. Lower perplexity values correspond to improved predictive performance [13].\nPass@k: Evaluating Code Correctness and Utility. Pass@k for a model is defined as the probability that at least one of the top-k code samples generated by the model for a query problem passes the unit tests defined for the problem. Higher pass@k values indicate better performance in providing relevant and accurate code solutions [4]. For each trained model, this measurement is calculated using the EvalPlus [23] benchmark, which builds upon the HumanEval [4] benchmark. EvalPlus enhances the scope and robustness of HumanEval by incorporating a more diverse set of real-world coding problems."}, {"title": "2.5 Memorization Evaluation Methods", "content": "As our research goal is to investigate the memorization of each participant's training data, we adapt the data extraction strategies used by Al-Kaswan et al. [17], which formulates a targeted data extraction security game to extract data from models. In the targeted attack scenario, the adversary is provided with a prefix and is tasked with recovering the suffix associated with the prefix from the training data. Targeted attacks are more critical for security because they allow the extraction of specific information, such as sensitive configuration, personal identifiers, or proprietary algorithms [16, 22].\nPrompt Construction for Data Extraction Different from the setting in [17], our training data is available, which allows us to construct prefix prompts directly from each organization's dataset instead of from an identified extractable dataset [17]. For constructing prompts, we choose to use function signatures with docstrings as our \"prefix\" prompt. This format better reflects real-world scenarios where an adversary has access to an API's function signature and functionality description document and aims to extract the function's coding details in the function body.\nSpecifically, we use static analysis to parse the source code from the training data into abstract syntax trees (ASTs) to extract functions. Subsequently, two filtering conditions are applied to construct prefix prompts: each function must have a corresponding docstring, and the combined length of the tokenized function signature and docstring must not exceed 512 tokens. Memorization Detection. We can detect memorized data by comparing the similarity between the outputs generated by the models and the individual participants' datasets. The availability of the organizations' training dataset in our study allows us to easily make the comparison to check if there are duplications between the generated code snippets and the training datasets. We adapt the memorization detection technique from Yang et al. [50], which employs the Simian clone detection tools to detect Type-1 clones between the generated code and the training code. A Type-1 clone, or exact clone, refers to identical segments of code (with minimum six lines as the default setting in Simian). If the model produces these exact replicas, it strongly suggests memorization. Therefore, we classify such a clone as an instance of memorization.\nMemorization Evaluation. To better quantify the extent of training data memorization in the model-generated code, we introduce the Memorization Ratio, which is defined in the following. Given a set of specific prompts, the code model generates a set of code. Simian is then used to detect x distinct blocks of code that are identical to some blocks of code in a training dataset; these x blocks of code are considered as memorized code. The Memorization Ratio is then calculated by summing the numbers of lines within all the blocks and then dividing by the total number of lines in all the generated code. Mathematically, this can be represented as:\n\\(Mem. Ratio = \\frac{\\sum_{i=1}^x lines\\ of\\ code\\ in\\ memorized\\ block_i}{\\sum lines\\ of\\ code\\ in\\ all\\ generations}\\)"}, {"title": "3 Datasets", "content": "This section presents some characteristics of the datasets we collected from different organizations (Section 2.2) and performs some preprocessing for the following evaluation.\nCollecting Organization's Codebase. We utilized Google's Big-Query to collect all open-source licensed Python files from the GitHub database, resulting in a total of 27,128,930 files, amounting to 188.3 GB of data. Additionally, to identify the repositories on GitHub that belong to a certain organization, we manually identified some repositories' names that are very likely related to Google, Microsoft, and Facebook, and use them to extract organization' codebase. Preprocessing and Splitting. As there can be duplicate files or low-quality code in the codebases that may affect model training, we respectively preprocessed each dataset using methods employed in the training of the CodeParrot and PyCodeGPT models [53]. These methods are based on heuristics proposed by OpenAI's Codex [4] and have been further refined and enriched.\nCross-Org Codebase Characteristics. As shown in Table 4, we measured average metrics per megabyte, including lines of code (LOC), number of classes, number of functions, and number of docstrings across three datasets. Notably, there are discernible variations in these metrics among the datasets."}, {"title": "4 Experiment Setup", "content": "This section describes the specific experimental settings implemented to provide answers to each research question."}, {"title": "4.1 Base Model", "content": "To minimize interference from existing training data, we chose GPT-2 as our base model because it was trained on the WebText dataset [34], which includes substantial web data but not specifically GitHub code. This choice ensures that the Python dataset we collected from GitHub is relatively new to GPT-2's training data, reducing the potential memorization effect of the base model. Although there are other models that meet our requirements, we selected GPT-2 because it serves as the foundation for many widely used models, such as CodeParrot."}, {"title": "4.2 Collaborative Training", "content": "We conducted all collaborative training using the NVIDIA A100-PCIE-40GB GPU, which features 40GB of high-bandwidth memory.\nTraining Settings.\nFor centralized learning (CL), we aggregated the three codebases into a single dataset and trained the model for 10 epochs. Due to computational resource constraints, we set the training batch size to 2. For other hyperparameters, we followed the configurations used by CodeParrot\nFor federated learning (FL), we conducted a total of 10 rounds of training, with each client training on its own codebase for 1 epoch each during each round using the Flower federated learning framework. For the federated learning aggregation methods FedAvg and FedYogi, we utilized the default hyperparameters implemented in previous work [35] and maintained the same setup as centralized learning for each client's own training.\nFor incremental learning (IL), we considered all six distinct sequences for the three codebases: Facebook (F), Microsoft (M), and Google (G). The models were trained sequentially on each codebase for 10 epochs using the same hyperparameters as in the centralized setting.\nTrained Models. We obtained nine collaborative models from collaborative training: one centralized model (Centralized_FMG), two federated learning models (Federated_Avg_FMG and Federated_Yogi_FMG), and six incremental learning models (Incremental_SEQUENCE). The SEQUENCE represents the training order of the datasets from Facebook (F), Microsoft (M), and Google (G) (either F2M2G, F2G2M, M2F2G, M2G2F, G2F2M, or G2M2F). Additionally, we trained three baseline models, one for each dataset (Facebook_Only, Microsoft_Only and Google_Only) for comparison with various collaborative models."}, {"title": "4.3 Effectiveness Evaluation Settings for RQ1", "content": "The combined evaluation dataset, derived from the unseen validation datasets of all participants, is used to calculate the Perplexity score. To assess correctness and utility, we estimated the pass@k metric in the EvalPlus benchmark with n_samples = 200, following the settings of previous work [4], performing sampling with temperatures ranging from 0.1 to 1.0, and selecting the optimal value for each metric, as outlined in earlier research [53]."}, {"title": "4.4 Memorization Evaluation Settings for RQ2", "content": "Prompt Construction. We followed the prompt construction method from Section 2.5, extracting all functions and selecting signatures and docstrings of appropriate lengths as prompts. Given the large number of prompts obtained, we randomly sampled 10% of the function prompts from each codebase for code generation. The outcomes of our prompt construction are presented in Table 6.\nData Extraction and Memorization Evaluation. To ensure efficient data extraction, we set the temperature to 0.6 and the top-p (nucleus sampling) to 0.6, following best practices outlined by Yu et al. [51], which assess various techniques for enhancing the training data extraction process from language models. Additionally, we configured the number of generations per prompt to 5 and limited the maximum number of newly generated tokens to 512. For the memorization evaluation, we followed the methods described in Section 2.5, using the Simian tool with a default threshold of a minimum of 6 lines of code to report Type-1 clones, which are considered instances of memorization."}, {"title": "4.5 Cross-Org Clone Memorization Evaluation Settings for RQ3", "content": "Collecting Cross-Organizational Clones. To evaluate how such clones are memorized in collaborative models, we first identified the clones within the training datasets using the Simian tool, applying a default threshold of six consecutive lines to define a clone. Since these clones may not be complete functions and there are no function headers or docstrings, we need to construct prompts for the clones differently from Section 2.5: we chose to use the first half of a clone snippet as prefix prompts, and fed them to the models to generate the rest of the code. For particularly long clones exceeding the 1024-token limit of GPT-2 after tokenization, we split them into smaller portions before creating the prefixes and suffixes.\nFirst, we detected cross-organizational clones that are common across all three training data. However, due to the significant size disparity among these datasets-19.34 MB for Facebook, 327.47 MB for Microsoft, and 501.77 MB for Google, there were only 41 common clones. The limited number of prefix prompts for common clones led to inconclusive results. To enhance the evaluation of cross-organizational clones and obtain more robust and evaluable samples, we focused only on the Microsoft and Google datasets. These two datasets are larger, allowing for more clones. We identified 316 common clones between Microsoft and Google, encompassing a total of 7,536 lines. To manage extra-long clones, we divided them into smaller portions to ensure the tokenized lengths of the prefixes were under 512 tokens. This process resulted in 349 prefix prompts, providing a more substantial basis for evaluation.\nModel Training with Two Datasets Only. To better evaluate the memorization of cross-organizational clone from Microsoft"}, {"title": "5 Empirical Evaluation Results", "content": "5.1 Effectiveness of Collaborative Models\nThe evaluation results for RQ1: What factors most significantly impact the effectiveness of collaborative training methods for code generation models? are shown in Table 8."}, {"title": "Baseline Models", "content": "The GPT-2 base model, not specifically trained on the provided datasets, showed poor performance with a high perplexity score and 0% pass rates across all k values in the Pass@k metric. Among the individual dataset models, the Google_Only model exhibited the best performance, underscoring the importance of a larger dataset size for better model effectiveness."}, {"title": "Synchronous Collaborative Settings", "content": "In synchronous collaborative training settings, the Centralized_FMG model demonstrated the best performance in next token prediction ability with the lowest perplexity score of 3.32. The two federated models, Federated_Avg_FMG and Federated_Yogi_FMG, also achieved comparable perplexity scores of 3.71 and 4.02, respectively. Overall, both centralized and federated models outperformed the incremental models in the perplexity metric. For the pass@k metric, federated learning models, particularly Federated_Avg_FMG and Federated_Yogi_FMG, surprisingly surpassed the Centralized_FMG model. This indicates that federated learning approaches can achieve effectiveness comparable to centralized training while keeping training data private."}, {"title": "Asynchronous Collaborative Settings", "content": "For asynchronous collaborative training settings, the effectiveness varied significantly based on the order in which datasets were used. The Incremental_F2M2G model performed the best among incremental models, suggesting that starting with smaller datasets and sequentially adding larger ones might be beneficial."}, {"title": "Summary of Findings for RQ1", "content": "Our evaluation underscores the importance of dataset size, diversity, and the order of data introduction in collaborative training. Federated learning emerged as a promising method, balancing privacy and performance better. However, the variability in effectiveness for incremental learning models highlights the need for careful planning and strategy when introducing datasets sequentially."}, {"title": "5.2 Memorization in Collaborative Models", "content": "The evaluation results for RQ2: To what extent is data from different participants memorized in various collaborative training settings? are presented in Table 9 and Table 10.\nFrom a Dataset Perspective. The Microsoft training data shows higher memorization compared to the other two datasets across different collaborative models. For example, in the Centralized_FMG model and the two incremental models ending with Microsoft datasets (Incremental_F2G2M and Incremental_G2F2M), the memorization ratios are 6.353%, 7.169%, and 8.130%, respectively. This can be attributed to intrinsic differences among the datasets. As illustrated in Table 4 and Figure 2, these datasets vary significantly in terms of internal duplicates, average file size, and the number of docstrings and functions. The high number of internal duplicates in the Microsoft dataset leads to increased memorization ratios across models. This aligns with the findings of Yang et al. [50], which indicate that frequently occurring code snippets in the training data are more likely to be memorized.\nFrom a Model Perspective. The ranked memorization ratios in Table 10 indicate that the models Incremental_G2F2M, Incremental_F2G2M, and Centralized_FMG exhibit the highest overall memorization ratios, significantly higher than others. By examining Table 9, it becomes clear that this high memorization is largely due to their substantial retention of the Microsoft dataset, which contains a higher level of internal duplicates. Furthermore, when examining other incremental learning settings, it becomes evident that all models exhibit the highest memorization ratio for the last dataset they trained on. This trend is concerning for collaboration, as it suggests that the final dataset in the training sequence is memorized at a disproportionately higher ratio, increasing the risk for the last participants. Such a pattern raises significant concerns for participants considering the use of incremental learning settings, as the final participants might face greater risks of data leakage and privacy issues.\nNotably, our experiments reveal that both federated learning methods, FedAvg, which aggregates weights from different participants, and Yogi, which adaptively adjusts the learning rate for non-IID datasets, exhibit relatively low levels of memorization across training datasets. This highlights federated learning as a promising approach for collaborative training, as it protects privacy better by keeping training data unseen during the training phase and maintains low memorization ratios during inference, all while achieving performance comparable to centralized models."}, {"title": "Summary of Findings for RQ2", "content": "Our evaluation reveals that datasets with a higher number of internal duplicates exhibit greater memorization in collaborative training. Centralized models demonstrate relatively high memorization ratios, whereas incremental learning settings display unstable memorization ratios, heavily influenced by their training sequence, with the last dataset in the sequence being memorized at a disproportionately higher ratio. Federated learning methods, such as FedAvg and Yogi, maintain a relatively low level of memorization for training data, highlighting their promise for collaborative training."}, {"title": "5.3 Cross-Org Clones Memorization Evaluation", "content": "The evaluation results for RQ3: How are cross-organizational clones memorized in collaborative models? are presented in Table 11. Based on our observations, it is evident that in Synchronous Collaborative Training settings, which include both Centralized Training and Federated Learning, models tend to exhibit higher memorization ratios for cross-organizational clones. This can be attributed to the repetitive learning of these clones during each weight update across multiple datasets. Specifically, the Centralized model shows a memorization ratio of 0.565%, while the Federated_Avg_MG and Federated_Yogi_MG models demonstrate memorization ratios of 0.552% and 0.480%, respectively. In contrast, cross-organizational clones are memorized at a relatively lower ratio in incremental learning settings. We believe this is due to catastrophic forgetting [37], where a model trained sequentially on different tasks or datasets tends to overwrite the knowledge gained from previous tasks with new information from the current task. Consequently, incremental learning models exhibit a lower memorization ratio than their synchronous counterparts, with the Incremental_G2M model having the lowest memorization ratio at 0.192%, followed by the Incremental_M2G model at 0.246%.\nOur findings indicate that memorization of cross-organizational clones is relatively higher in centralized and federated settings. Notably, in the context of federated learning, participants are restricted to processing their own datasets, making it challenging to reduce cross-organizational clones. Redundant training on these clones not only wastes valuable computing resources but also leads to unbalanced feature learning and increases the risk of memorization. This highlights a critical need in federated learning to deduplicate clones across distributed datasets. Addressing this issue is essential to ensure the trustworthiness of collaborative models."}, {"title": "Summary of Findings for RQ3", "content": "Our evaluation of cross-organizational clones in collaborative training settings revealed that synchronous methods, such as centralized training and federated learning, exhibit higher memorization ratios of cross-organizational clones than asynchronous methods like incremental learning. This underscores the need for effective preprocessing strategies in collaborative training scenarios to handle cross-organizational clones, ensure balanced feature learning, optimize computational resources, and mitigate memorization, especially when datasets are decentralized and access to them is restricted."}, {"title": "6 Discussion", "content": "6.1 Suggestions to Practitioners\nFor practitioners, it is crucial to focus on the size, diversity, and internal duplicates of datasets. A diverse, well-preprocessed dataset can significantly enhance performance and reduce memorization"}]}