{"title": "A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis", "authors": ["Changzhi Zhou", "Dandan Song", "Yuhang Tian", "Zhijing Wu", "Hao Wang", "Xinyu Zhang", "Jun Yang", "Ziyi Yang", "Shuhao Zhang"], "abstract": "Recently, Large Language Models (LLMs) have garnered increasing attention in the field of natural language processing, revolutionizing numerous downstream tasks with powerful reasoning and generation abilities. For example, In-Context Learning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box LLMs to execute downstream tasks by analogy learning without any fine-tuning. Besides, in a fine-tuning-dependent paradigm where substantial training data exists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods, enable LLMs to achieve excellent performance comparable to full fine-tuning.\nHowever, these fascinating techniques employed by LLMS have not been fully exploited in the ABSA field. Previous works probe LLMs in ABSA by merely using randomly selected input-output pairs as demonstrations in ICL, resulting in an incomplete and superficial evaluation. In this paper, we shed light on a comprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8 ABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation to unify \"multiple LLMs for multiple ABSA subtasks in multiple paradigms.\" For the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using instruction-based multi-task learning. For the fine-tuning-free paradigm, we propose 3 demonstration selection strategies to stimulate the few-shot abilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a new state-of-the-art performance compared to fine-tuned Small \u00b9 Language Models (SLMs) in the fine-tuning-dependent paradigm. More importantly, in the fine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still showcase impressive potential and even compete with fine-tuned SLMs on some ABSA subtasks.", "sections": [{"title": "1 Introduction", "content": "Aspect-Based Sentiment Analysis (ABSA) is the fine-grained Sentiment Analysis (SA) task that attracts widespread attention from academia and industry due to its practical applications (Liu 2012; Pontiki et al. 2016). Typically, ABSA involves four sentiment elements: aspect term, aspect category, opinion term and sentiment polarity (Zhang et al. 2023b). For example, in the sentence shown in Figure 1, the four corresponding sentiment elements are {burger, orange juice}, {food quality, food quality}, {delicious, not good}, and {positive, negative}. Based on the types of these elements in the input and output, ABSA can be divided into different subtasks.\nMany previous studies tailor complicated models for a single subtask. For example, Fan et al. (2019) propose a target-fused neural sequence labeling method for the AOE subtask, and Peng et al. (2020) propose a two-stage framework, first extracting elements and then identifying relations, for the ASTE subtask. However, this type of work struggles to generalize to other subtasks. Subsequently, some works fine-tune sequence-to-sequence models to unify multiple subtasks. BARTABSA (Yan et al. 2021) pioneers using a BART-base model (Lewis et al. 2020) to accomplish seven subtasks by reframing each subtask target as the generation of indexes. Paraphrase (Zhang et al. 2021a) based on the T5-base model (Raffel et al. 2020) unifies multiple subtasks by linearizing sentiment tuples to a natural language sequence. These crafted Small Language Models (SLMs) obtain state-of-the-art performance when fine-tuned with ample data. However, many real-world applications involve low-resource scenarios, such as narrow domains or complex tasks, where data annotation is time-consuming and labor-intensive. SLMs often ineffective in these data-scarce settings (Zhang et al. 2023b).\nRecently, Large Language Models (LLMs) have demonstrated powerful abilities due to their large number of parameters and extensive pre-training data (Zhao et al. 2023b). On the one hand, In-Context Learning (ICL), one of the emergent abilities of LLMs, pioneers a new fine-tuning-free paradigm (Brown et al. 2020; Dong et al. 2023). By providing demonstrations consisting of input-output examples, LLMs can learn by analogy like humans without any fine-tuning. When faced with fine-tuning-dependent paradigm with sufficient data, on the other hand, Parameter-Efficient Fine-Tuning (PEFT) methods (Han et al. 2024) allow LLMs to obtain powerful performance on downstream tasks in a cost-effective manner. With these innovative technologies, LLMs have revolutionized many tasks in the field of natural language processing (Zhao et al. 2023b). However, the majority of existing ABSA research still focuses on SLMs (Zhang et al. 2023b). Some recent works (Zhang et al. 2023a; Amin et al. 2023; Wang et al. 2024) conduct preliminary evaluation of LLMs for ABSA, but they are not comprehensive or in-depth. These evaluations either focus on a single ABSA subtask, a single LLM, or a single demonstration selection strategy in ICL.\nIn order to achieve a comprehensive evaluation of LLMs for ABSA subtasks, in this paper, we conduct extensive experiments on 13 datasets of 8 ABSA subtasks by fine-tuning open-source LLMs (fine-tuned LLMs) or calling out-of-the-box LLMS (API-based LLMs). The former aims to explore whether LLMs can surpass SLMs in the fine-tuning-dependent paradigm. The latter studies the zero-/few-shot abilities of LLMs and the effectiveness of demonstration selection strategies of ICL in the fine-tuning-free paradigm, where SLMs are ineffective. Concretely, we integrate existing ABSA datasets and propose an unified task formulation to unify \"multiple LLMs for multiple ABSA subtasks in multiple paradigms\". We use instruction-based multi-task learning and low-rank adaptation (LoRA), a computationally cost-friendly method with performance comparable to full fine-tuning (Hu et al. 2022), to efficiently fine-tune open-source LLMs. Besides, we construct three demonstration selection strategies, random-based selection, keyword-based selection and semantic-based selection, to unlock the potential of ICL.\nOur experimental results and analysis yield the following insightful conclusions: Firstly, in the fine-tuning-dependent paradigm, LLMs outperform full fine-tuned SLMs on all ABSA subtasks even with minimal parameter fine-tuning (e.g. LLaMA3-8B with 3.4M vs. T5-base with 220M). More importantly, in the fine-tuning-free paradigm where SLMs fail completely, API-based LLMs with ICL achieve remarkable performance, even matching that of fine-tuned SLMS in some subtasks. This sheds light on future ABSA research in low-resource scenarios. Besides, the keyword and semantic information in ICL can significantly improve the performance of LLMs, and combining these two orthogonal types of information can lead to better effectiveness. Finally, we have novel findings that the effectiveness of ICL varies depending on the specific ABSA subtask and LLMs used. In some cases, ICL can even cause adverse effects. More conclusions and analysis can be found in the Experiments section. The contributions of this paper are as follows:\n1) We thoroughly evaluate the performance of LLMs in the ABSA field, involving 13 datasets, 8 subtasks, and 6 LLMs, achieving \u201cmultiple LLMs for multiple ABSA sub-tasks in multiple paradigms with an unified formulation.\u201d\n2) We demonstrate that efficiently fine-tuned LLMs using instruction-based multi-task learning can comprehensively outperform fine-tuned SLMs in the fine-tuning-dependent paradigm.\n3) We study different demonstration selection strategies and greatly improve the performance of API-based LLMs in the fine-tuning-free paradigm, where SLMs fail completely.\nIn summary, our work demonstrates that, regardless of the paradigm, LLMs always outperform SLMs. We hope that the comprehensive LLM-based baselines established by this paper will promote the development of LLMs in the ABSA field."}, {"title": "2 Related Work", "content": "Aspect-Based Sentiment Analysis\nAspect-Based Sentiment Analysis (ABSA), a fine-grained sentiment analysis problem, attracts considerable attention owing to its practicability (Liu 2012; Pontiki et al. 2014; Zhang et al. 2023b). Previous works (Peng et al. 2020; Wan et al. 2020; Cai, Xia, and Yu 2021; Zhang et al. 2022; Zhou et al. 2024) aim to develop specialized models tailored for separate ABSA subtasks, which limits their scope of application. Therefore, more recent works attempt to unify multiple ABSA subtasks, namely \"one model for all subtasks\" (Wang, Xia, and Yu 2024). Concretely, BARTABSA (Yan et al. 2021) employs the BART model and the pointer mechanism (Vinyals, Fortunato, and Jaitly 2015) to convert extraction and classification tasks into generating pointer indexes and class indexes. Mao et al. (2021) develop a BERT-based machine reading comprehension framework with a dual structure (Dual-MRC). Zhang et al. (2021b,a); Mao et al. (2022) formulate ABSA sub-tasks as a text-generation problem to utilize label information and the power of the T5 model. Furthermore, LEGO-ABSA (Gao et al. 2022) propose a T5-based unified generative framework that solves multiple subtasks simultaneously by multi-task learning. MVP (Gou, Guo, and Yang 2023) provides element order prompts to direct the T5 model in generating numerous tuples, each with a distinct element order. UnifiedABSA (Wang, Xia, and Yu 2024) decouples the quadruple labels and designs task-specific instructions.\nLarge Language Models for ABSA\nThe development of Large Language Models (LLMs) has revolutionized the field of natural language processing, such as Retrieval-Augmented Generation (Gao et al. 2024) and AI Agents (Guo et al. 2024). LLMs have also received extensive attention in traditional information extraction tasks such as Named Entity Recognition and Relation Extraction (Wadhwa, Amir, and Wallace 2023; Li, Wang, and Ke 2023; Wang et al. 2023b; Xu et al. 2024). However, research on LLMs"}, {"title": "3 Methodology", "content": "Unified Task Formulation\nWe unify various subtasks into a list generation pattern by designing various prompt and output for LLMs. This pattern can be used for API-based LLMs and find-tuned LLMs simultaneously. As shown in Figure 2, the prompt (X) includes:\nInstruction (I): We design unique instructions for each ABSA subtask, guiding LLMs on what subtask to complete and which sentiment element to return.\nDemonstrations (D): Several input-output pairs compose demonstrations, from which LLMs can learn input-output mapping rules. Unless otherwise indicated, demonstrations are enabled solely in the fine-tuning-free paradigm.\nTested sample (T): It includes a sentence and a aspect term in ALSC and AOE subtasks and only a sentence in other subtasks.\nThe output (Y) is two-dimensional list of sentiment elements corresponding to the particular subtask."}, {"title": "Demonstration Selection Strategy", "content": "To explore the differences between different demonstrations and stimulate the few-shot abilities of LLMs, we utilize three selection strategies as follows:\nRandom-based selection: It is a vanilla demonstration selection strategy.\nKeyword-based selection: BM25 (Robertson and Zaragoza 2009) is a sparse retrieval function. It calculates the relevance score between sentences based on term frequency (TF), inverse document frequency (IDF) and sentence length. Therefore, the sentence pairs retrieved by BM25 share common keywords, which may be aspect terms or opinion terms to be extracted.\nSemantic-based selection: SimCSE (Gao, Yao, and Chen 2021) is a popular semantic-based dense retrieval model, which obtains sentence embeddings by leveraging contrastive learning. We use it to select semantically similar demonstrations."}, {"title": "4 Experiments", "content": "Experimental Setup\nDatasets and Metrics We evaluate all models on 13 datasets over 8 ABSA subtasks. All of the datasets originate from the Semeval Challenges (Pontiki et al. 2014, 2015, 2016). Detailed statistics of datasets are shown in Table 1. Following Yan et al. (2021), we leverage D17 annotated by Wang et al. (2017) for AE, OE and ALSC subtasks, D19 annotated by Fan et al. (2019) for AOE subtask, D20 annotated by Peng et al. (2020) for AESC, AOPE and ASTE subtasks, and D21 of the ASQP subtask come from Zhang et al. (2021a). Besides, to efficiently fine-tune LLMs using instruction-based multi-task learning, we combine all datasets as in previous work (Gou, Guo, and Yang 2023). Concretely, we combine all training sets and validation sets, and delete samples that overlap with any test sets. Then we split the data by 9:1 to obtain the final training and validation sets. In order to ensure a fair comparison with baselines, we use the F1 score (%) as the evaluation metric.\nImplement Details We use rank_bm25 library to execute BM25 retrieval, and use sup-simcse-roberta-large to execute Sim-CSE retrieval. We use three-shot as default ICL setting, and utilize LLAMA-Factory (Zheng et al. 2024), a easy and efficient LLM fine-tuning library \u00b3, to fine-tune all open source LLMs with LoRA. All weights of LLMs come from HuggingFace \u2074. All experiments run on an Ubuntu Server with 8 Nvidia RTX 3090 GPUs.\nBaselines (1) Full Fine-tuned SLMs: A variety of methods have been proposed for different ABSA subtasks (Zhang et al. 2023b). Here, we list state-of-the-art methods that target one or more of these subtasks: Dual-MRC (Mao et al. 2021), DCRAN (Oh et al. 2021), BARTABSA (Yan et al. 2021), LEGO-ABSA (Gao et al. 2022), TAGS (Xianlong, Yang, and Wang 2023), GenDA (Wang et al. 2023a), and MVP (Gou, Guo, and Yang 2023). Besides, we conduct instruction-based multi-task fine-tuning on the T5-base model using unified task formulation mentioned in the Methodology section, namely T5-Instruct. (2) Efficient Fine-tuned LLMs: We use LoRA to efficiently fine-tune the following open-source LLMs: LLaMA3-8B \u2075 (MetaAI 2024), ChatGLM3-6B (Du et al. 2022; Zeng et al. 2023), QWen1.5-7B (Bai et al. 2023), and Mistral-7B-v0.2 (Jiang et al. 2023). (3) API-based LLMs: We evaluate the zero-shot and few-shot abilities of the following LLMs by calling the APIs: LLaMA3-70B, ChatGPT (OpenAI 2022) (GPT-3.5-Turbo-0125), and GPT4 (OpenAI 2023) (GPT-4-Turbo).\nMain Results\nDue to space constraints, we divide ABSA subtasks into three groups, as shown in Tables 2, 3 and 4. These tables show the F1 scores of SLMs and LLMs under the fine-tuning-dependent paradigm, as well as the F1 scores of LLMs under the fine-tuning-free paradigm. The results for SLMs without fine-tuning are not displayed due to their ineffectiveness in all subtasks, with F1 scores close to 0.\nFull Fine-tuned SLMs vs. Efficient Fine-tuned LLMs\nWe efficiently fine-tune four open source LLMs with LoRA by instruction-based multi-task learning. Thanks to the powerful natural language understanding and generation abilities, LLMs achieve state-of-the-art performance with only fine-tuning minimal parameters (e.g., LLaMA3-8B with 3.4M vs. T5-Instruct with 220M). The sentiment analysis and structured extraction abilities stimulated by LoRA enable LLMs outperform SLMs 2.87, 4.67, and 2.39 points across three groups of subtasks, respectively. This indicates that LLMs are a better choice than SLMs when are fine-tuned with sufficient data.\nZero-shot and Few-shot Abilities of LLMS SLMs are ineffective in the fine-tuning-free paradigm due to their weak natural language understanding and instruction-following abilities. In contrast, LLMs exhibit surprisingly strong zero-shot performance, even rivaling fine-tuned SLMs in the ALSC subtask. This demonstrates that LLMs can comprehend the nature of a subtask and achieve it based on instructional descriptions. When using in-context learning (ICL), the performance of LLMs further improves, indicating they can learn the annotation rules of a subtask from demonstrations. Besides, GPT4 \u2076, as the currently strongest LLM, remains unbeatable. Overall, the performance of LLMs with ICL demonstrates that they can be effectively utilized in low-resource fine-tuning-free scenarios.\nDemonstration Selection Strategies The improvement brought by random demonstrations is limited. Even these demonstrations may cause negative effect, as shown by the underlined entries in Tables 2 and 3. This is because there is no correlation between the selected demonstrations and the tested samples. In contrast, the selection strategies based on BM25 and SimCSE can achieve comparable performance enhancements, and both outperform the random strategy. This indicates that the importance of keyword and semantic information is roughly equivalent. We show an example of two selection strategies for the ASTE subtask in Figure 3. As mentioned earlier in the Methodology section, BM25 can"}, {"title": "5 Conclusion", "content": "This paper evaluates the performance of 6 LLMs on 13 datasets of 8 ABSA subtasks, and unify \u201cmultiple LLMs for multiple subtasks in multiple paradigms.\" Comprehensive experiments demonstrate that the performance of LLMs is better than that of SLMs, regardless of fine-tuning. For fine-tuning-dependent paradigm, fine-tuned LLMs with LoRA obtain cost-effective and state-of-the-art performance, surpassing SLMs on all subtasks. For fine-tuning-free paradigm, although it is not yet comparable to fine-tuned models, LLMs with ICL shed light on the possibility of achieving ABSA in low-resource scenarios. Besides, the effectiveness of ICL varies across different ABSA subtasks, LLM, and demonstration selection strategies. In rare cases, ICL may fail. In summary, our comprehensive study demonstrates that the overall success of LLMs over SLMs."}]}