{"title": "LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with Class Taxonomies", "authors": ["Jia Shi", "Gautam Gare", "Jinjin Tian", "Siqi Chai", "Zhiqiu Lin", "Arun Vasudevan", "Di Feng", "Francesco Ferroni", "Shu Kong"], "abstract": "We tackle the challenge of predicting models' Out-of-Distribution (OOD) performance using in-distribution (ID) measurements without requiring OOD data. Existing evaluations with \"Effective Robustness\", which use ID accuracy as an indicator of OOD accuracy, encounter limitations when models are trained with diverse supervision and distributions, such as class labels (Vision Models, VMs, on ImageNet) and textual descriptions (Visual-Language Models, VLMs, on LAION). VLMs often generalize better to OOD data than VMs despite having similar or lower ID performance. To improve the prediction of models' OOD performance from ID measurements, we introduce the Lowest Common Ancestor (LCA)-on-the-Line framework. This approach revisits the established concept of LCA distance, which measures the hierarchical distance between labels and predictions within a predefined class hierarchy, such as WordNet. We assess 75 models using ImageNet as the ID dataset and five significantly shifted OOD variants, uncovering a strong linear correlation between ID LCA distance and OOD top-1 accuracy. Our method provides a compelling alternative for understanding why VLMs tend to generalize better. Additionally, we propose a technique to construct a taxonomic hierarchy on any dataset using K-means clustering, demonstrating that LCA distance is robust to the constructed taxonomic hierarchy. Moreover, we demonstrate that aligning model predictions with class taxonomies, through soft labels or prompt engineering, can enhance model generalization.", "sections": [{"title": "1. Introduction", "content": "Generalizing models trained on in-distribution (ID) data to out-of-distribution (OOD) conditions is a notoriously difficult task. Distribution shifts undermine the independent and identically distributed (IID) assumption between training and testing data, challenging the model's robustness. Numerous OOD datasets have been proposed to study the effects of different interventions, such as temporal shifts (Hu et al., 2022; Lomonaco & Maltoni, 2017; Lin et al., 2021), artificial noise (Hendrycks & Dietterich, 2019; Arjovsky et al., 2019; Larochelle et al., 2008), and natural distribution shifts (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Barbu et al., 2019; Recht et al., 2019). Maintaining model robustness becomes significantly more difficult with severe visual shifts in the image domain. However, many studies evaluate generalization on OOD datasets with limited visual shifts or only involve artificial noise, such as ImageNet-v2 or ImageNet-C (Recht et al., 2019; Arjovsky et al., 2019). Such datasets fail to fully reflect a model's generalization capability when confronted with severe distribution shifts (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Barbu et al., 2019), as there is often limited transfer of robustness from synthetic to natural distribution shifts (Taori et al., 2020).\nIn the realm of model generalization, numerous attempts have been made to predict a model's performance on OOD datasets based on in-distribution measurements, following the concept of effective robustness (Taori et al., 2020). These approaches, referred to as 'X-on-the-line' (Miller et al., 2021; Baek et al., 2022), suggest that a model's OOD performance is correlated to in-distribution accuracy (Miller et al., 2021; Recht et al., 2019; Miller et al., 2020; Roelofs et al., 2019) or models consensus on in-distribution accuracy (Jiang et al., 2021; Baek et al., 2022).\nMoreover, several prior attempts rely on domain generalization strategies that necessitate prior knowledge of the target domain or require an estimation of OOD domain information (Chen et al., 2021; Li et al., 2022a). These can lead to computationally intensive processes, particularly when involving multiple models or inferences (Baek et al., 2022; Deng et al., 2022).\nMost prior research has focused solely on estimating generalization among vision models (VMs) supervised on class labels trained on ImageNet (Taori et al., 2020; Mustafa et al., 2020). Emerging large-scale Vision-Language Models (VLMs) trained on datasets like LAION demonstrate exceptional generalization performance on out-of-distribution (OOD) data. However, as shown on the left plot of Fig. 1, existing evaluation (Miller et al., 2021) using ID accuracy fail to explain the effective robustness (Taori et al., 2020) gap between VMs and VLMs. This underscores the necessity to evaluate and compare models across different families under a unified evaluation framework. Recently, (Shi et al., 2023) observed the same problem and proposed evaluating OOD accuracy using multiple ID test sets, but their method requires multiple evaluation runs.\nUnlike VMs, VLMs leverage more diverse training data, contrastive loss, and language supervision. There have been attempts to measure VLM generalization (HaoChen et al., 2021; Fang et al., 2022; Schuhmann et al., 2022; Kaur et al., 2022), specifically suggesting that diversity in training data is an indicator of model generalization. However, it is non-trivial to measure data diversity, and even collect and train on such large-scale diverse data (Schuhmann et al., 2022).\nPrior attempts lack a unified, simple measurement for both VMs and VLMs to explain model generalization and convert it into actionable improvements. To address the issues of (1) lack of unified metrics for VLMs and VMs, or models trained on different data sources; (2) need for robustness to large domain shifts; (3) desire for computationally efficient metrics, we propose adopting the Lowest Common Ancestor (LCA) distance to measure model generalization. The LCA distance is the taxonomic distance between labels and predictions, given a predefined class hierarchy, such as WordNet. Through a series of empirical experiments involving 75 models (36 VMs and 39 VLMs) (cf. Fig. 2), we show that the in-distribution LCA distance strongly correlates with multiple ImageNet-OOD datasets under severe visual shifts (cf. Fig. 1 right plot). This finding may help explain the surprising result that zero-shot vision-language models with poor top-1 accuracy generalize better to novel datasets compared to state-of-the-art vision models. This spurs us to further investigate and discuss the potential of the LCA benchmark for improving model generalization. We also discuss the suitability of LCA as a generalization indicator in Section 3."}, {"title": "2. LCA Distance Measures Misprediction Severity", "content": "We propose using the in-distribution Lowest Common Ancestor (LCA) distance, also known as taxonomy loss, as a predictor for model generalization. Here, we formally define how taxonomy loss can be measured using in-distribution data. Taxonomy loss measures the class ranking difference between a model's prediction based on class likelihood, and a predefined class order encoded by class taxonomy. Lower taxonomy loss is expected when a model assigns higher likelihood to classes that are semantically closer to the ground-truth class, in other words, 'making better mistakes' (Bertinetto et al., 2020; Peri et al., 2023). For example, if a cat image is predicted as a dog by model-A and as a car by model-B, model-A would have a lower LCA distance as it makes a better mistake than model-B. Following previous research (Bertinetto et al., 2020; Deng et al., 2009b), we use WordNet (Miller et al., 1990), a large-scale lexical database inspired by psycholinguistic theories of human lexical memory (Miller, 1995), to encode class taxonomy. The WordNet taxonomy is well suited for the widely used ImageNet dataset which builds on WordNet. An example of LCA distance is shown in Fig 3.\nGiven two classes, $y$ (the ground-truth class) and $y'$ (the prediction class), we define the LCA distance according to (Bertinetto et al., 2020) as\n$D_{LCA}(y', y) := f(y) - f(N_{LCA}(y, y'))$\nwhere $f(y) \\ge f(N_{LCA}(y, y'))$ and $N_{LCA}(y', y)$ denotes the lowest common ancestor class node for classes $y$ and $y'$ within the hierarchy, and $f(\\cdot)$ represents a function of a node,"}, {"title": "3. Discussion: The Suitability of LCA as a Benchmark for Model Generalization", "content": "This section explores the hypothesis linking LCA distance with a model's generalization ability and discusses how these insights can be meaningfully and actionably applied.\nOur primary motivation is to use class hierarchy to capture correlation invariances across training environments, as proposed in the seminal work on 'invariant risk minimization' (Arjovsky et al., 2019). Since the class hierarchy remains consistent across both ID and OOD datasets, it can serve as a surrogate measure of the model's invariant features. Models that generalize well to OOD datasets typically learn universal or non-spurious features from the training dataset that are transferable to OOD datasets (Makar et al., 2022). Such models are more likely to misclassify an ostrich as another bird rather than a lion. These taxonomy-based mispredictions, quantified using the LCA distance, are shown to be a better indicator of a model's OOD performance in this work.\nObstacles to Model Generalization. In deep learning, models often learn predictive features from images by creating discriminative associations to class labels. This approach is susceptible to spurious correlations in the training data (Sturm, 2014; Torralba & Efros, 2011; Jabri et al., 2016). For instance, a model might erroneously associate the class 'ostriches' with the feature 'green grass' in the background, as ostriches often appear in grasslands. These correlations may fail when applied to an OOD dataset that only depicts the semantic concept of 'ostriches' (Zhang et al., 2021).\nEssentials for Model Generalization. ImageNet-R is a severely shifted OOD dataset where, despite significant distribution shifts, humans can effortlessly identify the correct classes. This is because humans can discern stable features across environments. A model's generalization capability depends on the transferability of the associations learned during training. As benchmarks often simulate human-world ontology, ideally, only features that align with human understanding of object semantics are universally transferable to any constructed OOD dataset. This underscores the importance of identifying transferable features aligning ontology that contribute to robust model generalization.\nHow can we measure what features a model has learned as predictive during training? The decision-making process of deep neural networks trained end-to-end has become less interpretable. While there have been attempts to decipher this process by forming decision-tree-like models (Wan et al., 2020; Gare et al., 2022) or through learnable activation functions (Liu et al., 2024), these efforts have not linked this understanding to measure model generalization.\nClass Taxonomy Alignment as a Feature Measurement. Class taxonomy or ontology has been widely utilized in literature to indicate class formation (Deng et al., 2009b; Van Horn et al., 2018) and semantic relationships between classes (Frome et al., 2013; Barz & Denzler, 2019; Wan et al., 2020; Redmon & Farhadi, 2017; Lin et al., 2022), offering a hierarchical organization of classes or categories.\nAs WordNet encodes class ontology, we hypothesize that transferable features are more likely to be shared among neighboring classes in the hierarchy (e.g., ostrich and crane). In contrast, confounding features are less supported by the hierarchy and tend to appear in less relevant classes that are often more distant in the hierarchy (e.g., lion and ostrich). When a model makes a mistake, its secondary prediction class can provide insight into the predictive features the model has learned during training. Specifically, it reflects that the model perceives the label class and the secondary prediction class to be more similar to each other based on these predictive features.\nConsequently, a model that captures more transferable features tends to 'make better mistakes (Bertinetto et al., 2020)' by predicting classes that are semantically closer to the ground-truth class. As illustrated in Fig. 4, models that learns to associate ostriches with features like 'long legs' and 'long neck', which are more transferable to OOD datasets, will likely predict classes like flamingos or cranes. In contrast, a model influenced by spurious correlations and associating ostriches with grass might predict a semantically distant class, like jaguars or lions, which also often appear on grass.\nOur method involves measuring model generalization based on the semantic severity of mistakes on in-distribution data. We use the LCA distance, the taxonomic distance between the model's prediction and the ground-truth class in a predefined taxonomic hierarchy like WordNet. If a model consistently makes better mistakes on in-distribution data, we can reasonably assume that the model has captured more transferable features for class discrimination.\nClass Taxonomy and Mistake Severity. The severity of a mistake in many studies is quantified as the shortest path from the prediction node to the lowest common ancestor (LCA) node in a predefined class hierarchy. This metric, known as 'LCA distance' or 'hierarchical error', was used in the early years of the ImageNet challenge (Deng et al., 2009b). However, it was largely dismissed as it was widely believed to follow the same ordering as Top 1 accuracy (Bertinetto et al., 2020). We revisit this metric and empirically demonstrate that Top 1 accuracy and LCA distance do not always align when VLMs are involved, challenging the common notion. We also appeal for community's attention to revisit this metric with its potential usage in measuring a model's feature awareness to indicate generalization.\nCausal/Invariant Representation Learning for OOD Generalization. Recently, there has been an increase in OOD generalization research towards formulating training and testing distributions with causal structures (Arjovsky et al., 2019; B\u00fchlmann, 2020; Peters et al., 2016), where distribution shifts primarily arise from interventions or confounding factors. Building upon this, methods (Sch\u00f6lkopf et al., 2021; Shen et al., 2022; Subramanian et al., 2022) such as CausalVAE (Yang et al., 2021) have been proposed, leveraging learned causal representations to capture the causal relationships underlying the data generation process (Kaur et al., 2022), which helps mitigate the distributional shifts caused by interventions.\nWhile the connection between OOD generalization and causal concepts is not entirely novel, previous attempts have focused on the causal structure at the latent or abstract level, lacking both interpretability and transparency. Our method aligns with this growing interest in causal/invariant learning, which aims to capture the invariant latent data generation process (Kaur et al., 2022). One should expect a model prediction that better aligns with the data generation process to be more robust under intervention, thus generalizing better. Although it is less feasible to model the data generation process of natural images (ImageNet), we essentially follow the same intuition and hypothesize that the WordNet class hierarchy serves as an approximation of invariant correlations between class concepts across environments (Arjovsky et al., 2019; Santurkar et al., 2020), robust to spurious relations in images or shortcuts in learning (Makar et al., 2022). WordNet is a widely recognized and effective means of encoding semantic relationships between concepts, making it an appropriate proxy for aligning human semantic knowledge (Miller et al., 1990). Unlike previous work, WordNet hierarchy provides interpretability, adding a level of transparency to our understanding of model generalization.\nLCA Illustration with Simulated Data. To illustrate our hypothesis that LCA distance can identify features supported by hierarchy, we created a controlled example using a simulated dataset, detailed in Appendix C. In this example, the data generation process is fully controlled. We designed a feature space that includes: 1) transferable causal features supported by hierarchy, 2) non-transferable confounding features not supported by hierarchy, and 3) random noise. Two logistic regression models were trained to mimic models capturing different predictive variables from the training data: one relying on the causal features and the other on the confounding features. The simulation results indicated that the model using causal features supported by hierarchy, which exhibited lower LCA distance, had better out-of-distribution (OOD) accuracy on the in-distribution (ID) test set, despite the model using confounding features achieving better ID accuracy. This example suggests that LCA can effectively identify models that capture relationships aligned with the hierarchical structure."}, {"title": "4. Experiments", "content": "We present experiments benchmarking the relationship between Lowest Common Ancestor (LCA) and generalization.\nDataset Setup. We leverage 75 pretrained models sourced from open repositories on GitHub for empirical analysis. Our selection comprises 36 Vision Models (VMs) pretrained on ImageNet and supervised from class labels, alongside 39 Vision-Language Models (VLMs) that incorporate language as part of the supervision. A comprehensive list of model details, ensuring reproducibility, is provided in Appendix A. We use ImageNet (Deng et al., 2009b) as the source in-distribution (ID) dataset, while ImageNet-v2 (Recht et al., 2019), ImageNet-Sketch (Hendrycks & Dietterich, 2019), ImageNet-Rendition (Hendrycks et al., 2021), ImageNet-Adversarial (Hendrycks et al., 2021), and ObjectNets (Barbu et al., 2019) are employed as out-of-distribution datasets, exemplifying severe natural distribution shifts. The ImageNet hierarchy, as depicted in (Bertinetto et al., 2020), is utilized.\nAlthough ImageNet-v2 is predominantly deemed an OOD dataset in most prior literature (Shankar et al., 2020; Miller et al., 2021; Baek et al., 2022), our experiments suggest that ImageNet-v2 aligns more closely with ImageNet than other OOD datasets; we delve into these details in Appendix B.\nNote that the terms in-distribution (ID) and out-of-distribution (OOD) are not model-specific in this context. Due to the varying distribution of training data across different models, ImageNet may not necessarily represent ID data for models like CLIP, where the training data distribution is not explicitly known. Instead, ID and OOD are relative concepts. ImageNet is used as a reference anchor dataset, serving as a baseline to evaluate the generalization capabilities of models on OOD datasets. This approach aligns with prior work, allowing us to consistently measure the shift in performance from ID to OOD datasets, despite the differences in the training data distributions of the models.\nMetric Setup. For our correlation experiment, we use $R^2$ (Coefficient of Determination) and PEA (Pearson correlation coefficient) to measure the strength and direction of linear relationships between two variables. Additionally, we employ KEN (Kendall rank correlation coefficient) and SPE (Spearman rank-order correlation coefficient) to assess the correspondence of the rankings of two variables."}, {"title": "4.1. LCA-on-the-Line: In-Distribution Taxonomic Distance (LCA) as an Out-of-Distribution (OOD) Performance Predictor", "content": "Accuracy-on-the-line (Miller et al., 2021) corroborated that a model's in-distribution (ID) accuracy and its out-of-distribution (OOD) accuracy are largely considered to be strongly correlated. This potent correlation forms a significant baseline for comparison in our research. Unlike the framework presented in (Miller et al., 2021), which only compares models within the same modality, our work bridges the gap by contrasting models of different modalities, involving both Vision Models (VM) and Vision-Language Models (VLM). In addition to the Top1 OOD accuracy, we also incorporate Top5 OOD accuracy, yielding a more comprehensive evaluation of model generalization.\nAs displayed in Table 1 and 2, the ImageNet in-distribution accuracy (Miller et al., 2021) forms a robust predictor for most OOD datasets, when the comparison is limited to models with similar setups (VMs or VLMs). However, this predictor fails to provide a unified explanation of generalization across models from both families. As highlighted in Figure 5 (indicated in red line), when adhering to Accuracy-on-the-Line' (Miller et al., 2021), all four OOD datasets plotted showcase two separate linear trends, representing models that belong to each family. This observation aligns with (Cherti et al., 2022), where it was found that VLM models, despite exhibiting significantly lower ID accuracy, could attain higher OOD performance than their state-of-the-art VM counterparts.\nAs shown in Figure 1, our method, adopting in-distribution LCA distance, could unify models from both families. As demonstrated in Table 2 and Figure 5 (colored in green line), the severity of in-distribution mistakes serves as a more effective indicator of model performance than in-distribution accuracy. It consistently exhibits a strong linear correlation with all OOD benchmark accuracies for natural distribution shifts (both $R^2$ and the Pearson correlation coefficient exceed 0.7, while (Miller et al., 2021) drop to 0 in ImageNet-A). Notably, our experiments showed that (Miller et al., 2021) is a more reliable indicator solely for ImageNet-v2, given its visual similarity to ImageNet. We will further discuss this in Appendix B.\nOur method restores the \"on-the-line\" linear relationship in front of both VMs and VLMs. Our method provides a compelling alternative to understand why vision-language models with lower in-distribution accuracy might generalize better to OOD datasets than vision models."}, {"title": "4.2. Predicting OOD Performance via ID LCA", "content": "We further highlight the effectiveness of the LCA-on-the-Line by estimating model OOD performance using a linear function derived from in-distribution LCA distance. For comparison, we included four competitive baselines: Average Confidence (AC), which leverages OOD logits after temperature scaling; two methods from Agreement-on-the-Line (Aline-D and Aline-S), utilizing consensus of pairs of models on OOD benchmarks; and 'Accuracy on the Line' (ID Top1), employing in-distribution accuracy of established measurement models to fit a linear function. Instead of performing a probit transform as done in (Baek et al., 2022) and (Miller et al., 2021), we implemented min-max scaling because LCA does not fall within the [0,1] range.\nAs illustrated in Table 3, in-distribution LCA distance proves to be a significantly more robust OOD error predictor than other baselines across four OOD benchmarks with varying distribution shifts. This robustness is especially evident for ImageNet-A, an adversarial dataset derived from ResNet50's misclassifications on ImageNet. Consequently, models pre-trained on ImageNet tend to underperform on this dataset, especially those with lower accuracy than ResNet50. This leads to decreased robustness for in-distribution indicators like in-distribution accuracy (Miller et al., 2021), methods calibrated from in-distribution validation sets (Hendrycks & Gimpel, 2017), and OOD agreement of models from different families (Baek et al., 2022). In contrast, LCA, which relies solely on the relative ranking of class predictions from a single model, is less sensitive to these issues and thus delivers more consistent performance. This further underscores the efficacy of LCA as a powerful predictor in challenging OOD scenarios."}, {"title": "4.3. Enhancing Generalization via Taxonomy Alignment", "content": "Building upon the earlier discussion, we explore how the devised method can be utilized to enhance a model's generalization capability."}, {"title": "4.3.1. INFERRING CLASS TAXONOMY FROM A PRETRAINED MODEL VIA K-MEANS CLUSTERING", "content": "In the previous experiment, we adopted the WordNet hierarchy as class taxonomy to calculate LCA distance. While the number of publicly available datasets providing class taxonomy is limited (Deng et al., 2009b; Van Horn et al., 2018), the usefulness of our method is unquestionable. Hence, we propose a method to construct a latent class taxonomy given a well-trained model on the task, expanding the potential applications of our work. We show that such a constructed taxonomy could achieve similar correlational performance to the WordNet hierarchy.\nThe essence of class taxonomy lies in its representation of inter-class distance, encoding class proximity, and identifying which classes cluster closely in feature space. In this spirit, we can construct a class taxonomy matrix using K-means clustering on image features. As illustrated in Fig. 6, for the ImageNet dataset, we adopt a well-trained model as the source pretrained model and extract average class features to cluster data hierarchically at different levels (we use n=9 for the 1000-class ImageNet dataset, as $2^9 < 1000$), with an increasing number of clusters to indicate class adjacency. K-mean is performed on each level of hierarchy independently. Experiments in Table 4 show that our method is very robust regardless of which model was used as the source model to construct the class hierarchy. This result demonstrate the potential in practice to use a latent hierarchy constructed by only one well-trained model for evaluating all models on a given task."}, {"title": "4.3.2. USING CLASS TAXONOMY AS SOFT LABELS", "content": "In this section, we investigate how leveraging LCA distance can enhance model generalization through improved supervision. Traditional models maximize the likelihood of the top-1 ground-truth class but often fail to generalize due to overfitting from spurious correlations. We argue that a generalizable model should accurately assign likelihoods to all classes in alignment with the class ontology. Building on this insight, we augment the standard cross-entropy loss, which maximizes the top-1 likelihood, with an auxiliary loss that uses soft labels encoded by the normalized pairwise class distance (LCA distance). This approach treats the problem as multi-label classification (Lin et al., 2022), guiding the model's decision boundary towards a more regularized feature distribution, thereby reducing susceptibility to spurious correlations and improving generalization. We balance the contributions of the cross-entropy and auxiliary losses using a lambda term: $L = \\lambda L_{(CE)} + L_{(softLCA)}$. The detailed formulation is provided in Appendix E.2.\nWordNet as Soft Labels. To evaluate our approach, we trained linear probe layers on five different models using either cross-entropy loss only (Baseline) or our cross-entropy plus LCA soft loss. We compared their performance on six ImageNet test sets. Inspired by the notion that models exhibit higher confidence where they excel (Wortsman et al., 2022), we applied linear interpolation between layers trained with cross-entropy and our proposed loss as our final classifier $W_{interp} = \\alpha W_{ce} + (1-\\alpha) W_{ce+soft}$. Table 5 shows that incorporating LCA soft loss consistently improved OOD performance without compromising ID performance, indicating more regularized decision boundaries beyond training data knowledge. Ablation study is presented in Table 9.\nLatent Hierarchy as Soft Labels. To demonstrate that our method generalizes beyond WordNet hierarchy, we constructed latent hierarchies using K-means clustering on pretrained models, forming soft labels to guide linear probing. We followed the same training procedure as above, using latent hierarchies instead of WordNet to construct soft labels. As shown in Table 6, adopting constructed hierarchies similarly boosted model generalization across all OOD datasets.\nVLMS Construct Better Soft Labels Compared to VMs. Drawing on the intuition of model distillation (Hinton et al., 2015), the hierarchy constructed from a model's pretrained features partially encapsulates the model's interpretation of interclass relationships. Thus we also examined if the source model affects the quality of derived soft labels."}, {"title": "4.3.3. IMPROVING GENERALIZATION BY CLASS TAXONOMY ALIGNMENT WITH PROMPT ENGINEERING", "content": "In this section, we discuss results on enhancing model generalization through prompt engineering in VLMs.\nFor VLM, integrating taxonomy-specific knowledge during zero-shot evaluation is straightforward. The WordNet hierarchy naturally indicates inter-class distances from class definitions. For example, 'dalmatian' and 'husky' are semantically close, both originating from the parent node 'dog'. We detail the results with CLIP-ViT32 (Radford et al., 2021) in Table 14. To test our hypothesis, we explicitly integrated hierarchical taxonomy relationships into the prompt for zero-shot VLM predictions. The prompt was designed as 'A, which is a type of B, which is a type of C', guiding the model to make taxonomy-aligned predictions. Additionally, we conducted two ablation studies: 1) Stack Parent: providing the correct taxonomy path without informing the model of the class name relationships; and 2) Shuffle Parent: informing the model of the hierarchical 'is-a' relationship but providing an incorrect taxonomy relationship randomly sampled from the tree. Our results demonstrate that informing the model of both the correct taxonomy and their hierarchical relationships significantly improves generalization. This improvement is evidenced by enhancements in Top-1 accuracy and test-time Cross-Entropy (CE) across all datasets for all tested models."}, {"title": "5. Conclusions", "content": "This work revitalizes the use of LCA distance, leveraging class taxonomies such as WordNet, to indicate model OOD performance. We assess the severity of model mispredictions in a manner agnostic to model modality, architecture or training data source, establishing a comprehensive metric for evaluating model generalization. Our findings across multiple ImageNet-OOD datasets highlight the superiority of LCA distance in reflecting the generalization capabilities of models trained with either class labels (VMs) or captions (VLMs), surpassing the traditional reliance on in-distribution Top-1 accuracy (Miller et al., 2021). To extend the application of LCA distance measurement to any dataset, we introduce a method for creating latent hierarchies using K-means clustering, showcasing the resilience of LCA distance regardless of the applied taxonomy or hierarchy. Additionally, we demonstrate that aligning model predictions with class taxonomies, through soft labels or prompt engineering, can enhance model generalization. Our results on demonstrating VLMs' lower LCA distance and better soft label construction offer new insights into VLMs' superior model generalization from a feature distribution perspective.\nFuture research could focus on providing theoretical justification for the LCA-on-the-Line framework. For instance, exploring causal discovery (Brouillard et al., 2020) methods on the ImageNet dataset to construct a causal graph between classes and underlying variables may offer a more accurate reflection of causal relationships between classes."}, {"title": "B. Discussion", "content": "Reestablishing LCA as a Comprehensive Measure of Model Generalization. While Top 1 ID accuracy (Miller et al., 2021) demonstrates a clear linear trend with OOD datasets in models with similar training mechanisms, this relationship becomes less distinct across VMs and VLMs. This finding, echoed in earlier studies (Fang et al., 2022; Wortsman et al., 2022; Cherti et al., 2022), suggests a more nuanced understanding of how zero-shot VLMs with lower Top-1 accuracy can outperform competitive vision models in generalizing to unfamiliar datasets. While previous works have emphasized the significant impact of data diversity on generalization (Fang et al., 2022; Schuhmann et al., 2022; Kaur et al., 2022), our results indicate that the LCA offers a more all-encompassing assessment of model generalization. By considering factors such as training data size, architecture, loss, and others, LCA better measures a model's ability to accurately capture semantic distinctions common across ID and OOD benchmarks. This establishes a comprehensive benchmark that encompasses various generalization factors, addressing the issue of inflated VLM effectiveness on \u201cEffective Robustness (Taori et al., 2020)\". Future research should delve into large-scale analytic studies of generalization factors in conjunction with LCA.\nImageNet-v2 Demonstrates Similar Class Discrimination Features to ImageNet. ImageNet-v2, a recollection of ImageNet, is often used as an OOD dataset for ImageNet-based studies (Shankar et al., 2020; Miller et al., 2021; Baek et al., 2022). Our experiments indicate that ImageNet-v2 more closely resembles ImageNet than other OOD datasets. We hypothesize that the minimal external intervention in ImageNet-v2's data collection process results in visual similarities to ImageNet (as ImageNet-v2 is a recollection of ImageNet), allowing even spurious relationships encoded on ImageNet to transfer successfully to ImageNet-v2. Consequently, models pretrained on ImageNet (VMs) inflate accuracy on ImageNet-v2, disrupting the alignment with trends observed in VLMs.\nIs it Possible for a Semantically-Aware (Low LCA) Model to Have Low Top 1 Accuracy? Our empirical analysis indicates a correlation: models not specifically tuned on class taxonomy, with lower Top 1 accuracy, tend to exhibit higher LCA distances. However, this relationship is correlational rather than causal. It remains feasible to design a model adversarially so it consistently predicts the semantically nearest class to the true class. In such cases, the model would show a low LCA distance while maintaining zero Top 1 accuracy. Therefore, while a correlation exists between Top 1 accuracy and LCA, causality cannot be inferred, and this relationship can be disrupted under deliberate adversarial training.\nDoes ImageNet LCA (Taxonomic Distance) Reflect ImageNet Top 1 Accuracy? It is often suggested that LCA and Top-1 accuracy exhibit similar trends on the same dataset (Deng et al., 2009b; Bertinetto et al., 2020). Intuitively, a high-performing model better fits the data distribution, leading to fewer severe errors. This pattern generally holds true for models under similar settings (either VM or VLM separately). However, when considering both VM and VLM models, ImageNet and ImageNet-v2 exhibit only a weak correlation between LCA and Top-1 accuracy, whereas other semantically distinct OOD datasets show a stronger relationship (validate in Section F.1). This finding challenges the prevailing belief that in-distribution Top-1 accuracy and LCA maintain the same ranking (Deng et al., 2009a; Bertinetto et al., 2020).\nWhy do we observe low LCA correlation numbers between IID test sets? From previous experiments, we observe that ImageNet LCA (Taxonomic Distance) does not correlate strongly with ImageNet/ImageNet-v2 Top-1 Accuracy, often showing a weak relationship, as illustrated in Figure 9. We hypothesize that this is due to ID accuracy inflation. In our LCA-on-the-Line framework, LCA is expected to be an unbiased measure of alignment to the class hierarchy. However, the VMs used in this work are trained on ImageNet and tend to 'inflate' ID accuracy when evaluated on IID test sets. As indicated in the bottom right two images of Figure 9, this inflation might causes datapoints to 'shift' in the direction of the red arrow, disrupting the unbiased linear relationship seen in VLMs that were not trained directly on ImageNet. Consequently, we should expect models evaluating LCA on unseen datasets to form a linear relationship, similar to the observed relationship on the other four severely shifted OOD datasets in Figure 9. Please refer to Section F.1 and Table 13 for a numerical comparison."}, {"title": "C. LCA Illustration with Simulated Data", "content": "To illustrate the hypotheses in Section 3: 1) Transferable features are more likely to be supported by the hierarchy and shared among neighboring classes; 2) Confounding features are less supported by the hierarchy and tend to appear in less relevant classes that are often more distant in the hierarchy; 3) LCA is useful in identifying features supported by the hierarchy, we created a simple example using a simulated dataset."}, {"title": "D. Metric"}]}