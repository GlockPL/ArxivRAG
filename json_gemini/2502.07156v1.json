{"title": "Explaining 3D Computed Tomography Classifiers with Counterfactuals", "authors": ["Joseph Paul Cohen", "Louis Blankemeier", "Akshay Chaudhari"], "abstract": "Counterfactual explanations in medical imaging are critical for understanding the predictions made by deep learning models. We extend the Latent Shift counterfactual generation method from 2D applications to 3D computed tomography (CT) scans. We address the challenges associated with 3D data, such as limited training samples and high memory demands, by implementing a slice-based approach. This method leverages a 2D encoder trained on CT slices, which are subsequently combined to maintain 3D context. We demonstrate this technique on two models for clinical phenotype prediction and lung segmentation. Our approach is both memory-efficient and effective for generating interpretable counterfactuals in high-resolution 3D medical imaging.", "sections": [{"title": "1. Introduction", "content": "Neural Networks can learn to identify features and make predictions from computed tomography (CT) scans predicting clinical phenotypes (Blankemeier et al., 2024) and survival (Thanoon et al., 2023). To understand why these predictions are made, it is crucial to have interpretable explanations, especially in the medical domain where high-stakes decisions are made. Synthetic counterfactual CT volumes can be created to simulate a change in the class label of an image, providing insights into the factors influencing the model's predictions.\nIn this work we extend the counterfactual generation method of Latent Shift, previously demonstrated for 2D counterfactuals (CFs) (Cohen et al., 2021), into the 3D domain to enable it to work for CT volumes. While a natural approach would be to use a 3D autoencoder, this presents difficulties such as the scarcity of training data for 3D autoencoders and the significant computational and memory demands, especially for large models and high-resolution data, both of which are likely required for generating high-quality counterfactuals.\nTo address this, we employ a 2D autoencoder by slicing the 3D volumes, then encoding, decoding, and concatenating the slices. This slice-based approach is more data-efficient than training directly on entire volumes, helping to mitigate overfitting by avoiding biases related to specific acquisition views (e.g., abdomen or head). By randomly sampling slices, the model is less likely to learn unwanted correlations connected to particular views."}, {"title": "2. Counterfactual Generation", "content": "The Latent Shift method (Cohen et al., 2021) is employed to generate counterfactuals (CFs). In order to make modifications to the image this method requires computing the gradient from the output of the classifier to the latent space of an autoencoder $\\frac{dz}{af(D(E(x)))}$ where $z = E(x)$. E and D are an encoder/decoder pair trained to represent the domain of CT Scans (see \u00a73). This gradient is then subtracted from the latent representation of the input image with a coefficient \u03bb. The coefficient A is selected to ensure a predetermined change in the classifier's output.\nComputing this gradient is memory-intensive, particularly when using a 3D classifier and autoencoder. In order to make this tractable, a 2D autoencoder is used which allows us to propagate the gradient to only a subset of slices, and their respective latent embeddings. We call this approach Slice AE and depict it in Figure 1. With this approach the entire volume is encoded slice by slice and then decoded and concatenated into a full volume and input into the classifier. During decoding, gradients are computed for only a subset of slices. The remaining slices have the gradient blocked. This approach allows the classifier to compute gradients for a few slices without sacrificing the context of the entire volume. Typically only 10 slices can have their gradient computed using 32GB of memory on a Tesla V100 used for this work.\nThis gradient can then be used to modify the latent representation and then decoded and concatenated and input into the classifier to determine if the the prediction has been sufficiently reduced. An iterative search is used by reducing the value of A until the prediction stops decreasing."}, {"title": "3. Model and Training", "content": "We train a VQ-GAN (Esser et al., 2021) which is a VQ-VAE (vector quantized variational autoencoder) with a perceptual adversarial loss. This approach generates high-resolution images while having an encoder/decoder which provides a latent space that can be navigated using CF generation methods. This is in contrast to diffusion models which also generate high-resolution images but the latent space requires iterations, so it is not trivially compatible with existing CF generation methods."}, {"title": "4. Data", "content": "The study utilizes three datasets selected for their size, public availability, coverage of various organ systems, and inclusion of both healthy and diseased states. The LUNA16 dataset (Setio et al., 2016) contains imaging data focused on the lungs and includes cancerous lesions. A total of 227,225 slices were obtained from 888 volumetric scans. The TotalSegmenter dataset (Wasserthal et al., 2022) provides imaging of all major organs, with 312,400 slices obtained from 1,204 scans. The DeepLesion dataset (Yan et al., 2017) includes images of multiple organs, both healthy and with lesions, offering a robust representation of diseased tissues. From 10,594 scans, 907,926 slices are obtained. Collectively, these datasets ensure comprehensive representation of both healthy and unhealthy organs, contributing to providing a good data representation to the model."}, {"title": "5. Experiments", "sections": [{"title": "5.1. Lung Size", "content": "In order to verify that this method works, an easily auditable CF can be generated for lung size. The work (Hofmanninger et al., 2020) released a lung segmentation model that we can transform into a regression task by taking the sum of the segmentation outputs for the lung class. As more pixels are classified as lung, the larger the sum will become. Figure 2A visualizes the CF generated for this task, demonstrating a strong visual signal that the lung size is reduced. Using that same model to segment the image and CF image, the size of the segmentation has been reduced. Here the segmentation is thresholded to 0.5 to make the comparison easier.\nThe reduction in lung size can also be validated by looking at the sum of the lung pixels (Figure 2B). We should observe that lowering the A value reduces the total sum of pixels, which is what is observed. We do not observe an infinite monotonic reduction because the CF generation process is limited by the latent variable model's ability to remove these features along the specific vector in the latent space identified by Latent Shift. Although it is more likely that a lung size smaller than what we achieve is outside the domain of the latent variable model and not something it can represent."}, {"title": "5.2. Plural Effusion", "content": "Plural effusion, characterized by the accumulation of fluid between the pleural layers surrounding the lungs (Krishna et al., 2025), can be visually identified in CT scans as bright regions along the lung periphery. To validate that the classifier utilized in (Blankemeier et al., 2024) is using the correct features, we applied our counterfactual generation method to this task.\nFigure 3A demonstrates the input and counterfactual (CF) slices, particularly at indices 33, 47, and 58. In these CF slices, reductions in the pleural effusion regions are visibly evident, outlined by dashed blue lines. This confirms that the generated counterfactuals correctly target the bright regions corresponding to fluid buildup, providing evidence that the classifier relies on appropriate features for its predictions.\nTo further analyze the classifier's decision-making process, we localized the slices that contributed most significantly to changes in the prediction. Figure 3B shows heatmaps highlighting areas within slices where the differences between the input and CF are most pronounced. By processing the volume in chunks of five slices and restricting changes to specific regions, we identified slices 30-35, 45-50, and 55-60 as having the greatest impact on the prediction. For instance, slices 45-50, visualized in Figure 3A, exhibit substantial reduction in the pleural effusion region, aligning with a decrease in the classifier's confidence."}]}, {"title": "6. Conclusion", "content": "In this work, we demonstrated the ability to generate counterfactual CT volumes using a slice-based VQ-VAE encoder-decoder architecture. Our approach effectively addresses the challenges of limited training data and high memory requirements associated with 3D medical imaging.\nThe case studies validate that the generated counterfactuals target clinically relevant features, highlighting the potential of this method for improving the transparency and trustworthiness of AI systems in medical applications. Furthermore, our approach allows for localized analysis, providing insights into the regions most influential to the classifier's predictions. This capability highlights the potential of counterfactual explanations to enhance transparency and trust in AI systems, particularly in high-stakes medical applications."}]}