{"title": "Forward-Cooperation-Backward (FCB) learning\nin a Multi-Encoding Uni-Decoding neural\nnetwork architecture", "authors": ["Prasun Dutta", "Koustab Ghosh", "Rajat K. De"], "abstract": "The most popular technique to train a neural network is backpropaga-\ntion. Recently, the Forward-Forward technique has also been introduced\nfor certain learning tasks. However, in real life, human learning does not\nfollow any of these techniques exclusively. The way a human learns is\nbasically a combination of forward learning, backward propagation and\ncooperation. Humans start learning a new concept by themselves and\ntry to refine their understanding hierarchically during which they might\ncome across several doubts. The most common approach to doubt solv-\ning is a discussion with peers, which can be called cooperation. Coopera-\ntion/discussion/knowledge sharing among peers is one of the most impor-\ntant steps of learning that humans follow. However, there might still be\na few doubts even after the discussion. Then the difference between the\nunderstanding of the concept and the original literature is identified and\nminimized over several revisions. Inspired by this, the paper introduces\nForward-Cooperation-Backward (FCB) learning in a deep neural network\nframework mimicking the human nature of learning a new concept. A\nnovel deep neural network architecture, called Multi Encoding Uni De-\ncoding neural network model, has been designed which learns using the\nnotion of FCB. A special lateral synaptic connection has also been intro-\nduced to realize cooperation. The models have been justified in terms of\ntheir performance in dimension reduction on four popular datasets. The\nability to preserve the granular properties of data in low-rank embedding\nhas been tested to justify the quality of dimension reduction. For down-", "sections": [{"title": "1 Introduction", "content": "Neural networks have proven to be a key component of artificial intelligence.\nIt has a wide area of applications like pattern recognition, image classification,\ndimensionality reduction, and natural language processing, among others. An\nessential aspect of a neural network is its learning technique which enables the\nnetwork to learn patterns on its own, adapt to new data and provide efficient\nrepresentations. Learning is a process of estimating weight parameters, based\non the input and output of the network, to meet certain objectives.\nThe predominant method for training neural networks is backpropagation.\nThe phrase \"backpropagation\" refers to a technique for efficiently computing\nthe gradient, not how the gradient is employed. The phrases \"reverse mode\nof automatic differentiation\" or \"reverse accumulation\" [12] are also used in\nplace of \"backpropagation\" [5]. Backpropagation, an application of the chain\nrule of differential calculus to neural networks, is sometimes used informally to\nrefer to the whole learning procedure. Backpropagation efficiently computes the\ngradient of a loss function with respect to the weights of the network at one layer\nat a time and proceeding backwards from the last layer. This way of proceeding\nbackwards avoids repeating calculations of the intermediate terms in the chain\nrule, which can be obtained through dynamic programming [9, 1, 5].\nThe traditional method of learning by a neural network using forward and\nbackward passes of backpropagation, storing the error derivatives layer-by-layer\nhas been observed to be biologically implausible. Thus, Hinton developed The\nForward Forward (FF) Algorithm [7], a novel greedy multi-layer learning proce-\ndure, influenced by Boltzmann machines [8] and Noise Contrastive Estimation\n[6]. The FF algorithm for neural networks discards backpropagation across the\nnetwork and learns using two layer-wise forward passes.\nIt has been observed that the way a\nconventional neural network learns, storing the error derivatives layer by layer\nand using forward and backward passes of backpropagation, is not biologically\nplausible. Hinton thus developed a new technique, called The Forward Forward\n(FF) Algorithm [7] for neural networks to learn with two forward passes applied\nin a layer-wise fashion, discarding backpropagation on the entire network. The\nFF algorithm deals with two types of data, namely, positive data and negative\ndata. Positive data are real observed data, whereas negative data are generated\nby the model for better representation learning. The first forward pass is a\npositive pass on the positive samples and the second forward pass is a negative\npass on the negative samples. The positive pass adjusts the weights to increase"}, {"title": "The Forward Forward Algorithm", "content": "the value of the goodness term in each hidden layer, whereas the negative pass\nadjusts the weights to decrease the same. For every hidden layer, the goodness\nterm is calculated as the squared sum of the output of each neuron of that\nlayer. The FF algorithm aims to correctly distinguish between positive and\nnegative data in every hidden layer. The probability of concluding that the\ndata is positive is given by\n$$p(positive) = \\sigma(\\Sigma_j y_j \u2013 \\theta)$$\nwhere p(positive) is the probability of determining if a sample is positive, \u03b8\nis a threshold to regulate the value of the goodness and $y_j$ is the output of\nhidden node j. The layer-wise training/learning tries to maintain the goodness\nterm well above the threshold value (0) for positive data and well below \u03b8 for\nthe negative ones. After distinguishing the positive and negative values in the\ncurrent hidden layer, the layer undergoes normalization to go forward for further\nprocessing by the next hidden layer. Layer normalization ensures that the next\nhidden layer is not influenced by the predictions of the current hidden layer.\nThe final hidden layer is connected to the output layer for final prediction.\nResearchers have tried to unify the advantages of backpropagation and the\nforward-forward algorithm to develop a more efficient learning technique. The\nFF algorithm has been generalized with predictive coding into a robust stochas-\ntic neural system by Ororbia and Mali [14]. The Integrated Forward-Forward\nAlgorithm (IntFF) [18], developed by Tang, leverages the design of multiple\nlocal loss functions to decompose the training target, eliminating the need to\npropagate gradients through the entire network structure. Instead, it employs\nshallow backpropagation, adjusting weights in just 1-3 layers of local hidden\nunits. Lorberbom et al. have introduced inter-layer communication during\ntraining of an FF network which strengthens the effective communication be-\ntween layers, thus leading to a better performance [13]. The FF Algorithm has\nbeen unified with backpropagation through a Model Predictive Control (MPC)\n[16] to incorporate feedback control over dynamic systems. Giampaolo et al.\nhave used the idea of local backpropagation to create a hybrid network, thus\navoiding the need for backward computations when needed [4]. The superiority\nof FF algorithm in comparison to backpropagation in modeling the nature of\nlearning in the cortex has been shown by Tosato et al. [19]. The extension of\nFF to CNN has been introduced in [17]. A recent variation of FF which does\nnot require the generation of additional negative samples during training has\nbeen developed by Zhao et al. [23].\nIn this paper, we come up with a novel learning technique called Forward-\nCooperation-Backward (FCB) learning. The FCB learning is philosophically\nbased on the nature of learning humans follow while learning a new concept as\na combination of forward-forward learning, cooperation and backpropagation.\nTo incorporate FCB learning, the novel Multi Encoding Uni Decoding (MEUD)\nneural network framework has been developed. The gradual development of\nMEUD, MEUD-FF, MEUD-Coop to MEUD-FF-Coop helps to completely re-\nalize the FCB learning mechanism."}, {"title": "2 MEUD, MEUD-FF, MEUD-Coop and MEUD-\nFF-Coop", "content": "This section describes the philosophy behind the Forward-Cooperation-Backward\n(FCB) way of learning that has been inspired by the way human learns. The\nFCB way of learning has gradually been realized through the novel MEUD-FF-\nCoop architecture. This is followed by the detailed delineation of the MEUD,\nMEUD-FF, MEUD-Coop and MEUD-FF-Coop architectures. Additionally, this\nsection discusses the learning strategies employed by each of these models."}, {"title": "2.1 Motivation", "content": "When a new subject is introduced to the students of a class, the students go\nthrough several steps in order to master the same. The students start progress-\ning chapter-wise to grasp the concepts. However, a student may not understand\nthe concept of a chapter holistically at once. First, the student tries to learn the\nmost important concepts of that chapter. The student may need to refine the\nlearned concept several times. Thus, the student forwards hierarchically. After\nsome steps, the student might understand some of the concepts and have a few\ndoubts. Now, the student tries to discuss them with their friends. This phase\nof learning can be called cooperation. The discussion helps the student to clear\nmost of the doubts, but might not all. For the remaining doubts, the student\nhas to identify the difference between their learning and the actual content. By\nidentifying the differences the student needs to refresh the previous knowledge\nand redo the above steps of learning. This process is nothing but backpropaga-\ntion. Inspired by this human centric learning procedure, we have developed a\nneural network based learning strategy, called Forward-Cooperation-Backward\n(FCB) way of learning."}, {"title": "2.2 Multi Encoding Uni Decoding (MEUD) neural net-\nwork", "content": "In order to realize the FCB learning strategy, a novel neural network ar-\nchitecture, called Multi Encoding Uni Decoding (MEUD) neural network, has\nbeen developed. The MEUD architecture starts with all the features of the in-\nput and proceeds forward layer-wise, processing information hierarchically. At\nevery step of this process, the accumulated information gets refined. This part\nof the concept has been accomplished using the FF algorithm (MEUD-FF neu-\nral network). After a certain number of steps, the bottleneck occurs and the\nforward path gets choked. Here for further processing, the cooperation layer\nis introduced. In this cooperation layer, any particular node is connected only\nwith its neighbours (MEUD-Coop neural network). This type of connection\nresembles cooperation among friends. We name this layer as the latent layer.\nAfter discussion among peers, to identify the remaining problems, the latent\nlayer nodes directly refer back to the original feature space. At this time, the\nnetwork needs to backpropagate. The network incorporating all these three\nconcepts has been named MEUD-FF-Coop neural network.\nStarting with the input layer of nodes, several hidden layers are there in\nthe architecture designed for hierarchical learning. This hierarchical forward\nlearning leads to the bottleneck layer of the network, where the cooperation\ntakes place with the latent layer nodes. The latent representation of the input\nis generated at this point as the output of this latent layer. We can think that\nthe processing of data from input to this layer is the encoding of information\nfrom its original feature space to its latent space. At this point, the comparison\nwith the original feature space needs to take place, and for this, the latent space\nrepresentation is projected back to the original space. Thus, in this latent to\noutput layer portion, the architecture tries to decode the learned latent space\nrepresentation back to the original space. Hence, there are multiple encoding\nlayers and a single decoding layer in the architecture. Thus, the model has been\nnamed Multi Encoding Uni Decoding (MEUD) neural network.\nIn the next sections, we first introduce the MEUD architecture (Section 2.2)\nin detail. Going further, the process of incorporating the FF algorithm in the\nMEUD architecture has been delineated (Section 2.3). Section 2.4 describes\nthe details of implementing cooperation in the MEUD network. Finally, the\noverall MEUD-FF-Coop neural network has been presented in Section 2.5. The\nlearning procedure of all these four networks along with the objective function\nhas been described in Section 2.6."}, {"title": "2.3 MEUD-FF neural network", "content": "The architecture of the MEUD-FF neural network is the same as that of the\nMEUD neural network, having a total of (s + 2) layers (Figure 1). The MEUD\nand MEUD-FF architecture differs in the weight initialization strategy of the\nfirst s layers of the network. In the MEUD architecture, the initial value of\nthe weights is drawn from a standard random normal distribution. In the case\nof MEUD-FF the first s layers i.e., layers $l_0$ to $l_{s-1}$ have been designed to be\ninitialized by the trained weights of the s shallow FF networks. The weights of\nthe remaining two layers, i.e., layers $l_s$ and $l_{s+1}$ have been initialized using the\nsame strategy as that of MEUD.\nA shallow FF model comprises two layers, namely, the input and a hidden\nlayer. The hidden layer has been designed to serve as the output layer of the\nshallow FF model. The input to the MEUD-FF (X(0)), i.e., the output of\nthe input layer $l_0$ denoted by $Y(0)$ is passed to the first shallow FF model for\ntraining. The trained weights of the first shallow FF model are used to initialize\nthe weight matrix W(0), connecting $l_0$ and $l_1$ layers of the MEUD-FF network.\nThe output of the first hidden layer (Y(1)) is computed using equation (2).\nNow, Y(1) serves as the input to the second shallow FF model, which is further\ntrained and the trained weights are used to initialize W(1), connecting layers $l_1$\nand $l_2$. The same fashion of weight initialization is followed for the remaining\nweight matrices up to the sth hidden layer of the MEUD-FF network."}, {"title": "2.4\nMEUD-Coop neural network", "content": "The MEUD-Coop follows the same architectural properties as MEUD. The dif-\nferentiating factor lies in the conjunction of the bottleneck and the latent layer.\nBoth the layers have the same number of nodes r. Philosophically, these two\nlayers act as a single layer of nodes with specific synaptic connections between"}, {"title": "2.5 MEUD-FF-Coop neural network", "content": "To realize the Forward-Cooperation-Backward learning as described in Sec-\ntion 2.1, the MEUD-FF-Coop neural network architecture has been designed.\nMEUD-FF-Coop is a combination of the MEUD-FF and MEUD-Coop mod-\nels. The framework of the MEUD-FF-Coop architecture is the same as that of\nMEUD-Coop (Figure 2) incorporating special neighbouring connections among\nthe bottleneck/latent layer nodes. The weight initialization strategy of the\nMEUD-FF-Coop neural network is the same as that of the MEUD-FF architec-\nture. Among the total (s + 2) layers of MEUD-FF-Coop architecture, the weight"}, {"title": "2.6 Learning", "content": "The MEUD, MEUD-FF, MEUD-Coop, MEUD-FF-Coop aims to obtain a re-\nconstruction (\\^{X}) of the original input (X). For this purpose, we use the mean\nsquared error loss between the original input data and the reconstructed output\ndata. We aim to minimize ||X - \\^{X}||F. The cost function C is defined as,\n$$C = \\frac{1}{2mn} \\Sigma_{i=1}^m \\Sigma_{j=1}^n (x_{ij} - \\^{x}_{ij})^2$$\nMEUD, MEUD-FF, MEUD-Coop, MEUD-FF-Coop have been trained using\nthe ADAM optimization technique [10]."}, {"title": "3 Experiments, Results and Analysis", "content": "The efficacy of the proposed FCB learning strategy mimicking the human way of\nlearning new concepts accoutered on the novel MEUD-FF-Coop has been estab-\nlished demonstrating its supremacy in dimensionality reduction over the others.\nThe step-by-step development of the MEUD-FF-Coop neural network model\nstarting from the standard deep autoencoder to the novel MEUD architecture,\nfrom there to MEUD-FF and MEUD-Coop framework, and finally attaining\nthe MEUD-FF-Coop model has been tested in terms of their competency in\ndimension reduction.\nThe effectiveness of dimensionality reduction has been examined through\nthree primary approaches. First, we have compared the ability of various mod-\nels to preserve the local structure of the data after reduction. Second, we have\nevaluated the quality of dimensionality reduction based on its impact on down-\nstream analyses, specifically in classification tasks. Third, we have conducted\nan experimental convergence analysis of all five models. In our study, we have\nconsidered four well-known datasets to assess these dimensionality reduction\ntechniques.\nSection 3.1 mentions the data sources used. The experimental setup, weight\ninitialization technique and the steps involved in data preparation have been\nprovided in Section 3.2. The magnitude of local structure preservation has\nbeen checked and the methodology used has been described in Section 3.3. The\nexperimental procedure for determining the quality of dimensionality reduction\nin terms of classification has been described in Section 3.3. Section 3.4 is a"}, {"title": "3.1 Data Sources", "content": "Four popular datasets namely, MNIST [3], Fashion MNIST [22], CIFAR-10 [11],\nand Extended MNIST [2] have been used to demonstrate the effectiveness of the\nFCB learning strategy. This strategy is evaluated in terms of dimensionality re-\nduction using various models, including MEUD, MEUD-FF, MEUD-Coop, and\nMEUD-FF-Coop neural networks, compared against a standard deep autoen-\ncoder."}, {"title": "3.1.1 MNIST Dataset", "content": "The MNIST (Modified National Institute of Standards and Technology) dataset\n[3] is a large collection of handwritten digits from 0-9. Comprising of 60,000\ntraining and 10,000 testing examples, each image is a grayscale image with\n28 \u00d7 28 pixels. Each image is associated with a label from 0-9 indicating the\ndigit it represents."}, {"title": "3.1.2 Fashion MNIST Dataset", "content": "The Fashion MNIST (FMNIST) dataset [22] was released by Zalando, an online\nfashion retailer, with the goal of promoting more meaningful advancements in\nimage recognition beyond the simple digits of MNIST. FMNIST contains 70,000\ngrayscale images of various fashion items from 10 different classes (e.g., T-shirts,\nbags, sneakers, etc.). FMNIST is split into 60,000 training and 10,000 testing\nexamples each having 28 \u00d7 28 pixels."}, {"title": "3.1.3 CIFAR-10 Dataset", "content": "The CIFAR-10 dataset [11] was developed by the Canadian Institute for Ad-\nvanced Research (CIFAR) to measure the effectiveness of image recognition\nalgorithms. CIFAR-10 is challenging due to its small image size and a wide\nvariety of objects and animals as classes. CIFAR-10 consists of 60,000 color\nimages categorized into 10 distinct classes. The dataset is split into 50,000\ntraining images and 10,000 testing images. Having 32 \u00d7 32 pixels, each image\nis composed of 3 colour channels (RGB)."}, {"title": "3.1.4 Extended MNIST Dataset", "content": "The Extended MNIST (EMNIST) [2] dataset is an extension of the original\nMNIST dataset, designed to broaden the scope of handwritten character recog-\nnition tasks. It includes both letters and digits as grayscale images. EMNIST"}, {"title": "3.2\nExperimental setup", "content": "The MEUD, MEUD-FF, MEUD-Coop, and MEUD-FF-Coop have been imple-\nmented on a Python version of 3.10.15 and a PyTorch version of 2.4.1 + \u0441\u0438118.\nThe scikit-learn, Matplotlib and seaborn libraries have been used for various\ndownstream analyses and plots.\nThe input data matrices have been preprocessed before being fed to the\nneural networks. The first p pixels of each image have been replaced with a one-\nhot encoding of the labels, where p denotes the number of classes of the dataset.\nTo create the positive or real samples, the one-hot encoding is performed with\nthe actual label associated with the image. The negative samples have been\ncreated using a one-hot encoding of any label but the original one associated\nwith the image. For example, if we consider an image from the MNIST dataset\nrepresenting 3, the positive label of the image will be 3, whereas the negative\nlabel will be drawn randomly from the set {0-9}/{3}. The images are then\nflattened and appended row-wise to create the input matrix X. Considering a\ntotal of m samples in X, the first m/2 samples represent positive samples and\nthe remaining are negative samples. All the dimensionality reduction techniques\nused here have been trained on the m samples. However, while performing\nthe classification tasks, each classifier has been trained on the positive m/2\ndimensionally reduced samples.\nEach of the four datasets has been reduced to 20 different r dimensions by all\nfive dimension reduction techniques. The value of r ranges from 25 to 500 with\ntwo consecutive r values differing by 25. The average of these 20 performance\nscores corresponding to twenty r values has been presented in Section 3.4."}, {"title": "3.3\nExperimental Procedure", "content": "The performance of MEUD-FF-Coop with respect to dimensionality reduction,\nin contrast to the progressive development from standard deep Autoencoder\n(AE) to MEUD, MEUD-FF and MEUD-Coop has been measured in two ways.\nFirst, the quality of local structure retention has been measured using the trust-\nworthiness metric. Second, downstream analyses have further been performed\nfor classification. Let T be a set containing the five aforementioned dimen-\nsionality reduction techniques. A single dimensionality reduction technique is\ndenoted as T. A dataset X, having m samples and n features per sample, is\nreduced to a dimension r (r < n). The dataset X is dimensionally reduced using\nall T in T for a certain value of r. Thus for a particular value of r, we obtain"}, {"title": "3.4 Results and Analysis", "content": "The performance of MEUD-FF-Coop has been presented and justified in two\nparts. First, the ability to preserve the local structure of data in the low-\ndimensional representation has been compared in terms of the trustworthiness\nmetric. Second, the effectiveness of the dimensionally reduced dataset is ex-\nplored for downstream analyses, like classification."}, {"title": "3.4.1 Quantifying the quality of low dimensional embedding: Local\nstructure preservation", "content": "The trustworthiness score has been computed to compare the ability of local\nstructure preservation in the dimensionally reduced instances of the datasets by\nMEUD-FF-Coop against the four other models. Trustworthiness metric is used\nto measure the extent of local structure retention in the latent space represen-\ntation of the data with respect to the original data [20, 21, 15]. The value of\ntrustworthiness lies in [0,1]. The higher the trustworthiness score, the better\nis the low-rank representation, indicating the dimension reduction technique is\nbetter.\nAs mentioned in Section 3.2, each of the four datasets has been reduced to\n20 different r dimension values by all five dimension reduction techniques of T.\nThus, we obtain five different trustworthiness scores for each r value. Figures 3-\n6 represent the resulting scores of the same. It can be clearly observed that the\ntrustworthiness scores generated by MEUD-FF-Coop are better than the others\nin most of the cases and are clustered in a small neighbourhood. They are not\nscattered, and thus, justify the efficacy of MEUD-FF-Coop in preserving local\nstructure in the low dimensional embedding. As the value of r ranges from 25\nto 500, the scalability of MEUD-FF-Coop has also been established."}, {"title": "3.4.2\nDownstream analyses: Classification", "content": "The effectiveness of dimensionality reduction has been assessed by performing\nclassification on the low dimensional embedding generated by MEUD-FF-Coop\nand the other four dimensionality reduction techniques. Different types of clas-\nsification algorithms have been considered to justify a better performance of"}, {"title": "3.5\nConvergence Analysis", "content": "An important characteristic of a deep learning model is its rate of convergence,\nwhich refers to how quickly the loss curve flattens. When comparing the loss\ncurves of different deep learning models on a specific dataset, the model that\nconverges to the lowest loss value is generally considered superior. Additionally,\nthe smoothness of the loss curve is a significant indicator of the learning quality\nof the model. A smoother curve typically reflects better model learning.\nFigures 12-15 depict the Loss vs. Epoch plots for all four datasets considered\nhere for a particular r value. Each figure displays the loss curves of all five deep\nlearning models included in this comparative analysis. For a given dataset, all\nfive dimensionality reduction models have reduced the input to the same target\ndimension, r. The value of the reduced dimension for MNIST dataset is 250,\nfor both FMNIST and CIFAR-10, it is 50 and for EMNIST the same value\nis 200. Across all the four datasets, MEUD-FF-Coop model has consistently\nachieved the lowest loss value. Additionally, MEUD-FF-Coop has demonstrated\nthe fastest convergence to this minimum loss for each dataset. The loss curves\ngenerated by MEUD-FF-Coop also show a well-defined elbow shape, indicating\nsuperior convergence behaviour compared to the other models. In terms of\nloss curve smoothness, MEUD-FF-Coop has again outperformed the other five\ndeep learning models, exhibiting the smoothest curve. This establishes that the\nMEUD-FF-Coop model has demonstrated superior convergence across multiple\naspects. This not only establishes the efficacy of the MEUD-FF-Coop neural\nnetwork model but also justifies the FCB learning algorithm developed here"}, {"title": "4 Conclusion", "content": "Learning algorithms have been developed to emulate human-like learning through\nmachines. The backpropagation method of learning has played a major role\nin the development of neural networks ever since its inception. Newer tech-\nniques, like the Forward Forward learning, have also tried to develop a more\nbiologically plausible learning mechanism. However, such techniques are not\ncompletely able to mimic the human nature of learning. In this paper, we have\ndeveloped a human-centric learning mechanism that combines forward-forward\nlearning, backpropagation, and cooperation. The cooperation technique estab-\nlishes the concept of discussion among peers to enhance one's knowledge. Thus,\nwe have come up with Forward-Cooperation-Backward (FCB) learning which\ntries to mimic the human nature of learning similar to a student trying to grasp\na new concept. FCB learning strategy has gradually been established through\nthe MEUD, MEUD-FF, MEUD-Coop and MEUD-FF-Coop frameworks. The\ncomplete realization of FCB learning algorithm has been accomplished using\nthe novel MEUD-FF-Coop neural network model.\nTo establish the superiority of MEUD-FF-Coop over others, the dimension-\nally reduced data obtained from the network have been tested using two ways,\nviz., the trustworthiness score, i.e., the ability to preserve the granular relation-\nship of data and the classification performance on the low dimensional embed-\nding produced by the algorithms. The trustworthiness scores and its depiction\nthrough the spider/star plot justify the efficiency of MEUD-FF-Coop over the\nothers. It is seen that the trustworthiness scores generated by MEUD-FF-Coop\nare clustered together and are mostly better than others. The area coverage in\nthe spider plot by the polygon relating to MEUD-FF-Coop is the highest, in\nturn justifying the superiority of the FCB learning technique. Different classi-\nfiers and classification performance metrics have been used to justify the efficacy\nof dimension reduction as downstream analyses establishing FCB learning pro-\ncedure. For three out of four datasets, the performance of MEUD-FF-Coop\nhas been found to be better among the others in terms of classification, and\ndisplays a competitive performance for the EMNIST dataset. The experimental\nconvergence analyses show that MEUD-FF-Coop converges the fastest among\nthe others and is able to reduce the loss to the lowest. Additionally, the smooth-\nness of the convergence curve in the case of MEUD-FF-Coop is the highest.\nThus, the FCB way of learning establishes its superiority along with the\nnovel MEUD-FF-Coop neural network framework in terms of the preservation\nof local structure in the transformed dataset and downstream analyses using\nclassification. The convergence analysis of the models also supports the efficacy\nof the developed FCB learning technique."}]}