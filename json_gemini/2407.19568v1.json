{"title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?", "authors": ["Kangda Wei", "Aayush Gautam", "Ruihong Huang"], "abstract": "Large Language Models (LLMs) have demonstrated proficiency in a wide array of natural language processing tasks. However, its effectiveness over discourse-level event relation extraction (ERE) tasks remains unexplored. In this paper, we assess the effectiveness of LLMs in addressing discourse-level ERE tasks characterized by lengthy documents and intricate relations encompassing coreference, temporal, causal, and subevent types. Evaluation is conducted using an commercial model, GPT-3.5, and an open-source model, LLaMA-2. Our study reveals a notable underperformance of LLMs compared to the baseline established through supervised learning. Although Supervised Fine-Tuning (SFT) can improve LLMS performance, it does not scale well compared to the smaller supervised baseline model. Our quantitative and qualitative analysis shows that LLMs have several weaknesses when applied for extracting event relations, including a tendency to fabricate event mentions, and failures to capture transitivity rules among relations, detect long distance relations, or comprehend contexts with dense event mentions.", "sections": [{"title": "Introduction", "content": "Event Relation Extraction (ERE) refers to the NLP tasks that identify and classify relationships between events mentioned in a text. The commonly studied event relations include coreference, temporal, causal and subevent relations. ERE tasks aim to comprehend the intricate relationships between events and are beneficial for many applications, such as event prediction (Chaturvedi et al., 2017; Bai et al., 2021), question answering (Oh et al., 2017), and reading comprehension (Berant et al., 2014).\nERE tasks remain difficult and the empirical performance on these tasks are often rather low. Recently, inspired by the recent success of LLMs,"}, {"title": "Related Works", "content": "Event Relation Extraction Event relation extraction (ERE) has been one of the fundamen- tal challenges for natural language processing (Chaturvedi et al., 2017; Rashkin et al., 2018; Zhang et al., 2020). As understanding relations between events is crucial for understanding human languages (Levelt, 1989; Miller and Johnson-Laird, 1976) and beneficial for various applications (Khashabi et al., 2019; Zhang et al., 2020; Choubey and Huang, 2018), many approaches have been developed for performing ERE tasks (Liu et al., 2014; Hashimoto et al., 2014; Ning et al., 2017), and many high performing approaches are based on supervised learning (Dligach et al., 2017; Aldawsari and Finlayson, 2019; Liu et al., 2020; Lu and Ng, 2021). However, few-shot LLMs have not been sufficiently explored for ERE tasks.\nLLMs for Extraction Tasks LLMs have been applied to several common information extraction tasks including event extraction, relation extraction and named entity recognition (Gonz\u00e1lez-Gallardo et al., 2023; Borji, 2023; Tang et al., 2023; Gao et al., 2023b; Wei et al., 2023b). However, to the best of our knowledge, LLMs have not been well explored for ERE tasks. Recently, Yuan et al. (2023) evaluates ChatGPT on temporal relation extraction and Gao et al. (2023a) evaluates Chat- GPT on causal reasoning with the binary Event Causal Identification task, in contrast, we evaluate LLMs on extracting multiple types of fine-grained event relations. The dataset we use in this study, MAVEN-ERE (Wang et al., 2022), has dense relations at the discourse-level for four common types of event relations: coreference, temporal, causal, and subevent.\nPrompt Engineering Many recent works have studied how to make LLMs perform better through applying various prompting techniques, including role-prompting (Zhang et al., 2023; Buren, 2023), re-sampling (Holtzman et al., 2020), one-shot or few-shot prompting (au2 et al., 2021; Shyr et al., 2023), etc. Other novel and advanced techniques include Chain of Thought prompting (Wei et al., 2023a), least-to-most prompting (Zhou et al., 2023) and retrieval augmentation (Lazaridou et al., 2022; Jiang et al., 2023). We refer to these techniques as guidelines when designing prompts in this work."}, {"title": "Experiment Setup", "content": "For this study, we use the MAVEN-ERE dataset cre- ated by Wang et al. (2022), which includes annotations of four types of event relations: coreference, temporal, causal, and subevent. MAVEN-ERE consists of 4480 English Wikipedia documents, containing 103,193 event coreference chains, 1, 216, 217 temporal relations, 57, 992 causal relations, and 15, 841 subevent relations. MAVEN-ERE is challenging as the documents contain comprehensive relation types, event relations at the discourse level and have denser relations among events comparing to other datasets. For example, MAVEN-ERE has an average of 272 temporal relation links per document comparing to 49 temporal relation links per document for MATRES (Ning et al., 2018); TimeBank-Dense (Cassidy et al., 2014) mainly focus on sentence-level relations; and TDDiscourse (Naik et al., 2019) only consider temporal relations.\nAs we test LLMs in a prompting setting without extra fine-tuning, we only utilize ten documents from the training set to extract examples included in a prompt, and we use ten documents from the validation set for prompt design and selection. We report the performance of LLMs on the whole test set of MAVEN-ERE, which contains 857 documents with 18,908 event coreference chains, 234, 844 temporal relations, 11,978 causal relations, and 3, 822 subevent relations."}, {"title": "Prompts", "content": "There are many possible ways to prompt LLMs for MAVEN-ERE. We design four different prompt patterns, namely Bulk Prediction, Iterative Prediction, Event Ranking referring to previous works (Yuan et al., 2023; Bohnet et al., 2023), and Pairwise. In the following sections, we describe each prompt pattern. Examples of prompt patterns can be found in Appendix D.\nFor all prompt patterns, we first label all event mentions as [xi Event_p] where xi is the triggering word in sentence S and Event_p is the Event ID given a document D = {S1, S2, ..., Sn}. TIMEX mentions are also considered for forming temporal relations, therefore, we label TIMEX mentions as [xi TIMEX_q] and TIMEX_q is the TIMEX ID. p and q starts from 0 and gets increased by 1 each time a new event mention or TIMEX mention is labeled. We define E to be the set of event mentions and T to be the set of TIMEX mentions. We define Y to be the set of four relation types where Y = {coreference, temporal, causal, subevent}. Ry is defined to be the set containing all the sub-relation types for y \u2208 Y, where\nRcoreference = {COREFERENCE}, Rtemporal = {BEFORE, CONTAINS, OVERLAP, BEGINS- ON, ENDS-ON, SIMULTANEOUS}, Rcausal = {CAUSE, PRECONDITION}, and Rsubevent = {SUBEVENT}."}, {"title": "Bulk Prediction", "content": "Using the Bulk Prediction prompting, we query the LLM four times for each test document, with each query asking LLMs to list all relation pairs for each y \u2208 Y. For each query, we also provide an example document followed by the gold relation pairs for the same y as the query. Notice this is a 1-shot setting since we provide a whole document as an example."}, {"title": "Iterative Prediction", "content": "Algorithm 1 sketches the Iterative Prediction prompting method. We query LLMs by iterating through the document D sentence by sentence. For each new sentence S, we append S to all the previous sentences that are already augmented with event relations predicted by the model. Each S is queried four times for each y \u2208 Y. For coreference relation, we follow the Link-Append approach proposed by Bohnet et al. (2023) to augment the queried sentences. For temporal, causal, and subevent relations, we augment the sentences by inserting predicted relation tuples after the Event ID or TIMEX ID. A tuple is in the form (e||t, ry, e||t), where e \u2208 E, t \u2208 T, and ry \u2208 Ry.\nWe experiment with two ways for providing demonstrations to the model: (1) whole doc, and (2) n-shot."}, {"title": "Event Ranking", "content": "For the Event Ranking prompting method, we query LLMs by iterating through e and t for Ve \u2208 E, Vt \u2208 T in test document D as shown in Algorithm 2. We ask LLMs to complete the query (?, ry, e||t), de \u2208 E,\u2200t \u2208 T,\u2200ry \u2208 Ry, and \u2200y \u2208 Y. Note that we only need to query TIMEX mentions for temporal relation since TIMEX mentions are only relevant to temporal relations. We also provide one example for each query. The example contains an example document, a query (?, ry, e' \\t'), where e' ||t' is an Event mention or a TIMEX mention from the example document, and the gold relation tuples as the answer. Notice the query for the test document, (?, ry, e||t), and the query in the example, (?, ry, e'||t'), have the same ry. This is also a 1-shot setting since we provide a whole document as an example."}, {"title": "Pairwise", "content": "For the pairwise prompting method, we query LLMs with all the event mentions and TIMEX mentions pairs. We ask LLMs to complete the query (e||t, Ry =?, e||t), Ve \u2208 E,\u2200t \u2208 T, and \u2200y \u2208 Y. Note that we only need to query TIMEX mentions for temporal relation since TIMEX mentions are only relevant to temporal relations. If there is no relation between two events then NONE should be predicted. We also provide one example for each sub-relation ry of all four relation types. Note that this prompt pattern is in purely natural language format. However, since the number of event pairs equals the number of times we query LLMs, which grows quickly as the number of events increases, this approach is not feasible to use for GPT-3.5 and GPT-4 if we take into account of financial and time costs. Therefore, we only test this prompt with LLaMA-2."}, {"title": "Model", "content": "For this study, we use both open-source LLMs and closed-sourced LLMs for evaluation. For open- source models, we use LLaMA-2 7B model, specifically Llama-2-7b-chat-hf, accessed through Huggingface2. For closed-source models, we consider the gpt-3.5-turbo-16k model 3 from OpenAI API 4 as ChatGPT has been the most successful commer- cial LLMs so far. We test Llama-2-7b-chat-hf and gpt-3.5-turbo-16k on both validation and test sets using various different prompts.\nTo get an idea of how the newest GPT model performs, we also tested gpt-4-1106-preview model on a subset of the validation set instead of the whole test set becuase API calls to GPT-4 models are 30 times more expensive than GPT-3.5 models5."}, {"title": "Prompt Decision", "content": "Before experimenting on the whole test set containing 857 documents, we test the four prompt pat-"}, {"title": "Results", "content": "SFT certainly improves the performance of LLaMA-2. However, it still underperforms the supervised baseline method for all the relation types as the number of used training documents increases. LLaMA-2 typically requires twice more training data to reach the same overall performance as the smaller supervised baseline model. LLaMA- 2 only outperforms the supervised baseline method when the available training data is very limited, which benefits from its zero-shot capability emergent from large-scale pre-training. It also deserves mentioning that fine-tuning LLMs is much more expensive than fine-tuning smaller language mod- els like ROBERTa. For example, fine-tuning using 200 training documents for 3 epochs requires approximately 72 hours for LLaMA-2 7B, while only about 1 hour is needed for the roberta-base baseline model."}, {"title": "Discussion", "content": "During the test of GPT-3.5 and LLaMA-2, we notice that both models create events or event relations that do not exist in text.\nIn addition, both models occasionally have dif- ficulties in generating formatted answers, and the outputs may consist of random words from the document rather than Event or TIMEX ID or even event trigger words.\nAs GPT-3.5 has achieved overall better performance compared to LLaMA-2, we conduct further analysis on model performance mainly based on the predictions of GPT-3.5 on the 10 validation documents."}, {"title": "GPT-3.5 failed to learn transitivity rules", "content": "By manually examine the output of GPT-3.5 on the 10 validation documents, we notice that this model failed to learn the transitivity rules from"}, {"title": "Event Pairs with a Varying Distance", "content": "We investigate model performance on predicting intra- and inter-sentence event relations separately. Given that event coreference resolution relies on undividable clusters, we mainly analyze performance on the other three tasks. As shown in Table 4, GPT-3.5 is more capable to capture the relations between events that appear in the same sentence (intra-sentence) and otherwise struggles with capturing the inter-sentence event relations.\nWe also investigate the impact of sentence distance on model performance for inter-sentence cases. As shown in 4, the performance on temporal relation extraction decreases quickly as the number of sentences between two events increases; the performance on causal relation extraction also decreases a little when the number of sentences in between increases from one to two, but then the performance seems to remain stable when we further consider causal relations with five or more sentences in between. Overall, we observe lower performance on event pairs with greater distances."}, {"title": "Event Pairs in Contexts of Varying Event Densities", "content": "We investigate the impact of event density on model performance by examining the predictions of GPT-3.5 on the 10 validation documents. We only consider event pairs within the same sentence. Event density is measured as the number of event and TIMEX mentions appeared in one sentence. As shown in Table 5, the performance of GPT-3.5 on temporal and causal relations decreases quickly as the event density increases, indicating GPT-3.5 struggles to capture event relations when the complexity of the context increases."}, {"title": "Conclusion", "content": "In this study, we systematically evaluated the effectiveness of LLMs in performing discourse-level ERE tasks featuring lengthy documents and intricate relations. Our experiments using multiple prompt patterns uncover a noteworthy underperformance of LLMs when compared to the baseline established through supervised learning. Even with supervised fine-tuning, LLMs like LLaMA-2 still underperform the much smaller supervised baseline model when trained on the same amount of data. Furthermore, our quantitative and qualitative analyses revealed that LLMs face challenges in obeying transitivity rules, capturing inter-sentence relations or relations with a long distance, as well as comprehending context with dense event mentions. For future work, we will further investigate these challenges and develop methods for enabling LLMs to better address some of these issues in event relation extraction."}, {"title": "Limitation", "content": "Although we tried several different prompt patterns, there is still a chance that there exists better prompt to be used to assist GPT to solve the ERE task better. Meanwhile, OpenAI constantly update the GPT models that can be accessed throught the API, making it hard to reproduce the results if older models are deprecated. While OpenAI has offered preliminary introductions to various versions of GPT models, the specific implementation details remain obscure. This opacity hampers thorough analysis of why distinct versions of GPT models exhibit varying performance levels and how each data set and training technique influences models' performance. Finally, OpenAI API is a paid service, conducting experiments can get expensive depending on the task and design of the evaluation, making it not accessible to larger community. We are also limited by the cost and response time of OpenAI API.\nFor LLaMA-2 models, larger models (13B and 70B) may have better performance, but we leave the thorough study of LLaMA-2 models for potential future works."}, {"title": "Ethics and Broader Impact", "content": "We are aware that such study is very expensive and not very accessible to some researchers in the NLP community as OpenAI API is a paid service and is restricted in many countries. Not all researchers in our community can afford thousands of dollars or even more to run such experiments. Experiments on exclusive models or API may further detriment the inclusiveness of NLP community. Therefore, we hope our work can provide insights to readers with limited resources and inspire them in other works. However, by no means that we are advocating the NLP community to include closed-source LLMs as the baseline for any of the future work as studying the performance and behavior of closed- source models can be extremely difficult. We aim for our work to serve as a valuable resource for readers, helping them make decisions as they leverage LLMs for complex ERE tasks at discourse- level."}]}