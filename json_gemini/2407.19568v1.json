{"title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?", "authors": ["Kangda Wei", "Aayush Gautam", "Ruihong Huang"], "abstract": "Large Language Models (LLMs) have demonstrated proficiency in a wide array of natural language processing tasks. However, its effectiveness over discourse-level event relation extraction (ERE) tasks remains unexplored. In this paper, we assess the effectiveness of LLMS in addressing discourse-level ERE tasks characterized by lengthy documents and intricate relations encompassing coreference, temporal, causal, and subevent types. Evaluation is conducted using an commercial model, GPT-3.5, and an open-source model, LLaMA-2. Our study reveals a notable underperformance of LLMs compared to the baseline established through supervised learning. Although Supervised Fine-Tuning (SFT) can improve LLMS performance, it does not scale well compared to the smaller supervised baseline model. Our quantitative and qualitative analysis shows that LLMs have several weaknesses when applied for extracting event relations, including a tendency to fabricate event mentions, and failures to capture transitivity rules among relations, detect long distance relations, or comprehend contexts with dense event mentions.", "sections": [{"title": "Introduction", "content": "Event Relation Extraction (ERE) refers to the NLP tasks that identify and classify relationships between events mentioned in a text. The commonly studied event relations include coreference, temporal, causal and subevent relations. ERE tasks aim to comprehend the intricate relationships between events and are beneficial for many applications, such as event prediction (Chaturvedi et al., 2017; Bai et al., 2021), question answering (Oh et al., 2017), and reading comprehension (Berant et al., 2014).\nERE tasks remain difficult and the empirical performance on these tasks are often rather low. Recently, inspired by the recent success of LLMs,"}, {"title": "Related Works", "content": "and the open-source model LLaMA-2.\nFurthermore, humans usually benefit from comprehending the meanings of event relations through examples (Hu et al., 2023) when conducting the ERE tasks, but the recent studies (Wei et al., 2023b; Yuan et al., 2023) often evaluate LLMs under the zero-shot setting and have a chance to limit the models in fully showcasing their true capabilities in event relation extraction. To fill the gap, we run experiments in the one-shot or few-shot settings, we also run Supervised Fine-Tuning (SFT) experiments aiming to further enhance LLMs on performing ERE tasks.\nWe systematically evaluate the effectiveness of LLMs on extracting four common types of event relations (coreference, temporal, causal, and subevent) using the MAVEN-ERE dataset (Wang et al., 2022) by experimenting with multiple prompts. The design of our prompts were informed by the prior work (Yuan et al., 2023; Bohnet et al., 2023) to ensure the most effective prompts applied. Comprehensive analysis, both quantitative and qualitative, lead to the following findings:\nPerformance: Even with careful prompting, both GPT-3.5 and LLaMA-2 significantly underperform the baseline established through full supervised learning, and this is true for all the four types of relations. Although Supervised Fine-Tuning (SFT) can yield improved performance for LLaMA-2, there is still a gap between its performance and the performance of the baseline model trained with the same size of data, not to mention SFT for LLaMA-2 requires much more time and computation.\nTransitivity and hallucinations: There is a discernible tendency for LLMs to violate the rules of transitivity among event relation predictions. Furthermore, LLMs display inconsistencies in adhering to the provided prompts. Both suggest a potential lack of reasoning abilities and accurate understanding on the assigned task.\nEvents Distance and Density: LLMs encounter challenges in capturing long distance event relations and inter-sentence event relations. LLMs also struggle to capture event relations in complex contexts that are dense with event mentions.\nEvent Relation Extraction Event relation extraction (ERE) has been one of the fundamental challenges for natural language processing (Chaturvedi et al., 2017; Rashkin et al., 2018; Zhang et al., 2020). As understanding relations between events is crucial for understanding human languages (Levelt, 1989; Miller and Johnson-Laird, 1976) and beneficial for various applications (Khashabi et al., 2019; Zhang et al., 2020; Choubey and Huang, 2018), many approaches have been developed for performing ERE tasks (Liu et al., 2014; Hashimoto et al., 2014; Ning et al., 2017), and many high performing approaches are based on supervised learning (Dligach et al., 2017; Aldawsari and Finlayson, 2019; Liu et al., 2020; Lu and Ng, 2021). However, few-shot LLMs have not been sufficiently explored for ERE tasks.\nLLMs for Extraction Tasks LLMs have been applied to several common information extraction tasks including event extraction, relation extraction and named entity recognition (Gonz\u00e1lez-Gallardo et al., 2023; Borji, 2023; Tang et al., 2023; Gao et al., 2023b; Wei et al., 2023b). However, to the best of our knowledge, LLMs have not been well explored for ERE tasks. Recently, Yuan et al. (2023) evaluates ChatGPT on temporal relation extraction and Gao et al. (2023a) evaluates ChatGPT on causal reasoning with the binary Event Causal Identification task, in contrast, we evaluate LLMs on extracting multiple types of fine-grained event relations. The dataset we use in this study, MAVEN-ERE (Wang et al., 2022), has dense relations at the discourse-level for four common types of event relations: coreference, temporal, causal, and subevent.\nPrompt Engineering Many recent works have studied how to make LLMs perform better through applying various prompting techniques, including role-prompting (Zhang et al., 2023; Buren, 2023), re-sampling (Holtzman et al., 2020), one-shot or few-shot prompting (au2 et al., 2021; Shyr et al., 2023), etc. Other novel and advanced techniques include Chain of Thought prompting (Wei et al., 2023a), least-to-most prompting (Zhou et al., 2023) and retrieval augmentation (Lazaridou et al., 2022; Jiang et al., 2023). We refer to these techniques as guidelines when designing prompts in this work."}, {"title": "Experiment Setup", "content": "For this study, we use the MAVEN-ERE dataset created by Wang et al. (2022), which includes annotations of four types of event relations: coreference, temporal, causal, and subevent. MAVEN-ERE consists of 4480 English Wikipedia documents, containing 103,193 event coreference chains, 1, 216, 217 temporal relations, 57, 992 causal relations, and 15, 841 subevent relations. MAVEN-ERE is challenging as the documents contain comprehensive relation types, event relations at the discourse level and have denser relations among events comparing to other datasets. For example, MAVEN-ERE has an average of 272 temporal relation links per document comparing to 49 temporal relation links per document for MATRES (Ning et al., 2018); TimeBank-Dense (Cassidy et al., 2014) mainly focus on sentence-level relations; and TDDiscourse (Naik et al., 2019) only consider temporal relations.\nAs we test LLMs in a prompting setting without extra fine-tuning, we only utilize ten documents from the training set to extract examples included in a prompt, and we use ten documents from the validation set for prompt design and selection. We report the performance of LLMs on the whole test set of MAVEN-ERE, which contains 857 documents with 18,908 event coreference chains, 234, 844 temporal relations, 11,978 causal relations, and 3, 822 subevent relations."}, {"title": "Prompts", "content": "There are many possible ways to prompt LLMs for MAVEN-ERE. We design four different prompt patterns, namely Bulk Prediction, Iterative Prediction, Event Ranking referring to previous works (Yuan et al., 2023; Bohnet et al., 2023), and Pairwise. In the following sections, we describe each prompt pattern. Examples of prompt patterns can be found in Appendix D.\nFor all prompt patterns, we first label all event mentions as $[x_i \\text{ Event\\_p}]$ where $x_i$ is the triggering word in sentence S and Event_p is the Event ID given a document $D = \\{S_1, S_2, ..., S_n\\}$. TIMEX mentions are also considered for forming temporal relations, therefore, we label TIMEX mentions as $[x_i \\text{ TIMEX\\_q}]$ and TIMEX_q is the TIMEX ID. p and q starts from 0 and gets increased by 1 each time a new event mention or TIMEX mention is labeled. We define E to be the set of event mentions and T to be the set of TIMEX mentions. We define Y to be the set of four relation types where $Y = \\{\\text{coreference, temporal, causal, subevent}\\}$. $R_y$ is defined to be the set containing all the sub-relation types for $y \\in Y$, where $R_\\text{coreference} = \\{\\text{COREFERENCE}\\}$, $R_\\text{temporal} = \\{\\text{BEFORE, CONTAINS, OVERLAP, BEGINS-ON, ENDS-ON, SIMULTANEOUS}\\}$, $R_\\text{causal} = \\{\\text{CAUSE, PRECONDITION}\\}$, and $R_\\text{subevent} = \\{\\text{SUBEVENT}\\}$."}, {"title": "Bulk Prediction", "content": "Using the Bulk Prediction prompting, we query the LLM four times for each test document, with each query asking LLMs to list all relation pairs for each $y \\in Y$. For each query, we also provide an example document followed by the gold relation pairs for the same y as the query. Notice this is a 1-shot setting since we provide a whole document as an example."}, {"title": "Iterative Prediction", "content": "Algorithm 1 sketches the Iterative Prediction prompting method. We query LLMs by iterating through the document D sentence by sentence. For each new sentence S, we append S to all the previous sentences that are already augmented with event relations predicted by the model. Each S is queried four times for each $y \\in Y$. For coreference relation, we follow the Link-Append approach proposed by Bohnet et al. (2023) to augment the queried sentences. For temporal, causal, and subevent relations, we augment the sentences by inserting predicted relation tuples after the Event ID or TIMEX ID. A tuple is in the form $(e||t, r_y, e||t)$, where $e \\in E, t \\in T$, and $r_y \\in R_y$.\nWe experiment with two ways for providing demonstrations to the model: (1) whole doc, and (2) n-shot."}, {"title": "Event Ranking", "content": "For the Event Ranking prompting method, we query LLMs by iterating through e and t for $\\forall e \\in E, \\forall t \\in T$ in test document D as shown in Algorithm 2. We ask LLMs to complete the query (?, $r_y$, e||t), $\\forall e \\in E,\\forall t \\in T,\\forall r_y \\in R_y$, and $\\forall y \\in Y$. Note that we only need to query TIMEX mentions for temporal relation since TIMEX mentions are only relevant to temporal relations. We"}, {"title": "Pairwise", "content": "For the pairwise prompting method, we query LLMs with all the event mentions and TIMEX mentions pairs. We ask LLMs to complete the query (e||t, $R_y$ =?, e||t), $\\forall e \\in E,\\forall t \\in T$, and $\\forall y \\in Y$. Note that we only need to query TIMEX mentions for temporal relation since TIMEX mentions are only relevant to temporal relations. If there is no relation between two events then NONE should be predicted. We also provide one example for each sub-relation $r_y$ of all four relation types. Note that this prompt pattern is in purely natural language format. However, since the number of event pairs equals the number of times we query LLMs, which grows quickly as the number of events increases, this approach is not feasible to use for GPT-3.5 and GPT-4 if we take into account of financial and time costs. Therefore, we only test this prompt with LLaMA-2."}, {"title": "Model", "content": "For this study, we use both open-source LLMs and closed-sourced LLMs for evaluation. For open-source models, we use LLaMA-2 7B model, specifically Llama-2-7b-chat-hf, accessed through Huggingface. For closed-source models, we consider the gpt-3.5-turbo-16k model from OpenAI API as ChatGPT has been the most successful commercial LLMs so far. We test Llama-2-7b-chat-hf and gpt-3.5-turbo-16k on both validation and test sets using various different prompts.\nTo get an idea of how the newest GPT model performs, we also tested gpt-4-1106-preview model on a subset of the validation set instead of the whole test set becuase API calls to GPT-4 models are 30 times more expensive than GPT-3.5 models."}, {"title": "Prompt Decision", "content": "Before experimenting on the whole test set containing 857 documents, we test the four prompt patterns on the first 10 documents of the MAVEN-ERE validation set as running through the entire test set is both financially and time wise expensive. Table 1 shows our estimates of time and money needed to run each model through the whole test set using each of the latter three relatively costly prompts.\nFigure 2 shows the results of GPT-3.5 and LLaMA-2 using different prompts on the first 10 validation documents. The left sub-figure shows the results of GPT-3.5. We do not consider the Pairwise prompt for GPT-3.5 due to the extremely high cost on money. Among the remaining three prompts, Event Ranking achieves the best overall performance and Iterative Prediction performs relatively close. If we look into their performance over the relation types, Iterative Prediction wins on coference and causal relations while Event Ranking wins on the other two types of relations. Considering that Event Ranking is over ten times more expensive than Iterative Prediction (table 1), we choose Iterative Prediction over Event Ranking for running our full experiments.\nThe right sub-figure of Figure 2 shows the results of LLaMA-2. LLaMA-2 failed to generate consistent outputs when using Bulk Prediction and Event Ranking, thus we do not include those numbers. Between the Pairwise prompt and Iterative Prediction, the Pairwise prompt yields a slightly better overall performance. However, across the four relation types, the Iterative Prediction prompt, IP whole-doc, wins on three relation types (coreference, causal and subevent relations) while the Pairwise prompt only wins on temporal relations. In addition to performance, we also consider the dramatic running time differences between the two prompts (table 1), and we choose Iterative Prediction over the Pairwise prompt for running full experiments using LLaMA-2.\nTo summarize, we choose Iterative Prediction with its variations as the final prompts for both models when running the full experiments."}, {"title": "Supervised Baseline", "content": "We consider the baseline model proposed by Wang et al. (2022) as our baseline model. The baseline model utilizes roberta-base model from Huggingface as the underlying language model and trains separate classification heads for each relation type $y\\in Y$. The baseline model is trained end-to-end and performs pair-wise classification. We train the baseline model for each relation type separately for fair comparison against GPT-3.5, and LLaMA-2. We strictly follow the training and evaluating processes according to Wang et al. (2022)."}, {"title": "Results", "content": "SFT certainly improves the performance of LLaMA-2. However, it still underperforms the supervised baseline method for all the relation types as the number of used training documents increases. LLaMA-2 typically requires twice more training data to reach the same overall performance as the smaller supervised baseline model. LLaMA-2 only outperforms the supervised baseline method when the available training data is very limited, which benefits from its zero-shot capability emergent from large-scale pre-training. It also deserves mentioning that fine-tuning LLMs is much more expensive than fine-tuning smaller language models like ROBERTa. For example, fine-tuning using 200 training documents for 3 epochs requires approximately 72 hours for LLaMA-2 7B, while only about 1 hour is needed for the roberta-base baseline model."}, {"title": "Discussion", "content": "GPT-3.5 and LLaMA-2 Struggle to Follow the Prompt Consistently\nDuring the test of GPT-3.5 and LLaMA-2, we notice that both models create events or event relations that do not exist in text.\nIn addition, both models occasionally have difficulties in generating formatted answers, and the outputs may consist of random words from the document rather than Event or TIMEX ID or even event trigger words.\nAs GPT-3.5 has achieved overall better performance compared to LLaMA-2, we conduct further analysis on model performance mainly based on the predictions of GPT-3.5 on the 10 validation documents."}, {"title": "GPT-3.5 failed to learn transitivity rules", "content": "By manually examine the output of GPT-3.5 on the 10 validation documents, we notice that this model failed to learn the transitivity rules from the provided examples. When the output from GPT-3.5 contains tuples like (Event_0, BEFORE, Event_1) and (Event_1, BEFORE, Event_2), the tuple (Event_0, BEFORE, Event_2) can be inferred from the existing predictions. However, GPT-3.5 failed to include such tuples that can be inferred using transitivity rules. On the contrary, GPT-3.5 sometimes predict the opposite of the correct tuple. In this case, instead of (Event_0, BEFORE, Event_2), (Event_2, BEFORE, Event_0) will be predicted by GPT model, which clearly violates transitivity rules.\nAccording to Wang et al. (2022), 88.8% temporal relations and 23.9% causal relations can be inferred with transitivity rules in the MAVEN-ERE dataset. Failure to follow the transitivity rules detriments GPT's performance. Moreover, Table 3 highlights that false negatives and false positives emerge as the predominant error types, indicating that GPT-3.5 faces challenges in accurately discerning the presence or absence of relationships. Notably, in the context of temporal relationships, a noteworthy increase in the F1 score is observed when incorporating all transitivity-inferred relations. This implies that GPT-3.5 indeed falls short of capturing a comprehensive array of transitive relations."}, {"title": "Event Pairs with a Varying Distance", "content": "We investigate model performance on predicting intra- and inter-sentence event relations separately. Given that event coreference resolution relies on undividable clusters, we mainly analyze performance on the other three tasks. As shown in Table 4, GPT-3.5 is more capable to capture the relations between events that appear in the same sentence (intra-sentence) and otherwise struggles with capturing the inter-sentence event relations.\nWe also investigate the impact of sentence distance on model performance for inter-sentence cases. As shown in 4, the performance on temporal relation extraction decreases quickly as the number of sentences between two events increases; the performance on causal relation extraction also decreases a little when the number of sentences in between increases from one to two, but then the performance seems to remain stable when we further consider causal relations with five or more sentences in between. Overall, we observe lower performance on event pairs with greater distances."}, {"title": "Event Pairs in Contexts of Varying Event Densities", "content": "We investigate the impact of event density on model performance by examining the predictions of GPT-3.5 on the 10 validation documents. We only consider event pairs within the same sentence. Event density is measured as the number of event and TIMEX mentions appeared in one sentence. As shown in Table 5, the performance of GPT-3.5 on temporal and causal relations decreases quickly as the event density increases, indicating GPT-3.5 struggles to capture event relations when the complexity of the context increases."}, {"title": "Conclusion", "content": "In this study, we systematically evaluated the effectiveness of LLMs in performing discourse-level ERE tasks featuring lengthy documents and intricate relations. Our experiments using multiple prompt patterns uncover a noteworthy underperformance of LLMs when compared to the baseline established through supervised learning. Even with supervised fine-tuning, LLMs like LLaMA-2 still underperform the much smaller supervised baseline model when trained on the same amount of data. Furthermore, our quantitative and qualitative analyses revealed that LLMs face challenges in obeying transitivity rules, capturing inter-sentence relations or relations with a long distance, as well as comprehending context with dense event mentions. For future work, we will further investigate these challenges and develop methods for enabling LLMs to better address some of these issues in event relation extraction."}, {"title": "Limitation", "content": "Although we tried several different prompt patterns, there is still a chance that there exists better prompt to be used to assist GPT to solve the ERE task better. Meanwhile, OpenAI constantly update the GPT models that can be accessed throught the API, making it hard to reproduce the results if older models are deprecated. While OpenAI has offered preliminary introductions to various versions of GPT models, the specific implementation details remain obscure. This opacity hampers thorough analysis of why distinct versions of GPT models exhibit varying performance levels and how each data set and training technique influences models' performance. Finally, OpenAI API is a paid service, conducting experiments can get expensive depending on the task and design of the evaluation, making it not accessible to larger community. We are also limited by the cost and response time of OpenAI API.\nFor LLaMA-2 models, larger models (13B and 70B) may have better performance, but we leave the thorough study of LLaMA-2 models for potential future works."}, {"title": "Ethics and Broader Impact", "content": "We are aware that such study is very expensive and not very accessible to some researchers in the NLP community as OpenAI API is a paid service and is restricted in many countries. Not all researchers in our community can afford thousands of dollars or even more to run such experiments. Experiments on exclusive models or API may further detriment the inclusiveness of NLP community. Therefore, we hope our work can provide insights to readers with limited resources and inspire them in other works. However, by no means that we are advocating the NLP community to include closed-source LLMs as the baseline for any of the future work as studying the performance and behavior of closed-source models can be extremely difficult. We aim for our work to serve as a valuable resource for readers, helping them make decisions as they leverage LLMs for complex ERE tasks at discourse-level."}, {"title": "Appendix", "content": "We use the default hyperparameters for gpt-3.5-turbo-16k and gpt-4-1106-preview with the temperature set to 1, top_p set to 1, frequency penalty set to 0, and presence penalty set to 0. It takes around 48 - 60 hours to run though the test set of MAVEN-ERE for inference with gpt-3.5-turbo-16k. For LLaMA-2, we set temperature to 0.6, and top_p to 0.9, following the llama-recipes GitHub repository maintained by Meta. It takes around 36 - 48 hours to run though the test set of MAVEN-ERE for inference with Llama-2-7b-chat-hf.\nFor the baseline model, we train the model following the hyperparameters in Wang et al. (2022). We train the model with the learning rate sets to $1e^{-5}$ for the RoBERTa model, the learning rate sets to $1e^{-5}$ for the classification head, and the batch size sets to 4. For coreference, temporal, and causal resolutions, we train the model for 50 epochs. For subevent resolution, we train the model for 20 epochs. The training and inference time in total varies from 30 minutes to 2.5 hours depending on the relation type.\nFor SFT with Llama-2-7b-chat-hf, we fine-tune the model for 3 epochs, with a learning rate of $2e^{-4}$, weght decay of 0.001 for AdamW optimizer. 4-bit quantization is used to save memory space, and LoRA with 64 attention dimension, and 0.1 dropout rate is used for speeding up the training process."}, {"title": "Models' Versions", "content": "The gpt-3.5-turbo-16k currently points to gpt-3.5-turbo-0613, which is a snapshot of gpt-3.5-turbo from June 13th 2023 and will be deprecated on June 13, 2024, according to the OpenAI website https://platform.openai.com/docs/models. All our final experiment runs were conducted in a relatively focused period of time, in November 2023, so the model we used is the gpt-3.5-turbo-0613.\nThe gpt-4-1106-preview was released on November 6th, 2023, and has knowledge of world events up to April 2023, according to the OpenAI website https://platform.openai.com/docs/models. All our final experiment runs were conducted in a relatively focused period of time, in January 2024."}, {"title": "GPT-3.5 Experiments Cost", "content": "We use the gpt-3.5-turbo-16k model from OpenAI API. The cost for gpt-3.5-turbo-16k is $0.001 per 1k tokens for input and $0.002 per 1k tokens for output. We run through the MAVEN-ERE test set (857 documents) for 5 times since we run through the whole test set once for each of the whole doc, 1-shot, 2-shot, 5-shot, and 10-shot prompt. The total cost is around $1650, resulting $330 on average for each different prompt pattern used, $0.385 on average for annotating a document."}, {"title": "Prompt Patterns", "content": "Here, we provide some examples for the whole doc and n-shot prompt described in Sec 3.2. The system prompts for coreference, temporal, causal and subevent relations are shown in Table 6. We show an example of whole doc prompt for coreference and temporal relations in Table 7, and an example of 2-shot prompt for coreference and temporal relations in Table 8. Causal and subevent relations follow the same pattern as temporal relation.\nWe also show some examples for the two other prompt patterns: Bulk Prediction and Event Ranking. Examples for these two prompt patterns are shown in Table 9. We don't choose to use the Bulk Prediction prompt is because the performance is overall the worst comparing to other prompts when testing on the first 10 documents of the validation set. Event Ranking has relatively good performance on the first 10 validation document but require much more number of queries. We estimate a total cost of $3,200 if use the event ranking prompt to run through the whole test set once. Event Ranking prompt is also very time-consuming with an estimated 400 hours to run through the test set. Since using event ranking prompt is both financially and time-wise impossible, we don't choose to use it."}, {"title": "Detailed report of GPT-3.5, GPT-4 and LLAMA-2 Performance Over Validation Set", "content": "Here, we report the performance (precision, recall, and F-1 scores) of GPT-3.5 on the first 10 validation documents using the Iterative Prediction prompt patterns in Table 10. We report the performance (precision, recall, and F-1 scores) of GPT-4 on the"}]}