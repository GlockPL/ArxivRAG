{"title": "An End-to-End Approach for Korean Wakeword Systems with Speaker Authentication", "authors": ["Geonwoo Seo"], "abstract": "Wakeword detection plays a critical role in enabling AI assistants to listen to user voices and interact effectively. However, for languages other than English, there is a significant lack of pre-trained wakeword models. Additionally, systems that merely determine the presence of a wakeword can pose serious privacy concerns. In this paper, we propose an end-to-end approach that trains wakewords for Non-English languages, particulary Korean, and uses this to develop a Voice Authentication model to protect user privacy. Our implementation employs an open-source platform OpenWakeWord, which performs wakeword detection using an FCN (Fully-Connected Network) architecture. Once a wakeword is detected, our custom-developed code calculates cosine similarity for robust user authentication. Experimental results demonstrate the effectiveness of our approach, achieving a 16.79% and a 6.6% Equal Error Rate (EER) each in the Wakeword Detection and the Voice Authentication. These findings highlight the model's potential in providing secure and accurate wakeword detection and authentication for Korean users.", "sections": [{"title": "INTRODUCTION", "content": "The evolution of voice-based assistive technologies has reshaped the way users interact with artificial intelligence models. Modern Al-driven voice assistants often leveraging advanced generative AI-can understand complex natural language commands and perform diverse tasks. A critical enabler of these systems is wakeword detection, which ensures that the AI assistant only listens when a specific keyword is uttered. Yet, most public wakeword detection models still focus heavily on English, leaving significant gaps in support for other languages such as Korean. This imbalance not only hinders accessibility for non-English speakers but also limits the applicability of voice-based AI in global contexts[1].\nMoreover, the always-listening nature of wakeword-based solutions raises concerns about unauthorized access, as anyone who knows or accidentally speaks the wakeword may activate the system. As generative Al capabilities expand and become increasingly proactive, a robust authentication layer becomes essential. Our work addresses these issues by proposing an end-to-end methodology for Korean wakeword detection coupled with a speaker authentication step. The overall process involves four key units working in sequence:\n1. Pre-processing converts raw audio into mel-spectrograms.\n2. A shared feature extraction backbone transforms these mel-spectrograms into compact speech embeddings.\n3. A wakeword classification module identifies whether the uttered phrase matches the designated wakeword.\n4. A speaker authentication module verifies the identity of the speaker and grants access only to authorized individuals.\nThrough this two-step verification\u2014wakeword detection and speaker authentication\u2014we aim to provide a fast, accurate, and privacy-focused voice interface for Korean-language users. By integrating a Korean"}, {"title": "SYSTEM DESIGN AND ARCHITECTURE", "content": "In this work, we extend the open-source OpenWakeWord platform to detect and authenticate a Korean wakeword for enhanced privacy and accessibility[2]. Specifically, we adapt the system to recognize the wakeword \"\ud558\ub098\", chosen for its relatively simple phonetic structure and partial resemblance to certain English phonemes. This choice not only streamlines model adaptation but also enables text-to-speech (TTS) data augmentation\u2014we can use TTS-generated \"hana\" audio samples to diversify our training set without requiring an entirely separate large-scale dataset. Our method comprises a structured processing pipeline that sequentially detects the wakeword and verifies the speaker's identity, ensuring both efficiency and security."}, {"title": "2.1 Processing Pipeline", "content": null}, {"title": "Pre-processing Unit", "content": "We continuously monitor the incoming audio at a 16 kHz sampling rate. Once the system deems the audio ready for processing, it segments the stream into manageable chunks. These chunks are then converted into mel-spectrograms, preserving essential spectral features crucial for both detecting the wakeword and performing subsequent speaker authentication."}, {"title": "Shared Feature Extraction Backbone", "content": "The mel-spectrograms pass into a lightweight convolutional neural network (adapted from a TFHub module), where they are converted into 96-dimensional embeddings. This backbone is trained on large-scale audio data for generalization and then frozen to reduce computational overhead during inference. Incorporating data augmentation methods\u2014such as reverberation (using MIT's Room Impulse Responses) and various background noises-further improves robustness to real-world conditions."}, {"title": "Wakeword Classification", "content": "The 96-dimensional embeddings flow into a fully-connected classifier that identifies whether the utterance contains the target wakeword \"\ud558\ub098\". We employ gradient accumulation to simulate large batch sizes during training without exceeding memory constraints. The classifier outputs a wakeword detection score, indicating the probability of the target keyword's presence."}, {"title": "Speaker Authentication", "content": "Once a wakeword is detected, the system proceeds to verify the speaker's identity. Depending on the configuration, the system follows one of two approaches. In Approach A (Parallel Pipeline), the 96-dimensional speaker embedding obtained during wakeword detection is directly used for verification. This method allows for a faster authentication process since it does not require additional embedding computation.\nIn contrast, Approach B (Post-classification Trigger) generates a higher-dimensional (256-dimensional) speaker embedding from a local audio cache after wakeword detection. This additional step enables a more refined verification process before granting access, potentially improving accuracy at the cost of a slight increase in latency."}, {"title": "2.2 Korean Wakeword Selection", "content": "By incorporating a Korean-specific wakeword, leveraging TTS-based data augmentation for diverse training samples, and applying lightweight yet robust modeling techniques, our pipeline addresses both linguistic inclusivity and privacy concerns. Additionally, integrating the CUDA platform enables parallel computations and reduces latency, making our approach suitable for real-time deployment on resource-constrained devices. Since OpenWakeWord primarily supports English wakewords, we first adapted the pipeline for the Korean wakeword \"\ud558\ub098\". This choice was motivated by the relatively simple phonetic structure, as well as its partial resemblance to certain English phonemes. This allows us to utilize TTS-generated \"hana\" audio samples for data augmentation."}, {"title": "MODULE IMPLEMENTATION DETAILS", "content": null}, {"title": "3.1 Integrate OpenWakeWord platform", "content": "Open Wake Word processes incoming audio through a structured three-stage pipeline, with each stage operating within separate threads. Each stage incrementally transforms input data and forwards the processed output to the next stage. This modular approach ensures efficient wake word detection while maintaining low latency. Below we will explain operating details and parameters of each stages."}, {"title": "Pre-processing Unit (mels_proc)", "content": "The first stage of processing involves transforming raw audio input into mel spectrogram representations. This stage continuously receives 16 kHz audio samples from each client, accumulating them until a 1,760 sample being stored. The accumulated audio is then fed into a TensorFlow Lite (TFLite) model (melspectrogram.tflite), which computes the mel spectrogram frames."}, {"title": "Shared Feature Extraction Backbone (embeddings_proc)", "content": "The second stage extracts embeddings from the mel spectrogram frames. Once a client's buffer accumulates at least EMB_FEATURES of new mel frames (client.new_mels > EMB_FEATURES), a batch is formed and processed by a second TFLite model (embedding_model.tflite). This model generates embeddings, which are 96-dimensional feature vectors.\nThe input to the embedding model has the shape [batch_size, EMB_FEATURES, 32, 1], where EMB_FEATURES represents the number of mel frames required per embedding extraction. The output consists of embeddings with dimensions [batch_size, 1, embedding_windows, 96], where each embedding has 96 features. These extracted embeddings are stored in the wake word buffer, and the client.new_embeddings counter is updated accordingly.\nTo maintain efficiency, old embeddings are overwritten by new ones in a ring buffer system. This process ensures continuous adaptation to incoming audio while keeping memory usage stable. The mel-to-embedding transformation is performed only once and shared across multiple wake words to minimize redundant computation."}, {"title": "Wakeword Classification (ww_proc)", "content": "The final stage evaluates the presence of wake words by processing embeddings from the client's buffer. For each registered wake word (ww_model_key), a fixed window of embeddings (ww_windows) is extracted and input into a TFLite classification model. This model takes an input of shape [1, ww_windows, 96] and outputs a probability score indicating the likelihood of a wake word being detected.\nWhen client_data.new_embeddings reaches or exceeds the required number of embeddings for classification (client_data.new_embeddings > ww_windows), a batch is processed by the wake word model. The model's output probability is then compared against a predefined threshold (client_data.wake_threshold). If the probability exceeds this threshold, an activation counter (client_data.activations) is incremented. To confirm wakeword detection, the system requires a certain number of consecutive positive detections, defined by client_data.trigger_level. Once this level is reached, the system triggers a speaker identification model to verify the speaker's identity."}, {"title": "3.2 Speaker Authentication Strategy", "content": "In addressing the privacy gap where any user uttering the wakeword could inadvertently trigger the assistant and gain access to private information, two distinct speaker authentication strategies are proposed, each characterized by specific parameters such as wake thresholds, authentication thresholds, and temporal constraints to ensure both accuracy and responsiveness."}, {"title": "Approach A (Parallel Pipeline)", "content": "This strategy leverages the existing 96-dimensional embeddings produced during wakeword detection. As these embeddings are fed into the classifier, a concurrent verification pathway utilizes the same embedding to ascertain the speaker's identity. The speaker verification runs in parallel by comparing the same low-dimensional embedding against a stored reference using a cosine similarity measure. The authentication is considered successful only if the computed similarity surpasses both authentication threshold (auth_threshold) and the wakeword threshold (wake_threshold). This dual-threshold gating mechanism ensures fast and efficient verification, but the limited dimensionality of the embedding may pose challenges in speaker discrimination under certain conditions."}, {"title": "Approach B (Post-classification Trigger)", "content": "In this approach, once the system detects a wakeword with a probability exceeding the wake_threshold, it temporarily halts further processing in a cooldown period (20 frames) to avoid repeated triggers. During this interval, the system retrieves a 4,000 recent audio frames (0.25 seconds) from a local cache. This audio chunk is then processed by an encoder to produce a more expressive 256-dimensional speaker embedding, which is better suited for capturing the intricacies of the speaker's voice. The similarity between this newly computed embedding and a stored reference embedding for the authorized speaker is then calculated using cosine similarity. Only if the similarity exceeds auth_threshold does the system confirm the speaker's identity and proceed to fully activate the assistant. The use of a higher-dimensional embedding at this stage reduces false activations by non-authorized speakers while maintaining user convenience."}, {"title": "3.3 Implementation Details", "content": "All experiments were conducted within the OpenWakeWord framework, where we adapted various models and threshold settings to improve wake word detection accuracy for Korean, specifically for the wakeword \"\ud558\ub098\". Our primary objective was to balance high detection accuracy with real-time responsiveness, ensuring a smooth user experience in AI assistant applications."}, {"title": "Parallel Computing and CUDA Integration", "content": "To enhance inference speed, we integrated the CUDA platform into the pipeline, allowing the speaker verification model to efficiently utilize GPU acceleration. By leveraging parallel computation, we significantly reduced latency, enabling near-instantaneous wake word detection and verification on compatible hardware."}, {"title": "Reverberation and Background Noise Augmentation", "content": "We first introduced reverberation effects using MIT's Room Impulse Responses (RIR) dataset, allowing us to simulate a variety of room acoustics. This helped the model adapt to echoes and reflections, making it more resilient to real-world reverberant environments.\nAdditionally, we incorporated background noise augmentation by using publicly available noise datasets, which included ambient sounds such as caf\u00e9 chatter, traffic noise, and home appliance sounds. These augmentations improved the model's robustness against challenging acoustic conditions."}, {"title": "Audiomentations-Based Augmentation", "content": "We utilized the audiomentations library to introduce diverse audio distortions that improve robustness.\n\u2022 SevenBandParametricEQ: Adjusts frequency bands to simulate variations in microphone characteristics.\n\u2022 TanhDistortion: Introduces controlled distortion to improve robustness against low-quality recordings.\n\u2022 PitchShift: Randomly alters pitch to make the model resilient to natural voice pitch variations.\n\u2022 BandStopFilter: Removes specific frequency bands to simulate hardware-related limitations or low-quality audio sources.\n\u2022 AddColoredNoise: Injects colored noise at varying signal-to-noise ratios (SNRs), simulating diverse background conditions.\n\u2022 Gain Augmentation: Modifies audio loudness, ensuring wake words can be detected at different volume levels.\nThrough this end-to-end methodology, we successfully customized OpenWakeWord to detect the Korean wake word \"\ud558\ub098\" while incorporating a robust speaker authentication mechanism. By utilizing lightweight classification and verification models, our system achieves high detection accuracy while maintaining low-latency inference, a crucial factor for AI assistants in real-time environments.\nAdditionally, our extensive data augmentation pipeline, incorporating reverberation modeling, background noise addition, and adaptive transformations using audiomentations, ensures strong performance across diverse audio conditions. The integration of CUDA for GPU acceleration further optimizes performance, enabling fast and scalable wake word detection.\nBy combining efficiency, adaptability, and security, our system is well-suited for real-world deployment in AI assistants, ensuring accurate and privacy-conscious wake word detection in Korean-language applications."}, {"title": "3.4 Pseudocode", "content": "The pseudocode below clearly describes the parallel processing flow of the wake word pipeline and distinctly branches the two speaker authentication approaches (A and B). The pseudocode implements branching within conditional statements for Approaches A and B, demonstrating that different authentication procedures are executed depending on the user-selected method."}, {"title": "4.1 Data Description", "content": "Table 1 summarizes the dataset collected from seven individual speakers, resulting in a total of 1,287 audio samples. These samples are stored in WAV format and capture each speaker uttering the Korean word \"\ud558\ub098\". To ensure speaker diversity, we split the dataset into training (n = 919) and testing (n = 368) sets. Each speaker contributed a varying number of utterances ranging from 30 to 360 in the training portion-with corresponding test data proportionally assigned.\nAlthough all utterances contain the same wakeword \"\ud558\ub098\", speaker identities differ. We designate SGW as the authorized user and the remaining six participants as unauthorized users. Consequently, these wakeword-containing samples are divided into two categories:\n\u2022 voice-authp (wakeword positive, authorized user): Utterances from SGW\n\u2022 voice-authn (wakeword positive, unauthorized user): Utterances from all other speakers\nIn addition to the human-recorded samples, we generated 2,000 synthetic audio clips using openai-piper, selecting \u201chaanaa\" for its close phonetic resemblance to the Korean word \"\ud558\ub098\". We then organized these samples into two TTS-based categories:\n\u2022 tts-wwp (wakeword positives): Contains TTS-generated \"haanaa\" utterances\n\u2022 tts-wwn (wakeword negatives): Contains TTS-generated utterances but not including \"haanaa\".\nLastly, to evaluate the system's false activation rates, we included a Korean conversation dataset containing non-wakeword audio segments from AI-Hub. This dataset was segmented into short units, each containing a single sentence, enabling sentence-level detection/authentication analysis. Consequently, the overall false rejection rate (FRR) and false acceptance rate (FAR) could be expressed as percentages based"}, {"title": "4.2 Data Collection Method", "content": "All recorded voice data in Tables 1 and 2 were collected using a custom Python script named recorder.py. We bundled this script into an executable using PyInstaller, which allowed participants to run the recorder on various operating systems without installing Python or additional dependencies. The script automatically records 1-second clips at a 16 kHz sampling rate, separated by a 2-second wait period. Participants can easily pause and resume recording by pressing the space bar, with this functionality managed by a background keyboard listener (pynput). This design enables users to control the recording process and capture multiple wakeword utterances under different acoustic conditions without restarting the entire program."}, {"title": "4.3 Preprocessing & Data Augmentation", "content": "Prior to training, we performed two core preprocessing steps: normalization and voice activity detection (VAD). We used a custom script (normalize.py\u00b2) based on the PyDub library to standardize audio amplitudes. For VAD, we employed vad.py \u00b3to detect and remove silent sections in each WAV file. However, since we wished to investigate VAD's impact on final performance, we retained two variants of each dataset: one with VAD-applied clips and one without. This approach allowed us to compare how removing silence affected both wakeword detection and speaker authentication.\nAfter optional VAD trimming, we augmented the audio clips to improve model robustness. We leveraged audiomentations and torch_audiomentations with fixed probabilities, including 75% for background noise addition, 25% for pitch shifting, and 50% for RIR-based reverberation. This augmentation process helps the model generalize more effectively to real-world acoustic variations, such as echo-prone rooms and noisy environments."}, {"title": "4.4 Experimental Setup", "content": "In this study, all wakeword models were trained using Google Colab's GPU-accelerated environment to support large-scale experiments, while inference and real-time tests were conducted on an NVIDIA Jetson Nano running Ubuntu 20.04. The Jetson Nano, equipped with a CUDA-capable GPU, allowed partial offloading of speaker authentication tasks to the GPU, resulting in faster response times. The software environment was based on a customized Ubuntu 20.04 image, PyTorch 1.13.0 (with CUDA 10.2), and Python 3.8."}, {"title": "4.5 Evaluation Metrics", "content": "The False Rejection Rate (FRR) is calculated using the formula:\n$FRR = \\frac{FN}{FN + TP}$\nThis represents the proportion of actual positive cases that are incorrectly classified as negative by the model. Here, FN (False Negatives) denotes the number of positive instances that the model fails to recognize, while TP (True Positives) denotes the number of positive instances that the model correctly identifies. Because the denominator (FN + TP) covers all actual positive cases, calculating FRR requires a positive dataset that contains examples where the event truly occurs. This metric reflects how often the system misses true positive events, such as failing to detect the wakeword when it is present or not recognizing an authorized speaker when they speak.\nThe False Acceptance Rate (FAR) is given by the formula:\n$FAR = \\frac{FP}{FP + TN}$\nFAR measures the proportion of actual negative cases that the model incorrectly classifies as positive. In this equation, FP (False Positives) is the number of negative instances that are wrongly identified as positive, and TN (True Negatives) is the number of negative instances that the model correctly classifies. Since the denominator (FP + TN) encompasses all actual negative cases, computing FAR requires a negative dataset that contains examples where the event of interest is absent. A high FAR would mean that the system often falsely identifies the absence of a wakeword as its presence or mistakenly authenticates an unauthorized speaker.\nIn addition to FRR and FAR, the Equal Error Rate (EER) is a key metric for evaluating the balance between these two types of errors. EER is defined as the error rate at which the False Acceptance Rate and the False Rejection Rate are equal[3]. To determine the EER, one varies the decision threshold of the model and computes FAR and FRR at each threshold. The EER is found at the threshold where FAR equals FRR. A lower EER indicates a more accurate and balanced model performance.\nTo calculate these metrics correctly, the dataset must consist of specific types of data that represent positive and negative cases. For FRR, a positive dataset is required. In the context of the wakeword detection model, this positive dataset might include audio samples from speakers saying the Korean wakeword \"\ud558\ub098\".\nFor instance, using the voice_919 combination, positive cases would come from both authorized (voice-authp) and unauthorized (voice-authn) users uttering \"\ud558\ub098\" while the mix_919 combination would additionally include TTS-generated \"hana\" samples (tts-wwp). These samples represent true positive scenarios for the model, where the wakeword is present and should be detected. The FRR is then computed by running the model on these positive samples, counting how often the wakeword is rejected (False Negative), and applying the FRR formula.\nFor FAR in the wakeword detection model, a negative dataset is necessary. This consists of audio samples that do not contain the wakeword, such as those from the tts-wwn category, where TTS-generated utterances"}, {"title": "5.1 Wakeword Detection Model", "content": "We evaluated wakeword detection performance by sweeping threshold values from 0.0 to 1.0 for each model variant-voice_919, voice_919_vad, mix_919, and mix_919_vad\u2014and measuring the False Reject Rate (FRR) and False Acceptance Rate (FAR). To determine the optimal threshold for each model, we identified the Equal Error Rate (EER) where FRR and FAR intersect. Table 5 summarizes the results.\nAs illustrated in Figure 3, voice_919 shows a high FRR at increasing thresholds, making it prone to rejecting genuine wakeword activations. It achieves an EER of 22.61% at a threshold of 0.10.\nFigure 4 presents voice_919_vad, which applies voice activity detection (VAD) to filter out non-speech segments. While this reduces FAR, it also increases FRR, yielding an EER of 22.95% at a threshold of 0.05.\nFigure 5 highlights mix_919, which benefits from incorporating TTS-based wakeword positives during training. This model achieves an EER of 20.52% at a threshold of 0.05, showing a more balanced trade-off between FRR and FAR compared to voice_919.\nFigure 6 demonstrates mix_919_vad, which combines VAD with TTS-based wakeword training. It achieves the lowest EER of 16.79% at a threshold of 0.05, making it the most balanced model in the evaluation."}, {"title": "5.2 Speaker Authentication Model", "content": "We evaluated speaker authentication using two approaches: Approach A (96-dimensional embeddings) and Approach B (256-dimensional embeddings from Resemblyzer, x-vector, or ECAPA). The threshold sweep results for each model are shown in Figures 1-4, highlighting how the False Rejection Rate (FRR) and False Acceptance Rate (FAR) change across different thresholds. Table 6 describes the optimal threshold and Equal Error Rate (EER) of each models."}, {"title": "5.3 CPU vs CUDA Acceleration", "content": "In addition to evaluating model accuracy and threshold optimization, we compared inference speed when running the speaker authentication module on the CPU versus leveraging CUDA acceleration. Table 7 summarizes the average loading time for the voice encoder model, as well as the mean authentication time per wakeword event across a series of test utterances (both successes and failures).\nIn summary, GPU acceleration significantly reduces per-wakeword authentication time, enhancing the responsiveness of AI assistants in continuous listening scenarios. However, developers must account for the longer initial model load on CUDA devices."}, {"title": "DISCUSSION", "content": "The evaluation results underscore the inherent trade-offs in wakeword detection and speaker authentication, highlighting the Equal Error Rate (EER) as a crucial metric. For wakeword detection, the mix_919_vad model-combining Voice Activity Detection (VAD) with TTS-based training-demonstrated the most promising performance. It achieved a notably low EER of 16.79% at a threshold of 0.05, significantly outperforming models relying solely on speech data (e.g., voice_919 with 22.61% EER) or applying only VAD (voice_919_vad with 22.95% EER). By integrating both real and synthetic wakeword data along with effective silence trimming, mix_919_vad reduced both False Reject Rates (FRR) and False Acceptance Rates (FAR), leading to an improved balance between security and user convenience as evidenced by the lower EER.\nIn the domain of speaker authentication, high-dimensional (256D) embedding approaches delivered superior results. Among the evaluated models, Resemblyzer achieved the lowest EER of 6.60% at a threshold of 0.80, outperforming other alternatives such as x-vector (10.37% EER), ECAPA (9.54% EER), and the lower-dimensional 96D embedding model (22.23% EER). The low EER of Resemblyzer indicates that high-dimensional embeddings are more effective at capturing speaker characteristics, thereby simultaneously reducing FRR and FAR. This improvement is particularly significant for privacy, as a lower EER means a reduced likelihood of unauthorized voices being accepted.\nFocusing on EER in our analysis suggests that dynamically adjusting thresholds-considering environmental factors like background noise or user-specific speaking patterns can further refine both wakeword detection and speaker authentication accuracy. Small improvements in EER can have a significant impact on large user bases and over extended periods of system use. Dynamic threshold adjustments-guided by environmental factors such as background noise levels or user-specific patterns like individual speaking styles-can refine both wakeword detection and speaker authentication accuracy. Ongoing data collection from a variety of environments can further enhance robustness. Over time, the system could autonomously learn to fine-tune these thresholds, ensuring minimal inconvenience without sacrificing security.\nIf multiple consecutive authentication attempts fail, the system can implement various security measures to prevent unauthorized access. For example, after three unsuccessful authentications, the voice interface might require additional verification (e.g., a passcode) or temporarily block voice access to safeguard sensitive data. Additionally, it could send a notification to the administrator's phone, allowing immediate awareness of potential security risks. These measures not only enhance security but also ensure that the system remains robust in real-world deployments.\nOverall, our end-to-end Korean wakeword and speaker authentication approach showcases promising performance but also illuminates areas especially FRR improvements for further innovation. By integrating a robust speaker verification stage, we ensure that simply uttering the wakeword \"\ud558\ub098\" does not automatically grant access to unauthorized individuals, thereby aligning with contemporary data privacy concerns."}, {"title": "CONCLUSION", "content": "In this paper, we presented an end-to-end Korean wakeword detection and speaker authentication approach that addresses key privacy and performance challenges. By leveraging an FCN-based wakeword classifier, data augmentation strategies, and a CUDA-accelerated speaker verification pipeline, our system demonstrates the viability of Korean wakeword detection without sacrificing security. Experimental results highlight that incorporating TTS data and VAD can reduce false activations, and that 256-dimensional embeddings (e.g., Resemblyzer) outperform lower-dimensional ones for speaker authentication on resource-constrained hardware.\nMoreover, our findings emphasize the delicate balance between FRR and FAR and underscore the importance of dynamic thresholding and robust data collection. While the proposed model already shows"}]}