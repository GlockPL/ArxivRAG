{"title": "Game-Theoretic Joint Incentive and Cut Layer Selection Mechanism in Split Federated Learning", "authors": ["Joohyung Lee", "Jungchan Cho", "Wonjun Lee", "Mohamed Seif", "H. Vincent Poor"], "abstract": "To alleviate the training burden in federated learning while enhancing convergence speed, Split Federated Learning (SFL) has emerged as a promising approach by combining the advantages of federated and split learning. However, recent studies have largely overlooked competitive situations. In this framework, the SFL model owner can choose the cut layer to balance the training load between the server and clients, ensuring the necessary level of privacy for the clients. Additionally, the SFL model owner sets incentives to encourage client participation in the SFL process. The optimization strategies employed by the SFL model owner influence clients' decisions regarding the amount of data they contribute, taking into account the shared incentives over clients and anticipated energy consumption during SFL. To address this framework, we model the problem using a hierarchical decision-making approach, formulated as a single-leader multi-follower Stackelberg game. We demonstrate the existence and uniqueness of the Nash equilibrium among clients and analyze the Stackelberg equilibrium by examining the leader's game. Furthermore, we discuss privacy concerns related to differential privacy and the criteria for selecting the minimum required cut layer. Our findings show that the Stackelberg equilibrium solution maximizes the utility for both the clients and the SFL model owner.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent innovations in Distributed Collaborative Machine Learning (DCML) have opened new possibilities for developing scalable and accurate Artificial Intelligence (AI) solutions. By leveraging diverse datasets across multiple clients without transmitting personal data to a central server, these methods significantly reduce the risk of privacy leakage. Federated learning (FL) and split learning (SL) are two popular approaches, though both face practical limitations [1]\u2013[3]:\n\u2022 FL: FL allows a server model to be trained in parallel across many clients, but each client is required to run an entire model, resulting in a significant training burden. This leads to high battery power consumption on clients, significant client-side computation requirements, and limited model privacy.\n\u2022 SL: SL splits the entire model into smaller portions, allowing only a shallow sub-network to be trained on the client side, thus reducing the training burden. However, the training process is sequential, and client models are not aggregated, leading to significant time overhead.\nTo overcome these limitations, a new distributed learning framework, Split Federated Learning (SFL), has recently been proposed. SFL combines the advantages of both FL and SL, aiming to reduce the training burden of FL while enhancing convergence speed. More specifically, in SFL, clients train only a portion of the entire model, known as the client-side model, which decreases their computational load. These client-side models are then synchronized to enhance convergence speed via FL process [2], [4].\nHowever, despite its notable advantages, SFL introduces additional communication overhead when interacting with servers and also raises privacy concerns due to the frequent exchange of client-side model outputs and updates, which are correlated to raw data. Optimizing SFL management to address these challenges remains a complex issue [1], [2]. Hence, there have been various approaches to managing SFL in the literature, aiming to achieve efficient communication in SFL operations. These approaches consider factors such as cut layer selection, clustering, management of radio and computational resources (e.g., CPU/GPU), and gradient compression [2], [5]\u2013[8].\nNevertheless, the aforementioned work assumes that clients are motivated to voluntarily participate in the SFL process without any incentives. In reality, participating in the SFL process requires substantial energy consumption, which depends on the complexity of the client-side model and the amount of data contribution to train the model from the client's perspective. Therefore, without suitable incentives that take into account the cut layer selection (i.e., the complexity of the client-side model), motivating clients to engage in the SFL process to generate an accurate model is challenging. Although there have been various studies on incentive mechanisms in FL (see, for example, [9] and the references therein), there exist no studies specifically addressing SFL's unique characteristics as follows. i) Competition arises between the SFL model owner, who aims to create a global model from the SFL process for AI services, and the clients, who participate in the SFL process. Additionally, given the incentives, there is further competition among clients striving to earn more rewards. Thus, understanding the clients' reactions to the optimal decisions and behaviors of the SFL model owner is crucial. ii) This necessitates a careful study of sophisticated incentive mechanisms and cut layer selection, considering how these competitive behaviors can influence the data contributions of clients. For instance, if the client-side model size increases, resulting in higher model complexity, clients will face a greater training burden. This may lead them to reduce their data contributions unless they receive sufficient incentives.\nTo the best of our knowledge, our work is the first to examine the competitive situation between clients and the SFL model owner during the SFL process. Our aim is to design an integrated approach to cut layer selection and incentive management from the SFL model owner's perspective, as well as data contribution management from the clients' perspective. Our contributions are summarized as follows:\n\u2022 We investigate the interactions between the SFL model owner and clients, which create competitive situations during the SFL process. This is formulated as a single-leader and multi-follower game.\n\u2022 To achieve this, we develop analytical models to explore the trade-offs for the SFL model owner, including satisfaction with the acquired training data, load reduction, and payment for incentives. For the clients, we examine the balance between the energy consumption required for the SFL process and the anticipated incentives.\n\u2022 We derive closed-form expressions for the Nash equilibrium among heterogeneous clients and develop algorithms to obtain Stackelberg strategies by proving the existence of these solutions. Additionally, we discuss privacy enhancements related to cut layer selection and differential privacy.\n\u2022 Our findings show that the Nash and Stackelberg strategies are desirable operating points for all participants (i.e., the SFL model owner and clients) in competitive situations. From our classification experiments using the CIFAR-10 dataset, we highlight the importance of incentive mechanisms in accelerating SFL and enhancing model accuracy. In our experiment, accuracy varies from 56% to 87% depending on the incentives provided.\nThe remainder of this paper is organized as follows. Section II presents the system model and formulates the utility functions for both the SFL model owner and the clients. In Section III, we describe the competitive game between the SFL model owner and the clients, rigorously formulating the problem as a Stackelberg game. We then characterize the closed-form solutions for the Nash Equilibrium of the clients and provide the algorithm to obtain the Stackelberg Equilibrium. Section IV discusses the impact of cut layer selection and differential privacy on the privacy level, offering guidelines for setting the minimum cut layer requirements during the SFL process. Section V reviews related work on cut layer selection and resource management for SFL. Finally, Section VI concludes the paper and suggests directions for future research."}, {"title": "II. SYSTEM MODEL", "content": "The SFL framework consists of two servers (that can be either physical or logical servers): (i) a federated learning server, referred to as the fed server, and (ii) a main server, along with multiple clients participating as part of the SFL process. In this setup, we define a set of clients as $\\mathbb{N} = {1,2,..., N}$, where each client is represented as $n \\in \\mathbb{N}$ and has its own local dataset $D_n$. The size of each dataset $D_n$ is indicated by the number of data items, denoted as $|D_n| = D_n$, with each data item comprising both features and labels. The entire model, denoted as $w$, is composed of $L$ layers. Specifically, the model $w$ is divided into two sub-models at the $L_c$-th layer, referred to as the cut layer. These sub-models are: (i) the client-side model $w_c$, including layers from the first layer up to layer $L_c$, and (ii) the server-side model $w_s$, encompassing layers from layer $L_c + 1$ to layer $L$. Thus, the full model can be represented as $w = [w_c; w_s]$. As shown in Fig. 1, each client $n$ in the set $\\mathbb{N}$ maintains its client-side model, denoted as $W_{c,n}$. These client-side models are trained in parallel using the clients' respective local datasets, through interactions with the server-side model $w_s$ hosted on the main server. To synchronize the client-side models, clients participate in federated learning, coordinated by the fed server, to construct a global client-side model $w_c$ in a collaborative manner [10].\nFinally, the entire SFL process is carried out over $T$ rounds. At the beginning of each round $t$, clients are provided with the latest global client-side model $w_c$ from the fed server. Following this, both the clients and the main server engage in training over $E$ epochs, updating their respective client-side models $w_{c,n}$ for each client $n \\in \\mathbb{N}$, and the server-side model $w_s$. Then, each client $n$ randomly samples a mini-batch instance $I_n$ from its dataset $D_n$. Note that during each epoch, clients randomly partition their datasets into $M$ mini-batches. Stochastic gradient descent (SGD) is applied to each mini-batch, allowing clients to update their client-side models $W_{c,n}$ and the main server to update the server-side model $w_s$ in a coordinated fashion. Once the $E$ epochs are concluded, the fed server collects the updated client-side models $w_{c,n}$ from all clients and combines them to form an updated global client-side model $w_c$."}, {"title": "A. Utility Function of SFL Model Owner", "content": "In our scenario, the SFL model owner determines the cut layer $L_c$ and the incentives $R$ provided to clients based on their data contributions, with the goal of maximizing the owner's utility function. In this context, we consider the following utility function:\n$U_{MO}(R, L_c, d) = \\tau_1 \\ln(1 + \\sum_{n=1}^{N} \\frac{d_n}{d_{req}}) + \\tau_2 \\frac{f_{FLOPS}(L_c)}{W_{FLOPS}} - R,$\n(2)\nwhere the vector $d = (d_1,d_2,...,d_N)$ represents the data contribution of each client from its local dataset $D_n$, while $d_{req}$ denotes the required amount of aggregated data contribution necessary to achieve an acceptable accuracy, which is a service-dependent requirement. $\\tau_1, \\tau_2 \\geq 0$ denotes the weighting factor for the first and second terms, respectively. Moreover, $W_{FLOPS}$ is the computation workload to proceed with the full model $w$ during its forward and backward propagation for a single data sample, measured in Floating Point Operations Per Second (FLOPS). Here, $f_{FLOPS}(L_c)$ as a function of the computation workload, measured in FLOPS, of the client-side model $w_c$ with respect to $L_c$. In this context, $f_{FLOPS}(L_c)$ is the increasing function of $L_c$ since the computation workload increases with the number of layers. Nevertheless, due to the difficulty of analytically understanding this model-dependency, a data-driven approach can be applied to model its relationship through offline training using empirical measurements. Hence, we adopted regression-based modeling, which is one of the most widely used approaches, exemplified as mobile CPU properties, such as CPU power and temperature variation modeling [11], [12]. Consequently, as shown in Fig. 2, we conclude that $f_{FLOPS}(L_c)$ is a linear function of $L_c$, modeled as $f_{FLOPS}(L_c) = aL_c + b$, where $a$ and $b$ are coefficient values. Finally, this utility function quantifies the balance between the satisfaction derived from aggregated data contribution from clients, and the gain from distributing training workload to clients, against the expenditure $R$ allocated to clients during the SFL process. The range of $R$ is set by the SFL model owner within $(0, R_{max}]$, where $R_{max}$ represents the upper limit of incentives regulated by the SFL model owner's policy. Note that in the right-hand side of"}, {"title": "B. Utility Function of Clients", "content": "Our design of the utility function for the clients comprises two terms. The first term denotes the incentive allocated by the SFL model owner, while the second term represents the energy expenditure incurred from participating in the SFL process.\n1) Overall Computation Energy of client: Using $f_{FLOPS}(L_c)$ to model the computation time for each client $n$, we obtain computation latency for the client-side model to process the mini-batch instance $\\mathbb{S}_k$ can be obtained by\n$t_n = \\frac{\\frac{d_n}{M}f_{FLOPS}(L_c)}{f_n\\kappa}, \\forall n \\in \\mathbb{N},$\n(3)\nwhere the size of the mini-batch instance $k$ is denoted by $|\\mathbb{S}_k| = d_n/M$. Furthermore, $f_n$ denotes the central processing unit (CPU) capability of client $n$, and $\\kappa$ represents the computing intensity. Consequently, the energy consumption for computing the client-side model for client $n$ is given by $E_{cmp,n} = p_n t_n$, where $p_n$ is the computation power of client $n$.\n2) Overall Communication Energy of Client: To streamline our analysis and focus on the core intuition, we adopt a simplified approach similar to [17]. This involves assuming fixed uplink and downlink channels, maintaining a quasi-static channel mode throughout the SFL process facilitating efficient analysis. Then, the transmission latency, both i) for transmitting the smashed data from client $n$ to the main server and ii) for receiving the gradient data from the main server to the client $n$, is given by:\n$t_{s:n} = \\frac{s_n}{r_{s:n}}, t_{g:n} = \\frac{g_n}{r_{g:n}}, \\forall n \\in \\mathbb{N}.$\n(4)\nHere, $s_n$ and $g_n$ denote the size of the smashed data and the gradient data of client $n$, respectively. Additionally, $r_{s:n}$ and $r_{g:n}$ represent the uplink transmission rate from client $n$ to the main server and the downlink transmission rate from the main server to client $n$, respectively. Similarly, the transmission latency, both i) for transmitting the client-side model from client $n$ to the fed server and ii) for receiving the global client-side model from the fed server to the client $n$, is given by:\n$t_{cm:n} = \\frac{m|w_c|}{r_{cm:n}}, t_{gcm:n} = \\frac{m|w_c|}{r_{gcm:n}}, \\forall n \\in \\mathbb{N}.$\n(5)\nHere, $r_{cm:n}$ and $r_{gcm:n}$ represent the uplink transmission rate from client $n$ to the fed server and the downlink transmission rate from the fed server to client $n$, respectively. $m$ represents the number of bits for a single model parameter, where $|w_c|$ is the total number of model parameters in the client-side model. Note that the size of the global client-side model is the same as that of the individual client-side model. In this context, $|w_c|$ is the function of $L_c$ since the number of model parameters increases with the number of layers. Similar to $f_{FLOPS}$, due to the difficulty of analytically understanding, we also adopted regression-based modeling. Consequently, as shown in Fig. 2, we conclude that $|w_c|$ is a convex function of $L_c$, modeled as $|w_c| = c \\exp(dL_c)$, where $c$ and $d$ are coefficient values. Then, with given $p^t_n$ and $p^r_n$ of transmission and receiving power consumption of client $n$, overall communication energy for interacting with main server $E_{com-m:n}$ and fed server $E_{com-f:n}$ can be calculated by\n$E_{com-m:n} = p_n^t t_{s:n} + p_n^r t_{g:n}, \\forall n \\in \\mathbb{N}.$\n(6)\n$E_{com-f:n} = p_n^t t_{cm:n} + p_n^r t_{gcm:n}, \\forall n \\in \\mathbb{N}.$\n(7)\n3) Overall energy consumption: The energy consumption for $E$ epochs of client $n$ with $M$ mini-batches is given by $E_{epoch} = EM(E_{cmp,n} + E_{com-m,n}), n \\in \\mathbb{N}$. Consequently, the overall energy consumption for $T$ rounds, covering the entire duration until the SFL system completes its operations, is calculated by\n$E_{tot,n} = T(E_{epoch} + E_{com-f:n}), n\\in \\mathbb{N}.$\n(8)\nThen, in order to demonstrate the influence of $L_c$ and $d_n$ on $E_{tot,n}$, (8) can be simplified as\n$E_{tot,n} = T((\\frac{d_nf_{FLOPS}(L_c)}{f_n\\kappa})A_n + B_n) + c \\exp (dL_c)C_n), n\\in \\mathbb{N},$\n(9)\nwhere $A_n = \\frac{p_n}{M}, B_n = EM E_{com-m,n}$, and $C_n = \\frac{mp^r_n}{T_{gcm:n}} + \\frac{mp^t_n}{T_{cm:n}}$. Similarly, (9) can be further simplified to highlight only the impact of $d_n$ on $E_{tot,n}$, which is given by\n$E_{tot,n} = d_n H_n + I_n, n\\in \\mathbb{N},$\n(10)\nwhere $H_n = T(\\frac{f_{FLOPS}(L_c)}{f_n\\kappa})A_n$ and $I_n = T(B_n + c \\exp (dL_c)C_n)$."}, {"title": "III. GAME-THEORETIC ANALYSIS", "content": "In this section, we examine the competitive dynamics between the SFL model owner and the clients, while also considering the competition among clients to gain more incentives. These competitive behaviors can be considered as a hierarchical noncooperative decision problem, analyzable as a Stackelberg game. Stackelberg games are a type of noncooperative game characterized by a hierarchy among players, where they are classified as leaders and followers [18]\u2013[20]. In this study, we consider a scenario where the clients follow the behavior of the SFL model owner. Treating the SFL model owner as the leader and the clients as the followers, the competition among players to maximize their utility is modeled as a two-level Stackelberg game. If the SFL model owner decides on the cut layer and the total amount of incentives, the clients will respond by selecting their optimal strategies regarding data contribution for the SFL process. The SFL model owner is aware that the clients will choose their best response strategies based on the owner's decisions. Therefore, the SFL model owner optimizes its strategy to maximize utility, taking into account the clients' best responses. To find the Stackelberg Equilibrium (SE), which is the solution to this game, we employ a backward induction technique introduced in [18], [19]. As illustrated in Fig. 3, this technique can be summarized as follows. First, in the lower-level game for the clients, referred to as Stage 1: Data Contribution Management Competition, the clients determine the amount of data to contribute to maximize their utility function (12), given the incentives and cut layer configuration. Then, in the upper-level game for the SFL model owner, referred to as Stage 2: Maximizing Utility of SFL Model Owner, the SFL model owner combines the clients' strategies with its utility function (2) and selects the cut layer and incentive amounts that maximize its own utility function (2)."}, {"title": "A. Stage 1: Noncooperative Game Among Clients", "content": "Each client aims to maximize the utility function (12) within the dataset constraints [0, $D_n$]. With given the vector $d = (d_1, d_2,...,d_N)$, let $d_{-n}$ denote the profile of strategies for the clients except $n$, i.e., $d_{-n} = (d_i)_{l\\in \\mathbb{N}\\{n}}$. Then, by definition, we have $d = (d_n, d_{-n})$.\nNote that, to simplify the problem, we relax the integer variable $d_n$ into a continuous variable. Thus, the optimization problem to find the optimal amount of data contribution for the SFL process is given by\n$\\underset{d_n}{\\text{maximize}} \\quad U_n (d, R, L_c)$\n(13)\nsubject to\n$0 \\leq d_n \\leq D_n, \\quad \\text{for } n\\in \\mathbb{N}$.\nDefinition 1. The best response (BR) function $B_n (d_{-n}, R, L_c)$ of client $n$ as a follower is the best strategy for client $n$ given the other clients' strategies $d_{-n}$ and SFL model owner strategies $R$ and $L_c$. The BR function is denoted as follows:\n$B_n(d_{-n}, R, L_c) = \\underset{d_n}{\\text{arg max }}U_n (d_n, d_{-n}, R, L_c) \\quad \\text{for } n \\in \\mathbb{N}.$\n(14)\nDefinition 2. A Nash equilibrium of the noncooperative game among the clients is a profile of strategies $d^* = (d^*_1, d^*_2,...,d^*_N)$ with the property that, given the SFL model owner strategies $R$, and $L_c$, we obtain\n$d^*_n = B_n(d^*_{-n}, R, L_c), \\quad \\text{for } n \\in \\mathbb{N}.$\n(15)\nTheorem 1. The utility function $U_n(d_n, d_{-n}, R, L_c)$ of client $n$ is strictly concave on $d_n$. Then, a Nash equilibrium exists in the noncooperative game among the clients."}, {"title": "Proof.", "content": "Taking the first and second derivatives of $U_n(d_n, d_{-n}, R, L_c)$ with respect to $d_n$, we obtain\n$\\frac{\\partial U_n (d_n, d_{-n}, R, L_c)}{\\partial d_n} = \\psi_n R \\frac{\\sum_{l \\in \\mathbb{N}\\{n\\}} d_l}{(\\sum_{l=1}^{N} d_l)^2} - H_n,$\n(16)\n$\\frac{\\partial^2 U_n (d_n, d_{-n}, R, L_c)}{\\partial d_n^2} = -2\\psi_n R \\frac{\\sum_{l \\in \\mathbb{N}\\{n\\}} d_l}{(\\sum_{l=1}^{N} d_l)^3} < 0,$\n(17)\nIt follows from (17) that the second derivatives of $U_n$ is negative. This implies that the utility function $U_n(d_n, d_{-n}, R, L_c)$ is strictly concave on $d_n$ where the dataset constraints [0, $D_n$] is a compact and convex set. Then, the proposed noncooperative client-level game has a Nash equilibrium.\nBy equating the first derivative of $U_n(d_n,d_{-n}, R, L_c)$ in (16) to zero, we obtain\n$\\psi_n R \\frac{\\sum_{l \\in \\mathbb{N}\\{n\\}} d_l}{(\\sum_{l=1}^{N} d_l)^2} = H_n.$\nThen, solving the above equation as\n$d_n = \\frac{\\psi_n R \\sum_{l \\in \\mathbb{N}\\{n\\}} d_l}{H_n} - \\sum_{l\\neq n} d_l.$\n(18)\nAdditionally, if the expression on the right-hand side of (18) exceeds $D_n$, the client $n$ will participate by setting $d_n = D_n$. Consequently, the BR function $B_n(d_{-n}, R, L_c)$ (also denoted as $d^*_n$) can be concluded as follows:\n$d_n^* = \\begin{cases}\n0, & \\text{if } \\frac{H_n}{\\psi_n R} < \\sum_{l \\in \\mathbb{N}\\{n\\}} d_l, \\\\\nD_n, & \\text{if } 0 < d_n < D_n, \\\\\n(18) & \\text{otherwise} .\n\\end{cases}$\n(19)\nTheorem 2. For the noncooperative game among clients, the unique Nash equilibrium has a closed-form expression given by the following:\n$d_n^* = \\frac{(\\mathbb{N}-1)R}{\\sum_{l \\in \\mathbb{N}} \\frac{H_l}{\\psi_l}} (1 - \\frac{H_n(\\mathbb{N}-1)}{\\psi_n \\sum_{l \\in \\mathbb{N}} \\frac{H_l}{\\psi_l}}), \\quad \\forall n \\in \\mathbb{N},$\nwhere $d_n^*$ should satisfy the constraint of [0,$D_n$] as specified in (19).\nProof. From (19), for any client $n \\in \\mathbb{N}$, we obtain\n$\\sum_{l \\in \\mathbb{N}} d_l = \\frac{\\psi_n R \\sum_{l \\in \\mathbb{N}\\{n\\}} d_l}{H_n}.$\nBy setting $\\eta = \\sum_{l\\in \\mathbb{N}} d_l^*$, we can derive that\n$d_n^* = \\eta - \\frac{\\eta^2 H_n}{\\psi_n R}.$\nBy summing (21) over $\\mathbb{N}$ clients, then we have\n$\\eta = \\mathbb{N} \\eta - \\frac{\\eta^2}{R} \\sum_{l \\in \\mathbb{N}} \\frac{H_l}{\\psi_l}.$\n(20)\n(21)\n(22)\nBy solving the problem of (22), we obtain\n$\\eta = \\frac{(\\mathbb{N}-1)R}{\\sum_{l \\in \\mathbb{N}} \\frac{H_l}{\\psi_l}}.$\n(23)\nBy substituting (23) into (21), we derive the closed-form expression for the Nash equilibrium as follows\n$d_n^* = \\frac{(\\mathbb{N}-1)R}{\\sum_{l \\in \\mathbb{N}} \\frac{H_l}{\\psi_l}} (1 - \\frac{H_n(\\mathbb{N}-1)}{\\psi_n \\sum_{l \\in \\mathbb{N}} \\frac{H_l}{\\psi_l}}).$\n(24)"}, {"title": "B. Stage 2: Utility Maximization for SFL Model Owner", "content": "The optimization problem for the SFL model owner is formulated as\n$\\underset{R,L_c}{\\text{maximize}} \\quad U_{MO}(R, L_c, d^*)$\n(25)\nsubject to\n$R_{min} < R \\leq R_{max} \\& L_{min} \\leq L_c \\leq L_{max},$\nwhere $d^*$ is given by (24). Note that, to simplify the problem, we relax the integer variable $R$ and $L_c$ into continuous variables. In this context, $R_{min}$ in the constraints can be set by the SFL model owner as the minimum requirement to motivate the SFL participants. Moreover, $L_{min}$ and $L_{max}$ can be set by the SFL model owner to ensure that privacy concerns from clients (e.g., reconstruction attacks) are not violated and to avoid imposing excessive burdens on clients during the SFL process, respectively.\nTheorem 3. An optimal solution for $R$ and $L_c$ exists for maximizing the utility function $U_{MO}(R, L_c, d^*)$.\nProof. Since $d^*$ in (24) is a linear function $R$, we can redefine it as\n$d^*_n = X_n(L_c)R,$\n(26)\nwhere $X_n$ is a function of $L_c$ that acts as the coefficient in the linear function specifying $d^*_n$. Then, by substituting (26) into $U_{MO}$, then $U_{MO}$ is given by\n$U_{MO}(R, L_c, d^*) = \\tau_1 \\ln(1+ \\frac{R \\sum_{n=1}^{N} X_n(L_c)}{d_{req}}) + \\tau_2 \\frac{f_{FLOPS} (L_c)}{W_{FLOPS}} - R.$\nFinally, for any feasible $R$ and $L_c$, taking the second derivatives of $U_{MO}$ with respect to $R$ and $L_c$, we have\n$\\frac{\\partial^2 U_{MO}}{\\partial R^2} = -\\frac{\\tau_1(\\sum_{n=1}^{N} X_n (L_c))^2}{d_{req} (1 + \\frac{R \\sum_{n=1}^{N} X_n(L_c)}{d_{req}})^2} < 0.$\n(27)\nAccordingly, $U_{mo}$ is strictly concave with respect to $R$ with given $X_n (L_c)$. Then, the optimal $R^*$ can be determined using a CVX solver. On the other hand, demonstrating the concavity of the function $X_n(L_c)$ with respect to $L_c$ analytically is challenging. Therefore, within the constraints from $L_{min}$ to $L_{max}$, we can find the optimal solution of $R^*$ and $L_c$ using the exhaustive search-based heuristic algorithm with the CVX solver described in Algorithm 1. This approach is reasonable because the search space of $L_c$ is relatively small, making it practically deployable.\nTheorem 4. A Stackelberg equilibrium exists in the proposed two-stage Stackelberg game in SFL.\nProof. Because the noncooperative game among the clients has a unique Nash equilibrium $d^* = (d^*_1, d^*_2,...,d^*_N)$, as shown in Theorem 2, and the SFL model owner can always find its optimal strategy $R^*$ and $L_c^*$, as shown in Theorem 3, we conclude that a Stackelberg equilibrium exists in the proposed two-stage Stackelberg game in SFL."}, {"title": "IV. DISCUSSION", "content": "A. Impact of Cut Layer Selection on Privacy level\nIn SFL, the main server can still potentially reconstruct the original input data from the smashed data (specifically, the embedded features) received from clients, a vulnerability known as a reconstruction attack. This risk is particularly relevant if the main server is honest but curious (HBC). From the study [21], we found that the selection of the cut layer significantly impacts the privacy level. For example, as the cut layer $L_c$ increases, indicating greater model complexity, the quality of the reconstructed image decreases. This degradation in image quality corresponds to enhanced privacy because the increased complexity of the client-side model introduces advanced nonlinearities into the output. Therefore, to address client concerns regarding privacy, the SFL model owner should carefully select $L_{min}$ to ensure an acceptable level of privacy.\nB. Interplay between Cut Layer Selection and Differential Privacy\nClients can enhance their privacy by proactively adding noise to the smashed data, a technique known as differential privacy (DP). Here, the amount of noise added is inversely proportional to the privacy leakage parameter $\\epsilon$. Formally, let $D$ represent a set of data points, and $M$ be a probabilistic function or mechanism acting on databases. The mechanism $M$ is considered $(\\epsilon, \\delta)$-DP if, for all subsets of possible outputs $S \\subseteq Range(M)$, and for all pairs of databases $D$ and $D'$ that differ by a single element, the following condition holds:\n$Pr(M(D) \\in S) \\leq e^\\epsilon Pr(M(D') \\in S) + \\delta,$\n(28)"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "This paper has studied the interaction between an SFL model owner and clients during the SFL process, formulated as a single-leader multi-follower Stackelberg game in competitive or conflict situations. Each client's strategy involves the amount of data contributed for local training, while the SFL model owner determines suitable incentives to motivate client participation in SFL. The SFL model owner also sets the cut layer to balance the training burden between clients and servers while trying to meet the required privacy level. Our results indicate that Stackelberg strategies are desirable operating points for all participants in competitive scenarios. For a more practical approach, future work involves designing a more sophisticated incentive management game that considers unshared information between clients, detailed evaluations of client contributions, and computing and networking dynamics (such as a Stackelberg Bayesian game-based and deep reinforcement learning-based incentive management)."}]}