{"title": "Can-Do! A Dataset and Neuro-Symbolic Grounded Framework for Embodied Planning with Large Multimodal Models", "authors": ["Yew Ken Chia", "Qi Sun", "Lidong Bing", "Soujanya Poria"], "abstract": "Large multimodal models have demonstrated impressive problem-solving abilities in vision and language tasks, and have the potential to encode extensive world knowledge. However, it remains an open challenge for these models to perceive, reason, plan, and act in realistic environments. In this work, we introduce Can-Do, a benchmark dataset designed to evaluate embodied planning abilities through more diverse and complex scenarios than previous datasets. Our dataset includes 400 multimodal samples, each consisting of natural language user instructions, visual images depicting the environment, state changes, and corresponding action plans. The data encompasses diverse aspects of commonsense knowledge, physical understanding, and safety awareness. Our fine-grained analysis reveals that state-of-the-art models, including GPT-4V, face bottlenecks in visual perception, comprehension, and reasoning abilities. To address these challenges, we propose NeuroGround, a neuro-symbolic framework that first grounds the plan generation in the perceived environment states and then leverages symbolic planning engines to augment the model-generated plans. Experimental results demonstrate the effectiveness of our framework compared to strong baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Large multimodal models extend the general capabilities of large language models by integrating aspects such as visual understanding. Impressively, they demonstrate the potential for real-world problem solving through reasoning and acting on multimodal inputs. However, it remains an open challenge for state-of-the-art models to effectively perceive, reason, plan, and act in complex practical situations. Concretely, we envision the future of large multimodal models as capable embodied planners. However, a potential bottleneck in advancing large multimodal models is the lack of suitable embodied planning benchmarks. While existing benchmarks such as ALFRED and TaPA do support multimodal scenarios, their rigidly defined environments may hinder the development of flexible and diverse planning scenarios. On the other hand, we observe that majority of the existing data focus on short to medium-length plans and explicit user instructions, which may not provide sufficient complexity for the rapidly advancing models. Therefore, in this work, we introduce CAN-DO, an embodied planning dataset for benchmarking large multimodal models. As shown in Table I, our dataset focuses on what we envision that effective embodied planners can do: To generalize to realistic and diverse scenarios in multimodal environments, tackling complex or ambiguous planning problems.\nBased on our fine-grained analysis of state-of-the-art models, we discover that existing models, including GPT-4V, face significant bottlenecks in visual perception, comprehension, and reasoning. This is primarily reflected in the following aspects: 1) when a model has weak visual perception, it may be unable to perceive the state of the environment. 2) when a model has poor comprehension ability, it may misinterpret the outcome or goal state that is required to satisfy the user intent. 3) when a model cannot reason step-by-step, it may generate an invalid plan that cannot achieve the goal.\nHence, there is an urgent need to address the bottlenecks of embodied planning with large multimodal models. To this end, we further propose NeuroGround, a neuro-symbolic framework which explicitly grounds the plan generation process in the initial and goal states of the environment, and consequently leverages symbolic engines to augment the model-generated plans.\nExperiments across multiple planning categories in CAN-Do and state-of-the-art models show that our proposed framework demonstrates significant benefits compared to strong baselines such as chain-of-thought prompting. Thus, our main contributions can be summarized as follows:\n\u2022\tWe introduce CAN-DO, an embodied planning dataset that can benchmark large multimodal models with more diverse and complex scenarios.\n\u2022\tBased on our fine-grained analysis, we diagnose significant planning bottlenecks in state-of-the-art models: visual perception, comprehension, and reasoning.\n\u2022\tTo address the planning bottlenecks, we propose Neu-roGround, a framework which leverages state-grounded planning and augments models with symbolic engines."}, {"title": "II. RELATED WORK", "content": "a) Large Multimodal Models: Recently, models such as GPT-4V have gained widespread attention through impressive visual and language capabilities such as composing stories"}, {"title": "III. CAN-DO: A MULTIMODAL BENCHMARK DATASET\nFOR EMBODIED TASK PLANNING", "content": "In this section, we introduce the construction of our CAN-Do dataset. To explore diverse and complex scenarios for evaluating the planning abilities of the multimodal models, we leverage both real scene images and synthetic images to depict the environment in our dataset, which we found to be realistic and of high quality. Thus, our dataset supports greater flexibility to generate diverse and complex objects and scenarios. Based on this dataset, we evaluate the ability of multimodal models to perceive visual scenarios, comprehend the user intent, and generate step-by-step plans. We provide several examples of our CAN-DO in Figure 1."}, {"title": "A. Data Construction", "content": "To construct diverse embodied planning scenarios, we consider three task categories that are applicable to everyday robotic assistant scenarios: physical understanding, commonsense, and safety. Our data collection process includes the following three steps:\na) Image Generation: While our dataset is based on real scene images, we are constrained in the diversity of images and scenarios that can be captured. For example, it may not be feasible or ethical to recreate safety hazard cases in real life. Thus, we additionally curated a collection of locations, objects, and scenarios inspired by real-life situations, and then prompted DALL-E 3 to generate images of the environment. However, the primary challenge in image generation is ensuring that the image generator can controllably produce scenes focusing on specific locations or objects without producing overly complex or exaggerated images. To tackle this, we employed two strategies to improve the image generation process when the generated images of the given scenario are hard to control. Firstly, we can reduce the complexity of the scene by reducing the number of objects or locations. Secondly, we prompt the image generator with descriptions of simple styles and bright backgrounds, thereby reducing the likelihood of generating extraneous objects.\nb) Instruction and State Annotation: For each scene of images, we manually design the questions and annotate the initial and goal state according to the user instructions. Our data design principles primarily focus on two aspects. Firstly, the instruction needs to test the ability of the multimodal model in specific categories. For instance, as shown in Figure 1, the physical understanding problem requires the model to recognize the shape of blocks in the image when planning. Consequently, we annotate the initial and goal states, which can determine the action plan.\nc) Data Verification: To ensure data quality, we engage five data annotators. Two of them are responsible for construct-"}, {"title": "IV. PRELIMINARY STUDY ON EMBODIED PLANNING\nBOTTLENECKS", "content": "While large multimodal models have demonstrated impressive capabilities in reasoning and problem-solving , it is not clear how this advantage extends to embodied planning tasks. Specifically, embodied planning involves multiple stages, including the visual perception of the environment, comprehending the goal of user requests, and generating a coherent plan that satisfies the environment constraints and planning goal. In practice, large multimodal models may face planning bottlenecks in any of these stages. Hence, we first conduct a preliminary study to diagnose potential planning bottlenecks. Concretely, we leverage a progressive prompting approach, by providing partial ground-truth information in the model input prompt. For example, providing the ground-truth initial state of the environment reduces the need for the model to perceive the environment visually. Based on the results in Section IV-C, we find significant bottlenecks in visual perception, comprehension, and reasoning."}, {"title": "A. Task Formulation", "content": "In this section, we aim to provide a concrete formulation of the embodied planning task. In general, the model inputs can contain multimodal data such as images depicting the environment and textual user instructions or task descriptions."}, {"title": "B. Evaluation Setting", "content": "To evaluate the performance of large multimodal models on our dataset, we provide all necessary information in the prompt, including the task description, exact location names, and object names. To judge if a plan generated by the model is correct, we use automatic methods by executing the plan step-by-step in a symbolic environment. Concretely, the symbolic environment executes the model-generated plan and checks if the outcome state matches our ground-truth goal state, as discussed in the formulation in Section IV-A. Hence, our main evaluation metric is the average plan validity across the data samples. To provide a more nuanced analysis, we report the average performance in each category of commonsense,"}, {"title": "V. NEUROGROUND: A NEURO-SYMBOLIC FRAMEWORK\nFOR GROUNDED PLANNING", "content": "Based on the preliminary study in Section IV which diagnosed significant planning bottlenecks in visual perception, goal comprehension, and reasoning for plan generation, we propose NeuroGround, a neuro-symbolic framework for grounded embodied planning. To enhance the visual perception and comprehension ability of the model, we leverage state-grounded planning which explicitly guides the model to generate and condition on the environment states before starting to generate the plan. To mitigate the plan generation limitations of the model, we augment it with a symbolic engine, inspired by neuro-symbolic approaches. In this way, our framework is able to enhance the planning ability of large multimodal models."}, {"title": "A. State-Grounded Planning", "content": "Based on the formulation in Section IV-A, the objective of the embodied planning model $M$ is to generate a plan $P$ based on the textual user inputs $X_{req}$ and visual environment inputs $X_{img}$. For exact matching during evaluation, the model is also provided with the location names $X_{loc}$ and object names $X_{obj}$. Hence, the general objective can be formulated as $P = M(X_{req}, X_{img}, X_{loc}, X_{obj})$, where $P = \\{(a_1,o_1), ..., (a_l, o_l)\\}$ is the generated action plan, with $a$ denoting the action type and $o$ denoting the corresponding location or object of each action. However, this objective poses a challenge in that it does not explicitly consider the initial environment state $S_{init}$ or the goal state $S_{goal}$, where the states represent the mapping between the specific locations and objects. Concretely, the model requires visual perception of the environment to determine the initial state, and comprehension of the user request to interpret the goal state. Without first grounding the planning process in the environment states, the model may output an inaccurate plan.\nTherefore, our framework as shown in Figure 5 introduces state-grounded planning, which explicitly generates and conditions on the initial environment state and goal state before starting to generate the action plan. Concretely, we need to modify the planning objective to first consider the initial state $S_{init}$ and goal state $S_{goal}$. As the model does not have access to the ground-truth initial state or goal state in practice, we require the model to self-generate the"}, {"title": "B. Neuro-Symbolic Augmentation", "content": "To further address the reasoning challenges for plan generation as discussed in Section IV, we augment the planning process with a symbolic solver. For example, as shown in Figure 5, the model $M$ may generate a correct initial state $\\hat{S}_{init}$ and goal state $\\hat{S}_{goal}$, but still generate an invalid plan due to hallucination. In this case, the plan attempted to pick the durian object from the pan location, which contradicts the initial state. On the other hand, many well-established planning algorithms can search for the optimal and valid plan when given the initial and goal states .\nThus, we incorporate a symbolic planning engine based on the A* search algorithm in the unified-planning library. Concretely, given the generated initial state $\\hat{S}_{init}$ and goal state $\\hat{S}_{goal}$, we directly parse and input the states into the planning engine $E$ to determine the action plan:\n$P_{engine} = E(\\hat{S}_{init}, \\hat{S}_{goal})$\nHowever, we observe challenges in directly using the model-generated states with the planning engine. Notably, there may be cases of parsing issues when the model does not follow the structured state format. Thus, we leverage the engine plan when possible, but otherwise fall back to the model-generated plan for robustness."}, {"title": "VI. EXPERIMENTS", "content": "To evaluate our framework for embodied planning, we use the same task formulation and evaluation setting as detailed in Section IV-A and Section IV-B. Notably, the model inputs contain multimodal content such as visual images depicting the environment and natural language user instructions. The main output will be the action plan, specifying that sequence of actions that the agent should perform. Our main evaluation metric is the average plan validity, based on whether the plan satisfies the goal state constraints."}, {"title": "A. Evaluated Large Multimodal Models", "content": "To investigate the limits of embodied planning abilities, we focus our study on the state-of-the-art large multimodal models at the time of writing. Specifically, we consider the following three models which are widely popular and show leading performance on multimodal tasks : 1. Claude Opus: We select the highest-performing model from Anthropic's Claude model family, which emphasizes near-human levels of comprehension and fluency on complex tasks and sophisticated vision capabilities on par with other leading models. Specifically, we use the \u201cclaude-3-opus-20240229\u201d API model version. 2. Gemini Pro: We include the most recent"}, {"title": "B. Planning Methods", "content": "To demonstrate the effectiveness of our planning framework, we compare it to existing planning methods. We note that while there are several related works in planning , they may be task-specific and not applicable to our embodied planning task in the multimodal setting. For example, Inner Monologue requires a live environment which is difficult to replicate, while ViLaIn  requires bounding box inputs that are not applicable to our setting. Hence, we restrict to general planning approaches that are easily applicable to our task setting as formulated in Section IV-A: 1. Direct Planning Baseline: Following the baseline planning method in the preliminary study of Section IV, the model is prompted to generate the action plan directly, based on the multimodal inputs. 2. Chain-of-Thought: We compare against chain-of-thought prompting as a planning approach, as it is widely applicable to many reasoning and problem-solving tasks, including embodied planning . We follow the planning format shown in the original work . 3. Reasoning Via Planning (RAP): We also include this comparison approach which uses the model as a reasoning agent for planning ."}, {"title": "C. Evaluation Results", "content": "We report the main evaluation results of our planning framework in Figure 6. Compared to the other methods, our framework shows notable and relatively consistent planning improvements across different categories and state-of-the-art models. We observe that chain-of-thought prompting which leverages intermediate reasoning steps does not consistently improve over the baseline approach of directly generating the plan. This may indicate reasoning steps alone are insufficient to ground the model, compared to our approach which"}, {"title": "a) Ablation", "content": "To evaluate different components of our framework, we include the ablation study in Table III. Notably, we find that not using the symbolic engine component results in less accurate plans, indicating that the neuro-symbolic augmentation can mitigate some model-generated plan failures as shown in Figure 5. Furthermore, we observe that even without the neuro-symbolic augmentation, the performance is greater than the baseline and chain-of-thought prompting, which indicates that the state-grounded planning component of our framework confers significant benefits."}, {"title": "VII. CONCLUSION", "content": "In this work, our research addresses the urgent need to advance large multimodal models in the area of embodied planning. We introduce CAN-DO, a novel dataset designed to benchmark these models in diverse and complex scenarios, pushing beyond the limitations of existing benchmarks. Through our fine-grained analysis, we identify significant bottlenecks in current state-of-the-art models, particularly in visual perception, comprehension, and reasoning. To mitigate these challenges, we propose NeuroGround, a neuro-symbolic framework that enhances model-generated plans by first grounding them in the environment states and then augmenting them with symbolic engines. Our experimental results demonstrate consistent benefits over existing methods, underscoring its potential to significantly advance the embodied planning capabilities of large multimodal models."}]}