{"title": "Advantages of Neural Population Coding for Deep Learning", "authors": ["Heiko Hoffmann"], "abstract": "Scalar variables, e.g., the orientation of a shape in an image, are commonly predicted using a single output neuron in a neural network. In contrast, the mammalian cortex represents variables with a population of neurons. In this population code, each neuron is most active at its preferred value and shows partial activity for other values. Here, we investigate the benefit of using a population code for the output layer of a neural network. We compare population codes against single-neuron outputs and one-hot vectors. First, we show theoretically and in experiments with synthetic data that population codes improve robustness to input noise in networks of stacked linear layers. Second, we demonstrate the benefit of population codes to encode ambiguous outputs, as found for symmetric objects. Using the T-LESS dataset of feature-less real-world objects, we show that population codes improve the accuracy of predicting object orientation from RGB-image input.", "sections": [{"title": "1. Introduction", "content": "In the mammalian cortex, many variables, e.g., object orientation [12] and movement direction [5], have been found to be encoded by populations of neurons. In a population code, each neuron responds maximally to its preferred value and partially to other values of the encoded variable, with activation levels shaped by tuning curves such as Gaussian or cosine functions of the distance from the neuron's preferred value. The activity of the group of neurons resembles a probability distribution of the encoded variable [17].\nA lot of computational neuroscience work, particularly the earlier work, has focused on decoding the information represented by a population code [2, 18, 19, 24]. But the brain does not need to decode population codes: information is processed from population code to population code throughout the cortex [3, 11, 16, 23].\nCommonly used artificial neural networks, such as convolutional neural networks (CNNs) and multi-layer perceptrons (MLPs), encode information also by groups of neurons, particularly in their intermediate layers [7]. However, in the output layer, information is typically mapped onto different representations. For classification tasks, outputs are commonly represented as one-hot vectors, using one neuron for each classification label. For prediction tasks, output neurons typically correspond to the variables of interest, such as the position and orientation of an object in an image.\nHere, we investigate the benefit of mapping onto population codes for prediction tasks. Other prior work already pointed out the benefits of neural population codes: such coding has been shown to improve linear separability of temporal information [15]. In addition, population codes can be used in the output layer for cleaning up noisy signals nearly as optimal as the maximum likelihood estimate [16]. Different here, we demonstrate that replacing prediction target variables with population codes improves noise robustness and accuracy, which we show using theoretical analysis and experiments with synthetic and real-world data.\nIn the remainder of this article, we first derive theoretically the noise robustness for a single-layer linear network and compare single-variable, population-code, and one-hot vector outputs. Here, we include the one-hot vector for the prediction task due to its structural similarity to the population code: it uses the same group of neurons, only the target activations differ, which are binary for one hot and continuous for the population code. Secondly, we compute the noise robustness using deeper MLPs in simulation. Here, we found that population codes lead to sparser information flow through the network compared to one-hot vectors. Finally, we use an image-to-pose prediction task to show that population codes are capable of handling ambiguous pose from symmetric objects, while improving accuracies.\nThis article makes the following three main contributions:\n1. Introducing population codes as output layers of CNNs and MLPs for prediction tasks and demonstrate their benefit,\n2. Analyzing theoretically the robustness to noise of single-layer networks, and\n3. Discovering that training networks with population-code outputs results in sparser information flows."}, {"title": "2. Theory", "content": "We first analyze noise robustness for single-layer networks with one output variable. Then, in comparison, we analyze noise robustness for one-hot vector and population code.\n2.1. Single-Variable Output\nFor simplicity and theoretical tractability, we first study single-layer linear networks. Let x be the input vector and y be the single-variable output,\n$y = w^T x + b,$\nwhere w is the weight vector and b the bias.\nAs training data, we assume x is a one-dimensional image of size n, where all pixel values are zero except one that equals 1, i.e., the image contains a 1-pixel shape in arbitrary location. The target output y is the location of this pixel, y = i/n, where i is the index of the input pixel that equals 1. Moreover, we assume that the training data contains all possible values of i.\nWe train this network with a squared error loss, resulting in the following weight updates,\n$w^{t+1} = w^t + \\eta (\\hat{y} - y) x_i$\n$b^{t+1} = b^t + \\eta (\\hat{y} - y),$\nwhere y is the target value, and \\eta is the learning rate. For our specific training input, $x_j = \\delta_{ij}$ (Kronecker delta) for the target $\\hat{y} = i/n$, the weight and bias update simplifies to\n$w_i^{t+1} = w_i^t + \\eta (i/n - w_i^t - b^t)$\n$b^{t+1} = b^t + \\eta (i/n - w_i^t - b^t).$\nFor the weights and bias to converge, that following equality must hold,\n$w_i = \\frac{i}{n} - b.$\nSince any b value will fulfill this equality, the weights are defined only up to a constant bias term that shifts all weight values. The actual value of b will depend on network initialization.\nNext, we probe the network's robustness to noise in the input. For this test, we perturb the input x by d, so that x + d is the new input. Moreover, let $d_j = a \\delta_{kj}$ for a given perturbed neuron k, and we use the same x and target, i/n, as above. As a result, the output equals,\n$\\hat{y} = \\frac{i}{n} + a w_k,$\nwith error $a w_k$.\nAssuming an error tolerance of 1.5/n (essentially, we tolerate a position error of one pixel but not two), we want to compute the rate of network failures. To compute the failure rate, we approximate that the trained weights will be in the range -0.5 to 0.5 since the weights are commonly initialized to be uniformly distributed with zero mean, and after training, their range is about 1. According to (6), the trained weights are uniformly distributed. Moreover, we assume that the target and perturbation indices are drawn with uniform probability from {0,1,...,n \u2212 1}. For the trials that fall within the error tolerance, we have\n$|w_k| < \\frac{1.5}{a n}.$\nThe ratio of trials, r, within the error tolerance is the fraction of w values fulfilling (8) compared to the total weight range after training,\n$r = min(1, \\frac{3}{a n}).$\nHere, we approximated the discretely distributed values with continuous uniform distributions. As a result, the failure rate is\n$r_F = max(0, 1 - \\frac{3}{a n}).$\nFor example, for n = 20, even for a relatively small perturbation of a = 0.2, we expect a failure rate of 25%. At a = 0.5, the failure rate increases to 70%.\n2.2. Population-Code Output\nIn comparison, we investigate a linear network with population-code output, y, instead of a single variable,\ny = Wx + b,\nwhere W is the weight matrix. For squared error loss, the weight update rule becomes\n$W_{ij}^{t+1} = w_{ij}^t + \\eta (Y_i - \\sum_k W_{ik}^t x_k - b_i) x_j.$\nFor our theoretical analysis, we first use a simplified population code that resembles a one-hot vector, i.e., only one neuron, $x_i$, with preferred value i/n gets active, and all other neuronal activations are zero. Later, we show the difference between population code and one-hot vector. Our target vector matches the input; so, the network has to learn the identity function (our arguments also hold for learning any permutation of the indices, e.g., shift in pixel position, because the assignment of neurons to indices is arbitrary). Using the same training data as above, the weight update simplifies to\n$w_{ij}^{t+1} = w_{ij}^t + \\eta (\\delta_{ij} - W_{ij}^t - b_i).$\nThis update converges to\n$W_{ij} = \\delta_{ij} - b_i.$"}, {"title": "3. Experiments With Synthetic Data", "content": "In numerical experiments using the same training data as for the theory, we compare the three approaches on deeper networks.\n3.1. Methods\nWe used various numbers of stacked linear layers of size n = 20, except for the output layer for the single-variable approach, which consisted of one neuron. In the architecture, each hidden layer was followed by a leaky ReLU function. For the population-code approach, a sigmoid function was applied to the output, and we used Gaussian tuning curves with a width of \u03c3 = 0.1.\nWe used the mean squared error (MSE) as the loss function for single-variable and population code models, and cross-entropy loss for one-hot encoding, which is the commonly used loss function for classification tasks. The networks were implemented and trained using the PyTorch framework. For training, we used the Adam optimizer with a learning rate of \u03b7 = 0.005 and 5,000 epochs (sufficient for convergence). In each simulation run, the networks were initialized (default initialization), trained from scratch, and evaluated. For each method and network size, we computed 100 simulation runs.\nIn each evaluation, for each possible clean input vector (n different ones), we presented 1,000 perturbations at a random input neuron (uniformly chosen) and with random amplitude a, uniformly chosen from the set {0.05, 0.1, 0.15, ..., 1}. For each amplitude, we computed the failure rate across all inputs and perturbation locations and then averaged rates across all simulation runs.\n3.2. Results on Noise Robustness\nFigure 1 shows the results for networks with 1, 3, 5, and 8 layers. The 1-layer network mirrored the theoretical assumptions, and the results are in good agreement with the theory. There was a small discrepancy between theory and simulation for the single-variable method for a = 0.15. The mismatch can be explained if considering that the weights are not exactly symmetrically distributed around zero, instead, the range was on average [-0.375; 0.575]. This bias towards positive values can in turn be explained by the weight and bias update rules, (4) and (5), which initially push both average weight and bias from zero to positive values.\nIn the following, we will investigate why the population code performs better than one hot for noise robustness. Both population code and one hot compute a sigmoid function on the output of the last linear layer; the cross-entropy loss includes a softmax on the logits. But the important difference is that for one hot, the target values are either 0 or 1, while the population code has continuous values between 0 and 1. When 0 is the target for a sigmoid function, due to its fast convergence to 0, the logits can be arbitrarily large. Beyond a certain size, the loss function is insensitive to their value.\nSo, we suspect that the one-hot method has larger linear-layer outputs compared to the population code. The larger values cause a problem for robustness because they make it more likely that a perturbation causes spurious activations that suppress the correct output value. To test this hypothesize, we evaluated the activations after the final linear layer, and compared one hot with population code. As a result, both the max and min values were indeed larger for one hot for the 3, 5, and 8-layer networks (Fig. 4). For the 8-layer network, the values were about 5x larger."}, {"title": "4. Experiments with Real-World Data", "content": "We compared the population-code method against the single variable and one-hot methods for predicting object orientation from grayscale images. For this test, we used the T-LESS dataset of industry-relevant featureless objects [8]. A particular challenge for this dataset is the ambiguity of object pose because most of the objects are symmetric and, therefore, have multiple equivalent pose representation. Here, we demonstrate that the population code can handle this ambiguity.\n4.1. Population Code for Object Orienation\nTo encode the object orientation with a neural population code, we first transformed the rotation matrix into a rotation axis and a rotation angle. For the axis, we arranged the preferred values of each neuron on the surface of a sphere. To obtain a near uniform distribution of axes, we computed a spherical Fibonacci lattice [4]. For the rotation angle, the preferred values were arranged in a circle. To combine rotation axis with angle, our population code had for each point on the Fibonacci sphere a circle of neurons for the rotation angle. So, the space to represent the orientation is a direct product of a sphere and a circle.\nGiven a rotation axis and angle, we activated all neurons using Gaussian tuning curves,\n$a_i = exp(-\\frac{d_{o_i}^2 + d_{o_i}^2}{2 \\sigma^2}),$\nwhere $d_{o_i}$ is the angle between the encoded axis and the preferred axis on the sphere for neuron i, $d_{oi}$ the angle between the encoded angle and the preferred angle of neuron i, and \u03c3 is the tuning width, here 20\u00b0.\nTo compute the target population code from a ground truth transformation matrix, $R_o$, we computed activations for all symmetry transformations of Ro,\n$R'_k = R_o R_{sym},$\nwhere {$R_{sym}$} is the set of symmetry transformations for a given object (in a real-world application, the symmetry of an object is usually known a priori). Our target population code is the sum of all activations $a_i$ over all k symmetry transformations (Fig. 6).\nThe transformation from rotation matrix to axis/angle, {r, $\u03c6$}, is not unique since {\u2212r, 2\u03c0 - \u03c6} is an equivalent axis/angle combination to {r, $\u03c6$}. Therefore, for each symmetry transformation, we computed the tuning-curve activations also for {\u2212r, 2\u03c0 - \u03c6} and added those to the target activations (resulting in four peaks in Fig. 6).\nFor objects with a symmetry axis, which would result in infinitely many equivalent poses, we computed a variant of our population code: since the population code for the\nrotation angle would show uniform activations, we simplified the code and omitted the rotation angle neurons, using just the neurons encoding the rotation axis on the Fibonacci sphere. Here, the code was computed simply as\n$a_i = exp(-\\frac{d_{o_i}^2}{2 \\sigma^2}),$\nwithout having to superimpose activations for symmetries.\n4.2. Methods\nFor predicting object orientation, we mapped gray-scale images of size 128x128 pixels onto the output layer. For the population code, we had a vector of size nm, where n is the number of preferred axes and m the number of preferred angles. Here, we used n = 2,562 and m = 36, matching the number of rotation candidates used in [20]. For the one-hot approach, we used a vector of the same size.\nFor the single-variable approach, we directly mapped onto the 6-dimensions of the first two columns of the rotation matrix. Mapping onto two columns has been shown to be advantageous over other representations like quaternions due to the continuity of the R6 space [25]. This representation was also used by one of the best performing pose-estimation methods [21]. The entire rotation matrix can be reconstructed from the two columns using Gram-Schmidt orthonormalization. In the special case of objects with a symmetry axis, we directly mapped onto the 3 coordinates of the rotation axis."}, {"title": "5. Conclusions", "content": "We found that using a population code as the output of a CNN or MLP instead of directly mapping onto prediction variables has the following advantages:\n\u2022 Increased robustness to input noise,\n\u2022 Higher accuracies in real-world prediction tasks, and\n\u2022 Ability to handle ambiguous output by effectively using a multimodal probability distribution as training target.\nMoreover, a population code has better noise robustness and prediction accuracy compared to a one-hot vector of the same size. The reason for this advantage might be the sparser information flow that we found with population codes and the reduction of extreme output values in the last linear layer of a network.\nIn many practical applications, the population-code output still has to be decoded, e.g., to read out an object's pose. Here, for simplicity, we just used the preferred value of the neuron with the maximum activation. This strategy is essentially the same as for decoding a one-hot vector. But given the research on decoding population codes, many alternative methods are already available [2, 16, 18, 19]. So, our results could be further improved. When using the maximum likelihood estimate based on the probability distribution of a target variable, we would not only improve noise robustness, but could also decode variables at a finer resolution than our population-code sampling would allow. In this way, a sparser sampling would be sufficient, which benefits prediction tasks with higher-dimensional output.\nIn our future research, we will apply population codes to other prediction tasks and already saw improvements in different domains."}]}