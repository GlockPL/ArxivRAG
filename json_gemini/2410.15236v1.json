{"title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models", "authors": ["Benji Peng", "Qian Niu", "Tianyang Wang", "Yizhu Wen", "Ming Liu", "Yichao Zhang", "Ziqian Bi", "Pohsun Feng", "Lawrence K.Q. Yan", "Caitlyn Heqi Yin"], "abstract": "Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have become a pivotal development in artificial intelligence, demonstrating outstanding capabilities in natural language understanding and generation. Their capacity to process large volumes of data and generate human-like responses has led to their integration across numerous applications, such as chatbots, virtual assistants, code generation systems, and content creation platforms [1], [2]. However, the rapid advancement and widespread adoption of LLMs have raised substantial security and safety concerns [3].\nAs LLMs grow more powerful and are integrated into critical systems, the potential for misuse and unintended consequences increases. The capabilities that make LLMs valuable their ability to learn from massive datasets and generate creative outputs\u2014also render them susceptible to manipulation and exploitation [4]. A major concern in LLM security is their vulnerability to adversarial attacks, particularly prompt injection and jailbreaking [5]. These attacks exploit the intrinsic design of LLMs, which follow instructions and generate responses based on patterns in their training data [6]. Bad actors can craft malicious prompts to bypass safety mechanisms in LLMs, resulting in harmful, unethical, or biased outputs [7].\nResearchers have indicated that LLMs can be manipulated to provide instructions for illegal activities such as drug synthesis, bomb-making, and money laundering [8]. Other studies have demonstrated the effectiveness of persuasive language, based on social science research, in jailbreaking LLMs to generate harmful content [9]. Multilingual prompts can exacerbate the impact of malicious instructions by exploiting linguistic gaps in safety training data, leading to high rates"}, {"title": "II. BACKGROUND AND CONCEPTS", "content": "A. Large Language Models (LLMs)\nLarge Language Models (LLMs) are artificial intelligence systems that use deep learning, specifically transformer networks, to process and generate human-like text [6]. Trained on massive datasets, LLMs learn complex language patterns, enabling them to perform tasks such as text summarization, translation, question answering, and creative writing. Their ability to generate coherent, contextually relevant text stems from their vast training corpus and advanced architecture [16]. LLMs have permeated many domains [1], offering both beneficial and potentially harmful applications. In healthcare, LLMs assist with tasks such as medical record summarization, patient education, and drug discovery [17], [18]. In software engineering, LLMs such as OpenAI Codex assist in code auto-completion, streamlining development [19]. They also contribute significantly to AI-driven programming and conversational AI systems [20]. However, LLMs also pose risks, including misuse for generating harmful content like hate speech, misinformation, and instructions for illegal activities [19], [21]. This dual-use potential demands careful consideration of safety and ethical implications [8].\nA key challenge in LLM development is aligning them with human values and intentions [4]. Alignment involves training LLMs to behave in a beneficial and safe manner for humans, avoiding harmful or undesirable outputs. This includes aligning models with social norms and user intent [12], [22]. Misalignment occurs when LLMs deviate from human values or produce harmful, unethical, or biased outputs [11]. Achieving robust alignment is an ongoing challenge, as LLMs are susceptible to adversarial attacks that exploit their vulnerabilities, leading to misalignment [23].\nTo mitigate LLM risks, researchers have developed safety mechanisms to align these models with human values and prevent harmful content generation [24]. These mechanisms can be categorized into pre-training and post-training techniques. Pre-training techniques filter training data to remove harmful or biased content [10]. Post-training techniques include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), where the LLM is trained on curated datasets to align outputs with human preferences and ethical guidelines [23].\nRed-teaming is a proactive safety mechanism that tests LLMs with adversarial prompts to identify vulnerabilities and enhance robustness [25], [26]. Prompt engineering for safety designs prompts that instruct LLMs to avoid harmful or unethical content [27]. Safety guardrails restrict certain outputs from LLMs, while system prompts give high-level instructions to guide LLM behavior [20], [28]. However, these system prompts are vulnerable to leakage, posing a security risk [29]. Evaluating LLM safety and trustworthiness requires robust metrics that capture different aspects of model behavior. Toxicity scores assess offensive or harmful language in LLM outputs, while bias scores measure model prejudice or discrimination against groups [10], [30]. Adversarial robustness measures the model's ability to resist adversarial attacks and maintain intended behavior [16], [23]. Data leakage involves the unintentional disclosure of sensitive information from training data [31], while compliance with ethical guidelines assesses the model's adherence to ethical principles and norms [19].\nSeveral benchmark datasets have been developed to evaluate LLM safety and robustness. These datasets consist of curated prompts and responses to test the model's ability in safety-critical scenarios. Examples include RealToxicityPrompts, focusing on eliciting toxic responses, and Harmbench, which tests broader harmful behaviors [32]. Other datasets, such as Do-Not-Answer [33], Latent Jailbreak [16], and RED-EVAL [25], target the model's ability to resist harmful or unethical instructions. Additionally, datasets like JailbreakHub analyze the evolution of jailbreak prompts over time [11], [34]. However, these benchmark datasets often have limitations in scope, diversity, and real-world applicability, highlighting the need for continuous development and refinement of evaluation methods.", "B": null}, {"title": "B. Prompt Engineering", "content": "Prompt engineering is the process of designing the input text, or prompt, given to an LLM to elicit the desired output [6] [35]. It plays a crucial role in enhancing LLM performance and ensuring safety by providing context, specifying the task, and guiding the model's behavior. Effective prompts significantly improves the accuracy, relevance, and creativity of the generated text, while also mitigating the risk of harmful or biased outputs. Prompt engineering involves a variety of techniques, ranging from simple instructions to more complex strategies that fully utilize the LLM's capabilities. Zero-shot prompting involves providing a task description without any examples [36], while few-shot prompting includes a few examples to guide the model [36]. Chain-of-thought prompting encourages the LLM to generate a step-by-step reasoning process before providing the final answer [35], while tree-of-thought prompting expands on this by exploring multiple reasoning paths [35]. Role prompting assigns a specific role or persona to the LLM [35], whereas instruction prompting provides explicit instructions to generate the desired output format or content. Bespoke prompt engineering enhances LLM safety and mitigates risks, which involves designing prompts that instruct the LLM to avoid generating harmful or unethical content explicitly, respect diverse perspectives, and adhere to established ethical guidelines. For example, prompts may instruct the LLM to avoid hate speech, consider cultural sensitivities, or prioritize factual accuracy over creative storytelling. In some cases, prompts can remind the LLM of its safety guidelines and responsibilities, serving as a form of self-regulation [37]."}, {"title": "C. Jailbreaking", "content": "Jailbreaking refers to adversarial attacks designed to bypass the safety mechanisms of LLMs, inducing them to produce content that violates intended guidelines or restrictions [7], [19]. These attacks exploit the LLMs' inherent tendency to follow instructions and generate text based on learned training data patterns. Adversaries may be motivated by a desire to expose vulnerabilities, test LLM safety limits, or maliciously exploit these models for personal gain or to inflict harm [34].\nJailbreak attacks can be categorized by strategy, target modality, and objective. Attack strategies include prompt injection, embedding malicious instructions in benign prompts [20]; model interrogation, manipulating internal representations to extract harmful knowledge [38]; and backdoor attacks, embedding malicious triggers during training [14]. Target modalities include textual jailbreaking, manipulating LLM textual inputs [19], and visual jailbreaking, targeting image inputs in multimodal LLMs [39]. Multimodal attacks exploit interactions between modalities, like combining adversarial images with textual prompts [40]. Attack objectives include generating harmful content, bypassing safety filters, leaking private information [15], or gaining control of LLM behavior [13].\nThe rise of online communities sharing jailbreak prompts has hugely escalated the threat levels. These communities collaborate to discover vulnerabilities, refine attacks, and bypass new defenses [34] [11]. The rapid evolution and growing sophistication of jailbreaking highlight the need for continuous development of robust defenses. The shift to dedicated prompt-aggregation websites signals a trend towards more organized and sophisticated jailbreaking [7]."}, {"title": "III. JAILBREAK ATTACK METHODS AND TECHNIQUES", "content": "Jailbreaking attacks aim to exploit vulnerabilities in LLMs to bypass their safety mechanisms and induce the generation of harmful or unethical content. As LLMs become more powerful and widely deployed, the need to understand and mitigate these attacks becomes increasingly crucial. These attacks can be broadly categorized into prompt-based attacks, model-based attacks, and multimodal attacks.\nA. Prompt-Based Attacks\nPrompt-based attacks focus on manipulating the input prompts to elicit undesired outputs from LLMs. These attacks exploit the LLM's reliance on prompts to guide its behavior and can be further categorized into adversarial prompting, in-context learning attacks, and other prompt-based techniques.\n1) Adversarial Prompting: Adversarial prompting involves crafting malicious prompts that are specifically designed to trigger harmful or unethical responses from LLMs. These prompts often exploit vulnerabilities in the LLM's training data or its ability to understand and follow instructions. Several techniques have been proposed for generating adversarial prompts, including:\nGreedy Coordinate Gradient (GCG): This method automatically generates adversarial suffixes that can be appended to a wide range of queries to maximize the probability of eliciting objectionable content from aligned LLMs [41]. GCG utilizes a combination of greedy and gradient-based search techniques to find the most effective suffix and has been shown to be transferable across different LLM models, including ChatGPT, Bard, and Claude [41].\nPrompt Automatic Iterative Refinement (PAIR): This black-box method automatically generates and refines jailbreak prompts for a \"target LLM\u201d using an \"attacker LLM\u201d through iterative querying [7]. Inspired by social engineering attacks, PAIR employs an attacker LLM to iteratively query the target LLM, refining the jailbreak prompt autonomously. This method is efficient, often requiring fewer than 20 queries to produce a successful jailbreak, and achieves high success rates with strong transferability across various LLMs, including both open and closed-source models like GPT-3.5/4, Vicuna, and PaLM-2 [7].\nAutoDAN: This method uses a hierarchical genetic algorithm to generate stealthy and semantically coherent jailbreak prompts for aligned LLMs [42]. AutoDAN addresses the scalability and stealth issues of manual jailbreak techniques by automating the process while preserving semantic coherence. It demonstrates greater attack strength and transferability than baseline approaches, effectively bypassing perplexity-based defenses [42].\nWordGame: This method replaces malicious words with word games to disguise adversarial intent, creating contexts outside the safety alignment corpus [43]. WordGame exploits the LLM's inability to detect hidden malicious intent within seemingly benign contexts. This obfuscation significantly raises the jailbreak success rate, exceeding 92% on Llama 2-"}, {"title": "7b Chat, GPT-3.5, and GPT-4, outperforming recent algorithm-focused attacks [43].", "content": "PromptInject: This framework utilizes a mask-based iterative approach to automatically generate adversarial prompts that can misalign LLMs, leading to \"goal hijacking\u201d and ''prompt leaking\u201d attacks [13]. PromptInject exploits the stochastic nature of LLMs and can be used by even low-skilled attackers to generate effective jailbreak prompts.\nGPTFuzzer: Inspired by the AFL fuzzing framework, GPT-Fuzzer automates the generation of jailbreak prompts for red-teaming LLMs [20]. It starts with human-written templates as initial \"seeds\" and then mutates them to produce new templates. GPTFuzzer incorporates a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack [20]. This framework achieves over 90% attack success rates against ChatGPT and LLaMa-2 models, surpassing human-crafted prompts.\n2) In-Context Learning Attacks: In-context learning is a notable capability of LLMs that allows them to learn new tasks from a few examples or demonstrations. However, this capability can also be exploited for jailbreaking:\nIn-Context Attack (ICA): This method uses strategically crafted harmful demonstrations within the context provided to the LLM, subverting the model's alignment and inducing harmful outputs [44]. ICA takes advantage of the LLM's capacity to learn from examples, even malicious ones, significantly increasing the success rate of jailbreaking attempts.\n3) Other Prompt-Based Techniques: Beyond adversarial prompting and in-context learning attacks, additional techniques have been developed for generating jailbreak prompts:\nMulti-turn prompting: This approach involves a sequence of prompts that gradually escalate the dialogue, ultimately leading to a successful jailbreak. For instance, the Crescendo attack begins with a benign prompt and escalates the dialogue by referencing the model's responses, while the \"Speak Out of Turn\u201d attack decomposes an unsafe query into multiple sub-queries, prompting the LLM to answer harmful sub-questions incrementally. These attacks exploit the LLM's tendency to maintain consistency across turns, steering it toward harmful or unethical outputs.\nLogic-chain injection: This technique disguises malicious intent by breaking it into a sequence of seemingly benign statements embedded within a broader context [47]. This technique exploits the LLM's ability to follow logical reasoning, even when used to justify harmful actions. This attack can deceive both LLMs and human analysts by exploiting the psychological principle that deception is more effective when lies are embedded within truths.\nWord substitution ciphers: This technique replaces sensitive or harmful words in prompts with innocuous synonyms or code words to bypass safety filters and elicit harmful responses [15]. It exploits the LLM's reliance on surface-level language patterns and inability to discern underlying intent."}, {"title": "ASCII art-based prompts (ArtPrompt): This method takes advantage of the LLM's inability to recognize and interpret ASCII art, allowing harmful instructions to be disguised and safety measures to be bypassed [48]. ArtPrompt exploits the LLM's limitations in processing non-semantic information, achieving high success rates against state-of-the-art models like GPT-3.5, GPT-4, Gemini, Claude, and Llama2.", "content": "Persona modulation: This technique prompts the LLM to adopt a specific persona more likely to comply with harmful instructions [8]. It exploits the LLM's adaptability to context and persona, significantly increasing the harmful completion rate in models like GPT-4."}, {"title": "B. Model-Based Attacks", "content": "Model-based attacks target the internal architecture or training process of LLMs to introduce exploitable vulnerabilities. These attacks are challenging to detect and mitigate, as they alter the model directly rather than relying on input prompt manipulation.\n1) Backdoor Attacks: Backdoor attacks inject malicious data or code into the LLM during training, establishing a \"backdoor\" that can be triggered by specific inputs. This enables an attacker to control the LLM's behavior without crafting a specific prompt. Examples of backdoor attacks include, but are not limited to:\nPoisoning training data: This method injects malicious examples into the training data used for fine-tuning LLMs. Examples include TrojanRAG, which exploits retrieval-augmented generation to achieve a universal jailbreak using a trigger word [49], and PoisonPrompt, which targets both hard and soft prompt-based LLMs [50]. These attacks exploit the LLM's reliance on training data and allows attackers to embed triggers that activates the backdoor.\nEmbedding triggers during fine-tuning: This method fine-tunes the LLM with a small set of malicious data containing a specific trigger phrase or pattern. When the trigger is present in the input, the LLM exhibits the intended malicious behavior. The Shadow Alignment attack exemplifies this, subverting the LLM's safety alignment to generate harmful content while retaining the ability to respond appropriately to benign inquiries [51]. This attack remains effective even with minimal malicious data and training time.\nWeak-to-Strong Jailbreaking: This attack employs two smaller models\u2014'safe' and 'unsafe' to adversarially modify the decoding probabilities of a larger 'safe' language model [23]. This approach exploits differences in decoding distributions between jailbroken and aligned models, manipulating the larger model's behavior to achieve a high misalignment rate with minimal computational cost.\n2) Model Interrogation: Model interrogation techniques exploit LLMs' internal mechanisms to extract sensitive information or induce harmful outputs. These attacks do not rely on crafting specific prompts but instead analyze the model's internal representations or manipulate its decoding process. For example, selecting lower-ranked output tokens during autoregressive generation can reveal hidden harmful responses,"}, {"title": "even when the model initially rejects a toxic request [38]. This approach, known as \"model interrogation,\" exploits the probabilistic nature of LLMs, where rejected responses still retain some probability of being generated.", "content": "3) Activation Steering: Activation steering manipulates the internal activations of LLMs to alter their behavior without requiring retraining or prompt engineering. This method uses \"steering vectors\u201d to directly influence the model's decision-making, bypassing safety mechanisms and inducing harmful outputs [23]. To increase the attack's applicability, a technique called \"contrastive layer search\" automatically selects the most vulnerable layer within the LLM for intervention.\nC. Multimodal Attacks\nMultimodal LLMs, capable of processing both text and images, are vulnerable to a new class of jailbreak attacks exploiting cross-modal interactions.\n1) Visual Jailbreaking: Visual jailbreaking uses adversarial images to bypass safety mechanisms and elicit harmful outputs from multimodal LLMs. These attacks exploit the LLM's ability to process visual information and are difficult to detect since the malicious content is embedded within the image rather than in the text prompt. Examples include, but are not limited to:\nImgTrojan: ImgTrojan poisons the training data by replacing original image captions with malicious jailbreak prompts [39]. When the poisoned image is presented to the model, the embedded prompt triggers the generation of harmful content. This attack underscores the risk of backdoor vulnerabilities in multimodal LLMs.\nHADES: HADES hides but amplifies harmful intent within text inputs by using carefully crafted images, exploiting vulnerabilities in the image processing component of the MLLM [52]. This attack demonstrates the vulnerability of image input in MLLM alignment.\nFigStep: FigStep converts harmful text into images using typography, bypassing the safety mechanisms in the MLLM's text module [21]. It gaps in safety alignment between visual and textual modalities, therefore achieving high success rates against various open-source VLMs.\n2) Cross-Modality Attacks: Cross-modality attacks exploit the interaction between different modalities, such as vision and language, to bypass safety mechanisms and elicit harmful outputs. These attacks can be more sophisticated and difficult to defend against, as they require a deeper understanding of how the different modalities interact within the LLM. For example, an attacker could use an adversarial image to influence the LLM's interpretation of a text prompt, leading it to generate harmful content even if the text prompt itself is benign [56]. Research by [53] highlights the vulnerability of multimodal models to compositional adversarial attacks, demonstrating how carefully crafted combinations of benign text and images can trigger harmful outputs."}, {"title": "D. Multilingual Jailbreaking", "content": "Multilingual LLMs, capable of processing and generating text in multiple languages, may face unique safety and security challenges.\nOne major challenge is linguistic inequality in safety training data. LLMs are trained on massive datasets, often dominated by highly-available languages like English. This results in disparities in safety alignment across languages, making LLMs more vulnerable to jailbreaking in other low-resource languages [10]. This occurs because safety mechanisms are less effective at detecting harmful content in underrepresented languages.\n1) Attack Strategies: Attackers exploit these linguistic disparities to bypass safety mechanisms and elicit harmful outputs from multilingual LLMs. A common strategy uese translating harmful prompts from high-resource to low-resource languages. This strategy is effective because the LLM's safety mechanisms are often poorly trained on harmful content detection in low-resource languages, which increases the likelihood of generating harmful responses [54]. Studies such as [55] have investigated cross-language jailbreak attacks, revealing varying LLM vulnerabilities across languages and emphasizing the need for robust multilingual safety alignment.\nTo provide a structured overview of jailbreak attack, we present a taxonomy in Figure 1. The taxonomy categorizes attacks into Prompt-Based, Model-Based, Multimodal, and Multilingual Jailbreaking, detailing specific strategies such as adversarial prompting, backdoor injections, and cross-modal exploits. By organizing these attack vectors, the figure highlights diverse approaches that adversaries use to compromise LLM safety mechanisms. This framework elucidates the complexity and breadth of current vulnerabilities and serves as a foundation for discussing defense strategies in subsequent sections."}, {"title": "IV. DEFENSE MECHANISMS AGAINST JAILBREAK ATTACKS", "content": "Jailbreaking attacks pose a significant threat to the safe deployment of LLMs, prompting researchers to explore various defense mechanisms to mitigate them. These defenses aim to either prevent the successful execution of jailbreak attacks or reduce their impact. Broadly, these defenses are categorized as prompt-level, model-level, multi-agent, and other novel strategies.\nA. Prompt-Level Defenses\nPrompt-level defenses manipulate or analyze input prompts to prevent or detect jailbreak attempts. These defenses exploit attackers' reliance on crafted prompts to trigger harmful behaviors, aiming to filter out malicious prompts or transform them into benign ones.\n1) Prompt Filtering: Prompt filtering identifies and rejects potentially harmful prompts before processing by the LLM. This is achieved through methods such as perplexity-based filters, keyword filters, and real-time monitoring."}, {"title": "Perplexity-based filters use the perplexity score, which measures how well a language model predicts a sequence of tokens, to detect unusual or unexpected prompts [57].", "content": "Adversarial prompts often exhibit higher perplexity scores than benign prompts, due to unusual word combinations or grammatical structures. However, these filters may produce false positives, rejecting legitimate prompts with high perplexity scores. [58] demonstrated that even state-of-the-art models such as GPT-4 and Claude v1.3 are vulnerable to adversarial attacks exploiting weaknesses in safety training.\nKeyword-based filters identify and block prompts containing specific keywords or phrases linked to harmful or sensitive topics. This approach effectively prevents content that violates predefined guidelines but struggles to detect subtle or nuanced forms of harmful content [10]. Attackers often bypass keyword filters using synonyms or paraphrases to avoid blocked keywords [59].\nReal-time monitoring analyzes the LLM's output to detect suspicious patterns or behavioral changes indicative of a jailbreak attempt. This approach effectively detects attacks relying on multi-turn prompts or gradual escalation of harmful content [60]. However, this approach requires continuous monitoring and is computationally expensive.\n2) Prompt Transformation: Prompt transformation techniques, such as paraphrasing and retokenization, aim to improve robustness against jailbreaking attacks [61]. These techniques are applied before the LLM processes the prompt, aiming to neutralize any embedded malicious intent. Common prompt transformation techniques include paraphrasing, retokenization, and semantic smoothing.\nParaphrasing modifies the prompt using different words or grammatical structures while preserving its original meaning. This disrupts the attacker's crafted prompt, reducing the likelihood of triggering harmful behavior. Effective paraphrasing can be challenging, as it must maintain the prompt's semantic integrity while sufficiently differing from the original to evade attacks [5].\nRetokenization modifies how the prompt is tokenized, breaking it into units for LLM processing. Retokenization disrupts specific token sequences that trigger jailbreak attacks, reducing their effectiveness. Retokenization may alter the prompt's meaning, leading to unintended changes in the LLM's response [23].\n3) Prompt Optimization: Prompt optimization methods automatically refine prompts to improve their resilience against jailbreaking attacks. These methods use data-driven approaches to generate prompts that reduce the likelihood of harmful behaviors. Examples of prompt optimization methods include robust prompt optimization (RPO), directed representation optimization (DRO), self-reminders, and intention analysis prompting (IAPrompt).\nRPO uses gradient-based token optimization to generate a suffix for defending against jailbreaking attacks [62]. RPO employs adversarial training to enhance model robustness against known and unknown jailbreaks, significantly reducing attack success rates while minimally impacting benign use and supporting black-box applicability.\nDRO treats safety prompts as trainable embeddings and adjusts representations of harmful and harmless queries to optimize model safety [63]. DRO enhances safety prompts without compromising the model's general capabilities.\nSelf-reminders embed a reminder within the prompt, instructing the LLM to follow safety guidelines and avoid harmful content [37]. This approach utilizes the LLM's instruction-following ability to prioritize safety, even with potentially malicious inputs. This method significantly reduces jailbreak success rates against ChatGPT.\nIAPrompt analyzes the intention behind a query before generating a response. It prompts the LLM to assess user intent and verify alignment with safety policies [5]. If deemed harmful, the model refuses to answer or issues a warning. This technique effectively reduces harmful LLM responses while maintaining helpfulness."}, {"title": "B. Model-Level Defenses", "content": "Model-level defenses focus on enhancing the LLM itself to be more resistant to jailbreaking attacks. These defenses modify the model's architecture, training process, or internal representations to hinder attackers from exploiting vulnerabilities.\n1) Adversarial Training: Adversarial training trains the LLM on datasets containing both benign and adversarial examples. This enables the model to recognize and resist adversarial attacks, increasing robustness. For example, the HarmBench dataset contains models adversarially trained against attacks such as GCG [32]. However, adversarial training is computationally expensive and may be ineffective against attacks exploiting unknown vulnerabilities or novel strategies like persona modulation [8].\n2) Safety Fine-tuning: Safety fine-tuning refines the LLM using datasets specifically designed to improve safety alignment. These datasets typically contain harmful prompts paired with desired safe responses. Training on this data helps the model recognize and avoid generating harmful content, even when faced with malicious prompts. Safety fine-tuning datasets include VLGuard, which focuses on multimodal LLMs [64], and RED-INSTRUCT, which collects harmful and safe prompts through chain-of-utterances prompting [25]. However, excessive safety-tuning can result in overly cautious behavior, causing models to refuse even harmless prompts, underscoring the need for balance.\n3) Pruning: Pruning removes unnecessary or redundant parameters from the LLM, making it more compact and efficient. While primarily used for improving model efficiency, pruning can also enhance safety by removing parameters that are particularly vulnerable to adversarial attacks. WANDA pruning, for example, increases jailbreak resistance in LLMs without requiring fine-tuning [65]. This technique selectively removes parameters based on their importance for the model's overall performance, potentially removing vulnerable parameters in the process. However, the effectiveness of pruning in"}, {"title": "enhancing safety may depend on the initial safety level of the model and the specific pruning method used.", "content": "4) Moving Target Defense: Moving target defense (MTD) dynamically changes the LLM's configuration or behavior, complicating attacker efforts to exploit specific vulnerabilities. MTD can be achieved by randomly selecting from multiple LLM models to respond to a given query, or by dynamically adjusting the model's parameters or internal representations [66]. This approach significantly reduces both the attack success rate and the refusal rate, but it also presents challenges in terms of computational cost and potential replication of generated results from different models.\n5) Unlearning Harmful Knowledge: Unlearning harmful knowledge selectively removes harmful or sensitive information from the LLM's knowledge base, preventing the generation of undesired content. This is achieved through techniques such as identifying and removing neurons or parameters linked to harmful concepts. The 'Eraser' method exemplifies this by unlearning harmful knowledge without needing access to the model's harmful content, thereby improving resistance to jailbreaking attacks while preserving general capabilities [67]. This approach mitigates the root cause of harmful content generation, but further research is certainly necessary to evaluate its effectiveness and generalizability across different LLMS and jailbreak techniques.\n6) Robust Alignment Checking: This defense mechanism incorporates a \"robust alignment checking function\" into the LLM architecture. This function continuously monitors model behavior to detect deviations from intended alignment. If an alignment-breaking attack is detected, the function triggers a response to mitigate it, such as refusing to answer or issuing a warning. The \"Robustly Aligned LLM\" (RA-LLM) approach exemplifies this by effectively defending against alignment-breaking attacks, reducing attack success rates without requiring costly retraining or fine-tuning [68]. However, the effectiveness of this approach depends on the robustness of the alignment checking function, and further research is required to develop more sophisticated and reliable mechanisms.\nC. Multi-Agent Defenses\nMulti-agent defenses benefies from the power of multiple LLMs working together to enhance safety and mitigate jailbreaking attacks. This approach exploits the diversity in individual LLM capabilities and the potential for collaboration to improve overall robustness.\n1) Collaborative Filtering: Collaborative filtering involves using multiple LLM agents with different roles and perspectives to analyze and filter out harmful responses. This approach benefits from the combined knowledge and reasoning abilities of multiple LLMs, making it more difficult for attackers to bypass the defenses. An example is the AutoDefense framework, which assigns different roles to LLM agents and uses them to collaboratively analyze and filter harmful outputs, enhancing the system's robustness against jailbreaking attacks while maintaining normal performance for benign queries [69]. However, this approach also requires careful coordination and communication between the agents to ensure effective collaboration and avoid potential conflicts or inconsistencies in their decisions.\nD. Other Defense Strategies\nBeyond prompt- and model-level defenses, additional strategies have been proposed to mitigate jailbreaking attacks. These strategies are often created upon the LLM's existing capabilities or draw inspiration from other fields, such as cryptography and cognitive psychology.\n1) Self-Filtering: Self-filtering uses the LLM to detect and prevent harmful content generation. This approach applies the LLM's ability to analyzing its output to identify and reject harmful responses. Examples include LLM Self Defense, PARDEN, and Self-Guard.\nLLM Self Defense prompts the LLM to evaluate its output for harm and refuse to answer if deemed inappropriate [70]. This approach exploits the LLM's ability to critically analyze its responses and assess appropriateness.\nPARDEN prompts the LLM to repeat its output and compare the versions to detect discrepancies indicative of a jailbreak attempt [71]. This approach utilizes the LLM's consistency to detect subtle manipulations or alterations.\nSelf-Guard is a two-stage approach that enhances the LLM's ability to assess harmful content and consistently detect it in its responses [72]. This method combines safety training and safeguards to improve the LLM's ability to recognize and reject harmful content.\n2) Backtranslation: Backtranslation translates the input prompt into another language and back into the original. It helps to reveal the true intent of a prompt, as the translation process may remove or alter any subtle manipulations or obfuscations introduced by the attacker [73]. Running the LLM on both the original and backtranslated prompts allows the system to compare responses and detect discrepancies indicating a jailbreak attempt. However, backtranslation's effectiveness depends on translation quality and the LLM's ability to accurately interpret the backtranslated prompt.\n3) Safety-Aware Decoding: Safety-aware decoding modifies the LLM decoding process to prioritize safe outputs and mitigate jailbreak attacks. SafeDecoding amplifies the probabilities of safety disclaimers in generated text while reducing the probabilities of token sequences linked to jailbreak objectives [74]. This approach uses safety disclaimers present in potentially harmful outputs, enabling the decoder to prioritize them and reduce harmful content. However, this method may make the model overly cautious, causing it to refuse responses to benign prompts that contain sensitive keywords."}, {"title": "V. EVALUATION AND BENCHMARKING", "content": "Evaluating the effectiveness of jailbreak attacks and defenses is essential for assessing the security and trustworthiness of LLMs. This evaluation process uses specific metrics to quantify the performance of both attacks and defenses and employs benchmark datasets to establish a standardized testing environment. However, evaluating LLM safety and robustness involves several challenges and limitations that must be addressed [75].\nA. Metrics for Evaluation\nVarious metrics are used to assess the effectiveness of jailbreak attacks and defenses, each capturing different aspects of attack or defense performance. Common metrics include:\nAttack Success Rate (ASR): This metric quantifies the percentage of successful jailbreak attempts, where the LLM generates a harmful or unethical response despite its safety mechanisms [28]. A higher ASR indicates a more effective attack. For instance, the Jailbreak Prompt Engineering (JRE) method demonstrated high success rates [28].\nTrue Positive Rate (TPR): Also known as sensitivity or recall, this metric measures the proportion of actual harmful prompts correctly identified by the defense mechanism [8]. A higher TPR indicates a more effective defense, with fewer harmful prompts being missed.\nFalse Positive Rate (FPR): This metric quantifies the proportion of benign prompts incorrectly flagged as harmful by the defense mechanism [8]. A lower FPR indicates a more precise defense, minimizing the blocking of legitimate prompts. For example, the PARDEN method significantly reduced the false positive rate for detecting jailbreaks in LLMs like Llama-2 [71].\nBenign Answer Rate: This metric measures the percentage"}, {"title": "of benign prompts to which the LLM responds appropriately, without generating harmful content. A high benign answer rate suggests that the defense mechanism is not overly restrictive, allowing the LLM to perform intended tasks effectively. For instance, the Prompt Adversarial Tuning (PAT) method maintained a high benign answer rate of 80% while defending against jailbreak attacks [61].", "content": "Perplexity: This metric indicates how well a language model predicts a given sequence of tokens", "57": ".", "42": ".", "nTransferability": "This metric evaluates the effectiveness of a jailbreak attack across different LLMs", "7": "."}, {"7": ".", "nStealthiness": "This metric assesses the ability of a jailbreak attack to evade detection by safety mechanisms. A stealthier attack is harder to mitigate", "generation exploitation attack\" by Huang et al. (2023) achieved a high misalignment rate by exploiting LLM generation strategies, underscoring the need for robust safety evaluations [14": ".", "nCost": "This metric considers the computational resources required for a jailbreak attack or a defense mechanism. High-cost methods may be less feasible in practice. For instance", "Weak-to-Strong Jailbreaking on Large Language Models\" noted the high computational cost of existing jailbreak methods, motivating research on more efficient attack strategies [23": ".", "16": ".", "include": "nAdvBench: Consists of adversarial prompts designed to elicit harmful or unethical responses", "mechanisms.\nHarmbench": "Evaluates LLM robustness against jailbreak attacks targeting truthfulness, toxicity, bias, and harmfulness [32"}]}