{"title": "Barriers and Pathways to Human-Al Alignment: A Game-Theoretic Approach", "authors": ["ARAN NAYEBI"], "abstract": "Under what conditions can capable AI agents efficiently align their actions with human preferences? More specifically, when they are proficient enough to collaborate with us, how long does coordination take, and when is it computationally feasible? These foundational questions of AI alignment help define what makes an Al agent \"sufficiently safe\u201d and valuable to humans. Since such generally capable systems do not yet exist, a theoretical analysis is needed to establish when guarantees hold\u2014and what they even are.\nWe introduce a game-theoretic framework that generalizes prior alignment approaches with fewer assumptions, allowing us to analyze the computational complexity of alignment across M objectives and N agents, providing both upper and lower bounds. Unlike previous work, which often assumes common priors, idealized communication, or implicit tractability, our framework formally characterizes the difficulty of alignment under minimal assumptions.\nOur main result shows that even when agents are fully rational and computationally unbounded, alignment can be achieved with high probability in time linear in the task space size. Therefore, in real-world settings, where task spaces are often exponential in input length, this remains impractical. More strikingly, our lower bound demonstrates that alignment is impossible to speed up when scaling to exponentially many tasks or agents, highlighting a fundamental computational barrier to scalable alignment.\nRelaxing these idealized assumptions, we study computationally bounded agents with noisy messages (representing obfuscated intent), showing that while alignment can still succeed with high probability, it incurs additional exponential slowdowns in the task space size, number of agents, and number of tasks.\nThus, our results show that even when agents are highly capable and motivated to cooperate, alignment is intrinsically constrained by the exponential complexity of the task space, number of agents, and number of tasks. We provide explicit algorithms for our general alignment protocols and identify specific conditions where it can be more feasible, involving strategies that either sidestep task space explosion or employ scalable mechanisms for coordination.", "sections": [{"title": "1 Introduction", "content": "The past decade, and most notably the past two years, has seen a surge of progress in artificial intelligence (AI), where the core goal is to construct intelligent agents that can perceive and act meaningfully in their environment, echoing ideas from game theory and economics of rational agents [47]. As these technologies have rapidly shifted from being confined to laboratories to multi-billion dollar industries [40], it becomes ever more pressing, both economically and societally, to understand conditions where we can guarantee that these systems can be deployed safely. One of the key problems in AI safety is the value alignment problem, ensuring that these agents make \"good\" decisions that are in line with human intentions and values [3, 44, 48], anticipated by Norbert Weiner as early as 1960 [55].\nMuch of the current alignment literature deals with the specifics of AI systems today, e.g., preventing the jailbreaking of large language models (LLMs) [20, 28, 32]. This focus makes practical sense for now, as these systems have yet to be as generally capable as humans across domains, and can easily produce undesired side effects (\u201cmisalignment\u201d) simply by malfunctioning and not carrying out the task they were instructed to do. But, as AI systems steadily improve, they will be able to successfully carry out more tasks to completion with us.\nAs such generally-capable AI systems have yet to be built, we cannot simulate these systems at the moment to empirically assess these questions; therefore, a theoretical treatment of the problem can be helpful here. However, there is currently little in the way of proven guarantees as to the conditions under which such capable systems could align with us in general. One of the reasons for this is that existing theoretical frameworks for alignment are tailored to particular toy scenarios, in order to (understandably) prove guarantees.\nAn early approach known as \u201cAI Safety via Debate\u201d [30], proposed to train computationally powerful agents via self-play in a zero-sum debate game, until the misalignment is identified on a manageable subtask of a few bits of disagreement\u00b9. Although this form of debate with optimal play is in PSPACE (and can be extended to NEXP if you include cross-examination [8]), all the methods from the theory of interactive proofs apply to problems which are mathematically precise so that exact misalignment can be efficiently verified. Furthermore, in their specific setup of querying black-box human judgements as an oracle, the main theorem of efficient verification of an unbounded prover (the AI), namely IP = PSPACE [35, 46], does not hold (\u201crelativize\u201d) [18, 19]. To circumvent the latter issue, in recent follow-up work [13], they enforce that the AI prover is also efficient (and such that an honest human verifier could run in polynomial, rather than exponential, time). However, this added constraint removes from consideration many scenarios which can produce an efficient strategy, but the computation to be verified does not have a polynomial-length human-verifiable transcript. (Moreover, the debate setup requires the human oracle to be correct and unbiased, further limiting its applicability.)\nNow, debate is certainly not the only theoretically-motivated approach to alignment. Reinforcement learning of agent actions based on human preferences encompasses a large class of approaches, most notably Reinforcement Learning from Human Feedback (RLHF) [6, 15, 57], which is widely deployed today in real-world systems. However, general provable guarantees are hard to come by in this domain. One can, for example, consider specific games, such as the off-switch game [21]. This game is related to the \u201cshutdown problem\u201d [49], presciently alluded to by Turing in his 1951 BBC radio address [52, pg. 6a], which studies the circumstances under which a rational agent that maximizes its utility would be incentivized to keep its off switch, where the human can also shut it off but the robot can disable its switch too. They prove that for a rational agent to keep"}, {"title": "2 Results", "content": "2.1 Notational Preliminaries\nDefine $[M] := {1, . . ., M}$ and $[N] := {1, . . ., N}$. We will use asymptotic notation throughout that is standard in computer science, but may not be in other fields. The asymptotic notation is defined as follows:\n\u2022 $F(n) = O(G(n))$: There exist positive constants $c\u2081 > 0$ and $c2 0$ such that $F(n) \u2264 C1 + c2G(n)$, for all $n \u2265 0$.\n\u2022 $F(n) = \\tilde{O}(G(n))$: There exist positive constants $C1, C2$, and $k > 0$ such that $F(n) \u2264 c\u2081 + c2G(n) log^k n$, for all $n \u2265 0$.\n\u2022 $F(n) = \u03a9(G(n))$: Similarly, there exist positive constants $c\u2081$ and $c2$ such that $F(n) \u2265 c1 + c2G(n)$, for all $n \u2265 0$.\n\u2022 $F(n) = \u0398(G(n))$: This indicates that $F(n) = O(G(n))$ and $F(n) = \u03a9(G(n))$. In other words, $G(n)$ is a tight bound for $F(n)$."}, {"title": "2.2 (\u039c, \u039d, \u03b5, \u03b4)-Agreement Framework", "content": "The framework we consider for alignment generalizes Aumann agreement [4] to probabilistic (\u03b5, \u03b4)-agreement [1] (rather than exact agreement), across M agreement objectives and N agents, without the Common Prior Assumption (CPA). The CPA dates back to at least Hars\u00e1nyi [24] in his seminal work on games with incomplete information. This is a very powerful assumption and is at the heart of Aumann's agreement theorem that two rational Bayesian agents must agree if they share a common prior [4]. As a further illustration of how powerful the CPA is from a computational complexity standpoint, Aaronson [1] relaxed the exact agreement requirement to (\u03b5, \u03b4)-agreement and showed that even in this setting, completely independent of how large the state space is, two agents with common priors will need to only exchange $O(1/(\u03b4\u03b5\u00b2))$ messages to agree within & with probability at least 1 \u2013 8 over their prior. However, the CPA is clearly a very strong assumption for human-AI alignment, as we cannot expect that our Als will always start out with common priors with every human it will engage with on every task. In fact, even between two humans this assumption is unlikely! For other aspects of agreement and how they relate more broadly to alignment, we defer to the Discussion (\u00a7 3) for a more detailed treatment.\nIn short, (\u039c, \u039d, \u03b5, \u03b4)-agreement represents a \u201cbest-case\u201d scenario that is general enough to encompass prior theoretical approaches to alignment (cf. Remark 2), such that if something is inefficient here, then it forms a prescription for what to avoid in practice, in far more suboptimal circumstances. As examples of suboptimality in practice, we will consider computational boundedness and noisy messages in \u00a72.5, to exactly quantify how the bounds can significantly (e.g. exponentially) worsen.\nDispensing with the CPA, we now make our \u3008M, N, \u03b5, \u03b4)-agreement framework more precise. For illustration, we consider two agents (N = 2), Alice (human) and \u201cRob\u201d (robot), denoted by A and R, respectively. Let $\\{Sj\\}_{j\u2208[M]}$ be the collection of (not necessarily disjoint) possible task states for each task $j \u2208 [M]$ they are to perform. We assume each $Sj$ is finite ($|Sj| = Dj \u2208 N$), as this is a standard assumption, and any physically realistic agent can only encounter a finite number of states anyhow. There are M agreement objectives, $f\u2081, . . ., fm$, that Alice and Rob want to jointly estimate, one for each task:\n$f_j: S_j \u2192 [0, 1], \u2200j\u2208 [M]$,\nto encompass the possibility of changing needs and differing desired $\\{(\u025b_j, dj)\\}_{j\u2208[M]}$-agreement thresholds for those needs (which we will define shortly in (2)), rather than optimizing for a single monolithic task. Note that setting the output of $f_j$ to [0, 1] does not reduce generality. Since $S_j$ is finite, any function $S_j \u2192 R$ has a bounded range, so one can always rescale appropriately to go inside the [0, 1] domain.\nAlice and Rob have priors $P_A^j$ and $P_R^j$, respectively, over task j's state space $S_j$. Let $v_j \u2208 [0, 1]$ denote the prior distance (as introduced by Hellman [25]) between $P_A^j$ and $P_R^j$, defined as the minimal L\u00b9 distance between any point $x_j \u2208 X_j = P_A^j\\times P_R^j$ and any point $p_j \u2208 D_j = \\{(p_j,p_j) | p_j \u2208 \u2206(S_j)\\}$, where $\u2206(S_j) \u2208 R^{D_j}$ is the probability simplex over the states in $S_j$. Formally,\n$v_j = \\min_{x_j\\in X_j,p_j\\in D_j} ||x_j - p_j||_1, \u2200j\u2208 [M]$. (1)\nIt is straightforward to see that there exists a common prior $CP_j \u2208 D_j$ between Alice and Rob for task j if and only if the task state space $S_j$ has prior distance $v_j = 0$. (Lemma 2.3 will in fact show that it is possible to find a common prior with high probability, regardless of the initial value $v_j$.)"}, {"title": "2.3 Proof of Upper Bound", "content": "Here we prove the upper bound in Theorem 2.1. For an explicit algorithm, see Algorithm 1 below.\nFirst, we need to figure out at most how many messages need to be exchanged to guarantee at least one proper refinement. To do so, we will have the N agents communicate using the \u201cspanning-tree\" protocol of Aaronson [1, \u00a73.3], which we generalize to the multi-task setting below:\nLEMMA 2.2 (PROPER REFINEMENT Message Mapping Lemma). If N agents communicate via a spanning-tree protocol for task j, where $g_j \u2208 N$ is the diameter of the chosen spanning trees, then as long as they have not yet reached agreement, it takes $O(g_j) = O(N)$ messages before at least one agent's knowledge partition is properly refined."}, {"title": "2.4 Discretized Extension and Lower Bound", "content": "A natural extension of Theorem 2.1 is if the agents do not communicate their full real-valued expectation (which may require infinitely many bits), but a discretized version of the current expectation, corresponding to whether it is above or below a given threshold (defined below), e.g. \u201cHigh\u201d, \u201cMedium\u201d, or \u201cLow\u201d (requiring only 2 bits). We show the bound from Theorem 2.1 remains unchanged in this case.\nPROPOSITION 2.5 (DISCRETIZED Extension). If N agents only communicate their discretized expectations, then they will \u3008M, N, \u03b5, \u03b4) -agree with overall failure probability d across M tasks as defined in (2), after T = O \\left(M^3 N^7\\over \\epsilon^2 \\delta^2 + MN^2D\\right)  messages, where D := maxj\u2208[M] Dj and \u025b := minj\u2208[M] \u025bj. Thus, for the special case of M = 1 tasks and N = 2 agents, this becomes T =  O \\left({1\\over \\epsilon^2 \\delta^2}+D\\right) messages before they (M, N, \u03b5, \u03b4)-agree with total probability \u2265 1 \u2212 \u03b4.\nSimilarly for s', hence the ratio \\pi_j^i (s_j | C)/\\pi_j^i (s'_j| C) differs by at most \u00b1\u025b with probability \u2265 1 \u2013 2\u03b4', as it is computed from two such independent estimates of \u03c4. Taking \u03b4' = \u03b4/(2ND)\nBelow is the best lower bound we can prove for \u3008M, N, \u03b5, \u03b4)-agreement:\nPROPOSITION 2.6 (LOWER BOUND). There exist functions fj, input sets Sj, and prior distributions $\\{P_i^j\\}_{i\u2208[N]}$ for all $j \u2208 [M]$, such that any protocol among N agents needs to exchange \u03a9 (\u039c\u039d\u00b2 log (1/\u03b5)) bits to achieve (M, N, \u03b5, \u03b4)-agreement on $\\{f_j\\}_{j\u2208[M]}$, for \u025b bounded below by $\\min_{j\u2208[M]} \u03b5_j$.\nPROOF. For each task $j \u2208 [M]$, let the input tuple to the N agents be\n$(x_{1,j}, x_{2,j}, \u2026, x_{N,j}) \u2208 S_j$,\nwhere $S_j$ is defined by\n$S_j := {(x_{1,j},\u2026, x_{N.j}) | \\forall i \\in {1, \u2026, N\\}, x_{i,j}  \\in {(j \u2212 1) \u2022 2^n + 1,\u2026, j \u2022 2^n}}}$.\nThus, each $x_{i,j}$ is an integer in an interval of size $2^n$ that starts at $(j \u2212 1) \u2022 2^n + 1$. We endow $S_j$ with the uniform common prior CPj (which will be necessarily difficult by the counting argument below), and define\n$\\displaystyle f_j(x_{1,j},\u2026, x_{N,j}) = {\\sum_{i=1}^{N} x_{i,j} \\over 2^{n+1}}$\nObserve that $\\sum_{i=1}^{N} x_{i,j}$ is minimally $N ((j \u2212 1) 2^n + 1)$ and maximally $N j 2^n$. Hence, the image of fj is contained within\n$\\displaystyle\\left[{N((j \u2212 1)2^n + 1) \\over 2^{n+1}}, {N j2^n \\over 2^{n+1}}\\right] = \\left[{N(j \u2212 1) \\over 2^{n+1}} + {N \\over 2}, {Nj \\over 2}\\right]$.\nTherefore, for j \u2265 1, each instance fj is structurally the same \u201cshifted\u201d problem, but crucially non-overlapping for each $j \u2208 N$. So it suffices to show that for each j, each instance individually saturates the \u03a9 (\u039d\u00b2 log(1/\u025bj)) bit lower bound, which we will do now:\nTwo-Agent Subproblem for N Agents. Because all agents must (\u025bj, dj)-agree on the value of fj, it follows that in particular, every pair of agents (say (i, k)) must have expectations of fj that differ by at most \u025bj with probability at least 1 \u2013 8j. But for any fixed pair (i, k), we can treat (xi,j, xk,j) as a two-agent input in which all other coordinates $x_{\\ell,j}$ for l \u2260 i, k are \u201cknown\u201d from the perspective of these two, or do not affect the difficulty except to shift the sum.\nSuppose agent k sends only t < $\\frac{1}{\\epsilon}$ ( log_2 \\frac{1}{2 \\epsilon_j} ) bits to agent i about its input $x_{k,j}$. Label the $2^t$ possible message sequences by m = 1,..., $2^t$, with probability $p_i^m$ each. Since $x_{k,j}$ is uniform in an interval of size $2^n$, then conditioned on message m, there remain at least $\\frac{2^n}{p_i^m}$ possible values of $x_{k,j}$. Each unit change in $x_{k,j}$ shifts fj by $1/2^{n+1}$, so even if agent i's estimate is optimal, the fraction of $x_{k,j}$ values producing $|E_i^{lst} - E_k| \u2264 \u025b_j$ is at most\n$\\frac{2^{n+1} \u025b_j}{\\frac{2^n}{p_i^m}}$ = $ \\frac{2 p_i^m}{\\epsilon_j}$.\nHence, the total probability of agreement (over all messages m) is bounded by\n$\\sum_{m=1}^{2^t} p_i^m\\frac{2 \\epsilon_j}{p_i^m}$ = $2 \\epsilon_j 2^t$.\nIf $2 \u025b_j 2^t < 1 - \u03b4_j$, the agents fail to (\u025bj, dj)-agree. Equivalently, $t \\geq log_2 \\frac{1-\u03b4_j}{2\u025b_j}$. Since every pair (i, k) needs \u03a9 (log (1/\u025bj)) bits for each of M tasks, and there are $\\binom{N}{2} = O(N^2)$ pairs, the total cost is\n\u03a9 ($MN^2$ log (1/\u03b5)),\nwhere \u03b5 := $\\min_{j\u2208 [M]} \u03b5_j$, corresponding to the most \u201cstringent\u201d task j."}, {"title": "2.5 Computationally Bounded Agents", "content": "Thus far, we have examined computationally unbounded rational agents, where we implicitly assumed that constructing and sending each message takes O(1) time. We also assumed it took O(1) time to find the common prior amongst the posterior belief states to then condition on. Furthermore, we implicitly assumed that access to (e.g. sampling of) each agents' distributions was O(1) time. Nevertheless, in this idealized scenario, we see that the linear scaling with respect to the task space size in Theorem 2.1 represents the main significant computational barrier if the task space is exponentially large in the length of the agents' inputs, as well as having exponentially many distinct tasks (or agents), by the lower bound in Proposition 2.6.\nHowever, one might be tempted to say, \u201cWell, this is only a linear scaling with respect to the task space size in the most ideal setting. How much more computationally difficult does \u3008M, N, \u03b5, \u03b4)-agreement generally become when we remove all of the assumptions above?\u201d Furthermore, what if the messages have noise in them, in order to start to account for (at a very basic level) obfuscated intent [7]?\nWe address all of these questions here, by analyzing the communication complexity of N computationally bounded rational agents. This is also important in the context of agents today, since the Bayes-suboptimality we see in current LLM agents can be partly attributed to the fact that they are computationally bounded. One thing we additionally account for is that in practice, querying a human is often much more costly than querying an AI agent. Therefore, we assume we have q humans that take time $T_H$ steps to query, and $N \u2212 q$ AI agents that take $T_{AI}$ steps to query. Note that the latter quantification takes into account recent reasoning models that take multiple \u201creasoning\" steps to produce an outcome (e.g. as of this writing, OpenAI's o1 model [31] and Google Deepmind's Gemini 2.0 Flash [17]). We can also do this without loss of generality, since we could of course have individual $T_{H_y}$ and $T_{AI_z}$ steps for each human y \u2208 [q] and AI agent z \u2208 [N \u2212 q], but the upper bounds we derive here in Theorem 2.7 will remain the same by simply taking the maximum within each group. Instead, what we will do is differentiate between the two groups is the time complexity of two basic subroutines:\nRequirement 1 (Basic Capabilities of Bounded Agents). We expect the agents to be able to:\n(1) Evaluation: The N agents can each evaluate fj(sj) for any state sj \u2208 Sj, taking time $T_{eval,a}$ steps for a \u2208 {H, AI}.\n(2) Sampling: The N agents can sample from the unconditional distribution of any other agent, such as their prior $P_i^j$, taking time $T_{sample,a}$ steps for a \u2208 {H, AI}.\nNote that we will be treating these subroutines as black boxes, where the agents do not get explicit descriptions of fj and the distributions (just as they would not necessarily get this in the real world), but can only learn about them through these subroutines. Just as in CIRL, this is meant to capture how we can tell whether or not the robot has done the task we intended, without necessarily being able to spell out all the steps to get there. Thus, what we prove can broadly apply to any implementations of them, especially with varying amounts of efficiency between querying humans and AI agents. As a result, we will express the total complexity of \u3008M, N, \u03b5, \u03b4)-agreement in terms of $T_{eval,H}, T_{eval,AI}, T_{sample,H}$, and $T_{sample,AI}$.\nThe justification for these subroutines is that they are the bare minimum needed to start to estimate each others' expectations, and are abilities we can imagine any capable agent that is entering into an alignment scenario can do, in order to be considered \u201ccapable\u201d in the first place. As will become clear in the proof of Theorem 2.7, we do not even need these subroutines to be exact (as long as they can be probabilistically evaluated in polynomial time). The second subroutine is a computationally bounded version of our earlier (standard [4, 5]) assumption that the agents\""}, {"title": "3 Discussion", "content": "We developed a game-theoretic framework, \u3008M, N, \u03b5, \u03b4)-agreement, to study alignment between capable rational agents, encompassing earlier theoretical approaches as special cases (Remarks 1 and 2). While alignment can be achieved with arbitrarily high probability (Theorem 2.1, Proposition 2.5, Theorem 2.7), the process can be exponentially costly.\nFor instance, among computationally bounded agents, communication complexity scales exponentially with task space size, number of agents, and number of tasks (Theorem 2.7). Even with unbounded agents (Theorem 2.1), alignment remains exponential when the task space itself grows exponentially with input length-common in real-world settings. Furthermore, it is impossible to be unilaterally improved if there are exponentially many distinct tasks (or agents) over which we want alignment, by the lower bound in Proposition 2.6.\nThese findings suggest that ensuring high-confidence alignment for complex tasks-whether due to large state spaces, many subtasks, or distributional complexity\u2014may be impractical, even under ideal conditions. This holds for both computationally unbounded Bayesian agents (Theorem 2.7) and bounded agents statistically indistinguishable from unbounded Bayesians, even if they share a common prior (Corollary 2.8), especially given the higher cost of human queries compared to Al queries in practice."}, {"title": "3.1 When Alignment Can Be More Feasible", "content": "More broadly, the advantage of having a mathematically rigorous and general framework for alignment allows us to also explicitly characterize how and when alignment can become more efficient, to guide future research and sharpen current approaches.\nThus, our current results can set a prescription for future alignment research, showing that it can be feasible in the following mutually inclusive instances:\n(1) Cutting Down the Task Space & Number: If the task space over which alignment is performed is sufficiently small enough, then our upper bounds in Theorem 2.1, Proposition 2.5, and Theorem 2.7 show that this is much more feasible in practice. These observations motivate a line of work that avoids steering the base model, but to align it externally by simplifying the model's output task space via filtering it through a more controllable bottleneck that has provable guarantees (cf. some very recent work in this direction [12, 43]). We can think of this approach as a specific prescription for \u201couter alignment\u201d [14, 29, 39]."}, {"title": "3.2 Critiques", "content": "In what remains, a natural question for discussion is why should any form of Aumannian agreement be seen as a theoretical framework for alignment? We note that we have come a long way from standard Aumannian agreement, generalizing to multiple agents and tasks, all without the Common Prior Assumption (CPA) and without requiring exact agreement\u2014thereby capturing prior approaches to both agreement in game theory (cf. Remark 1) and alignment (cf. Remark 2). However, there are certainly assumptions we have made thus far that can reasonably be challenged in practice (e.g. modeling agents as either approximately or aspirationally Bayesian). For example, while we do not actually think of humans as Bayes-rational (which has been debunked for a long time in the social sciences, through the work of Allais and Hagen [2], Tversky and Kahneman [53], and others [56]), one of the main reasons for why people are not Bayes-rational in general is that they tend to stick to their beliefs and agree with things that validate those beliefs. While very recent work has shown that interaction with LLMs can be used to dissuade people of their strongly held beliefs better than even other people can [16], this is not strictly necessary in our setup. This is because unlike standard Aumannian agreement setups which are only between humans, our desired outcome is for the AI agent(s) to align on a function fj that the human already internally prefers! In other words, the human is incentivized to collaborate here, by construction.\nTherefore, if something is already inefficient in our setting (as listed above), we should avoid it in current practice. This is especially true when dealing with non-cooperative or malfunctioning (and therefore non-Bayesian) agents, as it can only worsen inefficiencies and may fail to even ensure convergence to agreement."}, {"title": "A Appendix: Proofs of Theorem 2.7 and Corollary 2.8", "content": "Here we prove both Theorem 2.7 and Corollary 2.8. We do this by generalizing Aaronson's computational-boundedness treatment [1, \u00a74] from 2 agents to N agents (specifically, N \u2013 q agents and q humans that have differing, rather than equal, query costs) and M functions (rather than 1), using a message-passing protocol that combines his smoothed and spanning tree protocols, all without the Common Prior Assumption (CPA).\nA.1 Message-Passing Protocol\nThis is the multi-task generalization of Aaronson's [1, \u00a74.1] \u201csmoothed standard protocol\u201d, additionally extended to the multi-agent setting a spanning tree the agents use to communicate their messages.\nLet bj = $\\left[log_2(\\frac{50}{\\epsilon})\\right]$ be a positive integer we will specify later with a specific constant value C > 0, in (11). The N computationally bounded agents follow the \u3008M, N, \u03b5, \u03b4)-agreement algorithm (Algorithm 1), passing O(bj)-bit messages according to the following protocol P:\nProtocol P description (for each task j \u2208 [M]):\n(1) Current posterior expectation. The sending agent i \u2208 [N] has at timestep t\u22121 a real value $E_i^{jt-1}(s_j) \u2208 [0, 1]$, which is its conditional expectation of $f_j \u2208 [0, 1]$ given its knowledge partition $\\Pi_i^{jt-1}(s_j)$ and the current task state $s_j \u2208 S_j$. (Recall that $E_i^{jt-1}(s_j) := B_{P_i^{jt-1}}[f_j | \\Pi_i^{jt-1}(s_j)]$.) The knowledge partition $\\Pi_i^{jt-1}(s_j)$ is formed after updating this expectation using Bayes' rule, after having received the earlier message at time t \u2212 2.\n(2) Draw an integer $r_j$ via a triangular distribution. Agent i picks an integer offset\n$r_j\u2208 {-L_j, -L_j + 1, \u2026, L_j}$\naccording to a (continuous) triangular distribution $\u2206_{tri}(\u00b7; a_j)$ that places its mass in the discrete set of values $\\{-L_j,\u2026, L_j\\}$, and has effective width $2a_j$. These discrete offsets $r_j$ ensure that the messages will be discrete as well. Concretely,\n$\\displaystyle P[r_j = x ] = \\frac{L_j - |x|}{\\sum_{z=-L_j}^{L_j}(L_j -|x|)} = {L_j - |x| \\over L_j^2}$ for $x \u2208 {-L_j, \u2026, L_j}$,\nwhere Lj is chosen so that $2^{-b_j}L_j = a_j$ to bound the messages, as explained in the next step below. Note that the form above is chosen so that the \"peak\u201d of the discretized triangular distribution is at $r_j$ = 0. In other words, the form above is maximized in probability when the offset $r_j$ = 0 (which means that no noise is added to the messages with the highest probability).\n(3) Form the message with noise. The agent then sets\n$m^j\\pi_i^{jt} (s_j)) = round \\left(E_i^{jt-1}(s_j)\\right) + 2^{-b_j}r_j$\nwhere round $\\left(E_i^{jt-1}(s_j)\\right)$ denotes rounding $E_i^{jt-1}(s_j)$ to the nearest multiple of $2^{-b_j}$ (thereby keeping it in the [0, 1] interval). This ensures that the message $m_i^j$ is itself a multiple of $2^{-b_i}$ (thereby being encodable in O(bj) bits), and is offset by \u00b1aj from $\\mathrm{round}(E_i^{jt-1}(\\pi_i^{jt} (s_j)))$, since $|2^{-b_j}r_j| \u2264 2^{-b_j}L_j = a_j$, by construction. Hence, each $m_i^j \u2208 [-a_j, 1 + a_j]$.\n(4) Broadcast. This message $m_i^j (\\Pi_i^{jt-1}(s_j))$ is then broadcast (either sequentially or in parallel) to the relevant agents via an edge of the two spanning trees SP USP each of diameter gj, just as in Lemma 2.2, who update their knowledge partitions accordingly to Step 1."}, {"title": "A.2 Sampling-Tree Construction and Simulation for Each Task j \u2208 [M]", "content": "In our framework", "Sample$(\\pi^{jt}| \\Pi_i^{jt} (s_j))$\u201d in a black-box manner. Thus, before any messages are exchanged, each agent constructs a sampling tree $T_i^j$ offline of unconditional samples from the priors $P_i^j$ (which they are able to do by Subroutine 2 in Requirement 1). The idea is to precompute enough unconditional draws so that each new message can be simulated via \u201cwalking down\u201d the relevant path in the tree that is consistent with the current message history (including that new message), rather than enumerating or sampling from the newly refined partition directly.\nThat is the intuition. We now explain in detail how each agent can use sampling trees to simulate this protocol in a computationally bounded manner. This follows Aaronson's approach [1, \u00a74.2": "of dividing the simulation into two phases\u2014(I) Sampling-Tree Construction (no communication) and (II) Message-by-Message Simulation\u2014but extended here to our multi-task and multi-agent setting.\n(I) Sampling-Tree Construction (General N-Agent Version). For each task j \u2208 [M", "N": "that agent i builds a sampling tree $T_i^j$ of height $R_j$ and branching factor $B_j$. We fix an ordering of the O (Rj) messages for task j (so that we know which agent is \"active\u201d at each level). Formally", "ActiveAgentj": {"N": "denote the function specifying which agent sends the message at each round l = 0", "1": "n(a) Let aj := ActiveAgentj(l) be the agent whose distribution we want at level l (i.e. the agent who sends the l-th message).\n(b) Every node $v_j$ at level \u0142 is labeled by some previously chosen sample (if l = 0 and $v_j$ is the root, we can label it trivially or treat i's perspective by a do-nothing step).\n(c) Each of the Bj children $w \u2208 Children(v_j)$ at level l +1 is labeled with a fresh i."}}]}