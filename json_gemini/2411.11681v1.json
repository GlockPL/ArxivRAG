{"title": "PSPO*: An Effective Process-supervised Policy Optimization for Reasoning Alignment", "authors": ["Jiawei Li", "Xinyue Liang", "Yizhe Yang", "Chong Feng", "Yang Gao"], "abstract": "Process supervision enhances the performance of large language models in reasoning tasks by providing feedback at each step of chain-of-thought reasoning. However, due to the lack of effective process supervision methods, even advanced large language models are prone to logical errors and redundant reasoning. We claim that the effectiveness of process supervision significantly depends on both the accuracy and the length of reasoning chains. Moreover, we identify that these factors exhibit a nonlinear relationship with the overall reward score of the reasoning process. Inspired by these insights, we propose a novel process supervision paradigm, PSPO*, which systematically outlines the workflow from reward model training to policy optimization, and highlights the importance of nonlinear rewards in process supervision. Based on PSPO*, we develop the PSPO-WRS, which considers the number of reasoning steps in determining reward scores and utilizes an adjusted Weibull distribution for nonlinear reward shaping. Experimental results on six mathematical reasoning datasets demonstrate that PSPO-WRS consistently outperforms current mainstream models.", "sections": [{"title": "Introduction", "content": "Solving tasks that require complex reasoning, such as mathematical problems, remains a significant challenge for large language models (LLMs). Chain-of-Thought (CoT) has shown potential in significantly improving the performance of models on complex reasoning tasks by guiding them to break down and solve problems step by step (Wei et al. 2022). Studies have shown that an effective reasoning chain can significantly improve a model's performance on downstream tasks. Conversely, unreliable reasoning chains can mislead the model and produce incorrect results (Wang et al. 2023b; Jin et al. 2024). Therefore, quantifying an accurate reasoning process is crucial for effectively addressing the complex reasoning task. Using process supervision to achieve reasoning alignment is considered an effective way to quantifying reasoning process (Lightman et al. 2023; Luo et al. 2023; Liang et al. 2024). In process supervision, each step receives precise supervision. It provides feedback for each individual step in a chain-of-thought and rewards the model for aligning with the human-approved reasoning process (Uesato et al. 2022b; Lightman et al. 2023).\nThe implementation of process supervision involves training process-supervised reward models (PRMs) and computing a single PRM score by aggregating scores from multiple reasoning chains (Uesato et al. 2022b). Lightman et al. (2023) employed manual annotation to obtain 800K aligned data for training the PRMs. They defined the PRM score for a reasoning chain as the probability that every step is correct under the PRM, which is calculated as the product of the probabilities of correctness for each step. Wang et al. (2023a) avoided dividing the reasoning chain into multiple steps for PRM training; instead, they trained the PRM to evaluate the entire reasoning chain collectively. However, these methods overlook the impact of the length of the reasoning chain on the PRM score. In this paper, we propose that the effectiveness of process supervision is influenced by both the accuracy and the length of reasoning chains. As illustrated in Figure 1, inaccurate reasoning chains those that are excessively long or short can lead to wrong outcomes. Consequently, we aim to develop more effective process supervision methods to address this issue.\nIn this work, we propose a process-supervised policy optimization (PSPO*) paradigm. PSPO* systematically stan-"}, {"title": "Related Works", "content": "Alignment Methods\nLLMs trained on large-scale corpora have demonstrated outstanding reasoning abilities. Despite their significant per-\nperformance, these models are prone to limitations like misunderstandings of human instructions, logical errors that do not conform to common sense, and providing inaccurate information. Therefore, aligning LLMs with human expectations has become a focus of research (Wang et al. 2023c). The classical Reinforcement Learning from Human Feedback (RLHF) framework for alignment (Ouyang et al. 2022) typically consists of two stages: (1) Reward training based on human feedback, where a reward function is learned. (ii) Optimization, where the policy model uses a reinforcement learning algorithm(Proximal Policy Optimization, PPO (Schulman et al. 2017)) to optimize the rewards learned in the previous step. However, RLHF is a complex and often unstable procedure, Rafailov et al. (2023) introduced Direct Preference Optimization (DPO), which parameterizes reward model, solving the standard RLHF problem with a simple classification loss. In the classical RLHF framework, PPO is employed to learn from sparse, sentence-level rewards, to optimize its open-source implementation, Zhong et al. (2024) introduced a framework named Reinforced Token Optimization (RTO), which learns a token-wise reward function from preference data and performs policy optimization based on the learned reward signal. Azar et al. (2024) further proposed a general objective, termed \u03a8\u03a1\u039f, and revealed that in principle RLHF and DPO can be both prone to overfitting as both methods rely on the strong assumption that pairwise preferences can be substituted with ELo-score (pointwise rewards) via a Bradley-Terry modelization. Therefore, we propose a novel PSPO* paradigm for process supervision involving step-level pointwise rewards and policy optimization.\nReasoning Process Alignment\nLLMs have significantly progressed in complex multi-step reasoning tasks in recent years. Studies have shown that step-wise reasoning, such as CoT and ToT, can improve model performance on reasoning tasks (Cobbe et al. 2021; Wei et al. 2023; Yao et al. 2023) as it helps to decompose complex problems and guide models toward solutions. Such methods can enhance the reasoning abilities of LLMs, particularly in mathematical reasoning, where accuracy requires a precise chain of thought (Kojima et al. 2023). Uesato et al. (2022a) and Lightman et al. (2023) introduced both the Outcome-supervised Reward Model (which provides feedback for the final result) and the Process-supervised Reward Model (which provides feedback for step-level reasoning process), and demonstrated that process-based supervision is necessary for correct reasoning steps and avoiding false positives solutions that reach correct answer with incorrect reasoning. As ensuring the correctness of each reasoning step is critical, Lai et al. (2024) proposed Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. However, these methods require a large amount of supervised data, which is very costly. Zhang et al. (2024) further proposed a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step"}, {"title": "The Paradigm for Process Supervision", "content": "Similar to the standard RLHF paradigm, the process supervision based on human feedback primarily consists of two stages: learning the reward model and optimizing the policy based on the learned reward model (Azar et al. 2024). Considering the characteristics of process supervision, in this section, we will provide a detailed introduction to these two stages and propose a general theoretical paradigm applicable to process supervision."}, {"title": "Reward Model for Process Supervision", "content": "In the process of training the outcome-supervised reward models (ORMs), annotators are required to distinguish between human-preferred and non-preferred responses in the candidate responses for a given input (Ouyang et al. 2022). Based on this annotated data, researchers typically employ the Bradley-Terry model to construct a classification model, and subsequently train the reward model using pairwise loss (Wang et al. 2023c; Rafailov et al. 2023). For a given context x and action y, the Bradley-Terry model represents the preference function $p(Y_w > y_l|x)$ as a sigmoid of the difference of rewards:\n$P(Y_w > Y_l | x) = \\sigma(r(x,y_w) - r(x,y_l))$,\nwhere $\\sigma(\\cdot)$ denotes the sigmoid function and plays the role of normalisation, $r(x, y)$ denotes the pointwise reward of $y$ given $x$, $y_w$ denotes the human-preferred responses and $y_l$ denotes the non-preferred responses. Given the dataset $\\mathcal{D} = \\{(x_i, y_{w,i} y_{l,i})\\}_{1}^{N}$ one can learn the reward function by optimizing the following logistic regression loss:\n$\\mathcal{L}(x) = -\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}[\\log(p(y_w > y_l | x))]$.\nIn the process of training the PRMs, annotators are required to assess the correctness of each step in the model-generated solutions. Specifically, as illustrated in Figure 2, annotators typically need to determine whether the current reasoning step is negative, neutral, or positive, and correspondingly select from [-1, 0, 1] (Lightman et al. 2023; Ma et al. 2023). These annotated data are subsequently used to train the reward model, thereby enhancing its capability to distinguish and classify negative, neutral, and positive steps. However, due to the absence of pairwise comparison data regarding human preferences in this process, the Bradley-Terry model cannot be employed to construct a classification model. Here we redefine the training process of PRM.\nIn light of the coherence of reasoning steps, evaluating the accuracy of the k-th reasoning step $y_k$ necessitates the simultaneous consideration of the input x and the preceding k reasoning steps $y_{pre}^k$ as context. The reward model maps"}, {"title": "Policy Optimization with the Reward Score", "content": "Based on the reward model $r(x, y_{pre}^k, y_k)$, the score $R_k$ for the current k-th reasoning step only reflects the quality of\nthat individual step and not the whole reasoning process. In process supervision, the reward for the whole reasoning process can only be evaluated by accumulating the scores of all reasoning steps. We define the overall reward score for the whole reasoning process as $R(x, y)$, and let $\\mathcal{F}$ be the accumulation function. In previous studies, the construction of the reward function ($\\mathcal{F}$) typically only considered the impact of accuracy for the overall reward score $R(x, y)$ in the reasoning chains (Lightman et al. 2023). Our contribution lies in proposing that, the accumulation function $\\mathcal{F}$ should simultaneously account for both the accuracy and the length of reasoning chains in process supervision. Specifically, we define the length of reasoning chains by the number of steps in the reasoning process, then:\n$R(x,y) = \\mathcal{F}(R_1, R_2,\\ldots, R_t)$,\nwhere t denotes the total number of reasoning steps.\nThe objective of process supervision is to optimize the policy function $\\pi\\in \\Delta(x, y)$ through the overall reward score $R(x, y)$ of the reasoning process, thereby maximizing the expected reward. Simultaneously, it aims to minimize the KL divergence between $\\pi$ and the reference policy $\\pi_{ref} \\in \\Delta(x,y)$:\n$J(\\pi) = \\mathbb{E}_{\\pi}[R(x, y) \u2013 \\beta D_{KL}(\\pi || \\pi_{ref})]$,\nwhere $\\beta$ is a hyperparameter used to limit the difference between the new and reference policies, balancing the exploration and exploitation of the policy."}, {"title": "Nonlinear Reward Shaping", "content": "In traditional RLHF processes, there is typically a linear trend between the quality of the model-generated response\nand the reward score, such that higher response quality corresponds to higher reward scores. In process supervision, researchers commonly construct the accumulation function $\\mathcal{F}$ for the reasoning process based on this linear trend. For instance, Lightman et al. (2023) proposed that using the product of the reward scores for each reasoning step as the accumulation function $\\mathcal{F}$, thereby modeling the overall reward score for the entire reasoning process, as follows:\n$R(x,y) = \\prod_{j=1}^{t} P(y^{j} = 1|x^{j}, Y_{pre})$.\nHowever, when the number of reasoning steps is not fixed, the overall reward score is influenced by the number of reasoning steps. As the correctness probability is decimal, the more steps involved in reasoning, the smaller the product of probabilities, resulting in lower rewards, which leads to a tendency for the policy to subsequently generate fewer reasoning steps.\nFrom Equation 6, it can be observed that the accumulation function $\\mathcal{F}$ is not only related to the quality of the reasoning steps but also to the length of the reasoning chains. The accumulation function $\\mathcal{F}$ based solely on the linear trend of reasoning step quality is insufficient for comprehensively modeling process supervision. Therefore, further refinement of the accumulation function is necessary.\nA key contribution of our work is the introduction of nonlinear reward shaping to refine the accumulation function. Reward shaping refers to the process of transforming prior knowledge into additional rewards, guiding the policy to learn faster and better by combining the original rewards with these new rewards (Hu et al. 2020). In process"}, {"title": "PSPO-WRS: Process-supervised Policy Optimization with Nonlinear Reward Shaping", "content": "In process supervision, there is a nonlinear relationship between the number of reasoning steps and the overall reward score. The goal of the CoT reasoning is typically to solve the problem while minimizing computational complexity (Wei et al. 2022). Fewer reasoning steps imply higher efficiency, but this does not always correlate with higher accuracy or correctness. Conversely, a reasoning process with more steps might achieve greater accuracy, but at the cost of lower efficiency. Based on this prior knowledge, we employ the Adjusted Weibull distribution to shape the rewards for the number of reasoning steps. The reward shaping function is as follows:\n$R_s = C*(\\frac{t}{\\lambda})^{k-1}e^{-(\\frac{t}{\\lambda})^k}$,\nwhere $C$ is a constant coefficient used to adjust the overall reward score, $\\lambda$ is the scale parameter, which determines the spread of the distribution, and k is the shape parameter, which dictates the shape of the distribution.\nAdditionally, for the accumulation function $\\mathcal{F}$, to eliminate the linear trend and account for the number of reasoning steps, we standardized the step count based on the method proposed by Lightman et al. (2023), specifically:\n$\\mathcal{F} = \\prod_{j=1}^{t} [P(y^{j} = 1|x^{1}, Y_{pre})]^{1/t}$.\nFinally, we propose process supervision based on adjusted Weibull Reward Shaping (PSPO-WRS):\n$J(\\pi) = \\mathbb{E}_{\\pi}[R_s \\mathcal{F} \u2013 \\beta D_{KL}(\\pi || \\pi_{ref})]$.\nThe PSPO-WRS introduces nonlinear reward shaping, integrating both the accuracy and the length of reasoning chains into process supervision. In the experimental section, we will demonstrate the effectiveness of PSPO-WRS."}, {"title": "Ablation Analysis", "content": "Process supervision is nonlinear. In the ablation study, we evaluated the effectiveness of nonlinear rewards within the PSPO-WRS. Notably, upon the removal of the nonlinear reward, our method degenerated into the process-supervised reward method proposed by Lightman et al. (2023), and the results are illustrated in Table 4. Following the removal of the nonlinearity module, there is a noticeable decline in the performance of PSPO-WRS. The experimental results indicate that the introduction of nonlinear rewards led to significant performance improvements across all evaluated datasets. These findings underscore the critical importance of nonlinear rewards in the process supervision.\nIt is necessary to incorporate the length of reasoning chains into process supervision through nonlinear rewards. Our ablation study confirms that process supervision depends not only on the accuracy of the reasoning chain but also on its length, and introduced nonlinear rewards accordingly. Specifically, in our experimental setup, we measure the length of reasoning chains by the number of reasoning steps involved. Table 4 demonstrates that PSPO-WRS generates longer average reasoning chains. Further analysis of Figure 6 and Figure 7 reveals that without nonlinear rewards, the probability of the policy generating a particular reasoning process significantly decreases as the number of steps increases. Additionally, the reward scores for higher reasoning steps also diminish. However, the incorporation of nonlinear rewards mitigates this phenomenon.\nFigure 6 demonstrates that without nonlinear rewards, the"}, {"title": "Conclusion", "content": "In this paper, we substantiate the critical role of accuracy and the length of reasoning chains in enhancing the efficacy of process supervision. We demonstrate through empirical evidence that these factors are interrelated in a nonlinear manner, significantly impacting the reward scores of reasoning processes. Inspired by these insights, we propose a novel\nprocess supervision paradigm, PSPO*, which systematically outlines the workflow from reward model training to policy optimization, and highlights the importance of nonlinear rewards in process supervision. To enhance the nonlinear impact, we propose using nonlinear accumulation function related to the length of reasoning chains, along with nonlinear reward shaping within the PSPO* paradigm. Based on the PSPO* paradigm, we introduced PSPO-WRS, which leverages an adjusted Weibull distribution for nonlinear reward shaping. The experimental results confirm our hypothesis and demonstrate that our method enables the model to generate more accurate and logically structured reasoning chains. Furthermore, these findings confirm the potential applicability of PSPO* paradigm across various reasoning disciplines."}]}