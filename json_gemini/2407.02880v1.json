{"title": "Knowledge Composition using Task Vectors with Learned Anisotropic Scaling", "authors": ["Frederic Z. Zhang", "Paul Albert", "Cristian Rodriguez-Opazo", "Anton van den Hengel", "Ehsan Abbasnejad"], "abstract": "Pre-trained models produce strong generic representations that can be adapted via fine-tuning on specialised datasets. The learned weight difference relative to the pre-trained model, known as a task vector, characterises the direction and stride of fine-tuning that enables the model to capture these specialised representations. The significance of task vectors is such that simple arithmetic operations on them can be used to combine diverse representations from different domains. This paper builds on these properties of task vectors and aims to answer (1) whether components of task vectors, particularly parameter blocks, exhibit similar characteristics, and (2) how such blocks can be used to enhance knowledge composition and transfer. To this end, we introduce aTLAS, an algorithm that linearly combines parameter blocks with different learned coefficients, resulting in anisotropic scaling at the task vector level. We show that such linear combinations explicitly exploit the low intrinsic dimensionality of pre-trained models, with only a few coefficients being the learnable parameters. Furthermore, composition of parameter blocks enables modular learning that effectively leverages the already learned representations, thereby reducing the dependency on large amounts of data. We demonstrate the effectiveness of our method in task arithmetic, few-shot recognition and test-time adaptation, with supervised or unsupervised objectives. In particular, we show that (1) learned anisotropic scaling allows task vectors to be more disentangled, causing less interference in composition; (2) task vector composition excels with scarce or no labelled data and is less prone to domain shift, thus leading to better generalisability; (3) mixing the most informative parameter blocks across different task vectors prior to training can reduce the memory footprint and improve the flexibility of knowledge transfer. Moreover, we show the potential of aTLAS as a parameter-efficient fine-tuning method, particularly with less data, and demonstrate that it can be easily scaled up for higher performance.", "sections": [{"title": "Introduction", "content": "One practical advantage of neural networks is the fact that knowledge learned from a previous problem, in the form of network weights, can be transferred to solve other related problems. Commonly referred to as transfer learning [6, 71], this technique is often applied when a model trained on a general-purpose dataset\u2014ImageNet [52] for many years\u2014is fine-tuned on other datasets to improve"}, {"title": "Models and task vectors", "content": "As Ilharco et al. [28] demonstrated, task vectors exhibit many intriguing properties across a wide range of models, such as CLIP [47], GPT-2 [46] and T5-based models [48]. To facilitate more in-depth experimentation and analysis, we focus on the CLIP model in this paper, due to its wide availability and manageable size. In particular, we follow previous practice [28, 44] and acquire task vectors by fine-tuning the image encoder, with the text representations frozen. This ensures that image encoders fine-tuned on different datasets produce features residing in the same representation space, through a common text encoder. The task vectors obtained from these fine-tuned encoders can thus be combined more effectively to form a unified multi-task model.\nFormally, denote the CLIP image encoder by $f : X \\times \\Theta \\rightarrow Z$, such that for input image $x \\in X$ and parameters $\\theta \\in \\Theta$, $z = f (x; \\theta)$ is the learned latent representation for the input image. Denote the weights of a pre-trained model by $\\theta_0$, and the weights of its fine-tuned variant by $\\theta_i, i \\in \\mathbb{N}^+$, where $i$ indexes a dataset $D_i$. We follow Ilharco et al. [28] and define a task vector as $T_i = \\theta_i - \\theta_0$. In addition, we investigate task vectors produced by linearised variants of the image encoder using the first-order Taylor expansion,\n$g(x; \\theta) := f(x; \\theta_0) + (\\theta - \\theta_0)^\\top \\nabla_\\theta f (x; \\theta_0).$\nOrtiz-Jim\u00e9nez et al. [44] showed that, task vectors obtained from fine-tuning the linearised variants have low disentanglement errors, and exhibit strong compositional properties."}, {"title": "Learning task vector compositions", "content": "Parameters in a neural network, depending on the depth of the layer, often have different significance. For instance, early layers in convolutional neural networks [18, 53] are known for extracting generic, low-level features, such as edges, corners, etc., while deeper layers produce features more specific to the task. We recognise the non-uniform impacts parameters at different layers can have, and do not perform isotropic scaling on task vectors. Instead, weights, biases and any other forms of parameterisation, which we collectively refer to as parameter blocks, will be scaled independently.\nFormally, denote a task vector with $m$ parameter blocks by $T = (\\tau^{(1)}, ..., \\tau^{(m)})$, where each parameter block $\\tau^{(i)}$ is vectorised, and round brackets denote column vector concatenation. We learn a block diagonal matrix $\\Lambda$, parameterised as\n$\\Lambda =\\begin{bmatrix}\\lambda^{(1)} I^{(1)} & : & 0 \\\\ : & \\ddots & :\\\\ 0 & : & \\lambda^{(m)} I^{(m)} \\end{bmatrix}$\nwhere $\\lambda^{(i)} \\in \\mathbb{R}$ is a learnable coefficient; $I^{(i)}$ denotes an identity matrix with its number of columns matching the dimension of $\\tau^{(i)}$; and the superscript $j \\in \\mathbb{N}^+$ indexes a parameter block. This results in anisotropic scaling of a task vector, that is,\n$\\Lambda_i T_i = (\\lambda_i^{(1)}\\tau_i^{(1)}, ..., \\lambda_i^{(m)}\\tau_i^{(m)}),$\nwhere the subscript $i \\in \\mathbb{N}^+$ indexes a task vector. As such, assuming a supervised objective, finding the optimal composition of task vectors can be defined as the following optimization problem\n$\\arg \\min_{\\Lambda_1, ..., \\Lambda_n} \\mathbb{E}_{(x,y)\\in D_t} [L(f(x; \\theta_0 + \\sum_{i=1}^n \\Lambda_i T_i), y)],$\nwhere $L$ is the loss function for a target task; $n$ is the number of task vectors; $y$ is the labels corresponding to inputs $x$; $D_t$ denotes a target dataset. The number of learnable parameters, as a result, is precisely $mn$, Let us denote the solution to the aforementioned optimization problem by $\\{\\hat{\\Lambda}\\}_{i=1}^n$. In inference, model $f (x, \\theta_0 + \\sum_{i=1}^n \\hat{\\Lambda}_i T_i)$ will be deployed, which incurs no additional computational cost compared to models trained in the conventional way."}, {"title": "Relation to intrinsic dimensionality", "content": "A notable characteristic of aTLAS is its parameter efficiency. To offer more intuitions, we refer to previous findings [1, 33] that deep neural networks often produce solutions residing in a subspace with much lower intrinsic dimensionality. This is measured by finding a minimum number of $d$ parameters, such that learning these parameters $(\\hat{\\theta} \\in \\mathbb{R}^d)$ leads to approximately the same performance as optimising in the full parameter space $(\\theta \\in \\mathbb{R}^D)$. This can be expressed as follows\n$\\theta = \\theta_0 + P\\hat{\\theta},$\nwhere $\\theta_0 \\in \\mathbb{R}^D$ denotes the pre-trained weights and $P\\in \\mathbb{R}^{D\\times d}$ is a random projection matrix. We demonstrate that learning task vector compositions leads to the same formulation. For brevity of exposition, let us consider compositions at the block level. For the $j$-th parameter block, we have\n$\\theta^{(j)} = \\theta_0^{(j)} + \\sum_{i=1}^n \\Lambda_i^{(j)} \\tau_i^{(j)} = \\theta_0^{(j)} + \\sum_{i=1}^n (\\lambda^{(j)} I^{(j)}) \\tau_i^{(j)} = \\theta_0^{(j)} + (\\sum_{i=1}^n \\tau_i^{(j)}) [\\lambda_1^{(j)}, ..., \\lambda_n^{(j)}]^{\\top},$\n$\\theta^{(j)} = \\theta_0^{(j)} + [\\sum_{i=1}^n \\tau_i^{(j)}] [\\lambda_1^{(j)}, ..., \\lambda_n^{(j)}]^{\\top}.$\nWe draw a parallel between Eqs. 6 and 8 and note that aTLAS explicitly exploits the low intrinsic dimensionality by learning a small set of coefficients. The number of task vectors, i.e., $n$, is much smaller than the dimension of weight vector $\\theta^{(j)}$, and is analogous to the intrinsic dimensionality $d$. However, as opposed to using a random projection matrix $P$, aTLAS constructs the projection matrix from task vectors, making use of the learned representations."}, {"title": "Task arithmetic", "content": "Task arithmetic [28] is comprised of a few tasks aimed at editing pre-trained models using task vectors. Following previous practice [28, 44], we conduct experiments under the settings of task negation and task addition on eight image classification datasets (details included in Appendix A)."}, {"title": "Task negation", "content": "Task negation aims to reduce undesired biases, characterised by the performance, on a target task, while maintaining performance on a control dataset, ImageNet [52] in this case. Denote the validation sets for the target and control tasks by $D_t$ and $D_c$, respectively. We perform a simultaneous gradient ascent on the target task and gradient descent on the control task, described as follows,\n$\\arg \\min_{\\Lambda_t} \\mathbb{E}_{(x,y)\\in D_t}[-L(f(x; \\theta_0 + \\Lambda_t T_t), y)] + \\mathbb{E}_{(x,y)\\in D_c}[L(f(x; \\theta_0 + \\Lambda_t T_t), y)],$\nwhere $T_t$ is the task vector for the target dataset, and cross-entropy loss is used. The learning objectives with linearised task vectors can be derived easily based on Eq. 5, and so are omitted.\nWe summarise the task negation results in Table 1, and show that our method significantly improves upon standard task vectors, while the improvement upon linear task vectors is less prominent. In particular, we observe that weights matrices tend to have much larger negative coefficients, as shown in Figure 3a. To investigate this, we instead only learn coefficients for the weight matrices, with zero coefficients on other parameter blocks, effectively reducing the number of learnable parameters by two thirds. With ViT-B/32 as the backbone, we observe an average accuracy of 20.14 (vs. 18.76) on target tasks and 61.23 (vs. 61.21) on the control task, which shows that weight matrices carry majority of the knowledge required for task negation."}, {"title": "Task addition", "content": "Task addition aims at producing a multi-task model using task vectors acquired from a range of datasets. We utilise task vectors from the eight image classification datasets, and learn the anisotropic"}, {"title": "Knowledge transfer in low-data regimes", "content": "Beyond model editing for task arithmetic, we explore the idea of transferring existing knowledge in task vectors to previously unseen tasks. To this end, we use the CLIP [47] model and a total of 22 image classification datasets, each of which produces a task vector. We defer the details of datasets and the process to acquire task vectors to Appendix A. Denote the set of available task vectors by $T = \\{T_i\\}_{i=1}^n$, and the dataset corresponding to task vector $T_i$ by $D_i$. For each target dataset $D_t$, we"}, {"title": "Few-shot adaptation", "content": "Few-shot recognition requires learning new objects or concepts using a limited amount labelled data-k per class for k-shot. Following previous practice [67], we approach this problem by adapting a pre-trained CLIP model [47] to each target dataset $D_t$. We use the subset of task vectors $T \\setminus \\{T_t\\}$ and $k \\in \\{1,2,4, 8, 16\\}$ images from dataset $D_t$. During training, we adopt the cross-entropy loss and minimise objectives described in Eqs. 4 and 5 for standard and linear task vectors, respectively.\nWe compare against Tip-Adapter [67] and LP++ [25] using CLIP with ViT-B/32 backbone, across 22 datasets over three random seeds, and summarise the results in Figure 5a. We show that with $k = 1$, our approach, aTLAS, significantly outperforms previous methods, demonstrating the effectiveness of knowledge transfer with scarce labelled data. More importantly, we note that the idea of task vector composition is highly complementary to those presented in previous methods. As such, combining aTLAS with them results in significant improvements. This is also illustrated in Figure 5b as a Venn diagram, where we show the percentage of examples in the validation set that are incorrectly classified by the pre-trained model but correctly classified with few-shot methods. Out of the examples aTLAS improves upon, around half are unique compared against either Tip-Adapter or LP++, demonstrating its complementarity. We also found that standard task vectors generally perform better than their linearised counterparts, and so defer the results of linear task vectors to Appendix D.2.\nIn addition, due to the low number of learnable parameters, aTLAS exhibits strong generalisability. To demonstrate this, we learn task vector composition on ImageNet [52], and test it on out-of-domain (OOD) datasets: ImageNet-A [22], ImageNet-R [21], ImageNet-sketch [60] and ImageNetV2 [50]. We summarise the results in Figure 5c, which shows the performance difference against the pre-trained model. Notably, aTLAS is the only method that consistently improves upon the pre-trained model on OOD datasets, and combining aTLAS with other methods can improve their generalisability.\nWe also test our method and variants integrated with Tip-Adapter and LP++ using other backbones, including ViT-\\{B/16, L/14\\} and ResNet-\\{50, 101\\}, and find that the results are consistent with those for ViT-B/32. More details can be found in Appendix D.3."}, {"title": "Task vector budget and selection", "content": "In practical applications, there may only be a limited number of task vectors available, or the number of task vectors used in training may be restricted due to memory constraints. To this end, we study the influence of task vector budget $b$ on few-shot recognition performance. We experiment with four selection strategies: (1) random selection; (2) feature-based selection; (3) gradient-based selection; and (4) blockwise gradient-based selection. To elaborate, feature-based selection computes the mean image feature representation of each dataset, and selects $b$ task vectors from datasets most similar"}, {"title": "Test-time adaptation", "content": "Test-time adaptation (TTA) [35, 57, 59] assumes no labelled data is available for the target task, requiring the model to adapt in an unsupervised fashion. We conduct experiments under the offline adaptation setting, which allows access to the target dataset. We consider three categories of self-supervised techniques for TTA: constrastive objectives, entropy objectives and pseudo labelling. Contrastive objectives align representations of the same image under different data augmentations. For this category, we adopt SimCLR [9], a simple yet effective method. Entropy objectives encourage the pre-trained model to produce confident predictions on unseen datasets by minimising the entropy over the predictions. While effective in simpler cases, it can lead to catastrophic collapse on complex tasks. Therefore, we utilise a state-of-the-art sharpness-aware entropy minimisation algorithm named SAR [43]. Last, we experiment with an unsupervised pseudo-labelling algorithm inspired by FixMatch [54], which we refer as unsupervised FixMatch (UFM). UFM selects an equal number of highly confident examples per class as the labelled set, and then employs FixMatch to produce pseudo-labels from rest of the unlabelled examples. Details are available in Appendix E.\nWe summarise the results in Table 3 and compare our method, i.e., learning task vector compositions, against the conventional approach of tuning the layer normalisation parameters [43, 57, 59]. We show that under all self-supervised objectives, aTLAS achieves higher accuracy than tuning the LayerNorm. In particular, LayerNorm has 30k learnable parameters with ViT-B/32 while our method only has 3.5k learnable parameters. We note that with the UFM objective, aTLAS performs the best and improves the accuracy by an average of 6.5 absolute points over the zero-shot baseline."}, {"title": "Relation to parameter-efficient fine-tuning", "content": "One of the key advantages of aTLAS is its ability to adapt pre-trained models with few learnable parameters, making it suitable for parameter-efficient fine-tuning (PEFT). Similar to popular PEFT methods such as low-rank adaptation (LoRA) [23], our approach does not introduce additional modules, thereby avoiding an increase in inference complexity. In addition, since only the encoded weight matrices in LoRAs have non-zero weight difference, LoRAs are in fact sparse task vectors. They can thus be seamlessly integrated into our method, significantly reducing the memory cost."}, {"title": "LoRAS as task vectors", "content": "Due to the sparsity and rank deficiency, LoRAs as task vectors may have limited representation capacity and carry less knowledge. Therefore, they may be inferior to standard task vectors for knowledge transfer. We investigate this by learning linear combinations of LoRAs\u2074 using our method, under the settings of few-shot recognition. Results are summarised in Table 4. We first shed light on the impact of sparsity, and compare two variants of our method that either learns linear combinations of all parameter blocks or just the weight matrices. Results show that sparsity results in an accuracy decrease of around 0.5% on average, except for the one-shot setting. The rank deficiency, on the other hand, causes more substantial accuracy drop. Nevertheless, this can be largely mitigated by increasing the rank. Using a rank of 64 leads to similar performance compared to learning compositions of only weight matrices in standard task vectors. In conclusion, while the sparsity and rank deficiency introduce some performance drops, especially in low-shot settings, LoRAs are competitive alternatives to standard task vectors due to their low memory cost."}, {"title": "Scalability of aTLAS", "content": "Despite the parameter efficiency of aTLAS, its performance is not as competitive when sufficient training data is available. To address this, we devise a strategy to flexibly scale up the number of learnable parameters as needed. Specifically, we randomly divide each parameter block into $K$ partitions, and assign a learnable coefficient to each partition, naturally increasing the number of learnable parameters by $K$-fold. We denote these variants by aTLAS \u00d7K. We conduct experiments with these vari- ants using \\{1, 5, 10, 25, 35, 50, 100\\}\\% of the to- tal available training data across the 22 datasets used in Section 5."}, {"title": "Related work", "content": "Task vectors and model compositions. Recent studies have demonstrated the possibility of manipulating the behaviours of neural networks directly in the weight space [27, 62, 64]. In particular, task vectors [28], as a carrier of the domain-specific knowledge learned through fine-tuning, exhibit strong compositional properties. Such compositionality can be enhanced via linearisation using first-order Taylor expansion [44], and improves model editing with simple arithmetic, e.g., addition, negation, etc. Low-rank adaptations [23], as special forms of task vectors, were shown to also support such arithmetic operations. A recent study [3] also investigated the idea of learning combinations of LoRAs for few-shot recognition.\nModel-based transfer learning. One interpretation of transfer learning [71] is to exploit the knowledge encapsulated in a pre-trained model for a target domain. Amongst various sub-modules of a pre-trained model, transferring the feature extractor is the most extensively studied. This ranges from early convolutional neural networks [18, 53] to modern transformers [58], from vision backbones [14, 37] to language models [13, 46]. For vision applications, classification models trained on ImageNet [52] have been used as the medium for knowledge transfer. In recent years, contrastively pre-trained multi-modal models such as CLIP [47] have emerged as a prevelant choice. Such models are trained on large volumes of data by aligning image and language representations, leading to strong baselines well suited for transfer learning. CLIP representations have since been use for medical imaging [68], semantic segmentation [70], satellite imaging [40], etc.\nModel adaptation in low-data regimes. The performance of pre-trained models is often con- strained when applied to specific tasks with limited labelled data. To address this limitation, extensive research has been conducted on few-shot adaptation of CLIP [47]. These studies focus on var- ious techniques, including prompt engineering [69], feature adaptation [16], and more recently classifier adaptation [25, 67]. In addition to few-shot adaptation, test-time adaptation represents an even more challenging scenario where no annotated data is available. This typically requires leveraging self-supervised objectives to adapt the model, employing methods such as entropy minimi- sation [35, 43, 59], contrastive learning [8], pseudo labelling [35] and image rotation prediction [57]."}, {"title": "Conclusion", "content": "In this paper, we introduced aTLAS, a learning algorithm that leverages the rich knowledge en- capsulated in task vectors through learned linear combinations with anisotropic scaling. Unlike conventional methods that learn network parameters, our approach focuses on learning coefficients on task vectors, significantly reducing the number of learnable parameters. We conducted experiments across task arithmetic, few-shot recognition, test-time adaptation and parameter-efficient fine-tuning, demonstrating the effectiveness of our method with supervised and unsupervised objectives. In par- ticular, we highlighted several properties of aTLAS, including low disentanglement error, robustness against domain shift, effectiveness in low-data regimes, complementarity with existing few-shot methods, etc. These properties paved the way for efficient knowledge composition and transfer.\nLimitations. As a task vector is defined with respect to a specific pre-trained model, knowledge composition and transfer are not yet feasible across different architectures. This may become possible with suitable projections and remains part of the future work. In addition, combining large numbers of task vectors can consumes a substantial amount of GPU memory when training larger models. This can be mitigated by selecting a subset of task vectors, using LoRAs as task vectors or by offloading the computation of task vector composition to CPU, at the cost of training speed decrease. It is also possible to perform task vector composition at bit-width lower than floating point precision, e.g., 4-bit. Similar features are being tested with popular deep learning frameworks such as PyTorch, and we expect the memory requirement of larger models to be less of a constraint in the future."}, {"title": "Datasets and task vectors", "content": "We acquire task vectors by fine-tuning CLIP [47] on a variety of 22 image recognition datasets: (1) Stanford Cars [30], (2) DTD [11], (3) EuroSAT [20], (4) GTSRB [56], (5) MNIST [32], (6) RESISC45 [10], (7) SUN397 [63], (8) SVHN [41], (9) CIFAR10 [31], (10) CIFAR100 [31], (11) ImageNet [52], (12) STL10 [12], (13) Food101 [5], (14) Caltech101 [34], (15) Caltech256 [17], (16) FGVCAircraft [39], (17) Flowers102 [42], (18) Oxford Pets [45], (19) CUB200 [61], (20) PascalVOC [15], (21) Country211 [47], and (22) UCF101 [55]. Fine-tuning was conducted using AdamW optimiser [38], with a learning rate of 10-5, batch size of 128 and weight decay of 0.1. Details of the datasets, additional dataset-specific hyper-parameters, and the accuracy after fine-tuning for an assortment of backbones are shown in Table 5. We use the same hyper-parameters for the linearised variants of the model.\nTo shed light on the semantic relationships amongst datasets, we extract the features of all images for each dataset, and visualise the distributions as ellipses (Figure 8). Specifically, for each dataset, the mean \u03bc\u2081 \u2208 Rd and covariance \u03a3t \u2208 Rd\u00d7d of image features are computed. Principal component analysis (PCA) is used produce a projection matrix P \u2208 Rd\u00d72 from the mean features \u03bc\u2081. Sub- sequently, the mean and covariance with reduced dimensionality can be expressed as PT\u03bc\u2081 and PTP\u03a3tP, respectively."}, {"title": "Task negation", "content": "The evaluation of task negation is conducted on eight classification datasets (1-8 in Table 5), following previous practice [28, 44]. In particular, we learn anisotropic scaling using the validation set of each dataset. We also adjust the learning rates and training epochs on the same validation set. The details are shown in Table 6. We report detailed task negation results for each dataset in Table 7. In addition, for more evidence that weight matrices learn large negative coefficients, we show a detailed visualisation of the learned coefficients in Figure 9 and distribution of the coefficients in Figure 10."}, {"title": "Task addition", "content": "Task addition is also evaluated on datasets 1\u20138 shown in Table 5. The hyper-parameters are identical to fine-tuning, except the learning rate is modified to 10-3. We show detailed performance on each dataset in Table 8, where we compare our method against hyper-parameter search used in previous works [28, 44], and another variant with learned isotropic scaling. We also visualise the learned coefficients with L\u2081 regularisation in Figure 12. It can be easily observed that weight matrices, particularly those in the deeper layers, have significantly higher learned coefficients, which conforms to our observations in Figures 3b and 3c."}, {"title": "Comparison against full-parameter optimization", "content": "Since our method involves learning the coeffi- cients, unlike previous methods [28, 44] that only require a hyper-parameter search, we also compare against the direct fine-tuning approach. We fine-tune the pre-trained model on the union of eight datasets, assuming only the validation sets are available. The results are shown in Fig- ure 11. Unsurprisingly, task vector composi- tions, whether the coefficients are searched or learned, are less susceptible to the lack of data, as the accuracy only starts to drop with less than 35% of the data. The performance of full- parameter fine-tuning, however, drops substan- tially as the amount of data available decreases."}, {"title": "Disentanglement error", "content": "In addition, we provide more technical details and intuitions on the pairwise disentanglement error [44], which was visualised in Figure 4. Specifically, we make a few changes to the formulation proposed by Ortiz-Jim\u00e9nez et al. [44], and evaluate the disentanglement error only with the optimal"}, {"title": "Few-shot learning", "content": "Two variants of Tip-Adapter [67] were proposed for few-shot recognition where the weights of the adaptor are either fixed based on features of the few-shot examples or further fine-tuned. We only study the fine-tuned variant due to its higher performance. Tip-Adapter has two hyper-parameters, which in the original paper are optimised through hyper-parameter search on a separate validation set. This practice may not align with the principles of few-shot learning, where access to extensive validation data is typically limited. In addition, Huang et al. [25] note that the performance of Tip- Adapter is very sensitive to these hyper-parameters. We thus opt to learn these two hyper-parameters together with the feature adaptor through gradient descent. The learning rates for the feature adaptor and the hyper-parameters are set to 10\u20133 and 10\u20131, respectively.\nFor both Tip-Adapter and LP++ [25], we conduct experiments using the publicly available codebase \u2075. We train both LP++ and Tip-Adapter for 300 epochs on frozen zero-shot features. We apply a cosine annealing decay for Tip-Adapter and maintain fixed learning rates for LP++ as per the official implementation."}, {"title": "linearised task vectors", "content": "We report the average few-shot accuracy over the 22 datasets in Table 9, which corresponds to results in Figure 5a. In particular, we show results with linearised task vectors, as proposed by Ortiz-Jim\u00e9nez et al. [44]. As highlighted in Section 4, learned anisotropic scaling allows standard task vectors to achieve stronger performance than the linear variants in task addition. For few-shot recognition, we again observe that standard task vectors result in superior performance in most cases. We, however, note the exception that linear task vectors when combined with LP++ achieve higher performance in the 1-shot setting. Nevertheless, the margin over standard task vectors is not very significant, and aTLAS using standard task vectors when integrated with Tip-Adapter is generally a stronger few-shot model."}, {"title": "Integrating state-of-the-art methods into aTLAS", "content": "We use the AdamW [38] optimiser with a learning rate of 10-1 and a weight decay of 10-1. Our method by itself is trained for 10 epochs with ViT backbones and 30 epochs with ResNet backbones.\nWe show that state-of-the-art few-shot methods can be seamlessly integrated into our method, since both Tip-Adapter and LP++ focus on the classifier, while aTLAS improves the feature representations. We experiment with two strategies to combine aTLAS with previous methods, where we either (1) train our method first and use the frozen representations to train a previous method, or (2) train parameters in both methods jointly. Results in Table 10 shows that the joint training strategy results in higher performance, particularly in low-shot settings. We therefore adopt the joint training strategy when combing our method with Tip-Adapter. During training, we adopt different learning rates for different parameter groups, that is, 10\u00af\u00b9 for learnable coefficients in aTLAS and the hyper-parameters in Tip-Adapter, and 10-3 for the adaptor."}, {"title": "Task vector budget and selection", "content": "In this section, we provide details for selecting a budget of b task vectors with feature-based and gradient-based strategies, as introduced in Section 5.2.\nFeature based selection. For each dataset Di, we compute the average image representation zi of the dataset using the zero-shot model as follows\nzi = \\mathbb{E}_{x\\in D}[f(x; \\theta_0)].\nGiven a target dataset Dt, we simply compute the cosine similarity between its feature representation zt and that of each other dataset zi, i \u2260 t. Subsequently, b task vectors corresponding to the datasets with highest similarity will be selected.\nGradient-based selection.\nGiven a target dataset Dt, we may directly compute the gradient with respect to the m learnable coefficients for each of the n task vectors. However, as one important motivation behind task vector selection is to reduce memory consumption, using all n task vectors to compute the gradient defeats the purpose. Therefore, we instead only load a group of b task vectors (b < n), compute the gradient with respect to their learnable coefficients, and repeat for other groups. With this sequential computation, the gradient across different groups is not calibrated. Nevertheless, we empirically found this strategy to work well. Denote the partial derivative of the loss on dataset Dt with respective to a learnable coefficient $\\lambda^{(j)}$ by $\\frac{\\partial L}{\\partial \\lambda_i^{(j)}}$ such that\n$\\frac{\\partial L}{\\partial \\lambda_i^{(j)}} = \\mathbb{E}_{(x,y)\\in D_t} [\\frac{\\partial L(f(x; \\theta_0 + \\sum_{i=1}^n \\Lambda_i T_i), y)}{\\partial \\lambda_i^{(j)}}]$.\nFor the i-th task vector, we may compute its L\u2081 gradient norm, i.e., $\\|\\frac{\\partial L}{\\partial \\lambda_i^{(1)}}, ..., \\frac{\\partial L}{\\partial \\lambda_i^{(m)}}\\rVert_1$ and select task vectors with larger gradient. Alternatively, we may select task vectors block by block. Specifically, for the j-th parameter block, we inspect the absolute values of the partial derivatives for the corresponding coefficients, i.e., $|\\frac{\\partial L}{\\partial \\lambda_i^{(j)}}|$ and select task vectors with higher absolute values. This process is repeated for each parameter block, thus allowing different parameter blocks to have different selections. Crucially, for low budgets, particularly b = 1, this enables our method to effectively exploit more task vectors than the budget specifies. The impact of this can be observed in Table 14 (corresponding to Figure 6), that blockwise selection significantly outperforms other methods when the budget is low."}, {"title": "LoRAS as task vectors", "content": "We fine-tune LoRAs for ViT-B/32 using the LoRA-Torch [36] library with ranks 4, 16 and 64. We stop at rank 64 as we do not observe improvements beyond it. We train LoRAs on attention and MLP layers and use the same settings as for full finetuning but with a learning rate of 10-3.\nTable 15 shows additional results using LoRAs as task vectors. We study learning the effect of fine-tuning the LoRAs task vectors on attention layers only (as done in the original LoRA paper [23]) or on the MLPs. Although the original LoRA paper recommendeds training on the attention layers only [23], we observe that training on MLP layers is important to produce strong LoRA task vectors."}, {"title": "Unsupervised FixMatch", "content": "We provide more details on the Unsupervised FixMatch (UFM) approach in this section. Fix- Match [54"}]}