{"title": "AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation", "authors": ["Jiaqi Ma", "Guo-Sen Xie", "Fang Zhao", "Zechao Li"], "abstract": "Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5 and COCO-20\u00b0 datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at https://github.com/jarch-ma/AFANet.", "sections": [{"title": "I. INTRODUCTION", "content": "VER the past decade, deep learning methods have achieved remarkable performance in image segmentation tasks [58], [59], [60], [61]. However, most of these methods require a large amount of data for training, and the process of collecting data and manual annotation is time-consuming and costly. Previous statistics have shown that for a single 1280\u00d7720 pixel image, the time required for annotation is approximately 1.5 hours [1]. The cost of annotating massive datasets has become increasingly difficult to bear, particularly in cases where domain expertise is required, highlighting the issue of data scarcity. To address these challenges, weakly-supervised semantic segmentation (WSSS) and few-shot semantic segmentation (FSS) have been proposed and play a significant role in various fields such as medical image analysis [2], autonomous driving [3], and military target recognition [4].\nWSSS aims to provide pixel-level predictions for images and can be considered as a visually intensive task. There are four common weakly-supervised annotation methods: (1) Image-level labeling, providing only the category information of the image; (2) Bounding box labeling, offering rough positional information of objects; (3) Scribble labeling, marking the approximate object location with simple lines; (4) Point labeling, indicating key object positions with one or more points. Although image-level labeling does not provide object positional information, it offers the highest accessibility and lowest annotation cost, thus becoming the most prevalent standard setting in weakly-supervised semantic segmentation tasks. By contrast, FSS aims to predict various objects using a small number of samples, which is more in line with the characteristics of difficult-to-obtain and long-tail distributed data in realistic open scenarios. FSS typically consists of two stages: (1) The meta-training stage, where the model is trained on a base dataset with disjoint classes from the test stage; (2) The meta-testing stage, where the model can quickly adapt and generalize to few-shot tasks for unseen categories, thus segmenting novel objects from the new class. However, the annotations for the base dataset under FSS setting are still labor-intensive and costly.\nIn this work, we replace the traditional pixel-level labels in FSS task with more challenging image-level labels, namely weakly-supervised few-shot semantic segmentation (WFSS). Similar to the majority of existing WSSS or FSS network models, current research on WFSS primarily focuses on obtaining more precise seed class activation maps (CAM), also known as pseudo masks, or refining the object boundaries for clearer outlines [53], [54], [55], [56], [57]. However, the intrinsic nature of WFSS tasks indicates that base-class data can only provide the network with limited (few-shot) and meager (image-level label) semantic information. Therefore, the key to solving WFSS tasks lies in how to obtain more support information within the aforementioned limited constraints. Recently, Chen et al. [17], [18] demonstrate that features in networks can further decouple frequency domain distribution information. Motivated by this, we resort to the frequency domain during meta-training to pursue a desirable semantic transformation and propose an adaptive frequency-aware network (AFANet) for WFSS.\nAs shown in Fig. 1(a), current network models often rely on the RGB color domain of images to localize and predict objects by providing information such as color, texture, and surface features. However, this representation method still fails to meet the requirements of WFSS tasks. Natural images can further be decomposed into spatial frequency components (Fig. 1(b)) [31], [32]. That is, low-frequency distribution information represents slow or smooth changes in the image, and high-frequency distribution information represents rapid changes or image details. The former can provide global features and overall structural information of the image, while the latter can provide local details and minor feature information. Chen et al. [18] further demonstrate that the output feature maps of convolutional layers can also be viewed as a mixture of different frequency information, and these mixed feature maps can be decoupled through octave convolutions. Hence, we propose a cross-granularity frequency-aware module (CFM) to decouple the cross-granularity information features in the pyramid network, and further optimize the semantic spatial structural information through realignment. By realigning and optimizing semantic spatial structural information through CFM, AFANet effectively addresses the limitations of current network models in WFSS tasks, providing unparalleled detail and accuracy in object prediction and localization. For more details, see the ablation study Robustness Analysis of AFANet.\nIn addition, recognizing the limitations of current network models in meeting the requirements of WFSS tasks, researchers have explored alternative approaches. One promising option is leveraging cross-modal models such as CLIP (Contrastive Language-Image Pretraining) [12]. Unlike traditional models that rely on RGB color domain information, CLIP learns from a vast number of image-text pairs available on the web through joint learning. This model not only captures intricate visual details but also understands textual context, making it particularly adept at understanding and interpreting complex visual scenes. However, current research on CLIP mostly focuses on optimizing prompts or generating seed CAM through offline learning [36], [37]. Despite CLIP's demonstrated strong zero-shot capability in identifying unseen classes through extensive experiments, the rough utilization of offline learning, relying solely on prior knowledge, still exhibits significant discrepancies with the data distribution of downstream tasks [33], [34], [35].\nMotivated by this, we further propose a CLIP-guided spatial-adapter module (CSM). CSM can perform spatial domain adaptive transformations on CLIP's cross-modal textual information based on the semantic distribution characteristics of downstream tasks. Through such fine-tuning and adaptation, CSM can effectively address the disparity between prior knowledge and the data distribution of downstream tasks. To the best of our knowledge, this is the first time that CLIP engages in collaborative updates with downstream task network models in an online learning manner. This learning approach significantly enhances the model's adaptability to new tasks, achieving a tight integration between the model and the task, thereby achieving superior performance in practical applications.\nTo sum up, our contributions are: (1) We propose an Adaptive frequency-aware network (AFANet), which for the first time incorporates the frequency-domain distribution information of images into the WFSS task. Through the cross-granularity frequency-aware module, we demonstrate the effectiveness of providing frequency information other than RGB for WFSS. (2) We propose a CLIP-guided spatial-adapter module, which enables spatial domain adaptive transformations on CLIP's cross-modal textual information to adapt to the data distribution of downstream tasks. To the best of our knowledge, this is the first time that CLIP engages in collaborative updates with downstream task network models in an online learning manner. (3) Through extensive experiments and fair comparisons, AFANet has achieved state-of-the-art performance on the Pascal-5i and COCO-20\u00b9 datasets.\nThe remainder of this paper is organized as follows. In Section II, we show some related works. In Section III, we illustrate the proposed framework elaborately. Section IV reports the experimental results and presents the ablation studies. In Section V, we conclude this paper."}, {"title": "II. RELATED WORKS", "content": "Few-shot learning aims to recognize novel classes using a limited number of examples, which are disjoint from the known classes [62], [63]. It is a fundamental paradigm for evaluating the data efficiency and adaptability of learning algorithms and has broad applications in the field of computer vision. Most existing few-shot learning methods can be categorized into transfer learning or meta-learning. Transfer learning typically involves pre-training models on diverse datasets and then fine-tuning them on tasks with limited data [64], [65]. The core idea of meta-learning is to acquire task-agnostic meta-knowledge from a large number of similar tasks and then utilize it to provide auxiliary guidance for few-shot tasks [66], [67], [68]. Meta-learning can be further divided into gradient-based methods and metric-based methods. Metric-based methods treat the metric space or metric strategy as meta-knowledge, typically used for learning image embeddings and comparing distances between them to derive highly correlated prototypes [69]. Gradient-based methods consider the gradient optimization algorithm as meta-knowledge, enabling the model to quickly adapt to the characteristics of few-shot data distributions by updating hyperparameters [70], loss functions [71], and other parameters."}, {"title": "B. Few-shot Semantic Segmentation", "content": "Due to the necessity of extensive training data in traditional semantic segmentation methods [73], [74], [75], [76], the task of few-shot semantic segmentation has been proposed. This task aims to generate dense predictions for query images using a few annotated samples through meta-learning. Few-shot semantic segmentation methods are based on metric-based meta-learning and can be further divided into prototype-based guidance methods [77], [78] or prior-knowledge-based guidance methods [79], [80], [81]. Prototype-based methods typically extract global [82] or local features [83] from images and then use multi-scale feature fusion [84] or cosine similarity [85] to obtain semantic prototypes, which further guide the model in segmenting query images. Although prototype-based methods provide significant support for the model's general semantics, the prototype information obtained is ultimately limited due to the constraints of few-shot data. Consequently, relying solely on prototype-based methods may lead to reduced robustness of the segmentation results. To address this issue, methods based on prior knowledge have also been widely explored. This method can be further grouped into prior methods based on traditional knowledge or those based on large-language models. Prior methods based on traditional knowledge typically integrate inherent classifier knowledge [86] or further extract semantic embeddings [87] from the support and query sets, guiding the segmentation by either leveraging or suppressing the base class model knowledge [88], [89]. Prior methods based on large language models benefit from their powerful zero-shot semantic generalization capabilities, such as using CLIP or SAM to provide the model with extensive semantic prior information for guidance [81], [90], [91]. This method has significantly advanced the research progress in few-shot semantic segmentation and has also become a major area of focus in recent years."}, {"title": "C. Weakly-Supervised Few-shot Segmentation", "content": "Weakly-supervised few-shot semantic segmentation (WFSS) is closer to the practical scenarios in the real world, but the dual constraints of few-shot and weak supervision pose more severe challenges. There is only a small amount of relevant research. PANet and CANet [5], [6] have explored combining few-shot learning with weakly-supervised annotation methods. They replaced pixel-level labels with bounding boxes, but the model performance fell short of meeting the task requirements. Raza et al. [7] proposed the first WFSS model using image-level labels, but this model only uses such labels during the testing phase, thus the WFSS task setting is not thorough. Siam et al. [8] introduce multi-modal interaction information with word embeddings, reducing distribution discrepancy between visual and textual modalities by jointly guiding network attention with the visual module. Lee et al. [9] generate CAM for each training class and determine their similarity by measuring the distance between the associated label's word embeddings. However, such word embedding space may lead to ineffective discrimination due to its lack of accuracy. Recently, IMR-HSNet [10] has introduced the visual-language model CLIP to generate CAM as pseudo-masks for both support and query, refining masks through iterative interactions between each other. However, IMR-HSNet also relies on RGB domain information provided by images, thus having limited room for improvement under the constraints of few-shot and weak supervision. In contrast, our AFANet can overcome these limitations by offering new semantic support through spatial optimization in the frequency domain distribution."}, {"title": "D. Contrastive Language-Image Pre-training (CLIP)", "content": "Inspired by human cross-modal cognition [11], cross-modal learning aims to leverage data from other modalities to improve single-modal tasks. Recently, the vision-language pretraining model CLIP [12] has been extensively trained on vision-language annotated pairs. By learning broad visual semantic concepts, CLIP has shown immense potential in zero-shot tasks and has become a milestone in the field of vision-language pretraining models [38], [39]. DenseCLIP [13] first introduced CLIP into visually dense tasks, performing zero-shot object localization and segmentation in a context-aware manner. Besides, numerous works on optimizing CLIP prompts have been widely applied across various research fields. For example, researchers utilize the large-scale natural language model GPT-4 to optimize the category granularity of CLIP prompts [14], while CLIP-ES [15] focuses on enhancing prompts for image foreground and background. However, these studies primarily provide prior information support through offline learning and may not fully adapt to downstream sub-tasks. To address these issues, in this work, we propose the CSM module, which collaboratively updates the downstream task network in an online learning manner, enhancing CLIP's adaptability to new tasks and achieving superior performance in practical applications."}, {"title": "III. METHODOLOGY", "content": "For a conventional 1-way K-shot few-shot semantic segmentation task, the common approach is to adopt a meta-learning paradigm. This involves setting up a base dataset Dbase and a novel dataset Dnovel, where Dbase \u2229 Dnovel = \u2205. Dbase contains a set of seen classes Ctrain used for training, while Dnovel contains unseen classes used to evaluate the model's generalization ability to unseen categories. During training, Dbase is often further partitioned into support set S and query set Q, and multiple episodes are set up for periodic training using the metric learning method. It can be defined as\n$S = \\{(I_s,M_s)\\}, I_s^K\\_{k=1}, Q = \\{(I_q, M_q)\\},$ (1)\nwhere, $I_s \\in R^{C\\times H\\times W}$ and $M_s\\in R^{H\\times W}$ denote the input images and their corresponding binary masks, respectively. In the more challenging WFSS task, binary masks are replaced by image-level labels:\n$S = \\{(I_s,C_s)\\}, Q = \\{(I_q, C_q)\\}, I_s^K\\_{k=1},$ (2)\nwhere, $C_s$ and $C_q$ represent the class labels for the support and query respectively.\nHowever, image labels cannot be directly recognized by the segmentation network for model training. Therefore, it is necessary to further generate the CAM as pseudo-mask $M_s$ and $M_q$ corresponding to support S and query Q. For $M_s$ , the CAM is computed by weighted summing the feature maps $W$ of an image $I$ with embedding class weights $\\theta$ obtained from the CLIP text encoder for class $C_s$. The relu function and max normalization are then applied to remove negative activations and scale the CAM to the range [0, 1]. It can be calculated as\n$M_k = \\frac{relu(\\theta^T W)}{max(relu(\\theta^T W))}$ (3)\nDuring meta-testing, the trained model $f(*|\\theta)$ predicts the query mask $M_q$ using the given support images $I_s$ and the CAM $M_s$ under Dnovel. Thus, the prediction is calculated as\n$M_q = f((\\{I_s, M_s\\})^K\\_{k=1}, I_q | \\theta)$ (4)\nwhere, $f(*|\\theta)$ is the trained model."}, {"title": "B. Overview", "content": "AFANet (Fig. 2) aims to introduce frequency-domain distribution and cross-modal text information adaptation to provide more semantic support for weakly-supervised few-shot semantic segmentation models. To extract more visual semantic information within limited constraints, the cross-granularity frequency-aware module (CFM) decouples the input RGB images Is and Iq into high-frequency and low-frequency distributions. It then optimizes semantic structural information through realignment. Subsequently, the frequency-domain information guides CLIP, adapting the network model to cross-modal text information through online learning. Finally, the output features from CLIP-guided spatial-adapter module (CSM) are fed into a conventional segmentation network to achieve more accurate segmentation results. Detailed descriptions of each module of AFANet are provided in the following sections."}, {"title": "C. Cross-Granularity Frequency-Aware Module (CFM)", "content": "Compared to traditional RGB information, frequency domain information can provide more comprehensive information support. This viewpoint has also been validated in camouflage target detection tasks [16], [17]. As shown in Fig. 1, a common RGB image can also be decomposed into distributions of high-frequency and low-frequency. High-frequency information can reflect rapid changes in the image, such as foreground and background contours, thereby enhancing the global information of the image [41], [42]. On the other hand, low-frequency information changes more slowly and is coarser, typically appearing in the main parts of the image [43], [44]. In this work, we employ octave convolution [18] to decompose the output features of convolutional layers into high-frequency and low-frequency distributions, providing neural networks with more comprehensive information support.\nSpecifically, as shown in Fig. 2, given the input image support $I_s \\in R^{3\\times H\\times W}$ and query $I_q \\in R^{3\\times H\\times W}$ , we first extract the features at different layers from the pyramid network [19], including low-level (layer 3), mid-level (layer 9), and high-level (layer 12) features. The combination of these features at different granularities enables a more comprehensive representation of both local details and global information in the image. Subsequently, as depicted in Fig. 3, the frequency-aware module (FAM) decouples these features at different granularities into high-frequency features $F^H$ and low-frequency features $F^L$ using octave convolution, and optimizes the frequency-domain structural information by realigning them. This process can be described as:\n$Y^H\\_{s,q} = F(I_s,q; W^{H\\rightarrow H}) + Upsample(F(I_q; W^{L\\rightarrow H}), 2),$ (5)\n$Y^L\\_{s,q} = F(I_s,q; W^{L\\rightarrow L}) + F(pool(I^H\\_q, 2); W^{H\\rightarrow L}),$ (6)\nwhere, $F(I_{s,q}; W)$ represents the octave convolution of input images Is,q and learnable parameters W, H \u2192 H indicate the preservation of high-frequency features, while L \u2192 H represents the process of converting low-frequency to high-frequency, and vice versa. Besides, pool(IH, 2) is an average pooling operation with a kernel size of 2 \u00d7 2, and upsample(*, 2) is an upsampling operation using bilinear interpolation [45] with a dilation rate of 2. At this point, the output feature Ws,q from the convolutional layer has been decoupled into FH and FL. Chen et al. [18] argue that the low-frequency feature contains redundant information and thus discards it. Here, we empirically believe that the original convolutional feature Ws,q exhibits partial overlap in spatial distribution between FH and FL, leading to information redundancy or misguidance. Therefore, simply realigning it can optimize the structural information in the frequency domain (see ablation study Table III for more details). As follows:\n$F_{s,q} = Resize(Y^H\\_{s,q}\\oplus Y^L\\_{s,q}),$ (7)\nwhere, the resize adjusts $Y^H\\_{s,q}$ and $Y^L\\_{s,q}$ to a unified dimension, and $\\oplus$ representing element-wise addition.\nAs illustrated in Fig. 3, after passing through the FAM, we obtain frequency domain information from different network layers. However, due to the mixed characteristics of semantic information across cross-granularity and cross-frequency domains at this stage, simple linear addition cannot fully leverage the advantages of multi-angle semantic guidance. To address this issue, we utilize the neighbor connection decoder (NCD) [19] to establish contextual correlations between frequency domain features across different layers. The process is as follows:\n$f_{1,1} = x_1 g\\uparrow (x_2)$,\n$f_{1,2} = x_2 g\\uparrow (x_3)$,\n$f_{2,1} = g\\uparrow (f_{1,1}) \\otimes g\\uparrow (f_{1,2})$,\n$f_{2,2} = cat(f_{1,2}, g\\uparrow (x_3))$,\n$f_{3} = cat(g\\uparrow (f_{2,1}), f_{2,2}),$ (8)\nwhere, x1 is the low-level frequency domain feature, $x_1 = F_{low}$, x2 is the mid-level frequency domain feature, $x_2 = F_{mid}$, x3 is the hig-level frequency domain feature, $x_3 = F_{hig}$. $g\\uparrow (*)$ is an upsampling operation used to maintain feature dimension consistency. $\\otimes$ represents element-wise multiplication."}, {"title": "D. CLIP-Guided Spatial-Adapter Module (CSM)", "content": "CLIP, a large-scale vision-language model, has been trained on a dataset of over 400 million image-text pairs, enabling it to learn a broad range of visual semantic concepts and demonstrating significant potential in zero-shot classification tasks. Currently, CLIP is widely applied to downstream tasks such as image classification [46], object detection [47], and semantic segmentation [48], [49], [50], [51], [52]. However, these methods utilize CLIP's prior knowledge in an offline learning manner and may not fully adapt to downstream subtasks. In this work, we use frequency domain information as guidance and employ an online learning approach to adapt CLIP's textual prior knowledge to downstream tasks through spatial adaptation.\nHowever, the text information $t_h \\in R^{C_Nxhw}$ of CLIP's text encoder cannot directly adapt to the downstream network. Where, $C_N = N$ represents the number of categories, either Pascal-5 (N=20) or COCO-20 (N=80). Moreover, hw = 1024 represents the length of the feature vector $t_h$, we utilize a linear layer to reduce its dimensionality,\n$t_h = Linear(t, hw),$ (9)\nwhere, hw = 625. Then, we resize $t_h$ to the same size as the frequency domain information, denoted as\n$f^q\\_{s,q} = Resize(t),$ (10)\nwhere, $f^q\\_{s,q}$ represents the CLIP textual feature vector aligned with the frequency domain feature dimensions, $f^q\\_{s,q}\\in R^{C_Nxhxwh = w = 25}$.\nWith that, we can proceed to match the frequency domain information $f_{s,q}$ and CLIP prior knowledge $f^q\\_{s,q}$. As mentioned above, CLIP is pretrained on large-scale datasets with comprehensive categories, while the frequency domain features are extracted from smaller-scale datasets like Pascal-5\u00b9 dataset or COCO-20 dataset. This results in significant differences in feature distributions between them. Therefore, as shown in Fig. 2, we project $f_{s,q}$ into the spatial domain, utilizing bilinear interpolation to downsample its feature sizes from 50 \u00d7 50 to 25 x 25.\n$f^{s,q} = Proj\\downarrow (f_{s,q}),$ (11)\nwhere, $f^{s,q}$ represents the frequency domain features after downsampling, and Proj\u2193 is the bilinear interpolation with a scaling factor of 0.5. This is because a smaller feature size implies a larger receptive field, achieved by removing redundant features, which further facilitates aligning the spatial feature distributions of $f^{s,q}$ and $f^q\\_{s,q}$.\nOnce the spatial feature distributions of them are aligned, we apply element-wise multiplication to enhance the feature representations. Then, we use bilinear interpolation again, but upsampling to increase the feature size from 25\u00d725 to 50\u00d750.\n$f^\\_={Resize(f^\\cdot f^q\\_)},$ (12)\nwhere Proj\u2191 is the bilinear interpolation with a scaling factor of 2.\nBy upsampling, the network model AFANet can extract more fused feature details, enhancing the network's perceptual and expressive capabilities. This allows AFANet to capture semantic information in image segmentation tasks better, improving segmentation accuracy and performance."}, {"title": "IV. EXPERIMENTS", "content": "AFANet is trained in a meta-learning paradigm, and each episode is equivalent to a data sample in general learning algorithms. In segmentation, we using binary cross entropy (BCE) loss to update all parameters. In order to stay consistent with IMR-HSNet [10], we also add intermediate supervision to the output mask of each iteration. The final total loss is as follows:\n$L_{all} = \\sum^N\\_{t=1} [\\alpha\\cdot BCE(\\hat{M_t}, M_t) + \\beta \\cdot \u0412\u0421\u0415(\\hat{M_t}, M_q)],$ (13)\nwhere, $\\hat{M_t}$ and $M_s$ represent the predicted mask and pseudo ground-truth mask, respectively. M and Mq are not reiterated. Besides, N is the number of iterations, \u03b1 and \u03b2 are hyperparameters."}, {"title": "B. Datasets and Evaluation Metrics", "content": "For a fair comparison, our experimental setup remains consistent with IMR-HSNet, differing only in the replacement of ground-truth masks with image-level labels compared to the conventional FSS setup. The datasets used to evaluate the model include Pascal-5\u00b9 [21] and COCO-20 [22]. Pascal-5i contains 20 categories, divided into 4 folds, each fold consists of 5 categories, 3 folds are used for training, and the remaining 1 fold is used for testing. COCO-20\u00b9 contains more than 80,000 images with 80 categories, making it a more challenging dataset. We also set it to 4 folds, each fold containing 20 categories. The training and testing methods are the same as Pascal-5\u00b9. The mean intersection over union (mIoU) serves as our standard metric for evaluating model performance. Specifically, the IOU of each category is calculated based on the confusion matrix, and then the mIOU is obtained by averaging the IOU of each category."}, {"title": "C. Implementation Details", "content": "During both the training and testing phases, we set the input image size to 400 \u00d7 400. For Pascal-5\u00b9 and COCO-20, we respectively employ ResNet50 and VGG16 as backbones and set the number of epochs to 35. The difference lies in the training configurations: during Pascal-5\u00b9 training, the batch size is 16 with a learning rate of 4e-4, while during COCO-20 training, the batch size is 20 with a learning rate of le-4. For meta-testing under each cross-validation for the two datasets, we randomly sample 1,000 episodes (support-query pairs) from the test set and evaluate their metrics in 5-shot setting. All experiments are conducted with four RTX 3090 GPUs."}, {"title": "D. Comparison with State-of-the-art", "content": "In this section, we quantitatively and qualitatively compare AFANet with state-of-the-art weakly-supervised and conventional few-shot segmentation methods under the same evaluation metrics. These metrics are evaluated on the Pascal-5 and COCO-20 datasets, respectively.\nPascal - 51. The results under 1-shot and 5-shot settings, using VGG16 and ResNet50 as backbones respectively, are shown in Table I. We conclude that: (1) Regardless of the backbone or shot settings, AFANet achieves overwhelming performance improvements compared to existing WFSS methods. (2) Compared to our baseline model IMR-HSNet (VGG16, 5-shot), the maximum mIOU improvement reaches up to 6%. Additionally, in the 1-shot setting, AFANet's performance (61.5) with VGG16 as the backbone even surpasses that of IMR-HSNet using ResNet50 (61.0). (3) Compared to other few-shot semantic segmentation methods, AFANet also outperforms certain pixel-level supervised models, such as RPMG and DPnet. The outstanding performance of AFANet in the field of FSS fully demonstrates its robustness and adaptability across various scenarios. This further substantiates that frequency-domain information can extract more semantic information than RGB, and significantly enhances the adaptability of CLIP to downstream tasks.\nCOCO \u2013 20. Similarly, we illustrate the results in Table II. It still can be seen clearly that AFANet, when using VGG16 and ResNet as backbones respectively, consistently achieves state-of-the-art results in the WFSS task, regardless of whether it is in a 1-shot or 5-shot setting. For example, when using VGG16 as the backbone in a 1-shot setting, AFANet achieves the highest increase in mean mIOU compared to IMR-HSNet, with an increase of up to 3.6%. Even in the 1-shot setting, AFANet outperforms most pixel-level supervised methods, such as RPMG, PFENet++, and DRNet, regardless of whether it uses VGG16 or ResNet50 as the backbone. This confirms that AFANet is able to fully utilize the frequency domain distribution of images to extract richer semantic information. Additionally, through online learning of CLIP's prior knowledge, it better adapts to downstream tasks. We hope that our model can provide some inspiration for future WFSS research.\nVisualization. In this subsection, we conduct visual qualitative analysis based on diverse real-world task scenarios. (1) Dynamic object detection scenarios. As shown in the first column of Fig. 4, moving objects in imaging may appear blurred, and the degree of blur variation also contains frequency domain distribution information. Therefore, AFANet segments more complete contours of the airplane compared to IMR-HSnet. (2) Camouflage target detection. As shown in the second column, due to the similarity in color between the boat and the beach, it is easy to cause visual confusion. However, frequency domain information can somewhat mitigate visual deception [16]. AFANet's more accurate segmentation of the upper left corner of the boat further confirms this point. (3) Associative background suppression. Lee et al. [30] have shown that neural networks may misidentify the background of certain objects as foreground when processing certain images, a phenomenon known as correlated background. For example, birds often appear in trees (background), and fish often appear in water (background). As shown in the third column, IMR-HSNet incorrectly identifies the tree branch (background) as foreground, but AFANet correctly identifies it. We empirically believe that after fusion learning of frequency domain information and CLIP, AFANet has stronger global and local recognition capabilities. (4) Multiple object and occlusion detection. In urban road scenes, detecting multiple objects or occluded objects is one of the fundamental tasks in computer vision. As shown in the third and fourth columns, AFANet accurately identifies target objects in multi-target images, and it also provides more complete segmentation of occluded objects. Overall, through qualitative visual analysis, we have demonstrated that frequency domain distributions can provide richer information support for neural networks. Additionally, after online learning, the adaptation effect of CLIP to downstream tasks has significantly improved."}, {"title": "E. Ablation study", "content": "Pascal-5' is used to perform the following ablation studies to investigate the impact of different modules and adapter sizes in the AFANet framework.\nEffects of different modules. CFM and CSM are two crucial modules of AFANet, detailed in subsections 3.3 and 3.4, respectively. Table III shows the segmentation results under different folds. We can observe that, with the gradual addition of CFM and CSM modules, there has been a steady improvement in segmentation performance. Particularly, the addition of the CSM module led to a 3.3% increase in mean mIOU (four folds). This further demonstrates the importance of online learning in facilitating the adaptation of CLIP to downstream tasks.\nEffects of different adapter sizes. The adapter size is an important parameter in the CSM for adjusting the size of $f_{s,q}$ and the fused feature resulting from the combination of $f_{s,q}$ and $f^q_{s,q}$. As described in subsection 3.3, a smaller adapter size can filter out redundant features of CLIP prior knowledge that are irrelevant to downstream tasks, while a larger adapter size can explore more semantic details after the fusion of CLIP and frequency domain features. As shown in the results of Table IV, we compare different adapter sizes in a linearly increasing manner and ultimately select 25 as the final choice.\nRobustness Analysis of AFANet. The backbone network can be divided into shallow and deep networks. Shallow networks are responsible for extracting low-level features such as object edges, while deep networks can build high-level semantic information. Some previous studies have suggested that shallow features are more suitable for training models, whereas deep features are better for inference. Therefore, mid-level features are often used for feature extraction [10], [91]. However, recent studies have demonstrated that fixing the backbone weights and utilizing mid-level and high-level features from architectures like pyramid networks can significantly enhance the model's generalization ability [87].\nAs shown in Fig. 5, we conduct ablation experiments on AFANet by extracting features from various layers of the backbone. Features extracted from the low layers (0, 1, 2), mid layers (6, 7, 8), and high layers (10, 11, 12) yield average mIoU scores of 63.9, 63.8, and 63.8, respectively. In contrast, our cross-layer features achieve the highest mIoU score of 64.3. These experimental results are consistent with previous studies, which indicate that using features from different network layers improves the model's generalization ability. However, it is noteworthy that the performance gain from cross-layer features compared to individual layers is not particularly significant. Furthermore, as illustrated in Fig. 5, AFANet performs highly consistently across different folds, regardless of which layer's features are utilized.\nThis robust observation aligns with the initial design intention of AFANet. The primary challenge in WFSS lies in the scarcity of data sources and information, making the introduction of additional semantic supervision crucial. The experiment further demonstrates that the frequency domain provides strong semantic guidance beyond RGB information, while CLIP, through online learning, further adapts to the data distribution characteristics of downstream tasks."}, {"title": "V. CONCLUSION"}]}