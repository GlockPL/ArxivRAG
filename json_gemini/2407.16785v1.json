{"title": "PrISM-Observer: Intervention Agent to Help Users Perform Everyday Procedures Sensed using a Smartwatch", "authors": ["Riku Arakawa", "Hiromu Yakura", "Mayank Goel"], "abstract": "We routinely perform procedures (such as cooking) that include a set of atomic steps. Often, inadvertent omission or misordering of a single step can lead to serious consequences, especially for those experiencing cognitive challenges such as dementia. This paper introduces PrISM-Observer, a smartwatch-based, context-aware, real-time intervention system designed to support daily tasks by preventing errors. Unlike traditional systems that require users to seek out information, the agent observes user actions and intervenes proactively. This capability is enabled by the agent's ability to continuously update its belief in the user's behavior in real-time through multimodal sensing and forecast optimal intervention moments and methods. We first validated the steps-tracking performance of our framework through evaluations across three datasets with different complexities. Then, we implemented a real-time agent system using a smartwatch and conducted a user study in a cooking task scenario. The system generated helpful interventions, and we gained positive feedback from the participants. The general applicability of PrISM-Observer to daily tasks promises broad applications, for instance, including support for users requiring more involved interventions, such as people with dementia or post-surgical patients.", "sections": [{"title": "1 INTRODUCTION", "content": "Every day, we perform many tasks, ranging from cooking to crafting to self-care, which involve a series of atomic steps. Accurately executing all the steps of tasks can be difficult, especially when the tasks become routine and fail to capture our full attention [21] or when we face cognitive challenges such as dementia [25]. For example, people often forget to turn on the washing machine after loading it or turn off lights before leaving home [1]. Such mistakes where we omit essential steps or confuse the order of actions can lead to undesirable outcomes [41]. For instance, in a study with over a hundred participants, close to 20% of participants made critical errors while using COVID-19 self-test kits [39]. Thus, real-time assistance by sensing a user's actions and intervening as needed can help improve quality of life.\nMost of the existing task-support solutions in HCI are tailored for specific activities, often necessitating specialized equipment or advanced computers equipped with cameras and displays, such as Augmented Reality (AR) glasses. For instance, Uriu et al. [50] developed a sensor-equipped frying pan that offers context-sensitive information like the pan's current temperature. Also, AdapTutAR [22] is a machine task tutoring system designed to monitor users in following tutorials and offer feedback through AR glasses. However, we often need help with mundane tasks or in situations where instrumenting ourselves or the environment is not easy. There is a need for a solution that seamlessly integrates into supporting various routine tasks and is practical for constant use. Additionally, current systems predominantly rely on users actively seeking information, like consulting a recipe while cooking [43, 50]. There has been limited exploration into passive interactions, where systems proactively monitor and offer corrective feedback when errors occur. Designing such interactions is complex, as the system needs to model the user's spontaneous behavior to predict errors and intervene without becoming intrusive or annoying. The challenge intensifies when we shift focus from camera-based methods to more practical and ubiquitous methods such as motion and sound sensors on smartwatches. These solutions offer seamless assimilation into daily life while lowering privacy concerns but come at the cost of sensor accuracies. Our prior work [8] tried addressing the errors of sound and motion-based Human Activity Recognition (HAR) models by combining procedure knowledge with multimodal sensor data. However, we have yet to use these advances to build a reliable intervention system that guides a user through a variety of tasks in real-time.\nTo achieve an agent system that intervenes to mitigate errors in daily procedural tasks, this paper introduces PrISM-Observer. It preemptively models user task behavior and optimizes intervention methods and timing by considering uncertainties for the sensing data and anticipating the user's future behavior. Moreover, the framework allows the design of user-friendly reminder-based interventions that the user or a system designer can customize. Using the framework, we developed a prototype system that operates on a smartwatch (Figure 1) \u2013 a device chosen for its ubiquity, minimal privacy concerns compared to camera-based systems, and capability to monitor a user across various daily activities. The prototype offers timely and relevant interventions with minimal reliance on task-specific rules.\nPrISM-Observer either reminds users to execute a step in advance (REMIND IN ADVANCE), or if it infers that the user may have forgotten a step, it notifies them separately (NOTIFY IF FORGOTTEN). These interventions are time-critical as a reminder that comes too early or too late will be useless. To verify PrISM-Observer's ability to optimize the intervention timing, we applied the proposed framework to three tasks with different procedural complexities: wound care, cooking, and latte-making (Study 1). The results showed the proposed approach reduced the timing error compared to a baseline approach that does not use sensor information: by averaging across all steps, 26.8 (baseline) \u2192 24.1 (proposed) seconds in the wound care, 119.5 \u2192 61.5 seconds in the cooking, and 50.1 \u2192 22.3 seconds in the latte-making tasks, respectively.\nSubsequently, we built a real-time agent system that assisted the users in a specific cooking task of making a sunny-side-up and a grilled sausage. We evaluated the agent through a user study (Study 2, N = 10) to examine the system's performance and usability. The results showed that the participants perceived the triggered interventions to be accurate (20 out of the total 27 triggered interventions were scored five or higher for accuracy on the 7-point Likert scale in the post-task questionnaire). We also found a trend between the delay and perceived accuracy for the two kinds of intervention, highlighting the importance of optimizing their timing. In addition, the NOTIFY IF FORGOTTEN interventions were triggered correctly (22 out of the total 25 intervention chances), verifying the intervention policy's effectiveness. Overall, the participants found the system reliable and showed a positive behavioral intention (8 out of 10 participants). Furthermore, their comments validated our design of using different types of interventions and making them customizable, as well as offered implications for the interaction between humans and real-world task-support agents.\nIn this paper, we make the following contributions:\n(1) framework for modeling user behavior in procedural tasks and designing interventions, building upon the foundation of the existing multimodal procedure tracking module [8].\n(2) comprehensive evaluation across three daily-task datasets demonstrated the proposed approach's superiority in forecasting the optimal moments for interventions, highlighting its effectiveness in complex tasks with over 50% timing error reduction.\n(3) user study with a real-time prototype system on a smartwatch, which not only showed its preferable experience but also informed design implications to build reliable and acceptable task-support agents.\nIt is important to note that PrISM-Observer performs promisingly, even though the underlying HAR models are approximately only 50% accurate at detecting each atomic step of the procedures. This result demonstrates that it is possible to use imperfect sensing and machine learning to build a useful intervention system to aid an imperfect human prone to making mistakes. Our system will be particularly helpful for users who face cognitive challenges (e.g., patients with dementia or after surgery), and we are working closely with such populations as part of our future work. We open-source the framework and the dataset to facilitate the research in this domain (https://github.com/cmusmashlab/prism)."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "We first examine the cognitive psychology literature on human errors in daily tasks, recognizing that such errors are inevitable and necessitate support. Next, we review HCI studies concerning assistants for procedural tasks, underscoring the importance of using a prevalent device to support everyday procedures and its unique challenges. Finally, we explore multimodal sensing research related to activity recognition, especially using a smartwatch."}, {"title": "2.1 Human Errors in Everyday Tasks", "content": "The improper execution of everyday tasks, ranging from cooking and medical self-care to machine use, is a multifaceted issue rooted in cognitive psychology. Studies have shown that errors are likely to occur when the working memory load is high or the user is not fully attentive [13, 17]. For instance, multitasking during cooking can lead to oversights or mistakes in recipe execution [21]. Also, Beaver et al. [10] discussed that complex and integrative steps in daily activities may be the first to be affected by cognitive decline. In fact, cognitive challenges such as memory lapses contribute significantly to non-adherence to self-care activities by the elderly [36]. Additionally, misunderstanding instructions can easily lead to critical errors; almost 20% of participants made mistakes while using COVID-19 self-test kits [39]. These examples underscore the human error-prone nature in various daily scenes [41], highlighting the need for situated support that helps users avoid or recover from errors to compensate for cognitive limitations, as emphasized by Zhang [56]"}, {"title": "2.2 Assistants for Procedural Tasks", "content": "Supporting users in conducting complicated tasks in the real world has been a popular research theme in HCI research. A common approach involves crafting specialized devices tailored for particular activities or tasks. For example, Lee and Dey [29] developed a sensor-augmented pillbox and feedback system to improve medication compliance. In the cooking domain, Cooking Navi [19] is an interface providing multimedia recipe information (i.e., text, video, and audio) to aid in cooking. Uriu et al. [50] extended this support system by creating a sensor-equipped frying pan that offers context-sensitive information like the pan's current temperature. Similarly, MimiCook [43] combines a depth camera and projector to deliver on-the-spot guidance during cooking.\nComputer-vision-based approaches are popular to guide users through various tasks [20, 22, 32, 47, 55]. For instance, AR Cooking [20] used 3D animation of cookware on AR glasses. Serv\u00e1n et al. [47] developed a system to overlay work instruction in an assembly task using AR. Similarly, AdapTutAR [22] is a machine task tutoring system that monitors learners' tutorial-following status and provides feedback via AR glasses. HoloAssist [53] is a system where a human observer watches the task performer's egocentric video captured by AR glasses and guides them verbally. To support the creation of such technologies, researchers have compiled datasets capturing first-person perspectives on procedural tasks such as cooking [38] or assembly [44, 46].\nDespite the success of these systems, using cameras and displays can result in privacy-invasive and power-hungry systems. This issue becomes apparent when we want to support users' daily routines pervasively in contrast to specific, high-stakes situations such as assembly [42]. Furthermore, these solutions predominantly offer context-aware information, assuming that users actively seek the information. While benefiting those inexperienced with the task, these systems may not assist adept users who might still commit errors due to inattentiveness or cognitive overload. A similar motivation for error-checking systems was discussed by Bovo et al. [12], who proposed an approach for real-time error prediction in sequence-constrained procedural tasks. While showing promising performance in detecting errors in an item-picking-placement task, the method relies on the assumption of a predefined step sequence and task-specific heuristics like item location, which does not apply to various tasks where users are permitted to exhibit multiple behavioral patterns.\nHence, this paper aims to create a generalizable framework to monitor and intervene with users engaged in everyday tasks. Given such circumstances, users might not always prefer to utilize sophisticated equipment like AR glasses or external cameras for every task. Consequently, given its widespread use and minimal interference with the task, we have chosen a smartwatch as our device. This decision undoubtedly introduces a research question: how can we design a reliable intervention agent system for users' spontaneous behavior using imperfect sensing? Our solution is to stochastically model user behavior from sensor observation with transition knowledge to trigger situated interventions. We use minimized task-specific assumptions to offer flexibility and cater to different user preferences."}, {"title": "2.3 Smartwatch-Based Context Sensing", "content": "Human Activity Recognition (HAR) is a widely studied technology that senses user actions and behavior [54]. Among many modalities used for HAR, smartwatch-based systems often use audio and motion data [11, 18, 26, 27, 34]. For example, ViBand [27] enabled bio-acoustic sensing using commodity smartwatch accelerometers, detecting the use of different hand-held tools such as a toothbrush and heat gun. Ashry et al. [9] used cascading bidirectional long short-term memory to classify motion data of different daily activities. For audio, Ubicoustics [27] employed acoustic sensing to classify 30 daily activities, such as hand washing and typing. Recent work has proposed a multimodal learning approach to maximize the capability, for instance, GestEar [11], which combined audio and motion signals to distinguish different gestures, such as knocking and snapping. Moreover, SAMOSA [34] explored the downsampling of the audio modality by supplementarily using the motion data to make the system more privacy-aware. These advances in the smartwatch's HAR capability led to deployed applications, such as Apple Watch's Handwash detection feature [5].\nWe aim to create an intelligent agent that assists users in preventing errors in various tasks. Here, despite its potential, a significant challenge with HAR is its limited accuracy when extended to a wide range of real-world activities. For instance, Liaqat et al. [31] highlighted the difficulties posed by noise in tasks performed outside controlled environments. Additionally, when HAR is employed for tracking procedural tasks, distinguishing between steps with similar signal profiles is often hard. In this regard, studies like those by Nakauchi et al. [35] and Arakawa et al. [8] have suggested the potential of enhancing tracking accuracy by using transition information between steps of a task. While these approaches have improved the accuracy of procedure tracking, the performance is still far from perfect. Thus, a fully functional interactive system for procedural task support on a smartwatch has yet to be realized. This work provides an approach to using state-of-the-art activity recognition and procedure tracking algorithms to design a reliable agent system."}, {"title": "3 FRAMEWORK FOR ERROR MONITORING IN PROCEDURAL TASKS", "content": "In this section, we introduce a framework for monitoring users' actions and offering timely interventions. We first outline a user scenario to illustrate the utility of such an agent system and introduce our intervention design. Then, we present an algorithm using multimodal sensing to forecast user actions and a policy to trigger interventions."}, {"title": "3.1 User Scenario", "content": "As discussed in Section 2.1, a user's performance in daily tasks is affected by various factors, such as inattention and cognitive load or time pressure. Consider two scenarios: 1) Tom decided to prepare a sunny-side-up for breakfast instead of his usual choice of scrambled egg. Given he is not used to this task, he inadvertently forgets to add oil to the pan before cracking an egg. Consequently, the egg sticks to the surface of the pan. When Tom attempts to lift it with a spatula, the yolk breaks, leading to a disappointing start to the day. 2) Catherine is preparing a latte using a semi-automatic coffee machine in her office's shared kitchen. She is in a hurry to get to her next meeting and forgets to clean the steam wand after use. As a result, milk residue clogs the holes of the wand, necessitating maintenance. In these situations, it would be helpful if a system could remind users just in time to avoid errors or notify them when errors are detected in real-time. We envision a context-aware agent living on the user's watch that works in the following way: 1) Just as Tom is about to crack the egg, he receives a reminder on the watch to pour oil into the pan. 2) When Catherine is about to leave the kitchen with her latte, her watch notifies her to clean the steam wand. The following subsections describe the formulation and implementation to achieve such monitoring and intervention."}, {"title": "3.2 Interaction Design", "content": "To enable support for a diverse set of tasks, we design our interactions in a generalizable manner instead of using task-specific heuristics as discussed in Section 2.2. Let $S = \\{S_1, S_2, ... S_N\\}$ be the set of the atomic steps in the procedure where N is the total number of the steps. We assume a transition graph G, which contains information on the average time spent on each step and the transition probabilities between steps. G can be obtained from sample demonstrations of the task as described by Arakawa et al. [8].\nCurrent research on procedure tracking treats each step of a process equally, focusing on accuracy in recognizing each step on a frame-by-frame basis [8, 35]. In contrast, to develop a user-centered helpful agent, it is crucial to prioritize the steps each user needs support for. Considering Tom's cooking example above, it might not be necessary to verify whether he remembered to bring an egg from the refrigerator, as this action inevitably happens during the cooking process. Therefore, we assume a subset $S \\in S$ as a set of steps for which situated interventions can be helpful.\nGiven the nature of each step in $\\hat{S}$, suitable interventions may change. For example, users may want to receive notifications only when they forget a specific step, i.e., error detection. Or, users might appreciate receiving preemptive reminders for crucial steps, especially when the timing and sequence are critical and irreversible \u2013 for instance, adding oil to the pan before cracking an egg into it. Providing users with a global control like this is key to successfully developing human-AI interaction systems [2]. Accordingly, we prepared two types of interventions that will be assigned to each target step $\\hat{s} \\in \\hat{S}$: REMIND IN ADVANCE and NOTIFY IF FORGOTTEN. The REMIND IN ADVANCE intervention is intended to happen before the user starts the step. On the other hand, the NOTIFY IF FORGOTTEN intervention only happens when the user forgets the step. To reliably achieve this notification (i.e., preventing false positives or negatives), the system must be able to detect whenever the step happens accurately so that, when the step is not inferred, the system can be confident that the user missed the step. Thus, we enabled the system to suggest the possibility of using the NOTIFY IF FORGOTTEN intervention for a step based on the step's detectability from sensors, the detail of which is described in Section 3.3.2, hence satisfying a key requirement for successful human-AI interaction - communicating how well the system functions [2]. In addition, PrISM-Observer's default intervention messages support users' efficient dismissal in case of false predictions without annoying them [2]. For the REMIND IN ADVANCE intervention, the message is \"Don't forget to do $\\hat{s}$,\" while it is \"Have you done $\\hat{s}$?\" for the NOTIFY IF FORGOTTEN intervention.\nLastly, system designers or end-users can finalize the configuration, i.e., which $\\hat{s} \\in \\hat{S}$ are supported by interventions and the intervention type. For instance, a kitchen manager in Catherine's office may be particularly interested in ensuring that cleaning occurs post-use of the machinery, thus enabling the NOTIFY IF FORGOTTEN intervention for the step. Figure 2 summarizes the entire selection process for intervention."}, {"title": "3.3 Intervention Timing Optimization", "content": "In our framework, the system persistently observes user behavior to provide situated interventions, triggering them at opportune times. A significant obstacle arises from the fact that the training data often lacks instances of actual user errors, and it is hard to train a model to predict or detect errors directly. Thus, identifying omitted actions or reminding before a specific step in real-time is particularly challenging, especially when steps can be completed in various sequences, unlike prior work assuming a constrained sequence [12]. To address this, we propose an alternative strategy to forecast when a specific step, $\\hat{s}$, should occur based on the current belief about the user state - preemptively estimating the remaining time till $\\hat{s}$ happens. We refer to this remaining time till the user reaches $\\hat{s}$ from the current step (at time t) by using a stochastic variable D, which we describe in detail later in the next subsection. At a high level, this framework decides whether to trigger an intervention based on the expectation of when the user will perform the step $\\hat{s}$. Once the certainty of the timing of a step goes above an empirically determined threshold, PrISM-Observer prepares a timer to trigger an intervention corresponding to the step. This timer functions like this: if the system estimates the user will be doing a certain step in 10 seconds with high confidence, it waits for slightly less than 10 seconds to trigger the REMIND IN ADVANCE intervention, or the NOTIFY IF FORGOTTEN if the user does not do the step around those slightly more than 10 seconds.\nAlgorithm 1 presents the overview of the framework. In this pseudo-code, for simplicity, we assume a single target step $\\hat{s}$, but the framework is extendable to multiple target steps in parallel, as demonstrated in the user study later. In the following subsections, we discuss the modeling of D and intervention policy."}, {"title": "3.3.1 Stochastic Modeling of User Behavior", "content": "We use a stochastic variable D, indicating a remaining time at the given t till the user reaches the target step $\\hat{s}$. $D \\sim P(D)$. follows a probabilistic distribution The expectation can be calculated as,\n$E[D] = \\sum_{D} DP(D)$\n$= \\sum_{s \\in S}P(s)\\sum_{\\tau \\in T_s} P(\\tau)Time(s \\rightarrow \\hat{s}, \\tau)$\nwhere $T_s$ is the set of the possible trajectories from step s to the target step $\\hat{s}$, P(s) indicates the probability of the user being at step s at time t, $P(\\tau)$ indicates the probability of the user will follow the trajectory $\\tau$, and $Time(s \\rightarrow \\hat{s}, \\tau)$ means the average time it takes to transition from step s to $\\hat{s}$ by following the path $\\tau$. P(s) is obtained by PrISM-Tracker [8]'s output while $P(\\tau)$ and $Time(s \\rightarrow \\hat{s}, \\tau)$ are calculated based on the transition graph G.\nImportantly, this equation tells us two uncertainties with respect to user behavior: uncertainty in the current state (first sigma term) and uncertainty in the future trajectory (second sigma term). To gauge the total uncertainty the system has, entropy can be calculated as,\n$H[D] = - \\sum_D P(D) log P(D)$\nHigher entropy means more uncertainty, meaning a variance in the estimation of D. Conversely, a low entropy value indicates that the user behavior is more predictable. To computationally calculate the expectation E[D] and entropy H[D] in real-time, we used the Monte Carlo method [33] with the sample size of 10,000. Additionally, $T_s$ is enumerated with depth-first search over G from the current step s to the target step $\\hat{s}$.\nExample distribution of D calculated from one session data (cooking task consisting of 14 steps, which we detail in Section 4.1) is shown in Figure 3. In general, the distribution gets more certain over time till the target step because the uncertainties are mitigated by sensor observation. For instance, the estimation gradually becomes highly certain when the target step is $s_4$ (Figure 3 Left). On the other hand, when the target step is $s_{10}$ or $s_{13}$ (Figure 3 Center&Right), it has a high variance in the beginning primarily because there are several possible paths to the step (majorly whether cooking an egg or a sausage first). After approximately 200 seconds, the variance diminishes when the system detects the step of pouring an egg on the pan, resolving the uncertainty by considering the sensor observation and transition graph."}, {"title": "3.3.2 Intervention Policy", "content": "Lastly, we describe how the intervention is triggered based on the distribution of E[D]. The key idea is to preemptively forecast the timing of the target step and prepare the intervention. To achieve this, the system continuously characterizes user behavior by E[D] and H[D]. Figure 4 shows the example transitions related to $s_7$ and $s_{13}$ in different sessions of the cooking task. The expected remaining time (in Figure 4 Top) gets reduced as the user performs each step and gets closer to the target step. The measured entropy can remain quite noisy (as demonstrated in Figure 4 Bottom-Left) due to uncertainties in sensor data and future user behavior. However, we look for brief moments of certainty for the model where it has a reasonable idea of when the target step might happen.\nThe system starts a timer once the entropy (H[D]) gets lower than a step-dependent hyperparameter $h_s$, and E[D] is stable for a certain period afterward (\"1\" in Figure 4). We provide a further detailed implementation of this timer policy in Appendix B. This approach prevents the model from being confused later by sensor uncertainty, especially when the user takes unexpected behavior near the target step. Of course, this policy can still lead to imperfections, but we will quantify its effectiveness for different tasks in Study 1.\nOnce the timer starts (\"2\" in Figure 4), the duration of the timer depends on the type of intervention. In case of the REMIND IN ADVANCE intervention, the timer is set for $K^-$ seconds before the anticipated moment for the step of interest. At the end of the timer, the notification is triggered (\u201c3\u201d in Figure 4) to remind the user to make sure they do the step. In case of NOTIFY IF FORGOTTEN intervention, the timer is set for $K^+$ after the anticipated moment for the step of interest. Until the timer runs out, the system waits for the user to do the step of interest. If the user does not perform the step, the system generates a NOTIFY IF FORGOTTEN intervention. The values of constants $K^-$ and $K^+$ are decided by a system designer or can be adjusted by the end-user. For example, if a user wants a notification as soon as possible after they skip a step, a smaller $K^+$ is chosen, but it might lead to noisy interactions. We envision designers will experiment with optimal values for these constants to tailor the interventions to their goals."}, {"title": "4 STUDY 1: ALGORITHM EVALUATION IN MULTIPLE DAILY TASKS", "content": "We first investigate the effectiveness of the proposed algorithm in preemptively predicting the timing of specific steps. We use a dataset of multiple daily procedures with different task complexity."}, {"title": "4.1 Dataset", "content": "We used the dataset of procedural tasks introduced in [8], that is, latte-making (22 sessions, 15 participants) and wound care (23 sessions, 23 participants) tasks. Following the same protocol, we expanded the dataset by collecting new data on a cooking task with 17 sessions and 8 participants who were already familiar with the task. This data collection occurred in a single kitchen. Note that, unlike the latte-making and wound care tasks where the watch was worn on the right wrist, the watch was placed on the left for the cooking task. This difference in the setting led to differences in the motion data patterns since the participants were all right-handed, and thus, the watch might not have captured some crucial motions performed by the dominant hand. Still, we anticipated that this new cooking scenario - featuring users with the watch on their non-dominant hand's wrist - would enhance the system's usability and reflect more natural use cases. We preprocessed and obtained frame-level classification results using PrISM-Tracker [8] with the same frame length of 0.2 seconds, to which we applied our framework.\nThese three tasks encompass a range of procedural complexity, distinguished by the number of branching paths within each task. Wound care is a training task during perioperative counseling for skin cancer patients in a medical facility. Given the clinical staff trains the patients on how to clean their post-surgical wounds, the task is linear, where the sequence of steps is predetermined. However, given the individual variance in dexterity of patients, there is a high variance in the duration of each step across participants. In contrast, the latte-making task is done by regular users of a coffee machine in an academic building. These users were not given set instructions on how to use the machine. Thus, the task involves more flexibility, allowing for various sequences of steps to be performed with minimal restrictions on ordering. The cooking task represents an intermediate level of complexity. It includes 14 steps where users prepare a sunny-side-up and grilled sausage. The participants can choose the order in which they cook each item, which creates a major branch in the transition. Notably, the dataset did not include error cases, and all steps were properly executed. We will test our system's real-time capability to detect errors in Study 2."}, {"title": "4.2 Metric", "content": "Study 1 aims to measure the algorithm's accuracy in forecasting the timing of future target steps. To evaluate this, we processed session data from the beginning to emulate real-time prediction. When the intervention timer started at time t according to a policy, we calculated the error between the expected remaining time E[D] and the actual remaining time till the target step. The example of this error calculation is presented in Figure 6. Also, while the system will be designed for certain target steps ($\\hat{S}$) as discussed in Section 3.2, we assumed $\\hat{S} = S$ to examine the accuracy for all steps in this study."}, {"title": "4.3 Compared Intervention Policies", "content": "We used the policy described in Section 3.3.2 as the proposed condition. We conducted leave-one-session-out cross-validation, and hyperparameters $h_{s_i}$ ($s_i \\in S$) in this policy were obtained by grid search in each training fold.\nIn addition, we prepared the baseline policy, which is based on the expected time to the target step at the beginning, i.e., E[D]. This policy serves as a condition where the system does not use the sensor data and relies on the transition history to predict the timing of the target step."}, {"title": "4.4 Results", "content": "The absolute timing error in the three tasks by different target steps $\\hat{s}$ is presented in Figure 7. Overall, the proposed policy reduced the timing error from the baseline policy at varying target steps, especially in tasks with more complexity. At the same time, we observe the proposed policy did not contribute much to reducing the errors at certain steps. We delve deeper into the result of each task and discuss the effect of task complexity and sensing reliability on the error."}, {"title": "4.4.1 Wound Care", "content": "Both policies led to small errors (26.8 seconds for the baseline policy and 24.1 seconds for the proposed policy on average across all steps). The satisfactory performance of the baseline policy can be attributed to the single-threaded nature of the task. Furthermore, the tracker's accuracy for this task is limited, as the steps do not introduce significant differences in sensor readings, particularly for intermediate steps ($s_3$, ..., $s_9$), as illustrated in Figure 12 in Appendix A. The confusion led to the increased error for $s_6$ in the proposed policy. The result suggests that, when the sensing consists of much uncertainty, the system struggles to model user behavior from the observation. Still, when the sensing accurately detects some actions, it benefits the proposed policy. For instance, the tracker's effective detection of state $s_2$ and $s_{11}$ significantly lowers the error in predicting the subsequent state $s_3$ and $s_{12}$ in the proposed policy."}, {"title": "4.4.2 Cooking", "content": "The proposed policy largely reduced the timing error compared to the baseline policy (119.5 seconds for the baseline policy and 61.5 seconds for the proposed policy). In the baseline policy, the lack of context as to which item to cook first led to a large error, while the proposed policy could infer it from the observation. Similarly to the wound care result, it is implied that the larger errors in the proposed policy (i.e., $S_8, S_9, S_{10}$) came from the low sensing accuracy of $s_7$ and $s_8$, as shown in Figure 13 in Appendix A, in addition to the relatively longer duration of $s_9$."}, {"title": "4.4.3 Latte-Making", "content": "Lastly, the proposed policy again largely reduced the timing error compared to the baseline policy (50.1 seconds for the baseline policy and 22.3 seconds for the proposed policy). The error in the baseline policy got larger, especially around the middle steps where different transition branches exist. On the other hand, it can be seen that the proposed policy effectively used the sensor data to forecast the timing of different target steps."}, {"title": "4.5 Discussion", "content": "The results demonstrated the clear effectiveness of the proposed approach and its implications. First, when tasks are complex and feature multiple potential transition pathways, employing stochastic models to predict user behavior constantly updated by sensor observation proves particularly beneficial. It could be contended that the sequence of steps ought to remain constant, especially for routine tasks. However, we chose to allow users the flexibility to adjust the order of steps as per their specific circumstances. Such flexibility enables broader user scenarios as discussed in Section 3.1. Simultaneously, enabling the system to learn from the same user's data over days represents a promising direction for future deployment.\nThe results also corroborated that uncertainty remains. Tracking all steps in complex, everyday tasks with a common device like a smartwatch is quite challenging. The limited tracking accuracy of certain target steps affects the behavior modeling around them. Still, an effective intervention system does not always need to be built on highly accurate sensing. For example, step counters can be inaccurate sensors [49] but still an effective tool for behavior change [40]. Thus, we examine the utility of our intervention design in Study 2."}, {"title": "5 REAL-TIME AGENT SYSTEM IMPLEMENTATION", "content": "Study 1 showed the proposed approach's effectiveness in forecasting the timing of target steps, based on which our interventions are triggered. To examine the utility of such real-time interventions, we developed a prototype using Apple Watch (Series 7, watchOS 10.3) with Swift. Currently, there are two versions of the implementation: the laptop-server version and the watch-only version. For the laptop-server version, we used a MacBook Pro with 16GB Apple M1 Chip as a server. The Apple Watch streams the sensor data obtained through dedicated APIs (i.e., AVAudioEngine and CoreMotion) via the network to the server in real-time. The server runs PrISM-Tracker (frame-level HAR and the Viterbi correction), followed by PrISM-Observer (stochastic modeling and intervention policy). When the intervention is to happen, the laptop plays the audio file. This implementation is fast enough because it can utilize efficient computation libraries in Python. We used this version for the user study (Study 2).\nAt the same time, we developed a watch-only version to show the feasibility of a self-contained agent on the device. The process pipeline is shown in Figure 8. Here, the obtained multimodal data is funneled into a pre-trained feature extractor that operates on the principle of CoreML [4], Apple's machine learning framework. The feature extractor analyzes the incoming data streams and translates them into a feature-rich format suitable for the subsequent frame-level classifier, which CoreML also powers. Note that these models were already trained offline in the same manner with the prior work [8] and converted into CoreML format beforehand using coremltools. Following the classifier, the system applies the Viterbi tracking and intervention policy (including the stochastic modeling). These components leverage Apple's Accelerate [3] for high-performance computations. Finally, when the system decides to trigger interventions, the watch plays the corresponding audio file. In both versions, note that while the system emits sound for intervention, the sensor readings of both motion and audio stop to prevent noise from adding to the tracking."}, {"title": "6"}]}