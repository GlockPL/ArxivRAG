{"title": "PrISM-Observer: Intervention Agent to Help Users Perform Everyday Procedures Sensed using a Smartwatch", "authors": ["Riku Arakawa", "Hiromu Yakura", "Mayank Goel"], "abstract": "We routinely perform procedures (such as cooking) that include a set of atomic steps. Often, inadvertent omission or misordering of a single step can lead to serious consequences, especially for those experiencing cognitive challenges such as dementia. This paper introduces PrISM-Observer, a smartwatch-based, context-aware, real-time intervention system designed to support daily tasks by preventing errors. Unlike traditional systems that require users to seek out information, the agent observes user actions and intervenes proactively. This capability is enabled by the agent's ability to continuously update its belief in the user's behavior in real-time through multimodal sensing and forecast optimal intervention moments and methods. We first validated the steps-tracking performance of our framework through evaluations across three datasets with different complexities. Then, we implemented a real-time agent system using a smartwatch and conducted a user study in a cooking task scenario. The system generated helpful interventions, and we gained positive feedback from the participants. The general applicability of PrISM-Observer to daily tasks promises broad applications, for instance, including support for users requiring more involved interventions, such as people with dementia or post-surgical patients.", "sections": [{"title": "1 INTRODUCTION", "content": "Every day, we perform many tasks, ranging from cooking to crafting to self-care, which involve a series of atomic steps. Accurately executing all the steps of tasks can be difficult, especially when the tasks become routine and fail to capture our full attention [21] or when we face cognitive challenges such as dementia [25]. For example, people often forget to turn on the washing machine after loading it or turn off lights before leaving home [1]. Such mistakes where we omit essential steps or confuse the order of actions can lead to undesirable outcomes [41]. For instance, in a study with over a hundred participants, close to 20% of participants made critical errors while using COVID-19 self-test kits [39]. Thus, real-time assistance by sensing a user's actions and intervening as needed can help improve quality of life.\nMost of the existing task-support solutions in HCI are tailored for specific activities, often necessitating specialized equipment or advanced computers equipped with cameras and displays, such as Augmented Reality (AR) glasses. For instance, Uriu et al. [50] developed a sensor-equipped frying pan that offers context-sensitive information like the pan's current temperature. Also, AdapTutAR [22] is a machine task tutoring system designed to monitor users in following tutorials and offer feedback through AR glasses. However, we often need help with mundane tasks or in situations where instrumenting ourselves or the environment is not easy. There is a need for a solution that seamlessly integrates into supporting various routine tasks and is practical for constant use. Additionally, current systems predominantly rely on users actively seeking information, like consulting a recipe while cooking [43, 50]. There has been limited exploration into passive interactions, where systems proactively monitor and offer corrective feedback when errors occur. Designing such interactions is complex, as the system needs to model the user's spontaneous behavior to predict errors and intervene without becoming intrusive or annoying. The challenge intensifies when we shift focus from camera-based methods to more practical and ubiquitous methods such as motion and sound sensors on smartwatches. These solutions offer seamless assimilation into daily life while lowering privacy concerns but come at the cost of sensor accuracies. Our prior work [8] tried addressing the errors of sound and motion-based Human Activity Recognition (HAR) models by combining procedure knowledge with multimodal sensor data. However, we have yet to use these advances to build a reliable intervention system that guides a user through a variety of tasks in real-time.\nTo achieve an agent system that intervenes to mitigate errors in daily procedural tasks, this paper introduces PrISM-Observer. It preemptively models user task behavior and optimizes intervention methods and timing by considering uncertainties for the sensing data and anticipating the user's future behavior. Moreover, the framework allows the design of user-friendly reminder-based interventions that the user or a system designer can customize. Using the framework, we developed a prototype system that operates on a smartwatch (Figure 1) \u2013 a device chosen for its ubiquity, minimal privacy concerns compared to camera-based systems, and capability to monitor a user across various daily activities. The prototype offers timely and relevant interventions with minimal reliance on task-specific rules."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "We first examine the cognitive psychology literature on human errors in daily tasks, recognizing that such errors are inevitable and necessitate support. Next, we review HCI studies concerning assistants for procedural tasks, underscoring the importance of using a prevalent device to support everyday procedures and its unique challenges. Finally, we explore multimodal sensing research related to activity recognition, especially using a smartwatch."}, {"title": "2.1 Human Errors in Everyday Tasks", "content": "The improper execution of everyday tasks, ranging from cooking and medical self-care to machine use, is a multifaceted issue rooted in cognitive psychology. Studies have shown that errors are likely to occur when the working memory load is high or the user is not fully attentive [13, 17]. For instance, multitasking during cooking can lead to oversights or mistakes in recipe execution [21]. Also, Beaver et al. [10] discussed that complex and integrative steps in daily activities may be the first to be affected by cognitive decline. In fact, cognitive challenges such as memory lapses contribute significantly to non-adherence to self-care activities by the elderly [36]. Additionally, misunderstanding instructions can easily lead to critical errors; almost 20% of participants made mistakes while using COVID-19 self-test kits [39]. These examples underscore the human error-prone nature in various daily scenes [41], highlighting the need for situated support that helps users avoid or recover from errors to compensate for cognitive limitations, as emphasized by Zhang [56]"}, {"title": "2.2 Assistants for Procedural Tasks", "content": "Supporting users in conducting complicated tasks in the real world has been a popular research theme in HCI research. A common approach involves crafting specialized devices tailored for particular activities or tasks. For example, Lee and Dey [29] developed a sensor-augmented pillbox and feedback system to improve medication compliance. In the cooking domain, Cooking Navi [19] is an interface providing multimedia recipe information (i.e., text, video, and audio) to aid in cooking. Uriu et al. [50] extended this support system by creating a sensor-equipped frying pan that offers context-sensitive information like the pan's current temperature. Similarly, MimiCook [43] combines a depth camera and projector to deliver on-the-spot guidance during cooking.\nComputer-vision-based approaches are popular to guide users through various tasks [20, 22, 32, 47, 55]. For instance, AR Cooking [20] used 3D animation of cookware on AR glasses. Serv\u00e1n et al. [47] developed a system to overlay work instruction in an assembly task using AR. Similarly, AdapTutAR [22] is a machine task tutoring system that monitors learners' tutorial-following status and provides feedback via AR glasses. HoloAssist [53] is a system where a human observer watches the task performer's egocentric video captured by AR glasses and guides them verbally. To support the creation of such technologies, researchers have compiled datasets capturing first-person perspectives on procedural tasks such as cooking [38] or assembly [44, 46].\nDespite the success of these systems, using cameras and displays can result in privacy-invasive and power-hungry systems. This issue becomes apparent when we want to support users' daily routines pervasively in contrast to specific, high-stakes situations such as assembly [42]. Furthermore, these solutions predominantly offer context-aware information, assuming that users actively seek the information. While benefiting those inexperienced with the task, these systems may not assist adept users who might still commit errors due to inattentiveness or cognitive overload. A similar motivation for error-checking systems was discussed by Bovo et al. [12], who proposed an approach for real-time error prediction in sequence-constrained procedural tasks. While showing promising performance in detecting errors in an item-picking-placement task, the method relies on the assumption of a predefined step sequence and task-specific heuristics like item location, which does not apply to various tasks where users are permitted to exhibit multiple behavioral patterns.\nHence, this paper aims to create a generalizable framework to monitor and intervene with users engaged in everyday tasks. Given such circumstances, users might not always prefer to utilize sophisticated equipment like AR glasses or external cameras for every task. Consequently, given its widespread use and minimal interference with the task, we have chosen a smartwatch as our device. This decision undoubtedly introduces a research question: how can we design a reliable intervention agent system for users' spontaneous behavior using imperfect sensing? Our solution is to stochastically model user behavior from sensor observation with transition knowledge to trigger situated interventions. We use minimized task-specific assumptions to offer flexibility and cater to different user preferences."}, {"title": "2.3 Smartwatch-Based Context Sensing", "content": "Human Activity Recognition (HAR) is a widely studied technology that senses user actions and behavior [54]. Among many modalities used for HAR, smartwatch-based systems often use audio and motion data [11, 18, 26, 27, 34]. For example, ViBand [27] enabled bio-acoustic sensing using commodity smartwatch accelerometers, detecting the use of different hand-held tools such as a toothbrush and heat gun. Ashry et al. [9] used cascading bidirectional long short-term memory to classify motion data of different daily activities. For audio, Ubicoustics [27] employed acoustic sensing to classify 30 daily activities, such as hand washing and typing. Recent work has proposed a multimodal learning approach to maximize the capability, for instance, GestEar [11], which combined audio and motion signals to distinguish different gestures, such as knocking and snapping. Moreover, SAMOSA [34] explored the downsampling of the audio modality by supplementarily using the motion data to make the system more privacy-aware. These advances in the smartwatch's HAR capability led to deployed applications, such as Apple Watch's Handwash detection feature [5].\nWe aim to create an intelligent agent that assists users in preventing errors in various tasks. Here, despite its potential, a significant challenge with HAR is its limited accuracy when extended to a wide range of real-world activities. For instance, Liaqat et al. [31] highlighted the difficulties posed by noise in tasks performed outside controlled environments. Additionally, when HAR is employed for tracking procedural tasks, distinguishing between steps with similar signal profiles is often hard. In this regard, studies like those by Nakauchi et al. [35] and Arakawa et al. [8] have suggested the"}, {"title": "3 FRAMEWORK FOR ERROR MONITORING IN PROCEDURAL TASKS", "content": "In this section, we introduce a framework for monitoring users' actions and offering timely interventions. We first outline a user scenario to illustrate the utility of such an agent system and introduce our intervention design. Then, we present an algorithm using multimodal sensing to forecast user actions and a policy to trigger interventions."}, {"title": "3.1 User Scenario", "content": "As discussed in Section 2.1, a user's performance in daily tasks is affected by various factors, such as inattention and cognitive load or time pressure. Consider two scenarios: 1) Tom decided to prepare a sunny-side-up for breakfast instead of his usual choice of scrambled egg. Given he is not used to this task, he inadvertently forgets to add oil to the pan before cracking an egg. Consequently, the egg sticks to the surface of the pan. When Tom attempts to lift it with a spatula, the yolk breaks, leading to a disappointing start to the day. 2) Catherine is preparing a latte using a semi-automatic coffee machine in her office's shared kitchen. She is in a hurry to get to her next meeting and forgets to clean the steam wand after use. As a result, milk residue clogs the holes of the wand, necessitating maintenance. In these situations, it would be helpful if a system could remind users just in time to avoid errors or notify them when errors are detected in real-time. We envision a context-aware agent living on the user's watch that works in the following way: 1) Just as Tom is about to crack the egg, he receives a reminder on the watch to pour oil into the pan. 2) When Catherine is about to leave the kitchen with her latte, her watch notifies her to clean the steam wand. The following subsections describe the formulation and implementation to achieve such monitoring and intervention."}, {"title": "3.2 Interaction Design", "content": "To enable support for a diverse set of tasks, we design our interactions in a generalizable manner instead of using task-specific heuristics as discussed in Section 2.2. Let \\(S = \\{S_1, S_2, ... S_N\\}\\) be the set of the atomic steps in the procedure where N is the total number of the steps. We assume a transition graph G, which contains information on the average time spent on each step and the transition probabilities between steps. G can be obtained from sample demonstrations of the task as described by Arakawa et al. [8].\nCurrent research on procedure tracking treats each step of a process equally, focusing on accuracy in recognizing each step on a frame-by-frame basis [8, 35]. In contrast, to develop a user-centered helpful agent, it is crucial to prioritize the steps each user needs support for. Considering Tom's cooking example above, it might not be necessary to verify whether he remembered to bring an egg"}, {"title": "3.3 Intervention Timing Optimization", "content": "In our framework, the system persistently observes user behavior to provide situated interventions, triggering them at opportune times. A significant obstacle arises from the fact that the training data often lacks instances of actual user errors, and it is hard to train a model to predict or detect errors directly. Thus, identifying omitted actions or reminding before a specific step in real-time is particularly challenging, especially when steps can be completed in various sequences, unlike prior work assuming a constrained sequence [12]. To address this, we propose an alternative strategy to forecast when a specific step, \u015d, should occur based on the current belief about the user state - preemptively estimating the remaining time till \u015d happens. We refer to this remaining time till the user reaches \u015d from the current step (at time t) by using a stochastic variable D, which we describe in detail later in the next subsection. At a high level, this framework decides whether to trigger an intervention based on the expectation of when the user will perform the step \u015d. Once the certainty of the timing of a step goes above an empirically determined threshold, PrISM-Observer prepares a timer to trigger an intervention corresponding to the step. This timer functions like this: if the system estimates the user will be doing a certain step in 10 seconds with high confidence, it waits for slightly less than 10 seconds to trigger the REMIND IN ADVANCE intervention, or the NOTIFY IF FORGOTTEN if the user does not do the step around those slightly more than 10 seconds.\nAlgorithm 1 presents the overview of the framework. In this pseudo-code, for simplicity, we assume a single target step \u015d, but the framework is extendable to multiple target steps in parallel, as demonstrated in the user study later. In the following subsections, we discuss the modeling of D and intervention policy."}, {"title": "3.3.1 Stochastic Modeling of User Behavior", "content": "We use a stochastic variable D, indicating a remaining time at the given t till the user reaches the target step \u015d. D\u00ee follows a probabilistic distribution \\(P(D)\\). The expectation can be calculated as,\n\\(E[D] = \\sum DP(D)\n= \\sum_{s \\in S}P(s) \\sum_{\\tau \\in T_s} P(\\tau)Time(s \\rightarrow \\hat{s}, \\tau)\\)\nwhere \\(T_s\\) is the set of the possible trajectories from step s to the target step \u015d, \\(P(s)\\) indicates the probability of the user being at step s at time t, \\(P(\\tau)\\) indicates the probability of the user will follow the trajectory \u03c4, and \\(Time(s \\rightarrow \\hat{s}, \\tau)\\) means the average time it takes to transition from step s to \u015d by following the path \u03c4. \\(P(s)\\) is obtained by PrISM-Tracker [8]'s output while \\(P(\\tau)\\) and \\(Time(s \\rightarrow \\hat{s}, \\tau)\\) are calculated based on the transition graph G.\nImportantly, this equation tells us two uncertainties with respect to user behavior: uncertainty in the current state (first sigma term) and uncertainty in the future trajectory (second sigma term). To gauge the total uncertainty the system has, entropy can be calculated as,\n\\(H[D] = - \\sum P(D) log P(D)\\)\nHigher entropy means more uncertainty, meaning a variance in the estimation of D. Conversely, a low entropy value indicates that the user behavior is more predictable. To computationally calculate the expectation \\(E[D]\\) and entropy \\(H[D]\\) in real-time, we used the Monte Carlo method [33] with the sample size of 10,000. Additionally, \\(T_s\\) is enumerated with depth-first search over G from the current step s to the target step \u015d."}, {"title": "3.3.2 Intervention Policy", "content": "Lastly, we describe how the intervention is triggered based on the distribution of \\(E[D]\\). The key idea is to preemptively forecast the timing of the target step and prepare the"}, {"title": "4 STUDY 1: ALGORITHM EVALUATION IN MULTIPLE DAILY TASKS", "content": "We first investigate the effectiveness of the proposed algorithm in preemptively predicting the timing of specific steps. We use a dataset of multiple daily procedures with different task complexity."}, {"title": "4.1 Dataset", "content": "We used the dataset of procedural tasks introduced in [8], that is, latte-making (22 sessions, 15 participants) and wound care (23 sessions, 23 participants) tasks. Following the same protocol, we expanded the dataset by collecting new data on a cooking task with 17 sessions and 8 participants who were already familiar with the task. This data collection occurred in a single kitchen. Note that, unlike the latte-making and wound care tasks where the watch was worn on the right wrist, the watch was placed on the left for the cooking task. This difference in the setting led to differences in the motion data patterns since the participants were all right-handed, and thus, the watch might not have captured some crucial motions performed by the dominant hand. Still, we anticipated that this new cooking scenario - featuring users with the watch on their non-dominant hand's wrist - would enhance the system's usability and reflect more natural use cases. We preprocessed and obtained frame-level classification results using PrISM-Tracker [8] with the same frame length of 0.2 seconds, to which we applied our framework.\nThese three tasks encompass a range of procedural complexity, distinguished by the number of branching paths within each task. Wound care is a training task during perioperative counseling for skin cancer patients in a medical facility. Given the clinical staff trains the patients on how to clean their post-surgical wounds, the task is linear, where the sequence of steps is predetermined. However, given the individual variance in dexterity of patients, there is a high variance in the duration of each step across participants. In contrast, the latte-making task is done by regular users of a coffee machine in an academic building. These users were not given set instructions on how to use the machine. Thus, the task involves more flexibility, allowing for various sequences of steps to be performed with minimal restrictions on ordering. The cooking task represents an intermediate level of complexity. It includes 14 steps where users prepare a sunny-side-up and grilled sausage. The participants can choose the order in which they cook each item, which creates a major branch in the transition. Notably, the dataset did not include error cases, and all steps were properly executed. We will test our system's real-time capability to detect errors in Study 2."}, {"title": "4.2 Metric", "content": "Study 1 aims to measure the algorithm's accuracy in forecasting the timing of future target steps. To evaluate this, we processed session data from the beginning to emulate real-time prediction. When the intervention timer started at time t according to a policy, we calculated the error between the expected remaining time \\(E[D]\\) and the actual remaining time till the target step."}, {"title": "4.3 Compared Intervention Policies", "content": "We used the policy described in Section 3.3.2 as the proposed condition. We conducted leave-one-session-out cross-validation, and hyperparameters \\(h_{si} (s_i \\in S)\\) in this policy were obtained by grid search in each training fold.\nIn addition, we prepared the baseline policy, which is based on the expected time to the target step at the beginning, i.e., \\(E[D]\\). This policy serves as a condition where the system does not use the sensor data and relies on the transition history to predict the timing of the target step."}, {"title": "4.4 Results", "content": "The absolute timing error in the three tasks by different target steps \u015d is presented in Figure 7. Overall, the proposed policy reduced the timing error from the baseline policy at varying target steps, especially in tasks with more complexity. At the same time, we observe the proposed policy did not contribute much to reducing the errors at certain steps. We delve deeper into the result of each task and discuss the effect of task complexity and sensing reliability on the error."}, {"title": "4.4.1 Wound Care", "content": "Both policies led to small errors (26.8 seconds for the baseline policy and 24.1 seconds for the proposed policy on average across all steps). The satisfactory performance of the baseline policy can be attributed to the single-threaded nature of the task. Furthermore, the tracker's accuracy for this task is limited, as the steps do not introduce significant differences in sensor readings, particularly for intermediate steps (s3, ..., s9), as illustrated in Figure 12 in Appendix A. The confusion led to the increased error for s6 in the proposed policy. The result suggests that, when the sensing consists of much uncertainty, the system struggles to model user behavior from the observation. Still, when the sensing accurately detects some actions, it benefits the proposed policy. For instance, the tracker's effective detection of state s2 and $11 significantly lowers the error in predicting the subsequent state s3 and $12 in the proposed policy."}, {"title": "4.4.2 Cooking", "content": "The proposed policy largely reduced the timing error compared to the baseline policy (119.5 seconds for the baseline policy and 61.5 seconds for the proposed policy). In the baseline policy, the lack of context as to which item to cook first led to a large error, while the proposed policy could infer it from the observation. Similarly to the wound care result, it is implied that the larger errors in the proposed policy (i.e., S8, S9, S10) came from the low sensing accuracy of s7 and s8, as shown in Figure 13 in Appendix A, in addition to the relatively longer duration of s9."}, {"title": "4.4.3 Latte-Making", "content": "Lastly, the proposed policy again largely reduced the timing error compared to the baseline policy (50.1 seconds for the baseline policy and 22.3 seconds for the proposed policy). The error in the baseline policy got larger, especially around the middle steps where different transition branches exist. On the other hand, it can be seen that the proposed policy effectively used the sensor data to forecast the timing of different target steps."}, {"title": "4.5 Discussion", "content": "The results demonstrated the clear effectiveness of the proposed approach and its implications. First, when tasks are complex and feature multiple potential transition pathways, employing stochastic models to predict user behavior constantly updated by sensor observation proves particularly beneficial. It could be contended that the sequence of steps ought to remain constant, especially for"}, {"title": "5 REAL-TIME AGENT SYSTEM IMPLEMENTATION", "content": "Study 1 showed the proposed approach's effectiveness in forecasting the timing of target steps, based on which our interventions are triggered. To examine the utility of such real-time interventions, we developed a prototype using Apple Watch (Series 7, watchOS 10.3) with Swift. Currently, there are two versions of the implementation: the laptop-server version and the watch-only version. For the laptop-server version, we used a MacBook Pro with 16GB Apple M1 Chip as a server. The Apple Watch streams the sensor data obtained through dedicated APIs (i.e., AVAudioEngine and CoreMotion) via the network to the server in real-time. The server runs PrISM-Tracker (frame-level HAR and the Viterbi correction), followed by PrISM-Observer (stochastic modeling and intervention policy). When the intervention is to happen, the laptop plays the audio file. This implementation is fast enough because it can utilize efficient computation libraries in Python. We used this version for the user study (Study 2).\nAt the same time, we developed a watch-only version to show the feasibility of a self-contained agent on the device. The process"}, {"title": "6 STUDY 2: USER STUDY WITH A REAL-TIME AGENT SYSTEM IN COOKING", "content": "Finally, we conducted a user study to examine the accuracy of the real-time system and users' perception of the performance of in-situ interventions. We used the laptop-server version implementation with the cooking task introduced in Study 1, which showed that the task has relatively larger errors in forecasting the target step moment even though the proposed approach mitigates them. This study involves evaluating the intervention effect where such uncertainty remains."}, {"title": "6.1 System Configuration", "content": "We first trained PrISM-Tracker [8] (i.e., frame classifier and transition graph G) as the tracking module and obtained the best thresholds (\\(h_1, h_2, ..., h_{s14}\\)) for the intervention policy using all 17-session training data in the dataset. Moreover, we decided the remaining parameters in the system, that is, K\u00af and K+, governing the timing of the REMIND IN ADVANCE and NOTIFY IF FORGOTTEN interventions around the target step, respectively. The authors explored different values before the study and set K\u00af = 15 and K+ = 15 (seconds) as reasonable timing to trigger each type of intervention before and after the moment the target step is supposed to happen.\nIn addition, we chose candidate steps \u00a7 from all the 14 steps that users may forget or want to be reminded of (Recall the step selection process shown in Figure 2). Here, two system designers individually chose candidate steps first and then had a discussion to reach an agreement. As a result, they chose five steps: s2 washing hands with soap and water, s6 pouring oil on the pan, s8 dropping a small amount of water, s11 wiping the pan surface, and s14 cleaning the table. Moreover, as described in Section 3.2, the system suggests the intervention type option based on each step's detectability by frame-by-frame HAR (without the Viterbi correction [8], corresponding to the left figure in Figure 13 in Appendix A). The F1-score for the chosen steps were 0.83, 0.64, 0.28, 0.43, and 0.52, respectively. We first decided to assign REMIND IN ADVANCE to $11 due to its low accuracy. Moreover, the system designers suggested that doing s6 before putting an item on the pan is important in actual cooking, and thus, we assigned REMIND IN ADVANCE to s6 as well. We assigned NOTIFY IF FORGOTTEN to the other steps. Note that we used the same intervention set for all participants in this study to make the comparison meaningful, but the end-users would always have the flexibility to switch the type. The detection accuracy and the assigned intervention type for each \u015d \u2208 \u015c are summarized in Table 2."}, {"title": "6.2 Procedure", "content": "We recruited 10 participants (P1 \u2013 P10) through word-of-mouth. They self-reported how often they cook in their daily lives and were all right-handed. To test the robustness against different environments, we used the same kitchen (kitchen 1) as in the training data for five participants and a different kitchen (kitchen 2) for the rest of the participants. Their demographic information is presented in Table 3."}, {"title": "6.2.1 Step Selection", "content": "After consent, we showed a list of all the steps S and a tutorial video. Then, we shared interaction candidate steps S, and asked which steps the users believe they might miss at times and would prefer some intervention support. We asked them to choose at least two steps for the experiment. We also encouraged them to imagine the task would be their routine and which intervention they might want. Simultaneously, we emphasized that they could change the step order flexibly without having to follow the tutorial. We configured the agent system once they finalized the steps. Here, if their chosen steps included a step for the intervention of NOTIFY IF FORGOTTEN, we asked them to intentionally skip some of them to test the accuracy of the intervention policy. Note, if the participant intentionally skipped the last step $14, the study ran for an extra K+ + 23.6 (the average time for $14) seconds to give the system enough time to judge whether to trigger the NOTIFY IF FORGOTTEN intervention for $14. We did not specify what to do during the extra time, and the participants behaved naturally. The selected steps for each participant and which step they skipped are also shown in Table 3."}, {"title": "6.2.2 Task Execution", "content": "Once we configured the system, we answered their questions about the task to ensure they understood the procedure. We also explained that, during the task execution, they were expected not to pause for questions (except in case of an emergency). They pressed the button on the watch app to begin the task and hit the button again to end the task. After the task, they completed a questionnaire, followed by semi-structured interviews about their experience of the system. The entire session took approximately 30 minutes for one participant."}, {"title": "6.3 Metric", "content": "We measured the timing difference between the triggered intervention and the target step. Here, the timing for the target step was the moment the participant finished the previous step, which is the same timing as they started the target step unless they intentionally skipped the target step. Note that the triggered intervention timing here accounted for the offset, that is, K+ and K. We manually annotated these timings for each session.\nIn addition, for the NOTIFY IF FORGOTTEN intervention, we annotated if each intervention was accurate. True positive (TP) when"}, {"title": "6.4 Results", "content": "Figure 9 shows the delay of each intervention and how accurate the participants felt about the timing of the intervention. The circle markers indicate the REMIND IN ADVANCE intervention (i.e., s6 and s8) and the square markers indicate the NOTIFY IF FORGOTTEN intervention (i.e., $2, $11, and $14). The result suggests that the agent system could trigger most interventions with a small timing error, and the participants felt they were accurate (20 out of the total 27 triggered interventions were rated five or higher on the 1-7 Likert scale). Also, by looking at the trend lines, the relationship between the delay and perceived accuracy is implied. For instance, the participants perceived the REMIND IN ADVANCE intervention (circle markers) as inaccurate as the delay becomes positively larger. At the same time, it was best rated when triggered some seconds before the delay is 0. This is aligned with the type of the intervention-users want it to be triggered before the target step-validating our design, specifically, the use of the offset K\u00af with E[D]. Conversely, they rated higher for the NOTIFY IF FORGOTTEN intervention (square markers) if they happen some seconds after, also corroborating the use of the offset K+. While more samples are demanded to examine the trend, we can infer that the participants' expectations about the different interventions affected their perception of the intervention timing.\nAdditionally, there was no significant difference in the delay between kitchens 1 and 2 (p > 0.05) in this cooking scenario. While more investigation is demanded to quantify the effect, this result suggests the benefit of the smartwatch-based agent system: the location-independent HAR capability.\nFigure 10 shows whether each of the NOTIFY IF FORGOTTEN interventions was accurate. While the number of samples is small, the results of the low error rate (3 out of 25 chances) indicate that the agent system could detect whether the target step happened or not and notify the participants appropriately. More specifically, there was no false intervention for s2 thanks to the high HAR accuracy (See Table 2). On the other hand, there were false interventions for $11 and $14, which can be explained by a relatively lower HAR accuracy. Given this, the agent system could infer the risk of false interventions in the model training phase, which would help system designers configure the system.\nFigure 11 presents the relationship between the overall reliability and the behavioral intention answered by the participants. While a longer-term study is ideal, this shows initial evidence that such a task-support agent system is favorably accepted. The result also suggests a correlation between reliability and behavioral intention, which implies that reducing errors and achieving situated interventions is important."}, {"title": "6.5 User Comments", "content": "Finally, we summarized the qualitative insights from the semi-structured interviews after the task."}, {"title": "6.5.1 How Participants Selected Interventions", "content": "The number of participants who enabled each target step \u015d \u2208 \u015c in the step selection phase is 7, 6, 8, 9, and 5, for s2, S6, S8, $11, and $14, respectively. The individual difference shown in Table 3 reflects their preference. For instance, P7 mentioned, \"I never forget pouring oil when using the pan, so not including s2.\" P3 said, \"I thought I would be inattentive in the latter part of the procedure.\" P5 said, \"I'm experienced. I only need them if I make a mistake.\" P6 commented, \"I am a careless person. I would enable all notifications. I believe I would not be annoyed very much. It'd be like a fun assistant.\" In addition to these comments, they agreed that using different types of intervention made sense. P8 said, \"I don't need to be reminded every time to wash hands, so the system notifying me only if I make a mistake is a good design.\" The results underpin the design of our framework, enabling end-user customization of the intervention."}, {"title": "6.5.2 How Participants Perceived Interventions", "content": "Next", "I'm amazed by how it knows about my situation. The notification (for $14) was accurate.\" At the same time, a few participants mentioned a need for certain notifications to be more precise. P9 mentioned, \"For $11 (wiping the pan surface), it was after I already put my sausage, and thus I felt it is a bit inaccurate.\" P3 also suggested that it is safe to trigger interventions earlier as the steps can be irreversible in the cooking. Our framework is flexible enough to incorporate such adjustments, for example, by using a smaller K+ value to enable immediate detection or changing the type of the intervention into REMIND IN ADVANCE.\nSimilarly, regarding the NOTIFY IF FORGOTTEN intervention, P2, after experiencing FP (false positive), said, \u201cI was not annoyed much by the notification (s11). I just thought it was```json\n{\n      ": "itle"}, {"title": "6.5.3 Further Use Cases", "content": "Acknowledging the capability of the agent system, the participants shared an interest in extending the application. The examples include skin care (3 participants), gym/exercise routine (3 participants), furniture assembly (2 participants), medication (2 participants), house cleaning (2 participants), using a laundry room (1 participant), and using a maker space (1 participant). For example, P1 mentioned, \u201cI sometimes forget to sanitize the machine after I use it in the gym. Also, it would help me complete my exercise routine without forgetting an activity.\" Supporting these various needs is promising, given PrISM-Observer's generalizable performance suggested in Study 1."}, {"title": "6.5.4 Room for Improvement", "content": "Finally, we also gained insights about future improvement. P8 said, \"The system did not work well for me throughout the task. It was wrong from the beginning. I wanted to fix it by telling it.\" For this user, the system mistook the path P8 followed; P8 cooked the sausage first while the system guessed she cooked an egg first due to HAR error, resulting in overly early interventions. In such a case, as P8 expressed, offering a way for the user to correct the agent's belief will be ideal, which we discuss further later in Section 7.5.\nOn another note, P1 mentioned, \"I would like to see more variations in the audio message with different voice styles. If it were my favorite idol's voice, I might accept it even if the intervention is wrong.\" Such a social aspect between the task-support agent system and the user is an exciting research area [45]."}, {"title": "6.6 Summary", "content": "The results demonstrated the accuracy of our agent system's intervention, which the participants favorably accepted. The user comments highlighted the benefits of PrISM-Observer's design, such as the flexibility to cater to different user needs and to adjust parameters to enhance the user experience. We believe Study 2 produces insights into the human-AI interaction system powered by sensing technology in the physical world.\nIt should be noted that although seven participants completed the task in a sequence differing from that presented in the tutorial, we cannot rule out the possibility of a study-induced bias affecting task behavior. Acknowledging this limitation, we plan to undertake a longer-term study to assess the system's ability to adapt to spontaneous user behavior during tasks."}, {"title": "7 LIMITATIONS AND FUTURE WORK", "content": "The current formulation and implementation of PrISM-Observer is not without limitations. We also describe future work to advance the field further."}, {"title": "7.1 Long-Term Behavior Study", "content": "Study 2 is in a controlled setting and thus does not perfectly reflect the user behavior performing routine tasks under natural conditions, such as inattentiveness. A long-term real-world study is needed to further investigate the system's benefits and user perception. We want to emphasize that user behavior should be considered flexible instead of a fixed sequence for reasonable human-AI interaction so the system can adjust to spontaneous user behavior, such as switching the order of the steps. We believe PrISM-Observer's capability to incorporate such uncertainty in human behavior will be a fundamental solution.\nIn addition, a long-term study will also shed light on the agent's capability to learn from user behavior to adapt the intervention policy. For instance, if the user often forgets a certain step, the system switches the intervention from NOTIFY-IF-FORGOTTEN to REMIND-IN-ADVANCE. Conversely, if the user gets accustomed to the task enough and thinks REMIND-IN-ADVANCE is too much, it can be switched to NOTIFY-IF-FORGOTTEN. It is of interest to examine the optimal balance of the agent's adaptability and the end-user's controllability."}, {"title": "7.2 Health and Accessibility Applications", "content": "The proposed framework will be especially helpful for health applications to assist people in need. For example, people with dementia struggle to perform everyday routine tasks properly, and thus, interventions are helpful [24, 28]. Moreover, patients who need to perform self-care after surgery regularly could benefit from it to avoid the critical consequence of infection due to mistakes [30, 48]. We are currently developing and evaluating assistants for postoperative wound care with skin-cancer patients undergoing Mohs micrographic surgery [51]."}, {"title": "7.3 Detecting Errors within a Step", "content": "PrISM-Observer can detect and remind a user of a step in the procedure. Currently, it does not support dealing with more fine-grained errors, such as errors within a step, often necessitating visual information to be detected. For example, while steaming milk for the latte-making task, it is important to keep the jug at a proper angle to get a good milk texture, which the current system cannot sense. While aligning user expectations about the system's capability is crucial, we plan to investigate further sensing capability, such as applying multimodal anomaly detection [6] to compare the sensor data within the step with prior \"good\" behavior."}, {"title": "7.4 Refining Step Granularity", "content": "Automating and optimizing the process of dividing the procedure into steps is crucial to scaling our framework to diverse tasks. In this regard, Zhou et al. [57] proposed an approach to constructing an open-domain hierarchical knowledge base of procedures. Wake et al. [52] created a task model for decomposing human demonstration of procedural tasks for robots to model the process. In our case, it could be possible for the framework to use an automatically generated transition graph internally to model the user behavior and to trigger intervention. In contrast, the user specifies the desired intervention without being aware of the representation of the steps used by the system."}, {"title": "7.5 Interweaving Broader Interactions", "content": "This work focuses on passive interaction from the user's perspective; the system monitors user task behavior and proactively triggers intervention. Though our study demonstrated its effectiveness, the potential role of a task-support intelligent agent can be more versatile [23]. For instance, more dialogue between the user and the system could be an interesting area to study. PrISM-Tracker [8] showed a preliminary approach to update its tracking belief through dialogue. In the framework of PrISM-Observer, if the system could ask the user \"What are you doing?\", the system could resolve the uncertainty about the current step. Similarly, asking \"What will you do next?\" would resolve the uncertainty about the future step transition. Conversely, the user's reaction to the intervention could correct errors in the agent, as suggested in Section 6.5.4. Likewise, if there is a question-answering capability, the user's asking, \"What should I do after washing my hands?\" would also help the agent track the user's state. Given the surge of human-like chat capability enabled by large language models, investigating such real-time dialogue interactions along with sensor data is promising."}, {"title": "7.6 Extending to Other Sensing Platforms", "content": "We implemented our prototype on a smartwatch based on its capability to sense a user across various daily activities at different places. At the same time, the framework's stochastic modeling can be used as a post-process for other HAR systems. For example, VAX [37] uses ambient, privacy-sensitive sensors such as Doppler RADARS and LIDARs. We can install such a HAR system in the user's kitchen to monitor their everyday cooking and use PrISM-Observer on top of it. Existing voice assistants like Alexa could also be a platform for integrating our framework, leveraging acoustic sensing to obtain user context. In the context of voice assistant, while prior research [14, 16] has shown the benefit of dialogue-based guidance of steps, our work extends their capability by context awareness and user-centered intervention design, as recently emphasized by Jaber et al. [23]. Investigating different input sensors and their effect on end-user experience is an important future study."}, {"title": "8 CONCLUSION", "content": "We presented PrISM-Observer, a framework for designing interventions to mitigate errors in daily procedural tasks (e.g., forgetting a step), and developed a real-time agent system on a smartwatch leveraging multimodal Human Activity Recognition (HAR). The stochastic modeling of user behavior and intervention policy based on it enables situated triggering of interventions. Moreover, the framework is designed so users or system designers can customize which step they need intervention for and how. Study 1, involving three daily task datasets, verified the proposed approach's effectiveness in optimizing the intervention timing. In addition, Study 2, using the real-time smartwatch system in the cooking scenario, resulted in positive participant feedback, providing qualitative insights into such real-time intervention in procedural tasks. The results and discussion pave the way for achieving reliable human-agent interaction where the agent's sensing capability is limited, which is often the case in real-world tasks."}, {"title": "A PRISM-TRACKER'S PERFORMANCE", "content": "Figure 12 Figure 14 show the frame-level HAR confusion matrix of PrISM-Tracker [8] without and with the Viterbi correction, for the wound care, cooking, and latte-making task, respectively. Recall that one frame corresponds to 0.2 seconds. While the Viterbi correction improved the classification performance, inaccuracy remains, especially for steps with similar signal profiles: macro F1-Scores for the wound care, cooking, and latte-making tasks were 50.7%, 61.5%, and 52.9%, respectively."}, {"title": "B DETAILS OF INTERVENTION POLICY IMPLEMENTATION", "content": "The intervention policy described in Section 3.3.2 includes several hyperparameters. First, there is a step-dependent entropy threshold hs, which governs the timing to initiate an intervention timer. Then, after the intervention timer starts, the framework keeps monitoring E[D]. If there is a significant change within the next p = 10 seconds (before the timer ends), the timer is discarded, for which we use a threshold parameter e = 30 seconds. Moreover, as shown in Figure 4 Bottom, the entropy fluctuates due to the randomness in the Monte-Calro method, thereby necessitating the need for smoothing. We apply a moving average smoothing with the size of w = 2 seconds to the entropy. Parameters p, e, and w were empirically determined in the initial observation of D transitions of a few sessions. On the other hand, the entropy threshold hs was optimized through grid search in the process of the leave-one-session-out cross-validation.\nIn addition, for the NOTIFY IF FORGOTTEN intervention in the real-time system used for Study 2 (cooking), the system monitors the user action from the moment the timer started for E[D] + K+ seconds to judge whether the target step \u015d happens. Since frame-level prediction can fluctuate due to sensing noise, we apply a moving average smoothing with the size of 1 second, which corresponds to 5 frames. Moreover, the system judges the step \u015d happens if the probability for the step is highest for 5 seconds, which was also determined empirically based on the fact that every step in the cooking usually lasts more than 10 seconds (See Figure 5)."}]}