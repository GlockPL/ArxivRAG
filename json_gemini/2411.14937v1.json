{"title": "Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning", "authors": ["Junjie Shan", "Ziqi Zhao", "Jialin Lu", "Rui Zhang", "Siu Ming Yiu", "Ka-Ho Chow"], "abstract": "Foundation models that bridge vision and language have made significant progress, inspiring numerous life-enriching applications. However, their potential for misuse to introduce new threats remains largely unexplored. This paper reveals that vision-language models (VLMs) can be exploited to overcome longstanding limitations in gradient inversion attacks (GIAs) within federated learning (FL), where an FL server reconstructs private data samples from gradients shared by victim clients. Current GIAs face challenges in reconstructing high-resolution images, especially when the victim has a large local data batch. While focusing reconstruction on valuable samples rather than the entire batch is promising, existing methods lack the flexibility to allow attackers to specify their target data. In this paper, we introduce Geminio, the first approach to transform GIAs into semantically meaningful, targeted attacks. Geminio enables a brand new privacy attack experience: attackers can describe, in natural language, the types of data they consider valuable, and Geminio will prioritize reconstruction to focus on those high-value samples. This is achieved by leveraging a pretrained VLM to guide the optimization of a malicious global model that, when shared with and optimized by a victim, retains only gradients of samples that match the attacker-specified query. Extensive experiments demonstrate Geminio's effectiveness in pinpointing and reconstructing targeted samples, with high success rates across complex datasets under FL and large batch sizes and showing resilience against existing defenses.", "sections": [{"title": "1. Introduction", "content": "Federated learning (FL) is a privacy-enhancing technology for training machine learning models on data distributed across multiple clients [23, 34]. By enabling clients to share gradients rather than raw data with a coordinating server, FL has demonstrated transformative potential in privacy-sensitive domains [13, 22, 45]. However, FL is vulnerable to various malicious attacks, with gradient inversion attacks (GIAs) posing a particularly critical threat [19, 43]. These attacks enable a malicious FL server to reconstruct private data samples from the gradients shared by a victim client, leading to privacy breaches and decelerating FL's adoption.\nGIAs face a longstanding challenge: they can only reconstruct images from gradients produced by a small batch of data [10, 31]. This limitation exists because GIAs rely on searching for data that reproduces the victim-submitted gradients, and the search space expands exponentially with batch size. Thus, much research has focused on whether this limitation is fundamental, as existing GIAs struggle with practical batch sizes. While some approaches incorporate image priors (e.g., spatial smoothness [14]) to facilitate the search, a performance gap still remains, as illustrated in Figure 1(a), with images reconstructed from a batch of 128 samples. Another direction has been to narrow the scope to reconstruct only a subset of samples. While promising, existing methods lack a semantically meaningful way for the adversary to specify which samples are preferred and can only target, e.g., outliers [35] or images with particular brightness levels [11]. This raises an intriguing question: can reconstruction efforts be prioritized toward the data samples that truly matter most to the adversary? If so, how can we allow the adversary to specify their preferences in a meaningful, flexible, and generic way?\nIn this paper, we empower gradient inversion attacks with a natural language interface and propose Geminio. It enables the FL server to provide a natural language query describing the data of interest, allowing Geminio to prioritize and reconstruct matching data samples. Taking the batch of images from a victim's mobile phone in Figure 1 as an example, the adversary could submit queries like (b) \u201cshow me some faces\u201d to retrieve images containing faces to see the victim or their friends, (c) \"what vehicles are included?\" to identify cars associated with the victim, or (d) \u201cany weapon?\u201d to detect if the victim owns a weapon. The query does not need to relate to the FL system's ML task. By prioritizing reconstruction efforts, Geminio can pinpoint and retrieve targeted samples from large batches, offering high flexibility in defining valuable data. This capability is achieved by misusing pretrained vision-language models (VLMs) [25] to help craft a malicious global model. When shared and optimized by the victim client, gradients become dominated by samples that match the query. Existing reconstruction optimization algorithms [14, 39, 40, 49] can consume such gradients to recover high-quality, targeted data.\nOur main contributions are summarized as follows. First, we explore the misuse of pretrained VLMs to bridge the gap in gradient inversion, enabling semantically meaningful, targeted attacks. We investigate the first natural language interface for the adversary to describe the data samples that truly matter and prioritize them for reconstruction. Second, we propose Geminio, which exploits a VLM to reshape the loss surface of a global model, so that once optimized locally by the victim, the gradients are dominated by the samples matching the query. This method complements existing reconstruction optimizations and can augment them as targeted attacks. Third, we reveal the limitations of current defenses, discuss potential design improvements, and highlight their shortcomings to motivate future work. Experiments were conducted across three datasets, five attack methods, and four defense mechanisms, and various configurations to assess the threat posed by Geminio. The source code of Geminio is available at https://github.com/HKU-TASR/Geminio."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Gradient Inversion Attacks in FL", "content": "Federated Learning. Let \\(F_\\theta\\) be the ML model trained via FL with a loss function \\(\\mathcal{L}\\). At each learning round \\(t\\), the FL server sends the current global model parameters \\(\\Theta_t\\) to FL clients. Under the FedSGD protocol [23], each client \\(i\\) samples a data batch \\(B\\), having pairs of input \\(x\\) and label \\(y\\), from its private dataset to optimize the received model and submit the gradients\n\\[G(B; \\Theta_t) = \\frac{1}{|B|} \\sum_{(x, y) \\in B} \\nabla_{\\Theta_t} \\mathcal{L}(F_{\\Theta_t}(x); y) \\quad (1)\\]\nto the server. Then, the server aggregates the gradients submitted by all clients to update the global model parameters for the next round. The FL protocol has different variations, such as randomly selecting a subset of clients to participate in each round or running several data batches locally before submitting the gradients to the server [23].\nGradient Inversion Attacks. The FL server that receives the gradients \\(G(B; \\Theta_t)\\) from a participating client \\(i\\) at round \\(t\\) can reconstruct the private data batch \\(B\\) via gradient inversion attacks. As the batch size should be transparent to the FL server for proper aggregation, it can randomly initialize a batch of data samples \\(B'\\) and use the global model parameters \\(\\Theta_t\\) shared with the victim at the beginning of the learning round for reconstruction optimization:\n\\[B^* = \\underset{B'}{\\operatorname{argmin}} \\left[ \\delta(G(B'; \\Theta_t), G(B; \\Theta_t)) + \\mathcal{R}(B') \\right], \\quad (2)\\]\nwhere \\(\\delta\\) is a distance function that measures the dissimilarity between two sets of gradients, and \\(\\mathcal{R}\\) is a regularization function. The overarching idea is to optimize those random data samples in such a way that they can reproduce the gradients shared by the victim client."}, {"title": "2.2. Related Work", "content": "Early researches leverage the inherent shallow leakage in fully connected layers [47, 49] to reconstruct private data from gradient updates. Recent studies advance optimization and analytic methods to improve the accuracy and scalability of these attacks, but the underlying objective remaines the same, which is to extract private information from gradients or model parameters [9, 10, 24, 27, 36\u201338].\nReconstruction Optimization. The reconstruction optimization in Equation 2 has been the primary focus of GIA advancements. These improvements target (i) enhanced distance functions, such as Euclidean distance [49] and cosine similarity [14, 39] to measure gradient alignment, and (ii) refined regularization methods to incorporate prior knowledge like spatial smoothness [14, 40]. Such approaches have successfully extended GIAs to support high-resolution image reconstructions, previously limited to toy datasets like MNIST [49]. However, these methods attempt to recover the entire private batch, struggling with practical batch sizes due to the vast search space involved [26].\nNarrowing the Reconstruction Scope. Recognizing this limitation, recent work has explored reconstructing only a"}, {"title": "2.3. Threat Model", "content": "Consistent with prior studies [35, 44], we consider an FL server acting as an active adversary who (i) can modify the model parameters before sharing them with FL clients but not altering the neural architecture, (ii) can read the gradients submitted by a victim client and attempt to reconstruct private data samples from them, (iii) can provide a natural language description of the characteristics of data it deems valuable, and (iv) possesses an auxiliary, unlabeled image dataset that may originate from a completely different domain (e.g., public datasets like ImageNet [4] or images scraped from the Internet). The FL clients adhere to the FedSGD protocol, optimizing the received model with a batch of private data. We will consider other FL scenarios in Section 4, such as Geminio under FedAvg [23], as well as client-side defenses like gradient obfuscation and model parameter inspection."}, {"title": "3. Methodology", "content": "Figure 2 gives an overview of Geminio. It consists of two phases. During the preparation phase, Geminio takes a query Q (e.g., \u201cshow me all valuables\u201d) from the adversary to craft malicious global model parameters \\(\\Theta_\\Omega\\). During the attack phase, those parameters that are pretended to be legitimate will be shared with the victim client, who optimizes them with its private data batch \\(B\\) and uploads the gradients \\(G(B; \\Theta_\\Omega)\\) (from Equation 1) to the FL server. Then, any existing reconstruction optimization method can be directly applied to recover those private samples relevant to the query (e.g., the necklace and the fountain pen retrieved by InvertingGrad [14]).\nThe overarching idea of Geminio is to craft a malicious global model such that those private samples in the victim's data batch matching the query will dominate the"}, {"title": "3.1. VLM-Guided Loss Surface Reshaping", "content": "Given an auxiliary dataset \\(A\\), Geminio exploits a pretrained VLM to measure the similarity between each auxiliary image and the query. The top 3D surface plot in Figure 2 shows the similarity surface as a function of auxiliary images (projected onto a 2D space by PCA). Some images align with the query well, while others have close-to-zero relatedness. An untrained global model has a roughly flat loss surface (see the bottom 3D surface plot in Figure 2). We need to train the malicious global model to have a loss surface matching the aforementioned similarity surface such that those irrelevant samples will lead to a zero loss and matched ones will dominate.\nA VLM comprises two components [25]: an image encoder \\(V_{\\text{image}}\\) and a text encoder \\(V_{\\text{text}}\\). They can project image and text data onto a latent space that those similar will collocate. For an auxiliary sample \\((x, y) \\in A\\), we can calculate its similarity with the query \\(Q\\): \\(s(x; Q) = V_{\\text{image}}(x)^T V_{\\text{text}}(Q)\\). Based on the similarity score, we propose to train the malicious global model parameters \\(\\Theta_\\Omega\\) with the following routine. At each iteration, we sample a batch of auxiliary data \\(B_{\\text{aux}} \\subset A\\) and calculate the probability of each auxiliary image \\((x, y) \\in B_{\\text{aux}}\\) being aligned with the query, normalized across the batch via a softmax function:\n\\[\\alpha(x; Q, B_{\\text{aux}}) = \\frac{\\exp(s(x; Q))}{\\sum_{(x', y') \\in B_{\\text{aux}}} \\exp(s(x'; Q))}\\quad (3)\\]\nThe batch-wise normalization offers information about how one sample is more aligned with the query than another sample. Then, we can train the malicious global model parameters by minimizing\n\\[\\begin{aligned} \\mathcal{L}_{\\text{Geminio}} (B_{\\text{aux}}; F_{\\Theta}, Q) = \\sum_{(x, y) \\in B_{\\text{aux}}} \\mathcal{L}(F_{\\Theta_\\Omega}(x); y) (1 - \\alpha(x; Q, B_{\\text{aux}})) - \\sum_{(x', y') \\notin B_{\\text{aux}}} \\mathcal{L}(F_{\\Theta_\\Omega}(x'); y') (1 - \\alpha(x'; Q, B_{\\text{aux}})) \\frac{}{|B_{\\text{aux}}|} \\end{aligned}\\quad (4)\\]\nIntuitively, each per-sample loss is associated with a scaling factor (the coefficient). For an auxiliary sample that has a strong alignment with the query, the corresponding term will be negligible since the coefficient \\((1 - \\alpha(x; Q, B_{\\text{aux}}))\\) is close to zero. In contrast, the term corresponding to an irrelevant auxiliary sample will have its magnitude amplified because of the large coefficient. In order for the malicious model to minimize Equation 4, it must learn to reduce the per-sample loss value of such irrelevant samples. The batch-wise normalization will then increase the per-sample loss value of matched samples. Geminio's training routine reshapes the loss surface, originally flat (Figure 3a), to have an active response only for those matched samples (Figure 3b)."}, {"title": "3.2. VLM-Guided Auxiliary Label Generation", "content": "The calculation of the per-sample loss value requires the ground-truth label of that input. However, assuming the availability of such a labeled dataset in FL is unreasonable. We propose to misuse the pretrained VLM again to launch Geminio with an unlabeled, possibly off-domain, dataset. In particular, let the class names in a \\(K\\)-class classification problem be \\([C_1, C_2, ..., C_K]\\), we can generate a soft label for each auxiliary sample \\(x\\) by measuring the similarity of its image features and the text features of each class name. Formally, the soft label \\(\\mathbf{y} = [y_1, y_2, ..., y_K]\\) is a probability distribution, where \\(y_i\\) represents the probability of \\(x\\) being classified as class \\(c_i\\) and can be calculated by\n\\[y_i = \\frac{V_{\\text{image}}(x)^T V_{\\text{text}}(C_i)}{\\sum_{j=1}^K V_{\\text{image}}(x)^T V_{\\text{text}}(C_j)}\\quad (5)\\]"}, {"title": "4. Empirical Evaluation", "content": "We conduct extensive experiments to analyze Geminio's broad applicability to different datasets (ImageNet [4], CIFAR-20 [21], and FER [6]), different neural architectures (ResNet [16], MobileNet [18], EfficientNet [28], and ViT [5]), and different FL scenarios (FedSGD and FedAvg). CIFAR-20 is equivalent to CIFAR-100 but uses 20 super-classes as labels. It provides ground truths to evaluate Geminio's task-agnostic targeted retrieval quantitatively.\nBy default, we consider an FL system that trains a ResNet34 model using FedSGD as the protocol with a batch size of 64. For Geminio, we use the pretrained CLIP [25] to guide the optimization. The gradients are consumed by InvertingGrad [14] to reconstruct private samples. Detailed setup and the source code are provided in the supplementary materials to facilitate further research and reproducibility.\nOutline. With additional analysis provided in the supplementary material, we would like to deliver three messages via the empirical studies in this section:\n\u2022 The attacker can freely describe the data valuable to them and \"query\" the victim's dataset for targeted reconstruction from a large batch of data. (Section 4.1)\n\u2022 Geminio serves as a plugin to existing reconstruction optimization methods and is broadly applicable, even with limited access to auxiliary data. (Section 4.2)\n\u2022 Geminio has a high survivability under various FL and defense scenarios. (Section 4.3)"}, {"title": "4.1. Task-agnostic, Targeted Reconstruction", "content": "Qualitative Analysis. To showcase Geminio's targeted retrieval, Figure 4 provides two example batches of the victim's private data (top) from different datasets and the corresponding reconstructed images (bottom) for three cases: reconstruction with the vanilla GIA (1st row) and reconstruction with Geminio given two different queries (2nd and 3rd rows). First, while the vanilla GIA cannot produce recognizable images due to its failure to handle a large batch, Geminio narrows the reconstruction scope to the data samples that matter most and successfully recovers them with high fidelity. Second, the recovered images match the attacker-provided queries. For instance, a curious attacker may submit a query \u201cAny weapon?\u201d to understand whether the client is, e.g., a weapon enthusiast. Among the 64 images on ImageNet (Figure 4a), only the first three contain weapons and are all successfully reconstructed. Similarly, considering the query \"Person with beard and glasses,\u201d while the first five images on FER (Figure 4b) contain a person wearing glasses, only the first two are reconstructed, as the rest do not have a beard. Third, queries can be irrelevant to the ML task. FER classifies facial images into one of the seven emotion expressions (e.g., happy, sad). Even though our example queries describe the appearance of individuals, the targeted reconstructions are successful.\nComparison with Existing Methods. Geminio's targeted reconstruction is unique and not achievable by existing methods. Figure 5 shows another batch of private data with 32 images of the class \"Sombrero.\" Imagine that an attacker wants to recover images that contain human faces as a privacy-intrusive example. As shown in the first column (2nd row), Geminio successfully recovers the first two images in the victim's private data (1st row). The reconstructed images clearly reveal the facial features of the people with whom the client may interact. In contrast, other methods attempting to narrow the reconstruction scope cannot achieve the same goal. Fishing [35] can only return one random sample of a given class; GradFilt [44] returns all samples of a given class; SEER [12], LOKI [48], and Abandon [1] can only be random or specify semantic-irrelevant conditions (e.g., the brightness level). It is worth emphasizing that Geminio is instance-level. The matched data samples can belong to different classes. It is the only solution that achieves such a fine granularity."}, {"title": "Quantitative Analysis.", "content": "Geminio can pinpoint and reconstruct valuable data samples from a large batch. Figure 6 reports the attack recall and precision on CIFAR-20 over different batch sizes used by the victim. Aligned with evaluating an information retrieval system, the attack recall indicates the percentage of data samples matching the query being retrieved (recovered), while the attack precision refers to the percentage of recovered data samples that indeed match the query. We split the entire training set of CIFAR-20 (50, 000 images) into batches. For each of the 100 subclasses in the dataset, we use its name as the query to attack all batches, measure the attack recall and precision, and report their average across 100 subclasses. Following Fishing, we consider a data sample successfully reconstructed if its output-layer gradients dominate the batch-averaged gradients with a cosine similarity of at least 0.90. Figure 6 shows that Geminio remains effective even when the victim uses a large batch size, such as 256, with an attack recall of 64.96% and precision of 65.67%. Note that the malicious model was trained with a batch size of 64. We also compare Geminio with the baseline approach that uses a VLM to find data samples in the auxiliary dataset that match the query and poison their labels to increase their loss and gradients. As shown in Figure 6 (orange), it cannot provide a meaningful attack unless the batch size is extremely small (e.g., 2). This baseline demonstrates the effectiveness of Geminio in reshaping the loss surface."}, {"title": "4.2. Serving as a Plugin with Broad Applicability", "content": "Complementary to Reconstruction Optimization. Geminio can turn existing reconstruction optimization methods into targeted attacks. In addition to InvertingGrad (the default), we use DLG [49] to reconstruct the victim's local batch in Figure 4a using the query \"Any weapon?\". Figure 7 compares the two reconstruction techniques with and without Geminio's enhancement. We use the standard metric, LPIPS [42], to understand how well the reconstructed images match the ground truths. A lower score means a higher reconstruction quality. We also provide the reconstructed images closest to the handgun (i.e., the 1st image in the batch) as a visual reference. We can observe that Geminio-enhanced attacks are consistently much better than their vanilla counterparts, which cannot recover any recognizable images. An interesting observation is that while DLG is known to be incapable of recovering from large batches and high-resolution images (64 \u00d7 64 as reported in the original paper), it can recover the handgun image well with a resolution of 224 \u00d7 224 from a large batch. We conjecture that the gradient amplification in Geminio increases their variance, which will make the gradient matching during the reconstruction easier. We observe this phenomenon even for a batch of just one image.\nAuxiliary Data. Figure 8 reports the attack F-1 score on CIFAR-20 using different auxiliary datasets. Compared with the default setting with the number of data samples equivalent to 20% of the training dataset, using only 5% of it only leads to a small drop in attack F-1 score, from 68.13% to 60.37%. Alternatively, the attacker can also use a different dataset, such as ImageNet or Caltech256. Even though they are not for the same ML task, the attack F-1 score can still achieve 50.48% and 65.37%, respectively. These datasets are publicly available and can be a practical source of auxiliary data.\nNeural Architectures. Geminio can attack any neural architecture out of the box. Unlike many targeted attacks that need to inject a malicious module into the architecture, Geminio only modifies the model parameters in a stealthy manner. We conduct experiments to understand how it performs when different architectures are used in the FL system. According to the attack F-1 score reported in Figure 9, we observe that while Geminio works well on different architectures, the effectiveness slightly differs. It is more effective on ViT and EfficientNetV2 than ResNet34 and MobileNetV3. Interestingly, this particular order reflects the general capability of these models. Hence, we conjecture that for more capable neural architectures, their privacy leakage by Geminio will be more severe."}, {"title": "4.3. Resilience to FedAvg and Defenses", "content": "While resilience to defenses is not the primary goal for Geminio, we found it to be resistant to popular methods.\nFederated Averaging. Geminio can survive under FedAvg. Consider a victim having 256 ImageNet images in the private dataset as shown in Figure 10a (left). The victim uses a batch size of 8 and runs one epoch of training before sending the model updates to the server for aggregation. We employ Geminio using a query \u201cStorage for valuables\u201d to simulate a scenario where the attacker wants to know how the client stores the valuables. As shown in Figure 10a (right), it successfully recovers the image of a safe with high fidelity, even detailed enough to identify the specifics of it. The key enabler is to assign a small learning rate to the FL client, which is often the responsibility of the FL server. Figure 10b reports the attack F-1 score with different learning rates assigned to the victim. More local epochs weaken the attack because each iteration modifies the model parameters and may wash out the malicious patterns introduced by Geminio. Setting a small learning rate (e.g., le-6) can slow down the performance degradation effectively.\nGradient Pruning. A popular defense to prune gradients of small magnitudes. Figure 11 reports the reconstruction quality on CIFAR-20 with varying pruning ratios. We reconstruct 100 batches and measure the average LPIPS. We observe that even with 95% of small gradients being set to zero, the perceptual quality of reconstructed images is still comparable to no defense. The reconstructed images become barely perceptible when 99% of the gradients are zeroed. In practice, such a setting is prohibited because it also removes the useful learning signals for training the ML model. Hence, gradient pruning cannot mitigate Geminio.\nLaplacian Noise. Another popular defense is to add Laplacian noise to gradients. Figure 12 reports the reconstruction quality on CIFAR-20 with varying scales of Laplacian noise. Following [39], we use a per-layer noise injection. At each layer, we obtain its maximum gradient and scale it by a factor to be the standard deviation of the Laplacian noise with a zero mean for injection. A scale of 0.10 is already considered significant, but it barely affects the perceptual quality of reconstructed images. The reconstruction becomes severely affected when the noise scale is increased to 0.50, but it also washes out useful learning signals. Hence, injecting noise is not a viable defense against Geminio.\nModel Parameter Inspection. As the client regularly receives model parameters from the server, it is natural to inspect whether they contain anomalies as a detection mechanism. Figure 13 reports the maximum magnitude of model parameters of a clean model and three poisoned models by Fishing, GradFilt, and Geminio. We observe that Fishing and GradFilt send a model with parameters deviating significantly from the clean one (2772.89 and 1000, respectively). In contrast, Geminio is only 1.64, close to the clean model (i.e., 0.35). Hence, setting a threshold may be able to detect Fishing and GradFilt, but not Geminio.\nPer-sample Loss Inspection. The FL client may analyze the loss value per sample at each local training iteration. We use the batch in Figure 5 and show the loss magnitude for each of the first 8 samples. All three attacks introduce a high loss value to the targeted samples. For Fishing, it successfully isolates the 6th sample in the batch, causing its loss to be significantly higher than the others. For GradFilt, since all samples in this batch are of the same target class (i.e.,", "human faces": "have amplified loss while the rest remains small. These are expected behaviors because targeted GIAs use the same principle: magnifying the gradients of desired samples to make them dominate the average gradients. While loss inspection seems promising, an advanced adversary could conduct an adaptive attack to suppress the loss values when training the malicious model (see Geminio-adaptive in Figure 5). Hence, more robust defenses need to be developed as future work."}, {"title": "5. Conclusions", "content": "We have introduced Geminio, a gradient inversion attack that harnesses the image-text association capabilities of pretrained VLMs to enable language-guided targeted reconstructions. Our extensive experiments have yielded three key insights. First, Geminio enables the attacker to provide a natural language query to describe the data of value and reconstructs those matched samples from large data batches. Second, it serves as a plugin to enhance existing reconstruction optimization methods, broadly applicable to different neural architectures, auxiliary datasets, and FL protocols. Third, existing defenses are insufficient to mitigate Geminio. An advanced attacker can adapt Geminio to harden loss inspection. We believe that Geminio will inspire further research into the new threats posed by recent advancements in natural language processing, as they can be exploited as a \"communication\" interface for the adversary to express their goals and launch more flexible attacks."}, {"title": "A. Geminio Strengthens Label Inference Attacks", "content": "Label inference is a prerequisite for gradient inversion, with various attack methods being proposed [29, 40, 47]. Surprisingly, Geminio is not just compatible with them but also boosts their accuracy. We use five label inference attacks provided by the breaching library [20] and compare the original attack with the Geminio-enhanced one. Since our problem setting focuses on targeted reconstructions, we only need to make sure the class labels with matched samples in the local batch are inferred. The success or failure of inferring other class labels is unimportant because their gradients are small and negligible in the gradient matching (reconstruction optimization) process. Figure 15 reports the results measured on CIFAR-20. This dataset provides ground truths for conducting such quantitative studies. When gradients submitted by the victim are generated based on the Geminio-poisoned malicious model, all label inference attacks are consistently improved. This phenomenon can be explained by our observation in Figure 16 that the class labels containing matched samples in the local batch have their gradients amplified. Since those attacks share the same principle to examine the gradient magnitude of different classes, Geminio facilitates this label inference process."}, {"title": "B. Geminio Works Under Homomorphic Encryption", "content": "Our threat model considers an active attacker who is the FL server. The attacker can execute Geminio under homomorphic encryption by controlling only one client. As the malicious client can obtain the victim's gradients in plain text, Geminio can be run on the client side and perform identically to FL without homomorphic encryption. Figure 17 provides reconstruction results with \u201cluxury watches\u201d as the attacker's query. The two watches can be retrieved from the victim's gradients, leading to a high-quality reconstruction where we can even read the brand for the first image to be Rolex."}, {"title": "C. Geminio Supports Different Local Batch Sizes", "content": "During Geminio's optimization, minibatch training needs to be conducted but this training batch size does not need to match the local batch size used by the client. Figure 18 reports the attack recall with varying local batch sizes. We repeat the experiment using different training batch sizes for Geminio to optimize the malicious global model. We observe that their targeted retrieval performances are similar, with the smallest batch size of 8 being slightly worse. For instance, when Geminio uses a batch size of 64 for its optimization, the malicious global model can be sent to clients with any local batch size, which may or may not be controlled by the server (e.g., depending on the computing resources of the client device)."}, {"title": "D. Experiment Setup", "content": "Our experiments cover a wide range of datasets, ML models, and FL scenarios to analyze Geminio's properties. Here, we describe the default experiment setup."}, {"title": "D.1. Datasets", "content": "We conduct experiments on three datasets: ImageNet [4], CIFAR-20 [17], and Facial Expression Recognition (FER) [7]. By default, visual examples are based on ImageNet.\nThe scenario of fine-grained targeted retrieval by Geminio can be imagined as an attacker writing a \"query\" to search for relevant records in the victim's private database. Quantitative evaluation requires two ingredients: (i) a benchmark dataset with ground truths and (ii) a set of indicative performance metrics.\nBenchmark: CIFAR-20 The benchmark dataset should include a set of queries, each is a textual description and associated with a list of relevant images. Then, we can randomly sample a local batch from the dataset, use Geminio to reconstruct images given different queries, and measure how many relevant images are successfully reconstructed. This process repeats for a number of random local batches until, e.g., all training images are processed. To showcase instance-level retrieval better, the queries should not be the class names of the classification problem. Based on these requirements, we created a variant of CIFAR-100 and named it CIFAR-20. Each image in CIFAR-100 is associated with two official labels, a subclass and a superclass (see Table 1 for four superclasses and their subclasses). We use the 20 superclasses for the classification problem and the 100 subclasses as queries. With this design, we can easily obtain images in the local batch that should be retrieved for a given query (i.e., a subclass name).\nMetrics: Attack Recall and Precision We follow Fishing's approach [35] to determine whether an image in a local batch dominates and will be reconstructed. In particular, if the gradients produced by an image have a cosine similarity with the average gradients over a threshold, it is considered a reconstructed sample. While Fishing uses 0.95 as the threshold, we found that this is overly restrictive. Instead, we use 0.90. Note that we observe multiple examples where targeted reconstruction succeeds even if the cosine similarity is below 0.90. Our choice (i.e., 0.90) is still conservative. A more principled approach is considered as our future work. Based on this thresholding, we can measure the percentage of targeted images being reconstructed (i.e., Attack Recall) and, among all reconstructed images, the percentage of them being the actual targeted images (i.e., Attack Precision)."}, {"title": "D.2. FL Configuration", "content": "The FL system aims to train a ResNet34 [16] model. Following existing works [12, 14, 35, 40, 44, 48, 49], we use FedSGD to be the default protocol. The FL client receives a model from the server, updates it with a batch of private samples, and returns the gradients to the server, which is malicious, and attempts to reconstruct private samples from it."}, {"title": "D.3. Attack Configuration", "content": "For Geminio, we use CLIP [25] with the ViT-L/14 Transformer architecture as the pretrained VLM\u00b2 to process auxiliary data, which comes from the respective validation set. Geminio poisons the model with a training batch size 64 using Adam as the optimizer. For gradient inversion, we use InvertingGrad [14]."}, {"title": "D.4. Computing Environment", "content": "All experiments are conducted on a server with Intel\u00ae Xeon\u00ae Gold 6526Y CPU, 64GB RAM, and two NVIDIA RTX 5880 Ada Lovelace GPUs."}, {"title": "D.5. Implementation", "content": "Geminio is written in PyTorch and can be easily integrated into existing GIAs. Our implementation uses breaching [20], a collection of GIAs, to demonstrate such a plug-and-play feature. We first extracted image features from auxiliary data, which took about 7 minutes for ImageNet. Given a query from the attacker, Geminio can use those pre-generated image features to poison the model in less than 8 minutes."}, {"title": "E. Additional Visual Examples", "content": "We provide additional visual examples in Figure 19. Geminio can prioritize reconstruction to recover those samples that match the attacker-provided queries. For the first local batch (left), the query \"child\" leads to the reconstruction of the 1st image, while the query \"jewelry\u201d to the same batch recovers the necklace (i.e., the 2nd image). Similarly, for the second local batch (right), the green car (i.e., the 1st image) will be recovered if the attacker provides \u201cpersonal vehicle\" as the query. However, if the query is \"phone screen\" instead, the"}]}