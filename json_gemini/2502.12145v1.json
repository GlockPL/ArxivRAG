{"title": "Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control", "authors": ["Jinyan Su", "Jennifer Healey", "Preslav Nakov", "Claire Cardie"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations by incorporating external knowledge retrieval. However, existing RAG frameworks often apply retrieval indiscriminately,leading to inefficiencies-overretrieving when unnecessary or failing to retrieve iteratively when required for complex reasoning. Recent adaptive retrieval strategies, though adaptively navigates these retrieval strategies, predict only based on query complexity and lacks user-driven flexibility, making them infeasible for diverse user application needs. In this paper, we introduce a novel user-controllable RAG framework that enables dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two classifiers: one trained to prioritize accuracy and another to prioritize retrieval efficiency. Via an interpretable control parameter a, users can seamlessly navigate between minimal-cost retrieval and high-accuracy retrieval based on their specific requirements. We empirically demonstrate that our approach effectively balances accuracy, retrieval cost, and user controllability 1, making it a practical and adaptable solution for real-world applications.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) Lewis et al. (2020); Khandelwal et al. (2019); Izacard et al. (2023) has emerged as a promising approach to address large language models (LLM) hallucinations, outputs that appear accurate but are actually factually incorrect. Hallucinations more often occur when the facts necessary to answer a query are outside the scope of the model's training either because the facts are too recent, too obscure or answered by proprietary data Trivedi et al. (2022a); Yao et al. (2022). By integrating retrieval modules, RAG enables LLMs to access and incorporate relevant external information, helping them stay updated with evolving world knowledge and reduce hallucination.\nEarly work on retrieval-augmented LLMs primarily focused on single-hop queries Lazaridou et al. (2022); Ram et al. (2023), where a single retrieval step retrieves relevant information based solely on the query. However, many complex queries require multi-step reasoning to arrive at the correct answer. For instance, a question such as, \"When did the people who captured Malakoff come to the region where Philipsburg is located?\u201d, cannot be answered with a single retrieval step. Instead, such queries necessitate an iterative retrieval process, where the model retrieves partial knowledge, reasons over it, and conducts additional retrievals based on intermediate conclusions Trivedi et al. (2022a); Press et al. (2022); Jeong et al. (2024). While multi-step retrieval enables deeper reasoning, it also incurs significant computational overhead. Conversely, for simple queries, LLMs may already encode sufficient knowledge to generate an accurate response without retrieval. However, many existing RAG frameworks apply retrieval indiscriminately, either over-relying on external knowledge when parametric memory suffices or failing to retrieve iteratively when deeper reasoning is required. This lack of adaptability results in inefficiencies: unnecessary retrieval increases latency and computational cost, while insufficient retrieval leads to incomplete"}, {"title": "Relative Work", "content": "Recent research on adaptive retrieval has explored various strategies to determine when and how retrieval should be performed, often leveraging internal model states or external classifiers to optimize retrieval decisions.\nSeveral works focus on leveraging LLM internal states to adapt retrieval dynamically. Baek et al. (2024) utilize hidden state representations from intermediate layers of language models to determine whether additional retrieval is necessary for a given query. Similarly, Yao et al. (2024) extract self-aware uncertainty from LLMs' internal states to decide when retrieval should be performed. Jiang et al. (2023) propose an iterative retrieval approach, where the model predicts the upcoming sentence and uses it as a retrieval query if the sentence contains low-confidence tokens. However, these methods require access to the LLM's internal states, making them impractical for many real-world applications, especially given the increasing reliance on proprietary, closed-source LLMs.\nAnother line of work trains external classifiers to predict retrieval necessity. Wang et al. (2023) train a classifier for LLM self-knowledge, allowing models to switch adaptively between retrieval and direct response generation. Jeong et al. (2024) introduce an adaptive retrieval framework that dynamically selects among no retrieval, single-step retrieval, or multi-step retrieval based on query complexity. Tang et al. (2024) propose a multi-arm bandit-based approach, where the model explores different retrieval strategies and optimizes retrieval choices based on feedback. Wang et al. (2024) develop an adaptive retrieval framework for conversational settings, determining whether retrieval is necessary based on dialogue context. Mallen et al. (2023) take an entity-centric approach, retrieving only when the entity popularity in a query falls below a certain threshold.\nDespite these advancements, none of these approaches enable direct user control over retrieval strategies."}, {"title": "Problem Setting", "content": "We begin by introducing the varying complexity of user queries and their corresponding retrieval strategies. As illustrated in Figure 1, different levels of query complexity necessitate distinct retrieval approaches. Specifically, we consider three primary retrieval strategies: no"}, {"title": "Diverse User Query Complexities and Retrieval Strategies", "content": "retrieval, single-step retrieval, and multi-step retrieval, which correspond to zero, one, and multiple retrieval steps, respectively. These strategies align with the availability of relevant knowledge within the model's parameters and the extent to which retrieval and reasoning must interact to generate an answer."}, {"title": "Parametric Knowledge Queries & No Retrieval", "content": "Some user queries are considered simple by LLMs because they can be directly answered using their parametric knowledge-information stored within the model's parameters (Izacard et al., 2023). Since LLMs memorize widely known factual knowledge during pretraining, they can respond to such queries without relying on external retrieval. Scaling further enhances this memorization capability, improving the accuracy of responses to popular factual questions (Mallen et al., 2023). For example, queries such as \"Who wrote Romeo and Juliet?\" or \"What is the capital of France?\" can be answered accurately using only the model's internal representations. For these retrieval-free queries, retrieval augmentation is unnecessary because the model already encodes the relevant knowledge. Introducing retrieval in such cases may be redundant or even counterproductive\u2014it can increase latency, add computational cost, or introduce noise from irrelevant retrieved documents (Su et al., 2024). Consequently, for these parametric knowledge queries that are answerable by memorized knowledge, such as well-known facts or frequently encountered knowledge, it is more efficient to use a no-retrieval strategy, allowing the model to generate responses and answer directly."}, {"title": "Knowledge-Deficient Queries & Single-Step Retrieval", "content": "Some queries fall outside an LLM's parametric knowledge, such as less popular, long-tail factual knowledge that was not sufficiently represented in pretraining data(Mallen et al., 2023); time-sensitive information such as recent events and evolving regulations, or highly specialized domain knowledge that is uncommon in general corpora. For instance, a query like \"Who won the Nobel Prize in Physics in 2023?\u201d necessitates retrieval for models with a training cut-off date of 2022, as the event occurred afterward and is unlikely to be encoded in the model's parameters. Similarly, domain-specific questions such as \"What are the latest FDA guidelines for AI-based medical devices?\u201d may not be well covered in publicly available pretraining datasets, requiring retrieval from authoritative sources to provide an accurate and up-to-date response. We refer to these queries as Knowledge Deficient Queries, where retrieval is essential to supplement missing knowledge, but complex reasoning is not required. In these cases, retrieval serves as a direct knowledge lookup, providing factual information that the model can integrate into its response without needing complex reasoning or inference. Once the relevant information is retrieved, the model can directly generate an accurate answer."}, {"title": "Intertwined Retrieval-Reasoning Queries & Multi-Step Retrieval", "content": "Some complex queries require both retrieval and reasoning in an interdependent manner, where the model must retrieve information iteratively while reasoning at each step to determine the next retrieval. Unlike knowledge Deficient queries, where passively consuming content from the single retrieval step is sufficient to reach the correct answer, Intertwined Retrieval-Reasoning Queries demand both factual knowledge and the model's ability to reason, plan, infer, and integrate retrieved facts to iteratively refine its understanding and construct the final correct answer. For example, in response to the query \"Who was the US president when the inventor of the telephone was born?\u201d, the model must first recognize that additional information is needed about the inventor of the telephone. This reasoning step directs the retrieval process, leading the model to retrieve the name Alexander Graham Bell and his birth year. Once retrieved, this new information updates the model's understanding, allowing it to reason further and determine that it must now retrieve the name of the U.S. president during the retrieved year. This retrieval feeds reasoning, and reasoning directs retrieval, forming a continuous cycle that progresses until the final answer is constructed. A single retrieval step is likely to fail in such cases because the initially retrieved information alone is insufficient to answer the query. The sequential nature of retrieval in these queries requires the model to dynamically decides what to retrieve at each step, ensuring that each retrieval builds upon prior reasoning to progressively arrive at the correct answer. However, compared to no retrieval and single step retrieval, this multi-step retrieval process incurs higher computational cost, as each additional retrieval step increases latency and resource consumption."}, {"title": "Adaptive Retrieval", "content": "In real-world applications, user queries vary significantly. A fixed retrieval strategy can lead to unnecessary computational costs, increased latency, or suboptimal accuracy, depending on the characteristics of the query and no particular retrieval strategy is optimal for all the query types. For example, forcing retrieval for simple queries that can be answered directly from an LLM's parametric knowledge (e.g., \"Who wrote Pride and Prejudice?\u201d) wastes computational resources and increases latency without improving accuracy. Conversely, for time-sensitive queries (e.g. \"what is the current inflation rate in the U.S.?\u201d), relying solely on parametric knowledge leads to outdated or hallucinated responses, while employing multi-step retrieval unnecessarily increases cost and latency. Similarly, for complex queries that require multi-step reasoning, some level of latency is unavoidable. For these queries, using no retrieval or only a single-step retrieval would often result in incomplete or incorrect responses, reducing the utility and overall effectiveness of the system. Thus, it is crucial to develop an adaptive retrieval system that can dynamically balances accuracy, efficiency, cost.\nWhile previous works, such as Adaptive-RAG Jeong et al. (2024), have attempted to balance efficiency and cost by training classifiers using data collected from model predictions and the inductive dataset biases, they are adaptive only to static query complexities but not to user preferences, making them insufficient for handling the diverse needs of real-world applications. A truly adaptive retrieval system should incorporate both query complexity and user-level controllability. In addition to diverse query complexities, different users and application settings may prioritize accuracy over cost or vice versa, depending on their specific requirements. For example, a medical researcher using an LLM to summarize clinical trial results may prefer higher accuracy and completeness, even at the cost of increased retrieval latency and computational expenses. In contrast, a customer service chatbot handling frequent, simple user queries (e.g., \"What is the return policy?\u201d) might prioritize fast, cost-effective responses, as occasional inaccuracies pose only negligible risk. Similarly, a stock market analyst requiring precise, up-to-date financial information might opt for a retrieval-intensive approach, despite longer response times, whereas a real-time virtual assistant answering everyday factual queries should minimize retrieval overhead, emphasizing speed over exhaustive accuracy. Given these diverse application domains and varying user demands, a one-size-fits-all adaptive retrieval approach is still suboptimal. Instead, a truly adaptive RAG system should be both efficient and customizable, enabling dynamically optimize the trade-off between accuracy and cost while allowing flexible user control."}, {"title": "The Need for Adaptive Retrieval in Real-World Applications", "content": ""}, {"title": "Limitations of Prior Adaptive Retrieval Approaches", "content": ""}, {"title": "Problem Setting", "content": ""}, {"title": "Flare-Aug: Flexible Adaptive Retrieval Augmented Generation", "content": "In this section, we present the main component of our framework, which contains two classifier, Cost-Optimized Classifier, which is dynamic and LLM-dependent, and Reliability-Optimized Classifier, which is static and dataset-dependent. The overview of the Flare-Aug framework is illustrated in Figure 2."}, {"title": "Cost-Optimized Classifier", "content": "The Cost-Optimized Classifier selects the cheapest retrieval strategy that still ensures a correct answer. This classifier is LLM-dependent, since the training data is collected specifically from the LLM used for retrieval, and the prediction is sensitive and tailored to the LLM. The query in the training data is created by using the LLM to answer with all the three strategies, for each query, among the correctly answered strategies, using the strategy with minimal cost as the gold label. Thus, the labeling process itself is biased towards maximally eliminating unnecessary retrieval. The trained classifier, as a result, minimizes retrieval cost, latency, and computational complexity while maintaining an acceptable level of accuracy. However, a key concern of this classifier is that LLM may arrive at the correct answer by chance or by exploiting shortcuts in reasoning, leading to unverified accuracy. While this classifier is highly beneficial for cost-sensitive and efficiency-driven applications, it is unsuitable for high-risk domains where retrieval must be systematic and reliably grounded in external knowledge."}, {"title": "Training Data for Cost-Optimized Classifier", "content": "The labeling process for the cost-optimized classifier is straightforward: we aim to select the cheapest retrieval strategy that still ensures a correct answer. Formally, let $S = \\{S_{no}, S_{sg}, S_{multi}\\}$ denote the set of available retrieval strategies: no retrieval ($s_{no}$), single-step retrieval ($s_{sg}$) and multi-step retrieval ($s_{multi}$). For each query q in the training data, we evaluate whether it can be answered correctly under each retrieval strategy by assessing the LLM's correctness with these strategies. Denote"}, {"title": "Reliability-Optimized Classifier", "content": "The Reliability-Optimized Classifier assigns retrieval strategies based on intrinsic bias of the question answering, ensuring consistency, stability and reliability across different LLMs. This classifier are trained with only binary label: single-step retrieval and multi-step retrieval, with data from both single-hop QA dataset and multi-hop QA dataset. The accurate prediction of these dataset relies on the assumption that, single hop QA dataset and multi-hop QA dataset are intrinsically biases towards single-step retrieval and multi-step retrieval, respectively, and thus, may provide signals indicating which strategy to use during retrieval. Since there are only two labels, this classifier guarantees to have at least one retrieval, it may induce unnecessary computational overhead when using more powerful LLMs or handling straightforward queries that do not require retrieval. Despite this, since it is not LLM specific, it is more stable and reliable, encouraging higher accuracy and more reliable response. This classifier is effective in applications where accuracy is paramount, albeit at a higher cost."}, {"title": "Training Data for Reliability-Optimized Classifier", "content": "The Reliability-Optimized Classifier is trained using binary labels\u2014single-step retrieval and multi-step retrieval\u2014derived from question-answering datasets. Unlike the Cost-Optimized Classifier, which tailors retrieval strategy to a specific LLM, this classifier relies on dataset-level biases to infer retrieval needs, making it more stable across different LLMs. For each query q, we assign a label $s^*(q)$ based on the dataset it belongs to:\n\nThis labeling approach assumes that queries from single-hop QA datasets are best handled with single-step retrieval, while multi-hop QA datasets require multi-step retrieval. Since the classifier does not verify correctness based on a specific LLM, it ensures LLM-agnostic stability though it does not minimize unnecessary retrieval."}, {"title": "User-Controllable Adaptive Classifier", "content": "To fully realize user-controllability, we introduce a simple and interpretable parameter $\u03b1$ that allows users to define their preferred trade-offs, providing flexibility across diverse use cases while dynamically optimizing the retrieval strategy. Specifically, after training the Cost-Optimized Classifier (denoted by $W_{coc}$) and the Reliability-Optimized Classifier (denoted as $W_{roc}$), we construct a User-Controllable Adaptive Classifier by interpolating between their parameters based on the user-defined control parameter \u03b1. The resulting classifier is defined as: $W_\u03b1 = (1 - \u03b1)W_{coc} + \u03b1W_{roc}$, where $\u03b1 \\in [0,1]$ controls the trade-off between cost and accuracy. Setting \u03b1 = 0 results in a purely cost-optimized classifier, while"}, {"title": "Experiments", "content": "We describe the datasets, baselines, evaluation metrics, and experimental details in our study."}, {"title": "Experimental Setups", "content": ""}, {"title": "Dataset", "content": "To simulate a realistic retrieval scenario, where queries naturally vary in complexity and diversity, we evaluate on a mixed dataset with queries from three single-hop question answer datasets\u2014SQUAD (Rajpurkar, 2016), Natural Questions (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017)\u2014and three multi-hop QA datasets\u2014MuSiQue (Trivedi et al., 2022b), HotpotQA (Yang et al., 2018), and 2WikiMultiHopQA (Ho et al., 2020). This combination reflects the heterogeneous nature of real-world queries, encompassing both fact-based retrieval tasks and multi-hop reasoning tasks that require linking information across multiple sources. For training, we randomly sample 500 queries from each dataset, resulting in a total of 3,000 training examples. Similarly, for evaluation, we construct a test set by randomly sampling 500 queries from each dataset, yielding a total of 3,000 test queries."}, {"title": "Baselines", "content": "To assess the effectiveness of our approach in balancing accuracy, cost, and user control flexibility, we compare it against both adaptive and static retrieval strategies. Specifically, we evaluate our method against the adaptive retrieval approach Adaptive-RAG Jeong et al. (2024), as well as three static retrieval strategies: always using No Retrieval; always using Single-Step Retrieval and always using Multi-Step Retrieval (Trivedi et al., 2022a)."}, {"title": "Evaluation Metrics", "content": "We evaluate the trade-off between accuracy and retrieval cost using two key metrics. Accuracy measures whether the predicted answer contains the ground-truth answer. Retrieval cost is quantified as the number of retrieval steps required to generate an answer, where No Retrieval has a cost of 0, Single-Step Retrieval has a cost of 1, and Multi-Step Retrieval incurs a variable cost depending on the number of retrieval steps per query. By jointly considering these two metrics, we assess how different retrieval strategies balancing the trade-off between the response quality and computational efficiency."}, {"title": "Implementation Details", "content": "Following Jeong et al. (2024) and Trivedi et al. (2022a), we use BM25, a term-based sparse retrieval model, as the retriever. For queries from multi-step datasets, we utilize the pre-processed corpus from Trivedi et al. (2022a) as the external document corpus, while for queries from single-hop datasets, we use Wikipedia, preprocessed by Karpukhin et al. (2020), as the external document corpus. For answer generation, we employ two sizes of Flan-T5 models (Chung et al., 2024) (Flan-T5 XL and Flan-T5 XXL) and two sizes of GPT-40 models Hurst et al. (2024) (GPT-40 and GPT-40-mini). For the classifier, we train a T5-Large model Raffel et al. (2020) using 500 samples from each dataset, resulting in a total of 3,000 queries for the labeling process. The classifier is trained with a learning rate of 3e-5 and a batch size of 64 on a single 80GB A100 GPU. We train the model for 20 epochs, as the loss has converged stably without indications of overfitting. We demonstrate the robustness of the classifier in Appendix A, with experiments on different training epochs and different sizes of classifiers."}, {"title": "Main Results", "content": "Figure 3 and Figure 4 illustrate the accuracy and retrieval cost across different values of \u03b1. We observe that both accuracy and cost monotonically increase with the user-controllable parameter \u03b1, allowing users to easily adjust the retrieval strategy based on their preferred"}, {"title": "Accuracy Trends Across Different a", "content": "As shown in Figure 3, the No Retrieval strategy yields the lowest accuracy, with values below 0.2 for the Flan-T5 models, while achieving relatively higher performance GPT40 series models (approximately 0.43 for GPT-40 Mini and 0.53 for GPT-40). Applying Single-Step Retrieval significantly improves accuracy, particularly for Flan-T5 models, increasing their accuracy to around 0.39. However, the performance gains for GPT-40 models are relatively smaller, as these models already exhibit strong parametric knowledge storage and advanced reasoning abilities. In contrast, Flan-T5 models benefit more from retrieval, as they are less capable of answering queries without external information. The accuracy of Adaptive-RAG falls between Single-Step Retrieval and Multi-Step Retrieval. Meanwhile, our user-controllable approach achieves accuracy levels ranging between those of Single-Step Retrieval and Multi-Step Retrieval, allowing users to tailor the retrieval strategy to different application needs. Notably, our approach can even outperform Multi-Step Retrieval for Flan-T5-XL, Flan-T5-XXL, and GPT-40 Mini, demonstrating its effectiveness in optimizing retrieval decisions based on query complexity."}, {"title": "Cost Trends Across Different a", "content": "Figure 4 illustrates the retrieval cost, measured in retrieval steps, across different values of the user-controllable parameter \u03b1. Similar to the accuracy trends observed in Figure 3, retrieval cost monotonically increases with \u03b1, enabling users to adjust retrieval expenditure according to their cost constraints. For Flan-T5 models, the minimum retrieval cost (i.e., setting a = 0) is close to Single-Step Retrieval, indicating that these models require certain amount of retrieval to answer queries correctly. In contrast, for GPT-40 models, the minimum cost is closer to No Retrieval, as these models possess stronger parametric knowledge and reasoning abilities, allowing them to correctly answer a larger proportion of queries without retrieval. This behavior highlights the adaptability of the Cost-Optimized Classifier, which is LLM-specific and automatically adjusts to the retrieval needs of different models\u2014retrieving more for Flan-T5 models while retrieving less for GPT-40 models.Furthermore, while Figure 3 shows that our approach achieves accuracy comparable to or exceeding Multi-Step Retrieval as a increases, Figure 4 demonstrates that its retrieval cost remains consistently lower than that of Multi-Step Retrieval. Thus, compared to Multi-Step Retrieval, our approach incurs significantly lower retrieval overhead while without sacrificing accuracy."}, {"title": "Accuracy-Cost trade-off", "content": "In Figure 5, accuracy-cost trade-off of our approach across different values of a. For Flan-T5 XL, Flan-T5 XXL, and GPT-40 Mini, Adaptive-RAG is positioned on the lower right side of our trade-off curves, indicating that our approach consistently outperforms Adaptive-RAG in both accuracy and cost efficiency. Another way to interpret the accuracy-cost plot is by fixing either accuracy or cost and comparing the other metric. For example, when fixing accuracy to the level achieved by Multi-Step Retrieval, our approach requires significantly fewer retrieval steps, demonstrating its ability to achieve comparable or superior accuracy with lower retrieval overhead."}, {"title": "Practicality of Setting a.", "content": "To further demonstrate the practicality of our approach, we provide two simple and intuitive strategies that can be used for setting a:"}, {"title": "Incremental Adjustment", "content": "Since a increases monotonically with both cost and accuracy, users can start with an initial a and refine it based on observed retrieval cost and response quality. If the retrieval cost exceeds their budget, they can lower a; if the response quality is unsatisfactory, they can increase a."}, {"title": "Validation-Based Estimation", "content": "Users can estimate a suitable a directly by examining the accuracy-cost trade-off on the validation set, allowing them to select a value that aligns with their desired balance between retrieval cost and accuracy. In Appendix B.2, we conduct additional experiments on the test set to illustrate that a can be easily estimated based on results on validation set."}, {"title": "Conclusion", "content": "In this work, we present Flare-Aug, a flexible and adaptive retrieval-augmented generation framework that incorporate user controllability into RAG framework besides accuracy and cost trade-offs. Unlike existing adaptive RAG methods that provide only static adaptation to query complexity, Flare-Aug enables fine-grained and dynamic control over retrieval behavior, allowing users to balance accuracy and cost based on their specific needs. By enabling truly adaptive retrieval tailored to user preferences, Flare-Aug represents a step toward more efficient, customizable, and scalable retrieval-augmented generation, paving the way for future advancements in personalized RAG system."}, {"title": "Limitation and Future Work", "content": "Future research could extend our approach to more advanced adaptive retrieval-augmented generation systems that dynamically adjust not only retrieval strategies but also LLM selec-"}, {"title": "Ablation on classifier", "content": "Different sizes of classifiers Figure 6 shows accuracy and total steps predicted by different sizes of classifier and a on validation set, and Figure 7 shows a more clear comparison on the trade-offs of accuracy and cost by different sizes of classifiers. Though the performance of different classifiers varies, we find that there is no dominating advantage in using larger classifiers."}, {"title": "More Experimental Results", "content": ""}, {"title": "Experimental Results", "content": ""}, {"title": "Quantitative Results on Validation set", "content": "In Table 1, we detail the step and accuracy of different retrieval strategies on validation dataset."}, {"title": "Feasibility of estimating a based on Validation Set.", "content": "In Figure 11, Figure 12 and Figure 13, we show the accuracy and cost with various \u03b1, as well as the accuracy-cost plot on test set. (The quantitative result in Table 2). Combines with the experimental results for validation dataset, we empirically verify that users can estimate the accuracy-cost trade-off from validation results. As shown in Figure 11, Figure 12 and Figure 13, the test set follows the same trend as the validation set. For example, based on Figure 3, setting a = 0.4 or larger for flan-t5-xl achieves an accuracy level comparable to Multi-Step Retrieval, and using the same a value on test set, as shown in Figure 11, yields a similar accuracy-cost balance. Moreover, if the goal is to achieve an accuracy higher than Adaptive-RAG, the validation results suggest setting a = 0.2,0.2, 0.4, 0.6 for flan-t5-xl, flan-t5-xxl, gpt-4o-mini and gpt-40 respectively, which is consistent with that on test set (except for gpt4o model, where on test set, setting a = 0.4 is enough to achieve a comparable accuracy to Adaptive-RAG). Thus, users can rely on validation-based tuning of a for effective deployment."}]}