{"title": "Beyond Retrieval: Generating Narratives in Conversational Recommender Systems", "authors": ["Krishna Sayana", "Raghavendra Vasudeva", "Yuri Vasilevski", "Kun Su", "Liam Hebert", "Hubert Pham", "Ambarish Jash", "Sukhdeep Sodhi"], "abstract": "The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.\nFirst, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with Generative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.", "sections": [{"title": "1 Introduction", "content": "The Web is evolving towards richer and more interactive experiences, and conversational recommender systems are at the forefront of this evolution. These systems leverage natural language generation to provide personalized recommendations in the form of engaging narratives, enhancing user interaction and satisfaction. This paper introduces REGEN, a dataset specifically created to advance research in this area and improve the Web's ability to personalized and interactive recommendations.\nLarge Language Models (LLMs) have ushered in a new era of possibilities in natural language processing, enabling machines to generate human-quality text, translate languages, write different kinds of creative content, and answer user questions in an informative way. This in turn has led to a surge of interest in applying LLMs to various domains, including the long-standing challenge of building conversational recommender systems (Lin et al. 2021; Zhang et al. 2022; Jannach et al. 2020). Conversational recommender systems aim to improve upon traditional recommender systems by engaging users in dialogue to better understand and cater to their preferences. While traditional systems simply provide a list of items, conversational systems could explain recommendations, ask clarifying questions, and create narratives connecting items to user preferences.\nHowever, realizing this vision requires overcoming a fundamental challenge: ensuring that the generated language is relevant to both the recommendation itself and the user's preferences. This necessitates effectively integrating a latent representation of the user and the recommender's internal state with the LLM. Traditional approaches, such as collaborative filtering, excel at leveraging user-item interaction data to predict preferences and provide recommendations (Koren, Bell, and Volinsky 2009). Yet, they often fall short in generating the rich, informative, and engaging language that characterizes human interactions. This gap between the structured world of recommender systems and the nuanced world of human language presents a significant hurdle.\nBridging this gap requires a careful consideration of how to represent and integrate user interaction and item information into the LLM architecture. This poses several research and design questions that need to be addressed. Should we rely on external retrieval mechanisms to fetch relevant information, or should we directly encode user preferences and item representations into the LLM? How can we ensure that the generated language is not only informative but also aligned with the user's individual preferences and the underlying recommendation? Is it better to separately scale the recommender and language models, enabling efficient handling of large item spaces while benefiting from advancements in language modeling or should they be co-trained? How well do the fusion architectures similar to multimodal language models work on conversational recommendation tasks?"}, {"title": "2 Related Work", "content": "To help answer the questions articulated above, we believe that the research community will benefit from datasets specifically designed for conversational tasks. These datasets should include narratives that accurately reflect user preferences and behavior within the context of recommendations. However, many existing datasets either concentrate on next item recommendations, structured outputs or short summaries that lack the rich and varied conversational elements necessary for effectively training and evaluating the capabilities of these systems.\nTo accelerate research in this direction, we present REGEN (Reviews Enhanced with Generative Narratives). REGEN augments the Amazon Reviews dataset (Ni, Li, and McAuley 2019) by incorporating natural language outputs relevant to conversational recommendations. This augmented dataset now includes diverse examples such as purchase reasons, explanations, product endorsements, user summaries & concise user profiles and available for public use (Sayana et al. 2024). Furthermore, we use an autorater LLM ((Longpre et al. 2024)) to assess the generated outputs across multiple attributes, with a particular focus on enhancing grounding and factuality based on the user's historical interactions. In our work, we use a Gemini Pro model with few shot prompting instead of finetuning as in (Longpre et al. 2024). Our aim is to deliver a comprehensive framework and dataset that can serve as a foundation for further research.\nFurther, we introduce a task and framework for conversational recommender systems: generating rich natural language outputs from user-item interaction signals that are consistent with user preferences. This is made possible by the REGEN dataset presented in this paper, and requires models to go beyond traditional recommendation tasks and produce engaging and informative narratives tailored to individual users.\nTo establish a strong baseline for this task, we propose a fusion architecture that seamlessly integrates collaborative filtering signals and content embeddings as input to an LLM. This approach allows the LLM to leverage both user-item interaction data and rich item content representations to generate outputs that are not only informative but also aligned with user preferences. Our experiments demonstrate the effectiveness of this fusion strategy in producing human-like conversational recommendations, showcasing its potential for creating more engaging and personalized user experiences.\nOur key contributions can be summarized as follows:\n1. We generate a new dataset with a diverse set of narratives suitable for language recommendation tasks. The dataset is available for public use. It has been thoroughly evaluated for factuality, grounding, and accurately capturing user preferences and context .\n2. We propose an efficient, and scalable architecture fusing embeddings from collaborative filtering and content representations as inputs to an LLM."}, {"title": "2.1 Conversational Datasets", "content": "Several datasets have been used to explore the application of LLMs in recommender systems. ReDial (Li et al. 2017) is an annotated dataset of dialogues where users recommend movies to each other, consisting of over 10,000 conversations. MIND (Wu et al. 2020) is a large-scale dataset for news recommendation and personalization, containing news articles and user interactions for conversational news recommendation scenarios. E-ConvRec (Chen et al. 2022) provides a large-scale conversational recommendation dataset for e-commerce customer service, based on pre-sales dialogues. TG-ReDial (Zhang et al. 2021) extends ReDial by incorporating topic information to guide conversations and recommendations. DuRecDial (Liu et al. 2022b) is another conversational recommendation dataset based on the DuReader question-answering dataset, where users ask questions about products and receive recommendations. Some focus on short-form recommendations, while others lack the diverse conversational elements needed to evaluate recommender LLMs for generating extended, natural language responses.\nThe \"Justifying Recommendations\" dataset by Ni et al. (Ni, Li, and McAuley 2019) and by Chen et al. (Chen et al. 2024) also focus on generating explanations from reviews. Motivated by these works, we further extend to more open ended narratives, prompting the LLM with the entire user history, and covering use cases beyond purchase explanations. Our work targets natural language outputs that reflect the rapidly evolving capabilities of LLMs, with longer, more nuanced, and potentially semi-structured or unstructured responses, both in context and in aggregate that target conversational recommenders."}, {"title": "2.2 Recommender LMS", "content": "While various approaches have been proposed to integrate LLMs with recommender systems, they can be broadly classified into three main categories:\nI. Retrieval-Augmented Generation (RAG): RAG enhances traditional recommender systems by employing a retrieval model to select relevant items from a catalog based on user context. A generation model, often an LLM, then produces the final recommendations. However, RAG's effectiveness hinges on the retrieval model's accuracy, which"}, {"title": "3 REGEN", "content": "This section details the REGEN dataset, including the data generation process and evaluation methodology with a rater LLM."}, {"title": "3.1 Amazon Reviews", "content": "We use datasets from Amazon Product Reviews (Ni, Li, and McAuley 2019) as a basis for augmentation with rich narratives. The Amazon Reviews dataset (2018) is a massive collection of customer reviews spanning various product categories. This version, released in 2018, contains over 233 million reviews, making it one of the largest publicly available datasets for sentiment analysis and recommendation systems. Each review includes item features like title, description, category, price and review features including text, timestamp, score and summary. User sequences can be created by sorting each user review by timestamp, creating a sequential recommendation task.\nFor our work, we use \u201cOffice Products\" and \"Clothing, Shoes and Jewelry,\u201d verticals. These are chosen to be representative samples with different item counts, Office with 27k items and Clothing with 360k items. We plan to update these to other key verticals, and including the \"All Reviews\u201d."}, {"title": "3.2 Dataset Generation", "content": "Data Preprocessing User sequences  Su = {i1, i2, ..., in} consisting of interactions i are first created by aggregating reviews per user, and sorting them by timestamps. We then truncate all the sequences to the most recent 50 items and filter items with a missing title. This results in 27K items, 101K users for Office with an average of 7-8 items/user and 370K items, 1.2m users for Clothing with an average of 9.2 items/user.\nPrompting and Narrative Generation Our objective is to create a dataset of natural language narratives that mirrors the diverse interactions within a conversational recommender system. To achieve this, we focus on generating outputs that vary across two key dimensions,\n\u2022 Contextualization: We generate outputs both with/without explicit contextual information (e.g., user summaries vs explanations of the most recent purchase, endorsement of a recommendation) to understand its influence on the quality and relevance of the generated language.\n\u2022 Length: We further explore generating both short-form and long-form narratives for different conversational scenarios.\nWe use LLMs to generate this data from user reviews, em-"}, {"title": "3.3 Evaluation using an Auto Rater", "content": "To evaluate the quality of the generated data, we use Gemini Pro LLM as the rater. We give this rater LLM the task, the history, the generated outputs, and prompt it to score the outputs on different attributes.\nOverall Approach We use an ensemble rating process, repeating the rating process several times (10-15) and aggregate these scores to a final score. While this is computationally intensive, we find that it improves the attribute scores with more reliable results. We present distribution of scores across all conversations, and annotate users with corresponding scores in the released dataset.\nEvaluation Metrics and Confidence Scoring We prompt the rater LLM to score the generated data on up to seven attributes (veracity, grounding, clarity & specificity, foresight, personalization, user richness and confidence) using a seven point likert scale. The definitions are summarized in Appendix B. Further, we conduct the evaluation in four stages, prompting the LLM each time:\n\u2022 User richness and confidence: User richness captures the richness of the purchase history for understanding a user.\n\u2022 Purchase reason and explanation: These are scored together based on five key attributes: veracity, foresight, clarity, personalization, and grounding.\n\u2022 User Summaries: These are assessed on the same attributes as purchase reason, excluding foresight, which is only relevant with contextualized output.\n\u2022 Product endorsements: These are assessed on the same attributes as the purchase reasons.\nAn associated confidence score is generated for each stage. This multi-stage approach is used to avoid any correlation caused by Chain of Thought (CoT) bias across different feature types. For ensemble aggregation of individual attribute scores, we use a majority rule, where if over half of the ensemble runs agree on a score, that score becomes the final score. In cases where there's no majority agreement, i.e. with ambiguity, the average score (rounded down) is used. To determine the confidence scores per feature type, we employ an ensemble method using plurality and averaging. The overall confidence score is calculated by first identifying the most frequent score across all ensemble runs for each attribute. Then, we average these scores across all attributes for that feature type to produce a single, representative confidence score. This approach aims to provide a comprehensive and reliable assessment of the generated data's quality.\nAnalysis of Purchase Reasons and Summaries Figures 1 and 2 show the distribution of rated attributes for purchase reason and user summaries. Lower foresight implies less or no post-purchase information is used, while a higher foresight score indicates more or some post-purchase information (i.e, the last review) is incorporated into the reason. Analysis of the \"foresight\" scores reveals distinct patterns in purchase behavior between Office Products and Clothing. Office product purchases, often routine and predictable, show a greater reliance on past history (scores 1-4). Conversely, clothing purchases, with their higher item count and potential for evolving trends, emphasize the current context (scores clustered around 4 and above). Generated summaries demonstrate slightly higher scores overall, particularly in veracity and personalization, likely due to the model's attention to the complete purchase history. Associated confidence scores are generally high (4 or above).\nFigure 3 illustrates the user richness and confidence scores. Approximately 34% of office supply and 28% of clothing examples fall below average richness, with average or above-average confidence. The dataset is annotated with attributes rather than filtered to facilitate research on the impact of personalization and data quality on various techniques and methods. We will include the evaluations of product endorsements in a future revision of the paper."}, {"title": "4 Problem Formulation & Benchmarking Methodology", "content": "In this section, we describe the experimental setup used to generate narratives. The model architecture employs a multi-stage approach to generate narrative recommendations by integrating collaborative filtering signals and semantic embeddings into an LLM."}, {"title": "4.1 Task Definitions", "content": "We formalize the conversational recommendation task within the context of user interaction sequences. Consider a dataset comprising sequences  S = {i1, i2, ..., in} , where each interaction  i \u2208 I  is represented as a tuple (ID, T, P), where ID is the unique identifier of the item, T is a set of"}, {"title": "4.2 Model Architecture", "content": "natural language components  T = {t1,t2,...,tm} describing the item. This includes the title, detailed product description, product category, and other relevant textual metadata. P is the personalized text associated with the item in the context of a specific user. This could be a user review, rating, or other user-generated content related to the item.\nWithin this framework, we define three key tasks central to conversational recommendation:\n1. Narrative Generation from Recent Context: Generate natural language output  T(in|S = {ij | n \u2212 N < j \u2264 n}) corresponding to the recent interaction in, considering the history of interactions up to and including n (truncated to a certain window of observation N). This task focuses on capturing short-term user preferences and generating contextually relevant responses.\n2. Narrative Generation from Aggregate Context: Generate natural language output  T(S) corresponding to the entire interaction history. This task aims to capture long-term user preferences and generate a comprehensive narrative reflecting the user's overall experience.\n3. Next Item Prediction: Predict the next item  in+1 in the sequence given the user's past interactions S. This task represents the traditional recommendation objective.\nIn this paper, we focus on the the first two tasks, which are better suited to the proposed dataset, while the third task is evaluated extensively in several recent studies on the reviews data.\nTo effectively integrate recommender system knowledge in LLMs for natural language generation, we propose a modular architecture as shown in Figure 4 that decouples the encoding of collaborative filtering (CF) signals and semantic representations of user interactions. This approach offers two primary benefits. First, it allows us to systematically investigate the impact of different signal types on the LLM's ability to generate informative and engaging narratives. Second, it serves as a clear and effective baseline for evaluating models trained on the proposed dataset.\nFurther, by independently selecting best-in-class models for each encoding stage, we can leverage proven methods for generating high-quality item embeddings, such as those from matrix factorization (Yi et al. 2018; Koren, Bell, and Volinsky 2009), factorization machines (Rendle 2010), or neural collaborative filtering (Rendle et al. 2020), while simultaneously employing state-of-the-art sentence embedding models like Sentence-T5 (Ni et al. 2021), Universal Sentence Encoder (Cer et al. 2018), GECKO (Lee et al. 2024), Sentence-BERT (SBERT) (Reimers and Gurevych 2019), SimCSE (Gao, Yao, and Chen 2021), and InferSent (Conneau et al. 2017). We also note that this modular approach is also highly practical for real-world datasets with item counts orders of magnitude larger (O(1M-1B)) than those typically used in research, enabling efficient encoding of diverse data scales. Below, we describe the different components used in our multi-stage recommender LM.\nCF Encoder This encoder processes the user-item interaction history. For each user u, the interaction sequence Su = {11, 12, ..., in} consists of interactions i, where each interaction is represented as a tuple (idi, mi, Si, ri) with item ID idi, item metadata mi, user rating/score for the item si and user review for the item ri.\nWe construct a user-item rating matrix R from the interaction history and apply Weighted Alternating Least Squares (WALS) (Hu, Koren, and Volinsky 2008) matrix factorization to obtain user embeddings U \u2208 RNu\u00d7d and item embeddings V \u2208 RNi\u00d7d, where Nu is the number of users, Ni is the number of items, and d is the embedding dimension of the user and item embeddings. This results in a user embedding uu for each user and an item embedding vi for each interaction.\nSemantic Encoder This captures the semantic information from item metadata and user reviews. For each interaction i, we concatenate the item metadata mi and user review ri to form a text representation ti. We then apply a pre-trained sentence embedding model (e.g., sT5, Gecko) to ti to obtain a semantic embedding si \u2208 Rds, where ds is embedding size of the sentence embedding model.\nEmbedding Fusion To combine the collaborative filtering and semantic information, we fuse the embeddings from both encoders. We note that the semantic embeddings si produced by the Gecko model are each L2 normalized to 1. However the WALS embeddings are not, and the norm of these embeddings is related to the popularity of the item. To keep this intact, we perform a normalization in expected value as v\u2081 = vi/||V||2 , where ||V||2 is the mean L2 norm of all item embeddings. For each interaction i, the item embedding v and the semantic embedding si are then combined using a simple concatenation, resulting in a combined embedding c\u2081 = [v\u2081, si] \u2208 Rde, dc = di + ds. While the model can compensate for the normalization to some extent, we find that this additional preprocessing step improves the metrics.\nAdapter Model The sequence of combined embeddings Cu = {C1, C2, ..., cn} for user u is fed into into an adapter model (e.g an MLP or a transformer). This adapter model generates a sequence of soft tokens Zu = {Z1, Z2, ..., Zn }, where each z\u00bf \u2208 Rdi matches the model dimension d\u2081 of the LLM. These soft tokens serve as a condensed representation of the user's interaction history, tailored for integration with the LLM.\nLarge Language Model (LLM) The soft token sequence Zu from the adapter model is prepended to the text prompt token embeddings P = {P1, P2, ..., pm} to form the input sequence I = {Z1, Z2, ..., Zn, P1, P2, ..., pm} for the LLM. This allows the LLM to generate natural language narratives conditioned on both the user's interaction history and the specific prompt, which could indicate additional context. Notably, this just requires N soft tokens to represent N interactions."}, {"title": "5 Experiments", "content": "For our initial benchmarks, we use i) WALS encoder generating CF embeddings of dimension 128, ii) Gecko 1B encoder generating semantic embeddings of dimension 768 and PaLM 2 XXS (PaLM2 2023) for the LLM, and an MLP layer as the adapter."}, {"title": "5.1 Datasets and Tasks", "content": "To evaluate the models, we utilize the proposed REGEN dataset, focusing on the task of generating natural language narratives. We demonstrate the model's capabilities as follows:\nNarrative Generation The model takes a user's history of ratings and reviews as input and generates a comprehensive narrative summarizing their preferences and experiences. For the language input, we use a fixed prompt, essentially prompting the model to generate the corresponding output feature based on the observed history.\nEvaluation with Distinct Users The training and test sets contain different users. This forces the model to learn how to interpret user history from the provided embeddings, rather than memorizing specific users. This ensures the model can generalize to new users and accurately capture their preferences through the generated narratives.\nWe generate benchmarks using REGEN dataset with i) Office Products: with 27K items and ii) Clothing, Shoes,& Jewelry: with 376K items, to evaluate the model's performance across both small and large item vocabularies."}, {"title": "5.2 Evaluation Metrics", "content": "We assess the model using the following complementary methods:\n\u2022 Traditional NLP Metrics: We report BLEU and ROUGE scores to establish baselines. These metrics offer a high-level view of performance trends.\n\u2022 Similarity Scores: We compare the generated narratives to the REGEN refeence narratives using similarity measures based on sentence embedding models (e.g., Gecko). This helps assess the semantic closeness to the ground truth.\n\u2022 Side-by-Side Evaluations with raters: This provides a more nuanced evaluation of the quality and relevance of the generated narratives, either using a human raters or a rater LLM. We do not have comprehensive evaluations using this approach, but show sampled evaluation of some examples SxS with our annotations in Appendix H."}, {"title": "5.3 Key Results", "content": "The experiment results are shown in Table 1.\nNatural Language Tasks with Context These are tasks where the language outputs are based on immediate context, for example, explaining the reasoning and explanation behind the most recently purchased item. Combining collaborative filtering (CF) and semantic embeddings leads to up to 12%, +8%, and +8% gains in BLEU, ROUGE, and Semantic Similarity metrics, respectively, compared to using the best result using either of these embeddings alone.\nNatural Language Tasks without Context For tasks that don't rely on immediate context (e.g., generating a narrative based on an aggregate user profile), combining collaborative filtering and content embeddings offers no significant advantage over using content embeddings alone. This could be because the model does not need to focus on the last item specifically, which is more typical of a recommendation task, and content features are sufficient.\nOverall, we find that the BLEU/ROUGE scores are lower compared to those you'd expect in structured generation tasks. However, our SxS examples in Appendix H show that the model outputs compare well to the reference outputs. This is also reflected in the higher similarity scores."}, {"title": "6 Embedding Analysis", "content": "To analyze the learned embeddings, we consider a linear projection layer as the adapter, which yields comparable results (within 5%) of an MLP adapter. Let  ei \u2208 Rdi represent item embedding,  es \u2208 Rds as content embedding, and  W\u2208 Rdi\u00d7(di+ds) as the projection matrix (adapter layer). As we concatenate the item and semantic embeddings, this is equivalent to  W\u2081 \u2208 Rd\u0131\u00d7di submatrix of W operating on"}, {"title": "6.1 Soft Prompt Embeddings vs LM Token Embeddings:", "content": "ci and  W2 \u2208 Rdi\u00d7ds submatrix of W operating on es. The new embedding can be simply represented as,\nen = W\nCi\nes\n[W1 W2]\nCi\nes\n= W\u2081ei + W2es\nEssentially, the soft embeddings in the token embedding space are expressed as a projected CF and projected content components, where these projections are learnt by the adapter layer. This framework allows us to get more insight into the contribution of collaborative filtering and content signals."}, {"title": "6.2 CF vs Content Embeddings", "content": "Figure 6 shows the distribution of relative contribution (in norm) of projected CF and content embeddings (W\u2081ei, W2es) to the total norm of the soft prompt embeddings. This shows that the content based signals have more significant representation in the learnt soft tokens compared to the CF parts, though CF parts are not insignificant. This could explain the observations in our experiments, where we see Content only embedding model outperform the CF based embedding model, while combining the two yields the best results. In the Office Products dataset, most items appear in 10-20 users (see Appendix D), which does not give extensive CF signals. A similar trend is observed with item statistics in several categories, so it would be informative to repeat these experiments on different datasets with richer interaction data."}, {"title": "7 Limitations and Future Work", "content": "While we have utilized a state-of-the-art LLM as an automated evaluator for the generated narratives and observed high-quality results across various dimensions, we acknowledge that LLMs cannot fully replace human evaluation, particularly for tasks requiring subjective judgment (Chiang and yi Lee 2023). For instance, while LLMs excel at assessing summarization quality, they may struggle with more nuanced aspects like creativity and coherence in narrative generation. Nevertheless, we believe LLMs can effectively complement human evaluation, reducing the need for extensive human involvement and improving efficiency. Recent work like RoboRater (Goldman and Vasilevski 2024) further supports this notion, demonstrating promising results in automating the evaluation of task-oriented conversations. In future updates of REGEN, we plan to explore augmenting our automated evaluation with human assessments for a more comprehensive evaluation. For benchmarking, we employed a simple yet effective embedding input-based LLM architecture to incorporate recommender and semantic knowledge. This approach allows for reproducible benchmarks and facilitates the study of how different signal types impact narrative generation. Future work could explore other architectures, such as employing co-training of encoders or techniques like cross-attention and pre-training, to potentially improve performance."}, {"title": "8 Conclusions", "content": "This paper introduces REGEN, a new dataset designed to advance research in conversational recommender systems. By enhancing the Amazon Product Reviews dataset with richer user narratives, REGEN enables the development and evaluation of models capable of generating personalized explanations and summaries of user preferences. We highlighted the challenge of bridging the gap between traditional recommender systems and natural language generation, emphasizing the need for innovative methods that effectively combine LLMs with recommender system knowledge.\nThrough automated evaluation and analysis of generated outputs, we demonstrated REGEN's capacity to capture user preferences and context. Furthermore, we presented a simple, yet effective fusion architecture that combines collaborative filtering signals and semantic embeddings as input to an LLM, showcasing its potential for generating informative and engaging conversational recommendations. Our experiments demonstrated the effectiveness of this approach in improving natural language generation metrics, particularly when combining both collaborative filtering and semantic information compared to using either of these individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task."}]}