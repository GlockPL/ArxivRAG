{"title": "VISION LANGUAGE MODEL FOR INTERPRETABLE AND FINE-GRAINED DETECTION OF SAFETY COMPLIANCE IN DIVERSE WORKPLACES", "authors": ["Zhiling Chen", "Hanning Chen", "Mohsen Imani", "Ruimin Chen", "Farhad Imani"], "abstract": "Workplace accidents due to personal protective equipment (PPE) non-compliance raise serious safety concerns and lead to legal liabilities, financial penalties, and reputational damage. While object detection models have shown the capability to address this issue by identifying safety items, most existing models, such as YOLO, Faster R-CNN, and SSD, are limited in verifying the fine-grained attributes of PPE across diverse workplace scenarios. Vision language models (VLMs) are gaining traction for detection tasks by leveraging the synergy between visual and textual information, offering a promising solution to traditional object detection limitations in PPE recognition. Nonetheless, VLMs face challenges in consistently verifying PPE attributes due to the complexity and variability of workplace environments, requiring them to interpret context-specific language and visual cues simultaneously. We introduce Clip2Safety, an interpretable detection framework for diverse workplace safety compliance, which comprises four main modules: scene recognition, the visual prompt, safety items detection, and fine-grained verification. The scene recognition identifies the current scenario to determine the necessary safety gear. The visual prompt formulates the specific visual prompts needed for the detection process. The safety items detection identifies whether the required safety gear is being worn according to the specified scenario. Lastly, the fine-grained verification assesses whether the worn safety equipment meets the fine-grained attribute requirements. We conduct real-world case studies across six different scenarios. The results show that Clip2Safety not only demonstrates an accuracy improvement over state-of-the-art question-answering based VLMs but also achieves inference times two hundred times faster.", "sections": [{"title": "1 Introduction", "content": "Workplace safety remains a critical concern across various industries, including construction, manufacturing, and healthcare, leading to significant efforts in safety training programs, the implementation of protective equipment, and the enforcement of strict safety regulations to reduce accidents and injuries (Margaret et al., 2015; Albert and Routh, 2021). Although these measures have improved safety awareness and reduced overall incidents, serious workplace accidents and worker injuries continue to persist (U.S. Bureau of Labor Statistics, 2023a). According to the U.S. Bureau of Labor Statistics, the fatal work injury rate for all workers was 3.7 per 100,000 full-time equivalent workers in 2022, reflecting an increase from 3.6 in 2021 and 3.4 in 2020 (U.S. Bureau of Labor Statistics, 2023b). Effective accident prevention in high-risk workplaces typically involves several critical measures, including hazard screening, providing appropriate personal protective equipment (PPE), maintaining on-the-job vigilance, and promoting safety awareness and education (Foulis, 2021). Among the accident prevention approaches, the provision and proper use of PPE, such as hard hats, safety goggles, gloves, and high-visibility vests stand out as particularly vital (Occupational Safety and Health Administration, 2005). The U.S. Occupational Safety and Health Administration (OSHA) and similar agencies in other countries mandate that all personnel working in close proximity to site hazards wear proper PPE to minimize the risk of injury. In fact, OSHA reports that the proper use of PPE can prevent nearly 40% of occupational injuries and diseases, while 15% of injuries resulting in total disability are caused by the failure to wear proper PPE.\nTraditionally, safety inspections have been performed manually, such as supervisors conducting walk-throughs on construction sites to check for hard hats and safety harnesses, or factory managers inspecting assembly lines to ensure workers wearing gloves and protective eyewear. However, such an approach is labor-intensive, time-consuming, and susceptible to human error. Consequently, there has been a progressive shift towards utilizing sensor-based and vision-based systems to enhance the accuracy and efficiency of safety inspections. Sensor-based systems utilize a variety of technologies to monitor PPE usage in real time. For example, radio frequency identification (RFID) in conjunction with Zigbee technologies is commonly employed to detect the presence of PPE on workers and send compliance reports to central units (Barro-Torres et al., 2012). Vision-based systems, leveraging advancements in computer vision and machine learning, analyze images captured by cameras to automatically detect PPE. Techniques such as deep learning algorithms can identify PPE, including hard hats, safety goggles, and high-visibility vests by comparing images to predefined models (Nath et al., 2020; Abouelyazid, 2022).\nHowever, safety requirements vary significantly across diverse and complex work environments, as illustrated in Figure 1 (a). For example, construction sites mandate hard hats, high-visibility vests, gloves, long pants, and goggles, while"}, {"title": "2 Research Background", "content": "The detection of PPE has been significantly advanced through the application of deep learning and computer vision technologies. Early methods for PPE detection primarily relied on sensor-based systems, such as RFID technology (Barro-Torres et al., 2012). For example, one approach involved equipping each PPE component with RFID tags and using scanners at job site entrances to ensure workers were wearing the appropriate gear (Kelm et al., 2013). Another method investigated the use of short-range transponders embedded in PPE items, with a wireless system verifying compliance with safety regulations (Naticchia et al., 2013). Similarly, (Barro-Torres et al., 2012) investigated a local area network to monitor RFID tags on PPE continuously, ensuring compliance throughout the workday. Additionally, GPS technology has also been utilized to enhance PPE detection. For instance, (Zhang et al., 2015) employed GPS devices attached to workers' safety helmets to provide supplementary safety monitoring on construction sites. In a more recent advancement, (Pisu et al., 2024) introduced an operator area network system that leverages Machine Learning and RSSI analysis to detect improper PPE usage robustly. This system uses an SVM model with a customizable post-processing algorithm, reducing false positives by 80% and detecting issues within seven seconds. Furthermore, (Yang et al., 2020) designed an automated PPE-tool pairing detection system based on a wireless sensor network, which effectively monitors the wearability status of PPE. However, despite their effectiveness in certain scenarios, these sensor-based approaches were limited by their dependency on physical sensors attached to PPE items and necessitated significant investment in purchasing, installing, and maintaining complex sensor networks, which could hinder their practical implementation (Nath et al., 2020).\nWith the advancement of computer vision technology, vision-based systems have revolutionized PPE detection due to their lower cost, simpler configuration, and broader application scenarios compared to sensor-based technologies. These systems utilize cameras to capture real-time images and videos of workers, which are then analyzed using computer vision techniques. The extensive deployment of cameras, combined with significant progress in computer vision technology, has established a foundation for effective PPE detection. For instance, (Wu and Zhao, 2018) proposed a color-based hybrid descriptor that combines local binary patterns, human movement invariants, and color histograms to extract features of hardhats in various colors, which are then classified using a hierarchical support vector machine (HSV). (Li et al., 2018) developed a method for detecting workers utilizing the ViBe and the C4 framework, leveraging the HSV color space for hardhat classification. (Mneymneh et al., 2019) introduced a framework that first detects moving objects using a standard deviation matrix and then classifies them with an aggregate channel feature-based object detector. This approach integrates a histogram of oriented gradients-based cascade object detectors to identify hardhats in the upper regions of detected personnel, which are subsequently processed by a color-based classification component. Despite their effectiveness, these multi-stage methods heavily depend on hand-crafted features and encounter difficulties in complex scenes with varying conditions, different viewpoints and occlusions.\nThus, convolutional neural networks (CNNs) (LeCun et al., 2015) have become the backbone of the systems due to their robust image recognition capabilities. Recent studies employing CNNs for object detection have primarily utilized faster region-based CNNs (R-CNNs) (Ren et al., 2015) and you only look once (YOLO) (Redmon et al., 2016). These models facilitate the recognition of target objects, such as persons and helmets. When trained with sufficiently large datasets, they exhibit robust and improved performance, significantly enhancing the reliability of vision-based PPE detection systems. Faster R-CNN incorporates a region proposal network to improve detection speed and accuracy. For example, (Saudi et al., 2020) leverages Faster R-CNN to check workers' safety conditions based on PPE compliance. Also, (Chen et al., 2020) introduced retinex image enhancement to improve image quality for the outdoor complex scenes in substations based on Faster R-CNN. On the other hand, YOLO is known for its speed and efficiency making it suitable for real-time applications. YOLO divides the image into a grid and predicts bounding boxes and class probabilities directly from full images in one evaluation. Many researchers have utilized variants of YOLO for PPE detection. For instance, (Wu et al., 2019) utilizes the advantage of Densenet in model parameters and technical cost to replace the backbone of the YOLO V3 network for feature extraction, forming a YOLO-Densebackbone convolutional"}, {"title": "2.2 Vision Language Model", "content": "Recent years have witnessed substantial success in extending pre-trained vision language models to support new applications. Among the most successful efforts, models like Flamingo (Alayrac et al., 2022), OpenAI CLIP (Radford et al., 2021), and OpenCLIP (Cherti et al., 2023) have exhibited impressive performance in handling image-text matching, owing to their semantic knowledge and understanding of content that spans both modalities. These models have been applied successfully in downstream applications such as object detection (Shi et al., 2022), image captioning (Mokady et al., 2021), action recognition (Wang et al., 2021), task-oriented object detection (Chen et al., 2024), anomaly segmentation (Jeong et al., 2023), semantic segmentation (Liang et al., 2023), and dense prediction (Zhou et al., 2023). However, existing CLIP-based algorithms primarily focus on matching image patches with nouns in text, which poses challenges in understanding different people and objects within the images. Therefore, additional modules are needed to facilitate the matching between the visual attributes of image patches and the adjective phrases describing different people and items.\nTo enhance the alignment between images and text, a recent application of VLMs for PPE detection introduced a three-step zero-shot learning-based monitoring method (Gil and Lee, 2024). First, it detects workers on-site from images and crops body parts using human-body key points. Next, the cropped body images are described with image captioning. Finally, the generated text is compared to prompts describing PPE-wearing body parts, determining safety based on cosine similarity. However, this method still has limitations in accurately distinguishing between different types of PPE and their specific features in diverse environments.\nAlongside these advancements, significant progress has been made in question-answering based VLMs, which is usually called visual question answering models (VQA). (Bulian et al., 2022) pioneered the creation of the first free-form and open-ended VQA dataset, where human workers were asked to create questions that a smart robot might not answer and then collect human answers per question. The follow-up work, VQAv2 (Goyal et al., 2017), enhanced the previous dataset to reduce statistical biases in the answer distribution. Instead of relying on human workers, the GQA dataset (Hudson and Manning, 2019) employed scene graph structures from visual genome (Krishna et al., 2017) to generate question-answer pairs. These graph structures allowed the authors to balance the answer distributions for each question, reducing the dependency on answer statistics. Both datasets have been widely used for various VQA tasks (Alayrac et al., 2022; Dai et al., 2023). Due to the question-answer format of VQA models, it aligns more naturally with the process of inspecting safety equipment. (Ding et al., 2022) formulated a \"rule-question\" transformation and annotation system, turning safety detection into a visual question answering task, and leverages the strengths of VQA models to enhance the accuracy and efficiency of PPE compliance checks.\nDespite these advancements, a notable tradeoff exists between achieving high-performance VQA models and maintain-ing acceptable inference times. High-performing VQA models typically require significant computational resources and time, leading to longer inference times. However, real-time safety detection applications impose strict requirements on inference times to ensure timely and effective responses. Thus, making a new model for such applications necessitates a careful balance between performance and inference speed."}, {"title": "3 Research Methodology", "content": "The proposed Clip2Safety, an interpretable and fine-grained detection framework for diverse workplace safety compliance, comprises four main modules: scene recognition, the visual prompt, safety items detection, and fine-grained verification."}, {"title": "3.1 Scene Recognition Module", "content": "To address the challenges posed by the diversity of real-world scenes and the corresponding variety of safety gear requirements shown in Fig.1(a), we developed a scene recognition module to identify scenes in images. As illustrated in Fig.20, this module generates high-quality image captions that provide concise summaries of the visual content. However, this abstraction process can result in the loss of specific visual details, potentially affecting our framework's performance. To tackle this issue, we conducted a comprehensive survey of existing image captioning models, prioritizing those capable of generating more accurate scene descriptions while also considering our computational resource constraints. Ultimately, we selected Salesforce's BLIP2-OPT-2.7B model as our image captioning model due to its advanced multi-modal capabilities, which integrate both vision and language understanding, allowing it to generate more nuanced and context-aware descriptions.\nTo further improve the performance of the scene recognition module while adhering to computational resource constraints, we employed Low-Rank Adaptation (LoRA) (Hu et al., 2021) to provide accurate scene recognition by fine-tuning a minimal number of weights. For this purpose, we used the text prompt \"in the [scene]\" paired with the respective image as the textual input. LoRA works by introducing small additive weights into existing linear weight matrices. These weights retain the original weight matrix's dimensions and provide a parallel trainable pathway for the incoming feature map when multiplied. The core idea is that during adaptation the updates to the weight matrices have low intrinsic ranks.\nThe modification involves adding a rank-constrained product of matrices \\(A\\) and \\(B\\). For a weight matrix \\(W \\in \\mathbb{R}^{d \\times k}\\), the update is represented as \\(W' = W + BA\\), where \\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\), and \\(r < min(d, k)\\). Here, \\(d\\) and \\(k\\) are the dimensions of \\(W\\), and \\(r\\) is the rank of the adapters. During fine-tuning, \\(W\\) remains unchanged, while the weights of \\(A\\) and \\(B\\) are updated. Following the initialization method in Hu et al. (2021), \\(B\\) is initialized to zeros, and \\(A\\) with small random values from a Gaussian distribution. The updated matrix \\(W'\\) is given by:\n\\[\nW' = W + \\frac{\\alpha}{r} BA.\n\\]\nIn this equation, \\(\\alpha\\) is a scaling parameter that adjusts the influence of the new weights on the original weights. We implement this approach to the two weight matrices \\(W_q, W_k\\), corresponding to the query, key, and output matrices in the multi-head self-attention module of the transformer architecture. The fine-tuning process aims to minimize the negative log-likelihood loss \\(\\mathcal{L}\\), defined as:\n\\[\n\\mathcal{L} = - \\sum_{i=1}^{K} \\log P(C_i | I_i; \\theta)\n\\]\nwhere \\(P(C_i | I_i; \\theta)\\) is the probability of generating the correct caption \\(C_i\\) given the data sample \\(I_i\\) and model parameters \\(\\theta\\), includes the adapted weights \\(W'\\), and \\(K\\) is the total number of data samples."}, {"title": "3.2 Visual Prompt Module", "content": "In the visual prompt module, we address the diversity of safety requirements by employing a large language model to generate the necessary safety items based on the scene, as depicted in Fig.2. To bridge the semantic and reasoning gap between the fine-grained requirements and the safety items, we also employ the LLM to generate scene-relevant visual attributes, as illustrated in Fig.2 6. In Fig.3, we provide a detailed description of the prompts utilized to guide the LLM in generating the appropriate safety items and their visual attributes. To illustrate this process, we take a seafood factory as an example. The specific procedure unfolds as follows:\n(1) First, we receive the scene information from the image caption model. Based on our compiled and synthe-sized dataset, the scenes are as follows: hospital, construction site, chemical factory, seafood factory, and manufacturing zone.\n(2) Then we operate the scene information generated by the image caption model to query the LLM: \"List the items people should wear in a seafood factory.\" The response generated by the LLM encompasses several safety items, such as \"hairnet\", \"face mask\", \"gloves\", \"aprons\", and \"boots\".\n(3) Building upon the LLM's response, we proceed to the second prompt: \"Summarize the required visual features of the [\"hairnet\", \"face mask\", \"gloves\", \"aprons\", \"boots\"] in a seafood factory.\" This prompt is designed to guide the LLM in summarizing the specific visual attributes required for the requested safety items, as the same item may have different fine-grained requirements in different scenarios, as is shown in Fig.1(b).\n(4) Finally, we obtained the visual attributes that each safety item people in the seafood factory should wear from LLM responses. For instance, the \"boots\" should have visual attributes such as \"high-visibility color\", \"Non-slip soles\" and \"waterproof\". In total, we generated three attributes for each item, which belong to color, material, and functionality respectively."}, {"title": "3.3 Safety Items Detection Module", "content": "In the safety items detection module, we generate \\(N_{pbbox}\\) bounding boxes for all the person in the scene, where pbbox denotes person bounding box, as illustrated in Fig.2. As mentioned in Fig.1 (c), obtaining a multi-scene dataset suitable for the safety compliance task is extremely challenging. Additionally, most single-scene safety detection datasets suffer from severe class imbalance, as only a few workers in the images fail to wear the required items, resulting in the majority being marked as positive samples. To deal with the data scarcity and imbalance problem, we leverage an open vocabulary object detection model, specifically YOLO-World, which is pre-trained on large-scale datasets and with a strong open vocabulary detection capability. Note that, in the basic setting of our framework during inference, the open vocabulary object detection model is only responsible for generating bounding boxes for all the persons in the image. Based on the bounding box coordination, we extract \\(N_{pbbox}\\) person image patches. After generating \\(N_{pbbox}\\) person image patches and \\(N_{item}\\) prompts, we pass them into pre-trained VLMs, such as OpenAI CLIP (Radford et al., 2021), to generate text and image embeddings as is shown in Fig.2 . Here we utilize \\(L_{pbbox}\\) and \\(L_{item}\\) to represent the lists of person image patches and item prompt texts. The lengths of the lists are \\(N_{pbbox}\\) and \\(N_{item}\\), respectively. The computation process can be summarized as follows:\n\\[\nE_{person} = CLIP_{image} (L_{pbbox})\n\\]\n\\[\nE_{item} = CLIP_{text} (L_{item})\n\\]\nSuppose the embedding dimension is d, the shape of the generated vision embedding matrix and text embedding matrix will be \\(N_{pbbox} \\times d\\) and \\(N_{item} \\times d\\). After we get the person vision embedding \\(E_{person}\\) and item text embedding \\(E_{item}\\), We perform matrix to matrix multiplication between \\(E_{person}\\) and \\(E_{item}\\) to generate the predicted affinity matrix \\(A_{predict}\\), as is shown in Fig.26. The computation could be summarized as:\n\\[\nA_{predict} = E_{person} \\times E_{item}\n\\]\nThe shape of the affinity matrix \\(A_{predict}\\) is \\(N_{pbbox} \\times N_{item}\\). In traditional CLIP, to perform the zero-shot image classification, we will directly apply the softmax function over the affinity matrix to obtain the prediction. In this step, rather than performing image classification, we need to verify if the person is wearing the required items. For inference, we set a threshold value (\\(\\delta\\)), and we proceed as follows:\n\\[\nP_i =\\begin{cases}\n1 & \\text{if } P_i > \\delta, \\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\nHere, \\(P_i\\) represents the prediction of whether the person is wearing the item. To optimize our results, we also adopted an LLM-driven approach, where the affinity matrix is provided to the LLM, allowing it to make the decision for us. As shown in Table 3, using GPT-4o as the decision-making LLM yielded better results."}, {"title": "3.4 Fine-grained Verification Module", "content": "In the fine-grained verification module, we leverage the result from the safety items detection module. If the person in the image wears the item, we utilize the same open vocabulary object detection model to generate the bounding boxes of the items and extract the image patches, as is shown in Fig.2. As illustrated in Table 1, three distinct types of attributes are generated for each item within the visual prompt module. After we generate the prompts for each attribute of each item and the corresponding item's image patch, we pass them into the same pre-trained VLMs to generate text and image embeddings. Here we utilize \\(L_{ibbox}\\) and \\(L_{attribute}\\) to represent the lists of item image patches and the attribute's prompt texts, where ibbox means item bounding box. The lengths of the lists are \\(N_{ibbox}\\) and \\(N_{attribute}\\), respectively. The computation process can be summarized as follows:\n\\[\nE_{item} = CLIP_{image} (L_{ibbox})\n\\]\n\\[\nE_{attribute} = CLIP_{text} (L_{attribute})\n\\]\nwhere \\(E_{item}\\) and \\(E_{attribute}\\) are the item vision embedding matrix and attribute text embedding matrix, respectively. Then we extract the embedding vectors of each item image patch and its corresponding attributes' prompt text. We denote them as \\(v_{item}\\) and \\(v_{attribute}\\). During the feature verification process, we compute the cosine similarity between the item image patch and attribute prompt text embedding vectors to measure their alignment. The cosine similarity \\(w(v_{item}, v_{attribute})\\) is calculated as:\n\\[\nw(v_{item}, v_{attribute}) = \\frac{v_{item} \\cdot v_{attribute}}{|| v_{item} || || v_{attribute} ||}\n\\]\nIn this step, we need to check whether the item worn by the person meets the attribute requirements. So we set another threshold value (\\(\\tau\\)) and we have:\n\\[\nP_j =\\begin{cases}\n1 & \\text{if } P_j \\geq \\tau, \\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\nHere \\(P_j\\) is the prediction of whether the item worn by the person meets the attribute requirements. We can also leverage the LLM-driven approach to optimize the result. However, this step is different from 3.3. We will generate a similarity list for all the items' corresponding attributes, and then we give the list to GPT-4o to make desicion for us."}, {"title": "3.5 Evaluation", "content": "Ensuring compliance with safety item requirements involves detecting whether a person is wearing the necessary safety items and verifying that these items meet specified attribute standards. We structure this task into three stages: safety items detection, feature verification, and overall evaluation. For each stage, we define different metrics to comprehensively evaluate model performance.\nThe first stage, safety items detection, focuses on identifying the presence of required safety items, such as hard hats, safety goggles, and high-visibility vests, on individuals. We choose five required safety items for each environment, generated from LLM, as shown in 3.2. This stage leverages advanced object detection algorithms to accurately pinpoint and label each safety item within the visual input. The detection accuracy is measured by determining the proportion of safety items correctly identified by the model relative to the total number of items that should be present. To express this more precisely, the total number of individuals is considered, along with the actual number of safety items each person is wearing. The detection accuracy is then calculated by comparing the number of safety items the model correctly identifies with the expected number of safety items across all individuals.\nThe second stage, feature verification, involves validating that the detected safety items adhere to specific attribute requirements. For example, it is not only important to detect the presence of gloves but also to ensure that they comply with standards for chemical resistance or thermal protection, depending on the operational context. As detailed in table 1, the attributes are classified into three distinct categories: Directly Observable (DO), Situationally Observable (SO), and Inferentially Observable (IO), with each attribute being assessed individually. To assess the accuracy of the feature verification for each attribute type, we compare the number of correctly detected attributes within each category (DO, SO, and IO) against the total number of detected items. For Directly Observable attributes, the accuracy is calculated by determining the proportion of correctly identified attributes out of the total items detected. The same approach is applied to Situationally Observable and Inferentially Observable attributes, where the accuracy is measured by comparing the correctly identified attributes in each category to the total number of detected items."}, {"title": "4 Experiment", "content": "To meet the diverse scene requirements of our model, we collected and integrated data from four sources due to the lack of a suitable comprehensive public dataset. From these sources, we selected five distinct scenes, and each dataset is described in detail as follows:"}, {"title": "5 Conclusions", "content": "In this study, we introduce Clip2Safety, a novel and comprehensive framework designed to enhance safety detection across various environments. Clip2Safety efficiently leverages pre-trained knowledge and vision-language associations from the frozen CLIP model, distinguishing it from previous research efforts in this domain. Our approach addresses the challenge of diverse safety requirements by integrating an image captioning model to accurately interpret scene"}]}