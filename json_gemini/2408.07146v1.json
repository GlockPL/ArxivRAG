{"title": "VISION LANGUAGE MODEL FOR INTERPRETABLE AND\nFINE-GRAINED DETECTION OF SAFETY COMPLIANCE IN\nDIVERSE WORKPLACES", "authors": ["Zhiling Chen", "Hanning Chen", "Mohsen Imani", "Ruimin Chen", "Farhad Imani"], "abstract": "Workplace accidents due to personal protective equipment (PPE) non-compliance raise serious safety\nconcerns and lead to legal liabilities, financial penalties, and reputational damage. While object\ndetection models have shown the capability to address this issue by identifying safety items, most\nexisting models, such as YOLO, Faster R-CNN, and SSD, are limited in verifying the fine-grained\nattributes of PPE across diverse workplace scenarios. Vision language models (VLMs) are gaining\ntraction for detection tasks by leveraging the synergy between visual and textual information, offering\na promising solution to traditional object detection limitations in PPE recognition. Nonetheless,\nVLMs face challenges in consistently verifying PPE attributes due to the complexity and variability\nof workplace environments, requiring them to interpret context-specific language and visual cues\nsimultaneously. We introduce Clip2Safety, an interpretable detection framework for diverse workplace\nsafety compliance, which comprises four main modules: scene recognition, the visual prompt, safety\nitems detection, and fine-grained verification. The scene recognition identifies the current scenario to\ndetermine the necessary safety gear. The visual prompt formulates the specific visual prompts needed\nfor the detection process. The safety items detection identifies whether the required safety gear is\nbeing worn according to the specified scenario. Lastly, the fine-grained verification assesses whether\nthe worn safety equipment meets the fine-grained attribute requirements. We conduct real-world\ncase studies across six different scenarios. The results show that Clip2Safety not only demonstrates\nan accuracy improvement over state-of-the-art question-answering based VLMs but also achieves\ninference times two hundred times faster.", "sections": [{"title": "1 Introduction", "content": "Workplace safety remains a critical concern across various industries, including construction, manufacturing, and\nhealthcare, leading to significant efforts in safety training programs, the implementation of protective equipment, and"}, {"title": "2 Research Background", "content": ""}, {"title": "2.1 Personal Protective Equipment Detection", "content": "The detection of PPE has been significantly advanced through the application of deep learning and computer vision\ntechnologies. Early methods for PPE detection primarily relied on sensor-based systems, such as RFID technology\n(Barro-Torres et al., 2012). For example, one approach involved equipping each PPE component with RFID tags and\nusing scanners at job site entrances to ensure workers were wearing the appropriate gear (Kelm et al., 2013). Another\nmethod investigated the use of short-range transponders embedded in PPE items, with a wireless system verifying\ncompliance with safety regulations (Naticchia et al., 2013). Similarly, (Barro-Torres et al., 2012) investigated a local\narea network to monitor RFID tags on PPE continuously, ensuring compliance throughout the workday. Additionally,\nGPS technology has also been utilized to enhance PPE detection. For instance, (Zhang et al., 2015) employed GPS\ndevices attached to workers' safety helmets to provide supplementary safety monitoring on construction sites. In a more\nrecent advancement, (Pisu et al., 2024) introduced an operator area network system that leverages Machine Learning\nand RSSI analysis to detect improper PPE usage robustly. This system uses an SVM model with a customizable\npost-processing algorithm, reducing false positives by 80% and detecting issues within seven seconds. Furthermore,\n(Yang et al., 2020) designed an automated PPE-tool pairing detection system based on a wireless sensor network, which\neffectively monitors the wearability status of PPE. However, despite their effectiveness in certain scenarios, these\nsensor-based approaches were limited by their dependency on physical sensors attached to PPE items and necessitated\nsignificant investment in purchasing, installing, and maintaining complex sensor networks, which could hinder their\npractical implementation (Nath et al., 2020).\nWith the advancement of computer vision technology, vision-based systems have revolutionized PPE detection due to\ntheir lower cost, simpler configuration, and broader application scenarios compared to sensor-based technologies. These\nsystems utilize cameras to capture real-time images and videos of workers, which are then analyzed using computer\nvision techniques. The extensive deployment of cameras, combined with significant progress in computer vision\ntechnology, has established a foundation for effective PPE detection. For instance, (Wu and Zhao, 2018) proposed a\ncolor-based hybrid descriptor that combines local binary patterns, human movement invariants, and color histograms to\nextract features of hardhats in various colors, which are then classified using a hierarchical support vector machine\n(HSV). (Li et al., 2018) developed a method for detecting workers utilizing the ViBe and the C4 framework, leveraging\nthe HSV color space for hardhat classification. (Mneymneh et al., 2019) introduced a framework that first detects\nmoving objects using a standard deviation matrix and then classifies them with an aggregate channel feature-based object\ndetector. This approach integrates a histogram of oriented gradients-based cascade object detectors to identify hardhats\nin the upper regions of detected personnel, which are subsequently processed by a color-based classification component.\nDespite their effectiveness, these multi-stage methods heavily depend on hand-crafted features and encounter difficulties\nin complex scenes with varying conditions, different viewpoints and occlusions.\nThus, convolutional neural networks (CNNs) (LeCun et al., 2015) have become the backbone of the systems due to\ntheir robust image recognition capabilities. Recent studies employing CNNs for object detection have primarily utilized\nfaster region-based CNNs (R-CNNs) (Ren et al., 2015) and you only look once (YOLO) (Redmon et al., 2016). These\nmodels facilitate the recognition of target objects, such as persons and helmets. When trained with sufficiently large\ndatasets, they exhibit robust and improved performance, significantly enhancing the reliability of vision-based PPE\ndetection systems. Faster R-CNN incorporates a region proposal network to improve detection speed and accuracy. For\nexample, (Saudi et al., 2020) leverages Faster R-CNN to check workers' safety conditions based on PPE compliance.\nAlso, (Chen et al., 2020) introduced retinex image enhancement to improve image quality for the outdoor complex\nscenes in substations based on Faster R-CNN. On the other hand, YOLO is known for its speed and efficiency making\nit suitable for real-time applications. YOLO divides the image into a grid and predicts bounding boxes and class\nprobabilities directly from full images in one evaluation. Many researchers have utilized variants of YOLO for PPE\ndetection. For instance, (Wu et al., 2019) utilizes the advantage of Densenet in model parameters and technical cost to\nreplace the backbone of the YOLO V3 network for feature extraction, forming a YOLO-Densebackbone convolutional"}, {"title": "2.2 Vision Language Model", "content": "Recent years have witnessed substantial success in extending pre-trained vision language models to support new\napplications. Among the most successful efforts, models like Flamingo (Alayrac et al., 2022), OpenAI CLIP (Radford\net al., 2021), and OpenCLIP (Cherti et al., 2023) have exhibited impressive performance in handling image-text\nmatching, owing to their semantic knowledge and understanding of content that spans both modalities. These models\nhave been applied successfully in downstream applications such as object detection (Shi et al., 2022), image captioning\n(Mokady et al., 2021), action recognition (Wang et al., 2021), task-oriented object detection (Chen et al., 2024), anomaly\nsegmentation (Jeong et al., 2023), semantic segmentation (Liang et al., 2023), and dense prediction (Zhou et al., 2023).\nHowever, existing CLIP-based algorithms primarily focus on matching image patches with nouns in text, which poses\nchallenges in understanding different people and objects within the images. Therefore, additional modules are needed\nto facilitate the matching between the visual attributes of image patches and the adjective phrases describing different\npeople and items.\nTo enhance the alignment between images and text, a recent application of VLMs for PPE detection introduced a\nthree-step zero-shot learning-based monitoring method (Gil and Lee, 2024). First, it detects workers on-site from\nimages and crops body parts using human-body key points. Next, the cropped body images are described with image\ncaptioning. Finally, the generated text is compared to prompts describing PPE-wearing body parts, determining safety\nbased on cosine similarity. However, this method still has limitations in accurately distinguishing between different\ntypes of PPE and their specific features in diverse environments.\nAlongside these advancements, significant progress has been made in question-answering based VLMs, which is usually\ncalled visual question answering models (VQA). (Bulian et al., 2022) pioneered the creation of the first free-form and\nopen-ended VQA dataset, where human workers were asked to create questions that a smart robot might not answer\nand then collect human answers per question. The follow-up work, VQAv2 (Goyal et al., 2017), enhanced the previous\ndataset to reduce statistical biases in the answer distribution. Instead of relying on human workers, the GQA dataset\n(Hudson and Manning, 2019) employed scene graph structures from visual genome (Krishna et al., 2017) to generate\nquestion-answer pairs. These graph structures allowed the authors to balance the answer distributions for each question,\nreducing the dependency on answer statistics. Both datasets have been widely used for various VQA tasks (Alayrac\net al., 2022; Dai et al., 2023). Due to the question-answer format of VQA models, it aligns more naturally with the\nprocess of inspecting safety equipment. (Ding et al., 2022) formulated a \"rule-question\" transformation and annotation\nsystem, turning safety detection into a visual question answering task, and leverages the strengths of VQA models to\nenhance the accuracy and efficiency of PPE compliance checks.\nDespite these advancements, a notable tradeoff exists between achieving high-performance VQA models and maintain-\ning acceptable inference times. High-performing VQA models typically require significant computational resources and\ntime, leading to longer inference times. However, real-time safety detection applications impose strict requirements on\ninference times to ensure timely and effective responses. Thus, making a new model for such applications necessitates a\ncareful balance between performance and inference speed."}, {"title": "3 Research Methodology", "content": "The proposed Clip2Safety, an interpretable and fine-grained detection framework for diverse workplace safety com-\npliance, comprises four main modules: scene recognition, the visual prompt, safety items detection, and fine-grained\nverification."}, {"title": "3.1 Scene Recognition Module", "content": "To address the challenges posed by the diversity of real-world scenes and the corresponding variety of safety gear\nrequirements shown in Fig.1(a), we developed a scene recognition module to identify scenes in images. As illustrated\nin Fig.20, this module generates high-quality image captions that provide concise summaries of the visual content.\nHowever, this abstraction process can result in the loss of specific visual details, potentially affecting our framework's\nperformance. To tackle this issue, we conducted a comprehensive survey of existing image captioning models,\nprioritizing those capable of generating more accurate scene descriptions while also considering our computational\nresource constraints. Ultimately, we selected Salesforce's BLIP2-OPT-2.7B model as our image captioning model\ndue to its advanced multi-modal capabilities, which integrate both vision and language understanding, allowing it to\ngenerate more nuanced and context-aware descriptions.\nTo further improve the performance of the scene recognition module while adhering to computational resource\nconstraints, we employed Low-Rank Adaptation (LoRA) (Hu et al., 2021) to provide accurate scene recognition by"}, {"title": "3.2 Visual Prompt Module", "content": "In the visual prompt module, we address the diversity of safety requirements by employing a large language model to\ngenerate the necessary safety items based on the scene, as depicted in Fig.2. To bridge the semantic and reasoning\ngap between the fine-grained requirements and the safety items, we also employ the LLM to generate scene-relevant\nvisual attributes, as illustrated in Fig.2 6. In Fig.3, we provide a detailed description of the prompts utilized to guide the\nLLM in generating the appropriate safety items and their visual attributes. To illustrate this process, we take a seafood\nfactory as an example. The specific procedure unfolds as follows:"}, {"title": "3.3 Safety Items Detection Module", "content": "In the safety items detection module, we generate $N_{pbbox}$ bounding boxes for all the person in the scene, where pbbox\ndenotes person bounding box, as illustrated in Fig.2. As mentioned in Fig.1 (c), obtaining a multi-scene dataset\nsuitable for the safety compliance task is extremely challenging. Additionally, most single-scene safety detection\ndatasets suffer from severe class imbalance, as only a few workers in the images fail to wear the required items, resulting\nin the majority being marked as positive samples. To deal with the data scarcity and imbalance problem, we leverage an\nopen vocabulary object detection model, specifically YOLO-World, which is pre-trained on large-scale datasets and\nwith a strong open vocabulary detection capability. Note that, in the basic setting of our framework during inference,\nthe open vocabulary object detection model is only responsible for generating bounding boxes for all the persons in the\nimage. Based on the bounding box coordination, we extract $N_{pbbox}$ person image patches. After generating $N_{pbbox}$\nperson image patches and $N_{item}$ prompts, we pass them into pre-trained VLMs, such as OpenAI CLIP (Radford et al.,\n2021), to generate text and image embeddings as is shown in Fig.2 . Here we utilize $L_{pbbox}$ and $L_{item}$ to represent the\nlists of person image patches and item prompt texts. The lengths of the lists are $N_{pbbox}$ and $N_{item}$, respectively. The\ncomputation process can be summarized as follows:\n$E_{person} = CLIP_{image} (L_{pbbox})$\n$E_{item} = CLIP_{text} (L_{item})$\nSuppose the embedding dimension is d, the shape of the generated vision embedding matrix and text embedding matrix\nwill be $N_{pbbox} \\times d$ and $N_{item} \\times d$. After we get the person vision embedding $E_{person}$ and item text embedding $E_{item}$, We\nperform matrix to matrix multiplication between $E_{person}$ and $E_{item}$ to generate the predicted affinity matrix $A_{predict}$, as\nis shown in Fig.26. The computation could be summarized as:\n$A_{predict} = E_{person} \\times E_{item}$\nThe shape of the affinity matrix $A_{predict}$ is $N_{pbbox} \\times N_{item}$. In traditional CLIP, to perform the zero-shot image\nclassification, we will directly apply the softmax function over the affinity matrix to obtain the prediction. In this step,\nrather than performing image classification, we need to verify if the person is wearing the required items. For inference,\nwe set a threshold value (\u03b4), and we proceed as follows:\n$P_i = \\begin{cases}\n1 & \\text{if } P_i > \\delta, \\\\\n0 & \\text{else}.\n\\end{cases}$\nHere, $P_i$ represents the prediction of whether the person is wearing the item. To optimize our results, we also adopted\nan LLM-driven approach, where the affinity matrix is provided to the LLM, allowing it to make the decision for us. As\nshown in Table 3, using GPT-4o as the decision-making LLM yielded better results."}, {"title": "3.4 Fine-grained Verification Module", "content": "In the fine-grained verification module, we leverage the result from the safety items detection module. If the person in\nthe image wears the item, we utilize the same open vocabulary object detection model to generate the bounding boxes\nof the items and extract the image patches, as is shown in Fig.2. As illustrated in Table 1, three distinct types of\nattributes are generated for each item within the visual prompt module.\nAfter we generate the prompts for each attribute of each item and the corresponding item's image patch, we pass them\ninto the same pre-trained VLMs to generate text and image embeddings. Here we utilize $L_{ibbox}$ and $L_{attribute}$ to represent\nthe lists of item image patches and the attribute's prompt texts, where ibbox means item bounding box. The lengths of\nthe lists are $N_{ibbox}$ and $N_{attribute}$, respectively. The computation process can be summarized as follows:\n$E_{item} = CLIP_{image} (L_{ibbox})$\n$E_{attribute} = CLIP_{text}(L_{attribute})$\nwhere $E_{item}$ and $E_{attribute}$ are the item vision embedding matrix and attribute text embedding matrix, respectively.\nThen we extract the embedding vectors of each item image patch and its corresponding attributes' prompt text. We\ndenote them as $v_{item}$ and $v_{attribute}$. During the feature verification process, we compute the cosine similarity between\nthe item image patch and attribute prompt text embedding vectors to measure their alignment. The cosine similarity\n$w(v_{item}, v_{attribute})$ is calculated as:\n$w(v_{item}, v_{attribute}) = \\frac{v_{item} \\cdot v_{attribute}}{||v_{item}|| \\cdot ||v_{attribute}||}$\nIn this step, we need to check whether the item worn by the person meets the attribute requirements. So we set another\nthreshold value (\u03c4) and we have:\n$P_j = \\begin{cases}\n1 & \\text{if } P_j \\geq \\tau, \\\\\n0 & \\text{else}.\n\\end{cases}$\nHere Pj is the prediction of whether the item worn by the person meets the attribute requirements. We can also leverage\nthe LLM-driven approach to optimize the result. However, this step is different from 3.3. We will generate a similarity\nlist for all the items' corresponding attributes, and then we give the list to GPT-40 to make desicion for us."}, {"title": "3.5 Evaluation", "content": "Ensuring compliance with safety item requirements involves detecting whether a person is wearing the necessary\nsafety items and verifying that these items meet specified attribute standards. We structure this task into three stages:\nsafety items detection, feature verification, and overall evaluation. For each stage, we define different metrics to\ncomprehensively evaluate model performance.\nThe first stage, safety items detection, focuses on identifying the presence of required safety items, such as hard hats,\nsafety goggles, and high-visibility vests, on individuals. We choose five required safety items for each environment,\ngenerated from LLM, as shown in 3.2. This stage leverages advanced object detection algorithms to accurately pinpoint\nand label each safety item within the visual input. The detection accuracy is measured by determining the proportion of\nsafety items correctly identified by the model relative to the total number of items that should be present. To express this\nmore precisely, the total number of individuals is considered, along with the actual number of safety items each person\nis wearing. The detection accuracy is then calculated by comparing the number of safety items the model correctly\nidentifies with the expected number of safety items across all individuals.\nThe second stage, feature verification, involves validating that the detected safety items adhere to specific attribute\nrequirements. For example, it is not only important to detect the presence of gloves but also to ensure that they comply\nwith standards for chemical resistance or thermal protection, depending on the operational context. As detailed in table\n1, the attributes are classified into three distinct categories: Directly Observable (DO), Situationally Observable (SO),\nand Inferentially Observable (IO), with each attribute being assessed individually. To assess the accuracy of the feature\nverification for each attribute type, we compare the number of correctly detected attributes within each category (DO,\nSO, and IO) against the total number of detected items. For Directly Observable attributes, the accuracy is calculated by\ndetermining the proportion of correctly identified attributes out of the total items detected. The same approach is applied\nto Situationally Observable and Inferentially Observable attributes, where the accuracy is measured by comparing the\ncorrectly identified attributes in each category to the total number of detected items."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Dataset", "content": "To meet the diverse scene requirements of our model, we collected and integrated data from four sources due to the lack\nof a suitable comprehensive public dataset. From these sources, we selected five distinct scenes, and each dataset is\ndescribed in detail as follows:"}, {"title": "4.2 Implementation Details", "content": "For the scene recognition module, we used BILP 2 with configuration of blip2-opt-2.7b. For the object detection model,\nwe experiment with Yolo-world. The implementation of Yolo-world utilizes the yolov8l-worldv2 configuration in\nUltralytics. Regarding the large VLM, we employ Open-CLIP. Specifically, the vision transformer encoder configuration\nis ViT-L/14@224px and the text encoder is OpenAI's tokenizer. We generate required safety items and visual feature\nattributes for each item using OpenAI ChatGPT 40. All experiments are conducted with a single NVIDIA RTX 4090\n24GB GPU."}, {"title": "4.3 Compare to state-of-art methods", "content": "To align with our model's process of first verifying whether the required items are worn and then checking if the\nitems have the required attributes, we chose the VQA model as our baseline. In Fig.5, we use LLaVA-1.6-7b to show\nan example of how we do our task on VQA models. Similar to our model, we divide the questions to two steps.\nIn the first step, we utilize yes/no questions form interrogative sentences to check whether the person wearing the\nitems. In the second step, we use wh-question form interrogative sentence to check whether the item has the required\nattributes. To evaluate the correctness of an answer, it is required to compare a VQA model's output with an answer.\nWe begin with the simplest evaluation metric, called ExactMatch (EM). It involves preprocessing the model's output p,\na free-format text that resembles natural language with humanlike fluency, and the corresponding answer c by removing\nnon-alphanumeric characters, applying lowercase conversion, and normalizing spaces. However, for EM a prediction is\nconsidered correct only if the model's output exactly matches the correct answer. This strict criterion may not accurately\nevaluate the model's performance as it does not account for semantically correct but syntactically different answers. A\nless restrictive option is to consider a response correct if the prediction contains the true answer after preprocessing,\nnamed as Contains (Cont) (Xu et al., 2023)."}, {"title": "4.4 Ablation Study", "content": ""}, {"title": "4.4.1 Comparison of Performance Across Different Object Detection Models", "content": "In this section, we focus on the contribution of the different object detection models to the final accuracy. We compare\nthe performance with the baseline, which directly detects with the original images. To assess the impact of the object"}, {"title": "4.4.2 Comparison of Performance Across Different Pretrained VLMS", "content": "In Table 6, we present a comprehensive comparison of various pre-trained Vision Language models, highlighting their\nperformance across different configurations and optimization strategies. The table provides accuracy metrics for Step 1,\nStep 2 (DO, CO and IO), and average accuracy."}, {"title": "4.4.3 Comparison of Performance Across Different Threshold Values", "content": "In this section, we investigate the effect of different threshold values on our model's performance. Viewing detection as\na binary classification problem, Fig.6 presents the ROC curves for various steps of our model evaluation, emphasizing\nthe impact of varying thresholds on the true positive rate (TPR) and false positive rate (FPR) across different observable\nlevels. We explore the range of threshold values, \u03b4, from 0.58 to 0.65, as all the similarity scores fall within this interval.\nThe color gradient in each plot represents different threshold values within this range, showing their significant impact\non model performance. Higher thresholds generally result in fewer false positives but may also reduce the TPR, as\nindicated by the tighter clustering of points near the lower end of the FPR axis. Conversely, lower thresholds increase\nthe TPR but also raise the FPR, illustrating the trade-off between sensitivity and specificity.\nThe upper left plot presents the ROC curve for Step 1, which involves detecting whether a person is wearing the required\nitem. The model achieves an area under the curve (AUC) of 0.76, indicating a strong ability to distinguish between true\npositives and false positives at this initial stage. The curve shows that the model maintains a high TPR while keeping\nthe FPR relatively low, especially at moderate threshold values, which suggests that the model is effective in detecting\nthe presence of required safety items with fewer errors.\nThe upper right and lower left plots show the ROC curves for the directly observable and situationally observable steps,\nwith AUCs of 0.64 and 0.76, respectively. The decrease in AUC for the directly observable step compared to Step 1\nindicates that as the observables become more challenging to detect, the model's performance drops. The curve for\nthe directly observable step reveals a wider spread of points along the FPR axis, indicating increased false positives\nat certain thresholds. Despite the increased difficulty in observables compared to the directly observable step, the\nsituationally observable step maintains a similar AUC to Step 1. This indicates the model's robustness in handling\nsituational contexts where observables may vary depending on the scenario. The curve demonstrates that the model can\nstill achieve a high TPR with manageable FPR at optimal thresholds, which suggests that the model is adaptable and\nperforms well even when the context of observables is less straightforward.\nThe lower right plot presents the ROC curve for the inferentially observable step, which involves assessing the\nfunctionality of the item. This step has the lowest AUC of 0.55. This significant drop in performance indicates that the\nmodel struggles to accurately classify true positives and false positives when the observables require inference rather\nthan direct detection. The curve shows a broad distribution of points, reflecting high FPRs even at higher threshold\nvalues, which illustrates that the model's sensitivity is greatly challenged in inferential scenarios."}, {"title": "4.5 Visualization and Discussion", "content": "Fig.7 presents our model's detection results on various image samples. Red bounding boxes label the person and blue\nbounding boxes label the items. Specifically, Fig.7 (a) showcases an example where our model performs well, with\ncorrect results in both step 1 (Safety Items Detection) and step 2 (Feature Verification).\nHowever, Fig.7 (b) highlights an instance where our model's performance is suboptimal. While the results are accurate\nin step 1, identifying the required safety items, the model fails in step 2, incorrectly verifying the attributes of shoes.\nThis example illustrates a common challenge faced by our model: accurately verifying specific attributes of safety items\nin diverse and complex environments.\nAdditionally, Fig.7 (c) demonstrates a scenario where the model underperforms in both steps, failing to detect the\nrequired safety items and verify their attributes. This case underscores the difficulty of ensuring comprehensive safety\ncompliance in highly variable settings, highlighting the need for further refinement and enhancement of the model to\nimprove its robustness and accuracy across different scenarios."}, {"title": "5 Conclusions", "content": "In this study, we introduce Clip2Safety, a novel and comprehensive framework designed to enhance safety detection\nacross various environments. Clip2Safety efficiently leverages pre-trained knowledge and vision-language associations\nfrom the frozen CLIP model, distinguishing it from previous research efforts in this domain. Our approach addresses\nthe challenge of diverse safety requirements by integrating an image captioning model to accurately interpret scene"}, {"title": "Declaration of Competing Interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have\nappeared to influence the work reported in this paper."}]}