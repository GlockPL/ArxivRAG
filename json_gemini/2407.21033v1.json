{"title": "Multi-Grained Query-Guided Set Prediction Network for Grounded Multimodal Named Entity Recognition", "authors": ["Jielong Tang", "Zhenxing Wang", "Ziyang Gong", "Jianxing Yu", "Shuang Wang", "Jian Yin"], "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is an emerging information extraction (IE) task, aiming to simultaneously extract entity spans, types, and entity-matched bounding box groundings in images from given sentence-image pairs data. Recent unified methods employing machine reading comprehension (MRC-based) frameworks or sequence generation-based models face challenges in understanding the relationships of multimodal entities. MRC-based frameworks, utilizing human-designed queries, struggle to model intra-entity connections. Meanwhile, sequence generation-based outputs excessively rely on inter-entity dependencies due to pre-defined decoding order. To tackle these, we propose a novel unified framework named Multi-grained Query-guided Set Prediction Network (MQSPN) to learn appropriate relationships at intra-entity and inter-entity levels. Specifically, MQSPN consists of a Multi-grained Query Set (MQS) and a Multimodal Set Prediction Network (MSP). MQS combines specific type-grained and learnable entity-grained queries to adaptively strengthen intra-entity connections by explicitly aligning visual regions with textual spans. Based on solid intra-entity modeling, MSP reformulates GMNER as a set prediction, enabling the parallel prediction of multimodal entities in a non-autoregressive manner, eliminating redundant dependencies from preceding sequences, and guiding models to establish appropriate inter-entity relationships from a global matching perspective. Additionally, to boost better alignment of two-level relationships, we also incorporate a Query-guided Fusion Net (QFNet) to work as a glue network between MQS and MSP. Extensive experiments demonstrate that our approach achieves state-of-the-art performances in widely used benchmarks. Notably, our method improves 2.83% F1 in the difficult fine-grained GMNER benchmark.", "sections": [{"title": "1 INTRODUCTION", "content": "Extracting multimodal entity information, including entity spans, types, and entity-matched bounding box groundings through unified models is an emerging solution of Grounded Multimodal Named Entity Recognition (GMNER) [39]. Although recent unified works have made progress by employing machine reading comprehension (MRC-based) frameworks and sequence generation-based models, limitations still exist. The MRC-based frameworks [11, 12] utilize human-designed queries to provide untrainable prior instructions, struggling to model the connections between textual spans and visual regions, and leading to misdistinguishing same-type entity regions ('Same-Type Error' in Figure 1 (b)). On the other hand, Sequence generation-based methods [30, 39] autoregressively decode all span-type-region triples one by one in predefined sequence order resulting in current outputs highly sensitive to errors in preceding sequences ('Sequence-Sensitive Error' in Figure 1 (c)).\nWe argue that it is difficult for these unified works to understand two-level relationships of multimodal entities. The 'Same-Type Error' in the MRC-based framework presents a mismatch between spans and regions within a single multimodal entity and will not influence other entities which we call intra-entity relationship misunderstanding. Similarly, the 'Sequence-Sensitive Error' reveals that sequence generation-based models overly rely on dependencies between different multimodal entities which we define as inter-entity relationship overreliance. To address these issues, we propose a novel unified framework Multi-grained Query-guided Set Prediction Network (MQSPN) to focus on modeling appropriate relationships at intra-entity and inter-entity levels.\nMQSPN mainly consists of Multi-grained Query Set (MQS), Multimodal Set Prediction Network (MSP), and Query-guided Fusion Net (QFNet). The main motivation of MQS is to adaptively learn intra-entity relationships. Instead of human-designed queries in the MRC-based framework, MQS adopts a set of learnable queries [10, 14] (denoted as entity-grained query) for aligning visual regions with textual span. However, simply learnable queries are insufficient to detect regions and spans due to the lack of initialization of prior semantics. Inspired by Mask Language Modeling [7], where the prediction of the [MASK] token represents the corresponding semantic embedding in natural language, we feed a prompt with masked type statement into vanilla BERT to build type-grained queries with rich type-specific initial semantics. We then construct MQS by associating a learnable entity-grained query with a type-grained query to a multi-grained query pair. Notably, unlike input single query during each iteration in the MRC-based framework, multiple query pairs are jointly employed in MQS to learn entity-grained query semantics for different multimodal entities and establish explicit intra-entity connections.\nBased on solid intra-entity modeling, we further apply MSP to explore suitable inter-entity relationships. Different from previous sequence generation-based methods, MSP reformulates GMNER as set predictions [26, 27], as shown in the grey box of Figure 1(d). With the input of multiple queries from MQS, MSP parallelly predicts a set of multimodal entities in the non-autoregressive manner without the need for a preceding sequence. Given the set of predicted entities and ground truths, the objective of MSP is to find the optimal bipartite matching between them with minimal global matching cost. Additionally, since the number of queries is greater than the total quantity of gold entities, it is not easy to evaluate the predicted entities according to the gold entities. To address this, we first copy the ground truths to the size of MQS and then design a loss function based on bipartite matching. Finally, optimal matching can be efficiently solved by the off-the-shelf Hungarian Algorithm [13]. In this manner, the inference of MSP will not depend on redundant dependencies dictated by a predefined decoding order, thereby guiding models to establish suitable inter-entity relationships from a global matching perspective.\nBesides, since it has been validated [3] that directly fusing textual and visual information will impair MNER model performance due to the introduction of noise from irrelevant visual regions, we also propose a QFNet between MQS and MSP to filter this noisy information, thereby boosting better alignment of two-level relationships. Unlike direct fusion methods [34, 38, 40], QFNet employs queries as intermediaries to facilitate the separate integration of textual and visual region representations. It incorporates three interaction mechanisms: Query-text Cross-Attention (QCT), Query-region Prefix Integration (QPI), and Similarity-aware Aggregator (SAG). QCT and QPI seamlessly integrate information across different modalities, while SAG mitigates the noise caused by misaligned regions and textual spans. More details are illustrated in Section 3.5. Our contributions could be summarized as follows:\n\u2022 We rethink the essence of existing unified GMNER methods' weaknesses from a new perspective, two-level relationships"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Multimodal NER and Grounded MNER", "content": "Exiting multimodal named entity recognition (MNER) models primarily focused on how to utilize visual information to assist textual models in entity extraction, which attempted to design reasonable cross-modal fusion mechanisms [5, 34, 38, 40], explore vision-language alignment [2, 32, 35] or leverage external knowledge-augmented methods [15, 31, 33] to enhance the model's generalization capabilities. To facilitate the construction of a multimodal knowledge graph, a more challenging task named Grounded MNER (GMNER) is proposed to additionally output the bounding box coordinates of named entities within the image. To further expansion, Wang et al. [30] introduced fine-grained entity types in the GMNER task. Yu et al. [39] combined the previously superior MNER models with an off-the-shelf object detection toolkit to propose a series of strong baselines in the pipeline manner. Li et al [16]. and Ok et al [21]. respectively utilize large language models (LLMs) and Wikipedia retrieval to acquire additional knowledge related to entities, thereby improving the pipeline framework performance. To tackle the error propagation issue, H-Index [30] and TIGER [39] formulate GMNER as a sequence generation task and use the BART [36] or T5 [24] models to generate the span-type-region triples. MNER-QG [11] formulate GMNER as a machine reading comprehension task and utilize manually constructed type-specific queries to concurrently guide the model in performing both query grounding and entity span extraction. However, sequence generation-based methods autoregressively decode span-type-region triples in predefined sequence order, which excessively rely on inter-entity dependencies and influence the model's generalization capability. As for the MRC-based methods, the manually constructed queries are semantically impoverished and struggle to model robust intra-entity connections. In this work, we model the GMNER task as set prediction in a non-autoregressive manner, which maintains suitable inter-entity relationships in a global matching perspective. Meanwhile, we replace the manually constructed queries with a learnable multi-grained query set, in which fine-grained entity semantics and intra-entity connection can be learned."}, {"title": "2.2 Set Prediction", "content": "Set prediction is a well-known machine learning technique where the goal is to predict an unordered set of elements. It is widely used in various applications such as object detection [1], point cloud classification [22], and multi-label classification [37]. Unlike traditional prediction tasks that output a single value or a predetermined sequence, set prediction models generate a collection of predictions without any inherent order. In the field of named entity recognition (NER), previous state-of-the-art methods [36] utilize autoregressive sequence generation models to output entities in a predefined sequence order, which violates the permutation invariance of predicted entities. To tackle this issue, Tan et al. [27] reformulate the nested NER task as set prediction and propose an innovative sequence-to-set model. Subsequently, Shen et al. [26] associate each entity with a learnable instance query to capture the location and type semantics, training with a dynamic label assignment mechanism. Recently, Chen et al. [4] extended set prediction to other sequence labeling tasks such as slot filling and part-of-speech (POS) Tagging to enhance performance and inference speed. Different from these methods, we introduce the MQSPN, which exploit the set prediction paradigm in a new GMNER area to model the two-level relationship of multimodal entities."}, {"title": "2.3 Visual Grounding", "content": "Visual Grounding (VG) aims to detect the most relevant visual region based on a natural language query, i.e., phrase, category, or sentence. Most existing works can be divided into two branches. The first branch utilizes the one-stage detector, such as DETR [1] and YOLO [25], to directly output object bounding boxes in an end-to-end manner. The second branch first generates candidate regions with some region proposal methods, such as Region Proposal Network (RPN) [9] and selective search [28], and then selects the best-matching region based on language query. There are two main distinctions between VG and EG in the GMNER task. (1) VG models are typically trained on coarse-grained datasets, while EG is based on fine-grained named entities. (2) The language queries in the training set of VG can be grounded to corresponding visual regions, while most of the named entities in EG are ungroundable. These discrepancies make it difficult to train a solid one-stage visual detector. Hence, our work follows the two-stage paradigm and exploits VinVL [42] as a class-agnostic RPN."}, {"title": "3 OUR METHOD", "content": ""}, {"title": "3.1 Task Formulation", "content": "In this work, we approach GMNER as a set prediction, where the multimodal entities with their corresponding regions are represented as a set of quadruples. Specifically, given a training sample (X, I, Y), where X = (W1, W2, ..., wn) is the input sequence of length n, I is the corresponding image, and Y is the set of gold multimodal entities:\n$Y = \\{(Y_s^1, Y_e^1, Y_t^1, Y_r^1), ..., (Y_s^m, Y_e^m, Y_t^m, Y_r^m)\\},\\qquad(1)$\nwhere $(Y_s^i, Y_e^i, Y_t^i, Y_r^i)$ denote the i-th quadruple, $Y_s^i \\in [0, n-1]$ and $Y_e^i \\in [0, n - 1]$ are the start and end boundary indices of the i-th target entity span. $Y_t^i$ refers to its corresponding entity type, and $Y_r^i$ denotes visually grounded region of the i-th target entity. Note that if the target entity cannot be grounded in the given image I, $Y_r^i$ is None; otherwise, $Y_r^i$ consists of a 4-D spatial feature including the top-left and bottom-right positions of the grounded bounding box, i.e., $(x_{t1}^*, y_{t1}^*, x_{r2}^*, y_{r2}^*)$. To enable the set prediction, we assign u(u > m) learnable queries set for each training sample, which can be denoted as $Q = R^{u \\times h}$. Each query $q_i \\in Q$ (a vector of size h) is responsible for extracting one corresponding multimodal entity quadruple $(Y_s^i, Y_e^i, Y_t^i, Y_r^i)$. Consequently, we can define the GMNER task as follows: given an input sentence X and corresponding image I, the objective is to match the predicted quadruples based on learnable queries set Q with the gold quadruples set Y."}, {"title": "3.2 Overview", "content": "As illustrated in Figure 2, we present a set prediction-based method named MQSPN with four different components, consisting of the Feature Extraction Module, Multi-grained Query Set (MQS), Query-guided Fusion Net (QFNet) and Multimodal Set Prediction Network (MSP). During training, the Feature Extraction Module first generates candidate region proposals and then encodes them with the given text to obtain textual and visual features. Subsequently, learnable MQS, textual features, and visual features pass through QFNet to perform modality fusion and filter noisy information. Finally, the last hidden states of three features are fed into MSP to perform joint training via optimal bipartite matching. Notably, MQS guides the model to learn explicit intra-entity connections, while MSP formulates inter-entity relationships from a global matching perspective. Details of the model are provided as follows."}, {"title": "3.3 Feature Extraction Module", "content": "Text Representation. Given the input sentence X, textual encoder BERT [7] is used to tokenize it into a sequence of word embeddings $H_T = ([CLS], e_1, ..., e_n, [SEP])$, where $e_i \\in R^h$, h is the hidden dimension, [CLS] and [SEP] are special tokens of the beginning and end positions in word embeddings.\nVisual Representation. Given the input image I, we utilize the class-agnostic region proposal network(RPN) VinVL [42] to obtain all candidate regions. Following the work of [39] and [30], we also retain the top-k region proposals as our candidate regions, using the last hidden layer embeddings of VinVL as the initial visual representation for the candidate regions, denoted as $R = \\{r_1, r_2, ..., r_k\\}$, where $r_i \\in R^{2048}$. is the initial visual feature of the i-th candidate region. A linear projection layer is then used to map the initial regional embeddings to dimensions consistent with the textual representations, and the candidate regional representations are denoted as $V = \\{v_1, \u2026\u2026\u2026, v_k \\}$, where $V_i \\in R^h$.\nTo match those entities that are ungroundable in the images, we construct a special visual token embedding $v_{[ug]} \\in R^h$ by feeding a blank image T into ViT-B/32 [8] from pre-training CLIP [23]. Finally, $v_{[ug]}$ and V are concatenated to serve as the final visual representation $H_V \\in R^{(k+1) \\times h}$, where k is the number of candidate regions."}, {"title": "3.4 Multi-grained Query Set Construction", "content": "Previous MRC-based methods attempted to incorporate prior knowledge using manually constructed query statements. However, these fixed and coarse-grained queries struggled to provide distinguishable features for same-type entities, hindering the modeling of intra-entity connections. In this section, we propose a learnable multi-grained query set to overcome this issue.\nPrompt-based Type-grained Query Generator. The Entity type can provide effective information for entity span extraction and candidate region matching. We designed a prompt template: Prompt =\"[TYPE] is an entity type about [MASK]\", where [TYPE] refers to the entity type name, such as Person, Location, Organization, and Others. Then the prompt template is fed into a vanilla BERT model. The type-grained query embedding is calculated as the output embedding of the [MASK] position:\n$H_O^p = BERT (Prompt) _{[MASK]},\\qquad(2)$\nwhere $H_O^p \\in R^{p \\times h}$ is the type-grained query embedding, p denotes the number of entity types.\nLearnable Entity-grained Query Set. Entity-grained queries are randomly initialized as learnable input embeddings $H_O^l \\in R^{u \\times h}$. During training, the entity-grained semantics and corresponding relationships between candidate regions and entity spans can be learned automatically by these embeddings. To ensure that type-grained query embedding and entity-grained query embedding have the same dimensions, we replicated the former d times (note that p < u and d = u/p). Then the multi-grained query embedding is given by:\n$H_Q = H_O^l \\oplus [H_O^p]^d\\qquad(3)$\nwhere $H_Q \\in R^{u \\times h}$ refers to the multi-grained queries set, p < u, and d = u/p. u is the number of queries, we use the token-wise addition operation to fuse the multi-grained queries. $[\\cdot]^d$ denotes repeating d times."}, {"title": "3.5 Query-guided Fusion Net", "content": "Previous multimodal fusion approaches mainly suffered from semantic discrepancies and heterogeneity issues across different modalities. The direct fusion of textual and visual information could impair model performance. Different from previous fusion methods, we use queries as intermediaries to guide the integration of textual representations and visual region representations respectively, thereby reducing the introduction of noise from irrelevant information. To be specific, we present two interaction mechanisms (query-text cross-attention interaction and query-region prefix integration) to seamlessly integrate information across different modalities. To achieve fine-grained alignment between queries and both textual tokens and visual regions, we propose a similarity-aware aggregator that reduces noise introduced by irrelevant candidate regions. We will provide detailed explanations of each module in the following section.\nQuery-text Cross-attention Interaction. As shown in Figure 2 (b), the query set $H_Q$ (obtained from Section 3.4 and each query treated as a special textual token) and the textual sequence $H_T$ (obtained from Section 3.3) are fed into the transformer-based architecture [29]. The cross-attention mechanism is used to fuse these unimodal features, which can be expressed as follows:\n$C_{TQ\\rightarrow T}(H_Q, H_T) = softmax(\\frac{H_Q W_Q (H_T W_K)^T}{\\sqrt{d_k}} ) H_T W_V,\\qquad(4)$\n$C_{TT\\rightarrow Q}(H_T, H_Q) = softmax(\\frac{H_T W_Q (H_Q W_K)^T}{\\sqrt{d_k}}) H_Q W_V,\\qquad(5)$\nwhere $C_{T\\rightarrow \\beta}(\\cdot)$ is the cross-attention calculation from \u03b1 to \u03b2. $W_{\\alpha q} \\in R^{d_{\\alpha} \\times d_k}$, $W_{\\beta k} \\in R^{d_{\\beta} \\times d_k}$, and $W_{\\beta v} \\in R^{d_{\\beta} \\times d_v}$ represent the matrices used to projecting queries, keys, and values respectively.\nQuery-region Prefix Integration. Inspired by prefix tuning [18] and its successful applications [5, 6] in multimodal fusion, we propose query-region prefix integration to reduce the semantic bias and heterogeneity between multimodal data. Specifically, we use the query set features as a visual prefix, inserting them into the candidate region features at keys and values layers of each multi-head attention in the vision transformer [8]. First, the (l-1)-th layer candidate region features $H_V^{(l-1)}$ is projected into the query, key, and value vectors :\n$\\begin{cases} q_V^{(l)} = H_V^{(l-1)} W_{qv}^{(l)}\\ k_V^{(l)} = H_V^{(l-1)} W_{kv}^{(l)}\\ v_V^{(l)} = H_V^{(l-1)} W_{vv}^{(l)} \\end{cases} \\qquad(5)$\nAs for the (l \u2212 1)-th layer query set representation $H_Q^{(l)}$, we project it into the same embedding dimensions of key vector $k_V$ and value vector $v_V$ as visual prefix $\\rho, \\rho_v \\in R^{u \\times h}$.\n$\\begin{cases} \\rho_k^{(l)} = H_Q^{(l-1)} W_k^{(l)}\\ \\rho_v^{(l)} = H_Q^{(l-1)} W_v^{(l)} \\end{cases} \\qquad(6)$\nwhere $W \\in R^{h \\times 2 \\times h}$ represents the linear transformations (the middle dimension 2 means that we apply two isolated transformation parameters for key and value layers). The prefix integration attention can be formalized as follows:\n$PI^{(l)} = softmax(\\frac{[q_V^{(l)}; \\rho_k^{(l)}] \\cdot k_V^{(l)^T}}{\\sqrt{h}} ) [v_V^{(l)}; \\rho_v^{(l)}]\\qquad(7)$\nSimilarity-aware Aggregator. To mitigate the noise caused by misaligned candidate regions and entity spans, we propose a similarity-aware aggregator to learn fine-grained token-wise alignment between query tokens and regional features/textual tokens. We obtain the output vectors from the query-text cross-attention interaction module and query-region prefix integration module: denoted as $H_Q$, $H_T$, and $H_V$, respectively corresponding to the query set feature, textual feature, and visual feature. We compute the token-wise similarity matrix of the i-th query token as follows:\n$\\alpha_j^{\\phi} = \\frac{exp(H_{\\phi j}^T \\cdot H_{Qi})}{\\Sigma_{j'} exp (H_{\\phi j'}^T \\cdot H_{Qi})}\\qquad(8)$\nwhere $\u03c6 \u2208 \\{V, T\\}$ represents the visual or textual feature. $H_{\\phi j}$ refers to the representation of the j-th visual regions or textual tokens. Finally, a fine-grained fusion module is proposed to integrate similarity-aware visual or textual hidden states into the query hidden states:\n$F(H_Q) = Tanh(H_Q W_1 + \\Sigma_{\\phi} \\Sigma_j \\lambda_{\\phi} \\alpha_j^{\\phi} \\phi_j) W_2 + b\\qquad(9)$\nwhere $\u03bb_\u03c6$ is the trade-off coefficient to balance the visual similarity and textual similarity $\\Sigma_{\\phi} \\lambda_{\\phi} = 1$. $W_1$ and $W_2$ are linear transformations parameters and b is the bias term."}, {"title": "3.6 Multimodal Set Prediction Network", "content": "Instead of relying on the predefined decoding order in autoregressive models, we propose Multimodal Set Prediction Network to maintain suitable inter-entity relationships in a global matching perspective. Specifically, we first feed u query set features associated with visual and textual features into three prediction heads to obtain u predicted quadruple set. Subsequently, we apply Hungarian Algorithm [13] to figure out one-to-one matching between the predicted quadruple set and the gold set. The main difficulty of set prediction in training is to evaluate the predicted quadruples according to the gold quadruples. To solve this problem, we design a loss function based on optimal bipartite matching. The details are discussed below.\nSpan Boundary Localization. Given the output textual representation $H_T \\in R^{n \\times h}$ and the query set representation $H_Q \\in R^{u \\times h}$"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experiment Settings", "content": "GMNER Datasets. We conduct experiments on the Twitter-GMNER [39] and Twitter-FMNERG [30]. Twitter-GMNER only contains four entity types: Person (PER), Organization (ORG), Location (LOC), and Others (OTHER) for text-image pairs. Twitter-FMNERG extends the GMNER dataset to 8 coarse-grained and 51 fine-grained entity types. Both of them are built based on two publicly MNER Twitter datasets, i.e., Twitter-2015 [43] and Twitter-2017 [19]. Table 1 shows the details of Twitter-GMNER and Twitter-FMNERG.\nEvaluation Metrics. The GMNER task can be decomposed into multimodal entity recognition(MNER) and entity extraction and grounding(EEG). Following [39], The correctness of each prediction is computed as follows:\n$\\begin{cases} C_e = \\begin{cases} 1, & p_e = g_e;\\\\ 0, & otherwise. \\end{cases}\\\\ C_t = \\begin{cases} 1, & p_t = g_t;\\\\ 0, & otherwise. \\end{cases}\\\\ C_r = \\begin{cases} 1, & p_r = g_r = None;\\\\ 0, & max(IoU_1, ..., IoU_j) > 0.5;\\\\ 0, & otherwise. \\end{cases} \\end{cases} \\qquad(17)$\nwhere $C_e$, $C_t$ and $C_r$ represent the correctness of entity span, type, and region predictions; $p_e$, $p_t$, and $p_r$ refer to the predicted entity span, type, and region; $g_e$, $g_t$ and $g_r$ denote the gold span, type and region; and $IoU_i$ is the IoU score between $p_r$ with the j-th ground-truth bounding box $g_r$, j. The precision (Pre.), recall (Rec.), and F1 score are utilized as the evaluation metrics of the GMNER task:\n$\\begin{cases} correct = \\begin{cases} 1, & C_e\\ and\\ C_t\\ and\\ C_r;\\\\ 0, & otherwise. \\end{cases}\\\\ Pre = \\frac{\\#correct}{\\#predict}\\\\ Rec = \\frac{\\#correct}{\\#gold}\\\\ F1 = \\frac{2 \\times Pre \\times Rec}{Pre + Rec} \\end{cases} \\qquad(19)$\nwhere #correct, #predict, and #gold respectively denote the number of correct predictions, predictions, and gold labels."}, {"title": "4.2 Baselines", "content": "We categorize the existing baseline methods into three groups: (i) Text-only methods, (ii) Text+Image pipeline methods, and (iii) Unified methods.\n(i). The first group first uses representative text-based NER models to extract the corresponding entities and then sets the region prediction to the majority class, i.e., None. We consider the following text-based NER models: (1) Sequence-labeling methods: HBiLSTM-CRF [20] and BERT-CRF [7]. (2) MRC-based method: BERT-MRC [17]. (3) Sequence generation-based method: BARTNER [36]. (4) Set prediction-based method: Seq2Set [27].\n(ii). The second group first uses any previous state-of-the-art MNER method and object detector, i.e., VinVL [42] or Faster R-CNN [9], to pre-extract entity-type pairs and candidate regions. Entity-aware Visual Grounding (EVG) module [39] is then used to predict the matching relationship between entities and regions. The following MNER models are utilized as strong baselines: (1) GVATT [20] adopts a visual attention mechanism based on BiLSTM-CRF to extract multimodal entities. (2) UMT [38] proposes a multimodal transformer to capture the cross-modality semantics. (3) UMGF [41] solves text-image integration with a multimodal graph fusion mechanism. (4) ITA [32] utilizes the image-text translation and object tags to explicitly align visual and textual features. (5) BARTMNER [39] expands generative model BART with cross-modal transformer layer.\n(iii). The third group includes several unified GMNER approaches. (1) H-Index [39] uses a multimodal BART model with the pointer mechanism to formulate the GMNER task as a sequence generation. (2) TIGER [30] is T5-based generative model which converts all span-type-region triples into target paraphrase sequences. (3) MNER-QG [11] is a unified MRC framework that combines entity extraction and grounding with multi-task joint learning.\nFor a fair comparison, we did not compare with methods using large language models [16] or external knowledge-enhanced [21]."}, {"title": "4.3 Overall Performance", "content": "Performance on GMNER, MNER, and EEG. Following [39], we also report two subtasks of GMNER, i.e., Multimodal Named Entity Recognition(MNER) and Entity Extraction & Grounding(EEG). MNER aims to identify entity-type pairs, while EEG aims to extract entity-region pairs. Table 2 shows the performance comparison of our method with baseline models on twitter-GMNER benchmarks. First, Seq2Set-None can obtain comparable F1 scores in text-only methods, which underscores the potential of set prediction-based approaches in GMNER tasks. Second, unified models are significantly superior to pipeline methods due to joint training of MNER and EEG to mitigate error propagation. Third, compared with the previous state-of-the-art unified model H-index [39], our method MQSPN exhibits superior performance, achieving +0.7%, +1.22%, and +1.11% F1 scores improvements on the MNER, EEG, and GMNER tasks, respectively. Furthermore, MQSPN significantly outperforms MNER-QG by +3.61% F1 scores.\nWe attribute the performance improvements of MQSPN to the following reasons: (1) Compared with the sequence generation-based approach H-Index and TIGER, MQSPN eliminates the dependencies on predefined decoding order and establishes suitable inter-entity relationships from a global matching view. (2) Compared with the MRC-based approach MNER-QG, MQSPN can learn fine-grained query semantics and model intra-entity connections between regions and entities.\nPerformance on fine-grained GMNER. We validate the fine-grained GMNER ability of different methods on Twitter-FMNERG datasets, and the experimental results are shown in Table 3. It reveals that our proposed MQSPN achieves the best results on all fine-grained entity types except Other and Art among the state-of-the-art models. Specifically, we achieve a great improvement of +9.62%, +7.94%, +7.56%, +9.44%, +6.63%, and +2.83% F1 scores on the Person, Location, Building, Organization, Event, and All entity types respectively. The experimental results demonstrate that compared to the SOTA sequence generation-basd method TIGER, our MQSPN exhibits superior capability in modeling fine-grained GMNER task."}, {"title": "4.4 Ablation Study", "content": "Ablation Setting. To verify the effectiveness of each designed component in MQSPN, we conduct a comprehensive ablation study on the Twitter-GMNER and Twitter-FMNERG dataset: For Multi-grained Query Set (MQS), (1) w/o POQ: we replace the prompt-based type-grained part of the query set with randomly initialized embedding. (2) w/o LEQ: we remove the learnable entity-grained part in query construction. For Query-guided Fusion Net (QFNet), (3) w/o QCT: we encode sentences and the query set using the original BERT without query-text cross-attention. (4) w/o QPI: we eliminate the query-region prefix integration. (5) w/o SAG: we eliminate the similarity-aware aggregator. For Multimodal Set Prediction (MSP), (6) w/o BML: we replace the bipartite matching loss with a joint cross-entropy loss in the fixed permutation of the entities. The experimental results are shown in Table 4.\nMulti-grained Query Set. A notable contribution of MQSPN is the design of a Multi-grained Query Set (MQS), which includes a prompt-based type-grained part and a learnable entity-grained part. In Table 4, we observe a clear F1 scores drop in model performance (1.57% in Twitter-GMNER and 1.08% in Twitter-FMNERG) without the prompt-based type-grained queries, indicating that integrating type-grained information into the query can enhance model performance. Furthermore, adding an additional learnable entity-grained part to the query substantially improves model performance: in the Twitter-GMNER and Twitter-FMNERG datasets, the F1 scores increased by +2.50% and +2.82%, respectively. The main reason is that these learnable vectors can effectively learn the entity-grained semantics and model intra-entity connections between entity spans and regions, thereby boosting model performance.\nQuery-guided Fusion Net. We investigate the impact of the Query-guided Fusion Net (QFNet), which comprises query-text cross-attention (QCT), query-region prefix integration (QPI), and similarity-aware aggregator (SAG). The model without these three modules achieves consistent drops. We find the average F1 scores of these three modules decreased by 0.79% and 0.78% on Twitter-GMNER and Twitter-FMNERG, respectively. Experimental results demonstrate that the QFNet can fuse information across different modalities and filter the noise introduced by irrelevant visual regions, which is crucial for improving the performance of MQSPN."}, {"title": "4.5 Discussion and Analysis", "content": "Analysis of multi-grained queries quantity. To explore the impact of multi-grained query quantity (i.e., u) on GMNER and its 2 subtasks, we report the F1 scores of Twitter-GMNER in Figure 3 by tuning u from 20 to 90. We observe that as the number of queries increases from 20 to 60, the model's performance improves by +1.66%, +1.76%, and +1.45% in the GMNER, MNER, and EEG tasks, respectively. These results indicate that query quantity plays a crucial role in intra-entity connection learning. However, a query quantity exceeding 60 does not lead to better performance. This suggests that there is an optimal number of queries for MQSPN. In our experiments, we set u = 60 to achieve the optimal results.\nAnalysis of Query-guided Fusion Net Layers. We analyze the influence of the number of QFNet layers on the performance of MQSPN. As presented in Table 5, when the number of fusion layers is decreased from 3 to 2 and from 2 to 1, the overall"}]}