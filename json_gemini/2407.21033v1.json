{"title": "Multi-Grained Query-Guided Set Prediction Network for Grounded Multimodal Named Entity Recognition", "authors": ["Jielong Tang", "Zhenxing Wang", "Ziyang Gong", "Jianxing Yu", "Shuang Wang", "Jian Yin"], "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is an emerging information extraction (IE) task, aiming to simultaneously extract entity spans, types, and entity-matched bounding box groundings in images from given sentence-image pairs data. Recent unified methods employing machine reading comprehension (MRC-based) frameworks or sequence generation-based models face challenges in understanding the relationships of multimodal entities. MRC-based frameworks, utilizing human-designed queries, struggle to model intra-entity connections. Meanwhile, sequence generation-based outputs excessively rely on inter-entity dependencies due to pre-defined decoding order. To tackle these, we propose a novel unified framework named Multi-grained Query-guided Set Prediction Network (MQSPN) to learn appropriate relationships at intra-entity and inter-entity levels. Specifically, MQSPN consists of a Multi-grained Query Set (MQS) and a Multimodal Set Prediction Network (MSP). MQS combines specific type-grained and learnable entity-grained queries to adaptively strengthen intra-entity connections by explicitly aligning visual regions with textual spans. Based on solid intra-entity modeling, MSP reformulates GMNER as a set prediction, enabling the parallel prediction of multimodal entities in a non-autoregressive manner, eliminating redundant dependencies from preceding sequences, and guiding models to establish appropriate inter-entity relationships from a global matching perspective. Additionally, to boost better alignment of two-level relationships, we also incorporate a Query-guided Fusion Net (QFNet) to work as a glue network between MQS and MSP. Extensive experiments demonstrate that our approach achieves state-of-the-art performances in widely used benchmarks. Notably, our method improves 2.83% F1 in the difficult fine-grained GMNER benchmark.", "sections": [{"title": "1 INTRODUCTION", "content": "Extracting multimodal entity information, including entity spans, types, and entity-matched bounding box groundings through unified models is an emerging solution of Grounded Multimodal Named Entity Recognition (GMNER) [39]. Although recent unified works have made progress by employing machine reading comprehension (MRC-based) frameworks and sequence generation-based models, limitations still exist. The MRC-based frameworks [11, 12] utilize human-designed queries to provide untrainable prior instructions, struggling to model the connections between textual spans and visual regions, and leading to misdistinguishing same-type entity regions ('Same-Type Error' in Figure 1 (b)). On the other hand, Sequence generation-based methods [30, 39] autoregressively decode all span-type-region triples one by one in predefined sequence order resulting in current outputs highly sensitive to errors in preceding sequences ('Sequence-Sensitive Error' in Figure 1 (c)).\nWe argue that it is difficult for these unified works to understand two-level relationships of multimodal entities. The 'Same-Type Error' in the MRC-based framework presents a mismatch between spans and regions within a single multimodal entity and will not influence other entities which we call intra-entity relationship misunderstanding. Similarly, the 'Sequence-Sensitive Error' reveals that sequence generation-based models overly rely on dependencies between different multimodal entities which we define as inter-entity relationship overreliance. To address these issues, we propose a novel unified framework Multi-grained Query-guided Set Prediction Network (MQSPN) to focus on modeling appropriate relationships at intra-entity and inter-entity levels.\nMQSPN mainly consists of Multi-grained Query Set (MQS), Multimodal Set Prediction Network (MSP), and Query-guided Fusion Net (QFNet). The main motivation of MQS is to adaptively learn intra-entity relationships. Instead of human-designed queries in the MRC-based framework, MQS adopts a set of learnable queries [10, 14] (denoted as entity-grained query) for aligning visual regions with textual span. However, simply learnable queries are insufficient to detect regions and spans due to the lack of initialization of prior semantics. Inspired by Mask Language Modeling [7], where the prediction of the [MASK] token represents the corresponding semantic embedding in natural language, we feed a prompt with masked type statement into vanilla BERT to build type-grained queries with rich type-specific initial semantics. We then construct MQS by associating a learnable entity-grained query with a type-grained query to a multi-grained query pair. Notably, unlike input single query during each iteration in the MRC-based framework, multiple query pairs are jointly employed in MQS to learn entity-grained query semantics for different multimodal entities and establish explicit intra-entity connections.\nBased on solid intra-entity modeling, we further apply MSP to explore suitable inter-entity relationships. Different from previous sequence generation-based methods, MSP reformulates GMNER as set predictions [26, 27], as shown in the grey box of Figure 1(d). With the input of multiple queries from MQS, MSP parallelly predicts a set of multimodal entities in the non-autoregressive manner without the need for a preceding sequence. Given the set of predicted entities and ground truths, the objective of MSP is to find the optimal bipartite matching between them with minimal global matching cost. Additionally, since the number of queries is greater than the total quantity of gold entities, it is not easy to evaluate the predicted entities according to the gold entities. To address this, we first copy the ground truths to the size of MQS and then design a loss function based on bipartite matching. Finally, optimal matching can be efficiently solved by the off-the-shelf Hungarian Algorithm [13]. In this manner, the inference of MSP will not depend on redundant dependencies dictated by a predefined decoding order, thereby guiding models to establish suitable inter-entity relationships from a global matching perspective.\nBesides, since it has been validated [3] that directly fusing textual and visual information will impair MNER model performance due to the introduction of noise from irrelevant visual regions, we also propose a QFNet between MQS and MSP to filter this noisy information, thereby boosting better alignment of two-level relationships. Unlike direct fusion methods [34, 38, 40], QFNet employs queries as intermediaries to facilitate the separate integration of textual and visual region representations. It incorporates three interaction mechanisms: Query-text Cross-Attention (QCT), Query-region Prefix Integration (QPI), and Similarity-aware Aggregator (SAG). QCT and QPI seamlessly integrate information across different modalities, while SAG mitigates the noise caused by misaligned regions and textual spans. More details are illustrated in Section 3.5. Our contributions could be summarized as follows:\n\u2022 We rethink the essence of existing unified GMNER methods' weaknesses from a new perspective, two-level relationships"}, {"title": "2 RELATED WORK", "content": "2.1 Multimodal NER and Grounded MNER\nExiting multimodal named entity recognition (MNER) models primarily focused on how to utilize visual information to assist textual models in entity extraction, which attempted to design reasonable cross-modal fusion mechanisms [5, 34, 38, 40], explore vision-language alignment [2, 32, 35] or leverage external knowledge-augmented methods [15, 31, 33] to enhance the model's generalization capabilities. To facilitate the construction of a multimodal knowledge graph, a more challenging task named Grounded MNER (GMNER) is proposed to additionally output the bounding box coordinates of named entities within the image. To further expansion, Wang et al. [30] introduced fine-grained entity types in the GMNER task. Yu et al. [39] combined the previously superior MNER models with an off-the-shelf object detection toolkit to propose a series of strong baselines in the pipeline manner. Li et al [16]. and Ok et al [21]. respectively utilize large language models (LLMs) and Wikipedia retrieval to acquire additional knowledge related to entities, thereby improving the pipeline framework performance. To tackle the error propagation issue, H-Index [30] and TIGER [39] formulate GMNER as a sequence generation task and use the BART [36] or T5 [24] models to generate the span-type-region triples. MNER-QG [11] formulate GMNER as a machine reading comprehension task and utilize manually constructed type-specific queries to concurrently guide the model in performing both query grounding and entity span extraction. However, sequence generation-based methods autoregressively decode span-type-region triples in predefined sequence order, which excessively rely on inter-entity dependencies and influence the model's generalization capability. As for the MRC-based methods, the manually constructed queries are semantically impoverished and struggle to model robust intra-entity connections. In this work, we model the GMNER task as set prediction in a non-autoregressive manner, which maintains suitable inter-entity relationships in a global matching perspective. Meanwhile, we replace the manually constructed queries with a learnable multi-grained query set, in which fine-grained entity semantics and intra-entity connection can be learned.\n2.2 Set Prediction\nSet prediction is a well-known machine learning technique where the goal is to predict an unordered set of elements. It is widely used in various applications such as object detection [1], point cloud classification [22], and multi-label classification [37]. Unlike traditional prediction tasks that output a single value or a predetermined sequence, set prediction models generate a collection of"}, {"title": "2.3 Visual Grounding", "content": "Visual Grounding (VG) aims to detect the most relevant visual region based on a natural language query, i.e., phrase, category, or sentence. Most existing works can be divided into two branches. The first branch utilizes the one-stage detector, such as DETR [1] and YOLO [25], to directly output object bounding boxes in an end-to-end manner. The second branch first generates candidate regions with some region proposal methods, such as Region Proposal Network (RPN) [9] and selective search [28], and then selects the best-matching region based on language query. There are two main distinctions between VG and EG in the GMNER task. (1) VG models are typically trained on coarse-grained datasets, while EG is based on fine-grained named entities. (2) The language queries in the training set of VG can be grounded to corresponding visual regions, while most of the named entities in EG are ungroundable. These discrepancies make it difficult to train a solid one-stage visual detector. Hence, our work follows the two-stage paradigm and exploits VinVL [42] as a class-agnostic RPN."}, {"title": "3 OUR METHOD", "content": "3.1 Task Formulation\nIn this work, we approach GMNER as a set prediction, where the multimodal entities with their corresponding regions are represented as a set of quadruples. Specifically, given a training sample (X, I, Y), where X = (W1, W2, ..., wn) is the input sequence of length n, I is the corresponding image, and Y is the set of gold multimodal entities:\nY = {(Y\u02e2\u2081, Y\u1d49\u2081, Y\u1d57\u2081, Y\u02b3\u2081), ..., (Y\u02e2\u2098, Y\u1d49\u2098, Y\u1d57\u2098, Y\u02b3\u2098)}, (1)\nwhere (Y\u02e2\u1d62, Y\u1d49\u1d62, Y\u1d57\u1d62, Y\u02b3\u1d62) denote the i-th quadruple, Y\u02e2\u1d62 \u2208 [0, n-1] and Y\u1d49\u1d62 \u2208 [0, n - 1] are the start and end boundary indices of the i-th target entity span. Y\u1d57\u1d62 refers to its corresponding entity type, and Y\u02b3\u1d62 denotes visually grounded region of the i-th target entity. Note that if the target entity cannot be grounded in the given image I, Y\u02b3\u1d62 is None; otherwise, Y\u02b3\u1d62 consists of a 4-D spatial feature including the top-left and bottom-right positions of the grounded bounding box, i.e., (x\u1d57\u02e1\u2081, y\u1d57\u02e1\u2081, x\u1d47\u02b3\u2081, y\u1d47\u02b3\u2082). To enable the set prediction, we assign u(u > m) learnable queries set for each training sample, which can be denoted as Q = R\u1d58\u00d7\u02b0. Each query q \u2208 Q (a vector of size h) is responsible for extracting one corresponding multimodal entity"}, {"title": "3.2 Overview", "content": "As illustrated in Figure 2, we present a set prediction-based method named MQSPN with four different components, consisting of the Feature Extraction Module, Multi-grained Query Set (MQS), Query-guided Fusion Net (QFNet) and Multimodal Set Prediction Network (MSP). During training, the Feature Extraction Module first generates candidate region proposals and then encodes them with the given text to obtain textual and visual features. Subsequently, learnable MQS, textual features, and visual features pass through QFNet to perform modality fusion and filter noisy information. Finally, the last hidden states of three features are fed into MSP to perform joint training via optimal bipartite matching. Notably, MQS guides the model to learn explicit intra-entity connections, while MSP formulates inter-entity relationships from a global matching perspective. Details of the model are provided as follows."}, {"title": "3.3 Feature Extraction Module", "content": "Text Representation. Given the input sentence X, textual encoder BERT [7] is used to tokenize it into a sequence of word embeddings H\u1d40 = ([CLS], e\u2081, ..., e\u2099, [SEP]), where e\u1d62 \u2208 R\u02b0, h is the hidden dimension, [CLS] and [SEP] are special tokens of the beginning and end positions in word embeddings.\nVisual Representation. Given the input image I, we utilize the class-agnostic region proposal network(RPN) VinVL [42] to obtain all candidate regions. Following the work of [39] and [30], we also retain the top-k region proposals as our candidate regions,"}, {"title": "3.4 Multi-grained Query Set Construction", "content": "Previous MRC-based methods attempted to incorporate prior knowledge using manually constructed query statements. However, these fixed and coarse-grained queries struggled to provide distinguishable features for same-type entities, hindering the modeling of intra-entity connections. In this section, we propose a learnable multi-grained query set to overcome this issue.\nPrompt-based Type-grained Query Generator. The Entity type can provide effective information for entity span extraction and candidate region matching. We designed a prompt template: Prompt =\"[TYPE] is an entity type about [MASK]\", where [TYPE] refers to the entity type name, such as Person, Location, Organization, and Others. Then the prompt template is fed into a vanilla BERT model. The type-grained query embedding is calculated as the output embedding of the [MASK] position:\nH\u1d40\u1d60 = BERT (Prompt) [MASK] (2)"}, {"title": "3.5 Query-guided Fusion Net", "content": "Previous multimodal fusion approaches mainly suffered from semantic discrepancies and heterogeneity issues across different modalities. The direct fusion of textual and visual information could impair model performance. Different from previous fusion methods, we use queries as intermediaries to guide the integration of textual representations and visual region representations respectively, thereby reducing the introduction of noise from irrelevant information. To be specific, we present two interaction mechanisms (query-text cross-attention interaction and query-region prefix integration) to seamlessly integrate information across different modalities. To achieve fine-grained alignment between queries and both textual tokens and visual regions, we propose a similarity-aware aggregator that reduces noise introduced by irrelevant candidate regions. We will provide detailed explanations of each module in the following section.\nQuery-text Cross-attention Interaction. As shown in Figure 2 (b), the query set HQ (obtained from Section 3.4 and each query treated as a special textual token) and the textual sequence HT (obtained from Section 3.3) are fed into the transformer-based architecture [29]. The cross-attention mechanism is used to fuse these unimodal features, which can be expressed as follows:\nC\u1d40Q\u2192T(HQ, HT) = softmax ( \\frac{HQW^Q_a W^K_aH^T}{\u221adk} ),\nC\u1d40T\u2192Q(HT, HQ) = softmax ( \\frac{H^TW^Q_b W^K_bHQ}{\u221adk} ),\n(4)\nwhere C\u2090\u2192\u03b2(\u00b7) is the cross-attention calculation from a to \u03b2. W\u1d45\u2096 \u2208 R\u1d48\u03b1\u00d7\u1d48\u1d4f, WB\u2096 \u2208 R\u1d48\u03b2\u00d7\u1d48\u1d4f, and W\u1d45\u1d65 \u2208 R\u1d48\u03b1\u00d7\u1d48\u1d5b represent the matrices used to projecting queries, keys, and values respectively.\nQuery-region Prefix Integration. Inspired by prefix tuning [18] and its successful applications [5, 6] in multimodal fusion, we propose query-region prefix integration to reduce the semantic bias and heterogeneity between multimodal data. Specifically, we use the query set features as a visual prefix, inserting them into the candidate region features at keys and values layers of each multi-head attention in the vision transformer [8]. First, the (l-1)-th"}, {"title": "3.6 Multimodal Set Prediction Network", "content": "Instead of relying on the predefined decoding order in autoregressive models, we propose Multimodal Set Prediction Network to maintain suitable inter-entity relationships in a global matching perspective. Specifically, we first feed u query set features associated with visual and textual features into three prediction heads to obtain u predicted quadruple set. Subsequently, we apply Hungarian Algorithm [13] to figure out one-to-one matching between the predicted quadruple set and the gold set. The main difficulty of set prediction in training is to evaluate the predicted quadruples according to the gold quadruples. To solve this problem, we design a loss function based on optimal bipartite matching. The details are discussed below.\nSpan Boundary Localization. Given the output textual representation \u0124\u1d40 \u2208 R\u207f\u00d7\u02b0 and the query set representation \u0124Q \u2208 R\u1d58\u00d7\u02b0"}, {"title": "4 EXPERIMENTS", "content": "4.1 Experiment Settings\nGMNER Datasets. We conduct experiments on the Twitter-GMNER [39] and Twitter-FMNERG [30]. Twitter-GMNER only contains four entity types: Person (PER), Organization (ORG), Location (LOC), and Others (OTHER) for text-image pairs. Twitter-FMNERG extends the GMNER dataset to 8 coarse-grained and 51 fine-grained entity types. Both of them are built based on two publicly MNER Twitter datasets, i.e., Twitter-2015 [43] and Twitter-2017 [19]. Table 1 shows the details of Twitter-GMNER and Twitter-FMNERG.\nEvaluation Metrics. The GMNER task can be decomposed into multimodal entity recognition(MNER) and entity extraction and grounding(EEG). Following [39], The correctness of each prediction is computed as follows:\n1, C\u2091/C\u209c = g\u2091/g\u209c;\notherwise, (17)\nC\u2091, C\u209c and C\u1d63 represent the correctness of entity span, type, and region predictions; p\u2091, p\u209c, and p\u1d63 refer to the predicted entity span, type, and region; g\u2091, g\u209c and g\u1d63 denote the gold span, type, and region; and IoU\u2c7c is the IoU score between p\u1d63 with the j-th ground-truth bounding box g\u1d63,\u2c7c. The precision (Pre.), recall (Rec.), and F1 score are utilized as the evaluation metrics of the GMNER task:\ncorrect = 1, C\u2091 and C\u209c and C\u1d63;\notherwise. (19)"}, {"title": "4.2 Baselines", "content": "We categorize the existing baseline methods into three groups: (i) Text-only methods, (ii) Text+Image pipeline methods, and (iii) Unified methods.\n(i). The first group first uses representative text-based NER models to extract the corresponding entities and then sets the region prediction to the majority class, i.e., None. We consider the following text-based NER models: (1) Sequence-labeling methods: HBiLSTM-CRF [20] and BERT-CRF [7]. (2) MRC-based method: BERT-MRC [17]. (3) Sequence generation-based method: BARTNER [36]. (4) Set prediction-based method: Seq2Set [27].\n(ii). The second group first uses any previous state-of-the-art MNER method and object detector, i.e., VinVL [42] or Faster R-CNN [9], to pre-extract entity-type pairs and candidate regions. Entity-aware Visual Grounding (EVG) module [39] is then used to predict the matching relationship between entities and regions. The following MNER models are utilized as strong baselines: (1) GVATT [20] adopts a visual attention mechanism based on BiLSTM-CRF to extract multimodal entities. (2) UMT [38] proposes a multimodal transformer to capture the cross-modality semantics. (3) UMGF [41] solves text-image integration with a multimodal graph fusion mechanism. (4) ITA [32] utilizes the image-text translation and object tags to explicitly align visual and textual features. (5) BARTMNER [39] expands generative model BART with cross-modal transformer layer.\n(iii). The third group includes several unified GMNER approaches. (1) H-Index [39] uses a multimodal BART model with the pointer mechanism to formulate the GMNER task as a sequence generation. (2) TIGER [30] is T5-based generative model which converts all span-type-region triples into target paraphrase sequences. (3) MNER-QG [11] is a unified MRC framework that combines entity extraction and grounding with multi-task joint learning.\nFor a fair comparison, we did not compare with methods using large language models [16] or external knowledge-enhanced [21]."}, {"title": "4.3 Overall Performance", "content": "Performance on GMNER, MNER, and EEG. Following [39], we also report two subtasks of GMNER, i.e., Multimodal Named Entity Recognition(MNER) and Entity Extraction & Grounding(EEG). MNER aims to identify entity-type pairs, while EEG aims to extract entity-region pairs. Table 2 shows the performance comparison of our method with baseline models on twitter-GMNER benchmarks."}, {"title": "4.4 Ablation Study", "content": "Ablation Setting. To verify the effectiveness of each designed component in MQSPN, we conduct a comprehensive ablation study on the Twitter-GMNER and Twitter-FMNERG dataset: For Multi-grained Query Set (MQS), (1) w/o PTQ: we replace the prompt-based type-grained part of the query set with randomly initialized embedding. (2) w/o LEQ: we remove the learnable entity-grained part in query construction. For Query-guided Fusion Net (QFNet), (3) w/o QCT: we encode sentences and the query set using the original BERT without query-text cross-attention. (4) w/o QPI: we eliminate the query-region prefix integration. (5) w/o SAG: we eliminate the similarity-aware aggregator. For Multimodal Set Prediction (MSP), (6) w/o BML: we replace the bipartite matching loss with a joint cross-entropy loss in the fixed permutation of the entities. The experimental results are shown in Table 4.\nMulti-grained Query Set. A notable contribution of MQSPN is the design of a Multi-grained Query Set (MQS), which includes a prompt-based type-grained part and a learnable entity-grained part. In Table 4, we observe a clear F1 scores drop in model performance (1.57% in Twitter-GMNER and 1.08% in Twitter-FMNERG) without the prompt-based type-grained queries, indicating that integrating type-grained information into the query can enhance model performance. Furthermore, adding an additional learnable entity-grained part to the query substantially improves model performance: in the Twitter-GMNER and Twitter-FMNERG datasets, the F1 scores increased by +2.50% and +2.82%, respectively. The main reason is"}, {"title": "4.5 Discussion and Analysis", "content": "Analysis of multi-grained queries quantity. To explore the impact of multi-grained query quantity (i.e., u) on GMNER and its 2 subtasks, we report the F1 scores of Twitter-GMNER in Figure 3 by tuning u from 20 to 90. We observe that as the number of queries increases from 20 to 60, the model's performance improves by +1.66%, +1.76%, and +1.45% in the GMNER, MNER, and EEG tasks, respectively. These results indicate that query quantity plays a crucial role in intra-entity connection learning. However, a query quantity exceeding 60 does not lead to better performance. This suggests that there is an optimal number of queries for MQSPN. In our experiments, we set u = 60 to achieve the optimal results.\nAnalysis of Query-guided Fusion Net Layers. We analyze the influence of the number of QFNet layers on the performance of MQSPN. As presented in Table 5, when the number of fusion layers is decreased from 3 to 2 and from 2 to 1, the overall F1 scores drop by 0.72% and 0.30%, respectively. Furthermore, increasing the number of fusion layers from 3 to 5 results in only a marginal performance improvement of 0.05%; however, it comes at the cost of decreased inference speed in the model. It also reveals that MQSPN is insensitive to the layers of the cross-modal fusion Layers.\nSensitivity Analysis of Irrelevant Visual Regions. The performance of EEG is largely determined by the ground truth coverage of the top-k regions proposed by the class-agnostic RPN. However,"}, {"title": "CONCLUSION", "content": "In this paper, we propose a novel unified framework named MQSPN that consists of Multi-grained Queries Set (MQS), Multimodal Set Prediction Network (MSP), and Query-guided Fusion Net (QFNet). The three modules cooperate to model appropriate intra-entity and inter-entity relationships for the GMNER task. Compared with MRC-based methods, MQS provides learnable entity-grained queries to adaptively model solid connections between entity spans and regions, mitigating intra-entity relationship misunderstanding. Compared with sequence generation-based methods, MSP reformulates GMNER as a set prediction to eliminate inter-entity overreliance caused by predetermined sequence decoding order. Experimental results demonstrate that our MQSPN achieves state-of-the-art performance on both GMNER and its 2 subtasks across two Twitter benchmarks. Comprehensive ablation studies verify the effectiveness of each designed module. In the future, the extension of our MQSPN framework to the vision-language pre-training model will be one of our directions."}]}