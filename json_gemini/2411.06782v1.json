{"title": "QuadWBG: Generalizable Quadrupedal Whole-Body Grasping", "authors": ["Jilong Wang", "Javokhirbek Rajabov", "Chaoyi Xu", "Yiming Zheng", "He Wang"], "abstract": "Legged robots with advanced manipulation capabilities have the potential to significantly improve household duties and urban maintenance. Despite considerable progress in developing robust locomotion and precise manipulation methods, seamlessly integrating these into cohesive whole-body control for real-world applications remains challenging. In this paper, we present a modular framework for robust and generalizable whole-body loco-manipulation controller based on a single arm-mounted camera. By using reinforcement learning (RL), we enable a robust low-level policy for command execution over 5 dimensions (5D) and a grasp-aware high-level policy guided by a novel metric, Generalized Oriented Reachability Map (GORM). The proposed system achieves state-of-the-art one-time grasping accuracy of 89% in real world, including challenging tasks such as grasping transparent objects. Through extensive simulations and real-world experiments, we demonstrate that our system can effectively manage a large workspace, from floor level to above body height, and perform diverse whole-body loco-manipulation tasks. See our robot at work: quadwbg.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Quadrupedal loco-manipulation, which integrates legged locomotion with robotic arm manipulation, has emerged as a key research area due to its broad potential applications, including household assistance, urban maintenance, disaster relief, and autonomous field operations [1]-[4]. Recent advancements in reinforcement learning (RL) have enabled the development of end-to-end policies for whole-body locomotion and manipulation [5]\u2013[9], allowing robots to perform tasks that require seamless coordination of movement and object interaction. While end-to-end RL has substantially improved locomotion skills [10]-[22], loco-manipulation remains highly challenging due to the increased action dimensionality and complex physical interactions involved. These challenges often result in loco-manipulation policies with mediocre accuracy and limited generalizability [5], [7], especially when grasping objects of different shapes, sizes, and materials, thereby restricting their effectiveness in real-world applications.\nTo enhance both the performance and generalizability of whole-body grasping systems, we draw inspiration from the success of various grasp detection techniques [23]-[29]. These methods demonstrate robust performance in detecting grasp poses for diverse, unseen objects in cluttered environments, including challenging materials like transparent or specular surfaces. By integrating grasp pose detection with motion planning, these approaches consistently achieve impressive accuracy, typically exceeding a 90% grasping success rate across arbitrary object configurations in tabletop settings, as illustrated in 1.\nThis inspires us to take the best of both worlds via integrating legged locomotion with grasp detection to achieve high-performance and highly generalizable loco-manipulation. However, this combination is highly nontrivial. Directly applying grasp detection results for arm motion planning in legged robots is insufficient, as it ignores the coordination required between the robot's body and arm movements.\nTo address these challenges, we introduce QuadWBG (Generalizable Quadrupedal Whole-Body Grasping), a modular system consisting of four key modules: perception, planning, locomotion, and manipulation (see Figure 2). The perception module integrates object segmentation and grasp detection, handling object tracking and grasp pose prediction. The planning and locomotion modules function as high-level and low-level controllers, respectively, guiding the robot to approach the grasp pose. Finally, the manipulation module leverages motion planning to move the arm and execute the grasp while maintaining the body stationary.\nAt the core of this system is a key innovation: the Generalized Oriented Reachability Map (GORM). GORM acts as a metric for evaluating the reachability of the base pose relative to the target pose across six degrees of freedom. It efficiently guides the planning module during training by calculating the optimal base pose for grasping tasks. GORM also captures the robot's overall reachability from various positions and orientations, enabling the policy to select base"}, {"title": "II. RELATED WORKS", "content": "Traditionally, legged locomotion relied on control-based methods [30], [31] to achieve basic locomotion tasks, and even gymnastic maneuvers [32]. Despite their effectiveness under controlled conditions, these methods often require precise modeling and manual tuning. With the rise of deep neural networks, learning-based approaches are now more robust and adaptable, allowing robots to perform a wider range of actions with minimal intervention. These methods have demonstrated significant advancements in robust locomotion [10]\u2013[12], agile motor skills [13], [14], dynamic jumping [15], [16], fall recovery [17], [18], and complex parkour maneuvers [19], as well as excelling in challenging terrains [19], [20] and confined spaces [21], [22]. While these advancements in locomotion are impressive, incorporating manipulation capabilities could substantially amplify the versatility and practical applications.\nRecent advances in locomotion and manipulation have driven the development of integrated loco-manipulation systems, resulting in two main areas:\nM Meanwhile, other lines of work focused on achieving seamless loco-manipulation through unified whole-body control approaches. Zipeng et al. [5] initially proposed a unified policy for the simultaneous control of leg joints and manipulators. Building on this, Tifanny et al. [7] advanced the field with a whole-body force and position control. However, this approach inherits limitations from the earlier work [5], including restricted arm orientation and reduced tracking accuracy. A two-stage policy was introduced to address these issues, but accuracy remains insufficient for complex tasks [8]. Another SLAM-based method introduced a manipulation-focused whole-body controller, but it requires task-specific data and external processing, showing limited loco-manipulation capabilities [9]. All these methods rely on teleoperation, requiring human intervention and limiting autonomy. Liu et al. [6] integrated high-level task planning with low-level control, achieving some autonomy. However, this approach still struggles with low one-time grasping accuracy, limited generalization across tasks, and the need for extensive training. In our work, we aim to overcome the existing issues by taking advantage of both modular and unified methods to develop a robust controller."}, {"title": "III. METHOD", "content": "In this section, we present our framework Generalizable Quadrupedal Whole-Body Grasping (QuadWBG). This framework is divided into four key components: locomotion, perception, manipulation, and planning (Figure 2 A/B/C/D). First, a RL policy is trained to track 5D commands, allowing robust mobility. Meanwhile, the perception module generates real-time object masks and grasp poses, guiding the manipulation module as the arm transitions between tracking and grasping within the body frame. The planning module is trained based on proposed the Generalized Oriented Reachability Map (GORM) to optimize base positioning and enhance grasping performance. All policies are trained using Proximal Policy Optimization (PPO) in the Isaac Gym simulation.\nThe system is built on the Unitree B1 quadruped, equipped with a Unitree Z1 arm, Robotiq gripper, and an Intel RealSense D415 camera mounted on the wrist (Figure 2-E).\nAn agile locomotion policy is essential for achieving high-accuracy in our whole-body loco-manipulation system. To implement this, we adopt the teacher-student architecture [39], expanding the command set to five dimensions, including pitch (\u03b8) and height (h). Furthermore, to ensure stability and prevent undesirable behaviors, we constrain the sampling of command ranges based on height and pitch:\n$\\theta \\sim f(h), v_x, v_y, \\omega \\sim g(h, \\theta)$.\nThe low-level teacher observation consists of proprioceptive and privileged observations, $o^{teacher} = (o^{prop}, o^{priv})$,"}, {"title": "A. Locomotion Module", "content": "where the proprioceptive observation of $o^{prop} \\in R^{64}$ includes the commands $c^{emd} \\in R^5$, projected gravity vector $g^{hase} \\in R^3$, base angular velocity $\\omega^{oase} \\in R^3$, joint positions and velocities $q_t, \\dot{q_t} \\in R^{18}$, previous actions $a_{t-1} \\in R^{12}$, and phase variables $\\phi^{feet} \\in R^5$ for natural walking patterns[wtw]. In contrast, the privileged observation $o^{priv} \\in R^{21}$ includes of parameters not accessible in real-time, such as base linear velocities, friction coefficient, mass parameters, and motor strengths.\nWe defined our reward functions to track commands and achieve efficient, natural walking. We penalize vertical body velocity and angular velocities around roll and pitch. The behavior rewards are primarily based on [40], [11], we use the Raibert heuristic for foot placement, feet clearance to prevent tripping, and action smoothness for natural locomotion. Gait-conditioned and energy rewards further encourage stable, smooth gaits [12].\nLocomotion policy is required to accurately track 5D commands under the perturbations caused by arm tracking and grasping the target objects. In order to facilitate this, we adopt height, and pitch invariant spherical coordinate-based sampling from [5]. Furthermore, we applied extensive domain randomization techniques and observation noise to mitigate the sim-to-real gap in the robot's dynamics . Among these strategies, adding proprioceptive latency proved crucial in preventing unstable motions caused by onboard computational delays."}, {"title": "B. Perception Module", "content": "To achieve real-time tracking and precise grasp pose prediction, we utilized the Track Anything Model [42] and ASGrasp [25]. After generating the initial mask using SAM [43], real-time object masks are provided by XMem [44]. ASGrasp takes inputs infrared (IR) images and RGB and can predict accurate depth, even for transparent and specular surface. The predicted depth point cloud is then input into GSNet [23], generating more accurate 6-DoF grasp poses."}, {"title": "C. Manipulation Module", "content": "The proposed manipulation module is designed to actively track and grasp target objects while simultaneously adapting the robot's base motion. We use a motion planning approach to address control inaccuracies in end-effector control with whole-body RL policies [5], [7]. The system operates in two distinct phases: tracking and grasping.\nDuring tracking, we constrain the motion of the mounted camera within a predefined tracking sphere to minimize motion blur and prevent loss of tracking. As illustrated in 2-C, the tracking sphere is parameterized by its position and radius in the body frame. We use the Reachability Map (RM) to define the tracking sphere, ensuring that the camera operates exclusively within the dexterous workspace-where a valid inverse kinematics (IK) solution is available for arbitrary orientations. [45], [46]. For the IK solver, we apply a differential kinematics approach, as outlined in [47], to ensure continuous and smooth motion. This technique minimizes redundant movements while maximizing vision coverage and overall motion smoothness.\nThe switching mechanism is built using the Reachability Map (RM) with a threshold-based reachability criterion. At each planning step, we calculate the reachability for the selected grasp pose using the RM. Once the threshold is met, the system switch to grasping phase. Our motion planner generates trajectories online, enabling the system to adapt to small unexpected movements while moving towards the target."}, {"title": "D. Planning Module", "content": "The Reachability Map (RM) is a pose quality metric commonly used to provide priors and objectives for mobile manipulation tasks [48]\u2013[50]. The Oriented Reachability Map (ORM) efficiently represents potential base poses relative to the Tool Center Point (TCP) frame [45]. However, ORM is typically applied in platform-based mobile manipulation, where the robot's base is restricted to flat surfaces. We propose the Generalized Oriented Reachability Map (GORM), which supports robot base placements with six degree of freedom, as illustrated in Figure 3. For any target pose in the world frame $p \\in SE(3)$, the potential base-to-world distribution is computed via inverse of RM. We refine the space by removing base poses: 1) are out of locomotion range, 2) collide with the environment, and 3) fall below reachability threshold. Once a target pose is defined, GORM provides distribution of high quality potential base poses. We train the high-level policy to minimize the distance between current base pose to the nearest feasible pose:\n$GORM = exp(-(min(d_{GORM}))^2)$\nAt every high-level step, we compute the distance distribution $d_{GORM}$ by sum of Euclidean distance and geodesic distance between GORM and current base pose. We select the minimal distance from the distribution to encourage robot move close to the nearest base pose candidate. Since GORM is defined in the target pose frame, it only needs to be calculated once, making it highly efficient and well-suited for parallel training.\nTraining depth-based policies for obstacle avoidance [19], [51] and object grasping [6] is essential in vision-based control. However, these methods often introduce adding-noise and hole-filling techniques to compensate sim-to-real gap, which can degrade performance in precision tasks like grasping. Instead of relying on simulated depth images, our approach directly uses grasp pose detection results from an off-the-shelf perception module, as described in [35]. The high-level policy's observations are defined as:\n$O_t = [p^{(target)}, g^{(base)}, q_t, a_{t-1}]$\nThe vector $p^{target} \\in R^{12}$ contains a rotation matrix $rot^t \\in R^9$ and a translation vector $trans^t \\in R^3$. The gravity vector in the body frame is $g^{base} \\in R^3$, the joint positions are $q_t \\in R^{18}$, and the last action is $a_{t-1} \\in R^5$. The action comprises 5D commands $c^{emd} \\in R^5$, as discussed in Section III-A."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We first conduct experiments in simulation and then real world to validate the performance of our system. For our experiments, we deploy our system to on-board mini-computer with Intel Core i9-13900H and NVIDIA GeForce RTX 4070 Mobile (Figure 2-E).\nLocomotion module evaluation. We compared the performance and energy efficiency of several locomotion controllers, including AsymAC [52], ROA [5], StateEstimator [40], and Teacher-student [4]. All methods were trained to track velocity commands under consistent conditions and were tested in a simulation environment with substantial randomization and observation noise to replicate real-world scenarios. Performance metrics were assessed using command tracking rewards and average torque, respectively. Given our focus on balancing low energy consumption and high performance, the teacher-student method offered an optimal trade-off and was selected as the core of our locomotion controller.\nWhole-body workspace. By tracking vertical commands, our locomotion policy, informed by arm proprioceptive feedback, extends the whole-body workspace without sacrificing robustness as illustrated in Figure 4. We measured the maximum reachable grasping target positions relative to the base, analyzing the convex hull properties for both arm and whole-body workspace. Our controller increased the efficient workspace volume by 54%, and workspace area by 33%, effectively following all commands across various body poses(forward and backward leans, crouching).\nGeneralizable whole-body grasping. We follow the same simulation benchmark setting as VBC [6] which use 34 objects divided into 7 categories: Ball, Long Box, Square Box, Bottle, Cup, Bowl, Drill. In pick up experiment, we randomly reset the position and orientation of both robot and object at each round begin. We evaluate the success rate on each object for 300 trials, and the success condition is when the target object has been picked up before 150 high-level steps. We compare our method with no GORM reward and the baseline. For no GORM reward baseline, we applied the approach and assistant rewards from VBC. The aim of this comparison is to verify that our method can accurately perform mobile pick-up tasks across various object categories. As shown in Table II, our method achieves a significantly higher success rate across all tested objects. Notably, the standard deviation for end-to-end RL methods like VBC is 14.53, reflecting considerable variability. VBC performs well on simple and small objects, such as the Square Box (80%), but struggles with more complex or larger objects, such as the Drill (53.33%) and Long Box (28.57%). In contrast, our modular method has a much smaller standard deviation of 3.46, indicating more consistent performance across all object types. Our approach demonstrates consistent performance regardless of the object's size or geometric complexity. This robustness can be largely attributed to the integration of the grasp pose detector, which provides less redundant input to the high-level policy. It allows the system to adapt more effectively to different objects, ensuring consistent performance even in challenging scenarios. When the GORM reward is removed, we observe a significant drop in the success rate. This decline may be due to the basic reward approach being insufficient to lead the attached arm within the optimal workspace, especially in tasks that require precise manipulation skills.\nWe use the same environment settings as described above, we use only Cup as target object. We set four different height for target object each test for 500 trials: [0,0.3,0.75,1] meters. For each test, we perform 500 trials. After the 150th high-level step of each episode, we calculate the average reachability of the target pose relative to the body frame. We compare three policies: masked depth based policy, our policy without GORM reward, and our policy with GORM reward. Our policies take the observation mentioned in Section III-D. For masked depth based policy, we replace the goal information $p^{(target)}$ by the masked depth image which is obtained from the camera sensor on wrist in IsaacGym. The masked depth based and our method without GORM reward are trained based on approach and assistant rewards designed in VBC [6]. Shown in Table IV, the masked depth-based policy consistently under-performs across all height levels, particularly struggling when the target is positioned lower than the base's default range of motion. Without GORM, the policy shows a decline in reachability at different heights, as it fails to find the optimal base pose for grasping. In contrast, with GORM, the policy maintains high reachability across all heights by guiding the locomotion module to the best base position for grasping, maximizing performance regardless of object height. As shown in Table II, incorporating GORM significantly improves the overall grasp success rate. This suggests that guiding the robot to an optimal base pose enables the manipulation module to perform more precise motion planning, resulting in a more robust and reliable system for grasping tasks."}, {"title": "A. Evaluation in simulation", "content": "B. Evaluation in real-world\nComparison on VBC objects. We compare our system with end-to-end [6], and modular method [35]. Former work has the ability to grasp objects on different heights, whereas the latter effectively deals with only table heights settings. We replicate 14 real world objects following [6], randomly putting the object in random pose on floor and table. As shown in Table III, Our method shows the best performance across all tasks. While GAMMA [35] using model-based low-level controller, fails to grasp objects on the floor. VBC [6] shows a low success rate due to the RL based manipulation system.\nExperiment on arbitrary objects. illustrates the dataset used in our real-world testing, which consists of objects sourced from both the YCB dataset [53] and commonly encountered recyclables, such as plastic bottles, cans and crumpled paper or plastic packaging. Additionally,"}, {"title": "Experiment on transparent objects.", "content": "We conducted 10 trials on grasping transparent objects, as shown in Table V. Our results demonstrated 80% grasping accuracy relying on single camera under cluttered scenario. This challenging task was tackled through the adaptive coordination of components, inheriting the ability from perception module without performance decay."}, {"title": "V. CONCLUSION AND LIMITATIONS", "content": "In this paper, we introduced a modular whole-body manipulation system that integrates a learned 5D base locomotion policy with the novel Generalized Oriented Reachability Map (GORM) to achieve precise and robust loco-manipulation. Our approach effectively integrates accurate manipulation with coordinated whole-body locomotion, demonstrating significant improvements in handling a wide range of tasks in both simulation and real-world scenarios.\nDespite recent advancements, the system still has several limitations. The planning module lacks collision avoidance and is not optimized for navigating challenging terrains. Additionally, it relies on human annotations rather than language inputs for target object grounding. These areas will be addressed in future work."}]}