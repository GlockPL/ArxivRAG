{"title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs", "authors": ["Yiwen Tang", "Zoey Guo", "Zhuhao Wang", "Ray Zhang", "Qizhi Chen", "Junli Liu", "Delin Qu", "Zhigang Wang", "Dong Wang", "Xuelong Li", "Bin Zhao"], "abstract": "Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL, whose 7B model rivals the current state-of-the-art ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate the encoder-free architecture to be highly promising in the field of 3D LMMs.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (Touvron et al., 2023; Bai et al., 2023; Jiang et al., 2023; Cai et al., 2024) have gained unprecedented attention for their proficiency in understanding and generating complex language scenarios. Building upon these advances, many recent efforts have been made to develop Large Multimodal Models (LMMs), empowering LLMs with the capability to interpret multimodal information, such as 2D images (Liu et al., 2024; Li et al., 2024a; Zhang et al., 2024a;c;b; Li et al., 2024b) and 3D point clouds (Guo et al., 2023b; Xu et al., 2025; Guo et al., 2023a; Guo* et al., 2024; Zhang et al., 2022b; Jia et al., 2024).\nMainstream LMMs are typically encoder-based, relying on heavyweight yet powerful multimodal encoders (e.g., CLIP (Radford et al., 2021) for 2D (Liu et al., 2021; Oquab et al., 2023) and I2P-MAE (Zhang et al., 2023a) for 3D). While these pre-trained encoders offer robust multimodal embeddings enriched with pre-existing knowledge, they also introduce challenges that could limit the future advancement of multimodal understanding.\nSpecifically for 3D LMMs, the encoder-based architecture has the following potential drawbacks: (1) Point Cloud Resolution Limitation. 3D encoders are often pre-trained on point cloud data at a fixed resolution, such as 1,024 points in PointLLM (Xu et al., 2025). However, during inference, the resolution of point clouds may vary (e.g., 8,192 or 512 points). This difference between training and inference resolutions can result in the loss of spatial information when extracting 3D embeddings, leading to difficulties for LLMs to comprehend, as showcased in Figure 1 (a). (2) Embedding Semantic Discrepancy. 3D encoders are typically pre-trained using self-supervised methods like MAE (Pang et al., 2022; Tang et al., 2024a;b) and contrastive learning (Xie et al., 2020; Qi et al., 2023), but these training objectives may not align with the specific semantic needs of LLMs. In other words, they may not capture the most relevant semantics for LLMs to understand 3D objects, as visualized in Figure 1 (b). Even when a projection layer is used to connect 3D encoders with LLMs, simple MLPs are often insufficient for a complete semantic transformation. Given these issues, we ask: Is it possible to explore an encoder-free architecture for 3D LMMs, eliminating the 3D encoder and instead integrating its functionality directly within the LLM itself?"}, {"title": "2. Investigation of Encoder-free 3D LMM", "content": "In this paper, we present the first systematic investigation into the potential of an encoder-free architecture for 3D LMMs. To minimize external influences and ensure clarity, we use the pioneering and sufficiently concise PointLLM (Xu et al., 2025) as our encoder-based baseline, which consists of two progressive training stages: pre-training and instruction tuning. We evaluate the performance on 3D classification (Deitke et al., 2023) and 3D captioning (Deitke et al., 2023) tasks. Specifically, to remove the encoder while mitigating any performance degradation, we explore solutions to the following two key questions:\n(1) How can we compensate for the high-level 3D semantics originally extracted by the 3D encoder? In 3D LMMs, the raw point cloud input is first passed through a token embedding module for low-level tokenization, before being processed by the main 3D encoder, usually a Transformer (Vaswani, 2017), to generate high-level embeddings. Skipping the encoder entirely poses a challenge in capturing the complex spatial structures of 3D point clouds. To address this, we propose a strategy called LLM-embedded Semantic Encoding in the pre-training stage. First, we adopt a simple yet effective token embedding module that captures as much informative semantic content as possible. These 3D tokens are then directly fed into the LLM. Next, we aim to shift the responsibility of capturing high-level 3D semantics to the LLM itself. To facilitate this, we make the early layers of the LLM learnable, allowing them to specialize in 3D encoding. To guide this process, we explore various 3D self-supervised loss functions, such as reconstruction loss, masked modeling loss, and distillation loss, and ultimately propose the Hybrid Semantic Loss as the most effective choice.\nObservation: Our adopted token embedding module, learnable LLM layers, and Hybrid Semantic Loss achieve comparable effectiveness to that of a pre-trained 3D encoder, effectively substituting it for high-level 3D semantics.\n(2) How can we integrate inductive bias into LLMs for better perception of 3D geometric structures? Traditional 3D encoders typically embed explicit inductive bias into their architectures to progressively capture multi-level 3D geometries. For instance, models like Point-M2AE (Zhang et al., 2022a) use a local-to-global hierarchy, which is a concept also common in convolutional layers for 2D image processing (He et al., 2016). In contrast, LLMs employ standard Transformer architectures, where each layer processes the same number of tokens, representing the same semantic level across the network. In the absence of the encoder, we introduce the approach of Hierarchical Geometry Aggregation during the second fine-tuning stage. In the early layers of the LLM, we aggregate 3D tokens based on their geometric distribution using Farthest Point Sampling (FPS) and k-Nearest Neighbor (k-NN) sampling. This ap-"}, {"title": "2.1. Overall Architecture", "content": "Encoder-free modeling has been explored in the 2D vision domain to address issues related to image resolution and deployment overload. In this study, we conduct a comprehensive investigation to analyze the feasibility of adopting encoder-free architectures for 3D understanding tasks.\nTask Formulation.\nWe present the first attempt to extend the encoder-free architecture to 3D LMMs, such as PointLLM (Xu et al., 2023) and ShapeLLM (Qi et al., 2024), in order to efficiently handle the complex tasks like embodied agent (Guo et al., 2023a) and vision-language navigation. We select PointLLM as the baseline model for the exploration and evaluate the performance of different strategies on the Objaverse dataset (Deitke et al., 2023), using GPT-4 scores combined with traditional metrics as our evaluation met-"}, {"title": "2.2. LLM-embedded Semantic Encoding", "content": "The lack of the 3D encoder results in insufficient encoding of point cloud semantic information, which greatly hinders the LLM to understand the structural details of point clouds. Most existing 3D encoders use self-supervised losses to embed the high-level semantics of point clouds into the transformer, primarily categorized into four types: Masked Modeling Loss (Pang et al., 2022), Reconstruction Loss (Qi et al., 2023), Contrastive Loss (Khosla et al., 2020), and Knowledge Distillation Loss (Zhang et al., 2023a). Based on the proposed token embedding module and LLM learnable early layers, we implement and evaluate the effects of these losses on the encoder-free 3D LMM in the pre-training stage, as described in Figure 3. Finally, we propose the Hybrid Semantic Loss, which assists the LLM to learn the relationship between local spatial information in the point clouds and grasp the high-level 3D semantics.\nMasked Modeling Loss. In the pre-training stage, we apply the Masked Modeling Loss to the point tokens processed by the LLM, as shown in Figure 3 (a). Through the Far-"}, {"title": "2.3. Hierarchical Geometry Aggregation", "content": "3D encoders are designed with specific structures tailored for point clouds, such as local-to-global hierarchy (Zhang et al., 2022a) for exploring the geometric structure of the point cloud. However, in encoder-free architectures, the LLM itself does not have an explicit local modeling module. The self-attention mechanism is intended for modeling global interactions. Therefore, building upon the proposed Hybrid Semantic Loss, we explore in the instruction tuning stage how to enable the LLM to actively perceive 3D local details and complement the learned global semantics. To this end, we propose the Hierarchical Geometry Aggregation strategy in the LLM early layers.\nImplementation Details. As depicted in Figure 4, from the LLM second layer, the input point tokens ${F_{input}}_i^{M}_{i=1}$, based on their corresponding coordinates ${P_{input}}_i^{M}_{i=1}$, are downsampled using the Farthest Point Sampling (FPS), reducing the token number from M to M/2, denoted as $F_{input}'$, which serve as the local centers. Then, using the k-Nearest Neighbor (k-NN) algorithm, we obtain the neigh-"}, {"title": "3. Results and Visualization", "content": "Results. In Table 5, on the Objaverse benchmark (Deitke et al., 2023), ENEL-7B achieves the GPT score of 50.92% on the 3D object captioning task, setting a new state-of-the-art (SOTA) performance. In traditional metrics, Sentence-BERT and SimCSE reaches 48.61% and 49.31%, respectively, comparable to the ShapeLLM-13B. For the 3D object classification task, ENEL-7B outperformes prior encoder-based 3D LMMs, achieving a GPT score of 55%. Given the same training dataset as PointLLM, these results validate the effectiveness of our proposed LLM-embedded Semantic Encoding and Hierarchical Geometry Aggregation strategies for the encoder-free architecture. Additionally, on the 3D-VQA task of the 3D MM-Vet dataset (Qi et al., 2024), despite the lack of spatial and embodied interaction-related data in the training set, ENEL achieves the GPT score of 42.7%, surpassing PointLLM-7B by 1.5%.\nVisualization. In the Figure 5, we visualize the attention scores between the average text token and the point tokens in the last layer of both PointLLM and ENEL. Three object categories, including the chair, the airplane, and the desk lamp, are selected from the Objaverse dataset (Deitke et al., 2023). In the Figure 5, red indicates higher values. We observe that in encoder-based 3D LMMs, the semantic relevance between the text tokens and the processed point tokens is relatively low. In contrast, ENEL, with its encoder-free architecture, achieves a high correlation between the features of the two different modalities, with the average text token focusing on key geometric structures of the objects, such as the backrest of the chair, the wings of the airplane, and the lampshade of the desk lamp."}, {"title": "4. Conclusion", "content": "In this study, we investigate the potential of the encoder-free architecture in 3D understanding. Through a systematic analysis, we demonstrate that transferring the functionality of the 3D encoder to the LLM itself can effectively compensate for the performance degradation caused by the removal of the 3D encoder. To achieve this, we introduce the LLM-embedded Semantic Encoding strategy and the Hierarchical Geometry Aggregation strategy in the pre-training and instruction tuning stages. These strategies enable the encoding of high-level point cloud semantics while capturing critical"}, {"title": "A. Related Work", "content": "3D LMM. Recent advancements in integrating large language models (LLMs) with 3D data have led to significant progress in both object-level and scene-level understanding. At the object level, early approaches like (Hong et al., 2024) utilize 2D rendering to leverage 2D LLMs, but this sacrifices geometric details. More recent models, including Point-Bind LLM (Guo et al., 2023b), PointLLM (Xu et al., 2023) and ShapeLLM (Qi et al., 2024), directly encode point clouds and align them with LLMs, by combining the 3D encoder with a powerful language model, effectively fusing geometric, appearance, and linguistic information. At the scene level, models like Chat-3D (Wang et al., 2023) and Scene-LLM (Fu et al., 2024) focus on understanding complex spatial relationships through dialogue and tasks like captioning. Scene-LLM (Fu et al., 2024) enhances embodied agents' abilities in interactive 3D indoor environments by integrating both scene-level and egocentric 3D information. Grounded 3D-LLM (Chen et al., 2024b) utilizes referent tokens to reference specific objects within 3D scenes, enabling tasks such as object detection and language grounding.\nEncoder-free Vision-Language Models. Traditional vision-language models (VLMs) often rely on vision encoders to extract visual features before processing them with language models, integrating image encoders like CLIP (Radford et al., 2021) and DINO V2 (Oquab et al., 2023). However, recent efforts have explored encoder-free VLMs for their simplicity. Approaches like (ChameleonTeam, 2024; Xie et al., 2024) use VQ tokenizers (Esser et al., 2021) or linear projection layers (Diao et al., 2024a; Chen et al., 2024a) to represent images. Fuyu-8B (Bavishi et al., 2023), a pure decoder-only model, directly processes image patches through linear projections, handling high-resolution images but showing only average performance. The EVE (Diao et al., 2024b) eliminates the need for a separate vision encoder by bridging vision-language representation within a unified decoder and enhancing visual recognition capabilities through additional supervision."}, {"title": "B. Experimental Settings", "content": "Implementation Details. We use the LLaMA model (Touvron et al., 2023) as our LLM backbone, with the 7B Vicuna-v1.1 (Chiang et al., 2023) checkpoint as the default setting. In the token embedding layer, the point cloud is first processed by a linear layer to expand its dimension from 6 to 288. The input point cloud initially consists of 8192 points, followed by three iterations of farthest point sampling (FPS), reducing the size to 512, 256, and 128, respectively. After each FPS operation, k-Nearest Neighbors (k-NN) is applied with a cluster size of 81. And geometric features are extracted using triangular encoding, followed by linear layers that progressively increase the dimension to 576, 1152, and 2304. Finally, the projection layer maps the features to the LLM dimension of 4096. In the pre-training stage, we unfreeze the first four LLM layers. Within the LLM-embedded Semantic Encoding strategy, Hybrid Semantic Loss applies masked modeling to 30% of the tokens and reconstructs the patches for the remaining 70% visible tokens. In the instruction tuning stage, we apply geometric aggregation in the second LLM layer, reducing the number of point tokens from 128 to 64. MaxMean pooling is used to retain more information. After passing through two LLM layers, the geometric aggregation is applied in the fifth layer to restore the point size count to 128."}, {"title": "C. More Experiments", "content": "Additionally, the GPT evaluation prompts for classification and captioning are identical to those used in PointLLM, while the prompts for QA follow those in ShapeLLM."}, {"title": "C.1. Variants of Point Cloud Self-Supervised Learning Losses.", "content": "As seen in Figure 6 (a), in the Masked Modeling Loss, after the learnable tokens are processed by the LLM, the tokens are transformed to the point patches ${G_{pre}}_i^{M*r} \\in \\mathbb{R}^{M*r \\times k \\times 3}$ through a linear layer. We utilize the $l_2$ chamfer distance to align the predicted $G_{pre}$ with the point patches $G_{mask}$ corresponding to the masked tokens, reconstructing the spatial information. The optimization can be written as\n$$\\frac{1}{M*r} \\sum_{i=1}^{M*r} ( min_j ||a_i - b_j||_2 + min_i ||b_i - a_j||_2 ),$$\nwhere a = $G_{pre}$ and b = $G_{mask}$.\nAs shown in Figure 6 (b), after the point feature tokens ${F}_i^M$ are encoded by the LLM, the Mean Squared Error (MSE) is computed between the predicted $F_{pre}$ and the ground truth $F$. The optimization can be written as\n$$\\mathcal{L}_{mask} = \\frac{1}{M} \\sum_{i=1}^{M} (|| F_{pre_i} - F_i ||_2 ).$$\nFinally, in the Figure 6 (c) Hybrid Semantic Loss, the masked tokens and the corresponding patches are referred to as ${F_{mask}}_i$ and ${G_{mask}}_i^{M*r}$, respectively. The remaining tokens are denoted as${F_{vis}}_i^{M*(1-r)}$ and ${G_{vis}}_i^{M*(1-r)}$"}, {"title": "C.2. More Ablation Experiments", "content": "We begin the ablation experiments starting from the ENEL, which is the reverse order compared to the experiments in the main text, as showcased in Tabel 6\nThe Effects of LLM-embedded Semantic Encoding Strategy. In the Table 6, on the basis of ENEL, removing the Hybrid Semantic Loss during the pre-training stage significantly degrades performance. The GPT-4 score for the captioning task drops from 50.92% to 47.19%, and the GPT-4 score for the classification task decreases to 50.61%. This is because the proposed self-supervised learning function for point clouds effectively captures the detailed structures and high-level semantics of the point clouds.\nBased on ENEL, we find that setting the mask ratio in the Hybrid Semantic Loss to 30% consistently yields better results than 60%. Additionally, the configuration where the masked token part predicts features while the visible token part reconstructs patches outperforms the reverse setting\u2014where the masked token part predicts patches and the visible token part reconstructs features. This phenomenon can be explained as follows: a mask ratio of 30% retains critical information while facilitating the model to effectively utilize the visible tokens to derive the masked parts. When the mask ratio is set too high, the model fails to learn the global context knowledge adequately. Moreover, when the masked token part is tasked with predicting features, the model focuses on learning the high-level context semantics, while the patch reconstruction aids in accurately capturing low-level details. In contrast, when the masked token part predicts patches, the model becomes excessively dependent on local features during the process of semantic reconstruction.\nThe Effects of Hierarchical Geometry Aggregation Strategy. After removing the gating mechanism in the self-attention of the aggregation operation, the performance drops to 49.26% and 53.50% on the captioning and classification tasks, respectively. The gating mechanism helps the model to adaptively filter information, allowing it to focus on more discriminative features. Without the dynamic adjustment to focus on different parts of the input, the generated text from the LLM lacks accuracy and coherence, leading to a decrease in performance.\nThe performance generally degrades with an increasing number of aggregation and propagation operations. This degradation can be attributed to the progressive loss of local geometric details through repeated aggregation operations, while multiple propagation operations typically rely on interpolation which tends to amplify high-frequency noise. We observe that increasing the number of LLM layers between the final aggregation operation and the first propagation operation leads to improved performance. This suggests that cascaded aggregation operations necessitate deeper architectural capacity for high-level feature abstraction, as insufficient network depth may lead to degradation of hierarchical representations. Furthermore, the presence of LLM layers between each aggregation or propagation operation enhances performance by allowing the model to process and transform compressed information. Through self-attention mechanisms, these intermediate layers can recapture and restore details lost during the aggregation process."}, {"title": "D. Model Output", "content": "In Figure 7, we showcase more model output, where our ENEL provides precise and diverse responses with multi-modal 3D instruction input."}]}