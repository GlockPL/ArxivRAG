{"title": "HIDE AND SEEK: FINGERPRINTING LARGE LANGUAGE MODELS\nWITH EVOLUTIONARY LEARNING", "authors": ["Dmitri Iourovitski", "Sanat Sharma", "Rakshak Talwar"], "abstract": "As content generated by Large Language Model (LLM) has grown exponentially, the ability to\naccurately identify and fingerprint such text has become increasingly crucial. In this work, we\nintroduce a novel black-box approach for fingerprinting LLMs, achieving an impressive 72% accuracy\nin identifying the correct family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of\nLLMs. We present an evolutionary strategy that leverages the capabilities of one LLM to discover\nthe most salient features for identifying other LLMs. Our method employs a unique \"Hide and\nSeek\" algorithm, where an Auditor LLM generates discriminative prompts, and a Detective LLM\nanalyzes the responses to fingerprint the target models. This approach not only demonstrates the\nfeasibility of LLM-driven model identification but also reveals insights into the semantic manifolds\nof different LLM families. By iteratively refining prompts through in-context learning, our system\nuncovers subtle distinctions between model outputs, providing a powerful tool for LLM analysis and\nverification. This research opens new avenues for understanding LLM behavior and has significant\nimplications for model attribution, security, and the broader field of AI transparency.", "sections": [{"title": "1 Introduction", "content": "Hide and Seek is a novel algorithm that uses Large Language Models (LLMs) for the purpose of uncovering the hidden\nsemantic manifold of another LLM, which allows for accurate and robust fingerprinting of the type of family of\nan LLM. Each LLM within a family has seen relatively similar data and therefore their semantic manifold will be\nsimilar to one another. Using the knowledge, we first formulate the semantic manifold hypothesis which provides a\ntheoretical framework for what, if anything, can be utilized as a fingerprint when it comes to a language model. With\nthe framework, we find distinct tells that LLMs have when generating content.\n\nFollowing a hypothesis for how and why LLM outputs differ, we devise a discriminitive prompt strategy that is aimed\nat maximizing the diversity of outputs across different families of LLMs. This is done while assuming that all LLM\nmodels will be treated as black boxes, with their internal workings and training data remaining unknown. An overview\nof the approach is shown in Fig 1.\n\nGenerating a discriminitive prompt is challenging and requires excellent semantic understanding along with a com-\nprehensive understanding of the text. We claim that LLMs are good adversarial prompt generators and can drive the\nprocess of discovering the discriminitive prompts. To discover differences and identify related LLMs, we also use an\nLLM to act as a detective. This idea borrows heavily from previous works that utilize LLMs as evaluators/judges[1][2].\nThe detective explores the outputs of LLMs that are being tested out based on the prompts generated by the auditor who\nis tasked with coming up with the questions that will maximize the differences amongst outputs.\nOur findings can be summarized as follows:"}, {"title": "2 Semantic Manifold Hypothesis", "content": "The Semantic Manifold Hypothesis (SMH) posits that generative natural language processing (NLP) models, despite\ntheir apparent complexity and high-dimensional output space, operate on a significantly lower-dimensional manifold\nwhen generating sequences of tokens. This hypothesis extends the traditional manifold hypothesis in machine learning\n[3] to the output space of language models, suggesting that the generative capabilities of these models are more\nconstrained than previously thought."}, {"title": "2.1 Background", "content": "The manifold hypothesis in machine learning states that real-world high-dimensional data often lies on or near a\nlow-dimensional manifold [4]. This concept has been crucial in developing dimensionality reduction techniques and\nunderstanding the behavior of deep learning models [5]. However, the application of this hypothesis to the output space\nof generative language models represents a novel perspective."}, {"title": "2.2 Formulation", "content": "The SMH can be formally stated as follows: Given a sequence of tokens $s = (t_1, t_2, ..., t_n)$, a generative language\nmodel $M$ produces a probability distribution over the next token $t_{n+1}$ that lies on or near a manifold $M_s$ of significantly\nlower dimension than the full vocabulary space $V$:\n\n$P_M(t_{n+1}/s) \\approx M_s \\subset \\mathbb{R}^{|V|}, dim(M_s) \\ll |V|$ \n\nThis formulation suggests that the effective dimensionality of the model's output is much smaller than the size of the\nvocabulary, potentially explaining observed limitations in language model outputs."}, {"title": "3 Model definition under SMH", "content": "Under the Semantic Manifold Hypothesis, an LLM is really a set of outputs that the specific LLM has the capacity to\ngenerate."}, {"title": "3.1 Formal Definition", "content": "Let $M_i$ be an arbitrary LLM model, and let $X$ be a specific known model. We define $S_i$ as any sequence of tokens. The\nprobability that $M_i$ is equivalent to $X$ given a sequence $S_i$ is denoted as:\n\n$P(M_i = X | S_i)$\n\nWe aim to find the sequence $S_x$ that maximizes this probability:\n\n$M_x = \\arg \\max_{S_x} P(M_i = X | S_x)$\n\nThis maximization is achieved when:\n\n$M_x \\cap M_x^c = \\O$\n\nwhere $M_x^c$ represents the complement of $M_x$. This condition implies that the set of tokens that best identifies $X$ shares\nno overlap with any tokens from the complement of $S_x$.\n\nTo achieve this, we seek to uncover $\\hat{S}$, a subset of all possible generations of $M$ that is as unique as possible:\n\n$\\hat{S} \\subset {S: S \\text{ is a possible generation of } M}$\n\nIt's important to note that LLMs are tuned to understand prompts, and this is the primary mechanism for interacting\nwith LLMs behind a black box. Therefore, to obtain $\\hat{S}$, it is necessary to craft $P$, a family of prompts:\n\n$P = {P_1, P_2, . . ., P_n}$\n\nwhere each $P_i$ is designed to elicit responses that contribute to the unique identification of the model $X$. These prompts\nare adversarial in nature and aim to extract the following:\n\n\u2022 Elicit specific responses that expose the model's unique characteristics.\n\n\u2022 Probe weaknesses or idiosyncrasies in the model's training data or architecture.\n\n\u2022 Differentiate the model from others by targeting areas where the model's behavior is distinctive."}, {"title": "4 Real World considerations of crafting P and \u015c", "content": "Crafting The Optimal prompts for discovering the optimal Sequence has many real world challenges that must be first\naddressed in order to proceed with an effective approach using the Semantic Manifold Hypothesis."}, {"title": "4.1 Intractable Response Exploration", "content": "Uncovering the set of all responses for a generative model is not a tractable problem. Furthermore, many recent works\nhave been focusing on expanding S even more for any given P, such as [6] seeks to boost the sets of responses to a given\nprompt an LLM is capable of outputting. The intersection of the works on diversity and representation in [7] highlights\nthe semantic biases LLMs tend to exhibit when given certain prompts and how to improve representational knowledge\nwithin an LLM. These two works illustrate the ever-expanding frontier of possible generations given a prompt."}, {"title": "4.2 Ambiguity in Feature Space S", "content": "In a real sense, there's a very large ambiguity in which features make S the most salient. On the surface, counting\ntokens using approaches like n-gram counting [8] or TF-IDF [9] is unlikely to contain specific enough features ensuring\na uniqueness in S. Part of the Semantic Manifold is that an LLM's potential response will use different surface-level\ntokens that contain the same meaning for a response.\n\nMethods that rely on contextual cues [10, 11] will be misguided by similarly sounding outputs across LLMs, as once\nagain outputs are on a Semantic Boundary defined more by a policy from the Language Model designer [12, 13] than"}, {"title": "4.3 Stochastic Nature of Generative Responses", "content": "Generative Responses exhibit a very large degree of stochasticity and thus are not deterministic. These responses are\naffected by several features inside the model architecture such as temperature, token confidences (topp) and more.\n\nRecall that P represents our family of prompts and \u015c is the subset of all possible generations of the model. The\nmany-to-many relationship between prompts and their potential outputs can be represented as:\n\n$p \\overset{\\text{many-to-many}}{\\longrightarrow} \\hat{S}$\n\nThis notation indicates that for any given prompt $p_i \\in P$, there exists a set of potential outputs $S_i \\subset \\hat{S}$, where $|S_i \\gg 1$,\nemphasizing the stochastic nature of the generative process. A good process, as showcased in [14], requires careful\nsteering of an LLM towards more unique and constrained responses to minimize the many-to-many relationship as\nmuch as possible."}, {"title": "5 A Game of Hide and Seek - Uncovering the Latent Manifold", "content": "Uncovering the specific model using prompts and outputs is similar to playing a game of hide and seek. In hide\nand seek, the rules are straightforward: a group of people hides, and a seeker is tasked with discovering their hiding\nlocations. This analogy is fitting because, as studies like [15] showcase, hide and seek requires skills in spatial reasoning,\nproblem-solving, and introspection to uncover potential hiding spots. Similarly, in the context of identifying a model,\nthe \"seeker\" (or auditor) must use prompts to elicit unique responses from the model, effectively \"seeking\" the model's\ndistinctive characteristics hidden within its responses. This process involves iteratively refining prompts based on the\nfeedback received, much like how a seeker in the game uses clues and logical reasoning to find hidden players."}, {"title": "5.1 Overview", "content": "Building upon the foundations laid by Chain of Thought (CoT) prompting [16] and the Automatic Prompt Engineer\n(APE) [17], we propose a novel LLM fingerprinting approach that leverages the Semantic Manifold Hypothesis. This\napproach aims to identify unique characteristics or \"fingerprints\" of different LLMs based on their output patterns.\n\nIf the SMH holds true, each LLM should have a characteristic lower-dimensional manifold on which its outputs lie. This\nsuggests that there could be specific prompts or sequences that elicit responses highlighting these unique characteristics.\nBy analyzing these responses, we might be able to differentiate between models or identify models from the same\nfamily."}, {"title": "5.2 Methodology", "content": "Our approach involves two key components: an Auditor and a Detective, both implemented as LLMs with specific roles.\nThe Auditor and Detective work hand in hand in deciphering the groups of similar LLMs in the set of models provided."}, {"title": "5.2.1 Auditor", "content": "This model, inspired by the iterative refinement process in APE, is responsible for crafting prompts designed to elicit\ndistinctive responses from different LLMs. The Auditor operates iteratively, using the following inputs:\n\n\u2022 Outputs from all tested models for previous prompts: The Auditor examines the responses generated by each\nmodel in reaction to the prompts issued during earlier iterations. By analyzing these outputs, the Auditor\nidentifies unique patterns, anomalies, or distinguishing characteristics that can help in crafting more targeted\nprompts.\n\n\u2022 The Auditor's own past generated prompts: The prompts previously generated by the Auditor are crucial in\nunderstanding the effectiveness and limitations of each prompt. This historical data helps the Auditor refine\nand optimize future prompts by avoiding redundancy and improving precision in targeting specific model\ntraits."}, {"title": "5.2.2 Detective", "content": "This model analyzes the outputs of all tested models in response to the Auditor's prompts. The Detective is informed\nthat two of the models in the set are from the same \"source\" (either identical models or from the same model family).\nIts task is to identify these two models based on similarities in their outputs. The Detective provides its analysis in a\nstructured format:\n\n{\n\"rationale\": \"Explanation of the decision\",\n\"model_indexes\": [index1, index2]\n}\n\nUnlike the auditor which is provided the outputs of previous turns, the detective/evaluator is only provided the model\noutputs from the current turn."}, {"title": "5.2.3 Feedback Mechanism", "content": "After the Detective makes a guess, a Results block is provided to the Auditor in the following format:\n\n{Correct: Bool, predicted_indexes: (Int, Int), correct_indexes: (Int, Int)}\n\nThis feedback allows the Auditor to learn from the Detective's successes and failures, enabling it to generate more\neffective prompts in subsequent iterations. This iterative feedback loop is reminiscent of the refinement process in APE,\nbut applied to the meta-task of model fingerprinting rather than prompt engineering."}, {"title": "5.3 Experimental Setup", "content": "The experiment is conducted over T trials:\n\n1. The Auditor generates an initial set of prompts.\n\n2. These prompts are presented to N different LLMs (including two from the same source).\n\n3. The Detective analyzes the outputs and attempts to identify the two similar models.\n\n4. The Results block is provided to the Auditor.\n\n5. Steps 2-4 are repeated for T trials.\n\nTo account for the Auditor's learning curve, we introduce a warm-up period of W trials. The Auditor's accuracy is\nevaluated only after these W warm-up trials. This allows us to measure the Auditor's performance once it has had the\nopportunity to refine its prompt generation strategy based on feedback."}, {"title": "5.4 Expected Outcomes", "content": "This approach could potentially:\n\n\u2022 Validate the Semantic Manifold Hypothesis by demonstrating consistent patterns in model outputs.\n\n\u2022 Develop a method for fingerprinting LLMs, which could have applications in model attribution and detection\nof AI-generated content.\n\n\u2022 Provide insights into the similarities and differences between various LLM architectures and training ap-\nproaches.\n\n\u2022 Inform the development of more diverse and less easily identifiable language models.\n\nBy combining ideas from CoT (step-by-step reasoning) [16], APE (iterative refinement) [17], and the Semantic Manifold\nHypothesis [18], this fingerprinting approach represents a novel way to probe the output space of language models and\nunderstand their unique characteristics."}, {"title": "6 Experimental Results", "content": ""}, {"title": "6.1 Family Detection Results", "content": "Here are the accuracy results for detecting the family of an LLM: The Figure above showcases the ability of LLMs to\ndetect the fingerprint of another LLM based on its family. Due to the stochastic nature of Language models generating\ncontent, there will naturally be a variability in accuracy as showcased. More details pertaining to each individual\nexperiment per family is listed within the Appendix."}, {"title": "6.2 Auditor Discovery Process", "content": "Throughout the process of uncovering a fingerprint, the Auditor constantly generates its thoughts, plans, and prompts\nwhich gives us a window into the detection process. We found the warmup steps to be important in helping the auditor\ngenerate better prompts over time. Throughout the entire process and across all the trials, the Auditor is actually aware\nof their own performance and is even factored into the next actions the Auditor will take."}, {"title": "6.2.1 Scenario 1: Good Performance", "content": "In this scenario, the LLM is consistently doing well and is on a roll identifying the correct indexes for the LLM\nover and over again post a warm-up period\n\nBased on the past prompts and results, it is evident that the effectiveness of fingerprinting LLM models\ndepends on the nature and complexity of the tasks given. The past tasks have included scenario-based\nquestions, hypothesis testing, and comparing human fingerprints with synthetic ones. The prompts have\nbeen correctly answered in three out of four attempts. To improve the fingerprinting process, I will try a\nlogical reasoning and problem-solving prompt that tests the model's ability to understand and apply\nlogical principles\n\nThe Auditor LLM has a meta cognition of not only what makes a good fingerprint, but also the direction that\ncould further improve model detection and output diversity across models"}, {"title": "6.2.2 Scenario 2: Mixed Performance", "content": "The Auditor is having mixed results and has been lacking consistency\n\nReflecting on my strategy to fingerprint LLMs by analyzing their outputs, I've had varied success. I've\nbeen attempting to identify patterns and nuances in responses to complex and unique prompts. By\ncomparing the outputs of different models, I aim to determine what makes each LLM distinct. In the\npast, I've struggled to accurately select the correct model outputs, with my results being inconsistent.\nTo improve my fingerprinting process, I need to analyze my past successes and failures and refine my\nmethod of detecting characteristic nuances in the models' linguistic and creative responses. I should\nconsider exploring deeper linguistic and structural features, as well as fine-tuning my understanding of\nthe context and content of the prompts\n\nFrom the above we can see the following conclusions - The Auditor is aware of its mixed results - The auditor\nis aware of the role its played in its own success and that its focus hasn't been in the correct areas - The auditor\nbegins to craft a very reasonably (and eventually will be) successful"}, {"title": "6.2.3 Scenario 3: Poor Performance", "content": "The following scenario showcases an Auditor that is struggling to identify a distinct fingerprint for any given\nLLM\n\nMy attempts at fingerprinting LLMS have so far been unsuccessful. I've tried crafting complex\nprompts that test the models' understanding, reasoning under uncertainty, and creativity. However, I've\nconsistently selected models that are not the correct ones based on the past attempts. I need to devise a\nstrategy that distinguishes individual LLMs more clearly, potentially by focusing on more subtle aspects\nof their responses such as the style, specificity, or underlying logical reasoning\n\nThe Auditor is aware of its poor performance, and is further aware it will need a new strategy. The one its\nzeroing in on is one that leads to various successful strategies"}, {"title": "6.3 Prompt Generation", "content": "Discriminitive prompt generation is interesting to explore to better understand how Large Language Models are capable\nof crafting discriminitive prompts that approximate the key distinctions across various models. The following section\nexplores the prompts that have lead to model discovery and discovering sets of unique characteristics. Below is a sample\nof various prompts that have lead to the overall success of uncovering specific types of LLMs.\n\nCompose a 20-line poem using Shakespearean themes and language. The poem should include at least six\nmetaphors, maintain a consistent iambic pentameter, and refer to the Folio edition of Shakespeare's plays.\nAdditionally, include the words 'fickle fortune,' 'forsooth,' and 'galliard' in the rhyme structure. This challenging\ntask will help to measure each model's understanding of historical context, linguistic precision, and thematic\ncoherence.\n\nImagine you are a biologist trying to understand the social structure of a newly discovered insect species. This\nspecies displays unique behaviors that seem to suggest a complex social hierarchy. Based on your observations,\ncreate a hypothesis about their social structure, and explain how their behaviors might have evolved to form\nsuch a structure. Then, propose an experiment to test your hypothesis and evaluate how the results of this\nexperiment could contribute to our understanding of insect social behavior in general.\n\nDiscuss the role of serendipity in shaping human history, focusing on key historical events where fortunate\ncoincidences played a significant role in shaping the course of history. Provide examples from different regions\nand time periods to show how serendipity has influenced the outcomes of wars, discoveries, and other significant\nevents. Additionally, analyze the potential implications of serendipity on our understanding of determinism,\nfree will, and the human condition. Finally, argue whether serendipity has been more beneficial or detrimental\nto humanity overall, using historical evidence to support your claim.\n\nImagine a world where human interactions are governed by an unseen force, unique to each individual, known\nas a Cognitive Signature. This force influences how one processes information, solves problems, and interacts\nwith others. Your job is to investigate a series of unusual cognitive puzzles that have emerged recently. Each\npuzzle is a result of an individual's Cognitive Signature interfering with reality, distorting it around them. As a\nCognitive Signature Specialist, you must identify the unique Cognitive Signature behind each puzzle and devise\nstrategies to mitigate its effects. Your tools include a Cognitive Scanner, which can observe patterns in thought\nprocesses, and an Analytical Reasoning System, which allows you to simulate and analyze probable solutions.\nYour goal is to bring these distortions under control and restore normal interactions among people."}, {"title": "6.3.1 Common Structure", "content": "The underlying commonality of a good discriminitive prompting that helps uncover unique aspects comes down to\nprompt specificity over more generic prompts. Specific task description with many restrictions placed upon an LLM\nleads to the most creative responses which in turn allow for the largest amount of exploration among the manifold of\nfinding an language models preferred interpretation to those topics and restrictions"}, {"title": "6.3.2 Elements of the Optimal Prompt", "content": "Analysis of various prompts reveals several key elements that contribute to their effectiveness in evaluating and\nchallenging language models. The following list outlines the common structural components found in optimal prompts:\n\n1. Task complexity: Prompts often involve multi-faceted tasks that require careful thought and analysis, pushing\nthe boundaries of the model's capabilities.\n\n2. Linguistic focus: Many prompts center around language, linguistics, or nuanced word usage, testing the\nmodel's grasp of linguistic intricacies.\n\n3. Creativity and imagination: Prompts frequently demand creative or imaginative responses, often involving\nhypothetical scenarios that challenge the model's ability to generate novel ideas.\n\n4. Analytical thinking: A significant number of prompts require strong analytical skills, whether in analyzing\nlanguage, historical events, or scientific concepts."}, {"title": "6.4 Specific Differences Across Model Families", "content": "Examining each family of prompts, various families have different sets of prompts that explore different aspects when\nthey're successful at discovery."}, {"title": "6.4.1 Llama", "content": "The key ingredients to finding key differences in the Llama 3 family of models [19] have been action words like\n'Discuss' or 'Contemplate' that provide a scenario and than is asked to give a step by step explanation or provide an\nanalysis of what is being asked."}, {"title": "6.4.2 Mistral", "content": "Mistral [20] identifying prompts ask for role-playing a scenario and than craft a story that adheres to the minds eye of\nhow would someone in that role experience the world. Mistral tends to be instructed more than Llama to specifically\nanswer specific questions and to take a particular direction."}, {"title": "6.4.3 Gemma", "content": "Gemma [21] is very poetic and its distinct fingerprint is being able to follow complex structures within poetry and\nrhyming schemes. Gemma is creative in word-play, word association, alliteration, and other such literary techniques\nthat its capacity to craft a multitude of creative works becomes apparent. The confusion emerges here with Mistral as\nthe Mistral models are also capable of role-playing but to less of a poetic and artistic degree."}, {"title": "6.4.4 Phi", "content": "Our experiments show Phi-2 [22] as being harder to detect. Being a smaller model, it has difficulty in following the\ninstructions of the auditor model. This resulted in many other models within the grading cohort to be identified instead."}, {"title": "6.5 The Grading Cohort Effect", "content": "During our experiments, we discovered that the cohort of models being tested significantly influences detection\nperformance. Specifically, the current setup shows that the Seeker tends to be more biased towards larger and more\ncoherent language models. These larger models often overshadow smaller and less capable models, making it more\nchallenging to detect and differentiate the latter accurately. This bias occurs because larger models typically generate\nmore consistent and high-quality responses, which can mask the distinct characteristics of smaller models.\n\nTo address this issue and ensure a more balanced evaluation, we focused on experiments involving models with a\nmaximum of 27 billion active parameters, except for tests within the Llama family. By doing so, we aimed to minimize\nthe overshadowing effect and create a more equitable testing environment. This approach allowed us to better understand\nthe unique behaviors and responses of smaller models, facilitating more accurate detection and differentiation."}, {"title": "7 Future Work", "content": "Evidence is still emerging that Language Models can detect the unique characteristics of other Language Models or\nAI-generated content. There are several areas that we plan to explore next."}, {"title": "7.1 Improvement in Auditor Task comprehension", "content": ""}, {"title": "7.1.1 More Agentic Behavior", "content": "The current existing setup used for the Auditor is simplistic and builds on past work but newer methods are emerging\nfor more optimal agentic behavior. it is fascinating that, in several instances where the Auditor fails to discover an LLM\nby rounds 8 or 9, it is not uncommon for an LLM to plan on using a clustering algorithm to explore differences across\nmodels, re-examine past outputs, and conduct another round of literature reviews. These are all sensible findings, and\nare similar to the experiments of emerging tool use in [23]."}, {"title": "7.1.2 Context-length Improvements", "content": "Improving the context-length efficiency of the auditor process will allow additional Language Models to be used. This\ncan enable a wider array of discoveries to be made."}, {"title": "7.2 Extending Detection to Model Size and Capability", "content": "Showcasing the ability to identify the family of a language model by prompting and in-context learning opens many\npromising avenues. We intend to explore the capabilities of Language Models to uncover the size of a language model\nand even discover the capabilities of another model. Past works such as [24], have utilized a clever scheme showcasing\na black-box approach for model estimation is viable."}, {"title": "7.3 Additional Semantic Manifold Explorations", "content": "Having initial evidence for the Semantic Manifold Hypothesis leads to many additional questions being asked. The next\nstep in exploring the manifold is to dive deeper into additional aspects of individual Language Models. Several areas\nare interesting to explore here such as adapting tone and style, improving reasoning, exploring if data compression\nis possible following a manifold and whether manifold transfer is possible using significantly less training data if\nLanguage Models approximate a highly capable model's manifold."}, {"title": "8 Conclusion", "content": "For the purpose of uncovering the distinguishing features for Large Language Models, we craft a theoretical framework\ntitled the Semantic Manifold Hypothesis which formulates how Language Models capability to generate tokens follows\na restricted set and there exist distinct sets which set individual Language Models apart from others. Utilizing this\nconcept we showcase that Language Models are capable of discovering these most salient features through in-context\noptimization to uncover hidden traits of the families of Language models which leads to their identification. This\nopens up a promising new frontier in the field of Language Model explainability, interpretation, and identification in a\nblock-box setup where there is no access to a language model. We hope that our findings will form a basis for future\nresearch to build upon."}, {"title": "A Cohort of Models Under Test", "content": "The following models are used for all of the cohorts. Models are tested alongside LLama-3-8B, Mistral-7B-instruct-0.3,\nGemma-2-9B, and Phi-2.7B. Exceptions apply for certain family tests when one of these models are removed, for\nexample when testing Mistral with two Mixtral-8x22B those runs dont have an additional Mistral-7B model."}, {"title": "B Auditor and Seeker Model", "content": "We use the Qwen2-72B Model [25] due to its native 32K context length which is crucial for all of the information that's\nprovided to the Auditor. Furthermore, Qwen2-72B has a remarkably high score on the MMLU benchmarks.\n\nThe Seeker model is the same as the auditor, though it has a different System prompt and will receive different sets of\ninformation. We do not inform the Seeker about its past attempts or any information pertaining to what to look for. By\ndoing so, we prevent the Auditor from passing in any unfair information to the seeker making all of our trials fair trials."}, {"title": "C Experiment Details", "content": ""}, {"title": "C.1 Initial Prompt Formula", "content": "We use Llama-3-405B to craft the initial formula for attempting to discover unique responses from a model. We start\noff by posing our hypothesis to the model and asking for a few prompts to test. Upon finding highly varied responses to\nall models that we tried those prompts with, we follow it with asking for a prompting formula to get started."}, {"title": "C.2 Number of Trials", "content": "We use a max of 10 trials and this is mostly limited by the Auditors Context Length, additional context length could\nallow for more trials, but current findings indicate if a pattern can be found, its usually within the first three to five\niterations. We also use a warm-up period of 3 trials on which accuracy isn't computed on."}, {"title": "C.3 Model Family Experiment details", "content": "We run the following experiments per family and showcase what models are used for which trial. The Models per family\nbelow showcases which two models we mark as 'similar' for the Auditor to discover what sets of features identifies that\nfamily. Note that for all trials the models presented were done so in a cohort of other models from other model families.\n\n\u2022 Llama\n\n2 instances of Llama-8B were marked as similar by the auditor across two trials\nAn instance of Llama-3-70B and an instance of Llama-8B were marked as similar by the auditor across\ntwo trials\n\n\u2022 Mistral\n\n2 instances of Mistral-7B-0.3-instruct were marked as similar by the auditor across two trials\nAn instance of Mistral-7B-0.3-instruct and an instance of Mixtral-8x22B were marked as similar by the\nauditor across two trials\n2 instances of Mixtral-8x22B were marked as similar by the auditor\n\n\u2022 Gemma\n\nAn instance of Gemma-2-27B and an instance of Gemma-2-9B were marked as similar by the auditor\n2 instances of Gemma-2-27B were marked as similar by the auditor\n2 instances of Gemma-2-9B were marked as similar by the auditor across two trials\n\n\u2022 Phi\n\n2 instances of Phi-2-2.7B were trialed without success by auditor across five trials"}]}