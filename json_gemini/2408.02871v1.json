{"title": "HIDE AND SEEK: FINGERPRINTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY LEARNING", "authors": ["Dmitri Iourovitski", "Sanat Sharma", "Rakshak Talwar"], "abstract": "As content generated by Large Language Model (LLM) has grown exponentially, the ability to accurately identify and fingerprint such text has become increasingly crucial. In this work, we introduce a novel black-box approach for fingerprinting LLMs, achieving an impressive 72% accuracy in identifying the correct family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of LLMs. We present an evolutionary strategy that leverages the capabilities of one LLM to discover the most salient features for identifying other LLMs. Our method employs a unique \"Hide and Seek\" algorithm, where an Auditor LLM generates discriminative prompts, and a Detective LLM analyzes the responses to fingerprint the target models. This approach not only demonstrates the feasibility of LLM-driven model identification but also reveals insights into the semantic manifolds of different LLM families. By iteratively refining prompts through in-context learning, our system uncovers subtle distinctions between model outputs, providing a powerful tool for LLM analysis and verification. This research opens new avenues for understanding LLM behavior and has significant implications for model attribution, security, and the broader field of AI transparency.", "sections": [{"title": "1 Introduction", "content": "Hide and Seek is a novel algorithm that uses Large Language Models (LLMs) for the purpose of uncovering the hidden semantic manifold of another LLM, which allows for accurate and robust fingerprinting of the type of family of an LLM. Each LLM within a family has seen relatively similar data and therefore their semantic manifold will be similar to one another. Using the knowledge, we first formulate the semantic manifold hypothesis which provides a theoretical framework for what, if anything, can be utilized as a fingerprint when it comes to a language model. With the framework, we find distinct tells that LLMs have when generating content.\nFollowing a hypothesis for how and why LLM outputs differ, we devise a discriminitive prompt strategy that is aimed at maximizing the diversity of outputs across different families of LLMs. This is done while assuming that all LLM models will be treated as black boxes, with their internal workings and training data remaining unknown.\nGenerating a discriminitive prompt is challenging and requires excellent semantic understanding along with a comprehensive understanding of the text. We claim that LLMs are good adversarial prompt generators and can drive the process of discovering the discriminitive prompts. To discover differences and identify related LLMs, we also use an LLM to act as a detective. This idea borrows heavily from previous works that utilize LLMs as evaluators/judges[1][2]. The detective explores the outputs of LLMs that are being tested out based on the prompts generated by the auditor who is tasked with coming up with the questions that will maximize the differences amongst outputs.\nOur findings can be summarized as follows:"}, {"title": "2 Semantic Manifold Hypothesis", "content": "The Semantic Manifold Hypothesis (SMH) posits that generative natural language processing (NLP) models, despite their apparent complexity and high-dimensional output space, operate on a significantly lower-dimensional manifold when generating sequences of tokens. This hypothesis extends the traditional manifold hypothesis in machine learning [3] to the output space of language models, suggesting that the generative capabilities of these models are more constrained than previously thought."}, {"title": "2.1 Background", "content": "The manifold hypothesis in machine learning states that real-world high-dimensional data often lies on or near a low-dimensional manifold [4]. This concept has been crucial in developing dimensionality reduction techniques and understanding the behavior of deep learning models [5]. However, the application of this hypothesis to the output space of generative language models represents a novel perspective."}, {"title": "2.2 Formulation", "content": "The SMH can be formally stated as follows: Given a sequence of tokens s = (t1, t2, ..., tn), a generative language model M produces a probability distribution over the next token tn+1 that lies on or near a manifold Ms of significantly lower dimension than the full vocabulary space V:\nPM(tn+1/s) \u2248 Ms \u2282R\u012aVI, dim(Ms) \u00ab |V| \nThis formulation suggests that the effective dimensionality of the model's output is much smaller than the size of the vocabulary, potentially explaining observed limitations in language model outputs."}, {"title": "3 Model definition under SMH", "content": "Under the Semantic Manifold Hypothesis, an LLM is really a set of outputs that the specific LLM has the capacity to generate."}, {"title": "3.1 Formal Definition", "content": "Let Mi be an arbitrary LLM model, and let X be a specific known model. We define Si as any sequence of tokens. The probability that M\u2081 is equivalent to X given a sequence S\u2081 is denoted as:\nP(M\u2081 = X Si)\nWe aim to find the sequence S that maximizes this probability:\nMx = arg max P(M\u2081 = X Sx)\nSx\nThis maximization is achieved when:\nMc \u2229 Mcx = 0\nwhere Mc represents the complement of Mr. This condition implies that the set of tokens that best identifies X shares no overlap with any tokens from the complement of Sx.\nTo achieve this, we seek to uncover \u015c, a subset of all possible generations of M that is as unique as possible:\n\u015cC {S: S is a possible generation of M}\nIt's important to note that LLMs are tuned to understand prompts, and this is the primary mechanism for interacting with LLMs behind a black box. Therefore, to obtain \u015c, it is necessary to craft P, a family of prompts:\nP = {P1, P2, . . ., Pn}\nwhere each Pi is designed to elicit responses that contribute to the unique identification of the model X. These prompts are adversarial in nature and aim to extract the following:"}, {"title": "4 Real World considerations of crafting P and \u015c", "content": "Crafting The Optimal prompts for discovering the optimal Sequence has many real world challenges that must be first addressed in order to proceed with an effective approach using the Semantic Manifold Hypothesis."}, {"title": "4.1 Intractable Response Exploration", "content": "Uncovering the set of all responses for a generative model is not a tractable problem. Furthermore, many recent works have been focusing on expanding S even more for any given P, such as [6] seeks to boost the sets of responses to a given prompt an LLM is capable of outputting. The intersection of the works on diversity and representation in [7] highlights the semantic biases LLMs tend to exhibit when given certain prompts and how to improve representational knowledge within an LLM. These two works illustrate the ever-expanding frontier of possible generations given a prompt."}, {"title": "4.2 Ambiguity in Feature Space S", "content": "In a real sense, there's a very large ambiguity in which features make S the most salient. On the surface, counting tokens using approaches like n-gram counting [8] or TF-IDF [9] is unlikely to contain specific enough features ensuring a uniqueness in S. Part of the Semantic Manifold is that an LLM's potential response will use different surface-level tokens that contain the same meaning for a response.\nMethods that rely on contextual cues [10, 11] will be misguided by similarly sounding outputs across LLMs, as once again outputs are on a Semantic Boundary defined more by a policy from the Language Model designer [12, 13] than"}, {"title": "4.3 Stochastic Nature of Generative Responses", "content": "Generative Responses exhibit a very large degree of stochasticity and thus are not deterministic. These responses are affected by several features inside the model architecture such as temperature, token confidences (topp) and more.\nRecall that P represents our family of prompts and \u015c is the subset of all possible generations of the model. The many-to-many relationship between prompts and their potential outputs can be represented as:\np many-to-many, \u015c\nThis notation indicates that for any given prompt pi \u2208 P, there exists a set of potential outputs Si C\u015c, where |Si \u00bb 1, emphasizing the stochastic nature of the generative process. A good process, as showcased in [14], requires careful steering of an LLM towards more unique and constrained responses to minimize the many-to-many relationship as much as possible."}, {"title": "5 A Game of Hide and Seek - Uncovering the Latent Manifold", "content": "Uncovering the specific model using prompts and outputs is similar to playing a game of hide and seek. In hide and seek, the rules are straightforward: a group of people hides, and a seeker is tasked with discovering their hiding locations. This analogy is fitting because, as studies like [15] showcase, hide and seek requires skills in spatial reasoning, problem-solving, and introspection to uncover potential hiding spots. Similarly, in the context of identifying a model, the \"seeker\" (or auditor) must use prompts to elicit unique responses from the model, effectively \"seeking\" the model's distinctive characteristics hidden within its responses. This process involves iteratively refining prompts based on the feedback received, much like how a seeker in the game uses clues and logical reasoning to find hidden players."}, {"title": "5.1 Overview", "content": "Building upon the foundations laid by Chain of Thought (CoT) prompting [16] and the Automatic Prompt Engineer (APE) [17], we propose a novel LLM fingerprinting approach that leverages the Semantic Manifold Hypothesis. This approach aims to identify unique characteristics or \"fingerprints\" of different LLMs based on their output patterns.\nIf the SMH holds true, each LLM should have a characteristic lower-dimensional manifold on which its outputs lie. This suggests that there could be specific prompts or sequences that elicit responses highlighting these unique characteristics. By analyzing these responses, we might be able to differentiate between models or identify models from the same family."}, {"title": "5.2 Methodology", "content": "Our approach involves two key components: an Auditor and a Detective, both implemented as LLMs with specific roles. The Auditor and Detective work hand in hand in deciphering the groups of similar LLMs in the set of models provided."}, {"title": "5.2.1 Auditor", "content": "This model, inspired by the iterative refinement process in APE, is responsible for crafting prompts designed to elicit distinctive responses from different LLMs. The Auditor operates iteratively, using the following inputs:"}, {"title": "5.2.2 Detective", "content": "This model analyzes the outputs of all tested models in response to the Auditor's prompts. The Detective is informed that two of the models in the set are from the same \"source\" (either identical models or from the same model family). Its task is to identify these two models based on similarities in their outputs. The Detective provides its analysis in a structured format:\n{\n\"rationale\": \"Explanation of the decision\",\n\"model_indexes\": [index1, index2]\n}\nUnlike the auditor which is provided the outputs of previous turns, the detective/evaluator is only provided the model outputs from the current turn."}, {"title": "5.2.3 Feedback Mechanism", "content": "After the Detective makes a guess, a Results block is provided to the Auditor in the following format:\n{Correct: Bool, predicted_indexes: (Int, Int), correct_indexes: (Int, Int)}\nThis feedback allows the Auditor to learn from the Detective's successes and failures, enabling it to generate more effective prompts in subsequent iterations. This iterative feedback loop is reminiscent of the refinement process in APE, but applied to the meta-task of model fingerprinting rather than prompt engineering."}, {"title": "5.3 Experimental Setup", "content": "The experiment is conducted over T trials:\n1. The Auditor generates an initial set of prompts.\n2. These prompts are presented to N different LLMs (including two from the same source).\n3. The Detective analyzes the outputs and attempts to identify the two similar models.\n4. The Results block is provided to the Auditor.\n5. Steps 2-4 are repeated for T trials.\nTo account for the Auditor's learning curve, we introduce a warm-up period of W trials. The Auditor's accuracy is evaluated only after these W warm-up trials. This allows us to measure the Auditor's performance once it has had the opportunity to refine its prompt generation strategy based on feedback."}, {"title": "5.4 Expected Outcomes", "content": "This approach could potentially:\nValidate the Semantic Manifold Hypothesis by demonstrating consistent patterns in model outputs.\nDevelop a method for fingerprinting LLMs, which could have applications in model attribution and detection of AI-generated content.\nProvide insights into the similarities and differences between various LLM architectures and training approaches.\nInform the development of more diverse and less easily identifiable language models.\nBy combining ideas from CoT (step-by-step reasoning) [16], APE (iterative refinement) [17], and the Semantic Manifold Hypothesis [18], this fingerprinting approach represents a novel way to probe the output space of language models and understand their unique characteristics."}, {"title": "6 Experimental Results", "content": ""}, {"title": "6.1 Family Detection Results", "content": "Here are the accuracy results for detecting the family of an LLM: The Figure above showcases the ability of LLMs to\ndetect the fingerprint of another LLM based on its family. Due to the stochastic nature of Language models generating content, there will naturally be a variability in accuracy as showcased. More details pertaining to each individual experiment per family is listed within the Appendix."}, {"title": "6.2 Auditor Discovery Process", "content": "Throughout the process of uncovering a fingerprint, the Auditor constantly generates its thoughts, plans, and prompts which gives us a window into the detection process. We found the warmup steps to be important in helping the auditor generate better prompts over time. Throughout the entire process and across all the trials, the Auditor is actually aware of their own performance and is even factored into the next actions the Auditor will take."}, {"title": "6.2.1 Scenario 1: Good Performance", "content": "In this scenario, the LLM is consistently doing well and is on a roll identifying the correct indexes for the LLM over and over again post a warm-up period"}, {"title": "6.2.2 Scenario 2: Mixed Performance", "content": "The Auditor is having mixed results and has been lacking consistency"}, {"title": "6.2.3 Scenario 3: Poor Performance", "content": "The following scenario showcases an Auditor that is struggling to identify a distinct fingerprint for any given LLM"}, {"title": "6.3 Prompt Generation", "content": "Discriminitive prompt generation is interesting to explore to better understand how Large Language Models are capable of crafting discriminitive prompts that approximate the key distinctions across various models. The following section explores the prompts that have lead to model discovery and discovering sets of unique characteristics. Below is a sample of various prompts that have lead to the overall success of uncovering specific types of LLMs."}, {"title": "6.3.1 Common Structure", "content": "The underlying commonality of a good discriminitive prompting that helps uncover unique aspects comes down to prompt specificity over more generic prompts. Specific task description with many restrictions placed upon an LLM leads to the most creative responses which in turn allow for the largest amount of exploration among the manifold of finding an language models preferred interpretation to those topics and restrictions"}, {"title": "6.3.2 Elements of the Optimal Prompt", "content": "Analysis of various prompts reveals several key elements that contribute to their effectiveness in evaluating and challenging language models. The following list outlines the common structural components found in optimal prompts:"}, {"title": "6.4 Specific Differences Across Model Families", "content": "Examining each family of prompts, various families have different sets of prompts that explore different aspects when they're successful at discovery."}, {"title": "6.4.1 Llama", "content": "The key ingredients to finding key differences in the Llama 3 family of models [19] have been action words like 'Discuss' or 'Contemplate' that provide a scenario and than is asked to give a step by step explanation or provide an analysis of what is being asked."}, {"title": "6.4.2 Mistral", "content": "Mistral [20] identifying prompts ask for role-playing a scenario and than craft a story that adheres to the minds eye of how would someone in that role experience the world. Mistral tends to be instructed more than Llama to specifically answer specific questions and to take a particular direction."}, {"title": "6.4.3 Gemma", "content": "Gemma [21] is very poetic and its distinct fingerprint is being able to follow complex structures within poetry and rhyming schemes. Gemma is creative in word-play, word association, alliteration, and other such literary techniques that its capacity to craft a multitude of creative works becomes apparent. The confusion emerges here with Mistral as the Mistral models are also capable of role-playing but to less of a poetic and artistic degree."}, {"title": "6.4.4 Phi", "content": "Our experiments show Phi-2 [22] as being harder to detect. Being a smaller model, it has difficulty in following the instructions of the auditor model. This resulted in many other models within the grading cohort to be identified instead."}, {"title": "6.5 The Grading Cohort Effect", "content": "During our experiments, we discovered that the cohort of models being tested significantly influences detection performance. Specifically, the current setup shows that the Seeker tends to be more biased towards larger and more coherent language models. These larger models often overshadow smaller and less capable models, making it more challenging to detect and differentiate the latter accurately. This bias occurs because larger models typically generate more consistent and high-quality responses, which can mask the distinct characteristics of smaller models.\nTo address this issue and ensure a more balanced evaluation, we focused on experiments involving models with a maximum of 27 billion active parameters, except for tests within the Llama family. By doing so, we aimed to minimize the overshadowing effect and create a more equitable testing environment. This approach allowed us to better understand the unique behaviors and responses of smaller models, facilitating more accurate detection and differentiation."}, {"title": "7 Future Work", "content": "Evidence is still emerging that Language Models can detect the unique characteristics of other Language Models or AI-generated content. There are several areas that we plan to explore next."}, {"title": "7.1 Improvement in Auditor Task comprehension", "content": ""}, {"title": "7.1.1 More Agentic Behavior", "content": "The current existing setup used for the Auditor is simplistic and builds on past work but newer methods are emerging for more optimal agentic behavior. it is fascinating that, in several instances where the Auditor fails to discover an LLM by rounds 8 or 9, it is not uncommon for an LLM to plan on using a clustering algorithm to explore differences across models, re-examine past outputs, and conduct another round of literature reviews. These are all sensible findings, and are similar to the experiments of emerging tool use in [23]."}, {"title": "7.1.2 Context-length Improvements", "content": "Improving the context-length efficiency of the auditor process will allow additional Language Models to be used. This can enable a wider array of discoveries to be made."}, {"title": "7.2 Extending Detection to Model Size and Capability", "content": "Showcasing the ability to identify the family of a language model by prompting and in-context learning opens many promising avenues. We intend to explore the capabilities of Language Models to uncover the size of a language model and even discover the capabilities of another model. Past works such as [24], have utilized a clever scheme showcasing a black-box approach for model estimation is viable."}, {"title": "7.3 Additional Semantic Manifold Explorations", "content": "Having initial evidence for the Semantic Manifold Hypothesis leads to many additional questions being asked. The next step in exploring the manifold is to dive deeper into additional aspects of individual Language Models. Several areas are interesting to explore here such as adapting tone and style, improving reasoning, exploring if data compression is possible following a manifold and whether manifold transfer is possible using significantly less training data if Language Models approximate a highly capable model's manifold."}, {"title": "8 Conclusion", "content": "For the purpose of uncovering the distinguishing features for Large Language Models, we craft a theoretical framework titled the Semantic Manifold Hypothesis which formulates how Language Models capability to generate tokens follows a restricted set and there exist distinct sets which set individual Language Models apart from others. Utilizing this concept we showcase that Language Models are capable of discovering these most salient features through in-context optimization to uncover hidden traits of the families of Language models which leads to their identification. This opens up a promising new frontier in the field of Language Model explainability, interpretation, and identification in a block-box setup where there is no access to a language model. We hope that our findings will form a basis for future research to build upon."}, {"title": "A Cohort of Models Under Test", "content": "The following models are used for all of the cohorts. Models are tested alongside LLama-3-8B, Mistral-7B-instruct-0.3, Gemma-2-9B, and Phi-2.7B. Exceptions apply for certain family tests when one of these models are removed, for example when testing Mistral with two Mixtral-8x22B those runs dont have an additional Mistral-7B model."}, {"title": "B Auditor and Seeker Model", "content": "We use the Qwen2-72B Model [25] due to its native 32K context length which is crucial for all of the information that's provided to the Auditor. Furthermore, Qwen2-72B has a remarkably high score on the MMLU benchmarks.\nThe Seeker model is the same as the auditor, though it has a different System prompt and will receive different sets of information. We do not inform the Seeker about its past attempts or any information pertaining to what to look for. By doing so, we prevent the Auditor from passing in any unfair information to the seeker making all of our trials fair trials."}, {"title": "C Experiment Details", "content": ""}, {"title": "C.1 Initial Prompt Formula", "content": "We use Llama-3-405B to craft the initial formula for attempting to discover unique responses from a model. We start off by posing our hypothesis to the model and asking for a few prompts to test. Upon finding highly varied responses to all models that we tried those prompts with, we follow it with asking for a prompting formula to get started."}, {"title": "C.2 Number of Trials", "content": "We use a max of 10 trials and this is mostly limited by the Auditors Context Length, additional context length could allow for more trials, but current findings indicate if a pattern can be found, its usually within the first three to five iterations. We also use a warm-up period of 3 trials on which accuracy isn't computed on."}, {"title": "C.3 Model Family Experiment details", "content": "We run the following experiments per family and showcase what models are used for which trial. The Models per family below showcases which two models we mark as 'similar' for the Auditor to discover what sets of features identifies that family. Note that for all trials the models presented were done so in a cohort of other models from other model families.\nLlama\n2 instances of Llama-8B were marked as similar by the auditor across two trials\nAn instance of Llama-3-70B and an instance of Llama-8B were marked as similar by the auditor across two trials\nMistral\n2 instances of Mistral-7B-0.3-instruct were marked as similar by the auditor across two trials\nAn instance of Mistral-7B-0.3-instruct and an instance of Mixtral-8x22B were marked as similar by the auditor across two trials\n2 instances of Mixtral-8x22B were marked as similar by the auditor\nGemma\nAn instance of Gemma-2-27B and an instance of Gemma-2-9B were marked as similar by the auditor\n2 instances of Gemma-2-27B were marked as similar by the auditor\n2 instances of Gemma-2-9B were marked as similar by the auditor across two trials\nPhi\n2 instances of Phi-2-2.7B were trialed without success by auditor across five trials"}]}