{"title": "Toward General Object-level Mapping from Sparse Views with 3D Diffusion Priors", "authors": ["Ziwei Liao", "Binbin Xu", "Steven L. Waslander"], "abstract": "Object-level mapping builds a 3D map of objects in a scene with detailed shapes and poses from multi-view sensor observations. Conventional methods struggle to build complete shapes and estimate accurate poses due to partial occlusions and sensor noise. They require dense observations to cover all objects, which is challenging to achieve in robotics trajectories. Recent work introduces generative shape priors for object-level mapping from sparse views, but is limited to single-category objects. In this work, we propose a General Object-level Mapping system, GOM, which leverages a 3D diffusion model as shape prior with multi-category support and outputs Neural Radiance Fields (NeRFs) for both texture and geometry for all objects in a scene. GOM includes an effective formulation to guide a pre-trained diffusion model with extra nonlinear constraints from sensor measurements without finetuning. We also develop a probabilistic optimization formulation to fuse multi-view sensor observations and diffusion priors for joint 3D object pose and shape estima-tion. GOM demonstrates superior multi-category mapping performance from sparse views, and achieves more accurate mapping results compared to state-of-the-art methods on the real-world benchmarks. We will release our code: https://github.com/TRAILab/GeneralObjectMapping.", "sections": [{"title": "1 Introduction", "content": "Object-level mapping [1, 2, 3, 4, 5, 6, 7, 8, 9] builds a 3D map of multiple object instances in a scene, which is critical for scene understanding [10] and has various applications in robotic manipulation [11], semantic navigation [12, 13] and long-term dynamic map maintenance [14]. It addresses two closely coupled tasks: 3D shape reconstruction [15, 16] and pose estimation [17]. Conventional methods [18, 19, 20] approach these tasks from a perspective of state estimation [21], solving an inverse problem where low-dimensional observations (RGB and Depth images) are used to recover high-dimensional unknown variables (3D poses and shapes) through a known observation process (e.g., projection, and differentiable rendering). However, these methods require dense observations (e.g., hundreds of views for NeRF [18]) to fully constrain the problem. In robotics or AR applications, obtaining such dense observations is challenging due to limitations in the robot's or user's observation angle and occlusions in clustered scenarios. Therefore, it is crucial to develop methods that can map from sparse (fewer than 10) or even single observations.\nHuman vision can infer complete 3D objects from images despite occlusions by using prior knowledge of the objects, which represents the prior distributions of the shapes of specific categories, such as chairs, based on thousands of instances observed in daily life. We aim to introduce generative models [22] as providers of prior knowledge to constrain the 3D object mapping. Generative mod-els have demonstrated impressive abilities to generate high-quality multi-modal data by learning distributions in large-scale datasets, including texts [23], images [24], videos [25], and 3D mod-els [26, 27, 28, 29]. Previously, researchers have explored methods, to introduce generative priors"}, {"title": "2 Related Work", "content": "2.1 3D Object-level Mapping\n3D scene understanding is crucial for robots to perform high-level object-oriented tasks such as navi-gation and manipulation [10]. Early methods in the field of SLAM and SfM [40] built 3D scene mod-els from dense multi-view observations, leveraging multi-view geometric consistency [41]. How-ever, they used a single model to represent both foreground objects and backgrounds, unaware of separate instances [16, 42, 43]. Object-level mapping addresses the problem of estimating objects' shapes and poses as independent instances. SLAM++ [1] was pioneering in this area but requires a database of CAD models and could not handle unseen objects outside this database. To generalize"}, {"title": "2.2 Generative Models as Shape Priors", "content": "Introducing generative models as prior constraints for 3D object pose and shape estimation is an open problem. Currently, the literature explores two kinds of priors. (1) Prior for Specific Single-category. Early 3D generative models [27, 28, 47] train separate models for single-category objects. Object-level mapping systems [8, 3, 4, 48, 9] introduce DeepSDF-like [27] generative models as pri-ors to constrain object shapes. They estimate complete shapes and poses from sparse observations under occlusions, however, limited by the shortcomings of these generative priors, they are limited to a single category. (2) General Prior for Multi-categories. Recent research, including DreamFu-sion [32] and Magic3D [33], optimizes 3D shapes represented by NeRF through multi-view images from an image generation model [49]. However, since the prior is inherently 2D, it has limited 3D consistency, leading to multi-face artifacts and identity-preserving problems [32]. Although they can generate multi-category 3D shapes from texts, they can not reconstruct from real images or esti-mate object poses to form a map with multiple instances. In contrast to the methods above, we aim to leverage a 3D prior model with multi-category ability for improved 3D consistency. Recently, diffusion-based 3D generative models [37, 29] have become available, such as Shap-E [29] which is trained on millions of objects in thousands of categories. However, unlike VAE-variant models [27], diffusion-based models are not straightforward to integrate into an object-level mapping system. We discuss this further in the next section."}, {"title": "2.3 Pretrained Diffusion Models as Optimization Constraints", "content": "Training a diffusion model on millions of 3D objects is time-consuming [37, 29]. We aim to lever-age a pre-trained diffusion model without finetuning. One method to control a pre-trained diffusion model is through conditional generation. It introduces extra constraints to guide the original diffu-sion process, and has been investigated in image generation [31], restoration [30], and 3D human pose generation [50]. However, a conditional generation formulation is not flexible enough to esti-mate extra variables (i.e. poses) beyond the variable modeled by the diffusion model (i.e. shapes). Another method is to leverage the diffusion model as a constraint in optimization, which is flexible to introduce extra variables and constraints. However, unlike VAE-variant methods that have ex-plicit latent spaces [27, 3, 9], the latent space of diffusion models is still under investigation [51]. Diffusion models output scores instead of densities and require a complex sampling process to gen-erate a sample [24], making it nontrivial to formulate a joint optimization with additional variables. Recently, some work uses optimization with diffusions, in the fields of images [51, 52], 3D human poses [53, 54], and medical CT reconstruction [55]. Most similar to ours, Yang et al. [56] optimized a NeRF with a single-category diffusion model, but did not include pose estimation. Differing from the work above, we are the first to use a pre-trained multi-categories diffusion model in an optimiza-tion framework for both 3D object pose and shape estimation."}, {"title": "3 Methods", "content": "3.1 Framework Overview\nOur system, visualized in Figure 1, estimates 3D object shapes and poses in a scene using two information sources: multiple RGB-D observations and a generative shape prior model. Following"}, {"title": "3.2 3D Objects Shapes and Poses Representations", "content": "We aim to build a map of the 3D objects O in a scene. For each object, we estimate its 3D model (a NeRF), Q, in the object canonical coordinate frame, Fo, and a relative pose, $T_{ow}$, from the world coordinate frame, Fw, to the object coordinate frame Fo.\nPose Representation. A pose transformation with scale, $T \\in R^{4\\times4}$, is constructed from a transla-tion vector, $t \\in R^3$, a rotation vector, $\\varphi \\in so(3)$, and a scaling vector, $s \\in R^3$:\n$T = \\begin{bmatrix}\\exp(\\varphi ^\\wedge) \\text{diag}(s) & t\\\\ O^T & 1\\end{bmatrix}$                               (1)\nwhere $exp(\\cdot)$ is the exponential mapping from the Lie Algebra to the corresponding Lie Group. The operator $(\\cdot)^\\wedge$ converts a vector to a skew-symmetric matrix. Thus, we can represent the pose with a 9-DoF vector: $\\xi = [t, \\varphi, s] \\in R^9$. As a mapping system, we assume camera poses, $T_{wc} \\in SE(3)$, are known, e.g., by off-the-shelf SLAM methods [43], or by robot kinematics [57].\nShape Representation. We parameterize the object's shape with a Neural Radiance Field (NeRF) [18] through a neural network $f_\\Theta(.)$ with weights $\\Theta$. For any given 3D point, it outputs its density, $\\sigma$, color, c, and illumination, i, and can be rendered into RGB and depth images. Follow-ing Shap-E [29], it further outputs SDF values, s, and can generate meshes via Marching Cubes:\n$\\sigma, c, i, s = f_\\Theta(x, d)$                                        (2)\nwhere $x \\in R^3$ is the given 3D coordinate and $d \\in R^3$ is the viewing direction."}, {"title": "3.3 Leveraging Generative Models as Shape Priors", "content": "Common 3D objects, such as chairs and tables, exhibit diverse shapes and textures. This translates to high-dimensional NeRF parameters $\\Theta$. To further constrain the process, we propose to leverage a generative model with learnable parameters $\\beta$, denoted by $g_\\beta(.)$, to learn an approximate posterior distribution $P(\\Theta|C)$ of the true posterior distribution $P(\\Theta|C)$. Here C represents the conditioning information, e.g., an image or a text prompt. The generative model $g_\\beta(.)$ takes this conditioning information, C, as input and outputs a conditional distribution of the NeRF parameters $\\Theta$:\n$P_\\beta(\\Theta|C) = g_\\beta(C)$                                          (3)"}, {"title": "3.4 Optimization with Diffusion and Extra Observations", "content": "To update $\\Theta$ with multi-view RGB-D observations and further estimate a new unknown pose vari-able T, we propose an optimization formulation that integrates additional observations into the pre-trained diffusion model, as shown in Figure 2. We refer T as $T_{ow}$ without further notice. Given M observation frames ${F_i}_{i=1}^M$ and a condition C, we aim to estimate a Maximum Like-lihood Estimation for the unknown variables pose T and shape $\\Theta$, from a joint distribution of $P(T, \\Theta|F_1,...,F_M,C)$. By derivation (Proof provided in Appendix), we get a convenient form for numerical optimization:\n$T, \\Theta = \\arg \\max_{T,\\Theta} log P(F|T, \\Theta) + log P(\\Theta|C)$           (5)\nDiffusion Priors. The term $log P(\\Theta|C)$ measures the likelihood of current $\\Theta$ under a condition C by the diffusion model. However, diffusion models can not directly output the probability den-sity [24]. We derive a method to calculate gradients from diffusion priors for optimization, following a variational sampler [58], originally designed for controlling image generation. A diffusion model is trained to predict the noise $\\epsilon$ in a noisy variable $\\Theta_t$ at timestamp t, under a condition C:\n$\\min_\\beta = E_{t,\\epsilon} ||\\epsilon_\\beta(\\Theta_t, C, t) - \\epsilon||^2, t \\in U(0,T), \\epsilon \\in N(0, I_n)$ (6)\nWe fix the weights $\\beta$ of the pre-trained diffusion model, and use it to get the gradients $\\Delta_\\Theta log P(\\Theta|C)$ of the current shape $\\Theta$. First, we add noise to the current shape parameter $\\Theta$ to get a noisy $\\Theta_t$ with the predefined noise schedule of the diffusion model at timestamp t:\n$\\Theta_t = \\alpha_t \\Theta + \\sigma_t \\epsilon, \\epsilon \\in N(0, I_n)$                                  (7)\nwhere $\\sigma_t$ and $\\alpha_t$ are constants of the schedule, and $\\epsilon$ is a randomly sampled noise. Then, we predict the added noise and evaluate the predicted error $\\Delta_\\beta(\\Theta, C, t)$ with the diffusion model:\n$\\Delta_\\beta(\\Theta, C, t) = \\epsilon_\\beta(\\Theta_t, C, t) - \\epsilon$                   (8)\n$= \\epsilon_\\beta(\\alpha_t \\Theta + \\sigma_t \\epsilon, C, t) - \\epsilon, \\epsilon \\in N(0, I_n)$    (9)\nTo eliminate the timestamps t, we randomly sample K times from a uniform distribution $U(0, T)$ and take an expectation:\n$\\Delta_\\beta(\\Theta, C) = \\frac{1}{K}\\sum_{i=1}^K \\Delta_\\beta(\\Theta, C, t_i), t_i \\in U(0,T)$ (10)"}, {"title": "4 Experiments", "content": "Datasets. We conducted experiments utilizing the ScanNet [59] dataset, which comprises RGB-D scans of real indoor scenes. It presents significant challenges to object mapping methods due to real-world occlusions, clusters, blurs, and sensor noise. We use ground-truth shape annotations of ShapeNet [60] meshes provided by Scan2CAD [61] for evaluation. We focus on objects that have a"}, {"title": "4.1 Multi-view Mapping Performance", "content": "We present the mapping performance with 1, 3, and 10 RGB-D views on chairs in Table 1. We show qualitative results in Figure 3 and Figure 4. Compared to vMap [45], which lacks priors as additional constraints, our approach yields significant improvements. This is particularly evident with fewer views, where observations are compromised by occlusions and noise. In these instances, prior information serves as a valuable additional constraint. Compared to DSP [3], our method pro-duces more detailed shapes with textures, and outperforms it in both 3 and 10 views. DSP achieves better CD in single-view cases due to a smaller latent space with stronger constraints specifically trained on Chairs. However, it can not generalize to other categories. Shap-E can generate reason-"}, {"title": "4.2 Multi-categories Performance", "content": "We benchmark the open-vocabulary mapping performance for objects across 7 categories using 10 RGB-D views in Table 2. Our approach outperforms the naive use of ICP matching with shapes generated from Shap-E. The ICP matching stuck to a local minimum for the pose with a fixed shape. DSP [3] is a single-category system that requires separate network weights for each category, and since only the chair model is officially available, we present its performance solely for this category. This limitation also restricts the system's applicability. Compared to vMap [45], our approach yields an improved average CD and enhancements across most categories. This highlights the efficacy of our optimization formulation in integrating prior information with observations. We further demonstrate the Reconstruction performance in Table 2 (Recon), given the ground truth object poses. Our performance significantly improves compared with Mapping, demonstrating the potential for enhanced performance when more accurate initial poses are available. Our approach outperforms both the text and image-conditioned Shap-E model. This underscores that additional observations can enhance a pre-trained generative model for reconstruction tasks. We present more experiments on the CO3D dataset and analysis in the Supplementary Materials."}, {"title": "5 Conclusion", "content": "We present a General Object-level Mapping system, GOM, leveraging a pre-trained diffusion-based 3D generation model as shape priors. We propose an optimization formulation to couple multi-view RGB-D observations, and diffusion priors to constrain shapes and poses for 3D objects. We achieve state-of-the-art mapping performances among multiple categories without further finetuning. Fur-ther exploring the diffusion shape priors into inverse problems with more constraints, e.g., temporal constraints for dynamic tracking, and spatial constraints for complete SLAM, and the application to downstream robotics tasks such as robotics manipulation, will be valuable future work directions."}, {"title": "6 Appendix", "content": "6.1 Experiments on CO3D dataset\nTo demonstrate the generalization ability across diverse categories, we conducted additional experi-ments on 10 categories from the CO3D dataset [63]: Toy Truck, Bench, Donut, Broccoli, Toy Train, Apple, Teddy Bear, Hydrant, Book, and Toaster. We compared our results with the baseline method vMap [45], as shown in Figure 5. Our approach consistently outperformed the baseline, benefiting from the incorporation of generative priors from a single pre-trained model.\n6.2 More Qualitative Visualizations\nVisualization on More Categories on ScanNet dataset. We further illustrate the effectiveness of our approach in generalizing to a variety of categories through qualitative visualizations, as shown in Figure 6. Unlike the DSP method [3], which is based on a single-category model, DeepSDF [27], our system is capable of supporting multiple object categories using a single pre-trained diffusion model, Shap-E [29].\nScene-level Visualization. Figure 7 showcases a 3D map visualization of a scene on the ScanNet dataset, containing multiple objects from various categories, including four chairs, a sofa, and a table. Each instance is independently reconstructed using 10 RGB-D views.\nVisualization of Multi-view Inputs. We visualized 10 input images, 3D camera poses, and cor-responding object meshes in Figure 8, which includes two objects from the ScanNet dataset and one object from the CO3D dataset. The ScanNet dataset presents a challenging camera trajectory, with all inputs gathered from a single direction relative to the objects. This scenario is typical in real-world applications such as robotics and augmented reality, where a robot or user cannot easily circle an object. It also underscores the importance of sparse view mapping, where prior informa-tion is crucial for completing and providing reasonable estimates for occluded parts. An extreme case is illustrated where all camera views have small baselines, resembling single-view mapping. In the CO3D dataset, the cameras are intentionally positioned to circle the objects; however, we randomly sampled only ten views from this trajectory, resulting in a sparse coverage of the objects. We demonstrate that our system can effectively handle all these situations.\n6.3 More System Evaluations and Discussions\nPerformance on Single-view Inputs. Single-view mapping is particularly challenging due to its highly ill-posed nature. All systems experience significant performance losses when relying on only a single view. Our main challenges stem from the high-dimensional shape representation required for NeRF, which models multiple categories of objects, encompassing both texture and shape. In contrast, DSP employs DeepSDF, an SDF-based representation specifically trained on a single cat-egory, focusing solely on shapes. This approach has a much smaller parameter space and requires fewer constraints. However, this limited parameter space restricts its ability to generalize to other categories. Our method, on the other hand, can generalize across multiple categories using a sin-gle model and can continue to improve with additional observations. With the assistance of prior knowledge, we achieve effective enhancements in single-view scenarios compared to the baseline method, vMap. Additionally, we have a parameter that allows us to increase the weight of prior con-straints during optimization, enabling us to place greater trust in the prior information for producing complete and reasonable mapping results.\nPerformance on Out-of-distribution Objects. Dealing with out-of-distribution objects is a critical challenge for developing a general mapping system. In this paper, we present a formulation that leverages prior knowledge from a pre-trained generative model. While we use Shap-E as a repre-sentative example at the time of writing, it's important to note that our method is not specifically tailored to Shap-E but applies to a broader class of diffusion models. The field of 3D generation"}, {"title": "6.4 Ablation Study", "content": "Methods to Fuse Observations and Diffusion Priors. We compare three strategies to fuse obser-vations and diffusion priors, as shown in Table 3. (1) Optimize then diffuse, which first optimizes shape and pose with geometric loss only for a given number of steps, and then uses the diffusion model to diffuse the shape. We notice that the information from observations is often lost during the post-diffusion process. Consequently, the ultimate shape diverges from the ground truth, resulting in a large metric error. (2) Diffuse then optimize, which first uses the diffusion model to generate a shape with a text condition, then uses the geometric loss to optimize both shape and pose. We ob-serve that the unobserved segment of the shape is prone to corruption during the post-optimization process. Ultimately, this leads to a performance level that is similar to optimizing using only geo-metric observations without priors, which also remains more artifacts in the meshes and renderings. (3) Jointly Optimize and Diffuse, which fuses prior and observations iteratively during optimization with both diffusion prior and geometric loss so that both sources of information are active. This com-"}, {"title": "6.5 Computation Analysis", "content": "We conducted an evaluation of the system's computation using 10 RGB-D views on a 16GB V100 GPU. For each instance, GOM (Ours) requires 43.0 seconds for 200 optimization iterations, which includes 100 diffusion steps. In comparison, vMap [45] requires 38.1 seconds to complete 200 op-timization iterations, utilizing only geometric constraints. Our method, leveraging diffusion prior, significantly enhances the quality with minimal computational overhead. The Shap-E model [29] requires 45.6 seconds to generate a single instance by diffusing from random noise via a compu-tationally intensive sampling process. Our method, utilizing the prior information stored inside"}, {"title": "6.6 Failure Case and Limitation", "content": "The ScanNet dataset presents challenges due to occlusions and sensor noise. We illustrate a repre-sentative failure case in Figure 10. When the input observations are severely occluded or contain incomplete masks, our method can partially complete the object (for instance, the center occluded part by the tissue placed on the chair), but it fails to complete thin elements like legs and handles that the mask does not cover. The paper and pen placed on the chair are reconstructed as part of the texture. Despite these challenges, our method still generates a smoother surface with significantly fewer artifacts compared to the baselines. Future improvements could include the use of a more powerful segmentation model, such as SAM [69], and adaptively increasing the weights of the prior in areas with corrupted observations. Further, more flexible shape representation beyond a NeRF, such as Gaussian Splatting [70] can be explored to better model the details of objects.\nWe outline the limitations of our work to inform future research directions. Before deploying our method in real-world applications, practices such as data association and camera pose estimation are necessary for effective multi-object mapping. Our approach relies on text descriptions and seg-mented images from a segmentation algorithm as additional inputs. While these can be relatively easy to acquire using off-the-shelf models like Segment Anything, they do introduce extra input requirements. Additionally, the computation is not yet real-time, primarily due to the diffusion process and NeRF rendering. The challenges posed by the diffusion process could be mitigated by employing more lightweight generative models, such as those with smaller latent spaces. For the NeRF rendering, more efficient representations, like Gaussian splatting, could improve perfor-mance. Parameters such as the number of optimization steps can be adjusted to balance efficiency and effectiveness. Our current focus is on the priors of individual objects, leading us to evaluate them independently. However, more scene-level information and priors could enhance global con-sistency, such as cross-object relationships, structural knowledge about objects, and scene graphs. These aspects could provide valuable avenues for future work."}, {"title": "6.7 Analysis of the Generative Model Shap-E", "content": "Latent Space Interpolation. We illustrate a visualization of latent space interpolation from one chair to another, from a chair to a table, and from a chair to a plane in Figure 11. Unlike DeepSDF [27], which utilizes a 64-dimensional latent vector for an SDF-based shape, Shap-E em-ploys a considerably larger latent space for a NeRF-based shape, with a dimension of 1024 \u00d7 1024. Despite its high dimensionality, linear interpolation still provides a meaningful transition for changes in both texture and geometry. A smooth latent space aids the optimization process when incorporat-ing gradients from both observations and priors.\nGeneration from Text Prompt. The Shap-E model is capable of generating a variety of shapes based on a given text prompt, as demonstrated in Figure 12. The attributes specified in the text prompts, such as color, can influence the output shapes to a certain degree. The use of more complex text prompts, such as descriptions from large language models (LLMs) to assist in mapping object shapes and poses, presents an intriguing avenue for future research."}, {"title": "6.8 Derivation of Optimization with Prior", "content": "We provide the proof for Equation 5 in the main content. Given M observation frames ${F_i}_{i=1}^M$, and a condition C, we aim to estimate a Maximum Likelihood Estimation for the unknown variable pose T and shape $\\Theta$. We start from a joint distribution of $P(T, \\Theta|F_1, ..., F_M, C)$, and aim to get:\n$T, \\Theta = arg \\max_{T,\\Theta} P(T, \\Theta|F_1, ..., F_M, C)$                               (17)\nAccording to Bayes' rule:\n$P(T, \\Theta|F_1, ..., F_M, C) = \\frac{P(F_1, ..., F_M|T, \\Theta, C) P(T, \\Theta|C)}{P(F_1, ..., F_M|C)}$                                (18)"}, {"content": "Considering that any observation frames $F_1, ..., F_M$ are independent to the prior condition C, and we can assume the prior of the observation $P(F_i)$ is a constant, thus, $P(F_1, ..., F_M|C)$ is a constant. We can get:\n$P(T, \\Theta|F_1, ..., F_M, C) \\propto P(F_1, ..., F_M|T, \\Theta, C)P(T, \\Theta|C)$                              (19)\nThen, we consider the observation part $P(F_1, ..., F_M|T, \\Theta, C)$. Since the observations $F_1, ..., F_M$ are conditionally independent among each other given T and $\\Theta$, and are independent to C, the likelihood can be factorized as:\n$P(F_1, ..., F_M|T, \\Theta, C) = \\prod_{i} P(F_i|T, \\Theta)$                                 (20)\nSince we model the pose T and shape $\\Theta$ separately, they are independent to each other. Further considering that the condition C only applies to the shape, we have:\n$P(T, \\Theta|C) = P(T|C)P(\\Theta|C) = P(T)P(\\Theta|C)$                             (21)\nWe assume uniform distribution for the object pose T, so that P(T) is a constant. So we have:\n$P(T, \\Theta|C) \\propto P(\\Theta|C)$                              (22)\nInserting the observation part (Eq 20) and the prior part (Eq 22) into the joint distribution (Eq 19), we can estimate the unknown variables through:\n$T, \\Theta = arg \\max_{T,\\Theta}  \\prod_{i} P(F_i|T, \\Theta)P(\\Theta|C)$                             (23)\nFinally, taking the logarithm, we can get a more convenient form for numerical optimization:\n$T, \\Theta = arg \\max_{T,\\Theta} \\sum_{i} log P(F_i|T, \\Theta) + log P(\\Theta|C)$                     (24)"}]}