{"title": "Solving Decision Theory Problems with Probabilistic Answer Set Programming", "authors": ["Damiano Azzolini", "Elena Bellodi", "Rafael Kiesel", "Fabrizio Riguzzi"], "abstract": "Solving a decision theory problem usually involves finding the actions, among a set of possible ones, which optimize the expected reward, possibly accounting for the uncertainty of the environment. In this paper, we introduce the possibility to encode decision theory problems with Probabilistic Answer Set Programming under the credal semantics via decision atoms and utility attributes. To solve the task we propose an algorithm based on three layers of Algebraic Model Counting, that we test on several synthetic datasets against an algorithm that adopts answer set enumeration. Empirical results show that our algorithm can manage non trivial instances of programs in a reasonable amount of time. Under consideration in Theory and Practice of Logic Programming (TPLP).", "sections": [{"title": "1 Introduction", "content": "Logic-based languages are well-suited to model complex domains, since they allow the representation of complex relations among the involved objects. Probabilistic logic languages, such as ProbLog (De Raedt et al. 2007) and Probabilistic Answer Set Programming under the credal semantics (Cozman and Mau\u00e1 2020), represent uncertain data with probabilistic facts (Sato 1995), i.e., facts with an associated probability. The former is based on the Prolog language (Lloyd 1987), while the latter adopts Answer Set Programming (ASP) (Brewka et al. 2011). ASP has been proved effective in representing hard combinatorial tasks, thanks to expressive constructs such as aggregates (Alviano and Faber 2018). Moreover, several extensions have been proposed for managing uncertainty also through weighted rules (Lee and Wang 2016), increasing even further the possible application scenarios.\nUsually, decision theory (DT) problems are composed of a set of possible actions, a selection of which define a strategy, and a set of utility attributes, that indicate the utility (i.e., a reward, possibly negative) of completing a particular task. The goal is to find the strategy that optimizes the overall expected utility. Expressing DT problems with (probabilistic) logic languages allows the user to find the best action to take in uncertain complex domains. While DTProbLog (Van den Broeck et al. 2010) is a ProbLog extension that solves DT tasks represented with a ProbLog program, no tool, to the best of our knowledge, is available to solve them with a probabilistic answer set language. We believe that a (probabilistic) ASP-based tool, by providing expressive syntactic constructs such as aggregates and choice rules, would be of great support in complex environments. For example, we may be interested in modelling a viral marketing scenario, where the goal is to select a set of people to target with specific ads to maximize sales. In this scenario, uncertainty could come from the shopping behavior of individuals.\nIn this paper we close this gap and introduce decision theory problems in Probabilistic Answer Set Programming under the credal semantics (DTPASP). In particular, we extend Probabilistic Answer Set Programming under the credal semantics (PASP) with decision atoms and utility attributes (Van den Broeck et al. 2010). Every subset of decision atoms defines a different strategy, i.e., a different set of actions that can be performed in the domain of interest. In the viral marketing example, the decisions are whether to target an individual with an ad. However, there is uncertainty on the actual effectiveness of the targeting action. At the same time, targeting a person involves a cost that can be represented with a utility attribute with a negative reward. The fact that a person buys a product, instead, is associated with a positive reward. Moreover, there can be other factors to consider, such as preferences among the different possible items that can be bought which can be conveniently represented using ASP.\nIn PASP, since each possible world can have multiple models, queries are associated with probability ranges indicated by a lower and an upper probability, instead of point values. Similarly, in DTPASP, we need to consider lower and upper rewards, and look for the strategies that maximize either of the two. So a solution of a decision theory problem expressed with DTPASP is composed of two strategies.\nWe have developed two algorithms to solve the task of finding the strategy that maximizes the lower expected reward and the strategy that maximizes the upper expected reward, together with the values of these rewards. A first algorithm is based on answer sets enumeration. This can be adopted only on small domains, so we consider it as a baseline. We propose a second algorithm, based on three layers of Algebraic Model Counting (AMC) (Kimmig et al. 2017) that adopts knowledge compilation (Darwiche and Marquis 2002) to speed up the inference process. Empirical results show that the latter algorithm is significantly faster than the one based on enumeration, and can handle domains of non trivial size.\nThe paper is structured as follows: in Section 2 we briefly discuss background knowledge. Section 3 discusses DTProbLog, a framework to solve decision theory problems in Probabilistic Logic Programming. Section 4 illustrates the task of Second Level Algebraic Model Counting. Section 5 introduces DTPASP together with the optimization task to solve. In Section 6 we describe the algorithms to solve the task that we test against the"}, {"title": "2 Background", "content": "This section introduces the basic concepts of Answer Set Programming and Probabilistic Answer Set Programming."}, {"title": "2.1 Answer Set Programming", "content": "In the following, we will use the verbatim font to denote code that can be executed with a standard ASP solver. Here we consider a subset of ASP (Brewka et al. 2011). An ASP program (or simply an ASP) is a finite set of disjunctive rules. A disjunctive rule (or simply rule) is of the form\n$$h_1;...; h_m:-b_1,... b_n.$$\nwhere every $$h_i$$ is an atom and every $$b_i$$ is a literal. We consider only safe rules, where every variable in the head also appears in a positive literal in the body. This is a standard requirement in ASP. If the head is empty, the rule is called a constraint, if the body is empty and there is only one atom in the head, the rule is called a fact, and if there is only one atom in the head with one or more literals in the body the rule is called normal. A choice rule is of the form $$0\\{a\\}1 :- b_1,...,b_n$$ and indicates that the atom a can be selected or not if the body is true. Usually, we will omit 0 and 1 and consider them implicit. ASP allows the use of aggregate atoms (Alviano and Faber 2018) in the body. We consider aggregates of the form $$\\#\\phi\\{e_0; ... ; e_n\\} \\delta g$$ where g is called guard and can be either a constant or a variable, $$\\delta$$ is a comparison arithmetic operator, $$\\phi$$ is an aggregate function symbol, and $$e_0,..., e_n$$ is a set of expressions where each $$e_i$$ has the form $$t_1,..., t_n : F$$ and each $$t_i$$ is a term whose variables appear in the conjunction of literals F. An example of aggregate atom is $$\\#count\\{A : p(A)\\} = 2$$, that is true if the number of ground substitutions for A that make p(A) true is 2. Here, 2 is the guard, and $$\\#count$$ is the aggregate function symbol. The primal graph of a ground answer set program P is such that there is one vertex for each atom appearing in P and there is an undirected edge between two vertices if the corresponding atoms appear simultaneously in at least one rule of P.\nThe semantics of ASP is based on the concept of answer set, also called stable model. With $$B_P$$ we denote the Herbrand base of an answer set program P, i.e., the set of ground atoms that can be constructed with the symbols in P. A variable is called local to an aggregate if it appears only in the considered aggregate; if instead it occurs in at least one literal not involved in aggregations, it is called global. The grounding of a rule with aggregates proceeds in two steps, by first replacing global variables with ground terms, and then replacing local variables appearing in aggregates with ground terms. An interpretation I of P is a subset of $$B_P$$. An aggregate is true in an interpretation I if the evaluation of the aggregate function under I satisfies the guards. An interpretation satisfies a ground rule if at least one of the $$h_i$$s is true in it when all the $$b_i$$s are true in it. A model of P is an interpretation that satisfies all the groundings of all the rules"}, {"title": "2.2 Probabilistic Answer Set Programming (PASP)", "content": "PASP extends ASP by representing uncertainty with weights (Lee and Wang 2016) or probabilities (Cozman and Mau\u00e1 2016) associated with facts. Here we consider PASP under the credal semantics (CS) (Cozman and Mau\u00e1 2016). We will use the acronym PASP to also denote a probabilistic answer set program, the intended meaning will be clear from the context.\nPASP allows probabilistic facts of the form (De Raedt et al. 2007) $$\\Pi_i :: f_i$$ where $$\\Pi_i \\in [0,1]$$ and $$f_i$$ is an atom. We only consider ground probabilistic facts. Moreover, we require that probabilistic facts cannot appear in the head of rules, a property called disjoint condition (Sato 1995). Every possible subset of probabilistic facts (there are $$2^n$$ of them, where n is the number of probabilistic facts) identifies a world w, i.e., an ASP obtained by adding the atom of the selected probabilistic facts to the rules of the program. Each world w is assigned a probability computed as\n$$P(w) = \\prod_{f_i \\in w} \\Pi_i \\cdot \\prod_{f_i \notin w} (1 - \\Pi_i).$$\nWith this setting, we have two levels to consider: at the first level, we need to consider the worlds, each with an associated probability. At the second level, for each world we have one or more answer sets. Since a world may have more than one model, in order to assign a probability to queries we need to decide how the probability mass of the world is distributed on its answer sets. We can choose a particular distribution for the answer sets of a world, such as a uniform (Totis et al. 2023) or a distribution that maximizes the entropy (Kern-Isberner and Thimm 2010). However, here we follow the more general path of the credal semantics and we refrain from assuming a certain distribution for the answer sets of a world. This implies that queries are associated with probability"}, {"title": "3 DTProbLog", "content": "DTProbLog extends the ProbLog language with a set D of (possibly non-ground) decision atoms represented with the syntax $$? :: d$$ where d is an atom, and a set U of utility attributes of the form $$u \\rightarrow r$$, where $$r \\in R$$ is the reward obtained when the utility atom u is satisfied. In the rest of the paper, we will use the terms utility and reward interchangeably and we will use the notation utility(u,r) in code snippets to denote utility attributes. A set of decision atoms defines a strategy $$\\sigma$$. There are $$2^{|D|}$$ possible strategies. A DTProbLog program is a tuple $$(P, U, D)$$ where P is a ProbLog program, U is a set of utility attributes, and D a set of decision atoms. Given a strategy $$\\sigma$$, adding all decision atoms from $$\\sigma$$to P yields a ProbLog program $$P_{\\sigma}$$. The utility of a strategy $$\\sigma$$, Util($$\\sigma$$), is given by:\n$$Util(\\sigma) = \\sum_{w_{\\sigma} \\in P_{\\sigma}} P(w_{\\sigma}) \\cdot R(w_{\\sigma})$$\nwhere\n$$R(w) = \\sum_{(u\\rightarrow r) \\in U, w \\models u} r.$$"}, {"title": "4 Second Level Algebraic Model Counting", "content": "Kiesel et al. (2022) introduced Second Level Algebraic Model Counting (2AMC), needed to solve tasks such as MAP inference (Shterionov et al. 2015) and Decision theoretic inference (Van den Broeck et al. 2010), in Probabilistic Logic Programming, and inference in smProbLog (Totis et al. 2023) programs. These problems are characterized by the need for two levels of Algebraic Model Counting (2AMC) (Kimmig et al. 2017).\nThe ingredients of a 2AMC problem are:\n\\begin{itemize}\n    \\item a propositional theory $$\\Pi$$;\n    \\item a partition $$(V_i, V_o)$$ of the variables in $$\\Pi$$;\n    \\item two commutative semirings (Gondran and Minoux 2008) $$R_i = (R^i, \\oplus^i, \\otimes^i, n^i_e, n^i_a)$$ and $$R_o = (R^o, \\oplus^o, \\otimes^o, n^o_e, n^o_a)$$;\n    \\item two weight functions, $$w_i$$ and $$w_o$$, associating each literal of the program with a weight; and\n    \\item a transformation function $$f$$ mapping the values of $$R_i$$ to those of $$R_o$$\\end{itemize}\nLet us denote with T the tuple $$(\\Pi, V_i, V_o, R_i, R_o, w_i, w_o, f)$$. The task requires solving:\n$$2AMC(T) = \\bigoplus_{I_o \\in \\mu(V_o)} \\bigotimes_{\\alpha \\in I_o} w_o(\\alpha) \\otimes^o f(\\bigoplus_{I_i \\in \\mu(\\Pi | I_o)} \\bigotimes_{\\alpha \\in I_i} w_i(b)).$$\nwhere $$\\mu(V_o)$$ is the set of possible assignments to the variables in $$V_o$$ and $$(\\Pi | I_o)$$ is the set of possible assignments to the variables in $$\\Pi$$ that satisfy $$I_o$$. In practice, for every possible assignment of the variables $$V_o$$, we need to solve a first AMC task on the variables $$V_i$$. Then, the transformation function maps the obtained values into elements of the outer semiring and we need to solve a second AMC task, this time by considering $$V_o$$.\nWithin 2AMC, the DTProbLog task can be solved by considering (Kiesel et al. 2022)\n\\begin{itemize}\n    \\item as $$V_o$$ the decision atoms and as $$V_i$$ the remaining literals;\n    \\item as inner semiring the gradient semiring (Eisner 2002) $$R_i = (R^2, +, \\otimes, (0,0), (0, 1))$$ where + is component-wise and $$(a_o, b_o) \\otimes (a_1, b_1) = (a_o \\cdot a_1, a_o \\cdot b_1 + a_1 \\cdot b_o)$$;\n    \\item as inner weight function $$w_i$$ mapping a literal v to $$(p,0)$$ if $$v = a$$ where a is a probabilistic fact $$p :: a$$, to $$(1 - p,0)$$ if $$v = not \\: a$$ where a is a probabilistic fact $$p :: a$$, and all the other literals to $$(1,r)$$ where r is their utility;\n    \\item as transformation function $$f(p, u) = (u, \\{-\\infty\\})$$ if $$p \\neq 0$$ and $$f(0, u) = (-\\infty, D)$$;\n    \\item as outer semiring $$R_o = (R \\times 2^{|D|}, \\oplus, \\otimes, (-\\infty, D), (0, \\{\\}))$$ where $$(a_o, b_o) + (a_1, b_1)$$ is equal to $$(a_o, b_o)$$ if $$a_o > a_1$$, otherwise $$(a_1, b_1)$$ and $$(a_o, b_o) \\otimes (a_1, b_1) = (a_o + a_1, b_o \\cup b_1)$$; and\n    \\item as outer weight function $$w_o = (0, \\{a\\})$$ if a is a decision atom, $$(0, \\{\\})$$ otherwise.\\end{itemize}\nKiesel et al. (2022) also extended the aspmc tool (Eiter et al. 2021) to solve 2AMC tasks. aspmc converts a program into a tractable circuit (knowledge compilation) by first grounding it and then by generating a propositional formula such that the answer sets of the original program are in one-to-one correspondence with the mod-"}, {"title": "5 Representing Decision Theory Problems with Probabilistic Answer Set Programming", "content": "Following DTProbLog (Van den Broeck et al. 2010), we use utility(a, r) to denote utility attributes where a is an atom and $$r \\in R$$ indicates the utility of satisfying it. For example, with utility(a, -3.3) we state that if a is satisfied we get a utility of -3.3. A negative utility represents, for example, a cost, while a positive utility represents a gain. We use the functor decision to denote decision atoms. For example, with decision a we state that a is a decision atom. A decision atom indicates that we can choose whether to perform or not to perform the specified action. We consider only ground decision atoms.\nDefinition 2\nA decision theory probabilistic answer set program DTPASP is a tuple (P, D,U) where P is a probabilistic answer set program, D is the set of decision atoms, and U is the set of utility attributes.\nSince in PASP queries are associated with a lower and an upper probability, in DTPASP we need to consider lower and upper rewards and look for the strategies that maximize them. A strategy is, as in DTProbLog, a subset of the possible actions. Having fixed a strategy $$\\sigma$$, we obtain a PASP $$P_{\\sigma}$$, that generates a set of worlds. Each answer set A of a world $$w_{\\sigma}$$ of $$P_{\\sigma}$$ is associated with a reward given by the sum of the utilities of the atoms true in it:\n$$R(A) = \\sum_{(a,r) \\in U, a \\in A} r.$$\nLet us call $$\\underline{R(w_{\\sigma})}$$ the minimum of the rewards of an answer set of $$w_{\\sigma}$$ and $$\\overline{R(w_{\\sigma})}$$ the maximum of the rewards of an answer set of $$w_{\\sigma}$$. That is:\n$$\\underline{R(w_{\\sigma})} = \\min_{A \\in AS(w_{\\sigma})} R(A),$$\n$$\\overline{R(w_{\\sigma})} = \\max_{A \\in AS(w_{\\sigma})} R(A).$$\nSince we impose no constraints on how the probability mass of a world is distributed among its answer sets, we can obtain the minimum utility from a world by assigning all the mass to the answer set with the minimum reward and the maximum utility from a world by assigning all the mass to the answer set with the maximum reward. The first is"}, {"title": "6 Algorithms for Computing the Best Strategy in a DTPASP", "content": "To solve the optimization problems represented in Equation (13), we discuss two different algorithms. We have three different layers of complexity: i) the computation of the possible strategies, ii) the computation of the worlds, and iii) the computation of the answer sets with the highest and lowest reward (Equation (10)). Due to this, the task"}, {"title": "6.1 Implementation in aspmc", "content": "Before discussing the algorithm, let us introduce some concepts.\nDefinition 3\nA tree decomposition (Bodlaender 1988) of a graph G is a pair $$(T, X)$$, where T is a tree and $$\\chi$$ is a labeling of V(T) (the set of nodes of T) by subsets of V(G) (the set of nodes of G) s.t. 1) for all nodes $$v \\in V(G)$$ there is $$t \\in V(T)$$ s.t. $$v \\in \\chi(t)$$; 2) for every edge $$\\{v_1, v_2\\} \\in V(E)$$ there exists $$t \\in V(T)$$ s.t. $$v_1, v_2 \\in \\chi(t)$$; and 3) for all nodes $$v \\in V(G)$$ the set of nodes $$\\{t \\in V(T) \\: | \\: v \\in \\chi(t)\\}$$ forms a (connected) subtree of T. The width of $$(T, X)$$ is $$\\max_{t \\in V'} |\\chi(t)| - 1$$. The treewidth of a graph is the minimal width of any of its tree decompositions.\nIntuitively, treewidth is a measure of the distance of a graph from being a tree. Accord- ingly, a graph is a tree if and only if it has treewidth 1. The idea behind treewidth is that problems that are simple when their underlying structure is a tree, may also be simple when they are not far from trees, i.e., have low treewidth. Practically, we can use tree decompositions witnessing the low treewidth to decompose problems into smaller subproblems in such cases.\nWe assume that programs have already been translated into equivalent 3AMC in- stances, where the propositional theory is a propositional formula in conjunctive normal"}, {"title": "7 Experiments", "content": "We implemented the two aforementioned algorithms in Python. We integrated the enumeration based algorithm into the open source PASTA solver (Azzolini et al. 2022) that leverages clingo (Gebser et al. 2019) to compute the answer sets. The algorithm based on 3AMC is built on top of aspmc (Eiter et al. 2021) and we call it aspmc3. During the discussion of the results, we denote them as PASTA and aspmc3, respectively. We ran the experiments on a computer with Intel\u00ae Xeon\u00ae E5-2630v3 running at 2.40 GHz with 8 Gb of RAM and a time limit of 8 hours. Execution times are computed with the bash command time and we report the real field. We generated six synthetic datasets for the experiments. In the following examples, we report the aspmc3 version of the code. The programs are the same for PASTA except for the negation symbol: not for PASTA and + for aspmc3. All the probabilities of the probabilistic facts are randomly set. In the following snippets we will use the values 0.1, 0.2, 0.3, and 0.4 for conciseness. Moreover, every PASP obtained from every strategy has at least one answer set per world.\nAs a first test (t1), we fix the number n of probabilistic facts to 2, 5, 10, and 15 and increase the number d of decision atoms from 0 until we get a memory error or reach the timeout. We associate a utility of 2 to qr and -12 to nqr, as in Example 6. We use da/1 for decision atoms and a/1 for probabilistic facts. We identify the different individuals of the programs with increasing integers, starting from 0, and, in each of these, we add a rule qr : - a(j), da(i) if i is even and two rules qr : \u2013 da(i), a(j), + nqr and nqr :- da(i), a(j), + qr, if i is odd, where j = i%n. For example, with n = 2 and d = 4, we have:"}, {"title": "8 Related Work", "content": "This work is inspired to DTProbLog (Van den Broeck et al. 2010). If we only consider normal rules, the decision theory task can be expressed with both DTProbLog and our framework, but our framework is more general, since it admits a large subset of the whole ASP syntax.\nThe possibility of expressing decision theory problems with ASP gathered a lot of research interest in the past years. The author of (Brewka 2002) extended ASP by introducing Logic Programming with Ordered Disjunction (LPODs) based on the use of a new connective called ordered disjunction that specifies an order of preferences among the possible answer sets. This was the starting point for several works: Brewka (2003) proposes a framework for quantitative decision making while Confalonieri and Prade (2011) adopt a possibilistic extension of LPODs. Also Grabo\u015b (2004) adopts LPODs and casts the decision theory problem as a constraint satisfaction problem with the goal of identifying the preferred stable models. Differently from these works, we define uncertainty using probabilistic facts, possible actions using decision atoms, and associate weights (utilities) to"}, {"title": "9 Conclusions and Future Works", "content": "In this paper, we discussed how to encode and solve a decision theory task with Probabilistic Answer Set Programming under the credal semantics. We proposed the class of decision theoretic probabilistic answer set programs, i.e., probabilistic answer set programs extended with decision atoms, representing the possible actions that can be taken, and utility attributes, representing the rewards that can be obtained. The goal is to find the two sets of decision atoms that yield the highest lower bound and the highest upper bound for the overall utility, respectively. We developed an algorithm based on three layers of Algebraic Model Counting and knowledge compilation and compared it against a naive algorithm based on answer set enumeration. Empirical results show that our approach is able to manage instances of non trivial sizes in a reasonable amount of time. A possible future work consists of proposing a formalization of the decision theory task also for other semantics for probabilistic ASP. Moreover, we could consider decision problems also in the case that the probabilistic facts are annotated with probability intervals, exploiting the results for inference in Azzolini and Riguzzi (2024)."}]}