{"title": "Confidence-weighted integration of human and machine judgments for superior decision-making", "authors": ["Felipe Y\u00e1\u00f1ez", "Xiaoliang Luo", "Omar Valerio Minero", "Bradley C. Love"], "abstract": "Large language models (LLMs) have emerged as powerful tools in various domains. Recent studies have shown that LLMs can surpass humans in certain tasks, such as predicting the outcomes of neuroscience studies [1]. What role does this leave for humans in the overall decision process? One possibility is that humans, despite performing worse than LLMs, can still add value when teamed with them. A human and machine team can surpass each individual teammate when team members' confidence is well-calibrated and team members diverge in which tasks they find difficult (i.e., calibration and diversity are needed). We simplified and extended a Bayesian approach to combining judgments using a logistic regression framework that integrates confidence-weighted judgments for any number of team members. Using this straightforward method, we demonstrated in a neuroscience forecasting task [1] that, even when humans were inferior to LLMs, their combination with one or more LLMS consistently improved team performance. Our hope is that this simple and effective strategy for integrating the judgments of humans and machines will lead to productive collaborations.", "sections": [{"title": "1 Introduction", "content": "Modern environments increasingly stretch our ability to process the vast amounts of information available to us [2, 3]. In contrast, machine systems can often take advantage of vast information resources [4, 5, 6, 7]. As machines reach superhuman performance levels [6, 8, 9], one concern is whether machines will supplant human judgment in critical areas [10, 11].\nOne potential solution is forming human-machine teams in which judgments from humans and machines are integrated [12, 13]. It might be possible that humans can contribute to and make the overall team better even when their performance is worse on average than their machine teammates.\nWe will evaluate this possibility in a knowledge-intensive task where large language models (LLMs) surpass humans in predicting the outcomes of neuroscience studies [1]. Human-LLM teaming combines the individual judgments of humans and machines. Complementarity is realized when a team's performance improves beyond that of either teammate alone [12, 13]. We investigate whether human-LLM teams outperform LLMs even when humans have inferior performance compared to LLMs. There are two key conditions"}, {"title": "2 Methods", "content": "Our contribution relies on previous efforts that developed BrainBench [1] to assess the capacity of humans and LLMs to predict the outcomes of neuroscience studies. The benchmark includes test cases based on abstracts from the Journal of Neuroscience. Each test case contains an original abstract and an altered version (Figure 1), LLMs outperform humans by a large margin in test cases created either by expert neuroscientists or by prompting GPT-4 to create test cases [1]. Since substantial differences in performance may preclude complementarity [12], we used the GPT-4 generated test cases because the performance difference between humans and LLMs was smaller for these test cases, though LLMs were still clearly superior to humans on these test items."}, {"title": "2.1 Test cases", "content": "BrainBench [1] is a benchmark that includes 100 test cases generated by GPT-4 (Azure OpenAI API; version 2023-05-15). These test cases were created from abstracts in the Journal of Neuroscience published in 2023. These abstracts are categorized into five sections: Behavioral/Cognitive, Systems/Circuits, Development/Plasticity/Repair, Neurobiology of Disease, and Cellular/Molecular. Each test case contains a published abstract and an altered version produced by GPT-4 (see details in [1, Dataset Creation]). These modifications, though minimal, significantly change the results for instance, by changing the roles of brain regions or reversing a result's direction (e.g., from \"decreases\" to \"increases\"). The altered abstracts remain logically coherent despite the changes. The BrainBench task is to identify the correct study outcome by choosing between the original abstract and its altered counterpart."}, {"title": "2.2 Procedure for human participants", "content": "One hundred seventy-one neuroscience experts were recruited to complete an online study (see details in [1, Evaluations]). Each participant evaluated three out of the 100 test cases. Two versions of an abstract were presented, one with the actual results and one that was altered (Figure 1). Participants chose the version they believed to be the original and rated their confidence using a slider bar. After applying several exclusion criteria, the 171 participants yielded 503 observations (2 to 9 instances per test case)."}, {"title": "2.3 Procedure for LLMs", "content": "We considered Llama 2 chat models with 7B, 13B, and 70B weights [15]. LLMs chose the version of the abstract with the lower perplexity (PPL). Confidence was calculated as the absolute difference in PPL between the original and altered versions of the abstract (Figure 1)."}, {"title": "2.4 Bayesian combination model", "content": "Human and LLM judgments were combined by adapting a Bayesian framework for human-machine complementarity [12]. The problem setting for combining two team members, human and LLM, is as follows: Let N denote the number of test cases to be analyzed with two possible choices, \"0: first option\" or \"1: second option\". The ground truth labels of the original abstracts are $z \\in \\{0,1\\}^N$. For the human classifier, the predicted labels $y \\in \\{0,1\\}^N$ and their corresponding confidence ratings $r \\in \\{0, ..., R-1\\}^N$ with \u201c0: lowest possible confidence\" and \"R - 1: highest possible confidence\" are given. For the LLM classifier, the PPL $q \\in R^{N \\times 2}$ reflects a measure of uncertainty [1]. We used probability confidence scores, $\\pi = Softmax(-q)$. The first step of this model is to generate correlated probability confidence scores for human and machine classifiers using a bivariate normal distribution,\n$\\begin{pmatrix} \\pi_H \\\\ \\pi_M \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} \\mu_H \\\\ \\mu_M \\end{pmatrix}, \\begin{pmatrix} \\sigma_H^2 & \\rho \\sigma_H \\sigma_M \\\\ \\rho \\sigma_H \\sigma_M & \\sigma_M^2 \\end{pmatrix} \\right)$ (1)\nThe means of the underlying distribution, $\\mu_H$ and $\\mu_M$, depend whether the label of test case i, $z_i$, is correct or not, i.e.,\n$\\begin{aligned} \\mu_{H,i,j} &= b_H + (a_H - b_H) \\cdot 1_{Z_i}(j), \\\\ \\mu_{M,i,j} &= b_M + (a_M - b_M) \\cdot 1_{Z_i}(j), \\end{aligned}$\nwith $Z_i = \\{x | x = z_i\\}$. Note that the scalar parameters $a_H, a_M, b_H, b_M, \\sigma_H, \\sigma_M$, and $\\rho_{HM}$ in Eq. (1) are learned from data. The parameter $\\rho_{HM}$ from the covariance matrix learns the correlation between the human and machine classifiers. In the case of the machine classifier, $\\pi_M$ is compared to the empirical probability confidence scores, $\\pi$. Then, for the human classifier, $\\pi_H$ is a latent variable that is used to calculate classifications\n$y_H \\sim Categorical(Softmax(\\pi_H / \\tau))$,\nwhere $\\tau$ denotes a temperature parameter, usually small, that helps convergence [12]. The predicted classifications, $y_H$, are compared to actual human predictions, y. The confidence ratings are casted via an ordered logistic model [16] that maps the continuous probability confidence scores, $\\pi_H$, to an ordinal confidence rating, $r_H$. This means,\n$r_H \\sim OrderedLogistic(\\pi_H, c, \\delta)$,\nwhere the parameters $c \\in R^{R-1}$ are the breakpoints of the intervals that map $\\pi_H$ into $r_H$, and $\\delta$ is a scalar that controls the sharpness of the rating probability curves. Finally, $r_H$ is compared to the empirical human confidence ratings, r. See supplementary information for implementation details."}, {"title": "2.5 Logistic combination model", "content": "We introduce a logistic regression approach that combines the judgments of any number of teammates. The logistic combination model follows the principles of the Bayesian combination model, but is formulated within an easier to implement and extend regression framework. In its most basic form, which we consider here, each teammate is captured by a single predictor in the regression model. The value of the predictor on a trial depends on the teammate's choice and their confidence. In particular, the magnitude of the predictor is the teammate's confidence on that trial (i.e., confidence-weighted integration) and the sign is determined by the teammate's choice. In general, the fitted beta weight for a teammate will reflect their accuracy and calibration."}, {"title": "2.6 Cross-validation procedure", "content": "Given the 503 observations for the 100 test cases, we performed a leave-one-out cross-validation (LOOCV) [17, 18] that provides the best bias-variance trade-off for small datasets. Consider the evaluation of the i-th test case. With this procedure, we removed all instances of test case i (between 2 and 9) leaving the remaining 99 test cases with all their instances (between 494 and 501) to train the classifier teams. For testing, we utilized all the instances of test case i. This was repeated for all 100 test cases, yielding 503 predictions. Note that for individual teammates the evaluation comprised only the testing phase. When the predicted labels of any team or individual teammate in this study were randomly shuffled, the LOOCV accuracy dropped approximately to chance level (i.e., 50%)."}, {"title": "3 Results", "content": "We first assessed the conditions for effective collaboration (i.e., complementarity), namely calibration of confidence and classification diversity among team members. Both humans and LLMs were calibrated in that accuracy was positively correlated with confidence (Figure 2a). Diversity held in that LLMs and humans differed on which test items led to errors (Figure 2c).\nIn terms of accuracy (Figure 2b), LLMs numerically surpassed humans by a small margin (t(2) = 5.20, P < 0.05). Thus, we can consider whether humans can benefit teams consisting of machines that perform comparably or better."}, {"title": "4 Discussion", "content": "Can humans team effectively with LLMs when the humans perform worse? We developed a confidence-weighted regression approach that can integrate judgments from any number of teammates. Using this method and testing on a forecasting benchmark (Figure 1), we found that human-LLM teams achieve complementarity, that is their combined performance bests that of either teammate alone (Figure 3). Complementarity was achieved because two critical conditions were satisfied, namely confidence was well-calibrated and classification diversity held among teammates (Figure 2). Strikingly, every combination"}, {"title": "5 Data availability", "content": "The human participant data and LLM perplexity scores utilized in this study have been previously reported in [1], and are publicly available at"}, {"title": "6 Code availability", "content": "All computer code associated with this work including combination model implementations, team evaluations, and analyses are publicly available at"}]}