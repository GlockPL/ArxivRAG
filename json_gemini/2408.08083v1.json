{"title": "Confidence-weighted integration of human and machine judgments for superior decision-making", "authors": ["Felipe Y\u00e1\u00f1ez", "Xiaoliang Luo", "Omar Valerio Minero", "Bradley C. Love"], "abstract": "Large language models (LLMs) have emerged as powerful tools in various domains. Recent studies have shown that LLMs can surpass humans in certain tasks, such as predicting the outcomes of neuroscience studies [1]. What role does this leave for humans in the overall decision process? One possibility is that humans, despite per- forming worse than LLMs, can still add value when teamed with them. A human and machine team can surpass each individual teammate when team members' confi- dence is well-calibrated and team members diverge in which tasks they find difficult (i.e., calibration and diversity are needed). We simplified and extended a Bayesian approach to combining judgments using a logistic regression framework that inte- grates confidence-weighted judgments for any number of team members. Using this straightforward method, we demonstrated in a neuroscience forecasting task [1] that, even when humans were inferior to LLMs, their combination with one or more LLMS consistently improved team performance. Our hope is that this simple and effective strategy for integrating the judgments of humans and machines will lead to productive collaborations.", "sections": [{"title": "1 Introduction", "content": "Modern environments increasingly stretch our ability to process the vast amounts of in- formation available to us [2, 3]. In contrast, machine systems can often take advantage of vast information resources [4, 5, 6, 7]. As machines reach superhuman performance levels [6, 8, 9], one concern is whether machines will supplant human judgment in critical areas [10, 11]. One potential solution is forming human-machine teams in which judgments from humans and machines are integrated [12, 13]. It might be possible that humans can contribute to and make the overall team better even when their performance is worse on average than their machine teammates. We will evaluate this possibility in a knowledge-intensive task where large language models (LLMs) surpass humans in predicting the outcomes of neuroscience studies [1]. Human-LLM teaming combines the individual judgments of humans and machines. Com- plementarity is realized when a team's performance improves beyond that of either team- mate alone [12, 13]. We investigate whether human-LLM teams outperform LLMs even when humans have inferior performance compared to LLMs. There are two key conditions"}, {"title": "2 Methods", "content": "Our contribution relies on previous efforts that developed BrainBench [1] to assess the capacity of humans and LLMs to predict the outcomes of neuroscience studies. The benchmark includes test cases based on abstracts from the Journal of Neuroscience. Each test case contains an original abstract and an altered version (Figure 1), LLMs outperform humans by a large margin in test cases created either by expert neuroscientists or by prompting GPT-4 to create test cases [1]. Since substantial differences in performance may preclude complementarity [12], we used the GPT-4 generated test cases because the performance difference between humans and LLMs was smaller for these test cases, though LLMs were still clearly superior to humans on these test items."}, {"title": "2.1 Test cases", "content": "BrainBench [1] is a benchmark that includes 100 test cases generated by GPT-4 (Azure OpenAI API; version 2023-05-15). These test cases were created from abstracts in the Journal of Neuroscience published in 2023. These abstracts are categorized into five sections: Behavioral/Cognitive, Systems/Circuits, Development/Plasticity/Repair, Neu- robiology of Disease, and Cellular/Molecular. Each test case contains a published abstract and an altered version produced by GPT-4 (see details in [1, Dataset Creation]). These modifications, though minimal, significantly change the results for instance, by chang- ing the roles of brain regions or reversing a result's direction (e.g., from \"decreases\" to \"increases\"). The altered abstracts remain logically coherent despite the changes. The BrainBench task is to identify the correct study outcome by choosing between the original abstract and its altered counterpart."}, {"title": "2.2 Procedure for human participants", "content": "One hundred seventy-one neuroscience experts were recruited to complete an online study (see details in [1, Evaluations]). Each participant evaluated three out of the 100 test cases. Two versions of an abstract were presented, one with the actual results and one that was altered (Figure 1). Participants chose the version they believed to be the original and rated their confidence using a slider bar. After applying several exclusion criteria, the 171 participants yielded 503 observations (2 to 9 instances per test case)."}, {"title": "2.3 Procedure for LLMs", "content": "We considered Llama 2 chat models with 7B, 13B, and 70B weights [15]. LLMs chose the version of the abstract with the lower perplexity (PPL). Confidence was calculated as the absolute difference in PPL between the original and altered versions of the abstract (Figure 1)."}, {"title": "2.4 Bayesian combination model", "content": "Human and LLM judgments were combined by adapting a Bayesian framework for human- machine complementarity [12]. The problem setting for combining two team members, human and LLM, is as follows: Let N denote the number of test cases to be analyzed with two possible choices, \"0: first option\" or \"1: second option\". The ground truth labels of the original abstracts are \\(z \\in \\{0,1\\}^N\\). For the human classifier, the predicted labels \\(y \\in \\{0,1\\}^N\\) and their corresponding confidence ratings \\(r \\in \\{0, . . ., R-1\\}^N\\) with \u201c0: lowest possible confidence\" and \"R - 1: highest possible confidence\" are given. For the LLM classifier, the PPL \\(q \\in R^{N \\times 2}\\) reflects a measure of uncertainty [1]. We used probability confidence scores, \\(\\pi = Softmax(-q)\\). The first step of this model is to generate correlated probability confidence scores for human and machine classifiers using a bivariate normal distribution,\n\n\\begin{equation}\n\\begin{pmatrix}\n\\pi_H \\\\\n\\pi_M\n\\end{pmatrix} \\sim N\\left(\\begin{pmatrix}\n\\mu_H \\\\\n\\mu_M\n\\end{pmatrix}, \\begin{pmatrix}\n\\sigma_H^2 & \\rho \\sigma_H \\sigma_M \\\\\n\\rho \\sigma_H \\sigma_M & \\sigma_M^2\n\\end{pmatrix}\\right)\n\\tag{1}\n\\end{equation}\n\nThe means of the underlying distribution, \\(\\mu_H\\) and \\(\\mu_M\\), depend whether the label of test case i, \\(z_i\\), is correct or not, i.e.,\n\n\\begin{align*}\n\\mu_{H,i,j} &= b_H + (a_H - b_H) \\cdot \\mathbb{1}_{Z_i}(j), \\\\\n\\mu_{M,i,j} &= b_M + (a_M - b_M) \\cdot \\mathbb{1}_{Z_i}(j),\n\\end{align*}\n\nwith \\(Z_i = \\{x | x = z_i\\}\\). Note that the scalar parameters \\(a_H, a_M, b_H, b_M, \\sigma_H, \\sigma_M\\), and \\(\\rho_{HM}\\) in Eq. (1) are learned from data. The parameter \\(\\rho_{HM}\\) from the covariance matrix learns the correlation between the human and machine classifiers. In the case of the machine classifier, \\(\\pi_M\\) is compared to the empirical probability confidence scores, \\(\\pi\\). Then, for the human classifier, \\(\\pi_H\\) is a latent variable that is used to calculate classifications\n\n\\begin{equation}\ny_H \\sim Categorical(Softmax(\\pi_H/\\tau)),\n\\end{equation}\n\nwhere \\(\\tau\\) denotes a temperature parameter, usually small, that helps convergence [12]. The predicted classifications, \\(y_H\\), are compared to actual human predictions, \\(y\\). The confidence ratings are casted via an ordered logistic model [16] that maps the continuous probability confidence scores, \\(\\pi_H\\), to an ordinal confidence rating, \\(r_H\\). This means,\n\n\\begin{equation}\nr_H \\sim OrderedLogistic(\\pi_H, c, \\delta),\n\\end{equation}\n\nwhere the parameters \\(c \\in \\mathbb{R}^{R-1}\\) are the breakpoints of the intervals that map \\(\\pi_H\\) into \\(r_H\\), and \\(\\delta\\) is a scalar that controls the sharpness of the rating probability curves. Finally, \\(r_H\\) is compared to the empirical human confidence ratings, \\(r\\). See supplementary information for implementation details."}, {"title": "2.5 Logistic combination model", "content": "We introduce a logistic regression approach that combines the judgments of any number of teammates. The logistic combination model follows the principles of the Bayesian combination model, but is formulated within an easier to implement and extend regression framework. In its most basic form, which we consider here, each teammate is captured by a single predictor in the regression model. The value of the predictor on a trial depends on the teammate's choice and their confidence. In particular, the magnitude of the predictor is the teammate's confidence on that trial (i.e., confidence-weighted integration) and the sign is determined by the teammate's choice. In general, the fitted beta weight for a teammate will reflect their accuracy and calibration.\nAs in the Bayesian combination model, y, r, q are given. For the human classifier, the confidence ratings were of the form: \\(r \\in \\{1, ..., 100\\}^N\\). In the case of LLMs, the absolute difference in PPL \\(|\\Delta q_i| \\in \\mathbb{R}\\) corresponds to model confidence [1].\nThe logistic function for the i-th test case is of the form:\n\n\\begin{equation}\nP_i = \\frac{1}{1+ e^{-\\beta^T x}},\n\\end{equation}\n\nwhere \\(p_i\\) is the predicted probability of the arbitrarily assigned first option, and the evidence is\n\n\\begin{equation}\n\\beta^T x = \\beta_\\iota + \\beta_H \\cdot x_H + \\beta_M \\cdot x_M.\n\\tag{2}\n\\end{equation}\n\nThe fitted weights \\(\\beta_\\iota, \\beta_H\\), and \\(\\beta_M\\) correspond to the intercept, and human and machine teammates, respectively. The term \\(x_k\\) is the signed confidence for team member k. For human participants, \\(x_H\\) is \\(r_i\\) if \\(y_i = 0\\) and \\(-r_i\\) otherwise. Thus,\n\n\\begin{equation}\nx_H = r_i(1 - 2 \\cdot \\mathbb{1}_A(y_i)),\n\\end{equation}\n\nwhere \\(\\mathbb{1}_A(y_i)\\) is an indicator function with \\(A = \\{x | x = 1\\}\\). Hence, \\(\\mathbb{1}_A(x) = 1\\) if \\(x \\in A\\), and \\(\\mathbb{1}_A(x) = 0\\) otherwise. For LLM team members, it reads\n\n\\begin{equation}\nx_M = |q_i|(1 - 2 \\cdot \\mathbb{1}_A(y_i)) .\n\\end{equation}\n\nThis model can be easily expanded by including additional terms to Eq. (2). For example, a third fitted weight could be included for an interaction term \\(x_H x_M\\). Likewise, polynomial regression could be used to include \\(x_H^2\\) and \\(x_M^2\\), and corresponding fitted weights. This approach can be readily extended to more than 2 team members by using multinomial regression."}, {"title": "2.6 Cross-validation procedure", "content": "Given the 503 observations for the 100 test cases, we performed a leave-one-out cross- validation (LOOCV) [17, 18] that provides the best bias-variance trade-off for small datasets. Consider the evaluation of the i-th test case. With this procedure, we re- moved all instances of test case i (between 2 and 9) leaving the remaining 99 test cases with all their instances (between 494 and 501) to train the classifier teams. For testing, we utilized all the instances of test case i. This was repeated for all 100 test cases, yielding 503 predictions. Note that for individual teammates the evaluation comprised only the testing phase. When the predicted labels of any team or individual teammate in this study were randomly shuffled, the LOOCV accuracy dropped approximately to chance level (i.e., 50%)."}, {"title": "3 Results", "content": "We first assessed the conditions for effective collaboration (i.e., complementarity), namely calibration of confidence and classification diversity among team members. Both humans and LLMs were calibrated in that accuracy was positively correlated with confidence (Figure 2a). Diversity held in that LLMs and humans differed on which test items led to errors (Figure 2c).\nIn terms of accuracy (Figure 2b), LLMs numerically surpassed humans by a small margin (t(2) = 5.20, P < 0.05). Thus, we can consider whether humans can benefit teams consisting of machines that perform comparably or better.\nOf primary interest was whether teams including humans performed better than LLM only teams. All 15 possible team combinations, ranging from individual teammates to a 4-way human-LLM team, were considered (Figure 3). Adding a human teammate to LLM-only teams always improved the team's performance (t(6) = 8.22, P < 0.0001).\nOur confidence-weighted logistic regression approach follows from the principles of a Bayesian combination model that fosters human-machine complementarity [12]. One questions is how well our logistic regresion approach compares to the Bayesian approach. Our confidence-weighted regression model compared favorably (Figure 4) to the Bayesian model when evaluated on the three human-LLM teams for which the Bayesian model is intended to apply (t(2) = 4.08, P < 0.01). This success is impressive given that the regression approach takes seconds to compute on a current desktop whereas the Bayesian approach would take 12-13 days."}, {"title": "4 Discussion", "content": "Can humans team effectively with LLMs when the humans perform worse? We developed a confidence-weighted regression approach that can integrate judgments from any number of teammates. Using this method and testing on a forecasting benchmark (Figure 1), we found that human-LLM teams achieve complementarity, that is their combined perfor- mance bests that of either teammate alone (Figure 3). Complementarity was achieved because two critical conditions were satisfied, namely confidence was well-calibrated and classification diversity held among teammates (Figure 2). Strikingly, every combination of LLMs benefited from adding a human to the team (Figure 3).\nOur approach was informed by a Bayesian method for combining judgments of humans and machines [12]. Our approach has a number of advantages, including ease of implemen- tation, very fast runtime, an interpretable solution, and readily extendable to any number of teammates. Surprisingly, our confidence-weighted regression approach performed better than the Bayesian approach (Figure 4). One possibility is that the discretization of con- tinuous confidence measures, which the Bayesian model requires, limited its performance. Perhaps an alternative formulation would perform better. Unfortunately, reformulating the Bayesian model and properly implementing it requires substantial effort and expertise. In contrast, because our confidence-weighted integration model is formulated within a re- gression framework, it is straightforward to extend to include other factors. For example, nonlinear relationships (e.g., polynomial terms) between confidence-weighted predicitons and outcomes could be considered.\nWhile we selected three LLMs with superhuman performance on BrainBench, these LLMs are not the highest performing models on this benchmark [1]. Our choice was deliberate because a vastly superior teammate may hinder complementarity. In the limit, a teammate who is never wrong does not need to be part of a team. This limiting condition may become more prevalent should LLMs continue to improve and, therefore, diminish the benefits of human-LLM teaming. For the foreseeable future, we suspect there will be tasks for which humans and LLMs can effectively team. Moreover, our method for integrating the judgments of teammates is not limited to human-LLM teams. Instead, the method is general and applies to any set of agents (natural or artificial) that can report how confident they are in their decisions.\nThis study explored the possibility of a collaborative approach between humans and LLMs for superior decision-making in predicting neuroscience outcomes. Our confidence- weighted regression method effectively combined human and LLM judgments because teammates fulfilled the conditions of well-calibrated confidence and classification diversity. Our results suggest there is a place for humans in teams with machines even when the machines perform better. We hope our work facilitates successful collaborations between humans and machines in addressing important challenges."}, {"title": "5 Data availability", "content": "The human participant data and LLM perplexity scores utilized in this study have been previously reported in [1], and are publicly available at\nhttps://github.com/braingpt-lovelab/BrainBench."}, {"title": "6 Code availability", "content": "All computer code associated with this work including combination model implementa- tions, team evaluations, and analyses are publicly available at\nhttps://github.com/braingpt-lovelab/haico ."}, {"title": "Implementation of Bayesian combination model", "content": "Algorithm 1 Bayesian Combination Model\n~\\\n1: Set priors: \\(a = 1\\), \\(a_M \\sim N(0,10)\\), \\(b_M \\sim N(0,10)\\), \\(\\sigma_M \\sim Uniform(0,15)\\), \\(a_H ~\\\nN(0,10)\\), \\(b_H = 0\\), \\(\\sigma_H = 1\\), \\(\\rho \\sim Uniform(-1,1)\\), \\(\\tau = 0.05\\), \\(c ~ Uniform(0,1)\\) with\n\\(c_i < c_{i+1} \\forall i = 1, . . ., R \u2013 2\\), and \\(\\delta ~ Uniform(0,100)\\).\n2: start training:\n3: \\(p ~ Dirichlet(a)\\)\n4: \\(z \\leftarrow argmax(p)\\) \\(\\triangleright\\) Compare to actual data\n5: \\(\\mu_{M,i,j} \\leftarrow b_M + (a_M - b_M) \\cdot \\mathbb{1}_{Z_i}(j)\\)\n6: \\(\\mu_{H,i,j} \\leftarrow b_H + (a_H - b_H) \\cdot \\mathbb{1}_{Z_i}(j)\\)\n7: \\(\\pi_M ~ N (\\mu_M, \\sigma_M)\\) \\(\\triangleright\\) Compare to actual data\n8: \\(\\pi_H ~ N \\left(\\mu_H + \\rho \\sigma_H \\frac{(\\pi_M - \\mu_M)}{\\sigma_M}, \\sqrt{1 - \\rho^2} \\sigma_H\\right)\\)\n9: \\(y ~ Categorical (Softmax (\\pi_H/\\tau))\\) \\(\\triangleright\\) Compare to actual data\n10: \\(r ~ OrderedLogistic(\\pi_H, c, \\delta)\\) \\(\\triangleright\\) Compare to actual data\n11: end training\n12: start testing:\n13: \\(p ~ Dirichlet(a)\\)\n14: \\(z \\leftarrow argmax(p)\\)\n15: \\(\\mu_{M,i,j} \\leftarrow b_M + (a_M - b_M) \\cdot \\mathbb{1}_{Z_i}(j)\\)\n16: \\(\\mu_{H,i,j} \\leftarrow b_H + (a_H - b_H) \\cdot \\mathbb{1}_{Z_i}(j)\\)\n17: \\(\\pi_M ~ N (\\mu_M, \\sigma_M)\\) \\(\\triangleright\\) Compare to actual data\n18: \\(\\pi_H ~ N \\left(\\mu_H + \\rho \\sigma_H \\frac{(\\pi_M - \\mu_M)}{\\sigma_M}, \\sqrt{1 - \\rho^2} \\sigma_H\\right)\\)\n19: \\(y ~ Categorical (Softmax (\\pi_H/\\tau))\\) \\(\\triangleright\\) Compare to actual data\n20: \\(r ~ OrderedLogistic(\\pi_H, c, \\delta)\\) \\(\\triangleright\\) Compare to actual data\n21: end testing\nAlgorithm 1 illustrates the inference procedure of the Bayesian combination model, which comprises two stages: estimation of parameters (training) and prediction of label prob- abilities (testing). For the estimation of parameters, the given data is: true labels z, probability confidence scores \\(\\pi_M\\), and human classification y and confidence ratings r. We assumed that all human participants shared the same set of parameters (\\(a_H, b_H, \\sigma_H,"}, {"title": "c, \u03b4, and \u03c4). Human confidence ratings on the slider bar were mapped to range between 1 and 100. A wide range is computationally expensive, thus, we aggregate it into three levels: \"0: low confidence\", \"1: moderate confidence\", and \"2: high confidence\". Then, the aggregated confidence rating used for analysis, \\(r \\in \\{0, 1, 2\\}^N\\), reads", "content": "\\begin{equation*}\nr=\n\\begin{cases}\n0 & \\text{if} \\quad \\text{self-reported confidence < 33}, \\\\\n1 & \\text{if} \\quad 33 < \\text{self-reported confidence < 66}, \\\\\n2 & \\text{if} \\quad 66 < \\text{self-reported confidence}\n\\end{cases}\n\\end{equation*}\nThe utilized cutpoints (i.e., 33 and 66) produced a good agreement between confidence and accuracy. Among the evaluations of human participants, \u201clow\u201d had 63.2% average accuracy (n = 174), \u201cmoderate\" had 66.5% (n = 185), and \u201chigh\u201d had 81.9% (n = 144). To estimate the posterior over the underlying parameters, a No-U-Turn Sampler (NUTS) for Markov chain Monte Carlo (MCMC) [19] was used with nw = 500 warmup steps, nc = 3 chains, and ns = 25 samples. For the prediction of label probabilities the given data is the same as before but z, that is now latent. The prior distributions were set according to [12]. The correlation between team members was \\(\\rho ~ Uniform(-1,1)\\). For the LLM classifier, \\(a ~ N(0,10), b ~ N(0, 10)\\), and \\(\\sigma ~ Uniform(0,15)\\). For human classifications, a was the same but b and o were adjusted to b = 0 and \\(\\sigma = 1\\) for the purpose of identifiability [12]. In addition, we used \\(\\delta ~ Uniform(0, 100)\\) for the scaling parameter and uniform priors on the cutpoints, \\(c ~ Uniform(0,1)\\), with the constraint that the cutpoints are ordered (i.\u0435. \\(c_i < c_{i+1} \\forall i = 1, ..., R \u2013 2\\)). Finally, we set \\(\\tau = 0.05\\) for best convergence results [12]."}]}