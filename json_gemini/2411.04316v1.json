{"title": "A Multilingual Sentiment Lexicon for Low-Resource Language Translation using Large Languages Models and Explainable AI", "authors": ["Melusi Malinga", "Isaac Lupanda", "Mike Wa Nkongolo", "Phil van Deventer"], "abstract": "South Africa's and the Democratic Republic of Congo's (DRC) diverse linguistic landscape, with languages such as Zulu, Sepedi, Afrikaans, French, and Ciluba, poses significant challenges for Al-driven translation and sentiment analysis due to a lack of accurate, labeled data. This study addresses these challenges by proposing a lexicon originally designed for French and Tshiluba (Ciluba) to include translations in English, Afrikaans, Sepedi, and Zulu. Language-specific sentiment scores are integrated, enhancing cultural relevance in sentiment classification. A comprehensive testing corpus supports translation and sentiment analysis tasks, with various machine learning models, such as Random Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive Bayes, trained to predict sentiment across these languages. The Random Forest model demonstrated robust performance, effectively capturing sentiment polarity while managing language- specific nuances. Additionally, BERT, a Large Language Model (LLM), is applied to predict context-based sentiment with high accuracy, achieving 99% accuracy and 98% precision, outperforming other models. BERT's predictions were further explained using Explainable AI (XAI), improving transparency and fostering trust in sentiment classification. The findings reveal that the proposed lexicon and machine learning models significantly enhance translation and sentiment analysis for languages spoken in South Africa and the DRC. This study provides a foundation for future Al models supporting underrepresented languages, with practical implications for education, governance, and business in multilingual contexts.", "sections": [{"title": "1. INTRODUCTION", "content": "Multilingualism poses a significant challenge for machine learning and sentiment analysis in translation, especially in linguistically diverse regions like South Africa, with 11 official languages, and the Democratic Republic of Congo (DRC), with 4. Most lexicons and language translating machine learning models cater to global languages, leaving languages like Zulu, Sepedi, Afrikaans, and Ciluba underrepresented. This gap results in disadvantages for existing Al systems in generating accurate translation and sentiment analysis in the local languages. Furthermore, countries like South Africa and the DRC with many languages face challenges in developing comprehensive tools capable of transitioning between local languages while ensuring sentiment accuracy is maintained. Given the current popularity and need for Al systems supporting local languages, particularly in education, government applications, and business in South Africa and the DRC is crucial. Sentiment analysis and machine learning models are crucial in developing inclusive Al systems (Mazibuko et al., 2021, 66-68). The current gap is also due to the limitations of machine learning models and sentiment analysis which can seamlessly handle Low-Resource Languages (LRL) like Zulu, Afrikaans, Ciluba, and Sepedi. Most known lexicons cater to languages like English and French but have insufficient data for LRLs like Zulu, Sepedi, Ciluba, and Afrikaans. In addition, the lack of LRL translation APIs poses challenges in the development of sentiment analysis and machine learning models that can provide accurate data when using LRLs. Similarly, there is a huge gap in the integration of the lexicons with machine learning models to provide improved sentiment accuracy and quality language translation for LRLs. Most trained machine learning models use well know High Resource Languages (HRL) and this leaves a gap in the Al system performance when working with African languages (Mazibuko et. al., 2021, 80-83)."}, {"title": "1.1 Background", "content": "In the new decade of globalisation and digitization, linguistic diversity is both a challenge and an opportunity for artificial intelligence (AI) systems, particularly in multilingual sentiment analysis and in machine translation. These tasks are integral to different applications, including social media monitoring, customer feedback analysis, and automated translation services, which increasingly demands accuracy across a multitude of languages. In South Africa, the complexity of language diversity is particularly pronounced with the eleven official languages - Afrikaans, English, Zulu, Xhosa, Sepedi, Setswana, Sesotho, Xitsonga, SiSwati, Tshivenda and isiNdebele, spoken by a population with lots of rich cultural backgrounds and incredibly unique linguistic challenges (Alshabi et al., 2021). In the DRC, the linguistic landscape is also highly complex and diverse. The country recognises four official languages: French, Lingala, Kikongo, and Swahili (Fraiture, 2024). In addition to these, there are over 200 languages spoken across the country, such as Ciluba, Kinyarwanda, and other regional languages (Fraiture, 2024). The rich cultural diversity in the DRC contributes to significant linguistic challenges, particularly in terms of translation, communication, and integration of local languages into official and technological systems. Al applications within South Africa and the DRC address this challenge to ensure inclusivity, accuracy, and cultural relevance across different sectors such as government, education and business, where language accessibility is essential to effectively serve many diverse communities.\nCurrent sentiment analysis and machine translation systems predominantly cater to well-resourced languages, such as English, French and Mandarin, which have had extensive lexicons, labelled datasets, and computational resources, and literature supporting their integration into machine learning models (Chun et al., 2023; Brodowicz, 2024). Furthermore, South African and the DRC national languages are generally not as included as English and French throughout the computational linguistic sector. This places limits upon the capacity of Al to accurately capture the various differences of sentiment as well as various meanings across each of the languages (Chun et al., 2023). Furthermore, these languages generally contain culturally specific terms such as: idioms, contextual cues and unique words that are enormously different when compared to the global languages such as English and French. This complicates the Al system's ability to maintain appropriate translation values as well as sentiment accuracy when comparing and transitioning between different languages. This discrepancy not only affects individual Al tasks such as sentiment analysis but also creates restrictions on comprehensive multilingual Al models from offering decent linguistic support in diverse contexts (Brodowicz, 2024). Our approach of studying the topic of multilingual Al capabilities to further include different African languages is a complex and difficult task which requires a unique approach. Firstly, lexicons must be enriched with different sentiment labels as well as various polarity scores that correspond to contextual details that are unique to each African language. This allows for sentiment models to be further interpreted and to"}, {"title": "1.2 Problem Statement", "content": "South Africa' and the DRCs Linguistic Diversity presents a challenge for Al translation and sentiment analysis systems. There is a need for accurate, labelled data for LRLs spoken in these countries such as Zulu, Ciluba, Sepedi, and Afrikaans. The lack of labelled data limits the application of Al systems to these languages and in turn could hinder their long-term preservation. This study aims to begin to fill this gap by creating labelled training data for African languages while performing sentiment analysis on the created data."}, {"title": "1.3 Objectives", "content": "In order to achieve the study's aims the following objectives will be addressed:\n1. Create a lexicon that includes English, French, Afrikaans, Ciluba, Sepedi, and Zulu translations while ensuring accuracy of the assigned sentiment and translations.\n2. Expand the lexicon by adding language specific sentiment scores to translated words to account for cultural context in sentiment.\n3. Create a corpus testing dataset for translations and sentiment analysis.\n4. Train machine learning models to predict sentiment and to translate between languages in the lexicon.\n5. Implement existing LLM architecture to predict context-based sentiment of individual words, while using XAI to explain predictions.\n6. Evaluate the study's findings while making recommendations for future work."}, {"title": "2. LITERATURE REVIEW", "content": "There is an increase in the usage of Al in natural language processing (NLP) for processes such as sentiment analysis, however, in countries such as South Africa and the DRC, there is still a shortage of NLP resources. Hence, there is an opportunity to develop Al tools that integrate sentiment analysis and translation to help preserve LRLs in Africa."}, {"title": "2.1 Language Preservation and Low-Resource Languages in NLP", "content": "It is of utmost importance that we preserve LRLs in diverse, culturally rich, and multilingual countries such as South Africa and the DRC. Developing NLP technologies will help preserve low-resource languages by offering economic development, preventing language extinction, and encouraging the growth of these languages beyond the African borders. It is therefore important that we preserve these African languages using artificial intelligence tools and advancing technological applications in these areas (Magueresse et al.,202). Issues that arise in in preserving low-resource languages include the lack of availability of multilingual datasets which creates a demand for the creation of multilingual lexicons and sentiment datasets, this will help in preserving low-resource languages and increase their use in Al-driven systems (Gamb\u00e4ck et al.,2017). In addition, being able to develop these lexical resources specifically for languages like Sepedi, Ciluba, Zulu, and Afrikaans can contribute to their digital presence. Joshi et al. (2020) argues that creating NLP tools and lexicons for low- resource languages can help preserve these languages while improving Al's ability to be able to process multilingual data; this will help ensure that sentiment analysis reflects the cultural and linguistic diversity of African countries."}, {"title": "2.2 Sentiment Analysis in NLP", "content": "Sentiment analysis is the process of collecting and gathering text about people's views and opinions on certain matters to later analyse how the users or people feel about a certain topic, experience or product (Wankhade et al., 2022; Nkongolo Wa Nkongolo, 2023). Sentiment analysis over the years has moved from utilising lexicon-based methods to more advanced approaches including machine learning, deep learning, and transfer learning methods (Nkongolo Wa Nkongolo, 2023; Birjali et al., 2021). The applications of sentiment analysis include analysing customer reviews to gain insights on how consumers feel about a certain product or service, the monitoring of people's comments and other interactions on social media to gain perspectives on how different users feel about a current trend or topic and to conduct market research (Birjali et al., 2021). Over the course of many years there have been improvements in the method of which sentiment analysis has been performed. However, there are still a few problems such as the interpretation of different meanings of words in different types of contexts such as the various use of homonyms and homophones (Nkongolo Wa Nkongolo, 2023). Furthermore, Al systems have further struggled to detect sarcasm and negation. This has made sentiment analysis incredibly difficult to classify as well as analyse various sentiments as positive, negative or neutral (Markovml, 2024; Nkongolo Wa Nkongolo, 2023). We propose a unique challenge for sentiment"}, {"title": "2.3 Machine Learning in NLP", "content": "Machine Learning in NLP consists of algorithms and models that can be used to understand human text in processes such as sentiment analysis. The models and algorithms used are trained to identify the trends and patterns found in text to understand the sentiment behind the texts. In NLP, machine learning methods include supervised learning, unsupervised learning, semi-supervised learning, and deep learning methods. Supervised learning includes linear classifiers and probabilistic classifiers, models such as Random Forest that make use of a linear classifier approach, while Naive Bayes makes use of a probabilistic classifier (Saravanakumar, 2023). A study by Rahat et al. (2019) used a review dataset to compare the performance of Naive Bayes and SVM by evaluating each model's ability to perform sentiment analysis on the reviews of an airline regarding the customer's experience. The reviews contained either positive or negative experiences from the customer on which sentiment analysis was carried out. The SVM classifier outperformed the Naive Bayes algorithm in correctly identifying the sentiment in the reviews. Another study seeking to compare the performance of SVM and Naive Bayes in identifying hate speech or non-hate speech showed high performance of the SVM classifier (Asogwa et al. 2022). It seems as though the SVM classifier performs well in sentiment analysis where the dataset is binary in nature and it is able to correctly classify sentiments as either positive or negative. However, it might not yield the same result when the nature of the dataset is more complex. To compare the performance of Random Forest to SVM, a study by Khan et al. (2024) found that SVM still outperformed Random Forest using a dataset to perform sentiment analysis. Although SVM seems to be performing really well in differentiating between positive and negative texts according to accuracy metrics, the best choice of a model to perform sentiment analysis will vary according to the nature of the dataset and its complexity."}, {"title": "2.4 Explainable Artificial Intelligence (XAI)", "content": "Explainable Al in NLP generally focuses on the development of methodologies to ensure that NLP models are more interpretable and transparent, thus, helping users to understand how the models make decisions or generate relevant outputs. According to Khun et al (2021) explainable natural language processor (XNLP) is an interactive browser-based system embodying a living survey of recent state of the art research in the field of XAI within the domain of NLP. Furthermore, these systems are incredibly important since the NLP models, especially deep learning complex ones, can function as black boxes so to speak, making it difficult to understand how they arrive at specific predictions or classifications. XAI can help us understand these so- called black-boxes. An example of this is that when Al system rejects a loan application, the applicant should be entitled to understand why that decision has been made to ensure that the compliance with laws and regulations are met. This helps the user to understand what needs to be achieved in terms of being compliant with regulatory regulations (Saeed & Omlin, 2023)."}, {"title": "2.5 BERT in Contextual Language Modelling", "content": "BERT can be defined as a transformer-based language model that utilises a bidirectional approach to linguistic modeling, allowing it to be trained to understand text in both directions\u2014forward and backward. This bidirectional capability distinguishes BERT from traditional unidirectional language models (Cesar et al., 2023). BERT also utilises a masked language modelling (MLM) approach that is able to randomly mask out certain words in a sentence that allows for a model to be trained and to predict the masked words which makes it unique from traditional language models (Cesar et al., 2023; Rokon, 2023). Masked Language Modeling (MLM) enables BERT to learn context in a non-linear manner, making it highly robust and reliable in understanding the intricate relationships between words and phrases. Additionally, BERT employs Next Sentence Prediction (NSP), which allows the model to determine logical sentence flow, helping it capture relationships at the sentence level. This capability is essential for BERT to handle complex language tasks effectively (Rokon, 2023)"}, {"title": "3. METHODOLOGY", "content": null}, {"title": "3.1 Lexicon Expansion", "content": "The Figure 1 outlines a process for expanding and refining a multilingual lexicon with sentiment scores for various languages, followed by data visualisation to analyse sentiment trends across languages."}, {"title": "3.1.1 Original Lexicon Cleaning", "content": "The initial lexicon contained just over 3000 French and Ciluba words with an assigned sentiment score, the part of speech (POS), and the words categorical sentiment (neutral, negative, and positive). With none of the authors being able to speak either of these languages it was difficult to identify any potential spelling errors or missing special characters. Therefore, the lexicon was fed to ChatGPT 0-1 preview, which has advanced reasoning capabilities and is able to deal with large amounts of text, to identify any errors in the French words before these errors are carried downstream in the translation process. The identified errors were corrected manually on the lexicon in Microsoft Excel. Exact duplicates, the same word with the same sentiment and part of speech, were removed from the data frame and any trailing or leading blank spaces were removed. This allowed for a clean lexicon to be expanded to South African and the DRC languages."}, {"title": "3.1.2 Lexicon Expansion", "content": "The Lexicon was expanded in both columns and rows. Firstly, Google Translates' API was used to translate the French words into English, which all authors can speak and could therefore identify errors. Thereafter 250 commonly used English words that were not present in the lexicon were added to create a more comprehensive set of words. The Google Translate API was then again used to translate the English words, including those added, into three South African languages namely, Afrikaans, Sepedi, and Zulu, extending the lexicon to not only have more languages but also more words. The authors who could speak the respective translated languages then manually went through the lexicon to identify and fix errors in translations from the google translate API, resulting in an accurately expanded Lexicon."}, {"title": "3.1.3 Language Specific Sentiment Addition", "content": "It was noticed through the process that there was only a single sentiment score across the Lexicon for all of the languages, which raised the question whether this would be an accurate representation of sentiment for each of the languages. In an attempt to capture sentiment for words that could differ across languages due to cultural significance, the expanded lexicon was given to ChatGPT 0-1 to generate sentiment scores for each word per language. The new version of the LLM has the ability to process these languages and it was therefore hypothesised that it would be able to add sentiment scores to the words. To evaluate the difference in sentiment across languages a new lexicon was created which contained sentiment scores per language, to serve as a comparison for the expanded lexicon with a single sentiment score across all languages."}, {"title": "3.1.4 Exploratory Data analysis (EDA)", "content": "EDA was performed on the lexicon with the language specific sentiments in order to determine if there was any effect on sentiment based on the language (Figure 2). The correlation between the original sentiment values and the language specific sentiment values were calculated and visualised. To gain an understanding into the distribution of sentiment in the expanded lexicon. And where there are potential gaps in the data, the distribution of sentiment scores and the parts of speech were visualised as well. With a cleaned expanded lexicon and an understanding of the data sentiment analysis, machine learning could be performed."}, {"title": "3.2 Sentiment Analysis with Machine Learning", "content": "After the cleaned data was formed from the EDA the research team proceeded to train a Random Forest, SVM, Gaussian Naive Bayes and a Decision tree classification model. We firstly started off by utilising the polarity values from -9 to 9 for each word row across our cleaned lexicon where each row had scores assigned to the Positive, Negative or Neutral sentiment of each word. Therefore, for each word there was a polarity value as well as an attached sentiment label. This allows each of our chosen models to be enriched with a standardised score as well as a standardised label. This allows for each model to generate predictions from a normalised dataset for different languages. This was generated to allow for words of similar sentiment scores to predict other words from another language with similar sentiment and scores. For example, trees from English being similar to boom in Afrikaans with a similar sentiment score as well as a similar sentiment label. Therefore, in our cleaned dataset each word and its relevant part of speech was able to be enriched with this standardised data to allow for the Random Forest, Support Vector, Gaussian Naive Bayes, and Decision tree models to have an accurate basis for predicting and translating words throughout the lexicon. This allowed us to split our datasets in terms of an 80% training dataset and a 20% validation dataset. We also created a confusion matrix to identify how well the model classified true values from predicted values. Furthermore, each of the classes were based on the word's part of speech and was used for accurate depiction within the confusion matrix for each of the models. After this, various accuracy metrics were also calculated such as overall accuracy, precision, recall and F1-scores for each model created to appropriately inspect which algorithm performed the best. After we identified the best model, we further deployed it in the translations system."}, {"title": "3.3 Translations and Sentiment Analysis", "content": "In this section we look at translations performed with sentences created using the expanded lexicon. Once the sentences have been created, the translation is performed from the source language into the user selected target language. Once the translation is performed a sentiment analysis is conducted, to better understand the sentiment of each sentence. Each word in the lexicon has a given sentiment score that is used in the analysis. Two main methods of sentiment analysis are performed, namely averaging the sentiment scores and a more advanced stepwise logic. There are sometimes multiple repeat words in the lexicon with the same translations, only differing in sentiment score. This is due to words being used in different contexts having different sentiments (Yang and Chao, 2018). The simple sentiment analysis calculates sentence sentiment by adding all the sentiment scores of the first matched words/phrases sentiment scores and averaging them, regardless of the remaining words' sentiments. To account for the other sentiment values of words in different contexts, a more advanced method of analysing the sentiment was employed. The more advanced technique, V2, determines if there is one, two or more exact words with different sentiment scores (Elmurngi and Gherbi, 2017). If there is a single match, then the single score is used; if there are two scores, the more extreme of the values are used; if there are three or more matched words with differing sentiment scores, the majority polarity group is averaged for a generalised score in the dominant polarity (positive or negative). The Vader labelling logic tool is used using its built-in lexicon that considers intensity of words (Barik and Misra, 2024). However, the lack of punctuation reduces the effectiveness of the Vader tools."}, {"title": "3.4 Aspect-based Sentiment Analysis", "content": null}, {"title": "3.4.1 Testing Dataset Creation", "content": "Words in the lexicon that exhibited both positive and negative sentiments were identified as context- dependent, capable of conveying different sentiments based on their usage. These words were selected as target words, and sentences were constructed using these targets alongside other lexicon words. The sentences were crafted to reflect either a negative, neutral, or positive sentiment, depending on the context in which the target words were used. A total of 1,000 sentences were generated, with each target word labeled according to its sentiment classification in the lexicon."}, {"title": "3.4.2. Testing Set Preparation", "content": "To allow models to focus on the specific target word within the sentence, each target word for a sentence was marked with a token [Target] and closed with [/Target] tokens. For example, the sentence \u2018Earth is the third planet from the sun' if earth is the target word it would be marked with the tokens [Target] Earth [/Target]. Tokenization was then applied to the test sentences using BERT's tokenizer (BertTokenizerfast), which separated sentences into tokens, breaking up the sentences into words with the words around the target word defining the sentiment that the target word is assigned (Vayadande et al., 2024). Each of the words within each sentence were assigned a sentiment score through the BERT tokenizer. The testing set was split into 70:20:10 for training, validation and testing sets respectively."}, {"title": "3.4.3 Model Implementation", "content": "The BERT model, which is an LLM, was called using a HuggingFace API (Sindane and Marivate, 2024). This was chosen for this task due to its proven strong performance and wide use in NLP. BERT processes text in both directions looking at the words before and after the target word in the sentence to determine the sentiment that is assigned to the word (Sindane and Marivate, 2024; Vayadande et al., 2024). The model also leverages a pre-trained architecture, enabling it to apply the insights gained from its training data while being fine-tuned for sentiment prediction tasks."}, {"title": "3.4.4 Model Training", "content": "A trainer class from Hugging Face's Transformers library was employed to manage the training loop, and the BERT model was trained to predict the sentiment of a target word based on the context in which it appeared within a sentence (Sindane and Marivate, 2024). Upon initial training it was noticed that there was a lack of neutral testing sentences as a class imbalance. To address this, the class weights were adjusted to ensure that the neutral class was not being dominated by the majority negative and positive classes, assigning greater importance to neutral sentences."}, {"title": "3.4.5 Evaluation", "content": "To understand the performance of the BERT-based sentiment analysis model, evaluation metrics were calculated and visualised. The accuracy, precision, recall and F-1 Score were calculated and visualised in a table for each of the sentiment classes (Rainio, Teuho, and Kl\u00e9n, 2024). A confusion matrix was presented showing the actual classes of the validation set and the predicted classes. ROC graphs were plotted to visualise how the model performs in discriminating between sentiment classes at different discrimination thresholds, giving a line on the graph for each class vs the other classes (Rainio, Teuho, and Kl\u00e9n, 2024)."}, {"title": "3.4.6 \u03a7\u0391\u0399 \u0395valuation", "content": "To better understand the model's decision-making process in assigning sentiment to a word based on context, Integrated Gradients, a post-hoc XAI technique, is implemented using the Captum library (Sithakoul, Meftah, and Feutry, 2024). As a post-hoc method, it was applied after predictions were made to explain the reasoning behind those predictions. Integrated Gradients were used as it is a model agnostic, meaning there was no need to change the architecture of the BERT model for explanations to be made using the XAI technique. For each test sentence, the target word was marked using the same [Target] tokens as described in section 3.4.2, which placed focus on the target word for attribution calculations within the sentence. The input embeddings corresponding to the tokenized sentence were extracted from BERT's embedding layer and gradient computation allowed for the attributions of each token in the sentence with regard to its influence in sentiment prediction of the target word. The attributions were visualised using heat maps to show the attribution scores of each word in a sentence, tokens with higher attribution scores would indicate that they place a large role in determining the sentiment of the target word."}, {"title": "4. RESULTS", "content": null}, {"title": "4.1 Exploratory Data Analysis", "content": "Figure 4 reveals the distribution of Neutral, Positive and Negative sentiment values for a lexicon after being translated to English.\nPositive sentiment. The bar chart shows that there are more positive words on the lexicon. There are approximately 2500 positive words in the lexicon, and this makes the positive sentiment dominant class in this lexicon. This skew is because of the positivity being many compared to the other words.\nNeutral and negative sentiment. Words categorised under this sentiment are fewer compared to the positive sentiment. There are about 200-300 words under the neutral and negative category, and this indicates that there are few words to create a negative or neutral sentence. This imbalance means more positive emotional sentences can be created rather than negative or neutral sentences. More neutral and negative words were manually added to balance the distribution and the skewness of the bar chart."}, {"title": "4.2 Sentiment Analysis", "content": null}, {"title": "4.2.1 Source Language: Afrikaans", "content": "Figure 14 shows translated texts with positive sentiment performed by the V2 advanced sentiment analysis. In comparison to this, the Vader sentiment analysis shows neutral and negative sentiment for the above sentences. The Vader sentiment is a built in tool to perform sentiment analysis, but it is not correctly identifying sentiments which could be because of the languages used in the expanded lexicon, therefore, the V2"}, {"title": "4.2.2 Source Language: English", "content": "The sentences have positive sentiments according to the sentiment average values, the V2 advanced sentiment analysis is able to correctly identify the sentiment analysis for the given sentences, while the Vader sentiment struggles to correctly identify the sentiments in the given texts (Figure 19). There are a number of reasons why the Vader sentiment is not performing well, it is due to the nature of the sentences and the lack of punctuation. Vader sentiment struggles in correctly identifying sentiments behind these sentences (Figure 20)."}, {"title": "4.2.3 Source Language: Sepedi", "content": "The V2 advanced sentiment analysis was able to correctly identify the sentiments in the sentences according to the total score average, while the Vader sentiment struggled to correctly identify the sentiments and misclassified some as neutral that should have been positive (Figure 22). The V2 advanced sentiment analysis is able to correctly identify the sentiments as either positive or negative, while the Vader sentiment analysis struggles by identifying the sentiments as neutral (figures 23-24)."}, {"title": "4.2.4 Source Language: Zulu", "content": "The Vader sentiment once again classifies the sentiments as neutral, while the V2 advanced sentiment analysis correctly classified the sentiments in the sentences translated from the Zulu language to the target languages (figures 27-28). The V2 sentiment analysis correctly identifies the sentiments as either positive or negative but has a few sentences that do not match the total score average, although correctly identified as positive or negative (figures 29-30). The distribution of sentiment scores is almost identical in the V2 sentiment analysis and the total score average, while there is no distribution for the Vader compound as the sentences are classified as neutral. The Vader compound struggles to identify the sentiments from languages such as Zulu and Sepedi and classifies the sentiments as neutral (figures 30-31). The Vader sentiment classifies the sentiments as neutral, while the V2 advanced sentiment analysis correctly classified the sentiments in the sentences to the target languages to either positive or negative."}, {"title": "4.3 Machine Learning", "content": null}, {"title": "4.3.1 Random Forest", "content": "The Random Forest model managed to achieve an accuracy of 66% on our Lexicon (Figure 31). The confusion metrics shows that classes like 'Mot' and 'nombre' are predicted well on the model with a recall and precision value of 0.96 and 0.68 (Figure 31). This indicates that these classes are identified correctly on the model. The precision, however, varies significantly across classes, with many classes for example, \"adjectif,\" \"adverb,\" \"pronompersonnel\", having zero precision, indicating that the model did not successfully predict these classes. \"Mot\" and \"nombre\" have relatively higher precision scores, suggesting that when the model predicted these classes, it was often correct. The F1-score, which balances precision and recall, is low for most classes, with a weighted average of 0.56. The model struggles particularly with classes like \"verbe,\" which has an F1-score of only 0.12, indicating poor performance in both precision and recall for this class (Table 1). This could mean that the model might need more data or features to improve prediction for underperforming classes."}, {"title": "4.3.2 SVM", "content": "The SVM precision focuses on the measures of true positives against all true positives obtained from the model prediction. Classes adjectif, adverb, adverbe, article, and pronompersonnel have not been predicted correctly at all (Figure 36-37). This is more likely due to the lower number of the classes during the randomized sampling processes. By looking at the support, it is clear that while these classes were present in the sampled dataset, they were of the least weight (Figure 36-37). Classes such as 'mot', 'nombre' and 'verbe' were present in large proportions and were classified with the weak to moderate precision (Table 2). Nombre class had about 2.49% portion of the Lexicon and was classified with high precision of 0.79 indicating the effectiveness of the model to identify the class well. The model overall aggregates the predictions very well when focusing on all classes altogether with the AUC of 0.95 (Figure 38-39). This shows that with all the classes combined, the model has learnt the dataset well and is able to differentiate sentiments with high accuracy such that the positively predicted sentiments are indeed positive from the Lexicon. However, getting to individual classes, the class 'adverb' and 'adjectif perform poorly with the AUCs of 0.05 and 0.11 respectively (Figure 38-39). This indicates that while the model is able to distinguish the sentiments with all classes combined, it struggles to predict the sentiments when dealing with the classes \u2018adverb' and 'adjectif'. Classes \u2018verbe' and 'mot' are in large proportion of the dataset being second largest and largest portions of the Lexicon but still not easily classified with the AUCs just being 0.54 and 0.63 showing that the model still finds it difficult to accurately predict the sentiment."}, {"title": "4.3.3 Gaussian Na\u00efve Bayes", "content": "The model exhibits inconsistent performance across various linguistic categories due to disparities in precision and recall, especially evident in how it processes classes with different frequencies. For example, while \"nombre\" (number) achieves a high recall rate (0.94) and moderate precision (0.79), showing it is often recognised but sometimes confused with other classes, \"verbe\" (verb) shows significant weakness with an F1- score of just 0.17 (Table 3). This suggests the model performs unevenly, excelling in some areas but struggling to accurately classify less frequent classes. Such results indicate a tendency to favour high-frequency classes over sparse ones, compromising the model's reliability. The data imbalance further skews performance metrics, with a noticeable gap between macro and weighted averages. While weighted averages slightly elevate the overall scores due to frequent class representation, macro averages demonstrate the model's poor handling of underrepresented classes (Table 3). The limited support for classes like \"adjectif\" and \"adverb\" results in inadequate generalisation across categories. Improving the model's effectiveness may require addressing this imbalance, possibly by rebalancing data or exploring alternative methods better suited to sparse classes (Figure 42)."}, {"title": "4.3.4 Decision Tree", "content": "The confusion matrix shown in Figure 43 presents the model performance over the different classes within the decision tree. The model had poor performance over most classes. Adjectif, adverb, adverbe, article, and pronompersonnel had no correctly identified values. This is most likely due to the lack of samples available in these classes, with each class listed having poor support. The model performed well in the mot, nombre and verbe classes, identifying 387 mot values, 15 nombre values and 184 verbe values correctly. Overall, the model performed poorly, however it was able to identify mot, nombre and verbe values (Table 5)."}, {"title": "4.3.5 Comparison", "content": "The performance matrix highlights the strengths and weaknesses of Random Forest, SVM, Decision Tree, and Gaussian Naive Bayes models. Random Forest leads with the very best accuracy (0.66) and F1-score, demonstrating robustness against elegance imbalance because of its ensemble method, which improves generalisation and performance consistency (Table 6). SVM carefully follows, with an accuracy of 0.654. This model successfully handles complicated limitations, even though it could war with pretty imbalanced classes, as its binary margin optimisation limits adaptability to numerous elegance frequencies (Table 6). Gaussian Naive Bayes, with the lowest accuracy (0.599), is hindered by its simplicity and reliance on feature independence, which does not completely suit the lexicon. This model's moderate precision makes it a less optimal choice for tasks requiring nuanced classification- (Table 6). Trade-offs in precision and recall suggest SVM and Decision Tree slightly sacrifice precision for higher recall, helpful in cases where capturing true positives is prioritised. While Gaussian Naive Bayes might work when precision is critical, Random Forest generally offers the most balanced performance across metrics, making it the preferred choice for this task (Figure 47)."}]}