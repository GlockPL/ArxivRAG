{"title": "Stepwise Reasoning Error Disruption Attack of LLMs", "authors": ["Jingyu Peng", "Maolin Wang", "Xiangyu Zhao", "Kai Zhang", "Wanyu Wang", "Pengyue Jia", "Qidong Liu", "Ruocheng Guo", "Qi Liu"], "abstract": "Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise reasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have remarkably improved complex tasks by adopting various enhanced reasoning approaches (Besta et al., 2024; Yang et al., 2024; Yao et al., 2024). These approaches have boosted their performance and drawn attention to the trustworthiness of the reasoning processes, including faithfulness (Lanham et al., 2023; Turpin et al., 2024), fairness (Shaikh et al., 2023), and safety (Xu et al., 2024).\nPrevious work has exposed significant LLM vulnerabilities in simple tasks such as classification and generation (Wang et al., 2024a; Zhao et al., 2023; Xu et al.). However, their susceptibility to attacks during the complex reasoning processes where the stakes are often higher and the consequences are more severe in some critical area remains largely unexplored. Recent advances in reasoning methods have introduced effective mechanisms, enabling models to iteratively build upon prior steps: from basic chain-of-thought approaches (Wei et al., 2022; Kojima et al., 2022) to advanced reflection mechanisms (Madaan et al., 2024; Zhao et al., 2024) and multi-step reasoning (Wu et al., 2024; Wang et al., 2024b). This critical dependence on step-wise reasoning introduces a new type of vulnerability in LLMs, where manipulation of initial reasoning steps can propagate errors, causing cascading failures throughout the reasoning chain.\nExploiting such vulnerability in LLMs introduces two fundamental challenges: feasibility and imperceptibility. Technically, unlike traditional adversarial attack methods, which often leverage internal information of models such as gradients and logits, state-of-the-art LLMs are now primarily deployed as proprietary APIs (Achiam et al., 2023; Team et al., 2023). Therefore, only prompt-based attacks are feasible, where adversaries have to operate through input manipulation. While existing attempts to compromise LLM reasoning (Xu et al., 2024; Xiang et al., 2024) have demonstrated success in specific scenarios, they still face severe limitations in practice. A key challenge in attack design is crafting imperceptible attacks. While obvious manipulations like changing final answers or inserting irrelevant steps are easily detected, modifying the reasoning output while maintaining narrative coherence is rather challenging. Existing approaches struggle to achieve this balance between attack effectiveness and stealth in complex reasoning tasks.\nAmong the most relevant approaches, Xiang et al. (2024) employs misleading demonstrations to induce errors in LLMs. However, these methods are limited to in-context learning scenarios, requiring demonstrations as input, which limits their generalizability to the zero-shot settings. Furthermore, their strategy introduces an additional step that modifies the final answer, making it quite easy to identify. Another related approach, the preemptive answer 'attack' (Xu et al., 2024) alters the reasoning paradigm of the model by producing conclusions before deriving reasoning steps. Despite its novelty, this approach often generates easily identifiable outputs, reducing its imperceptibility and effectiveness in practice. These limitations are further evidenced by our experimental results in Section 3.2.\nTo address these two limitations, we propose the Stepwise rEasoning Error Disruption (SEED) attack. First, SEED addresses the feasibility challenge by leveraging LLMs' reliance on step-by-step reasoning. Instead of depending on demonstrations or backpropagated gradients, SEED strategically introduces subtle errors into the early reasoning steps. This approach achieves high success rates across a wide range of scenarios without the need for task-specific training or examples, proving its effectiveness within the constraints of proprietary API-based LLM deployments in both zero-shot and few-shot settings. Second, SEED overcomes the challenge of imperceptibility by maintaining the original prompt structure while subtly manipulating the reasoning process. The carefully introduced errors seamlessly integrate into the reasoning flow, naturally propagating through the reasoning chain to produce incorrect yet plausible-looking outcomes. This ensures that the disruptions remain covert, avoiding detection while preserving the model's perceived trustworthiness. This novel approach not only addresses the identified limitations but also introduces a fresh perspective on how reasoning vulnerabilities in LLMs can be exploited. Our contributions can be summarized as follows:\n\u2022 We define the task of disrupting the step-by-step reasoning process of LLMs and introduce SEED, a versatile and effective attack method that is both highly efficient in execution and challenging in detection.\n\u2022 We demonstrate the effectiveness and stealth of SEED across four representative LLMs on four datasets with different characteristics, which include diverse and challenging reasoning tasks presented in two different formats.\n\u2022 We naturally validate the vulnerability of LLMs"}, {"title": "2 Method", "content": "In this section, we first provide an explicit definition of attacks that target the step-by-step reasoning process of LLMs. Following that, we introduce our two implementations of the proposed SEED attack."}, {"title": "2.1 Problem Formulation", "content": "We first present a formal definition of a step-by-step reasoning task of LLMs as shown in Fig 1. For a given problem p, we define the query to the LLM, denoted as q, as follows:\n$q = [I_{solve} || D || p]$,\nwhere D = [d1, . . ., dk] and dk represents the k-th demonstration in few-shot setting. Each demonstration dk is structured as [pk, [r_1^k,...,r_t^k], ak], with $r_t^k$ being the t-th step in the reasoning process for the problem pk, and ak representing the final answer. If K = 0, the setting is reduced to a zero-shot scenario from few-shot.\nGiven q as input, the corresponding output o of the LLM is expressed as:\no = LLM(q) = [R || a],\nwhere Rt = [r1, . . ., rT] is the reasoning process. Attacks targeting the reasoning process of LLMs focus on altering o and its corresponding a by modifying q into q', which can be formulated as:\n$\\arg \\max_{q'} LLM_{a'} (q')$\ns.t. a'\u2260ai, diff(R, R') \u2264 \u03b4,\nwhere LLMa' represents the probability of the output answer being equal to a' and diff(\u00b7) represents the difference in terms of the narrative structure and semantic similarity."}, {"title": "2.2 Overview of Stepwise Reasoning Error Disruption Attack", "content": "Due to certain observations (as detailed in Section 3.2), modifications to $I_{solve}$ appear to be easily detectable, which could be partially explained by the sensitivity of the model to perturbations in problem-solving inputs. Similarly, changes to p seem to be detectable by prompting the LLM to repeat the problem, potentially leveraging its tendency toward consistent reasoning in generating responses. Meanwhile, modification on demonstrations is not supported under zero-shot setting. Therefore, SEED attack performs the attack by adding misleading steps Ratt = [$r^{att}_1$,...,$r^{att}_T$] and eliciting the LLM to output the subsequent reasoning steps R' = [$r'_1$, ...,$r'_T$] and the final answer a' based on R':\nd' = R'\\\\d' = LLM([I_{solve}||D||p||R_{att}]).\nTherefore, our work focuses on how to implement a M(\u00b7) where $R_{att}$ = M(p), that satisfy the variation of Eq. 1:\n$\\arg \\max_{R_{att}} LLM_{a'}(I_{solve}||D||p||R_{att})$\ns.t. a'\u2260a, diff(R, [$R_{att}$||R']) \u2264 \u03b4,\nIt's worth noting that, as we take some reasoning steps Ratt as input, we will display [$R_{att}$||R'] for the victim user to maintain the integrity of reasoning process. Therefore, the constraint diff(R, R') is converted to diff(R, [$R_{att}$||R']).\nBesides, we assume that the reasoning steps are continuous, with each step depending on the previous ones. Therefore, we can get:\ndiff(R, [$R_{att}$||R']) \u00d7 diff(R[: Tatt], Ratt),\nwith the constraint that $T_{att} + T' = T$. In practice, as the number of reasoning steps T varies, we introduce \u03c3 = $T_{att}$/T as a hyperparameter to control the $T_{att}$. To generate Ratt that both closely resembles R[: Tatt] and effectively misleads the LLM into providing an incorrect answer, we developed two LLM-based implementations. In the following two subsections, we propose two implementations of SEED attack: SEED-S (Step Modification) and SEED-P (Problem Modification). SEED-S directly modifies the final step of the reasoning process, while SEED-P generates a modified problem that leads to the desired incorrect answer."}, {"title": "2.3 SEED-S: SEED Attack by Step Modification", "content": "As shown in Figure 2, one intuitive and straightforward approach is to modify the final step of R[: Tatt] with the help of an assistant LLM:\n$r^{mod}$ = LLMassist($I_{mod}||p||R'[T_{att}])$\nRatt = R[: Tatt - 1]||$r^{mod}$),\nwhere $r^{mod}$ refers to the modified reasoning step and $I_{mod}$ refers to the instruction given to the LLM to modify the reasoning step in a way that leads to an incorrect answer. It is important to note that we instruct the LLM to only modify certain digits or words related to the final answer, rather than regenerate an entirely different step, ensuring that the similarity and length constraint is still met.\nHowever, this naive implementation has some drawbacks. Firstly, it has been observed that LLMs tend to pay more attention to the head and tail parts of the input. As a result, LLMs are more likely to notice inconsistencies between the last two steps. Secondly, the final result a' is easily noticeable, for example, if there is a significant discrepancy in magnitude compared to a, or if the final answer in a multiple-choice question does not appear among the available options. In such cases, the LLM is likely to recognize and correct the error."}, {"title": "2.4 SEED-P: SEED Attack by Problem Modification.", "content": "To solve the limitation of SEED-S due to LLMS' heightened attention to sequence endings and potential magnitude discrepancies in final answers, we propose a more meticulously designed implementation involving modifying the raw problem, as illustrated in Figure 3. The process begins by prompting the assistant LLM to solve the original problem and obtain the raw answer. With knowledge of this answer, the LLM is more likely to generate a modified problem that is both similar to the original and aligned with its corresponding answer. The whole process can be expressed as:\n$P_{mod}||R_{mod}||@_{mod}$ = LLMassist(p,a).\nBy providing more fluent reasoning steps Ratt = $R_{mod}$[: Tatt], the target LLM becomes more susceptible to being misled, ultimately producing incorrect reasoning steps and an incorrect answer.\nFor reasoning tasks with answer choices, the LLM is first instructed to select an answer choice, and then generate a problem based on the chosen answer. This ensures that the generated question aligns with the provided answer choices, maintaining the necessary consistency for successful attack.\nTo further enhance the attack's effectiveness, inspired by Xu et al. (2024), we prepend the corresponding incorrect answer to Ratt. Finally, the modified output of the target LLM is obtained by feeding the modified problem's incorrect answer and partial reasoning steps into it:\nq' = LLM(Di||I||Pi||a'||Ratt).\nSince we prepend a' to Ratt, the proportion of a' relative to the entire input q' is minimal, and its position is central. Thus, we assume that its impact on the length of R' and the similarity between model outputs R' and R[Tatt :] is negligible."}, {"title": "2.5 Properties Analysis", "content": "Both implementations demonstrate unique characteristics in achieving adversarial objectives. SEED-S, by directly modifying the final reasoning step, alters only a small portion of the input, resulting in weaker attack capabilities but making it less detectable. In contrast, SEED-P demonstrates stronger attack effectiveness by generating a fully modified problem with coherent reasoning chains, showcasing broader applicability across various problem types, including multiple-choice questions and open-ended reasoning tasks."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Dataset. Building on prior studies targeting reasoning processes in LLMs (Xu et al., 2024; Xiang et al., 2024), we evaluate our method using four datasets that encompass diverse and challenging reasoning tasks presented in two formats. Specifically, MATH (Hendrycks et al.) and GSM8K (Cobbe et al., 2021) focus on arithmetic reasoning with open-ended formats, while MATHQA (Amini et al., 2019) presents math problems in a multiple-choice format. CSQA (Talmor et al., 2019), on the other hand, features multiple-choice commonsense reasoning tasks. As for the budget constraints, we follow the approach of Xiang et al. (2024), randomly sampling 500 questions from each dataset for our experiments. Further details about datasets are provided in Appendix B.\nLLMs. We evaluate four cutting-edge LLMs, encompassing both open-source and proprietary models: Llama3-8B (Dubey et al., 2024), Qwen-2.5-7B (Hui et al., 2024), Mistral-v0.3-7B (Jiang et al., 2023), and GPT4-0 (Achiam et al., 2023). These models are chosen for their state-of-the-art performance and strong capabilities in solving complex reasoning tasks, providing a comprehensive benchmark to evaluate the effectiveness and versatility of our proposed attack methodology.\nSettings. To assess the generalizability of SEED attack, we test its performance in both zero-shot and few-shot settings, following the traditional prompt-based Chain-of-Thought (CoT) paradigm (Wei et al., 2022; Kojima et al., 2022). In the main experiments, we set \u03c3 to 0.6, and the impact of varying \u03c3 is explored in Section 3.3. Our experiments technical specifications and implementation details are available in Appendix D.\nMetrics. We assess the performance using three key metrics: accuracy (ACC), attack success rate (ASR) and detection rate. ACC measures the percentage of problems solved correctly by the model. ASR quantifies the proportion of originally correct answers that are rendered incorrect due to the attack, serving as a direct indicator of the attack's effectiveness in disrupting the model's reasoning capabilities. The Detection rate measures the proportion of solutions identified as originating from attacked input prompts. Further information on the metrics is available in the Appendix A.\nBaselines. To the best of our knowledge, the only comparable methods are UPA and MPA proposed by Xu et al. (2024). Both approaches instruct the LLM to provide the answer first, followed by the reasoning steps. MPA goes a step further by introducing an LLM-generated false answer to mislead the model into producing corresponding incorrect reasonings. Although BadChain (Xiang et al., 2024) makes a notable contribution by achieving an ASR close to 100% across all datasets, its applicability is restricted to the few-shot setting. Furthermore, as shown in Table 1, its detection ratio approaches 100%, as it simply modifies the final answer. Due to this limitation, BadChain is excluded from subsequent discussions."}, {"title": "3.2 Overall Performance", "content": "Evaluation on Covert Detection As outlined in Section 1, we hypothesize that modifying $I_{solve}$ will lead to LLM outputs that are more easily identifiable, thereby diminishing the covert nature of the attack. To test this hypothesis, we evaluated the detection rates of SEED-generated solutions against baseline methods using GPT4-0. Specifically, GPT4-0 was employed to classify whether the input prompt had been attacked by analyzing the corresponding output solutions. Table 1 presents detection rates on the MATH and GSM8K datasets, demonstrating that UPA and MPA fail to satisfy the covert requirements. In contrast, both SEED-S and SEED-P exhibit substantial improvements in detection rates, particularly with GPT4-0, where the average improvement exceeds 90%. As a result, GPT4-0 is substantially less likely to detect the outputs of attacked LLMs under SEED attack, validating that SEED achieves a higher level of stealth by maintaining a natural reasoning flow without apparent manipulations.\nPerformance Comparison in Baseline Settings To ensure a fair evaluation of effectiveness, we adapted the SEED attack to match the same settings as UPA and MPA, incorporating instructions for the LLM. As shown in Table 2, SEED attack achieves improved attack performance in most cases, compared to UPA and MPA. The performance gap on CSQA is especially evident. On GPT-40, SEED achieved an ASR more than 2x that of the baseline. This is due to the inclusion of additional reasoning steps in SEED attack that further enhance attack performance compared to UPA and MPA in most cases, indicating that SEED attack is compatible with UPA or MPA. This improvement is attributed to the SEED attack's ability to introduce additional reasoning steps. Furthermore, these results demonstrate that SEED attack is not only a standalone approach but also compatible with methods like UPA and MPA, potentially offering a hybrid strategy for further enhancing attack performance.\nEffectiveness Evaluation We evaluated the effectiveness of the two implementations on all datasets and all models. As shown in Table 3, although the results vary significantly across different datasets and models, all LLMs demonstrate vulnerability under the proposed SEED attack, substantially reducing the original ACC across all tasks under both Zero-shot and Few-shot settings.\nSEED-S occasionally fails, as demonstrated in CSQA and MATHQA, where the ASR is only 0.069 and 0.064 under few-shot setting, respectively. In contrast, SEED-P consistently outperforms SEED-S across all tasks. Specifically, in the CSQA and MATHQA multiple-choice datasets, SEED-S shows a low attack success rate, while SEED-P, with its more effective design, substantially boosts the ASR and reduces ACC.\nComparing the performance across different models, we find that Qwen and GPT-40 show greater robustness to the SEED attack than other models, particularly GPT-40 on MATH and GSM8K, and Qwen on CSQA and MATHQA, with ASR all under 0.4. Additionally, these models exhibit relatively higher original accuracy on the corresponding datasets. Based on these observations, we infer that a model's performance and robustness on a task are positively correlated.\nTo validate this inference, we further conduct SEED-P on questions that are answered correctly and incorrectly independently. The results shown in Table 4 reveal a significant ASR gap between the two groups. The largest gap is observed in Llama-3 under the few-shot setting, achieving an ASR of 0.417. This suggests that LLMs are more robust on questions that they are capable of answering correctly, which aligns with our intuition. We also conducted experiments to investigate the transferability of SEED-P; due to space constraints, the results are presented in Appendix E."}, {"title": "3.3 Parameter Analysis", "content": "In the SEED attack, \u03c3 is the hyperparameter that controls the proportion of injected reasoning steps, which intuitively influences the attack performance. To explore its impact, we evaluate the performance under different values of \u03c3. The results are shown in Figure 4. We observe that the performance varies across different models and tasks. In most cases, a range between 0.4 and 0.6 yields competitive performance. When \u03c3 is lower, fewer reasoning steps are injected. Consequently, the target LLM pays less attention to the injected steps and is more inclined to rely on its original reasoning process, resulting in a significant drop in ASR.\nOn the other hand, when \u03c3 is higher, the ASR also drops noticeably in some cases, particularly with GPT4-o and Qwen-2.5 on MATH. We hypothesize that the reason behind this behavior is that over-injecting reasoning steps sometimes makes the LLM more robust. When too many incorrect steps are injected, the model may become aware of the inconsistencies and try to correct them, leading to a more cautious approach. This increased awareness might cause the LLM to rely more on its internal reasoning, which could prevent the attack from being as effective. More results are provided in Appendix C due to space limitation."}, {"title": "3.4 Ablation Study", "content": "Two key components of SEED-S are the prepending of a wrong answer and the 2-stage reasoning step generation, which involves: 1) solving the raw problem to generate the correct solution, and 2) in multiple-choice tasks, selecting a different answer and generating a corresponding solution with reasoning steps that lead to the selected answer. For open-ended tasks, the solution is directly created with reasoning steps that lead to the incorrect answer, without the need to choose a different answer.\nFigure 5 illustrates the impact of these components, showing that both contribute to the overall performance. Notably, on CSQA, the 2-stage generation has a more significant effect, as in multiple-choice tasks, the LLM tends to notice when the final answer is not among the provided answer choices, prompting it to correct the error. The 2-stage reasoning generation ensures alignment between the given answer choice and the generated solution, specifically in multiple-choice tasks. More results are presented in Appendix C."}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 Prompt-based Reasoning of LLMs", "content": "Enhancing the reasoning capabilities of large language models (LLMs) remains a prominent research focus (Yang et al., 2024; Ning et al.). Among various approaches, the Chain of Thought (CoT) paradigm has proven particularly effective, as shown by foundational works like Wei et al. (2022) and Kojima et al. (2022). These studies demonstrated that integrating explicit reasoning steps-via reasoning exemplars or step-by-step instructions-significantly boosts LLM reasoning performance. Subsequent research has refined CoT with methods like the self-consistency strategy (Wang et al.), which employs majority voting across reasoning paths, and Least-to-Most (Zhou et al.), a two-stage approach that decomposes problems before solving sub-tasks. Moreover, techniques like Automatic and semi-automatic Prompting enhance CoT by optimizing prompts (Zhang et al.; KaShun et al.; Pitis et al., 2023). Extensions to more complex structures, such as trees (Yao et al., 2024) and graphs (Besta et al., 2024), further expand CoT's potential."}, {"title": "4.2 Prompt-based Attack on LLMs", "content": "A key area of research aimed at ensuring the safety and robustness of LLMs involves developing methods to attack these models, prompting the generation of undesirable content (Deng et al., 2023; Chu et al., 2024; Yu et al., 2024). One prominent category within this field focuses on \"jailbreak\" attacks, which bypass alignment mechanisms to elicit harmful or unsafe outputs (Yi et al., 2024; Mehrotra et al., 2023; Zheng et al., 2024). However, our work is not directly related to jailbreak attacks. Instead, we focus on adversarial attacks, which subtly manipulate outputs without noticeable input modifications (Xu et al., 2022; Kandpal et al.; Xu et al.). While earlier studies targeted traditional NLP tasks such as sentiment analysis and classification (Wang et al., 2024a; Zhao et al., 2023), recent efforts have increasingly focused on attacking LLM reasoning processes (Xiang et al., 2024; Xu et al., 2024). BadChain leverages backdoor vulnerabilities by embedding triggers within in-context learning demonstrations, but its applicability remains limited to specific contexts (Xiang et al., 2024). Moreover, a critical drawback of BadChain is its nearly 100% detection rate, rendering it unsuitable for practical deployment. Similarly, UPA and MPA methods proposed by Xu et al. (2024), which instruct LLMs to generate answers before reasoning, often yield outputs that are easily identifiable, compromising their covert nature."}, {"title": "5 Conclusion and Future Works", "content": "We propose Stepwise Reasoning Error Disruption attack (SEED), a novel method targeting LLMs' reasoning capabilities by injecting misleading steps with deliberate errors to disrupt their reasoning process. Through experiments on four datasets and LLMs, we demonstrate our method's effectiveness with two variations, achieving high success rates while remaining stealthy. Our attack reveals LLMs' vulnerability to adversarial reasoning steps, especially in multi-turn scenarios where early errors can cascade through the reasoning chain. Our findings highlight the need for robust defenses to protect LLMs' reasoning integrity in critical applications."}, {"title": "6 Limitation", "content": "We believe our primary limitation lies in the inability to extend experiments to the entire dataset due to budget constraints. While we consider SEED to be stable and effective across various tasks, resource limitations have restricted the breadth and depth of our evaluations. Comprehensive testing across diverse datasets and scenarios would provide stronger evidence of SEED's robustness and generalizability, which remains as our future work. Additionally, our attack method may inadvertently generate potentially harmful or offensive content in the output solutions for the modified questions. This risk arises due to the nature of adversarial attack, which alter the model's responses in unintended ways. Without rigorous safeguards, including targeted controls and thorough examination of outputs, the potential for generating inappropriate or harmful content cannot be fully mitigated. Future efforts should focus on integrating content moderation techniques and ethical safeguards to minimize these risks while maintaining the effectiveness of the attack method."}]}