{"title": "Collaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation", "authors": ["Zheqi Lv", "Tianyu Zhan", "Wenjie Wang", "Xinyu Lin", "Shengyu Zhang", "Wenqiao Zhang", "Jiwei Li", "Kun Kuang", "Fei Wu"], "abstract": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is\na promising research direction that has demonstrated exceptional\nperformance in this field. However, its inability to capture real-time\nuser preferences greatly limits the practical application of LLM4Rec\nbecause (i) LLMs are costly to train and infer frequently, and (ii)\nLLMs struggle to access real-time data (its large number of pa-\nrameters poses an obstacle to deployment on devices). Fortunately,\nsmall recommendation models (SRMs) can effectively supplement\nthese shortcomings of LLM4Rec diagrams by consuming minimal\nresources for frequent training and inference, and by conveniently\naccessing real-time data on devices.\nIn light of this, we designed the Device-Cloud LLM-SRM\nCollaborative Recommendation Framework (LSC4Rec) under a\ndevice-cloud collaboration setting. LSC4Rec aims to integrate the\nadvantages of both LLMs and SRMs, as well as the benefits of cloud\nand edge computing, achieving a complementary synergy. We en-\nhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent re-\nquest. During training, LLM generates candidate lists to enhance\nthe ranking ability of SRM in collaborative scenarios and enables\nSRM to update adaptively to capture real-time user interests. Dur-\ning inference, LLM and SRM are deployed on the cloud and on\nthe device, respectively. LLM generates candidate lists and initial\nranking results based on user behavior, and SRM get reranking\nresults based on the candidate list, with final results integrating\nboth LLM's and SRM's scores. The device determines whether a\nnew candidate list is needed by comparing the consistency of the\nLLM's and SRM's sorted lists. Our comprehensive and extensive\nexperimental analysis validates the effectiveness of each strategy\nin LSC4Rec.", "sections": [{"title": "1 Introduction", "content": "Traditional sequential recommendation models, such as DIN [64],\nGRU4Rec [12], SASRec [15], and so on, have achieved great success\nin both academia and industry. Here, we refer to these models that\ndo not use large language models (LLMs) as their backbone and\ndo not integrate user behavior into textual prompts as input for\nrecommendation as small recommendation models (SRMs). Cur-\nrently, LLMs have been explored for recommendation scenarios\n(LLM4Rec) [11, 45, 61], demonstrating stronger multi-task gener-\nalization capabilities and superior single-task performance, which\nposition LLM4Rec as a highly promising research direction.\nHowever, LLMs face challenges in capturing real-time user pref-\nerences for two reasons: (i) Expensive to train and inference\nfrequently. LLMs are built with an immense number of parame-\nters [1, 11, 42], posing difficulties for the efficiency of LLM's training"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLM for Recommendation", "content": "LLMs are increasingly prominent in natural language process-\ning [16, 18, 21, 25, 26, 40, 43, 46, 50, 65, 66], driving research in\ntheir application for recommendation systems [6, 11, 17, 22, 23, 31,\n52, 57, 62]. The advent of generative models like GPT transformed\nLLM-based recommendation into generative processes, treating rec-\nommendations as natural language tasks [44]. Early methods relied\non prompting [10, 39] or in-context learning [24] but often fell short\nof task-specific models. Recent advancements involve fine-tuning\nLLMs for better alignment with recommendation tasks. P5 [11] in-\ntroduced a unified framework for fine-tuning FLAN-T5 [36] across\nvarious recommendation tasks. InstructRec [53] and TALLRec [1]\nfurther adapted FLAN-T5 and LLaMA models, respectively, for\nrecommendation tasks using instruction tuning. GenRec's [13] ap-\nproach involves direct instruction tuning on the LLAMA model for\nthe generative recommendation, showcasing the evolving strategies\nin integrating LLMs with recommendation systems. P5 [11] and\nPOD [17] are two large recommendation models based on the T5,\naiming to unify the multiple recommendation tasks. They trans-\nform the recommendation task into textual format and use the text\nas input to the LLM to obtain recommendation results. LLMs pos-\nsess stronger generalization capabilities compared to SRMs but face"}, {"title": "2.2 Sequential Recommendation", "content": "Sequential recommendation algorithms model users' behavior se-\nquences and predict their next actions. They have been widely\napplied in scenarios such as e-commerce, short video recommenda-\ntions, education, and healthcare. Classic recommendation models\nsuch as Caser [41], GRU4Rec [12], DIN [64], and SASRec [15] are\nstill commonly used today. In recent years, many studies have fo-\ncused on different aspects of sequential recommendation, such as\npersonalization [32-34, 38], multimodality [14, 51, 54, 55], privacy\nprotection [20], user feedback [9, 56], cross-domain [27, 63], cold\nstart [4, 28], long-tail [59], session-based [37], debias [3], structure\nlearning [7], disentanglement [5], LLM-based [2, 58], generative-\nbased [23, 60], fairness [47], etc. Although Sequential Recommen-\ndation Models (SRMs) require fewer hardware resources and are\neasier to deploy on devices, they have limitations in feature extrac-\ntion and generalization capabilities. These studies, however, have\nnot addressed the deployment of SRMs on devices or the associated\nlimitations."}, {"title": "2.3 Device-Cloud Recommendation", "content": "In more realistic recommendation scenarios, the collaboration of\ndevice intelligence and cloud intelligence is often required [8, 19, 29,\n30, 33-35, 48, 49]. For instance, DCCL [49] and MetaController [48]\nmade early attempts at device-cloud collaborative recommendation\nmodels. DUET [34] and IntellectReq [33] explored ways to help\ndevice models overcome the generalization limitations imposed by\ntheir model size. Their specific approach involves having the device\nmodel handle personalization, while the cloud model generates\nthe necessary parameters for the device model based on data from\nthe device. However, when LLMs are used as cloud models, they\nintroduce greater cloud computing loads and pose challenges in\ndirectly collaborating with SRMs. These studies do not address\ndevice-cloud collaborative recommendation strategies when LLMs\nserve as the cloud model."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Problem Formulation and Notation", "content": ""}, {"title": "3.1.1 Data", "content": "We use $X = {u, v, s}$ to represent a piece of data, and\n$y = {y}$ to represent the corresponding label. Specifically, u, v, s\nrepresent user ID, item ID and user's click sequence respectively.\nHistorical data collected for a period of time from devices is rep-\nresented by $D_H = {X_H^{(i)}, y_H^{(i)}} _{i=1}^{NH}$. The data augmented based\non DH is denoted as $D_{H-Aug}$. In the inference stage, users will\ngenerate real-time behaviors on the devices. These real-time be-\nhavioral data are represented\nas $D_R = {X_R^{(i)}, y_R^{(i)}} _{i=1}^{NR}$. The lagged\nbehavioral data are represented as $D_{\\bar{R}}$. $D_{\\bar{R}}$ is also data generated\nby users during the inference stage, and it represents the last set of\ndata uploaded to the cloud by the user. Since the device-to-cloud\nsynchronization is not real-time, there is a time difference between\nDR and D\u0154. More precisely, $D_{\\bar{R}}$ lags behind DR by $\\Delta t$ time units."}, {"title": "3.1.2 Model", "content": "LLM and SRM are denoted as Mr and Ms, respec-\ntively. The output of LLM, that is, the candidate list recalled by LLM\nbased on DH or DR, is denoted as $S = {i}_{i=1}^{N_{candidate}}$. The order of\nitems in this list is considered to be the initial ranking, denoted\nas Yinit. The output of SRM re-ranking based on real-time data is\ndenoted as \u0176rerank."}, {"title": "3.1.3 Formula", "content": "To facilitate the description of the training process,\nhere we use $\\phi(D)$ to represent that the model is trained using data\nD, $\\phi_{re}(D)$ to represent that the model is re-trained using data D,\nAug(D) to represent data augmentation based on data D, $\\otimes$\nto represent collaborative algorithms, \u220f to represent the function that\ndetermines the inconsistency or similarity of the sorting of two\nlists. We formulate our proposed LSC4Rec as follows,\nCollaborative Training:\n$$M_L = \\phi(D_H), M_S = \\phi(D_H);$$\n$$M_S = \\phi_{re}({D_H, D_{H-Aug} = Aug(M_L(D_H))}); \\tag{1}$$\n$$M_S = \\phi(D_R).$$\nCollaborative Inference:\n$$S, \\hat{Y}_{init} = M_L(D_R);$$\n$$\\hat{Y}_{rerank} = M_S(S; D_R); \\tag{2}$$\n$$\\Upsilon = \\hat{Y}_{init} \\otimes \\hat{Y}_{rerank}.$$Collaborative-Decision Request:\n$$Request = 1 (\\Pi(\\hat{Y}_{init}, \\hat{Y}_{rerank}) \\geq Threshold).  \\tag{3}$$\nNote that the function symbols here are rough and are only used\nto formulate the task description. The details of the formula will be\nelaborated in subsequent subsections of this section."}, {"title": "3.2 LSC4Rec", "content": "This section describes our proposed LSC4Rec. As shown in Fig-\nure 2, our LSC4Rec framework includes (a) Collaborative Training,\n(b) Collaborative Inference, and (c) Collaborative-Decision Request.\nCollaborative Training aims to enable SRM to provide benefits to the\nLSC4Rec framework. Collaborative Inference aims to complement\nthe strengths of LLM and SRM during the inference stage, result-\ning in more accurate recommendation outcomes. Collaborative-\nDecision Request targets to determine whether there has been a"}, {"title": "3.2.1 Retropy of Existing Sequential Recommendation Diagrams", "content": "Here we review the paradigm of sequential recommendation.\nIn the training stage, if we disregard the process of data textual-\nization required for LLM4Rec, the method by which LLM4Rec and\nSRM4Rec obtain loss is the same, and can be formalized as follows,\n$$L = \\sum_{u,v,s, y \\in D_H} l(y, \\hat{y} = M(u, v, s)).   \\tag{4}$$\nIn the above equation, l represents loss function, M generically\nrepresents both LLM and SRM. M() denotes the output of the\nmodel when is given as input. During gradient backpropagation,\nLLM4Rec and SRM4Rec differ slightly. LLM4Rec has the option to\nupdate the instruction, prompt, and model, while SRM4Rec typically\nfocuses on updating the model itself.\nIn the inference stage, if we disregard the process of data textu-\nalization required for LLM4Rec, the inference processes of both are\nessentially consistent."}, {"title": "3.2.2 Collaborative training", "content": "Collaborative Training consists of In-\ndependent training, Cooperative training, and Adaptive re-training.\nIndependent training. When LLM and SRM are trained sepa-\nrately, the calculation of loss is the same as equation 4. The back-\npropagation for SRM can be formalized as arg mines L. In the above\nformula, \u03b8s represents the parameter of Ms. Based on prior knowl-\nedge, we fix the instruction and prompt of the LLM and then fine-\ntune the model's parameters. The backpropagation of LLM can be\nformalized as arg mine, L. In the above equation, \u03b8\u2081 represents the\nparameter of ML.\nCooperative training. After independent training, we input XH\ninto the LLM to obtain inference results. For each sample x, we\ninfer Ncandidate items, forming the candidate list S. Furthermore,\nwe freely combine items that are difficult for the LLM to distinguish,\nthat is, items with similar tokens, to obtain an enhanced candidate\nlist SAug. Subsequently, we also input each sample x into the SRM,\nallowing the SRM to re-rank within the candidate list SAug. The\nresulting prediction is denoted as \u0177. So the loss function can be\nformulated as,\n$$S = M_L(x), S_{Aug} = Augment(S), \\tag{5}$$"}, {"title": null, "content": "$$L = \\sum_{u,v,s,y \\in D_H} l(y, \\hat{y} = M_S(u, v, s)|S_{Aug}).   \\tag{6}$$\nWe fix the model parameters of LLM so that the gradient generated\nby loss only backpropagates to SRM, so the optimization function\nis arg mines L.\nAdaptive re-training. After LLM and SRM are trained, LLM is\ndeployed in the cloud and SRM on the device. However, as men-\ntioned in the Introduction section, due to network bandwidth limi-\ntations and delays in data processing on the device side, it is difficult\nfor LLM to access real-time data. Even if it can access such data,\nits large number of parameters makes it challenging to train and\ndeploy the model quickly. Therefore, SRM needs to be trained based\non real-time data on the device, with the optimization goal being\narg mines L, where the loss function is as follows,\n$$L = \\sum_{u,v,s,y \\in D_R} l(y, \\hat{y} = M_S(u, v, s)).   \\tag{7}$$"}, {"title": "3.2.3 Collaborative inference", "content": "Collaborative Inference consists of\nCooperative Inference and Result Fusion.\nCooperative Inference. After training is completed, LLM and\nSRM are deployed separately in the cloud and on the device. Here,\nwe introduce how to merge the initial ranking results \u0176init output by\nLLM with the re-ranking results \u0176rerank of SRM during the inference\nstage.\n$$S, \\hat{Y}_{init} = M_L(D_R);$$\n$$\\hat{Y}_{rerank} = M_S(S; D_R). \\tag{8}$$\nResult Fusion. LLM and SRM have different ranking principles;\nLLM is generative while SRM is non-generative, leading to different\nranking scales for each. Therefore, an important step is normaliza-\ntion, which places \u0176init and \u0176rerank on the same scale to better merge\nthe ranking results. We use Pinit to represent the interaction proba-\nbility between users and products, and \u0176init is also obtained from\nPinit. Similarly, Prerank is also used to obtain \u0176rerank. The process of\nnormalization can be formalized as follows,\n$$p^{norm}_{init} = \\frac{P_{init} - min(P_{init})}{max(P_{init}) - min(P_{init})}$$\n$$p^{norm}_{rerank} = \\frac{P_{rerank} - min(P_{rerank})}{max(P_{rerank}) - min(P_{rerank})} \\tag{9}$$"}, {"title": null, "content": "After normalization and filtering, we adjust the hyperparameter\n\u03b1 and \u03b2 to merge Yinit and \u0176rerank,\n$$\\hat{P} = \\alpha \\cdot p^{norm}_{init} + (1-\\alpha)  p^{norm}_{rerank};$$\n$$\\Upsilon = sort(S, \\hat{P}).  \\tag{10}$$"}, {"title": "3.2.4 Collaborative-Decision Request", "content": "After collaborative training\nand collaborative inference, the LSC4Rec has formed a complete\nframework. This framework can compensate for the performance\ndecline due to LLM's difficulty in obtaining real-time data as much\nas possible through SRM, making LSC4Rec more effective than\nusing LLM4Rec or SRM4Rec independently in this system. However,\na noteworthy point is that when LSC4Rec should invoke LLM to\nprovide new inference results based on real-time data has not been\naddressed. To fill this gap, we designed the Collaborative-Decision\nRequest feature for LSC4Rec."}, {"title": null, "content": "We utilize the positional difference in rankings of the same ele-\nment by \u0177init and \u0177rerank to calculate a inconsistency score c. As-\nsuming $pos_p (q)$ represents the position of element q in list p, the\ninconsistency score c can be formalized as follows,\n$$c = \\frac{1}{n} \\sum_{u \\in \\hat{Y}_{init} \\cap \\hat{Y}_{rerank}} |pos_{\\hat{Y}_{init}} (u) - pos_{\\hat{Y}_{rerank}} (u)|,   \\tag{11}$$\nwhere $\\hat{Y}_{init} and \\hat{Y}_{rerank}$ In the above formula, $\\hat{Y}_{init}\\cap\\hat{Y}_{rerank} = \\hat{Y}_{init} =$\n$\\hat{Y}_{rerank}$. This is because the elements in lists \u0177init and \u0177rerank are\nexactly the same, only their order differs.\n$$Request = 1 (c \\geq Threshold). \\tag{12}$$\nIn the equation above, 1(\u00b7) is the indicator function. To get the\nthreshold, we need to collect user data for a period of time, then\nget the inconsistency c corresponding to these data on the cloud\nand sort them, and then set the threshold according to the load of\nthe cloud server. That is, the threshold can be obtained during the\ntraining on the training set. For example, if the load of the cloud\nserver needs to be reduced by 90%, that is, when the load is only 10%\nof the previous value, only the minimum 10% position value needs\nto be sent to each device as the threshold. During the inference\nprocess, each device determines whether it needs to upload real-\ntime data to the LLM for inference, based on formulas 11 and 12.\nThis is done to ensure the most optimal LLM invocation under\nany device-cloud communication resources and LLM invocation\nresources."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.1.1 Datasets", "content": "We evaluated our method on three widely used\ndatasets, which are all collected from an e-commerce platform\nAmazon\u00b9 and cover various product categories such as books, elec-\ntronics, home goods, and more, they usually include user reviews,\nratings, product descriptions, prices, and other information. We\nalso evaluated our method on Yelp\u00b2 which contains details about\nbusinesses, user reviews, ratings, business categories, addresses,\nand more. In accordance with convention, all user-item pairs in\nthe datasets were considered as positive samples. For the training\nand test sets, user-item pairs that did not exist in the datasets were\nsampled as negative samples [11, 17]. During testing, we rank on a\nsample set consisting of all items. In these datasets, we use a user's\nlast behavior for testing, the second to last behavior for validation,\nand the remaining behaviors for training."}, {"title": "4.1.2 Baselines", "content": "We used the following SRMs and LLMs for the\nexperiments.\n\u2022 SRMs. DIN [64], GRU4Rec [12], and SASRec [15] are three\nhighly prevalent SRMs in both academic research and the\nindustry. They each incorporate different techniques, such as\nGRU (Gated Recurrent Unit), Attention, and Self-Attention,\nto enhance the recommendation process. One point to note is\nthat although models like BERT4Rec and E4SRec achieve bet-\nter performance, their significantly larger number of model\nparameters results in far greater resource consumption com-\npared to the aforementioned lightweight SRMs. Therefore, we"}, {"title": "4.1.3 Evaluation metrics", "content": "In the experiments, we use the widely\nadopted NDCG, HitRate(HR) and Precision as the metrics to evalu-\nate model performance. The details of the metrics are in Appendix."}, {"title": "4.2 Experimental Results", "content": "We use bold to denote the best value and underline to denote\nthe second-best value (if applicable). Typically, we conduct such\ncomparisons for each dataset. However, in some cases, such as\nin Figure 5, we compare values for each Request Frequency. In\nthe experimental analysis section, when results across all datasets\nwould take up significant space and consistent conclusions are\ndrawn for each dataset, we only present the results for one dataset\n(e.g., Figure 5) to save space for more critical content."}, {"title": "4.2.1 Overall performance", "content": "Table 1 presents the performance com-\nparison between LSC4Rec, SRMs, and LLMs under both real-time\n(RT) and near-real-time (NRT) data settings. Compared to real-time"}, {"title": "4.2.2 The impact of choosing different SRMs on performance", "content": "To ob-\nserve the impact of small model selection on LSC4Rec, we conducted\nexperiments using various small models. As shown in Table 2, the\nexperimental results indicate that: LSC4Rec achieves significant\nperformance improvements on POD, indicating that the LSC4Rec\nframework can effectively mitigate the performance degradation\nof various LLMs caused by their inability to access real-time data."}, {"title": "4.2.3 The impact of choosing different SRMs on performance", "content": "To\nobserve the impact of small model selection on LSC4Rec, we con-\nducted experiments based on various small models. As shown in\nTable 3, the experimental results indicate that:"}, {"title": "4.3 Ablation Study", "content": "For simplicity, we use \"w.\" to represent \"with\" and \"w/o.\" to repre-\nsent \"without\". In these experiments, the SRM on the device uses\nthe SASRec model."}, {"title": "4.3.1 The impact of collaborative training", "content": "As shown in Table 4,\nwe analyze the effectiveness of each component in collaborative\ntraining via ablation study. There are four rows for each dataset,\ncorresponding to four types of ablations, we provide the following\nexplanations for them:\n\u2022 w/o. Collaborative Training and w/o. Adaptive Re-training\n(Row.1) indicates that both the LLMs and SRMs are trained\nindependently based on historical user data.\n\u2022 w. Collaborative Training and w/o. Adaptive Re-training\n(Row.2) indicates that after both the LLMs and SRMs are\ntrained independently based on historical user data, the SRM\nthen learns to re-rank the candidate list generated by the LLM.\n\u2022 w/o. Collaborative Training and w. Adaptive Re-training\n(Row.3) indicates that after both the LLMs and SRMs are\ntrained independently based on historical user data, the SRM\nis then re-trained on the device based on the user's real-time\ndata.\n\u2022 w. Collaborative Training and w. Adaptive Re-training (Row.4)\nindicates that after both the LLMs and SRMs are trained in-\ndependently based on historical user data, the SRM learns to\nre-rank the candidate list generated by the LLM. Then the\nSRM is re-trained on the device based on the user's real-time\ndata."}, {"title": null, "content": "The experimental results show that with cooperative training\nand adaptive re-training performs the best, followed by with adap-\ntive re-training and without cooperative training. The worst case\noccurs when neither is used, and the gap compared to the other\nthree cases is significant. This indicates that on-device SRM can-\nnot perform well within the LSC4Rec framework solely through\nindependent training. These results demonstrate the effectiveness\nof cooperative training and adaptive re-training."}, {"title": "4.3.2 The impact of collaborative Inference", "content": "To analyze the effec-\ntiveness of each component in our collaborative inference strategy,\nwe conducted ablation experiments on the inference strategy. As\nshown in Table 5, each dataset has four rows of experimental re-\nsults, corresponding to four types of ablations. For the four rows of\ndata for each dataset, we provide the following explanations:\n\u2022 w. LLM, w/o. SRM, w/o. Result Fusion (Row.1) indicates that only\nuse on-cloud LLM to recall the candidate list, the initial ranking\nresults can be regarded as the full-ranking results based on the\nless real-time data.\n\u2022 w/o. LLM, w. SRM, w/o. Result Fusion (Row.2) indicates that only\nuse on-device SRM to do ranking.\n\u2022 w. LLM, w. SRM, w/o. Result Fusion (Row.3) indicates first to\nlet on-cloud LLM recall a candidate list, then on-device SRM do\nrerank the candidate list based on the real-time\n\u2022 w. LLM, w. SRM, w. Result Fusion (Row.4) indicates first to let\non-cloud LLM recall a candidate list and initial ranking resuls,\nthen on-device SRM do rerank the candidate list based on the\nreal-time, further, reranking and initial ranking ard fused as the\nfinal ranking results."}, {"title": null, "content": "The experimental results show that the collaborative inference and\nresult fusion of LLMs and SRMs perform the best, followed by col-\nlaborative inference of LLMs and SRMs without result fusion. Using\neither the LLM or the SRM alone results in poorer performance.\nThe above results demonstrate the effectiveness of our designed\ncollaborative inference and result fusion."}, {"title": "4.3.3 The impact of collaborative-decision request", "content": "As shown in\nTable 6, we analyze the collaborative-decision request.\n\u2022 Random (Row.1) indicates that choosing some users to update\nthe candidate list and corresponded initial ranking results ran-\ndomly."}, {"title": null, "content": "\u2022 The choice of small models indeed has some influence on\nLSC4Rec, which can be attributed to differences in the abil-\nity of different SRMs to interpret user preferences. Over-\nall, SASRec provides the most significant improvement for\nLSC4Rec. We conducted analyses on the Beauty, Toys, and\nYelp datasets. Although SASRec does not outperform GRU4Rec\nand DIN across all metrics on all datasets, it generally pro-\nvides the most significant improvements to LSC4Rec. There-\nfore, to save space, we present only the results on the Beauty\ndataset.\n\u2022 Within the LSC4Rec framework, regardless of the SRM used,\nthe performance is superior to that of LLMs without access\nto real-time data."}, {"title": "4.4 In-Depth Analysis", "content": ""}, {"title": "4.4.1 The impact of length of candidate list", "content": "In Table 7, we ana-\nlyze the impact of the length of the LLM's recall list on perfor-\nmance. When the length of the candidate list is 20 and 50, the range\nof performance variation is not significant, which indicates that\nLSC4Rec is not very sensitive to this hyperparameter of the recall\nlist's length. Additionally, shorter candidate lists tend to perform\nbetter on stricter metrics (e.g., @5), while longer candidate lists\nshow advantages on more lenient metrics (e.g., @20). The advantage\nof a longer candidate list lies in its higher likelihood of including\nthe ground-truth item, but the downside is the greater challenge for\nthe SRM to re-rank longer candidate lists effectively. On a relatively\nbalanced metric (e.g., @10), the advantages and disadvantages of\nlonger candidate lists are approximately equal."}, {"title": "4.4.2 Analysis of resource consumption", "content": "To validate the resource\nconsumption of our method, we analyzed the resource usage of our\napproach in Table 8. Since the choice of either P5 or POD as the\nLLM does not affect the conclusion, we will use one of the models as\nan example. Here, we choose P5 to describe the conclusion. In terms\nof the number of parameters, if we uses P5 as LLM and SASRec as\nSRM, the total number of parameters is the sum of P5 and SASRec\n(on-device consumption is same as SASRec). In terms of training\nduration, since our method does not require real-time training of\nLLMs, the time consumption is reduced compared to training LLMs."}, {"title": "5 Conclusion", "content": "The proposed Device-Cloud Collaborative Framework for LLM and\nSRM (LSC4Rec) effectively combines the strengths of large language\nmodels and small recommendation models. By leveraging collab-\norative training, collaborative inference, and intelligent request\nstrategies, LSC4Rec addresses the limitations of LLMs in captur-\ning real-time user preferences while optimizing the utilization of\ncloud and edge computing resources. Extensive experimental re-\nsults validate the effectiveness of each strategy, highlighting the\nframework's potential for practical applications. Future work will\nfocus on further refining the collaboration mechanisms to enhance\nrecommendation performance and user satisfaction."}, {"title": "A Appendix", "content": "This is the Appendix for \"Collaboration of Large Language Models\nand Small Recommendation Models for Device-Cloud Recommen-\ndation\"."}, {"title": "A.1 Supplementary Methodology", "content": ""}, {"title": "A.1.1 Pseudo code of LSCRec", "content": "Algorithm 1 shows the pseudo code\nof LSCRec. (x) represents that x is a intermediate variable.\nAlgorithm 1: Pseudo Code of LSC4Rec\nStrategy 1: \u25b7 Collaborative Training\nTarget: Pretrained LLM ML, Pretrained SRM Ms \u2192 Updated\nSRM MS\nInput: Historical data DH\nOutput: Prediction \u00dd\nStrategy 2: \u25b7 Collaborative Inference\nTarget: Real-time data DR \u2192 Prediction \u00dd\nInput: Real-time data DR\nOutput: (Candidate Item Set S), (Initial Ranking Yinit gener-\nated by ML), (Re-Ranking \u0176rerank generated by Ms), Predic-\ntion \u0176\nStrategy 3: \u25b7 Collaborative-decision Request\nTarget: Initial Ranking Yinit, Re-Ranking \u0176rerank \u2192 Inconsis-\ntency c\nInput: Initial Ranking Yinit, Re-Ranking \u00ddrerank\nOutput: Inconsistency c\nOverview: \u25b7 Training Procedure\nInput: Historical data DH, Real-time data DR\nOutput: Prediction \u0177.\nInitialization: Randomly initialize the Mr and Ms\nrepeat\nif ML and Ms have not yet been well-trained then\nCalculate loss as follows (see Eq.4 for the details),\nL = \u2211u,v,s, y\u2208 DH l(y, \u0177 = M(u, v, s)).\n|end\nuntil Convergence;\nrepeat\nif Ms has not yet been well-trained then\nCalculate loss as follows (see Eq.5~7 for the details),\nL = \u2211u,v,s,y\u2208DH l(y, \u0177 = Ms(u, v, s)|SAug).\nend\nuntil Convergence;\nreturn ML, Ms.\nOverview: \u25b7 Inference Procedure\nInput: Real-time data DR\nOutput: Prediction \u0176, Inconsistency c\nS, Yinit predicted by LLM, \u00ddrerank predicted by SRM based on\nS.\nc calculated based on Yinit and \u0176rerank to help decide\nwhether to request new S, Yinit from LLM."}, {"title": "A.2 Supplementary Experiments", "content": ""}, {"title": "A.2.1 Datasets", "content": "The statistics of the datasets used in the experi-\nments is shown in Table 9."}, {"title": "A.2.2 Hyperparameters and Training Schedules", "content": "We summarize\nthe hyperparameters and training schedules of the LLM and SRM\nused in the experiments. Table 11 shows the settings of the SRM\ntraining. Table 12 shows the setting of the collaborative training,\ncollaborative inference, and collaborative-decision request."}]}