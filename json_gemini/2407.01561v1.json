{"title": "Synthetic Data in Radiological Imaging: Current State and Future Outlook", "authors": ["E. Sizikova", "A. Badal", "J. G. Delfino", "M. Lago", "B. Nelson", "N. Saharkhiz", "B. Sahiner", "G. Zamzmi", "A. Badano"], "abstract": "A key challenge for the development and deployment of artificial intelligence (AI) solutions in radiology is solving the associated data limitations. Obtaining sufficient and representative patient datasets with appropriate annotations may be burdensome due to high acquisition cost, safety limitations, patient privacy restrictions or low disease prevalence rates. In silico data offers a number of potential advantages to patient data, such as diminished patient harm, reduced cost, simplified data acquisition, scalability, improved quality assurance testing, and a mitigation approach to data imbalances. We summarize key research trends and practical uses for synthetically generated data for radiological applications of AI. Specifically, we discuss different types of techniques for generating synthetic examples, their main application areas, and related quality control assessment issues. We also discuss current approaches for evaluating synthetic imaging data. Overall, synthetic data holds great promise in addressing current data availability gaps, but additional work is needed before its full potential is realized.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) applications are becoming more and more prevalent in radiology and other types of medical imaging applications. AI techniques are used to aid clinical professionals in faster and more accurate detection of findings, optimize image quality while reducing dose, and improve other facets of analyzing complex and multidimensional radiological data. A key feature of AI is its reliance on large-scale datasets for learning meaningful features. The goal of this paper is to review and discuss the emerging use of synthetic data for AI applications in radiology.\nAI applications are often reliant on neural networks to perform predictions such as classification, segmentation or detection of objects of interest. Neural networks require large and diverse data collections to perform appropriate training and evaluation procedures. However, collecting sufficient examples from real patient sources comes with limitations due to patient privacy concerns, acquisition and annotation"}, {"title": "2 Terminology", "content": "A general definition of synthetic data in health care has been proposed as artificial data that mimic the properties and relationships seen in real patient data [17]. Synthetic examples are examples that have been partially or fully generated using computational techniques rather than acquired from a human subject by a physical system. The techniques used to generate synthetic examples (images and objects), described later in this article, vary in the fundamental origin of the information and are typically either knowledge-based or image-based approaches.\nThe terms in silico imaging and in silico trials are closely related concepts which encompass computational approaches for generating data and evaluating imaging technology using computational models. In silico medicine refers to the discipline that encompasses the use of patient-specific computer simulations involving all aspects of the prevention, diagnosis, prognostic assessment, and treatment of disease [18]. In turn, as defined in [19], in silico imaging trials are \u201ccomputational studies that seek to ascertain the performance of a medical device for the intended population, collecting this information entirely in the digital world via computer simulations"}, {"title": "3 Techniques for Synthetic Data Generation", "content": "Techniques for synthetic imaging data generation can be broadly grouped into three categories: statistical generative modeling, physics-based modeling, and hybrid, physics-informed modeling. A summary of popular models, applicable imaging modalities anatomies can be found in Table 2."}, {"title": "3.1 Statistical Generative Models", "content": "Generative models learn to synthesize outputs (images) that capture patterns and structures observed from existing patient images [13] by processing the distribution of pixel intensities. Most recent models are based on various neural network architectures developed in the ML community.\nGenerative Adversarial Networks (GANs). The key idea behind GANs, a popular type of generative model, involves two competing networks [20]: the first network (generator) aims to synthesize data that resembles the distribution of real data while the second network (discriminator) aims to differentiate the synthetic data from the real data. The GAN training process is adversarial and approximately solves a min-max optimization problem, with the objective of creating new data that matches the statistical distribution of training data. GANs have been used for generating synthetic training images [13], creating annotations [21], cross-domain [22] and pseudo-healthy synthesis [23]. Extensions of GANs include CycleGAN [24], which enables image domain transformation without the need for paired data, and AmbientGAN [25], which learns implicit generative models from lossy measurements of the distribution of interest. Both variants have found numerous applications in medical imaging [26].\nNormalizing Flow (NF). Normalizing flows are part of the generative model family that learn an invertible transformation, typically represented by a neural network, from a well-understood base distribution (e.g., multivariate Gaussian) to a complex data distribution [29]. This base distribution serves as the starting point (\u201cprior\u201d) from which data is generated. NF learns a series of transformations to morph this base distribution to the target data distribution, enabling the generation of synthetic data that mimics the original. As compared to GANs, NFs offer an opportunity for a more profound interaction with the inherent data properties. While NFs do not inherently model physical properties of the data, their ability to provide exact likelihood evaluation allows them to better capture these properties if they significantly influence the data distribution. Nevertheless, incorporating domain-specific knowledge or physical laws directly into the flow structure is still an active area of research [29]. In radiology, NFs have recently gained some attention in applications such as image reconstruction [30] and data augmentation [31].\nVariational Autoencoders (VAEs). VAEs leverage the principles of autoencoding and variational inference [32], and consist of two components: an encoder network and a decoder network. The encoder transforms the input data into a specific distribution in the latent space. The decoder samples points from this latent distribution and attempts to reconstruct the original data. Through this process, VAEs can learn a stochastic, continuous bidirectional mapping between the data and latent space. When only a limited number of training examples are available, combining variational inference with GANs may help avoid mode collapse, i.e., generation of uniform or blurry examples [33].\nDenoising Diffusion Probabilistic Models (DDPMs). DDPMs are a type of generative model that represents image formation as a diffusion process. This process starts with the actual data and gradually adds noise until a simple noise distribution is reached [35]. To generate new data, the procedure is reversed by taking a sample from the simple noise distribution and iteratively applying a learned denoising operation, until the original data distribution is recovered. Here, noise operations are typically parametrized by a deep neural network, allowing the model to learn complex transformations between the noise and"}, {"title": "3.2 Physical Modeling", "content": "Synthetic data generation using physical modeling typically includes two components [19]: a digital model representing a patient or patient populations, and a digital model of an acquisition device (imaging system)."}, {"title": "3.2.1 Digital Human Models", "content": "Digital human models for computational simulations have been developed extensively over the past decades for different applications, particularly radiation dosimetry [79]. Recent research has focused on the development of models with increased spatial resolution and anatomical realism. The level of detail and anatomic diversity in these models depend on the method of generation and the range of anatomy covered (whole body or specific regions). The majority of digital human models are derived from detailed segmentations of tomographic images of patients [80]. The voxelized organs resulting from the segmentation process can be converted to surface mesh models to allow modifications and repositioning. Each organ is then assigned appropriate material properties depending on the intended use of the model. An early example is the Virtual Population VIP3.0 [37], a collection of digital human models developed for electromagnetic (EM) exposure evaluations. Another popular digital human model, the Extended Cardiac Torso (XCAT) phantom [80], used a few reference surface phantoms and registered them to patient images to create large cohorts of digital models. The XCAT incorporates respiratory and cardiac motion, and has sufficient resolution to be used in imaging. Detailed digital human models have been used extensively in a range of applications [81], ranging from developing image processing and reconstruction methods to motion compensation. Anatomic models of specific parts of the body are also commonly used, particularly for breast imaging applications. For example, a procedurally-generated stochastic breast model including a skin layer, blood vessels, glandular ducts, fat and other components was created by Graff et al. [15], and used in the evaluation of full-field digital mammography and tomosynthesis in the Virtual Imaging Clinical Trial for Regulatory Evaluation (VICTRE) project [9]."}, {"title": "3.2.2 Digital Acquisition Device Models", "content": "Radiological images can be reliably simulated in silico because the physical processes underlying the generation, propagation and detection of radiation (from optical light to gamma rays) are well understood. Physics-based digital replicas of radiation sources and detectors, coupled with realistic transport of radiation through digital phantoms, are used to create synthetic images that reproduce the features of images acquired with physical devices. The required accuracy of the image generation process depends on the context of use (COU) of the images. Typically, more realistic images can be generated by implementing more sophisticated physics models, at the expense of increasing the computational complexity. As an example, x-ray projections of digital phantoms can be efficiently simulated using Siddon's ray-tracing algorithm, which models x-rays as straight lines from the source to the center of each pixel. However, if the pixel noise statistics or the contribution from scattered radiation are relevant to the context of use, more sophisticated Monte Carlo (MC) methods that track the interactions of individual x-rays might be necessary.\nNumerous software packages have been developed to simulate different imaging modalities. For example, DukeSim [46] and XCIST [47] simulate commercial computed tomography (CT) scanners using"}, {"title": "3.3 Hybrid, Physics-Informed Models", "content": "Although recent advances in parallel computing, including graphical processing units (GPU), have allowed for complex physics-based simulations, such simulations may still often be prohibitive due to the computational overhead. Hybrid, physics-informed models address this concern by accelerating select components of synthetic data generation with deep learning. Alternatively, physics-informed neural networks embed physical constraints to create more realistic outputs or reduce the amount of training samples needed to learn a task. For example, deepDRR [52] speeds up generation of fluoroscopy and digital radiology from computed tomography (CT) scans by performing ML for scatter estimation and material decomposition, while retaining an analytic approach for other pipeline components. There are several other examples. [54] proposed a deep scatter estimation (DSE) technique that is within 2% of traditional Monte Carlo simulations used for cone beam CT acquisition. In fact, a neural network (NN) can learn to sample from a given probability density function (PDF) with high sampling efficiency [55], making it useful for noise modeling in physical simulations [93]. Finally, when known operators are combined together with NNs to inform the latter about known prior information during the training and inference, a NN may require less training data, training iterations, or achieve better performance levels [56]."}, {"title": "3.4 Synthesizing Disease Models", "content": "The lack of well-curated and labeled data is particularly acute for diseased cases. Synthetic examples generated using generative modelling have been explored for creating various types of lesions [27, 65, 74]. Alternatively, lesions could also be synthetically in-painted (i.e., removed) to reduce impact on image processing tasks such as registration or segmentation [66]. In silico, knowledge-based models of disease have been developed for various organs [16, 38, 83]. An important consideration for lesion models is their growth pattern [39, 45], since lesion presentation may be affected by properties of the surrounding tissue."}, {"title": "3.5 Limitations of Data Generation Techniques", "content": "Statistical generative models are typically trained using images (e.g., collections of x-rays) and are able to rapidly generate examples from the learnt generative distributions. However, they may not learn appropriate physical constraints or causal links between attributes and physical findings, and thus often suffer from generating hallucinated findings or unrealistic anatomy. On the other hand, physics-based approaches are grounded in physiology naturally embedded in the digital human model, and are able to generate high-quality and fully-detailed outputs controlled by the input parametrization. These approaches, however, may require more time-consuming and computationally intensive simulations. In addition, physical modelling approaches are constrained by the variability of the parameter space of the digital human model and acquisition system, but the complexity of the model can be adjusted based on the task of interest. Hybrid, physics-informed models are typically designed to accelerate components of physics-based approaches using neural networks, which may result in loss of realism,"}, {"title": "4 Applications", "content": ""}, {"title": "4.1 Algorithm Development and Training", "content": "Synthetic examples have been widely used as a source of training data, either on their own or combined with real patient images. This approach has been well-explored across many types of radiological imaging [85]. For instance, [57] showed that augmenting limited patient x-rays with synthetic images reduced marker localization error. [78] demonstrated that the addition of synthetic mammograms generated using in silico imaging improved performance according to breast mass detection free-response receiver operating characteristic (FROC) as compared to results from patient data alone. Several studies have used GANs for data augmentation and improved the performance of their algorithms, as seen in liver lesion classification on CT images [58], brain segmentation on CT and MRI images [97]. Synthetic images can address class imbalance concerns, but only if the synthetic images deviate sufficiently from the existing patient data [85].\nImage Reconstruction and Cross-modality Synthesis. There has been a number of works that aim to predict one modality (e.g., CT) from another (e.g., x-ray) for improving image quality and decreasing number of artifacts [67], reducing radiation exposure [59], and improving prediction accuracy (e.g., lesion detection [60]). CT prediction from MRI has been particularly well-explored [86], as tissue electron density information from CT is needed for radiotherapy planning [87]."}, {"title": "4.2 Algorithm Testing", "content": "Synthetic data can be used for generating standardized testing examples that would be otherwise too difficult to acquire from patient images [41, 75]. When a synthetic dataset is used for testing, it is particularly important to ensure that this dataset is representative of the intended patient population in order for performance estimates to be accurate. Thus, compared to the scenario where synthetic data is used for training, the evaluation requirements for synthetic testing data are more stringent.\nSizikova et al. [10] introduced the idea of using synthetic images for comparative performance testing in medical imaging, where AI is evaluated on known trends with respect to physical properties (e.g., mass size). For this application, physics-based synthetic simulations are particularly useful since they can be used to easily re-generate examples with modifications to physical properties (e.g., size or radiation dose), while obtaining similar patient examples may not be practically possible. An emerging application of synthetic data is within in silico clinical trials, where results from computer simulations are used in development or regulatory evaluation of a medicinal product, device, or intervention [18, 83]. Here, synthetic data complements patient data for evaluation of novel treatment methodologies or medical devices. [9] has shown that an in silico clinical trial comparing digital mammography (DM) and digital breast tomosynthesis (DBT) imaging modalities replicated the results of an in situ (non synthetic) clinical study involving hundreds of enrolled women. As discussed in [98], in silico trials are not identical to their in situ counterparts, and could provide evidence not found in traditional clinical trials [99]."}, {"title": "4.3 Patient Privacy Preservation", "content": "Synthetic data can act as an anonymization tool to protect patient characteristics while sharing data. For instance, a recent study [88] has evaluated the quality of GAN-generated synthetic chest radiographs as an alternative to sharing patient chest radiographs and brain CT, and showed that NN performance matched closely when trained on either synthetic or real examples, but suffered when a larger number of classes (labels) was considered. However, the risk of generative models inadvertently memorizing specific data points, thereby compromising patient privacy, cannot be ignored [100]. We refer the reader to [101] for a discussion of synthetic healthcare data privacy and associated risk mitigation measures. Finally, a recent set of recommendations for utilizing and evaluating differential privacy (AI) published by the National Institute of Standards and Technology (NIST) can be found in [102]."}, {"title": "4.4 Addressing Bias and Other Limitations of Patient Datasets", "content": ""}, {"title": "4.4.1 Class Imbalance and Modality Availability", "content": "Many datasets are prone to data imbalance, i.e., an uneven data distribution across classes, due to, for instance, the secondary use of data [103]. A popular technique to address this issue in imaging studies is the use of resampling techniques that synthetically resize training datasets to obtain more balanced distributions [89]. Algorithmic fairness approaches [104] may be used to balance out uneven distributions in available patient datasets. [36, 105] demonstrated the benefits of synthetic radiologic data created using"}, {"title": "4.4.2 Enrichment of Underrepresented Populations", "content": "An attractive feature of synthetic data is that it can be used to generate examples from known and underrepresented populations, such as patients with rare diseases and protected populations. For instance, [62] reported the creation of synthetic training data by inserting rare abnormal tumors into MRIs and demonstrated that this process improved model performance on patient examples. in another work, [63] investigated TB classification using synthetically generated CT over patient X-ray alone, and discussed the potential applications of synthetic data in supplementing costly imaging procedures for resource-poor communities.\nProtected populations are also notoriously difficult to obtain data points from, and are a potential candidate for synthetic data use. For example, pediatric patients represent 20% of the US population, but make up only 5% of imaging studies [106]. Pediatric radiology datasets are particularly difficult to acquire due to a lack of domain specialist annotators, lower study numbers, added safety concerns and regulatory requirements, all of which contribute to a lack of AI applications for these patients [107]. [76] simulated pediatric-size phantoms to evaluate AI denoising algorithms in newborn to adolescent sizes. [42] demonstrated that synthetically generated pediatric liver CT images with in-painted lesions were indistinguishable to real counterparts when read by radiologists. [77] showed that CT images of synthetic and patient lung nodules in pediatric patients were perceptually indistinguishable. Finally, [72] explored applications of synthetic data to AI-based segmentation of brain tissues in fetal MRI.\nConsiderations. A key challenge in synthetic data use for underrepresented populations is that it is inherently hard to find samples to build a robust training dataset for the data generation model to ensure that it does not perpetuate existing biases [108]. While approaches such as class-specific few-shot learning [69, 109] may mitigate the issue, under such conditions, physical modeling, which typically requires fewer parameters, may be advantageous. In either case, attention must be given not to perpetuate existing biases present in the data or the knowledge model."}, {"title": "5 Data Assessment Metrics", "content": "Fidelity and Utility. Evaluating synthetic radiological data is important to ensure that the generated data can serve its intended purpose. Synthetic data is often assessed in terms of its fidelity, i.e., whether it captures statistical inter-relationships of patient datasets, or its utility, i.e., whether it achieves similar results (e.g., downstream task performance) as patient data. A high-fidelity dataset therefore should have high utility [17], however, high utility may not be necessary for applications such as an understanding of relative trends. Fidelity metrics (e.g., Frechet Inception Distance (FID)) may capture summary statistics, single or pairwise distributional patterns, more complex interrelationships between variables in the synthetic and/or patient data points and or consistency with clinical domain expertise [90]. As the number of data dimensions increase, measuring fidelity becomes increasingly complex due to the exponentially increasing number of interrelationships [110].\nTypes of Utility Metrics. As discussed in [17], utility metrics measure distance between patient and synthetic data and could be grouped into work-aware evaluations (metrics that compare real and synthetic data performance in tasks of interest), generic utility measures (metrics that compare general distance"}, {"title": "6 Challenges for the Use of Synthetic Data", "content": "As in other fields, applications of synthetic data in radiological imaging suffer from data complexity (multi-scale models spanning several scientific disciplines), disclosure limitations (no robust platform to develop and disseminate models), data privacy and data ownership concerns. Below, we discuss some existing challenges associated with practical use and evaluation of synthetic radiological data."}, {"title": "6.1 Scientific Challenges", "content": "To ensure the safety and effectiveness of new biomedical technologies developed or evaluated with synthetic data, additional research is needed to better understand the uncertainty and bias of synthetic data generation approaches. An open area of research is the development of metrics for characterizing individual or population representation in a synthetic dataset and for evaluating the reliability of algorithm performance (e.g., does the algorithm performance reported on a synthetic dataset match the performance on a real patient dataset). In addition, techniques to ensure unbiased outcomes from utilizing synthetic data need to be developed, in particular, to ensure that using synthetic data does"}, {"title": "6.2 Evaluation Challenges", "content": "The key challenge of using synthetic data in the context of medical device evaluation concerns validation requirements that should be sufficiently strict to support the data usage. Synthetic data may be used in multiple ways within the context of regulatory evaluations with different evidentiary requirements. Regardless of the specific application, evidence must exist to show that synthetic data within a regulatory submission can be sufficiently relied upon to support the claims made. Depending on how the synthetic data is used, this may include synthetic examples with patient data, cross-validation of synthetic data using different data generation techniques, or distribution gap analysis between patient and synthetic data. The greater the prominence of synthetic data in the submission, the higher the evidentiary requirements. As an example, a computer-aided diagnostic device may use a data augmentation strategy incorporating synthetic examples into its training data. Validation of such supplementary data may include demonstrating that the synthetic data distribution follows a similar distribution in specific features as with real data, and that its use improves performance of the studied algorithm in real data. Primary research supporting the safety and effectiveness of the device would continue to come from a robust test set of real patients.\nAs all models carry inherent assumptions, ensuring congruence between the use of the synthetic data in technology assessment and the original purpose of the data generation model is essential. The US Food and Drug Administration (FDA) applies the term \u201cContext of Use (COU)\u201d to describe the way the synthetic data/algorithm/model is to be used. For a radiological device reliant on an AI algorithm, the context of use is the specific role and scope of the device, a detailed description of what will be modeled and how the device outputs will be used to answer specific questions of interest\u00b9. The context of use, the influence of the model in the regulatory decision, and the consequences of decision on patients, inform the validation and performance criteria necessary for the synthetic data to be relied upon for regulatory purposes."}, {"title": "7 Summary", "content": "Synthetic data shows great promise in advancing radiological imaging, especially AI-based technologies. Developing or evaluating these technologies with synthetic data allows conserving resources and can help to ensure that device approvals consider the entire intended patient population. Looking forward, a recent study [117] introduced the concept of a metaverse of \u201cmedical technology and AI\u201d (MeTAI) that would augment regulatory evaluation of medical devices with virtual patient and scanner models, highlighting the interplay of synthetic data and healthcare applications. Badano et al. demonstrated this proof of concept for mammographic imaging with the VICTRE trial [9]. Continued development and"}]}