{"title": "Cognitive-Aligned Document Selection for Retrieval-augmented Generation", "authors": ["Bingyu Wan", "Fuxi Zhang", "Zhongpeng Qi", "Jiayi Ding", "Jijun Li", "Baoshi Fan", "Yijia Zhang", "Jun Zhang"], "abstract": "Large language models (LLMs) inherently display hallucinations since the precision of generated texts cannot be guaranteed purely by the parametric knowledge they include. Although retrieval-augmented generation (RAG) systems enhance the accuracy and reliability of generative models by incorporating external documents, these retrieved documents often fail to adequately support the model's responses in practical applications. To address this issue, we propose GGatrieval (Fine-Grained Grounded Alignment Retrieval for verifiable generation), which leverages an LLM to dynamically update queries and filter high-quality, reliable retrieval documents. Specifically, we parse the user query into its syntactic components and perform fine-grained grounded alignment with the retrieved documents. For query components that cannot be individually aligned, we propose a dynamic semantic compensation mechanism that iteratively refines and rewrites the query while continuously updating the retrieval results. This iterative process continues until the retrieved documents sufficiently support the query's response. Our approach introduces a novel criterion for filtering retrieved documents, closely emulating human strategies for acquiring targeted information. This ensures that the retrieved content effectively supports and verifies the generated outputs. On the ALCE benchmark, our method significantly surpasses a wide range of baselines, achieving state-of-the-art performance.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) combines retrieval mechanisms with large language models (LLMs) to address the limitations of purely generative models (Lewis et al., 2020). By retrieving external information relevant to user queries through retrieval mechanisms, RAG enhances generated outputs (Khandelwal et al., 2019; Min et al., 2020) and mitigates the hallucination problem in LLMs (Cheng et al., 2024), allowing outputs to incorporate up-to-date real-world information (Gupta et al., 2024), often without requiring additional training (Izacard et al., 2023).\nDespite these advancements, the performance of current RAG systems remains constrained by several factors, such as the quality of retrieved documents, prompt design, and the scale of the generative model. Among these, the quality of retrieved documents is pivotal in determining system performance (Li et al., 2025). High-quality retrieval is mainly dependent on the effectiveness of the retrieval mechanism. Previous research has proposed various optimization strategies to enhance retrieval mechanisms, including recursive retrieval for richer content (Li et al., 2024b; Yao et al., 2023), adjusting corpus chunk sizes for optimized outcomes (Sarthi et al.; Raina and Gales, 2024; Wang et al., 2024), fine-tuning retrievers (Yan et al., 2024; Zhang et al., 2024; Hei et al., 2024; Izacard et al., 2023; Cheng et al., 2024), re-ranking retrieved content for diversity and quality (Saad-Falcon et al., 2023; Li et al., 2024a; Sun et al., 2023), and rephrasing retrieved text to better leverage generative models (Wang et al., 2023; Arora et al., 2023). Beyond re-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Verifiable Generation", "content": "Verifiable generation refers to the process of generating text that can be independently verified, ensuring users can trace the source of each piece of knowledge. Existing paradigms of verifiable generation can be broadly categorized into two approaches: (1) Direct generation of responses with citations: These methods aim to enhance content verifiability by augmenting the generative capabilities of language models, allowing them to incorporate citation information directly during generation. For example, Weller et al. (2024) utilizes the large language model's intrinsic abilities to generate citations by prompting it with phrases like \"according to Wikipedia\". Lee et al. (2023) critically assesses the quality of generated text and provided feedback based on these evaluations, guiding the language model towards improvement. (2) Retrieval-based"}, {"title": "2.2 Optimizing retrieval mechanisms for RAG", "content": "In retrieval-augmented question-answering systems, the reliability of the retrieved documents is crucial for ensuring both the accuracy and trustworthiness of the generated responses. The technologies for optimizing retrieval mechanisms can be broadly classified into two categories: (1) Model-training Retrieval Augmentation: CRAG (Yan et al., 2024) and DR-RAG (Hei et al., 2024) enhance the quality of documents by training retrievers and classifiers to evaluate and filter the retrieved documents. Additionally, Mao et al. (2024) develops a query rewriting model that incorporates feedback from a re-ranker. Self-Retrieval (Tang et al., 2024) allows large models to conduct self-retrieval on pre-trained datasets by incorporating retrieval corpora into their training. (2) LLM-based Retrieval Augmentation: In query rewriting, Anand et al. (2023) introduces Context-Aware Query Rewriting, which leverages LLMs for query understanding in text ranking tasks, thereby improving the accuracy and relevance of the query. For document re-ranking, Neural PG-RANK (Gao et al.) optimizes decision quality metrics using the Plackett-Luce ranking strategy, incorporating LLMs into the training process. Similarly, RankGPT (Sun et al., 2023) uses ChatGPT-generated document ranking to reorder documents effectively. Furthermore, in terms of pipeline optimization, LLatrieval (Li et al., 2024b) combines query rewriting and re-ranking through an iterative updating mechanism, continuously refining the retrieval results and verifying that the documents retrieved are sufficient to support the generated answers until the validation criteria are met.\nIn contrast to recent related studies (Yan et al., 2024; Sun et al., 2023; Li et al., 2024b), our work focuses on the alignment criterion between re-"}, {"title": "3 Methods", "content": "Figure 2 shows the overview of GGatrieval at inference."}, {"title": "3.1 Document Toxonomy", "content": "The meaning of complex expressions is constructed from the meanings of their simpler components (Drozdov et al., 2022). Analyzing syntactic structure is critical in understanding sentence meaning (Lesmo and Lombardo, 1992). Therefore, the cognitive process of human document retrieval can be conceptualized as follows: (1) decomposing the query into its grammatical components; (2) identifying the corresponding segments within the retrieved documents that align semantically with the query's components; and (3) classifying the document as high-quality if all components of the query align with the retrieved text. Based on this cognitive process, we propose a selection criterion for evaluating retrieval documents: whether there are continuous grounded segments within the document that align with the query's grammatical components. This criterion categorizes retrieval documents into three types:\nFull Alignment: A continuous segment in the document aligns semantically with all query components.\nPartial Alignment: A continuous segment in the document aligns semantically with at least one component of the query but not all.\nNo Alignment: No continuous segment in the document semantically aligns with any query component.\nThe retrieval mechanism of GGatrieval prioritizes documents labeled with Full Alignment, while those labeled with Partial Alignment contribute to system robustness. Documents with No Alignment serve as exclusion criteria in the retrieval process."}, {"title": "3.2 Fine-grained Grounded Alignment", "content": "In GGatrieval, filtering reliable and verifiable documents requires the acquisition of alignment labels for the retrieved documents. To achieve this, we design the FGA strategy. Given the strong performance of LLMs in tasks like syntactic analysis and natural language understanding (Tian et al., 2024; Zhou et al., 2024; Li et al., 2023), we utilize LLMs to implement this strategy. The procedure is outlined as follows:\nQuery Syntactic Analysis: The system processes the user's query $Q$ and decomposes it into key syntactic components, including the subject, predicate, and object, among others, as described in Equation (1):\n$C = \\{C_s, C_v, C_o, C_c, C_{attr}, C_{adv}, C_{supp}, C_{app}\\} = f_{LLM}(Q)$\n(1)\nHere, $C$ represents the set of syntactic components extracted from the query $Q$, where $C_s, C_v, C_o, C_c, C_{attr}, C_{adv}, C_{supp}$ and $C_{app}$ represent the subject, predicate, object, predicative, attributive, adverbial, complement, and apposition, respectively.\nFine-grained Grounded Alignment: The system compares each syntactic component of the query with the candidate document $D$, producing a list of query components that can be semantically aligned with the document. This alignment is mathematically represented as:\n$M(Q, D) = \\{C_i | C_i \\in C, f_{LLM}(C_i, D) = 1\\}$\n(2)\nIn this expression, $f_{LLM}(C_i, D)$ denotes the alignment function computed by the LLM, which outputs 1 if alignment is found or 0 if no alignment is detected. The set $M(Q, D)$ contains the query components that align semantically with the document. The greater the number of aligned components, the higher the probability that the candidate document answers the user's query, making this list vital for filtering candidate documents.\nReflection and Query Optimization: As highlighted by Liu et al. (2024), the self-reflection abilities of LLMs can enhance their performance. Accordingly, we incorporate a reflection step where the LLM generates a reflective list of aligned components $M'(Q, D)$. For any unaligned components $C_i \\in C$, the LLM generates synonymous descriptions $C'$ and updates the query $Q'$ as follows:\n$Q' = \\begin{cases} = Synonym(C_i) & \\text{if } f_{LLM}(C_i, P) = 0 \\\\ else & C_i \\end{cases}$ (3)\nwhere $C'$ represents the synonymous description of the component $C_i$. The updated query $Q'$, now incorporating these synonymous descriptions, forms the foundation for the subsequent phase of the SCQU (Section 3.3)."}, {"title": "3.3 Semantic Compensation Query Update", "content": "Dense retrieval models effectively retrieve documents semantically related to a given query (Karpukhin et al., 2020; Lewis et al., 2020). Building on this property, we design the SCQU strategy as follows:\nCategorizing candidate documents: Candidate documents are classified based on $r$, the proportion of syntactic components in the query aligned by the document. Documents are categorized into two types: high-alignment documents (where $r >= \\tau$) and low-alignment documents (where $r < \\tau$), with $\\tau$ being a user-defined threshold.\nUpdating query: For high-alignment documents $D$, the system concatenates the current query $Q$ with the document $D$ to generate an updated query $Q''$. This operation is based on the premise that documents with high alignment are likely to yield even better results when the semantic content of the query is enriched. For low-alignment documents, the system generates a pseudo-document $D_{pseudo}$ semantically aligned with the suggested query $Q'$ (Section 3.2). This pseudo-document is then concatenated with $Q'$, thereby enriching the query with relevant semantic information. As a result, the updated query facilitates the retrieval of documents that are more closely aligned with the original query. The final query update strategy is expressed as follows:\n$Q'' = \\begin{cases} Q + D, & \\text{if } \\frac{|M'(Q,D)|}{|C|} >= \\tau \\\\ Q' + D_{pseudo}, & \\text{if } \\frac{|M'(Q,D)|}{|C|} < \\tau \\end{cases}$ (4)\nThis approach dynamically updates the query $Q$ in each retrieval cycle based on alignment conditions. Compared to LLatrieval (Li et al., 2024b), Our method reduces document retrieval by 95% on the ASQA dataset and 67% on the QAMPARI dataset, significantly enhancing retrieval efficiency. The number of documents retrieved by the two methods is provided in Appendix B."}, {"title": "3.4 Align-update Iteration", "content": "Algorithm 1 outlines the workflow of our system. The process begins with parsing the user query into syntactic components $C = \\{C_s, C_v, C_o, C_c, C_{attr}, C_{adv}, C_{supp}, C_{app}\\}$ (Line 3~4), initiating the iterative process. In each iteration, the system applies the SCQU strategy to retrieve a refined set of candidate documents $D_c$ (Lines 6~9), ensuring that the retrieved documents are increasingly semantically aligned with the query. Next, the FGA strategy is employed to assign alignment labels to each document in $D_c$ (Lines 10~11). These documents are then re-ordered based on the alignment labels and relevance, resulting in a prioritized set $D_o$ (Line 13), which includes documents that meet the verifiability criteria. Finally, the system employs the Progressive Selection and Document Verification methods proposed by Li et al. (2024b) to select and validate the final supporting documents (Lines 14~18). GGatrieval defines a robust selection criterion to establish clear retrieval objectives. Through the align-update iteration, the retrieval results are progressively refined to yield documents that better align with the retrieval goal, thereby enabling the LLM to generate both accurate and verifiable answers."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental Setting", "content": "Datasets. We evaluate our approach using the ALCE benchmark (Gao et al., 2023). ASQA (Stelmakh et al., 2022) is a long-form factoid question answering (QA) dataset in which each query requires multiple short answers to address its multi-faceted nature. QAMPARI (Amouyal et al., 2023)"}, {"title": "4.2 Main Results", "content": "Table 1 presents experimental results across three datasets, using two LLMs\u2014\u201cgpt-3.5-turbo\u201d and \u201cMeta-Llama3-8B-Instruct\u201d\u2014as base models. The choice of these two distinct models, along with representative baselines from distinct technologies, is designed to assess the generalization ability and effectiveness of GGatrieval across diverse scenarios.\nGGatrieval consistently outperforms baseline methods on all three datasets. Specifically, the substantial improvement in Citation F1 underscores GGatrieval's ability to retrieve high-quality documents, while the increase in Correctness highlights its positive impact on the overall performance of the RAG system. Notably, on the ELI5 dataset, GGatrieval achieves improvements of 22% and 28% in these two metrics, respectively. A key strength of GGatrieval over the baseline methods is its ability to evaluate and filter documents based on specific criteria, validating both the necessity and effectiveness of robust selection criteria.\nGGatrieval consistently outperforms baseline methods across distinct base models. Further analysis reveals that the \u201cgpt-3.5-turbo\u201d model consistently outperforms the \u201cMeta-Llama3-8B-Instruct\u201d model across all methods, confirming the scaling laws principle (Kaplan et al., 2020). Moreover, GGatrieval performs best on both base models, demonstrating its superior generalization ability. This capability can be attributed to the optimized retrieval mechanism of GGatrieval, which leverages the characteristics of the retriever to design query update strategies and simulates the human thought process in selecting retrieved documents, thereby enhancing overall system performance."}, {"title": "4.3 Ablation Study", "content": "Impact of each alignment label: This study evaluates the effectiveness of the proposed alignment labels through ablation experiments. As shown in Table 3, we systematically remove documents labeled as Full Alignment, Partial Alignment, and No Alignment from the final selection and assessed model performance on the ASQA and QAMPARI datasets. To ensure a fair comparison, the remaining documents are re-ranked based on alignment and relevance, with the highest-ranked documents filling any gaps. The results reveal that removing more aligned documents leads to a significant decline in Correctness and Citation F1 scores, underscoring the critical role of the FGA strategy. Notably, removing Full Alignment documents have the most pronounced impact on system performance, emphasizing the importance of our selection criterion.\nInteractions between components during iterations: To explore the relationships between query updates, document retrieval, and system performance, we conduct four rounds of iterative testing using the ASQA and QAMPARI datasets. Figure 3 illustrates the evolution of system performance, the number of updated queries, and the distribution of Full Alignment documents across iterations. The most significant improvement in system performance occurs in the second iteration, which coincides with the greatest decrease in samples containing zero Full Alignment documents and the largest increase in those containing five Full Alignment documents. As the iterations progress, system performance gradually stabilizes. Notably, the performance improvements are closely correlated with the increasing density of Full Alignment documents, underscoring the critical role of the pro-"}, {"title": "4.4 Analysis of Proportions for Different Alignment Labels", "content": "We analyze the proportion of alignment labels in the final selection of documents, as shown in Table 4. In the ASQA, QAMPARI, and ELI5 datasets, Full Alignment and Partial Alignment documents have not dominated the final selection. This observation could be primarily due to variations in label distributions across samples and the relatively low"}, {"title": "5 Conclusion", "content": "We introduce GGatrieval to enhance the retrieval process within the context of verifiable generation. Unlike traditional methods, our approach simulates the human cognitive process of document retrieval and proposes novel criteria for selecting retrieved documents. The criterion is then used to categorize the documents, and based on this categorization, we present two strategies: the SCQU and the FGA. Together, these strategies optimize the retrieval mechanism, ensuring retrieved documents satisfy verifiability standards and improve overall system performance. Experimental results show that GGatrieval outperforms existing methods, achieving state-of-the-art results."}, {"title": "Limitations", "content": "Our approach semantically aligns retrieved documents with the syntactic components of the query"}]}