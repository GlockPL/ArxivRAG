{"title": "Operator Feature Neural Network for Symbolic Regression", "authors": ["Yusong Deng", "Min Wu", "Lina Yu", "Jingyi Liu", "Shu Wei", "Yanjie Li", "Weijun Li"], "abstract": "Symbolic regression is a task aimed at identifying patterns in data and representing them through mathematical expressions, generally involving skeleton prediction and constant optimization. Many methods have achieved some success, however they treat variables and symbols merely as characters of natural language without considering their mathematical essence. This paper introduces the operator feature neural network (OF-Net) which employs operator representation for expressions and proposes an implicit feature encoding method for the intrinsic mathematical operational logic of operators. By substituting operator features for numeric loss, we can predict the combination of operators of target expressions. We evaluate the model on public datasets, and the results demonstrate that the model achieves superior recovery rates and high R\u00b2 scores. With the discussion of the results, we analyze the merit and demerit of OF-Net and propose optimizing schemes.", "sections": [{"title": "Introduction", "content": "With the rapid advancement of deep learning, artificial intelligence algorithms have achieved satisfactory outcomes in various tasks within computer vision and natural language processing. A faction of scientists has endeavored to utilize artificial intelligence methodologies for knowledge discovery, specifically referring to the extraction of symbolic expressions from numerical data to map its patterns, a category of tasks known as symbolic regression.\nUp to the present, foundational and mainstream methods of symbolic regression can primarily be categorized into three directions. Symbolic Physics Learner (SPL) (Sun et al. 2022) utilizes the methods of Monte Carlo tree search to discover expressions. Another approach, known as the Deep Symbolic Regression (DSR) (Petersen et al. 2019), employs the strategy gradient of reinforcement learning to guide neural networks searching in the solution space. These methods has served as the baseline for many subsequent improvements (Mundhenk et al. 2021; Li et al. 2024) based on solution space searching. Another notable direction, represented by Neural Symbolic Regression that Scales (NeSymReS) (Biggio et al. 2021), centralizes on leveraging the learning and fitting capabilities of neural networks to generate predicted candidates end-to-end. Specific enhancements to this category of methods have been made by many researches (Kamienny et al. 2022; Vastl et al. 2024; Meidani et al. 2023; Wu et al. 2023; Li et al. 2022), achieving commendable results. Furthermore, Silviu-Marian Udrescu (Udrescu and Tegmark 2020; Udrescu et al. 2020) utilizing neural networks' fitting capabilities as the approximation of target expressions, leveraging the automatic differentiation mechanism to discover the structure between varying trends of different variables, which has been used to infer properties such as translational symmetry and additive separability, thereby facilitating the dissection and piecewise parsing through variable separation. Additionally, numerous other methods (Landajuela et al. 2022; Xu, Liu, and Sun 2023; Holt, Qian, and van der Schaar 2023; Liu et al. 2023; Chen et al. 2023), integrating the strengths of baseline methods, make more detailed enhancements.\nHowever, in the vast majority of these approaches, different operator symbols, such as '+' and 'x', are merely treated as two characters of equal operational status during the prediction. Though the divergence in symbols can, to a certain extent, influence algorithm parameters via backward propagation, thus reflecting the differences between symbols, such disparities can intermingle and lack precision. For instance, in the case of $y = e^{x_1 + x + x_2}$, assuming the predicted outcome is $y = e^{x_1 + x_1 \\times x_2}$, even if the content and sequence of each characters in predicted expression are entirely accurate except only '+' character predicted incorrectly as 'x', this could lead to a significant numerical loss, which is then distributed dispersedly among all characters. We can evidently see that equating symbolic expressions directly to natural language characters is irrational.\nSilviu-Marian Udrescu's methods (Udrescu and Tegmark 2020; Udrescu et al. 2020) proposed to simplify expressions by determining the properties among variables, which indeed notably focuses on the fundamental mathematical logic inherent to the differing operators like '+' or 'x'. However, these methods necessitates that the variables can be segregated, specifically that they remain mutually independent within each segment. Their stringent explicit judgments have confined its application, rendering it ineffective when confronted with highly intertwined expressions. Hence, invigorated by the quintessential principles of these"}, {"title": "Related Work", "content": "methods, we employ the implicit feature representation of symbolic operators, utilizing soft constraints and loosely-coupled relationships, in association with deep operator networks (DeepONets) (Lu, Jin, and Karniadakis 2019), feature encoders, and other techniques, endeavoring to find the underlying functional operational logic inherent to unknown expressions from different known operators. Pertinent details of related neural networks will be expounded upon in 'Related Work', whereas a more precise algorithmic process will be elucidated in 'Method'.\nDeepONets\nIt is presented by T. Chen (Chen and Chen 1995) that a neural network with single hidden layer can theoretically approximate arbitrary nonlinear continuous operator, which refers to a mapping from one function space to another. Building upon this foundation, considering neural network optimization on generalization errors, Lu leads to the proposition of Deep Operator Networks (DeepONets) (Lu, Jin, and Karniadakis 2019), which consists of a trunk net and a branch net, structured in a manner consonant with the format of the approximation theorem.\nThe branch net accepts the original function's features as input and outputs new features of target function transformed by the operator. It is essentially a mapping of different feature fields, representing the process of the operator acting on the function. Simultaneously, the trunk net takes the coordinates in the space of target function as inputs and outputs the encoded positional features. The function feature from branch net and the position feature from the trunk net together compute the value of target function at the respective coordinates. This process that culminating in an effective approximation of target function is regarded as the fitting procedure for the operator.\nNumerous researchers have subsequently made further modifications (Lu et al. 2022a,b; Jin, Meng, and Lu 2022; Mao et al. 2021) based on this foundation, which yet are mainly tailored towards specific issues in designated scenarios like equations coupling or multiple function space, offering considerable reference value when addressing related problems with the core concept and fundamental framework remaining unchanged. Operator networks were initially designed for dynamic systems and problems related to partial differential equations and have been following this trajectory of development. Nonetheless, it is evident that they can be readily applied to simpler symbolic operators as encountered in elementary mathematics.\nDiscrimination and Multimodal\nThe process of transitioning from symbolic expressions to operator architectures can largely be understood as a multi-target discrimination task. Specifically, this involves transforming the issue of determining which operators appear in a particular place into a discrimination problem of identifying the presence of the interaction of each operator, thus necessitating the neural network to perform judgment actions. Since the introduction of the transformer (Vaswani et al. 2017) architecture, centered around the attention mechanism, it has garnered extensive and widespread recognition. The encoder part of the transformer was isolated as the bidirectional encoder representations from transformers (BERT) (Devlin et al. 2018) architecture was proposed for feature extraction and processing which has been extensively utilized in tasks such as text classification as well as other long sequence discrimination challenges. Due to its effective performance and broad applicability, this structure has been adopted in a variety of fields including vision for classification, regression, and other tasks, achieving notable results. Moreover, when handling multimodal data, the feature encoders and decoders effectively accomplish the task of mapping features between different domains. In the context of symbolic regression, the set-transformer (Lee et al. 2019) is widely employed as an encoder. It is capable of accepting input-output pairs, reducing the computational scale, and ensuring that the order of data points does not affect the outcome."}, {"title": "Methods", "content": "Directed graph for symbolic expression\nrepresentation\nPresently, the majority of symbolic regression methods employ tree representation, in which each node denotes a character and the edges symbolize the sequential relationships among characters, typically arranged in preorder traversal. This approach is facing with two issues.\nOne is that for a given expression, selecting various nodes as the root or different sequence may lead to different structures but representing for a semantically equivalent tree. This is profoundly influential for neural networks. Given a network without random, under the circumstances of determined parameters and unchanged inputs, a specific output will certainly be generated. If training input associated with multiple output labels, the neural network's parameter optimization during training will target multiple objectives. Each direction will be influenced by other objects, causing deviation. The overall loss would be a trade off among various labels, hindering the accurate capture of the correct results.\nAnother predicament is that the prediction of trees utilizes the logic of natural language represented by strings. Despite the string, that forms the tree through preorder traversal, partially reflects some character features to a certain extent, this feature is imprecise in mathematics as mentioned earlier. Therefore, instead of interpreting the expressions through the character sequence in natural language, we place our focus on the mathematical operating logic beneath the expression. For this purpose, we have revamped the way of expression representation.\nWe utilizing directed graphs as our foundational framework where the nodes are categorized into two distinct types: variable nodes and operator nodes. Variable nodes denote the functions being inputted, and are defined as $f = x_i$ explicitly in symbolic regression problems. To streamline the overall operator structure, we adopt $f = c \\times x_i$ as our elemental input function, where 'c' symbolizes a constant to be optimized, thereby broadening the expressive range of the operator under equivalent scale constraints. Operator nodes represent the operators that act upon functions, with the transformed functions as outputs. Expressions are realized through the sequential combination of multiple operators with connections between operators denoted by the edges.\nWe employ the adjacency matrix form to denote the one-hot encoding of operator graphs. $label[x, y] = 1$ implies the existence of operator x acting on the output of operator y, where certain pieces of data flow from operator y to operator x. At the same time, this representation method resolves the issue of multiple encodings corresponding to a single expression. Although the process of decoding expressions from their corresponding encodings may introduce redundant skeleton candidates, due to the fact that the process of mapping concrete expressions from adjacency matrices isn't bijection, this can be partly resolved via search algorithms. Hence, this graphical representation still solves the issues encountered with tree representation to a certain extent, greatly reducing the amount of redundant and useless possibilities.\nIn an ideal scenario, a basic operator set is complete and independent that can perform any function representable in mathematics through nested combinations with each mathematical expression corresponds to only one operator combination structure. In our established operator set, for instance, operator 2 for multiplication and operator 7 for power ($c > 1$) are dependent as the square of X is the same as x times x. Yet, these two operators cannot be substituted in most instances, like $sin(x) \\times sin(1.5x)$, which will miss a part of the potential solution space if they are substituted. Only when both sides are completely consistent, including constant optimization, can operator 2 be represented by operator 7. But this is obviously impractical, because constant optimization exists after the expression skeleton. The reason why we choose not to use multiplication and $pow (0 < c < 1)$ to replace the pow ($c > 1$) is because for slightly complex functions, this replacement will introduce a large number of candidates and constants, leading to an unacceptable burden for path searching and constant optimization. Therefore, we make a trade-off between operator independence, completeness and search complexity, forming our operator set in table 1, which can represent most mathematical formulas without extensive repetitious presentation while decreasing search difficulty. Increase of thresholds is effective in solving the overlap of operators.Naturally, this is likely not the optimal choice, but it is basically sufficient.\nThe Network\nThe network's structure is shown in figure 2, which can be divided into forward fitting and backward inference.\nThe forward network employs the basic infrastructure of the operator network, accomplishing an overall fit of each operator's computational process and capturing operator features intermediately in the branch net. The data sampled from positions are projected into a positional feature through a position encoder, corresponding to the trunk net. The branch network component includes distinct operator encoders for each operator and just one shared operator decoder for every different operators. The function samples pass sequentially through the operator encoder and decoder, and the resulting vector multiply with position features getting the final numerical fitting outcome. This process constitutes the network's fitting for a single operator, and upon successful fitting, the network is considered capable of representing the target operator.\nThe trunk net is responsible for positional encoding is solely dependent on the input positions and is unaffected by the input function or the target operator. Consequently, the positional encoding remains consistent across different operators. However, the target operator's variation leads to differing numerical results at the same coordinates, necessitating that only the branch network represents the unique characteristics of each operator. The use of both an encoder and a decoder in the branch network is due to the fact that the final output of the branch net possesses the ability to fit the operator under the given positional encoding, representing the numerical fitting characteristics, which we refer to as numerical features. What we require, however, is the structural characteristic in terms of mathematical operations, known as operator features. Therefore, a mapping in the feature space is necessary. Employing a single, shared decoder ensures that for different operators, only the operator encoder varies. This approach concentrates the differences between operators within the encoder, allowing the encoder to extract operator features that encapsulate the unique operational logic, thereby more effectively highlighting their distinct attributes.\nThe backward network is responsible for deriving the final predictive expression from the basic and target operator features. Notably, for an unknown expression, we only have input-output pairs and a value range. Therefore, the ideal process involves fixing the operator decoder and position encoder while using known data to train an operator encoder for the target operator, allowing it to fit the final numerical labels, before the next step of prediction. However, this process is time-consuming that impractical to retrain a network module for each new operator. To resolve this, we add two modules to facilitate the reverse inference process of the forward network. The numerical decoder retrieves numerical features given known positional features and numerical labels, while the operator inverse decoder maps from the numerical feature space to the operator feature space. This allows us to use a forward propagation of the backward network instead of reverse training to obtain operator features. The judgment network takes the basic and target operator features as input and predicts their relationship, outputting an adjacency matrix. The search module, not actually part of the neural network, is a search strategy to constructs functions by searching paths in operator adjacency matrix. We balance search completeness and efficiency so that most common functions are discovered earlier. Details will be introduced in 'Searching'.\nThe judgment block is based on BERT due to its intrinsic attention mechanism that capably discerns the relation of diverse operator, with all features treated equitably and without unidirectional perception masks. Another module impervious to the sequence of inputs is the numerical decoder, where the order of sample points should not bias the function's interpretation, therefore we employ a set-transformer with canonical structure. The operator decoder also utilizes a transformer framework, which eschews the typical practice of feeding different sequence elements as input and instead maps one operator feature to multiple spaces, aggregating the mapped features to enrich the operator feature with multidimensional information. The judgment block and numerical decoder employ standard stacked modules while remaining sections include deep fully connected networks with residual links, arranged alternately with single layers and residual blocks. Both the network scale and feature lengths are adjustable hyper-parameters.\nSearching\nThe search referenced here diverges from algorithms like DSR and does not aim to navigate a vast solution space directly toward a target. Instead, guided by an adjacency matrix, it adopts a traversal method like breadth-first search within a constrained area,.\nThe search process is directed by the pre-obtained operator adjacency matrix, significantly narrowing the solution space. Consequently, the approach shifts away from random exploration towards a pseudo-traversal method designed to preserve more completeness. This \"pseudo\" derives from the strategy not being a comprehensive exploratory search like depth-first search but one that discards highly uncommon solution spaces to more quickly access and identify frequent structures.\nFirstly, we impose restrictions on the nesting of operators: the duplication of trigonometric, power, exponential, and logarithmic operators within their sub-nodes is restricted, which is a common constraint in symbolic regression to maintain the simplicity of expressions, as most conventional expressions do not involve nesting of these operators or possess uncomplicated equivalent forms without nesting. For addition and multiplication operators, however, there is an upper limit to the number of times their sub-nodes may duplicate themselves. This measure is adopted because certain common expressions inherently contain loops, exemplified by $y = cos(x_1) + cos(x_1 + x_2)$, the potential infinite loop between addition operators and cosine operators without nesting limits.\nWe set $c \\times x$ as the unique initial variable, designating it as a leaf in the computational graph. Once this node is encountered during the search, that branch of the search is terminated and no further extension is performed. Other operators, serving as non-leaf nodes, will sequentially act on the initial variable. When searching for candidate expressions from the graph, the root must first be determined. We evaluate the potential of each operator as the root based on its out-degree and in-degree. An operator with an in-degree of zero and a non-zero out-degree is most likely to be the root and will be prioritized. If an operator has a non-zero in-degree but an out-degree greater than its in-degree, it may also serve as a root, and this category of operators will be considered when the search has not yielded an ideal solution. The greater the difference between the out-degree and in-degree, the more likely the operator is to become the root of the target expression, which is merely based on empirical observations rather than rigorous mathematical measure of probability.\nMeanwhile, we have made adjustments to the situation that adjacency matrix has errors, ensuring that the search returns relatively accurate candidate expressions without collapse. When encountering operators at non-leaf nodes without out-degree, the strategy dictates returning an empty set or the initial variable. In instances where multiple operators lack in-degree, we sequentially initiate the search with these operators until completion or until an expression is identified. With addition or multiplication operators receiving multiple inputs, it can be unclear whether they truly represent the addition of multiple terms or if some inputs are erroneously interpreted. Consequently, we opt to first search for the addition of two terms, incorporating a placeholder for potential expansion. This placeholder is disregarded during subsequent optimization of constants, yet should no suitable expression emerge in a given round, we extend one term at the placeholder's position, conduct an independent search, and integrate the outcome into the original candidate expression. To prevent indefinite expansion of placeholders, we set a maximum number of e expansions as a hyperparameter.\nConstant optimization is not conducted all the time, but occurs after generating a list of candidate skeletons in each round. Skeletons are added sequentially to the list according to the order so that it suffices to simply traverse the list in sequence for constant optimization. We employ the classical BFGS method for the constants optimization where 80 points are randomly selected as reference points. Each candidate expression undergoes constant initialization 30 times, with the first ten in the range (0, 1) and the subsequent twenty in the range (-10, 10), while the range for operator $pow(f, c)(0 < c < 1)$ remains consistently within (0, 1). It's noteworthy that for the pow operator, the BFGS algorithm struggles to effectively optimize constants, as constants are represented in floating-point format. When the exponent includes a constant and the base might be negative, complex numbers are introduced, undermining the relevance of the $R^2$ because an incorrect complex value's $R^2$ can significantly exceed 1. Even though we recognize that legitimate expressions do not encompass such cases, it's impossible to guarantee the absence of complex numbers during random initialization and the computational process.\nTo address this challenge, we explored two alternative strategies. If it's assumed a priori that the expression does not contain floating-point exponent terms, one method involves iteratively trying different integer. This approach reliably captures the correct skeleton but at the cost of introducing an extensive amount of futile computations for expressions with higher-order terms, thus wasting considerable time. Another method involves respectively modulating the subtraction in Mean Squared Error (MSE) and $R^2$ during numerical optimization and assessment, substituting square with euclidean norm. This can enhance the precision of constant optimization to a certain degree. However, empirical tests showed that the improvement in precision is limited, and some expressions still fail in constant optimization even though their skeleton is accurate. Therefore, we apply different optimization strategies tailored to various expressions, with detailed discussions in 'Experiments and Discussion'."}, {"title": "Experiments and Discussion", "content": "Conditions and Setup\nThe operator feature length is set at 500, numerical feature length at 200, with 200 function sampling points in branch net and 1600 position sampling points in trunk net. The position encoder consists of univariate block and bivariate block that each incorporates six single hidden layers and five residual blocks, with a maximum layer length of 800. Both the operator encoder and the operator inverse decoder are structured with five single hidden layers and four residual blocks, with a maximum layer length of 1000. The operator decoder and judgment block each utilize a stack of three attention layers. We utilize complex constant optimization and integer traversal optimization respectively when the exponent is greater and is not greater than 5 for univariate expressions, while 3 serves as the threshold for bivariate expressions.\nWe compared our method against four different approaches, denoted as Genetic Programming (GP) for searching, NeSymReS and T-JSL(Li et al. 2022) for pre-trained, and DSO/NGGP(Mundhenk et al. 2021), which is known as SOTA, for combined model. We set pop size at 1000 and generation at 40 in GP and utilize standard configuration of others. The test set includes univariate and bivariate expressions in SRBench and the majority of the known public datasets currently available. We believe that recovery rate is one of the most significant indicators of performance in symbolic regression, as achieving complete recovery and discovering the true formula aligns with the fundamental goals of symbolic regression. However, due to the involvement of constant fitting, we approached the issue from a numerical perspective, making the R\u00b2 score indispensable. An R\u00b2 score of 0.999 and 0 does not imply equivalent algorithm performance. Therefore, we tested each expression 10 times, recording the R\u00b2 scores and recovery rates. If complete recovery was achieved at least once, we recorded the R\u00b2 as 1; otherwise, we averaged the two median values. All experiments were conducted using the same hardware configuration: NVIDIA Corporation GV100 [TITAN V].\nResult\nThe experimental results are illustrated in figure 3, with detailed outcomes in appendices. Notably, due to the introduction of random in most methodologies, such as the initialization of BFGS constant optimization and position sampling, as well as potential variations in the runtime environment, some discrepancies between our test results and the descriptions in original articles are reasonable. The results we present were retested under uniform software and hardware conditions.\nIn evaluating whether a prediction is completely recovered, we first consider the R\u00b2 value. If R\u00b2 exceeds 0.999999, we believe that this prediction has the possibility to be correct. Subsequently, for predictions meeting the R\u00b2 criterion, we conduct a manual verification, where we manually calculate and compare the predicted expression with the correct label to determine a complete recovery. Therefore, even if the predicted skeleton is accurate but the constant optimization is inadequate, we do not consider it to be completely recovered.\nExperiment result indicates that OF-Net achieved the highest recovery rate across the holistic dataset, an outcome anticipated given its design tailored specifically for operator features. OF-Net can almost invariably reconstruct the correct skeleton if successfully deducing the right operator graph, thereby highly likely getting the accurate expressions as well. While OF-Net's R\u00b2 performance was slightly inferior to NGGP, it surpassed both T-JSL and NeSymRes, and was comparable to GP. This was an unexpectedly positive result because methods like GP, progressively minimizing losses, can maintain high R2 values without identifying the correct expressions. In contrast, OF-Net, which is not driven by numerical loss, incurs more significant errors when operator graph are incorrect. Its commendable R\u00b2 outcomes are, to a large extent, contingent upon the complete recovery leading to scores '1.0'."}, {"title": "Discussion", "content": "According to the distribution of the test set, we evenly segmented the expression lengths, which is represented by character number, into five intervals and charted the performance of each method. Variance was calculated as\n$Var = \\frac{\\sum_{i=1}^{5} N_i \\times (X_i \u2013 \\bar{x})^2}{\\sum_{i=1}^{5} N_i}$\nwhere n is expressions' number and means the average. We get 0.023 for OF-Net second only to NGGP, which is satisfactory for a pre-trained model. The visual and variance indicate that OF-Net is little impacted by expression length, showcasing superior stability, suggesting that OF-Net possesses splendid extrapolative capabilities. For expressions exceeding 15 characters, a decline in performance was observed across all methods, largely due to the limitations in constant optimization. For instance, both OF-Net and NGGP encountered scenarios where the skeleton was accurately identified but the correct constants were not determined. Therefore, should ideal constant optimization be achievable, OF-Net's extrapolative potential could be further enhanced.\nOF-NET shows a significant difference in performance between univariate and bivariate data. It achieves exceptional results in univariate expressions, matching the optimal R2 of NGGP while leading significantly in recovery rates. Detailed analysis reveals that OF-NET almost always predicts the correct skeleton for univariate expressions, with only 8.8% of errors due to constant optimization, mainly contributing to its outstanding performance. However, for bivariate data, OF-NET's predictions are less satisfactory, with lower R2 and recovery rates compared to other methods. Notably, 37.7% of expressions lacked the correct skeleton, and 32% of these errors were due to issues with operator prediction, which indicates the current limitations. We preliminarily attribute this issue to the training data. As bivariate operator structures are far more complex than univariate ones, it necessitate more training data. However, constrained by hardware limitations, we used the same data volume for both.\nAddition and multiplication operators may introduce extraneous constants, which impedes constant optimization, resulting in certain non-minimal correct skeletons not being optimized successfully, and rendering some correct skeletons more complex. For instance, the standard solution for Nguyen-7 is $log(x_0 + 1) + log(pow(x_0, 2) + 1)$, but the expression (four decimal places) derived might be $log(5.2422 \\times x_0 + 5.2422) + log(0.1908 \\times pow(x_\u03bf, 2) + 0.1908)$. Although mathematically equivalent, the redundant constants clearly complicate the expression, increasing the computational burden. Moreover, expressions like Koza-3 and Nguyen-4 encounter difficulties in optimizing the correct skeleton to an R2 of 1.0 due to issues with complex numerical optimization of higher-order power.\nUpon analysing results in greater detail, it becomes apparent that the range of variable exerts an influence on the outcomes. For instance, the structure $sin(x_0) \\times cos(x_1)$ is prone to misjudgment when variables are in range (-1,1), leading to unsuccessful recovery for expressions involving this structure, such as Keijzer-13 and Nguyen-10. However, Livermore-10, which assumes a variable range of (0,1), is successfully identified.\nAccording to the analysis of demerits, it is merited to take improvements in future work for superior performance. Specifically as follow:\n\u2022 Enrich bivariable training dataset with broader diversified data.\n\u2022 Revise operator set to minimize overlap, thereby reducing the introduction of constants.\n\u2022 Propose an effective constant optimization algorithm especially in complex domain, tailored for the power operator.\n\u2022 Implement uniform scale transformation prior to executing the algorithm, followed by an inverse transformation on predicted expression."}, {"title": "Conclusion", "content": "In terms of the demerits of string predicting for the tree representation of expressions in symbolic regression, OF-Net uses the operator graph for representation and encodes the mathematical operations into feature codes, which are utilized instead of numerical value loss to discern the operator structure within unknown target expressions, thereby obtaining candidate skeletons. After constant optimizing, OF-Net can finally recover the expression. OF-Net utilizes the neural network fitting to represents the calculation process of the operators, in which multi-dimensional features of different operators are aggregated so as to extract more comprehensive and representative operator features.\nThe experiments are conducted on available public data sets and compared with four classical methods, GP, NeSymReS, T-JSL ans NGGP, involving every category of methods in symbolic regression. It confirms that OF-Net, compared to contrast methods, achieves a satisfactory performance with the highest recovery rate and excellent R2, as well as less variance for expression lengths which leads to a splendid stability and extrapolative capabilities."}]}