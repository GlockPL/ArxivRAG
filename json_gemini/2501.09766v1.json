{"title": "Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning", "authors": ["Yirong Zeng", "Ding Xiao", "Yuxian Wang", "Weiwen Liu", "Wu Ning", "Yutai Hou", "Xu Huang", "Bing Qin", "Ting Liu"], "abstract": "Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities. Effectively leveraging this potential for complex tasks hinges crucially on improving their ability to use tools. Synthesizing tool use data by simulating the real world is an effective approach. Nevertheless, our investigation reveals that training gains significantly decay as the scale of these data increases. The primary factor is the model's poor performance (a.k.a deficiency) in complex scenarios, which hinders learning from data using SFT. Driven by this objective, we propose an iterative reinforced fine-tuning strategy to continually guide the model to alleviate it. Specifically, we first identify deficiency-related data based on feedback from the policy model, then perform a Monte Carlo Tree Search to collect fine-grained preference pairs to pinpoint deficiencies. Subsequently, we update the policy model using preference optimization to align with ground truth and misalign with deficiencies. This process can be iterated. Moreover, before the iteration, we propose an easy-to-hard warm-up SFT strategy to facilitate learning from challenging data. The experiments demonstrate our models go beyond the same parametric models, outperforming many larger open-source and closed-source models. Additionally, it has achieved notable training gains in complex tool use scenarios.", "sections": [{"title": "1 Introduction", "content": "Tool use is one of the key features of large language models. By combining LLMs with external tools, their ability to solve complex tasks in the real world has been significantly enhanced (Qin et al., 2023; Qu et al., 2024). The tool use capability allows LLMs to access up-to-date information, perform precise calculations, and reduce the likelihood of hallucinations (Schick et al., 2023). This unlocks a wide range of potential applications in various domains, such as the automation of complex tasks (Zhong et al., 2023), reasoning tasks (Manduzio et al., 2024), and the scheduling of applications on devices (Gunter et al., 2024; Chen et al., 2024b). In essence, tool use involves the following process: Given one or more tools, a user presents a question, and the LLM selects the appropriate functions from the candidate functions and performs the tool call to fulfill the user's question. In this paper, tools are used interchangeably with APIs, functions, and plugins.\nRecent advancements have introduced a variety of relevant datasets and benchmarks (Qin et al., 2023; Liu et al., 2024; Yan et al., 2024a), along with the release of powerful models specifically designed for tool use of LLMs. Research has found that LLMs can handle simple tool use scenarios by prompt engineering (Tang et al., 2023), but they face challenges in more complex, real-world scenarios (Yan et al., 2024b). Therefore, some works simulate real-world scenarios, such as ticketing systems, to mimic more realistic use cases (Lin et al., 2024) for collecting synthetic data. Then, employing Supervised Fine-Tuning (SFT) on it to address tool use in complex scenarios"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Tool use of LLMs", "content": "Pioneering works like Toolformer (Schick et al., 2023) and ToolAlpaca (Tang et al., 2023) have explored the potential of LLMs in tool use. Previously, several tuning-free methods were proposed, which involves manipulating prompts (e.g., (Xu et al., 2023; Shi et al., 2024; Qiao et al., 2024)) or enhancing execution frameworks (e.g., ReAct (Yao et al., 2023), RestGPT (Song et al., 2023)) to unlock inherent capabilities.\nDue to the limitation of user-defined tools in prompts of the above methods, tuning-based methods with synthetic data have been focused. ToolLlama (Qin et al., 2023) notably expanded the toolset and investigated the impact of data scaling on performance. More efficient data synthesis techniques have been proposed for tool use (e.g., Toolace (Liu et al., 2024), BUTTON (Chen et al., 2024a), and XLAM (Zhang et al., 2024)). However, emerge marginal gains decreases deficiency when using synthetic data, which we aim to alleviate in this work."}, {"title": "2.2 Preference Optimization", "content": "Learning from human feedback is crucial in aligning LLMs with human values and intentions (Leike et al., 2018), refer to preference optimization. Online preference optimization algorithms (Schulman et al., 2017; Zheng et al., 2023) are complex and difficult to optimize. Recently, Direct Preference Optimization (DPO (Rafailov et al., 2024)), a simpler offline algorithm, reparameterizes the reward function to learn a policy model from preference data directly, enhancing simplicity and training stability. Besides, a variety of preference optimization objectives have been proposed (e.g., SimPo (Meng et al., 2024), IPO (Azar et al., 2024), ORPO (Hong et al., 2024) and KTO (Ethayarajh et al., 2024).\nFurther studies have extended this approach to an iterative training setup, by continuously updating the reference model with the most recent policy model or generating new preference pairs at each iteration (Dong et al., 2024; Yuan et al., 2024; Kim et al., 2024; Rosset et al., 2024)"}, {"title": "3 Task Overview", "content": "In tool use scenarios, the LLM receives a user query $q$ along with a set of candidate tools, represented as $T = {t_0, t_1, ..., t_{|T|}}$. The purpose of LLM is to fulfill the user's intent by executing a specific"}, {"title": "4 Method", "content": ""}, {"title": "4.1 Overall", "content": "The Figure 2 shows the overall architecture of our proposed iTool. It consists of a warm-up training and an iterative reinforcement learning."}, {"title": "4.2 Warm-up training", "content": "In real-world applications, the tool-use model should select multiple tools from complex candidate toolset and schedule them correctly (a.k.a., hard mode), instead of directly using a single candidate tool to response (a.k.a., easy mode). Similar to human learning procedures, tool learning models can benefit from an easy-to-hard curriculum during model training (Xu et al., 2020). Therefore, we propose an easy-to-hard SFT for warm-up training. In the warm-up stage, we first divide the dataset evenly into three subsets (i.e., easy, medium, hard) based on difficulty levels. We follow the criteria: 1) candidate toolset number 2) the string length of the toolset, 3) the number of tool calls needed in response.\n$D = D_{easy} \\cup D_{medium} \\cup D_{hard}.$\nSubsequently, we fine-tune the LLM M sequentially on each subset $D_i$ using the supervised loss:\n$L_i = - \\sum_{(q,y) \\in D_i} log P_M(y \\mid q, T)$,\nwith $D_1$ (easy), $D_2$ (medium) and $D_3$ (hard). The total warm-up loss is:\n$L_{warm-up} = \\sum_{i=1}^{N=3} L_i$."}, {"title": "4.3 MCTS-Based Iterative Reinforcement Learning", "content": "In order to alleviate training gains decreases using synthetic tool use data for LLM, in this module, we propose an Iterative Reinforcement Learning scheme to continuously remedy this deficiency. As shown in Figure 2, it iteratively refreshes replay buffer to sample complex data and generates preference data for preference optimization.\nSampling complex data. Given a warm-up model from the previous stage, it is used to refresh the replay buffer by feedback the complexity of samples. The replay buffer is initialized using a leave-one-out strategy from the tool use dataset, where each example in the buffer is represented as: $X_{buff} = (q,T, c)$, where c is denote the complexity of sample. In practice, model generation perplexity $h$ is used to measure the complexity of the samples, i.e., $c = h$. The generation perplexity of the target response can be factorized as follows:\n$h = \\frac{1}{P_M(y \\mid q, T)},$\nwhere the $P_M(y \\mid q, T)$ is the generation probability. Since perplexity h represents the degree of generation uncertainty, samples with higher perplexity h require further training in subsequent iterations (Gao et al., 2024).\nMCTS for Step-Level Preference. The success of OpenAI 013 provides a compelling illustration of the effectiveness of step-by-step thinking. The integration of MCTS as a policy improvement operator transforms the current policy model into an improved one(Grill et al., 2020). Inspired by these, we propose to integrate MCTS to collecting step-level preference data.\nThe step-wise MCTS is achieved by breaking down the expand step. To transform instance-level rewards into granular, step-level signals, we break down the action into discrete steps, each represented by a tool call or sentences. This process begins from a root node $s_0$, as the sentence start or incomplete response, and unfolds in three iterative stages: selection, expansion, and backup.\n(1) Select. It is guided by two key variables: $Q(s_t, a)$, the value of taking action a in state $s_t$, and $N(s_t)$ is the visitation frequency of state $s_t$. To navigate the trade-off between exploring new nodes and exploiting visited ones, we employ the Predictor+ Upper Confidence bounds applied to Trees (PUCT) (Rosin, 2011). At node $s_t$, the choice of the subsequent node follows the formula:\n$s_{t+1} = arg \\max_{s_t} [Q(s_t, a) + c \\frac{p(a \\mid s_t) \\sqrt{N(s_t)}}{1+ N(s_{t+1})}].$"}, {"title": "5 Experiments", "content": "In this section, we show the superiority of our method in performance and robustness across various benchmarks, and in-depth analysis to verify the effectiveness of our method."}, {"title": "5.1 Experimental Setup", "content": "We take the widely used open-source LLM, LLaMA3.1-8B-Instruct\u00b9 as our base model. We use synthetic data from Toolace for experiments, 90% for warm-up training, and the rest for reinforcement learning. For warm-up training, we adopt the parameter-efficient training strategy LORA (Hu et al., 2022). For reinforced fine-tuning, we vary the learning rate and beta \u1e9e to find the best hyperparameter setting. For more implementation details, see Appendix A.\nDataset. The Berkeley Function-Calling Leaderboard v3(BFCL-v3) (Yan et al., 2024a) provides a comprehensive dataset comprising 4k+ instances (updating), consisting of Non-live(with expert curated simple tools), Live (with user-contributed complex tools), Multi-turn (with multi-turn&step tool use) and Hallucination (relevance and irrelevance) samples. API-Bank (Li et al., 2023) consisting of 314 tool-use dialogues and 753 API calls, evaluates models' ability to correctly invoke a known API (L-1) based on a query and to retrieve and call APIs from a candidate list (L-2).\nBaselines We compare the overall performance with the state-of-the-art closed-source models (e.g., GPT-series, DeepSeek-series(DeepSeek-AI, 2024)) and open-source models (e.g., Llama-3.1-8B-Instruct, Qwen2.5-7B (Team, 2024), ), as well as fine-tuned open-source models with tool use dataset, including ToolACE-8B (fine-tuning Llama-3.1-8B-Instruct on Toolace) model, xLAM-series (Zhang et al., 2024) and Hammer-series(Lin et al., 2024)."}, {"title": "5.2 Overall Performance", "content": "The overall performance of iTool-8B and various representative models are shown in Table 1 and Table 2. The full evaluation of the BFCL-v3 leaderboard is shown in Table 4. The results indicate that our model consistently achieves corresponding sota performance at comparable scales (~ 8B). Furthermore, our model shows consistent advantageous performance on API-Bank compared with open-source models. Table 1 shows our model outperforms most closed-source and open-source models in BFCL, and demonstrate comparable performance with GPT-4-series models. Our model demonstrated its superiority on hard samples (Live and Multi-turn). It is mainly attributed to our iterative reinforced fine-tuning for hard data, which compensates for its deficiencies."}, {"title": "5.3 Ablation Analysis", "content": ""}, {"title": "5.3.1 Module Ablation", "content": "To evaluate the effectiveness of two components in our method, we conduct an ablation study by removing (1) warm-up training (w/o warm-up). (2) Iterative Reinforcement Learning (w/o IRT). We also include Llama-3.1-8B-Instruct as a base model for comparison. The results are presented in Figure 3. demonstrates that all components are essential within the framework and that warm-up training is indispensable."}, {"title": "5.3.2 Deeper Ablation", "content": "(1) In warn-up training, we conducted a study on the easy2hard SFT strategy. We present the performance progression from easy to hard and compare it with the conventional mixed SFT strategy as well as the base model. For a fair comparison, we utilized an identical amount of data from Toolace to implement the above strategies. The experimental results are summarized in Figure 4. From the results, we observe that our strategy shows a gradual improvement and surpasses the mixed SFT. There is a significant leap from base to easy, and the second largest improvement occurs from the medium to hard. This indicates that the model benefits from the curriculum learning process that goes from easy to hard. In the synthetic data, the model can quickly learn the task patterns"}, {"title": "5.3.3 Base Model Analysis.", "content": "To further validate the effectiveness of base models, we applied our method to other base models. Due to computational resource constraints, we compared the following base models (< 10\u0412): (1) Llama-3.2-3B-Instruct\u00ba (2) Qwen2.5-7B-Instruct (Team, 2024). We also add fine-tuned base models on the same dataset for comparison. The results are illustrated in Table 3. We find that our"}, {"title": "5.4 Training Gains Analysis", "content": "To analyze how the training gains of our model change as the synthetic training data increases, we conducted experiments on two datasets: ToolACE (in-domain with test data) (Liu et al., 2024) and ToolBench (out-of-domain with test data) (Qin et al., 2023). The performance of the models trained on the different datasets, ToolACE and ToolBench, was evaluated using BFCL-v2 for analysis. The experimental results are presented in Figure 7. The results indicate that although our method initially yields modest performance, it remains respectable as the data scale increases, compared to training with SFT. This conclusion is more evident on Live. The reason is that our approach has an advantage in overcoming difficult scenarios, which is a flaw in the source model."}, {"title": "6 Conclusion", "content": "Equipping LLMs with external tools is becoming a viable method to enhance their capabilities. In this paper, we study boosting tool use of LLMs. Given the training decay issues associated with synthesized tool data, we propose an iterative reinforced fine-tuning strategy to continually guide the model in overcoming this weakness. Based on a step-wise MCTS, it can identify, pinpoint, and alleviate the model's deficiencies. Experimental results demonstrate the effectiveness and superiority of the proposed method."}, {"title": "7 Limitaiton", "content": "todo"}, {"title": "A More Implementation Details", "content": "individually search\nWe train.\nWe vary the percent a in 10, 15, 20, 25, 30 and find that the a = 20 achieves the best performance.\nWe optimize the model using Qlora citeptodo with the learning rate of 5e-5 and the weight decay coefficient of 0.01. The training of our model can be done within 20 hours with 4 NVIDIA A100-PCIE-80GB GPUs. This is an appendix."}, {"title": "B Extra Experimental Results", "content": ""}, {"title": "C An Example of tool use", "content": ""}]}