{"title": "Medical Slice Transformer: Improved Diagnosis and Explainability on 3D Medical Images with DINOv2", "authors": ["Gustav M\u00fcller-Franzes", "Firas Khader", "Robert Siepmann", "Tianyu Han", "Jakob Nikolas Kather", "Sven Nebelung", "Daniel Truhn"], "abstract": "Background: MRI and CT are essential clinical cross-sectional imaging techniques for diagnosing complex conditions. However, large 3D datasets with annotations for deep learning are scarce. While methods like DINOv2 are encouraging for 2D image analysis, these methods have not been applied to 3D medical images. Furthermore, deep learning models often lack explainability due to their \u201cblack-box\" nature. This study aims to extend 2D self-supervised models, specifically DINOv2, to 3D medical imaging while evaluating their potential for explainable outcomes.\nMethod: We introduce the Medical Slice Transformer (MST) framework to adapt 2D self-supervised models for 3D medical image analysis. MST combines a Transformer architecture with a 2D feature extractor, i.e., DINOv2. We evaluate its diagnostic performance against a 3D convolutional neural network (3D ResNet) across three clinical datasets: breast MRI (651 patients), chest CT (722 patients), and knee MRI (1199 patients). Both methods were tested for diagnosing breast cancer, predicting lung nodule dignity, and detecting meniscus tears. Diagnostic performance was assessed by calculating the Area Under the Receiver Operating Characteristic Curve (AUC). Explainability was evaluated through a radiologist's qualitative comparison of saliency maps based on slice and lesion correctness. P-values were calculated using Delong's test.\nResults: MST achieved higher AUC values compared to ResNet across all three datasets: breast (0.94\u00b10.01 vs. 0.91\u00b10.02, P=0.02), chest (0.95\u00b10.01 vs. 0.92\u00b10.02, P=0.13), and knee (0.85\u00b10.04 vs. 0.69\u00b10.05, P=0.001). Saliency maps were consistently more precise and anatomically correct for MST than for ResNet.\nConclusion: Self-supervised 2D models like DINOv2 can be effectively adapted for 3D medical imaging using MST, offering enhanced diagnostic accuracy and explainability compared to convolutional neural networks.", "sections": [{"title": "Main", "content": "Deep learning (DL) has demonstrated significant potential in medical imaging for diagnosis and analysis (1,2). However, its integration into clinical practice is hindered by the need for large, annotated datasets and limited model interpretability. Generating medical annotations is time- consuming and requires expert input (3,4). Furthermore, the \"black-box\" nature of DL models raises concerns about trustworthiness in clinical settings (5-7).\nTo enhance model explainability, techniques like saliency maps and Gradient-weighted Class Activation Mapping (Grad-CAM) (8) have been used to visualize network attention and provide insights into model decisions. However, these methods often lack accuracy, especially in 3D medical imaging (9,10). Transformers offer an alternative through their inherent attention mechanisms (11,12). Unlike convolutional models requiring post-hoc visualization, Transformers provide attention matrices highlighting the most relevant input features. Studies have shown that attention-based saliency maps improve interpretability in 2D medical imaging (13-15).\nTo address the scarcity of large annotated datasets, self-supervised learning combines labeled and unlabeled data to enhance model performance (16). Foundation models like DINOv2 (17) use self- supervised learning to extract features from vast amounts of unlabeled data. Despite being trained on natural images, these models have shown promise in transferring features to medical imaging tasks (18-24).\nFor instance, Huix et al. (18) found that DINOv2 outperformed other foundation models and even in-domain self-supervised ResNet152 in tasks like fundoscopy, mammography, dermoscopy, and chest radiography. Similarly, Truong et al. (23) and Nielsen et al. (24) demonstrated DINOv2's effectiveness on microscopic and endoscopic images. Huang et al. (19) reported that DINOv2 generally outperformed VGG and ResNet50 models pre-trained on ImageNet for chest radiographic, funduscopic, and dermoscopic images but not for brain MRI slices.\nDespite these successes in 2D medical image analysis, the application of DINOv2 to 3D medical imaging remains unexplored. The present study aims to bridge this gap by introducing the Medical Slice Transformer (MST), a novel approach that adapts 2D self-supervised models like DINOv2 for 3D medical image analysis. We hypothesized that the MST framework would outperform standard 3D convolutional neural networks (such as ResNet) in terms of diagnostic accuracy across a set of clinical cross-sectional CT and MRI datasets. Furthermore, we hypothesized that MST would provide better localization information for identifying specific image findings, such as breast lesions, lung nodules, and meniscus tears, compared to ResNet.\nThe primary contributions of this study are as follows:\n\u2022 Development of MST: We introduce MST, a technique designed to use self-supervised models like DINOv2 for 3D medical image analysis (Figure 1).\n\u2022 Enhanced Explainability: By leveraging the Transformer's attention mechanism, we demonstrate how model explainability can be improved, providing more insight into the decision-making process of DL models in 3D medical imaging contexts."}, {"title": "Results", "content": "Patient Characteristics\nThree publicly available datasets were included: (i) breast MRI \u201cDuke-Breast-Cancer-MRI\" (DUKE) (25), (ii) chest CT \"Lung Image Database Consortium and Image Database Resource Initiative\" (LIDC- IDRI) (26), and (iii) knee MRI \u201cMRNet\u201d (27). Inclusion criteria were met by 651, 722, and 1199 patients for the breast MRI, chest CT, and knee MRI datasets, respectively (Figure 1c). The DUKE dataset comprised contrast-enhanced breast MR subtraction images from cancer patients and was used to evaluate the models' performance in breast cancer detection. The LIDC-IDRI dataset included chest CT scans with lung nodule annotations by four radiologists, allowing assessment of the models' ability to detect and analyze the dignity of lung nodules. The MRNet dataset contained knee MRI studies with meniscal tears, enabling evaluation of the models' effectiveness in diagnosing meniscal tears."}, {"title": "MST Outperforms Standard CNN Architectures", "content": "We compared the performance of a 3D ResNet50 (28) with the proposed MST architecture using DINOv2 as the image encoder by calculating the Area Under the Receiver Operating Characteristic Curve (AUC) for each dataset (Figure 2). MST exhibited higher AUC values compared to the 3D ResNet on all datasets: breast MRI: 0.94 \u00b1 0.01 vs. 0.91 \u00b1 0.02 (P = 0.02), chest CT: 0.95 \u00b1 0.01 vs. 0.92 \u00b1 0.02 (P = 0.13), and knee MRI: 0.85 \u00b1 0.04 vs. 0.69 \u00b1 0.05 (P = 0.001).\nWhen systematically varying the reference model's complexity, we found that ResNet18 or ResNet101 did not improve performance over ResNet50 (Supplementary Table S1). ResNet18, ResNet50, and ResNet101 have 33 million, 46 million, and 117 million parameters, respectively. In contrast, the MST model has 23 million parameters, with 1 million parameters belonging to the Transformer part and 22 million to the DINOv2 part."}, {"title": "MST Provides Better Model Explainability", "content": "A major challenge with deep learning models in medical imaging is their lack of transparency, often called the \"black-box\" problem. For reference, we included the traditional Gradient-weighted Class Activation Mapping (Grad-CAM) to visualize the attention of the ResNet model.\nTo complement the quantitative analysis, we evaluated the saliency maps provided by both methods. A radiologist (R.S.) with five years of clinical experience performed a blinded evaluation of 50 saliency maps from each dataset, rating the models based on two key criteria (Figure 3):\n(i) Slice correctness \u2013 does the saliency map highlight the slice(s) containing the lesion?\n(ii) Lesion correctness - does the saliency map accurately point to the core of the lesion?"}, {"title": "Ablation Experiments", "content": "We conducted ablation studies to analyze the optimal architecture for the Medical Slice Transformer (MST) framework. First, we evaluated the importance of the Transformer for aggregating slices on the diagnostic performance. Second, we explored different architectures and associated parameters for the encoder."}, {"title": "Impact of Slice Transformer", "content": "To assess the impact of the Transformer in the MST framework, we analyzed the performance when replacing the Transformer with a linear layer to aggregate the information of all slices. Additionally, we tested the performance when averaging the feature vectors. To assess the impact of positional encoding, we compared the performance of MST models with a variant that uses additive positional encoding. Specifically, each slice was assigned a positional embedding based on its slice number, enabling the model to capture spatial dependencies. For all tests, we used DINOv2 as the backbone.\nThe Transformer model without positional embedding achieved the highest performance across all three datasets (Table 2). Replacing the Transformer with a linear layer or averaging the features resulted in consistently lower AUC values across all datasets."}, {"title": "Impact of Backbone Architecture", "content": "To further examine the influence of the feature extractor on MST's performance, we conducted experiments with different backbone architectures. We evaluated the effect of increasing the model size using a larger DINOv2 variant (\"base\") with 86 million parameters instead of 21 million. We also tested augmenting DINOv2 with register tokens (29) to study if this architectural modification would enhance feature representation and overall performance. Additionally, we assessed the impact of freezing the weights of the DINOv2 backbone during training to determine the necessity of fine- tuning. Lastly, we replaced DINOv2 with a pretrained 2D ResNet to compare self-supervised learning features with those obtained from supervised ImageNet pretraining.\nThe results are summarized in Table 3. The larger DINOv2 variant performed comparable to the smaller DINOv2. Incorporating registers into DINOv2 did not yield improvements. Freezing DINOv2 weights led to a significant decrease in AUC values on breast MRI and chest CT (reduced to 0.62 \u00b1 0.03 and 0.66 \u00b1 0.03, respectively), highlighting the necessity of fine-tuning. Replacing DINOv2 with a pretrained ResNet decreased AUC values across all datasets, with a significant drop on the chest CT dataset (0.92 \u00b1 0.02 vs. 0.95 \u00b1 0.01; P = 0.01)."}, {"title": "Discussion", "content": "This study addresses two pivotal challenges in applying deep learning to medical imaging: enhancing diagnostic accuracy and improving model explainability through more accurate saliency maps. By introducing the MST framework, which adapts 2D self-supervised models like DINOv2 to 3D medical images, we demonstrate significant advancements in both areas.\nOur MST model achieved superior diagnostic performance across diverse datasets\u2014including breast MRI, chest CT, and knee MRI\u2014evidenced by higher AUC values compared to conventional 3D ResNet architectures. This finding underscores the efficacy of leveraging 2D self-supervised models for 3D medical imaging tasks, extending the benefits observed in previous studies focused on 2D modalities (30-32). Previous studies have demonstrated the effectiveness of slice-wise feature extraction using Transformer architectures for 3D medical image analysis, primarily in brain MRI. However, these efforts did not explore using DINOv2 as a self-supervised feature extraction backbone. Our study demonstrates the effectiveness of DINOv2 across a range of modalities, including breast MRI, knee MRI, and chest CT.\nAt least of equal importance is the substantial improvement in model explainability. The attention mechanisms inherent in the Transformer architecture provided more precise and informative saliency maps than traditional Grad-CAM methods used with convolutional neural networks. This enhanced localization of pathologies aligns with findings from prior research indicating that Transformer-based methods yield more accurate visual explanations in medical imaging (15,33,34). This improvement is important since improved saliency maps aid in understanding the model's decision-making process and increase the trustworthiness of deep learning models in clinical settings (5-7).\nHowever, limitations need to be acknowledged. The computational demands of processing high- resolution 3D imaging data pose practical challenges: CT scans can have hundreds of slices and spatial resolutions of 512x512 pixels or higher, exceeding current GPU memory capacities. This constraint necessitates downsampling, which may result in the loss of critical diagnostic information. The current study focused on single MRI sequences (i.e., subtraction images and T2-weighted fat- saturated sequences) and unenhanced CT images for this proof-of-concept in medical imaging analysis. In the clinic, multiple contrast phases and reconstructions are used for CT evaluations, and multiple sequences and orientations are used for MRI. Current technical limitations that narrow the input data stream (i.e., the number of input image datasets) may limit the model's ability to fully capture the complex anatomic and pathologic information. Future research is needed to explore the integration of different imaging modalities to enhance model performance and robustness.\nIn conclusion, adapting 2D self-supervised models like DINOv2 to 3D medical imaging enhances diagnostic accuracy and significantly improves model explainability through better saliency maps. These advancements may provide a cornerstone for overcoming key barriers in deploying deep learning models in clinical practice. Future research should focus on refining these methods and integrating multi-sequence and multi-phase cross-sectional imaging data as well as multimodal data to enhance diagnostic capabilities further."}, {"title": "Methods", "content": "Dataset Collection and Preprocessing\nWe used three publicly available datasets to train and test our approach (Table 4)."}]}