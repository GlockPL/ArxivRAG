{"title": "Tacit Learning with Adaptive Information Selection for Cooperative Multi-Agent Reinforcement Learning", "authors": ["Lunjun Liu", "Weilai Jiang", "Yaonan Wang"], "abstract": "In multi-agent reinforcement learning (MARL), the centralized train- ing with decentralized execution (CTDE) framework has gained widespread adoption due to its strong performance. However, the further development of CTDE faces two key challenges. First, agents struggle to autonomously assess the relevance of input informa- tion for cooperative tasks, impairing their decision-making abilities. Second, in communication-limited scenarios with partial observ- ability, agents are unable to access global information, restricting their ability to collaborate effectively from a global perspective. To address these challenges, we introduce a novel cooperative MARL framework based on information selection and tacit learning. In this framework, agents gradually develop implicit coordination during training, enabling them to infer the cooperative behavior of others in a discrete space without communication, relying solely on local information. Moreover, we integrate gating and selection mecha- nisms, allowing agents to adaptively filter information based on environmental changes, thereby enhancing their decision-making capabilities. Experiments on popular MARL benchmarks show that our framework can be seamlessly integrated with state-of-the-art algorithms, leading to significant performance improvements.", "sections": [{"title": "1 INTRODUCTION", "content": "Cooperative Multi-Agent Reinforcement Learning (MARL) has emerged as a robust framework for addressing practical challenges across various domains, including autonomous driving [44], gam- ing [1], swarm robotics [23, 24], and smart grids [26, 27, 38]. Despite its success, learning complex cooperative strategies remains a major challenge. Firstly, neglecting the influence of other agents on the system introduces non-stationarity from the perspective of an individual, potentially leading to environmental instability. Addi- tionally, as the number of agents increases, the observation space for joint actions expands exponentially, which may impede the learn- ing process. To effectively address these challenges, the approach of Centralized Training and Decentralized Execution (CTDE) has been proposed and gained popularity in MARL. CTDE utilizes global information during training while achieving decentralized decision- making based on local information. It serves as the foundation for several prominent methods, including VDN [32], MADDPG [22], QMIX [28], and COMA [8].\nDespite making progress, CTDE methods still face two major challenges. Firstly, agents in CTDE often rely on specific informa- tion for decision-making, and an excessive amount of information may overwhelm them. Secondly, many existing CTDE methods assume mutual independence among agents during training, only incorporating global information in the mixing network. This ap- proach overlooks the importance of information sharing among agents [39], thus hindering the establishment of cooperation. There- fore, addressing how agents handle and share information in MARL remains a pressing challenge.\nVarious explicit communication methods have been proposed to address the challenge of information sharing in multi-agent systems, including CommNet [31], BicNet [25], and NDQ [40]. To reduce the delays and costs associated with these communication methods, some approaches have shifted toward implicit commu- nication frameworks, which aim to simulate communication by promoting mutual understanding among agents [20, 45]. However, in these implicit methods, agents often depend too heavily on oth- ers to evaluate information, rather than autonomously assessing its importance, effectively delegating decision-making to other agents. A potential solution to this issue is to allow agents to communicate during the training phase while relying solely on local observations during decision-making.\nConstrained by cognitive limitations and individual perspec- tives, humans exhibit selectivity when receiving information. They process this information based on their knowledge and past experi- ences, selecting the most relevant details for the present moment. In collaborative settings, individuals often develop a tacit understand- ing through specific training, enabling them to accurately predict and comprehend their peers' intentions without explicit communi- cation. Inspired by human information processing and cooperation patterns, we propose a novel framework called Selective Implicit Collaboration Algorithm (SICA) for multi-agent systems. SICA is built upon the QMIX framework and can be extended to various methods based on CTDE paradigm. The framework comprises three key blocks: the Selection Block, the Communication Block, and the"}, {"title": "2 RELATED WORK", "content": "CTDE Framework The Centralized Training and Decentralized Execution (CTDE) framework presents a novel MARL approach aimed at addressing the scalability challenges of centralized learn- ing [3] and the environmental non-stationarity of decentralized learning [33]. This framework operates by training agents based on global information while making decisions based on local infor- mation. Two prominent works based on this framework are VDN [32] and QMIX [28], which introduce value decomposition meth- ods. VDN aggregates the value functions of individual agents by directly summing them to obtain the joint action function, while QMIX further optimizes this process by ensuring the Individual- Global Maximum (IGM) condition through monotonicity. More recent methods, such as QPLEX [37], introduce double adversar- ial networks to overcome representation limitations. Additionally, CTDE encompasses a series of centralized critic methods, including MADDPG [22], COMA [8], FOP [43], etc., which do not impose any restrictions on the representation of the joint action-value function.\nContinous Communication CTDE methods typically in- corporate global information solely in the mixing network, with agents limited to observing only local information. To augment the decision-making capabilities of agents, numerous studies have adopted communication methods to introduce global information to the agents. One pioneering work in this domain is RIAL with DIAL [7], which aims to facilitate communication in CTDE through predefined topological structures. Meanwhile, CommNet was in- troduced as the first differentiable framework in MARL [31]. Subse- quent works, such as ATOC [16], I2C [5]), and DHCG [31], utilize state-dependent communication graphs to address these challenges. ATOC and I2C also introduce gating mechanisms to regulate com- munication links between agents. Recently, novel communication methods such as TarMAC [4] and NDQ [40]have been proposed. TarMAC employs multi-layer attention modules for multi-round communication to strengthen connections between agents. NDQ aims to reduce communication overhead by learning nearly decom- posable value functions.\nCommunication-Free Execution In certain scenarios, fac- tors such as delays and costs can impede the practical deployment of communication methods. To address this challenge, some ap- proaches permit communication only during the training phase to establish cooperative patterns among agents, while prohibiting communication during the execution phase. Previous work intro- duced a multi-agent framework that utilizes actions as a form of implicit communication, successfully applying it to various robotic tasks [17]. Building on this concept, researchers developed the PBL framework [34], which fosters implicit communication through actions and introduces a secondary reward to incentivize this be- havior. Furthermore, PBL incorporates a social influence reward specifically for Sequential Social Dilemma (SSD) multi-agent en- vironments [15], enabling agents to cooperate more effectively in SSD scenarios.\nOur Works While the aforementioned communication-free execution methods rely on gradient flow to facilitate coopera- tion-thereby increasing model complexity-our approach shares similarities with COLA [42] and TACO [20]. Both methods con- vey information through specific communication protocols and gradually transition to a decentralized framework. Additionally, to address the challenge of agent information filtering, we designed an adaptive selection mechanism, which effectively enhances the agents' information processing capabilities."}, {"title": "3 PRELIMINARY", "content": "Problem Formulation In a multi-agent cooperative environ- ment, our work adheres to the definition of the Decentralized Par- tially Observable Markov Decision Process (Dec-POMDP) [2], de- noted as $(N, S, U, \\mathcal{O}, \\Omega, P, r, \\gamma)$. Here, N represents the set of n agents, and S is the global state space of the environment. The joint action space $u = \\{U_1, U_2, ..., U_n\\} \\in U = U^n$ consists of the indepen- dent actions of each agent, where $\\gamma$ represents the discounted factor. In the Partially Observable Markov Decision Process (POMDP), agents lack access to global information. Consequently, at each time step t, each agent i can only observe that $o_i \\in \\mathcal{O}$ based on the observation function $\\Omega(s, u): S \\times U \\rightarrow \\mathcal{O}$. Agents then select an action $u_i$ according to the policy $\\pi(u_i|o_i)$. Subsequently, the state transition function $P(s'|s, u)$ is updated to the next state $s'$. Agents collectively receive global rewards according to a reward function r(s, u) and share them. The objective for all agents is to find an optimal joint policy $\\pi^*$ to maximize the global expected re- turn. Our aim is to achieve this goal by designing effective implicit communication between the agents.\nThe S6 Layer Given an input scalar x(t), we consider a continuous-time invariant State Space Model (SSM) defined by the following first-order differential equation:\n$h(t) = Ah(t)_{k-1} + Bx(t)_k$, $y(t) = Ch(t)_{k-1} + Dx(t)_k$ (1)\nHere, x(t) is an input function mapped to produce the output y(t). Previous research has shown that the SSM can capture remote dependencies by initializing matrix A using the HIPPO matrix [11]. Similarly, D is interpreted as a parameter-based skip connection and set to 0, following prior work [12, 13]. Additionally, as the SSM operates on continuous sequences, it is discretized using the"}, {"title": "4 SICA", "content": "In this section, we delve into the core details of SICA, outlining its adaptive information selection mechanism, communication strat- egy, and the transition from explicit to implicit communication. We then elucidate the overarching objectives of the agents and the learning process. Notably, our proposed method operates within the agent network and can be seamlessly integrated with any value decompostition method.\n4.1 Adaptive information selection\nIn cooperative multi-agent tasks, a crucial aspect is the agents' capacity to autonomously filter input information. This involves selecting the most relevant information for ongoing cooperation, guided by experience and task requisites, while minimizing atten- tion to irrelevant data. Currently, there is no CTDE method that equips agents with such discernment. To address this, we propose an adaptive information selection mechanism for agents. This mech- anism operates by integrating historical information at each time step, giving more detail to data closer to the current time step while abstracting information that is further away. Subsequently, based on the prevailing cooperation demands, agents discern effective information and filter out irrelevant data. For instance, Figure 1 illustrates this selection mechanism in action within the SMAC environment, where two Marines and one Medivac from the red team engage a blue Medivac. Considering the red Medivac, given the proximity of both factions, it disregards distance information. Additionally, with no units generating shields on the field, the red Medivac also discounts shield information.\n4.2 SICA Framework\nTo address the mentioned challenges and facilitate framework ex- tension, SICA adopts the classic QMIX [28] framework. This frame- work comprises a mixing network and agent networks. However, unlike QMIX, SICA does not presume agent independence during training. SICA primarily enhances the agent network, as depicted in the overall framework in Figure 2 left. This framework encompasses the Selection Block, Communication Block, and Regeneration Block, as delineated in Figure 2 middle. The Selection Block empowers the framework with information selection capabilities. Agents can dy- namically choose desired input information, as elucidated in Figure 2 right.\nSelection Block The Selection Block consists of two MLPs and an S6 layer [10]. Since the time intervals of the inputs in the selective tasks are variable, a time-varying model is required, so we integrated the S6 layer into the framework. The two MLPs"}, {"title": "4.3 Learning Objective", "content": "The overall learning objective of our method is divided into two parts: the TD loss function, which constitutes the end-to-end opti- mal value decomposition, and the minimization of the regeneration information error.\nThe TD loss function part is the same as in QMIX in Equation 5 and has the following form:\n$\\mathcal{L}_{TD}(\\theta) = \\mathbb{E}_{D}[(r + \\gamma \\max_{u'} Q'_{tot} (\\tau', u', s'; \\theta^{-}) - Q_{tot} (\\tau, u, s; \\theta))^{2}]$ (11)\nHere, $\\theta$ represents the learnable parameters, $\\theta^-$ are the parame- ters of a target network as in DQN, $\\mathbb{E}[\\cdot]$ denotes the expectation function, and $\\mathcal{D}$ represents the replay buffer of transitions. The part of minimizing the regenerated information error is to ensure that the regenerated information $\\hat{v}_i$ closely approximates the true information $v_i$, facilitating the smooth transition of SICA from a centralized architecture to a decentralized one. To achieve this objective, we introduce an auxiliary loss function called alignment loss function $\\mathcal{L}_{Align}$, formulated as follows:\n$\\mathcal{L}_{Align}(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[(\\hat{v}_{i}(h_{i};\\theta^{-}) - v_{i}(h;\\theta))^{2}]$ (12)\nWhere $\\mathbb{E}[\\cdot]$ denotes the expectation function and n denotes the number of agents.\nThese two parts are trained simultaneously, but with a gradual increase in the importance of the alignment loss function $\\mathcal{L}_{Align}$ to ensure the performance of the architecture. The total loss function can be expressed as follows:\n$\\mathcal{L}_{tot}(t, u, s, h_{i}, h; \\theta) = \\mathcal{L}_{TD} + \\sigma(t)\\mathcal{L}_{Align}$ (13)\nt denotes a time step, and $\\sigma(t)$ is a threshold function defined as follows:\n$\\sigma(t) = \\begin{cases} \\beta_{1} & t \\leq T \\\\ \\beta_{2} & t > T \\end{cases}$ (14)\nT, $\\beta_{1}$ and $\\beta_{2}$ are hyperparameters, and they are fixed values that need to be satisfied, i.e., the proportion of reconstruction loss in- creases as the information increases."}, {"title": "5 EXPERIMENTS", "content": "In this section, we performed a series of experiments to determine whether 1. SICA outperforms traditional CTDE methods 2. SICA outperforms or rivals explicit communication methods. 3. SICA outperforms other methods as the volume of information increases."}, {"title": "5.1 Performance on Multi-Agent Benchmarks", "content": "In this subsection, our goal is to address questions 1 and 2 by assess- ing SICA's performance across widely-used MARL benchmarks. SMAC We initiate our evaluation by assessing SICA's perfor- mance in the SMAC, where our aim is to control a team of allied units against an enemy team governed by built-in policies. Vic- tory is achieved by eliminating all enemy units within a chapter's time limit. Our metric for evaluation is the alliance team's win rate, which we aim to maximize. We compare SICA against sev- eral robust baselines grounded in the CTDE framework. These include clasSICAl methods like VDN, QMIX, and QTRAN [30], as well as communication-based approaches like QMIX-Attention [14] and NDQ [41]. Given that most methods perform adequately in easy maps, we focus on evaluating their performance in more challenging environments, particularly two hard maps (5m_vs_6m, 2c_vs_64zg) and four super hard maps (3s5z_vs_3s6z, MMM2, 27m_ vs_30m, Corridor) to offer a comprehensive assessment of SICA's capabilities.\nThe median win rates across different maps are depicted in Fig- ure 3. SICA consistently outperforms the baselines across all maps, even surpassing explicit communication methods. This underscores the effectiveness of SICA in information processing and highlights the robustness of its information regeneration capability. Across all methods, there is a noticeable decline in win rates as we transi- tion from hard to super hard maps, which aligns with expectations given the heightened complexity of the latter scenarios. It's worth mentioning that QTRAN and NDQ exhibit suboptimal performance. QTRAN exhibits suboptimal performance across all maps, poten- tially attributable to challenges in credit assignment resulting in the development of passive agents. Meanwhile, NDQ demonstrates efficacy solely on select maps, potentially stemming from instability in its message passing methodology.\nSMACv2 Next, we evaluate SICA on SMACv2. SMACv2 is an enhanced version of SMAC. In SMACv2, each agent is equipped with randomly generated unit types and initial positions, introduc- ing unpredictability into the environment. These unit types are generated according to a fixed probability distribution, with agents unaware of their own unit types, necessitating adaptable strategies capable of addressing all potential unit types. Figure 4 presents a comparative analysis between SICA and baseline methods on three randomly generated maps for the Protoss, Terran, and Zerg races. Remarkably, SICA consistently outperformed the baselines, even in highly stochastic SMACv2 environments. This finding underscores the effectiveness of SICA in task completion, highlighting the ro- bustness of its selection mechanism and information regeneration capabilities.\nGRF Finally, we evaluated SICA's performance on GRF, a MARL benchmark based on the open-source game Gameplay Foot- ball. In this environment, agents collaborate to orchestrate attacks, with rewards solely granted upon goal scoring. In Table 1, we se- lected the explicit communication method QMIX-Attention and the current state-of-the-art method CDS-QMIX [19] as baselines. We then compared SICA with these baselines across the two most challenging scenarios. The agents were trained for 10 million steps using 8 threads in all scenarios. The results demonstrate that SICA significantly outperformed the other methods, thereby highlighting its efficacy in diverse environments."}, {"title": "5.2 Ablation Studies", "content": "In this subsection, we will conduct ablation studies. Three sets of experiments were conducted to address questions 3, 4, and 5, respectively. All experiments were performed in the challenging SMACv2 scenario.\nDifferent numbers of agents Given SICA's ability to en- capsulate historical information, we hypothesized that it could effectively handle larger volumes of data, prompting question 3. To answer this question, we tested scenarios with different numbers of agents. We compared SICA with the baseline, QMIX-Attention, the best-performing explicit communication method identified in earlier experiments. As illustrated in Figure 5, SICA consistently outperformed the baseline across all scenarios, highlighting its effectiveness in handling larger-scale multi-agent environments.\nReplace Selection Block To address question 4, we intro- duce a variant of SICA called ICA. In this variant, the Selection is replaced by a gated structure consisting of a MLP Block and a GRU Cell. The results of SICA and ICA in protoss_5_vs_5 and terran_5_vs_5 scenarios are shown in Figure 6. We observe that the performance of SICA markedly surpassed that of ICA. This suggests that the Selection Block consistently enhances the agents' ability and underscores the significance of information filtering in multi-agent tasks.\nFixed a Does SICA really need progressive information regen- eration? To answer question 5, we set up two scenarios: one where $\\alpha(t)$ is fixed at 0, disregarding $\\mathcal{L}_{Align}$, which we call SICA-ZERO; and another where $\\alpha(t)$ is fixed at 1 and suddenly switches to 0 near the end of the training, which we call SICA-ONE. For SICA- ZERO, the communication module does not participate, making SICA equivalent to a traditional CTDE method, relying on local information for training. For SICA-ONE, the training process fully depends on communication, forcing the agents to learn complex information directly within a short time. The comparison between these two scenarios and SICA is shown in Figure 7. Regardless of the value of $\\alpha(t)$, SICA consistently performs better. Therefore, progressive information regeneration is necessary as it guides the learning process and enhances the agents' capabilities."}, {"title": "5.3 More Studies", "content": "Applying SICA to VDN To demonstrate the versatility of SICA as a plug-and-play framework, we apply SICA to VDN, referred to as SICA-VDN, in this section. The comparison curve between SICA-VDN and VDN is illustrated in Figure 8, showing that SICA markedly improves the performance of the baseline VDN.\nComparing SICA with another communication-free ex- ecution method. In this section, we compare SICA with an- other communication-free execution baseline, QMIX-CADP [45],on SMACv2. QMIX-CADP adopts direct pruning to transition the framework into a decentralized framework. In Figure 9, we can observe that SICA's performance is significantly better than that of QMIX-CADP, indicating that a gradual framework transition is more reasonable than a direct transition."}, {"title": "Visualization", "content": "To verify whether the Regeneration Block can truly approximate the true information, we visualize the true in- formation and the regenerated information from the Protoss and Terran maps using t-SNE [35] compression into two-dimensional embeddings. In this section, we adopt the 3_vs_5 mode with higher difficulty and randomness to enhance the difficulty of regeneration. As shown in Figure 10, although some bias is present in both scenar- ios, the regenerated information can reflect the distribution of the true information effectively. Therefore, the regeneration process is effective.\nScalability We combined SICA with the actor-critic method MADDPG and compared it to the traditional MADDPG algorithm in the Predator - Prey, Navigation, and Pantomime environments. Table 2 presents the experimental results after 2 million training steps, demonstrating that SICA continues to enhance the overall performance of the framework."}, {"title": "6 LIMITATIONS", "content": "In this section, we examine three potential limitations of SICA. First, the Regeneration Block may encounter difficulties when there is no correlation between the trajectories or observations of the agents, as it relies on similar historical information for regeneration. Although we mitigate this limitation by incorporating observations from other agents into a mini buffer, there may still be instances where those agents fail to utilize this portion of historical data. Second, while we are confident in SICA's performance in large- scale multi-agent environments, our experiments are constrained by computational resources, necessitating further evaluation of its effectiveness in larger-scale scenarios. Thirdly, when applying SICA to new tasks, adjustments to multiple hyperparameters will be necessary. Finally, we implemented parameter sharing among the agents to accelerate the training process, as not using this technique would significantly increase the training time."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduced a novel MARL architecture named SICA, designed to enhance agents' information handling capa- bilities and improve the framework's generality. By integrating information selection with communication mechanisms, SICA em- powers agents to autonomously choose relevant information while incorporating information from other agents. To accommodate to communication-limited environments, SICA gradually learns the tacit understanding between agents, eventually transitioning to a fully decentralized framework. Experimental results illustrate SICA's effectiveness in regenerating global information and signif- icantly enhancing performance in challenging multi-agent tasks through information selection. In future work, we aim to seamlessly extend the framework to encompass various CTDE methods."}]}