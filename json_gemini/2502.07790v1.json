{"title": "Can Generative AI be Egalitarian?", "authors": ["Philip Feldman", "James R. Foulds", "Shimei Pan"], "abstract": "The recent explosion of \"foundation\" generative AI models has been built upon the extensive extraction of value from online sources, often without corresponding reciprocation. This pattern mirrors and intensifies the extractive practices of surveillance capitalism [46], while the potential for enormous profit has challenged technology organizations' commitments to responsible AI practices, raising significant ethical and societal concerns. However, a promising alternative is emerging: the development of models that rely on content willingly and collaboratively provided by users. This article explores this \u201cegalitarian\u201d approach to generative AI, taking inspiration from the successful model of Wikipedia. We explore the potential implications of this approach for the design, development, and constraints of future foundation models. We argue that such an approach is not only ethically sound but may also lead to models that are more responsive to user needs, more diverse in their training data, and ultimately more aligned with societal values. Furthermore, we explore potential challenges and limitations of this approach, including issues of scalability, quality control, and potential biases inherent in volunteer-contributed content.", "sections": [{"title": "1 Introduction", "content": "Generative AI technologies such as AI chatbots are poised to deeply impact many facets of daily life. These systems are now capable of creating realistic content and answering sophisticated user queries, and are controlled by powerful technology organizations.\nIt was not always clear that this would happen, and the trajectory of the AI field to reach this point raises concerns about the future. The field of Artificial Intelligence (AI) has experienced a series of booms and busts since its inception in 1956. Early optimism and ambitious predictions led to significant investment and research, but the inability to meet those lofty goals resulted in the first \"AI winter\" in the mid-1970s [32]. A resurgence of interest in the 1980s, fueled by projects like Japan's Fifth Generation Computer Project [17],\nled to further advancements but again fell short of expectations, leading to a second AI winter around 1987.\nAlongside advances in machine learning such as support vector machines and statistical learning, the early 1990s saw the early stages of a resurgence of neural network technologies. Later rebranded as \u201cdeep learning,\" advances in GPU-based training, improvements in models and algorithms, and especially, larger datasets, eventually led to their dominant status from the mid-late 2010s to today. [12]. These breakthroughs in what has become known as \"generative AI\" has developed into a third wave of investment and research.\nThroughout these cycles, the funding landscape for AI has also evolved, shifting from reliance on bureaucratic decision-makers in Washington, D.C., and other government entities to the venture capitalists and startups of Silicon Valley, Beijing, London, and Tel Aviv, among others. This shift is not merely geographic; it represents a fundamental change in how machine intelligence is conceived, developed, and deployed.\nThe US Department of Defense (DoD)'s interests have historically been tied to fulfilling national security needs. Whether the goal was self-driving military vehicles or automated threat detection systems, the emphasis was on trust and reliability. However, the venture capital model, fueled by the desire for rapid and profitable exits, operates with a different ethos. This is particularly evident in Silicon Valley, where a form of arbitrage has become commonplace. In this model, technologies developed with federal subsidies, often originating from defense research, are used to disrupt regulated industries, generating substantial returns for investors [16].\nThe current wave of AI development marks a further evolution in this dynamic. The goals are no longer solely technological, but increasingly social and commercial. The focus is on creating systems that can be monetized on a massive scale, with the global population serving as both resource and customer. This shift is facilitated by the vast computational infrastructure that has grown alongside the internet, and the enormous troves of data that can be extracted from it, often without regard for copyright or consent."}, {"title": "2 Open AI: A Case Study of a Non-Profit Becoming For-Profit", "content": "The tension between profit motivations and altruistic motivations for generative AI systems has been laid bare in OpenAI's public and ongoing struggle with balancing these goals. OpenAI was founded in December 2015 as a non-profit research company. In its initial press-release, it stated \u201cOur goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. Since our research is free from financial obligations, we can better focus on a positive human impact\u201d [33].\nThis egalitarian non-profit focus allowed the company to hire a number of top AI researchers at its inception, including renowned AI scientist Ilya Sutskever, who became a co-founder of OpenAI and its chief scientist. However, the company's non-profit status was significantly eroded as early as March 2019, when it announced the formation of OpenAI LP, a for-profit subsidiary operated under the direction of its non-profit parent, now called \u201cOpenAI Nonprofit\u201d [33]. The for-profit company is legally obligated to pursue OpenAI Nonprofit's mission. OpenAI LP is a \"capped-profit\" company, meaning that returns for investors are limited, in this case being capped at 100 times their investment. This unusual structure aims to strike a balance between profit and egalitarianism, and aims to attract the investment and talent that OpenAI claims are necessary to achieve its goals [33]. It has received criticism, however. An article from the Observer points out that \u201cOpenAI's profit cap is so high that it might as well not exist\u201d [37].\nOpenAI entered into a strategic partnership with Microsoft shortly after forming its for-profit subsidiary in 2019 [33], and as of March 2024, Microsoft held a 49% ownership stake and rights to up to 75% of OpenAI's\nprofits until it recoups its investment in the company [34]. The partnership also allows Microsoft to leverage OpenAI's technologies into its products, which it has done with aplomb, e.g., its Azure OpenAI service which serves API access to OpenAI's models using Microsoft's cloud infrastructure [31]. In 2024, entrepreneur and former OpenAI board member Elon Musk sued OpenAI and its CEO Sam Altman over breach of contract, alleging that they have abandoned the altruistic, charitable goals that their founding agreement requires in favor of seeking profits, specifically for its biggest investor, Microsoft [35]. Microsoft's investment in OpenAI has also brought regulatory scrutiny, with the European Commission, the executive arm of the European Union stating it will be \"looking into\" whether it requires review under the EU Merger Regulation [44].\nThe brief ouster [33], and subsequent reinstatement, of OpenAI's CEO Sam Altman in November 2023 exposed fault lines within the company regarding the profit vs. egalitarianism debate [2, 29]. It was reported at the time that OpenAI board members, including Dr. Sutskever, had raised concerns about Altman's push toward rapid commercialization of OpenAI's technologies despite risks and safety concerns [2]. An anonymous letter from former OpenAI employees alleged that \"a significant number of OpenAI employees were pushed out of the company to facilitate its transition to a for-profit model\" [3]. Altman's job was saved when 91% of OpenAI's 770 employees threatened to leave the company unless he was reinstated [2]. Microsoft's CEO, Satya Nadella, applauded the move [29], which apparently indicated that the company approved of Altman and his priorities. Sutskever stepped down from the board after Altman's return, did not return to the office, and left the company in May 2024 [36]. Sutskever had co-led OpenAI's \"Superalignment\" team, aiming to ensure that artificial superintelligence technologies developed by the company were aligned with the public benefit, and the new startup company he founded, Safe Superintelligence Inc., has the same goal [11]. The other co-lead of the Superalignment team, Jan Leike, departed at the same time, which was attributed to growing concerns around OpenAI's commitment to AI safety [36].\nIn September 2024, OpenAI's leadership exodus continued, with OpenAI's chief technology officer, Mira Murati, who briefly led the company in Sam Altman's absence, and two other top executives stepping down; insiders associated this with the company's moves toward becoming a for-profit company [22]. As of that event, nine of the company's eleven founders had stepped down or taken an extended sabbatical, with only Sam Altman and one other founding executive remaining [38]. One week later, OpenAI completed a fundraising deal that valued the company at $157 billion USD. According the New York Times, the deal included stipulations that the company restructure itself into a for-profit business within two years, or the funding will be converted to debt [28]. At the time of writing in late 2024, therefore, OpenAI's full transition into a for-profit company appears to be a near certainty. It was reported by the Financial Times that OpenAI is considering restructuring as a public benefit corporation (PBC), a relatively new type of for-profit company which several of its rivals, including Anthropic and xAI, have also adopted, and which would help protect against hostile takeovers by investors. If it became a PBC, OpenAI would be required to balance the interests of shareholders with positive societal impacts, and the interests of other stakeholders including employees and society at large. Regarding the PBC plan, in a guest opinion essay for the New York Times, Andrew Kassoy, co-founder of the egalitarian non-profit network B Lab, wrote \"That's a good start. But this structure alone does not ensure that OpenAI will be held accountable\" [25]. To summarize the sequence of events:\n1. OpenAI's egalitarian mission and non-profit status was useful in the early recruitment of top AI talent;\n2. Non-profit egalitarianism has increasingly been sidelined in favor of for-profit goals;\n3. The company has not been able to reconcile egalitarianism and the pursuit of profit, with most of those who advocated for the former either being forced out or quitting the company as it continues its slide toward a full for-profit status;\n4. Many of the employees who left or were forced out of the company, as well as outside experts, felt that its increasing for-profit direction had compromised its egalitarian goals such as safety and alignment with human values.\nThis suggests an apparent incompatibility between OpenAI's egalitarian and for-profit goals. Arguably, this bodes poorly for the future of egalitarian ideals in other for-profit generative AI companies as well. In an opinion article in the LA Times, two MIT professors said that Sam Altman's reinstatement \"confirms that the future of AI is firmly in the hands of people focused on speed and profits, at the expense of all else."}, {"title": "3 Google: A Case Study of Responsible AI in a For-Profit", "content": "The conflict between profit-making and the public benefit has also arisen in other companies. At Google, for example, efforts within the company to ensure ethical and responsible AI principles have led to dramatic clashes with senior management. There is growing awareness that AI systems, especially powerful modern generative AI models, can lead to substantial harm if not designed with care, necessitating responsible AI approaches. Some of the core responsible AI principles include fairness (ensuring that AI systems do not encode discriminatory bias or other unjust behavior), accountability (ensuring that appropriate human parties are held responsible for negligence, crimes, or other harmful behavior in AI systems), and transparency (ensuring that decisions made by AI systems can be understood by impacted parties) [14]. The challenge of making AI systems behave as desired, called the AI alignment problem, is pernicious, especially when the goal is that the system conforms to human values and norms [43]. The goal of profit competes with each of these responsible AI principles, so for-profit companies cannot necessarily be trusted to prioritize them, let alone achieve them.\nMost large technology companies currently employ ethical and responsible AI research teams, and Google is no exception. Issues arise, however, when responsible AI researchers' findings present a challenge to profit-making priorities. At Google, ethical AI researchers Timnit Gebru and Margaret Mitchell collaborated on a research paper in 2020, published in 2021, that critiqued generative AI language models such as those created at their company [5]. Their work discussed concerns around risks and harms from these models including:\n\u2022 The environmental cost of training these models,\n\u2022 training data that encodes hegemonic worldviews and amplifies societal bias,\n\u2022 static training data that does not keep up with changing societal values,\n\u2022 documentation debt, in which the collection of data outpaces documentation about what that data is,\n\u2022 the willingness of humans to attribute \"intelligence\" to fluent text from systems that are merely replicating patterns in text data without true understanding of it (which they called \u201cstochastic parrots", "Shmargaret Shmitchell\" [5], while Timnit Gebru requested further discussion, and said that she would resign at a later date if Google could not explain its decisions and answer her further concerns. Google responded that it would not do so, and accepted her resignation, effective immediately. Dr. Gebru characterized this decision as a firing [30]. Dr. Mitchell was fired by Google around two months later, citing code of conduct and security concerns [21], although the timing might lead one to suspect that the company": "ent looking"}, {"title": "4 Generative AI and Surveillance Capitalism", "content": "We have discussed examples of how for-profit motivations can conflict with egalitarian intentions for generative AI companies. With less pure intentions, the results may be far worse. In particular, the theory of surveillance capitalism, due to Soshana Zuboff [45, 46, 47], provides a lens for examining problematic aspects of certain Big Tech companies' business models. Surveillance capitalist organizations exploit human experiences and actions as raw data for machine learning models, which they obtain via an extractive, non-reciprocal relationships with those humans. In turn, these companies use their dominance of the technological landscape to nudge, coax, or control people toward profitable outcomes. Zuboff formed her surveillance capitalism theory before the advent of OpenAI's ChatGPT service and other similar AI-based chatbots, but it has proved apt for analyzing this rapidly evolving technological landscape.\nGenerative AI technologies are insatiably data hungry, with the amount of training data currently being the primary limiting factor in their performance, and hence, ability to outperform competitors and attract users. Essentially, increasing the number of parameters (i.e., the model's size and complexity) makes these models more powerful (cf. Fig. 3 in [13]), and larger models require more data in order to train them well. More than ever, this situation steers any profit-driven generative AI company toward a surveillance capitalist business model. To build a generative AI system, the first step is to acquire as much data as possible, at minimal cost, and hence minimal compensation to the data providers, leading to a non-reciprocal extractive data collection process.\nGenerative AI is fast becoming an essential tool for every facet of daily life, both in the work and home spheres. As such, generative AI companies have users \"over a barrel,\" as it becomes difficult to meaningfully opt out and still be competent and competitive as a worker. It seems almost inevitable that providing personal data will become a condition of using generative AI services. And the better these systems become, the more embedded they are in daily life, the more the generative AI companies will be able to exert control. For example, it will be possible for them to continuously nudge users toward providing access to data on more and more aspects of their lives. Any profit-based generative AI company is motivated to do so in order to out-compete their rivals in AI performance.\nZuboff points out that as Big Tech companies mediate more of our digital and actual lives, they seize the power to monitor and enforce contracts, taking this power from the government, which in a democratic society is chosen by the people [45]. If a user does not make a car payment on their self-driving car, for example, the manufacturer can instruct the car to drive back to them. As generative AI becomes embedded in more facets of life, the companies that make these technologies will obtain similar powers within each of those facets. For instance, if a user asks an AI chatbot to help design their dream home, it can offer to find contractors to build the home, and facilitate the hiring process, a convenient proposition that many users would likely accept. As the mediator, it is in a position to censure the contractor for not completing the work under the AI company's own imposed time frame, potentially denying access to future work on the platform. It can also block access to its services, which have now likely become essential to daily life, if the user does not pay on time, a transaction in which the AI company takes a cut. Now, consider a near future in which AI chatbots mediate transactions in this manner in most spheres of life. The AI company has then inserted itself into a position of power and control throughout our entire society. This is the surveillance capitalists dream, and a nightmare for the rest of us."}, {"title": "5 The Egalitarian Alternative", "content": "The Free and Open-Source Software (FOSS) movement presents an alternative approach to for-profit generative AI. FOSS is grounded in the idea that software should be accessible, modifiable, and distributable with minimal restrictions. At its core, FOSS champions the freedom to use, study, share, and improve software for any purpose. This philosophy is driven by values of collaboration, transparency, and user empowerment, leading to a sea change in software development. Where once large scale systems were built and sold by for-profit entities, now a mixture of individuals and organizations work together using communal structures such as GitHub to create high-quality software that serves diverse needs.\nThe \"Free\" in FOSS, however, often leads to confusion. In this context, \"Free\" refers primarily to freedom, not necessarily to price. While some FOSS is indeed free of charge (\"Free as in (a complementary) Beer\"), the core idea is that users have the freedom to use, modify, and distribute the software like one would have the freedom to share a beer with friends. This freedom extends to examining the \"recipe\" (source code) and even brewing your own version (modifying the software) to suit your tastes.\nThe FOSS movement has produced some of the world's most important software, from the Linux operating system to the Apache Foundation, which provides everything from web servers to databases. These projects are embraced by for-profit entities for their utility and cost-effectiveness [24]. This symbiotic relationship, where the community-driven development of open-source software benefits both individuals and corporations, suggests a potential path for open-source Generative AI models such as Large Language Models (LLMs). As profit margins in the generative AI space narrow due to increased competition and the rising costs of training and maintaining proprietary models, the appeal of open-source alternatives, with their collaborative development and transparent training data, may grow [19].\nSuch models, trained on content willingly provided by users or organizations, could lead to a more equitable and transparent AI ecosystem. The collaborative creation of knowledge exemplified by Wikipedia, where individuals contribute their expertise to build a free and accessible encyclopedia, provides a potential blueprint [8]. A large-scale, organic approach to \"Egalitarian AI Foundation Models,\" drawing on content explicitly contributed for this purpose, could shift the power dynamics in AI development, empowering individuals and communities to participate in the creation and governance of these powerful tools.\nThe reliance on volunteered and open-source content might initially limit the scope and diversity of training data, potentially impacting the model's performance on certain tasks or in specific domains. However, it could also encourage the creation of specialized, community-driven models tailored to specific languages, cultures, or areas of expertise, fostering a richer and more diverse AI landscape that caters to a wider range of needs and perspectives. The transparent nature of such models, with their open training data and collaborative development, could also enhance trust and accountability, addressing concerns about bias, misuse, and the concentration of power that plague many proprietary models.\nAdditionally, as competition intensifies among major AI providers like OpenAI, Google, and Amazon, the pricing strategies for their models have become increasingly aggressive. This \"race to the bottom\" may lead to unsustainable business models. Many AI companies have yet to establish profitable revenue streams, relying heavily on investor funding to sustain operations. If the trend continues, it could undermine the financial viability of these companies, leading to a potential collapse similar to past technological bubbles [40].\nAs the limitations and ethical concerns of the surveillance capitalism model become increasingly apparent, the allure of collaborative, transparent, and community-driven AI development may prove compelling. The future of generative AI may lie not in the extraction of value from the digital commons, but in its collective cultivation, fostering a more equitable, accessible, and accountable AI ecosystem that benefits society as a whole.\nIn this egalitarian model, users would actively participate in the creation and/or curation of training data, ensuring that the models are representative of diverse perspectives and less susceptible to biases inherent in data scraped from the internet. The Wikimedia Foundation, the non-profit organization behind Wikipedia, demonstrates the viability of this approach, relying on a global community of volunteers to create and maintain one of the largest and most comprehensive knowledge repositories in the world. The egalitarian Generative AI model would prioritize transparency and user control, allowing individuals to understand and influence the training process and the underlying algorithms. This approach would foster a sense of ownership and empowerment, ensuring that the benefits of Generative AI are shared equitably among its users."}, {"title": "6 Challenges", "content": "The development, training, and deployment of open-source foundational generative AI models using egalitarian principles presents a distinct set of challenges. Large amounts of data must be ethically collected, and models have to be trained and deployed in a way that reflects community values.\n6.1 Data\nAssembling a dataset of sufficient size and quality [9] as those used by OpenAI, Google, Anthropic, and Mistral may be impossible, since an egalitarian dataset would not use copyrighted material without permission. It may require novel approaches to data collection, curation, and licensing, potentially involving partnerships with public institutions, academic bodies, and open-source communities.\nHowever, a reliance on crowd-sourced data and volunteer moderators introduces the question of oversight. Who are the gatekeepers responsible for ensuring the quality of data used to train these egalitarian models? While a distributed, community-driven approach is more democratic and open than private corporations, such an approach may have problems with potential biases and inconsistencies in data curation. All individuals carry their own perspectives and interpretations, which would inevitably influence the data and any subsequent model trained on the data.\nAn additional question to ask is: is it even feasible for volunteers to effectively curate such datasets? The task would almost certainly require the development of automated tools to assist human curators in identifying, tagging, and de-duplicating content which could in turn introduce biases and result in significant model performance limitations.\nAs these are foundation AI models that can be used in many different downstream applications, curating content without a clear understanding of its intended use introduces additional complexity. The appropriateness of certain data can vary wildly depending on the specific application context. For instance, a dataset containing tax law and monetary policy might be valuable for training a model focused on retirement planning, but perhaps inappropriate for a model designed to generate children's stories. This disconnect between data curation and downstream tasks will require flexibility and nuance in any kind of useful data curation and annotation process.\nThis challenge is further compounded by the evolving nature of language and cultural norms. What might be considered acceptable language in one context or at one point in time could easily become offensive or outdated later. Therefore, ongoing data monitoring and re-evaluation will be essential to ensure continued applicability. This adds another layer of complexity to the already demanding task of curating massive datasets for egalitarian generative AI models.\n6.2 Models\nTraining large Generative AI models requires substantial computational resources including specialized hardware. The now-obsolete GPT3 Davinci model is estimated to have cost $4.6 million and required approximately 355 GPU-years to train [39]. Such compute is typically the domain of well-funded commercial entities or governments. Egalitarian approaches might involve offloading compute to idling computers and servers, a modern version of project SETI@home [26], or by partnering with organizations that have access to high-performance computing clusters. Research into optimization techniques and novel distributed training methods could also help.\nGiven the emphasis on ethical considerations and potential biases, a broader range of model evaluation criteria may be needed. This could involve developing new methodologies and benchmark datasets that assess fairness, transparency, and social impact, alongside traditional measures of accuracy and fluency. A diverse and skilled volunteer community would provide a distinct advantage in this type of process, since the volunteer pool would include a greater variety of perspectives to these tasks.\nSuch limitations on data could potentially encourage a shift towards quality over quantity. Smaller but high-quality curated datasets could be focused on extracting maximum value from well-tagged records. This could lead to the development of specialized models, each trained on a specific subset of the data, resulting in a unique approach to the \"mixture of experts\" that collectively covers a wide range of knowledge and capabilities [10]. Such an approach could offer unique advantages, allowing for greater control over model behavior and potentially exceeding the performance of general-purpose models in specific domains.\nDeployment strategies for egalitarian models open up interesting possibilities. While the OpenAI model of providing API access is certainly an option, inference might be distributed among multiple computers in ways that might be similar to the distributed training discussed above [7]. Open source models can also be downloaded, using the Hugging Face model. This would empower users to have greater control over their AI interactions and potentially foster further innovation within the community.\nRecent history has shown that open-source and even content protected by copyright has been exploited to provide training material for for-profit entities. Strong community governance and well-crafted licensing agreements could help ensure that the egalitarian principles underpinning the project are upheld and that the models are used in a way that benefits society as a whole.\n6.3 Learning from Community-Driven Initiatives\nThe challenges faced in creating egalitarian generative AI models can find inspiration and guidance in the successes of existing community-driven projects. Wikimedia, with its collaborative Wikipedia and Wikidata"}, {"title": "7 Conclusion", "content": "In a 2022 article for IEEE Intelligent Systems, Michael Wooldridge, Professor of Computer Science at the University of Oxford wrote:\n\"The scale of Big AI systems the datasets and compute resources required to train them, and\nthe salaries paid to the researchers that design and build them mean that Big AI is effectively\nowned by Big Tech. For the most part, individual universities and research groups simply\ncannot compete, even if the source code is released, this means relatively little without the data\nand compute resources to train the model. Should we be concerned? I think yes. We would\nsurely be concerned if physics or chemistry research could only be done by private companies.\nFor all sorts of reasons, I believe we should be very concerned that a key future technology is\nproprietary and closed\u2014and that we and our students can only really access it if we\nare prepared to go behind locked doors.\" [41]\nThe internet, as it stands, is largely a reflection of the dominant culture - white, male, and increasingly corporate. Commercial generative AI models, trained on this internet data inherit and perpetuate these biases. The companies that produce these models are primarily driven by returning the investment of their shareholders. They are not incentivized to do more than the minimum to acknowledge and include the voices and experiences of underrepresented groups.\nAs Wooldridge argues, the concentration of AI development within Big Tech creates an environment where key future technologies are proprietary and closed. This limits innovation and reinforces existing power structures. The development of a more inclusive and fair technological landscape requires a shift away from corporate dominance, towards systems that can grow and evolve according to the needs of users, not just the owners.\nFor technology to be more inclusive and fair, it has to have the ability to grow according to user's needs, not the owners. An example of this was the emergent use of the hashtag (#) by the users of Twitter, and its subsequent incorporation into the codebase. The hashtag allowed for \"counterpublics\" such as Black Twitter and Occupy Wall Street to emerge spontaneously [23, 20]. These and other online communities that represent minority and marginalized voices are rarely a product of intentional design. Rather, they emerge within the affordances of the technology these individuals and groups are able to adapt to their needs.\nAs long as generative AI remains primarily within the domain of large, corporate interests, it is likely to perpetuating the biases of the dominant culture. To achieve truly representative and inclusive AI, we need to foster the development of egalitarian technological ecosystems that are structured to support diverse perspectives. This means supporting open-source initiatives, promoting data sovereignty, and advocating for policies that prioritize ethical considerations and user rights over corporate profit.\nWhile the vision laid out in this paper may not represent the definitive form of an egalitarian AI ecosystem, we believe that it provides an initial roadmap for navigating the complexities of creating a more equitable and user-centric approach to AI development. By embracing the principles of collaboration, community ownership, and mutual benefit, we can build a future where AI serves as a tool for collective empowerment and creative expression for the many, rather than a means for the extraction of value by the few. Like the FOSS movement before it, such an endeavor could allow generative AI to more prioritize the needs and interests of users.\nIt is becoming clear that the current trajectory of generative AI may be becoming unsustainable. We call for a radical re-imagining of the relationship between AI and society, one where individuals are not merely passive consumers but can be active participants."}]}