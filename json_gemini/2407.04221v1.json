{"title": "Autoverse: an Evolvable Game Language for Learning\nRobust Embodied Agents", "authors": ["Sam Earle", "Julian Togelius"], "abstract": "We introduce Autoverse, an evolvable, domain-specific language for single-player\n2D grid-based games, and demonstrate its use as a scalable training ground for\nOpen-Ended Learning (OEL) algorithms. Autoverse uses cellular-automaton-like\nrewrite rules to describe game mechanics, allowing it to express various game\nenvironments (e.g. mazes, dungeons, sokoban puzzles) that are popular testbeds\nfor Reinforcement Learning (RL) agents. Each rewrite rule can be expressed\nas a series of simple convolutions, allowing for environments to be parallelized\non the GPU, thereby drastically accelerating RL training. Using Autoverse, we\npropose jump-starting open-ended learning by imitation learning from search. In\nsuch an approach, we first evolve Autoverse environments (their rules and initial\nmap topology) to maximize the number of iterations required by greedy tree\nsearch to discover a new best solution, producing a curriculum of increasingly\ncomplex environments and playtraces. We then distill these expert playtraces into\na neural-network-based policy using imitation learning. Finally, we use the learned\npolicy as a starting point for open-ended RL, where new training environments\nare continually evolved to maximize the RL player agent's value function error (a\nproxy for its regret, or the learnability of generated environments), finding that this\napproach improves the performance and generality of resultant player agents.", "sections": [{"title": "1 Introduction", "content": "The idea of open-ended learning in virtual environments is to train agents that gradually get more\ncapable and behaviorally complex. This idea comes in many shapes, but what unites them all is\nthat there is no fixed objective or set of objectives; rather, the objectives depend in some way on the\nagent itself and its interaction with the environment and other agents. This is true for early work\non competitive coevolution in evolutionary robotics, work on artificial life simulations, and also for\nmore recent work on open-ended learning.\nHowever, we have yet to see any literally open-ended learning take place in these environments.\nThere have been interesting results, but learning generally stops at a rather low capability ceiling. We\nhypothesize that this is at least partly because the poverty of the environments, and the associated\nlimitations in the variability of the environments. It has been observed that the complexity of the\nbehavior of a living being, such an ant or a human, is at least partly a function of the complexity\nand variability of the environment it is situated in. And it stands to reason that even a very capable\nand motivated agent would not learn much in an empty white room with no toys, nor in a barren\ngridworld.\nA secondary hypothesis of ours is that open-ended learning is hampered by the complexity of \"cold-\nstarting\" learning policies from rewards in generated environments, as these may have rare rewards\nthat can only be accessed through uncommon action sequences for which the agents have no priors."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Autoverse: a batched game engine with evolvable components", "content": "In this section, we develop a framework for batched simulation of grid-world games, allowing game\ndesigners to rapidly generate robust agents and complex environments for a broad family of games.\nWe propose a game engine-in the form of a domain specific language (DSL)\u2014that is both general\nenough to encode a diversity of interesting and complex individual games, while also allowing for\nbatched simulation so as to make rapid agent training accessible on a single GPU. Whereas prior\nstudies have largely fixed the semantics of the generated environments-for example constraining\nthem to always take place in a maze, on 2D navigable terrain [Brockman et al., 2016], or a 2.5D space\nwith moveable objects and rigid-body physics (i.e. XLand Team et al. [2021])\u2014we are interested in\ngenerating environments that may carry a broader diversity of possible agent-environment interactions\nto further push the generality of OEL-trained controllers. In this section we propose a method to\neasily batch a surprisingly large category of games.\nWe focus on games whose dynamics involve discrete elements interacting on a grid. At the core of\nthe DSL are rewrite rules, which specify transformations applied to local patterns of tiles. Despite\ntheir seeming simplicity, rewrite rules have been leveraged in prior game description languages, and\nin particular, in the popular puzzle game engine PuzzleScript [Lavelle, 2013], to generate games\nranging from rogue-likes (in which players navigate dungeons, collect treasure and fight enemies),\nSuper Mario Bros-type side-scrolling platformers, and Sokoban-like box-pushing puzzle games and\nsimulacra of circuit-building.\nFor example, in a roguelike game where a player is tasked with exploring a dungeon littered with\nobstacles, enemies, and treasure, a rewrite rule might describe the event of a player's stepping into\nlava by indicating that, if a player tile and a lava tile are overlapping, at the next timestep, the player\ntile should disappear while the lava remains. A similar logic can be used to allow for basic player\nmovement: we allow the player agent to place invisible 'force' tiles at any cell adjacent to the current"}, {"title": "2.1.1 Possible extensions to the Autoverse language", "content": "In addition to binary patterns, we can generate networks for propagating scalar \u201cflows"}, {"title": "2.2 Warm-starting open-ended learning from search", "content": ""}, {"title": "2.2.1 Evolving game environments to maximize search-based complexity", "content": "The first component of our co-learning algorithm involves generating a large and diverse initial set of\nenvironment mechanics and layouts prior to agent training. We begin with a basic maze environment,\nin which the player moves as described above (by applying force tiles to adjacent cells), is blocked\nfrom traversing 'wall', and receives a reward when it consumes a 'food' tile by moving onto the cell\nit occupies. In addition to the 3 rules and 4 tile-types necessary for instantiating these dynamics,\nwe add a handful of \u201cno-op\u201d tiles and rewrite rules, which start off empty and thereby leave the\nenvironment dynamics unchanged. During our evolutionary algorithm, these additional rules are\nrandomly mutated by changing the value of tiles present in the rewrite rule. The tiles changed may\nbe in the input and/or output pattern, and at various spatial positions relative to one another. For\nexample, an empty rule might eventually be mutated such that it contains adjacent force and wall tiles\nin the input pattern, and a food tile in the output, resulting in a game mechanic wherein whenever\nthe player approaches a wall (by placing force next to it), the wall tile is transformed into a food tile.\nSuch a rule would incentivize a play strategy in which the player races to \u201cconsume\u201d as many wall\ntiles as possible over the course of an episode. In practice, mutated rules may involve new tiles, such\nthat these new tiles appear to exhibit certain behavior and relationships relative to the initial set of\ntiles, and rules may interact so as to form increasingly complex dynamics.\nIn addition the the modification of tiles in rewrite rules, the reward associated with the application\nof a given rewrite rule may also be modified. In the base, maze-like game, a separate rewrite rule\ndescribes the action of a player consuming a food tile (by applying force to the goal tile, moving\nonto it, and causing the food to disappear). Whenever this rule is applied, it results in a reward of 1\nfor the player agent. Similarly, mutable rules may be assigned positive negative rewards. From the\nstandpoint of getting an early sense of the player's awareness of and ability to adapt to new rules,\nthis is a useful feature because it allows for two copies of an environment with identical dynamics to"}, {"title": "2.2.2 Imitation learning: distilling search-based solutions", "content": "Throughout this process of environment evolution, we store trajectories corresponding to the solutions\nof all environments encountered. If the same environment (i.e. a ruleset and initial level layout)\nappears twice, we keep the trajectory that led to higher reward. (This situation may arise when a\nclone of an environment is re-evaluated at a later stage in evolution, with a higher cap on the amount\nof search afforded to our fitness evaluation.)\nWe perform behavior cloning on this archive of trajectories, in effect distilling the set of solutions\ndiscovered by search into a neural network. Behavior cloning is a simplistic form of imitation\nlearning, wherein the model is given (state, action) pairs and is trained to predict the corresponding\naction for each state. Observations consist of a local patch of the surrounding tiles, centered at the\nplayer's current position, in addition to a binary representation of the evolved rules of the current\nenvironment (so that the agent may adapt its strategy to suit the given mechanics)."}, {"title": "2.3 Open-ended reinforcement learning in evolving environments", "content": "Once the behavior cloning algorithm has converged, we continue training the agent with reinforce-\nment learning, randomly sampling one of the unique evolved environments contained in the set of\ntrajectories above, and using Proximal Policy Optimization (PPO) [Schulman et al., 2017] to update\nthe agent's parameters. Our PPO Jax implementation is based on PureJaxRL Lu et al. [2022] which\nis in turn adapted from CleanRL Huang et al. [2022], which allows our entire training loop to be\njust-in-time compiled to run on the GPU. Following work in Unsupervised Environment Design\n(UED) [Jiang et al., 2021, Parker-Holder et al., 2022], we continue to evolve environments in order to\ngenerate an adaptive curriculum for our RL player agent.\nFixing some interval $k_{evo}$ as a hyperparameter, after every $k_{evo}$ updates in our RL loop, we evolve\nAutoverse environments\u2014both the binary array corresponding to the initial map layout, and the"}, {"title": "3 Results", "content": "Tables 1 and 2 show the importance of observations on agent performance when imitation learning on\ntrajectories generated from greedy tree search on evolved environments. Table 1 shows that generally,\nlarger observations of the map allow for increased performance both during training, and on test\nenvironments (environments also generated by the evolutionary process, but held out for testing). The\nbest performance comes from agents that are able to fully observe the map (where the observation is\ncentered at the agent's current position, and 0-padding is added to the map as necessary).\nTable 2 shows that agents that are allowed to observe each environment's rule-set perform better\nthan agents for whom the rule-set is replaced by 0-padding. This shows that the mechanics of the\ngenerated environments are sufficiently distinct, such that agents cannot perform effectively without\nobserving the rule-sets.\nWe show some preliminary qualitative results of the search-based evolutionary process in Figure 1.\nWe observe a variety of distinct environment dynamics in evolved environments. The majority of\nenvironments exhibit highly unstable dynamics, in which the majority of cells on the map change\nstate from one timestep to the next, as exhibited in Figure 1n. The prevalence of such environments\nmay partially be explained by the fact that, in an environment where almost all states are different\nfrom one another, a search-based agent is less likely to encounter the same state twice, thus forcing it"}, {"title": "4 Related work", "content": "Reinforcement learning research has long relied on benchmarks of various kinds. These are often\ntaken from, or inspired by, games, including board games and video games, and sometimes from\nrobotics and other disciplines. An issue with these benchmarks is the risk of overfitting. If the\nbenchmark does not have appropriate degree of variability, the RL algorithm will tend to learn a\npolicy that will work only for a particular configuration of a particular environment. For example,\nwhen training deep RL methods to play Atari games in the ALE framework, they will typically learn\na policy that works for only one game, and only the particular levels of that game, and break if you"}, {"title": "5 Conclusion", "content": "We introduce autoverse, a scalable testbed for open-ended learning algorithms, and run some initial\nexperiments exploring the use of an evolutionary strategy to search for autoverse environments\ncomprising difficult game environments with respect to a search-based player agent. We formalize\nthe underlying mechanics of autoverse by association with cellular automata and the rewrite-rule\napproach to game logic developed by other popular game languages. We walk through some examples\nof popular game and reinforcement learning environments, showing how mazes, dungeons, sokoban\npuzzles, can be implemented with relatively straightforward sets of rewrite rules. We also show how\nautoverse update rules in general can be implemented with a simple series of convolutions, allowing\nenvironment simulation to occur on the GPU, making for fast simulation and neural-network learning\nin particular within the framework.\nOur evolutionary search for challenging environments relative to a search-based player discovers a\nlarge number of distinct environments, each constituting a potentially novel and interesting task for\nan RL-based player agent. This evolutionary search also returns a large number of expert trajectories,\nwhich can ultimately be used for imitation learning and to jump start RL. Because the cap on\nsearch depth is increased incrementally over the course of evolution, we also obtain a curriculum of\nincreasingly expert trajectories and/or increasingly complex environments. Future work will study\nhow this data can be used to jump-start a generalist reinforcement learning game playing agent by\npre-training its weights using imitation learning.\nOf particular interest in the evolved autoverse environments is the degree to which a given environ-\nment's dynamics are stable or chaotic. We note that a large number of environments tend toward\nchaos, and argue that more human-relevant environments can be found in the middle ground of\nsemi-stable environments, where stable or oscillating patterns tend to be reached, but the player agent\ncan intervent to disrupt or alter them to some degree. Future work is needed to investigate quantitative\nmetrics that may be used to guide the search process toward such environments. More broadly, using\npre-trained foundation models or humans-in-the-loop could also allow us both to align the process\nwith notions of human interestingness, as well as to introduce additional human-authored complexity\ninto the learning process."}]}