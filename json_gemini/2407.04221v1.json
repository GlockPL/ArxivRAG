{"title": "Autoverse: an Evolvable Game Language for Learning Robust Embodied Agents", "authors": ["Sam Earle", "Julian Togelius"], "abstract": "We introduce Autoverse, an evolvable, domain-specific language for single-player 2D grid-based games, and demonstrate its use as a scalable training ground for Open-Ended Learning (OEL) algorithms. Autoverse uses cellular-automaton-like rewrite rules to describe game mechanics, allowing it to express various game environments (e.g. mazes, dungeons, sokoban puzzles) that are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite rule can be expressed as a series of simple convolutions, allowing for environments to be parallelized on the GPU, thereby drastically accelerating RL training. Using Autoverse, we propose jump-starting open-ended learning by imitation learning from search. In such an approach, we first evolve Autoverse environments (their rules and initial map topology) to maximize the number of iterations required by greedy tree search to discover a new best solution, producing a curriculum of increasingly complex environments and playtraces. We then distill these expert playtraces into a neural-network-based policy using imitation learning. Finally, we use the learned policy as a starting point for open-ended RL, where new training environments are continually evolved to maximize the RL player agent's value function error (a proxy for its regret, or the learnability of generated environments), finding that this approach improves the performance and generality of resultant player agents.", "sections": [{"title": "1 Introduction", "content": "The idea of open-ended learning in virtual environments is to train agents that gradually get more capable and behaviorally complex. This idea comes in many shapes, but what unites them all is that there is no fixed objective or set of objectives; rather, the objectives depend in some way on the agent itself and its interaction with the environment and other agents. This is true for early work on competitive coevolution in evolutionary robotics, work on artificial life simulations, and also for more recent work on open-ended learning.\nHowever, we have yet to see any literally open-ended learning take place in these environments. There have been interesting results, but learning generally stops at a rather low capability ceiling. We hypothesize that this is at least partly because the poverty of the environments, and the associated limitations in the variability of the environments. It has been observed that the complexity of the behavior of a living being, such an ant or a human, is at least partly a function of the complexity and variability of the environment it is situated in. And it stands to reason that even a very capable and motivated agent would not learn much in an empty white room with no toys, nor in a barren gridworld.\nA secondary hypothesis of ours is that open-ended learning is hampered by the complexity of \"cold-starting\" learning policies from rewards in generated environments, as these may have rare rewards that can only be accessed through uncommon action sequences for which the agents have no priors."}, {"title": "2 Methods", "content": "In this section, we develop a framework for batched simulation of grid-world games, allowing game designers to rapidly generate robust agents and complex environments for a broad family of games. We propose a game engine-in the form of a domain specific language (DSL)\u2014that is both general enough to encode a diversity of interesting and complex individual games, while also allowing for batched simulation so as to make rapid agent training accessible on a single GPU. Whereas prior\nWe focus on games whose dynamics involve discrete elements interacting on a grid. At the core of the DSL are rewrite rules, which specify transformations applied to local patterns of tiles. Despite their seeming simplicity, rewrite rules have been leveraged in prior game description languages, and in particular, in the popular puzzle game engine PuzzleScript [Lavelle, 2013], to generate games ranging from rogue-likes (in which players navigate dungeons, collect treasure and fight enemies), Super Mario Bros-type side-scrolling platformers, and Sokoban-like box-pushing puzzle games and simulacra of circuit-building.\nFor example, in a roguelike game where a player is tasked with exploring a dungeon littered with obstacles, enemies, and treasure, a rewrite rule might describe the event of a player's stepping into lava by indicating that, if a player tile and a lava tile are overlapping, at the next timestep, the player tile should disappear while the lava remains. A similar logic can be used to allow for basic player movement: we allow the player agent to place invisible 'force' tiles at any cell adjacent to the current"}, {"title": "2.1 Autoverse: a batched game engine with evolvable components", "content": "player position; we then use a rewrite rule to ensure that whenever a player tile is adjacent to a force tile overlapping with a 'floor' tile (i.e. a grid cell unobstructed by obstacles preventing player movement), at the next timestep, the player should move onto this adjacent floor tile, consuming the force tile in the process.\nWe propose a novel approach to rewrite rules by taking advantage of the fact that they can be implemented with convolutions, allowing our environment to be both differentiable and easily hardware accelerated. A rewrite rule says that when an n \u00d7 m patch I of tiles is present on the map at timestep t, it should be replaced by an n \u00d7 m patch O at timestep t + 1. We construct a convolutional kernel $K_1$, with dimensions c \u00d7 c\u00d7 n \u00d7 m (where c is the number of tile types) for recognizing the input pattern I. We set $K_1[:, 0, :, :] := I$, with 0s everywhere else. We define I to be the sum of the elements of the matrix I (i.e. $I = 1^T \\cdot I \\cdot 1$). Then, the transition of the entire board D can be written as\n$D_{t+1} = ReLU (conv_{K_1}(D_t) \u2013 I + 1)$\nTo generalize this implementation to rewrite rules with output patterns in which more than just the center cell is changed, we first apply a convolution (similar to that above) to generate binary activations indicating whether given input patterns are present, then apply a transposed convolution to generate the change to the board required by corresponding output patterns. Focusing on a single rewrite rule with input and output patterns I and O, and a patch of the board B, all with patch sizes cxnx m (and one-hot encoded over the number of tile types), our network applies the following"}, {"title": "2.1.1 Possible extensions to the Autoverse language", "content": "In addition to binary patterns, we can generate networks for propagating scalar \u201cflows\". To simulate a \"source\" of water using the binary rules above, we might specify that water cells can replicate downward when unobstructed, and otherwise sideways (when unobstructed to the side). When water flows to an adjacent tile, we update an additional channel, denoting the \"level\" of the water at that point, for example, decrementing once with each horizontal tile transition, such that water is \"absorbed\" by land tiles after a certain time. Using similar auxiliary, integer-valued channels, we can effectively \"count\" the distance some substance has travelled from a source, and thereby can move beyond rewrite rules based on local patterns to instantiate more complex algorithms like breadth/depth-first search-based pathfinding (again as a batched, differentiable NCA, as demonstrated in our prior work [Earle et al., 2023]). Though in this work, we limit the games to only involve binary activations, we note that certain games can exhibit phenomena that appear \u201cflow-like\", as an emergent property of interaction between evolved rules.\nFinally, we note that it is also possible to adapt the rewrite-rules, encoded as convolutions, to support applying each rule only once or a fixed number of times, and/or in a random order, by selecting tiles to rewrite by taking the maximum over an additional channel of ordered or randomly-generated index values. For maximum parallelism, we opt to apply rules in parallel, but can use masking to guarantee that certain rules inhibit others."}, {"title": "2.2 Warm-starting open-ended learning from search", "content": "The first component of our co-learning algorithm involves generating a large and diverse initial set of environment mechanics and layouts prior to agent training. We begin with a basic maze environment, in which the player moves as described above (by applying force tiles to adjacent cells), is blocked from traversing 'wall', and receives a reward when it consumes a 'food' tile by moving onto the cell it occupies. In addition to the 3 rules and 4 tile-types necessary for instantiating these dynamics, we add a handful of \u201cno-op\u201d tiles and rewrite rules, which start off empty and thereby leave the environment dynamics unchanged. During our evolutionary algorithm, these additional rules are randomly mutated by changing the value of tiles present in the rewrite rule. The tiles changed may be in the input and/or output pattern, and at various spatial positions relative to one another. For example, an empty rule might eventually be mutated such that it contains adjacent force and wall tiles in the input pattern, and a food tile in the output, resulting in a game mechanic wherein whenever the player approaches a wall (by placing force next to it), the wall tile is transformed into a food tile. Such a rule would incentivize a play strategy in which the player races to \u201cconsume\u201d as many wall tiles as possible over the course of an episode. In practice, mutated rules may involve new tiles, such that these new tiles appear to exhibit certain behavior and relationships relative to the initial set of tiles, and rules may interact so as to form increasingly complex dynamics.\nIn addition the the modification of tiles in rewrite rules, the reward associated with the application of a given rewrite rule may also be modified. In the base, maze-like game, a separate rewrite rule describes the action of a player consuming a food tile (by applying force to the goal tile, moving onto it, and causing the food to disappear). Whenever this rule is applied, it results in a reward of 1 for the player agent. Similarly, mutable rules may be assigned positive negative rewards. From the standpoint of getting an early sense of the player's awareness of and ability to adapt to new rules, this is a useful feature because it allows for two copies of an environment with identical dynamics to"}, {"title": "2.2.1 Evolving game environments to maximize search-based complexity", "content": "have inverted goals. For example, in one game, red tiles may provide reward, and in another, they may provide negative reward, ensuring that the player cannot ignore the specifics of mutated rules and apply the same strategy to both environments. It is also worth noting that when mutating rewrite rules, we allow for rules to emerge which \u201ckill\" the player and end the game (i.e. with one player tile in the input pattern and none in the output), which similarly raises the stakes and decreases the likelihood that a rule-agnostic strategy can be successfully applied to all environments.\nFinally, we also mutate the initial level layout, a multi-hot array of tiles, by randomly flipping bits in the array. It is important to jointly evolve the initial level layouts as some initial levels, when paired with certain rulesets, may result in unsolvable environments or environments with uninteresting dynamics (e.g. where certain rewrite rules are never applied because some particular tile type necessary for the rule's application is initially absent from the level). Conversely, the same rule-set can result in multiple diverse tasks when paired with different initial level layouts. As a sanity check, we can also disable ruleset-mutation and evolve the initial level layout of our base maze-like environment. This results in mazes requiring increasingly many steps to traverse the optimal path to a food tile.\nA simple mu + lambda evolution strategy is used to evolve environments using the above-mentioned mutation operators. As a fitness metric, we compute a proxy for the complexity or difficulty of the environment using search. In particular, we use best-first search to explore possible sequences of actions that can be taken by a player agent, prioritizing those trajectories that lead to higher reward. The fitness of an environment is equal to the number of states visited by search prior to it finding the highest-reward solution. A \"node\" in the search tree corresponds to a game state, i.e. the current player reward, the position and orientation of the player agent, and the multihot array corresponding to the state of the level at a given timestep; an edge in the search tree is a player action (rotating left or right or moving forward). When a game state is encountered that is equivalent to some state seen earlier in search, the shallower node \u2013 closer to the root of the tree and thereby occurring after fewer player actions \u2013 is kept, and the deeper node is pruned from the search tree. If two states are equivalent except for their reward, then the state with higher reward is kept. The budget of breadth-first search is limited, and this limit is increased whenever there appears in the population an environment whose best solution required a number of search iterations approaching this limit to some degree.\""}, {"title": "2.2.2 Imitation learning: distilling search-based solutions", "content": "Throughout this process of environment evolution, we store trajectories corresponding to the solutions of all environments encountered. If the same environment (i.e. a ruleset and initial level layout) appears twice, we keep the trajectory that led to higher reward. (This situation may arise when a clone of an environment is re-evaluated at a later stage in evolution, with a higher cap on the amount of search afforded to our fitness evaluation.)\nWe perform behavior cloning on this archive of trajectories, in effect distilling the set of solutions discovered by search into a neural network. Behavior cloning is a simplistic form of imitation learning, wherein the model is given (state, action) pairs and is trained to predict the corresponding action for each state. Observations consist of a local patch of the surrounding tiles, centered at the player's current position, in addition to a binary representation of the evolved rules of the current environment (so that the agent may adapt its strategy to suit the given mechanics)."}, {"title": "2.3 Open-ended reinforcement learning in evolving environments", "content": "Once the behavior cloning algorithm has converged, we continue training the agent with reinforce-ment learning, randomly sampling one of the unique evolved environments contained in the set of trajectories above, and using Proximal Policy Optimization (PPO) [Schulman et al., 2017] to update the agent's parameters. Our PPO Jax implementation is based on PureJaxRL Lu et al. [2022] which is in turn adapted from CleanRL Huang et al. [2022], which allows our entire training loop to be just-in-time compiled to run on the GPU. Following work in Unsupervised Environment Design (UED) [Jiang et al., 2021, Parker-Holder et al., 2022], we continue to evolve environments in order to generate an adaptive curriculum for our RL player agent.\nFixing some interval $k_{evo}$ as a hyperparameter, after every $k_{evo}$ updates in our RL loop, we evolve Autoverse environments\u2014both the binary array corresponding to the initial map layout, and the"}, {"title": "3 Results", "content": "convolutional kernels corresponding to the input-output patterns of the set of rewrite rules. To evaluate the mutated environments, we freeze the weights of the RL-trained player and have it play through 1 or more episodes in the environment. Following Jiang et al. [2021], we compute the mean absolute value function error of the agent over the course of an episode, and use this as the candidate environment's fitness. The value function error is intended as a proxy measure of regret that is, the difference in return (i.e. discounted reward) accumulated by the learned player over the course of an episode, and that of a hypothetical optimal player. Dennis et al. [2020] show that, when the adversarial loop between the environment-generator agent (in our case an evolutionary algorithm) and player agent is seen as a multi-agent game, wherein the generator's objective to increase, and the player's objective to decrease, such a measure of regret, then this game converges to a Nash equilibrium, implying that the generator has discovered maximally complex and challenging environments with respect to the agent, and the agent has discovered a maximally capable policy with respect to the environments produced by the generator (given some simplifying assumptions). Intuitively, we can think of the value function error as indicating the extent to which the learned agent is \u201csurprised\" by the outcome of its episode (i.e. having either over- or under-estimated its performance during play).\nTables 1 and 2 show the importance of observations on agent performance when imitation learning on trajectories generated from greedy tree search on evolved environments. Table 1 shows that generally, larger observations of the map allow for increased performance both during training, and on test environments (environments also generated by the evolutionary process, but held out for testing). The best performance comes from agents that are able to fully observe the map (where the observation is centered at the agent's current position, and 0-padding is added to the map as necessary).\nTable 2 shows that agents that are allowed to observe each environment's rule-set perform better than agents for whom the rule-set is replaced by 0-padding. This shows that the mechanics of the generated environments are sufficiently distinct, such that agents cannot perform effectively without observing the rule-sets.\nWe show some preliminary qualitative results of the search-based evolutionary process in Figure 1. We observe a variety of distinct environment dynamics in evolved environments. The majority of environments exhibit highly unstable dynamics, in which the majority of cells on the map change state from one timestep to the next, as exhibited in Figure 1n. The prevalence of such environments may partially be explained by the fact that, in an environment where almost all states are different from one another, a search-based agent is less likely to encounter the same state twice, thus forcing it"}, {"title": "4 Related work", "content": "Reinforcement learning research has long relied on benchmarks of various kinds. These are often taken from, or inspired by, games, including board games and video games, and sometimes from robotics and other disciplines. An issue with these benchmarks is the risk of overfitting. If the benchmark does not have appropriate degree of variability, the RL algorithm will tend to learn a policy that will work only for a particular configuration of a particular environment. For example, when training deep RL methods to play Atari games in the ALE framework, they will typically learn a policy that works for only one game, and only the particular levels of that game, and break if you"}, {"title": "5 Conclusion", "content": "We introduce autoverse, a scalable testbed for open-ended learning algorithms, and run some initial experiments exploring the use of an evolutionary strategy to search for autoverse environments comprising difficult game environments with respect to a search-based player agent. We formalize the underlying mechanics of autoverse by association with cellular automata and the rewrite-rule approach to game logic developed by other popular game languages. We walk through some examples of popular game and reinforcement learning environments, showing how mazes, dungeons, sokoban puzzles, can be implemented with relatively straightforward sets of rewrite rules. We also show how autoverse update rules in general can be implemented with a simple series of convolutions, allowing environment simulation to occur on the GPU, making for fast simulation and neural-network learning in particular within the framework.\nOur evolutionary search for challenging environments relative to a search-based player discovers a large number of distinct environments, each constituting a potentially novel and interesting task for an RL-based player agent. This evolutionary search also returns a large number of expert trajectories, which can ultimately be used for imitation learning and to jump start RL. Because the cap on search depth is increased incrementally over the course of evolution, we also obtain a curriculum of increasingly expert trajectories and/or increasingly complex environments. Future work will study how this data can be used to jump-start a generalist reinforcement learning game playing agent by pre-training its weights using imitation learning.\nOf particular interest in the evolved autoverse environments is the degree to which a given environ-ment's dynamics are stable or chaotic. We note that a large number of environments tend toward chaos, and argue that more human-relevant environments can be found in the middle ground of semi-stable environments, where stable or oscillating patterns tend to be reached, but the player agent can intervent to disrupt or alter them to some degree. Future work is needed to investigate quantitative metrics that may be used to guide the search process toward such environments. More broadly, using pre-trained foundation models or humans-in-the-loop could also allow us both to align the process with notions of human interestingness, as well as to introduce additional human-authored complexity into the learning process."}]}