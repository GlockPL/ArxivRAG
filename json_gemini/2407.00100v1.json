{"title": "Enhancing In-Context Learning via Implicit Demonstration Augmentation", "authors": ["Xiaoling Zhou", "Wei Ye", "Yidong Wang", "Chaoya Jiang", "Zhemg Lee", "Rui Xie", "Shikun Zhang"], "abstract": "The emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL's effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions.", "sections": [{"title": "1 Introduction", "content": "Large pre-trained language models (PLMs) have showcased exceptional abilities in in-context learning (ICL) (Brown et al., 2020; Wang et al., 2023; Rubin et al., 2022), which assists the model in discerning the underlying patterns within demonstrations and make more accurate predictions (Chan et al., 2022; Wu et al., 2023). As a new paradigm, ICL offers compelling advantages, allowing for natural language interaction with PLMs (Wei et al., 2022; Yang et al., 2023), as well as reduced computational costs (Li et al., 2023a; Rubin et al., 2022). While promising, ICL's performance is highly dependent on provided demonstrations and templates (Liu et al., 2022; Zhang et al., 2022b; Sorensen et al., 2022), resulting in subpar and unstable performance. This promotes research aimed at improving the quality (Rubin et al., 2022; Li et al., 2023b), quantity (Li et al., 2023a; Choi et al., 2022), and permutations (Lu et al., 2022; Tang et al., 2023) of demonstrations. Other research avenues include prediction adjustment (Zhao et al., 2021; Han et al., 2023; Fei et al., 2023) and learning process design (e.g., channel models (Min et al., 2022a) and meta-training frameworks (Min et al., 2022b)). Despite ongoing efforts, ICL still struggles with efficiently and reliably capturing sufficient knowledge from context, leaving performance stability as a persistent bottleneck.\nIn this study, we propose enriching contextual knowledge for PLMs by augmenting demonstrations. We first attempt to enhance the representation of demonstrations by transforming them along semantic directions sampled from the deep feature space of demonstration examples. This operation stems from the observation that the deep features in a network are usually linearized (Bengio et al., 2013; Cheung and Yeung, 2021; Cho, 2016), implying the existence of numerous semantic directions within the deep feature space, hence potentially enabling us to incorporate richer contextual knowledge without extending input length. From this novel perspective, we theoretically prove that when the number of augmented pieces approaches infinity, its effect approximately equals a logit adjustment operation. Specifically, we derive a refined Softmax function that integrates the statistical properties of demonstrations. Consequently, rather than explicitly executing the augmentation procedure, we can efficiently conduct implicit demonstration augmentation using the derived prediction function, obtaining an improved ICL method with theoretical guidance.\nWe conduct extensive experiments across seven PLMs and various classification tasks. The empirical results demonstrate that our approach remarkably enhances prediction accuracy and reduces performance variability across different demonstrations, permutations, and templates. Notably, our method is straightforward, effective, and generalizable, enabling seamless integration with other ICL methods to enhance their performance.\nOur contributions can be summarized as follows:\n\u2022 We introduce Implicit Demonstration Augmentation-based ICL (IDAICL), a pioneering work that incorporates demonstration augmentation into ICL. Instead of solely enhancing demonstration quality, quantity, or order, our method explores context augmentation within the deep feature space, offering a new perspective to enrich demonstrations bypassing input length limitations.\n\u2022 We theoretically establish that as the number of augmented pieces approaches infinity, our augmentation strategy approximates a logit-adjusted prediction function that integrates statistical properties derived from the input data distribution. Equipped with this function, IDAICL provides a straightforward yet theory-guided solution to enhance ICL.\n\u2022 Extensive experiments conducted across diverse tasks and PLMs conclusively illustrate that IDAICL considerably improves average and worst-case accuracy compared to existing ICL methods. Moreover, it effectively enhances performance stability."}, {"title": "2 Background and Related Work", "content": "2.1 In-Context Learning\nBrown et al. (2020) showcased the ICL capability of PLMs, wherein PLMs generate predictions solely based on a concatenation of training examples for few-shot learning without updating parameters. Subsequent studies (Holtzman et al., 2021; Min et al., 2022a,b) have developed this approach, yielding promising outcomes across various tasks. Nevertheless, recent research has uncovered certain limitations. To begin with, the volume of input knowledge for each query is constrained by the maximum input length of PLMs (Hao et al., 2022), and the computational cost increases as the number of demonstrations grows (Li et al., 2023a), making it challenging to integrate significant knowledge from demonstrations to PLMs. Additionally, ICL's performance is sensitive to the input of PLMs (Davison et al., 2019; Jiang et al., 2020), thus exhibiting high variance and poor worst-case accuracy (Perez et al., 2021; Lu et al., 2022).\nResearchers have explored various techniques to address the biases and instability of ICL. These techniques encompass learning process design (Min et al., 2022a,b), demonstration retrieval (Rubin et al., 2022; Zhang et al., 2022b), prompt engineering (Sorensen et al., 2022; Lu et al., 2022), and prediction calibration (Zhao et al., 2021; Fei et al., 2023). However, these methods have yet to fully address the issue of severely limited knowledge transfer from demonstrations to large PLMs.\n2.2 Data Augmentation\nData augmentation (Chen et al., 2023), which involves artificially creating training data through transformations, is a well-established research area in machine learning. Although data augmentation techniques have undergone extensive exploration in diverse machine learning domains (Maharana et al., 2022; Shorten and Khoshgoftaar, 2019), applying them to text data poses challenges due to the complexity of preserving labels during textual transformations (Kobayashi, 2018). Nonetheless, data augmentations in the latent space, such as adversarial training (Zhang et al., 2022a; Zhu et al., 2020; Cheng et al., 2020), interpolation (Chen et al., 2022b; Wu et al., 2022), and generative techniques (Li et al., 2022; Malandrakis et al., 2019), have demonstrated notable enhancements when applied alongside large PLMs.\nRecently, Wang et al. (2019) introduced the concept of implicit data augmentation in the context of image classification. This approach involves transforming training data within the deep feature space and boils down to the optimization of a novel robust loss function. Subsequent studies (Chen et al., 2022c; Li et al., 2021; Zhou and Wu, 2023a) for image classification tasks have further improved upon this approach. This study introduces an algorithm for implicitly augmenting demonstrations within the realm of ICL."}, {"title": "3 Methodology", "content": "3.1 In-Context Learning with PLMs\nConsidering a PLM G, this study focuses on the following task: given a query input text \u00e6 and a candidate answer set Y = {y1, y2, \u2026, y|y|}, we aim to predict the answer \u0177 based on m demonstration examples C = {c1, c2, \u2026\u2026\u2026, cm}, where each ci represents a training example (xi, yi) after template formulation and m denotes the quantity of demonstration examples for each test sample. Formally, give a model G, we first compute the probability of each answer yj:\nP\u00e7 (yj | C, x).  (1)\nSubsequently, the ultimate prediction \u0177, characterized by the highest probability is chosen from the candidate answer set Y:\n\u0177 = arg max P\u00e7 (yj | C,x). (2)\nYjEY\nTo simplify, the contextual input is denoted as x = [C, x] in the subsequent text. Then, the probability of answer yj, represented as P\u00e7(yj|x), is computed using the Softmax function\u00b9:\nP\u04ab(yj|x) := P\u04ab(yj|hz) = \\frac{e^{w_{k,y_j}^Th_z+b_{k,y_j}}}{\\sum_{k'eY}e^{w_{k',y_j}^Th_z+b_{k',y_j}}} (3)\nwhere hz = G(x) signifies the hidden state of the last block at the final position for x. wk and bk are the weight vector and bias corresponding to the final fully connected layer for the k-th token.\n3.2 Demonstration Augmentation\nRecognizing the established efficacy of data augmentation in machine learning (Feng et al., 2021), this study investigates demonstration augmentation and suggests enhancing the deep features of demonstrations by transforming them along semantic directions sampled from the deep feature space of demonstration examples. This strategy is motivated by the intriguing observation that the deep features in networks are often linearized (Bengio et al., 2013; Chen et al., 2022a). Building on this observation, we hypothesize that hz lies within the subspace spanned by hc and hx: hz = ahc + \u03b2hx, where hc and hx represent the components of hz linked respectively to the demonstrations and the query. The necessity of this assumption stems from intricate relationships among token representations and the exclusive augmentation of the component related to demonstrations. Notably, this decomposition is not necessary in practical applications. In the subsequent text, we directly refer to ahc and \u03b2hx as hc and hx.\nTo augment hc, we randomly sample vectors from the deep feature space of demonstrations. In particular, vectors are drawn from a multivariate normal distribution N(\u03bc, \u03a3), where \u03bc and \u2211 denote the feature mean and covariance matrix. These statistical properties are estimated from the deep features of the demonstration set D, which includes demonstration examples linked to all queries. The feature mean \u00b5 is computed as\n\u03bc = \\frac{1}{|D|} \\sum_{i=1}^{D} h_i, (4\nwhere hi = G(ci) represents the hidden state of the last block at the final position for the i-th demonstration example ci in D, and |D| denotes the size of D. The covariance matrix \u2211 is computed as\n\u03a3 = \\frac{1}{|D|} \\sum_{i=1}^{D} (h_i \u2013 \u03bc) (h_i \u2013 \u03bc). (5)\nSubsequently, hc is shifted in the extracted semantic vectors, resulting in augmented features, \u0125c, which follows\n\u0125c ~ N (hc + \u03bb\u03bc, \u03bb\u03a3), (6)\nwhere \u00e0 refers to a positive coefficient controlling the strength of semantic augmentation. In real-world applications, it can be directly assigned a value of 0.5. Sensitivity tests for A are discussed in Section 5.4.\n3.3 Novel Prediction Function\nSelecting the answer with the highest probability is equivalent to favoring the answer with the lowest inverse probability. Therefore, the prediction can be determined by\n\u0177 = arg min P\u00e7 (yj | hz)-1.  (7)\nYjEY\nAssume that each hc is augmented for M times, resulting in an augmented demonstration feature set {h 1c , h 2c..., h Mc } with size M. Here, hi c represents the i-th augmented feature for hc. Then, the final prediction for the query \u00e6 depends on all augmented features of hc and can be expressed as\nP_{M} (x) = \\frac{1}{M}\\sum_{i=1}^{M} P_G(y_j | h_{i}^c, h_x), (8\n\u0177 = arg min P_{M}(x).  (9)\nYjEY\nGiven that the performance of ICL benefits from an increased number of demonstration instances (Liu et al., 2022; Wu et al., 2023), we explore the scenario of augmenting an infinite number of times for the deep representation of demonstrations. Subsequently, an easily computable surrogate for the expected prediction can be derived, resulting in a highly efficient implementation. The whole pipeline of IDAICL is depicted in Figure 2.\nAs M\u2192\u221e, on the basis of the aforementioned decomposition of hz, the expected prediction for answer yj (denoted as P(x) within the augmented feature set can be expressed as follows:\nP_{Y_j}^{\\infty}(x) = E_\\delta[e^{{Aw_{k,y_j}}^T(h_c + \\delta) + w_{k,y_j}^Th_x + \\Delta b_{k,y_j}}], (10)\nwhere \u2206wk,yj = wk -wyj; and \u2206bk,yj = bk - byj.\nHowever, accurately calculating P(x) is challenging. Alternatively, we proceed to derive a surrogate calculation for it. Applying the linearity of expectation, Eq. (10) can be expressed as:\nP_{Y_j}^{\\infty}(x) =  \\sum_{k} E_{\\Delta w_{k,y_j}, \\delta} [ e^{{Aw_{k,y_j}}^T(h_c + \\delta) + w_{k,y_j}^Th_x + \\Delta b_{k,y_j}} ] (11)\nGiven that hc is a Gaussian random variable conforming to N (hc + \u03bb\u03bc, \u03bb\u03a3), we know that \u2206w^{T}_{k,y_j}\u03b4 follows the multivariate normal distribution: N (\u2206w^{T}_{k,y_j}\u03bb\u03bc, \u2206w_{k,yj}\u03bb\u03a3\u2206w_{k,yj}). Then, utilizing the moment-generating function\nE[etX] = etu+\\frac{1}{2}t202, X ~ \u039d(\u03bc, \u03c3\u00b2), (12)\nEq. (11) can be derived as\nP_{Y_j}^{\\infty}(x) = \\sum_{k} M_{k,y_j}N_{k,y_j} e^{{Aw_{k,y_j}}^T(h_c + h_x) + \\Delta b_{k,y_j}},  (13)\nwhere Mk,yj = exp(\u03bbw\u03bc) and Nk,yj = exp(\\frac{1}{2}\\Delta w_{k,yj}\u03a3\\Delta w_{k,yj}).\nSubsequently, our newly proposed prediction function, referred to as IDA-Softmax, is defined as\nPIDA (2) := \\sum_{k} M_{k,y_j} N_{k,y_j} e^{{Aw_{k,y_j}}^T \\hat{h}_c + \\Delta b_{k,y_j}}  (14)\nConsequently, instead of conducting the augmentation process explicitly, we can directly employ IDA-Softmax, PIDA, for prediction. IDA-Softmax essentially utilizes two modulating factors associated with statistical properties derived from D to calibrate the sample logits. Previous studies (Min et al., 2022c; Chan et al., 2022) have underscored the pivotal role of knowledge about the input data distribution in predictions made by PLMs. Intuitively, PLMs can better capture the patterns and underlying structures within data, such as the spatial relationships between demonstrations and queries, ultimately enhancing their prediction performance. Furthermore, to mitigate the imbalance among different answer types in demonstrations (Holtzman et al., 2021; Zhao et al., 2021), we adopt a post-hoc adjustment approach inspired by Menon et al. (2021), which adjusts predictions by considering the class proportions within D. Thus, the prediction for answer yj is computed as\nP^{IDA}_{Y_j}(x) = PIDA(x) + \u03c4log \u03c0yj, (15)\nwhere 7 is a positive hyperparameter, and \u03c0yj denotes the proportion of answer yj in D. In practical applications, the value of t can be fixed at 1.\nThis approach compensates for predictions of minor classes. When different answers are uniformly distributed, \u03c4log \u03c0yj exerts an equal influence on all answer types. Consequently, the final prediction is given by\n\u0177 = arg min PIDA (2).  (16)\nYjEY"}, {"title": "4 Experimental Setup", "content": "4.1 Models and Datasets\nWe evaluated the performance of IDAICL across seven large PLMs, including GPT-2 (Radford et al., 2019) (with 0.1B, 0.3B, 0.8B, and 1.5B parameters), GPT-Neo (Black et al., 2021) (with 2.7B parameters), and LLaMA (Touvron et al., 2023) (with 13B and 33B parameters). Following previous research (Min et al., 2022a; Han et al., 2023; Lu et al., 2022), our evaluation encompasses ten text classification datasets. Among these, SST-2 (Socher et al., 2013), SST-5 (Socher et al., 2013), MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), and Amazon (McAuley and Leskovec, 2013) are five sentiment classification tasks. Subj (Pang and Lee, 2004), TREC (Voorhees and Tice, 2000), DBPedia (Lehmann et al., 2015), and AGNews (Zhang et al., 2015) cater to subjectivity, question, ontology, and news classification tasks, respectively. Additionally, CB (De Marneffe et al., 2019) is utilized for natural language inference. Among these datasets, SST-5, Amazon, TREC, and CB are characterized by imbalanced training data. Details of all datasets are provided in Section A of the Appendix.\n4.2 Compared Baselines\nBesides Vanilla ICL, we compared and integrated IDAICL with three popular ICL algorithms, focusing on learning process design and demonstration retrieval. These include MetaICL (Min et al., 2022b), Channel ICL (Min et al., 2022a), and Efficient Prompt Retrieval (EPR) (Rubin et al., 2022). Moreover, we compared IDAICL with other advanced prediction calibration methods: Contextual Calibration (ConCa) (Zhao et al., 2021), Prototypical Calibration (PROCA) (Han et al., 2023), and Domain-Context Calibration (D-ConCa) (Fei et al., 2023). Introductions to all compared methods and comprehensive experimental settings are presented in Sections B and C of the Appendix."}, {"title": "5 Experimental Results", "content": "5.1 Main Results\nTable 1 displays the comparison results between IDAICL and four ICL baselines (Vanilla ICL, MetaICL, Channel ICL, and EPR) across GPT-2 models (with 0.8B and 1.5B parameters) and the GPT-Neo model. These results lead to three main findings. Firstly, IDAICL consistently exhibits high effectiveness across various model sizes and datasets, highlighting its strong generalization capacity, even under scenarios involving imbalanced training data. Compared to Vanilla ICL, IDAICL outperforms by an average of 17.7% and 18.4% across diverse datasets and m values for GPT-2 with 0.8B and 1.5B parameters, respectively. Secondly, in comparison to other ICL baselines like Channel ICL, MetaICL, and EPR, the integration of IDAICL consistently delivers notable performance improvements, emphasizing the efficacy of enhancing demonstrations for refined predictions. The inclusion of IDAICL led to an average performance boost of 7.3% for MetaICL and 8.2% for Channel ICL. Lastly, IDAICL notably enhances worst-case accuracy and diminishes performance variance across different seeds, showcasing its ability to improve prediction stability. Additional results on LLAMA and smaller GPT-2 models are available in Tables 7 and 8 of the Appendix.\n5.2 Comparison with Calibration Methods\nWe compared IDAICL with three advanced prediction calibration methods (ConCa, PROCA, and D-ConCa) across three PLMs: GPT-2, GPT-Neo, and LLaMA. Table 2 presents the comparison results for the LLaMA models, where IDAICL consistently achieves state-of-the-art performance, except for TREC using the LLaMA model with 33B parameters. These findings suggest that IDAICL which leverages statistical information derived from the input data distribution for prediction calibration, generally outperforms methods relying on estimated biases for correction. Further comparison results can be found in Table 9 of the Appendix.\n5.3 Stability Analysis\nPrevious studies (Zhao et al., 2021; Sorensen et al., 2022; Min et al., 2022a; Zhang et al., 2022b) have highlighted the considerable variability in ICL's performance. In this section, we verified that IDAICL can effectively enhance performance stability across diverse scenarios.\nVarying numbers of demonstrations We have presented the results across different numbers of demonstrations in Table 1. For a clearer depiction, the outcomes regarding GPT-Neo are illustrated in Figure 3. As the number of demonstration examples (represented by m) increases, both Vanilla ICL and IDAICL exhibit improved performance, emphasizing the importance of comprehensive statistical properties of the input data for IDAICL's effectiveness. Notably, IDAICL significantly enhances performance stability across various numbers of demonstrations and consistently outperforms Vanilla ICL. The performance improvement is particularly pronounced when m takes on smaller values, indicating the efficacy of IDAICL in enriching the available knowledge for PLMs.\nVarying demonstrations To confirm that augmenting demonstrations can enhance the robustness of the ICL strategy across various demonstrations, we investigated three distinct demonstration selection settings. Setting I: Training samples most similar to the test sample are chosen. Setting II: Samples are randomly selected from the training data. Setting III: Training samples exhibiting the greatest dissimilarity from the test sample are selected. As shown in Figures 4(a) and (b), IDAICL significantly outperforms Vanilla ICL and demonstrates greater robustness across the three selection settings. Additionally, our discoveries suggest that selecting demonstrations that are more similar to the test samples leads to better performance than exclusively selecting dissimilar ones, which aligns with the findings obtained by Wang et al. (2022).\nVarying templates To assess the performance of IDAICL across various templates, we employed fifteen templates on the SST-2 dataset following those outlined by Zhao et al. (2021). The templates are elaborated in Table 10 of the Appendix. Figures 4(c) and (d) display the performance of Vanilla ICL and IDAICL across six templates. Some templates achieve higher average performance than others. Nevertheless, IDAICL consistently enhances both average and worst-case accuracy, simultaneously reducing performance variance across different templates. The complete results are available in Figure 7 of the Appendix.\nImpact of imbalance in labels Figures 5(a) and (b) depict comparison results among Vanilla ICL, MetaICL, Channel ICL, and IDAICL across different degrees of imbalances. It is evident that the performance of Vanilla ICL is sensitive to class imbalance, while that of IDAICL and Channel ICL exhibit robustness to the imbalance. Moreover, notable performance improvements are observed with higher levels of imbalance. Additionally, Figures 5(c) and (d) illustrate the confusion matrices for CR and Subj datasets, with the proportion of one category (i.e., \"Negative\" and \"Subjective\") in demonstrations setting to 0.1 and 0.2. IDAICL significantly improves the accuracy of the underrepresented classes when compared to Vanilla ICL, thereby contributing to enhanced fairness among classes. In the subsequent section, we demonstrate that the strong performance of IDAICL in handling imbalanced label distributions stems from both the statistical properties and the class proportion term.\n5.4 Sensitivity and Ablation Studies\nWe conducted ablation studies on IDAICL to investigate the influence of the two modulating factors and the class proportion term. The parameters \u5165 and govern the augmentation strength and the impact of the class proportion term, respectively. In Figure 6(a), a significant performance drop is observed when predictions are not calibrated using statistical properties derived from the demonstrations. Additionally, optimal performance is achieved when A equals 0.5.\nFigure 6(b) showcases the accuracy of SST-2 and MR datasets with the negative class proportion in demonstrations setting to 0.1. Results indicate that solely leveraging statistical properties (i.e., Tequals 0) enhances performance under imbalanced demonstrations, with further improvements observed upon the inclusion of the class proportion term. Additionally, optimal performance is attained when T equals 1. Consequently, we recommend setting A to 0.5 and 7 to 1 for practical applications. More results are presented in Appendix F.\n5.5 Further Discussion\nTo further investigate the effect of statistical properties within demonstrations on model performance, we exclusively employed queries along with statistical information for inference, excluding the inclusion of demonstrations for each test sample. These statistics were estimated using deep features of all training samples. As shown in Table 3, IDAICL relying solely on statistical properties distinctly outperforms Vanilla ICL across scenarios with zero, one, and even four demonstrations. This emphasizes the crucial role of prior statistics obtained from training data in PLMs' predictions. This phenomenon is understandable as statistical properties inherently encompass richer global information compared to individual demonstrations."}, {"title": "6 Conclusion", "content": "This study introduces IDAICL, a novel ICL approach designed to enhance demonstrations by utilizing semantic directions sampled from the deep feature distribution of demonstration examples. Our augmentation strategy enriches the knowledge available to PLMs without extending the context length. A new prediction function is then theoretically established considering the number of augmented pieces approaching infinity. This eliminates the need for explicit augmentation and allows for direct utilization of this derived function for predictions. Our extensive experiments, spanning various tasks and PLMs, demonstrate that IDAICL significantly enhances both prediction accuracy and stability when compared to other ICL baselines."}, {"title": "Limitations", "content": "While IDAICL proves to be competitive in few-shot learning, there are limitations that open up avenues for future research. First, due to the necessity of accessing the parameters of the final fully connected layer in PLMs, IDAICL is exclusively suitable for open-source models. Future research is expected to develop alternative augmentation strategies tailored for black-box PLMs. Second, our evaluation of IDAICL focused on seven PLMs and ten text classification tasks. We defer further explorations involving other PLMs and non-classification tasks for future work. Additionally, IDAICL relies on a small set of demonstrations to estimate the feature mean and covariance matrix. If such a collection is unavailable or extremely scarce, IDAICL may need to be used in conjunction with demonstration generation methods.\nOther avenues for future work involve exploring more effective augmentation distributions. This entails exploring finer-grained distributions, such as category-level or sample-level distributions, to emphasize the unique characteristics of individual categories or samples, and extending these distributions beyond the constraints of training data. Furthermore, given the effectiveness of data augmentation in model training, future research could explore the utilization of our derived prediction function in both the training and fine-tuning phases of large PLMs."}]}