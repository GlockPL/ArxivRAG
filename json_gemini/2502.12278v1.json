{"title": "Towards Practical First-Order Model Counting", "authors": ["Ananth K. Kidambi", "Guramrit Singh", "Paulius Dilkas", "Kuldeep S. Meel"], "abstract": "First-order model counting (FOMC) is the problem of counting the number of models of a sentence in first-order logic. Since lifted inference techniques rely on reductions to variants of FOMC, the design of scalable methods for FOMC has attracted attention from both theoreticians and practitioners over the past decade. Recently, a new approach based on first-order knowledge compilation was proposed. This approach, called CRANE, instead of simply providing the final count, generates definitions of (possibly recursive) functions that can be evaluated with different arguments to compute the model count for any domain size. However, this approach is not fully automated, as it requires manual evaluation of the constructed functions. The primary contribution of this work is a fully automated compilation algorithm, called GANTRY, which transforms the function definitions into C++ code equipped with arbitrary-precision arithmetic. These additions allow the new FOMC algorithm to scale to domain sizes over 500,000 times larger than the current state of the art, as demonstrated through experimental results.", "sections": [{"title": "Introduction", "content": "First-order model counting (FOMC) is the task of determining the number of models for a sentence in first-order logic over a specified domain. The weighted variant, WFOMC, computes the total weight of these models, linking logical reasoning with probabilistic frameworks [30]. It builds upon earlier efforts in weighted model counting for propositional logic [4] and broader attempts to bridge logic and probability [13, 15, 19]. WFOMC is central to lifted inference, which enhances the efficiency of probabilistic calculations by exploiting symmetries [10]. Lifted inference continues to advance, with applications extending to constraint satisfaction problems [24] and probabilistic answer set programming [1]. Moreover, WFOMC has proven effective at reasoning over probabilistic databases [8] and probabilistic logic programs [17]. FOMC algorithms have also facilitated breakthroughs in discovering integer sequences [21] and developing recurrence relations for these sequences [6]. Recently, these algorithms have been extended to perform sampling tasks [31]."}, {"title": "Preliminaries", "content": "We begin this section by describing some notation that we will use throughout the paper. Then, in Sections 2.1 and 2.2, we introduce the basic terminology of first-order logic and formally define (W)FOMC. Section 2.3 outlines the principles of FOKC, particularly in the context of CRANE. Finally, in Section 2.4, we introduce the algebraic terminology used to describe the output of CRANE, i.e., functions and equations that define them.\n\nWe use $\\mathbb{N}_0$ to represent the set of non-negative integers. In both algebra and logic, we write $S\\sigma$ to denote the application of a substitution $\\sigma$ to an expression $S$, where $\\sigma = [X_1 \\rightarrow Y_1, X_2 \\rightarrow Y_2,..., X_n \\rightarrow Y_n]$ signifies the replacement of all instances of $x_i$ with $y_i$ for all $i = 1,...,n$.\nAdditionally, for any variable n and a, b \u2208 No, let $[a \\leq n \\leq b] := \\begin{cases}\n1 & \\text{if } a < n < b \\\\\n0 & \\text{otherwise}\n\\end{cases}$"}, {"title": "First-Order Logic", "content": "This section reviews the basic concepts of first-order logic used in FOKC algorithms. We focus specifically on the format used internally by FORCLIFT and its descendants. See Section 2.3.1 for a brief overview of how GANTRY transforms an arbitrary sentence in first-order logic into this internal format.\n\nA term can be either a variable or a constant. An atom can be either $P(t_1,..., t_m)$ (i.e., $P(t)$) for some predicate P and terms $t_1,...,t_m$, or $x = y$ for some terms x and y. The arity of a predicate is the number of arguments it takes, i.e., m in the case of the predicate P mentioned above. We write P/m to denote a predicate along with its arity. A literal can be either an atom (i.e., a positive literal) or its negation (i.e., a negative literal). An atom containing no variables, only constants, is called ground. A clause is of the form\n$\\forall x_1 \\in \\Delta_1. \\forall x_2 \\in \\Delta_2... \\forall x_n \\in \\Delta_n. \\Phi(x_1,x_2,...,x_n)$, where $\\Phi$ is a disjunction of literals that contain only the variables $x_1,..., x_n$ (and any constants). We say that a clause is a (positive) unit clause if there is only one literal with a predicate, and it is a positive literal. Finally, a sentence is a conjunction of clauses. Throughout the paper, we will use set-theoretic notation, interpreting a sentence as a set of clauses and a clause as a set of literals.\n\nConforming to previous work [30], the definition of a clause includes universal quantifiers for all its variables. While it is possible to rewrite the entire sentence with all quantifiers at the front, the format we describe has proven convenient for practical use."}, {"title": "First-Order Model Counting", "content": "In this section, we will formally define FOMC and its weighted variant. Although this work focuses on FOMC, computing the FOMC using GANTRY requires using WFOMC for sentences with existential quantifiers. For such sentences, preprocessing (described in Section 2.3.1) introduces predicates with non-unary weights that must be accounted for to compute the correct model count.\n\nLet $\\phi$ be a sentence. For each predicate P/n in $\\phi$, let $(\\Delta_P)^n$ be a list of the corresponding domains. Let $\\sigma$ be a map from the domains of $\\phi$ to their interpretations as finite sets, ensuring the sets are pairwise disjoint and contain the corresponding constants from $\\phi$. A structure of $\\phi$ is a set $M$ of ground literals defined by adding to $M$ either $P(t)$ or $\\neg P(t)$ for every predicate $P/n$ in $\\phi$ and every n-tuple $t \\in \\Pi \\sigma(\\Delta_P)$. A structure is a model if it makes $\\phi$ valid.\n\nLet us consider the following sentence (previously examined by Dilkas and Belle [6]) that defines predicate P as a bijection between two domains $\\Gamma$ and $\\Delta$:\n\n$\\begin{aligned}\n&(\\forall x \\in \\Gamma. \\exists y \\in \\Delta. P(x,y)) \\land \\\\\n&(\\forall y \\in \\Delta. \\exists x \\in \\Gamma. P(x, y)) \\land \\\\\n&(\\forall x \\in \\Gamma. \\forall y, z \\in\\Delta. P(x, y) \\land P(x, z) \\Rightarrow y = z) \\land \\\\\n&(\\forall x, z \\in \\Gamma. \\forall y \\in \\Delta. P(x, y) \\land P(x,y) \\Rightarrow x = z).\n\\end{aligned}$"}, {"title": "Crane and First-Order Knowledge Compilation", "content": "As our work builds on CRANE, in this section, we will briefly outline the steps CRANE goes through to compile a sentence into a set of function definitions. We divide the inner workings of the algorithm into two stages: preprocessing and compilation."}, {"title": "Preprocessing", "content": "This stage transforms an arbitrary sentence into the format described in Section 2.1, primarily by eliminating existential quantifiers. For example, the first conjunct of sentence (1), i.e.,\n\n$\\forall x \\in \\Gamma. \\exists y \\in \\Delta. P(x,y)$\n\nis transformed into\n\n$\\begin{aligned}\n&(\\forall x \\in \\Gamma. Z(x)) \\land \\\\\n&(\\forall x \\in \\Gamma. \\forall y \\in \\Delta. Z(x) \\lor \\neg P(x,y)) \\land \\\\\n&(\\forall x \\in \\Gamma. S(x) \\lor Z(x)) \\land \\\\\n&(\\forall x \\in \\Gamma. \\forall y \\in \\Delta. S(x) \\lor \\neg P(x,y)),\n\\end{aligned}$\n\nwhere Z/1 and S/1 are two new predicates with $w^-(S) = -1$. One can verify that the WFOMC of sentences (3) and (4) is the same."}, {"title": "Compilation", "content": "After preprocessing, CRANE compiles the sentence into the triple (E, F, D), where E is the set of equations, and F and D are auxiliary maps. F maps function names to sentences. D maps function names and argument indices to domains. E can contain any number of functions, one of which (which we will always denote as f) represents the solution to the FOMC problem. Computing the FOMC for specific domain sizes involves invoking f with those sizes as inputs. D records this correspondence between function arguments and domains.\n\nCRANE compiles sentence (1) for bijection counting into\n\n$E = \\begin{cases}\n\nf(m,n) = \\sum_{l=0}^{n} {n \\choose l}(-1)^{n-l}g(l,m), \\\\\n\\\\\ng(l,m) = \\sum_{k=0} {m \\choose k}g(l - 1, m - k)\n\\end{cases}$\n\n$D = { (f, 1) \\leftrightarrow \\Gamma, (f, 2) \\leftrightarrow \\Delta, (g, 1) \\leftrightarrow \\Delta, (g, 2) \\leftrightarrow \\Gamma }$,\n\nwhere $\\Delta_T$ is a newly introduced domain. (We omit the definition of F as the sentences can become quite verbose.) To compute the number of bijections between two sets of cardinality 3, one would evaluate f(3,3); however, the definition of g is incomplete: g is a recursive function presented without any base cases. D encodes that in f(m, n), m and n represent |\u0393| and |\u0394|, respectively. Similarly, in g(l, m), l represents |\u0394|, and m represents |\u0393|.\n\nCompilation is performed primarily by applying (compilation) rules to sentences. CRANE has two modes depending on the selection process for compilation rules when multiple alternatives are available. The first option is to use greedy search: there is a list of rules, and the first applicable rule is the one that gets used, disregarding all the others. The second option is to use a combination of greedy and breadth-first search (BFS). In this approach, we classify each compilation rule as greedy or non-greedy. Greedy rules are applied as soon as possible at any stage of the compilation process, while BFS is executed over all applicable non-greedy rules, identifying the solution that necessitates the smallest number of non-greedy rules."}, {"title": "Algebra", "content": "In this paper, we use both logical and algebraic constructs. While the rest of Section 2 focused on the former, this section describes the latter. We write expr for an arbitrary algebraic expression. In the context of algebra, a constant is a non-negative integer. Likewise, a variable can either be a parameter of a function or a variable introduced through summation, such as i in the expression $\\sum_{i=-1}^{n}$ expr. A function call is $f(x_1,...,x_n)$ (or f(x) for short), where f is an n-ary function, and each $x_i$ is an algebraic expression consisting of variables and constants. A (function) signature is a function call that contains only variables. Given two function calls, f(x) and f(y), we say that f(y) matches f(x) if $x_i = y_i$ whenever $x_i, y_i \\in \\mathbb{N}_0$.\nAn equation is $f(x) = expr$, where $f(x)$ is a function call.\n\nLet f(x) be a function call where each $x_i$ is either a constant or a variable. Then the function call f(y) is a base case of f(x) if $f(y) = f(x)\\sigma$, where $\\sigma$ is a substitution that replaces one or more variable $x_i$'s with a constant while leaving constants unchanged."}, {"title": "Technical Contributions", "content": "Figure 1 provides an overview of GANTRY's workflow. We will briefly describe and motivate each procedure before going into more detail in the corresponding subsection.\n\nCompileWithBaseCases (see Section 3.1), the core procedure of GANTRY, is responsible for completing the function definitions produced by CRANE with the necessary base cases. To do so, it may recursively call itself (and CRANE) on other sentences. We prove that the number of such recursive calls is upper bound by the number of domains.\n\nSection 3.1 also describes the Simplify procedure for algebraic simplification. It is crucial for simplifying, e.g., a sum of n terms, only some of which are non-zero. More generally, the equations returned by CRANE often benefit from easily detectable algebraic simplifications such as 0 * anything = 0 and anything\u2070 = 1.\n\nFindBaseCases (described in Section 3.2) inspects a set of equations to identify a sufficient set of base cases for a given set of equations. We prove that the returned set of base cases ensures that the evaluation of the resulting function definitions will never get stuck in an infinite loop.\n\nSection 3.3 introduces the Propagate procedure, which takes a sentence $\\phi$, a domain $\\Delta$, and n \u2208 No. It returns $\\phi$ transformed under the assumption that |\u2206| = n, with n new constants added and all variables quantified over $\\Delta$ eliminated. For example, when computing a base case such as f(0, y), Propagate will significantly simplify $\\phi$ with the assumption that the domain associated with the first parameter of f (i.e., D(f, 1)) is empty. CompileWithBaseCases (Propagate($\\phi$, D(f, 1), 0)) will then return the equations for the base case f(0, y).\n\nSection 3.4 describes a new smoothing procedure that ensures Propagate preserves the correct model count. Smoothing is a well-known technique in knowledge compilation algorithms for propositional model counting [5]. Although FOMC algorithms have used smoothing before [30], our setting requires a novel approach."}, {"title": "Completing the Definitions of Functions", "content": "Algorithm 1 presents our overall approach for compiling a sentence into equations that include the necessary base cases. First, we use CRANE to compile the sentence into three components: E, F, and D (as described in Section 2.3.2). After some algebraic simplifications (described below), the algorithm passes E to the FindBaseCases procedure (see Section 3.2). For each base case f(x), we retrieve the sentence F(f) associated with the function name f and simplify it using the Propagate procedure (explained in detail in Section 3.3). We do this by iterating over all indices of x, where $x_i$ is a constant, and using Propagate to simplify $\\psi$ by assuming that domain D(f, i) has size $x_i$. Finally, on line 6, CompileWithBaseCases recurses on these simplified sentences and adds the resulting base case equations to E."}, {"title": "Simplify", "content": "The main responsibility of the Simplify procedure is to handle the algebraic pattern $\\sum_{m=0}^{n}[a \\leq m \\leq b]f(m)$. Here, n is a variable, a and b are constants, and f is an expression that may depend on m. Simplify transforms this pattern into $f(a) + f(a + 1) + \\ldots + f(\\text{min}{n, b})$.\n\nWe return to the bijection-counting problem from Example 2 and its initial solution described in Example 5. Simplify transforms\n\n$g(l,m) = \\sum_{k=0}^{m}[0 \\leq k \\leq 1] {m \\choose k}g(l-1, m- k)$\ninto\n\n$g(l, m) = g(l - 1, m) + mg(l - 1, m - 1)$.\n\nThen FindBaseCases identifies two base cases: g(0,m) and g(1,0). In both cases, CompileWithBaseCases recurses on the sentence F(g) simplified by assuming that one of the domains is empty. In the first case, we recurse on the sentence $\\forall x \\in \\Gamma. S(x) \\lor \\neg S(x)$,"}, {"title": "Identifying a Sufficient Set of Base Cases", "content": "Algorithm 2 summarises the implementation of FindBaseCases. It considers two types of arguments when a function f calls itself recursively: constants and arguments of the form $x_i - c_i$. When the argument is a constant $c_i$, line 5 adds a base case with $c_i$ to the set of all base cases B. In the second case, line 7 adds a base case to B for each constant from 0 to (but not including) $c_i$.\n\nConsider the recursive function g from Example 5. FindBaseCases iterates over two function calls: g(l - 1, m) and g(1 \u2013 1, m \u2013 1). The former produces the base case g(0, m), while the latter produces both g(0, m) and g(1, 0).\n\nIn the rest of this section, we will show that the base cases identified by FindBaseCases are sufficient for the algorithm to terminate. Let $\\varepsilon$ denote the equations returned by CompileWithBaseCases.\n\nGiven an n-ary function f in $\\mathcal{E}$ and $x \\in \\mathbb{N}$, the evaluation of f(x) terminates.\n\nWe prove Theorem 13 using double induction. First, we apply induction to the number of functions in $\\mathcal{E}$. Then, we use induction on the arity of the 'last' function in $\\mathcal{E}$ according to a topological ordering. Before proving Theorem 13, we make a few observations about this and previous [6, 30] work.\n\nFor each function f, there is precisely one equation $e \\in \\mathcal{E}$ with f(x) on the LHS where all $x_i$'s are variables (i.e., e is not a base case). We refer to e as the definition of f.\n\nThere is a topological ordering $(f_i)_i$ of all functions in $\\mathcal{E}$ such that equations in $\\mathcal{E}$ with $f_i$ on the LHS do not contain function calls to $f_j$ with j > i.\n\nObservation 15 prevents mutual recursion and other cyclic scenarios.\n\nFor each equation (f(x) = expr) $\\in \\mathcal{E}$, the evaluation of expr terminates when provided with the values of all relevant function calls.\n\nIf f is a non-recursive function with no function calls on the RHS of its definition, then the evaluation of any function call f(x) terminates.\n\nFor each equation (f(x) = expr) $\\in \\mathcal{E}$, if x contains only constants, then expr cannot include any function calls to f.\n\nAdditionally, we introduce an assumption about the structure of recursion.\n\nFor each equation (f(x) = expr) $\\in \\mathcal{E}$, every recursive function call f(y) $\\in$ expr satisfies the following:\neach $y_i$ is either $x_i - c_i$ or $c_i$ for some constant $c_i$ and\nthere exists i such that $y_i = x_i - c_i$ for some $c_i > 0$.\n\nFinally, we assume a particular order of evaluation for function calls using the equations in $\\mathcal{E}$; specifically, base cases precede the recursive definition.\n\nWhen multiple equations in $\\mathcal{E}$ match a function call f(x), preference is given to the equation with the most constants on its LHS."}, {"title": "Propagating Domain Size Assumptions", "content": "Algorithm 3, called Propagate, modifies the sentence $\\phi$ based on the assumption that |$\\Delta$| = n. When n = 0, some clauses become vacuously satisfied and can be removed. When n > 0, partial grounding replaces all variables with domain A with constants. (None of the sentences examined in this work had n > 1.) Algorithm 3 handles these two cases separately. For a literal or clause C, we write the set of corresponding domains as Doms(C). In the case of n = 0, there are three types of clauses to consider:\nthose that do not mention A,\nthose in which every literal contains variables quantified over A and\nthose with some literals containing such variables and some without.\n\nWe transfer clauses of Type 1 to the new sentence $\\phi$' without any changes. For clauses of Type 2, $C'$ is empty, so these clauses are filtered out. As for clauses of Type 3, lines 7-9 perform a new kind of smoothing, the explanation of which we defer to Section 3.4.\n\nIn the case of n > 0, n new constants are introduced. Let C be an arbitrary clause in $\\phi$, and let m \u2208 No be the number of variables in C quantified over A. If m = 0, C is added directly to $\\phi'$. Otherwise, a clause is added to $\\phi'$ for every possible combination of replacing the m variables in C with the n new constants.\n\nLet $C := \\forall x \\in \\Gamma. \\forall y, z \\in \\Delta. \\neg P(x,y) \\lor \\neg P(x,z) \\lor y = z$. Then Doms(C) = Doms($\\neg P(x,y)$) = Doms($\\neg P(x,z)$) = {$\\Gamma,\\Delta$}, and Doms(y = z) = {$\\Delta$}. A call to Propagate({C}, $\\Delta$, 3) would result in the following sentence with nine clauses:\n\n$\\begin{aligned}\n&(\\forall x \\in \\Gamma. \\neg P(x,c_1)\\lor \\neg P(x, c_1) \\lor c_1 = c_1) \\land \\\\\n&(\\forall x \\in \\Gamma. \\neg P(x,c_1)\\lor \\neg P(x, c_2) \\lor c_1 = c_2) \\land \\\\\n&:\\\\\n&(\\forall x \\in \\Gamma. \\neg P(x,c_3)\\lor \\neg P(x, c_3) \\lor c_3 = c_3).\n\\end{aligned}$\n\nHere, $c_1, c_2$, and $c_3$ are the new constants."}, {"title": "Smoothing the Base Cases", "content": "Smoothing modifies a circuit to reintroduce eliminated atoms, ensuring the correct model count [5, 30]. This section describes a similar process performed on lines 7\u20139 of Algorithm 3. Line 7 checks if smoothing is necessary, and lines 8 and 9 execute it. If the condition on line 7 is not satisfied, the clause is not smoothed but omitted.\n\nSuppose CompileWithBaseCases calls Propagate with arguments (\u03a6, \u0394, 0), i.e., we are simplifying the sentence $\\phi$ by assuming that the domain \u0394 is empty. Informally, if there is a predicate P in $\\phi$ unrelated to A, smoothing preserves all occurrences of P, even if all clauses with P become vacuously satisfied.\n\nLet $\\phi$ be\n\n$\\begin{aligned}\n&(\\forall x \\in A. \\forall y, z \\in \\Gamma. Q(x) \\lor P(y, z)) \\land \\\\\n&(\\forall y, z \\in \\Gamma'. P(y, z)),\n\\end{aligned}$"}, {"title": "Generating C++ Code", "content": "In this section, we will describe the final step of GANTRY as outlined in Figure 1, i.e., translating the set of equations $\\varepsilon$ into C++ code. Recall that this step is crucial for the usability of the algorithm; otherwise, function definitions would remain purely mathematical, with no convenient way to compute the model count for particular domain sizes. Once a C++ program is produced, it can be executed with different command-line arguments to determine the model count of the sentence for various domain sizes.\n\nSee Algorithm 4 for the typical structure of a generated C++ program. Each equation in $\\varepsilon$ turns into a C++ function with a separate cache for memoisation. Hence, Algorithm 4 has a function and a cache for f(\u00b7,\u00b7), g(\u00b7,\u00b7), g(, 0), and g(0,). The implementation of an equation consists of three parts. First (on line 5), we check if the arguments already exist in the corresponding cache. If they do, we return the cached value. Second (on lines 6 and 7), we check if the arguments match any of the base cases (as defined in Section 2.4). If so,"}, {"title": "Experimental Evaluation", "content": "Our empirical evaluation sought to compare the runtime performance of GANTRY with the current state of the art, namely FASTWFOMC\u00b2 [23, 25] and FORCLIFT\u00b3. Our experiments involved two versions of GANTRY: GANTRY-GREEDY and GANTRY-BFS. Like its predecessor (see Section 2.3.2), GANTRY has two modes for applying compilation rules to sentences: one that uses a greedy search algorithm similar to FORCLIFT and another that combines greedy and BFS.\n\nThe experiments used an Intel Skylake 2.4 GHz CPU with 188 GiB of memory and CentOS 7. We used the Intel C++ Compiler 2020u4 for C++ programs, Julia 1.10.4 for FASTWFOMC, and the Java Virtual Machine 1.8.0_201 for FORCLIFT and GANTRY. Although implemented in different languages, GANTRY and FASTWFOMC use the same GNU Multiple Precision Arithmetic Library for arbitrary-precision arithmetic.\n\nWe ran each algorithm on each benchmark using domains of size 2\u00b9, 2\u00b2, 2\u00b3, and so on until an algorithm failed to handle a domain size due to a timeout (of one hour) or out-of-memory or out-of-precision errors. While we separately measured compilation and inference time, we primarily focus on total runtime, dominated by the latter. We verified the accuracy of the numerical answers using the corresponding integer sequences in the On-Line Encyclopedia of Integer Sequences [16]."}, {"title": "Benchmarks", "content": "We compare these algorithms using three benchmarks from previous work. The first bench-mark is the bijection-counting problem from Example 2. The second benchmark is a variant of the well-known Friends & Smokers Markov logic network [20, 28], which takes the form of\n\n(\\forall x,y \u2208 \u0394. S(x) \\land F(x,y) \u21d2 S(y)) \\land (\\forall x \u2208 \u0394. S(x) \u21d2 C(x)).\n\nIn this sentence, we have three predicates, S, F, and C, that denote smoking, friendship, and cancer, respectively. The first clause states that friends of smokers are also smokers, and the second clause asserts that smoking causes cancer. Common additions to this sentence include making the friendship relation symmetric and assigning probabilities to each clause. Finally, we include the function-counting problem [6]\n\n(\\forall x \u2208 \u0393. \\exists y \u2208 \u0394. P(x, y)) \\land (\\forall x \u2208 \u0393. \\forall y, z \u2208 \u0394. P(x,y) \\land P(x, z) \u21d2 y = z)\n\nas our third benchmark. Here, predicate P represents a function from \u0393 to \u0394. The first clause asserts that each x must have at least one corresponding y, while the second clause ensures the uniqueness of such a y.\n\nWe formulate the Bijections and Functions benchmarks using two domains, \u0393 and \u0394, as this formulation is known to help FOKC algorithms find efficient solutions [6]. To compare GANTRY and FORCLIFT with FASTWFOMC, which has no support for multiple domains, we set |\u0393| = |\u0394\u0399."}, {"title": "Results", "content": "Figure 2 presents a summary of the experimental results. Only FASTWFOMC and GANTRY- BFS could handle the bijection-counting problem. For this benchmark, the largest domain sizes these algorithms could accommodate were 64 and 4096, respectively. On the other two benchmarks, FORCLIFT had the lowest runtime. However, since it can only handle model counts smaller than 231, it only scales up to domain sizes of 16 and 128 for Friends & Smokers and Functions, respectively. FASTWFOMC outperformed FORCLIFT in the case of Friends & Smokers, but not Functions, as it could handle domains of size 1024 and 64, respectively. Furthermore, both GANTRY-BFS and GANTRY-GREEDY performed similarly on both benchmarks. Similarly to the Bijections benchmark, GANTRY significantly outperformed the other two algorithms, scaling up to domains of size 8192 and 67,108,864, respectively.\n\nOne might notice that the runtime of FASTWFOMC and FORCLIFT is slightly higher for the smallest domain size. This peculiarity is the consequence of just-in-time (JIT) compilation. As GANTRY is only run once per benchmark, we include the JIT compilation time in its overall runtime across all domain sizes. Additionally, while the compilation time of FORCLIFT is generally lower compared to GANTRY, neither significantly affects the overall runtime. Specifically, FORCLIFT compilation typically takes around 0.5s, while GANTRY compilation takes around 2.3 s.\n\nBased on our experiments, which algorithm should one use in practice? If FORCLIFT can handle the sentence and the domain sizes are reasonably small, it is likely the fastest algorithm. In other situations, GANTRY will likely be significantly more efficient than FASTWFOMC regardless of domain size, provided both algorithms can handle the sentence."}, {"title": "Conclusion and Future Work", "content": "In this work, we have presented a scalable, automated FOKC-based approach to FOMC. Our algorithm involves completing the definitions of recursive functions and subsequently translating all function definitions into C++ code. Empirical results demonstrate that GANTRY can scale to larger domain sizes than FASTWFOMC while supporting a wider range of sentences than FORCLIFT. The ability to efficiently handle large domain sizes is particularly crucial in the weighted setting, as illustrated by the Friends & Smokers example, where the model captures complex social networks with probabilistic relationships. Without this scalability, these models would have limited practical value.\n\nFuture directions for research include conducting a comprehensive experimental com- parison of FOMC algorithms to better understand their comparative performance across various sentences. The capabilities of GANTRY could also be characterised theoretically, for example, by proving completeness for logic fragments liftable by other algorithms [11, 22, 26]. Additionally, the efficiency of FOMC algorithms can be further analysed using fine-grained complexity, which would provide more detailed insights into the computational demands of different sentences."}]}