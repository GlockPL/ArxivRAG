{"title": "Assessing Modality Bias in Video Question Answering Benchmarks with Multimodal Large Language Models", "authors": ["Jean Park", "Kuk Jin Jang", "Basam Alasaly", "Sriharsha Mopidevi", "Andrew Zolensky", "Eric Eaton", "Insup Lee", "Kevin Johnson"], "abstract": "Multimodal large language models (MLLMs) can simultaneously process visual, textual, and auditory data, capturing insights that complement human analysis. However, existing video question-answering (VidQA) benchmarks and datasets often exhibit a bias toward a single modality, despite the goal of requiring advanced reasoning skills that integrate diverse modalities to answer the queries.\nIn this work, we introduce the modality importance score (MIS) to identify such bias. It is designed to assess which modality embeds the necessary information to answer the question. Additionally, we propose an innovative method using state-of-the-art MLLMs to estimate the modality importance, which can serve as a proxy for human judgments of modality perception. With this MIS, we demonstrate the presence of unimodal bias and the scarcity of genuinely multimodal questions in existing datasets. We further validate the modality importance score with multiple ablation studies to evaluate the performance of MLLMs on permuted feature sets. Our results indicate that current models do not effectively integrate information due to modality imbalance in existing datasets. Our proposed MLLM-derived MIS can guide the curation of modality-balanced datasets that advance multimodal learning and enhance MLLMs' capabilities to understand and utilize synergistic relations across modalities.", "sections": [{"title": "1 Introduction", "content": "In recent years, trends in AI development have leaned towards multimodal models, particularly multimodal large language models (MLLMs), as many complex problems necessitate the integration of diverse modalities to achieve more accurate and comprehensive reasoning.\nVideo question answering (VidQA) stands out as a particularly challenging task, requiring the integration of various modalities along with complex spatial and temporal reasoning (Xiao et al. 2021). As such, this task serves as a vital benchmark for assessing the vision-language understanding capabilities of AI systems.\nIn recent years, several VidQA benchmarks have been developed to train and evaluate the capabilities of MLLMs in these areas (Yu et al. 2019; Gupta et al. 2022). However, a fundamental question remains: Are these models genuinely integrating information from various sources, or are they simply leveraging biases inherent in the datasets? Our observations suggest that many existing benchmarks are limited in their ability to assess this integration. The questions often tend to be biased toward a single modality, or modality bias, lacking the complexity that would require genuine multimodal integration. For instance, the video question Q1 depicted in Fig. 1b can be answered using only the video alone or the subtitles alone. Although having redundant information across modalities may be beneficial for learning cross-modal relationship, it doesn't fully represent the complexity of real-world multimodal reasoning tasks.\nAs illustrated in Q2 from Fig. 1b, some multimodal questions require integrating distinct pieces of information from the text (not wanting to go to the hospital) and from the video (material of clothing) to accurately deduce the answer. Unfortunately, such questions that demand genuine integration of multiple modalities are notably scarce in current datasets.\nTo address these limitations, we need a method that quantitatively assesses modality bias in questions. To this end, we introduce a novel modality importance score (MIS), which evaluates the extent to which each modality contributes to answering a given question. Using this score, we perform a comprehensive assessment of modality bias in existing VidQA benchmarks. Our analysis reveals significant limitations in current datasets and highlights the need for more balanced and challenging multimodal questions.\nOur main contributions are as follows:\n\u2022 We propose a novel modality importance score (MIS) and a method that leverages multimodal large language models (MLLMs) to estimate the MIS. We show that this approach could serve as a proxy for human judgements of modality perception.\n\u2022 Using the proposed modality importance score, we demonstrate the existence of a unimodal bias and the scarcity of truly multimodal questions in current multimodal datasets.\n\u2022 We evaluate several state-of-the-art multimodal models on questions with permuted features for modalities with low importance scores. The results reveal that current multimodal models do not optimally combine information from different sources due to modality imbalance in existing multimodal datasets.\nBy addressing these limitations in VidQA benchmarks,"}, {"title": "2 Related Work", "content": "Video question answering (VidQA) is a well-explored field in AI, presenting the challenge of integrating multimodal input from videos, understanding temporal and causal relations, and selecting the correct answer (Lei et al. 2019). Many recent VidQA models are pretrained on large datasets using contrastive learning objectives (Kim et al. 2021), masked language modeling (Fu et al. 2021), and other techniques to learn joint representations and improve spatio-temporal understanding (Zhao et al. 2017; Jiang et al. 2020). These models are subsequently fine-tuned on downstream tasks, such as open-ended or multiple choice video-question answering (Wang et al. 2023), video-text retrieval (Luo et al. 2020), and video captioning (Fu et al. 2023).\nIn this study, we focus on four approaches that have been developed to utilize both subtitle and video information for video question answering. Merlot Reserve (Zellers et al. 2022) is pretrained to predict either the correct text or audio snippet hidden by a MASK token, given uniformly sampled images from a video. Its architecture includes pretrained encoders for each modality input and a joint encoder trained with a contrastive spanning objective. FrozenBiLM (Yang et al. 2022a) employs a frozen bidirectional language model trained on web-scale multimodal data. Llama-VQA (Ko et al. 2023) builds upon the Llama model, incorporating additional learnable parameters through the Flipped-VQA framework. This approach leverages the LLM's prior knowledge of temporal and causal reasoning. MiniGPT4-Video (Ataallah et al. 2024) is an open-source multimodal large language model designed for video-language tasks. Its training process involves pretraining using either Llama2 or Mistral on video-text pairs consisting of frame sequences and subtitles appended to a predefined prompt. In addition, other VidQA approaches utilize captions and videos, such as VindLu or MMFT-BERT, and MSAN (Cheng et al. 2023; Khan et al. 2020; Kim et al. 2020). Additional tasks and approaches outside the scope of this study can be found in a survey by Zhong et al. (2022).\nWhile these models show improved performance by integrating language and video inputs for video understanding, a critical issue remains: they are trained on datasets that have questions with modality bias. This bias raises the question of whether these models can leverage both modalities for each question and context and whether they are biased in their ability to leverage either modality as appropriate. Our research examines whether current models can effectively identify and use the most relevant modality, even with irrelevant information. Our findings reveal limitations in their ability to perform this task optimally."}, {"title": "2.2 VidQA Datasets and Benchmarks", "content": "Several notable datasets and benchmarks have been proposed for multiple-choice VidQA approaches.\nTVQA The TVQA dataset (Lei et al. 2018) comprises over 150K question-answer pairs derived from 21,793 clips across six TV shows. These clips average 76 seconds, with each question providing a localized timestamp indicating where the answer can be found within the clip.\nIn TVQA's test-public set, human accuracy varied across different modality combinations: 61.96% for video-only, 73.03% for subtitles-only, and 89.41% for both. While the authors interpret this result as evidence for the necessity of both visual and textual understanding, we propose an alternative perspective. We hypothesize that many questions in the dataset contain redundant information across both video and subtitle sources rather than requiring the integration of information from distinct sources. Furthermore, we believe this result insufficiently captures how questions depend on different modalities or their combinations.\nLifeQA The LifeQA dataset (Castro et al. 2020) comprises 2.3K questions derived from 275 real-life YouTube videos. These videos were recorded by individuals in uncontrolled environments, capturing meaningful visual and linguistic interactions. The human performance on this dataset varied significantly: when given only video, participants achieved 48.5% accuracy; with audio alone, accuracy rose to 63.4%; and with all modalities combined, accuracy peaked at 90.6%. Interestingly, these results contradict the authors' Venn diagram (Castro et al. (2020), Fig. 3) categorization of LifeQA questions by answer type. Their categorization suggests that over 60% of questions are visual-based, while only 29% are speech-based, with the remaining questions (10%) requiring both modalities. This distribution seems at odds with the observed human performance across different modality combinations. We argue this discrepancy suggests that the authors' categorization of answer types may have been based on the perceived nature of the question rather than actual modality dependency. This method may be less accurate, as some questions labeled as \u201cVisual\u201d like \u201cWhere are they located?\u201d might also be answered based on dialogue or background sounds.\nAVQA The AVQA (Audio-Visual Question Answering) dataset (Yang et al. 2022b), derived from the VGG Sound dataset (Chen et al. 2020), contains over 57K question-answer pairs derived from 57K real-life videos focusing on object-generated sounds rather than human speech. It was designed to require information from both audio and visual modalities for most questions, to ensure that relying on just one modality would be insufficient or ambiguous for an accurate answer. However, the annotators who designed the questions also categorized the question types. Similar to LifeQA, this approach could introduce bias, as annotators might focus on the perceived modality requirements rather than objectively assessing whether relevant information is present in each modality."}, {"title": "2.3 Modality Contribution in Multimodal Tasks", "content": "The concept of quantifying modality contributions in multimodal tasks was explored in perceptual score paper (Gat, Schwartz, and Schwing 2021). They introduced a \"perceptual score\" to measure a model's reliance on specific input modalities or subsets. Their method involved removing the influence of a modality M from the set of all modalities and measuring the resulting change in accuracy.\nOthers, such as Yang et al. (2024), revealed that multimodal models often prefer certain modalities, leading to less robust performance when a modality is missing or perturbed. Their research showed that models tend to rely on one specific modality even when trained on multiple modalities, demonstrating vulnerability to unimodal attacks. To address this issue, they introduced Certifiable Robust Multi-modal Training, a method designed to mitigate the influence of the model's modality preference and regulate essential components to improve its robustness.\nWhile such works aim to analyze models' bias towards specific modalities and suggest solutions for reliable and robust performance, our work focuses on quantifying the modality contribution in the dataset, specifically in multiple-choice VidQA datasets. We identify modality bias in these datasets and provide a more fine-grained categorization of question types. This approach aims to guide the development of more balanced datasets, a crucial first step toward enabling multimodal models to utilize modalities effectively."}, {"title": "3 Method", "content": "Understanding the contribution of each modality is crucial in multi-modal question-answering tasks. Our goal is to distinguish between questions answerable by a single modality, those with redundant signals from multiple modalities, and those requiring integration of modalities.\nConsider the scenario in Figure 1, with two input modalities: video and subtitles (audio in the form of text). Three input combinations are possible: video alone, subtitle alone, and video + subtitle. The importance of a modality, such as video, can be quantified by estimating the increase in accuracy when video is present in the input combination (video, video+subtitle) relative to when it is not (subtitle).\nIn Figure 1b, the question Q1 is an example where accuracy does not increase when the video is added. From the phrase, \"stitch me up\" in the subtitle, one can reasonably infer that the lady is likely bleeding. The video confirms this fact by displaying a bleeding lady, but adds redundant signals rather than providing essential new details. In contrast, question Q2, exemplifies a multimodal question that cannot be answered correctly with a single modality. When considering only the video, two answer choices, (a) and (c), become confusing as both mention the correct visual detail \"lady in the jean jacket\". Similarly, with only subtitles, three plausible answer choices are given (b), (c), and (e). The question requires integrating information from both modalities for an accurate response. We formalize this intuition by defining the modality importance score."}, {"title": "3.1 Modality Importance Score", "content": "Definition. Given an input question $q_i$, its corresponding ground truth label $y_i$, and a set of source modalities $M = \\{M_1, M_2, ..., M_k\\}$, we denote combinations of modalities in $M$ as the power set of $M$ excluding the $\\O$, $P(M) \\backslash \\O$. We first define the performance measurement function as:\n$\\perf (q_i | M') = \\frac{\\sum_{S \\subseteq M'} 1[A'_s = y_i]}{|M'|}$, (1)\nwhere $M'$ is a subset of modalities defined as $M' \\subseteq P(M)\\backslash \\O$, and $|M|$ is the cardinality. $1[A'_s = y_i]$ is the response accuracy function we use to measure the performance in VidQA tasks defined as,\n$1[A'_s = y_i] = \\begin{cases}1 & \\text{if } A'_s = y_i\\\\0 & \\text{if } A'_s \\neq y_i\\end{cases}$ (2)\nThis is an indicator function that returns 1 if the answer for question $q_i$, $A'_s$, obtained using a subset of modalities S matches the ground truth, and 0 otherwise. While our current performance measurement function $\\perf (q_i | M')$ considers only response accuracy, it can be generalized to incorporate other performance metrics."}, {"title": "3.2 Categorizing Question Types with MIS", "content": "Using the MIS, we can identify unimodal-biased questions. If $\\text{MIS}_{m_j} < 0 < \\text{MIS}_{m_k}$, $\\text{MIS}_{m_j} \\neq \\text{MIS}_{m_k}; m_k \\in M$ where $m_k \\neq m_j$, we classify question $q_i$ as $m_j$-biased. Such questions can be answered using only $m_j$, but cannot be answered correctly using any other single modality $m_k$. For instance, with video and subtitle modalities, video-biased questions can manifest in two ways. First, correct answers might be obtained whenever the video modality is included, but using only subtitles leads to incorrect answers due to their irrelevance. Alternatively, the video alone might yield correct answers, but combining video and subtitles could result in incorrect answers. In this latter case, the MIS for subtitles becomes negative, indicating interference.\nIn addition to identifying unimodal-biased questions, we use MIS to provide a more fine-grained categorization of questions. This categorization helps our understanding of multimodal questions and the relationships between different modalities in answering them.\nAs shown in Table 1 rows 1 and 8, there are cases where the same MIS is obtained regardless of which the subset of modalities, correct or incorrect. We define these as modality-agnostic questions, where $\\forall m_j \\in M, \\text{MIS}_{m_j} = 0$. We further divide modality-agnostic questions into two subcategories:\n$\\forall S \\subseteq P(M), 1[A'_s = y_i] = 1$\n$\\forall S \\subseteq P(M), 1[A'_s = y_i] = 0$\nAs illustrated in row 5 of Table 1, there exist questions where no single modality can strongly determine the answer and signals from multiple modalities can be combined to determine the correct answer. We define these questions as complementary questions, where $\\forall m_j \\in M, \\text{MIS}_{m_j} > 0$. In this case, all modalities contribute to answering the question correctly when combined with other modalities.\nNote that in the case of only two modalities, complementary questions cannot be answered correctly unless both modalities are utilized. For scenarios with more than two modalities, complementary questions may involve varying contributions from each modality."}, {"title": "Unimodal-bias questions", "content": "Modality-agnostic vs Complementary questions"}, {"title": "Modality-agnostic Question", "content": "\u2022 Modality-agnostic correct questions:\n\u2022 Modality-agnostic incorrect questions:"}, {"title": "Complementary Questions"}, {"title": "4 Evaluation", "content": "For our experiments, we utilized GPT-4 Turbo (OpenAI et al. 2024), one of the top-performing MLLMs that supports both image and text inputs. We prompted the model to select the correct answer by providing the question, answer choices, and the corresponding modality combination under evaluation. Specific constraints and image extraction were applied to account for GPT-4 Turbo's token limitations and allow longer video clips. Detailed information about our prompts and process can be found in Appendix A.\nWe evaluated three VidQA datasets, each containing both video and subtitle/audio components. For TVQA (Lei et al. 2018) and LifeQA (Castro et al. 2020), we use transcripts/subtitles provided by the dataset. AVQA (Yang et al. 2022b) does not provide transcripts, but we use the audio labels from VGG Sound dataset (Chen et al. 2020) as the subtitle. Due to the large number of questions, we limited evaluation to the validation or test sets. For TVQA and AVQA, we uniformly sampled 1,019 and 796 questions, respectively, representing approximately 6-10% of the total questions. For LifeQA, we evaluated the entire test set of 372 questions.\nOur study evaluates four multimodal VidQA models, listed in Table 3, capable of processing both visual and textual (audio captions or subtitle) inputs to answer multiple-choice questions. We use the MLLM-derived MIS to identify unimodal-biased questions. Our feature permutation experiments show how effectively these models integrate and utilize information across different modalities."}, {"title": "4.1 Experimental Setup and Overview", "content": "To assess human perception of modality importance, we employed a split-group methodology involving four participants, each evaluating 197 TVQA questions. The detailed methodology is in Appendix A, along with Figure 5 depicting the study and Table 4 showing accuracy distributions across confidence levels. Our study aimed to validate the alignment between MLLM-derived MIS and human perception of modality importance. The evaluation yielded a substantial inter-annotator agreement (Fleiss' kappa: 0.76) for questions answered with both modalities, with an average accuracy of 87.8%."}, {"title": "4.2 Human Study Validation of MLLM-derived Modality Importance", "content": "In this section, we analyze the distribution of question types based on MLLM-derived MIS."}, {"title": "4.3 Evaluation of Modality Bias in VidQA Datasets", "content": "The results, reported in Table 2, support our assumption that many questions in TVQA would be modality-agnostic correct. About 35% of the questions were classified as modality-agnostic correct, while only 2% were identified as complementary, requiring information from both modalities. We had 7% of questions that were modality-agnostic incorrect. As shown in Figure 2, GPT has limited visual understanding compared to humans, as 8 out of 11 modality-agnostic incorrect questions were actually video-biased. While the subtitle does not provide relevant information for these questions, GPT fails to extract or comprehend details from the sequence of images. Consequently, the model consistently incorrect regardless of input modality. Overall, the results show a potential discrepancy between the dataset's intended multimodal nature and the actual distribution of question types.\nThe distribution of question types based on our MIS categorization shown in Table 2 revealed that modality-agnostic correct questions formed the largest category, accounting for approximately 36% of the dataset. Video-biased questions followed closely, comprising 33% of the dataset, and subtitle-biased questions accounted for 19.9%. Less than 10% of questions were modality-agnostic incorrect for \"Sound\" and \"View\" types. For \u201cView\u201d types, we found out that GPT-4's had limitations in identifying image details. For \"Sound\" types, errors were primarily due to insufficient information in the provided automated captions. The low percentage of complementary questions (2%) indicates that most questions in the LifeQA dataset can be answered using a single modality or are modality-agnostic.\nFigure 3a compares our MI-based categorization with the annotated answer types. - For \u201cSound\u201d answer types, 46.8% were classified as subtitle-biased, aligning with the annotated type. However, a significant 41% were categorized as modality-agnostic. This suggests that many questions annotated as language-dependent can actually be answered with all modalities. Similarly, for \u201cView\u201d answer type questions, while the majority were video-biased, a significant number were modality-agnostic correct. These observations indicate that our categorization generally aligns with human-annotated answer types. Moreover, the significant proportion of modality-agnostic correct questions in both \"Sound\" and \"View\" answer types suggests that many questions may not be single modality-dependent."}, {"title": "TVQA", "content": "The results presented in Table 3 demonstrate the effectiveness of MIS in capturing unimodal bias across different models. We observe that permuting features with low MIS leads to a significantly smaller decrease in accuracy than permuting features with high MIS. For instance, with the subtitle-biased question, \"Why did Marshall think they should have their marriage waiting period waived?\" First, we permute the less important video features by providing the correct subtitle with the wrong images from a different TV show. Then, we permuted the more important feature by providing the wrong subtitle with the correct images. If our MIS effectively categorizes the questions, we would expect the model to perform well in the former case but fail in the latter. This expectation aligned with our results, as the average decrease in accuracy between low-MIS and high-MIS feature permutations was 33.5%, considering both subtitle and video-biased questions.\nOur evaluation reveals several key insights. First, the significant decrease in accuracy between low and high-importance feature permutations confirms that our modality importance score effectively identifies unimodal-biased questions. Second, models generally show degraded performance on video-biased questions than subtitle-biased ones. This difference suggests a limitation in understanding visually relevant features across the evaluated models. This may be due to the prevalence of subtitle-biased and modality-agnostic questions in the original TVQA datasets. Although we were unable to determine the total number of unimodal-biased questions in the TVQA dataset, we can infer from human performance on the TVQA test set. In the original TVQA results, human accuracy with subtitles exceeded that with video by 11%, encompassing both subtitle-biased and modality-agnostic questions. Consequently, we hypothesize that models were trained to focus more on subtitles than video. This is also supported by our observation that permuting video in video-biased questions resulted in a lower accuracy decrease than permuting subtitles in subtitle-biased questions. Lastly, when we permuted features with low importance scores, all models showed decreased accuracy except FrozenBiLM with the subtitle modality. This observation indicates that most models struggle to optimally combine information from different modalities, even when one modality is deemed less important for a given question.\nThese findings highlight the challenges in multimodal learning and the need for improved strategies in integrating information across modalities."}, {"title": "4.4 Multimodal Model Evaluation"}, {"title": "5 Discussion and Limitations", "content": "In future work, we aim to experiment with creating complementary questions requiring genuine integration of multiple modalities. This would allow comparison of model performance across different question types, providing further insights into multimodal reasoning capabilities and limitations of current multimodal AI systems.\nOur study's main limitation is its reliance on a single MLLM. Other models, such as Bard (Google 2023) or Claude (Anthropic 2021), might yield different results. Our approach's performance depends heavily on the MLLM's capabilities, particularly its visual processing ability, which we observed to be weaker than its language processing ability. This limitation most likely affected our categorization of some video-biased questions. Future studies should explore the use of multiple MLLMs and provide a more comprehensive evaluation of modality importance and modality bias in multimodal datasets."}, {"title": "6 Conclusion", "content": "Our findings reveal a significant challenge in the field of multimodal AI: current Video Question Answering datasets may not be optimally enabling multimodal reasoning. Our novel method for assessing the relative importance of different modalities, the MLLM-derived MIS, shows that across three VidQA benchmarks, a substantial 89.8% to 94.8% of questions can be answered using a single modality or are modality-agnostic. Complementary questions, which require integration from multiple sources, account for only 0.6% to 2% of the evaluated samples. This question-type imbalance may limit the full potential of multimodal AI systems. Our analysis shows that our MLLM-derived MIS correlates with the human perception of modality importance and can guide the scalable curation of more balanced datasets. By increasing the proportion of questions that require genuine multimodal integration, we can more effectively assess and improve a model's abilities to reason across diverse inputs, ultimately advancing the field of multimodal \u0391\u0399."}, {"title": "A Appendix", "content": "To accommodate GPT-4 Turbo's token limitations, we implemented the following constraints: For subtitles, we did not impose any limitations as all were less than the maximum tokens. For video-only inputs, we limited the number of images to 10. For combined video and subtitle inputs, we reduced the image limit to 8.\nGiven that many clips in our dataset exceed one minute in duration, we adopted a systematic approach to image extraction: we sampled frames at 1 Hz, starting from the provided localized timestamp. For clips exceeding the image number limit, we parsed them into multiple segments and prompted the model to analyze each segment separately. If the correct answer was identified in any segment, we considered the overall response correct."}, {"title": "A.1 Experimental Setup", "content": "You are tasked with answering a question with five multiple-choice options for a clip. For each clip, you will be given a question and five answer choices, along with the subtitles from the video.\nSelect the most likely answer from the given choices based solely on the information provided in the [Input Modality]. Do not make assumptions or rely on external knowledge If the [Input Modality] do not contain enough information to confidently answer the question, choose the answer that is most plausible given the limited context.\nIn addition to selecting the most likely answer, specify the [Input Modality's Content Segment] where the relevant information for the correct answer can be found. Also, state the reason you chose the answer. The reason should be no longer than two sentences. If you made a random guess because you were not able to select any plausible answer, then put 'None' in the [Input Modality's Content Segment] but keep the random answer and state the reason as \"Could not find answer, I selected random answer.\".\nFor each video clip, format your output as follows:\nWe utilize the above prompt for evaluation, adapting it to various input combinations: subtitles only, video only, or both subtitles and video. The phrase \"Input Modality's Content Segment\" in the prompt refers to different elements depending on the given modality. For subtitles, it indicates timestamp ranges; for video, it denotes image numbers; and when both are present, it includes both timestamp ranges and image numbers. This approach allows us to assess GPT-4's ability to identify relevant information from subtitles and/or video when selecting the correct answer.\nFor each prompt, we append the question, answer choices, and corresponding input modalities. When subtitles are involved, we extract the relevant subtitle text that overlaps with the localized timestamp from TVQA. To optimize API request costs, we group five questions, their answer choices, and associated subtitles into a single prompt for subtitle-only evaluations. For video-based evaluations, whether video-only or video with subtitles, we adopt a different approach. In these cases, we include only one question and its answer choices per prompt, accompanied by the corresponding video frames. When evaluating both video and subtitles together, we follow the same structure as video-only prompts but additionally incorporate the relevant subtitle text.\nGiven this prompt, GPT-4-Turbo successfully outputted the correct JSON format. However, although we gave clear instructions in the prompt that the model should choose answer from the input choices, the model did not consistently follow these instructions. In cases where it couldn't find the answer, it frequently outputted \u201cNone\u201d or \u201cselected random answer\" for the \"answer\" field. We regarded these responses as \u201cincorrect\u201d."}, {"title": "A.2 Evaluation Prompts"}, {"title": "A.3 Human Study Validation of MLLM-derived Modality Importance Regarding Confidence Score", "content": "As shown in the Figure 5, our human study involved four participants divided into two groups, assessing a total of 197 questions from TVQA. These questions were sampled from the 1,019 questions evaluated (see Section 4.3), ensuring representation across all categories. Each group was presented with the same set of questions but different single-modality inputs initially, followed by combined modality input. To account for the confidence level of responses, we asked participants to rate their confidence for each answer (1-5).\nWhile our evaluation process yielded substantial inter-annotator agreement, 0.76, and the average accuracy of 87.8% with both modalities, we identified a significant variance between annotators' confidence. This is observed in accuracy scores in the low confidence group in Table 4.\nWe then calculated a weighted accuracy score by multiplying each individual's accuracy with their confidence score normalized by maximum confidence score of 5. Then we sum these products, and divide by the number of people who used that modality. This weighted accuracy was then rounded to either 0 or 1, considering a modality to contain a strong signal for answering the question if the average accuracy exceeded 0.5.\nUsing these rounded response accuracies, we computed the modality importance score across all modality combinations. Our findings indicate that human perception of unimodal bias in questions aligns similarly with MLLM-based assessments as shown in Figure 6. The correlation between the model's categorization and human judgment demonstrated a Cohen's kappa score of approximately 0.3.\nWhile this score indicates moderate agreement, several factors contribute to the observed variance:\nLimited Sample Size: Our study involved only four participants due to resource constraints. This small sample size may not fully capture the diversity of human perceptions and could contribute to the variance in results.\nDataset Imperfections: Misaligned subtitles and incorrect speaker information in the TVQA dataset may have led to discrepancies between human and MLLM interpretations.\nBackground Knowledge Disparities: Despite instructions to avoid using external knowledge, MLLMs demonstrated their use of background information to infer scenes and characters' behaviors. This reveals that MLLMs leveraged their extensive knowledge about TV show characters, potentially enabling them to answer questions that some human annotators found challenging due to limited familiarity with specific characters or plot elements.\nDespite these factors, the moderate agreement between human and MLLM-based assessments is encouraging, especially considering the study's limitations. It suggests that our computational approach captures significant aspects of human-like understanding of modality relevance in complex video question-answering tasks."}, {"title": "A.4 Example Questions from Evaluated Dataset", "content": "Our approach identified several interesting examples of modality-agnostic correct responses and complementary questions in both the LifeQA and AVQA datasets. These examples provide valuable insights into the nature of multimodal questions and the performance of MLLM like GPT-4 in video question answering tasks.\nIn LifeQA dataset, as shown in Figure 7, we observed various scenarios where GPT-4 provided modality-agnostic correct responses. These include cases where direct answers were present in both modalities, as well as where one modality offered a direct answer (Figure 7a) while the other allowed for indirect inference (Figure 7b and 7c). In AVQA dataset, as illustrated in Figure 8, exhibited a slightly different pattern in its modality-agnostic correct examples. We found that object sound labels provided as subtitles in these videos typically aligned well with image content, thus presenting strong signals from both modalities.The modality-agnostic questions in both datasets highlights the redundancy of information across modalities, suggesting that current datasets may not be optimally designed to challenge models' multimodal integration capabilities.\nWe also examine complementary examples from both datasets. While we anticipated that complementary questions would require combining weak signals from both modalities to be answerable, our findings revealed a more complex cases. In Figure 9 from LifeQA, we discovered that the video had incorrect start and end timestamps in the annotation. Although the manual captions provided by the dataset appear to be extracted from the correct video segment, the actual video frames showed significant misalignment and therefore did not contain the relevant information. Interestingly, despite these misaligned modalities, we observed that having both information sources actually aided GPT-4 in focusing on the broader context within the subtitle, allowing it to infer details about character actions that weren't explicitly stated. Conversely, the complementary example from AVQA in Figure 10 was simply the result of a random selection between two correct choices.\nThe scarcity of complementary questions in both datasets limited our ability to analyze how models integrate information from multiple modalities. However, the LifeQA example demonstrates that combining weak signals from misaligned modalities can lead to correct answers, suggesting the potential of complementary questions in fostering effective multimodal integration. This highlights the need for more complementary questions in multimodal datasets, which could push the boundaries of model capabilities in integrating diverse information sources and drive advancements in multimodal reasoning."}, {"title": "A.5 Configuration for Evaluated Multimodal Models", "content": "Below are the specific models and configurations used in our evaluation of four models:\nMerlot Reserve Model (Zellers et al. 2022) We used the base model fine-tuned on TVQA. Our configuration followed the original Merlot Reserve implementation: we extracted 8 frames and the corresponding subtitle from a 35-second window centered around the middle of the timestamp.\nFrozenBiLM (Yang et al. 2022a) We selected a model pretrained with a frozen DeBERTa-V2-XLarge language model and fine-tuned on the TVQA dataset. Following the original experimental set up, we use 10 frames for every clip and subtitles from localized timestamp.\nLlama-VQA (Ko et al. 2023) For our evaluation, we used the Llama 7B as the base model, with a checkpoint fine-tuned on TVQA. For each video clip, we processed 10 frames as input to the model and subtitles from localized timestamp. As other models, we followed the original implementation.\nMiniGpt4-Video (Ataallah et al. 2024) We evaluated the Llama2-7B based version of MiniGPT4-Video, which processes 45 frames per video and subtitles from localized timestamp. Note that this model was not fine-tuned on the TVQA dataset, unlike the other models in our evaluation. For assessment, we used the evaluation script provided by MiniGPT4-Video, which utilizes GPT-3.5 to compare the predicted answer with the ground-truth."}]}