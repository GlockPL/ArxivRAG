{"title": "EVALUATING LARGE LANGUAGE MODELS WITH GRID-BASED GAME COMPETITIONS: AN EXTENSIBLE LLM BENCHMARK AND LEADERBOARD", "authors": ["Oguzhan Topsakal", "Colby J. Edell", "Jackson B. Harper"], "abstract": "We introduce a novel and extensible benchmark for large language models (LLMs) through grid-based games such as Tic-Tac-Toe, Connect-Four, and Gomoku. The open-source game simulation code, available on GitHub, allows LLMs to compete and generates detailed data files in JSON, CSV, TXT, and PNG formats for leaderboard rankings and further analysis. We present the results of games among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and GPT-40 by OpenAI, and Llama3-70B by Meta. We also encourage submissions of results from other LLMs. In total, we simulated 2,310 matches (5 sessions for each pair among 7 LLMs and a random player) across three types of games, using three distinct prompt types: list, illustration, and image. The results revealed significant variations in LLM performance across different games and prompt types, with analysis covering win and disqualification rates, missed opportunity analysis, and invalid move analysis. The details of the leaderboard and result matrix data are available as open-access data on GitHub. This study enhances our understanding of LLMs' capabilities in playing games they were not specifically trained for, helping to assess their rule comprehension and strategic thinking. On the path to Artificial General Intelligence (AGI), this study lays the groundwork for future exploration into their utility in complex decision-making scenarios, illuminating their strategic thinking abilities and offering directions for further inquiry into the limits of LLMs within game-based frameworks.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have marked significant progress in the field of Artificial Intelligence (AI) [1]. These developments prompt questions about the potential for achieving Artificial General Intelligence (AGI) [2] and the timeline for such advancements. Predictions on the timeline for AGI vary [3], [4], with some experts suggesting its inevitability [5]. A critical challenge in the journey towards AGI is developing benchmarks to assess Al's evolving intelligence.\nIn this study, we introduce a novel and extensible benchmark for LLMs using grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku, utilizing three distinct types of prompts (list, illustration, image). This benchmark helps assess the capabilities of LLMs, including rule comprehension, strategic thinking, and the ability to process and understand complex text and image prompts. The benchmark provides open-source code for simulating these board games among LLMs and generating data files that store details of the simulated games. This study also includes the analysis of a total of 2,310 games played among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by"}, {"title": "2 Background and Related Research", "content": "The deep learning revolution has profoundly transformed natural language processing (NLP) since the 2010s, with the introduction of the Transformer architecture in 2017 [6] playing a pivotal role in this evolution. The Transformer architecture enabled parallel word processing, significantly improving the efficiency and handling of long-range text dependencies. This innovation led to the creation of models like BERT (Bidirectional Encoder Representations from Transformers) [7] and OpenAI's GPT (Generative Pre-trained Transformer) series [8]. BERT advanced context under-standing by analyzing word relationships within sentences, while the GPT series excelled in generative language capabilities [1].\nThe scale of LLMs expanded exponentially, resulting in models with billions of parameters and exceptional performance across various NLP tasks. Recent models include GPT-4 by OpenAI [9], Gemini by Google [10], Claude by Anthropic [11], Grok by xAI [12], and open-source options like LLaMA by Meta [13] and Mistral by Mistral [14]. These models have pushed the boundaries of what is possible with LLMs, showcasing significant advancements in the field. LLMs are employed in diverse tasks such as text summarization, language translation, content generation, and question-answering [15]."}, {"title": "2.1 Large Language Model Benchmarks", "content": "LLMs produce outputs such that their responses can vary even with identical input [16]. Traditional metrics like accuracy, precision, F1 score, and mean squared error (MSE) are not suitable for evaluating LLM performance. Instead, specialized datasets and benchmarks are needed to assess LLM capabilities comprehensively [17].\nBenchmarks such as GLUE [18], SuperGLUE [19], HELM [20], MMLU [21], BIG-bench [22], ARC [23], TruthfulQA [24], HellaSwag [25], and LiveBench [26] provide diverse tasks that test various aspects of LLMs. GLUE, introduced in 2018, includes tasks like sentiment analysis and question-answering to evaluate natural language understanding. SuperGLUE, launched in 2019, extends GLUE with more demanding tasks such as multi-sentence reasoning and complex reading comprehension. The Massive Multitask Language Understanding (MMLU) benchmark tests LLMs across a wide array of subjects, including mathematics, history, computer science, and law, requiring extensive world knowledge and problem-solving abilities [21]. BIG-bench offers 204 varied tasks in areas such as linguistics, mathematics, reasoning, biology, and software development, allowing researchers to evaluate LLMs comprehensively while managing operational costs [22]. HELM emphasizes transparency and performance in specific tasks, using a multi-metric approach that includes fairness, bias, and toxicity assessments. It continually adapts to add new scenarios, metrics, and models [20]. The AI2 Reasoning Challenge (ARC) from the Allen Institute for Artificial Intelligence assesses AI systems' complex reasoning capabilities through multiple-choice questions. The ARC includes an Easy Set for basic retrieval methods and a Challenge Set for advanced reasoning, pushing AI towards deeper knowledge-based understanding [23]. The TruthfulQA benchmark assesses the accuracy and truthfulness of LLM responses, specifically designed to measure how well models can generate accurate answers and avoid hallucinations [24]. The HellaSwag benchmark tests common sense reasoning by presenting models with sentences and multiple possible endings, requiring them to choose the most logical continuation [25]. LiveBench addresses the test set contamination issue in LLM evaluation by offering a benchmark immune to such contamination and biases from human or LLM judging [26]. It features frequently updated questions from recent sources, automatic scoring against objective ground-truth values, and diverse tasks spanning math, coding, reasoning, language, instruction following, and data analysis [26].\nThe recent survey papers have provided comprehensive frameworks and methodologies that are invaluable for developing robust benchmarks. One such paper, \"A Survey on Evaluation of Large Language Models,\" presents a detailed review focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate [27]. It encompasses a broad spectrum of tasks, including general natural language processing, reasoning, medical applications, ethics, education, and more. This survey emphasizes the critical role of evaluation methods and benchmarks in assessing LLM performance, summarizing both successes and failures across different tasks, and highlighting future challenges in the field [27]. Similarly, \"Evaluating Large Language Models: A Comprehensive Survey\" categorizes LLM evaluation into knowledge and capability, alignment, and safety [28]. This survey underscores the importance of rigorous assessment and the development of comprehensive evaluation platforms to ensure the safe and beneficial development of LLMs. It aims to guide responsible LLM advancement, ensuring that their evolution maximizes societal benefits while minimizing"}, {"title": "2.2 Utilizing Games for Evaluating LLMS", "content": "Existing benchmarks primarily focus on language understanding tasks such as sentiment analysis, question-answering, and comprehension. Although some tasks in BIG-bench involve game-like problem-solving skills, they do not assess LLMs' performance in conventional games like chess or Go, which are valuable for evaluating strategic thinking and decision-making abilities. Using games as a benchmarking tool provides a unique perspective on LLM capabilities, highlighting their proficiency in understanding rules, formulating strategies, and making decisions. Strategic games like chess and Go emphasize predicting opponents' moves, while games involving linguistic interaction test language mastery and contextual understanding. The dynamic nature of games allows researchers to observe LLMs' adaptability and learning in real-time.\nEngaging in gameplay offers a standardized framework for comparing various LLMs' performances under the same conditions, evaluating their strategic and creative problem-solving abilities, and capacity for innovative solutions. The controlled environment of games is instrumental for safely testing LLMs, allowing researchers to observe behaviors and mitigate potential risks or ethical concerns. Games involving human-AI interaction reveal how LLMs collaborate with or compete against humans, shedding light on human-AI relationship dynamics. Therefore, testing LLMs within the gaming domain extends beyond evaluating their ability to play games; it offers a comprehensive examination of strategic thinking, language processing, creativity, and adaptability, which is crucial for advancing AI research and ensuring the responsible development and deployment of these technologies.\nText-based games present a distinctive and challenging domain for benchmarking LLMs. These interactive fiction games require models to understand natural language, interpret evolving game states, and generate appropriate commands within narrative-driven environments, demanding a profound grasp of language, context, and strategic application [9]. Studies on models like FLAN-T5, Turing, and OPT in the text-based game \"Detective\" reveal that these LLMs fall short of state-of-the-art or human performance levels, facing challenges in adapting to game dynamics, learning from past interactions, and goal-oriented processing [30].\nThe \"GameEval-Evaluating LLMs on Conversational Games\" paper introduces a framework for assessing LLMs through goal-driven conversational games, highlighting their abilities in complex discussions, decision-making, and problem-solving [31]. The SmartPlay benchmark assesses LLMs across diverse games, emphasizing their evolution as intelligent agents [32]. The MindAgent infrastructure evaluates multi-agent collaboration, enhancing human-AI coordination [33].\nStudies on LLM behavior in social interaction games like the iterated Prisoner's Dilemma and the Battle of the Sexes show challenges in adapting to strategies requiring mutual understanding and flexibility [34]. Research by Lor\u00e8 and Heydari on \"Strategic Behavior of Large Language Models\" underscores the role of context in strategic decision-making [35]. Tsai et al. highlight limitations in LLMs like ChatGPT and GPT-4 in constructing world models and leveraging knowledge in text-based games, suggesting the potential for targeted benchmarks [36].\nThe study \"Can Large Language Models Serve as Rational Players in Game Theory?\" evaluates LLMs' potential in game theory, identifying gaps in mimicking human rationality [37]. Another study explores models like Claude 2, GPT-3.5, and GPT-4 in processing game strategy and spatial information through Tic-Tac-Toe, finding that prompt design significantly impacts performance [38].\nGTBENCH evaluates LLMs' strategic reasoning in competitive game-theoretic tasks [39]. It features 10 tasks covering complete vs. incomplete information, dynamic vs. static, and probabilistic vs. deterministic scenarios. Results show LLMs struggle in complete, deterministic games but perform better in probabilistic ones. Commercial LLMs like GPT-4 outperform open-source models such as CodeLlama-34b-Instruct. Code-pretraining aids strategic reasoning, but advanced methods like Chain-of-Thought (CoT) and Tree-of-Thought do not consistently help. Detailed error profiles are provided to understand LLM behaviors [39].\nGAMEBENCH evaluates strategic reasoning in LLMs across nine game environments, each highlighting key reasoning skills [40]. Using GPT-3 and GPT-4, along with Chain-of-Thought prompting and Reasoning Via Planning (RAP), the study finds that while these frameworks improve performance, no model matches human capabilities, with GPT-4 sometimes performing worse than random actions. The games are selected to avoid overlap with the models' pretraining corpuses.\nA recent survey paper explores the state of the art in applying LLMs to gaming, identifying the various roles LLMs can play within games. It highlights underexplored areas and promising directions for future research, reconciling the"}, {"title": "3 Methodology", "content": "We have developed a benchmark to evaluate the capabilities of LLMs in rule comprehension and decision-making through grid-based games. This benchmark includes open-source web-based software for simulating games, accessible on GitHub [43]. The web application is built using JavaScript, HTML, and CSS, with server-side AWS Lambda functions written in Python to leverage LLMs hosted on AWS Bedrock. The game simulation web app enables LLMs to compete against each other, recording the details of each move for further analysis in JSON, CSV, TXT, and PNG formats, as well as summarizing game results.\nCurrently, the benchmark includes Tic-Tac-Toe, Connect-Four, and Gomoku, and is designed to be extensible to accommodate additional board games. A step-by-step guide for adding new game simulations is provided. As illustrated in Figure 1, the user interface of the game simulation web app allows users to select a game and the LLMs for the first and second players from a curated list. Users can also choose the type of predefined prompts (e.g., list, illustration, image) and specify the number of consecutive games for the selected game, prompt, and player combination.\nThe game simulation initiates by sending the selected prompt to the web API of the chosen LLM for the first player, then awaits its move. Upon receiving a response, the application updates the user interface to reflect the game's progress, as demonstrated in Figure 1, subsequently queries the chosen LLM for the second player, and awaits its move. The prompts, which include the current state of the game, are continuously sent to each LLM's web service until a player wins, the game ends in a draw, or a player is disqualified for making invalid moves. Each query and response are recorded for every move. This methodology ensures seamless interaction between the application and the LLMs via web API calls. The interactions are illustrated in Figure 2."}, {"title": "3.1 Games Available on the Benchmark and Possibility of Expansion to New Games", "content": "We have utilized three games in the benchmark; Tic-Tac-Toe, Connect Four and Gomoku. All of these games are classical two-player games played on a grid; 3 by 3, 6 by 7, and 15 by 15, respectively [44] [45] [46]. These games can be adapted to larger grids. The explanations of these games are given in Table 1 and the same explanations are used in the prompts.\nTic-Tac-Toe, Connect Four, and Gomoku are all solved games meaning their outcome (win, lose, or draw) can be correctly predicted from any position, assuming that both players play perfectly. In Tic-Tac-Toe, optimal play from both participants guarantees a draw. The first player can always win with optimal play in Connect Four. In the Gomoku game, the first player is guaranteed to win with optimal play [47].\nWe designed this benchmark to be extensible, allowing for the addition of new games such as checkers and chess. The code is modular, facilitating the easy integration of additional games. Additionally, we prepared a step-by-step guide on how to add a new game to the benchmark, which can be found on the game simulation page under the 'How to Add Your Own Game' link. We encourage interested individuals to contribute to the development of the benchmark"}, {"title": "3.2 LLMs Tested & New Result Submission to the Leaderboard", "content": "Numerous LLMs are available for evaluation. To ensure a meaningful and comprehensive assessment, we carefully selected LLMs based on several criteria. Firstly, we chose LLMs that are not specifically trained for the games used in the benchmark. Although the training data of proprietary LLMs is not publicly disclosed, we assume they are not trained explicitly for any of the benchmark games. We prioritized well-known, high-performing LLMs developed by industry leaders such as OpenAI and Google, given their significant contributions to AI advancements. Additionally, we included LLMs from emerging startup companies that have gained attention in the AI community, such as Anthropic. To further enrich our evaluation, we aimed to test open-source models and included Meta's Llama3-70B model in our evaluation. This selection covers a broad spectrum of innovative approaches, technological capabilities, and accessibility options.\nThe landscape of LLMs changes rapidly, with new models frequently emerging with improved capabilities. Therefore, we provide game simulation software in the benchmark that generates submission files and encourage new submissions to the leaderboard. Contributors can evaluate other LLMs by integrating their LLM web service URL or API keys, generating new results, and submitting them to the leaderboard. We believe the leaderboard will allow people to see the progress of LLMs in different games as the leaderboard continues to be updated.\nCurrently, the benchmark includes results and detailed files for the following LLMs: Claude 3.5 Sonnet and Claude 3 Sonnet from Anthropic, Gemini 1.5 Flash and Gemini 1.5 Pro from Google, GPT-4 Turbo and GPT-40 from OpenAI, and Llama3-70B from Meta. To access these models, we utilized the web APIs provided by Google and OpenAI for the"}, {"title": "3.3 Details of the Prompts", "content": "We utilized three types of prompts: list, illustration, and image. Each prompt is divided into eight main components: 1) an explanation of the game, 2) an explanation of the format for the game status, 3) the current game status, 4) a definition of the LLM's role followed by a request for its next move, 5) an explanation of the response format, 6) an explanation of invalid moves, 7) a warning if the previous move was invalid, including an explanation of why it was deemed invalid, and 8) the current number of invalid moves made by the player, as well as the number of invalid moves until the player is dis-qualified. The current game status, the invalid move warning, and the invalid move counts are dynamically generated and updated as the game progresses. Table 2 presents the components of a 'list' type prompt for the Tic-Tac-Toe game. This standardized format ensures consistency in prompts throughout the game while allowing for dynamic updates of the game state."}, {"title": "3.5 Metrics and Methods for Evaluation", "content": "We evaluated the performance of LLMs across three games (Tic-Tac-Toe, Connect Four, and Gomoku) using different prompt types (list, illustration, and image) to assess their ability to handle various formats of game state representation. Performance comparisons were made against a random play strategy to establish a baseline, highlighting the strategic advantages of the LLMs. The primary metrics for evaluation included win rates, draw rates, and disqualification rates, providing an overview of the LLMs' performance as both the first and second players. Additionally, we tracked the number of invalid moves per game and the average number of moves per game to assess rule adherence and game engagement. To delve deeper into the LLMs' strategic thinking, we analyzed missed opportunities to win or block an opponent's win, counting instances where the LLMs failed to make critical moves. We presented the missed opportunities per game by averaging the missed opportunities across all games that resulted in a win, draw, or disqualification. We also normalized the number of missed opportunities by the number of valid moves to calculate the percentage of missed opportunities per valid move. The results were visualized through charts and tables to provide a clear depiction of performance metrics and trends, as shown in the Results section. Additionally, we present the outcomes of each match between seven LLMs and a random play generator across different games (a total of 2,310 matches) in a results matrix table. We also maintain a leaderboard on the GitHub page that allows for filtering and sorting results by different metrics. We encourage community contributions to suggest and implement new evaluation metrics and methodologies, fostering a collaborative approach to advancing the understanding of LLM capabilities."}, {"title": "4 Results", "content": "In this section, we present the outcomes of games played among LLMs. These results are based on data files generated by the open-source game simulation web software and shared on the GitHub page.\nFigure 3 displays the outcomes of Tic-Tac-Toe games using the list prompt type, where seven LLMs competed against others and a random play opponent, engaging in five matches per opponent for a total of 280 games. The chart summarizes the performance of the seven LLMs as well as random play in terms of win rates, draw rates, and dis-qualification rates as both the first and second players. Claude 3.5 Sonnet has the highest winning percentage as the first player (88.57%) but a lower winning percentage as the second player (17.14%). GPT-40 and Gemini 1.5 Pro show strong performance as both the first and second players, while random play results in the highest disqualification rates\nFigure 4 displays the performance metrics of seven LLMs and a random play strategy in terms of win rates, draw rates, and disqualification rates when playing Tic-Tac-Toe as the first and second player, based on the illustration prompt format. The chart shows significant performance variations among the LLMs, with Claude 3.5 Sonnet exhibiting the highest winning rates as the first player, indicating a strong strategic advantage. Llama3-70B and GPT-4 Turbo also demonstrate strong performance. Disqualification rates are generally low but notable for some models, such as Gemini 1.5 Flash, indicating occasional invalid moves. The random player serves as a baseline comparison, with lower winning rates and higher disqualification rates, highlighting the superior strategic capabilities of the LLMs.\nFigure 5 displays the performance metrics of various LLMs and a random play strategy in terms of win rates, draw rates, and disqualification rates when playing Tic-Tac-Toe as the first and second player, based on the image prompt format. Key observations indicate that Claude 3.5 Sonnet has a high disqualification rate as both the first (46.67%) and second player (53.33%), indicating a struggle with rule compliance. Similarly, GPT-4 Turbo and Gemini 1.5 Pro also show significant disqualification rates. GPT-4 Turbo and GPT-40 exhibit the highest winning rates. The random play baseline has high disqualification rates, highlighting the strategic advantages of LLMs compared to random strategies. No draws occurred in the Tic-Tac-Toe games using the image prompt. Llama3-70B does not accept images, so it was not used when testing any of the games with the image prompt type.\nThe chart in Figure 6 displays the performance metrics of various LLMs and a random play strategy in terms of win rates, draw rates, and disqualification rates when playing Connect Four as the first and second player using the list prompt type. Claude 3.5 Sonnet and Gemini 1.5 Pro show outstanding performance with a high winning rate of 88.57% as the first player. Most LLMs demonstrated strong performance when considering their total win rates as both first and second players. The random player, serving as a baseline, has lower winning rates and some disqualifications, highlighting the strategic advantages of the LLMs. No draws occurred in the Connect Four games using the list prompt.\nFigure 7 presents the performance metrics of various LLMs and a random play strategy in terms of win rates and disqualification rates when playing Connect Four as the first and second player using the illustration prompt type. GPT-4 Turbo has the highest disqualification rates as both the first and second players. The random play, serving as"}, {"title": "6 Limitations and Future Directions", "content": "The study's methodology primarily focuses on grid-based games, which, while useful, may not fully capture the breadth of real-world strategic interactions. Future benchmarks should incorporate a wider variety of game types, including those with more complex rules and longer-term strategic planning, to provide a more comprehensive assessment of LLM capabilities. Designing new, custom, purpose-built games to test specific aspects of LLM capabilities, such as adapting to unusual rules, would enhance benchmarking effectiveness and prevent the possibility of LLMs becoming familiar with the game, even if they were not specifically trained for it.\nThe simplicity of the games used in this benchmark facilitates basic evaluation but may not challenge LLMs' strategic capabilities as much as more complex games like chess or Go might. The fact that current LLMs have not mastered even these simple games provides valuable insights into their capabilities and limitations. Expanding the evaluation to larger grids, such as 4 x 4 or 5 x 5 for Tic-Tac-Toe or 19 \u00d7 19 for Gomoku, could present additional challenges and provide a clearer indicator of LLM performance.\nRelying on predefined prompts to guide LLMs' moves may not adequately capture their potential for independent strategic thinking or their ability to respond to changing game states. Although we updated prompts dynamically to warn LLMs of invalid moves, further techniques, such as providing all previous invalid moves, could be explored to reduce invalid move numbers and disqualifications.\nThis study tested LLMs using structured prompts. Future research should investigate how these prompts influence LLM performance and how variations in prompt structure might affect their understanding of game states and subsequent moves. Such insights could help optimize LLMs for more complex and varied applications.\nThe evaluation metrics used in this study revealed a wide range of LLM capabilities. While these metrics provide a good indication of performance, they may not fully capture the strategic complexity of the models. Further analysis of the moves\u2014drawn from the JSON and PNG files\u2014could offer a more detailed assessment of game progress over time. The new analysis can be conducted using the Game Simulation web app to generate data for new LLMs or the open-access data on GitHub. For example, evaluating the creation of winning opportunities, such as aligning moves for Tic-Tac-Toe, Connect Four, and Gomoku, can provide insights into proactive strategic thinking by the LLMs. The authors encourage and welcome contributions to the repository in the form of suggestions and implementations of new metrics and methods to evaluate the capabilities of LLMs.\nFocusing on a select group of LLMs might not capture the full diversity of strategic approaches across available models, highlighting the importance of including a broader array in future research. The rapidly expanding landscape of LLMs, with new models and improved versions emerging frequently, necessitates continuous updates to benchmarks. We welcome submissions of other and new LLMs using the open-source game simulation software.\nFuture work could explore several promising directions to extend research and deepen our understanding of LLM capabilities in strategic games and beyond. Multi-agent collaboration scenarios, where multiple LLMs work together against a common opponent or compete in teams, could assess their abilities in coordination, cooperation, and competitive strategy. Comparing newer versions of LLMs against those tested in this study could track progress and improvements in AI strategic gaming capabilities over time.\nThis study suggests several avenues for future research and development. Firstly, improving LLMs' abilities to interpret and act on visual data is crucial, as evidenced by high invalid move rates in illustration and image prompts. Enhancing visual processing capabilities could significantly boost overall performance and utility. Secondly, further research is needed to enhance LLMs' decision-making processes in more complex environments."}, {"title": "7 Conclusion", "content": "This study introduces a novel and extensible benchmark for LLMs through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku. The open-source game simulation code, available on GitHub, enables LLMs to compete and generates data files in JSON, CSV, TXT, and PNG formats for leaderboard rankings and further analysis. We present the results of games among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and GPT-40 by OpenAI, and Llama3-70B by Meta, and encourage submissions from other LLMs. By analyzing the performance of these models over 2,310 games, we observed significant variations in their capabilities, particularly highlighting their struggles with complex and"}]}