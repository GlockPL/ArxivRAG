{"title": "A Comprehensive Survey on Generative AI for Video-to-Music Generation", "authors": ["Shulei Ji", "Songruoyao Wu", "Zihao Wang", "Shuyu Li", "Kejun Zhang"], "abstract": "The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: visual feature extraction, music generation frameworks, and conditioning mechanisms. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained classification of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.", "sections": [{"title": "1 INTRODUCTION", "content": "Video-to-music generation is a rapidly emerging field at the intersection of computer vision and audio synthesis, aiming to generate music that aligns semantically and temporally with video content. Studies [1, 2, 3] have shown that a piece of melodious music can vastly attract the audience's attention and interest in watching the video [4]. Traditional video soundtracks rely on professional editors to manually align video and audio or custom compositions by music producers, requiring significant resources and lacking flexibility. For video creators like vloggers, finding suitable, royalty-free background music can also be a cumbersome and time-consuming task. Additionally, while the state-of-the-art music generation techniques such as Suno\u00b9 [5] offer high-quality outputs, their text-to-music mode struggles to achieve precise temporal alignment with video content, limiting their applicability.\nRecent advancements in deep learning, especially multimodal generative models, such as multimodal diffusion models and multimodal large language models (MLLMs), have propelled progress in the realm of cross-modal generation, including video-to-music generation. With the growth of video streaming platforms like YouTube\u00b2 and TikTok\u00b3 and the rise of automated video generation technologies (such as Sora and Veo), the video-to-music task has gained significant attention due to its potential applications in entertainment, content creation, and personalized media experiences. For instance, automatic video-to-music generation can streamline the production process for filmmakers, enhance user-generated content, and enable adaptive soundtracks for interactive platforms such as virtual reality (VR) and gaming, which reduces production costs substantially. Since there are plenty of preexisting and higher quality music, some video-music retrieval (VMR) methods [6, 7, 8] are also capable of automatically scoring music. However, it may not align well with the dynamic content of the video. Moreover, the selection is constrained by the size of the database and copyright restrictions, limiting the number of available tracks and leading to music homogenization [9].\nEnsuring both the musicality of the generated content and the consistency between the music and the video makes video-to-audio generation a challenging task. The key to video-to-music generation lies in effectively bridging the gap between two inherently different modalities-vision and music. Videos often contain complex temporal dynamics and semantic cues, such as human movements, scene transitions, and emotional undertones, which are supposed to be translated into corresponding musical elements like rhythm, melody, and harmony. This requires sophisticated cross-modal understanding, incorporating temporal synchronization, contextual relevance, and musical creativity.\nAccording to the prior work, the pipeline for video-to-music generation typically involves three components: visual feature extraction, music generation framework, and conditioning mechanisms that effectively bridge two modalities, as illustrated in Figure 1. Visual feature extraction includes the extraction of features such as rhythm, semantics, and emotion, while the generation framework encompasses two paradigms: autoregressive language modeling and probabilistic diffusion models. Note that in some studies [10, 11, 12], only visual feature extraction and music generation are explicitly included, even some work [13, 14] directly employs end-to-end models to map from video to music. The conditioning mechanisms of these studies are implicitly embedded within the models. Additionally, some methods [15, 16, 17] introduce the text modality to facilitate the alignment between vision and music modalities.\nBefore the video-to-music task gained popularity, there were already several endeavors attempting to infer plausible audio from visual signals, including tasks such as image-to-audio [18], video-to-sound effect [19, 20, 21], and lip-to-speech [22, 23, 24]. Owens et al. [19] synthesize plausible impact sounds from silent videos. Chen et al. [18] proposed a conditional generative adversarial network to achieve cross-modal image-audio generation of musical performances. Zhou et al. [20] introduced a video encoder plus a Sample RNN-based sound generator [25] to generate sounds (such as baby crying, human"}, {"title": "2 INVOLVED MODALITY", "content": "This section provides a fine-grained classification of the two modalities involved in video-to-music, as shown in Figure 2. In particular, videos are usually represented by 3D signals indicating RGB values in both spatial (i.e., height \u00d7 width) and temporal dimensions, while music is in 1D waveform digits or discrete symbolic events across the temporal dimension [31]. The distinct classifications are related to the datasets utilized and the specific objectives targeted in various studies, influencing the design of different components in video-to-music generation."}, {"title": "2.1 Video", "content": "Videos can be categorized into human-centric videos and general videos based on whether they contain human movements, such as dances and sports."}, {"title": "2.1.1Human-centric.", "content": "Human-centric videos feature rhythmic movements synchronized with the music beat, such as dance, floor exercises, and figure skating. When generating music for such videos, the primary focus is typically on whether the generated music's rhythm aligns with the movements in the video. Consequently, most studies extract rhythmic features from these videos, as detailed in Section 3.1.2. It is worth noting that silent performance videos also fall under human-centric videos, as the instrumental playing motions also change in accordance with the musical beats."}, {"title": "2.1.2General.", "content": "In contrast, the general videos discussed in this paper refer to those that do not explicitly include human movements synchronized with the music rhythm, such as typical music videos (MVs), commercials, and daily life vlogs. Compared to human-centric videos, extracting rhythm from general videos is more challenging. Typically, scene transitions are used to guide the generation of music rhythm, while video semantics help shape the melody and emotion. Since this type of video constitutes a large portion of real-world content, recent studies have concentrated on generating background music for general videos."}, {"title": "2.2 Music", "content": "According to the representations, music can be categorized into symbolic and audio formats [32], usually corresponding to MIDI files and WAV files, respectively. The former represents music as a sequence of discretized events (e.g., beats and notes), while the latter processes music audio as waveforms or spectrograms. There has been extensive prior research in both the fields of symbolic music [27] and audio music generation [33, 34, 35, 36, 37]."}, {"title": "2.2.1Symbolic.", "content": "Symbolic music encompasses musical events with explicit semantics, such as pitch, duration, and tempo, enabling clear correspondences with video elements like timing and motion speed, as demonstrated in approaches of CMT [38]. However, symbolic music has several limitations: 1) it is difficult to collect, with datasets typically being small; 2) it lacks expressiveness and emotional depth; 3) it usually involves a limited range of instruments, restricting diversity and hinders the model's ability to synthesize complex and varied music. There are many ways to represent symbolic music, among which the most common and widely used method is MIDI event representation [27]."}, {"title": "2.2.2Audio.", "content": "In contrast, musical audio is more complex and computationally expensive, lacking explicit semantic information but containing expressive and nuanced details. Additionally, it is easier to collect, as a large amount of paired video-music audio data is available online. As a result, many studies take the initial step of gathering video-music datasets from the internet to serve as training data. Due to the high dimensionality and complexity of waveforms, audio is typically first transformed into spectrograms or latent space representations before being processed by the models."}, {"title": "3 VIDEO-TO-MUSIC GENERATION", "content": "Previous studies have employed specialized models or multimodal generative models, including MLLMs (Multimodal Large Language Models) and multimodal diffusion models, to achieve video-to-music generation. Specialized models are designed specifically for video-to-music generation. In contrast, multimodal generative models handle a broad range of multimodal tasks, making them more general-purpose but less fine-tuned for this task. As a result, specialized models typically excel in producing high-quality, domain-specific music, while multimodal generative models may sacrifice performance in video-to-music generation due to their broader applicability. Whether using specialized models or multimodal generative models, methods for video-to-music generation typically encompass three key components (as illustrated in Figure 2): visual feature extraction, music generation, and conditioning mechanisms that enable the music generator to effectively reference visual conditions, resulting in music that aligns with various aspects of the video, such as rhythm, semantics, emotion, and genre. This section reviews existing studies in video-to-music generation, focusing on these three critical components, as shown in Table 1. Each component can be implemented using different methods, which we have categorized into subgroups according to their goals or techniques.\nFrom Table 1, we conclude that video-to-music generation task has recently become a hot topic, attracting increasing attention from a growing number of researchers. Additionally, with the iteration of methods, fusion features that comprehensively consider multiple aspects of the video are increasingly adopted, and cross attention has emerged as a common and convenient conditioning mechanism across various music generation methods. In terms of music generation, both autoregressive and non-autoregressive modeling remain mainstream methods."}, {"title": "3.1 Visual Feature Extraction", "content": "Video encoders across different studies are designed to capture various aspects of video content. This section categorizes the extracted visual features based on their characteristics, dividing them into four main categories: spatiotemporal features, motion & rhythm features, semantic features, and fusion features, as shown in Figure 3."}, {"title": "3.1.1Spatiotemporal Features.", "content": "To effectively capture temporal and spatial information in videos, some studies have employed convolutional networks to extract spatiotemporal features from consecutive frames. Koepke et al. [13] and Su et al. [14] both generate music for a video of a musician playing the piano, serving as pioneering studies in the field of video-to-music generation. They use ResNet18 [39] as the backbone to process five consecutive video frames for capturing temporal and spatial information. The key differences lie in that Koepke et al. utilize a 3D convolution-based aggregation module to integrate temporal information from consecutive frames, alongside a slope module that explicitly models the keyboard layout to preserve spatial localization. In contrast, Su et al. introduce a feature transform module to transform multi-scale features at residual blocks for detecting the visual cues on various scales better, coupled with a multi-scale feature attention network to capture spatial dependencies and semantic relevance. Additionally, Lin et al. [40] modified the original Hiera model [41] to encode video efficiently."}, {"title": "3.1.2Motion & Rhythm Features.", "content": "Human-centric videos contain human movements that are synchronized with the music rhythm. Therefore, to automatically generate soundtracks for such videos, motion and rhythm features are frequently extracted to serve as conditions for guiding music generation. Regarding human body motions, several different forms of data representations such as 3D Skinned Multi-Person Linear model (SMPL) [42] or 2D body keypoints [43] are often utilized.\nGan et al. [10] synthesize plausible music for a silent video clip about people playing musical instruments. They first use the open-source OpenPose toolbox [43] and API [44] to obtain key points of the human body and hand fingers from video frames, which represent human skeleton sequence as an undirected spatial-temporal graph, where the node corresponds to a key point of the human body and edges reflect the natural connectivity of body keypoints. Afterward, the graph convolutional network (GCN) encoder that possesses a spatial-temporal graph convolution strategy is utilized to produce visual latent vectors over time. Su et al. [45] used a bidirectional Gated Recurrent Unit (GRU) as the body movement encoder to encode the 2D human pose keypoints into a latent representation. Aggarwal et al. [46] also use OpenPose to estimate the person's pose for each extracted dance video frame. The pose sequence is then used to compute the dance similarity matrix that is sent into the CNN layers and global average pooling to get the dance feature representation. For detecting musical beats, Pedersoli et al. [47] extract the skeletal body keypoints of a dancer from each video frame and use Temporal Convolutional Neural Network (TCN) architectures, which can guide the generation of rhythmic music.\nIn addition to extracting motion features, especially for dance music generation, some studies extract style characteristics as well, which contribute to the overall rhythmic and stylistic aspects of the dance or movement. Su et al. [48] encoded video into rhythm correlated with human body movements. Specifically, keypoints are extracted from the video and processed using a spatiotemporal GCN [49] combined with a transformer encoder [50] to capture motion features for estimating music beats. Given the periodic nature of music beats and the diverse visual dynamics of human movements, an additional style stream is proposed to capture fast movements. The combination of the two streams constitutes the movements rhythm. Similarly, Han et al. [51] utilized two branches to extract dance movement features and dance style features, which are concatenated to form conditional control information. Specifically, the dance video is first input into the Mediapipe framework [52] to extract the coordinates of the human body's joints. Then, these coordinates are input into a GCN encoder and a GCN-GRU network to encode the spatiotemporal features of dance movement and dance style embedding, respectively. Afterward, Liu et al. [53] proposed InteractiveBeat to generate an evolving interactive soundtrack in real-time for a camera input that captures the person's movements. InteractiveBeat contains VisBeatNet and MuStyleNet to generate drums rhythm. Specifically, the GRU-based VisBeatNet first predicts kinematic offsets, visual beats, and estimates tempo from a live stream of human motion. Then, the adversarial style transfer module, MuStyleNet, translates the kinematic offsets into drum 'style' (i.e., a vector representing the rhythm of the drum audio) using a Generative"}, {"title": "3.1.3Semantic Features.", "content": "Recently, pre-trained models have been leveraged to extract semantic features from videos, enabling a deeper understanding of the content beyond mere motion and rhythm. These models, often trained on large-scale datasets, can capture high-level concepts and contextual information. By utilizing such pre-trained models, researchers can bypass the need for extensive domain-specific training and instead focus on aligning the extracted features with the music to be generated. Li et al. [59] claim that visual dynamic changes are linked to music rhythm and visual semantics influence the melody and atmosphere of the music. They found during the unconditional music generation process, models tend to generate the melody first, followed by the rhythm. However, most existing models struggle to intuitively reflect the music generation process with corresponding control signals and lack good interpretability. Therefore, they proposed a feature selector to select different features as conditions at different timestep intervals. The visual feature of each frame is extracted using the pre-trained Video CLIP [60] as video encoder, while language features are extracted for the video captions using the pre-trained bert-base-uncased model [61] as language encoder. Zhang et al. [16] extracted semantics information, i.e., video description, from input video using Video-LLaMA [62], which is then passed through an LLM to generate simpler, descriptive tags. Then CLAP [63] processes these tags and any additional user-provided tags for customization to produce prompt features. By leveraging descriptive tags, this method avoids the use of paired video-music datasets. Additionally, Zuo et al. [64] used a pretrained VIT-L [66] with spatial self-attention to extract deep visual features.\nMultimodal large language models (MLLMs) are also capable of performing cross-modal generation tasks, including video-to-audio. These studies often utilize pre-trained models for feature extraction. The Multimodal Music Understanding and Generation (M\u00b2UGen) framework proposed by Liu et al. [65] integrates LLM's abilities to comprehend and generate"}, {"title": "3.1.4Fusion Features.", "content": "Focusing solely on the rhythmic alignment between video and music while neglecting the correspondence of semantics (as well as emotions) may lead to conflicting styles, thereby diminishing the overall sense of immersion and experience. Conversely, prioritizing semantic matching while ignoring rhythmic synchronization may result in a lack of coherence between the music and the video's actions or scene transitions, causing unnatural temporal misalignments that disrupt the flow and dynamic expression of the content. To achieve consistency in video-to-music generation, it is essential to balance rhythm alignment with semantic matching. Consequently, recent studies extract not only motion features for rhythm synchronization but also semantic-relevant features for enhanced alignment.\nExtending from dance to multiple sports scenarios such as floor exercise and figure skating, Yu et al. [72] introduced a series of context-aware conditioning, e.g., human motions, video frame, and genre. Specifically, they extract visual rhythms based on the cadent movement of human motions, then introduce the Hawkes Process [73, 74] on the visual rhythms to take temporal context into consideration. They also model the temporal relationship by adding a Bi-LSTM over the I3D features [56]. Besides, the one-hot genre labels are embedded into genre embeddings. To encode the SMPL-formatted 3D motion data for integrating music into games and animation, Tan et al. [75] define motion features at each frame as the set of four features that capture the dynamic and diverse nature of dance movements. These features include the position and orientation of each joint, described respectively in global coordinates and relative to parent joint axes, with the orientation further converted into a continuous 6D representation [76]. Additionally, linear and angular velocities are incorporated to reflect the temporal dynamics of the motion, providing a detailed depiction of how movements evolve over time. Although there is no explicit rhythmic alignment, this rich combination of static and dynamic attributes enables an effective encoding of motion, suitable for generating beat-aligned and rhythmically coherent dance music. Same as [72], they conditioned the model with music genre as a high-level control. The one-hot encoded genre labels are concatenated with the motion features as the conditioning signal.\nHowever, the genre labels adopted as style constraints are weak features that may result in stylistic bias. The inclusion of style constraints may influence rhythmic patterns, potentially leading to rhythmic bias [77]. To address these issues, Liang et al. [77] proposed DanceComposer to achieve rhythmic alignment and stylistic matching simultaneously. DanceComposer contains the Music Beats Prediction Network (MBPN) for modeling the rhythmic correspondence and the Shared Style Module (SSM) for constructing a unified cross-modal style feature space, preventing the generation of similar melodies for different dances. Specifically, the human skeleton sequence extracted using OpenPose is fed into a spatial-temporal graph convolutional network (ST-GCN) to extract spatial-temporal features, which are then fed into a Transformer encoder to predict music beats. The SSM adopts a two-branch architecture composed of a dance style embedding network and a music style embedding network. The former takes as input the human keypoint sequence while"}, {"title": "3.2 Music Generation", "content": "A significant amount of previous work has focused on unimodal music generation models, whether for symbolic or audio format [32]. Music generation methods can be broadly categorized into two types based on their generative paradigms: autoregressive and non-autoregressive modeling, as depicted in Figure 4. Autoregressive methods are often applied in music generation to predict MIDI music events or discrete audio tokens, while non-autoregressive methods excel at handling image-like audio spectrograms or latent features derived from encoding. As one of the most prevalent generative models nowadays, diffusion models [92] are commonly used in non-autoregressive approaches."}, {"title": "3.2.1Autoregressive Modeling.", "content": "Transformers are widely used in the symbolic music generation for given videos. Given the extracted motion features, Gan et al. [10] used a decoder-only transformer [93] to autoregressively predict MIDI events proposed by Oore et al. [94] and then utilized a standard synthesizer (i.e., FluidSynth) to generate the music waveforms. Su et al. [48] divided video to rhythmic music into three steps: Video2Rhythm, Rhythm2Drum, and Drum2Music, dubbed RhythmicNet. Given the generated rhythm sequence, a transformer is utilized to autoregressively generate drum onsets, which are then fed into a Unet to generate velocity and offset. After generating the drum track, the REMI representation [95] and a transformer-XL-based [96] encoder-decoder architecture are adopted to generate additional piano or guitar instruments. Similarly, Han et al. [51] first utilized a transformer model to generate drum rhythm given the features of dance movements and dance style, and then introduced the BERT-like [61] model to enrich the remaining music track based on the drum track. The InteractiveBeat proposed by Liu et al. [53] also contain three modules to generate interactive rhythmic audio generation based on human movements, i.e., VisBeaNet and MuStyleNer for obtaining drum 'style', and DrumGenNet for synthesizing polyphonic drum track based on the estimated tempo and drum 'style'. The transformer-based DrumGenNet encodes style at the encoding side and generates the 2D drum hits matrix at the decoding side conditioned on the encoded style. In contrast to the Rhythm2Drum, DrumGenNet discards the UNet that generates velocity and offset matrices and uses the continuous 1D rhythm input as velocity. These changes make the drum generation network compact with the inference overhead compatible for real-time.\nBased on compound words (CP)-like music representation [97], Di et al. [38] and Tian et al. [90] propose the controllable music transformer (CMT) models for generating music for arbitrary video. V-MusProd [12] decouples music generation into three progressive transformer stages: chord, melody, and accompaniment. It first predicts a chord sequence, then generates the melody conditioned on chords, and finally generates accompaniment conditioned on chords and melody. These generation processes are all autoregressive and conditioned on specific video features. Kang et al. [58] propose an Affective Multimodal Transformer (AMT) to generate music given a video. Specifically, the video embedding is fed into the transformer encoder, and the chord embedding concatenated with the key is fed into the transformer decoder to generate chord sequences autoregressively. Finally, a post-processing step uses a regression model based on bidirectional Gated Recurrent Units (biGRU) to estimate the note density and loudness based on the video embeddings. These predicted values are subsequently used to dynamically adjust the rendering of the generated chords for a more expressive musical output that better matches the video, such as selecting chord arpeggiation patterns and performing MIDI velocity adjustments.\nTo ensure rhythmic alignment and stylistic matching, Liang et al. [77] introduced the Progressive Conditional Music Generator (PCMG), which gradually employs the dance kinematic rhythm and dance style features to guide music generation. The PCMG comprises a two-stage generation pipeline to avoid mutual interference of rhythm constraints and style constraints. Each stage operates as a sequence-to-sequence task modeled autoregressively using an encoder-decoder Transformer. In the first stage, the Drum Transformer generates a drum track conditioned on predicted music beats. In the second stage, the Multi-track Transformer creates a multi-track MIDI conditioned on previously generated drum track and dance style features. A fusion embedding is designed to combine the drum track with the shared style feature.\nHowever, symbolic music generation is often limited to a few classical instruments, and this kind of representation is not expressive enough to cover the diverse range of sounds we hear in typical soundtracks, which yields highly formulated results and hinders the model from synthesizing complex and diverse music. Moreover, the fidelity of the generated music is largely contingent upon the quality of a sound synthesizer or a MIDI playback engine, which may not adequately reflect the full depth and complexity of the actual instruments. Lastly, the small scale and limited genre diversity of MIDI annotations typically lead to poor generalization [40]."}, {"title": "Audio", "content": "When employing the autoregressive paradigm for audio generation, directly processing long-term music from the raw waveform is computationally intensive and challenging [72]. Consequently, existing research typically first encodes the audio into discrete tokens using an audio compression algorithm, such as SoundStream [98], EnCodec [99], and Improved RVQGAN [100]. Subsequently, an autoregressive sequence modeling approach is used to predict the discrete tokens, which are then reconstructed back into audio through the codec's audio decoder. In such approaches, the audio quality of the generated music is limited by the capabilities of the codec and substantial computational resources are required to train the music token prediction model.\nTo bridge the gap between the coarser video representation and the high-resolution audio representation, Su et al. [78] proposed V2Meow, a high-fidelity music audio waveform generator conditioned on diverse video inputs. They adapt the AudioLM [33] pipeline to train V2Meow, which encompasses three main stages of autoregressive modeling. Firstly, a transformer is used to predict music semantic tokens autoregressively given visual features and optional MuLan [81] embedding. Then, two decoder-only transformers are used to map music semantic tokens to coarse acoustic tokens and to map coarse tokens to fine acoustic tokens. Finally, all levels of tokens are passed to the SoundStream decoder to reconstruct the audio. Tian et al. [4] uses the transformer-based Music Token Decoder to predict the discrete audio tokens given visual features, following the next token prediction paradigm in LLM techniques. The discrete audio tokens are obtained by encoding audio using the EnCodec encoder and are restored to audio through the EnCodec decoder. Lin et al. [40] and Zuo et al. [64] transform the audio into discrete tokens with EnCodec and use a transformer architecture to predict music tokens autoregressively.\nAlthough pre-trained text-to-music models [34, 101] demonstrate the ability to generate high-quality and diverse music, most current models only provide control over global attributes of the music and cannot manipulate local properties such as rhythm. To address this issue, Li et al. [15] employ a two-stage training model named VidMusician, which is built upon the pretrained text-to-music model MusicGen [34]. In the first stage, semantic features are introduced to help the model understand the overall video content, during which only the semantic conditioning module is trained. In the second stage, rhythm features are added for comprehensive control, training both the semantic and rhythm conditioning modules. This stage uses zero initialization and identity initialization to seamlessly model rhythm without compromising the progress achieved in the first stage. This parameter-efficient method avoids training the whole generative model. Li et al. [17] learn two pseudo-words that represent rhythm information inherent in arbitrary dance videos and genres, and then integrate them into the vocabulary of the text encoder. The encoded text embedding can be fed into different text-to-music backbones for video-to-music generation, such as transformer-based MusicGen [34] as well as diffusion-based Riffusion [101] and AudioLDM [102]. Moreover, recent MLLMs [65, 70] also adopt the pretrained MusicGen as the generator for cross-modal music generation. However, they use the MUVideo dataset curated by gathering music samples from the AudioSet [103] with their corresponding videos. Since the samples in AudioSet are all 10 seconds, they can only generate music for 10-second videos."}, {"title": "3.2.2Non-autoregressive Modeling.", "content": "Symbolic\nGiven five consecutive grayscale video frames, Koepke et al. [13] and Su et al. [14] predict all note onsets occurring around the middle video frame. Koepke et al. [13] pass all outputs of a given video through a Gaussian filter ($\\sigma$ = 5) to add temporal smoothing and threshold the smoothed signal resulting in a binary signal for every note for saving as MIDI data. Su et al. [14] train a generative adversarial network (GAN) [104] to refine the predicted results to closer to Pseudo GT MIDI obtained using the Onset and Frames framework [105], and then a deep-learning-based MIDI synthesizer converts MIDI to the spectrogram. Li et al. [59] fused video features with music features in the latent space of the diffusion model. To achieve fine-grained temporal alignment between the music rhythm and the video, they applied time encoding to both and introduced specially designed masks to conduct sequential attention, facilitating precise alignment between music and\nAudio\nTo generate multi-instrumental music from videos in an unsupervised setting, Su et al. [45] introduce a multi-band residual 1D convolutional Vector Quantized VQ-VAE for learning a discrete latent representation of various instruments music from log-spectrogram. The multi-band residual (MBR) learning method is used on the audio encoder and the decoder to better capture the spectral features of musical overtones. Then, an autoregressive prior is trained given the body movement features. A MIDI can further condition the latent space such that the pipeline will generate the exact content of the music being played by the instrument in the video. Post training, the prior network first autoregressively generates discrete latent representations via ancestral sampling conditioned on the musicians' movement features. Then, the discrete latent representations will be fed to the decoder of VQ-VAE and generate the instrument's music associated with the exact instrument and content in the video.\nTo tackle the challenge of high dimensionality of audio data, Zhu et al. [54] proposed D2M-GAN, a novel adversarial multi-modal framework that generates complex musical samples conditioned on dance videos. The convolution-based VQ generator takes the fused motion-visual data as input and outputs the desired VQ audio representations. The multi-scale discriminator evaluates the generated and real VQ representations. The generated VQ representation are used to perform a lookup in the pre-learned codebook, and the retrieved codebook entries are decoded to raw musical samples via a pre-trained and fine-tuned decoder from the JukeBox [106]. Experimental results show that D2M-GAN outperforms the previous methods including Foley Music [10], Dance2Music [46], and CMT [38]. However, the high variability of waveform data (e.g., variable and high-dynamic phase, energy, and timbre of instruments) makes it difficult to directly model high-quality waveforms. As a result, the generated music often contains strange noises [51]. And the slow decoding speed of the VQ audio representation results in short-length music (2~6s) [11, 72].\nThe emergence of diffusion probabilistic models (DPMs) [92, 107] has revolutionized the generation of images, video, and audio. Inspired by recent advances in the vision field, some work adapts diffusion models as music generators. Unlike conventional approaches that operate directly on high-dimensional raw data, Latent Diffusion Models (LDMs) work in a compressed latent space, which is a lower-dimensional representation of the original data. The process begins with an encoder, such as a Variational Autoencoder (VAE) or Vector Quantized VAE (VQ-VAE), which maps the raw data into this latent space while retaining its core structure and semantic features. Diffusion processes are then applied within the latent space, making the generative modeling more computationally efficient due to the reduced dimensionality. Once the generation process is complete, a decoder reconstructs the data from the latent space back into its original form.\nAs the follow-up work of D2M-GAN [54], Zhu et al. [55] proposed a contrastive diffusion framework that combines latent diffusion training and contrastive learning [108] to achieve diverse multimodal conditional synthesis tasks including dance-to-music generation. Different from continuous latent space, they train a VQ-based model to obtain the discrete representation of music. Then, they introduce a Conditional Discrete Contrastive Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to effectively incorporate the CDCD loss into the denoising process. One is the step-wise parallel diffusion with intra-negative samples that invokes multiple parallel diffusion processes during contrastive learning, and the other is the sample-wise auxiliary diffusion with inter-negative samples, which maintains one principal diffusion process. The former is more beneficial for capturing the intra-sample correlations, e.g., musical rhythms, while"}, {"title": "3.3 Conditioning Mechanism", "content": "The visual feature extraction and music generation components handle the individual video and music modalities separately. To facilitate video-to-music generation, a component is required to bridge the two modalities, which is referred to as the conditioning mechanism in this paper. This component establishes a correspondence between video and music, enabling the generation of music that is both semantically aligned and rhythmically synchronized with the corresponding video. As shown in Figure 5, this survey divides the existing conditioning mechanism into three categories, i.e., model input, cross attention, and contrastive learning."}, {"title": "3.3.1Model Input.", "content": "This section introduces the most straightforward conditioning mechanism, which involves using video features as the input or partial input to the music generator to guide music generation.\nSome studies [13, 14] have directly mapped video to music by feeding video information into an end-to-end network that outputs music (midi or piano roll). To autoregressively predict the discrete latent code of musical audio generated by the encoder of VQ-VAE, Su et al. [45] concatenated the last hidden state of the body keypoints encoder to every time steps of the discrete latent representation and fed the concatenated features into the Transformer encoder. Aggarwal et al. [46] concatenate the encoded dance feature with note sequences up to time t as input to predict the note for time t + 1. CMT [38] makes use of three newly defined rhythmic relationships between video and music. The extracted rhythmic features from MIDI (in training) or the video (in inference) are embedded in the music representation as conditions to generate music that aligns with the rhythm of the video. XMusic [90] also places the visual conditon tokens at the specified position within the symbolic music representation to control the generation process of music. Zhu et al. [54] input the fused motion-visual features into the VQ generator of GAN to predict the audio VQ representations. Su et al. [78] adopted the AudioLM [33] pipeline, thereby learning the mapping from visual inputs to the music semantic tokens in the first stage. Tian et al. [4] apply LLM techniques to the audio field, taking as input the visual features to predict the discrete audio tokens autoregressively. Lin et al. [40] feed the previously computed video features into every multi-head attention as contextual features for autoregressive music generation. Li et al. [82] transformed the visual condition and music latent representation into the same dimension and summed element-wise as the input of the DiT generator [116]. Then the classifier-free guidance (CFG) [117] is utilized for unconditional or conditional generation. This element-wise summation facilitates point-to-point precise alignment. Similarly, You et al. [111] concatenate the noisy input with the embedded conditional inputs and the embedded diffusion timesteps along the temporal dimension. The fused input is then padded to match a specified maximum length and combined with positional embeddings prior to being processed by the DiT model."}, {"title": "3.3.2Cross Attention.", "content": "Cross-attention is first introduced as part of the Transformer architecture, enabling the decoder to focus on the encoder's outputs when generating predictions. It is now widely used in conditional generation tasks to integrate contextual information from one modality into another. This mechanism ensures that the generated content is contextually aligned with the provided input, making it an essential component in many modern conditional generative frameworks. Especially in video-to-music generation tasks, cross-attention is commonly used as a conditioning mechanism in two scenarios.\nFirst, when utilizing Transformer-based architectures as generator, the conditions are fed"}]}