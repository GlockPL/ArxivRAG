{"title": "Chip Placement with Diffusion", "authors": ["Vint Lee", "Chun Deng", "Leena Elzeiny", "Pieter Abbeel", "John Wawrzynek"], "abstract": "Macro placement is a vital step in digital circuit design that defines the physical location of large collections of components, known as macros, on a 2-dimensional chip. The physical layout obtained during placement determines key performance metrics of the chip, such as power consumption, area, and performance. Existing learning-based methods typically fall short because of their reliance on reinforce- ment learning, which is slow and limits the flexibility of the agent by casting placement as a sequential process. Instead, we use a powerful diffusion model to place all components simultaneously. To enable such models to train at scale, we propose a novel architecture for the denoising model, as well as an algorithm to generate large synthetic datasets for pre-training. We empirically show that our model can tackle the placement task, and achieve competitive performance on placement benchmarks compared to state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "Placement is an important step of digital hardware design where components such as logic gates (standard cells), or large collections of components (macros) have to be placed on a 2-dimensional physical chip based on a connectivity graph (netlist) of the components. Because the physical layout of objects determines the length of wires (and where they can be routed), this step has a significant impact on key metrics, such as power, area, and performance, of the produced chip. In particular, the placement of macros, which is the focus of our work, is especially important because of their large size and high connectivity relative to standard cells.\nTraditionally, macro placement is done with commercial tools such as Innovus from Cadence, which requires input from human experts. The process is also time-consuming and expensive. The use of ML techniques on this task shows promise in automating this process, as well as creating better-optimized placements than commercial tools, which rely heavily on heuristics. Existing works mostly use reinforcement learning (RL) [1, 2], an approach with several key limitations. First, RL is challenging to scale it is sample inefficient, and has difficulty generalizing to new problems. Despite efforts to incorporate offline training, RL-based methods still require a significant amount of additional training for each new netlist [1, 2]. Second, by casting placement as a Markov Decision Process (MDP), these works require agents to learn a sequential placement of objects (standard cells or macros), which creates challenges when suboptimal choices near the start of the trajectory cannot be reversed.\nTo circumvent these issues, we instead adopt a different approach, leveraging powerful generative models, in particular diffusion models, to produce near-optimal chip placements for a given netlist. Diffusion models address the weaknesses with RL approaches because they can be trained offline at scale, then used zero-shot on new netlists, and because such models simultaneously place all objects, as shown in Figure 1. Moreover, our approach takes advantage of the great strides in training and sampling techniques (such as guided sampling [3]) to achieve better results.\nTraining a large and generalizable diffusion model, however, comes with its own challenges. First, the vast majority of circuit designs and netlists of interest are proprietary, severely limiting the"}, {"title": "Related Work", "content": "Several techniques have been applied to macro placement. We consider two categories, reinforcement learning and generative approaches.\nAn RL approach from Google named CircuitTraining [7, 8] employs a Graph Neural Network to provide a netlist embedding to several RL agents. Although Google's technique successfully optimized on their heuristic, their success failed to translate to optimal power, performance, and area [9]. In addition, [9] showed the volatility of their techniques. On complex benchmarks, shuffling macro order could increase the final wirelength by up to 30% and total runtime by 20%. Several RL approaches follow to iterate on runtime [10\u201312], macro ordering [13], and proxy cost predictions [14\u201317]. While ChiPFormer [2] improves on generalization abilities by combining offline and online RL, their method still trains on in-distribution examples, and requires hours of online training on each new netlist for best results.\nIn contrast, Flora [18] and GraphPlanner [19] pull away from a sequential placement formulation, instead using a VAE [20] model to generate placements. Flora also proposes a synthetic data generation scheme, but does not vary object sizes. Moreover, their method connects objects only to their nearest neighbors, which our experiments indicate is detrimental when applying models to realistic circuits (see Section 5.2.2). The models in these works also struggle with learning the underlying distribution of legal placements, producing mostly overlapping outputs.\nAll related works except MaskPlace rely on the optimizer DREAMPlace[21] to legalize macro placements and obtain their final results."}, {"title": "Background", "content": "Our goal is to learn a diffusion model to sample from f (xc), where the placement x is a set of 2D coordinates for each object and the netlist c describes how the objects are connected in a graph, as well as the size of each object. We normalize the coordinates to the chip boundaries, so that they are within [-1, 1].\nWe represent the netlist as a graph (V, E) with node and edge attributes {pi}iev and {qij}(i,j)\u2208E\u2022\nWe define pi to be a 2D vector describing the normalized height and width of the object, while qij is a 4D vector containing the positions of the source and destination pins, relative to the center of their parent object. We convert the netlist hypergraph into this representation by connecting the driving pin of each netlist to the others with undirected edges. This compact representation contains all the geometric information needed for placement, and allows us to leverage the rich body of existing Graph Neural Network (GNN) methods."}, {"title": "Evaluation Metrics", "content": "To evaluate generated placements, we use legality, which measures how easily the placement can be used for downstream tasks (eg. routing); and half-perimeter wire length (HPWL), which serves as a proxy for chip performance.\nWhile a legal placement has to satisfy other criteria, in this work we focus on a simpler, commonly used constraint [1, 2]: the objects cannot overlap one another, and must be within the bounds of the canvas. We can therefore define legality score as Au/As, where Au is the area of the union of all placed objects that lie entirely within bounds, and As is the sum of areas of all individual objects. Note that legality of 1 indicates that all constraints are satisfied.\nRouted wirelength influences critical metrics because long wires create delay between components, influencing the timing and power consumption. HPWL works as an approximation to evaluate placements prior to routing [22, 23]. Because the scale of HPWL varies greatly between circuits, for our experiments we report the HPWL ratio, defined for a given netlist as lgen/ldata, where lgen is the HPWL for the model-generated placement, while ldata is the HPWL of the placement in the dataset.\nOur objective is therefore to generate legal placements with minimal HPWL."}, {"title": "Diffusion Models", "content": "Diffusion models [24, 25] are a class of generative models whose outputs are produced by iteratively denoising samples using a process known as Langevin Dynamics. In this work we use the Denoising Diffusion Probabilistic Model (DDPM) formulation [25], where starting with Gaussian noise x, we perform T denoising steps to obtain xT-1, xT-2,...,xo, with the fully denoised output xo as our generated sample. In DDPMs, each denoising step is performed according to\nxt\u22121 = xt.  xt + \u03b2t \u00b7 \u0109o(xt, t, c) + \u03c3\u03c4\u00b7 z\nWhere at, \u03b2t, \u03c3\u03c4 are constants defined by the noise schedule, z ~ N(0, I) is injected noise, and \u0109e is the learned denoising model taking xt, t and context c as inputs. By training \u0109e to predict the noise added to samples from the dataset, DDPMs are able to model arbitrarily complex data distributions."}, {"title": "Methods", "content": "We developed a novel architecture for the denoising model, shown in Figure 2. We highlight below several key elements of our design that we found to be important for the placement task:\nWe use the message-passing GNN layers for their computational efficiency in capturing node neighborhood information, while the interleaved attention [26] layers address the oversmoothing problem in GNNs by allowing information transfer between\nnodes that are distant in the netlist graph, but close on the 2D canvas. We find that combining the two types of layers is critical, and significantly outperforms using either type alone.\nWe found (see Section 5.5) that inserting residual 2-layer MLP blocks between each GNN and Attention block improved performance significantly for a negligible increase in computation time.\nThe model receives 2D sinusoidal position encodings, in addition to the original (x, y) coordinates, as input. We find (see Section 5.5) that this improves the precision with which the model places small objects.\nIn this work, we use 3 sizes of models: Small, Med, and Big, with 233k, 1.23M, and 4.60M parameters respectively. Model hyperparameters can be found in Appendix A."}, {"title": "Guided Sampling", "content": "One of the key advantages of using diffusion models is the ability to optimize for downstream objectives through guided sampling. We use universal guidance [3] with easily computed potential functions to improve the HPWL and legality of generated samples without training additional models. The guidance potential f(x) is defined as the weighted sum Wlegality \u00b7 Olegality + Whpwl. Shpwl of potentials for each of our optimization objectives.\nThe legality potential legality(x) for a netlist with objects V is given by:\nlegality (x) = \u2211 min(0, dij(x))2\ni,jEV\nwhere dij is the signed distance between objects i and j, which we can compute easily for rectangular objects. Note that the summand is 0 for any pair of non-overlapping objects, and increases as overlap increases.\nWe define hpwl(x) simply as the HPWL of the placement x. We compute this in a parallelized, differentiable manner by casting HPWL computation in terms of the message-passing framework used in GNNs [27] and implementing a custom GNN layer with no learnable parameters in PyG [28].\nInstead of gradients from a classifier [29], we follow Bansal et al. [3] in using g(xt) = \u221ax+\\(x0), where xo is the prediction of xo based on the denoising model's output at time step t. The combined diffusion score is then given by fo(xt) + g(xt)."}, {"title": "Datasets", "content": "We obtain datasets (Table 1) consisting of tuples (x, c) using different two methods, outlined below."}, {"title": "Generated dataset", "content": "We utilize ArtNetGen [30] to produce an artificial netlist and map the nodes to an open-source ASAP7 PDK [31], then we adapt it to include realistic SRAM macros. The Cadence EDA tool then performs concurrent placement of the macros and standard cells. This produces realistic, legal, and\nnear-optimal placements, but is slow. To combat this, the netlist generation and placement occur in parallel and the clock optimization, routing, and power cell placement steps are removed from the Cadence flow. Although these steps can affect final component placements, their optimizations are to be performed in evaluation after we initialize the placement. Currently, the flow still takes 15 minutes to generate a single sample of around 100 macros and 15k instances. Thus, to further improve the number of realistic samples, we augment the final placements by performing legal transformations."}, {"title": "Synthetic dataset", "content": "To generate larger datasets, we randomly generate objects (sizes sampled from uniform distribution), and place them at random, ensuring legality by retrying if objects overlap. Following Rent's Rule [32], we then sample a number of pins for each object using a power law. Next, we generate edges between pins by sampling independently from Bernoulli(p) for each pair of pins on different objects. To approximate the structure of real circuits, we have p decay exponentially with L1 distance between the pins connected by the edge. This method, depicted in Figure 3 allows us to generate ~100k \"circuits\" each containing ~200 objects in a day using 32 CPUs.\nWe vary p, as well as several other parameters, to generate synthetic datasets SynthM and SynthM- Short with different levels of wirelength optimality. Notice that the wires in SynthM are, on average, much longer than those in SynthM-Short, corresponding to placements that are less HPWL-optimal. A detailed list of data generation parameters can be found in Appendix A."}, {"title": "Implementation", "content": "We evaluate the performance of our model on circuits presented in two sets of public benchmarks, ISPD2005 [33] and ICCAD04 [34]. Our models are implemented using Pytorch [35] and Pytorch- Geometric [28], and train on machines with 8 Intel Xeon Gold 6330 CPU cores, using a single Nvidia RTX A5000 GPU. We pre-train our models using the Adam optimizer [36] for 3M steps, with 100k steps of fine-tuning where applicable."}, {"title": "Results", "content": ""}, {"title": "Model Performance", "content": "Table 2 shows evaluation results on SynthM and SynthL datasets. In all cases, the model achieves legality better than 0.9, with HPWL ratio (of diffusion-sampled to original placements) very close to 1. This validates our model design, and shows that our model is capable of learning approximately- correct, near-optimal placements in both realistic and synthetic circuits. Examples of generated samples are shown in Table 3\nMore importantly, our experiments with different model sizes on SynthM show significant increases in performance as model size increases over a modest range, with HPWL ratio decreasing and legality increasing with bigger models. This is an encouraging result, suggesting that not only does our model obtain lower loss with increasing size, but that evaluation metrics, such as legality, improve too.\nIt is also worth noting that performance deteriorates with larger circuits, as shown in Table 2. This is likely due to the precision required to place many small objects without overlaps, and suggests that larger models are needed to tackle larger circuits (benchmark circuits have thousands of objects). The data and compute required to train such models on large examples presents a significant challenge, which we address in Section 5.2."}, {"title": "Unsupervised Pre-training", "content": "Because of the difficulty in generating realistic netlists and placements with many objects, we propose the following approach, inspired by LLM training: we first train on a large set of small-circuit synthetic data (which is easily generated), then fine-tune on a much smaller set of large-circuit examples, which can be either synthetic or real. In this section we investigate the usefulness of this approach, examining the performance of models pre-trained on SynthM when deployed on SynthL in the zero-shot and fine-tuned settings."}, {"title": "Zero-shot and Fine-tuned Performance", "content": "We see from our results in Table 4 that models of all sizes exhibit decent zero-shot performance on SynthL, despite only training on netlists with far fewer objects. With only a small amount of fine-tuning, performance improves dramatically, with the Big model achieving > 0.95 legality and HPWL close to the training distribution. Moreover, we observe that the favorable scaling properties of our models apply to the zero-shot and fine-tuned settings as well, with performance increasing substantially with increased model size."}, {"title": "Impact of Pre-training Dataset", "content": "We also investigated the relative effectiveness of pre-training on different datasets.\nRunning zero-shot evaluations on SynthL, we find in Table 5 that pre-training on the more-optimized SynthM-Short does lead to significantly better zero-shot HPWL than SynthM. However, the improved HPWL comes at the expense of legality, where SynthM-Short performs almost 10% worse. Note that HPWL tends to improve as legality degrades, since overlapping objects minimize wirelengths better than non-overlapping ones. This indicates that pre-training on synthetic datasets with shorter wirelengths (and therefore more optimal placements) do not produce significantly better models than datasets with longer wirelengths. One possible reason for this is that because of the increased decay of p over distance in SynthM-Short, objects have to be much closer to each other than in SynthM to have a reasonable chance of being connected. While this can allow the model to better optimize HPWL, it also restricts the flow of information within the GNN layers and could result in poorer representations being learned. This effect could be especially detrimental when attempting zero-shot transfer to other datasets or real circuits that contain edges between distant objects due to functional requirements.\nOur finding stands in contrast to prior work on generating synthetic data [18], which proposes generating data that is optimal with respect to swaps between objects."}, {"title": "Guided Sampling", "content": "As shown in Table 6, guidance significantly improves legality and HPWL during zero-shot sampling, with up to a 14% improvement in legality for models pre-trained on SynthM-Short. Moreover, we find that adding HPWL guidance improves wirelength significantly without deteriorating legality. This result shows that our guidance method is effective in optimizing generated samples without requiring additional training."}, {"title": "Placing Real-World Circuits", "content": "Our experiments with synthetic datasets show that diffusion models have great potential for tackling the placement problem. We focus in particular on the mixed-size placement problem, which requires us to place several hundred large macros, as well as many (potentially hundreds of thousands) small standard cells. This is an incredibly challenging problem, and would require prohibitively large computational resources to solve with a diffusion model alone. Therefore, we follow Mirhoseini et al. [1] in first clustering the standard cells to reduce their number to a manageable level, and then use the diffusion model to generate a near-optimal placement for the macros and the clusters. For the following, we use a Med model pre-trained on SynthM with no fine-tuning, and with both legality and HPWL guidance."}, {"title": "Ablations", "content": "Ablations over several elements of our model architecture are shown in Table 9. When either the sinusoidal encodings or MLP blocks are removed, the model performs substantially worse in both legality and HPWL. Removing attention layers also causes sample quality to plummet, as evidenced by poor legality scores. This demonstrates the importance of these components to model performance."}, {"title": "Limitations", "content": "While we found that synthetic datasets with shorter wirelengths can cause models to transfer more poorly to out-of-distribution circuits, we believe more work can be done to investigate the effects of other parameters such as the distribution of object sizes.\nAnother limitation lies in the reliance on guidance to optimize generated samples. While guidance is effective and flexible, it functions by biasing the model to sample from a subset of the training distribution. This fundamentally limits the HPWL that can be achieved. Exploring other optimization methods that involve fine-tuning the model, such as reward-weighted regression or DDPO [37], is a promising area for future work.\nLike previous methods, our model relies on a final optimizer to complete mixed-sized placements of millions of cells. The quality of this optimizer will inevitably influence the final quality of placement."}, {"title": "Conclusion", "content": "In this work, we explored an approach that departs from many existing methods for tackling macro placement: using diffusion models to generate placements. To train and apply such models at scale, we developed a novel neural network architecture, as well as a synthetic data generation algorithm. We show that our models generalize to new circuits, and when combined with guided sampling, can generate optimized placements even on large, real-world circuit benchmarks.\nDespite the success of our guidance method, we believe significant improvements can be made in optimizing the generated samples through further training with DDPO, or fine-tuning on realistic circuit datasets.\nIn conclusion, we find that pre-training diffusion models on synthetic data is a promising approach, with our models generating competitive placements despite never having trained on realistic circuit data. We hope that our results inspire further work in this area."}, {"title": "Implementation Details", "content": "Hyperparameters for our model are listed in Table 10. We used the linear noise schedule and DDPM formulation found in Ho et al. [25]. We also list parameters for generating our synthetic datasets in Table 11."}, {"title": "Additional Details of Benchmarks", "content": "We show the statistical distribution of our mixed placement workload in Table 12. We have removed the ibm05 benchmark as it contains only standard cells."}]}