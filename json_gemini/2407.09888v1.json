{"title": "FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments", "authors": ["Dimitris Papadopoulos", "Nikolaos Matsatsinis", "Katerina Metropoulou", "Nikolaos Papadakis"], "abstract": "Our collective attention span is shortened by the flood of online information. With FarFetched, we address the need for automated claim validation based on the aggregated evidence derived from multiple online news sources. We introduce an entity-centric reasoning framework in which latent connections between events, actions, or statements are revealed via entity mentions and represented in a graph database. Using entity linking and semantic similarity, we offer a way for collecting and combining information from diverse sources in order to generate evidence relevant to the user's claim. Then, we leverage textual entailment recognition to quantitatively determine whether this assertion is credible, based on the created evidence. Our approach tries to fill the gap in automated claim validation for less-resourced languages and is showcased on the Greek language, complemented by the training of relevant semantic textual similarity (STS) and natural language inference (NLI) models that are evaluated on translated versions of common benchmarks.", "sections": [{"title": "Introduction", "content": "Motivation: The wider diffusion of the Web since the dawn of Web 2.0 has enabled instantaneous access to an expanding universe of information. The entire nature of news consumption has shifted dramatically, as individuals increasingly rely on the Internet as their major source of information. While people access, filter and blend several websites into intricate patterns of media consumption, this wealth of information contained in billions of online articles inevitably creates a poverty of attention and a need to efficiently allocate this attention among the many sources that may absorb it. Verifying whether a given claim coheres with the knowledge hidden in the vast amount of published information is a fundamental problem in NLP, taking into account that the arrival of new information may weaken or retract the initially supported inference. The problem is more apparent in less-resourced languages that lack the necessary linguistic resources for building meaningful NLP applications.\nApproach and Contribution: FarFetched is a modular framework that enables people to verify any kind of textual claim based on the incorporated evidence from textual news sources. It combines a series of processes to periodically crawl for news articles and annotate their context with named entities. Given a user claim, FarFetched derives a relevant subset of the stored content based on its"}, {"title": "Related Work", "content": "Our work comprises functionalities comparable to those of fact checking frameworks, targeting the assignment of a truth value to a claim made in a particular context (Vlachos and Riedel, 2014). For most related approaches (Zhang et al., 2021; Majithia et al., 2019; Zhou et al., 2019; Ciampaglia et al., 2015; Goasdou\u00e9 et al., 2013) the evidence to support or refute a claim is derived from a trustworthy source (e.g. Wikipedia, crowdsourced tagging or expert annotators). Interesting deviations are DeClarE (Popat et al., 2018) that searches for web articles related to a claim considering their in-between relevance using an attention mechanism, and ClaimEval (Samadi et al., 2016), based on first-order logic to contextualise prior knowledge from a set of the highest page-ranked websites.\nFarFetched can be distinguished from the aforementioned works by four major points: a) evidence collection is disentangled from manual annotation but relies on a constantly updating feed of news articles instead; b) claim validation based on the accumulated evidence relies on the effective combination of entity linking and attention-based models; c) our approach provides interpretable reasoning based on the aggregated evidence of multiple sources without assessing their truthfulness as opposed to most fact checking frameworks; and d) the outcome of the process is dynamic as the continuous integration of new information may lead to a shift in the verdict of the validated claim.\nRecent advances in the field of event-centric NLP have introduced event representation methods based on narrative event chains (Vossen et al., 2015), knowledge graphs (Tang et al., 2019; Vossen et al., 2016), QA pairs (Michael et al., 2018) or event network embeddings (Zeng et al., 2021) to capture connections among events in a global context. Our method relies on an entity-centric approach instead, where the identified entities are used as connectors between events, actions, facts, statements or opinions, thus revealing latent connections between the articles containing them. A few similar approaches have been proposed for combining world knowledge with event extraction methods to represent coherent events, but rely either on causal reasoning to generate plausible predictions (Radinsky et al., 2012) or on QA models that require the accompanying news source to be provided along with the user's question (Jin et al., 2021).\nThe latest advances regarding the technological concepts that comprise our methodology are provided below:\nEntity linking (EL) resolves the lexical ambiguity of entity mentions and determines their meanings in context. Typical EL approaches aim at identifying named entities in mention spans and linking them to entries of a KG (e.g. Wikidata, DBpedia) thus resolving their ambiguity. Recent methods combine the aforementioned tasks using local compatibility and topic similarity features (Delpeuch, 2019), pagerank-based wikification (Brank et al., 2017a) \u2014used also in FarFetched- or neural end-to-end models that jointly detect and disambiguate mentions with the help of context-aware mention embeddings (Kolitsas et al., 2018).\nThe recent interest for encapsulating diverse semantic sentence features into fixed-size vectors has resulted in SotA systems for Semantic Textual Similarity (STS) based on supervised cross-sentence attention (Raffel et al., 2020), Deep Averaging Networks (DAN) (Cer et al., 2018) or siamese and triplet BERT-Networks (Reimers and Gurevych, 2019) to acquire meaningful sentence embeddings that can be compared using cosine similarity. The latter approach is leveraged in our case to train an STS model for the Greek language using transfer learning.\nFinally, the task of Natural Language Inference"}, {"title": "Methodology", "content": "Given a user claim in free text, we tackle the problem of deciding whether this statement is plausible based on the currently accumulated knowledge from news sources. We also acknowledge the problem of constructing relevant evidence from multiple sources by analysing the information contained in online articles and the need for efficiently extracting only contextually and semantically relevant excerpts to verify or refute the user's claim. While our work does not primarily focus on better sentence embeddings and natural language inference techniques, we also target the lack of such models for the Greek language."}, {"title": "Our approach", "content": "FarFetched combines a series of offline (i.e. performed periodically) operations to accumulate data from various news sources and annotate their context with named entities. It also encompasses a number of online operations (i.e. upon user input) to assess the validity of a claim in free text. First, it identifies the entities included in the provided claim and leverages these as a starting point to derive a relevant subset of the stored textual information as candidate evidence. Each candidate is then compared with the claim in terms of textual similarity, in order to finally conclude on the most relevant evidence (premise) to reason about the validity of the claim (hypothesis) in an NLI setting. The distinct modules that comprise the framework are visualised in Figure 2. The process that FarFetched follows to evaluate a claim is summarized in Algorithm 1, while each module is described in greater detail in the following subsections."}, {"title": "News Collection", "content": "A multilingual, open-source crawler and extractor for heterogeneous website structures is leveraged to incorporate information from various news sources (Hamborg et al., 2017). It is capable of extracting the major properties of news articles (i.e., title, lead paragraph, main content, publication date, author, etc.), featuring full website extraction and requiring only the root URL of a news website to crawl it completely."}, {"title": "Graph Database Population", "content": "The crawled articles are forwarded to a graph database (Webber, 2012) that initially stores only two types of nodes: Article, which represents a news article with its aforementioned properties and Section that represents a sentence of each article's main text (i.e. concatenated title and article body). Each Article node is linked to one or more Section nodes via the HAS_SECTION relationship."}, {"title": "Entity Linking", "content": "Given that our approach relies on largely unstructured textual documents that lack explicit semantic information, Entity Linking (EL) constitutes a central role in revealing latent connections between seemingly uncorrelated article sections. To this end, FarFetched employs a type of semantic enrichment and entity disambiguation technique known as wikification (Brank et al., 2017b), which involves using Wikipedia concepts as a source of semantic annotation. It applies pagerank-based wikification on input text to identify phrases that refer to entities of the target knowledge base (Wikipedia) and return their corresponding WikiData Entity ID. The latter is used as a unique identifier for storing the entities as Entity nodes to the graph database and for linking them with the crawled article Section nodes, resulting to a more tightly connected graph, where article sections are connected to WikiData entities via the HAS_ENTITY relationship. The virtual graph of Figure 3 represents the structure (labels and relationships) of the graph database. It should be noted that an entity node might have an additional label (e.g. Person, City, Business) except for the generic Entity one, based on the WikiData class taxonomy."}, {"title": "Evidence Constructor", "content": "In a typical NLI setting, a premise represents our knowledge or evidence regarding an event and is used to infer whether a relevant hypothesis follows from it or not. In our case, article sections focusing on the same entities as the user's claim could potentially lead to the construction of useful evidence towards the validation of this claim. We can therefore leverage the entity-annotated article sections of our graph database to collect relevant evidence by aggregating information from multiple sources. To this end, we developed an evidence construction process that comprises the following steps:\n1. The claim provided by the user passes through the Entity Linking phase and one or more entities (WikiData concepts) are identified.\n2. The graph database is queried for all possible shortest paths that contain article sections between the identified entities. Given the implemented graph structure and n Entity nodes, this translates to a minimum path length of 2(n - 1) alternating Entity-Section nodes as shown in Figure 4. Since the existence of such path is not guaranteed, in cases that no path is found the algorithm will select an article section if it contains at least one mentioned entity.\n3. The article sections contained in these paths are concatenated to form a set of candidate evidence sequences. Their relevance with the claim at hand is assessed during the Semantic Textual Similarity phase."}, {"title": "Semantic Textual Similarity", "content": "We train and apply a sentence embeddings method to extract and compare the vector representations"}, {"title": "Natural Language Inference", "content": "The last step of our process leverages NLI to determine whether the user claim (hypothesis) is entailed by, contradicted, or neutral to the most relevant evidence (premise) of the previous phase. To tackle the aforementioned multilinguality issues of pretrained language models on less-resourced languages, we finetuned a Greek sentence-transformers Cross-Encoder (Reimers and Gurevych, 2019) (XLM-RoBERTa-base) model for the NLI task. The model was trained on the Greek and English version of the combined SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) corpora (AllNLI). We used the English-to-Greek machine translation model by Papadopoulos et al., 2021 to create the Greek version of the AllNLI dataset. The trained model takes the premise-hypothesis pair as input and predicts one of the following labels for each case: \"contradiction\": c, \"entailment\": e or \"neutral\": n. The logits for each class are then converted to probabilities using the softmax function. These labels along with their probability scores can be used to assess whether the claim is verified by the accumulated knowledge of the candidate evidence."}, {"title": "Experiments", "content": "The technical details for each building block of FarFetched are provided below:\nNews Collection and Storage: The news-please (Hamborg et al., 2017) Python library was used to ingest an initial corpus of news articles to support our experiments. The root URLs of two popular Greek news sites served as the starting point in order to recursively crawl news from a diverse topic spectrum, spanning from 2018 until 2021. We collected 13,236 articles, containing 31,358 sections in total. A Neo4j graph DBMS was used to store the crawled articles and sections as nodes and create their in-between relationships.\nEntity Linking: A Python script producing POST requests to the JSI Wikifier web API (Brank et al., 2017a) was implemented to annotate the article sections and enrich the graph database with WikiData entities. A total of 2,516 WikiData entities of different types (e.g. sovereign states, cities, humans, organizations, academic institutions etc.) were identified in the crawled articles. A pageRankSqThreshold of 0.80 was set for pruning the annotations on the basis of their pagerank score.\nEvidence Constructor: We implemented Algorithm 1 as a Python script that executes a parametrizable Cypher query to construct candidate evidence sequences; the identified entities in the claim are used as parameters and the concatenated article sections that link these entities together are returned. For our experiments, the maximum number of relationships between the alternating Sections and Entities was set to 2(n - 1) (shortest path), while the script returns candidate evidence sequences in descending order based on path length. These parameters can be modified if longer candidate evidence sequences are required."}, {"title": "Main results", "content": "In this section we perform a quantitative and qualitative demonstration of FarFetched's overall performance and also provide individual results for our STS and NLI models based on benchmark datasets."}, {"title": "End-to-end performance", "content": "Given the particularity of FarFetched in evidence collection (data originating from constantly updating web content), a quantitative evaluation of its performance is quite challenging. To combat the lack of relevant benchmarks for the Greek language, we leveraged the FEVER dataset by Thorne et al. 2018, which models the assessment of truthfulness of written claims as a joint information retrieval and natural language inference task using evidence from Wikipedia. Each row of the dataset comprises a claim in free text, a list of evidence information including a URL to the Wikipedia page of the corresponding evidence and an annotated label (SUPPORTS, REFUTES, NOT ENOUGH INFO). We manually translated a subset of 150 claims from the FEVER validation set from English to Greek and populated the graph database with the content of the corresponding Wikipedia URLs, which was automatically translated into Greek (due to its size), using the NMT model by Papadopoulos et al., 2021. We report FarFetched's performance in terms of accuracy, precision, recall and F1-score on Table 1.\nThe results indicate a balanced precision and recall for the REFUTES and SUPPORTS classes, while precision is relatively lower for the NOT ENOUGH INFO case. This can be partially attributed to the challenges of applying wikification on the automatically translated evidence content, leading to some claims not being linked to their corresponding evidence. Although the above results are not directly comparable to those of similar systems tested on the original English FEVER dataset, they show a significant gain over the baseline model of Thorne et al. 2018 (label accuracy of 0.49). Based on a large comparative study conducted by Bekoulis et al. 2021, FarFetched scores in the upper 30th percentile in terms of accuracy (scores ranging from 0.45 to 0.84); however, to the best of our knowledge none of these systems covers the Greek language.\nWe also provide a set of qualitative examples based on real data that aim at showcasing the capabilities of our system while also acknowledging the dynamicity of the evidence collection process. These scenarios are translated into English to facilitate readability. They include two parts each and are shown in Tables 2, 3 and 4. The original examples (in Greek) are available in the Appendix.\nIn Scenario 1, two contradicting user claims (1a, 1b) with the same entity mentions are provided by the user (Table 2). Since they refer to the same entities, the Evidence Constructor returns the same candidate evidence sequences for both claims in order to evaluate their validity. The most relevant one (STS score in bold) is selected for the NLI phase, where the verdict is that the evidence entails the first claim (1a) and contradicts the second (1b).\nIn Scenario 2, we investigate the sensitivity of our approach in exploiting new information to evaluate a claim (Table 3). The claim initially triggers the Evidence Constructor which returns multiple candidate evidence sequences, in descending STS order (yellow rows). During the NLI evaluation phase, the verdict is entailment, but with a low probability of 0.571 (2a). The same hypothesis is evaluated in Scenario 2b, after the addition of new information appended to the evidence list (blue"}, {"title": "STS performance", "content": "The performance of our semantic similarity model was evaluated on the test subset of the STS2017 benchmark dataset (Cer et al., 2017). Given that the original dataset does not provide sentence pairs in Greek, we manually created a cross-lingual version for the English-Greek pair. The performance is measured using Pearson (r) and Spearman (\u03c1) correlation between the predicted and gold similarity scores (Table 5). We also provide results regarding translation matching accuracy, evaluating the source and target language embeddings in terms of cosine similarity. Our model achieves a slightly"}, {"title": "NLI performance", "content": "We benchmark our trained NLI model on the Greek subset of the XNLI dataset (Conneau et al., 2018) that contains 5,010 premise-hypothesis pairs (Table 6). Despite not having used the XNLI dataset during the training phase, we achieve a 1% gain over the multilingual XLM-R (Conneau et al., 2020) and are on par with the monolingual Greek-BERT by Koutsikakis et al., 2020. Since our model was trained on a mixture of Greek and English sentence pairs, it is more suitable for corpora that also contain English terms (e.g. technology, science topics) without suffering from the under-representability of the Greek language occurring in multilingual models."}, {"title": "Limitations", "content": "We acknowledge that FarFetched is possible to encounter errors in 3 main areas; these limitations are briefly addressed below.\nEntity Linking: Highly ambiguous entities and name variations pose challenges to any entity linking method. Since we claim that our approach is entity-centric, a wrong entity annotation may lead to irrelevant candidate evidence sequences and increase the probability of \"neutral\" NLI verdicts. Moreover, the tunable sensitivity of the integrated wikification module implies a trade-off between a precision-oriented and a recall-oriented strategy, the latter resulting in more annotated articles, but also being prone to false-positive annotations.\nEvidence Construction: This initial version of our approach relies solely on the STS comparison between the evidence and the claim, based on a shortest path approach as discussed in Section 3.2.4. In cases that involve a larger number of entities in the user claim, calculating the shortest path between the alternating Entity-Section nodes can be computationally cumbersome. Moreover, there is no guarantee that the shortest path is able to capture the most relevant candidate evidence sequences; to this end, outputting the top n best candidates is considered, providing a user with an overview of the extracted news excerpts together with their NLI outcome. Finally, neither a temporal evaluation of the evidence with regard to the claim nor a distinction between opinions and facts is considered; all candidates are treated as equal.\nNatural Language Inference: Recognizing the entailment between a pair of sentences partially depends on the tense and aspect of the predications. Tense plays an important role in determining the temporal location of the predication (i.e. past, present or future), while the aspectual auxiliaries signify an event's internal constituency (e.g. whether an action is completed or in progress). While the work of Kober et al., 2019 indicates that language models substantially encode morphosyntactic information regarding tense and aspect, they are unable to reason based only on these properties. To this end, claims with a high presence of such semantic properties should be avoided."}, {"title": "Conclusions", "content": "In this work, we presented a novel approach for claim validation and reasoning based on the accumulated knowledge from the continuous ingestion and processing of news articles. FarFetched is able to evaluate the validity of any arbitrary textual claim by automatically retrieving and aggregating evidence from multiple sources, relying on the pillars of entity linking, semantic textual similarity and natural language inference.\nWe showcased the effectiveness of our method on the FEVER benchmark as well as on diverse scenarios and acknowledged its limitations. As byproducts of our work, we trained and open-sourced an NLI and an STS model for the less-resourced Greek language, achieving state-of-the-art performance on the XNLI and STS2017 bench-"}]}