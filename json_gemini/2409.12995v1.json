{"title": "Improving generalisability of 3D binding affinity models in low data regimes", "authors": ["Julia Buhmann", "Ward Haddadin", "Luk\u00e1\u0161 Pravda", "Alan Bilsland", "Hagen Triendl"], "abstract": "Predicting protein-ligand binding affinity is an essential part of computer-aided drug design. However, generalisable and performant global binding affinity models remain elusive, particularly in low data regimes. Despite the evolution of model architectures, current benchmarks are not well-suited to probe the generalisability of 3D binding affinity models. Furthermore, 3D global architectures such as GNNS have not lived up to performance expectations. To investigate these issues, we introduce a novel split of the PDBBind dataset, minimizing similarity leakage between train and test sets and allowing for a fair and direct comparison between various model architectures. On this low similarity split, we demonstrate that, in general, 3D global models are superior to protein-specific local models in low data regimes. We also demonstrate that the performance of GNNs benefits from three novel contributions: supervised pre-training via quantum mechanical data, unsupervised pre-training via small molecule diffusion, and explicitly modeling hydrogen atoms in the input graph. We believe that this work introduces promising new approaches to unlock the potential of GNN architectures for binding affinity modelling.", "sections": [{"title": "1 Introduction", "content": "Computer-aided drug design relies on the accurate prediction of protein-ligand binding affinity to achieve a therapeutic effect, ensuring selectivity against other proteins and avoiding off-target toxicity\nApart from classical approaches for binding affinity prediction (usually docking methods which use a combination of empirical molecular and statistical force fields [Jones et al., 1997, Eberhardt et al., 2021]), a diverse array of machine learning (ML) strategies have been proposed in the last decade. There has been increasing interest in developing ML models that use 3D data of protein-ligand complexes as input. In principle, these models are the best suited for predicting binding affinity since they should be able to capture fundamental interaction mechanisms such as hydrogen bonds or hydrophobic and ionic interactions between the protein and the ligand. Numerous types of 3D ML models are presented in the literature. In Ballester and Mitchell [2010] and Wang et al. [2021], the protein and ligand interactions were condensed into contact map features and used in tree-based models (Random Forest) and convolutional neural networks (CNN). In Volkov et al. [2022], they use a graph neural network (GNN) which uses the protein-ligand graph as input. They also propose including interacting nodes in the graph to indicate known interactions between protein and ligand atoms. Using GNNs as well, Zhang et al. [2023] add thresholds to the distance encoding to avoid overfitting on small distance variations.\nThe majority of the literature indicates that it is not yet clear whether a specific model type consistently achieves the best results [Durant et al., 2023]. Part of this ambiguity is due to the lack of consistent benchmarks to evaluate the performance of the diverse array of models. The common dataset used for benchmarking 3D binding affinity models is the PDBBind [Liu et al., 2015], a dataset of crystal structures from the Protein Data Bank (PDB) with curated binding affinity measurements. Over the last few years, many splits have been proposed for the PDBBind dataset to probe performance and generalisation of different model types [Volkov et al., 2022, Durant et al., 2023, Li et al., 2024], but each come with their own drawbacks.\nOverall, the results indicate that simple models and baselines perform just as well as the more complicated 3D models that use structural information [Durant et al., 2023]. This indicates that 3D models are not learning generalizable information but only dataset biases, hence they have not yet met their projected expectations [Volkov et al., 2022].\nIn this work we investigate the performance of binding affinity model families in a robust setting to probe what they learn and how they generalise. We compare 3D global models to protein-specific local models commonly used in real world drug discovery and also to baseline bias models. To achieve this, we propose a new split of the PDBBind dataset based on protein and ligand similarity and constructed to suit bechmarking the various model families fairly and consistently. We use the new split and strong baselines to test multiple novel improvements to a plain 3D GNN model to push the boundaries of binding affinity modelling using 3D GNNs.\nWe find that in low data regimes, 3D models significantly outperform protein-specific local models. With more data for a specific protein, local models quickly catch up. We also investigate the effect of hydrogen atoms on generalisability. As the structures in the PDBBind are not consistently prepared, we use protein preparation software to prepare them consistently and include hydrogen atoms explicitly in the GNN encoding. We again find that at low data regimes, including hydrogen atoms explicitly is very important for generalisation. This advantage goes away with more data. Finally, we propose two pre-training methods to improve global 3D model performance. We pre-train GNNs on supervised quantum mechanical energy prediction and unsupervised small molecule diffusion. We show that both result in improvements at low data regimes. As far as we are aware, this is the first application of quantum mechanical pre-training and diffusion pre-training for binding affinity prediction."}, {"title": "2 Methods", "content": "In this section, we present an overview of our methods and benchmarks. We discuss the dataset, structure preparation, model families benchmarked, and the new proposed split. We have made the code, prepared structures, and splits in this work publicly available at and ."}, {"title": "2.1 PDBBind dataset", "content": "We choose the PDBBind dataset (release v2020) as our benchmarking data set [Liu et al., 2015]. The data consists of crystal structures of bound protein-ligand complexes deposited in the PDB with curated binding affinity values (K1, KD or IC50). We use the protein-ligand subset of the general set from PDBBind consisting of 19443 unique protein-ligand structures. The dataset is unbalanced. Many proteins have binding affinities measured only against a single ligand (one structure), some have measurements for a few ligands (few structures), and very few have measurements against more than 100 ligands.\nIt has been demonstrated, in both predictive and generative settings [Durant et al., 2023, Li et al., 2024, Buttenschoen et al., 2023], that the splits routinely used in model benchmarking on PDBBind contain data leakage. The similarity between proteins and ligands across training and test sets inflates metrics for certain models or tasks and makes rigorously probing their performance and generalisability difficult."}, {"title": "2.2 Structure preparation", "content": "We queried the PDB [wwPDB consortium, 2019] for biological assemblies of all the structures listed in the PDBBind dataset. Due to the nature of structure-determining techniques, the structures contain not only ligands of biological nature, but also residues of crystalization buffers or cryoprotectants. Hydrogen atoms are also mostly missing.\nTo address both of these issues, each structure was prepared using CCDC software [Groom et al., 2016], namely the Python API (v.3.0.16). Hydrogen atoms were added and the ligands listed by PDBBind as biologically relevant were extracted into separate files. The remaining ligands of non-biological nature along with water molecules were removed.\nAfter preparation, 18310 structures out of the total 19443 remained (1133 failed)."}, {"title": "2.3 Models", "content": "Next, we discuss the models used in this study. An overview is presented in Table 1. A full description of the hyperparameters of the features and models is provided in the Supplementary Information."}, {"title": "2.3.1 Model families", "content": "Before presenting the models used, we establish clear groups to classify them. There are numerous ways to categorize binding affinity model types, but in this work we introduce a particular grouping focused on two key aspects: the scope of application, global vs. local, and the type of data input, non-3D vs. 3D.\nWe denote models intended for use on different protein targets by global models and ones intended for use against a single protein target by local models. Local models are typically trained on ligand activity data measured against a single protein. Binding affinity models can be further grouped according to how ligands and proteins are represented. We denote models which do not use 3D coordinates by non-3D models. These models often use ligand descriptors such as fingerprints to encode ligands. If protein information is used, it is most commonly encoded with its amino acid sequence. 3D models use the 3D coordinates of a ligand or a protein-ligand complex alongside non-3D information."}, {"title": "2.3.2 Single-Protein local models", "content": "To compare to standard practices in the drug discovery community and in real world projects [Lo et al., 2018], we benchmark the performance of Single-Protein local models built from ligand-based features and trained on binding activity data measured against a single protein. Analogous to established workflows [Jiang et al., 2021, Deng et al., 2023], we use classical ML models (Random Forest [Breiman, 2001], XGBoost [Chen and Guestrin, 2016], CatBoost [Prokhorenkova et al., 2018], Support Vector Machine [Cortes, 1995]) combined with a selection of fingerprints (ECFP, FCFP, atom pairs, topological torsion) and molecular descriptors for features. More details on features and models are listed in the Supplementary Information."}, {"title": "2.3.3 EGNN models", "content": "We use the EGNN [Satorras et al., 2022] model as the base architecture for our 3D global models. We use the implementation in the PHYSICSML python package Exscientia [2024b]. As in previous work, we extract the protein pocket by selecting protein atoms within 5\u00c5 of any ligand atom. The graph is constructed from the pocket and ligand atoms as nodes and a 5\u00c5 cut-off is used to define the edges. We use one-hot encoded atomic numbers as node features and no edge features.\nPre-Trained EGNNs In this benchmark, we use a few different versions of the EGNN model. In addition to the basic model, we use two pre-trained versions. The first model, called EGNN-QM, is trained on ANI1ccx, a dataset of 500k small molecules with quantum mechanical energies computed at the ccsd(t) level. The knowledge learned from quantum mechanical interactions and internal energies should in principle better inform binding affinity prediction. The second model, EGNN-DIFF, is trained as a small molecule diffusion model on the QM9 dataset (as described in Hoogeboom et al. [2022]). By pre-training on diffusing stable QM9 molecules, the model will have learned to distinguish between low and high energy conformations. In principle, this should allow it to better understand binding affinity interactions.\nTo transfer these models to the PDBBind dataset, we use a two stage procedure. First, we freeze the backbone and add a new randomly initialised pooling head and train until convergence. Then, we unfreeze the backbone and train all parameters at a lower learning rate until convergence.\nInformation about both pre-training strategies and transfer learning is available in the Supplementary Information.\nHydrogens Previous works modelling binding affinity via GNNs have chosen to omit hydrogen atoms from the input graph (Li et al. [2021], Volkov et al. [2022]). Since hydrogen atoms contribute significantly to binding via hydrogen bonds, we wanted to assess the effect of including hydrogen atoms as nodes in the graph. We benchmark the models with no hydrogen atoms (None), with only the polar hydrogen atoms (Polar), and with all hydrogens (Explicit).\nSingle-Graph vs. Multi-Graph Finally, to probe whether the models learn to identify the interac-tions from the protein-ligand pose, we also train models on the pockets and ligands as separate graphs. We use the same backbone to generate embeddings for both and then combine these embeddings in a pooling head to make the final prediction. Practically, this removes any edges between protein and ligand nodes in the graph. We refer to these models, which treat proteins and ligands as separate graphs, as Multi-Graph models. On the other hand, the conventional models that operate on the interacting pose are referred to as Single-Graph models."}, {"title": "2.3.4 RF-Score and OnionNet-2", "content": "We include in our analysis two additional 3D global models, RF-Score and OnionNet-2. We select those models due to their performance on other splits of the PDBBind dataset as indicated in the study by Durant et al. [2023]. Specifically, RF-Score was one of the top performer on the CASF-2016 split while the OnionNet-2 model was superior on the 2019-Holdout and Peptides-Holdout sets relative to other models tested."}, {"title": "2.3.5 Baseline models", "content": "Ligand-Bias model To probe the ligand dataset bias in the benchmarking splits, we follow the work of Durant et al. [2023] and design a Ligand-Bias global model. This model is trained on the identical data used to train the global models (binding affinity measurements of ligands against different proteins), but only uses ligand-based features as input (analogous to Single-Protein models; see 2.3.2). Effectively, this mixes binding affinity values of compounds measured against different proteins to probe the amount of dataset bias available in ligand information alone.\nMolecular-Weight model The molecular weight of a compound tends to be a strong predictor of its binding affinity, with larger compounds generally exhibiting stronger affinity [Olsson et al., 2008]. Within the context of drug discovery, it is particularly important to avoid building binding affinity models that strongly make use of the molecular weight property, as larger drug candidates have a higher probability of failure [Hopkins et al., 2014]. In this study, we employ a Molecular-Weight model that uses the molecular weight of the ligand as its sole input as a baseline. We use the same architectures and training data as for the Single-Protein local models."}, {"title": "2.4 Splitting", "content": "To build a robust benchmark and effectively probe model generalisation, we propose a new split of PDBBind based on protein and ligand similarity, which we call the Low-Sim split. Although many splits have been proposed, we believe that none achieve our goal of probing generalisation. Close inspection of the proposed splits [Volkov et al., 2022, Durant et al., 2023, Li et al., 2024] shows non-negligible levels of similarity between train and test set, with all splits sharing some proteins (UniProts) across sets. Table 2 shows the amount of UniProt overlap in previously proposed splits. Furthermore, the splits were not constructed to benchmark the variety of model families available (local vs. global, 3D vs. non-3D).\nCase-Study-Proteins We design our newly proposed dataset splits such that we can compare between global and local models. This comparison is particularly relevant given that local Single-Protein models are still widely used in drug discovery projects, owing to their robustness, cost-effectiveness, and trainability on smaller datasets. To obtain a direct and fair comparison to these local models, we select eight proteins from the PDBBind dataset which have more than 100 datapoints as our case study. The threshold of 100 points per protein is to allow enough data to train local models. In total, the Case-Study-Proteins consists of 1857 structures. These proteins will be used to benchmark the generalisability of global models and also to train local models for each specific protein.\nSimilarity filtering We apply two similarity filtering steps to the remaining structures in the PDBBind dataset to create a subset that is dissimilar to the Case-Study-Proteins set. We call this reduced dataset Other-Proteins.\nTo account for protein similarity, we compute the similarity to the Case-Study-Proteins using FoldSeek [van Kempen et al., 2024], which uses 3D structural and residue information to efficiently compute a similarity score [0, 1] between two protein structures. To probe the generalisability of the models, we filter out any structures which have more than 0.5 similarity to the Case-Study-Proteins structures. Additionally, we compute the tanimoto similarity between the Case-Study-Proteins ligands and the ligands of the remaining structures. We filter out any structures with ligands with more than 0.5 tanimoto similarity. A total of 5431 similar structures are removed and leaves 11022 structures in the Other-Proteins set.\nLow-Sim 0%, 5%, 30%, 80% We aim to examine the change in model performance as the amount of data increases, from low data regimes (when binding affinity values are available for approximately 0 to 30 ligands for a specific protein) in comparison to a medium data scenario where increasingly more data is available.\nWe use the Low-Sim split (1875 Case-Study-Proteins structures and 11022 Other-Proteins structures) to construct the benchmarking splits as follows. We stratify the Case-Study-Proteins data by protein and split the structures by tanimoto ligand similarity with increasing fraction of training data, 5%, 30%, 80%. At each percentage, we generate three folds with a different starting seed ligand for the similarity splits. Local models (Single-Protein and Molecular-Weight) are trained on these splits for each protein individually. For the global models (RF-Score, OnionNet-2, EGNN, Ligand-Bias baseline), we further augment the train sets of these splits with the Other-Proteins. Additionally, we construct the 0% split where all Case-Study-Proteins are in the test set and only the Other-Proteins are in the train set. This is to probe generalisation to completely new proteins. The final benchmarking splits are shown schematically in Figure 1. In absolute numbers, those splits correspond to the following number of training samples per protein: 11\u00b15 (5%), 69\u00b130 (30%), 185\u00b183 (80%)."}, {"title": "2.4.1 Training and validation", "content": "For each split (0%, 5%, 30%, and 80%), we perform a cross-validation split of (random 80:20 split) for model selection. The models are then retrained on the combined train and validation set and the performance is measured on the test set.\nFor deep learning models (EGNN architectures), we additionally use a random 80:20 split of the train set for early stopping."}, {"title": "2.4.2 Metrics", "content": "Typically, binding affinity models are assessed using the Pearson correlation coefficient, which measures the correlation between predicted and actual binding affinity values. However, this doesn't measure the absolute predictive performance, which is crucial in real-world drug discovery for optimising multi-parameter objectives. Therefore, here we focus on the absolute $R^2$ metric for benchmarking performance, while also providing root mean squared error and Pearson correlation results in the Supplementary Information.\nOn the Low-Sim test sets, we compute the metrics in two different ways. The overall performance refers to metrics computed on the predicted and actual binding affinity values across all eight proteins. Mixing predictions of all the different protein-ligand pairs is the common approach when reporting results on the PDBBind dataset [Meli et al., 2022]. We also report the performance stratified by protein, where metrics are calculated individually for each protein."}, {"title": "3 Results", "content": "We now present the results of the benchmarking. First, we present the results and comparisons across model families. We follow this with more detailed analyses of the 3D EGNN models."}, {"title": "3.1 Model family comparisons", "content": "The results reveal a clear advantage for global 3D models over local models in low data regimes. As seen in Figure 2, the global 3D models (EGNN, pre-trained EGNN, and RF-Score) have moderate generalisation even with 0% protein-specific training data and outperform the Single-Protein model at lower train data fractions (5% and 30%). With enough data (80%), most models plateau at similar performance (EGNN, pre-trained EGNN, RF-Score, Single-Protein). We hypothesise that this is due to the difficulty of the tanimoto similarity splits. At low data levels, the local models not only have very little data to learn from but also a train set which is very dissimilar to the test ligands. In drug discovery settings where generalisation is important, this benchmark clearly highlights the importance of global models.\nNotably, the pre-trained EGNN models outperform all other models and baselines when considering the overall performance (Figure 2, left). See Section 3.2 for a detailed analysis of their performance.\nOne thing to note is the relatively good performance of the Ligand-Bias model (red bar in Figure 2). Although not as good as the global 3D models, the Ligand-Bias model is able to generalise to the unseen proteins. We hypothesise that this is due to the model picking up on functional group importance in the ligands, given that a lot of binders rely on generic functional groups.\nIt is also interesting to note that, as seen in Figure 3, the performance of the global 3D models is relatively consistent with the level of training data (improving slightly) whereas the Single-Protein local model suffers greatly at low data, dramatically improving with increasing amounts of training data."}, {"title": "3.2 Improvements to EGNNS", "content": "We now present a detailed analysis of the global 3D EGNN models."}, {"title": "3.2.1 Pre-Training", "content": "First, we look at the effect of pre-training on model performance. As Figure 4 shows, pre-training significantly improves model performance. Quantum mechanical pre-training provides the largest advantage followed closely by diffusion pre-training. As expected, the advantages are more noticeable at low levels of training data, gradually fading out as more training data is added. To our knowledge, this is the first application of using pre-trained models for 3D binding affinity prediction."}, {"title": "3.2.2 Hydrogens", "content": "Next, we assess the effect of including hydrogen atoms on model performance. To our knowledge, in all previously published work, hydrogen atoms were ignored in the structures. It was not clear if this occurred due to better model performance on the benchmarks or due to knowledge of the poorly prepared PDBBind structures. As described in section 2.2, we noticed that the structures in PDBBind did not have consistent hydrogen preparation and used CCDC software to add them consistently."}, {"title": "3.2.3 Single-Graph vs. Multi-Graph", "content": "Finally, we look at the effect of the interacting pose on performance. In the default architecture (single-graph), the ligand and pocket are given to the model as a single 3D graph. In the multi-graph architecture, the ligand and the protein are first encoded with separate graphs. In theory, we expect the single-graph to outperform the multi-graph architecture.\nAs we can see in Figure 4, using the pose information (single-graph model) does not necessarily improve the model performance. This could either be due to the model being able to infer the interactions without the exact pose or not properly learning the interactions in the first place (and so we do not notice its effect). Either way, since these models outperform the Ligand-Bias model, they must be using the protein information in some way. Further investigation is required to understand what the models are learning."}, {"title": "4 Discussion", "content": "In this paper, we demonstrate that for binding affinity prediction on new proteins and chemical spaces, global 3D models outperform local models on the PDBBind dataset. Furthermore, we show that explicit hydrogen atoms in the structures and novel pre-training strategies using quantum mechanical data and diffusion modelling provide performance improvements in low data regimes for GNNs.\nThere are limitations to the above benchmark. This work focuses on the PDBBind dataset, a dataset comprising only crystal structures. We explicitly chose to do so to eliminate any sources of noise or error from computationally generated structures and poses. However, this has two drawbacks. First, the structures and chemical space of ligands in the dataset are not representative of the distribution of ligands in a real-world drug discovery projects. The relatively good performance of the Ligand-Bias model indicates the lack of diversity in the ligand distribution. Second, since crystal structures can only be obtained for binding ligands, this benchmark does not probe the performance of models for non-binding ligands. This is important for virtual high throughput screens where many ligands might not be binders.\nIn light of these limitations, it is crucial to replicate a similar analysis on additional datasets, ideally resembling real-world drug discovery datasets. If insights from this study are confirmed with more datasets, this research could represent a significant stride towards developing more universally applicable 3D binding affinity models that leverage pre-training strategies."}, {"title": "D Metrics", "content": "Below are the explicit equations for the metrics used in the benchmarks\nPearson Correlation Coefficient = $\\frac{\\Sigma_i(x_i \u2013 \\bar{x})(Y_i \u2014 \\bar{y})}{\\sqrt{\\Sigma_i (x_i-\\bar{x})^2 \\Sigma_i(Y_i-\\bar{y})^2}}$\n$R^2 = 1 \u2212 \\frac{\\Sigma_i(x_i - Y_i)^2}{\\Sigma_i (Y_i \u2013 \\bar{y})^2}$"}]}