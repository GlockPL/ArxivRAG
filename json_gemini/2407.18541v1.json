{"title": "Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised Speech Models", "authors": ["Neil Shah", "Shirish Karande", "Vineet Gandhi"], "abstract": "We propose a novel approach to significantly improve the intelligibility in the Non-Audible Murmur (NAM)-to-speech conversion task, leveraging self-supervision and sequence-to-sequence (Seq2Seq) learning techniques. Unlike conventional methods that explicitly record ground-truth speech, our methodology relies on self-supervision and speech-to-speech synthesis to simulate ground-truth speech. Despite utilizing simulated speech, our method surpasses the current state-of-the-art (SOTA) by 29.08% improvement in the Mel-Cepstral Distortion (MCD) metric. Additionally, we present error rates and demonstrate our model's proficiency to synthesize speech in novel voices of interest. Moreover, we present a methodology for augmenting the existing CSTR NAM TIMIT Plus corpus, setting a benchmark with a Word Error Rate (WER) of 42.57% to gauge the intelligibility of the synthesized speech.", "sections": [{"title": "1. Introduction", "content": "Verbal communication is a highly efficient form of social interaction, primarily facilitated by the intricate coordination of various physiological processes. The expulsion of air from the lungs triggers the vibration of vocal folds, and the articulation of the tongue, cheeks, and lips further shapes this airflow into speech. However, in cases of vocal tract pathology [1], characterized by complete or partial airway obstruction, the conventional process of speech production is disrupted. Furthermore, in situations involving private communication on mobile phones or conversations in places such as healthcare facilities or quiet public spaces, individuals may prefer to refrain from engaging in normal speech. Therefore, exploring alternatives by advancing speech and signal processing is crucial, driving research in Silent Speech Interfaces (SSI).\nSSI represents a unique form of spoken communication where an acoustic signal is absent; the individual articulates silently without generating sound. Techniques in SSI comprehend speech content by analyzing silent articulatory movements or vibrations resulting from airflow movement across the neck. Several SSI techniques include Lip reading [2], Ultrasound Tongue Imaging (UTI) [3], Real-Time Magnetic Resonance Imaging (RT-MRI) [4], Electromagnetic Articulography (EMA) [5], Permanent Magnet Articulography (PMA) [6], Electrophysiology [7], Electrolarynx (EL) [8], and Electro-Optical Palatography [9]. However, many of these techniques face challenges for everyday use due to their non-real-time nature and invasive characteristics. For instance, UTI employs ultrasound to capture tongue movement, rtMRI records the mid-sagittal plane of the upper airway through MRI, and EMA measures sensory vibrations on articulators like lips and tongue, with limitations including intense vibrator and MRI equipment noise [4], poor lighting [3], and extended time for speech proficiency [8].\nApproximately two decades ago, Nakajima et al. [10] pioneered a non-invasive SSI technique utilizing a specialized microphone to capture flesh-conducted NAM vibrations behind the ear. Their work demonstrated speech recognition viability based on NAM vibrations collected in the Japanese language. About ten years later, Yang et al. [11] introduced the CSTR NAM TIMIT Plus corpus, a 40-minute dataset featuring NAM and its corresponding whisper speech in the English language. This resulted into several efforts to translate NAM vibrations into conventional speech [12, 13]. However, several limitations impede current initiatives in this domain.\n\u2022 All these methods explicitly record ground-truth speech [13].\n\u2022 The intelligibility and quality of the synthesized speech remains excessively low.\n\u2022 All these approaches predicts Mel-based features from NAM vibrations, limiting their method's ability to synthesize speech in novel voice of interest.\n\u2022 Due to the limited size of the database, current research cannot fully leverage the capabilities of modern deep learning techniques in this field.\nThese challenges further compound pre-existing issues in NAM vibrations, such as the absence of a fundamental frequency [10] and attenuations in high-frequency components [14]. In this study, we propose a novel method to synthesize speech from NAM signals using recent Self-Supervised Learning (SSL) methods. Our study makes the following contributions:\n\u2022 Unlike other methods, our framework functions without the explicit necessity for studio-recorded ground-truth speech.\n\u2022 To improve intellegibility of the synthesized speech, we propose a novel data augmentation technique for simulating speech in NAM voices and introduce a Dynamic Time Warping (DTW) method to optimize alignment with its corresponding speech.\n\u2022 We introduce a Seq2Seq learning algorithm for cross-modality learning between NAM and the ground-truth speech representations in the latent space. This enables our method to effectively clone speech content to novel voices."}, {"title": "2. Related work", "content": "Pioneering work by Nakajima et al. [10] devised a specialized microphone to collect NAM samples and proposed to employ traditional Hidden Markov Models (HMMs) for NAM-to-text recognition. Toda et al. [15] initially endeavored to generate direct speech from NAM vibrations. However, they encountered challenges in estimating fundamental frequency contours, resulting in converted speech lacking prosody and intelligibility. They proposed synthesizing whispers from NAMs to address this issue. Since then, the most recent efforts [12, 13] focused on extracting whisper representations for NAM-to-speech conversion tasks. Subsequent efforts to [15] focused on enhancing the recording device for improved design and usability [16]. Significant contributions were made by Shimizu et al. [17, 18], who explored the frequency characteristics and sensitivities of various NAM microphone designs. Shah et al. [12] employed Generative Adversarial Networks (GANs), while Malaviya et al. [13] utilized multiple auto-encoders aligned in the latent space to derive efficient speech representations. However, existing approaches heavily rely on studio-recorded ground-truth speech [13]. To the author's knowledge, no study has yet demonstrated the possibility of synthesizing speech without explicitly recording studio-quality ground-truth speech data while showing improved intelligibility or explored the feasibility of employing machine-learning-based self-supervision techniques.\nOur research is related to the broader field of speech conversion, where translation occurs between two distinct modalities [19, 20, 2]. A significant aspect of our study is the reliance on SSL, where information extracted from the input audio serves as the label for learning representations in subsequent iterations. We achieve the Seq2Seq translation by extracting self-supervised embeddings using HuBERT [21] from both the input NAMs and the simulated ground-truth speech. This work is motivated by HuBERT's ability to operate directly at the raw waveform level, preventing loss of information due to input quantization and showing superior performance compared to other methods, as demonstrated in [22, 23, 24, 20]."}, {"title": "3. Method", "content": "Figure 1 provides an overview of our proposed method, encompassing three key stages: ground-truth speech simulation from available whisper data, data augmentation to generate additional NAM voice samples, and a Seq2Seq network trained and paired with a speech vocoder for synthesizing speech in novel voices during inference."}, {"title": "3.1. Speech encoder", "content": "Current SOTA techniques in NAM-to-speech conversion, as exemplified by [13], utilize Mel-cepstral features for encoding raw audio. However, these features encapsulate all aspects of the given audio, including speaker and ambient noise. Consequently, during training, the network is compelled to reconstruct the intended speech content in addition to the speaker and ambient noise information. This requirement complicates the training process and negatively influences the intelligibility of the converted speech and its ability to synthesize speech in novel voices.\nRecent progress in widely-used SSL models like BASE HuBERT (Hsu et al., 2021) shows promise in capturing detailed speech representations while excluding speaker and background noise information. The network employs a BERT-like masked-prediction loss on a substantial volume of unlabeled speech data. The k-means algorithm clusters the features, with cluster IDs serving as pseudo labels for a classification loss. These pseudo-labels refine the self-supervised representations iteratively. Instead of extracting cluster IDs as discrete units for the final speech representation, we choose to use 768-dimensional embeddings for training our Seq2Seq mapping network. This approach allows us to better preserve crucial speech content during synthesis."}, {"title": "3.2. Ground-truth speech simulation", "content": "The current leading method in NAM-to-speech conversion [13] relies on manually recording studio-quality speech data for training. Our approach departs from this conventional manual recording method and instead simulates ground-truth speech using paired whisper audio available in the CSTR NAM TIMIT Plus corpus. Figure 1(A) illustrates our proposed simulation step. Initially, we obtain quantized HuBERT representations from utterances in the LJSpeech [25] dataset and train a speech vocoder to resynthesize speech in the LJSpeech voice. Subsequently, we acquire quantized HuBERT representations from the whisper audio and feed them into the trained vocoder to generate corresponding speech in the LJSpeech speaking style. This approach performs voice cloning from a whispering style to normal speech, which is time-aligned by the method (using one quantized unit for each 1/50th second). We rely on whisper audio, as opposed to NAM audio, to simulate ground-truth speech, driven by lower error rates and the presence of a fundamental frequency in the whisper audio."}, {"title": "3.3. Data augmentation", "content": "The only existing corpus for NAM-to-speech conversion is limited to just 40 minutes, posing a significant challenge for learning a transformer-based Seq2Seq framework to derive intelligible and high-quality speech representations. Recent work by Jun Rekimoto [24] proposed the mechanical conversion of speech data to a whisper voice, to generate additional training samples, using an LPC-based audio conversion tool [26]. However, no such tools or techniques exist for augmenting NAM voice due to the challenges discussed in Section 1. To address this issue, as illustrated in Figure 1(B), we propose augmenting the existing NAM corpus using speech-to-speech synthesis techniques. Similar to section 3.2, we use the encoder-decoder speech cloning architecture. We first obtain quantized HuBERT representations of the NAM vibrations and train a speech vocoder to re-synthesize speech in the NAM voice. Subsequently, we derive HuBERT representations from utterances in the LJSpeech dataset [25] and pass them through the trained speech vocoder to synthesize speech in the NAM voice (speaking style). The derived HuBERT representations from the LJSpeech dataset only retain the speech content [22], neglecting the speaker and ambient noise characteristics, thus significantly benefiting our proposed data augmentation technique. This method allows us to augment the existing NAM corpus with content from the LJSpeech [25] dataset, totaling approximately 24 hours of NAM data. We refer to this augmented NAM dataset as LJNAM and its corresponding speech data as LJSpeech."}, {"title": "3.4. Time alignment of representations", "content": "As discussed in section 3.2, the simulated ground-truth speech is time-aligned by virtue of the ground-truth data simulation technique. However, the augmented LJNAM dataset and its corresponding LJSpeech data are not inherently time-aligned. Time alignment between the input and the ground-truth is essential for learning a Non-Autoregressive (NAR) Seq2Seq architecture. To address this, we propose using FastDTW [27], a DTW technique that aligns representations of varying lengths, allowing non-linear warping to find an optimal match. The DTW matching algorithm is applied to the extracted SSL embeddings from both signals. The algorithm starts with the assumption that the embeddings of both sequences at the 0th frame match and then identifies the most matching frames for the shorter sequences from the available frames in the longer sequence. This method aligns the augmented LJNAM and its corresponding LJSpeech utterances, a crucial step for learning the Seq2Seq mapping between the two modalities."}, {"title": "3.5. Seq2Seq network", "content": "We utilize a NAR transformer-based Seq2Seq network (Figure 1(C)) to learn the mapping between the two latent spaces. Our network takes as input the SSL embeddings derived from NAM and LJNAM samples, while the SSL embeddings from simulated ground-truth speech and LJSpeech data serve as the target ground-truth representations. We opt for NAR models over autoregressive models because they generate all output tokens simultaneously in a single iteration, thus facilitating real-time applications [24].\nThe encoder and decoder, each comprising six layers, consist of feed-forward transformer blocks with two multi-head self-attention mechanisms [28] and 1-dimensional convolutions inspired by Fastspeech2 [29]. The encoder processes NAM embeddings into a sequence of fixed-dimensional vectors, while the decoder predicts ground-truth speech embeddings. For training, we set the batch size to 16 and the maximum number of steps to 20,000. The Adam optimizer is employed with an initial learning rate of 4.4 \u00d7 10\u22122, an annealing rate of 0.3, and annealing steps at [3000, 4000, 5000]. The HuBERT model encodes speech into embeddings at a frame rate of 50Hz. The model utilizes Mean Squared Error (MSE) loss, quantifying the difference between the decoded and ground-truth speech embeddings. The objective can be written as:\n$L_{MSE} = \\frac{1}{T} \\sum_{i=1}^{T} ||S_{ssl_i} - \\hat{S}_{ssl_i}||^2, $ (1)\nwhere $S_{ssl_i}$ are the ground-truth speech embeddings, $\\hat{S}_{ssl_i}$ are the decoded speech embeddings, and T is the time-steps.\nWe add an extra fully connected linear layer to improve the model's capability in predicting Connectionist Temporal Classification (CTC) tokens immediately following the transformer encoder layer. The ground-truth text sequences are tokenized using the Wav2Vec2 tokenizer [30]. Let $N_{ssl}$ represent the input NAM embeddings, and $Enc_{ssl}$ denote the output of the transformer encoder. If C denotes the character labels corresponding to the ground-truth text, the objective is to minimize the negative log-likelihood using $P_{CTC}(C|Enc_{ssl})$ [31]. It is mathematically defined as :\n$L_{CTC} := -log P_{CTC} (C|Enc_{ssl}).$ (2)\nBy calculating the weighted sum of the MSE and CTC loss functions, the final objective function can be written as:\n$L_{Tot} = \\alpha_{CTC} * L_{CTC} + \\alpha_{MSE} * L_{MSE},$ (3)\nwhere $\\alpha_{CTC} \\in \\mathbb{R}$ and $\\alpha_{MSE} \\in \\mathbb{R}$ are the hyperparameters that balances the influence of the two loss terms. We set $ \\alpha_{CTC} $ and $ Q_{L1} $ to 0.001 and 1, respectively."}, {"title": "3.6. Speech vocoder", "content": "The speech vocoder takes as input the speech embeddings predicted by a Seq2Seq network and synthesizes speech. We train a customized version of HiFiGAN-v2 [32, 22] for synthesizing speech from the SSL embeddings. In the generator, transposed convolutions upsample the SSL embeddings of the ground-truth speech, and a residual block is used for receptive field expansion, ultimately generating the synthesized signal. The discriminator distinguishes between the synthesized and original signals, utilizing multi-period and multi-scale networks to capture temporal patterns, details, and global structure. To enable the generation of content in the voice of the user's preference, we train a multi-speaker speech vocoder using voices available from [25, 33]. For model configuration, we set the batch size to 16, the learning rate to 2 \u00d7 10\u22124, the number of embeddings to 100, the embedding dimension to 128, and the model input dimension to 256."}, {"title": "4. Dataset", "content": "We evaluate our proposed framework on the CSTR NAM TIMIT Plus corpus [11], a publicly available dataset comprising NAM vibrations along with their corresponding whisper audio and text. The dataset comprises 421 sentences spoken by a female speaker, extracted from the Herald text in a studio setup. The entire dataset spans 40 minutes, with a sampling frequency of 16,000 Hz. Consistent with prior studies [13, 12], we randomly allocate 13% of the data for the test set, while the remainder, along with the augmented LJNAM dataset, is used for training. Additionally, we reserve 5% of the training data for computing the validation loss."}, {"title": "5. Results and discussion", "content": "In this section, we conduct a quantitative assessment of the synthesized speech, comparing it to the current SOTA method [13]. We also illustrate the benefits of integrating CTC loss and data augmentation into the existing CSTR NAM TIMIT Plus corpus, to enhance the intelligibility of the synthesized speech. Unlike many existing works in this field that solely report MCD, our study goes beyond and includes error rates as a quantitative measure of intelligibility for synthesized speech. We utilize Whisper-ASR [34] to transcribe the synthesized speech, computing both WER and Character Error Rate (CER). The simulated ground-truth speech using our proposed ground-truth speech simulation (Section 3.2) exhibits a CER of 12.43% and a WER of 24.73%."}, {"title": "5.1. Recognition performance with no data augmentation", "content": "Table 1 presents the quantitative evaluation of our proposed method without data augmentation compared to current state-of-the-art approaches. Due to the unavailability of studio-recorded ground-truth speech from the authors of MSpec-Net [13], we cannot train their model, and consequently, we cannot compute WER and CER on our test split. Their paper exclusively reports the MCD metric. The introduction of CTC loss to the Seq2Seq network resulted in improvements across all three metrics. This indicates that leveraging ground-truth text significantly enhances the learned speech representations. Remarkably, our proposed method with inclusion of CTC loss, achieves a significant decrease of 29.08% and 42.42% in the MCD metric compared to the current SOTA MSpec-Net and DiscoGAN frameworks. It is essential to note that MSpec-Net [13] calculates the MCD metric based on studio-recorded ground-truth speech, while our method utilizes simulated ground-truth speech as discussed in Section 3.2."}, {"title": "5.2. Recognition performance with data augmentation", "content": "Table 2 presents the quantitative evaluation of our proposed method when augmenting the existing corpus with LJNAM samples, derived using the method described in Section 3.3. Although we did not observe any improvement in MCD with data augmentation compared to when no data is augmented, we did notice a significant enhancement in the intelligibility of the synthesized speech. By augmenting the existing corpus with LJNAMs and utilizing CTC loss, we achieved reductions of 12.82% and 10.96% in WER and CER, respectively. This suggests that augmenting the existing corpus with synthetically simulated data using our proposed data augmentation technique substantially enhances the intelligibility of the synthesized speech. Additionally, applying the data augmentation method to the LibriTTS [35] corpus did not result in further improvement in perceived intelligibility."}, {"title": "5.3. Qualitative evaluation", "content": "Figure 2 presents a comparison of Mel spectrograms between the original NAM signal and the synthesized speech by DiscoGAN, MSpec-Net, and our proposed method. DiscoGAN struggles to produce coherent speech, while MSpec-Net captures some intelligible content but lacks naturalness and fails to preserve lower-frequency components present in the original NAM signal. In contrast, our approach, depicted in Figure 2 (D), enhances lower-frequency formants and accurately predicts the higher-frequency formants. This dual capability significantly enhances intelligibility, establishing a novel benchmark in NAM-to-speech conversion. Our proposed setup can synthesize speech using novel voices, addressing a limitation in existing methods within this domain. Samples of these synthesized voices are available on our demo page."}, {"title": "6. Conclusion", "content": "This paper presents a novel framework for the NAM-to-speech conversion task utilizing self-supervision. Instead of relying on conventional methods to record studio-quality ground-truth speech, we employ self-supervision and speech-to-speech synthesis techniques to simulate ground-truth speech. The proposed framework, without the application of data augmentation, achieves a 29.08% reduction in the MCD compared to the current SOTA method. Moreover, our data augmentation method enhances synthesized speech intelligibility, leading to a 12.82% decrease in WER compared to using only samples from the existing corpus for training. Additionally, we demonstrate the model's capability to synthesize speech in novel voices. Our future aim is to enhance speech-to-speech synthesis architecture, focusing on improving the intelligibility of simulated ground-truth speech, vital for training Seq2Seq networks."}]}