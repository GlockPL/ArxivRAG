{"title": "STDCformer: A Transformer-Based Model with a Spatial-Temporal Causal De-Confounding Strategy for Crowd Flow Prediction", "authors": ["Silu He", "Peng Shen", "Pingzhen Xu", "Qinyao Luo", "Haifeng Li"], "abstract": "Crowd Flow Prediction is critical to urban management, with the goal of capturing the arrival and departure characteristics\nof crowd movements under different spatial and temporal distributions, which is fundamentally a spatial-temporal prediction task.\nExisting works typically treat spatial-temporal prediction as the task of learning a function $F$ to transform historical observations to\nfuture observations. We further decompose this cross-time transformation into three processes: (1) Encoding ($E$): learning the in-\ntrinsic representation of observations, (2) Cross-Time Mapping ($M$): transforming past representations into future representations,\nand (3) Decoding ($D$): reconstructing future observations from the future representations. From this perspective, spatial-temporal\nprediction can be viewed as learning $F = EMD$, which includes learning the space transformations {$E, D$} between the observa-\ntion space and the hidden representation space, as well as the spatial-temporal mapping $M$ from future states to past states within\nthe representation space. This leads to two key questions: Q1: What kind of representation space allows for mapping the past\nto the future? Q2: How to achieve mapping the past to the future within the representation space? To address Q1, we propose\na Spatial-Temporal Backdoor Adjustment strategy, which learns a Spatial-Temporal De-Confounded (STDC) representation space\nand estimates the de-confounding causal effect of historical data on future data. This causal relationship we captured serves as the\nfoundation for subsequent spatial-temporal mapping. To address Q2, we design a Spatial-Temporal Embedding (STE) that fuses\nthe information of temporal and spatial confounders, capturing the intrinsic spatial-temporal characteristics of the representations.\nAdditionally, we introduce a Cross-Time Attention mechanism, which queries the attention between the future and the past to guide\nspatial-temporal mapping. Finally, we integrate the process of learning the STDC representation space and the spatial-temporal\nmapping into an E-M-D skeleton for spatial-temporal prediction. The skeleton is further instantiated with a Transformer model,\nbuilding a Transformer model with Spatial-Temporal De-Confounding Strategy (STDCformer). Experiments on two real-world\ndatasets demonstrate that STDCformer achieves state-of-the-art predictive performance and exhibits stronger out-of-distribution\ngeneralization capabilities.", "sections": [{"title": "1. Introduction", "content": "Crowd flow prediction aims to use historical flow data in a region to predict the inflow and outflow of during\nfuture time periods. It is a typical spatial-temporal prediction task that plays a crucial role in urban planning, traffic\nmanagement, public safety, and other fields [1, 2]. The essence of spatial-temporal prediction lies in capturing the\nmapping $F_{past \\rightarrow future}$ from historical data to future data, enabling the inference of future from historical observations.\nIn recent years, a series of spatial-temporal prediction models have emerged with the goal of solving $F_{past \\rightarrow future}$, and\nthe deep learning-based models have become mainstream. Among these, models based on Spatial-Temporal Graph\nNeural Networks (STGNNs) and Spatial-Temporal Transformers (ST Transformers) have demonstrated exceptional\nperformance in various spatial-temporal prediction tasks, such as traffic flow prediction and crowd flow prediction\n[3, 4]. These two types of models follow a unified paradigm, where temporal representation modules (e.g., RNN[5, 6,\n7, 8, 9, 10, 11], CNN[12, 13, 14, 15, 16], and Transformer[17, 18, 19, 20]) and spatial representation modules (e.g.,\nCNNs [21], GNNs [5, 6, 7, 10, 12, 13, 15, 15, 16], and Transformers [8, 17, 20, 22]) are used to separately capture the\ntemporal and spatial representations from the observed data. A spatial-temporal fusion module then combines these\nrepresentations to obtain the final representation, which is used to infer unobserved spatial-temporal data.\n$F_{past \\rightarrow future}$ needs to be realized through three subprocesses. First, the encoder $E$ is used to encode historical\nobservations into representations in a latent space. Second, a cross-time mapping $M$ is applied to transform the repre-\nsentation of past states into the representation of future states. Third, the decoder $D$ projects the future representations\nback into the observation space to reconstruct the future observations. From this perspective, the objective of learn-\ning $F_{past \\rightarrow future}$ is to learn $F = E\\cdotM\\cdotD$, which includes the space transformation $E, D$ between the observation\nspace and the representation space, as well as the spatial-temporal mapping $M$ from future states to past states within\nthe representation space. The purpose of space transformation is to identify a representation space that can capture\nthe essential information in the observations, serving as the foundation for ensuring the feasibility of the subsequent\nspatial-temporal mapping. The purpose of spatial-temporal mapping is to learn the intrinsic, dynamic transformation\nrelationships between the past and future from their essential representations. These two processes lead to two key\nquestions behind spatial-temporal prediction: Q1: What kind of representation space allows for mapping the past\nto the future? Q2: How to achieve mapping the past to the future within the representation space? The answers\nto these questions and the corresponding methods are as follows.\nA1: The premise for mapping the past to the future is the accurate transmission of causal effects from the\npast to the future. Therefore, the mapping from the past to the future can be achieved in a Spatial-Temporal\nDe-Confounded (STDC) representation space.\nThe premise of inferring the future from the past is based on the assumption that there exists some form of \"in-\nfluence\" of the past on the future, which determines the mapping relationship between the past and future. Historical\ndata and future data are not simply related as input and output, but implicitly involve assumptions about the causal\nrelationship between the past and future. Therefore, fusing the representation of causality can capture the essential\ninformation within the observational data. Under this assumption, such \"influence\" can be characterized through\ncausal effects. Existing methods typically build neural network models to fit the associations $P(Future|Past)$ between\nhistorical and future data distributions from the observational data to learn the representation space. However, when\nuncontrolled confounding bias [23] is present, the model fails to capture the true causal effect $P(Future|do(Past))$.\nAs shown in Figure 1, the basic unit in spatial-temporal data is analogous to a token in text, which serves as the\ntrial unit for estimating $P(Future|do(Past))$, referred to as the ST token (STT), represented as $STT_{ij} : S = S_i, T = T_j$.\nIt can be observed that, similar to different participants with varying physical conditions in a drug trial, each STT\nalso has temporal and spatial characteristics. These characteristics reflect the attributes of the spatial region $S_i$ (e.g.,\nfunctionality, travel cost, travel safety, etc.) and the temporal window $T_j$ (e.g., travel necessity, travel suitability,\netc.), which are potential factors influencing human movement. These characteristics simultaneously affect both\nhistorical and future crowd flow observations, serving as confounders in the spatial-temporal prediction process. If\nthe distribution bias of these confounding factors in the sample data is ignored, the model will learn incorrect patterns.\nEstimating the correct causal effect requires eliminating the distributional bias of confounders, a process known\nas de-confounding. In this paper, we argue that the ideal spatial-temporal representation space should enable de-\nconfounding, i.e., learn a Spatial-Temporal De-Confounded (STDC) representation space. The challenge of spatial-\ntemporal observational data de-confounding lies in simulating the intervention operation $do(Past)$ on the observed\nsamples, with the key being sample stratification and confounder control. Sample stratification entails classifying\nsamples into treatment groups based on their differences, while confounder control then re-weight the samples in\neach stratum to estimate the true causal effect. However, due to the complexity of spatial-temporal activities and\nthe limited nature of observational data, confounders are often unobservable, and the distribution of such hidden\nconfounders in the observational samples is unknown, which makes it difficult to directly measure the differences\nbetween samples, consequently, challenging to accurately pre-define the number of sample strata and their weights. In\norder to obtain the representation of hidden confounders, existing methods often learn the representations confounders\nfrom the original observation time series [24, 25]. However, these abstract representations are hard to be aligned to the\nspecific semantics in the real world, and the separability between different strata heavily depends on the sampling and\nquality of the observation data. Some methods also introduce Point-of-Interest (POI) data to learn the classification\nof confounding representations [26], but they need to pre-define the number of categories k of confounding and their\ndistribution. The ideal hidden confounding stratification in crowd data should accurately indicate the key influence\nfactors behind crowd movements, and different confounding representations should be naturally distinguishable and\nassigned appropriate weights. To approximate this ideal hidden confounder stratification and allow for subsequent\ncontrol, this paper proposes a Spatial-Temporal Backdoor Adjustment strategy, specifically:\n(1) To improve the reliability and semantics of confounders' representations, we collected auxiliary information\nthat can separately characterize the temporal and spatial characteristics of STTs based on the analysis of human\nmovement behavior. This auxiliary information is fused into the prediction model to learn the representations of\nhidden confounders. Compared to one-dimensional time series data from uncontrollable sampling process, this type\nof information has more stable quality and clearer semantics.\n(2) To reduce the bias caused by assumptions on the distribution of confounding factors, and based on the two\nmost critical attributes of human mobility, \"When\", \"Where\u201d, we naturally categorize the confounders into temporal\nand spatial confounders. This approach avoids the need to predefine the number of clusters k while ensuring the\ncompleteness of the stratification. Additionally, both temporal and spatial confounders are fed into learnable modules\nto derive the weights, eliminating the need to assume a predefined distribution for the confounders.\nA2: The intrinsic relationship between the past and the future depends on the characteristics of their\nspatial-temporal contexts, which can be portrayed by spatial and temporal confounders. Therefore, the repre-\nsentations of confounders can be used for querying the relationships between the past and future.\nChanges in both time and spatial sampling values can lead to shifts in the spatial-temporal mapping relationships.\nAs shown in Figure 2, taking the crowd flow data from Manhattan Island, New York City, as an example, the flow data\nis divided into different samples for prediction ${(PastTimeID, FutureTimeID)_{ZoneID}, TimeID = {1, 2}, ZoneID = {43, 75, 79}}$,\nwith a time window size of 6 hours. For the same time sampling, the future trends of both samples ${(Past_1, Future_1)}_{43}$\nand ${(Past_1, Future_1)}_{75}$ are similar to their historical trends, showing an overall downward trend. In contrast, the fu-\nture trend of ${(Past_1, Future_1)}_{79}$ first follows the historical trend and then reverses. For the same region sample, the\nfuture trend of ${(Past_1, Future_1)}_{75}$ is opposite to its past trend, while ${(Past_1, Future_1)}_{75}$ shows the same future and past\ntrends.\nThis difference arises from the spatial-temporal characteristics of the observation samples, i.e., the different spatial-\ntemporal contexts, and spatial-temporal confounders can portray this information. For example, due to the different\nfunctionalities of Zone 79 compared to the other two zones, even at the same time, their ability to attract crowd flow\ndiffers, leading to different trends. While for Zone 75, the travel demand of people varies at different time windows,\nresulting in different trends in how the spatial region is visited across different time periods.\nTherefore, we propose a Cross-Time-Attention-based Past-to-Future Mapping mechanism. To better capture the\ninherent spatial-temporal characteristics of STT representations, we construct Spatial-Temporal Embeddings (STE)"}, {"title": "2. Related Works", "content": "2.1. Spatial-Temporal Graph Prediction\nCrowd Flow Prediction in Urban Areas is a typical spatial-temporal graph prediction task, and existing spatial-\ntemporal graph prediction methods can potentially be applied to crowd flow prediction. Therefore, this paper provides\na general review of spatial-temporal graph prediction methods. Spatial-temporal graph prediction aims to forecast the\nfuture states of nodes, edges, or graphs in data constructed as graphs. In such graphs, nodes typically represent spatial\nentities (e.g., regions), while edges represent relationships between spatial entities (e.g., adjacency, interaction). Typi-\ncal spatial-temporal graph prediction tasks include traffic flow prediction, crowd flow prediction, epidemic prediction,\nand air quality prediction. Unlike spatial-temporal data constructed as regular grids (e.g., remote sensing images), the\nspatial regions in graphs are mostly irregular units, and the relationships between them are often more complex. Un-\nlike purely time-series prediction or graph learning tasks, spatial-temporal graph prediction requires the simultaneous\nmodeling of temporal and spatial features, and thus involves modules for capturing both temporal dependencies and\nspatial dependencies.\nCurrently, mainstream spatial-temporal graph prediction frameworks can be divided into two categories: Spatial-\ntemporal Graph Neural Networks (STGNNs) [4, 27, 28] and Spatial-temporal Transformers (ST Transformers) [22,\n29]. As shown in Figure 4, STGNNs utilize time-series representation models and GNNs to learn temporal repre-\nsentation $R_T$, and spatial representation $R_S$, then fuse these two representations to obtain the final spatial-temporal\nrepresentation $R_{ST}$, for prediction. In another way, ST Transformers introduce temporal embeddings $E_T$, and spatial\nembeddings $E_S$ to preserve the positions and structural information of tokens in the original spatial-temporal sequence,\nand use temporal self-attention and spatial self-attention modules to derive temporal representations $SA_T$ and spatial\nrepresentations $SA_S$, respectively. These two representations are then fused to generate the final spatial-temporal\nrepresentation for prediction (as shown in Figure 3a).\n2.1.1. Spatial Representation\nThe main goal of the spatial representation module is to model the relationships between spatial regions and\nencode these relationships into representations. Based on the fundamental structure of spatial representation models,"}, {"title": "2.1.2. Temporal Representation", "content": "The primary goal of the temporal representation module is to model temporal features such as trends and pe-\nriodicity in time series data. According to the fundamental structure of temporal representation models, existing\nmethods can be categorized into RNN-based, CNN-based, and transformer-based spatial-temporal prediction mod-\nels. RNN-based models primarily utilize Recurrent Neural Networks (RNNs)[32, 33] and their variants[34] to extract\ntemporal features. CNN-based models extract temporal features by applying convolutions over the time dimension.\nTransformer-based temporal representation models, on the other hand, use self-attention mechanisms to capture de-\npendencies across different time steps.\nRNN-based models. Models such as ASTGCRN[5], T-GCN [10], and KST-GCN[7] all employ Gated Recur-\nrent Units (GRU). DCRNN [6] utilizes GRU to construct a Diffusion Convolutional Gated Recurrent Unit for ex-\ntracting temporal features. ST-MetaNet [11] employs a GRU as the first layer in a Seq2Seq framework. Traffic\nTransformer[8]uses a simple Long Short-Term Memory (LSTM) to obtain temporal embeddings for extracting tem-\nporal features. MASTGN [9]uses LSTM to process features after transformations based on spatial and temporal\nattention.\nCNN-based models. STGCN[16] utilizes 1-D causal convolution and Gated Linear Units (GLU) to construct\ntemporal convolutional layers. Graph Wavenet[13] uses Gated Temporal Causal Convolution to expand the tempo-\nral receptive field. STFGNN[15] proposes a new gated CNN module for extracting temporal features. ASTGCN\n[12] combines temporal attention with standard convolution to extract temporal features. HGCN [14] replaces the\noriginal gated temporal convolution with dilation convolution whose dilation factor is 2 to construct temporal gated\nconvolution.\nTransformer-based models. ASTTGN[20] employs an adaptive temporal transformer module to capture long-\nrange temporal dependencies. STTN[17] construct a temporal transformer to capture bidirectional, long-range tem-\nporal dependencies. LSTTN [19] trains a transformer-based encoder to represent temporal features of subsequences\nthrough a self-supervised task of mask reconstruction."}, {"title": "2.1.3. Spatial-Temporal Fusion", "content": "Existing methods for spatial-temporal fusion can be divided into two categories: direct fusion and adaptive fusion.\nThe former typically connects the temporal and spatial representation modules sequentially, interleaves them in a\nblock or simply adding the two types of representations. In contrast, the latter involves inputting both representations\ninto a learnable module for fusion."}, {"title": "2.2. Spatial-Temporal De-confounding Representation Learning", "content": "Spatial-temporal de-confounding representation learning involves modeling causal variables in spatial-temporal\ndata, and estimating the causal effect of the treatment variable on the outcome variable by removing the influence of\nconfounding variables (confounders). In different data and task scenarios, the confounders within the system vary, and\nthe de-confounding methods differ accordingly. Existing research can further be categorized into methods based on\nstructural causal model (SCM) and potential outcome framework. The underlying ideas of the two types of approaches\nare unified, differing only in the causal language used."}, {"title": "2.2.1. Structural Causal Model based Method", "content": "Methods based on SCM can directly implement interventions on causal graphs, transforming causal problems into\nstatistical language that can represent the data through front-door, backdoor adjustments and other methods. Backdoor\nadjustment realizes de-confounding by stratifying and balancing confounding factors, being adopted by many studies.\nSome works categorize confounding factors into pre-defined layers. For instance, STCTN [26] considers regional\nattributes in the region network as confounders, which may cause existing spatial-temporal prediction models to\nabsorb spurious correlations between the historical and future data. To address this, spectral clustering is used to\npartition regional attributes, and an unbiased prediction model is constructed in an independent parameter space for\neach partitions to achieve de-confounded predictions. SEAD[38] aims to eliminate the confounding effects of social\nenvironments on pedestrian trajectories. This work assumes that the confounders follow a uniform distribution, and\nthe joint distribution of confounders and causal variables is learned through a cross-attention module. CISTGNN\n[39] learns representations of confounders from external weather environments and applies backdoor criteria for de-\nconfounding. STEVE[25] focuses on the finiteness of the number of confounders and the completeness of their\ncategories, dividing confounders into invariant and variant layers. CaST [24] discretizes the confounders through\na temporal environment codebook, so that the representations of confounder fall into the most appropriate layers.\nCTSGI[40] leverages images at pedestrian trajectory points as the source of confounder representations, extracting\nenvironmental representations through a semantic segmentation model, and then adjusting confounding effects using\nbackdoor criteria. Front-door adjustment, on the other hand, applies to causal graphs where no back-door paths exist,\nproviding a de-confounding strategy different from the backdoor criterion. STNSCM [41] uses GLUs to obtain the\ndistribution of causal variables after intervention from historical observational data and external environment data. In\nthis distribution, causal variables can combine with different confounders obeying an unbiased probability, allowing\nfor de-confounded effect estimation. A similar strategy is used in vehicle trajectory prediction, where the contextual\nscene is treated as a confounder and eliminated through a counterfactual representation inference module based on\nfront-door adjustment strategies [42]. CASPER [43] uses front-door adjustment to transform time-series completion\ntasks into summing over subcategories of input data representations, and calculates de-confounded causal effects\nthrough learnable prompt vectors."}, {"title": "2.2.2. Potential Outcome Framework based Method", "content": "The main idea of methods based on the potential outcome framework is to treat different observational units\nas samples and simulate interventions by processing these samples comprehensively, thus obtaining de-confounded\ncausal effects. Sample re-weighting is one of the classical methods for achieving sample balance [44], which is used\nto assign different cities and regions to different treatment groups and calculate the causal effects corresponding to\neach treatment. For example, it has been used in studies to assess the impact of changes in POI on regional pedes-\ntrian flow[45]. With the development of representation learning, balancing confounders based on the obtained unified\nrepresentations of the samples through representation learning can also ensure that the background attributes distribu-\ntion across different experimental groups are similar. For instance, CAPE[46] generates representations for different\nlocations from historical event sequences and measures the differences in the representations using the Integral Prob-\nability Metric (IPM), minimizing the overall differences between samples in different experimental groups to achieve\nrepresentation balance. SINet[47] uses the Hilbert-Schmidt Independence Criterion (HSIC) as a regularizer to guide\nrepresentation balance and remove the effect of hidden confounders. CIDER[48] applies the Wasserstein distance to\nbalance representations across different administrative regions.\nIn the spatial-temporal prediction module, the proposed method constructs temporal and spatial representations\nbased on transformer models, and employs adaptive spatial-temporal fusion. In the spatial-temporal de-confounding\nmodule, the de-confounding strategy proposed in this paper is based on SCM, utilizing the backdoor criterion for\nde-confounding. The distinctions between this method and the aforementioned related studies are as follows: 1) The\nstrategy employed in this paper innovatively categorizes confounders into two abstract types: temporal confounder and\nspatial confounder. Furthermore, the probability distributions of the confounders are learned adaptively, which avoids\nthe pre-setting of the number of categories and the distribution of confounders; 2) The constructed spatial-temporal\nde-confounded representations can simultaneously participate in both spatial-temporal representation learning and\nspatial-temporal fusion, maximizing information sharing between the two modules."}, {"title": "3. Methodology", "content": "3.1. Hypothesis on Spatial-Temporal Prediction\nSpatial-temporal prediction aims to infer the future state for $T_f$ steps based on the known previous observa-\ntions for $T_P$ steps. Taking the prediction of crowd flow as an example, the spatial-temporal prediction task is typ-\nically formalized as $X_{n \\times f} \\rightarrow Y_{n \\times f}$, where $X_{n \\times f} = [(Inflow_{1-T_P+1}, Out flow_{1-T_P+1}), ..., (In flow_1, Out flow_1)]$, $Y_{n \\times f} =$\n$[(Inflow_{t+1}, Out flow_{t+1}), ..., (Inflow_{t+T_f}, Outflow_{t+T_f})]$, $n$ represents the number of spatial regions to be predicted,\nand $f$ represents the number of features to be predicted. For the case of predicting the inflow and outflow, $f = 2$.\nAs shown in Figure 5a, existing methods typically use spatial-temporal prediction model to approximate $F(X \\rightarrow Y)$,\nwhere the underlying assumption is that there exists some learnable pattern between past and future observations.\nHowever, in this paper, we argue that the nature of this pattern is to describe the transformation relationship between\nthe past and future in different spaces, rather than simply viewing it as a mapping between input and output values.\nTherefore, as shown in Figure 5b, we further decompose the spatial-temporal prediction task into space transforma-\ntions $E, D$ between the observation space and the hidden representation space, and the spatial-temporal mapping $M$\nfrom future states to past states within the representation space. The past inflow and outflow values (IO) are first\nmapped into a high-dimensional hidden space through a representation model $E(\\cdot)$. Then, in the high-dimensional\nlatent space, they undergo a spatial-temporal transformation $M(\\cdot)$ to obtain the future IO vector, which is finally\ntransformed back to the observable IO values through another representation model $D(.)$. Therefore, the crowd flow\nprediction problem can be formalized as Eq. 1:\n$IO_{t+1}, ..., IO_{t+T_f} = F(IO_{t-T_P+1}, ..., IO_t), F = E \\cdot M \\cdot D$ (1)\nBased on this decomposition, the spatial-temporal prediction model must address two key questions. Q1: What\nkind of representation space allows for mapping the past to the future? Q2: How to achieve mapping the past\nto the future within the representation space? For Q1, we argue that the predictability of spatial-temporal data\nis ensured by the transmitting the influence of the past on the future, and this influence can be measured through\ncausal effects. Therefore, based on causal assumptions, we propose a spatial-temporal backdoor adjustment strategy"}, {"title": "3.1.1. A Spatial-Temporal De-Confounded (STDC) Representation Space", "content": "The foundation of the ability to infer future states from historical states for spatial-temporal prediction models is\nthat the past has some form of \"influence\" on the future. This influence determines the mapping relationship between\nhistorical input data and future output data, which is the basis of the predictability of spatial-temporal data. Causal\ninference can measure the influence of the past on the future using causal effects, aiming to build causal relation-\nships between variables rather than merely correlations, enabling a more accurate and stable representation of the\nvariables[49]. The underlying assumption is that causality is a more reliable form of association, where information\nflows along the causal links, transmitting the causal effect of the cause on the outcome. Based on this assumption,\nthe spatial-temporal prediction process can be viewed as the process of estimating the causal effect of historical data\nrepresentations on future data representations, inferring the outcome from the values of causes. However, the hidden\nconfounding bias makes it impracticable to estimate the true causal effect directly from the observational data. This\nsection takes the basic unit of spatial-temporal prediction, STT, as a starting point, to describe the causality underlying\ncrowd flow prediction. We also propose a spatial-temporal backdoor adjustment strategy to learn a de-confounded\nrepresentation space.\nSpatial-Temporal Token (STT). Based on the form of input data, the basic unit in spatial-temporal data is the\nobservation on a spatial entity $S_i$ at a specific time step $T_j$. In this paper, this smallest unit in spatial-temporal data is\nanalogized to a token in text, referred to as the Spatial-Temporal Token (STT), represented as: $STT_{ij} = (S = S_i \\in$\n$\\mathbb{R}^{1 \\times s}, T = T_j \\in \\mathbb{R}^{1 \\times t}, V = IO_{ij} \\in \\mathbb{R}^{1 \\times f})$. where $V$ is the variable to be predicted, and $S$ and $T$ describe its spatial and\ntemporal characteristics, respectively. These can be considered as background attributes of STTs, and they influence\nthe estimation of the causal effect $V_{past} \\rightarrow V_{future}$.\nCausal Graph underlying Spatial-Temporal Prediction. The prediction model infers future data representa-\ntions from historical data representations, where the historical representation serves as the cause for the future rep-\nresentation. However, for each STT, the background attributes influence both the past and future IO representations,\nmaking $S$ and $T$ serve as confounder in the prediction process of $V$. The causal graph underlying spatial-temporal\nprediction can be illustrated as Fig. 6, which suggests that the causal effect from $V_{past} \\rightarrow V_{future}$ may vary under\ndifferent values of confounders. This is similar to how the effect of a drug may differ for patients of different ages in\na medical trial. To estimate an accurate and fair causal effect, it is necessary to configure the trial groups in such a\nway that the distribution of confounders across all groups becomes uniform, i.e., to remove the confounding bias. The\nintervention operation on the causal graph provides an intuitive representation of de-confounding, where the arrow\n$C \\rightarrow X$ is removed to make the distribution of $C$ independent of $X$. The causal effect after intervention is expressed\nas $P(Y|do(X))$.\nSpatial-Temporal Backdoor Adjustment. Backdoor adjustment modifies the weights of observational samples\nwith different values of confounders based on their distribution, simulating the balanced sample distribution under an\nintervention experiment, and is an effective way to de-confounding in observational data. It can be applied when the\nconfounder satisfies the backdoor criterion for $X \\rightarrow Y$. After backdoor adjustment, the causal effect of $X \\rightarrow Y$ is\nexpressed as: $P(Y|do(X)) = \\sum P(Y|X,C = c_k) \\cdot P(C = c_k)$. where the confounder is discretized into k categories.\nHowever, in spatial-temporal prediction, confounders are often hidden, making it difficult to predefine the value\nof k and the distribution of each category. To address this issue, we consider the fundamental decision factors of\nhuman movement in the real world-namely, \"where\" and \"when\u201d-to construct the minimal descriptive set of human\nmovement: \"when\", \"where\u201d. We then categorize the confounders into two abstract types: temporal confounders $C_T$\nand spatial confounders $C_S$. Furthermore, since STTs naturally contain the background variables $S^{1 \\times s}$ and $T^{1 \\times t}$, which\ncan serve as information sources for $C_S$ and $C_T$, respectively. Based on this partition, we propose a spatial-temporal\nbackdoor adjustment method as our de-confounding strategy, which is expressed as Eq. 2:\n$P(Y|do(X)) = P(Y|X, C = C_S) \\cdot P(C = C_S) + P(Y|X, C = C_T) \\cdot P(C = C_T)$ (2)"}, {"title": "3.1.2. A Cross-Time-Attention (CTA)-based Past-to-future Mapping", "content": "The relationship between past and future states is very abstract, if using a function to approximate this kind of\nrelationship, the parameters of this mapping function will have different value in different temporal periods. Past\nand future can be viewed as a pair of Cross-Time spatial-temporal entity. Capturing the transformation relationship\nbetween these entities requires addressing the following two issues:\n(1) Representation of spatial-temporal Entities' Intrinsic Characteristics. Based on the observations in Fig.\n2, we argue that spatial confounder and temporal confounder can jointly characterize the contextual differences be-\ntween different STTs, which represent the intrinsic characteristics of tokens. Therefore, fusing the representations\nof temporal confounders with spatial confounder effectively captures the spatial-temporal characteristics of different\nspatial-temporal entities. For each STT that constitutes the Past and Future, we integrate their temporal and spatial\nconfounder's representations to obtain a Spatial-Temporal Embedding (STE). This embedding retains the temporal\nand spatial characteristics of each STT within the entire dataset, thereby representing the contextual features of differ-\nent spatial-temporal entities.\n(2) Querying Relationships Between Cross-Time Entities. The relationships between Cross-Time entities are\ndiverse and dynamically evolving. Therefore, attention mechanism can be effective for capturing their similarities.\nSince this type of querying spans across time, we refer it to Cross-Time Attention (CTA). We use the STEs of Past\nand Future as the Key and Query, respectively, and employ the attention mechanism to perform the query. The\nspatial-temporal mapping is then implemented based on the results of queried attention."}, {"title": "3.2. Overview", "content": "Based on the assumptions in Section 3.1, we combine the physical institutions of space transformation and spatial-\ntemporal mapping in the spatial-temporal prediction process. We implement the representation models $E(\\cdot), D(\\cdot)$ and"}, {"title": "The STDCformer consists of the following three components, as depicted in Fig. 8:", "content": "1. STDC Encoder: Projects historical data into a Spatial-Temporal De-Confounded Representation Space.\nIt includes a Data Embedding Module that integrates the information of the input STTs, and a stack of Spatial-\nTemporal De-Confounded Attention Blocks to implement temporal representation, spatial representation, and\nspatial-temporal fusion.\n2. CTA-based Past-to-Future Mapping: Explicitly captures the relationship between the past and future,\nand performs the mapping. It includes Cross-Time Attention Blocks to query the attention between the STEs\nof future and past, and further projects the past representation into the future representation.\n3. STDC Decoder: Reconstructs future observation. It includes a stack of Spatial-Temporal De-Confounded\nAttention Blocks to implement temporal representation, spatial representation, and spatial-temporal fusion; and\na Prediction Head that maps"}, {"title": "3.3. STDC Encoder", "content": "3.3.1. Data Embedding\nFor a $STT_{ij"}, "S = S_i \\in \\mathbb{R}^{1 \\times s}, T = T_j \\in \\mathbb{R}^{1 \\times t}, V = IO_{ij} \\in \\mathbb{R}^{1 \\times f})$, the data embedding layer maps it into a repre-\nsentation vector $\\overrightarrow{STR_{ij}} \\in \\mathbb{R}^{1 \\times 2d}$, which is used to effectively integrate both the spatial-temporal characteristics and\nobservational information of the STTs. The representation of the STT is constructed using the following two types of\ninformation:\n1. Observational Value. This mainly contains the observational feature of the STT. Therefore, temporal rep-\nresentation is used to preserve this part of the information. The flow observation value $V^{1 \\times f}$ is mapped into\nd-dimensional space by a temporal encoder, resulting in $V' \\in \\mathbb{R}^{1 \\times d}$ as the value representation.\n2. Spatial-Temporal Characteristics. This mainly contains the spatial-temporal location and structural infor-\nmation of the STT. Since the temporal and spatial confounders representation can portray the temporal and\nspatial properties of the STT, a spatial-temporal embedding fusing the information of two types of confounders\ncan preserve this part of information. A confounder encoder is used to map $T_i \\in \\mathbb{R}^{1 \\times t}$ and $S_i \\in \\mathbb{R}^{1 \\times s}$ into a\nd-dimensional space, yielding $C_T \\in \\mathbb{R}^{1 \\times d}$ and $C_S \\in \\mathbb{R}^{1 \\times d}$, respectively. $S_i$ is first concatenated with its repre-\nsentation $Laplacian_i \\in"]}