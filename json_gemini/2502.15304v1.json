{"title": "SVDq: 1.25-bit and 410\u00d7 Key Cache Compression for\nLLM Attention", "authors": ["Yankun Hong", "Xing Li", "Hui-Ling Zhen", "Xianzhi Yu", "Wulong Liu", "Mingxuan Yuan"], "abstract": "For the efficient inference of Large Language Models (LLMs), the effective com-\npression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into \u201clatent channels\" using SVD basis representations. Since\nthe values in latent channels decay rapidly and become negligible after only a few\nlatent channels, our method then incorporates importance-aware quantization and\ncompression for latent channels. This enables the effective allocation of higher\nprecision to more significant channels. Theoretically, we prove that SVDq results\nin quantization errors (\u00d70.1 or even lower) that are much lower than those of\nper-channel key quantization in the original space. Our findings based on RULER\nand LongBench benchmarks demonstrate that SVDq can achieve an equivalent\nkey cache precision as low as 1.25-bit. When combined with key sparsity, it can\nreach a key compression ratio of up to 410\u00d7 for attention computation, all while\nmaintaining comparable model performance. Notably, our method is nearly lossless\nfor LongBench datasets. This indicates that SVDq enables high-precision low-bit\nquantization, providing a more efficient solution for KV cache compression in\nLLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have started a new era of artificial intelligence by demonstrating\nremarkable capabilities in handling complex tasks (OpenAI et al., 2024; Grattafiori et al., 2024;\nQwen et al., 2025; DeepSeek-AI et al., 2025). Most of these recently developed LLMs are founded\nupon the attention mechanism based auto-regressive decoder transformers (Vaswani et al., 2023).\nConsequently, they need to encode past information into intermediate hidden tensors, specifically\nKV caches, for subsequent and efficient inference.\nHowever, in natural language tasks with large batches or long contexts, KV cache often expands\nsignificantly in size, posing a significant challenge to fast inference (Pope et al., 2022; Liu et al.,\n2023). The substantial memory consumption and latency required to save and load KV cache,\ncoupled with the computational demands of attention operations, become critical bottlenecks for\nLLM inference. Considering the rapid advancement of computability and the increasing demand for\nefficient LLM inference, we recognize the importance of high-ratio KV cache compression (even\nwith a slight concession in computational overhead), enabling the inference of LLMs on devices with\nlimited memory.\nExisting approaches to KV cache compression can be categorized into three main directions:\nsequence-axis compression, channel-axis compression, and digit-type compression. (i) Sequence-axis"}, {"title": "2 Related Works", "content": "Sparsity: With different feature extraction based attention estimation algorithms, methods such as\nFastgen (Ge et al., 2023), H2O (Zhang et al., 2024c), Quest (Tang et al., 2024), SparQ (Ribar et al.,\n2024), PQCache (Zhang et al., 2024a), ShadowKV (Sun et al., 2024), and AttentionPredictor (Yang\net al., 2025) selectively retain only the most important tokens in the sequence and effectively prune\nthe others. Loki Singhania et al. (2024) is another sparsity method that uses the SVD approximation\nto accelerate attention estimation for critical tokens selection.\nChannel Compression: These methods, such as ThinK (Xu et al., 2024), reduce the dimensionality\nof KV cache by truncating channels or employing low-rank approximations. Prominent examples\ninclude SVD-based approaches like SVD-LLM (Wang et al., 2024b), LoRC (Zhang et al., 2024b),\nPalu (Chang et al., 2024), and Eigen Attention Saxena et al. (2024). Notably, techniques like Grouped\nQuery Attention (GQA) Ainslie et al. (2023), Multi-head Latent Attention (MLA) (DeepSeek-AI\net al., 2025), and transformations from Multi-Head Attention to GQA (Jin et al., 2024; Chen et al.,\n2024) can also be viewed as forms of channel compression, as they effectively reduce the number of\nattention dimensions.\nQuantization: Methods like KIVI (Liu et al., 2023), KVQuant Hooper et al. (2024), AlignedKV\n(Tan et al., 2024), BitStack Wang et al. (2024a), and KVTuner (Li et al., 2025) reduce the memory\nfootprint with low precision KV cache. QServe Lin et al. (2024) introduces several quantization\nand system co-design methods to achieve efficient W4A8KV4, where SmoothAttention is utilized to\nmigrate the key quantization difficulty to query.\nSome works explore the combination of these approaches. In addition to the mentioned ShadowKV\n(Sun et al., 2024) and ThinK Xu et al. (2024), Liu et al. (2024b) integrates quantization with matrix\ndecomposition to apply different quantization precision for the two decomposed matrices, and\nPalu Chang et al. (2024) applies per token quantization to the latent vector of the SVD low-rank\napproximation.\nImportantly, the concept of using SVD for mixed-precision quantization has been explored in other\ncontexts. For instance, Delta-CoMe (Ping et al., 2024) applies this principle to compress LLM\nweights, while SVDQuant (Li et al., 2024a) utilizes it for compressing diffusion models. The novelty\nof this work over the mentioned works lies not only in the application of this principle to K cache\ncompression but also in the theoretical foundation upon which we derive the principle and method,\nand the error analysis we provide."}, {"title": "3 SVD and Quantization", "content": "Singular Value Decomposition: Let K \u2208 $\\mathbb{R}^{s\u00d7d}$ denotes the K cache matrix for a given head in\na transformer layer, where s and d represent the sequence length and hidden embedding (channel)\ndimension, respectively, with s > d typically holding for long context applications. Let K be\ncentered by subtracting its per-channel mean K \u2208 $\\mathbb{R}^{d}$, i.e., K \u2190 K \u2013 K and maintain the same\nnotation.\nAssuming K is full-rank. Its SVD is given by\nK = U. D. $\\text{V}^{H}$ (1)\nwhere U \u2208 $\\mathbb{R}^{s\u00d7d}$ has orthonormal columns, V \u2208 $\\mathbb{R}^{d\u00d7d}$ is orthonormal, satisfying $\\text{U}^{H}$ . U = $\\text{I}_{d}$ and\n$\\text{V}^{H}$ . V = $\\text{I}_{d}$, and D \u2208 $\\mathbb{R}^{d\u00d7d}$ is a diagonal matrix containing the singular values in its diagonal with\nelements arranged in descending order, given by D = Diag([$\\lambda_{1}$, ..., $\\lambda_{d}$]).\nQuantization Let $\\text{k}_{min}$ := (min $\\text{K}_{:,1}$, ..., min $\\text{K}_{:,d}$), i.e., the column-wise minimum vector, and\nanalogously define $\\text{k}_{max}$. The per-channel asymmetrical b-bit quantization and dequantization\noperations are given by:\n$Q_{b}(K) := \\bigg[\\frac{K - k_{min}}{(k_{max} - k_{min})/(2^{b} \u2013 1)} \\bigg] $ (2)\n$D_{b}(K_{b}) := Q_{b}(K) \u00d7 \\frac{k_{max} - k_{min}}{2^{b}-1} + k_{min}$, (3)\nwhere [] denote the rounding operator. Naturally, $D_{b} \\circ Q_{b}(K) \u2248 K$."}, {"title": "4 Methods", "content": "Although the theory of the proposed SVD-quantization method, discussed in the previous section,\nis expected to be applicable to a much wider range of applications, this work focuses on KV cache\ncompression in the long context inference scenario. For long context LLMs, KV cache generated in\nthe prefilling stage generally dominates the memory usage. Our method is proposed to address this\nchallenge."}, {"title": "4.1 SVD Quantization", "content": "Consider the rows of $\\text{V}^{H}$ in Equation (1) as a basis for the row space of K. For the projection\n$\\text{P}_{\\text{V}_{:,j}}$ of the rows of K into the j-th basis vector, defined by $\\text{P}_{\\text{V}_{:,j}}(K) := K \u2022 V:j, following the\nEckart-Young-Mirsky theorem (Mirsky, 1960), we have:\nTheorem 4.1. For the K cache matrix K, the variance of its projection satisfies\nVar($\\text{P}_{\\text{V}_{:,j}}(K)$) = $\\lambda_{j}^{2}$. (4)\nCorollary 4.1.1. Let k \u2208 $\\mathbb{R}^{d}$ be a K cache vector with K subtracted, i.e., k \u2190 k \u2013 K. For any\nindices 0 < i < j < d, the squared expectations of its projections satisfy:\nE(($\\text{P}_{\\text{V}_{:,i}}(k))^{2}$) \u2265 E(($\\text{P}_{\\text{V}_{:,j}}(k))^{2}$). (5)\nProof. For any 0 < j < d, the projection of K is given by\n$\\text{P}_{\\text{V}_{:,j}}(K) = K \u2022 V:j = U \u2022 D \u2022 $\\text{V}^{H} . V:j = \\lambda_{j}U:j$.\nSince E($\\text{P}_{\\text{V}_{:,j}}(K)$) = $\\text{P}_{\\text{V}_{:,j}}(E(K)$) = 0, we have\nVar($\\text{P}_{\\text{V}_{:,j}}(K)$) = Var($\\lambda_{j}U_{:j}$) = $\\lambda_{j}^{2}$Var($\\text{U}^{H}_{:j} . U_{:j}$) = $\\lambda_{j}^{2}$.\nThis proves Theorem 4.1. Corollary 4.1.1 follows directly from Theorem 4.1 when the given vector k\nfollows the distribution of the rows of K."}, {"title": "4.2 Algorithm", "content": "In our SVDq method, we first apply SVD to the prefilling K cache, obtaining the projection operator\n$\\text{P}_{\\text{V}}(.)$ using the right SVD matrix V. Next, we determine a precision schedule for the quantization on\neach latent channel based on the singular values [$\\lambda_{1}$, ..., $\\lambda_{d}$]. Specifically, a latent channel associated\nwith a large singular value \u03bb is assigned a high quantization bit width b, and channels with small \u03bb\nare assigned low b or even be truncated with notation b = 0. This yields a schedule vector b, and the\nequivalent mixed bit width of this quantization schedule for the K cache is given by\n$\\bar{b} = \\frac{1}{d} \\sum_{i=1}^{d} b_{i}$. (6)\nSequently, we use Qb in (2) to quantize $\\text{P}_{\\text{V}}(K)$. The low-bit quantized $\\text{P}_{\\text{V}}(K)$ is then saved as the\ncache. In the decoding process, we dequantize the cache, reconstruct K in its original representation\nusing K \u2248 $\\text{P}_{\\text{V}}(K) \u00b7 $\\text{V}^{H}$, and then proceed with the attention computation. We summarize the\nalgorithm using pseudo-code in Algorithm 1 and an abstracted diagram in Figure 2."}, {"title": "4.3 Theoretical Error Analysis", "content": "We begin by presenting a lemma for later analysis.\nLemma 4.1. If data X are distributed uniformly within their value range r, then the expectation\nof the square absolute error, \u025b, of an asymmetrical b-bit quantization applied to X is equal to the\nvariance of a uniform distribution with a range of $\\frac{r}{2^{b}}$, that is\n$\\mathbb{E}(\\epsilon^{2}) = \\frac{1}{12}(\\frac{r}{2^{b}})^{2}$.\nLet K be centered by subtracting the key\u2019s per-channel mean K \u2208 $\\mathbb{R}^{d}$, and let $\\text{P}_{\\text{V}}(K)$ be its latent\nchannel representation. The Frobenius norm is invariant under this transformation, as\n||$\\text{P}_{\\text{V}}(K)||^{2} = \\sum_{i=1}^{s} |\\text{P}_{\\text{V}}(K_{i,:})|^{2} = \\sum_{i=1}^{s} ||K_{i,:}||^{2} = ||K||^{2}$.\nLet [$\\sigma_{1}^{2}$, ..., $\\sigma_{d}^{2}$] and [$\\lambda_{1}^{2}$, ..., $\\lambda_{d}^{2}$] denote the variance vectors of the channels for the original and latent\nchannel representations of K, respectively. Thus,\n$\\sum_{j=1}^{d} \\sigma_{j}^{2} = ||K||^{2} = ||\\text{P}_{\\text{V}}(K)||^{2} = \\sum_{j=1}^{d} \\lambda_{j}^{2}$\nWe further assume that the key cache distributions in each original channel and latent channel follow\nuniform distributions. Then, according to Lemma 4.1, the value ranges of the j-th original channel\nand j-th latent channel are $r_{j} = 2\\sqrt{3}\\sigma_{j}$ and $r_{j} = 2\\sqrt{3}\\lambda_{j}$, respectively.\nError analysis for direct quantization Figure 1a shows that the variances in the original channels\noften exhibit similar orders of magnitude. We therefore assume that they are approximately identical,\nwith $\\sigma^{2} \u2248 \\frac{||K||}{d}$ and $r^{2} \u2248 \\frac{12||K||}{d}$. Applying a per-channel, direct b-bit quantization to K, and\nfollowing Lemma 4.1 and the above analysis, results in a quantization error $\\epsilon_{K}$ with the expected\nvalue:\n$\\mathbb{E}(\\epsilon_{K}^{2}) = \\frac{1}{d} \\sum_{j=1}^{d} \\frac{1}{12}(\\frac{r_{j}}{2^{b}})^{2} = \\frac{1}{12*2^{2b} d} \\sum_{j=1}^{d} 4*3 \\sigma^{2} = \\frac{||K||}{2^{2b} d}$. (8)\nError analysis for SVDq The singular values of a matrix often exhibit exponential decay. We model\nthe variance vector for K\u2019s latent channel representation as\n$\\lambda_{j}^{2} = c e^{-\\rho j} = \\lambda_{1}^{2} e^{-\\rho (j-1)}$, (9)"}, {"title": "5 Experiments", "content": "In this section, we apply our method in different model settings to showcase its efficiency in K cache\ncompression.\nWe focus on long context applications using four large language models: Llama-3.1-8B-Instruct\n(Grattafiori et al., 2024), Qwen2.5-14B-Instruct, Qwen2.5-7B-Instruct, and Qwen2.5-3B-Instruct\n(Qwen et al., 2025). The numerical experiments are based on the RULER benchmarks (Hsieh et al.,\n2024) and LongBench benchmarks Bai et al. (2023). We omit the scores for RULER NIAH Single\ntests because in our tests, almost all methods achieved perfect scores (100) on these tests, indicating"}, {"title": "5.1 Results of SVDq", "content": "In our first experiment, we implement the SVD quantization method directly in K cache compression\nand summarize the results in Table 2. Detailed experiment settings and descriptions are provided in\nthe Appendix A.1.\nThe results demonstrate that the proposed SVDq method generally results in lower performance\ndegradation compared to direct quantization and channel compression across almost all tests. On av-\nerage, the SVDq method achieves higher scores despite having a lower equivalent mixed quantization\nbit width. This clearly showcases the significant advantage of truncating and quantizing the SVD\nl atent channels over operating directly on the original channels.\nPlease note that in our tests, both direct 2-bit quantization of the original K and equivalent 2-bit ThinK\nthat retains original channels and combines 4-bit quantization result in much more significant\nperformance degradation. Therefore, we opted to compare our SVDq method in 2- and 3-bit setting\nwith direct 3-bit quantization and equivalent 3-bit ThinK for a more meaningful evaluation."}, {"title": "5.2 Results of SVDq with Sparsity", "content": "Although SVDq can improve model performance while using small bit quantizations, significant\nperformance loss can still occur when the bit width is extremely low, such as $\\bar{b}$ = 2. Hence, we\ncombine our SVDq method with a sparsity technique to investigate its compatibility with other\ntechniques and explore potential performance improvements.\nWe adopt the sparsity strategy proposed in the ShadowKV method (Sun et al., 2024). Table 3 presents\nthe results for sparsity (\"ShadowKV SparsityOnly\") and ShadowKV (\"ShadowKV\") as baselines.\nPlease see a brief introduction of the ShadowKV and the description of these baseline settings in\nthe Appendix A.2. For the SVDq method, we investigate different quantization bit schedules with\nvarying equivalent mixed bit widths: $\\bar{b}$ = 2.25, 1.75, and 1.25. Detailed schedules are provided in\nTable 5 in the Appendix A.2. We apply the SVDq in conjunction with the sparsity method from\nShadowKV. The scores are presented in the yellow rows in Table 3.\nOur observations reveal that, when combined with sparsity, our SVDq compression method does\nnot result in significant performance degradation, even with extremely low quantization bit widths\nsuch as $\\bar{b}$ = 1.25. Decreasing the bit width from $\\bar{b}$ = 2.25 to $\\bar{b}$ = 1.75 has a negligible impact on the\nscore. Further decreasing $\\bar{b}$ to 1.25 results in a slight performance loss, although it remains relatively\ninsignificant. Notably, our quantization method, even with $\\bar{b}$ = 1.25, outperforms the low-rank\napproximation used in ShadowKV, demonstrating the ineffectiveness of directly truncating SVD\nranks. Taking into account the sparsity compression ratio of 32\u00d7, SVDq contributes an additional\nratio of up to 12.8\u00d7, resulting in a total compression ratio of 400x.\nNotably, by comparing Tables 2 and 3, the introduction of sparsity does not result in performance\ndegradation; it can even improve the performance of models that solely use SVDq or low-rank\ncompression. We observe that with sparsity, the model can withstand higher compression ratios. This\nmay be attributed to the fact that quantization and low-rank approximation introduce errors across all\ntokens, potentially leading to significant error accumulation in the full attention mechanism. However,\nsparsity discards unimportant tokens, which can help to mitigate the error from these tokens and\nimprove overall performance."}, {"title": "5.3 Results of SVDq with Sparsity and V Quantization", "content": "In the final experiment, we repeat the second experiment while additionally introducing a quantization\nmethod to the V cache to further reduce the required memory for model loading. Please find the\nexperiment settings in Appendix A.3. The results are presented in the green rows in Table 3.\nOur observations indicate a very small performance loss compared to the yellow rows (without V\ncache quantization) in Table 3. This suggests that, despite being an approximation method with a very\nlow compression rate, SVDq does not significantly degrade model performance even when combined\nwith sparsity and V cache compression.\nThe resulting insignificant performance degeneration while combining sparsity and V cache quantiza-\ntion not only demonstrate the effectiveness of the SVD quantization method in K cache compression\nbut also highlight its compatibility with existing compression techniques."}, {"title": "5.4 Results of LongBench benchmark", "content": "We also implement numerical experiments based on the LongBench benchmark Bai et al. (2023) and\nexclude the tests of which the sequence lengths are less than 4K. The baselines and configurations\nof our method are the same as those presented in Section 5. The results are shown in Table 4. Note\nthat the second row for each model, which includes the results for \"ShadowKV SparsityOnly,'\n\"ShadowKV\", three \"SVDq+Sparsity\", and three \"SVDq+Sparsity+$\\nu$V\" configurations, corresponds\nto the results in Table 3. For most of the models and method configurations, our SVDq method either\noutperforms or exhibits comparable performance to the baselines, including per-channel quantization\nLiu et al. (2024c), ThinK Xu et al. (2024), and ShadowKV Sun et al. (2024). Notably, the performance\ndegradation of our method compared to the full, non-quantized model is insignificant and nearly\nlossless for LongBench datasets. These results further corroborate the conclusions drawn from our\nanalysis of the RULER benchmark."}, {"title": "6 Conclusions", "content": "We present a mixed precision quantization approach for KV cache compression, which is grounded in\nprojection representation within the SVD and singular vector space. In this method, we assign higher\nquantization bit widths to the initial latent channels and gradually reduce the bit widths for subsequent\nl atent channels. Additionally, there is an option to truncate the final channels. Through comprehensive\nexperiments, we show that this approach outperforms direct per - channel quantization in terms of\nmodel performance, even when using lower mixed bit widths. Moreover, we explore the performance\nof our proposed method when integrated with other KV cache compression techniques, such as\nsparsity and V cache quantization. Our results reveal that our method incurs minimal performance\ndegradation, even when extremely low equivalent quantization bit widths (mixed 1.75 and 1.25 bits\nfor the K cache) are utilized. Overall, these findings convincingly demonstrate the effectiveness and\nefficiency of our proposed method in K cache compression."}, {"title": "7 Limitations", "content": "Although our method demonstrates good effectiveness in K cache compression, it primarily reduces\nthe required memory space for model loading without directly addressing computational cost. In fact,\nour current implementation may even slightly increase inference time.\nSpecifically, we utilize the pre-RoPE setting in our implementation. Our method extracts a quantized\nlow-bit K cache of the SVD projection representation before the application of Rotary Position\nEmbeddings (RoPE) and shares this low-bit representation across all heads. Due to the online\ncomputation of RoPE, which depends on the incoming position index, the reconstruction from\nthe projection representation to the original representation cannot be efficiently integrated into the\nmodel\u2019s forward pass. Consequently, this leads to an increase in computational cost for each head.\nThis increase in computational cost could potentially be remedied by switching to the post-RoPE\nsetting, where K cache is handled after the application of RoPE. However, as reported in ShadowKV\nwork (Sun et al., 2024) and observed in our numerical tests, the post-RoPE setting generally exhibits\ndegraded performance compared to the pre-RoPE setting.\nTherefore, investigating methods to accelerate the computation of our SVD quantization method,\npotentially by exploring alternative approaches or optimizations within the pre-RoPE framework, is\nan interesting direction for future research."}, {"title": "A Experiments Descriptions", "content": "A.1 Descriptions for Section 5.1\nIn this experiment, we include the below baselines for comparison:\nDefault No compression is applied, and 16-bit widths are used for all values. This is the default\nconfiguration of each models;\nDirect 3-bit Quantization 3-bit per-channel quantization Liu et al. (2023) is applied directly to the\nK matrix in its original space (as depicted in Figure 1a).\nThinK Direct channel truncation in the original space by ThinK (Xu et al., 2024) that retains $\\frac{4}{4}$\nchannels, in conjunction with 4-bit quantization, results in an equivalent 3-bit setting.\nThe equivalent mixed quantization bit width in this experiment are selected as $\\bar{b}$ = 3,2 for the\nSVDq method. The quantization schedule b is set to (8, 4, 4, 4, 2, 2, 0, 0) and (8, 4, 4, 0, 0, 0, 0, 0),\nrespectively.\nA.2 Descriptions for Section 5.2\nShadowKV Sun et al. (2024) and its sparsity techniques act as baselines and utilized in this work.\nBriefly, this strategy divides the K cache in the prefilling stage into small chunks, each containing\n8 tokens. It computes the mean embedding of each trunk as the landmark and then uses these\nlandmarks to identify important chunks. Specifically, the top-k chunks with the highest attention\nscores are considered important and retained, while the remaining chunks are neglected in the\ncomputation of attention. Note that this method also includes an auxiliary selection mechanism for\noutlier chunks, which are identified based on low cosine similarity. These outliers are not clipped\nduring the sparsity process. In addition to sparsity, the full ShadowKV method incorporates SVD\nlow-rank approximation of the K cache, retaining 160 out of the full 1024 ranks. This low-rank\napproximation can be considered equivalent to approximately 2.5-bit quantization, as the default\nnumerical precision is 16 bits.\nBased on ShadowKV, the baseline results for comparison that shown in Table 3 (the first three rows\nof each model) are:\nDefault Scores obtained with the default 16-bit digital precision;\nSparsity Scores obtained using the ShadowKV sparsity method without low-rank approximation or\nquantization;\nShadowKV Scores obtained using the full ShadowKV method, including both sparsity and equivalent\n2.5-bit quantization.\nThe detailed quantization schedules are shown in Table 5.\nA.3 Descriptions and Results for Section 5.3\nIn this experiment, the configuration of K cache compression and sparsity remains the same as in the\nsecond experiment: the mixed quantization bit schedules are set according to Table 5, consistent with\nthe previous experiment, and the sparsity method employs the ShadowKV sparsity technique (Sun\net al., 2024). In addition to these settings, we observe the very weak low-rank property of V cache\nand hence apply a direct 4-bit per-token quantization to the V cache."}]}