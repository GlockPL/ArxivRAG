{"title": "Coordinated Multi-Armed Bandits for Improved Spatial Reuse in Wi-Fi", "authors": ["Francesc Wilhelmi", "Boris Bellalta", "Szymon Szott", "Katarzyna Kosek-Szott", "Sergio Barrachina-Mu\u00f1oz"], "abstract": "Multi-Access Point Coordination (MAPC) and Artificial Intelligence and Machine Learning (AI/ML) are expected to be key features in future Wi-Fi, such as the forthcoming IEEE 802.11bn (Wi-Fi 8) and beyond. In this paper, we explore a coordinated solution based on online learning to drive the optimization of Spatial Reuse (SR), a method that allows multiple devices to perform simultaneous transmissions by controlling interference through Packet Detect (PD) adjustment and transmit power control. In particular, we focus on a Multi-Agent Multi-Armed Bandit (MA-MAB) setting, where multiple decision-making agents concurrently configure SR parameters from coexisting networks by leveraging the MAPC framework, and study various algorithms and reward-sharing mechanisms. We evaluate different MA-MAB implementations using Komondor, a well-adopted Wi-Fi simulator, and demonstrate that AI-native SR enabled by coordinated MABs can improve the network performance over current Wi-Fi operation: mean throughput increases by 15%, fairness is improved by increasing the minimum throughput across the network by 210%, while the maximum access delay is kept below 3 ms.", "sections": [{"title": "I. INTRODUCTION", "content": "The ever increasing demand for high performance and reliability in wireless networks has driven the development of sophisticated features for technologies like IEEE 802.11 (commercially known as Wi-Fi). This includes the ongoing work on Multi-Access Point Coordination (MAPC) for Wi-Fi 8 [1] by the IEEE 802.11bn task group, which addresses issues associated with the coexistence of multiple APs, where achieving optimal performance remains a significant challenge. MAPC entails a paradigm shift with respect to current distributed channel operations in Wi-Fi, since it is foreseen as a mechanism for enabling APs to collaborate and optimize resources across the network by exchanging information. Some potential features enabled by MAPC are Coordinated Spatial Reuse (C-SR), Coordinated Beamforming (C-BF), and Coordinated Orthogonal Frequency-Division Multiple Access (C-OFDMA) [2].\nRegarding C-SR [3], it aims to evolve the current Overlapping Basic Service Set Packet Detect (OBSS/PD) SR mechanism, which was introduced in IEEE 802.11ax (2020) [4]. OBSS/PD SR builds on top of BSS coloring (allowing for fast inter-BSS transmission identification) to unlock the usage of OBSS/PD thresholds that are less restrictive than Clear Channel Assessment (CCA) so that more Transmission Opportunities (TXOPs) can be created. However, OBSS/PD SR has seen little adoption in commercial equipment so far because of the moderate performance improvements compared to Distributed Coordination Function (DCF). One of the main limitations of OBSS/PD SR operation stems from its decentralized implementation, which leads to imposing too restrictive constraints in terms of transmit power, thus significantly limiting the mechanism's achievable gains. C-SR aims at overcoming OBSS/PD SR decentralization by leveraging MAPC inter-BSS communication (e.g., to exchange interference measurements), thus allowing multiple BSSs to perform efficient simultaneous transmissions. Significant research and standardization efforts are already being put into C-SR, which recently became a candidate feature for IEEE 802.11bn (Wi-Fi 8) after being accepted into the group's Specification Framework Document (SFD). However, there is still no consensus on how C-SR will materialize, so its implementation remains open.\nThis paper investigates the potential of MAPC-enabled coordinated Multi-Armed Bandits (MABs) for enhancing SR in coordinated Wi-Fi networks. Our approach presents a promising alternative to C-SR, with the aim of achieving similar network-wide benefits while reducing the complexity of joint Transmit Power Control (TPC) and PD adjustment decisions. MAB is a popular framework for sequential decision-making under uncertainty, which makes it well-suited for overcoming the complex OBSS interactions that occur in Wireless Local Area Networks (WLANs). This underlying complexity is precisely the main motivation for adopting Artificial Intelligence (AI) and Machine Learning (ML) techniques such as MAB, which are expected to address the dynamic and unpredictable nature of wireless environments, for which static approaches (even if coordinated) can fail. In fact, the adoption of AI/ML in the IEEE 802.11 is gaining momentum with the establishment of the AI/ML Study Group (SG) and the AI/ML Standing Committee (SC) [5]. This paper's contributions are as follows:\n\u2022\nWe propose a coordinated MAB solution to learn the best SR policies (combining PD and transmit power configurations) on a scenario basis, so that multiple neighboring BSSs can wisely identify favorable TXOPs. Our coordinated MAB solution leverages the MAPC"}, {"title": "II. RELATED WORK", "content": "The achievable performance of C-SR mechanisms has been evaluated in multiple ways and through various implementations. In [6], for example, a model-based evaluation of C-SR was shown to double DCF performance in some scenarios. Similarly, the work in [7] delved into different scheduling strategies for performing C-SR once MAPC groups are established. In that regard, the authors showed that AP-centric strategies (e.g., aiming to prioritize highly loaded APs) perform better than group-centric ones (e.g., aiming to maximize aggregate performance). Another implementation of C-SR in ns-3 demonstrated the superiority of C-SR against uncoordinated OBSS/PD SR, with a throughput improvement factor of 2.3 [8]. Furthermore, a prototype solution of a centralized C-SR mechanism was presented in [9], [10] to bring 33% goodput enhancements when enabled. Other studies confirm the appeal of applying C-SR when combined with other techniques. An example is the work in [11], which showed the benefits of combining C-SR with C-TDMA. In particular, C-SR with C-TDMA was shown to improve throughput by 30% and reduce latency by a factor of 2, compared to applying C-SR only.\nAnother prominent research line, as part of future AI-native radios [5], is AI-driven SR, which aims to overcome the limitations of the static approaches currently proposed for C-SR. In particular, existing proposals use radio measurements such as the Received Signal Strength Indicator (RSSI) statically (e.g., averaged values) to derive C-SR policies that allow for an acceptable Signal-to-Noise Ratio (SNR). However, such an approach may lose effectiveness in real deployments where the variability of both RSSI and SNR is very high. Some works exploring AI-driven SR solutions are [12], [13], where different MAB solutions were proposed and studied to drive the optimization of SR parameters such as PD and transmit power. The exploration-exploitation paradigm embedded in MAB showed potential for addressing the non-stationarity experienced in OBSSs implementing SR solutions. However, these solutions were based on decentralized mechanisms (no communication between agents was provided), which led to several issues as a result of the competition arising among networks. MAB solutions have also been assessed in other Wi-Fi problems, such as channel bonding, demonstrating their effectiveness as a lightweight, efficient, ready-to-use solution [14]. In this work, similar to what was done in [12], [13], we focus on an AI-native solution to improve SR. As a step forward, we study the advantages that coordination may bring to the operation of decentralized agents, which are assumed to cooperate within an MAPC framework."}, {"title": "III. COORDINATED MAB", "content": "We consider a Multi-Agent MAB (MA-MAB) setting in which multiple agents are individually enrolled in different BSSs for optimizing SR by tuning the PD (which determines to which extent inter-BSS interference can be ignored) and the transmit power (which determines to which extent inter-BSS interference is created). Moreover, by leveraging MAPC capabilities, we allow agents to exchange feedback about their performance regularly, thus aiming at creating collaborative strategies that enhance global performance. Formally, based on some action-selection strategy $\\mathcal{E}$ (cf. Section III-A), each player (or agent) $p \\in \\mathcal{P}$ sequentially plays an action (or arm) $k \\in \\mathcal{A}^{(p)}$ at each time step $t = 1,...,T$ and obtains a reward $r^{(p)}(t+1) \\in \\mathbb{R}$ that depends on the joint action profile $K(t) = \\{k^{(1)}(t), ..., k^{(\\vert\\mathcal{P}\\vert)}(t)\\}$ and on the reward calculation strategy $\\mathcal{R}$ (cf. Section III-B). For the SR problem, we consider actions composed of discrete PD $(\\gamma)$ and transmit power $(\\zeta)$ values, which leads to an individual action space $\\mathcal{A}^{(p)} = \\vert\\gamma\\vert \\times \\vert\\zeta\\vert$ and a global action space $\\mathcal{A}^{|\\mathcal{P}|} = (\\vert\\gamma\\vert \\times \\vert\\zeta\\vert)^{|\\mathcal{P}|}$."}, {"title": "A. Action Selection Strategy (E)", "content": "We focus on two exploration-exploitation strategies, namely $\\varepsilon$-greedy [15] and Thompson sampling [16], for driving the training of agents towards finding the best actions. First, $\\varepsilon$-greedy is a classic strategy used in reinforcement learning whereby an agent samples an arm uniformly at random (U) with a certain fixed probability (exploration) and selects the arm yielding the best-observed performance the rest of the time (exploitation). The $\\varepsilon$-greedy algorithm, despite its simplicity, has been shown to be effective in complex settings such as OBSS [14]. In particular, an agent implementing $\\varepsilon$-greedy selects an arm as:\n$k = \\begin{cases} k \\sim U(1, |A^{(p)}|), & \\text{with prob. } \\varepsilon \\text{ (exploration)}, \\\\ \\underset{k \\in A^{(p)}}{\\text{argmax }} r_k^{(p)}, & \\text{with prob. } 1 - \\varepsilon \\text{ (exploitation)}. \\end{cases}$                                                                                             (1)\nThompson sampling addresses exploration-exploitation in a different way than $\\varepsilon$-greedy: that it samples arms based on their probability of being optimal, according to a probabilistic model constructed from the observed rewards. In particular, as previously done in [12], we assume that the distribution of the reward associated with each arm is given by a Gaussian distribution $\\mathcal{N}$. Based on that, an agent implementing Thompson sampling selects, in each iteration, the arm maximizing $\\underset{k \\in A^{(p)}}{\\text{argmax }} \\theta_k$, where\n$\\theta_k \\sim \\mathcal{N}\\left(\\frac{\\bar{r}_k^{(p)}}{N_k^{(p)} + 1},\\frac{1}{N_k^{(p)} + 1}\\right)$                                                                                                                  (2)\nwhere the estimated reward $\\bar{r}_k^{(p)}$ is given by\n$\\bar{r}_k^{(p)} \\leftarrow \\frac{r_k^{(p)}}{N_k^{(p)} + 2}$                                                                                                                                (3)"}, {"title": "B. Reward Calculation Strategy (R)", "content": "To compute the reward, we consider different sharing strategies, which are based on a selfish reward (SELF) that is firstly computed by each agent individually. In particular, the selfish reward is calculated as the normalized individual throughput $r^{(p)} = \\Gamma^{(p)}/\\Gamma^{*(p)}$, where $\\Gamma^{*(p)}$ is the maximum achievable throughput in isolation, derived from the maximum transmission capabilities of the BSS given by the best applicable Modulation and Coding Scheme (MCS).\u00b9 As for the sharing strategies, enabled by the MAPC framework, they allow the different agents to play actions according to a shared performance among the players. From the MAB algorithms' perspective, MAPC is assumed to allow for a perfect monitoring setting, which means that every agent has access to the rewards experienced by others. According to this, we study the following shared reward criteria:\n\u2022 Average, AVG: The shared reward is calculated as the average value of each individual reward, so that $r'_{AVG} = \\frac{1}{|\\mathcal{P}|} \\sum r^{(p)}$.\n\u2022 Max-min, MAX-MIN: The shared reward is computed as the minimum value of each individual reward, so that $r'_{MAX-MIN} = \\underset{p \\in \\mathcal{P}}{\\text{min}}(r^{(p)})$.\n\u2022 Proportional fairness, PF: The shared reward is calculated as the sum of the logarithms of each individual reward, so that $r'_{PF} = \\sum \\text{log}(r^{(p)})$.\nIn this work, we assume that the communication between agents for sharing the rewards is negligible and lossless, and occurs at the end of each learning iteration of fixed duration $\\Delta$. Future work is expected to shed light on the overheads associated with the coordinated MABs, as well as on the implications of sharing information through wireless links exposed to contention and interference effects."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, we study the different strategies for improving SR through simulations. We use Komondor [17], an IEEE 802.11 simulator with embedded AI-capable agents. The various MAB implementations were simulated along with the network operation using the parameters collected in Table I. More details on the considered IEEE 802.11 frame types, sizes, and interframe periods are in [4, Table B.6]."}, {"title": "A. Interactions between 2 BSSS", "content": "We start with the deployment depicted in Figure 2 (later referred to as the toy scenario), which contains two BSSS where different contention and interference interactions occur depending on the power and PD used by the BSSs. To keep complexity low and clearly devise the effect of applying different bandit strategies, we consider four possible configurations (or actions) combining $\\gamma = \\{-72,-82\\}$ dBm and $\\zeta = \\{10, 20\\}$ dBm, thus leading to the per-agent action space $A = \\gamma \\times \\zeta$. Figure 2 also shows the performance in terms of throughput and airtime in the particular case where the two BSSs jointly apply each of the $\\vert A \\vert = 4$ considered configurations. Given the symmetry of the considered deployment, the optimal performance is obtained when the two BSSs use configuration $\\{k^{(1)}, k^{(2)}\\} = \\{A_1, A_1\\}$.\nNext, we show the performance achieved by the BSSs of the toy scenario when using coordinated bandits with $\\varepsilon$-greedy as an action-selection strategy $\\mathcal{E}$ and AVG as the sharing reward policy $\\mathcal{R}$.\nAs shown in Figure 3, the coordinated MABs allow for maximizing the average performance when compared to OBSS/PD SR and uncoordinated bandits. This is because the coordinated mechanism drives the two BSSs playing $A_1$ most of the time , which matches with the optimal configuration shown in Figure 2. In contrast, OBSS PD SR statically uses the same configuration and the uncoordinated MABs get stuck in the most conservative action ($A_4$), as a result of the competition among the two BSSs. This effect was previously shown in [13] and occurs when optimal actions lead to poor performance when the other agent selects a different action. As a result, agents using the SELF reward get stuck in a weak equilibrium by conservatively selecting $A_4$. This result motivates the adoption of cooperative strategies."}, {"title": "V. CONCLUSIONS", "content": "This paper explored the potential of coordinated MA-MAB to improve SR in MAPC-equipped Wi-Fi networks. Our findings demonstrate that coordinated MABs offer a significant advantage over the OBSS/PD SR operation and uncoordinated MABs. Moreover, we studied different exploration-exploitation strategies and showed that explicit exploration ($\\varepsilon$-greedy) grants control on the exploration but might not adapt properly to different situations, whereas implicit exploration (Thompson sampling) leads to higher instability but adapts well to various contexts and types of rewards and eventually leads to optimal policies. In addition, we studied different reward-sharing strategies and showed that metrics such as AVG and PF can drive agents to find global configurations that maximize overall performance. Meanwhile MAX-MIN is useful to maximize minimum performance, but fails to maximize overall performance in some situations."}]}