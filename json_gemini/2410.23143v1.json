{"title": "The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in Lie Detection", "authors": ["Haimanti Bhattacharya", "Subhasish Dugar", "Sanchaita Hazra", "Bodhisattwa Majumder"], "abstract": "We investigate how low-quality AI advisors, lacking quality disclosures, can help spread text-based lies while seeming to help people detect lies. Participants in our experiment discern truth from lies by evaluating transcripts from a game show that mimicked deceptive social media exchanges on topics with objective truths. We find that when relying on low-quality advisors without disclosures, participants' truth-detection rates fall below their own abilities, which recovered once the AI's true effectiveness was revealed. Conversely, high-quality advisor enhances truth de-tection, regardless of disclosure. We discover that participants' expectations about AI capabilities contribute to their undue reliance on opaque, low-quality advisors.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) has recently garnered unprecedented attention from both popular media (e.g., Economist, 2024; New York Times, 2023; Washington Post, 2024) and leading researchers (e.g., Acemoglu, 2024; Ludwig et al., 2023; Elliott, 2019; Rahwan et al., 2019; Kleinberg et al., 2018; Brynjolfsson et al., 2017; Makridakis, 2017) due to its transformative impacts on contemporary life. Fueled by myriad applications, AI has gained prominence as advisors for individuals across diverse spheres of daily life and is poised to expand its advisory role in plenty of other domains (Fast and Schroeder, 2020).\nAs AI-assisted decision-making becomes increasingly common, evidence shows people either overly rely on AI advice (\u201cAI appreciation\") or lack sufficient confidence in it (\"AI aversion\"). Despite remarkable advances in AI, such suboptimal reliance on AI may have adverse consequences across various user domains. For instance, as we usher in a new era of detecting deception online (Alom et al., 2020; Monaro et al., 2020), such suboptimal reliance on AI may hinder efforts to curb the spread of lies the primary focus of our work. Malicious actors may exploit these behavioral biases to spread lies and evade detection, a concern recognized in the AI policy treatises of the US, EU, OECD, and G7. As the efficacy of AI tools remains largely opaque to users in the current digital milieu, the looming danger has spurred a consensus on the need to unveil the black box of AI, enhancing its transparency, reliability, and usability. While common transparency mandates focus on algorithmic clarity and explainability to foster trust among downstream users, a crucial remedy proposed to combat nefarious intentions and promote proper AI use entails disclosing quantitative metrics of the AI's quality."}, {"title": "", "content": "Set against this backdrop, this paper explores a relatively novel application of AI: providing individuals with Al advisors of varying quality to aid them in detecting lies in textual form, where the AI's efficacy in detecting such deceptions may or may not be available to the user. While text-based lies flourish in the online realm, the role of AI in their detection within economic research remains strikingly understudied. We advance this emerging body of work by investigating a hitherto unexplored context where motives for deception abound and individuals attempt to discern truth from lies while perusing written discussions on topics with objectively verifiable facts unknown to them. Lever-aging recent advances in Large Language Models (LLMs), we deliberately simulate AI advisors of varying caliber inferior, average, and superior - quantified by their accuracy rates. Participants in our experiment, offered the choice to consult an AI advisor of a specific quality to detect the truth, are exposed to one of two AI environments: blackbox (AI efficacy undisclosed) or with information (AI efficacy disclosed). To the best of our knowledge, no prior research has investigated the complex calculus of how the quality spectrum of AI advisors and the (lack of) disclosure of their effectiveness collectively influence individuals' reliance on AI across any domain, including lie detection.\nAs text-based lies manifest in diverse forms, we let our participants discern the objective truth by inspecting naturally generated text fraught with lies wherein parties with conflicting interests discuss a fact-based topic in a back-and-forth Q&A format. Our setup thus captures the essence of popular Q&A forums such as Reddit, Quora, Answers.com, etc., where lies about facts are often spread via written exchanges by opposing parties. Appendix A presents a Quora excerpt, exemplifying the type of text-based lies about facts we probe. We opted to focus on this specific form of text-based deception since online Q&A forums, the prominent conduit for spreading text-based lies, routinely disseminate lies in this format (Tsou, 2023). These platforms, characterized by user-generated content and minimal oversight, create fertile grounds for deception on critical topics grounded in facts. Moreover, they boast enormous engagement, wielding con-siderable influence in shaping individual opinion. Furthermore, research indicates that"}, {"title": "", "content": "these forums are particularly vulnerable to exploitation by malicious actors due to their financial dynamics (Gazan, 2011; Shah et al., 2008). The increasing use of bots and algorithms in \u201cinformation wars\" to amplify messaging impacts highlights the need for transparent AI systems to assist users in discerning between truth from lies.\nSeveral reasons justify our exclusive attention to text-based lies concerning objectively verifiable facts. First, it aligns with the economic literature on deception, typically in-vestigating objective falsities akin to the one documented in Appendix A, such as those investigated in Gneezy (2005) or the die-rolling experiments in Fischbacher and F\u00f6llmi-Heusi (2013) (see Abeler et al., 2019 for a review). Second, the focus is consistent with the observed preference of individuals to rely more on AI advice for objective than subjective matters (Laakasuo et al., 2021; Castelo et al., 2019; Logg et al., 2019). Third, recent re-search has discovered a troubling trend: individuals often struggle to differentiate between facts and lies in contexts involving objective truths, contrary to the presumption that such distinctions should be an easy task (e.g., Belot and van de Ven, 2017; Ockenfels and Selten, 2000; Bond and DePaulo, 2006). Fourth, although information in audio and/or visual forms is more prevalent and elicits stronger emotional reactions than comparable textual information, evidence shows that the latter is equally persuasive (Glasford, 2013; Yadav et al., 2011; Appiah, 2006; Messaris and Abraham, 2001) and harmful (Wittenberg et al., 2021; Powell et al., 2018; Tukachinsky et al., 2011).\nAs source material for Q&A-based text faithfully mirroring the relevant online content, we utilize transcripts of multiple sessions of the first season (1956\u20131959) of the American TV show To Tell The Truth (T4). We financially incentivize participants to discern the objective truth from lies embedded in each transcript corresponding to a distinct session of T4 featuring a host, four judges, and three contestants. All contestants claim to be the genuine John/Jane Doe or the real character (RC) in the show, with only one being truthful, while the others aim to deceive the judges into believing they are the RC. The show begins with the revelation of facts about the RC from an affidavit signed by the RC before filming. The judges then interrogate the contestants through back-and-forth"}, {"title": "", "content": "Q&A. After Q&A, judges guess who the RC is. Contestants earn an equal share of $250 for each incorrect guess, potentially amassing up to $333 if all guesses are incorrect a large sum in the 1950s, equating to approximately $3500 in 2024.\nOur decision to use lies embedded in transcripts from multiple sessions of T4, rather than real-life examples curated from Q&A sites, is primarily aimed at minimizing the influence of motivated reasoning on our participants' ability to distinguish truth from lies. Motivated reasoning, documented extensively in lie detection literature (e.g., Kahan, 2015; Kunda, 1990), describes how people interpret information to align with their existing beliefs or preferences. By utilizing T4 transcripts that consistently center on neu-tral topics, we aim to mitigate, if not entirely bypass, the influence of strong prior beliefs, thereby enabling participants to unearth lies with greater objectivity. By sidestepping motivated beliefs, our approach provides a cleaner evaluation of individuals' abilities to distinguish between factual information and deceptive text. T4 transcripts also closely resemble the scenario in Appendix A, bestowing an important benefit. Appendix B pro-vides an excerpt from a T4 transcript we use, showcasing a style akin to the content in Appendix A. From the participant's perspective, analyzing T4 transcripts mirrors situa-tions where an impartial third party, with no or some prior knowledge of a topic, seeks factual information in discussions - akin to individuals consulting Q&A forums to un-cover truths (as in Appendix A). We posit that both scenarios pose comparable challenges in discerning truth amid discussions tinged with conflicting interests.\nOur database of transcripts is constructed in two phases. In Phase 1, we downloaded 385 of 429 sessions from YouTube. From these, we randomly selected and transcribed 195 sessions using an open-source transcription tool. After applying strict inclusion criteria, detailed in Section 3, we further refined this selection to 132 transcripts paired with corresponding affidavits that included sufficient textual cues for identifying the RC. Phase 2 utilized GPT-4, a state-of-the-art LLM, to generate guesses for all 132 transcripts. The model achieved an overall accuracy rate of 56.06%, significantly outperforming the"}, {"title": "3.2 Identification of the RC using AI", "content": "In Phase 2, we used a generative AI model to identify the RC in each transcript. Frontier generative models such as GPT-4 (Open AI, 2023) show remarkable performance in reasoning and language understanding tasks (Hendrycks et al., 2020), making them an ideal candidate for algorithmic lie detectors from the text. We used in-context learning, where we created a set of natural language statements (prompts) informative about the rules of T4 and defined the truth detection task for the AI model. The output entails predicting who the RC is: Number One, Number Two, or Number Three. Our best model achieved an accuracy rate of 56.06% in correctly identifying the RC for 132 transcripts (see Hazra and Majumder, 2024 for more details on the AI model). Furthermore, our model was able to detect deception in cases where all human judges failed to identify the RC, underscoring the potential for AI-human collaborations in improving lie detection."}, {"title": "3.3 Creation of Three AI Advisors", "content": "Finally, we created three distinct types of AI advisors: LQ, MQ, and HQ. Each quality level corresponds to a distinct cluster of five transcripts. To determine the composition of these clusters, we divided the 132 transcripts into two groups: AI correct guesses (74 transcripts) and AI incorrect guesses (58 transcripts). Thereafter, we randomly drew transcripts from each group without replacement to create the three AI advisors with varying quality. For the LQ advisor, which has an accuracy rate of 20%, we randomly selected one transcript from the AI correct group and four from the AI incorrect group. This cluster represents an under-performing AI advisor relative to chance. The MQ advisor has an accuracy rate of 40%, better than random guessing. To construct this cluster, we randomly sampled two transcripts from the AI correct group and three from the AI incorrect group. The HQ advisor is designed to have an accuracy rate of 60%, demonstrating an accuracy rate significantly better than chance and roughly similar to the overall accuracy rate of 56.06% achieved by our AI model. It is important to note that ex-ante, we do not know if these clusters present distinct or similar challenges in identifying the RC for our participants. Instead, they are constructed to systematically vary the"}, {"title": "4 A Behavioral Model", "content": "We develop a simple behavioral framework aimed at developing testable hypotheses about how variations in the quality of AI advisors and the availability of information regarding the AI advisor's capability to identify the RC influence an individual's likelihood of relying on the AI advisor and their expected utility from the truth detection task. We begin by considering an individual, i, tasked with identifying the RC while reviewing only one transcript. For simplicity, we will assume that the quality or the difficulty level of the representative transcript remains constant across all conditions studied below. If the individual correctly guesses the identity of the RC, they receive a monetary reward x; if incorrect, they earn y, where x > y \u2265 0. Hence, the individual's expected utility from the truth-detection task is given by E(U) = p\u22c5u(x) + (1 \u2212 p)\u22c5u(y), where p\u2208 [0, 1] represents the probability of correctly identifying the RC. Without loss of generality, we assume x = 1, y = 0, and u(x) = 1, u(y) = 0. Thus, the individual's expected utility simplifies to E(U) = p. Note that E(U) thus represents the truth detection probability.\nThe individual i knows their own probability of correctly identifying the RC from the transcript, denoted as pi \u2208 [0, 1]. Therefore, individual i is aware of their own ability to identify the RC. If the individual makes a random guess, then p\u2081 = \u2153, as there are three contestants, one of whom is the RC in the transcript. Thus, the expected utility of a random guesser is \u2153. This leads us to our first hypothesis."}, {"title": "Hypothesis 1.", "content": "A random guesser will correctly identify the RC with probability \u2153.\nSuppose the individual can access an AI advisor's guess about the RC's identity at no cost. The individual has the option to either submit their own guess or use the AI's guess as their final choice, which will determine their ultimate material payoff. Suppose that the true probability of the AI advisor correctly identifying the RC is pa \u2208 [0, 1], representing the advisor's true capability. We examine two information conditions. In the first, the individual i is not provided with any information about pa, denoted as the Blackbox (BB) condition. In the other information condition, referred to as the With Information (WI) condition, the individual is made aware of the AI advisor's true"}, {"title": "", "content": "capability, pa, when deciding whether to rely on the AI's guess or their own guess. Below, we will first analyze how variations in the value of pa influence the individual's decision to switch or not switch to the AI advisor's guess under each information condition.\nSuppose that for the given transcript, pa changes to pa' such that pa' > pa. In the BB condition, the individual receives no information about pa. Therefore, in BB, the individual's decision to rely on the AI's guess hinges on comparing pi and their belief or expectation about the AI's ability to correctly identify the RC, denoted by pe \u2208 [0, 1]. Thus, in BB, the individual's expected utility from choosing to switch (s) to the AI's guess becomes E(Us) = pe. Conversely, the expected utility from choosing not to switch (ns), thereby relying on their own guess instead of the AI's, is given by E(Uns) = Pi\u00b7 The individual opts for s if E(Us) > E(Uns), that is, if pe > pi. Similarly, the individual chooses ns if E(Us) < E(Uns), that is, if pe < pi. They remain indifferent between the two choices when E(Us) = E(Uns), which occurs if pe = pi. Therefore, in the absence of any knowledge about pa, a change in pa's value does not impact the individual's decision to follow the AI's recommendation in BB. The decision remains solely dependent on comparing pi and pe, not pa. However, the realized expected utility from choosing s will be pa, while the realized utility from choosing ns will be pi. Hence, the individual's realized utility can change with a change in pa despite no change in the individual's decision to choose s or ns. The individual's switch decision and the effect of a change in Pa's value on the realized expected utility in BB is summarized in Table M1 below."}, {"title": "Hypothesis 2a.", "content": "In the BB condition, the individual's decision to switch to the AI advisor's guess will not change with an increase in the value of pa."}, {"title": "Hypothesis 2b.", "content": "In the BB condition, the individual's realized truth detection prob-ability will increase weakly with an increase in the value of pa.\nBy contrast, in the WI condition, the individual's decision to rely or not rely on"}, {"title": "Hypothesis 3a.", "content": "In the WI condition, the individual will weakly prefer to switch to the AI advisor's guess with an increase in the value of pa."}, {"title": "Hypothesis 3b.", "content": "In the WI condition, the individual's realized truth detection prob-ability will increase weakly with an increase in the value of pa.\nNext, we analyze, for a given pa value, how disclosing information about pa affects the individual's decision to switch to the Al's guess compared to the BB condition. Depending upon the relative magnitudes of pi, pe, and pa, one of the following cases listed in Table M3 will arise. As before, each case will lead to two key outcomes: a change in the individual's reliance on the AI and a corresponding shift in expected utility."}, {"title": "Hypothesis 4.1a.", "content": "The individual's switching decision between the BB and WI will remain the same if both pa and pe are greater than, less than, or equal to pi."}, {"title": "Hypothesis 4.1b.", "content": "The individual's realized truth detection probability between the BB and WI will remain the same if both pa and pe are either greater than, less than, or equal to pi."}, {"title": "Hypothesis 4.2a.", "content": "The individual's switching decision will change, weakly or strictly preferring s over ns when moving from BB to WI if pe < pa and pi\u2264 Pa"}, {"title": "Hypothesis 4.2b.", "content": "The individual's realized truth detection probability will weakly increase when moving from BB to WI if pe < pa and pi \u2264 Pa"}, {"title": "Hypothesis 4.3a.", "content": "The individual's switching decision will change, weakly or strictly preferring ns over s when moving from BB to WI if pa < pe and pi > Pa\u00b7"}, {"title": "Hypothesis 4.3b.", "content": "The individual's realized truth detection probability will weakly increase when moving from BB to WI if pa < pe and pi \u2265 Pa\u00b7\nAs mentioned earlier, our experiment incorporates two information conditions: BB and WI. By design, pa \u2208 {0.2, 0.4, 0.6} in our experiment. We also elicit participants' pe. With these experimental parameters in place, we proceed to test the above hypotheses."}, {"title": "5 Experimental Design and Procedure", "content": "Our experiment induces a 2 \u00d7 3 between-subjects design with six treatments. The two information conditions, BB and WI, represent the absence and presence of AI accuracy information, respectively. Within each information condition, we vary the AI advisor accuracy with LQ = 0.2, MQ = 0.4, and HQ = 0.6. The treatments are thus labeled as BBLQ, BBMQ, BBHQ, WILQ, WIMQ, and WIHQ. A total of 545 participants took part in the experiment, conducted over several days on the Prolific online platform, with the following distribution: 91 in BBLQ, 89 in BBMQ, 94 in BBHQ, 91 in WILQ, 90 in WIMQ, and 90 in WIHQ. Each participant was randomly assigned to only one treatment.\nWe used oTree (Chen et al., 2016) to develop the software program for the experiment. Since T4 featured contestants and judges predominantly from the United States and included sessions with historical, social, and cultural references pertinent to the US, we filtered participants from the Prolific pool based on nationality and place of residence, both set to the US, to ensure familiarity with such references. The Prolific platform enabled us to screen participants according to our geographical requirements.\nParticipation in the experiment was voluntary. Participants were required to provide informed consent before proceeding. Participants who did not consent exited the exper-iment and did not receive any payment. Participants consented to participate in the experiment were presented with the instructions for Task 1. After reading the instruc-tions of Task 1, participants were required to answer three screening questions. These questions were designed to assess the participants' understanding of Task 1 instructions. Correctly answering these questions was a prerequisite for moving forward with Task 1."}, {"title": "5.1 Tasks in the BB condition", "content": "The BB condition is grounded in the rationale that end users often do not have access to the true effectiveness of AI systems, which can lead to either overreliance on or insuf-ficient trust in the AI's recommendations (Vasconcelos et al., 2023; Zhang et al., 2020; Dietvorst et al., 2015). In contrast, by disclosing the AI's efficacy, as in the WI condi-tion, individuals are empowered to calibrate their trust and reliance on the AI's counsel more appropriately. Hence, the BB condition serves as a benchmark for examining the dependence on AI when its efficacy is unknown against when it is transparent.\nEach participant completed five tasks in the BB condition. The specifics of each task were not disclosed beforehand. Task 1 was divided into two sub-tasks. In sub-task 1, each participant was randomly assigned one of the three AI-advisor qualities. For each of the five transcripts in that cluster, the participant was required to guess which of the following three options represented the RC: Number One, Number Two, or Number Three. We randomized the sequence in which participants encountered the transcripts within a given cluster. Their guess for each transcript was recorded and referred to as their initial guess. Regardless of accuracy, participants received a fixed payment of $0.75 for each of the five initial guesses. In sub-task 2, participants indicated their level of absolute confidence for each of the five initial guesses on a scale from 0 (not confident at all) to 100 (completely confident). We used the quadratic scoring rule (QSR) (Charness et al., 2021) to financially incentivize accurate absolute confidence reporting. Participants earned higher rewards for accurate guesses when their confidence was high, and for incorrect guesses when their confidence was low. For example, a participant could earn up to $1.00 if they were completely confident that their guess was correct. By default, the confidence level was set at 50 in the experiment, which resulted in a payment of $0.75. Participants could drag a slider to choose their desired level of absolute confidence for each initial guess.\nTask 2 consisted of two sub-tasks. In each sub-task, participants answered one ques-tion per transcript. The first sub-task required participants to classify the difficulty level"}, {"title": "5.3 Eliciting Participants' Beliefs about AI Advisor's Efficacy", "content": "We elicited participants' expectations or beliefs about the AI advisor's ability to identify the RC. We asked participants the following question as part of Task 5 (see the eighth question under Task 5 in Appendix D): \u201cOut of the five sets, how many of the AI's guesses do you think are correct?\u201d Participants were not specifically incentivized to report their beliefs, aside from receiving a flat bonus of $0.50 for completing Task 5. The literature on the relationship between monetary rewards and the quality of elicited beliefs is mixed, influencing our decision not to pay participants for the belief question.\nAs evident, we collected these beliefs after participants had engaged with the AI advi-sor in both information conditions. Eliciting their beliefs at the end in the BB condition made sense because we never informed participants about the number of correct guesses made by the AI in any of the treatments. In contrast, in the WI condition, participants were already aware of the AI advisor's accuracy, which rendered the belief question re-dundant. However, we elicited their beliefs in the WI condition to maintain procedural consistency between the conditions. Additionally, we chose to gather these beliefs in the exit questionnaire rather than at the beginning of Task 3 before participants were informed of the AI's guesses and provided a choice to switch to the AI's guesses. We speculated that asking participants about the AI's performance prior to their switch-ing decision might affect their subsequent switching behavior. This is because existing research (e.g., Schlag et al., 2015) suggests that individuals often adjust their actions ret-rospectively to align with their reported beliefs, which could bias the switching data an essential variable for measuring individuals' reliance on the AI advisor in our context.\nTo address the potential influences of the timing of the belief question and financial incentives on belief data, we implemented a second belief elicitation scheme. The second scheme, implemented for both information conditions, referred to as the Modified Black-box (MBB) and Modified With Information (MWI), elicited participants' beliefs just"}, {"title": "6 Results", "content": "We begin by defining several key terms we use in the data analysis. An accurate guess occurs in our experiment when a participant correctly identifies the RC from a transcript. Initial accuracy refers to the accuracy rate computed from a participant's own five guesses made before being informed of the availability of an AI and its guesses. Final accuracy, by contrast, is the accuracy rate computed from the final five guesses submitted by a participant, which may consist of their own five guesses, the AI's five guesses, or a mix of their own and the AI's guesses. When a participant submits the AI's guess as their final answer instead of their own, and the AI's guess differs from the participant's guess, we call this a 'switch.\u2019The switching decisions provide us with a measure of a participant's degree of reliance on the AI. Participants might also choose to submit the AI's guess even when their own guess matches the Al's, as they are indifferent between the two. However, such a submission does not indicate true reliance on the AI in a strict sense, so we do not categorize it as a switch.\nOur analysis proceeds as follows. We first assess the initial accuracy to gauge the participants' intrinsic ability to discern the truth from the transcripts. Next, we scrutinize the switch rates to evaluate the participants' reliance on AI. Finally, we consider the implications of switches for truth detection by analyzing whether the final accuracy shows an improvement or decline compared to the initial accuracy."}, {"title": "6.1 Initial Accuracy", "content": "The participants' initial accuracy rate aggregated across all six treatments is roughly 35 percent, significantly higher than the 33 percent accuracy rate expected from a random guess (see the first row of the third panel for Initial accuracy rate in Table 1). It implies that the aggregate data does not lend support to Hypothesis 1. When we compute the combined initial accuracy rate for BBHQ and WIHQ, the rate rises to about 37 percent, and for BBMQ and WIMQ, it increases further to 41 percent; both are significantly higher than the 33 percent theoretical benchmark. In contrast, the combined initial accuracy rate for BBLQ and WILQ is roughly 28 percent, which is significantly lower than 33 percent (see the third panel for Initial accuracy rate in Table 1). Therefore, overall, participants demonstrated a better-than-chance ability to identify text-based lies. This trend persists across the clusters of transcripts associated with the MQ and HQ advisors. However, participants demonstrated a worse-than-chance ability to identify text-based lies for the cluster of transcripts corresponding to the LQ advisor. In other words, participants' initial accuracy rate is different from a random guess in each of the three transcript clusters, and as a result, Hypothesis 1 is rejected.\nWithin each information condition, the initial accuracy rate is significantly lower in LQ than in MQ or HQ, while it is equivalent between MQ and HQ (see the first and second panels for the Initial accuracy rate in Tables 1 and 2). Hence, in the cluster of transcripts where AI accuracy is the lowest (LQ), participants' accuracy is also the lowest. Next, we compare the initial accuracy rates between the information conditions. The initial accuracy rate is approximately 36 percent in BB and 35 percent in WI (see the first row of the first and second panels for the Initial accuracy rate in Table 1). These rates are statistically equivalent (see the first row of the third panel for the Initial accuracy rate in Table 2). Furthermore, the initial accuracy rates remain statistically equivalent between BB and WI for any given AI quality \u2014 LQ, MQ, and HQ (see the third panel for the Initial accuracy rate in Table 2). This consistency is expected, as participants made their choices before being exposed to different information environments. A broader implication of this uniformity in initial accuracy is that the participants' innate ability"}, {"title": "6.2 Switch Rates", "content": "We now analyze the extent to which participants relied on the AI advisors by opting to switch to the AI's guess when their own guess differed from the AI's guess. The aggregate switch rate in the experiment is approximately 21 percent (see the first row in the third panel for the Switch rate in Table 1). Participants switched to the AI's guess in roughly 24 percent of the transcripts in BB and 18 percent in WI (see the first row in the first and second panels for the Switch rate in Table 1). The switch rate is significantly lower in WI than BB (see the first row in the third panel for the Switch rate in Table 2). This suggests that, overall, participants were more inclined to rely on AI when they lacked information about the AI's accuracy than when such information was available.\nThe switch rates are 22, 24, and 25 percent for BBLQ, BBMQ, and BBHQ, respectively (see Figure 1 or the first panel for Switch rate in Table 1). These switch rates are statistically equivalent (see the first panel for Switch rate in Table 2). The equivalence between the switch rates across all BB treatments suggests that without information about the quality of the AI advisor, participants exhibit similar reliance behavior across all three AI advisors. Therefore, the data validates Hypothesis 2a.\nThe switch rates are 14, 17, and 23 percent for WILQ, WIMQ, and WIHQ, respectively (see Figure 1 or the second panel for Switch rate in Table 1). While the switch rates are equivalent for WILQ and WIMQ, the switch rate is significantly higher in WIHQ than in WILQ and WIMQ (see the second panel for Switch rate in Table 2). This suggests that when information about the AI's quality is disclosed, there is a significant increase in reliance on a high-quality AI advisor compared to relatively low-quality AI advisors. As a result, the data confirms Hypothesis 3a.\nA comparison of switch rates between the information conditions for a given AI quality reveals that the switch rate is significantly lower in WILQ compared to BBLQ, and also"}, {"title": "6.3 Expectations about the Capability of AI Advisors", "content": "We elicited participants' beliefs about the AI advisors' ability to identify the RC at the end of the BB treatments as part of the exit questionnaire. We first examine whether participants' expected AI accuracy indeed influenced their decision to switch to the Al's guess. We find that controlling for a participant's confidence in their own guess, perceived difficulty, and indicator of the cluster of transcripts that an AI advisor represents, a participant's expected AI accuracy is a significant predictor of the likelihood to switch in BB (see column (5) in Table 5). More specifically, as the expected AI accuracy increases, so does the probability of switching to the Al's guess. Therefore, the premise of our behavioral model that in the absence of AI accuracy information, an individual's expectation about AI accuracy plays an influential role in shaping their decision to switch to the AI advisor's guess appears well-supported.\nThe summary statistics reveal that the median and approximate mean of participants' expected AI accuracy is 3 out of 5 transcripts (i.e., 60 percent) across all three treatments \u2013 BBLQ, BBMQ, and BBHQ (see the first panel for Expected AI accuracy in Table 3). As a result, on average, while the participants' expected AI accuracy is significantly higher than the actual AI accuracy in BBLQ and BBMQ, the participants' expected AI accuracy is equivalent to the actual AI accuracy in BBHQ (see the first panel in Table 6).\nRecall that in the behavioral model, we denote the AI's actual accuracy as pa, and the individual's expectation about AI accuracy as pe. Therefore, on average pe = pa for HQ. If we interpret a participant's confidence in their own guess as a measure of pi, then on average Pi pa for HQ, as the mean confidence in BBHQ (as well as in WIHQ) is approximately 60 percent (see the first two panels in Table 3, and Table 7). Thus, on average, pi = pa = pe for HQ. Therefore, since pe = pa = pi for HQ, participants' switch rate does not change between BB and WI, in agreement with Hypothesis 4.1a. In other words, when AI accuracy is revealed and aligns with the participants' expectations, as is the case for HQ, participants do not feel the need to alter their AI reliance.\nThe expected AI accuracy statistics also imply that pa < pe for LQ and MQ, on average. Furthermore, participants' mean confidence in their own guess is significantly higher than the LQ advisor's accuracy rate of 20 percent in BBLQ (as well as in WILQ)"}, {"title": "6.4 Final Accuracy", "content": "The final accuracy rate is approximately 37 percent in BB and 38 percent in WI (see the first row in the first and second panels for the Final accuracy rate in Table 1). These rates are statistically equivalent (see the first row in the third panel for the Final accuracy rate in Table 2). This suggests that the higher reliance on AI observed in BB compared to WI does not improve final accuracy.\nThe final accuracy rates are approximately 26, 38, and 48 percent for BBLQ, BBMQ, and BBHQ, respectively; the corresponding statistics are 25, 44, and 46 percent for WILQ, WIMQ, and WIHQ, respectively (see Figure 2 or the first and second panels for Final accuracy rate in Table 1).\nWithin each information condition, the final accuracy rate significantly improves as the quality of AI advisors increases, with the sole exception being WIMQ and WIHQ, where the final accuracy rates remain statistically equivalent (see the first and second panels for Final accuracy rate in Table 2). This suggests that, overall, the final accuracy rate tends to improve, albeit weakly, as AI quality increases within each information condition. These findings thus validate Hypothesis 2b and Hypothesis 3b.\nA comparison of the final accuracy rates between information conditions reveals the following. First, the final accuracy rate is statistically equivalent between BBHQ and WIHQ (see the third panel for the Final accuracy rate in Table 2). This finding aligns with Hypothesis 4.1b. Second, while the final accuracy rate is equivalent between BBLQ and WILQ, it is higher in WIMQ than in BBMQ (see the third panel for Final accuracy rate in Table 2). These findings align with Hypothesis 4.3b.\nAt first glance, these findings might suggest that participants only benefited from AI accuracy information in the MQ condition, but not in LQ or HQ. However, given our experiment's between-subject design"}]}