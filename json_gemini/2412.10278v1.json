{"title": "Final Report of the 2024 NSF Workshop on Envisioning National Resources for Artificial Intelligence Research", "authors": ["Shantenu Jha", "Yolanda Gil"], "abstract": "This workshop aimed to identify initial challenges and opportunities for national resources for Al research (e.g., compute, data, models, etc.) and to facilitate planning for the envisioned National Al Research Resource (NAIRR). Participants included Al and cyberinfrastructure (CI) experts.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) significantly impacts all human endeavors, from accelerating discovery and innovation to solving critical societal and global challenges [Gil and Selman 2019]. Al has matured into an experimental science, where Al research on many problems requires significant computing resources, large amounts of data, comprehensive models, and other experimental needs. While these resources are available in industry labs, many researchers in academia and government lack access to the resources needed for their investigations and for training the next generation of researchers. Traditionally, high-performance computing (HPC) resources and other cyberinfrastructure (CI) resources have been available to academic researchers in science and engineering through the NSF Office of Advanced Cyberinfrastructure. However, the specific needs of the Al community must be better understood in terms of the nature of computations, the test data and evaluation methodologies, the design of testbeds, the dissemination of foundation models, and other resource requirements. Al research is very different from computational science and from other science and engineering domains that have recently driven cyberinfrastructure development. The unique requirements and needs of Al research must be better understood.\nA broader context is to understand the needs of the Al research community in light of the emerging National Artificial Intelligence Research Resource (NAIRR) [NAIRR 2023]. NAIRR is described as:\n\"NAIRR is a concept for a shared national research infrastructure to bridge this gap by connecting U.S. researchers to responsible and trustworthy Artificial Intelligence (Al) resources, as well as the needed computational, data, software, training, and educational resources to advance research, discovery, and innovation. The aim of NAIRR is to ensure that Al resources and tools are equitably accessible to the broad research and education communities in a manner that advances trustworthy Al and protects privacy, civil rights, and civil liberties.\nSeveral Federal Agencies, with NSF in the lead, are collaborating on a Pilot implementation of the NAIRR over the next several years. The NAIRR Pilot brought together government-supported and non-governmental-contributed resources to demonstrate the NAIRR concept and to deliver early capabilities to the U.S. research and education community. Goals for the NAIRR Pilot included demonstrating the NAIRR concept, spurring innovation, increasing diversity of talent, improving capacity, and advancing safe, secure, and trustworthy Al in research and society.\nThe NAIRR aims to address researcher needs by increasing access to a diverse ensemble of Al-related infrastructure resources, including computational capabilities, Al-ready datasets, pre-trained models, software systems, and platforms.\u201d [NSF 2024]\nThis workshop was intended to start a dialogue about what experimental resources are needed to support Al research, the immediate resource needs in the Al community, and the suitability of current national cyberinfrastructure resources to meet those immediate needs."}, {"title": "2. Current State and Major Observations", "content": "Al researchers across various institutions utilize a range of local and shared resources to advance their work:\n\u2022 Local resources include individual lab GPU workstations and lab-level GPU clusters, typically funded by grants from individual principal investigators (PIs). These resources range from low-end GPUs (NVIDIA 2080, 4090, V100) to higher-end models with limited multi-GPU setups (A6000, A10, A40, A100) shared among lab members.\n\u2022 Shared campus resources usually consist of departmental, university-level, or regional supercomputers accessible to researchers. Typical resources include a few hundred datacenter-class GPUs (NVIDIA A6000, A100, and recently H100). Researchers access these resources based on allocated credits or hours, submitting jobs through cluster management software such as slurm. This often results in significant delays and a need for more flexibility for researchers.\n\u2022 Industry platforms like Google Cloud, AWS, and Hugging Face provide computing and other resources and are accessible through grants or purchases.\n\u2022 National infrastructure resources. Researchers can also apply for credits or grants from supercomputers at other universities that are part of national science cyberinfrastructure.\n\u2022 Industry collaborations, such as project collaborations and student internships, provide some researchers access to internal clusters of companies like Google, Meta, and NVIDIA. These clusters can include hundreds to tens of thousands of GPUs. However, the research is not always publishable or shareable.\n\u2022 Industry services to access APIs to large generative Al models, such as GPT-4 from OpenAl. There is a cost to access the API to more modern models. The models themselves are unavailable, limiting the kinds of Al research that this can support in practice.\nAl researchers use these resources in a variety of ways:\n\u2022 Prototyping new Al methods is often done on research lab resources and then scaled up using larger clusters on campus.\n\u2022 Data quality and data processing pipelines to create training data in many domains is a common task in machine learning that requires significant effort.\n\u2022 Datasets that contain sensitive or protected data are very challenging to share across institutions, severely limiting their use for Al research in real problems that require large amounts of data\n\u2022 Al is often embedded in interactive settings or physical environments rather than just a collection of archival datasets.\n\u2022 Fine-tuning smaller foundation models is often done on individual computers.\n\u2022 Commercial services are widely used, including cloud services, services to access pre-trained models, model hosting services, data and software repositories, and a range of other functions.\n\u2022 Classroom use of computing and data resources for education and teaching purposes is also common, with students having varying access to large resources.\nParticipants reflected on the current situation and converged on several major observations:\nThe Al research community is confronting unprecedented scale in many research areas and dimensions. Generative Al is typically seen as driving the thirst for computation and the need for infrastructure in the research Al community. Without a doubt, Large Language Models (LLMs) and foundation models have driven a lot of the need for scale. However, the need for scale goes beyond generative Al. There are a lot of other areas in Al that are progressing very fast and driving the need for resources. One area is machine learning applications and causal inference. Another area is large knowledge graphs, which is a multi-billion-dollar industry. Another important area is constraint reasoning and optimization at scale. Al on the edge, mobile, and IoT is pushing data collection at an unprecedented scale with significant onboard processing, with the scale spanning the gamut from the edge to the server. All these areas are moving very fast and pushing scale in Al, not just Generative Al.\nWorld governments and private entities are investing in Al at billion-dollar levels, which presents a challenge due to a limited talent pool and resources. Economic growth and security may hinge on advances in Al, so it is critical to scale up the talent pool and workforce significantly. The infrastructure available for Al research must be concomitant with the desire to lead in and advance Al innovation.\nThe need to scale in Al concerns many dimensions. Compute resources are one. Data is another important dimension, with higher data quality being another. More Al researchers and a much larger workforce are needed, so the training and education capabilities need to be scaled up. The need to scale in Al is in all those dimensions.\nAcademic Al researchers face significant barriers to accessing resources. New Al faculty need more resources to carry out cutting-edge research with a group of students. For more senior researchers and students, industry engagement provides access to significant computing power, unique data, the latest models, and other Al resources. Faculty may have dual appointments, where they spend a day a week or more working in the industry, or they might take extended leaves of absence. Universities accommodate this for retention reasons. If universities had better infrastructure and greater access to data and resources, there could be a healthier ecosystem and better options for academic researchers.\nMany industry organizations are investing heavily in Al resources. These investments tend to be concentrated in a handful of elite academic institutions but need to be more broadly distributed. These resources are insufficient because the field of Al is vast. We provide two points of reference taken from the 2024 Al Index Report [Maslej et al 2024]:\n\u2022 In 2021 alone, there were over 49,000 publications in Al from the US alone\n\u2022 In 2023, over 63,000 people attended major Al conferences\nThe unprecedented scale needed in Al research (see finding #1) and the community's growth have created an unprecedented need for research resources. However, national investments in Al resources have not been concomitant with this growth [NITRD 2024].\nThe available resources are shaping the areas of experimental Al research pursued by academics. Al researchers have come to accept that they have access to limited resources and are resigned to doing feasible work in a resource-constrained research environment. Today, many Al researchers make do with the resources they can obtain with reasonable effort. When a researcher runs their own computing cluster, the resource is always available, allowing their students to reliably do their work and graduate on time. Their research agendas are steered towards what can be achieved with those available resources. Local universities provide some infrastructure resources that Al researchers use, but they must often obtain and set up their own resources at many universities. In some cases, affinity groups of researchers pool their resources together and collaborate to maintain them. For some key resources, Al researchers rely on the goodwill of the industry to provide them at little or no cost, particularly cloud resources, software services, open models, etc.\nThe effort and learning curve required to find or set up the best infrastructure for a large Al experiment may be significant. As a result, existing cyberinfrastructure resources are less likely to be used, as investigator-owned or local resources are perceived as easier to access and more manageable.\nThe aggressive tempo and high stakes of Al conference publications set the pace for the resource needs and setup that academic Al research groups would require. This makes the requirements of Al research very different from those of other computational sciences, where new findings are submitted to journals without deadlines or time constraints.\nAl conferences have become highly regarded and are the top targets for academic papers. Key Al conferences are now rated at the level of Nature and the New England Journal of Medicine and higher than Science and other scientific journals. In Al, as in computer science, most publications are in conferences, which can be very competitive and have very low acceptance rates.\nPaper submissions to top Al conferences have an aggressive tempo of deadline after deadline. Preparing for that tempo and submitting high-quality papers that will be accepted means running experiments continuously and in sufficient numbers to produce significant findings. Some of the experiments will fail, requiring a rethink of the general approach. In this mode of operation, the ownership of resources may be seen as desirable because availability is guaranteed. Some Al groups that pool resources together have agreed to prioritize different areas based on the conference deadlines. In this context, current cyberinfrastructure resources that use priority queues and uniform policies for competing allocation requests are seen as potentially unreliable, particularly during key conference submission deadlines. To rely on national resources, Al researchers would need some quality of service guarantees that would make it an attractive proposition.\nAn important aspect of this is that the kind of Al research requiring more resources tends to be highly exploratory, and the methods may be immature by design and become better understood as the experiments proceed. The way current cyberinfrastructure resources are accessed is by writing a competitive proposal that lays out what will be done and what the expected outcomes are. This may be very difficult to predict, and in preparing proposals for allocations of cyberinfrastructure resources, Al researchers might be at a disadvantage in that they need a lot of flexibility and adaptation to whatever the resource needs may be for an experiment.\nWorkshop participants recognized that the current situation burdens graduate students tremendously. This is a significant problem. Many research groups set up and manage their computing clusters, for which graduate students are responsible. While acknowledging that they are ultimately the primary beneficiaries of those resources, these graduate students take on system administration tasks. Clearly, they are not doing exciting work on scalable computing but rather low-level tasks such as dealing with a failing machine, upgrading software libraries, or performing other routine maintenance. This detracts from their research and does not add valuable skills to their education. These tasks could be handled more effectively by cyberinfrastructure providers or research engineers. In contrast, researchers in industry running similar experiments do not have to do that kind of work.\nOnce students make the investment needed to run their research in local resources, there is no easy path to using current national cyberinfrastructure resources. There is a substantial learning curve involved, and this investment in connecting with current cyberinfrastructure resources is not typically perceived as being worth the effort. Ideally, Al research groups should be able to easily move their software stack and experimental setup from local to national resources.\nAccess to resources for Al could be more evenly distributed across the country. This includes smaller Al research groups with limited budgets for resources, universities with one or two Al faculty, universities that don't have PhD programs, and universities with limited campus infrastructure. For those universities, the NSF often provides unique grant mechanisms for shared resources. The experience and coaching needed to use national infrastructure resources are usually inaccessible. Unfortunately, it is becoming harder for everyone in these institutions to compete for access to national resources and Al research funding.\nThe training and reach for current cyberinfrastructure resources are also uneven. Many Al researchers need to be made aware of the national resources available, and others are often uncomfortable competing for allocation. Most have not been willing to invest in learning how to use national resources.\nMany Al faculty have dual appointments that allow them to spend some portion of their time in industry. This enables them to access unique datasets, significant compute resources, and proprietary models. Many take this approach to address the limited availability of resources in academia. Although they are often not allowed to publish about their research in the industry, they gain an understanding of the kinds of results that are possible with significant Al resources.\nHPC providers typically perceive Al as \u201cweighted by usage,\u201d with a lot of attention on the needs of large model training runs that consume enormous resources. In contrast, a large proportion of academic Al research is conducted by students and focuses on well-scoped, publishable chunks of work that require compute resources, but not at that large scale. As a result, much of the innovative research exists in the long tail with smaller computing requirements. In essence, there are many academic Al projects that each require a small or medium amount of computing and a smaller number that would require substantial amounts of computing resources. This could lead to differences in perceptions of the cost/benefit tradeoffs since, for the smaller efforts, the overhead of obtaining approval for and setting up the use of HPC resources is proportionally higher. Additionally, the approval process and the priority status of these smaller jobs in the HPC queues may lead to delays in obtaining results in time for conference submission deadlines.\nAl researchers would also prefer a flexible and dynamically updated software stack, rather than the cyberinfrastructure practice of preset operational settings that are upgraded slowly.\nParticipants agreed that Al has unique research resource needs. They raised several characteristics of Al research that would lead to new cyber-infrastructure requirements. Other scientific domains, such as the life sciences, physical sciences, and social sciences, have also required new capabilities and setups in cyberinfrastructure. Al also brings new requirements that have yet to be identified. For example, given that Al researchers are computer science researchers, they would likely expect to access the entire software stack to customize it for their experiments.\nThe existing national infrastructure focuses on computational sciences and engineering, with specific requirements and use cases. Al has different needs and will present new use cases and applications. For example, many Al researchers address various aspects of user interaction, such as intelligent user interfaces, affective computing to understand the user's emotional state, social robotics, and closed-loop science experiments where sensing and execution are interleaved. These areas are doing what could be called \u201cAl in the wild,\u201d where Al needs to provide dynamic responses in real-time, since a user state (frustrated, stressed) throughout the interactions, protect sensitive data, personalize the interactions to individuals, and mind guardrails as well as domain constraints. These research areas will lead to new requirements for national infrastructure.\nAnother novel aspect of Al is that data is vital fuel and does not come from one focal source or domain. Some Al researchers develop general approaches that are evaluated with data from diverse sources and several domains. A lot of work is involved in converting the data to uniform schemas and representations, checking the integrity and quality of the data, and other data preparation tasks. The term \u201cAl-ready\u201d has been used for data and other resources to indicate the minimal effort required to use large amounts of data for Al research. As with any data analysis or scientific research, the better the data, the better the models and the better the results. However, Al researchers use data from very different domains and, in many cases, from sources of varying quality and continuously increasing diversity and volume. In addition, there is less of a tradition in Al of data curation and quality, mainly if Al researchers use a dataset for testing but are unfamiliar with the particulars of the data. Ensuring the creation of high-fidelity datasets with appropriate tools to improve data quality and create Al-ready data should be a requirement for Al research resources.\nHybrid architectures will also be a new requirement. For example, GPUs connected to CPUs in hybrid resources, along with proper connectivity between systems, are crucial for robotics simulations and scientific applications. Al workloads require low latency, high-bandwidth interconnects, and high-performance I/O systems.\nAl research will also require a high-performance, fast-updating Python stack, perhaps different from the traditional stacks in other science and engineering domains."}, {"title": "3. Challenges", "content": "Current Al workflows have immediate resource needs, which result from the astounding effectiveness of deep learning and transformer architecture and their applications to many domains and problems. This section overviews current prototypical Al research workflows that require the most computational resources and other ambitious areas of Al research that could be significantly advanced with more resources."}, {"title": "3.1 Current Prototypical Al Research Workflows", "content": "Al researchers report their work on a conference publishing system that tends to incentivize many relatively small, exploratory research projects, each resulting in an 8-page conference paper. Most of this work is done by a faculty, one or more students, or a small group of collaborating researchers. This work typically involves many small projects and a few subsequent larger projects. It is typical for a researcher to prototype a small project or a new method using smaller computational resources and then turn to large-scale computation when running more extensive experiments to apply, compare, scale up, or benchmark their new approach.\nIn all these Al workflows, the process must always be iterated many times for model exploration, hyperparameter tuning, debugging, and systematic experimentation.\nTypical Al workflows in machine learning can be described as:\n\u2022 Exploring algorithmic and architectural novelty: Many Al researchers are focused on creating new methods and approaches to machine learning (ML). Thus, they frequently explore unproven computational methods or even heterogeneous systems incorporating new approaches to software or hardware systems.\n\u2022 Focusing on data novelty and data quality: Another common theme in Al research is the focus on working with novel kinds of data (including but also going beyond commonly used well-curated public image and text datasets), including data that may have special restrictions on use, such as education data, health data, and other sensitive data.\n\u2022 Using very large datasets: It is increasingly common for teams of Al researchers to work with large, shared data sets. This requires shared infrastructure for managed access to data and robust, well-documented, shareable, adaptable workflows using standard software stacks, often with some customization. They also need access to substantial HPC and GPU resources beyond those available within individual research labs. At some institutions, such resources are provided as part of campus infrastructure. However, scaling up promising exploratory projects initiated using institutional resources requires computational resources well beyond those available at individual universities. This problem is even more acute at smaller institutions.\n\u2022 Comparing the performance of different approaches: Almost every publication compares a new approach with existing algorithms or architectures, but it can be challenging. These comparisons require access to software, benchmark datasets, configurations, and other details that are only sometimes easy to reconstruct from prior publications as they appear in the literature. Each approach could require different resources that may not be easily accessible to the researcher.\nFor generative Al, some of the more common research workflows that have become widespread, in decreasing order of resource requirements, are the following:\n\u2022 Training or pre-training large models with a large amount of data leads to what is known as \"foundation models.\u201d These include large language models (LLMs), computer vision, and multimedia models. However, foundation models can also be created for specific types of data (e.g., foundation models for time series data or pathology images). Pre-training these models from scratch is the most expensive workflow; it requires a cluster of accelerators large enough to hold the model and gradients. With modern optimization methods, duplicating this usually leads to nearly linear speedups in training. While large players in the industry distribute open-weight models, training models is a very important research activity. Some examples of why this is important include the Phi models, which are relatively small models that Microsoft has trained on small, well-curated corpora: this is an example of a line of research that could help democratize Al but requires extensive computing resources to explore systematically. Special foundation models have also been trained using self-supervised techniques (much like LLMs) for scientific datasets, like biological sequences and pathology images, and similar models have been proposed for other domains.\n\u2022 Analyzing pre-trained models. An increasing amount of Al research reuses Al models pre-trained on large data, often called \u201cfoundation models.\" Because training models do pre-training to run on large GPU resources, for some experiments, researchers require similar GPU resources to apply the pre-trained models, even if their experiments are at a smaller scale. This has led to a situation where even exploratory work, considered \u201cvery small,\u201d requires some nontrivial baseline GPU resources.\n\u2022 Fine-tuning or compressing foundation models or training a new architecture that builds on existing models is less expensive but still requires the ability to perform inference (plus holding gradients, etc.) at scale. By compressing models, we mean techniques such as quantization or sparsification of weights.\n\u2022 Interrogating or probing existing models through inferences. This is generally the least expensive operation, but several variants must be supported. A popular application is measuring model bias through carefully designed benchmark prompts. One variant is online, interactive inference with a large model that can be accessed through a service API. In contrast, distilling models (to faster, smaller models designed for a specific task) requires large-scale batch inference. Large-scale batch inference might be needed for active learning methods as well.\n\u2022 Using existing models in combination with structured knowledge. Models may be probed through prompts that include structured knowledge. For example, large amounts of documents in a domain may be indexed in a database through vector embeddings and retrieved based on their relevance to a model prompt, which results in improved performance. In some cases, graphs consisting of entities of interest and their relationships can be indexed in a database and used to augment model prompts. The combination of graphs and vector embeddings for indexing and prompting is also becoming more widespread.\n\u2022 Combining agent tasking with model prompting. Al systems can be designed to use agents or services when high reliability is desired while resorting to generative Al models for other tasks. These Al systems combine these Al technologies by statically or dynamically orchestrating access to distributed resources.\nCurrently, these are all common workflows that present many research opportunities and do not require significant resources. In the sections below, we discuss more foundational and long-term research avenues in generative Al that are needed to understand and extend this paradigm."}, {"title": "3.2 Al Research That Would Be Enabled by Significant National Resources", "content": "This section gives an overview of representative areas of fundamental Al research that could be tackled with more access to significant research resources. It is not an exhaustive collection but a compilation indicative of Al research that is possible with better access to resources. [Gil and Selman 2019] describe critical areas of Al research.\nIntelligence has many facets that together can result in very powerful Al systems. Further progress will require foundational advances in Al. Of particular interest are principled approaches to integrating data-driven and knowledge-driven Al; closed-loop integration of knowledge, observation, learning, inference, planning, experimentation, and interaction; frameworks for forming, incentivizing, coordinating, human, Al, and human-Al teams, among others. These advances will require computing resources, rich data sets, knowledge bases, and complex Al workflows for learning, optimization, inference, experimentation, and interaction with the physical world and with humans at scale to explore alternative Al architectures and Al algorithms. Reference architectures would enable progress where a broad range of algorithms for specific aspects of Al can be integrated and evaluated at scale.\nWe mentioned earlier prototypical workflows for generative Al commonly used for research. Still, fundamental research must be enabled to understand this paradigm and discover new approaches and future Al architectures. Indeed, foundation models are very broadly helpful but not well understood. Without significant computing resources, tasks that can be performed only with API access to models (e.g., new prompting methods) dominate the academic literature. Many questions of crucial scientific importance remain beyond the scope of most academics because they are impossible to answer by simply fine-tuning and prompting existing models. A few of these questions are:\n\u2022 What features of a deep learning architecture result in more capable and powerful models? How can we analyze the weights and activations of large models? What are alternative architectures to transformers? While a handful of general-purpose open-weight models can be extended by academia, these remain difficult to experiment with at scale without access to accelerators similar in size and performance to the ones used to train the model. Critical research questions that require significant computational resources are underexplored.\n\u2022 Specific parts of a pre-training corpus are associated with the emergence of specific abilities (e.g., reasoning, common sense, etc.) Can the scaling laws for training be refined by discovering how different sections of the sub-corpora could be used? This understanding would facilitate foundation models for new subdomains (e.g., biological sequence modeling, reinforcement learning for complex simulated systems, etc.).\n\u2022 How is knowledge in a corpus naturally represented in foundation models, and how can that knowledge be most effectively queried, updated, or transferred between models? How different is the representation of knowledge between models trained on variants of the same corpus? These questions are difficult to answer using either small models (which may not be representative of larger foundation models) or models that have been fine-tuned to support specific ways of being queried. Generally, commercial models are fine-tuned for knowledge access via closed-book question-answering.\n\u2022 What general capabilities currently not supported by foundation models can be enabled by large-scale fine-tuning on plentifully available data? Examples of such capabilities might be on-the-fly data integration over large amounts of heterogeneous data, student modeling for refining education, or multiple-sequence alignment in biology \u2013 all of which have been recently enabled by longer contexts in some recent models. Enabling these would require an expensive process of training large models with many large contexts.\nA widely used form of knowledge representation is knowledge graphs, whose nodes are entities of interest linked by useful relationships among them. Knowledge graphs capture useful domain knowledge that can be used to answer queries that require reasoning and inference. Although graph knowledge bases can support these inferences efficiently, more expressive forms of knowledge require reasoning algorithms that can get computationally expensive. Reasoning with very large knowledge graphs and incorporating more expressive knowledge requires scaling up computing resources and distributed reasoning.\nFor some Al applications, important knowledge is cast as constraints over sets of variables interacting with one another. Reasoning with constraints at scale is another important area of Al research. A particular case are temporal and spatial constraints, which are pervasive in spatial computing and scientific applications.\nProgress in artificial intelligence, cognitive science, and related areas requires developing and experimenting with computational models of human intelligent behavior at scale. In addition to increasing our understanding of human intelligence, this research can inspire new Al approaches and architectures. For example, research on understanding language acquisition by humans and machines would benefit from collecting large multimodal data sets collected in naturalistic settings and by ML models and training ML models on such data. A wealth of data, including neuroscience and behavioral data, could shed light on key aspects of human intelligence. However, the complexity of developing accurate models with this data makes this research extraordinarily challenging and requires significant computational resources.\nAl offers unprecedented opportunities to dramatically accelerate all aspects of science: generating hypotheses, prioritizing, optimizing, and executing experiments, integrating data, models, and simulations across disparate data modalities and scales, drawing inferences and constructing explanations, and organizing and orchestrating collaboration. However, realizing the promise and potential of Al to accelerate science presents a grand challenge for Al. Addressing this Al grand challenge of accelerating science requires significant investments in infrastructure to support collaborative research by interdisciplinary teams aimed at concerted advances across multiple areas of Al and the deployment and evaluation of Al solutions in the context of pressing problems of scientific and societal importance. Some examples include improving individual and population health outcomes, coping with climate change, discovering novel materials, and deciphering life's and the universe's mysteries.\nFoundational advances in Al have the potential to dramatically transform Al applications across all areas of human endeavor, such as addressing social disparities, optimizing education and training of the next-generation workforce, combating misinformation, securing critical infrastructure, and enhancing participatory democracy."}, {"title": "4. Requirements for Al Research Resources", "content": "This section gives an overview of the requirements raised by Al researchers during the discussions, summarized in Table I. While it is not a systematic or comprehensive list, it provides a good starting point for future efforts to gather requirements for Al research resources."}, {"title": "4.1 Overall Considerations", "content": "Only some academic groups currently have access to computing or data resources at the scale concentrated in a few large companies. Consequently, the only way for most academics to work on Al research at scale is to collaborate with companies, typically on problems of interest to the companies. This skews the research focus toward issues of immediate interest to the industry. Significant infrastructure investments would enable academic research groups to work on Al research at scale, on a diverse range of Al problems and applications beyond those of immediate interest to industry, train the next generation of Al researchers, and propel Al to new heights.\nMost university researchers and students have limited or no access to computing resources, often large datasets and other resources. By addressing the specific needs of the Al research community and fostering collaboration between academia, industry, and government, a more robust and inclusive environment for Al innovation can emerge.\nOpen science practices should be adopted to promote shared and persistent community resources. Adherence to best practices such as the FAIR (Findable, Accessible, Interoperable, and Reusable) principles for research artifacts (datasets, software, workflows, etc.) should be fostered. Provisions should be made to facilitate compliance with federal agencies' data and resource-sharing requirements. Data Management Plans should be written to draw from and contribute to community Al resources."}, {"title": "4.2 Requirements for Computing Resources", "content": "In Al research, hardware accelerators such as GPUs are required for deep learning and transformer architectures. NVIDIA is typically preferred since most Al software libraries run on its hardware. Most universities have the older A100s, some have the more modern H100s, and all are awaiting the newest NVIDIA Blackwell GPU.\nAlthough GPUs are expensive, significant associated costs must be considered. GPU units need to be connected through high-bandwidth networks, and they also require significant power and cooling. Experienced technical staff should be available to address hardware failures, software upgrades, and other issues.\nPre-training large models requires significant computing resources. Ideally, hundreds, if not thousands, of GPUs are needed to create these models (the industry's numbers range in the hundreds of thousands) since this requires iterative exploration, and models do not always converge. Efficient network bandwidth is also critical for efficient computations in large amounts of GPUs. Data storage is also needed.\nThousands of Al researchers and students focus on fine-tuning or doing inference with an existing foundation model; in this case, a typical requirement might be 4-8 H100s. An architecture close to the model's training is needed to run a foundation model. While porting weights to different machine architectures is technically possible, it is tough to make work in practice-more challenging than many other tasks involving large amounts of computing. This is because ML/AI systems are not well-understood, carefully designed systems but have \u201cevolved\" by many incremental changes over time, and there is little detailed understanding of how small perturbations will affect performance. For this reason, most Al researchers' hardware needs are for accelerators similar in size and performance to the ones used to pre-train a model.\nIt is important to emphasize that accelerator hardware is needed not only for research on foundation models but also for any fundamental research using such models. Few researchers will need to use hundreds of GPUs for weeks, but many will require access to interactive environments with 4-8 GPUs. Although the few tremendous model training runs that each consumes massive GPU resources are essential and eye-catching, there is also a significant need for small-scale GPU use, with many projects requiring resources on the scale of a single node with a few GPU cards, even though they might not saturate that resource.\nBecause Al research is iterative and exploratory, there are better fits for per-project capacity approvals typical of current national HPC resources. National-level HPC resources ask for approval for each project, creating a lot of additional review periods and the possibility of rejection, which may not be a good fit for frequent, short, exploratory Al methods research. Al researchers prefer the approach of university-level resources, which are typically allocated to labs for more extended periods (e.g., for the lifetime of machines paid for by grants from a PI's lab). It may be appropriate to structure national Al resources to better match the Al research cycle by approving capacity use in larger units, e.g., for a more extensive research theme rather than an individual project or for a Pl's lab for a while.\nExpert support is also needed to deploy Al experiments in HPC resources, which is why Al researchers prefer using campus resources. In addition, there is a need for local advocates for national resources who would play an analogous role to the local HPC specialists at the university level, helping with both training and advocacy (in both directions). The NSF has previously run a \"campus champion\u201d program, which may be a good model for addressing the needs of Al researchers to have HPC expertise readily available.\nWith the emergence of quantum machine learning, research groups need access to hybrid quantum-classical infrastructure in the near term and quantum computing infrastructure in the long term. National resources are required because no academic institution can afford its own quantum computing infrastructure. National investments in quantum computing infrastructure have been disconnected mainly from national investments in Al. Augmenting the Al infrastructure with quantum computing capabilities would be critical to progress in quantum Al and quantum machine learning.\""}, {"title": "4.3 Requirements for Data", "content": "To enable experimental Al research at scale, it is crucial to facilitate and incentivize the availability and accessibility of large amounts of shared data across the nation in different domains of interest. To make data available as a national resource, best practices for data sharing should be adopted, such as the FAIR data principles and other best practices for open science and digital scholarship.\nAlthough not yet well defined, the concept of \u201cAl-ready data\" captures the idea that Al researchers typically gravitate towards working with datasets that are easier to use in an Al experiment, for example, because of their high quality and easy accessibility. Ideally, easy-to-use tools that provide intelligent assistance would be available to curate datasets and provide appropriate machine-readable metadata, data characteristics and properties, provenance, and other helpful documentation.\nEnabling the efficient creation of Al-ready data would lead to a new generation of data management tools to automate many data integration, data management, and data sharing processes. This would bring data-sharing practices to a new level, potentially increasing data sharing and reuse in all engineering and science disciplines.\nThere should be efforts to identify and disseminate exciting datasets or expose essential research challenges. For example, benchmark datasets have driven significant advances in some areas of Al. Barriers should be identified for the reuse of critical datasets. For example, datasets with sensitive information could be prepared with the right pre-processing and use agreements so the community could effectively reuse them.\nSignificant amounts of government data (notable through data.gov) and many other datasets are made public and could become part of national Al research resources. Unfortunately, most require significant effort to find, access, or reuse.\nData needed for Al has a myriad of governance issues. At universities, research datasets increasingly default to being deemed by universities as imposing harm to the university if not protected. This conservative approach to research data will impede Al research on a national Al resource. Community governance models and Data Use Agreements (DUAs) could be negotiated for university-held Al critical datasets to allow the datasets to be managed on national resources for Al use.\nDespite these obstacles, it is increasingly common for research groups across multiple institutions to collaboratively develop, share, evaluate, and deploy Al-powered scientific workflows to large, shared data sets to address scientific questions in specific domains. Consider, for example, the NSF National Synthesis Center for Emergence in Cellular and Molecular Sciences, whose mission is to support community-scale Al-powered integrative analyses of many publicly available data sets to answer the most fundamental questions in molecular and cellular biology. Such large-scale efforts require not only substantial hardware resources for data-and-computation-intensive ML but also software infrastructure for managing data access, data use agreements, resource allocation, supporting shared development, testing, and deployment of ML-powered data analysis workflows among an extensive network of participants, and across multiple projects involving participants drawn from dozens of institutions. While software like CyVerse offers useful capabilities, extensive development is needed to support emerging collaborative Al-powered data-intensive research use cases. Finally, given participating researchers' diverse backgrounds and expertise, such infrastructure must offer training and research support services (e.g., AI/ML support for science).\nLarge amounts of data are more representative of complex phenomena and increase the effectiveness of Al algorithms. However, the difficulties and cost of data integration are often barriers to successful Al research and applications. Data integration remains a challenge"}]}