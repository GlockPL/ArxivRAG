{"title": "AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion Model", "authors": ["Zhenyu Yan", "Qingqing Fang", "Wenxi Lv", "Qinliang Su"], "abstract": "Anomaly detection is a critical task in industrial manufacturing, aiming to identify defective parts of products. Most industrial anomaly detection methods assume the availability of sufficient normal data for training. This assumption may not hold true due to the cost of labeling or data privacy policies. Additionally, mainstream methods require training bespoke models for different objects, which incurs heavy costs and lacks flexibility in practice. To address these issues, we seek help from Stable Diffusion (SD) model due to its capability of zero/few-shot inpainting, which can be leveraged to inpaint anomalous regions as normal. In this paper, a few-shot multi-class anomaly detection framework that adopts Stable Diffusion model is proposed, named AnomalySD. To adapt SD to anomaly detection task, we design different hierarchical text descriptions and the foreground mask mechanism for fine-tuning SD. In the inference stage, to accurately mask anomalous regions for inpainting, we propose multi-scale mask strategy and prototype-guided mask strategy to handle diverse anomalous regions. Hierarchical text prompts are also utilized to guide the process of inpainting in the inference stage. The anomaly score is estimated based on inpainting result of all masks. Extensive experiments on the MVTec-AD and VisA datasets demonstrate the superiority of our approach. We achieved anomaly classification and segmentation results of 93.6%/94.8% AU-ROC on the MVTec-AD dataset and 86.1%/96.5% AUROC on the VisA dataset under multi-class and one-shot settings.", "sections": [{"title": "1 INTRODUCTION", "content": "Anomaly detection is a critical computer vision task in industrial inspection automation. It aims to classify and localize the defects in industrial products, specifically to predict whether an image or pixel is normal or abnormal [3]. Due to the scarcity of anomalies, existing methods typically assume that it is possible to collect training data from only normal samples in the target domains. Therefore, existing mainstream anomaly detection methods mainly follow the unsupervised learning manner and can be divided into two paradigms, i.e.,reconstruction-based [18, 34, 38] and embedding-based methods [5, 6, 25]. Reconstruction-based methods mainly use generative models such as Autoencoders (AE) [27] or Generative Adversarial Networks (GAN) [11] to learn to reconstruct normal images and assume large reconstruction errors when reconstructing anomalies.\nWhile embedding-based methods aim to learn an embedding neural network to capture embeddings of normal patterns and compress them into a compact embedding space [6, 25].\nAlthough the astonishing performance achieved by previous methods, most of them assume that there are hundreds of normal images available for training. But in the real world, this assumption cannot always be fulfilled. It is likely that there are only a few normal data available due to the high cost of labeling or data privacy policies, which is called the few-normal-shots setting. Under such a scenario, performance degeneration of traditional anomaly detection methods is observed as they require abundant normal data to well capture the normal pattern [30]. Parallel to this, the bulk of existing works follow the one-for-one paradigm as shown in Figure 1(a), which needs to train a bespoke model for each category. This paradigm results in heavy computational and memory costs and more resources are required to store different models. Moreover, the one-for-one paradigm is not flexible for real-world applications. In practice, different components may be produced on the same industrial assembly line, which needs to switch different models to detect different anomalies or deploy multiple networks with different weights but the same architecture. The additional expenses caused by the lack of flexibility are completely prodigal.\nThe issues mentioned above have been already noticed in the most current study. Given only a few normal data, enlarging the training dataset [16, 33] and patch distribution modeling [26, 30] are promising approaches. GraphCore [33] directly enlarges the normal feature bank through data augmentation and trains a graph neural network on it to figure out anomalies. RegAD [16] is based on contrastive learning to learn the matching mechanism, which is"}, {"title": "2 RELATED WORK", "content": "Anomaly Detection. The mainstream anomaly detection methods can be divided into two trends: embbeding-based methods, reconstruction-based methods. Embedding-based methods primarily detect anomalous samples in the feature space. Most of these methods utilize networks pre-trained on ImageNet [9] for feature extraction and then calculate the anomaly score by measuring the distance between anomalous and normal samples in the feature space [5, 6, 25]. While some embbeding-based methods employ knowledge distillation to detect anomalies based on the differences between teacher and student networks [4, 8]. Reconstruction-based methods primarily aim to train the model to learn the distribution of patterns in normal samples. AE-based methods [10, 41] and inpainting methods [18, 34, 38] are both based on the assumption that the model can effectively reconstruct normal images but fail with anomalous images, resulting in large reconstruction errors. In the case of VAE-based generative models [7, 21] learn the distribution of normal in the latent space. Anomaly estimation is carried out by assessing the log-likelihood gap between distributions. For GAN-based generative models [1, 2, 14, 19, 28, 29], the discriminator compares the dissimilarity between test images and images randomly generated by the generator as a criterion for anomaly measurement.\nFew-Shot Anomaly Detection. Few-shot anomaly detection is developed for the situation where only a few normal data are available. TDG [30] proposed to leverage a hierarchical generative model that learns the multi-scale patch distribution of each support image. DiffNet [26] normalizing flow to estimate the density of features extracted by pre-trained networks. To compensate for the lack of training data, RegAD [16] introduced additional datasets to learn the matching mechanism through contrastive learning and then detect anomalies by different matching behaviors of samples. Although these methods can handle the few-shot anomaly detection task, none of them take the multi-class setting into consideration. Recently, WinCLIP [17] revealed the power of pre-trained vision-language model in few-shot anomaly detection task. It divides the image into multi-scale patches and utilizes CLIP [22] to calculate the distance between patches and the designed description of normality and anomaly as the anomaly score. Thanks to the generalization ability of CLIP [22], WinCLIP also has the ability to handle multi-class setting."}, {"title": "Multi-Class Anomaly Detection", "content": "Multi-class anomaly detection methods aim to develop a unified model to detect anomalies of different categories to save computational resources. Most of them are based on the reconstruction-based paradigm [12, 35, 36]. UniAD [36] adopted Transformer architecture to reconstruct image tokens extracted by pre-trained networks and proposed neighbor masked attention mechanism, which prevents the image token from seeing itself and its similar neighbor, to alleviate the \"identical shortcut\". PMAD [35] deems that the objective of reconstruction error results in more severe \"identical shortcut\" issue, they use Masked Autoencoders architecture to predict the masked image token by unmasked image tokens. The uncertainty of predicted tokens measured by cross-entropy is used as anomaly score. Due to the diffusion model's powerful capability of image generation, DiAD [12] adopted the latent diffusion model to reconstruct normal images and calculate the anomaly score in feature space. However, all of them require plenty of normal images to capture normal patterns while our method needs only a few normal images for fine-tuning."}, {"title": "Anomaly Detection Based on Diffusion model", "content": "Diffusion models [13, 31] have garnered attention due to their powerful generative capabilities, aiming to train the model to predict the amount of random noise added to the data and subsequently denoising and reconstructing the data. Stable Diffusion Model (SD) [23] introduces condition through cross-attention, guiding the noise transfer in the generation phase towards the desired data distribution. In the field of anomaly detection, AnoDDPM [32] has made initial attempts to apply diffusion models in reconstructing medical lesions within the brain. DiffusionAD [39] comprises a reconstruction network and a segmentation subnetwork, using minimal noise to guide the reconstruction network in denoising and reconstructing synthetic anomalies into normal images. Anomaly Diffusion [15] leverages the potent generative capabilities of Diffusion to learn and generate synthetic samples from a small set of test anomalies. However, current Diffusion-based anomaly detection methods primarily focus on denoising without sufficient exploration of how to systematically and controllably reconstruct anomalies into normal samples."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 Denoising Diffusion Probabilistic Model", "content": "The Denoising Diffusion Probabilistic Model (DDPM) [13] is a type of generative model inspired by the data diffusion process. DDPM consists of a forward diffusion process and a reverse denoising process. In the forward diffusion process, at every time step, we add a small noise into the data as $x_t = \\sqrt{1 - \\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon_t$, where $x_0$ denotes the original image; $\\epsilon_t \\sim N(0, I)$ represents the standard Gaussian noise; and $\\beta_t$ is used to control the noise strength added at the time step $t$, which is often a very small value from $\\epsilon (0, 1)$. It can be easily shown that $x_t$ can be directly obtained from $x_0$ as\n$x_t = x_0 \\sqrt{\\bar{\\alpha}_t} + \\epsilon_t \\sqrt{1 - \\bar{\\alpha}_t}$,\t(1)\nwhere $\\bar{\\alpha}_t = \\Pi_{i=1}^t \\alpha_i$ with $\\alpha_i \\approx (1 - \\beta_i)$. In DDPM, it seeks to learn the reverse process of the forward diffusion process by learning a model to predict $x_{t-1}$ solely based on $x_t$. It is shown in DDPM that $x_{t-1}$ can be predicted as below\n$\\tilde{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(\\frac{x_t - \\beta_t \\epsilon_\\theta (x_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}}) + \\sqrt{\\beta_t} \\tilde{\\epsilon}_t$,\t\t\t\t\t(2)\nwhere $\\eta \\sim N(0, I)$ is another Gaussian noise and $\\beta_t = \\frac{\\beta_t}{1-\\alpha_t}$ ;\nand $\\epsilon_\\theta (x_t, t)$ denotes the prediction of noise $\\epsilon_t$ in (1), which in practice is realized by using a U-Net structured network [24]. The model parameter $\\theta$ is trained by minimizing the error between the predicted noise $\\epsilon_\\theta (x_t, t)$ and the truly added noise $\\epsilon_t$\n$L = E_{x_0\\sim q(x_0), \\epsilon \\sim N(0,I), t \\sim [1,T]} || \\epsilon - \\epsilon_\\theta (x_t, t) ||^2$,\t(3)\nwhere $T$ denotes the total number of steps used in the forward diffusion process."}, {"title": "3.2 Stable Diffusion Model", "content": "Based on DDPM, the Stable Diffusion Model(SD) [23] introduces a pre-trained AutoEncoder with the encoder $\\&(\\cdot)$ to compress image $x$ into latent representation $z = \\& (x)$ and the decoder $D(\\cdot)$ to recover the latent features to image $D(z)$. The adaptation of AutoEncoder helps SD to generate high-resolution images in good quality when the DDPM process is utilized in the latent space by replacing $x$ in Sec 3.1 with $z$. Besides, SD introduces the condition mechanisms to guide the generation of images by the cross-attention modules in the denoising U-Net. For the condition $y$, SD first encodes it by a pre-trained encoder $\\tau(\\cdot)$, like CLIP text encoder for text condition. Then, SD introduces the condition $y$ into the i-th intermediate layer of U-Net with a cross-attention mechanism\n$attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}}). V$,\t(4)\nwith $Q = W_Q^{(i)} \\epsilon_\\theta(z_t), K = W_K^{(i)} \\tau (y), V = W_V^{(i)} . \\tau (y)$, where $\\epsilon_\\theta^{(i)} (z_t)$ represents the flattened output from the intermediate layer i of denoising network $\\epsilon_\\theta(\\cdot)$, $W_Q^{(i)} \\in \\mathbb{R}^{d\\times d}, W_K^{(i)}, W_V^{(i)} \\in \\mathbb{R}^{d\\times d_\\tau}$ are learnable weight parameters. Using the attention mechanisms, conditional information like text prompt can be introduced to guide the denoising process, leading to the training objective function\n$L_{SD} = E_{\\&(x), y, \\epsilon \\sim N(0,I), t \\sim [1,T]} [|| \\epsilon - \\epsilon_\\theta(z_t, t, \\tau(y) ||^2]$.\t\t\t\t\t\t\t(5)\nIn our paper, the conditional information $y$ specifically refers to textual prompts and $\\tau(\\cdot)$ means the text encoder in CLIP. With the learned denoising network $\\epsilon_\\theta(z_t, t, \\tau(y))$ in SD, we first sample a standard Gaussian noise $z_T$ and then feed it into the denoising network. After many steps of denoising, a denoised latent representation $z_0$ is obtained, which is then passed into the pre-trained decoder $D(\\cdot)$ to produce the image $x$."}, {"title": "4 THE PROPOSED METHOD", "content": "For the few-shot anomaly detection and localization, we assume the availability of several normal images from multi-classes C:\n$X_N = \\{X_1, X_2, ..., X_c\\}$,\t(6)\n$X_c = \\{x_{c,1}, ..., x_{c,K}\\}$.\nThe collected dataset comprises various texture and object images from different classes in which the possible value c can be carpets, bottles, zippers and other categories. The number of images available for each class is limited, with only K shots available, with K"}, {"title": "4.1 Adapt SD to Anomaly Detection", "content": "Distinguishing from the SD for image generation tasks, adapting SD for anomaly detection requires to accurately inpaint anomalous areas into normal ones. To achieve this target, we specifically fine-tune the denoising network and decoder of VAE in SD for anomaly detection. With only few shots of normal samples from multi-classes, to obtain a unified model that can be applied to all categories, we specifically design mask $m$ and prompt $y$ to guide the fine-tuning process of the inpainting pipeline of SD.\nGiven an image $x \\in \\mathbb{R}^{3\\times H\\times W}$ and mask $m \\in \\{0,1\\}^{H\\times W}$, we first encode the original image $x$ and masked image $(1-m) x$ by the encoder of VAE in SD, get $z = \\&(x)$ and $z^0 = \\&((1 - m) x)$. Through the forward diffusion process, we can get the noisy latent feature of timestamp $t$ by:\n$z_t = z_0 \\sqrt{\\bar{\\alpha}_t} + \\epsilon_t\\sqrt{1 - \\bar{\\alpha}_t}$, $\\epsilon_t \\sim N(0, 1)$,\t(7)\nwhere $z_0 = z$ is the feature of timestamp 0. The mask $m$ can also be downsampled to the same size of $z$ and results as $\\tilde{m}$. By concatenate"}, {"title": "4.2 Inpainting Anomalous Areas in Images with Finetuned SD", "content": "For an image $x$, if there is a given mask $m$ which can mask most parts of the anomalous areas, we can utilize the fine-tuned SD to inpaint the masked area and obtain a inapinted image $\\hat{x}$. In the inference stage of fine-tuned SD, if we use standard Gausian noise $z \\sim N(0, I)$ as the starting step of the denoising process, the generated images could show significant variability comparing with the original ones, making them not suitable for anomaly detection and localization.\nTo address this issue, we control the added noise strength by setting the starting step of the denoising process at $T = \\lambda \\cdot T$, where $\\lambda \\in [0, 1]$. Given the latent feature $z = \\&(x)$, we get a noisy latent feature $\\bar{z}_\\lambda$ according to (1). Thus, in the inference stage, the initial input\n$\\tilde{z}_\\lambda = [\\bar{z}_\\lambda; z^0;\\tilde{m}]$\t(11)\nis calculated for the denoising network. After multiple iterations of denoising, the denoised latent feature can be obtained, which we feed into the decoder of VAE to output the final inpainted image $\\hat{x}$. As mentioned before, we assume that the mask $m$ can mask most areas of anomalies. However, the actual anomalous area is not known in real applications, thus the mask used for inpainting needs to be designed carefully so that the anomalous area can be mostly masked and inpainted into normal patterns. To achieve this target, we design two types of masks including multi-scale masks and prototype-guided masks for inpainting in the inference stage.\nProposals of Multi-scale Masks. Anomalies can appear in any area of the image of any size. In order to inpaint anomalous areas of different sizes and positions, we design multi-scale masks to mask the corresponding areas. Specifically, the origin image x can be divided into $k \\times k$ patches with $k \\in K = \\{1, 2, 4, 8\\}$. For every patch $p_{i,j}^{(k)}$ with $i, j \\in [1, k]$ in the $k \\times k$ squares, we can generate the corresponding mask $M_{i,j}^{(k)}$ to mask the patch and get the masked image. Then, the masked image and mask can be used for inpainting the area, and we can get the inpainted image $\\hat{x}_{i,j}^{(k)}$ .\nProposals of prototype-guided Masks. The multi-scale masks help inpaint images in different patch scales, but anomalous areas can cross different patches, thus there may still exist anomalous areas in the masked images, reducing the quality of inpainting into normal images. To solve the problem, we propose prototype masks. At the image level, every pixel only has 3 channels that are hard to be used for filtering anomalous pixels, so the pre-trained EfficientNet $\\phi(\\cdot)$ is employed for extracting different scale features from different layers. For any image $x$, we can extract its features to get\n$\\psi(x) = [\\phi^{(2)}(x); Upsample(\\phi^{(3)}(x))]$,\t(12)\nwhere $\\phi^{(2)}(\\cdot)$ and $\\phi^{(3)}(\\cdot)$ extract features from layers 2 and 3 from $\\phi(\\cdot)$ respectively. The feature from layer 3 is upsampled to the same size as that from layer 2 and the two features are combined by the operate concatenation [;] along the channel. For every category $c$, we get the features of few shot normal samples and form its corresponding normal prototype set:\n$P_c = \\{[\\psi(Aug(x))]_{h,w} | h \\in [1, H'], w \\in [1, W'], x \\in X_c\\}$,\t(13)\nin which H' and W' are the height and width of the feature map extract from layer 2, and Aug(.) augment the normal image by rotation to enrich the prototype bank. In the inference stage, for any test sample $x$ from category $c$, we can also get its corresponding $\\psi(x)$. Searching the normal prototype bank, an error map E measures the distance to the normal prototype can be obtained as\n$[E]_{h,w} = min_{f\\in P_c} ||[\\psi(x)]_{h,w} - f||^2$,\t(14)\nDue to the limited number of few-shot samples, E is not precise enough to predict anomaly, but it can be used for producing a mask of any size and shape that may contain the anomalous area."}, {"title": "4.3 Anomaly Score Estimation", "content": "For a test image $x$, we can get its corresponding inpainted image $\\hat{x}$ which are assumed to be recovered into normal ones. However, the difference between inpainted and original images is hard to measure at pixel level which only has 3 channels. Considering LPIPS loss used in fine-tuning the decoder of VAE, we also use features extracted from pre-trained AlexNet to measure the difference between $x$ and $\\hat{x}$. For the features extracted from layer $l$ of AlexNet, the distance of original and inpainted images in position (h, w) of corresponding feature maps can be calculated as\n$[D^{(l)}(x, \\hat{x})]_{h,w} = || w^{(l)} \\odot ([\\phi^{(l)} (x)]_{h,w} - [\\phi^{(l)} (\\hat{x})]_{h,w}) ||$.\t(16)\nWe upsample $D_l$ to the image size and add distances from all layers to get the score\n$D(x, \\hat{x}) = \\sum_l Upsample(D^{(l)}(x, \\hat{x})).\t(17)\nFor the multi-scale masks, in scale k, using every mask $M_{i,j}^{(k)}$ can get inpainted image $\\hat{x}_{i,j}^{(k)}$, thus distance map $D(x, \\hat{x}_{i,j}^{(k)})$ can be obtained according to (17). For the scale k, combining masks for different patches, we can get a score map\n$\\phi^{(k)} = \\sum_{i,j}D(x, \\hat{x}_{i,j}^{(k)})$.\t(18)\nAggregating different scale anomaly maps by harmonic mean, we can get the final anomaly map from multi-scale masks:\n$S_{ms} = |K| \\cdot [\\sum_{k\\in K} (\\phi^{(k)})^{-1}]^{-1}$t(19)\nFor the prototype-guided mask $M^{(p)}$ and inpainted image $\\hat{x}^{(p)}$, we can also get anomaly score according to (17) and mask $M^{(p)}$:\n$S_{pg} = M^{(p)} \\cdot D(x, \\hat{x}^{(p)})$.t(20)\nFinally, we can get the final anomaly score map by the combination of $S_{ms}$ and $S_{pg}$:\n$S_{map} = (1-\\alpha)S_{ms} + \\alpha S_{pg}$.\t(21)\nFor the image-level anomaly score, we use the maximum scores in $S_{map}$, thus we get $S_I$."}, {"title": "5 EXPERIMENT", "content": ""}, {"title": "5.1 Experimental Setups", "content": "Datasets. Our experiments are conducted on the MVTec-AD and VisA datasets, which simulate real-world industrial anomaly detection scenarios. The MVTec-AD [3] dataset consists of 10 object categories and 5 texture categories. The training set contains 3629 normal samples, while the test set comprises 1725 images with various anomaly types, including both normal and anomalous samples. The VisA dataset [42] comprises 10,821 high-resolution images, including 9,621 normal images and 1,200 abnormal images with 78 different types of anomalies. The dataset includes 12 distinct categories, broadly categorized into complex textures, multiple objects, and single objects.\nEvaluation metrics. Referring to previous work, in image-level anomaly detection, we utilize metrics such as the Area Under the Receiver Operating Characteristic Curve (AUROC), Average Precision (AUPR), and F1-max for better evaluation under situation of data imbalance. For pixel-level anomaly localization, we employ pixel-wise AUROC and F1-max, along with Per-Region Overlap (PRO) scores.\nImplementation Details. All samples from MVTec-AD and VisA datasets are scaled to 512 \u00d7 512. In the fine-tuning stage, data augmentations such as adjusting contrast brightness, scaling, and rotation are applied to the training samples to increase the diversity of few-shot samples. For the threshold $\\gamma$ used for IoU of training mask $m$ and object foreground $m_f$, we randomly set it to 0.0, 0.2, and 0.5 with equal probability. Adam optimizer is used and the learning rate is set to 1e-4. We train the denoising network for 4000 epochs and the decoder for 200 epochs on NVIDIA GeForce RTX 3090 with a batch size of 8. In the inference stage, the denoising step is set to 50, and the noise strength $\\lambda$ is specified for each category because the details of them are different. For the prototype-guided masks, we employ EfficientNet B6 as the feature extractor, and during the test we set the weight $\\alpha$ to 0.1 for multi-scale and prototype score map fusion. For the anomaly map, we smooth it by using a Gaussian blur filter with $\\sigma = 4$."}, {"title": "5.2 Anomaly Detection and Localization Results", "content": "Few-shot anomaly classification and segmentation. The results of anomaly detection and localization under zero-shot and few-normal-shot settings are in Table 1. In the table, we compared our experimental results with previous works on the average performance of all classes in MVTec and VisA. For zero-shot learning, the multi-scale masks and noise strength control proposed in our approach can also be employed, thus we compare with the original SD with designed text prompts to show the effectiveness of these proposed modules. It can be observed that even without fine-tuning, our method still significantly outperforms the original SD. In other"}, {"title": "5.3 Ablation Study", "content": "All of the ablation experiments are conducted on the MVTec dataset under 1-shot setting, we report the performance of image-level AUROC(i-AUROC) and pixel-level AUROC(p-AUROC)."}, {"title": "5.4 Conclusion", "content": "We propose a SD-based framework AnomalySD, which detects and localizes anomalies under few shot and multi-class settings. We introduce a combination of hierarchical text prompt and mask designs to adapt SD for anomaly detection and use multi-scale masks, prototype-guided masks to mask anomalies and restore them into normal patterns via finetuned SD. Our approach achieves competitive performance on the MVTec-AD and VisA datasets for few-shot multi-class anomaly detection. For further improvement, adaptive prompt learning and noise guidance is a promising direction to reduce the reliance on manually set prior information and transition to a model-adaptive learning process."}]}