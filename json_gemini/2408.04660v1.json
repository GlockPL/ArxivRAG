{"title": "XMainframe: A Large Language Model for Mainframe Modernization", "authors": ["Anh T. V. Dau", "Hieu Trung Dao", "Anh Tuan Nguyen", "Hieu Trung Tran", "Phong X. Nguyen", "Nghi D. Q. Bui"], "abstract": "Mainframe operating systems, despite their inception in the 1940s, continue to support critical sectors like finance and government. However, these systems are often viewed as outdated, requiring extensive maintenance and modernization. Addressing this challenge necessitates innovative tools that can understand and interact with legacy codebases. To this end, we introduce XMainframe, a state-of-the-art large language model (LLM) specifically designed with knowledge of mainframe legacy systems and COBOL codebases. Our solution involves the creation of an extensive data collection pipeline to produce high-quality training datasets, enhancing XMainframe 's performance in this specialized domain. Additionally, we present MainframeBench, a comprehensive benchmark for assessing mainframe knowledge, including multiple-choice questions, question answering, and COBOL code summarization. Our empirical evaluations demonstrate that XMainframe consistently outperforms existing state-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30% higher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the BLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times higher than GPT-3.5 on COBOL summarization. Our work highlights the potential of XMainframe to drive significant advancements in managing and modernizing legacy systems, thereby enhancing productivity and saving time for software developers.", "sections": [{"title": "Introduction", "content": "Large Language Models for code (CodeLLMs) excel in processing and understanding source code across various programming languages such as Python, C++, Java, C#, Rust, Go, etc., as well as descriptive texts Qin et al. [2023], Touvron et al. [2023], Roziere et al. [2023], Jiang et al. [2024], Team [2024], Manh et al. [2023], Zheng et al. [2024], Li et al. [2023], Wang et al. [2023b], Feng et al. [2020], Wang et al. [2021], Bui et al. [2023].. Their ability to recognize patterns, syntax, and semantics makes them highly effective at tasks such as code completion, bug detection, and generating human-readable explanations. These models can bridge the gap between code and documentation by comprehending and generating natural language descriptions\nMainframe Modernization: Mainframe software systems are crucial to the daily operations of many of the world's largest corporations, including numerous Fortune 1000 companies IBM. These systems are used extensively in domains such as banking, finance, and government, where they manage large-scale user bases and applications. Despite their origins in the 1950s, COBOL (Common Business Oriented Language) remains widely used in mainframe applications. It is estimated that over 220 billion lines of COBOL code are currently in use, with 1.5 billion lines written annually Taulli [2020]. Additionally, COBOL systems manage USD 3 trillion in commerce daily Cassel [2017]. However, the retirement of many COBOL developers and mainframe experts poses a significant challenge for maintaining and modernizing these systems. In 2014, American Banker reported that banks face difficulties in attracting young tech talent and there is a shortage of professionals with mainframe and COBOL skills Crosman [2014]. This highlights the urgent need for innovative solutions to bridge the"}, {"title": "Related Work", "content": "2.1 Code Large Language Models\nThere have been numerous Code-LLMs trained on massive datasets to achieve remarkable advance-ments across diverse coding tasks, including code generation Roziere et al. [2023], Touvron et al. [2023], Li et al. [2023], Jiang et al. [2024], Feng et al. [2020], code summarization Ahmed and Devanbu [2022], Lu et al. [2021], Gao et al. [2023], Su and McMillan [2024], program repair Xia and Zhang [2022], Wei et al. [2023], Xia et al. [2023], etc. Besides, LLMs showcase unforeseen capabilities that allow them to adapt to different domains without modifying their parameters by using discrete prompting. These prompts created by humans or by LLMs themselves involve providing instructions along with relevant contexts as input to refine the generation Luo et al. [2023], Wang et al. [2023a]. Related to instruction tuning is chain-of-thought promoting, where models are encouraged to explain the reasoning when given a complex problem, thereby increasing the likelihood that their final answer is correct Wei et al. [2022]. Recently, several studies proposed multi-agent collaborations, where each agent focuses on a unique task, such as code generation or task planning, to enhance the effectiveness of LLM-based code generation Chen et al. [2023], Qian et al. [2023], Huang et al. [2023].\n2.2 LLMs for domain-specific tasks\nAlthough general LLMs were trained to cover a broad range of topics, they were beaten by much smaller models using only domain-specific data on tasks within those domains Wu et al. [2023], Pal et al. [2024], Arefeen et al. [2024]. Training specialized LLMs valued in various areas, such as finance Wu et al. [2023], Yang et al. [2023], law Cui et al. [2023], health Yang et al. [2022], Peng et al. [2023], IT operations Guo et al. [2023],... Therefore, it sheds light on the benefits and necessity of tailoring AI models to specific fields.\nAlthough understanding mainframe systems is crucial, it is rare to find an AI model to support this task. Granite Mishra et al. [2024] from IBM is the first model developed for this purpose. However, Granite has two limitations: it only maintains IBM's Z-system and focuses primarily on documents rather than source code. As a result, it delivers low-quality performance on coding tasks for legacy systems, such as code completion or code summarization of the COBOL codebase. Mainframer from BloopAI Gordon-Hall [2024] is also one of the few models to support a coding task for a legacy COBOL system, achieving good performance on COBOL code completion. Nevertheless, it is trained only on a dataset specific to code completion, resulting in almost zero accuracy for other tasks like question and answering or code summarization. Unlike them, we aim to build a universal model that supports various tasks in this domain with high performance.\n2.3 Benchmark for COBOL and mainframe systems.\nCode-related datasets have been introduced to verify empirical research across various programming languages and tackle challenges in multiple areas of software engineering Odena et al. [2021], Iyer et al. [2018], Chen et al. [2021], Nguyen et al. [2023]. However, low-resource language such as COBOL has not received enough attention from scientific and academic researchers. Therefore, a big barrier exists to training LLMs for COBOL on a large scale. OpenCBS Lee et al. [2022] is one of the pioneering efforts leveraging public forums to create a COBOL set for defect detection. Another dataset, X-COBOL Ali et al. [2023], comprises 84 COBOL repositories collected from GitHub. Despite undergoing a data extraction pipeline, this dataset falls short of the intended quality as the authors relied on GitHub stars for filtering the repositories, which is not a sufficient metric for determining the quality of a repository Borges and Valente [2018]. More recently, BloopAl announced the COBOLEval Gordon-Hall [2024], a benchmark designed to evaluate legacy code completion tasks. It consists of 146 coding problems converted into COBOL from HumanEval benchmark Chen et al. [2021], which is a Python dataset. Nevertheless, it is unrealistic since COBOL is primarily used for business and finance systems, not for solving programming challenges. To the best of our knowledge, there is no dataset that covers diverse tasks related to COBOL language and legacy systems."}, {"title": "Data Construction", "content": "Data quality plays a vital role in training large language models, which directly affects their per-formance Shi et al. [2022], Dau et al. [2024]. Although training datasets for language and code are popular and high-quality Lauren\u00e7on et al. [2022], Nguyen et al. [2023], finding a dataset to support various tasks within mainframe system understanding and legacy coding is challenging. To support fine-tuning XMainframe, we build from scratch our own dataset specific to this domain. In the following sections, we introduce our Mainframe-Training Dataset 3.1 and Mainframe-Instruct Dataset. 3.2, which are used for training and instruction tuning, respectively.\n3.1 Dataset for Pretraining\nThis section details the data extraction process for training XMainframe. We utilized two different sources: using the GitHub API to collect COBOL projects hosted on GitHub and gathering online document data relevant to mainframes.\nWe initially retrieved all GitHub repositories containing COBOL and Mainframe system code, amassing approximately 4GB of data. To ensure high-quality training samples, we removed overly short repositories and files, eliminated alphanumeric character fractions, binary data, JSON, XML data, and node modules, resulting in 40,960 COBOL files. We further refined our dataset using"}, {"title": "Dataset for Model Instruct", "content": "In order to maintain a high-quality synthetic dataset, we employ a pipeline to construct the Instruction dataset, which consists of five distinct phases and is shown in Figure 1.\nBesides using LLMs to solve tasks, recent works have treated LLMs as data generators Ye et al. [2022, 2023], Yu et al. [2024]. With only a few examples, LLMs are able to generate more high-quality data through in-context learning and prompting. The experiments showed that task-specific models trained on generated data can beat the performance of original LLMs while maintaining a low inference cost. Inspired by the self-instruct approach Ouyang et al. [2022], we further enrich Mainframe-Instruct from the seed data by harnessing the capabilities of popular LLMs, including OpenAI GPT-3.5-turbo, Mistral-Instruct 7B Jiang et al. [2023], Neural-Chat 7B Intel [2023], and Mixtral-Instruct 8x7B model Jiang et al. [2024], which are trained on numerous languages and achieved high performance on various NLP and software benchmarks. Below are the examples of prompts that we use to generate data from a sub-topic and seed data:\nPrompt to generate data from sub-topic: You have been provided with a Mainframe-related topic, specifically [sub-topic]. Your task is to produce a comprehensive list of question-answer pairs adhering to the following guidelines:\n1. All generated questions must pertain to the specified topic.\n2. The question should be detailed.\n3. The answers must accurately address the corresponding question, eliminating unnecessary details while retaining essential information.\n4. Format: You are allowed to provide only a list of parsable JSON format data. Each entry should include:\n\"question\" field containing the question related to the given topic;\n\"answer\" field containing a clear, short, and concise response to the question\nTo ensure a strict standard of data quality, we combine OpenAI GPT-4-turbo with careful manual validation. These steps improve the overall quality of our created data while guaranteeing its integrity and dependability. GPT-4 is utilized as an evaluator to judge model responses, scoring the outputs and ranking responses in a pairwise manner. We design prompts meticulously for this task, making GPT-4 easier to locate and remove any instances of poor-quality data. Finally, the dataset undergoes a rule-based filter and manual inspection by our domain experts. All entries that do not fit our standard are fixed or deleted from the dataset. The prompt used for GPT-4 is presented below:\nQuality Prompt: You are given a list of question-answer pairs that are related to Mainframe Migration and COBOL legacy. By thinking step by step to give the final answer, please help me rate the following pairs according to my requirements.\nRequire:\n1. Scoring perspective: whether the question is related to my topic, the answer should be exactly to the corresponding question.\n2. Point scale: 10-point scale, from 1-very poor to 10-excellent.\n3. Format: At the end of your response, you need to add a list of integers, corresponding the final score for each question-answer pair.\nNow, please follow the above requirements to annotate the following data and return your annotated results in a list at the end.\nConsequently, the final version of Mainframe-Instruct comprises a total of 53,351 entries and is divided into three tasks: Multiple Choice Questions, Question Answering, and COBOL summariza-tion. Figure 2, 3, and 4 are examples corresponding to three tasks. The statistic of this dataset is shown in Table 1. MainframeBench, our benchmark for mainframe knowledge, is the testing set in Mainframe-Instruct Dataset"}, {"title": "Overview of XMainframe", "content": "In this section, we detail the selection of the backbone mode 4.1, our training process 4.2, and the method to scale up the backbone model 4.3.\n4.1 Pretrained Model\nWe utilize the pre-trained weights DeepSeek-Coder Guo et al. [2024] as our base model. DeepSeek-Coder's architecture is based on a decoder-only Transformer and is pre-trained on a high-quality project-level code corpus comprising 87 programming languages. It also incorporates Rotary Position Embedding (RoPE) Su et al. [2024], which extends the context window to 16K, enhancing its ability to handle extended context lengths."}, {"title": "Training Details", "content": "We train XMainframe through two stages: pre-training and instruction tuning, as illustrated in Figure 5. In the first stage, XMainframe-base is initially trained on top of DeepSeek-Coder-base 7B using data from our Mainframe-Training Dataset combined with SlimOrca-Dedup Lian et al. [2023]. This combination enriches the model's mainframe knowledge while retaining its general capabilities. We employ standard autoregressive sequence modeling to predict the next token and utilize the efficient optimization of FlashAttention 2 Dao et al. [2022] for training. Subsequently, in the second stage, the model undergoes instruction tuning on our Mainframe-Instruct Dataset for three epochs. This tuning process enhances the model's ability to comprehend and execute natural language instructions, resulting in XMainframe-instruct.\n4.3 Model Upscale\nInspired by Kim et al. [2023], we employ the depth up-scaling method to expand the base model without introducing additional modules or dynamic expert selection methods like Mixture of Experts (MoE) Shazeer et al. [2017], Komatsuzaki et al. [2022]. This approach maintains high efficiency during both training and inference. The depthwise scaling process, illustrated in Figure 6, involves two steps: expanding the base model and continuing pretraining. First, the base model, consisting of $n$ layers, is duplicated. We then remove the last $m$ layers from the original model and the first $m$ layers from its duplicate, creating two separate models with $n - m$ layers each. These parts are combined to form a scaled model with $s = 2(n - m)$ layers. For our purposes, we choose $m = 6$. With $n = 30, m = 6, s = 48$, this process is depicted in Step 1 of Figure 6. As a result, we scale up DeepSeek-Coder 7B, originally with 30 layers, to a 10.5B model with 48 layers."}, {"title": "Experiments", "content": "5.1 Experimental Settings\nWe conduct a comparison with other popular LLMs on MainframeBench, comprising three subsets: Multiple Choice Questions, Question Answering, and COBOL summarization. Our LLMs are compared with a range of previous state-of-the-art LLMs, including GPT 3.5, GPT-4, Mistral 7B, Mixtral 8x7B, Neural-Chat, DeepSeek Coder 6.7B and 33B. We evaluate these LLMs using zero-shot prompting and fixing the temperature hyperparameter to approximately 0, leading to more exploitation of the model's current knowledge.\n5.2 Metrics\nMetrics for Multiple Choice Question task: Because it involves the direct model to select a single answer from the provided options (A, B, C, D), it is considered a classification task. We use Accuracy to report the performance of methods on multiple-choice questions.\nMetrics for Question Answering and COBOL Summarization task: We use various metrics in NLP, including MAP, F1-Score, BertScore, RougeL, Meteor, BLEU-4, as the evaluation metrics for these tasks. These metrics are commonly used to assess the quality and similarity of generated text compared to reference texts.\n5.3 Experiment Results on MainframeBench.\n5.3.1 Multiple Choice Question\nTable 2 presents the accuracy scores of various models on a multiple-choice question task. XMAiNframe-Instruct-10.5B stands out with an accuracy score of 77.89%, which is notably higher than most other models in the comparison. GPT 4 and GPT 3.5 show competitive accuracies of 73.9% and 74.56%, respectively. Mixtral-Instruct 8x7B, Mistral-Instruct 7B, and Neural-Chat follow with accuracies ranging from 66.35% to 69.29%. Although XMainframe-Instruct-7B achieves an accuracy of only 68.5%, it is 20% higher than the base model, DeepSeek-Coder-Instruct 7B, and 15% higher than DeepSeek-Coder-Instruct 33B. It suggests that XMainframe-Instruct performs exceptionally well on the multiple-choice question task, demonstrating its effectiveness and reliability in this specific domain."}, {"title": "Question Answering", "content": "XMainframe-Instruct demonstrates exceptional effectiveness on the question-answering task, as shown in Table 3. With a remarkable MAP of 0.45 and an F1-Score of 0.42, XMainframe-Instruct surpasses all other models in this comparison. Its BLEU-4 score of 20.43 is +9 higher than Mixtral-Instruct 8x7B, which has parameters six times greater than XMainframe-Instruct's. This substantial improvement in scores highlights XMAiNframe-Instruct's ability to provide accurate and contextually relevant answers, making it a highly effective model for question-answering tasks.\n5.3.3 COBOL Summarization\nBased on our observations, developers tend to favor concise and comprehensive summary sentences for COBOL code functions over lengthy ones. As a result, the COBOL summarization set in MainframeBench is tailored to reflect this preference. XMainframe-Instruct has the ability to recognize and apply this observation, producing concise and comprehensive summaries. In contrast, other LLMs often generate longer responses that may not align as closely with developers' preferences for summaries. Comparing these models, Table 4 shows that XMainframe-Instruct outperforms the others by a significant margin, achieving notably higher scores on all metrics. Particularly, it achieves a much higher BLEU-4 score, surpassing GPT 3.5 and Neural Chat by approximately six-fold and outperforming GPT-4, DeepSeek-Coder-Instruct 6.7B, and 33B by nine times. This indicates a substantial improvement in the quality and similarity of its generated text compared to the references. While other models show competitive scores, XMainframe-Instruct stands out as the model that excels marginally in this task, showcasing its effectiveness and superior performance on COBOL code understanding."}, {"title": "Conclusion", "content": "In this paper, we present XMainframe, an LLM specifically designed for mainframe operating systems and COBOL legacy codebases. Additionally, we introduce a pipeline to collect and produce high-quality datasets, resulting in a mainframe benchmark that includes three downstream tasks: question answering, multiple choice questions, and COBOL code summarization. Our experiments reveal that XMainframe outperforms existing state-of-the-art LLMs across multiple tasks, demonstrating significant improvements in accuracy and BLEU scores. With its advanced capabilities in knowledge understanding and documentation assistance, XMainframe can boost productivity and transform developers' interaction with and maintenance of mainframe systems and COBOL codebase. Our work not only highlights the benefits of XMainframe but also sets the stage for promoting innovation and efficiency in the management and modernization of legacy systems."}]}