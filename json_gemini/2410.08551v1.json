{"title": "CONTEXT-AWARE FULL BODY ANONYMIZATION USING TEXT-TO-IMAGE DIFFUSION MODELS", "authors": ["Pascal Zwick", "Kevin Roesch", "Marvin Klemp", "Oliver Bringmann"], "abstract": "Anonymization plays a key role in protecting sensible information of individuals in real world\ndatasets. Self-driving cars for example need high resolution facial features to track people and their\nviewing direction to predict future behaviour and react accordingly. In order to protect people's\nprivacy whilst keeping important features in the dataset, it is important to replace the full body of a\nperson with a highly detailed anonymized one. In contrast to doing face anonymization, full body\nreplacement decreases the ability of recognizing people by their hairstyle or clothes. In this paper, we\npropose a workflow for full body person anonymization utilizing Stable Diffusion as a generative\nbackend. Text-to-image diffusion models, like Stable Diffusion, OpenAI's DALL-E or Midjourney,\nhave become very popular in recent time, being able to create photorealistic images from a single text\nprompt. We show that our method outperforms state-of-the art anonymization pipelines with respect\nto image quality, resolution, Inception Score (IS) and Frechet Inception Distance (FID). Additionally,\nour method is invariant with respect to the image generator and thus able to be used with the latest\nmodels available.", "sections": [{"title": "Introduction", "content": "Image based deep learning models require a lot of high quality images for training, be it for classification, movement\nprediction or other problems. In order to not infringe the privacy of individuals, training data needs to be anonymized,\nleading to less data with a reduction in quality. As an example, the detection and viewing direction of pedestrians is\nneeded for movement prediction in self driving car applications.\nWith the rise of text-to-image models, like Stable Diffusion [26], DALL-E [23] or Midjourney [20], it is now possible\nto generate realistic looking images from text prompts. Although so-called diffusion models are compute intensive\ncompared to previous methods, like GANs, there exist a lot of pretrained models with different characteristics and\noutput resolution, up to 1024 \u00d7 1024 for Stable Diffusion XL [22]. However, resolution is often not the main concern,\nthe image quality and realism is sometimes of higher importance, depending on the purpose of the application. Simple\nanonymization methods, like blur or pixelizing, conserve privacy and are enough when it comes to detection tasks [12].\nIn contrast, getting the view direction or other keypoints of a pedestrian are lost using these approaches. In order to\nensure no corruption of the data, we propose to anonymize the full body with a generated, highly detailed, one that\nis dissimilar to the original person whilst still retaining information, like skin color, anatomy, i.e. (see fig. 1). This is\nalready done partially in DeepPrivacy2 [11], but with limited image resolution quality and without focus on retaining\nfeatures. Our main contributions are:\n\u2022 A novel pipeline that anonymizes people in arbitrary images for the use in neural network training, dataset\ncreation and data storage (i.e. on a blackbox for vehicles)\n\u2022 Evaluation that shows the impact of image anonymization on model training\n\u2022 Use of replaceable pre-trained diffusion models for general anonymization purposes\nThis work is structured the following way. We first describe related work in the area of person anonymization and\ndiffusion models (section 2). We then start to explain our method in detail in section 3, describing the different building\nblocks in detail. At the end in section 4, we show results of our method mainly compared to DeepPrivacy2 [11], a state\nof the art full body anonymization method. The main test cases are image quality, anonymization guarantee and YOLO\n[25, 13] training behaviour."}, {"title": "Related Work", "content": "Image anonymization is widely used in practice, mainly preserving the privacy of people, but also for number plates\nand other sensible information. Some classic methods are the blur, pixelization or masking filter, which are easy to\nimplement, but also degrade the image quality and corrupt important features [12]. Recently, generative adverserial\nnetworks (GANs) were used to generate high quality realistic images with the capability to be used for image inpainting\n[14, 23]. Hukkelas et al. [11] propose the use of multiple GANs for full body and face anonymization of people. They\nuse pose estimation and continuous surface embeddings [21] to guide the generative network and get impressive results.\nHowever, their model outputs at 256 \u00d7 256 resolution which is enough for smaller images, but fails at the reconstruction\nof details, such as eyes and facial features, needed for tasks like intention recognition. It is shown that low resolution\nresults impact the model performance when comparing against the original dataset [12]."}, {"title": "Diffusion Models", "content": "Diffusion models (DMs) [9, 27] are an alternative to GANs for image generation. They operate on the basis of reversing\na Markov chain.\nThe procedure starts with defining a scheduler which sets the noise added at each timestep $n \\in [0, N]$ in the chain.\nThe original image $x_0$ is then distorted by applying the distortion $q(x_{n+1}|x_n)$ defined by the scheduler iteratively.\nThis is illustrated in fig. 2. During training, a single chain link is sampled and the model is trained to output the\nnoise $p_\\theta(x_n | x_{n+1})$. For image generation, $x_N$, a random normalized distributed image, is used as a starting point.\nTo get a deterministic output, a seed can be used for the random number generator to generate consistent $x_N$. For\ninpainting, masking regions to not contain noise is possible, as well as starting at an arbitrary $x_n$ to corrupt the image\nless. Originally proposed as an unconditional generation model [9], conditional embeddings can also be added to the\nmodel, like text. This leads to the recently proposed Stable Diffusion [26, 22] for the possibility of context aware\ngeneration. The latest model is trained to output 1024 \u00d7 1024 images containing stunning detail and facial features.\nAdditionally, using ControlNet [36], embeddings can be changed to match certain features, like preserving edges, image\ndepth or poses for people. Previously, face anonymization using DMs [16] showed promising results. On the contrary,\nrunning diffusion models is much slower than GANs, because of the need of iteratively applying the model to solve the"}, {"title": "Anonymization Pipeline", "content": "In this section, we explain our anonymization pipeline in detail. We call our method FADM (Full-Body Anonymization\nusing Diffusion Models) and mainly focus on full body people anonymization in this paper, but the pipeline proposed\ncan also be adapted to different classes, which will be elaborated in the future. We start with a brief overview of the\nwhole method and later discuss each building block in detail.\nIn fig. 3, a high level overview of our pipeline is given. Object detection and instance segmentation is applied to the\ninput image to get the bounding box and per-pixel segmentation mask of each object. For every instance, we dispatch\na text-to-image diffusion model with a general prompt to inpaint the mask with plausible information. When using\nparallelization, the resulting list of cropped images need to be ordered back to front for compositing, which we do based\non pixel coverage."}, {"title": "Object Detection and Segmentation", "content": "The first step in our pipeline is to detect the objects to anonymize. This is done by using a pre-trained YOLOv8 [13]\ndetector by ultralytics (namely \"yolov8_m-seg\" trained on COCO) paired with instance segmentation for masking\nobjects. The result is a bounding box and instance mask for every object used for the per-object pipeline described in\nthe next section."}, {"title": "Diffusion Process", "content": "After retrieving cropped images per object, we inpaint the segmentation mask using a text to image diffusion model.\nFor the text prompt, we set \"RAW photo, subject, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\" as the\npositive prompt and \"deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime),\ntext, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers,"}, {"title": "Coverage based Merge", "content": "Depending on the implementation of the per-object process, the resulting images are unordered. This means that we\ndo not know the back to front order of all objects, but this is of essential importance, as objects in the foreground can\nstill contain some parts of background objects and vice versa. When doing everything iteratively, this does not pose a\nproblem, as for every anonynmization, the previous one is finished and already merged into the final image.\nIn order to improve performance, we use parallel processing for precomputing the cropped images before executing\nthe diffusion process. This has the benefit of being able to batch the per object images together in a single call to the\ndiffusion model if enough memory is availbale on the gpu. Images generated that way always contain the original\nbackground pixel data for every object. To reduce overlapping artifacts and get a back to front ordering, we sum up the\ninstance segmentation mask values of each object, which effectively gives us the coverage in pixels. We assume that\nobjects in the foreground are larger and occupy more space than objects in the background. Then, we sort the results\nupwards based on the coverage values and merge the images together to get the final anonymized result. This method is\nproposed by Hukkelas et al. [11] as Recursive Stitching.\nAn alternative to coverage based sorting is using depth estimation [24, 2, 35] and ordering the image crops from back to\nfront. Although we have not tested this approach and do not explain more details about it in this paper, we think it is\ninteresting to look at in the future. The reason is that coverage based merging assumes objects in the foreground to\nbe larger than ones in the background. In general, this is not the case, especially when children are in the image that\nare smaller than adults. Correct per-pixel depth estimation has no such assumptions. In contrast to depth estimation,\nexpanding our method to different object classes poses a problem for coverage based merging, as object scales can vary\ndrastically then."}, {"title": "Results", "content": "In this section, we show results of the proposed pipeline. We start with assessing the general image quality, then\nshow if anonynmization has an impact when training a YOLOv8 [13] object detector, as well as a Mask2Former\n[4] segmentation model. Last but not least, we explain that our method ensures anonymization by testing against\nre-identification algorithms on the market1501 and LAST datasets. Additionally, we also perform a face only re-"}, {"title": "Image Quality", "content": "A very important part of anonymization is preserving image quality. We already see some examples in fig. 1, where our\nmethod is used to anonymize high resolution images from different scenarios. Previous methods, like DeepPrivacy2\n[11], are limited to 256 \u00d7 256 output resolution, which is way too low for high resolution images everywhere today. Our\nmethod can generate high quality images up to 1024 \u00d7 1024 resolution, greatly improving sharpness of the anonymized\nimage part.\nHowever, resolution is not everything, the context and overall look of an image, like lighting, shadows and features are\nalso very important. fig. 5 compares DeepPrivacy2 (DP2) with our method on an image of a group of people. Looking\nat the orange and green part, we clearly see the improvement in sharpness. Additionally, our method preserves the\ngeneral image feel, in this case a warmer look, correct skin tone and shadows. The image from DP2 looks more out of\ncontext and not very well integrated into the original image with respect to lighting and compositing. This behaviour is\nseen in many images, like fig. 6, where we anonymize a group of people. Our method creates a much more realistic\nlook than DP2. Especially when looking at faces in that image, as well as the clothes, which look more natural for the\ncontext of that scene (shot at the Oscars)."}, {"title": "Object Detection", "content": "We already showed that our method significantly improves the image quality compared to previous methods. The next\nquestion is: \"Does it even matter for training AI models?\" There are many different problems in the computer vision\nspace. We cannot test all of them in this paper. We focus on object detection, as it is an important task and a good\nindicator if anonymization impacts training performance [12].\nAs a first experiment we trained a YOLOv8 model multiple times on a subset of the COCO dataset [19] to detect people\nand tested the result on a static validation set of real images. The first training was done with the reference dataset,\nthe second one used anonymization by DP2 and the third one used our method. In fig. 8, we plot the precision of the\nmodel on the validation set for the first 50 epochs. We clearly see that training with DP2 results in a worse precision\nthan using the original dataset. In contrast, using our method, the model performance is not degraded compared to the\noriginal one, showing that anonymization has no impact on people detection in this task."}, {"title": "Re-Identification", "content": "So far, we have only shown that our method produces realistic images and does not sacrifice model performance when\nit comes to object detection. However, this paper is designated to anonynmization and we have to validate that the\nproposed method actually anonymizes people. To measure anonymization, we opted to use a person re-identification\nmodel as a metric. Keep in mind, this is in no means a proof that there exists no algorithm that can defeat our\nanonymization method.\nAs a re-identification method, we use OSNet [39, 40] via the torchreid [38] library and anonymize the Market1501 [37]\ndataset. OSNet is trained on the dataset, which contains 1501 identities in 32k images. The task of OSNet is then to\nmatch a given image with a target identity from the dataset. Our test used the anonymization method to anonymize the\nperson before OSNet is applied and a method succeeds when the model outputs the wrong identity.\nThe result is described in table 3, where the Rank-1 accuracy and mean average precision (mAP) is shown. Rank-1\naccuracy describes the number of identities correctly matched by the algorithm. Mean average precision is based on\nmultiple metrics and mainly calculated over recall values. We see that OSNet does a great job at identifying people\nin the original dataset. DP2 anonymizes the images and yields a much lower accuracy for the algorithm, showing its\nanonymization capabilities. In contrast to the original paper [11], we get worse results in this test with the code provided\nby the authors. They original reported a Rank-1 accuracy of 44.7% and mAP of 8.5%. Our method is not as good\nas DP2 when it comes to anonymization in this test, but still reduces the overall score of the OSNet re-identification\nmethod significantly. In general, we can say that our method trades anonymization for a large increase in image quality.\nOf course, destroying an image using masking, bluring or pixelization would yield a high anonymization at the cost of\nimage quality.\nPlease note that our method is designed for high resolution image inpainting. The dataset used for testing, Market1501,\nhas a resolution of 128 \u00d7 256, which is very low to start with.\nAdditionally, OSNet is trained with only 1501 identities, so there is the possibility that many generated identities map\nto a close latent space representation. That's why we also tested our method on the LaST dataset [30], which contains\nover 10000 identities in around 228000 images. fig. 9 shows an example of the rank 10 retrieval. The query image is anonymized using our method and the positive\nand negative matches are shown on the right. This sample shows that re-identification is very dependent on the color\ndistribution on the image. As soon as the color of the clothes changes, as in the image anonymized by DeepPrivacy2\nand FADM, the main clothing in the images retrieved from the gallery match the query images. Even the slighter color\nchange in the FADM image is enough to fool the re-identification algorithm."}, {"title": "Limitations", "content": "In this section, we want to show some limitations of our approach. Our method highly depends on the generation quality\nof the diffusion model. Although current models produce photorealistic, high quality images, some cases exist where\nthe output is corrupted.\nWe show some problems in fig. 11. The face looks slightly deformed and the right eye is not very well reconstructed.\nWe think this happens due to the model being trained on less data for certain angles of human faces as well as the\ngeneral problem of generating faces at lower resolutions. In this example, the model had to generate a complex pose\nof a whole person. The second example shows a problem reconstructing hands, which is very common when using\nthe latest diffusion models and may be improved in future versions. The third one shows that faces are sometimes\ncompletely removed, as the one of the woman in the background. Additionally, the hand gets blurred and does not look\nlike a hand anymore.\nThese scenarios do not occur as often as when generating an image from scratch, i.e. from full noise, but can occasionally\noccur. However, our method is still valid as the generator can be replaced by any other generative model. We think\nwith research advances in the next years, the hand problem of generative models can be reduced or eliminated and then\ndirectly used with our approach.\nThere is also no support for video streams yet. Though the pipeline is capable of working on image sequences, Stable\nDiffusion is not capable of generating temporally consistent image sequences out of the box. At the end of doing\nthe research for this paper, Stable Video Diffusion [3] was proposed, showing significant improvements in temporal\nstability that should be easily integratable into our pipeline."}, {"title": "Conclusion", "content": "In this work, we proposed a novel pipeline for full-body people anonymization. Our method achieves higher image\nquality than previous methods regarding resolution, realism and image composition. We provided justification that\nthe proposed method anonymizes people by using re-identification algorithms and deceiving them. Using an object\ndetector trained on our anonymized dataset, we showed that the resulting model is on par and sometimes outperforms a\nmodel trained on the original dataset. This is in contrast to the results of previous method [12], where image quality and\nresolution are worse. The proposed pipeline can be used with any text-to-image model and thus can be updated with the\nlatest models in the future. This is important as new methods, like Adverserial Diffusion Destillation [29], improve\nthe speed and quality of diffusion models and thus our pipeline. For future research, combining a prompt sampler\nwith the diffusion model seems interesting to improve variety and improve anonymization strength. Furthermore, the\nproposed method is only executed on single images, making it unsuitable for videos as the diffusion model relies heavily\non the initial noise. Integrating a temporal step combined with Stable Video Diffusion [3] can lead to usable video\nanonymization."}]}