{"title": "TEXT2INSIGHT: TRANSFORM NATURAL LANGUAGE TEXT INTO INSIGHTS SEAMLESSLY USING\nMULTI MODEL ARCHITECTURE", "authors": ["PRADEEP SAIN"], "abstract": "The increasing need for dynamic and user-centric data analysis and visualization solutions is\nevident in various domain including healthcare, finance, research and other. Traditional data\nvisualization systems, while valuable, often fall short in meeting user expectations as they are\nstatic and predefined by the system, not aligning with the unique requirements of individual\nusers.\nThis research introduces Text2Insight, a novel solution designed to address this limitation by\ndelivering data analysis and visualizations tailored to users' specific needs. The developed\napproach employs a multi-model architecture that provides data analysis and visualization from\nnatural language text, specifying the user's analysis requirements.\nThe methodology involves an initial data analysis of the input file to extract relevant\ninformation such as shape, columns, and related values. Subsequently, a pre-trained Llama3\nmodel is employed to convert the user-provided natural language text into an SQL query. This\ngenerated query is refined with the provided input data to formulate an accurate SQL query\nusing NER model, ensuring accurate results. The methodology then utilizes a chart predictor\nmethod to determine the most fitting chart type based on the inputted text. Then, the\nmethodology uses the Llama3 model to generate insights from the data resulting from the\nexecution of the SQL query. Ultimately, the developed methodology generates visually\ninformative charts as the output for user-friendly data visualization. Additionally, to provide\ncomprehensive data analysis, the study developed a question-answering model and a predictive\nmodel. These models, utilizing the BERT model, can offer insights into past and future\noutcomes.\nThe performance of each inner model has assessed individually, followed by an overall\nevaluation of the complete model. The evaluation using accuracy, precision, recall, F1-score,\nsyntactical evaluation, and BLEU score are found to be 99, 100, 99, 99, and 0.5 respectively\nfor the Text2Insight model. For the question-answering mode, the accuracy is 89, precision 72,\nrecall 69, and F1-score 69. For the predictive model, the accuracy is 70, precision 70, recall 68,\nand F1-score 68. These results determine the viability and effectiveness of Text2Insight as a\npromising solution for transforming natural language text into dynamic and user-specific data\nanalysis and visualizations.", "sections": [{"title": "Introduction", "content": "In today's world, we're flooded with data from all directions, making it essential to find ways\nto make sense of it all. Data visualization steps in as a handy tool that transforms complex\ninformation into easy-to-understand pictures like charts or graphs. Imagine trying to read a giant\nspreadsheet versus seeing a clear graph \u2013 that's the power of data visualization. It's not just\nabout making things look pretty, it helps us spot trends, connections, and important details in a\nway our brains can grasp effortlessly.\nIn recent years, the intersection of Natural Language Processing (NLP) and data analysis and\nvisualization has emerged as a dynamic field with the potential to revolutionize information\ncommunication and comprehension. One notable application within this domain is the\ngeneration charts from textual input, providing a more dynamic and user-friendly presentation\nof data. The synthesis of text and charts holds promise across diverse domains, including\nbusiness analytics, scientific research, healthcare, and journalism.\nBy leveraging NLP techniques to convert natural language to charts and then extract insights\nfrom charts, Decision-making processes can be enhanced and facilitate deeper understanding\nof complex datasets. This integration opens new opportunity for exploring and interpreting data,\nultimately enabling more effective communication and knowledge.\nSince the arrival of Large Language Models (LLMs), it has become common practice to use\nthem to improve results in many different areas. Nowadays, LLMs are being applied\nextensively across various fields to achieve better outcomes and create systems that interact\nmore like humans.\nAdding large language models to data analysis and visualization not only makes complex data\neasier to understand but also can easy the process of decision making. Using these advanced\nlanguage models helps create better visualizations that understand human language better. This"}, {"title": "Problem Statement", "content": "The current landscape of data analysis and visualization systems is characterized by a static\nnature, where visualizations are generated based on predefined data and chart types. This\nrigidity becomes a significant drawback when user requirements deviate from the\npredetermined visualization techniques, making it difficult to obtain customized visual\nrepresentations from the existing systems. The problem at hand is the lack of adaptability and\nflexibility in current data visualization tools, which restricts their capacity to effectively cater\nto varying user needs and preferences, as per (Wu et al., 2022).\nFurthermore, the interactive aspect in current data visualization presents a substantial obstacle.\nWhen data visualization lacks interactivity, it fails to provide meaningful insights or convey\nnarratives associated with the depicted data. Moreover, extracting insights from charts remains\na predominantly manual process, necessitating a comprehensive understanding of diverse chart\ntypes and the methodologies for deriving insights from them, thus presenting additional\nchallenges.\nAs technology grows and data gets more complex, visualization tools that can change with the\ntimes are needed. Imagine if charts and graphs could adjust automatically as data changes or as\nper user's ask for different questions. That's the idea behind adaptive visualization. Instead of\nbeing stuck with the same old static graphs, new tools would be smart enough to adapt to new\ndata and user needs. This could make exploring data easier and more insightful, helping user to\nuncover hidden patterns and tell better stories with data. It's like having a dynamic assistant\nthat helps to make sense of information in real-time.\nFurthermore, after generating a chart for data visualization, it becomes easier to understand the\ninsights it offers. This reduces the need for manually analyzing complex charts to uncover\ninsights and allows for making informed business decisions based on the data presented in the\nchart."}, {"title": "Research Questions", "content": "The following research questions are proposed for the study, as highlighted as follows:\n1. How can user-friendly data analysis and visualization problems solved and make it\neasier to create interactive chart?\n2. How can Natural Language Processing techniques or specifically LLMs can be used to\nenhance user-friendly data visualization?\n3. How can data analysis tool effectively give useful information from the charts they\ngenerate, helping to make better business decisions?"}, {"title": "Aim and Objectives", "content": "The research aims to propose a multi-model architecture designed for data analysis by\ngenerating charts dedicated to visualization from input data as per user's natural language\ninputs, with insights from the resultant visual representations. Also, to make the data\nvisualization tool adaptable to different fields and datasets. It focuses on improving the accuracy\nof data visualizations to reduce the chances of making incorrect decisions. Furthermore, this\nresearch aims to develop question-answering model capable of deriving both historical and\nfuturistic insights from contextual data within the input document, by leveraging the capabilities\nof the BERT model.\nThe research aim is divided into objectives, which are as follows:\n\u2022\nTo develop a model dedicated to converting natural language input into data\nvisualizations and their associated insights, leveraging the capabilities of Llama3.\n\u2022\nTo develop a model that provides natural language responses to user queries based on\nthe input document by leveraging capabilities of BERT model.\n\u2022\nTo develop a model for providing future predictions based on available historical data,\nleveraging BERT's capabilities."}, {"title": "Significance of the Study", "content": "Data analysis nowadays requires a flexible approach to meet the different needs of various\nindustries. Presently, most data insights are shown through pre-defined charts and static\nanalyses, which don't give users much flexibility. Organizations usually provide set\nvisualizations based only on specific data sets. So, when users come across situations where\nthey need different analyses for various inputs, they face significant difficulties.\nThe significance of this research lies in its potential to address this inherent limitation. By\ndeveloping a model facilitating the transformation of natural language input, into personalized\nand context-specific data analysis and visualizations, the study seeks to empower users with the\ncapability to conduct data analysis and visualization according to their distinct preferences. This\nnovel approach aims to enhance the adaptability and user-friendliness of data visualization,\ncontributing to a more dynamic and intuitive exploration of diverse analytical scenarios."}, {"title": "Scope of the Study", "content": "This research paper is delimited to the analysis, visualization and predictive modelling of data\nwithin the context of cricket dataset, primarily due to time constraints. The chosen scope\nensures focused and in-depth exploration of the proposed methodology within the specified\ntimeframe. The dataset used for experimentation and validation is exclusively related to cricket.\nFurthermore, the research predominantly concentrates on the English language, considering the\nchallenges and resource-intensive nature of training models in multiple languages. Given the\ncomplexity and time requirements associated with multilingual model training, the decision to\nnarrow the scope to English facilitates a more efficient and targeted investigation.\nHowever, it is important to note that the designed model, once established and validated on the\ncricket dataset in English, holds the potential for broader applicability. Future endeavors can\nextend the use of the developed approach to diverse datasets beyond cricket and languages other\nthan English. This adaptability allows for the scalability and versatility of the proposed model,\nopening opportunities for cross-domain and multilingual applications beyond the initial cricket-\nfocused and English-centric scope."}, {"title": "Research Methodology", "content": "This chapter will discuss the research methodology proposed for data analysis and visualization\nof data derived from input text. Through a thorough examination of existing literature in this\nfield, suitable approaches or models for data visualization have been identified. It has been\nobserved that distinct pre-trained models, each designed for specific tasks, can effectively\naddress the challenges present in current data visualization practices. These methods can\nsubsequently be implemented sequentially to achieve the desired outcomes. Furthermore, the\npredictive modeling to predict cricket match outcomes based on contextual data is discussed.\nThe research methodology employed for transforming natural language to data visualization\nand their insights encompasses several crucial steps. These include the analysis of data from\ninput file, the transformation of natural language input text into SQL queries, refinement of the\nSQL query using the data analysis information from the input file, identification of chart types\nbased on either the input text or the generated output of the SQL query, followed by the\nvisualization of the chart.\nAnother, research methodology employed for providing answers based on the user's natural\nlanguage questions from the provided dataset using BERT model. Which consist of context-\nbased data preparation then model training on these datasets to develop a model which can\nanswers the user's questions from the provided input documents.\nAdditionally, a BERT-based classification model is employed to predict future outcomes by\nanalyzing historical data. This phase of the methodology involves creating a suitable dataset\nand training it using the BERT model. Ultimately, the methodology establishes evaluation\nmetrics for all proposed models in the study."}, {"title": "Dataset Selection", "content": "The dataset utilized in this study related to cricket match details, encompassing various formats\nsuch as Test matches, One Day Internationals (ODIs), Twenty20 (T20) matches, League\nmatches, Premier leagues, and Domestic matches for both men and women. The data is\norganized into folders corresponding to each match format, with each folder containing files in\nYAML or JSON format that document the matches specific to that format. This collection\nprovides a rich source of information for detailed analysis and insights into the game of cricket.\nThe study uses Indian Premier League (IPL) dataset due to its manageable size and current\nrelevance, given that the IPL season is ongoing. The IPL dataset comprises all matches played\nin the league from its inception in 2007 up to 2024. Each file within this dataset represents a\nsingle match and includes extensive details about that match. These files are structured to\nprovide both metadata and detailed match information, ensuring a thorough representation of\neach game's events and outcomes.\nThe dataset found to be clean, and the missing values are significant, as they might indicate that\na match was not played due to rain or other reasons. Therefore, the dataset is used as it is, with\nno pre-processing done. The study focuses on analysing the data as it is, without altering their\nvalues."}, {"title": "Text2Insights: A Multi Model Architecture", "content": "The study introduces a novel multi-model architecture, Text2Insights. As shown in Figure 3.1,\nthis architecture operates on two primary inputs. The first input comprises tabular data\nformatted such as CSV, serving as a foundational database from which users can pose various\ninquiries concerning data visualization. The second input consists of natural language text,\nutilized in the creation of visual representations.\nUpon receiving the tabular data, Text2Insights initiates data analysis, extracting insights from\nthe data without altering its contents. This approach mitigates the risk of erroneous data\nvisualization that may arise from the manipulation or deletion of missing or null values. The\nmodel's functionality depends on the nature of the input data it receives."}, {"title": "Comprehensive Data Analysis of Input file", "content": "The proposed methodology is designed to accept two inputs: firstly, an input file designated by\nthe user for data visualization, and secondly, natural language text specifying the criteria for\nthe desired data visualization. In the initial phase of input data analysis, upon receiving the input\nfile, the model initiates an exhaustive examination, extracting important details such as the total\nnumber of columns, rows, numeric and string columns, column names, primary key column,\nand other relevant attributes associated with the input file. These values are systematically\nstored utilizing the Pandas and NumPy libraries in Python. Additionally, the dataset columns\nwill be adjusted into their correct types. Subsequently, the input file is transformed into a\nrelational database management system (RDMS) using the SQLite library in Python.\nImportantly, this process does not fill in missing data or removing any incomplete entries.\nLikewise, it does not involve modifying any existing values within the dataset provided. This\ncautious approach is necessary changing the dataset could lead to inaccuracies in the results.\nAdditionally, given that the dataset is not specific to any field and could originate from various\ndomains, it would not be good idea to engage in any form of data cleaning as part of the\nmethodology. Therefore, this study assumes that the dataset provided is sufficiently prepared\nfor visualization purposes without the need for further preprocessing.\nThe shape of the dataset, indicating the number of rows and columns, can be obtained using\nlibraries like pandas in Python. Numeric columns can be identified by performing statistical\nanalyses to assess the distribution and characteristics of numerical data. Extracting column\nnames provides insight into the variables present in the dataset. Identifying the primary key\ninvolves examining unique identifiers that uniquely identify each record in the dataset."}, {"title": "Transformation: From Natural Language Text to SQL Query", "content": "The subsequent phase in the proposed methodology involves the transformation of the second\ninput, consisting of natural language text requesting the desired outcome, into an SQL query\nutilizing an open-source pre-trained decoder only transformer-based model architecture\nLlama3.\nThe reason behind conversion of input text to SQL is imperative due to the critical nature of\nensuring accuracy in the results generated by the proposed model. Given the potential impact\nof these results on business decision-making based on charts, the model is designed to either\nfurnish accurate outputs or prompt the user to refine their query, thereby ensuring the provision\nof valid input and preventing the delivery of inaccurate results.\nPre-trained Llama3 (llama3-70b-8192) model is employed to convert natural language text into\nSQL queries by leveraging its ability to capture contextualized semantic representations.\nInitially trained on a 15 trillion parameters, pre-trained Llama3 model inherently grasp the\ncontextual nuances and relationships between words."}, {"title": "Refinement of SQL Query", "content": "The subsequent stage in the proposed study involves the refinement of the generated SQL\nquery, utilizing insights derived from the initial step of data analysis applied to the input file.\nThis refinement process incorporates the application of Spacy's \u2018en_core_web_sm' model and\nsimilarity index.\nThe rationale behind fine-tuning the SQL query lies in its direct applicability to the converted\ninput file turned database, enabling the provision of results. In the event of an invalid query,\nSQLite would raise an error, prompting the proposed methodology to guide the user towards\nupdating their input text requirements for accuracy and coherence.\nBy utilizing Spacy's similarity index, proposed model can process a given text and extract the\nrelevant keywords, which can help to fine-tune the resulted SQL query.\nThe output from preceding stages serves as the input for this step, where it undergoes\nrefinement. Previous stages yield schema of input file, column names and a SQL query. In this\nstep, the SQL query is refined. For instance, the initial SQL query may employ keywords such as\n\u2018player_name,' \u2018wickets,\u201d and 'players' to execute the query on a RDBMS. However, these\ncannot be used directly as column names in the RDMS may differ. Therefore, the refinement of\ngenerated queries is necessary to provide accurate results."}, {"title": "Data Subset Generation", "content": "In the subsequent stage of the proposed methodology, the refined query is applied to the\ngenerated database to generate outcomes, thereby constituting a unique subset of the initial\ndataset. The SQLite library is utilized for executing SQL commands, given that the input data\nhas been transformed into a relational database management system. This process enables the\nretrieval of specific information aligned with the user's predetermined criteria.\nThis stage provides a subset of the dataset, generated from the execution of refined SQL query\nderived from the previous step on the converted database. The resultant table is utilized for\ncrafting visual representations of the data and extracting insights."}, {"title": "Chart Type Prediction", "content": "In the subsequent phase of the proposed methodology, the attention goes to predicting the\nappropriate chart type based on the resulting subset of the dataset obtained from prior steps or\nthrough user-specified input text.\nInitially, the model checks if user has mentioned any types of charts they prefer, if they\nmentioned, the model gives those preferences more importance. If the user hasn't specified any\npreferences, the proposed model leverages characteristics of the derived dataset subset. This\nincludes considerations such as the number of categorical, continuous, univariate, bivariate,\nmultivariate, and time-series columns, as well as the presence of specific named columns. For\ninstance, if the subset comprises two continuous columns and one categorical column, the\nmodel strategically determines the most fitting chart type. Furthermore, the model provides\ninsights into the optimal placement of columns on the x-axis and y-axis, guided by the unique\nattributes of the dataset subset.\nThe predictive capabilities of the proposed model enhance its adaptability, ensuring that chart\ntypes align with user expectations or data characteristics. To achieve this, the proposed\nmethodology has employed a chart predictor method. By utilizing this Chart Predictor method,\nthe methodology gains the ability to predict specific chart types based on given input\nparameters."}, {"title": "Chart Generation", "content": "In the subsequent phase of the proposed methodology, the generation of charts for user-oriented\ndata visualization is undertaken, leveraging the outputs obtained from previous steps. This\nprocess is accomplished through the utilization of Python's Matplotlib and Seaborn libraries.\nThe integration of these libraries facilitates the creation of visually informative charts, aligning\nwith user-specified criteria and preferences as determined in earlier stages of the methodology.\nThe utilization of established Python libraries ensures a robust and standardized approach to\nchart generation, enhancing the overall coherence and reliability of the data visualization\noutcomes within the proposed framework.\nMatplotlib and Seaborn are powerful Python libraries widely employed for creating diverse and\nvisually appealing charts and plots. Upon acquiring the complete output from the preceding\nstages, the chart can be generated using Matplotlib or Seaborn."}, {"title": "Insights Generation", "content": "The following step in proposed methodology, involves deriving insights from the generated\nchart. Open-source pre-trained LLM model Llama3 is employed for this step. The Llam3\n\u2018llam3-70b-8192' will analyze a subset of the data obtained in the previous step and generate\ninsights. These insights can then be utilized by businesses to inform important decisions\nregarding the data. This stage aims to provide a concise summary of the insights derived from\nthe user's data, limited to 500 words. By imposing this word limit, it has been ensured that the\nmodel does not oversimplify the data or introduce any irrelevant information."}, {"title": "Question-Answering Model", "content": "To enhance data analysis, it is beneficial to enable users to ask natural language questions based\non contextual data and receive responses in natural language. This approach allows users to\nanalyse data more effectively and intuitively. To achieve this the study also developed a\ndocument question-answering model that allows users to ask natural language questions about\nan input file and receive accurate answers. The model employs the \u2018distilbert-base-uncased'\npre-trained model to correctly respond to the input queries.\nThe reason for using BERT is its ability to understand the context of a word in all its\nsurroundings (bidirectionally), rather than just the context that precedes or follows it. This\nbidirectional understanding makes BERT particularly effective for question-answering tasks,\nwhere grasping the nuances of the context is crucial for generating accurate answers.\nThe study created a dataset for training the model, consisting of textual context, various types\nof questions, and their corresponding answers. These datasets were derived from the Indian\nPremier League's (IPL) cricket data. The context includes detailed information about each\nmatch, such as the teams playing, the toss winner, the match winner, the player of the match,\nthe match location, individual player scores, and other related information. Based on this\ncontext, specific questions and answers were formulated, which were then used for model\ntraining. The created dataset is then divided into training and testing sets, with 80% of the data\nallocated for training and 20% reserved for testing.\nThe BERT model is trained on the train dataset for question-answering task to provide\nresponses to user queries. The training parameters include a learning rate of approximately 5e-\n05, a batch size of 8, five epochs, an adam epsilon of le-06, a weight decay of 0.0, and 500\nwarmup steps. The remaining 20% of the dataset is used to evaluate the model's performance\non accuracy, precision, recall, and F1-score.\nAfter training the model, the study implemented a context retrieval system using TF-IDF\nvectorization to identify the most relevant context paragraphs for a given question. When a\nquestion is asked, the system retrieves the top matching context from the dataset, which is then\nfed into the BERT model to generate the answer. The developed model is useful for detailed\nanaly of the provided data, allowing users to ask natural language questions and receive\nresultant natural language responses."}, {"title": "Predictive Model", "content": "When analysing data, obtaining future predictions based on historical contextual data is\ninvaluable. This approach provides users with insights into potential future outcomes, allowing\nthem to take necessary actions based on these predictive insights. To achieve this, the study\ndeveloped a predictive model that analyses historical contextual data to generate future\noutcomes in response to user queries. This model empowers users to make informed decisions\nby leveraging the patterns and trends present in past data.\nThe model utilizes Bert for Sequence Classification, employing the \u2018bert-base-uncased' pre-\ntrained model. The reason behind using \u2018bert-base-uncased' is its exceptional performance and\nproficiency in sequence classification tasks like sentiment analysis and others. This pre-trained\nmodel demonstrates significantly high performance in classifying natural language into\nmultiple labels.\nThe study created a dataset for training the model, consisting of textual context, and the label.\nThis dataset is derived from the Indian Premier League's cricket data. The contextual input data\nfor the classification model includes all matches details such as venue, participating teams,\nplaying eleven, match dates represented in textual contextual data, and the ultimate match\nwinner (label). The model is trained on IPL match data from its inception in 2007 up to the\ncurrent year. This data is in natural language and includes the winner of each match\ncorresponding to each context column.\nThe dataset is divided into an 80% training dataset and a 20% test dataset. The training dataset,\ncomprising 80% of the data, is utilized for training the predictive model. Subsequently, the\nremaining 20% of the data is reserved for evaluating the performance of the developed model.\nThe historical data includes information on ten different cricket teams currently playing Indian\nPremier League. However, each record only contains details about the two teams playing in a\nspecific match.\nThe study has developed two different approaches: binary classification and multiclass\nclassification. Binary classification predicts the match outcome with two labels: Team 1 and\nTeam 2. Multiclass classification uses ten different labels, each representing one of the ten\nteams currently playing in the IPL."}, {"title": "Binary Classification", "content": "The binary classification model uses BERT for sequence classification with two labels for the\nteams playing the match. If Team 1 wins, the label is 0; if Team 2 wins, the label is 1. Team 1\nand Team 2 are determined by their order of appearance in the context: the first team mentioned\nis Team 1, and the second team is Team 2. Therefore, if the first mentioned team wins, the label\nis 0; otherwise, it is 1.\nThe training data, which comprises 80% of the total dataset, is used to train the binary\nclassification model. The training parameters include a learning rate of approximately 3.5e-5,\na batch size of 8, five epochs, a weight decay of 3.8e-5, and 466 warmup steps. The remaining\n20% of the dataset is used to evaluate the model's performance on accuracy, precision, recall,\nand F1-score."}, {"title": "Multi-Class Classification", "content": "The multiclass classification model uses BERT for sequence classification with ten different\nlabels, each representing one of the ten teams currently playing in the IPL. The labels are as\nfollows: SRH-0, CSK-1, DC-2, MI-3, KXIP-4, RCB-5, GT-6, KKR-7, RR-8, and LSG-9. The\nmodel predicts the results based on these labels, assigning the highest weight to the label derived\nfrom the context.\nThe training data, comprising 80% of the total dataset, is used to train the multiclass\nclassification model. The training parameters include a learning rate of approximately 1.43e-5,\na batch size of 16, four epochs, a weight decay of 0.02, and 447 warmup steps. The remaining\n20% of the dataset is used to evaluate the model's performance on accuracy, precision, recall,\nand F1-score."}, {"title": "Model Evaluation Matric", "content": "The concluding step in the research methodology involves the comprehensive evaluation of the\nemployed machine learning models. As mentioned in the proposed methodology, various types\nof models are employed within the study framework. Each of these models is carefully\nevaluated, examining various factors to assess how well they perform and how effective they\nare. This thorough evaluation process guarantees a rigorous assessment of the models' abilities,\nproviding the study with detailed insights into their strengths and weaknesses.\nInitially, the Llama3 model is used to translate natural language text into SQL queries. The\ngenerated SQL queries are evaluated for both syntactical correctness and overall accuracy.\nSyntactical evaluation, conducted using a SQL parser, checks how well the SQL query adheres\nto syntax rules. The correctness of the SQL query is analysed using the BLEU score, which\nmeasures how closely the generated query matches the actual SQL queries. The BLEU score\nproves instrumental in assessing the quality of SQL query generation, as it involves comparing\nthe n-grams of the generated query with those of the reference query. This methodological\napproach enhances precision in evaluating the effectiveness of the Llama3 model's ability to\nseamlessly transform natural language inputs into coherent and accurate SQL queries.\nBLEU score can be calculated with the formula (3,1):\n$BLEU = BP \\times e^{(1/n \\sum_{i=1}^{n}log(precesion i))}$\n(3,1)\nSubsequently, a second model is used to refine SQL queries through Spacy's Named Entity\nRecognition model. This step refines the SQL query generated by Llama3. The refined query\nundergoes the same syntactical evaluation and BLEU score evaluation as the initial query.\nIn addition to the internal evaluation of individual models, the study has conducted a holistic\nassessment of the entire data visualization model. This comprehensive evaluation process\ninvolves initially gathering sample inputs from multiple users for cricket dataset. Subsequently, a\nsmall dataset is curated using these inputs, and charts are manually generated for each sample\ninput text. This sample dataset, not previously encountered by the model during its training\nphase, will then be inputted into the developed model for evaluation.\nAccuracy, Precision, Recall and F1-Score, is calculated by the formula (3,2), (3,3), (3,4) and\n(3,5):\nTP =Number of True Positive Predictions\nTN =Number of True Negative Predictions"}, {"title": "Summary", "content": "This study introduces a novel approach Text2Insights, for data visualization within a multi-\nmodel framework. This approach adheres to a sequential pipeline structure wherein the output\nof one phase serves as the input for the subsequent phase. The development of this architecture\nis done by a comprehensive analysis of existing research gaps and constraints within the data\nvisualization domain. The choice of the models depends on its ability to provide users with\nprecise data visualization or to indicate errors. It is imperative that the introduced model avoids\ngenerating erroneous visualizations, as such inaccuracies could significantly impact crucial\nbusiness decisions.\nThe introduced framework requires two inputs: an input query and a dataset corresponding to\nthe user's query. Initially, the model conducts a comprehensive analysis of the supplied dataset,\nyielding essential outputs such as the dataset's row count, shape, column names, and primary\nkey, which are pivotal for subsequent procedures. Parallelly, the second query input is inputted\ninto another model for processing. Specifically, the utilized model in this stage is pre-trained"}, {"title": "Introduction", "content": "The chapter will provide an in-depth examination of the Indian Premier League dataset, data\npreparation, covering a range of statistical, exploratory, time series, and predictive analyses.\nThis chapter offers an understanding of the various facets of IPL matches, teams, and player\nperformances over the seasons from 2007 to 2024.\nPreparing datasets involves understanding the YAML file and then converting it to JSON and\nCSV formats for better data analysis. The context-based dataset for the document question-\nanswering model includes context, questions, and their respective answers. Another dataset,\nprepared for predictive modelling, contains context with match information and the winner of\neach match.\nThe statistical analysis, where insights like the total number of matches played, the teams\ninvolved, and the venues used are presented. Following this, the exploratory data analysis\nsection discusses about team performances, revealing critical statistics like match-winning\npercentages and toss-winning outcomes. This analysis helps to identify patterns and trends that\ndistinguish the successful teams from the rest, providing a detailed understanding of what\nfactors contribute to a team's success.\nThen the study progresses to time series analysis, which examines team performance across\ndifferent IPL seasons. This section highlights how teams have improved or declined over time,\noffering insights into their scoring trends and consistency. The team and player performance\nanalysis sheds light on individual and team achievements. By focusing on detailed metrics such\nas run totals and player contributions, this section emphasizes the importance of consistent\nperformance and highlights standout players who have significantly impacted their teams'\nfortunes."}, {"title": "Dataset Description", "content": "The dataset utilized in this study related to cricket match details, encompassing various formats\nsuch as Test matches, One Day Internationals (ODIs), Twenty20 (T20) matches, League\nmatches, Premier leagues, and Domestic matches for both men and women. The data is\norganized into folders corresponding to each match format, with each folder containing files in\nYAML or JSON format that document the matches specific to that format. This collection\nprovides a rich source of information for detailed analysis and insights into the game of cricket.\nFor the purposes of analysis, the Indian Premier League dataset has been used due to its\nmanageable size and current relevance, given that the IPL season is ongoing. The IPL dataset\ncomprises all matches played in the league from its inception in 2007 up to 2024. Each file\nwithin this dataset represents a single match and includes extensive details about that match.\nThese files are structured to provide both metadata and detailed match information, ensuring a\nthorough representation of each game's events and outcomes.\nThe metadata within each file includes essential information such as the creation date of the file\nand its version, ensuring that the dataset's integrity and updates can be tracked. The \u201cinfo\u201d\nobject provides comprehensive details about each match, including the date it was played, event"}, {"title": "Dataset Preparation", "content": "For analyzing the cricket dataset, the necessary data is extracted and stored in CSV files,\nenabling conversion into a panda Data Frame for further analysis. Three primary datasets are\ncreated from the IPL directory to facilitate different analytical techniques such as statistical\nanalysis, exploratory data analysis, time series analysis, and predictive analysis.\nThe first dataset comprises all the innings-related data, crucial for detailed statistical and\nexploratory analysis. This dataset includes columns such as the match ID, creation date, city,\nmatch dates, outcome details including the winner, margin of victory by runs or wickets, player\nof the match, participating teams, season, toss winner and decision, venue, team batting, over\nnumber, ball number, batter, bowler, non-striker, runs scored by the batter, extra runs, total\nruns, player out, and the type of dismissal. This dataset provides a comprehensive view of each"}, {"title": "Statistical Analysis", "content": "Statistical analysis is conducted on the first dataset, which contains innings-related data. This\nanalysis focuses on several key aspects of the Indian Premier League, providing insights into\nvarious dimensions of the league's matches and team performances.\nIt is observed from analysis that there are a total of 1,093 matches played in the IPL as per the\ndataset available, starting from 2007 and continuing annually through 2024, with no"}, {"title": "Exploratory Data Analysis", "content": "Exploratory Data Analysis is also conducted on the first dataset, which contains innings-related\ndata. Providing insights into team performance, match-winning percentages, and toss-winning\npercentages. EDA helps in understanding patterns and trends within the data, revealing which\nteams consistently perform well, their likelihood of winning matches based on various factors,\nand how often teams win the toss.\nAs shown in Figure 4.4"}]}