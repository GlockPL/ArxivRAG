{"title": "Improving Post-Earthquake Crack Detection using Semi-Synthetic Generated Images", "authors": ["Piercarlo Dondi", "Alessio Gullotti", "Michele Inchingolo", "Ilaria Senaldi", "Chiara Casarotti", "Luca Lombardi", "Marco Piastra"], "abstract": "Following an earthquake, it is vital to quickly evaluate the safety of the impacted areas. Damage detection systems, powered by computer vision and deep learning, can assist experts in this endeavor. However, the lack of extensive, labeled datasets poses a challenge to the development of these systems. In this study, we introduce a technique for generating semi-synthetic images to be used as data augmentation during the training of a damage detection system. We specifically aim to generate images of cracks, which are a prevalent and indicative form of damage. The central concept is to employ parametric meta-annotations to guide the process of generating cracks on 3D models of real-word structures. The governing parameters of these meta-annotations can be adjusted iteratively to yield images that are optimally suited for improv-ing detectors' performance. Comparative evaluations demonstrated that a crack detection system trained with a combination of real and semi-synthetic images outperforms a system trained on real images alone.", "sections": [{"title": "1 Introduction", "content": "After a catastrophic event, such as an earthquake, it is crucial to perform a rapid structural safety assessment of the endangered areas. Nowadays, common approaches involve the direct inspection by human experts of photos and videos acquired in the field, generally using Unmanned Aerial Systems (UAS) [25,32]. Such an examination is a slow process that can be sped up by Computer Vi-sion (CV) and Deep Learning (DL) solutions [33]. Automatic damage detection algorithms are particularly helpful for this task, since they can provide a fast preliminary screening of images and videos, thus reducing the amount of data that experts need to analyze.\nHowever, the creation of damage detectors that utilize Deep Convolutional Neural Networks (DCNN) usually necessitates a substantial amount of labeled data. Indeed, one of the primary challenges in building such systems is the limited availability of large, open-source datasets of annotated images. Furthermore, many of the currently available datasets consist of images derived from routine structural monitoring. Such datasets typically represent less severe damage levels compared to what might be observed in the following of an earthquake [1,38].\nData augmentation approaches have been proposed in literature, either based on standard image transformations [30, 42], generative models [4, 18, 22] or cus-tom techniques for specific types of structures [3,37]. Nevertheless, to the best of authors' knowledge, no data augmentation approach exists for a post-earthquake scenario, in which the images to be generated may include diverse types of severely damaged structures, framed at different distance and under different meteorological conditions.\nIn the present work, we propose a method for creating semi-synthetic images whereby computer-generated damage is applied on photorealistic real-world 3D models. We focus here on the generation of cracks, which is one of the most common and possibly more revealing types of damage in the domain considered. Real-world cracks can vary widely in shape, size, and location, depending on the level of severity and the type of structure affected. To handle such complexity and variety, producing realistic outcomes, we designed a set of parametric meta-annotations that could govern the random generation of crack instances within a specific set of constraints defined by human experts. The overall procedure starts from 3D models of real-world buildings and bridges obtained by photogrammetric elaboration of images acquired by UAS. Meta-annotations are then added and defined in appropriate locations. Finally, images are rendered by specifying virtual camera paths over the 3D models, with varying lights and ambient con-ditions. Following this procedure, it is possible to generate a large number of images, also simulating different scenarios and levels of damage. We believe that this approach could produce more realistic outcomes than those achievable from fully artificial CAD models, by also harnessing inspection by UAS flights that are possibly simpler and less expensive to acquire. In comparison to the usage of generative AI, such as diffusion models, the proposed method is intended to allow a higher degree of expert-based control over the generation and variety of crack instances than that achievable by generative network conditioning.\nBy design, semi-synthetic images produced in this way are intended as an augmentation to datasets of real images, and not as a complete replacement. In addition, the meta-annotation parameters can be tuned incrementally to match the progress of the training process. The working hypothesis is that the iterative application of this tunable augmentation strategy could improve the performance of a DCNN-based crack detector. To prove it, we performed comparative exper-iments using various off-the-shelf YOLO DCNN architectures. In these experi-ments, three detectors were trained: one using real images only, another using semi-synthetic images only, and the last one using a combination of real and semi-synthetic images. All these combinations were evaluated on the same test"}, {"title": "2 Previous Works", "content": "The assessment of structural safety is a well-researched topic, with numerous machine learning and deep learning solutions proposed in scientific literature [15,33]. These solutions primarily focus on detecting damage, particularly cracks. [17,38]. The approaches can be categorized into three main Computer Vision tasks: classification, segmentation and object detection.\nAmong the several crack classification methods proposed, notable examples include: the DCNN designed by [6] for recognizing cracks in concrete structures; the SVM-based model used by [11] to classify crack pattern in reinforced concrete beams and slabs; the DCNN architecture employed by [13] to find cracks in gusset joints in steel bridges; the DCNN proposed by [36] specialized for classifying cracks on bridges. Transfer learning methods, such as the one proposed by [39], have been explored, too.\nRegarding crack segmentation, various networks have been tested, including those by [43] for concreted buildings, and [10] for masonry walls. A DCNN-based semantic segmentation approach was proposed by [40] for identifying and measuring cracks on pavements and concrete walls. Custom crack segmentation networks have also been developed, such as CiNet [41], for concrete beams, and SDDnet [20], for buildings. Some researchers have treated the crack segmentation problem as an anomaly detection task, using convolutional autoenconders [8]. Hybrid solutions combining detection and segmentation have also been inves-tigated, such as those for concrete cracks [7] and road cracks [27]. Recently, a Bayesian approach was proposed by [5] to optimize the hyperparameters of a DCNN-based crack segmentation algorithm.\nIt should be noted that both classification and segmentation methods primar-ily operate on close-up images or small patches of larger images. As a result, they are better suited for monitoring purposes where the target areas are predefined. In contrast, post-earthquake surveys typically occur at a mid-distance from the structures being inspected, to grant rapid information acquisition. Object detec-tion algorithms are better suited for this scenario. Meaningful examples of crack detection methods include: the road damage detection for smarthphone pro-posed by [24]; the Meta-Learning Convolutional Neural Architectures proposed by [26], designed to identify multiple classes of damage on concrete bridges; the End-to-end Defect Detection Network (EDDN) proposed by [23], designed for metal defect detection; and the crack detector for dams proposed by [37]."}, {"title": "3 Methodology", "content": "3.1 Semi-Synthetic Image Generation Procedure\nThe overall procedure presented here has the objective to create a large number of semi-synthetic annotated images to be used as data augmentation during the training of a DCNN-based crack detector.\nThe proposed approach is designed to work with different types of civil struc-tures, including masonry and concrete buildings and bridges. 3D models of real-world, undamaged structures obtained via photogrammetry will be used as in-put. The core idea is to use a set of parametric meta-annotations, manually placed by human experts, from which cracks are automatically generated. This\nmeta-annotation process has been designed in strict cooperation with structural engineers, to guarantee the realism of the outcome. This is the major difference with respect to similar approaches described in literature [37], in which crack patterns extracted from real images are modified and applied to real-world 3D models of structures. In our opinion, the proposed method allows a higher degree of control and greater variability in the image generation process, as well as the applicability to different types of surfaces and structures.\nFigure 1 describes the workflow of the entire procedure. All steps, detailed in the following, were implemented with Blender (version 3.6 LTS) [2] by means of a series of node-based workflows and add-on Python scripts.\n3D Model Setup The first step of the procedure involves the examination and preparation of the 3D model in input. Since a 3D model may contain multiple buildings and/or structures (e.g., it could be a large scan of an entire neigh-borhood), specific components are selected, supposedly with the higher quality, and considered relevant for the following stages (Fig. 2). If necessary, 3D mesh defects in the chosen components can be fixed at this stage before further pro-cessing. The remaining parts of the model will be used as background for the final rendering.\nTo improve the variability of the outcomes, external textures of the compo-nents can be modified or substituted as well, for instance to turn a plastered wall into one with exposed masonry.\nMeta-Annotation Phase Meta-annotations are intended to define the posi-tioning and general constraints that will govern the generation of cracks. The basic graphical primitive for defining a meta-annotation consists in a couple of points connected by a line, as shown in Fig. 3. Meta-annotations are manually defined by a structural engineering expert who places them in specific positions of the selected component.\nObviously, real-world cracks could vary widely in terms of shape, extension and severity. To encompass such variability, each meta-annotation is associated to a set of parameters that will govern the final aspect of the corresponding cracks. Among them, the fundamental parameters are:\nLength, the percentage range of the line to be covered.\nRoughness, the range of low and high frequency perturbations of the line.\nThickness, the range of crack width.\nDepth, the range of crack inner depth.\nProbability of appearance.\nBy randomly picking values within the specified parametric ranges, it is pos-sible to generate crack instances having a wide variety of patterns and aspects. For example, Fig. 4 shows the possible effects of varying roughness, while Fig. 5 shows different crack instances generated by the same meta-annotation.\nReal-world cracks may also cause the appearance of spalling (i.e., detach-ments of the external layers of a surface). Therefore, in our method, it is possible to specify the probability to generate small spallings in random positions along the crack path (Fig. 6).\nMeta-annotations can also be aggregated to make them share a unique set of parameter values. This allows, for example, to have different groups of cracks each corresponding to the same degree of severity (e.g., a group could contain only short and thin cracks, while another may contain long and large ones). Ag-gregation of meta-annotations can aslo be used to simulate the actual evolution\nDamage Generation The generation of crack instances involves the alteration of the original 3D mesh. This operation is automatically performed by the system before the rendering process. At this stage, the straight line corresponding to each meta-annotation is converted into a 3D curve, which is altered randomly in length, roughness, thickness and depth within the parametric range of values specified. The obtained curve is then subtracted from the 3D mesh applying a boolean operation. The shape of each crack will also depend on the type of structure and surface being considered. For example, on a masonry wall, cracks will follow the spaces between the bricks. In case of spalling, since the original 3D models are limited only to the outermost surface of structures, all the internal layers will be automatically simulated by creating surfaces with ad-hoc textures and altering the 3D mesh in the surroundings of the crack line (Fig. 6).\nImage Rendering Given that the objective is to generate images as similar as possible to those acquired during a UAS survey, sequences of camera move-ments and angles need to be specified to simulate possible flight paths. World parameters for the entire 3D model can be specified as well, for each virtual flight. To this purpose, we made use of a specific Blender add-on, called Sun Position, which simulates various lighting and meteorological conditions given a geographic location, time and date. This makes it possible to reproduce the same conditions of the day of the actual acquisition of the 3D model, thus helping to match shadows that are already present in the model itself.\nThe complete setup of a rendering sequence may contain multiple flight paths with repeated generations of different levels of damage and different lighting and meteorological conditions. Once the setup is completed, the rendering process runs automatically producing a large number of images.\nFor every virtual flight, multiple frames are rendered. Figures 8 and 9 show examples of individual frames that could be generated for various types of build-ings and bridges. For each frame, the system also produces a segmentation mask that describes the area corresponding to each crack. In turn, bounding box an-notations are generated from segmentation masks and translated into the Pascal VOC XML format [14]. Optionally, for debugging purposes, frame copies with overlapping bounding boxes can be generated as well."}, {"title": "3.2 Tunable DCNN Training Strategy", "content": "As already described, the proposed meta-annotation mechanism allows to gener-ate semi-synthetic images tuned to improve the performance of a DCNN-based\ncrack detector. In the intended strategy, a DCNN is iteratively trained with a combination of real and semi-synthetic images. At each stage, the results ob-tained by the detector on the same test set of real images are examined in detail, to find out those critical cases in which crack detection is problematic. There-fore, the setup of the generation process can be altered to produce more damage instances of these critical cases. For example, if the detector frequently misses cracks on masonry walls, the process can be oriented towards the generation of more images of that kind. This operation can be performed by changing a limited number of parameter values and/or re-using existing meta-annotations."}, {"title": "4 Case Study", "content": "4.1 Real-Image Dataset\nThe IDEA (Image Database for Earthquake damage Annotation) dataset\u00b9, cre-ated by the EUCENTRE Foundation, contains high-resolution images relating to three recent major events in Italy: L'Aquila (2009), Emilia (2012) and Cen-tral Italy (2016-2017). To the best of the authors' knowledge, to date, this is the\n4.2 Semi-Synthetic Dataset\nThis dataset was generated incrementally with the proposed image generation method. We used as input 3D models of four building compounds and four bridges obtained via the Structure for Motion (SfM) photogrammetric technique [29]. The high-resolution images needed for photogrammetry were acquired using various UAS devices (Dji Mavic 2 Pro, Air 2S, Mini 2) during multiple flights\nperformed between 2018 and 2022. Agisoft Metashape was the photogrammetry software of choice. The produced 3D meshes were cleansed of minor defects, due to acquisition inaccuracies, using Meshlab [9].\nThis semi-synthetic dataset was extended incrementally during the training process, in keeping with the strategy described in Section 3.2. At the end of the process, the dataset contained 33836 semi-synthetic images of buildings and bridges, 30059 damaged and 3777 non-damaged. In total, the generated cracks were more than 135K."}, {"title": "4.3 Experiments", "content": "To validate the advantages of the proposed approach, we conducted a series of comparative experiments. We trained various DCNN-based crack detectors: (i) the baseline model, using only real images from the training split of the IDEA dataset; (ii) a second model, using only the semi-synthetic dataset; (iii) the augmented model, using the same training split of the IDEA dataset used for the baseline model with augmentation of the semi-synthetic dataset. The same testing split of the IDEA dataset was used in all cases.\nAs DCNN architectures, we tested the YOLOv8 architecture developed by Ultralytics [19], YOLOv9 [35] and YOLOv10 [34]. More precisely, for each of them, we used the small model, 1088 image size, and 50 epochs for training. The choice of the small model was dictated by the requirement of having the smallest possible computational footprint for the detector, to make it usable on field with limited computational capabilities. For validation reliability, all hyperparameters were tuned separately for each detector."}, {"title": "5 Experimental Results", "content": "At first, we trained the baseline model. After some initial tests, the need to apply a correction to the bounding boxes of the IDEA dataset emerged. In fact, since cracks have imprecise boundaries, human experts may draw bounding boxes that do not cover them completely. To compensate for possible inaccuracies of this kind, we expanded the bounding boxes in all directions. We tested expansions from 1-pixel to 7-pixel wide, and we attained the best outcomes with a 3-pixel expansion. Table 1 shows the results obtained with the three YOLO architectures considered, measured in terms of Average Precision (AP) with the threshold of Intersection-over-Union (IoU) set at 0.5. As can be noticed, the best result was achieved with YOLOv8. Given the clear difference in performance, in the subsequent experiments we report only the results of YOLOv8.\nFigure 10(a) shows the Precision-Recall (PR) curve for the baseline model. Figure 10(b) shows instead the PR curve obtained with a model trained with semi-synthetic images only. In this case, the results are indeed poor, meaning that semi-synthetic images could not be used as a pure replacement for real-world images. Figure 10(c) shows the best PR curve obtained with the augmented model, which was trained using semi-synthetic images as augmentation of the\nreal ones. Given that semi-synthetic images largely outnumber the real ones, we applied a rebalancing strategy to avoid biasing the detector towards semi-synthetic data. More precisely, we oversampled with a ration 1:6 the images from the IDEA training split to match the amount of semi-synthetic images.\nComparing Fig. 10(a) and (c), we can notice an improvement in performance of the augmented model with respect to the baseline, as the AP@0.5 grows from 0.382 to 0.407. Figure 11 shows an example of the enhanced detection capabilities of the augmented model with respect to the baseline.\nIn the same example, Fig. 11(a)(c), we can notice that bounding boxes vary significantly in both number and size between ground truth and prediction. Nonetheless, most of the cracks are detected anyhow. This is a common outcome in the considered scenario. In fact, unlike standard target objects like cars or pedestrians, cracks may have ambiguous shapes and boundaries, which could be annotated and detected in several different ways. This phenomenon may lead to the underestimation of the actual performance of a detector, since standard IoU metrics are typically applied to the best one-to-one match between ground-truth and predicted bounding boxes only. To better deal with this, the Many-to-Many metrics were proposed in [12]. These new metrics assume that the same Ground Truth Box (GTB) can be matched by multiple Prediction Boxes (PBs), and vice versa. In the Many-to-Many approach, Precision and Recall are computed using two different variants of the IoU metric: Intersection over Prediction (IoP, see Eq. 1) is used to evaluate Precision, while Intersection over Ground Truth (IoG, see Eq. 2) is used to evaluate Recall.\n$IoP = \\frac{areas\\ of\\ overlap}{area\\ of\\ PB}$ (1)\n$IoG = \\frac{areas\\ of\\ overlap}{area\\ of\\ GTB}$ (2)\nFigure 10(d) shows the PR curve for the augmented model recomputed using the Many-to-Many metrics, with thresholds for both IoP and IoG at 0.5. We can notice a significant increase in the AP@0.5, from 0.407 to 0.749, as well as a smoother curve. In reference to the example in Fig. 11, the Many-to-Many metrics appear to yield a better estimation of the actual performance of the crack detector."}, {"title": "6 Conclusions", "content": "In this work we have presented a method for generating a large number of semi-synthetic images of damaged buildings and bridges, starting from 3D models of real-world structures. The generation process is guided by parametric meta-annotations, which offer significant flexibility and control over the outcome. Specifically, they allow to tune the image generation process to match the gaps in coverage that could be present in a real-world dataset. The semi-synthetic im-ages thus obtained can be used as an effective data augmentation of real images during the training process of a DCNN-based detector.\nExperiments conducted on a real-world dataset demonstrate that the pro-posed tunable training strategy improves the performance of a crack detector. In this perspective, this work contributes to implementing more effective screen-ing procedures during post-earthquake surveys, with the final goal of speeding up the assessment of the impacted areas.\nFuture steps will involve the extension of the semi-synthetic image generation process to other classes of damage, such as spalling and exposed rebars. We also plan to move the analysis from single images to full videos acquired by UAS. Video analysis will allow us to track damage instances and to employ temporal filtering to further refine the detection."}]}