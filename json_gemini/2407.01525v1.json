{"title": "Empowering 3D Visual Grounding with Reasoning Capabilities", "authors": ["Chenming Zhu", "Tai Wang", "Wenwei Zhang", "Kai Chen", "Xihui Liu"], "abstract": "Although great progress has been made in 3D visual grounding, current models still rely on explicit textual descriptions for grounding and lack the ability to reason human intentions from implicit instructions. We propose a new task called 3D reasoning grounding and introduce a new benchmark ScanReason which provides over 10K question-answer-location pairs from five reasoning types that require the synergization of reasoning and grounding. We further design our approach, ReGround3D, composed of the visual-centric reasoning module empowered by Multi-modal Large Language Model (MLLM) and the 3D grounding module to obtain accurate object locations by looking back to the enhanced geometry and fine-grained details from the 3D scenes. A chain-of-grounding mechanism is proposed to further boost the performance with interleaved reasoning and grounding steps during inference. Extensive experiments on the proposed benchmark validate the effectiveness of our proposed approach. Codes, datasets, and benchmarks will be available at https://zcmax.github.io/projects/ScanReason.", "sections": [{"title": "1 Introduction", "content": "Understanding and reasoning in the 3D visual world is critical for applications such as robotics and augmented reality, where embodied agents are expected to understand the 3D layout and predict the 3D locations of objects based on human instructions. The example in Fig. 1 demonstrates a scenario where the question can only be solved with a comprehensive understanding of the 3D scene and joint reasoning of the question and the 3D environment. However, current 3D visual grounding models [23,38,42,49] trained on [1,6] localize objects based on explicit descriptions of the object category, attribute, and 3D spatial relationships, and lack the ability to reason the user intentions and predict object locations with implicit human instructions such as \"I am thirsty, can I have something to drink?\".\nTo bridge the aforementioned gap and to push the boundaries of what embodied agents can understand and how they can interact with the 3D world, we propose a new task of 3D reasoning grounding and introduce a new benchmark"}, {"title": "2 Related Work", "content": "3D Vision and Language Learning 3D Vision-language learning (3D-VL) is garnering increasing attention, with many 3D-VL tasks focusing on how to connect the 3D world with natural language. Among them, 3D Question Answering (3D QA) [2,44] aims to enable models to provide text answers based on natural language questions. Situation Question Answering in 3D Scenes (SQA3D) [28] requires an agent to first understand its location based on a text description, then provide a reasonable answer based on the surrounding environment, which can be seen as an extension of 3D QA in the embodied AI area. 3D Visual Grounding [1, 3, 6, 27, 30, 38] demands that models identify and locate target objects in a 3D scene based on given descriptions, outputting the objects' coordinates and 3D bounding boxes. These descriptions usually explicitly rely on the objects' attributes and their spatial relationships. 3D Dense Captioning [4, 7, 13, 15, 24, 35, 45, 48] requires models to output a series of object coordinates and corresponding scene-based descriptions based on a given scene. Different from these 3D-VL tasks, the questions in 3D reasoning grounding could be more implicit and complex.\n3D Visual Grounding The task of 3D visual grounding is aimed at localizing the objects that are explicitly referred to by free-form guided language expressions in the 3D scene. Inspired by the success of transformers in natural language processing, recent 3D visual grounding approaches [11,16,23,38,49,52] have started to adopt transformer [34] architectures for handling the complex"}, {"title": "3 ScanReason Benchmark", "content": "3.1 3D Reasoning Grounding Task\nGiven a 3D scene and an implicit free-form query, 3D reasoning grounding requires the model to predict the 3D bounding boxes of the target objects as well as the textual answers and explanations. As shown in Fig. 2, different from traditional 3D visual grounding, the queries of 3D reasoning grounding are implicit and complex, requiring strong reasoning, commonsense, and world knowledge. The number of target objects in 3D reasoning grounding is flexible, and any object in the 3D scene that satisfies the query requirements should be considered as the target object.\n3.2 Question Types\nTo comprehensively evaluate the 3D reasoning grounding abilities, we define 5 types of questions depending on which type of reasoning is required. Spatial"}, {"title": "4 Method", "content": "Solving the task of 3D reasoning grounding requires the synergization of the perception, reasoning, and grounding capability of the embodied agent. Intuitively, we can first conduct reasoning based on the implicit instruction such as \"where should first aid kits be placed?\" and the visual environment. The reasoning process provides us with information about the rough location and semantics of the object we are looking for. Then keeping that information in mind, we look back to the 3D environment to precisely locate the object. For complex scenarios, alternate reasoning and looking back are required for multiple rounds until we obtain the final answer.\nInspired by this intuition, we propose ReGround3D, consisting of a visual-centric reasoning module and a 3D grounding module with geometry-enhanced look-back, as illustrated in Fig. 3. The visual-centric reasoning module (Sec. 4.1) performs joint reasoning of language instruction and visual scene, and predicts"}, {"title": "4.1 Visual-Centric Reasoning", "content": "Due to the complexity of the 3D scene and user instructions, particularly given the implicit intention behind the human instruction, ReGround3D starts with a visual-centric reasoning module that can perceive the scene, comprehend the human instructions, and conduct joint reasoning of 3D scene and instructions. We believe the reasoning process eventually implies the grounding intention, i.e., implicitly encodes the information indicating the target object to solve the task. Thus, we design the visual-centric reasoning module to predict grounding queries for localizing the target objects in the following 3D grounding module.\nSpecifically, for simplicity, we leverage 3D-LLM to serve as the visual-centric reasoning module because of its strong reasoning abilities inherited from the LLM. Based on the BLIP2 architecture [25], 3D-LLM uses pre-trained image encoders to extract multi-view 2D image features and unprojects them into 3D spaces. The visual features are encoded by the Q-Former to produce 32 tokens as the visual input to the LLM. By leveraging the pretrained image encoders and the Q-Former, the visual tokens encode rich semantics but lack the 3D structures, spatial interactions, and fine-grained details. Therefore, instead of directly predicting the object locations by the 3D-LLM, we ask the 3D-LLM to predict the feature representation as the output of the reasoning process, and the predicted feature is further used to ground the target object in the grounding module.\nTo enable the prediction of the grounding feature, we expand the original vocabulary of the 3D-LLM with a special <LOC> token. The <LOC> token is laden with the contextual scene and the target object information which can guide the 3D grounding module to accurately localize target objects."}, {"title": "4.2 3D Grounding with Geometry-Enhanced Look-Back", "content": "Once obtaining the <LOC> token, ReGround3D extracts the last-layer embedding hloc of the <LOC> token and sends it into the 3D grounding module to predict the 3D bounding boxes. The 3D grounding module is devised with a \"look-back\" mechanism which allows the model to access the 3D geometry and fine-grained details from a 3D point cloud encoder. The fine-grained geometry-enhanced 3D visual features and hloc are sent into a query selection module to retrieve the most relevant object features. Those features are further decoded into 3D bounding boxes with the 3D box decoder."}, {"title": "4.3 Chain-of-Grounding Mechanism", "content": "Existing design conduct the reasoning and grounding process sequentially, i.e., the reasoning process is finished before grounding. We argue that the grounding results can also facilitate the reasoning process, especially for those requiring spatial information. Thus, to further synergize the reasoning and grounding process, inspired by chain-of-thought (CoT) [37], we propose Chain-of-Grounding (CoG), which introduces a chain of interleaved steps of reasoning and grounding to find the targeted objects during inference, as shown in Fig. 4. Such a process allows the model to actively find relevant objects that help solve the problem, and then conduct reasoning with the assistance of the additional information of these relevant objects so that the model can more precisely find the target objects.\nSpecifically, given a question provided by users, CoG translates it into another question of finding the explicit mentioned objects in the original question. The generated new question is sent into ReGround3D to ground the objects mentioned in the original question in the 3D scene with corresponding confidence scores. An object can be seen as a successfully located object when its confidence score is above the threshold, and the located object information could serve as explicit guidance for 3D-LLM in the next reasoning stage. As shown in Fig. 4, after obtaining the 3D information of objects explicitly mentioned in the original question, the located object information is inserted to update the question. The updated question is then sent to ReGround3D to perform reasoning and grounding to output the target object locations."}, {"title": "4.4 Instruction Tuning", "content": "Training Objective We use the pretrained weights of 3D-LLM as the initialization for the visual-centric reasoning module. Except for freezing the 3D visual encoder pretrained on [36], the rest of parameters in the visual-centric reasoning module and 3D grounding module in our framework are trained in an end-to-end manner. The training supervision is a weighted sum of the next token prediction loss from 3D-LLM and the 3D detection loss from the 3D grounding module."}, {"title": "5 Experiment", "content": "5.1 Implementation Details\nNetwork Architecture For the 3D grounding module, we adopt the pre-trained multi-modal 3D encoder in EmbodiedScan as the 3D visual encoder. During the trainning stage, We use LoRA to effeciently finetune the 3D-LLM to preserve the original 3D scene understanding capability and reduce the computation costs. The number of object queries k in the query selection module is set to be 256."}, {"title": "5.2 Results Comparison", "content": "Evaluation on 3D Visual Grounding In order to verify the superiority of our model in grounding ability and facilitate comparison between current models, we report the explicit grounding performance on the existing 3D visual grounding task. Since the evaluation settings of Nr3D and Sr3D [1] are based on ground-truth object proposals, while ScanRefer [6] requires models to output 3D bounding boxes, we choose ScanRefer as our benchmark for comparison. We divide the existing methods into two categories, one is the grounding model designed specifically for the 3D visual grounding task, and the other is generalist MLLMs which can understand a variety of 3D vision-language tasks. The original 3D-LLM embed 3D locations in the vocabularies and represent the grounded 3D bounding boxes by a sequence of discrete location tokens. However, since the fine-tuned model of 3D-LLM on ScanRefer and related location tokens implementation are not accessible, we adapt 3D-LLM to directly output 3D numerical coordinates representing 3D bounding boxes by fine-tuning the pre-trained model on our reformulated 3D visual grounding data, denotes as 3D-LLM (vg). As shown in Tab. 2, current generalist MLLM models still lag behind the specialist models in terms of the grounding ability. By incorporating the 3D grounding module into MLLM, ReGround3D shows the SOTA performance on the traditional 3D visual grounding task."}, {"title": "Evaluation on 3D Reasoning Grounding", "content": "The various visual grounding models rely on explicit text-object alignment in the input object expression to achieve localization, which fall to be applied to 3D reasoning grounding task. We performed a comparision between our proposed method ReGround3D and existing MLLM methods, including 3D-LLM(vg) and Chat3D-v2 [21]. Besides, inspired by Chat-3D v2, which first segments objects, then equips them with unique object identifiers to conduct effective object grounding, we set up a LLM-based 3D reasoning grounding baseline: We first segment the objects from the scene using a 3D instance segmentor [31], then convert the segmented object information including their categories and 3D bounding boxes into text as input of LLM (InternLM2-7B [32]). Besides, to better validate the performance of our model and ensure the fair comparison, we remove the ScanReason dataset from the training data, denoted as ReGround3D*. As show in Tab. 3, the LLM-based reasoning method (Mask3D [31] + InternLM2-7B [32]) possesses a very strong function reasoning ability, but struggles understanding of 3D spatial relationship. Additionally, irrespective of whether ScanReason is used in training, our model significantly outperforms the existing MLLMs. By synergize the reasoning and grounding process utilizing the Chain-of-Grounding (CoG) mechanism, the 3D reasoning grounding performance of ReGround3D can be further improved (from 28.98 to 30.62), especially on the spatial reasoning and logistic reasoning questions. The qualitative results comparison shown in Fig. 5 demonstrates superiority of our method in reasoning human complex instruction based on the 3D scene."}, {"title": "5.3 Ablation Study", "content": "In this section, we conduct an extensive ablation study to verify the effectiveness of each component.\nEffectiveness of 3D Grounding Module In order to more comprehensively verify the effectiveness of the 3D grounding module we proposed, we step by step verify the performance changes from 3D-LLM to ReGround3D. Apart from the 3D-LLM(vg), we fine-tuned the 3D-LLM model respectively on full reformulated existing data and all instruction tuning data, including ScanReason, denoted as"}, {"title": "6 Conclusion", "content": "This paper introduces a new 3D vision language learning task: 3D reasoning grounding, which requires the model to perform active reasoning over complex and implicit human instruction, localize the target objects and give corresponding explanation. Furthermore, we propose ScanReason, a new dataset and benchmark to further unlock and throughly evaluate the 3D reasoning grounding capability. Based on this dataset, we propose a novel approach: ReGround3D, which utilizes the strong reasoning capability of MLLM guiding the 3D grounding module to obtain accurate object locations, and a Chain of Grounding (CoG) mechanism is presented to further boost the performance with interleaved reasoning and grounding steps during inference. We believe that our work will further the natural interaction between embodied agents and humans in open 3D environments. For the current ScanReason benchmark, we find that the questions in three high-level 3D reasoning categories may have overlaps. For a certain reasoning question, similar questions may appear in one or two other categories. We leave the problem as a future challenge for better reasoning grounding ability evaluation."}, {"title": "7 More Evaluations", "content": "Considering the expected outputs of 3D reasoning grounding questions consist of not only the target objects 3D bounding boxes, but also text response including either demonstrating the explanation (e.g. Why to choose these objects) or offering reasonable suggestions (e.g. How to use these objects.). We argue that it is also necessary to evaluate the text response correctness. However, due to the complexity and diversity of the answer, it is non-trival to use or design a proper evaluation method which can make sure the evaluation accuracy. To ensure the evaluation accuracy with limited human and time resources, we uniformly sample 100 reasoning grounding pairs from evaluation datasets and test GPT-4 [29], 3D-LLM [20] and ReGround3D on the datasets. Then we manually score the 300 responses using an integer ranging from 1 to 5, while 1 indicates an incorrect answer, 5 is a correct answer. The matching score di represents levels of the similarity between the response and ground-truth answer. The correctness metric is denoted as:\n$S = \\frac{1}{N} \\sum_{i=1}^{N} (\\frac{d_{i}}{4}) \\times 100\\%$"}, {"title": "8 More Visualizations", "content": "8.1 More Examples\nIn this section we will illustrate more examples of our ScanReason benchmark for each type of reasoning questions in Fig. 7, Fig. 8, Fig. 9, Fig. 10 and Fig. 11. Each example consists of the reasoning question, target object locations (3D bounding boxes) and the corresponding text response.\n8.2 More Qualitative Results\nIllustrated in the qualitative results in the paper, we find that our model tends to output much less 3D bounding boxes compared with the ground-truth 3D bounding boxes when multiple objects are regarded as the target objects. Besides, as shown in Fig. 6, even if the 3D grounding module is introduced to more accurately localize the target objects, ReGround3D still struggles to recognize and localize the small and long-tailed objects in the 3D scene."}, {"title": "9 ScanReason Annotations Generation Prompts", "content": "We show five prompt templates for generating five types of reasoning question-answer-location pairs, each comprising system messages and manually crafted context examples. In our attempts, since GPT-3.5 struggles to understand the 3D spatial relationships of objects in the scene based on the provided 3D spatial coordinates of objects, we resort to GPT-4 for data generation, which is verified to be much better than GPT-3.5 in understanding the spatial relationships. We input the category information and 3D bounding boxes of the objects in the 3D scenes, providing information about the semantics and spatial locations of the scene in a textual representation. Then we provide specific instructions to the GPT-4 [29] to generate diverse data. As shown in Fig. 12, Fig. 13, Fig. 14, Fig. 15 and Fig. 16, to further make the generated question-answer-location pairs more"}, {"title": "10 Details of Instruction Tuning Datasets", "content": "10.1 Data Reformulation\n3D Object Detection data. Generally speaking, 3D object detection datasets contain information about 3D bounding boxes of all objects in the pre-defined list of categories. In order to cover as many objects as possible, we chose to construct question and answer pairs based on the EmbodiedScan [36] dataset, which includes 160k 3D-oriented boxes spanning over 760 categories. During the model training process, we convert the annotations of 3D boxes into a specific question answering pair template: \"User: <scene> Where is the <category> in this 3D Scene? Assistant: Sure, <LOC>.\" Here, <category> is randomly selected from the ground-truth categories contained in the current 3D scene, <scene> is the placeholder of 3D scene tokens."}]}