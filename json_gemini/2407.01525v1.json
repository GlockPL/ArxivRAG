{"title": "Empowering 3D Visual Grounding with Reasoning Capabilities", "authors": ["Chenming Zhu", "Tai Wang", "Wenwei Zhang", "Kai Chen", "Xihui Liu"], "abstract": "Although great progress has been made in 3D visual grounding, current models still rely on explicit textual descriptions for grounding and lack the ability to reason human intentions from implicit instructions. We propose a new task called 3D reasoning grounding and introduce a new benchmark ScanReason which provides over 10K question-answer-location pairs from five reasoning types that require the synergization of reasoning and grounding. We further design our approach, ReGround3D, composed of the visual-centric reasoning module empowered by Multi-modal Large Language Model (MLLM) and the 3D grounding module to obtain accurate object locations by looking back to the enhanced geometry and fine-grained details from the 3D scenes. A chain-of-grounding mechanism is proposed to further boost the performance with interleaved reasoning and grounding steps during inference. Extensive experiments on the proposed benchmark validate the effectiveness of our proposed approach. Codes, datasets, and benchmarks will be available at https://zcmax.github.io/projects/ScanReason.", "sections": [{"title": "1 Introduction", "content": "Understanding and reasoning in the 3D visual world is critical for applications such as robotics and augmented reality, where embodied agents are expected to understand the 3D layout and predict the 3D locations of objects based on human instructions. The example in Fig. 1 demonstrates a scenario where the question can only be solved with a comprehensive understanding of the 3D scene and joint reasoning of the question and the 3D environment. However, current 3D visual grounding models [23,38,42,49] trained on [1,6] localize objects based on explicit descriptions of the object category, attribute, and 3D spatial relationships, and lack the ability to reason the user intentions and predict object locations with implicit human instructions such as \"I am thirsty, can I have something to drink?\".\nTo bridge the aforementioned gap and to push the boundaries of what embodied agents can understand and how they can interact with the 3D world, we propose a new task of 3D reasoning grounding and introduce a new benchmark"}, {"title": "2 Related Work", "content": "3D Vision and Language Learning 3D Vision-language learning (3D-VL) is garnering increasing attention, with many 3D-VL tasks focusing on how to connect the 3D world with natural language. Among them, 3D Question Answering (3D QA) [2,44] aims to enable models to provide text answers based on natural language questions. Situation Question Answering in 3D Scenes (SQA3D) [28] requires an agent to first understand its location based on a text description, then provide a reasonable answer based on the surrounding environment, which can be seen as an extension of 3D QA in the embodied AI area. 3D Visual Grounding [1, 3, 6, 27, 30, 38] demands that models identify and locate target objects in a 3D scene based on given descriptions, outputting the objects' coordinates and 3D bounding boxes. These descriptions usually explicitly rely on the objects' attributes and their spatial relationships. 3D Dense Captioning [4, 7, 13, 15, 24, 35, 45, 48] requires models to output a series of object coordinates and corresponding scene-based descriptions based on a given scene. Different from these 3D-VL tasks, the questions in 3D reasoning grounding could be more implicit and complex.\n3D Visual Grounding The task of 3D visual grounding is aimed at localizing the objects that are explicitly referred to by free-form guided language expressions in the 3D scene. Inspired by the success of transformers in natural language processing, recent 3D visual grounding approaches [11,16,23,38,49,52] have started to adopt transformer [34] architectures for handling the complex"}, {"title": "3 ScanReason Benchmark", "content": "3.1 3D Reasoning Grounding Task\nGiven a 3D scene and an implicit free-form query, 3D reasoning grounding requires the model to predict the 3D bounding boxes of the target objects as well as the textual answers and explanations. As shown in Fig. 2, different from traditional 3D visual grounding, the queries of 3D reasoning grounding are implicit and complex, requiring strong reasoning, commonsense, and world knowledge. The number of target objects in 3D reasoning grounding is flexible, and any object in the 3D scene that satisfies the query requirements should be considered as the target object.\n3.2 Question Types\nTo comprehensively evaluate the 3D reasoning grounding abilities, we define 5 types of questions depending on which type of reasoning is required. Spatial"}, {"title": "4 Method", "content": "Solving the task of 3D reasoning grounding requires the synergization of the perception, reasoning, and grounding capability of the embodied agent. Intuitively, we can first conduct reasoning based on the implicit instruction such as \"where should first aid kits be placed?\" and the visual environment. The reasoning process provides us with information about the rough location and semantics of the object we are looking for. Then keeping that information in mind, we look back to the 3D environment to precisely locate the object. For complex scenarios, alternate reasoning and looking back are required for multiple rounds until we obtain the final answer.\nInspired by this intuition, we propose ReGround3D, consisting of a visual-centric reasoning module and a 3D grounding module with geometry-enhanced look-back, as illustrated in Fig. 3. The visual-centric reasoning module (Sec. 4.1) performs joint reasoning of language instruction and visual scene, and predicts"}, {"title": "4.1 Visual-Centric Reasoning", "content": "Due to the complexity of the 3D scene and user instructions, particularly given the implicit intention behind the human instruction, ReGround3D starts with a visual-centric reasoning module that can perceive the scene, comprehend the human instructions, and conduct joint reasoning of 3D scene and instructions. We believe the reasoning process eventually implies the grounding intention, i.e., implicitly encodes the information indicating the target object to solve the task. Thus, we design the visual-centric reasoning module to predict grounding queries for localizing the target objects in the following 3D grounding module.\nSpecifically, for simplicity, we leverage 3D-LLM to serve as the visual-centric reasoning module because of its strong reasoning abilities inherited from the LLM. Based on the BLIP2 architecture [25], 3D-LLM uses pre-trained image encoders to extract multi-view 2D image features and unprojects them into 3D spaces. The visual features are encoded by the Q-Former to produce 32 tokens as the visual input to the LLM. By leveraging the pretrained image encoders and the Q-Former, the visual tokens encode rich semantics but lack the 3D structures, spatial interactions, and fine-grained details. Therefore, instead of directly predicting the object locations by the 3D-LLM, we ask the 3D-LLM to predict the feature representation as the output of the reasoning process, and the predicted feature is further used to ground the target object in the grounding module.\nTo enable the prediction of the grounding feature, we expand the original vocabulary of the 3D-LLM with a special <LOC> token. The <LOC> token is laden with the contextual scene and the target object information which can guide the 3D grounding module to accurately localize target objects."}, {"title": "4.2 3D Grounding with Geometry-Enhanced Look-Back", "content": "Once obtaining the <LOC> token, ReGround3D extracts the last-layer embedding hloc of the <LOC> token and sends it into the 3D grounding module to predict the 3D bounding boxes. The 3D grounding module is devised with a \"look-back\" mechanism which allows the model to access the 3D geometry and fine-grained details from a 3D point cloud encoder. The fine-grained geometry-enhanced 3D visual features and hloc are sent into a query selection module to retrieve the most relevant object features. Those features are further decoded into 3D bounding boxes with the 3D box decoder."}, {"title": "4.3 Chain-of-Grounding Mechanism", "content": "Existing design conduct the reasoning and grounding process sequentially, i.e., the reasoning process is finished before grounding. We argue that the grounding results can also facilitate the reasoning process, especially for those requiring spatial information. Thus, to further synergize the reasoning and grounding process, inspired by chain-of-thought (CoT) [37], we propose Chain-of-Grounding (CoG), which introduces a chain of interleaved steps of reasoning and grounding to find the targeted objects during inference, as shown in Fig. 4. Such a process allows the model to actively find relevant objects that help solve the problem, and then conduct reasoning with the assistance of the additional information of these relevant objects so that the model can more precisely find the target objects.\nSpecifically, given a question provided by users, CoG translates it into another question of finding the explicit mentioned objects in the original question. The generated new question is sent into ReGround3D to ground the objects mentioned in the original question in the 3D scene with corresponding confidence scores. An object can be seen as a successfully located object when its confidence score is above the threshold, and the located object information could serve as explicit guidance for 3D-LLM in the next reasoning stage. As shown in Fig. 4, after obtaining the 3D information of objects explicitly mentioned in the original question, the located object information is inserted to update the question. The updated question is then sent to ReGround3D to perform reasoning and grounding to output the target object locations."}, {"title": "4.4 Instruction Tuning", "content": "Training Objective We use the pretrained weights of 3D-LLM as the initialization for the visual-centric reasoning module. Except for freezing the 3D visual encoder pretrained on [36], the rest of parameters in the visual-centric reasoning module and 3D grounding module in our framework are trained in an end-to-end manner. The training supervision is a weighted sum of the next token prediction loss from 3D-LLM and the 3D detection loss from the 3D grounding module."}, {"title": "5 Experiment", "content": "5.1 Implementation Details\nNetwork Architecture For the 3D grounding module, we adopt the pretrained multi-modal 3D encoder in EmbodiedScan as the 3D visual encoder. During the trainning stage, We use LoRA to effeciently finetune the 3D-LLM to preserve the original 3D scene understanding capability and reduce the computation costs. The number of object queries k in the query selection module is set to be 256."}, {"title": "Training Parameters", "content": "The training is done on 8 NVIDIA A100 GPUs. We adopt AdamW optimizer with a learning rate of 1e-4 and use a learning rate scheduler WarmupDecayLR with the warmup steps of 100. The total batch size is set to be 16. The loss weight parameters text and \u03bbdet in total loss L are set to 1.0 and 1.0, respectively, and the weight \u03bbL1, \u03bbIOU and \u03bbcontrast in Ldet are set to 1.0, 2.0 and 1.0."}, {"title": "5.2 Results Comparison", "content": "Evaluation on 3D Visual Grounding In order to verify the superiority of our model in grounding ability and facilitate comparison between current models, we report the explicit grounding performance on the existing 3D visual grounding task. Since the evaluation settings of Nr3D and Sr3D [1] are based on ground-truth object proposals, while ScanRefer [6] requires models to output 3D bounding boxes, we choose ScanRefer as our benchmark for comparison. We divide the existing methods into two categories, one is the grounding model designed specifically for the 3D visual grounding task, and the other is generalist MLLMs which can understand a variety of 3D vision-language tasks. The original 3D-LLM embed 3D locations in the vocabularies and represent the grounded 3D bounding boxes by a sequence of discrete location tokens. However, since the fine-tuned model of 3D-LLM on ScanRefer and related location tokens implementation are not accessible, we adapt 3D-LLM to directly output 3D numerical coordinates representing 3D bounding boxes by fine-tuning the pre-trained model on our reformulated 3D visual grounding data, denotes as 3D-LLM (vg). As shown in Tab. 2, current generalist MLLM models still lag behind the specialist models in terms of the grounding ability. By incorporating the 3D grounding module into MLLM, ReGround3D shows the SOTA performance on the traditional 3D visual grounding task."}, {"title": "Instruction Tuning Dataset", "content": "We load the pretrained weights of 3D-LLM and the 3D visual encoder, and finetunes the LORA of 3D-LLM, the query selection module, and the 3D box decoder with an instruction tuning dataset. To construct the instruction tuning dataset, we reformulate the data annotations from existing 3D datasets into question-answer or question-answer-bounding-box pairs. Specifically, the 3D visual grounding data from ScanRefer [6], SR3D, NR3D [1] and the 3D object detection data from EmbodiedScan [36] are formulated into question-answer-bbox pairs, and the 3D/spatial question answering data from SR3D [1], CLEVR3D [40], SQA3D [28] are formulated into question-answer pairs without bounding boxes. The information shown in Tab. 1 illustrates how we unify the instruction and output with task-specific templates. More details can be found in the Appendix. The reformulated data combined with our proposed ScanReason dataset, serve as the instruction tuning dataset."}, {"title": "Effectiveness of 3D Grounding Module", "content": "In order to more comprehensively verify the effectiveness of the 3D grounding module we proposed, we step by step verify the performance changes from 3D-LLM to ReGround3D. Apart from the 3D-LLM(vg), we fine-tuned the 3D-LLM model respectively on full reformulated existing data and all instruction tuning data, including ScanReason, denoted as"}, {"title": "5.3 Ablation Study", "content": "In this section, we conduct an extensive ablation study to verify the effectiveness of each component."}, {"title": "Evaluation on 3D Reasoning Grounding", "content": "The various visual grounding models rely on explicit text-object alignment in the input object expression to achieve localization, which fall to be applied to 3D reasoning grounding task. We performed a comparision between our proposed method ReGround3D and existing MLLM methods, including 3D-LLM(vg) and Chat3D-v2 [21]. Besides, inspired by Chat-3D v2, which first segments objects, then equips them with unique object identifiers to conduct effective object grounding, we set up a LLM-based 3D reasoning grounding baseline: We first segment the objects from the scene using a 3D instance segmentor [31], then convert the segmented object information including their categories and 3D bounding boxes into text as input of LLM (InternLM2-7B [32]). Besides, to better validate the performance of our model and ensure the fair comparison, we remove the ScanReason dataset from the training data, denoted as ReGround3D*. As show in Tab. 3, the LLM-based reasoning method (Mask3D [31] + InternLM2-7B [32]) possesses a very strong function reasoning ability, but struggles understanding of 3D spatial relationship. Additionally, irrespective of whether ScanReason is used in training, our model significantly outperforms the existing MLLMs. By synergize the reasoning and grounding process utilizing the Chain-of-Grounding (CoG) mechanism, the 3D reasoning grounding performance of ReGround3D can be further improved (from 28.98 to 30.62), especially on the spatial reasoning and logistic reasoning questions. The qualitative results comparison shown in Fig. 5 demonstrates superiority of our method in reasoning human complex instruction based on the 3D scene."}, {"title": "Discussion of CoG Mechanism", "content": "While CoG Mechanism boosts the performance with interleaved reasoning and grounding steps during inference, it uses the relevant objects information explicitly presented in the question to help find the target objects. A natural question arises: if we input the information of all the objects instead of relavant objects into ReGround3D during CoG, will this make the model more accurately find the target objects? We first use the existing 3D Segmentor [31] to segment all objects in the scene, then update the original question using all the object 3D bounding boxes information according to the template in Sec. 4.3 and send into ReGround3D. However, experiments show that using all the object information will instead reduce the 3D reasoning grounding performance from 28.98 to 27.67. One possible reason could be the model's attention is dispersed by too many irrelavent objects."}, {"title": "6 Conclusion", "content": "This paper introduces a new 3D vision language learning task: 3D reasoning grounding, which requires the model to perform active reasoning over complex and implicit human instruction, localize the target objects and give corresponding explanation. Furthermore, we propose ScanReason, a new dataset and benchmark to further unlock and throughly evaluate the 3D reasoning grounding capability. Based on this dataset, we propose a novel approach: ReGround3D, which utilizes the strong reasoning capability of MLLM guiding the 3D grounding module to obtain accurate object locations, and a Chain of Grounding (CoG) mechanism is presented to further boost the performance with interleaved reasoning and grounding steps during inference. We believe that our work will further the natural interaction between embodied agents and humans in open 3D environments. For the current ScanReason benchmark, we find that the questions in three high-level 3D reasoning categories may have overlaps. For a certain reasoning question, similar questions may appear in one or two other categories. We leave the problem as a future challenge for better reasoning grounding ability evaluation."}, {"title": "L", "content": "= \u03bbtextLtext + \u03bbdetLdet"}, {"title": "Ldet", "content": "= \u03bbL1LL1 + \u03bbIOULIOU + \u03bbcontrastLcontrast"}, {"title": "7 More Evaluations", "content": "Considering the expected outputs of 3D reasoning grounding questions consist of not only the target objects 3D bounding boxes, but also text response including either demonstrating the explanation (e.g. Why to choose these objects) or offering reasonable suggestions (e.g. How to use these objects.). We argue that it is also necessary to evaluate the text response correctness. However, due to the complexity and diversity of the answer, it is non-trival to use or design a proper evaluation method which can make sure the evaluation accuracy. To ensure the evaluation accuracy with limited human and time resources, we uniformly sample 100 reasoning grounding pairs from evaluation datasets and test GPT-4 [29], 3D-LLM [20] and ReGround3D on the datasets. Then we manually score the 300 responses using an integer ranging from 1 to 5, while 1 indicates an incorrect answer, 5 is a correct answer. The matching score di represents levels of the similarity between the response and ground-truth answer. The correctness metric is denoted as :"}, {"title": "S", "content": "= 1/N \u2211(\u03b4i/4) \u00d7 100%"}, {"title": "8 More Visualizations", "content": "8.1 More Examples\nIn this section we will illustrate more examples of our ScanReason benchmark for each type of reasoning questions in Fig. 7, Fig. 8, Fig. 9, Fig. 10 and Fig. 11. Each example consists of the reasoning question, target object locations (3D bounding boxes) and the corresponding text response.\n8.2 More Qualitative Results\nIllustrated in the qualitative results in the paper, we find that our model tends to output much less 3D bounding boxes compared with the ground-truth 3D bounding boxes when multiple objects are regarded as the target objects. Besides, as shown in Fig. 6, even if the 3D grounding module is introduced to more accurately localize the target objects, ReGround3D still struggles to recognize and localize the small and long-tailed objects in the 3D scene."}, {"title": "9 ScanReason Annotations Generation Prompts", "content": "We show five prompt templates for generating five types of reasoning question-answer-location pairs, each comprising system messages and manually crafted context examples. In our attempts, since GPT-3.5 struggles to understand the 3D spatial relationships of objects in the scene based on the provided 3D spatial coordinates of objects, we resort to GPT-4 for data generation, which is verified to be much better than GPT-3.5 in understanding the spatial relationships. We input the category information and 3D bounding boxes of the objects in the 3D scenes, providing information about the semantics and spatial locations of the scene in a textual representation. Then we provide specific instructions to the GPT-4 [29] to generate diverse data. As shown in Fig. 12, Fig. 13, Fig. 14, Fig. 15 and Fig. 16, to further make the generated question-answer-location pairs more"}, {"title": "10 Details of Instruction Tuning Datasets", "content": "10.1 Data Reformulation\n3D Object Detection data. Generally speaking, 3D object detection datasets contain information about 3D bounding boxes of all objects in the pre-defined list of categories. In order to cover as many objects as possible, we chose to construct question and answer pairs based on the EmbodiedScan [36] dataset, which includes 160k 3D-oriented boxes spanning over 760 categories. During the model training process, we convert the annotations of 3D boxes into a specific question answering pair template: \"User: <scene> Where is the <category> in this 3D Scene? Assistant: Sure, <LOC>.\" Here, <category> is randomly selected from the ground-truth categories contained in the current 3D scene, <scene> is the placeholder of 3D scene tokens."}, {"title": "3D Visual Grounding data", "content": "3D visual grounding data aims at localizing the unique object in 3D scenes given the descriptive object expression. The descriptions of objects in these data typically explicitly include the object attributes and their spatial relationships with other objects. To ensure diversity in training data, we selected ScanRefer [6], SR3D [1], NR3D [1] as training data. Considering the variety of object descriptions, it is difficult to simply reformulate the object expression using a simple template like: \" User: <scene> Where is <expr> in the 3D Scene?. Assistant: It is <LOC>.\" Therefore, we choose to retain the original object description as much as possible and use a template: \"Here is a description about an object: \" <expr> \", where is the object in the 3D Scene? Assistant: It is <LOC>.\", where <expr> represents the object description in the data. Besides, we have created a range of similar question templates that are randomly selected during the training process."}, {"title": "Spatial Question Answering data", "content": "We hope the model can understand 3D position in a more natural way using numerical values in Natural Language. We use [x, y, z, dx, dy, dz] to represent a 3D box, where [x, y, z] represents the center of a 3D area and [dx,dy, dz] represents the 3D box size, these coordinates can appear anywhere in the input text. Since there are no explicit coordinate question answering pairs in the 3D vision-language datasets, we turned our attention to the SR3D dataset. SR3D is a template-based generated dataset that not only provides expressions of target objects but also provides object ids for"}, {"title": "3D Question Answering data", "content": "Considering that we expect the model can also output reasonable answers in the conversation, we also introduce 3D QA data during the training process to further enhance the model's 3D visual question answering and scene understanding capabilities. We reformulate CLEVR3D [40] data into a simple question-answer template: \"User: <scene> <question>. Assistant: <answer>.\" The SQA3D [28] dataset not only provides questions but also provides the situation in which the questions are asked. We reformulate it into a template: \"User: <scene> <situation> <question>. Assistant: <answer>.\" wherein the <question> the placeholder for the question and <situation> is the placeholder of situation while raising the corresponding question."}, {"title": "3D Reasoning Grounding data", "content": "In addition to the types of data mentioned above, We also employed our own proposed 3D reasoning grounding data to train the model, further enhancing its capability to handle complex reasoning questions. The output format is more akin to a combination of 3D Question Answering (QA) and localization, where the model's response not only includes"}, {"title": "10.2 Output Type Templates", "content": "In the actual interaction between users and the model, questions are generally not divided according to the aforementioned tasks but are more concerned with whether the model's output format meets the user's needs. For example, in the 3D QA data, there exists a question: \"Where is the pillow on the bed?\", with the corresponding answer being \"near the headboard\". Simultaneously, such questions may also appear in our reformulated 3D Object Detection and Visual Grounding datasets, where the desired model output is the specific location coordinates of the object in the scene. To make the interaction between the model and users more natural and the outputs more in line with user needs, we accordingly employ output type templates appended to user instructions. Such instructions enable the training data to break free from the constraints of its original task and integrate more naturally according to the output type, thereby further enhancing the model's understanding and response to complex and varied inputs in natural dialogue."}]}