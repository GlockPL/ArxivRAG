{"title": "RenderWorld: World Model with Self-Supervised 3D Label", "authors": ["Ziyang Yan", "Wenzhen Dong", "Yihua Shao", "Yuhang Lu", "Haiyang Liu", "Jingwen Liu", "Haozhe Wang", "Zhe Wang", "Yan Wang", "Fabio Remondino", "Yuexin Ma"], "abstract": "Abstract-End-to-end autonomous driving with vision-only is not only more cost-effective compared to LiDAR-vision fusion but also more reliable than traditional methods. To achieve a economical and robust purely visual autonomous driving system, we propose RenderWorld, a vision-only end-to-end autonomous driving framework, which generates 3D occupancy labels using a self-supervised gaussian-based Img2Occ Module, then encodes the labels by AM-VAE, and uses world model for forecasting and planning. RenderWorld employs Gaussian Splatting to represent 3D scenes and render 2D images greatly improves segmentation accuracy and reduces GPU memory consumption compared with NeRF-based methods. By applying AM-VAE to encode air and non-air separately, RenderWorld achieves more fine-grained scene element representation, leading to state-of-the-art performance in both 4D occupancy forecasting and motion planning from autoregressive world model.", "sections": [{"title": "I. INTRODUCTION", "content": "With the wide application of autonomous driving [1], [2], [3], researchers gradually focus on better perception and forecasting methods [4], which are related to the decision-making ability and robustness of the system [5], [6]. Most current frameworks consist of perception [7], forecasting, and planning separately [8]. The most commonly used perception method is 3D target detection using vision and LIDAR fusion [3], [9], [1], allowing the model to better forecast future scenes and do motion planning. Since most 3D target detection methods [10], [11], [12] are unable to obtain fine-grained information in the environment, they are non-robust in planning [13] in the subsequent model, which affects the system safety. Current perception methods primarily utilize both LiDAR [14], [15] and cameras [16], but the high cost of LiDAR and the computational demands of multimodal fusion pose challenges to the real-time performance and robustness of autonomous driving systems.\nIn this paper, we introduce RenderWorld, an autonomous driving framework for prediction and motion planning, which is trained on 3D occupancy labels generated by a Gaussian-based Img2Occ module. RenderWorld proposes an self-supervised Img2Occ module with Gaussian Splatting [17], trained on 2D multi-view depth and semantic images to generate 3D occupancy labels required for the world model. To enable the world model to better understand the scene represented by 3D occupancy, we propose the Air Mask Variational Autoencoder (AM-VAE) upon a vector-quantized variational autoencoder (VQ-VAE) [18]. This improves the inference capability of our world model by enhancing the granularity of the scene representation.\nIn order to verify the efficiency and reliability of RenderWorld, we evaluate the 3D occupancy generation and motion planing on NuScenes [19] separately. In summary, our contributions are mainly as follows:\n1) We propose RenderWorld, a pure 2D autonomous driving framework that uses labeled 2D images to train a Gaussian-based occupancy prediction module (Img2Occ) for generating the 3D labels required by the world model.\n2) To improve spatial representation abilities, we introduce AM-VAE, which improves forecasting and planning in world models while reducing memory consumption by separately encoding air and non-air voxels."}, {"title": "II. RELATED WORK", "content": "A. 3D Occupancy Prediction\n3D occupancy is gaining attention as a viable alternative to LiDAR perception [20]. Most previous works [21], [22], [1], [23] utilize 3D Occupancy Ground Truth for supervision, which is challenging to annotate. With the widespread adoption of Neural Radiance Fields (NeRF) [24], some methods [25], [20], [26], [27], [28], [29] have attempted to use 2D depth and semantic labels for training. However, using continuous implicit neural fields to predict occupancy probabilities and semantic information often leads to high memory cost. Recently, GaussianFromer [30] leverages sparse Gaussian points as a means of reducing GPU consumption to describe 3D scenes while GaussianOcc [31] utilizes a 6D pose network to eliminate the reliance on ground truth poses, but both of them suffers from a significant drop in overall segmentation accuracy. In our work, we employ an anchor-based Gaussian initialization method to gaussianize voxel fratures and represent the 3D scenes with denser Gaussian points that achieving higher segmentation accuracy while avoiding the excessive memory consumption of ray sampling in NeRF-based methods.\nB. World Model in Autonomous Driving\nWorld models [32] are often used for future frame prediction and to assist robots in making decisions [33]. As end-to-end autonomous driving [8], [34] is gradually evolving,"}, {"title": "III. METHODOLOGY", "content": "In this section, We describe the overall implementation of RenderWorld. We firstly propose an Img2Occ Module for occupancy prediction and 3D occupancy labels generation (Sec III-A). Subsequently, we introduce a module based on the Air Mask Variational Autoencoder (AM-VAE) to optimize occupancy representation and enhance data compression efficiency (Section III-B). Finally, we elaborate on how to integrate the World Model for accurate prediction of 4D scene evolution (Section III-C).\nA. 3D Occupancy prediction with Multi-frame 2D Labels\nTo enable 3D semantic occupancy prediction and future 3D occupancy labels generation, we design an Img2Occ Module which is illustrated in Figure 2. Using images from multi-cameras {Imgi}Ni=1 as inputs, we firstly extract 2D image features using a pretrained BEVStereo4D [42] backbone and Swin Transformer [43]. Then, these 2D messages are interpolated into 3D space to produce volume features by leveraging the known intrinsic parameters {I}Ni=1 and extrinsic parameters {E}Ni=1. To project the 3D occupancy voxels onto multi-camera semantic maps, we apply Gaussian Splatting [17], an advanced real-time rendering pipeline.\nInspired by [44], we initialize anchor points with a learnable scale at the center of each voxel to approximate scene occupancy. The attributes of each anchor are determined based on the relative distance and viewing direction between the camera and the anchor. This anchor set is then used to initialize a Gaussian set with semantic labels {G}N1. Each Gaussian point x is then represented by a full 3D covariance matrix \u03a3 in world space and its center position \u00b5, and the color of each point is decided by the semantic label at that point.\n\\(G(x) = e^{-(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\\)\nDirectly optimizing \u03a3 may lead to infeasible matrices as it must be positive semi-definite. To ensure the validity of \u03a3, it is decomposed into the scaling matrix S and the rotation matrix R to characterize the geometry of a 3D Gaussian ellipsoid:\n\\(\\Sigma = RSST^R\\)\nThen the 3D Gaussians are projected to 2D for rendering by computing the camera space covariance matrix \u03a3' :\n\\(\\Sigma' = JW\\Sigma WT JT,\\)\nwhere J is the Jacobian matrix of the affine approximation of the projection transformation and W is the viewing transformation. The semantic / depth of each pixel can then be calculated by applying alpha blending onto sorted Gaussians:\n\\(D = \\sum_{i}^{N}(d_i)a_i \\prod_{j}^{i-1}(1-a_j)\\),\n\\(S = \\sum_{i}^{N}(S_i)a_i \\prod_{j}^{i-1}(1-a_j),\\)\nwhere si di is the rendered semantic / depth of a 3D Gaussian, a\u017c is the product of an evaluated 2D Gaussian projection and its corresponding opacity.\nTo calculate the difference between ground truth depth and rendered depth, we utilize the Pearson correlation which can measure the distribution difference between 2D depth maps follows the following function:\n\\(L'_{dep} = Corr(d_i, d_i) = \\frac{Cov(d_i, d_i)}{Var(d_i, d_i)},\\)\nwhere di is the ground truth depth image and di is the rendered depth image.\nFinally, we construct the loss function with a cross-entropy loss Lsem for supervising semantic segmentation and Ldep for depth supervision, the overall loss can be computed as follows:\n\\(L_{i20} = L'_{sem} + L'_{dep}\\)\nUsing the well-trained checkpoint, we generate 3D occupancy labels, which are then input into the subsequent AM-VAE module.\nB. Air Mask Variational Autoencoder (AM-VAE)\nTraditional Variational Autoencoders (VAEs) fail to encode the distinct features of non-air voxels which hampers the model to represent scene elements as fine-grained level. To address this issue, we introduce the Air Mask Variational Autoencoder (AM-VAE), a novel VAE involves training two distinct Vector Quantized Variational Autoencoders (VQ-VAE) [18] to encode and decode air and non-air occupancy voxels separatly.\nAssuming o represents the input occupancy representation, and OAir and ON-Air represent the air and non-air voxels. We first utilize a 3D convolutional neural network to encode the occupancy data, with the output being a continuous latent space representation denoted as f. The encoder q\u03c6(s|o) maps the input f to the latent space s. Then, we use two latent variables sAir and sN-Air to represent the air and non-air voxels, respectively:\n\\(s_{Air} \\sim q_{\\phi}(s_{Air} | o_{Air}), s_{N-Air} \\sim q_{\\phi'}(s_{N-Air} | o_{N-Air})\\)\nEach encoded latent variable sAir or sN-Air uses learnable codebook CAir or CN-Air to obtain discrete token, which is then replaced by the most similar codebook entry before being fed into the decoder. This process is represented as:\n\\(s'_{Air} = arg\\min_{c_{Air} \\in C_{Air}} ||s_{Air} - c_{Air}||,\\)\n\\(s'_{N-Air} = arg\\min_{c_{N-Air} \\in C_{N-Air}} ||s_{N-Air} - c_{N-Air}||\\)\nThen, the decoder p\u03b8(o|s) reconstructs the input occupancy from the quantized latent variables s'Air and s'N-Air:\n\\(\u00f4_{Air} = p_{\\theta}(o_{Air} | s'_{Air}), \u00f4_{N-Air} = p_{\\theta}(o_{N-Air} | s'_{N-Air})\\)\nTo facilitate the separation of air and non-air elements within the occupancy representation, we denote M as the set of non-air categories. Then the indicator function for air and non-air in the modified occupancy can be defined as follows:\n\\(I_M(o) = \\begin{cases}1 \\text{ if } o \\in M,\\\\0 \\text{ otherwise.}\\end{cases}\\)\nThe modified air occupancy O'Air and non-air occupancy O'N-Air are given by the following equations:\n\\(o'_{Air} = (1 - I_M(o)) \\cdot o_{Air},\\)\n\\(o'_{N-Air} = I_M(o) \\cdot o_{N-Air} + (1 - I_M(o)) \\cdot o_{Air}\\)\nTo reconstruct the original occupancy representation, we use a mask = (\u00d4Air != 0) to distinguish areas filled only with air. Then the reconstructed occupancy \u00f4 combines the air and non-air components as follows:\n\\(\u00f4 = \u00f4_{Air} \\cdot mask + \u00f4_{N-Air} \\cdot (1 - mask)\\)\nWe then build the loss function LVAE for training the AM-VAE with reconstruction loss LRecon and commitment loss LReg:\n\\(L_{Recon} = E_{q_{\\phi}(s_{Air} | o_{Air})}[log p_{\\theta}(o_{Air} | s'_{Air})] + E_{q_{\\phi'}(s_{N-Air} | o_{N-Air})}[log p_{\\theta}(o_{N-Air} | s'_{N-Air})],\\)\n\\(L_{Com} = ||s_{Air} - s'_{Air}||^2 + ||s_{N-Air} - s'_{N-Air}||^2,\\)\n\\(L_{VAE} = L_{Recon} + \\beta L_{Com}\\)\nAM-VAE utilizes separate codebooks for air and non-air voxels within a unified encoder-decoder setup. This method effectively captures the unique features of each voxel type, thereby improving both reconstruction accuracy and generalization potential.\nC. World Model\nBy applying a world model in autonomous driving to encode 3D scenes into high-level tokens, our framework can effectively capture environmental complexity, enabling accurate autoregressive anticipation of future scenarios and vehicle decisions.\nInspired by OccWorld [45], we use a 3D occupancy to represent the scene and employ a self-supervised tokenizer to derive high-level scene tokens T, and encode the spatial position of vehicles by aggregating the vehicle token zo. The world model is defined as w based on the current timestamp T and the number of historical frames t, then we establish the prediction with the following formula:\n\\(w(T_T, ..., T_{T-t}) = T_{T+1},\\)\nwhere TT+1 represents the scene tokens at the next time step.\nAt the same time, a temporal generative transformer architecture is adopted to effectively predict the future scene. It firstly processes scene tokens through spatial aggregation and downsampling, and then generates a hierarchical set of tokens {To,\u2026\u2026,TK}. So as to predict the future at different spatial scales, we take multiple sub-world models w = {wo,\u2026\u2026,w\u03ba} to achieve it and each sub-model wi applies temporal attention to the tokens at each position j using the following formula:\n\\(z^{T+1}_{ji} = TA(z^T, z^{<T}_{ji}),\\)\nwhere TA represents masked temporal attention, which predicts future tokens from influencing previous tokens. zT denotes the j-th world token at scale i and timestamp t.\nIn the prediction module, we firstly utilize a self-supervised tokenizer e to convert the 3D scene into high-level scene tokens T, and a vehicle token zo to encode the spatial position of the vehicle. After predicting the future scene tokens, a scene decoder d is applied to decode the predicted 3D occupancy \u0177T+1 = d(zT+1), and learn a vehicle decoder dego which is for generating the vehicle displacement that relative to the current frame pT+1 = dego(z0). The prediction module provides decision support for trajectory optimization of the autonomous driving system by generating continuous predictions of future vehicle displacements and scenario changes, ensuring safe and adaptive path planning.\nWe have implemented a two-stage training strategy to effectively train our prediction module. In the first phase, we train the scene tokenizer e and the decoder d using a 3D occupancy loss:\n\\(L_{e,d} = L_{soft}(d(e(y)), y) + \\lambda_1 \\cdot L_{lovasz}(d(e(y)), y),\\)\nwhere Lsoft denotes the softmax loss and Llovasz represents the Lovasz-softmax loss. The term \u03bb\u2081 serves as a balancing factor between them.\nThen we use the learned scene tokenizer e to obtain the scene tokens z for all frames and constrain the difference between the predicted tokens 2 and z. And a softmax loss is used to enforce the correct classification of 2 to the correct code in the codebook C. For the vehicle token, we simultaneously learn the vehicle decoder dego and apply an L2 loss on the predicted displacement p = dego(z0) and the ground truth displacement p. The overall loss in phase two can be formulated as follows:\n\\(L_{w,d_{ego}} = \\sum_{t=1}^{T} \\sum_{j=1}^{M_0} (L_{soft} (\\hat{z}^t_{j,0}, C(z^t_{j,0})) + \\lambda_2 L_{L2}(d_{ego}(z^t_{j,0}), p^t)),\\)\nwhere T and Mo are the number of frames and the number of spatial tokens at the original scale, respectively. C(.) denotes the index of the corresponding code in the codebook C. LL2 measures the L2 difference between the two trajectories."}, {"title": "IV. EXPERIMENTS", "content": "In this section we evaluate the performance of Render-World using NuScenes [19] dataset. We also performed extensive ablation experiments on the same dataset as reported in sub-section C - to deeper understand the proposed approach.\nA. Experimental Setup\nWe adopt NuScenes as our evaluation dataset. NuScenes is a large-scale autonomous driving dataset that includes 700 scenes for training, 150 scenes for validation, and 150 scenes for testing, totaling approximately 40,000 frames across 17 classes. For self-supervised training, we generate ground truth depths and 2D segmentation ground truths by projecting LiDAR point clouds with their 3D segmentation labels onto corresponding 2D views. During the semantic occupancy prediction, each sample covers a range of [x:(-40 m, 40 m), y:(-40 m, 40 m), z:(-1.0 m, 5.4 m)] with a voxel size of 0.4 m. The evaluation experiments of our model are conducted on the 150 validation sets with one NVIDIA A30 GPU.\nB. Main Result\n3D semantic occupancy prediction: To demonstrate the performance of our model, we compare it against 10 occupancy prediction models, which are the existing common models evaluated on the NuScenes dataset. The results in Table I indicate that RenderWorld outperforms most state-of-the-art occupancy prediction methods in mIoU, ranking second overall, and only surpassed by CTF-OCC [46], which uses 3D occupancy GT as input. Furthermore, our method achieves outstanding performance in vehicle segmentation, including trailers, construction vehicles, trucks, etc and surpasses all other methods in segmenting various environmental terrains, such as vegetation, sidewalk etc. This is due to the 3D Gaussian representation, which effectively leverages the sparsity and object diversity in driving scenes, scaling with flexible location and covariance properties [30].\n4D occupancy forecasting: We evaluated the 4D occupancy forecasting performance under several settings as shown in Table II\nIn order to capture finer-grained scene features and provide precise information for predictions, air-separation technique is applied to prioritize crucial non-air components in the scene, boosting prediction accuracy and computational efficiency. The results show that RenderWorld can generate non-trivial future 3D occupancy, with results far superior to OccWorld and Copy&Paste, which indicates that our model learns the underlying scene evolution.\nMotion planning: As shown in Table III, We compare the motion planning performance between the proposed Render-World and state-of-the-art methods, and evaluate our model across various settings used in the 4D occupancy forecasting task. RenderWorld outperforms all compared methods in L2 metrics when takes 3D occupancy as input. Without any auxiliary support, our approach also achieves competitive results in collision rate and even outperforms OccWorld-S in when only uses 2D as input.\nC. Ablation Study\nWith the aim of showing the effectiveness of our innovative modules, we conduct three ablation studies and the results are shown in Table V, IV and Table VI:\nEfficiency comparisons among different representations: In Table V, we present the efficiency comparisons of various representations, highlighting that 3D Gaussian surpasses all competitors with significantly reduced memory usage. Leveraging its explicit representation, this approach assigns specific semantic data to individual 3D Gaussians, facilitating the transition from scene depiction to occupancy forecasts. This method also circumvents the high memory usage linked to the ray initialization step in NeRF-based techniques. Although our method has higher GPU memory overhead compared to GaussianFormer, it avoids the trade-off of reducing the number of Gaussian points to save memory, but leading to a loss of semantic information."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduce RenderWorld, a end-to-end autonomous driving framework trained on 3D occupancy labels generated by a Gaussian-based Img2Occ module and use world model for forecasting and motion planning. By leveraging Gaussian Splatting and AM-VAE, we successfully reduce GPU memory usage by at least half in 3D occupancy label generation compared to NeRF-based approaches, while simultaneously attaining minimal memory requirements in 4D occupancy forecasting. Experimental results demonstrate that our approach can achieve state-of-the-art performance in semantic segmentation of 3D occupancy, 4D occupancy forecasting with 2D input and motion planning among all input types. Our work offers a valuable contribution to the autonomous driving community, enhancing real-time, efficient robot perception, forecasting and motion planning."}]}