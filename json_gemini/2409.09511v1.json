{"title": "Explaining Deep Learning Embeddings for Speech Emotion Recognition by Predicting Interpretable Acoustic Features", "authors": ["Satvik Dixit", "Daniel M. Low", "Gasser Elbanna", "Fabio Catania", "Satrajit S. Ghosh"], "abstract": "Pre-trained deep learning embeddings have consistently shown superior performance over handcrafted acoustic features in speech emotion recognition (SER). However, unlike acoustic features with clear physical meaning, these embeddings lack clear interpretability. Explaining these embeddings is crucial for building trust in healthcare and security applications and advancing the scientific understanding of the acoustic information that is encoded in them. This paper proposes a modified probing approach to explain deep learning embeddings in the SER space. We predict interpretable acoustic features (e.g., f0, loudness) from (i) the complete set of embeddings and (ii) a subset of the embedding dimensions identified as most important for predicting each emotion. If the subset of the most important dimensions better predicts a given emotion than all dimensions and also predicts specific acoustic features more accurately, we infer those acoustic features are important for the embedding model for the given task. We conducted experiments using the WavLM embeddings and eGeMAPS acoustic features as audio representations, applying our method to the RAVDESS and SAVEE emotional speech datasets. Based on this evaluation, we demonstrate that Energy, Frequency, Spectral, and Temporal categories of acoustic features provide diminishing information to SER in that order, demonstrating the utility of the probing classifier method to relate embeddings to interpretable acoustic features.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech Emotion Recognition (SER) involves automatically identifying emotional states from spoken language [1] and is an important task in several fields, including human- computer interaction [2] and mental health assessments [3]. While conventional methods rely on handcrafted features, recent breakthroughs in this domain have come from deep neural networks, especially those trained in a self-supervised manner [1]. These networks learn speech embeddings that outperform traditional features in terms of SER accuracy [4], [5], but the mechanisms behind their success remain unclear. In particular, the question of what kind of acoustic information deep learning (DL) models use for a particular task remains largely unanswered [6]. This paper focuses on explainability by using probing classifiers to investigate the acoustic information contained in DL embeddings. Using a novel tiered prediction strategy, we aim to identify the specific interpretable acoustic feature information that is more relevant for distinguishing emotions using DL embeddings. This can enhance our understanding of the mechanisms driving the success of DL models in SER. In this study, our contributions are that we first provide insights into what types of acoustic features characterize different emotions, using the standard eGeMAPS feature set in a purely interpretable model. Second, we quantify how well these interpretable features are predicted from WavLM DL embeddings, offering insights into the information con- tained by these embeddings. Our methodological contribution is that we demonstrate which type of acoustic features are better represented in the subset of embedding dimensions that most characterize a given emotion and provide a new metric, information increase, to quantify this. We hypothesize that these acoustic features are relevant to that emotion. Our primary focus is explainability: gaining an understanding of the information encoded in these models can support their further improvement and their applications in various tasks"}, {"title": "II. RELATED WORK", "content": "Probing classifiers have been widely used to analyze text embeddings [7]. However, less work has been done in the audio domain. One recent study probed transformer-based au- dio models for emotion recognition content to understand how much information related to emotions is contained in different models and layers [8], but did not probe for specific acoustic information. Another study fine-tuned pre-trained models to detect emotional properties (a multitask output: arousal, va- lence, and dominance) [9]. They then probed these models for a set of acoustic features, comparing a pre-trained Wav2Vec 2.0 [10] model fine-tuned with an added output head versus additionally fine-tuning the transformer layers. If a feature is represented more effectively after fine-tuning the transformer layers, resulting in improved predictions of acoustic features, then it is hypothesized that this information is encoded in the model or captured by the model. Since fine-tuning these layers improved performance, more information about certain acoustic features would indicate that they are relevant to the task. However, they did not find changes in information except that audio duration information became less important for the improved model. Another recent study compares Wav2Vec 2.0 representations with selected eGeMAPS features [11], however they used canonical correlation analysis instead of probing."}, {"title": "III. METHODS", "content": "We selected two well-established emotional speech datasets to evaluate how outcomes vary between them: the Ryer-"}, {"title": "B. Audio representations", "content": "We looked at two categories of audio representations: Handcrafted acoustic features: These are interpretable fea- tures designed to capture specific aspects of the audio signal, such as intensity, frequency, spectral, and temporal elements. For this study, we used eGeMAPS, a widely adopted stan- dard set of acoustic features (implemented using OpenSMILE eGeMAPSv02 [15]), which has proven to be somewhat effec- tive for emotion recognition [16]. Deep learning embeddings: These are representations learned through neural networks that can capture complex and abstract patterns in the audio signal. In this study, we used WavLM Large [17], a pre-trained speech self-supervised model that has demonstrated state-of-the-art performance on the SUPERB benchmark for emotion recognition [5]. The em- beddings were mean-pooled over time to get one embedding per utterance."}, {"title": "C. SER classification using eGeMAPS and WavLM", "content": "As displayed in Figure 1, we employ a binary classifier for each distinct emotion category (emotion vs neutrality). Specifically, we divide the dataset such that half of the samples represent one particular emotion, while the remaining half are utterances of the neutral emotion. This is done so we can focus on the features that characterize an emotion versus neutral, which we find simpler to interpret than what characterizes an emotion versus other emotions (e.g., anger should be louder than neutral). Prior to classification, we perform speaker normalization. For the classification task, we employ Logistic Regression with L2 regularization - this helps reduce collinearity issues [18]. We perform hyperparameter tuning on the regularization parameter or 'C' with the values from the set {0.01, 0.1, 1, 10, 100} using use 5-fold nested cross-validation (i.e., the optimal parameter is chosen within the training set). Samples of a specific speaker are either in the training or test set, not both. We use F1 score to assess classifier performance."}, {"title": "D. Determining top eGeMAPS Features and WavLM Embed- ding Dimensions using SHAP", "content": "We rank the eGeMAPS features in order of importance for SER classification using SHAP [19] in order to determine the most important features and feature categories for predicting each emotion, albeit in a less optimal, but interpretable model. The eGeMAPS feature categories [16] are described in Table I. We also find the most important dimensions for the WavLM DL model for each emotion in the same way to rank each of the 1024 WavLM embedding dimensions in order of importance. We perform a post-hoc analysis to determine the minimal set of most important dimensions by taking the lowest number of features at which the performance is the highest in classifying each emotion (also with Logistic Regression with L2 regularization). To determine this set for the WavLM embedding dimensions, we sweep the feature importance vector in steps of 10 starting from the 10th most important feature. This number for the estimated minimal set for each emotion is reported in Table II. Next we described our probing approach to better understand the information encoded in these subsets of embedding dimensions."}, {"title": "E. Probing handcrafted eGeMAPS Features from Top WavLM Embedding Dimensions", "content": "We estimate how much of each acoustic feature is contained in the WavLM embeddings by the ability of the embedding to predict the feature. We train our model on the minimal subset of top dimensions to predict the eGeMAPS features one at a time. For prediction, we use a Ridge regression model and do hyperparameter tuning on the regularization strength coefficient or \u2018alpha' with the values {0.001, 0.01, 0.1, 1, 10, 100}. We only used a linear classification model, as is common in probing classification, to avoid allowing more flexible models to infer new features as we wish to only look at information in the DL embeddings [7]. For every feature, we compute the information increase between all WavLM embedding dimensions and top WavLM embedding dimensions weighted by how well the feature is encoded in the minimum subset (to avoid highlighting features that are not encoded well) using the following custom metric:\nInformation increase = $\\frac{RMSE_{all}}{RMSE_{top}} * \\frac{1}{RMSE_{top}}$\nHere $RMSE_{all}$ and $RMSE_{top}$ are the RMSE for pre- dicting the given acoustic feature using all dimensions and top dimensions of the WavLM embedding. We are trying to identify handcrafted features which are encoded much better in the top dimensions of the embedding compared to all dimensions and therefore have a high value of $\\frac{RMSE_{all}}{RMSE_{top}}$. These features should also be encoded significantly well in the top dimensions of the embedding (have low error); to enforce this, we add a $\\frac{1}{RMSE_{top}}$ term which weighs the score down if the prediction by the top dimensions has a large error. This would indicate that the feature is not well captured by the top dimensions, even if they are better than using all dimensions."}, {"title": "IV. RESULTS", "content": "The DL-based embeddings outperform the handcrafted fea- tures for every emotion for both datasets in terms of F1 scores (see Table II), which justifies using the DL models."}, {"title": "V. DISCUSSION", "content": "By providing a novel probing method and metric, we demonstrate how to estimate interpretable acoustic information contained in DL-based embeddings. Our method helped us find that energy-based features have the most information increase across datasets across almost all emotions. Therefore, we hypothesize that the WavLM embeddings use this information to better perform the SER task. Even though the SHAP scores suggest different feature categories are important for every emotion, the energy based features always have the highest median information gain. The lower information increase for temporal features is consistent with the time-pooled nature of the embeddings, suggesting that some time-dependent infor- mation may be lost in the process. Furthermore, this method can help us understand why a given emotion is detected better by higher-performing DL-based models. For instance, the embeddings seem to use energy features to classify sadness more than other feature categories, but energy is similarly important to other categories in the handcrafted model, which may explain its lower performance as shown in Table II. More generally, we see an ordering of feature categories that correlates with performance; when this fails in eGeMAPS models for sad and disgust, performance drops. Overall, we can leverage methods that compare models or embeddings that are trained on the same task, but one performs better than the other, and we can compare their relative information content. A limitation of this method is that while it estimates what information from eGeMAPS is and is not encoded in DL- based embeddings, it does not imply that this is the most important information the DL-based embeddings is using for classification; it might be using other information beyond the eGeMAPS features we tested. However, given a black-box, providing even part of the information encoded using our method can improve explainability. An important future direction is investigating the general- izability of our results across more datasets and languages (including those with recordings of naturalistic emotion pro- duction such as MSP [20]). The analysis of other handcrafted features or DL-based embeddings could also be explored."}]}