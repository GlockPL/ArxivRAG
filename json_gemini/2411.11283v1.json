{"title": "Multi-Hyperbolic Space-based Heterogeneous Graph Attention Network", "authors": ["Jongmin Park", "Seunghoon Han", "Jong-Ryul Lee", "Sungsu Lim"], "abstract": "To leverage the complex structures within heterogeneous graphs, recent studies on heterogeneous graph embedding use a hyperbolic space, characterized by a constant negative curvature and exponentially increasing space, which aligns with the structural properties of heterogeneous graphs. However, despite heterogeneous graphs inherently possessing diverse power-law structures, most hyperbolic heterogeneous graph embedding models use a single hyperbolic space for the entire heterogeneous graph, which may not effectively capture the diverse power-law structures within the heterogeneous graph. To address this limitation, we propose Multi-hyperbolic Space-based heterogeneous Graph Attention Network (MSGAT), which uses multiple hyperbolic spaces to effectively capture diverse power-law structures within heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness of MSGAT. The experimental results demonstrate that MSGAT outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, the demand for effective methods to learn semantic information and complex structures in heterogeneous graphs, which consist of various node and link types, has been steadily increasing due to their ability to represent real-world scenarios. In heterogeneous graphs, metapaths are defined as sequences of node/link types. Leveraging metapaths enables us to capture semantic information and complex structures within such graphs effectively. Accordingly, recent studies [1]-[3] have focused on efficiently learning heterogeneous graph representations by leveraging metapaths.\nDespite their notable achievements, they may struggle to effectively capture complex structures (e.g., power-law structure) within heterogeneous graphs because they use Euclidean space as the embedding space. In heterogeneous graphs, we often observe hierarchical or power-law structures where the number of nodes grows exponentially, corresponding to specific metapaths. Using Euclidean space as the embedding space to learn such complex structures may result in distortions and limitations [4]. For example, as shown in Figure 1(a), given a graph with a hierarchical structure, distortions can occur where the geodesic distance between two nodes (v2 and v3) is far, but the vector distance in the embedding space is represented as close."}, {"title": "II. PRELIMINARIES", "content": "Definition 1 (Poincar\u00e9 ball model). Poincar\u00e9 ball model with curvature -c (c > 0) is defined by the Riemannian manifold (Dn,c, g), where\nDn,c = {x \u2208 Rn : c||x||\u00b2 < 1},\ng = $\\frac{4}{(1 - c||x||^2)^2}$ Id.\nHere, Dn, is the open n-dimensional unit ball with radius $\\frac{1}{\\sqrt{c}}$ and g0 is the Riemannian metric tensor where A = $\\frac{4}{(1 - c||x||^2)^2}$ and Id is the identity matrix. We denote TxDn, as the tangent space centered at point x\nDefinition 2 (M\u00f6bius addition). Given the point x, y \u2208 Dn,c, M\u00f6bius addition which represents the equation for the addition operation in the Poincar\u00e9 ball model with curvature -c (c > 0) is defined as follows:\nx \u2295c y = $\\frac{(1 + 2c(x, y) + c||y||^2) x + (1 \u2212 c||x||^2) y}{1 + 2c(x, y) + c\u00b2||x||^2||y||^2}$ ,\nwhere (\u00b7, \u00b7) is the Euclidean inner product and || \u00b7 || is the Euclidean norm.\nDefinition 3 (Exponential and logarithmic maps). In Poincar\u00e9 ball model with curvature -c (c > 0), the exponential map expx: TxDn,c \u2192 Dn,c and logarithmic map logx: Dn,c \u2192 TxDn,c are defined as shown below:\nexpx(v) = x \u2295c (tanh ($\\sqrt{c}$||v||) $\\frac{v}{\\sqrt{c}||v||}$),\nlogx(y) = $\\frac{1}{\\sqrt{c}}$-tanh\u00af\u00b9 ($\\sqrt{c}|| - x \u2295c y||$) $\\frac{-x \u2295c y}{|| - x \u2295c y||}$ ,\nwhere x and y are the points in the hyperbolic space Dn,c and x \u2260 y. v is a nonzero tangent vector in the tangent space TxDn,c.\nDefinition 4 (Hyperbolic matrix-vector multiplication). Given a point x \u2208 Dn, and a matrix M\u2208 Rm\u00d7n, the matrix multiplication operation in hyperbolic space is defined as follows:\nM c x \u2261 expo (Mlog (x)),\nwhere 0 \u2208 Rn is a zero vector.\nDefinition 5 (Hyperbolic non-linear activation function). Given the point x \u2208 Dn,c, the hyperbolic non-linear activation function is defined as follows:\n\u03c3c (x) = expo (\u03c3 (log (x))),\nwhere \u03c3 is the Euclidean non-linear activation function."}, {"title": "III. METHODOLOGY", "content": "A. Metapath Instance Sampling\nTo capture the structural properties within a heterogeneous graph G, we sample a metapath instance set P for a given embedding target node v in Vt. Each metapath instance p in Po starts from node v and has a length within a maximum"}, {"title": "B. Intra-Hyperbolic Space Attention", "content": "1) Hyperbolic mean-linear encoder: After metapath instance sampling, we compose the hyperbolic mean-linear encoder to transform all the node features within a metapath instance into a single feature. This transformation function can be formulated as follows:\nxi = $\\frac{1}{j} \\sum_{i=1}^{j} x_i^{(j)}$,\nxH,\u03c6 = Wtc exp(x).\nIn Equation (1), xi \u2208 Rn denotes the features of node i, j denotes the length of the metapath instance p, Wt \u2208 Rnxn denotes a transformation matrix, and x \u2208 Rn denotes the Euclidean feature of the metapath instance p.\nIn Equation (2), given Euclidean metapath instance feature x, we first map x to a metapath-specific hyperbolic space Dn,c,d via the exponential map exp : ToDn,c,\u00a2 \u2192 Dn,c,\u00a2. To adopt exponential map, we assume x is included in the tangent space ToDn,c,\u00a2 at point x = 0. Note that, x \u2208 Dn,c,d denotes hyperbolic metapath instance feature. Here, Dn,c,d is a metapath-specific hyperbolic space that effectively represents structural properties of metapath instances following a specific metapath \u03c6.\nAdditionally, -c (c > 0) is a learnable parameter that represents the negative curvature of hyperbolic space, and each metapath-specific hyperbolic space for metapath \u03c6 has a distinct negative curvature.\n2) Hyperbolic metapath instance embedding: We utilize hyperbolic linear transformation with a hyperbolic non-linear activation function to obtain metapath instance embedding in hyperbolic space. The formulation of this process is as follows:\nh,\u03c6 = \u03c3c (W1cx) c exp (b1).\nIn Equation (3), h, \u2208 Dd,c,\u00a2 is a latent representation of metapath instance p in metapath-specific hyperbolic space Dd,c,d. Here, d is the dimension of hyperbolic space for latent metapath instance representations. Additionally, W1 \u2208 Rd\u00d7n is a weight matrix and b1 \u2208 Rd is a bias vector.\n3) Intra-metapath specific hyperbolic space attention: To aggregate different latent metapath instance representations, we define attention mechanisms in metapath-specific hyperbolic space. First, we calculate the importance of each metapath instance \u03b1p as follows:\nep = a.log (h),\n\u03b1\u03c1 = $\\frac{exp (e_p)}{\\sum_{q \u2208P_o} exp (e_q)}$\nIn the above equations, ep denotes the importance of each metapath instance p \u2208 P\u03c6, where logo'\u00a2 : Dd,c,\u00a2 \u2192 ToDd,c,\u00a2 denotes logarithmic map function and Po denotes a subset of P consisting of metapath instances that follow a specific metapath and a \u2208 Rd is an attention vector for metapath instance. After calculating the importance of each metapath instance, we apply the softmax function to these values to obtain the weight of each metapath instance.\nThen the metapath-specific embedding for node v is obtained from the weight of each metapath instance and their latent representations in metapath-specific hyperbolic space. This process can be formulated as below:\nh = \u03c3\u03b5 (expo ($\\sum_{p\u2208P} \u03b1_p.log(h_{p}^{H, \u03c6}))$),\nwhere h\u2208 Dd,c,& denotes metapath specific embedding for node v. Note that, in Equation (3) and (6), \u03c3\u2297c denotes the hyperbolic non-linear activation function with LeakyReLU.\n4) Hyperbolic multi-head attention: As shown in the equation below, we adopt multi-head attention in hyperbolic space to enhance metapath-specific embeddings and stabilize the learning process. Specifically, we divide the attention mechanisms into K independent attention mechanisms, conduct them in parallel, and then concatenate the metapath-specific embedding from each attention mechanism to obtain the final metapath-specific embedding h.\nh = $\\parallel_{k=1}^{K}$ \u03c3\u03b5 (exp ($\\sum_{p\u2208P} \u03b1_k.log(h_{p}^{H, \u03c6}))$)."}, {"title": "C. Inter-Hyperbolic Space Attention", "content": "1) Node embedding space mapping: Once the metapath-specific embedding h is obtained for each metapath in \u03a6, we aggregate them using attention mechanisms to learn the importance of each metapath-specific embedding, which represents semantic structural information."}, {"title": "D. Model Training", "content": "As shown in Equation (12), we employ the non-linear transformation f(.) to map node embedding vectors into a space with the desired output dimension, conducting various downstream tasks.\nf(zv) = \u03c3 (Wozv),\nwhere Wo\u2208 Rdo\u00d7d' denotes the weight matrix, do denotes the dimension of the output vector, and \u03c3 is the activation function.\nThen, we train MSGAT by minimizing the loss function L.\nFor node-level tasks, MSGAT is trained by minimizing the cross-entropy loss function Ln which is defined as below:\nLn = - $\\sum_{v\u2208Vt} \\sum_{c=1}^{C}$ yv [c] log (f(z)[c]),\nwhere vt is the target node set extracted from the labeled node set, C is the number of classes, y is the one-hot encoded label vector for node v, and f(z) is a vector predicting the label probabilities of node v."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose the Multi-hyperbolic Space-based heterogeneous Graph Attention Network (MSGAT). Instead of the Euclidean space, MSGAT uses multiple hyperbolic spaces to capture various power-law structures effectively, and finally MSGAT aggregates metapath-specific embeddings to obtain more enhanced node representations.\nWe conduct comprehensive experiments to evaluate the effectiveness of MSGAT with widely used real-world heterogeneous graph datasets. The experimental results demonstrate that MSGAT outperforms the other state-of-the-art baselines. Additionally, it has been shown that using multiple hyperbolic spaces for learning various power-law distributions is effective.\nFor the future work, we plan to develop methods to enhance the interpretability of the hyperbolic spaces learned for each metapath in heterogeneous graphs."}]}