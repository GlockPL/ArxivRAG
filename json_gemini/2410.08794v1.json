{"title": "M\u00b3-Impute: Mask-guided Representation\nLearning for Missing Value Imputation", "authors": ["Zhongyi Yu", "Zhenghao Wu", "Shuhan Zhong", "Weifeng Su", "S.-H. Gary Chan", "Chul-Ho Lee", "Weipeng Zhuo"], "abstract": "Missing values are a common problem that poses significant challenges to data\nanalysis and machine learning. This problem necessitates the development of an\neffective imputation method to fill in the missing values accurately, thereby en-\nhancing the overall quality and utility of the datasets. Existing imputation methods,\nhowever, fall short of explicitly considering the \u2018missingness' information in the\ndata during the embedding initialization stage and modeling the entangled feature\nand sample correlations during the learning process, thus leading to inferior perfor-\nmance. We propose M\u00b3-Impute, which aims to explicitly leverage the missingness\ninformation and such correlations with novel masking schemes. M\u00b3-Impute first\nmodels the data as a bipartite graph and uses a graph neural network to learn node\nembeddings, where the refined embedding initialization process directly incorpo-\nrates the missingness information. They are then optimized through M\u00b3-Impute's\nnovel feature correlation unit (FCU) and sample correlation unit (SCU) that effec-\ntively captures feature and sample correlations for imputation. Experiment results\non 25 benchmark datasets under three different missingness settings show the\neffectiveness of M\u00b3-Impute by achieving 20 best and 4 second-best MAE scores\non average.", "sections": [{"title": "Introduction", "content": "Missing values in a dataset are a pervasive issue in real-world data analysis. They arise for various\nreasons, ranging from the limitations of data collection methods to errors during data transmission\nand storage. Since many data analysis algorithms cannot directly handle missing values, the most\ncommon way to deal with them is to discard the corresponding samples or features with missing\nvalues, which would compromise the quality of data analysis. To tackle this problem, missing value\nimputation algorithms have been proposed to preserve all samples and features by imputing missing\nvalues with estimated ones based on the observed values in the dataset, so that the dataset can be\nanalyzed as a complete one without losing any information.\n\nThe imputation of missing values usually requires modeling of correlations between different features\nand samples. Feature-wise correlations help predict missing values from other observed features\nin the same sample, while sample-wise correlations help predict them in one sample from other\nsimilar samples. It is thus important to jointly model the feature-wise and sample-wise correlations\nin the dataset. In addition, the prediction of missing values also largely depends on the \u2018missingness'\nof the data, i.e., whether a certain feature value is observed or not in the dataset. Specifically,\nthe missingness information directly determines which observed feature values can be used for\nimputation. For example, even if two samples are closely related, it may be less effective to use them\nfor imputation if they have missing values in exactly the same features. It still remains a challenging\nproblem how to jointly model feature-wise and sample-wise correlations with such data missingness."}, {"title": "Related Work", "content": "Traditional methods: These imputation approaches include joint modeling with expectation-\nmaximization (EM) [8, 14, 20], k-nearest neighbors (kNN) [13, 40], and matrix comple-\ntion [16, 4, 5, 30]. However, joint modeling with EM and matrix completion often lack the flexibility\nto handle data with mixed modalities, while kNN faces scalability issues due to its high computational\ncomplexity. In contrast, M\u00b3-Impute is scalable and adaptive to different data distributions.\n\nLearning-based methods: Iterative imputation frameworks [21, 1, 22, 42, 38, 41], such as MICE [42]\nand HyperImpute [21], have been extensively studied. These iterative frameworks apply different\nimputation methods for each feature and iteratively estimate missing values until convergence. In\naddition, for deep neural network learners, both generative models [47, 27, 48, 25, 33, 50] and\ndiscriminative models [22, 9, 45] have also been proposed. However, these methods are built upon\nraw tabular data structures, which may fall short of capturing the complex correlations in features,\nsamples, and their combination. In contrast, M\u00b3-Impute is based on the bipartite graph modeling of\nthe data, which is more suitable for learning the data correlations for imputation.\n\nGraph neural network-based methods: GNN-based methods [49, 37] are proposed to address\nthe drawbacks mentioned above due to their effectiveness in modeling complex relations between\nentities. Among them, GRAPE [49] transforms tabular data into a bipartite graph where features\nare one type of nodes and samples are the other. A sample node is connected to a feature node only\nif the corresponding feature value is present. This transformation allows the imputation task to be\nframed as a link prediction problem, where the inner product of the learned node embeddings is\ncomputed as the predicted values. However, these methods do not explicitly encode the missingness\ninformation of different samples and features into the imputation process, which can impair their\nimputation accuracy. In contrast, M\u00b3-Impute enables explicit modeling of missingness information\nthrough FCU and SCU as well as our novel initialization unit so that feature-wise and sample-wise\ncorrelations can be accurately captured in the imputation process."}, {"title": "M\u00b3-Impute", "content": "We here provide an overview of M\u00b3-Impute to impute the missing value of feature f for a given\nsample s, as depicted in Figure 1. Initially, the data matrix with missing values is modeled as an\nundirected bipartite graph, and the missing value is imputed by predicting the edge weight \u00easf of\nits corresponding missing edge (Section 3.2). M\u00b3-Impute next employs a GNN model, such as\nGraphSAGE [15], on the bipartite graph to learn the embeddings of samples and features. These\nembeddings, along with the known masks of the data matrix (used to indicate which feature values\nare available in each sample), are then input into our novel feature correlation unit (FCU) and\nsample correlation unit (SCU), which shall be explained in Section 3.3 and Section 3.4, to obtain\nfeature-wise and sample-wise correlations, respectively. Finally, M\u00b3-Impute takes the feature-wise\nand sample-wise correlations into a multi-layer perceptron (MLP) to predict the missing feature value\n\u00easf (Section 3.5). The whole process, including the embedding generation, is trained in an end-to-end\nmanner."}, {"title": "Initialization Unit", "content": "Let $A \\in R^{n\\times m}$ be an n\u00d7m matrix that consists of n data samples and m features, where Aij denotes\nthe j-th feature value of the i-th data sample. We introduce an n \u00d7 m mask matrix $M \\in {0, 1}^{n\\times m}$\nfor A to indicate that the value of Aij is observed when Mij = 1. In other words, the goal of\nimputation here is to predict the missing feature values Aij for i and j such that Mij = 0. We\ndefine the masked data matrix D to be $D = A \\odot M$, where $\\odot$ is the Hadamard product, i.e., the\nelement-wise multiplication of two matrices.\n\nAs used in recent studies [49], we model the masked data matrix D as a bipartite graph and tackle the\nmissing value imputation problem as a link prediction task on the bipartite graph. Specifically, D is\nmodeled as an undirected bipartite graph $G = (S \\cup F, E)$, where $S = {s_1, s_2, . . ., s_n}$ is the set of\n'sample' nodes and $F = {f_1, f_2,..., f_m }$ is the set of \u2018feature' nodes. Also, $E$ is the set of edges that\nonly exist between sample node s and feature node f when Dsf \u2260 0, and each edge $(s, f) \\in E$ is\nassociated with edge weight esf, which is given by esf = Dsf. Then, the missing value imputation\nproblem becomes, for any missing entries in D (where Dsf = 0), to predict their corresponding edge\nweights by developing a learnable mapping F(\u00b7), i.e.,\n\\begin{equation}\n\\hat{e}_{sf} = F(G, (s, f) \\notin E).\n\\end{equation}"}, {"title": "Feature Correlation Unit", "content": "To improve the accuracy of missing value imputation, we aim to fully exploit feature correlations\nwhich often appear in the datasets. While the feature correlations are naturally captured by GNNs,\nwe observe that there is still room for improvement. We propose FCU as an integral component of\nM\u00b3-Impute to fully exploit the feature correlations.\n\nTo impute the missing value of feature f for a given sample s, FCU begins by computing the\nfeature 'context' vector of sample s in the embedding space that reflects the correlations between\nthe target missing feature f and observed features. Let $h_f \\in R^d$ be the learned embedding vector\nof feature f from the GNN, and let HF be the d \u00d7 m matrix that consists of all the learned feature\nembedding vectors. We first obtain dot-product similarities between feature f and all the features\nin the embedding space, i.e., $H_f^T h_f$. We then mask out the similarity values with respect to non-\nobserved features in sample s. Here, instead of applying the mask vector ms of sample s directly,\nwe use a learnable \u2018soft' mask vector, denoted by $m_s'$, which is defined to be $m_s' = \\sigma_1(m_s) \\in R^m$,\nwhere $\\sigma_1 (\\cdot)$ is an MLP with the GELU activation function [18]. In other words, we obtain feature-wise\nsimilarities with respect to sample s, denoted by $r_f^s$, as follows:\n\\begin{equation}\nr_f^s = \\sigma_2 ((H_f^T h_f) \\odot m_s') \\in R^d,\n\\end{equation}\nwhere $\\sigma_2(\\cdot)$ denotes another MLP with the GELU activation function. FCU next obtains the\nHadamard product between the learned embedding vector of sample s, $h_s$, and the feature-wise\nsimilarities with respect to sample s, $r_f^s$, to learn their joint representations in a multiplicative manner.\nSpecifically, FCU obtains the feature context vector of sample s, denoted by $c_f^s$, as follows:\n\\begin{equation}\nc_f^s = \\sigma_3 (h_s \\odot r_f^s) \\in R^d,\n\\end{equation}\nwhere $\\sigma_3(\\cdot)$ is also an MLP with the GELU activation function. That is, FCU fuses the representation\nvector of s and the vector that has embedding similarity values between the target feature f and the\navailable features in s through the effective use of the soft mask $m_s'$. From (3) and (4), the operations\nof FCU can be written as\n\\begin{equation}\nc_f^s = FCU(h_s, m_s, H_F) = \\sigma_3 (h_s \\odot \\sigma_2 ((H_f^T h_f) \\odot \\sigma_1(m_s))).\n\\end{equation}"}, {"title": "Sample Correlation Unit", "content": "To measure similarities between s and other samples, a common approach would be to use the\ndot product or cosine similarity between their embedding vectors. This approach, however, fails\nto take into account the observability or availability of each feature in a sample. It also does\nnot capture the fact that different observed features are of different importance to the target fea-\nture to impute when it comes to measuring the similarities. We introduce SCU as another in-\ntegral component of M\u00b3-Impute to compute the sample 'context' vector of sample s by incor-\nporating the embedding vectors of its similar samples as well as different weights of observed\nfeatures. SCU works based on the two novel masking schemes, which shall be explained shortly.\n\nSuppose we are to impute the missing value of feature f for a given sample\ns. SCU aims to leverage the information from the samples that are similar\nto s. As a first step to this end, we create a subset of samples $P \\subseteq S$ that\nare similar to s. Specifically, we randomly choose and put a sample into P\nwith probability that is proportional to the cosine similarity between s and\nthe sample. This operation is repeated without replacement until P reaches\na given size.\n\nMutual Sample Masking: Given a subset of samples P that include s,\nwe first compute the pairwise similarities between s and other samples\nin the subset P. While they are computed in a similar way to FCU, we\nonly consider the commonly observed features (or the common ones that\nhave feature values) in both s and its peer $p \\in P \\setminus {s}$, to calculate their\npairwise similarity in the sense that the missing value of feature f is inferred.\nSpecifically, we compute the pairwise similarity between s and $p \\in P \\setminus {s}$,\nwhich is denoted by sim(s, p | f), as follows:\n\\begin{equation}\nsim(s, p | f) = FCU(h_s, m_p, H_F) \\cdot FCU(h_p, m_s, H_F) \\in R,\n\\end{equation}\nwhere he and hp are the learned embedding vectors of samples s and p from the GNN, respectively,\nand ms and mp are their respective mask vectors. Note that the multiplication in the RHS of (6) is\nthe dot product.\n\nIrrelevant Feature Masking: After we obtain the pairwise similarities between s and other samples\nin P, it would be natural to consider a weighted sum of their corresponding embedding vectors, i.e.,\n$\\sum_{p\\in P\\setminus {s}} sim(s, p | f) h_p$, in imputing the value of the target feature f. However, we observe that\nhp contains the information from the features whose values are available in p as well as possibly\nother features as it is learned via the so-called neighborhood aggregation mechanism that is central to\nGNNs, but some of the features may be irrelevant in inferring the value of feature f. Thus, instead\nof using {hp} directly, we introduce a d-dimensional mask vector $r_f^p$ for hp, which is to mask out\npotentially irrelevant feature information in hp, when it comes to imputing the value of feature f.\nSpecifically, it is defined by\n\\begin{equation}\nr_f^p = \\sigma_4 ([m_p; m_f]) \\in R^d,\n\\end{equation}\nwhere mf is an m-dimensional one-hot vector that has a value 1 in the place of feature f and O's\nelsewhere, $[\\cdot;\\cdot]$ denotes the vector concatenation operation, and $\\sigma_4(\\cdot)$ is an MLP with the GELU\nactivation function. Note that the rationale behind the design of $r_f^p$ is to embed the information on the\nfeatures whose values are present in p as well as the information on the target feature f to impute.\nThe mask $r_f^p$ is then applied to hp to obtain the masked embedding vector of p as follows:\n\\begin{equation}\n\\phi_p(h_p, r_f^p) = \\sigma_5 (h_p \\odot r_f^p) \\in R^d,\n\\end{equation}\nwhere $\\sigma_5(\\cdot)$ is also an MLP with the GELU activation function. Once we have the masked embedding\nvectors of samples (excluding s) in P, we finally compute the sample context vector of sample s,\ndenoted by $z_f^s$, which is a weighted sum of the masked embedding vectors with weights being the\npairwise similarity values, i.e.,\n\\begin{equation}\nz_f^s = \\sigma_6 (\\sum_{p\\in P\\setminus {s}} sim(s,p | f) \\phi_p(h_p, r_f^p)) \\in R^d,\n\\end{equation}\nwhere $\\sigma_6(\\cdot)$ is again an MLP with the GELU activation function. From (6)\u2013(9), the operations of\nSCU can be written as\n\\begin{equation}\nz_f^s = SCU(H_P, M_P, H_F) = \\sigma_6 (\\sum_{p\\in P\\setminus {s}} sim(s, p | f) \\sigma_5 (h_p \\odot \\sigma_4 ([m_p; m_f]))),\n\\end{equation}\nwhere HP = {hp, p \u2208 P} and MP = {mp, p \u2208 P}."}, {"title": "Imputation", "content": "For a given sample s, to impute the missing value of feature f, M\u00b3-Impute obtains its feature context\nvector $c_f^s$ and sample context vector $z_f^s$ through FCU and SCU, respectively, which are then used for\nimputation. Specifically, it is done by predicting the corresponding edge weight \u00easf as follows:\n\\begin{equation}\n\\hat{e}_{sf} = \\varphi ((\\mathbf{1} - \\alpha) c_f^s + \\alpha z_f^s),\n\\end{equation}\nwhere $\\varphi(\\cdot)$ denotes an MLP with a non-linear activation function (i.e., ReLU for continuous\nvalues and softmax for discrete ones), and \u03b1 is a learnable scalar parameter. This scalar parameter\n\u03b1 is introduced to strike a balance between leveraging feature-wise correlation and sample-wise\ncorrelation. It is necessary because the quality of $z_f^s$ relies on the quality of the samples chosen in\nP, so overly relying on $z_f^s$ would backfire if their quality is not as desired. To address this problem,\ninstead of employing a fixed weight \u03b1, we make a learnable and adaptive in determining the weights\nfor $c_f^s$ and $z_f^s$. Note that this kind of learnable parameter approach has been widely adopted in natural\nlanguage processing [34, 43, 24, 32] and computer vision [7, 51, 52], showing superior performance\nto its fixed counterpart. In M\u00b3-Impute, the scalar parameter \u03b1 is learned based on the similarity values\nbetween s and its peer samples $p \\in P \\setminus {s}$ as follows:\n\\begin{equation}\n\\alpha = \\varphi_A (\\\\ || \\text{sim} (s,p | f) ||),\\\\\nqquad p\\in P\\setminus {s}\\\\\n\\end{equation}\nwhere $||$ represents the concatenation operation, and $\\varphi_A(\\cdot)$ is an MLP with the activation function\n$y(x) = 1 - 1 / e^{|x|}$. The overall operation of M\u00b3-Impute is summarized in Algorithm 1. To learn\nnetwork parameters, we use cross-entropy loss and mean square error loss for imputing discrete and\ncontinuous feature values, respectively."}, {"title": "Experiments", "content": "Datasets: We conduct experiments on 25 open datasets. These real-world datasets consist of mixed\ndata types with both continuous and discrete values and cover different domains including civil\nengineering (CONCRETE, ENERGY), physics and chemistry (YACHT), thermal dynamics (NAVAL),\netc. Since the datasets are fully observed, we introduce missing values by applying a randomly\ngenerated mask to the data matrix. Specifically, as used in prior studies [21, 22], we apply three\nmasking generation schemes, namely missing completely at random (MCAR), missing at random\n(MAR), and missing not at random (MNAR).\u00b9 We use MCAR with a missing ratio of 30%, unless\notherwise specified. We follow the preprocessing steps adopted in Grape [49] to scale feature values\nto [0, 1] with a MinMax scaler [23]. Due to the space limit, we below present the results of eight\ndatasets that are used in Grape and report other results in Appendix.\n\nBaseline models: M\u00b3-Impute is compared against popular and state-of-the-art imputation methods,\nincluding statistical methods, deep generative methods, and graph-based methods listed as follows:\nMEAN: It imputes the missing value \u00easf as the mean of observed values in feature f from all\nthe samples. K-nearest neighbors (kNN) [40]: It imputes the missing value \u00easf using the KNNs\nthat have observed values in feature f with weights that are based on the Euclidean distance to\nsample s. Multivariate imputation by chained equations (Mice) [42]: This method runs multiple\nregressions where each missing value is modeled upon the observed non-missing values. Iterative"}, {"title": "Overall Performance", "content": "We first compare the feature imputation performance of M\u00b3-Impute with popular and state-of-the-art\nimputation methods. As shown in Table 1, M\u00b3-Impute achieves the lowest imputation MAE for\nsix out of the eight examined datasets and the second-best MAE scores in the other two, which\nvalidates the effectiveness of M\u00b3-Impute. For KIN8NM dataset, M\u00b3-Impute underperforms Miracle.\nIt is mainly because each feature in KIN8NM is independent of the others, so none of the observed\nfeatures can help impute missing feature values. For NAVAL dataset, the only model that outperforms\nM\u00b3-Impute is HyperImpute [21]. In the NAVAL dataset, nearly every feature exhibits a strong linear\ncorrelation with the other features, i.e., every pair of features has correlation coefficient close to one.\nThis allows HyperImpute to readily select a linear model from its model pool for each feature to\nimpute. Nonetheless, M\u00b3-Impute exhibits overall superior performance to the baselines as it can be\nwell adapted to datasets with different levels of correlations over features and samples. In other words,\nM\u00b3-Impute benefits from explicitly incorporating the missingness information with our carefully\ndesigned masking schemes to better capture feature-wise and sample-wise correlations.\n\nFurthermore, we evaluate the performance of M\u00b3-Impute under MAR and MNAR settings. We\nobserve that M\u00b3-Impute consistently outperforms all the baselines under all the eight datasets and\nachieves an even larger margin in the improvement compared to the case with MCAR setting. This\nimplies that our explicit modeling of the missingness information through our novel soft masking\nschemes in FCU and SCU as well as the initialization unit is effective in handling different patterns of"}, {"title": "Ablation Study", "content": "To study the effectiveness of three integral components of M\u00b3-Impute, we consider three variants of\nM\u00b3-Impute, each with a subset of the components, namely initialization only (Init Only), initialization\n+ FCU (Init + FCU), and initialization + SCU (Init + SCU). The performance of these variants\nare evaluated against the top-performing imputation baselines such as Grape and HyperImpute. As\nshown in Table 2, the three variants derived from M\u00b3-Impute achieve lower MAE values than both\nbaselines in most datasets, demonstrating the effectiveness of our novel components in M\u00b3-Impute.\n\nSpecifically, for initialization only, the key difference between M\u00b3-Impute and Grape lies in our\nrefined initialization process to explicitly leverage missingness information in node embeddings. The\nreduced MAE values observed by the Init Only variant demonstrate that our proposed initialization\nprocess is more effective in utilizing information between samples and their associated features,\nincluding missing ones, as compared to the basic initialization used in Grape [49]. In addition, we\nobserve that when FCU or SCU is incorporated, MAE values are further reduced for most datasets.\nThis validates that explicitly modeling of missingness information through our novel masking schemes\nin FCU and SCU indeed improves imputation accuracy. When all the three components are combined\ntogether as in M\u00b3-Impute, they work synergistically to lower MAE values, validating the efficacy of\nincorporating the missingness information when capturing sample-wise and feature-wise correlations\nfor missing data imputation."}, {"title": "Robustness", "content": "Missing ratio: In practice, datasets may possess different missing ratios. To validate the model's\nrobustness under such circumstances, we evaluate the performance of M\u00b3-Impute and other baseline\nmodels with varying missing ratios, i.e., 0.1, 0.3, 0.5, and 0.7. Figure 3 shows their performance.\nWe use the MAE of HyperImpute (HI) as the reference performance and offset the performance of\neach model by MAE \u2013 MAEHI, where x represents the considered model. For clarity, we here\nonly report the results of four top-performing models. As shown in Figure 3, M\u00b3-Impute outperforms\nother baseline models for almost all the cases, especially under YACHT, CONCRETE, ENERGY,\nand HOUSING datasets. It is worth noting that modeling feature correlations in these datasets is\nparticularly challenging due to the presence of considerable amounts of weakly correlated features,\nalong with a few strongly correlated ones. Nonetheless, FCU and SCU in M\u00b3-Impute are able to\nbetter capture such correlations with our efficient masking schemes, thereby resulting in a large\nimprovement in imputation accuracy. In addition, for KIN8NM dataset, M\u00b3-Impute ties with the\nsecond-best model, Grape. As mentioned in Section 4.2, each feature in KIN8NM is independent\nof the others, so none of the observed features can help impute missing feature values. For NAVAL\ndataset, where each feature strongly correlates with the others, M\u00b3-Impute surpasses Grape but falls\nshort of HyperImpute, due to the same reason as discussed in Section 4.2. Overall, M\u00b3-Impute is\nrobust to various missing ratios. Comprehensive results can be found in Appendix.\n\nSampling strategy in SCU: While SCU uses a sampling strategy based on pairwise cosine similarities\nto construct a subset of samples P, the simplest sampling strategy to build P would be to choose"}, {"title": "Running time Analysis", "content": "We present a running time comparison in Table 4. The results show that our method is both accurate\nand time-efficient. For example, for inference with GPU, the time taken to impute all the missing\nvalues for any dataset we tested is less than one second under the setting of MCAR with 30%\nmissingness. More results can be found in Appendix."}, {"title": "Different GNN Variants", "content": "We also conduct experiments using different GNN variants such as GraphSAGE, GAT, and GCN.\nThe results in Table 5 indicate that different aggregation mechanisms may introduce varying errors,\nbut our method consistently outperforms its Grape counterpart, demonstrating its effectiveness."}, {"title": "Conclusion", "content": "In this paper, we highlighted the importance of missingness information and presented M\u00b3-Impute,\na mask-guided representation learning method for missing value imputation. M\u00b3-Impute improved\nthe embedding initialization process by considering the relationships between samples and their\nassociated features (including missing ones). In addition, for more effective representation learning,\nwe introduced two novel components in M\u00b3-Impute \u2013 FCU and SCU, which explicitly model the\nmissingness information with our novel soft masking schemes to better capture data correlations\nfor imputation. Extensive experiment results on 25 open datasets demonstrate the effectiveness of\nM\u00b3-Impute, where it achieves overall superior performance to popular and state-of-the-art methods,\nwith 20 best and 4 second-best MAE scores on average under three different settings of missing value\npatterns."}, {"title": "Appendix", "content": "In this section, we elaborate on extensive and comprehensive experiment results. We first provide an\noverview of the dataset details in Section A.1, and present the performance of the imputation methods\nunder different missingness settings, namely MAR and MNAR, in Section A.2. We then provide\nthe comprehensive results across different missingness ratios in Section A.3. For more thorough\nanalysis, we extend our evaluation of M\u00b3-Impute on 17 additional datasets, totaling 25 datasets, in\nSection A.4, and elaborate on the computational resources used in Section A.5. We further assess\nthe quality of imputed values generated by M\u00b3-Impute by leveraging them in downstream tasks in\nSection A.6. Finally, we perform a sensitivity analysis on the hyperparameters of M\u00b3-Impute in\nSection A.7, and provide the implementation details of baselines, including their hyperparameter\nchoices, in Section A.8."}, {"title": "Dataset Details", "content": "Table 6 presents the statistics of the eight UCI datasets [10] used throughout Section 4. Figure 4\nillustrates the Pearson correlation coefficients among the features. In the Kin8nm dataset, all\nfeatures are linearly independent, whereas the Naval dataset exhibits strong correlations among its\nfeatures. Under the MCAR setting, M\u00b3-Impute performs comparably to the baseline imputation\nmethods on these two datasets (shown in Table 1). However, in real-world scenarios, features are not\nalways entirely independent or strongly correlated. In the other six datasets, we observe a mix of\nweakly correlated features along with a few that are strongly correlated. In these cases, M\u00b3-Impute\nconsistently outperforms all baseline methods."}, {"title": "Detailed Results of Different Missingness Settings", "content": "We adopt the same procedure outlined in Grape [49] to generate missing values under different\nsettings.\n\n\u2022 MCAR: An n \u00d7 m matrix is sampled from a uniform distribution. Positions with values no greater\nthan the ratio of missingness are viewed as missing and the remaining positions are observable."}, {"title": "Downstream Task Performance", "content": "We further evaluate the quality of imputed values from different imputation methods by performing a\ndownstream label prediction task. In particular, each sample in the eight examined datasets contains\na continuous label, and the task is to predict the label using the feature values. Starting with an\ninput data matrix with 30% missingness, we first impute the data using the corresponding imputation\nmethods, and then do linear regression on the completed data matrix to predict labels. As shown in\nTable 11, M\u00b3-Impute consistently achieves good performance across different datasets, with six best\nperformance and two second best, indicating its effectiveness in the missing value imputation."}, {"title": "Analysis of HyperParameters in M\u00b3-Impute", "content": "In this experiment, we evaluate the sensitivity of M\u00b3-Impute to various hyperparameter settings.\nTables 12, 13, and 14 summarize the performance of M\u00b3-Impute across different hidden dimensions,\nGNN layer counts, and edge dropout ratios. Overall, the results show that M\u00b3-Impute is robust\nto different hyperparameter settings across the tested datasets. Based on these observations, we\nrecommend setting the hidden dimension to 128, the number of GNN layers to 3, and the edge\ndropout ratio to 50% as a general guideline."}]}