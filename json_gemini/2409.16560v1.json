{"title": "Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference", "authors": ["Zongyue Qin", "Zifan He", "Neha Prakriya", "Jason Cong", "Yizhou Sun"], "abstract": "Large language models (LLMs) based on transformer architecture have shown outstanding performance across numerous real-world tasks. However, the autoregressive nature of these models makes the inference process slow and costly. Speculative decoding has emerged as a promising solution, leveraging a smaller auxiliary model to draft future tokens, which are then validated simultaneously by the larger model, achieving a speed-up of 1-2x. Although speculative decoding matches the same distribution as multinomial sampling, multinomial sampling itself is prone to suboptimal outputs, where as beam sampling is widely recognized for producing higher-quality results by maintaining multiple candidate sequences at each step. This paper explores the novel integration of speculative decoding with beam sampling. However, there are four key challenges: (1) how to generate multiple sequences from the larger model's distribution given drafts sequences from the small model; (2) how to dynamically optimize the number of beams to balance efficiency and accuracy; (3) how to efficiently verify the multiple drafts in parallel; and (4) how to address the extra memory costs inherent in beam sampling. To address these challenges, we propose dynamic-width speculative beam decoding (DSBD). Specifically, we first introduce a novel draft and verification scheme that generates multiple sequences following the large model's distribution based on beam sampling trajectories from the small model. Then, we introduce an adaptive mechanism to dynamically tune the number of beams based on the context, optimizing efficiency and effectiveness. Besides, we extend tree-based parallel verification to handle multiple trees simultaneously, accelerating the verification process. Finally, we illustrate a simple modification to our algorithm to mitigate the memory overhead of beam sampling. Experimental results show that our approach achieves a 1.5-1.9\u00d7 speed-up and 1.8-2.5\u00d7 lower energy consumption compared to beam sampling, with no loss in downstream performance. Moreover, it can produce significantly higher-quality outputs than speculative decoding, while maintaining similar time, memory, and energy costs. In summary, our method offers a more efficient and effective inference process for LLMs.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models based on transformer architecture (Vaswani et al. 2017), such as GPT-4 (Achiam et al. 2023), Llama-3 (AI@Meta 2024), and PALM (Anil et al. 2023), have demonstrated remarkable performance across a wide range of real-world tasks, including text generation, summarization, and translation. However, the autoregressive nature of these models, where tokens are generated one at a time, leads to slow inference speeds and high computational costs. As the size and complexity of LLMs continue to increase, the demands on computational resources and energy consumption during inference have become major concerns, limiting their scalability and accessibility.\nSpeculative decoding has recently emerged as a promising technique to accelerate LLM inference by leveraging a smaller auxiliary model to generate draft tokens. These tokens are then validated by the large model, resulting in a significant reduction in inference time. The primary advantage of speculative decoding is its ability to maintain the same quality of output as multinomial sampling while achieving a 1-2x speed-up. However, multinomial sampling itself is limited to generating a single sequence based on local optimality. This limitation makes it prone to returning suboptimal results, as it lacks the diversity that could be achieved by considering multiple candidate sequences simultaneously.\nMotivated by the need to improve the effectiveness of multinomial sampling, we explore the integration of speculative decoding with beam sampling, a technique that maintains multiple candidate sequences (beams) at each step to enhance the diversity and quality of the generated output. This fusion, however, presents several challenges. First, while previous studies focused on obtaining a single token from the large model's distribution given draft tokens from the smaller model, our approach requires generating multiple tokens (beams) simultaneously, which necessitates a new verification scheme. Second, determining the optimal number of beams is critical: too many beams can lead to inefficiency due to a high rejection rate, while too few beams may result in underutilization of the small model's potential and low effectiveness. Third, efficiently verifying multiple draft sequences in parallel requires a technique that can process and validate multiple beams concurrently. Fourth, addressing the additional memory cost of storing multiple key-value caches is crucial to enable LLMs to use beam sampling in practice.\nTo address these challenges, we propose dynamic-width speculative beam decoding (DSBD) that combines speculative decoding with beam sampling through a series of innovations. First, we introduce a draft and verification scheme that processes beam decoding trajectories as forests of trees, which are verified layer by layer by the large model. This approach allows us to efficiently generate multiple beams while maintaining the large model's sampling distribution. Second, we propose an adaptive mechanism to dynamically adjust the number of beams based on the context, ensuring a balance between efficiency and effectiveness. Third, we extend existing tree-based parallel verification techniques (Miao et al. 2023) to operate on multiple trees, incorporating a forest-based parallel verification strategy that enhances the speed of the verification process. Finally, we introduce a simple modification to DSBD that reduces the memory cost by storing only one set of key-value caches, while still delivering better output quality than multinomial sampling.\nOur experimental results demonstrate that our approach achieves a 1.5 - 1.9\u00d7 speed-up and 1.8 - 2.5\u00d7 smaller energy consumption compared to beam sampling, without sacrificing performance on downstream tasks. Besides, it can produce significantly higher-quality outputs than speculative decoding, while maintaining similar time, memory, and energy costs. These findings suggest that our approach successfully bridges the gap between speculative decoding and beam sampling, providing a more efficient and effective inference process for large language models."}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Decodings of LLMs", "content": "Let p denote the distribution defined by a large language model Mp. Given an input prefix, the optimal decoding algorithm is to generate a sequence of N tokens with maximum likelihood p(x1:N|input).\nMultinomial Sampling. Multinomial sampling, also known as standarized sampling, samples the next token Xt based on $Top(\\cdot|x_{1:t-1}, input)$, where T is a warping operation applied to enhance the high probability region. Some common warping operations include top-k warping, which limits the selection to the top k tokens, and top-p warping, where tokens are sampled from the smallest possible subset of the vocabulary whose cumulative probability mass exceeds a specified threshold. The deterministic version of multinomial sampling is a special case when k = 1.\nBeam Sampling. Beam decoding aims to do a better job than multinomial sampling. For each position t (1 \u2264 t \u2264 N), it maintains W > 1 candidate sequences, which are also called beams. Assume we have already kept the W sequences $I_{t-1} = \\{x_{1:t-1}^{(1)}, ..., x_{1:t-1}^{(W)}\\}$ at position t \u2013 1, W sequences with length t are then sampled from $Top_{beam}$, where $p_{beam}:I_{t-1} \\times V \\rightarrow [0, 1]$ is the beam sampling probability:\n$P_{beam}(x_{1:t-1}^{(i)}, x_t) = \\frac{P(x_{1:t}^{(i)}|input)}{\\sum_{x \\in V}P(x_{1:t-1}^{(i)}, x|input)}$\nNotice that $p(x_{1:t-1}^{(i)}, x_t| input) = p(x_t|x_{1:t-1}^{(i)}, input) \\cdot p(x_{1:t-1}^{(i)}|input)$. In practice, beam sampling stores the likelihood $p(x_{1:t-1}^{(i)}|input)$ for each beam, and the computation complexity of $p_{beam}$ is O(W|V|). In deterministic beam sampling, the top W sequences with the highest likelihood $P_{beam}(x_{1:t})$ will be kept.\n(Shi et al. 2024) shows that beam sampling in general has better downstream effectiveness than multinomial sampling."}, {"title": "2.2 Vanilla Speculative Decoding", "content": "Speculative decoding utilizes a small model to generate the next \u03b3 tokens and then employs the large model to verify these drafted tokens in parallel. The process is summarized as follows:\n1. Given input, the small model samples \u03b3 draft tokens $x_1,..., x_\\gamma$ using greedy decoding, based on the warped predicted conditional probability $q(x_t|x_{1:t-1}, input)$ for t = 1,..., \u03b3, where $q = T_o q$ and q is the small model's output distribution.\n2. The large model verifies the draft tokens in parallel by computing the conditional probability $p = T_o p(x_t| x_{1:t-1}, input)$ for t = 1, ..., \u03b3.\n3. Each draft token $x_t$ is accepted with a probability $min(1,p(x_t)/q(x_t))$. The draft tokens before the first rejected token are kept as the decoding output. An additional token is sampled from a residual distribution as a correction for the first rejected token. The accepted tokens and the resampled token are then appended to the context prefix as the input for the next iteration.\n4. Repeat steps 1-3 until reaching the stopping criteria, such as a length limit.\nBy verifying \u03b3 tokens in parallel with one run of the large model, speculative decoding reduces the time cost compared to calling the large model \u03b3 times. Additionally, although the small model still runs in an autoregressive manner, its inference speed is much faster than the large model. This makes speculative decoding an effective method to accelerate the inference process of LLMs. Moreover, it is proven that each token $x_t$ generated by speculative sampling follows the identical sampling distribution as multinomial sampling."}, {"title": "3 Methodology", "content": "The primary goal of our method is to enhance the efficiency and effectiveness of large language model (LLM) inference by combining the speed advantages of speculative decoding with the accuracy and diversity benefits of beam sampling. We first introduce a novel draft and verification scheme that keeps identical distribution as beam sampling. Then, we describe an adaptive beam management strategy. Next, we illustrate a forest-based parallel verification mechanism. Finally, we discuss how to resolve the additional memory cost inherent in beam sampling."}, {"title": "3.1 Draft and Verification Scheme", "content": "Overview As illustrated in Figure 2, the core idea of our method is to leverage a smaller, auxiliary model to generate multiple draft sequences, referred to as draft beams, which are then verified and refined by the larger model. This approach enables us to maintain multiple candidate sequences throughout the decoding process, thereby achieving better output quality than multinomial sampling, while improving the overall efficiency of beam sampling.\nFor now, assume that the number of beams (also referred to as the width, denoted as $W_L$) is fixed. In each iteration of our method, the input consists of the beams generated in the previous iteration. For the first iteration, the input is the initial input context. At each iteration, our method first uses the small model to perform beam sampling with a width of $W_S$ for \u03b3 steps. Notice that we want $W_S > W_L$ because some draft beams might be rejected later. As illustrated in Figure 2a, it generates a trajectory that can be represented as a forest consisting of $W_L$ trees, which we refer to as the \"draft forest\". In this forest, each tree originates from an input beam, with the maximum depth of each tree being \u03b3 + 1. Starting from the second layer, each layer of the forest contains $W_S$ nodes, representing the intermediate beams at each step of the beam sampling process.\nOnce the draft forest is generated, our method leverages the large model to predict the distribution for the next token of each node (beam) in parallel. Using these distributions, DSBD then verifies each layer of the draft forest sequentially. For each layer, it calculates the joint probability of the beams and sequentially determines whether each beam should be accepted. If $W_L$ beams are accepted at a given layer, the remaining beams are discarded, and the method moves on to verify the next layer. If fewer than $W_L$ beams are accepted at layer l, the method rejects this layer and terminates the verification process.\nWhen verification ends, either because it is terminated or there are no more layers to verify, our method samples an additional layer with $W_L$ beams. This additional layer either corrects the first rejected layer or adds a new layer if all draft layers are accepted. The output beams from this additional layer then serve as the input beams for the next iteration, continuing until the stopping criteria are met (e.g., reaching the maximum number of tokens).\nThis approach allows each run of the large model to produce at least one, and possibly multiple, steps of beam sampling. Previous studies have shown that memory operations during LLM runs contribute significantly to both runtime and energy consumption (Leviathan, Kalman, and Matias 2023; Allen and Ge 2016; Chen et al. 2011). By generating multiple tokens in a single run, DSBD reduces the number of memory operations required, which in turn improve both the speed and energy efficiency of LLM inference.\nDetails Let p denote the distribution output by the large model, and q denote the distribution of the small model. We will start by explaining how to verify the first draft layer (which is the second layer of the draft forest) during each iteration.\nLet $I = \\{x_{1:t}^{(1)},..., x_{1:t}^{(W_L)}\\}$ represent the input beams, and $S = \\{X_{1:t+1}^{(1)}, ..., X_{1:t+1}^{(W_S)}\\}$ represent the draft beams in the first layer of the draft forest. Note that $x_{1:t+1}^{(i)}$ is sampled from the distribution $q_{beam}(x_{1:t+1}^{(i)}) = T_o q(x_{1:t+1}^{(i)})$, where T denotes the warping operation and $Q = \\sum_{x_{1:t+1} \\in Z_{X}V} q(x_{1:t+1})$. Similarly, let $p_{beam}$ denote the beam sampling distribution of the large model, we have $p_{beam}(x_{1:t+1}^{(i)}) = T_op(x_{1:t+1}^{(i)})$, where $P = \\sum_{x_{1:t+1} \\in Z_{X}V} p(x_{1:t+1})$.\nDuring verification, our method begins by setting $p' = p_{beam}^{(i)}$. For each draft beam $x_{1:t}^{(i)}$, DSBD accepts it with a probability $min(1, \\frac{p'(x_{1:t})}{q_{beam}^{(i)}(x_{1:t})})$. If $x_{1:t}$ is rejected, the method updates p' by setting it to $norm(max(0, p' - q_{beam}))$, where norm denote the normalization operation. Then it continues to verify the next draft beam with the updated p'. If the beam is accepted, p' is reset to $p_{beam}$. If $W_L$ draft beams have already been accepted at this layer, the method will reject all remaining beams.\nNow we illustrate how to verify the second draft layer. The difference is that some beams in the first layer are already rejected. In this case, all the beams stem from the rejected beams are directly rejected. For the remaining beams, DSBD applies the same verification process as above.\nIf all layers in the draft forest have $W_L$ accepted beams, the method proceeds to sample an additional layer with $W_L$ beams directly from $p_{beam}$. However, if at any layer l fewer than $W_L$ beams are accepted, the method will first sample one beam from the adjusted distribution p'. If the number of accepted beams still falls short of $W_L$, additional beams will be sampled from the original distribution $p_{beam}$ to meet the required number.\nTheorem 3.1. Correctness of Draft and Verification Scheme. Let $I = \\{x_{1:t}^{(1)},..., x_{1:t}^{(W_L)}\\}$ denote input beams, $S = \\{x_{1:t+1}^{(1)}, ..., x_{1:t+1}^{(W_S)}\\}$ denote draft beams, and $O = \\{x_{1:t+1}^{(1)},..., x_{1:t+1}^{(W_L)}\\}$ denote the output beams obtained by our algorithm. We have $x_{1:t+1}^{(i)} \\stackrel{iid}{\\sim} p_{beam}(Ni = 1,..., W_L)$, where $p_{beam}(x_{1:t+1}) = T_o (p(x_{1:t+1})/P)$, $P = \\sum_{x_{1:t+1} \\in Z_{X}V} p(x_{1:t+1})$.\nThe proof is illustrated in technical appendix in the supplementary materials."}, {"title": "3.2 Dynamic-Width Speculative Beam Decoding", "content": "The draft and verification scheme described above ensures that our method matches the sampling distribution of beam sampling. However, it has a limitation: the beam width $W_L$ remains fixed across all layers. While this fixed width works well for standard beam sampling, it is not suitable for our method. The key challenge is that the discrepancy between the small model's predictions ($q_{beam}$) and the large model's true distribution ($p_{beam}$) vary from token to token. In some layers, $q_{beam}$ closely aligns with $p_{beam}$, resulting in a high acceptance rate. In other layers, the gap is much wider, leading to a lower acceptance rate.\nTo address this variability, the decoding algorithm should dynamically adjust the number of beams it expects to accept based on the alignment between $q_{beam}$ and $p_{beam}$. By doing so, it can (1) reduce the target width for challenging layers, preventing the entire layer from being rejected and thus maintaining progress, and (2) increase the target width for easier layers, enhancing the exploration of diverse sequences and reducing the risk of getting trapped in local optima. This adaptive approach would optimize the balance between efficiency and accuracy, making the decoding process more robust and effective. So we propose a self-adjusting method where the target width $W_L^{(l)}$ for layer l is determined based on the context of that layer.\nLet $P_{p,q}(m, k)$ represent the probability that k out of m draft beams are accepted at the l-th layer. This probability is computed using the following recursive equation:\n$P_{p,q}^{(l)}(m, k) = \\sum_{i=1}^{m}(\\frac{m}{i}) P_{p,q}^{(l)}(m - i, k - 1)$ \nHere, $P_{p,q}^{(l)}(m, i)$ is the probability that the i-th beam is the first to be accepted among the m draft beams:\n$P_{p,q}^{(l)}(m, i) = a_i^{(l)}[1 - a^{(l)}]^{i-1}$\nwhere $a_j^{(l)}$ is the probability that the j-th beam is accepted, given that all previous beams (from the 1st to the (j \u2013 1)-th) were rejected.\n$a^{(l)} = \\sum q_{beam}min(p^{(j)}/q_{beam}, 1)$\n$p^{(j)} = p_{beam}, p^{(l)} = norm(max(p^{(j-1)}-q_{beam}, 0))$\nUsing these equations and the fact that $P_{p,q}^{(l)}(m, k) = 0$ if k > m and $P_{p,q}^{(l)}(0,0) = 1$, we can calculate the probability that at least K beams are accepted at the l-th layer as:\n$1- \\sum P_{p,q}^{(l)}(M_S,k)$\nFinally, the width $W_L^{(l)}$ for the l-th layer is set based on Eq 5, ensuring that it is not less than a minimum width $W_{min}$:\n$W_L^{(l)} = max(W_{min}, W_L^{(l)}(t))$\nIn this expression, t \u2208 [0, 1] is a pre-defined threshold. The value of $W_L^{(l)}(t)$ is computed as follows:\n$W_L(t) = max\\{K \\in N |1 - \\sum P_{p,q}(M_S,k) \\geq t\\}$\nThis formula gives us the maximum width $W_L^{(l)}(t)$ such that the probability of accepting at least $W_L^{(l)}(t)$ beams at the l-th layer is greater than or equal to the threshold t. Eq 6 ensures that the width is dynamically adjusted to maintain a high likelihood of accepting a sufficient number of beams, while also ensuring that it does not fall below the minimum width $W_{min}$. \nLet $B = \\sum_{k=W_{min}}^{W_s}P_{p,q}^{(l)}(W_S,k))$, which is the probability that at least $W_{m}$in beams are accepted at layer l. Based on the definition of $W_L^{(l)}$, the probability that layer L is accepted is $min(t, B_{W_{min}})$. So t and $W_{min}$ both control the average acceptance rate of our algorithm, and hence determine efficiency. Let $\u03b2 = E_{B_{W_{min}}}$, we have the following theorem for the efficiency of DSBD.\nTheorem 3.2. The expected number of generated steps per iteration is $\\frac{1-min(t,\\beta)^{\\gamma+1}}{1-min(t,\\beta)}$.\nProof. As described above, the average acceptance rate is min(t, \u03b2). With the Theorems in (Leviathan, Kalman, and Matias 2023), we can compute the average number of generated layers to be $\\frac{1-min(t,\\beta)^{\\gamma+1}}{1-min(t,\\beta)}.$"}, {"title": "3.3 Forest-based Parallel Decoding", "content": "As pointed out by (Miao et al. 2023), efficiently managing the key-value cache is crucial for avoiding redundant computations when running the large model during verification, which impacts the overall efficiency. SpecInfer (Miao et al. 2023) introduced tree-based parallel decoding, which reuses the same key-value cache and employs a topology-aware causal mask to accelerate the computation of the large model. However, this tree-based parallel decoding approach cannot be directly applied to our algorithm because, unlike SpecInfer, our method retains multiple beams as inputs at each iteration. Although these beams share the same initial input, the tokens generated in each beam can differ significantly as length increases. As a result, the draft tokens in DSBD form not a single tree but a forest.\nSo we propose forest-based parallel decoding, an extension of tree-based parallel decoding that accommodates multiple trees. As shown in Figure 3, DSBD maintains a separate key-value cache for each input beam. For each beam, we apply tree-based parallel decoding to compute the tree attentions across all tokens. Finally, after the iteration ends, DSBD updates the key-value caches according to the output beams. For example, if the output beams in Figure 3 are b5 and b6, which are both originated from b\u2081, then the caches for b5 and b6 are kept for the next iteration."}, {"title": "3.4 Reducing Memory Cost", "content": "In practice, key-value caches take up a large portion of memory cost for LLM inference (Kang et al. 2024). A critical disadvantage of beam sampling is that is has to maintain a separate key-value cache for each beam, significantly increasing the memory cost. But our method can mitigate this issue with a simple modification. Notice that with the forest-based parallel decoding, the number of key-value caches kept during generation equals to the number of input beams. So an effective way to reduce the memory cost of our method is to limit the number of input beams. It can be achieved by only selecting the output beam with the lowest perplexity as the input beam to the next iteration. In this way, only one key-value cache is needed during generation, so the memory cost will be similar to the cost of multinomial sampling and speculative decoding. Notice that although there is only one input beam, more than one beams can be accepted at each layer of the draft forest. Hence, it will be more effective than multinomial sampling."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experiment Setups", "content": "The experiments are conducted on a server with one NVIDIA-L40 GPU. We use top-k and top-p warping in the experiments with k = 10, p = 0.8. We measure the GPU energy consumption following the guidance of (Yang, Adamek, and Armour 2023).\nLLMs. Following previous work (Miao et al. 2023), we evaluate our method using two publicly available LLM families: OPT (Zhang et al. 2022) and Llama-2 (Touvron et al. 2023). We use Llama-2-13B and OPT-13B as the large models as they are the largest models our GPU could run. And we use Llama-68M and OPT-125M as the small models.\nDatasets. We use two public datasets: SQUAD (Rajpurkar, Jia, and Liang 2018) and Spider (Yu et al. 2018). SQUAD is a natural language question answering dataset using exact match (EM) as evaluation metric. Spider is a text-to-SQL code dataset that uses execution accuracy (EA) as the metric. We choose the two datasets because their downstream metrics are clearly defined without using additional models or APIs. In addition, the two datasets cover both the natural language and program language."}, {"title": "4.2 Comparison with Beam Sampling", "content": "We begin by comparing DSBD with beam sampling, focusing on the relationship between efficiency (e.g., energy consumption and throughput) and effectiveness. The width of beam sampling ranges from 1 to 4. When width equals to 1, beam sampling is equivalent to multinomial sampling. In addition, we observe the improvement in downstream effectiveness and output perplexity begins to converge when width is larger than 3. So keep increasing beam width is meaningless. For our method, we vary the draft beam width $W_S \\in \\{2,3,4,5,6\\}$, the threshold $t \\in \\{0.7,0.9\\}$, the draft forest depth $\u03b3 \\in \\{2,3\\}$, and set $W_{min} = 1$. For a more comprehensive comparison, we also include the performances of speculative decoding (Leviathan, Kalman, and Matias 2023) and SpecInfer (Miao et al. 2023).\nThis comparison is illustrated in Figures 4, 5, and 6. The points mark the performance of different methods under different parameter settings. Speculative decoding and SpecInfer each have only one point in the figures because they do not offer a trade-off between efficiency and effectiveness. We plot the curves of beam sampling, and the Pareto fronts of DSBD. Notably, we report the perplexity of the OPT model on the Spider dataset instead of execution accuracy, as its execution accuracy remains consistently close to zero, rendering it uninformative for this analysis.\nThe figures clearly demonstrate that DSBD consistently outperforms beam sampling, signifying that DSBD achieves superior effectiveness at any given level of throughput or energy consumption. More importantly, when we fix the effectiveness, DSBD can be 1.5-1.9\u00d7 faster than beam sampling, while simultaneously reducing energy consumption by 1.8-2.5\u00d7. These results underscore the significant advantages of DSBD in achieving both higher efficiency and effectiveness, making it a more optimal choice for real-world applications."}, {"title": "4.3 Comparison under Memory Constraint", "content": "As discussed in Section 3.4, DSBD can mitigate the memory issue of beam sampling by selecting only one output beam for the next iteration. It allows DSBD to achieve memory usage comparable to that of multinomial sampling.\nTo assess the performance of DSBD under memory constraints, we compare it with multinomial sampling, speculative decoding and SpecInfer, as shown in Table 1. The results highlight that DSBD delivers a significant improvement in output quality, far surpassing the baselines in downstream scores. Moreover, DSBD achieves speed and energy efficiency close to that of speculative decoding while being more efficient and effective than multinomial sampling. This makes DSBD a compelling solution for optimizing LLM inference"}, {"title": "5 Related Work", "content": "EFFICIENT DECODING INFERENCE. Numerous studies have focused on improving the efficiency of large model inference. Among the most prominent methods are model quantization (Frantar et al. 2022; Lin et al. 2023), model pruning (Gale, Elsen, and Hooker 2019; Sanh, Wolf, and Rush 2020), and model distillation (Hinton, Vinyals, and Dean 2015). While these techniques achieve significant speed-ups, they often come with trade-offs, most notably a reduction in the model's overall effectiveness.\nA closely related direction to our work is non-autoregressive decoding, enabling parallel generation of multiple tokens (Gu et al. 2017; Wang et al. 2019; Sun et al. 2019; Ghazvininejad et al. 2019; Lee, Mansimov, and Cho 2018; Guo, Xu, and Chen 2020). However, these methods typically require extensive retraining of the model and often face challenges in either maintaining model effectiveness or achieving comparable performance without relying on task-specific techniques (Kim et al. 2023).\nSPECULATIVE DECODING. Speculative decoding is initially introduced in (Leviathan, Kalman, and Matias 2023; Chen et al. 2023). Spectr (Sun et al. 2023) extends this concept by allowing the small model to generate multiple independent draft sequences. SpecInfer (Miao et al. 2023) uses one or multiple small models to generate a draft token tree to increase the average acceptance length for each iteration. All these methods only maintains a single sequence during generation, making them prone to sub-optimal results.\nAnother complementary direction to enhance speculative decoding is to improve the effectiveness of the small draft model. A more effective draft model leads to a higher acceptance rate of draft tokens, which in turn accelerates the overall inference process. BiLD (Kim et al. 2023) achieves this by employing model prediction alignment to better train the small model. Liu et al. (Liu et al. 2023) propose online speculative decoding, which continually updates the draft model based on the input data observed during inference. Alternatively, Rest(He et al. 2023) utilizes a retrieval model to generate draft tokens. Another way to improve the accuracy of draft tokens is to train additional heads in the large model to predict future tokens. Representative works include EAGLE (Li et al. 2024) and MEDUSA (Cai et al. 2024). Importantly, these works are orthogonal to speculative decoding techniques, including our proposed method. This orthogonality means that the improvements offered by more accurate draft tokens could be combined with our method for better effectiveness."}, {"title": "6 Conclusion", "content": "This work introduces a novel method that integrates speculative decoding with beam sampling to enhance the efficiency of large language model (LLM) inference. By addressing the challenges of maintaining multiple candidate sequences and dynamically adjusting the beam width based on the context, our approach achieves significant speed-ups while preserving the quality of the generated outputs. Experimental results demonstrate that our method outperforms traditional beam sampling methods, achieving a significant speed-up and energy reduction without compromising downstream task performance. This work extends the effectiveness of speculative decoding, which opens new directions for the study of speculative decoding."}, {"title": "A Correctness of Speculative Beam Decoding", "content": "Given input beams $I = \\{x_{1:t"}, {"x_{1": "t"}, {"X_{1": "t+1"}, {"X_{1": "t+1"}, {"x_{1": "t+1"}, {"x_{1": "t+1"}, {"x_{1": "t+1"}, {"x_{1": "t+1"}, {"p(x_{1": "t+1"}, {"sum_{x_{1": "t+1"}, "in Z_{X}V} p(x_{1:t+1})$.\nProof. Step (1). We first prove $x_{1:t+1}^{(1)} \\sim p_{beam}$. Let $p_1 = p_{beam}$, $p_{k+1} = norm(max(p^{(k)} - q_{beam}, 0))$ for $k = 1,..., W_S - 1$.\nNote that we have\n$P(x_{1:t+1}^{(1)} = x_{1:t+1}) = p_{1,1} + p_{2,1}$\nwhere\n$p_{1,1} = P(x_{1:t+1}^{(1)} = x_{1:t+1}, x_{1:t+1} accepted)$\n$p_{2,1} = P(x_{1:t+1}^{(1)} = x_{1:t+1}, x_{1:t+1} rejected)$\nIn addition, for i > 1 let\n$P_{1,i} = P(x_{1:t+1}^{(i)} = x_{1:t+1}, x_{1:t+1} accepted | previous i - 1 beams all rejected)$\n$P_{2,i} = P(x_{1:t+1}^{(i)} = x_{1:t+1}, x_{1:t+1} rejected | previous i - 1 beams all rejected)$\nNotice that\n$p_{1,1} = q_{beam}(x_{1:t+1}) min(1, \\frac{p_1(x_{1:t+1})}{q_{beam}(x_{1:t+1})})$\nwhere $q_{beam}$ is the beam sampling probability from the small model. And we have\n$p_{2,1} = P(x_{1:t+1} rejected)(p_{1,2} + p_{2,2})$\n$=(1 - a_1)(p_{1,2} + p_{2,2})$\nwhere $a_j = \\sum_{x_{1:t+1} \\in I} q_{beam} (x_{1:t+1}) min(1, \\frac{p_j(x_{1:t+1})}{q_{beam} (x_{1:t+1})})$ is the probability that j-th beam is accepted given that the first j - 1 beams are all rejected.\nSimilarly, for i = 2, ..., $W_S$ we have\n$p_{1,i} = q_{beam} (x_{1:t+1}) min(1, \\frac{p_i(x_{1:t+1})}{q_{beam} (x_{1:t+1})})$\nfor i = 2,..., $W_S$ -1 we have\n$p_{2,i} = P(x_{1:t+1} rejected)(p_{1,i+1} + p_{2,i+1})$\n$=(1 - a_i)(p_{1,i+1} + p_{2,i+"]}