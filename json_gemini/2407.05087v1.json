{"title": "Linear Attention Based Deep Nonlocal Means Filtering for Multiplicative Noise Removal", "authors": ["Xiao Siyao", "Huang Libing", "Zhang Shunsheng"], "abstract": "Multiplicative noise widely exists in radar images, medical images and other important fields' images. Compared to normal noises, multiplicative noise has a generally stronger effect on the visual expression of images. Aiming at the denoising problem of multiplicative noise, we linearize the nonlocal means algorithm with deep learning and propose a linear attention mechanism based deep nonlocal means filtering (LDNLM). Starting from the traditional nonlocal means filtering, we employ deep channel convolution neural networks to extract the information of the neighborhood matrix and obtain representation vectors of every pixel. Then we replace the similarity calculation and weighted averaging processes with the inner operations of the attention mechanism. To reduce the computational overhead, through the formula of similarity calculation and weighted averaging, we derive a nonlocal filter with linear complexity. Experiments on both simulated and real multiplicative noise demonstrate that the LDNLM is more competitive compared with the state-of-the-art methods. Additionally, we prove that the LDNLM possesses interpretability close to traditional NLM.", "sections": [{"title": "1 Introduction", "content": "Multiplicative noise is also known as speckle, a common noise among active imaging systems, like ultrasound imaging and synthetic aperture radar (SAR) imaging. This kind of noise affects the works based on the imaging results seriously. Unlike additive noise, the visual effect of multiplicative noise is usually more severe. Meanwhile, the coherent process of the active imaging system makes the multiplicative noise inevitable, therefore it is difficult to obtain clean references for training. The removal of multiplicative noise can help the application of various imaging systems in remote sensing, medical treatment and so on.\nThe typical multiplicative noise denoising method contains 3 categories: spatial filtering, transform domain filtering and nonlocal filtering. Spatial filtering mainly works by weighted averaging the pixels in the local range, and the weights can be set for different purposes. Transform domain filtering mainly works by transforming the origin image to another domain to separate"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Nonlocal Denoising", "content": "The nonlocal methods search pixels similar to the pixel to be processed on a large range and obtain the filtered result by weighted averaging of them.\nThe traditional local filtering can be formulated as follows:\n$\\2 = \u0391\u00b0\u03a9,$\nwhere A is the convolution kernel, \u03a9 refers to the patch centered on the pixel to be denoised. \u03a9 is represented in matrix form, which is called the search window here. Essentially, this operation can be understood as the weighted averaging of pixels in \u03a9.\n$\\x = \\sum_{\u03b1\u0395\u0391,\u039e\u0395\u03a9}axx.$\nOn this basis, nonlocal filtering expands the search window \u03a9, computes the weights based on the similarity calculations and then obtains filtered results by weighted averaging.\nIn many situations, the noise in the images is serious and the noise model is not completely subject to a particular distribution. It's impossible to restore the original pixel based on only the local pixels, while nonlocal filtering can find pixels relatively cleaner from a large search area to supplement the denoising. As long as the search area is large enough and there is a probability of finding similar pixels, the nonlocal method is capable of performing well.\nBased on this idea, NLM extracts the pixel information with the neighborhood matrices and calculates the similarities using the Euclidean distance. BM3D takes the image patches as prediction units and combines the 3D transform and wiener filtering to improve the denoising performance.\nAlthough these nonlocal filtering methods improve denoising performance, they lack a flexible similarity calculation and a faster inference speed. We employ neural networks to replace the key steps inside the nonlocal filtering, and then linearize the similarity calculation with kernel-based transforming."}, {"title": "2.2 Interpretability", "content": "Interpretable AI deals with developing explainable models. Scholars defined interpretability as how an Al agent provides reasoning for their decision-making. This makes the system more reliable and trustworthy.\nMultiplicative noise usually exists in radar images and medical images. The denoising of these images helps the downstream tasks such as radar target tracking, and medical diagnosis. The application scenarios of these tasks are mostly critical situations where a small change can have a significant impact. In these scenarios, interpretability in decision-making is critical.\nGenerally, traditional denoising methods embody strong interpretability, such as NLM. They have a clear structure and rigorous mathematical derivation. However, deep learning based methods usually treat neural networks as black boxes, where the removal of noise is entirely predicted based on the statistical characteristics of the training data. Therefore, Lefkimmiatis et al. incorporated the NLM algorithm into CNN, which uses a deep CNN to replace the similarity computation in the NLM algorithm. They use the image information in the search window as the model input, and calculate the model output by exploiting the similarity between neighborhood matrices. The final pixel prediction is calculated with the weight values. Cozzolino et al. and Denis et al. introduced the above idea to multiplicative noise removal, but these methods still have low interpretability as there are many components lacking clear operational logic.\nWe enhance the key steps in traditional nonlocal filtering, and optimize the pixel information representation step and similarity calculation step using deep networks. Additionally, we use the kernel function to speed the inference and improve the performance. By doing so, the proposed LDNLM has a better performance and a faster inference speed while the rigorous logical derivation is also highly retained."}, {"title": "3 Method", "content": "In this section, we propose a linear attention mechanism based on deep nonlocal means filtering (LDNLM). On the basis of traditional NLM, we first use the deep channel CNN to extract the pixels' neighborhood information, mapping the pixels to high dimension space. Then we further linear map them to Query, Key and Value vectors. After that, we employ the inner product and weighted averaging of multi-head attention instead of the similarity calculation and weighted averaging of NLM. Finally, we replace the similarity calculation with kernel mapping, and change the calculating order of inner product and weighted averaging, making the complexity of nonlocal denoising reduce to linear."}, {"title": "3.1 Deep Nonlocal Means Filtering", "content": "The framework of LDNLM can be draw as Fig. 1. Assuming in the search window \u03a9 in a noisy image, the I(t, k) refers to a neighborhood matrix centered at t with"}, {"title": "3.2 Linear Attention Based Deep Nonlocal Means Filtering", "content": "Whether the traditional nonlocal means filtering, or the above deep nonlocal means filtering, both suffer from low inference speed and high memory usage. They need to calculate the similarities of every pixel with each other, the complexity of this whole process is O(n\u00b2). Theoretically, the nonlocal filtering is able to find a pixel similar to the pixel to be processed in the search window, and is possible to get a good denoising result. However, the quadratic complexity limits the search window to be so large, that this unfortunately limits the performance.\nThe fundamental reason for the quadratic complexity lies in the Eq. (7). Where all N pixels in the search window should be used to calculate similarity with all other N-1 pixels. For the resulting vector Vi' of i-th pixel, we can write a generalized attention equation for any similarity function as follows:\n$\\V = \\frac{\\sum_{j=1}^{N} sim (Qi, Kj) V}{\\sum_{j=1}^{N} sim (Qi, Kj)}$"}, {"title": "4 Experiments", "content": "In this section, we first conduct comparative experiments on images containing simulated multiplicative noise. Then, to evaluate the LDNLM's performance on real images, we conduct comparative experiments on SAR images containing multiplicative gamma noise. After that, the ablation study is performed to analyze the effect of each improvement proposed. Finally, we discuss and validate the interpretability of LDNLM.\nAll experiments are conducted on an Nvidia RTX 2080Ti GPU with 11GB of video memory, and the deep learning code framework is PyTorch 1.10.1 based on Python 3.9.5"}, {"title": "4.1 On Simulation Images", "content": "We generate the testing dataset with the same method as synthesizing training data. In this experiment, UC Merced land-use data is utilized to synthesize the noisy images. The data was extracted from large remote sensing images from various areas across the United States. It contains clear optical images from 21 scenarios, and with size of 256 \u00d7 256. We select 8 images from each category to synthesize training data, 1 image to synthesize test data and 1 image to synthesize validation data.\nAdditionally, Peak noise-to-signal ratio (PSNR) and structure similarity (SSIM) are chosen to quantitative evaluate the denoising performance. PSNR and SSIM evaluate the smoothing degree and the structure preservation degree of the denoising results, respectively.\nIt can be observed that the LDNLM outperforms other state-of-the-art methods significantly. Traditional methods NLM suffers from over smoothing, BM3D and SAR-CAM are weak in removing the speckles in the single look situation. Deep learning methods SAR-CNN, MONet, CNN-NLM and LDNLM all achieve remarkable denoising performance, while the LDNLM removes the speckles more thoroughly and retains more structure details."}, {"title": "4.2 On Real SAR Images", "content": "Without loss of generality, we select a type of radar image, namely SAR image, as the noisy real images for denoising experiments. The SAR system uses electromagnetic wave echoes to achieve high-resolution imaging of ground objects. However, due to the principle of coherent imaging, SAR images inevitably suffer from the impact of speckles, i.e., the multiplicative noise.\nIn this experiment, we employ 2 SAR images from TerraSAR-X. The TerraSAR-X satellite operates in the X-band with a resolution of 3m x 3m in strip mode. We employ single-look images of urban and mountain scenes. The size of the SAR image from the urban scene is 256 \u00d7 256, and the size of the SAR image from the mountain scene is 512 \u00d7 512.\nThe calculation of PSNR and SSIM requires a clear reference image, however, it is impossible to obtain a clear reference for the image with multiplicative gamma noise. Therefore, we choose the equivalent number of looks (ENL) and the unassisted quantitative evaluation (M) to evaluate the performance. ENL is a rough measure of smoothness within a homogeneous region, the higher the better. The ENL of a selected image patch P can be calculated as follow:\n$\\ENL = \\frac{E [P2] 2}{Var (P2)}$\nAs shown in Eq. (19), M is the combination of the 'ENL,\u00fb and dh. 'ENL, is the absolute value of the relative residual due to deviations from the ideal ENL, and dh provides an objective measure for ranking despeckled results within the related ratio images.\nThe selected mountain SAR image and the filtered results of each model are shown in Fig. 3, and the selected urban SAR image and the filtered results of each model are shown in Fig. 4. The ENL and M of all filtered results are shown as Tab. 2, the blue boxes in the figures indicate the homogeneous regions bounded for the calculation of ENL. To better compare the results of each model, we enlarge the part bounded by the red box in Fig. 4 as Fig. 5. Additionally, Fig. 6 shows the ratio images for each model.\nThe ratio image of LDNLM is nearly pure noise, while others' contain many structures of streets. We can observe that the LDNLM gets the relative best texture detail repairing performance in two common SAR scenes. Compared to other methods, LDNLM achieves a competitive noise removal degree. Combing the ratio images and enlarged images of urban scenes, we can find that LDNLM is able to repair structures like roads and buildings whereas SAR-CNN and MONet are doing not good. It can be concluded that the LDNLM is better at balancing multiplicative noise removal and structure restoration."}, {"title": "4.3 Ablation Study", "content": "To verify the effectiveness of our strategy, we conduct extensive experiments on the major optimizing components of the proposed LDNLM.\nThe most important optimizations of LDNLM can be summarized as CNN based pixel information extraction, linear nonlocal denoising and larger search window. Therefore, we conduct comparative experiments on NLM, LDNLM replacing CNN based pixel information extraction with linear projection (LDNLM w/o CNN), LDNLM with standard multi-head attention mechanism (LDNLM w/o linear), LDNLM without CNN based pixel information extraction and linear attention (LDNLM w/o CNN, w/o linear), LDNLM with smaller layer numbers 1 and smaller head numbers 2 (LDNLM-small), LDNLM with larger search window 64, larger neighborhood radius 10 (LDNLM-Large). All models are trained"}, {"title": "4.4 The Interpretability of LDNLM", "content": "Since the LDNLM is an improvement of NLM, the LDNLM remains the same interpretability. The LDNLM can be seen as an improved NLM algorithm, replacing the neighborhood matrix with high-dimension vectors extracted based on CNNs, replacing the similarity calculation with kernel mapping based linear attention, replacing the pixel-wise weighted averaging with vector-wise weighted averaging and adding dimension reduction to predict the final pixel.\nWhile following the NLM idea, the attention mechanism bring the LDNLM a more accurate calculation of similarity. To validate that the linear attention is able to replace the traditional similarity calculation successfully, and can compute the high dimension representation of the filtered results. We obtain the results of attention calculation and interpret them with the visualization.\nAfter the first attention calculation of LDNLM, we get the provisional calculation result. The provisional result theoretically contains 73 \u00d7 73 vectors, corresponding to all pixels in the search window. For the purpose of making the results easier to read, we firstly extract 36 \u00d7 36 vectors from all 73 \u00d7 73 vectors, corresponding to the patch indicated as the yellow box in the filtered image Fig. 8. The vectors are dimension reduced to 2-dimension based on tSNE as Fig. 7. The color represents the final pixels' color in the filtered result, i.e., the amplitude value of synthetic image containing multiplicative noise.\nThrough the visualization, it can be observed that: The high dimension vectors computed by linear attention are obviously clustered into 2 clusters. The right cluster contains a small number of dots with black color, and the left cluster contains a large number of dots with gray color. Meanwhile, the filtered patch"}, {"title": "5 Conclusion", "content": "In this paper, we optimize the NLM denoising algorithm based on the deep CNN and linear attention, and propose a linear attention based deep nonlocal means filtering for multiplicative noise removal (LDNLM). Inheriting the framework of NLM, we employ deep channel CNN to extract pixel information, and we replace the similarity calculation and weighted averaging part with kernel-based linear attention. These optimizations make the nonlocal algorithm have linear complexity, and improve its performance greatly. Meanwhile, the interpretability of LDNLM is also largely inherited from traditional NLM. In the future, we will explore the LDNLM-based self-supervised strategy for multiplicative noise removal."}]}