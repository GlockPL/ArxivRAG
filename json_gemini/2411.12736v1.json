{"title": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language Models", "authors": ["Salma Kharrat", "Fares Fourati", "Marco Canini"], "abstract": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly depends on the quality of the instructions, which often require fine-tuning through extensive human effort. This highlights the need for automated instruction optimization; however, this optimization is particularly challenging when dealing with black-box LLMs, where model parameters and gradients remain inaccessible.\nWe propose ACING, a task-specific prompt optimization approach framed as a stateless continuous-action Reinforcement Learning (RL) problem, known as the continuum bandit setting. ACING leverages an actor-critic-based method to optimize prompts, learning from non-differentiable reward signals. We validate ACING by optimizing prompts for ChatGPT on 30 instruction-based tasks, as well as a summarization task. ACING consistently outperforms baseline methods, achieving a median score improvement of 10 percentage points compared to the best baseline considered. Furthermore, ACING not only recovers but also surpasses human-crafted expert instructions, achieving up to a 39 percentage point improvement over human benchmarks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks (Zhao et al., 2024; Touvron et al., 2023). This success is largely attributed to their strong instruction-following capabilities, which enable adaptation to diverse downstream applications (Chen et al., 2023; Liu et al., 2023). These instructions, commonly referred to as prompts, play a crucial role in the performance of LLMs (Wei et al., 2022; Zhu et al., 2024; Liu et al., 2023). Given the importance of prompts, researchers are increasingly interested in automating their optimization to reduce the need for manual adjustments, which is often a labor-intensive and costly process (Reynolds"}, {"title": "2 Problem Formulation", "content": null}, {"title": "2.1 Prompt Learning Objective", "content": "We aim to improve the performance of a black-box LLM, denoted by $f$, which can only be accessed through its API, while its internal parameters remain unknown. Given a task represented by an (unknown) distribution $(x, y) \\sim D$\u2014where $x$ denotes possible inputs and $y$ the corresponding correct outputs our goal is to find the optimal prompt $\\tau^*$ that maximizes the likelihood of $f$ producing correct outputs for a given task. This is evaluated using a scoring function $q(\\cdot, \\cdot) \\in [0, 1]$. The black-box model $f$ processes an input formed by concatenating $(\\oplus)$ the prompt $\\tau$ with the sentence $x$, producing a predicted output $\\hat{y} = f(\\tau \\oplus x)$. More formally, the objective is to maximize the expected score of the LLM in solving the task $D$, defined as:\n$\\max_{\\tau} E_{(x,y)\\sim D} [q(y, f (\\tau \\oplus x))]$."}, {"title": "2.2 Learning over a Continuous Space", "content": "Directly optimizing the prompt $\\tau$ for the black-box LLM model $f$ presents substantial challenges due to the discrete combinatorial nature of token selection in $\\tau$. \u03a4\u03bf mitigate this challenge, prior approaches like InstructZero (Chen et al., 2024) and INSTINCT (Lin et al., 2024) employ a white-box LLM, represented by $h$, to convert the discrete optimization problem into a continuous one by introducing a soft prompt vector $z \\in \\mathbb{R}^d$, which is a continuous $d$-dimensional vector representing the token embedding of a set of virtual tokens.\nThe white-box model $h$ has access to a dataset of exemplars, $E = \\{(u_j, v_j)\\}_{j=1}^m$, where each pair $(u_j, v_j)$ defines input-output text sequences that exemplify a downstream task. The white-box LLM $h$ serves as a proxy, mapping the soft prompt $z$ into a discrete prompt $\\tau$ suitable for input to the black-box LLM. Specifically, given the exemplars $E$ and the vector $z$, their concatenation is input to the white-box LLM, generating the discrete prompt $\\tau(z) = h(z,\\mathcal{E})$. This generated prompt $\\tau(z)$ is prepended to a test input $x_j$ from the validation set $V$, and the combined input is provided to the black-box LLM $f$ to generate an output $\\hat{y}_j = f(\\tau(z) \\oplus x_j)$. The output $\\hat{y}_j$ is then evaluated against the true output $y_j$ using the scoring function $q(\\hat{y}_j, y_j)$. By using a fixed set of exemplars $\\mathcal{E}$, the original discrete problem (Equation (1)) of finding the optimal prompt $\\tau$ is effectively transformed into a continuous optimization problem over the soft prompt vector $z$, as follows:\n$\\max_{z \\in \\mathbb{R}^d} E_{(x,y)\\sim D} [q(y, f (\\tau(z) \\oplus x))]$."}, {"title": "2.3 Reducing the Continuous Space", "content": "The soft prompt $z$ is typically high-dimensional (e.g., $d = 5120 \\times N_z$, with $N_z$ the number of virtual tokens, when $h$ is Vicuna-13B (Chiang"}, {"title": "3 Actor-Critic Approach for Instruction Learning in Black-Box LLMS", "content": null}, {"title": "3.1 Instruction Learning as an RL Problem", "content": "In this work, we frame the prompt-learning problem as an agent navigating an infinite action space within a $d'$-dimensional unit hypercube, searching within an infinite number of actions $a \\in [0, 1]^{d'}$, in contrast to previous works that consider a finite number of actions initially generated with Sobol (Eriksson et al., 2019; Lin et al., 2024). As part of the environment, these continuous actions are projected into a higher-dimensional space (\u00a7 2.3) and then transformed into discrete prompts, through a white-box LLM (\u00a7 4). These discrete prompts are evaluated by a black-box LLM over a validation dataset, after which a reward is computed and returned to the agent. We consider a finite horizon $T$, representing the maximum number of actions played (i.e., the budget of evaluated instructions), after which the agent ceases exploration. The agent's goal is to discover the optimal continuous action, $a^*$, which produces the optimal discrete prompt and the highest reward.\nUnlike in general RL settings, where the agent encounters multiple states with varying reward distributions, the agent we consider operates in a stateless, stochastic environment, commonly referred"}, {"title": "3.2 Actor-Critic Continuum Bandits", "content": "RL techniques are generally used to learn optimal policies and actions leading to best rewards. These techniques can be classified into value-based (critic-centric), policy-gradient (actor-centric), or hybrid approaches. Actor-critic algorithms merge the benefits of both by employing two key components: the actor, responsible for policy optimiza-within an infinite number of actions $a \\in [0, 1]^{d'}$tion, and the critic, which evaluates the policy (Sutton and Barto, 2018). This hybrid approach has demonstrated success in various domains, including continuous control problems (Mnih et al., 2016; Haarnoja et al., 2018, 2019) and with generative adversarial networks (GANs) (Goodfellow et al., 2014), where a generator and a critic network collaboratively optimize performance.\nInspired by the success of actor-critic approaches, we introduce a (stateless) actor-critic algorithm, as provided in Algorithm 1, tailored to our infinite continuum bandit setting, enabling autonomous learning of effective prompts with a constrained evaluation budget and outperforming previous state-of-the-art black-box prompt learning approaches. Both the actor and the critic are parameterized by neural networks. In typical RL problems, the agent interacts with an environment with varying states, so its policy depends on the current state. That is, the policy network generates actions based on the current state $s, \\pi(.|s; \\theta)$, where"}, {"title": "3.3 Enhancing Exploration with Entropy", "content": "In general, RL learning approaches aim to maximize the expected cumulative rewards up to a horizon $T$, expressed as $\\max_{\\pi(\\cdot)} \\sum_{t=1}^T E_{a_t \\sim \\pi(\\cdot)} [r(a_t)]$. In contrast, black-box prompt learning, framed as a best-arm identification problem, seeks the best prompt without necessarily maximizing the expected cumulative rewards. To maximize the cumulative rewards, the agent must manage the exploration-exploitation trade-off. Conversely, for the best-arm identification problem, the agent should explore as extensively as possible within the $T$ evaluation budget to potentially identify the best action,"}, {"title": "4 Methodology", "content": "The methodology of our proposed algorithm, AC-ING, is illustrated in Fig. 1, showing the actor-critic interaction with the environment. In Fig. 2, the environment is zoomed in, using the larger_animal dataset as an example. In the following, we provide a detailed explanation of the full methodology.\nOverview. In each iteration $t < T$, the actor-critic agent generates a continuous vector \"action\" $a$ (step 1). This action is then played on the environment, which projects $a$ into the appropriate space using a fixed projection matrix $P$ to obtain $z$. The environment then concatenates the projected vector $z$ with a set of exemplars' embeddings from $E$ and feeds it into a white-box LLM $h$ (step 2). The white-box LLM produces a discrete prompt, $\\tau$, which is evaluated using the validation dataset $V$ based on the responses from the black-box LLM $f$ (step 3). The black-box LLM generates a prediction, which is then contrasted to the true labels of the validation examples, and a score function provides a final reward to the critic. This reward is used to compute the critic's loss and update both the critic and actor networks accordingly.\nStep \u2460. The policy-network (actor) outputs a mean and variance of a distribution from which the action is sampled. Specifically, the action, represented as a soft prompt vector, $a \\in \\mathbb{R}^{d'}$, is obtained by sampling from a Gaussian distribution with the outputted parameters. The network also computes the associated log probability, which is crucial for policy optimization, as it guides the learning process by balancing exploration and exploitation, by controlling the policy loss, as shown in Eq. (6).\nStep \u2461. As depicted in the left side of Fig. 2, the examples describing the task from the set of exemplars $E$, along with additional text such as \u201cThe instruction was to,\u201d are input into the embedding layer of the white-box LLM to generate continuous vectors. These continuous vectors are then concatenated with the soft prompt $z$, projected from the action $a$. The layers of the white-box LLM subsequently process the resulting concatenated vector to produce the discrete prompt $\\tau$ (using the instruction generation template depicted in Fig. 1 top right). This transformation is essential for converting the"}, {"title": "5 Experiments", "content": "We tackle instruction learning for ChatGPT. We conduct instruction induction tasks using 30 datasets from (Honovich et al., 2023; Chen et al., 2024), along with a data summarization task from (Gliwa et al., 2019). Furthermore, we compare the best-learned instructions from our ACING algorithm against human-provided instructions from (Honovich et al., 2023), as shown in Table 1, with additional best-learned instructions in Table 9. Moreover, we compare ACING with three recent representative baselines for black-box instruction learning: APE (Zhou et al., 2023), In-structZero (Chen et al., 2024), and INSTINCT (Lin"}, {"title": "5.1 ACING vs. Humans", "content": "We use human instructions provided in (Honovich et al., 2023) for various instruction-induction tasks, test them, and compare their scores against those of our best instructions. The complete list of selected tasks is provided in Table 1 in the Appendix. Several instructions were solved by both human experts and our approach, i.e., achieving a perfect test score of 1.00. For clarity and brevity, we omit"}, {"title": "5.2 ACING vs. Other Optimization Methods", "content": "We compare our method against recent baselines on the 30 instruction-induction datasets. Table 2 presents the average test accuracy (along with the standard deviation) over three independent runs, using three different seeds. For each seed, we selected the best instruction achieved by each method and evaluated it on the testing dataset. The table demonstrates that our method, ACING, outperforms others by achieving the highest accuracy in 14 out of 30 tasks, compared to INSTINCT, InstructZERO, and APE which succeeded in 9 tasks or less each. Additionally, ACING achieves the highest median accuracy across tasks, with a value of 0.71, which is 22 percentage points higher than APE. Further"}, {"title": "5.3 Splitting the Budget", "content": "Due to the stochastic nature of the black-box LLM, the same instruction may yield different rewards when evaluated by the LLM. For more robust prompt selection, the budget T can be split into two parts: an actor-critic exploration phase (repeating steps 1 to 4), followed by a uniform exploration phase. In the latter phase, the top p prompts discovered by the actor-critic are evaluated multiple times, k times each, using the black-box LLM. Specifically, the first phase uses T \u2013 pk API calls, while the second uses the remaining calls. The prompt with the highest average reward across repetitions"}, {"title": "5.4 Ablation Studies", "content": null}, {"title": "5.4.1 Using Smaller Budgets", "content": "In the main paper, we report the final test score after a fixed budget of 165 black-box LLM calls, mainly following previous work (Lin et al., 2024; Chen et al., 2024), avoiding any potential advantage that could arise from optimizing this number. In the Appendix, we provide reward plots for the ACING approach, showing the best achieved reward over the calls. As shown in various plots in Figure 3,"}, {"title": "5.4.2 Using a Different White-box Model", "content": "In the main paper, we present results using Vicuna as the white-box language model, following the setup of prior work. To evaluate the impact of different white-box models on the performance of our method, we also tested ACING with WizardLM-13B-v1.2 (Xu et al., 2024) compared to Vicuna-13B-v1.3. The experiments in Table 8 indicate that WizardLM achieved a higher median test score across all tasks and excelled in a greater number of top-performing tasks, demonstrating that tuning the choice of the white-box model can improve results."}, {"title": "5.4.3 Using Different Action Dimensionalities", "content": "In the main paper, we present results using actions with dimension d' = 10, following the setup of prior work. To evaluate the performance of ACING with different dimensionalities, we conducted experiments with $d' \\in \\{5, 10, 20, 40\\}$, keeping other parameters fixed, for a budget of 165 calls. The results, shown in Table 6, indicate that while the smallest dimension, d' = 5, recovered the best scores for some tasks, it generally has the lowest performance across most tasks. Furthermore, both d' = 10 and d' = 20 yield similar performance in terms of the number of best-performing tasks (9-10 tasks), indicating a low sensitivity to this parameter. For the much larger dimension, d' = 40, the method achieved the highest number of best-performing tasks, showing that tuning (increasing) d' can improve results."}, {"title": "5.4.4 Using a Different Number of Exemplars", "content": "In the main paper, we present results using five exemplars, following the setup of prior work. To evaluate the performance of ACING with a smaller number of exemplars, we conducted an experiment using a single exemplar, i.e., $|E| = 1$. Results shown in Table 7 confirm that using five exemplars generally yield higher test scores and a greater number of best-performing tasks than a single exemplar. However, a single exemplar still performs"}, {"title": "6 Related Work", "content": "Many early approaches such as AutoPrompt (Shin et al., 2020), FluentPrompt (Shi et al., 2023), and other soft prompt-based methods (Lester et al., 2021; Li and Liang, 2021; Zhong et al., 2021) focused on optimizing prompts for white-box LLMs and cannot be directly applied to black-box LLMs. Moreover, several techniques have also been proposed for optimizing prompts for grey-box LLMs: BBT (Sun et al., 2022b), BBTv2 (Sun et al., 2022a), and clip-tuning (Chai et al., 2022) use evolutionary algorithms for this purpose. However, these methods still require access to input token embeddings and output logits, making them unsuitable for settings where only query access is allowed.\nOther approaches (Li and Liang, 2021; Zhong et al., 2021) employ continuous soft prompts by fine-tuning parameters of specific input tokens, but these often generate human-unreadable prompts and are impractical for API-accessible LLMs. Some other methods (Deng et al., 2022; Zhang et al., 2023), created for optimizing discrete prompts instead of continuous ones, can optimize discrete tokens but require access to the LLM's output distribution, which is typically unavailable in black-box LLMs. Zhou et al. (2023) proposed generating candidate prompts through resampling without specific optimizations, while Guo et al. (2024) extended this model-free approach by using evolutionary algorithms. However, these methods usually require a large number of queries.\nInstruction optimization in black-box LLMs recently focused on searching through the space of soft prompts, then relying on open-source white-box LLMs as a means to transform these soft prompts into discrete prompts. InstructZero (Chen et al., 2024) learns discrete prompts for ChatGPT by searching through the continuous space of soft prompts using Bayesian optimization, which are then transformed into discrete prompts by a white-box LLM. INSTINCT (Lin et al., 2024), a successor to this approach, relies on the same approach. However, instead of Bayesian optimization, it uses NeuralUCB (Zhou et al., 2020), a contextual bandit"}, {"title": "7 Conclusion", "content": "ACING presents a promising approach for learning prompts for black-box LLMs by framing it as a stateless continuous-action RL problem. Our method shows significant improvements over recent baseline methods. Evaluations across 31 datasets reveal that ACING enhances prompt quality, achieving a median score improvement of 10 percentage points and outperforming human-crafted instructions by up to 39 percentage points. This sets a new benchmark for automated instruction optimization in black-box LLMs."}, {"title": "8 Limitations", "content": "While our proposed results were achieved using default and commonly used values for hyperparameters in network design, learning rates, and following previous works for some common parameters, the results presented depend on these choices. These results could be further improved with fine-tuning, as demonstrated in the ablation studies (\u00a7 5.4). Another limitation of this work, similar to previous studies, is the risk of overfitting when dealing with a limited number of validation examples. This can result in a prompt that minimizes the loss on the validation set but does not generalize well to the test dataset, leading to a discrepancy between training and test performance.\nAnother limitation of this work, similar to previous work (Lin et al., 2024; Chen et al., 2024), is the reliance on a white-box model, where the choice of model may lead to performance variations, as demonstrated in \u00a7 5.4.2. Avoiding this limitation requires acting directly on the black-box LLM, which involves handling actions in a large discrete action space. This necessitates adapting current RL approaches for large discrete action spaces, as studied in (Fourati et al., 2024) and the references therein."}, {"title": "A Experimental Details", "content": null}, {"title": "A.1 Hyperparameters Remark", "content": "Across the diverse tasks, in the main paper, the same hyperparameters were used, which shows that the algorithm generalizes well across the 30 tasks without specifically tuning hyperparameters in each task. A summary of the key parameters cab be found in the following Table. An implementation of ACING is available at https://github.com/salmakh1/ACING."}, {"title": "A.2 Actor-Critic Details", "content": "Across all the tasks, we used three fully-connected layers for both the actor (1 \u2013 1024 \u2013 256 \u2013 10) and the critics (10 \u2013 128 \u2013 128 \u2013 1) networks, with learning rates fixed at 3 \u00b7 10\u20134 for each. We used two critic networks and consider their minimum as the actual critic. We learn the entropy parameter \u03b1 using a learning rate of 9\u00b710-4."}, {"title": "B ACING Rewards over the (Calls) Steps", "content": "In the main paper, we report the final test score after a fixed budget of 165 black-box LLM calls. In this section, we provide reward plots for the ACING approach, showing the best-achieved reward within the conducted calls. As shown in various plots in Figure 3, the ACING approach found the optimal prompt (achieving a reward of 1) within just a few black-box calls. Some tasks required fewer than 10 API calls to find the optimal instruction, such as for 'active to passive' and 'letters list', and fewer than 20 for tasks like 'translation' and 'diff'. It can be seen that the vast majority of tasks achieved their best reward value within the first 60 to 80 calls, demonstrating that ACING can even be used for much more constrained budgets. The choice of 165 calls was mainly based on previous work (Lin et al., 2024; Chen et al., 2024), avoiding any potential advantage that could come from optimizing this number."}, {"title": "D ACING with Different Intrinsic (Action) Dimensions", "content": "In the main paper, we present results using actions with a dimension of d' = 10, following the setup of prior work. To evaluate the performance of ACING across different dimensionalities, we conducted experiments with d' \u2208 {5, 10, 20, 40}, keeping other parameters fixed, for a budget of 165. We report the test results over different tasks and dimensionalities for a fixed seed. The results, shown in Table 6, indicate that while the smallest dimension, d' = 5, recovered the best scores for some tasks, it generally has the lowest performance across most tasks. Furthermore, both d' = 10 and d' = 20 yield similar performance in terms of the number of best-performing tasks (9-10 tasks), indicating low sensitivity to this parameter. For the much larger dimension, d' = 40, the method achieved the highest number of best-performing tasks, demonstrating improved performance with increased dimensionality. Further increasing the dimensionality to d' = 100 can still yield high results, outperforming d' \u2208 5, 10, 20. However, while it remarkably outperformed d' = 40 in some tasks, such as the second word letter task, synonyms, and antonyms, it only achieved"}, {"title": "E ACING with Different Number of Exemplars", "content": "In this section, we test ACING with a single exemplar, in contrast to the main results in the paper, which use five exemplars for ACING and all other benchmarks. For these experiments, we fix all hyperparameters as in the main paper and run tests with a budget of 165. Intuitively, providing more exemplars to the language model should facilitate prompt learning, so five exemplars are expected to yield better prompts than a single exemplar. Our experiments, summarized in Table 7, support this intuition. The results show that using five exemplars leads to higher test scores, as reflected in a greater number of best-performing tasks and an increase in median test scores across tasks. However, it is notable that performance did not decrease drastically with only one exemplar, suggesting that a single exemplar is sufficient to achieve decent results. In fact, across several tasks and categories (e.g., phonetics, summation, morpho-syntax, and translation), a single exemplar achieves the same performance of using five exemplars, and even outperforms the use of five exemplars in certain tasks. Nevertheless, using a single exemplar resulted in lower performance mainly in more cognitively challenging tasks, which is understandable, as more complex tasks are likely to benefit from additional exemplars."}, {"title": "F ACING with Different White-box models", "content": "In this section, we evaluate the impact of the choice of white-box model on the ACING method. Specifically, we applied ACING for instruction learning with a GPT-3.5-turbo as the black-box LLM (as in the main paper), but using different white-box LLMs. In the main paper, we reported ACING with Vicuna-13B-v1.3; in Table 8, we further test it with WizardLM-13B-v1.2. As shown in the table, changing the white-box LLM results in slight variations in performance. WizardLM achieved a higher median test score across all tasks and excelled in a greater number of top-performing tasks."}, {"title": "G Demonstrations with Human Instructions", "content": null}]}