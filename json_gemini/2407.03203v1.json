{"title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts", "authors": ["Ruida Wang", "Jipeng Zhang", "Yizhen Jia", "Rui Pan", "Shizhe Diao", "Renjie Pi", "Tong Zhang"], "abstract": "Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. Similar methods have shown promising results in code generation. However, most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data. This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address the challenges, this paper proposes TheoremLlama, an end-to-end framework to train a general-purpose LLM to become a Lean4 expert. This framework encompasses NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing. Using the dataset generation method, we provide Open Bootstrapped Theorems (OBT), an NL-FL aligned and bootstrapped dataset. A key innovation in this framework is the NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leveraging the NL reasoning ability of LLMs for formal reasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. We have also open-sourced our model checkpoints and generated dataset\u00b9 , and will soon make all the code publicly available\u00b2.", "sections": [{"title": "1 Introduction", "content": "The ability to perform logical reasoning has always been regarded as a cornerstone of human intelligence and a fundamental goal of machine learning systems (Newell and Simon, 1956). Among these tasks, mathematical reasoning is considered crucial for evaluating the capabilities of Large Language Models (LLMs). However, in modern mathematics, verifying the correctness of theorem proofs written in natural language is challenging, complicating the assessment of LLMs' mathematical reasoning in advanced topics. Additionally, the rapid development of modern mathematics and the increasing complexity of proofs pose significant barriers to reviewing their correctness. This has led to erroneous proofs that require considerable effort to be identified by the mathematical community, as exemplified by the process of proving Fermat's Last Theorem (Taylor and Wiles, 1995). To address these issues, formal mathematical languages such as Lean (De Moura et al., 2015; Moura and Ullrich, 2021), Isabelle (Paulson, 1994), and HOL Light (Harrison, 2009) have been developed. These languages allow computers to automatically verify proofs, providing a clear standard for evaluating mathematical theorem proofs and significantly impacting both the mathematical and computer science communities.\nHowever, writing mathematical proofs in Formal Language (FL) requires significant expertise and effort. Additionally, formal proofs involve much repetitive and tedious work, which is not customary for mathematicians who are more familiar with high-level proofs. Consequently, there has been significant demand for automated theorem-proving using FL, leading to a considerable number of works on this task (Polu and Sutskever, 2020; Polu et al., 2022; Jiang et al., 2021, 2022b,a; Yang et al., 2024). However, most of these works rely on searching methods in an infinite space of possible tactics to complete the proof, resulting in unaffordable computational costs (e.g., Polu et al. (2022) uses 2,000 A100 GPU hours for training) for complex proofs and not fully leveraging NL proofs. Recent advancements in LLMs, especially in reasoning and coding, have prompted researchers to explore using them to write formal proofs guided by natural language (Wu et al., 2022; Jiang et al., 2022a).\nIn this paper, we focus on enabling LLMs to write formal Lean4 proofs guided by natural language (NL) proofs. We have chosen Lean4 because it has recently garnered considerable attention from the mathematical community (Tao et al., 2023; Tao, 2023; Avigad et al., 2020), whereas Lean3 and Isabelle are older formal languages. Despite the potential demonstrated by previous works (Wu et al., 2022; Jiang et al., 2022a) in similar tasks using Isabelle, the few-shot performance of LLMs in Lean4 remains relatively unsatisfactory. This is because Lean4 is a more concise FL that differs significantly from NL, making the direct transfer of reasoning abilities from NL to Lean4 infeasible. The situation is exacerbated by the inclusion of confusing Lean3 code in the LLMs' pre-training data (details in Appendix A). More importantly, there is a significant lack of aligned data between NL and FL, making the training of LLMs to write Lean4 proofs an overlooked and challenging task. Additionally, Jiang et al. (2022a) indicates that there remains a large potential for researchers to fully utilize LLMs in writing formal proofs.\nTo address these challenges, we propose TheoremLlama, an end-to-end framework that transforms a general-purpose LLM into a Lean4 expert. The framework overview is presented in Fig. 1. Our framework comprises three major components:\n(a) NL-FL Aligned Data Generation: This component tackles the data scarcity problem. During generation, we identified Mathlib4, a pure Lean4 repository containing 100k proofs of important mathematical theorems. We deformalize Mathlib4 (i.e., write natural language theorem statements and proofs based on Lean4 code) using a powerful LLM with retrieved examples from a fine-tuned T5 encoder. Subsequently, we bootstrap the NL-FL aligned data by integrating the natural language proofs into Lean4 code via comments. This process of embedding natural language reasoning within the code helps the LLM better understand the theorems and leverages its natural language reasoning ability to perform formal reasoning. Following this generation and bootstrapping method, we create the Open Bootstrapped Theorems (OBT) dataset.\n(b) Lean4 Prover Training: This component introduces training methods that are currently understudied in the field. It includes block training techniques to improve the LLM's in-context learning ability and curriculum data sorting tactics to ensure a smoother training process. Using this method, we fine-tune Llama3-8B-Instruct to be a Lean4 expert with the OBT dataset.\n(c) Iterative Proof Writing: This component enhances the LLM's ability to write formal proofs by using previously generated correct proofs as in-context examples to further improve its formal reasoning capabilities."}, {"title": "2 Methodology", "content": "In this section, we present the details of TheoremLlama, including generation methods for the OBT dataset. The key idea for our framework is to enable LLMs to perform well in the unfamiliar Lean4 theorem proving task under the circumstances of limited or even confusing data during its pre-training. We introduce the Dataset Generation method in Section 2.1, illustrate the training techniques for Lean4 prover training using the OBT dataset in Section 2.2, and propose an Iterative Proof Writing method LLM prover in Section 2.3. The task that this methodology works on can be defined as: \"Training the LLM to have an improved ability in Lean whole-proof generation under the guidance of Natural Language Proofs.\""}, {"title": "2.1 NL-FL Aligned Data Generation", "content": "This section describes the Natural Language (NL) - Formal Language (FL) Aligned Dataset Generation method. As previously discussed, we chose Lean4 as the formal language for our study. The dataset generation aims to enhance the LLM's ability in Theorem proving from the dataset point-of-view. To the best of our knowledge, no open-source Lean4 NL-FL aligned dataset exceeds 1k records, our dataset generation provides Open Bootstrapped Theorems (OBT) dataset containing 106,852 NL-FL aligned and bootstrapped theorems."}, {"title": "2.1.1 Lean4 Proof Extration", "content": "Although there is no NL-FL aligned dataset, for Lean4, there is Mathlib4, a repository containing 100k high-quality, human-crafted proofs. It is a general repository that contains the most important definitions and theorems from basic mathematics, including logic, set theory, number theory, and algebra; to advanced topics like topology, differential geometry, and real/complex analysis. Mathlib4 offers the LLM a high-quality, and comprehensive foundation for dataset generation tasks. Previous works have used such a dataset for tree-search prover training (Yang et al., 2024; Polu et al., 2022). We directly extract Mathlib4's theorems from the LeanDojo repository for further dataset generation."}, {"title": "2.1.2 Deformalization with Example Retrival", "content": "To the best of our knowledge, the potential for using Mathlib4 as a dataset for training LLMs to generate formal proofs based on natural language guidance is an understudied field. This is due to Mathlib4 does not contain corresponding natural language statements for most of the theorems. However, with the development of modern LLMs, we propose a way to generate the NL-FL aligned dataset for Mathlib4. This method, which writs informal proofs from formal proofs, is called deformalization.\nDue to the mix of Lean4 and Lean3 data on the internet, LLMs pre-trained on web-scale data only have limited ability to recognize Lean4 proofs and may be interfered by perplexing Lean3 data. Therefore, it is important to have high-quality in-context examples for deformalization. We develop the example retrieval method to extract such high-quality examples. The first step for our example retrieval is using the Natural Language annotated MiniF2F dataset (Jiang et al., 2022a) to fine-tune the ByT5-Tacgen model provided by Yang et al."}, {"title": "2.1.3 NL-FL Bootstrapping", "content": "We find that due to the significant differences between performing natural language reasoning and Lean4 theorem proving, externally NL-guided training data is not sufficient to enable LLMs to develop strong Lean4 theorem-proving abilities. It is common for the LLMs to lose track of the proof and repeatedly generate the final Lean4 tactic. Inspired by findings in LLM coder (Song et al., 2024), where NL comments of code task description can largely improve the performance of LLM coders. We propose the novel NL-FL Bootstrapping. This is a simple but effective method to enhance the LLMs' Lean4 proof writing ability by integrating natural language reasoning into Lean4 proofs in Mathlib4.\nWe achieve such an integration by providing Gemini with NL and FL of the theorem and asking it to document the natural language proof to the Lean4 code through comment. We ensure the correctness of the bootstrapped data by running a check algorithm that removes all comments in the generated code and makes sure it is the same as the original code.\nThis bootstrapping approach aims to lower the barrier between complex and unfamiliar-to-LLM Lean4 formal language reasoning and natural language reasoning. We find that most modern LLMs possess relatively strong natural language reasoning abilities but lack familiarity with formal reasoning. This method helps LLMs transfer their natural language reasoning skills to Lean4 theorem proving by bootstrapping the dataset, prompting the LLM to perform both formal and informal reasoning simultaneously. LLMs trained with the bootstrapped dataset will learn to better utilize the NL steps to guide Lean4 proof writing. Following above generation and bootstrapping method, we have the Open Bootstrapped Theorems (OBT) dataset for training LLMs."}, {"title": "2.2 LLM Prover Training", "content": "Training LLMs to generate whole proof based on natural language guidance is an under-explored field of study due to the lack of datasets. There are only a few studies that discuss the training method of the LLM for such a task. This section proposes two instruction fine-tuning techniques to train the LLMs for formal reasoning tasks, namely Block Training and Curriculum Data Sorting."}, {"title": "2.2.1 Block Training", "content": "The Block Training method aims to incorporate the in-context learning ability during training. For standard instruction fine-tuning in formal theorem proving, we use natural language as the input and the corresponding Lean4 with bootstrapped NL reasoning as the target output to fine-tune the LLM. In the Block Training, we view the tokenized training dataset as a ring of text. We take full advantage of the context length of LLM by filling it with examples of previous records. Formally, the original training data for i-th record is:\n{\"Instruction\": $NL_i$, \"Target\": $FL_i$}\nwhere $NL_i$ is the natural language of i-th record and $FL_i$ is its corresponding bootstrapped Lean4 code. After Block Training, the i-th data record is:\n{\"Instruction\": \"$NL_{i-k}, FL_{i-k};\u00b7\u00b7\u00b7 FL_{i-1}; NL_i$,\"\n\"Target\": \"$FL_i$}\nwhere k is the number of examples that just fill the context length.\nUsing the block training method, we enhance the LLM's in-context learning ability for Lean4, providing a better understanding of examples when writing proofs."}, {"title": "2.2.2 Curriculum Data Sorting", "content": "Because modern LLMs have limited exposure to writing Lean4 proofs with NL guidance during pre-training, they are unfamiliar with this task. This issue is evident as LLMs with a significant difference in parameters show only slight performance differences in these tasks (details in Section 3.3). Inspired by previous work in Curriculum Learning (Polu and Sutskever, 2020; Soviany et al., 2022), we propose a training data sorting technique named Curriculum Data Sorting to enable LLMs to learn this unfamiliar task from easy to difficult.\nSpecifically, we reorganize the generated training dataset by difficulty level. We measure the difficulty of a Lean4 proof by the steps it takes to solve all goals and sort the training data records with easier data at the beginning and harder data at the end. This sorting method allows the LLM to first learn to solve trivial and easy problems before tackling complex proofs. It largely stabilizes the loss curve of training and improves the performance of the LLM prover."}, {"title": "2.2.3 Instruction Fine-tuning", "content": "Using Blocked Training and Curriculum Data Sorting on the OBT dataset, we perform the Instruction Fine-tuning on Llama3-8B-Instruct using SFT trainer in an autoregressive manner. The given instruction is a natural language statement and proof of a theorem, along with a Lean4 theorem statement, and use examples in the dataset filling context windows. The target output is the Lean4 proof bootstrapped with natural language explanations. This process trains the LLM to leverage its natural language reasoning ability to write Lean4 proofs."}, {"title": "2.3 Iterative Proof Writing", "content": "Jiang et al. (2022a) have demonstrated that the potential of LLMs in formal reasoning is largely undervalued. Typically, LLMs possess relevant knowledge but lack appropriate methods to extract this knowledge in Lean4 form. To further harness the LLM's ability to prove Lean4 theorems, inspired by Wang et al. (2023), we propose the Iterative Proof Writing strategy. This method involves initially having the prover finish as many theorems in a dataset as possible. Then, use the Lean-verified correct proofs written by the current prover as additional examples for proof writing in the next iteration. The iteration stops when the maximum number of steps is reached or no additional theorems can be proved with the given examples. This step is effective because there are potential distribution shifts between the generated and the real-world natural language statement and proof, using examples from the same dataset, such differences can be largely mitigated."}, {"title": "3 Experiments", "content": "We conduct extensive experiments on the MiniF2F-Lean4 dataset (Zheng et al., 2021) to test the effectiveness of TheoremLlama framework on formal reasoning with NL guidance. We also conduct ablation studies (Section 3.4) and case studies (Section 3.6) to further validate the TheoremLlama."}, {"title": "3.1 Experiment Setup", "content": ""}, {"title": "3.1.1 Dataset and Task", "content": "In this work, we evaluate the TheoremLlama Lean4 formal reasoning ability on MiniF2F-Test and Validation dataset (Zheng et al., 2021) and NL theorem statement and proofs provided by Jiang et al. (2022a). We contribute the Lean4 version of MiniF2F-Test based on (Yang et al., 2024). MiniF2F is a standard testing dataset for evaluating the performance of formal provers. Both the test and validation datasets contain Lean4 statements of 244 problems. The range of problems varies from high-school competition questions to undergraduate-level theorem proofs. Specifically, MiniF2F comprises a total of 488 problems from three sources: (1) 260 problems sampled from the MATH dataset (Hendrycks et al., 2021); (2) 160 problems from high-school mathematical competitions (including AMC, AIME, and IMO); (3) 68 manually crafted problems at the same difficulty level as (2).\nOur task is to query LLM to generate the complete Lean4 proofs for the mathematical problems in MiniF2F based on their Lean4 statement and NL statement and proof together using no more than 16 in-context examples. All the imports are manually set to lighten the workload of LLM."}, {"title": "3.1.2 Baseline", "content": "Due to the lack of previous studies that use LLMs to generate complete proofs for Lean4; and most of the existing works working on Reinforcement Learning (RL) or searching methods, there are universally approved baselines for comparison. Many existing works are focusing on Isabell (Jiang et al., 2022a,b; Wu et al., 2022), a language that is largely different from Lean4, making direct comparison infeasible (Yang et al., 2024). Many Lean-based methods concentrate on online iteration with Lean (Lample et al., 2022; Polu et al., 2022).\nTherefore, our baseline selection focuses on three-based methods without RL and few-shot LLM proof writing. The baselines we use include: (1) Expert Iteration (Polu et al., 2022): A tree search method based on GPT-f (Polu and Sutskever, 2020) that applies expert iteration to enhance the performance. 3; (2) ReProver (Yang et al., 2024): The Lean4 tree-search baseline that builds on ByT5 to search for tactics based on current formal state and goal. (3) Few-shot LLMs: This baseline focuses on directly querying LLMs to get the full proof of a formal theorem in a few-shot manner. In particular, we choose GPT-4-Turbo (Achiam et al., 2023)4, Gemini-1.5 (Reid et al., 2024), and Llama3-8B-Instruct (AI@Meta, 2024). This baseline is set to compare the TheoremLlama's ability to perform formal reasoning effectively.\nSince we focus on generating the whole proof at a time we choose pass@1 result for all tree-search methods because the tree-searched method will exponentially increase computation time when the candidate tactic number exceeds 1."}, {"title": "3.2 Implementation Details", "content": "The OBT dataset is generated using Gemini-1.5-Pro-0409 (Reid et al., 2024) for writing the natural language of the theorems and performing NL-FL bootstrapping. We use Gemnin-1.5 because it can give more human-like proofs and a better ability in NL-FL combination, details can be found in Appendix C. The OBT dataset contains NL-FL aligned and bootstrapped 106,852 theorems, the data record format is in Appendix D. We perform Instruction Fine-tuning on Llama3-Instruct-8B (AI@Meta, 2024) with 1,000 warm-up steps and a learning rate of 1E-5. Training takes approximately 32 hours on an 8 GPU A6000 machine. During the evaluation, we perform a uniform 128 generation for LLM's whole-proof generation. The initial examples for in-context learning are obtained from the proved theorem list of Yang et al. (2024). Depending on the context length, we use 10-16 examples for all LLM queries. We stop at the second round of iterative proof writing."}, {"title": "3.3 Results", "content": "We present the main experimental results in Tab. 1. From the table, we can observe that TheoremLlama achieves a cumulative accuracy rate of 36.48% on MiniF2F-Valid and 33.61% on MiniF2F-Test, suppressing all the baselines.\nIt is also notable that the result of un-finetuned Llama3-8B-Instruct and GPT-4 have a similar accuracy rate on both the Test and Valid set despite the great difference in model size, this demonstrates most modern LLMs are under-trained on Lean4 reasoning. Surprisingly, Gemini has the best performance among all the baselines rather than GPT-4; this demonstrates its superior ability to understand formal language and gives indirect evidence that Gemini is a better choice of LLM to perform De-formalization and Bootstrapping.\nFor the tree-search method, the large search space limits the choice of model in relatively small size and they only achieve an average 27.2% accuracy rate, which is relatively low, demonstrating the limitation of such a method."}, {"title": "3.4 Ablation Studies", "content": "Due to the low GPU footprint of TheoremLlama, we are able to perform a comprehensive ablation study to test the effectiveness of each component in the framework, namely, NL guidance, NL-FL bootstrapping, block training, and curriculum data sorting. In the ablation studies, we use the result of the first iteration with the default example list from Yang et al. (2024). The results are demonstrated in Tab. 2. From the table, we can find that the removal of any component of our framework will lead to a significant performance drop compared to the full framework result.\nIn the removal of NL Guidance, we perform the experiment under the setting without NL in training data but use the NL examples and NL guidance in the testing data. The accuracy rate dropped significantly, and the fine-tuned model does not outperform the untrained model. This indicates that merely more exposure to Lean4 proof code does not improve the ability of LLM in formal reasoning. When we remove the NL-FL bootstrapping, the performance drops because the LLM often loses track of the proof and keeps on generating the same tactic. With bootstrapping, the performance is much better due to the NL guidance.\nThe results table shows that removing block training results in a performance drop, which we attribute to the distribution shift between training and testing data. Without block training, the training data lacks information about in-context examples, while the testing phase includes this in-context knowledge for the LLM to learn. Additionally, removing curriculum data sorting also leads to a performance decline. Curriculum data sorting provides a smoother training process by ensuring that lengthy and difficult examples do not appear early on and disrupt learning.\nDespite performance drop when removing the individual component, with other components, our method still outperforms un-finetuned Llama3 except for without the NL guidance. It supports the effectiveness of other components from another perspective."}, {"title": "3.5 Effectiveness of Example Retrieval", "content": "This section studies the effectiveness of our fine-tuned T5 for example retrieval using graphical methods. We encode all the formal theorem statements in Mathlib4 and the natural language theorem statement in the example list following Section 2.1.2. We then compare the cosine similarity of all possible combinations of the formal and natural language theorem statement set.\nFrom the two peaks in the histogram at cos similarities of 1.0 and 0.0 in Fig. 2, we can see that our example selection model can distinguish similar and different informal statements in the example list and formal statements in Mathlib4."}, {"title": "3.6 Case study", "content": "We analyze the behavior of TheoremLlama by examining the proofs generated by the theorem prover from MiniF2F-Test in Lean4. Since MiniF2F-Test is a dataset that does not have publicly available proof, it is highly unlikely it will be included in the training dataset of any LLM. For more case studies, kindly refer to Appendix B.\nNL Statement: Let a and b be two real numbers such that $a^2 + b^2 = 1$. Show that $ab + (a \u2013 b) \\leq 1$\nNL Proof: We have that $(a \u2013 b \u2212 1)\u00b2 \u2265 0$. By expanding, we have: $0 \u2264 a\u00b2 aba ab + b\u00b2 + b \u2212 a + b + 1$. Since $a\u00b2 + b\u00b2 = 1$, we get $0 < 2-2ab-2a+2b$. As a result, $ab+(a\u2212b) \u2264 1$.\nGenerated Proof:"}, {"title": "4 Related work", "content": ""}, {"title": "4.1 Formal Reasoning", "content": "Formal mathematical languages express mathematical statements in first-order logic. Such verification systems for mathematics are also known as Interactive Theorem Provers (ITPs). There are many ITP languages such as Isabelle (Paulson, 1994), Lean (De Moura et al., 2015; Moura and Ullrich, 2021), Coq (Coq, 1996), Metamath (Megill and Wheeler, 2019), and HOL Light (Harrison, 2009). The formal languages embed mathematical definitions and theorems onto a concrete logical foundation in their kernels. Following Polu et al. (2022), we work on Lean because Lean proves are typically more concise and relatively understudied.\nMany works focus on automatically completing formal reasoning using machine learning-based methods. Some use more traditional methods like K Nearest Neighbor (KNN) (Gauthier et al., 2021) or Graph Neural Network (GNN) (Yang and Deng, 2019). Others take advantage of the recent development of deep transformer-based methods that treat theorems as plain texts. Among them, Expert Iteration (Polu et al., 2022) and ReProver (Yang et al., 2024) focus on training existing LLMs to generate tactics and perform tree-search to complete the proofs. Other methods focus on exploring the few-shot capability, allowing LLMs to directly generate the whole proof based on the guidance of natural language (Wu et al., 2022; Jiang et al., 2022a). However, due to the lack of a dataset, there are currently no universally recognized methods for training LLMs to generate whole proof directly."}, {"title": "4.2 Dataset Generation", "content": "Modern machine learning methods typically require massive datasets to learn an application. However, it is impractical to have high-quality data for every corner case, prompting researchers to explore dataset generation. By combining existing incomplete data with the rich knowledge in LLMs, dataset generation can leverage this knowledge to produce a complete dataset suitable for model training. Initial attempts have achieved this through fine-tuned generative models (Anaby-Tavor et al., 2020; Chen et al., 2020). Other researchers explore zero-shot or few-shot performance for modern LLMs by directly querying the LLMs to obtain the intended dataset (Meng et al., 2022; Ye et al., 2022; Gao et al., 2022; Wang et al., 2023; Gao et al., 2023). In this work, we take advantage of these ideas for dataset generation to obtain the OBT dataset."}, {"title": "5 Conclusion", "content": "This paper proposes TheoremLlama, an end-to-end framework for transforming a general-purpose LLM into a Lean4 expert, along with the Open Bootstrapped Theorems (OBT) dataset, a NL-FL aligned, bootstrapped dataset for training an LLM Lean4 prover. This work largely addresses the significant data scarcity problem by introducing the NL-FL Aligned Dataset Generation method, which is used to create the OBT dataset. Subsequently, we demonstrate block training and curriculum data sorting techniques to enhance LLM training. Furthermore, we present the Iterative Proof Writing tactic to better utilize the LLM's capability in theorem proving. The major innovation of this work is the NL-FL bootstrapping method, which enables the LLM to better transfer its natural language reasoning ability to Lean4 proof writing through generated data. We also conduct comprehensive experiments to evaluate the effectiveness of TheoremLlama, where our framework successfully proves 36.48% and 33.61% of the theorems in MiniF2F-Valid and Test, respectively, surpassing the GPT-4 and Gemini-1.5 baselines. We will open-source all the datasets to facilitate further development in the field."}, {"title": "6 Discussion", "content": "Although large-scale pre-train provides LLMs with strong abilities in most general tasks, there are many corner cases that lack data for any machine learning methods to be effective. Formal reasoning is one of the most significant examples. From a border perspective, TheoremLlama sheds light on a general framework to further apply modern LLMs to such corner cases. It contains methods to leverage existing incomplete data, techniques to better train LLMs for unfamiliar tasks, and strategies to enhance LLM's performance in application. Thus, the contribution of this paper is not limited to the field of formal reasoning but gives a general framework for the further usage of LLMs in corner cases."}, {"title": "Limitations", "content": "Despite the promising results of TheoremLlama, there are still some limitations in our work that can be addressed in future research. First, even with natural language proofs to guide Lean4 proof writing, all existing formal provers, including TheoremLlama, struggle with solving difficult IMO-level problems. We conclude that LLMs currently lack the ability to understand the intricate technical aspects of human proofs. Integrating the \"kindles\" in human-written proofs into LLMs is an overlooked area in current research. Secondly, due to the complexity of the Lean4 kernel, this paper does not explore the potential of RL methods for enabling LLMs to write formal proofs through online interaction with Lean, nor does it incorporate feedback from Lean to refine incorrect proofs. This requires deeper integration of Lean and Python. Thirdly, although formal language provides a concrete foundation for verifying the correctness of mathematical proofs, there are potential risks that a natural language-guided Lean4 prover may automatically correct some errors in natural language. This could lead to errors in natural language being considered correct and cause wrong natural language proofs to be subsequently propagated within the mathematical community."}, {"title": "A Confusing Lean3 Data in LLM Pre-train", "content": "While studying how to directly generate Lean4 formal proofs using LLMs, we found that most LLMs have a serious problem with hallucination. The most significant issue is that, even with clear prompt instructions, the LLMs tend to write Lean3 proofs that are incompatible with Lean4.\nThe results for GPT-4 are shown in Tab. 3 and 4. From these tables, we can observe that even with clear instructions to use Lean4 for writing proofs, the LLM still uses Lean3 syntax for all imports and proofs. The imports are from Lean3 repositories or sometimes do not exist, and the proof segments, indicated by \"begin\" and \"end,\" are also from Lean3. This issue also occurs with Llama-3-8B-Instruct and Gemini-1.5-Pro, but less frequently. We attribute this behavior to the massive amount of Lean3 data used in the pre-training of these LLMs. This causes LLMs to fail in fully utilizing their knowledge in formal reasoning, as many generated formal proofs are incorrect in format.\nAlternatively, TheoremLlama uses extensive Lean4 data during instruction fine-tuning to significantly reduce this problem, as detailed in Section 3.6 and Appendix B."}, {"title": "B Case Study", "content": "This section provides additional case studies to further evaluate the performance of TheoremLlama in proving Lean4 theorems with NL guidance. Here, we select most examples from MiniF2F-Valid to avoid revealing too much proof information about MiniF2F-Test and contaminating the dataset. We present the examples in Tab. 5, 6, 7, 8, and 9.\nFrom case 1 in Tab. 5, we can see that the LLMs learn how to perform complex reductions stated in Lean4 code based on the natural language. The \"calc\" section demonstrates the LLM's ability to correctly reduce algebraic formulas based on conditions that are not explicitly stated in the natural language proof.\nCase 2 in Tab. 6 demonstrates that under the TheoremLlama framework, the LLM learns how to add sub-goals for proving in the correct Lean4 form from natural language proof. This is not observed in any of the correct proofs in the untrained model.\nFrom cases 3, 4, and 5 in Tab. 7, 8, and 9, we can see the ability of our LLM to perform step-by-step reasoning in both natural language and formal language in relatively complex proofs, demonstrating the effectiveness of NL-FL bootstrapping."}, {"title": "C Different LLM's behavior in Deformalization", "content": "This section details why we use Gemini-1.5-Pro as the LLM for deformalization and NL-FL cootstrapping rather than the commonly used OpenAI GPT family models through examples. We demonstrate an example of deformalization in Table 10. From the table, we can see that the GPT-4-generated proof is more like an explanation of the Lean4 code, while the Gemini-1.5-Pro-generated proof resembles the wording commonly used in mathematics. This demonstrates that Gemini-1.5-Pro has a better ability to understand Lean4 code for deformalization. This is also indirectly supported by Gemini's superior performance in writing Lean4 proofs for the MiniF2F dataset, as shown in Table 1."}, {"title": "DOBT dataset record", "content": "The data record in OBT contains the following components:\n1.  Name: the name of the theorem, following the name of dataset extracted in Yang et al. (2024)\n2.  Statement: Lean4 statement of the theorem\n3.  Proof: Lean4 theorem statement together with the proof, directly extracted from Mathlib4\n4.  File_path: The git repository for the given data record (for OBT dataset, it should be Mathlib4)\n5.  Commit: The exact commit number of the theorem\n6.  Generated_informal_statement_and_proof: The generated theorem informal theorem statement and proof\n7.  Commented_proof: The NL-FL bootstrapped Lean4 code."}, {"title": "E License", "content": "Our dataset will be distributed under the CC BY 2.0 license, code will be distributed under the MIT license. The dataset extracted from LeanDojo (Yang et al., 2024) is under CC BY 2.0. The original Mathlib4 and Lean is under Apache 2.0 license."}]}