{"title": "A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion", "authors": ["Lixia Zhang", "Tianxu Liu", "Kaihui Shen", "Cheng Chen"], "abstract": "With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness.\nThis study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware.\nIn terms of model construction, a CNN-BILSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability.\nThe research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of Internet technology, malware poses an increasingly severe threat to global cybersecurity [2]. Malware not only compromises individual users' privacy but also endangers business continuity, secure transactions in financial institutions, and the operation of critical national infrastructures [1]. The diversity, complexity, and stealth of malware make its detection and defense one of the most challenging tasks in the field of cybersecurity. In recent years, the number of malware instances has grown exponentially, with attack methods constantly evolving. Particularly, malware variants and polymorphism techniques have significantly increased detection difficulty [4].\nCurrently, malware detection techniques are broadly divided into two categories: static analysis and dynamic analysis. Static analysis involves parsing the binary files, instruction sequences, or assembly code of malware, relying on feature extraction and matching for classification [8]. Its advantages include high detection speed and the ability to analyze code without execution. However, static analysis struggles against code obfuscation, packing techniques, and malware variants, making it susceptible to evasion [9]. Dynamic analysis, on the other hand, executes malware in a virtual environment, recording its behavior for analysis [10]. Although dynamic analysis effectively handles obfuscated code and identifies malware behavior, its high computational cost and time requirements limit its scalability [14]. Additionally, dynamic analysis faces the challenge of avoiding detection by malware, which may prevent the method from functioning effectively in certain cases [17].\nIn response to these challenges, machine learning and deep learning techniques have recently been applied to malware detection, aiming to enhance detection accuracy and robustness through automated feature extraction and model training [18]. Compared to traditional methods, deep learning can handle more complex feature spaces and is well-suited for modeling large-scale, multi-dimensional data [20]. However, existing deep learning-based malware detection methods still suffer from limitations in feature extraction, as most rely on either static or dynamic features, making it difficult to fully capture the diversity of malware behavior [22]. As a result, combining multiple feature sources and effectively integrating them has become a key focus in current research [24].\nTo address these challenges, we present a deep learning-based static malware detection method that fuses image texture features with opcode sequence features using a CNN and a Bidirectional Long Short-Term Memory (BiLSTM) architec-"}, {"title": "II. RELATED WORK", "content": "Malware detection techniques have evolved from traditional signature-based methods to sophisticated machine learning and deep learning approaches. This section reviews key developments in static analysis, dynamic analysis, deep learning-based detection, and hybrid methods that enhance detection accuracy."}, {"title": "A. Traditional Malware Detection Techniques", "content": "Static analysis is a common malware detection method that inspects binary code without execution. Traditional techniques, like signature-based detection, compare known malware signatures or hash values against a database [29]. While effective against known threats, this method struggles to detect unknown or modified malware, especially those using code obfuscation, packing, or encryption to disguise their behavior [30].To address these limitations, researchers have proposed advanced static analysis techniques, including control flow, opcode sequence, and API call analysis, to capture higher-level structural and semantic features of malware.\nOpcode sequence analysis focuses on specific instruction sequences in the binary code, enabling the detection of behaviorally similar malware variants with shared operational characteristics.Despite these improvements, static analysis still struggles to detect polymorphic and metamorphic malware, which change their structure dynamically to evade detection [33].\nDynamic analysis executes malware in a controlled environment (e.g., a sandbox) to monitor its real-time behavior. By observing system calls, file manipulations, and network communications, it detects previously unseen malware based on its actions rather than static features. This makes dynamic analysis particularly effective against obfuscated malware. Tools like Cuckoo Sandbox have become popular for automating dynamic malware analysis [36].\nDynamic analysis faces challenges such as high computational costs and time demands for executing malware in a virtual environment. Modern malware often employs anti-analysis techniques like virtual machine detection or execution delays, effectively evading detection. Thus, dynamic analysis alone is often inadequate for large-scale malware detection [37]."}, {"title": "B. Machine Learning and Deep Learning in Malware Detection", "content": "In recent years, machine learning (ML) and deep learning (DL) have become powerful tools for malware detection, enabling automatic extraction of complex patterns from data without the need for manually engineered features. Traditional machine learning models, such as Support Vector Machines (SVMs), Decision Trees, and Random Forests, are widely used in malware detection tasks [38]. These models rely on manually selected features, such as opcode frequencies, byte sequences, and system call traces, to train classifiers that distinguish between benign and malicious files [39].\nWhile these classical ML methods have achieved success, they require significant domain expertise to define effective features, and their performance is often limited by the quality and completeness of the feature set [63]. In contrast, deep learning models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) can automatically learn high-level features from raw data, enhancing their adaptability and scalability to large datasets [42].\nCNNs have been successfully applied to malware detection by converting malware binaries into grayscale images, with pixel values representing byte patterns in the binary file [43]. This allows CNNs to capture spatial patterns in malware,"}, {"title": "C. Hybrid Detection Methods and Feature Fusion", "content": "To address single feature limitations, recent research has explored hybrid detection methods, combining static and dynamic features for a more comprehensive view of malware [50].Some studies combine static features, like opcode sequences and byte n-grams, with dynamic features, such as system calls and network logs, improving detection accuracy and resilience against obfuscated malware [52].\nA related line of research focuses on multi-modal feature fusion, which seeks to combine diverse data sources into a unified model [54]. Image-based features from malware binaries can be fused with opcode sequence features to provide a comprehensive view of malware's structural and behavioral properties [55]. This approach effectively handles malware variants by reducing reliance on a single feature type, making the model more robust to evasion techniques.\nA key example is CNN-BiLSTM models, which combine CNNs' strength in extracting spatial features from malware images with BiLSTMs' ability to model opcode sequences [57]. These hybrid models excel in detecting malware, especially packed or metamorphic variants where traditional methods struggle. By fusing features from multiple modalities, they significantly enhance the robustness and generalization of malware detection systems."}, {"title": "III. METHODOLOGY", "content": "This section details the proposed deep learning-based static malware detection method, organized into three key components: feature extraction, feature fusion, and model construction. As shown in Figure 1, the feature extraction stage focuses on obtaining static features from malware, including grayscale texture features from binary files and opcode features through reverse engineering. These features offer comprehensive insights into the malware's global structure (image features) and local behavior (opcode sequences).\nIn the feature fusion stage, extracted texture and opcode features are combined to enhance feature representation. This integration enables the model to capture broader malware characteristics, addressing the limitations of traditional methods that rely on a single feature source.\nThe model construction stage develops a CNN-BiLSTM fusion model, combining convolutional neural networks (CNN) for global image pattern recognition and bidirectional long short-term memory networks (BiLSTM) for sequential opcode data dependencies. This integration effectively processes both global and local information, improving detection accuracy and robustness. Comparative experiments demonstrate the proposed method's effectiveness in detecting various malware types, including polymorphic and obfuscated samples."}, {"title": "A. Feature Extraction", "content": "Feature extraction is key for static malware detection and greatly impacts model performance. This paper combines grayscale image texture features with opcode sequence features to capture both global and local characteristics. The process includes data acquisition, preprocessing, grayscale image generation, and texture and opcode feature extraction."}, {"title": "1) Data Acquisition and Preprocessing:", "content": "The quality of the dataset directly impacts feature extraction effectiveness. In this paper, we utilize the malware dataset from the VirusShare platform, which includes samples uploaded from September 2023 to February 2024, primarily consisting of executable files (EXE) for the Windows platform. Since the original dataset contains various file types, such as PDFs and DLLs, we first filtered it to retain only Portable Executable (PE) format files. The data filtering process involved using the VirusShare API to obtain the MD5 hash list of the malware samples, followed by recognizing each file's format based on its MD5 hash and selecting only PE format files. Finally, we deduplicated the selected samples to ensure dataset uniqueness and accuracy.After filtering and labeling, we obtained a dataset containing 14 malware families with a total of 12,021 samples"}, {"title": "2) Grayscale Image Generation:", "content": "A grayscale image is represented using only the grayscale channel, consisting of three colors: black, white, and shades of gray (ranging from 0 to 255). Using grayscale images effectively reduces data complexity, shortens processing time, and mitigates distortion issues associated with more complex images. After converting malicious code into grayscale images, malware from the same family exhibits similar texture features, while malware from different families shows distinct differences.\nDuring the conversion to a grayscale image, the binary file of the malicious code is processed byte by byte, with each 8-bit segment mapped directly to a pixel. This method creates a two-dimensional grayscale image where each byte value (0-255) corresponds to a pixel's grayscale value.\nBefore generating the grayscale image, the opcode sequence is extracted from the executable file using the decompilation tool IDA Pro, which produces assembly (asm) files. Opcodes are extracted through batch processing with Python scripts, identifying the \".text\u201d segment to locate and extract the opcode sequences. The Minhash algorithm is then used to generate hash signatures.\nTo convert these hash signatures into grayscale images hash values (typically exceeding 256x256) are processed by dividing by 256 and applying the modulus operation. This maps the hash values to x and y coordinates on a two-dimensional plane, with the grayscale value (z) corresponding to the modulus result. The image size is set to 128x128, balancing information retention and computational efficiency. Each malicious code hash signature is thus transformed into a grayscale image, with each pixel representing a grayscale value derived from the hash signature."}, {"title": "3) Texture Feature Extraction:", "content": "The GIST algorithm is used to extract global texture features from an image by applying convolution with multi-scale and multi-directional Gabor filters to capture its spatial layout and global patterns. The grayscale image is divided into regular sub-blocks, with each sub-block representing a local area. Convolution operations are performed on these sub-blocks using various Gabor filters of different scales and orientations to extract global features. The feature vectors from each sub-block are then concatenated to form the GIST features for the entire grayscale image, capturing the global patterns of the malicious code.\nGabor filters effectively extract edge, texture, and spatial frequency information by operating in both spatial and frequency domains, reflecting the image's overall layout. Specifically, the GIST algorithm's workflow involves filtering the grayscale image with Gabor filters, dividing the filtered image into sub-blocks, and calculating the mean or other statistical metrics for each sub-block to obtain its feature vector. The mathematical representation is as follows:\n$G_{ij} = \\sum_{k=1}^{K} f_k(I_{ij})$\nIn this context, $G$ represents the GIST feature of the image at position (i, j), $f_k$ denotes the k-th Gabor filter, and $I$ is the pixel value of the image. Through this approach, the GIST features can capture the global spatial layout information of the grayscale image, thereby reflecting the global characteristics of the malicious code.\nNext, the LBP (Local Binary Patterns) algorithm is used to extract local texture features from an image. LBP is a local texture descriptor that compares grayscale values of each pixel with its neighboring pixels. The process involves selecting P neighboring pixels around each pixel and comparing their grayscale values to that of the center pixel. If a neighboring pixel's value is greater than or equal to the center pixel's value, it is assigned a value of 1; otherwise, it is assigned 0. These binary values combine to form a binary number representing the LBP value for that pixel. After traversing the entire image, the LBP values create the local feature description. LBP is rotationally and grayscale invariant, making it highly effective for describing local texture features, particularly for detecting edges and corners. The computational procedure of the LBP algorithm can be summarized as follows:\n$LBP(x_c, y_c) = \\sum_{p=0}^{P-1} S(g_p - g_c) \\cdot 2^P$\nAmong other things, the $g_p$ is the gray value of the neighborhood pixel. $g_c$ is the gray value of the center pixel and the function s(x) is defined as:\n$s(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\nLBP extracts local texture features by analyzing the relationship between each pixel and its surrounding pixels, making it effective for capturing edges and local details in an image. The local features derived from LBP reflect variations in malicious code at a detailed level, aiding in the differentiation of various strains and families of malware.\nAfter extracting GIST and LBP features, the paper integrates these two types to form the final texture representation. GIST provides global spatial layout information, while LBP captures local texture details. This combination enables a comprehensive characterization of malicious code from multiple perspectives. By integrating global and local features, the distinctiveness of the texture characteristics is enhanced, offering rich feature data for subsequent classification and detection of malicious code."}, {"title": "4) Opcode Sequence Extraction:", "content": "To accurately represent malware's behavioral characteristics through static analysis, this paper employs the IDA Pro disassembly tool to convert binary files into assembly code sequences. By analyzing the Control Flow Graph (CFG), we obtain function call relationships, which facilitate the extraction of opcode sequences. The opcode extraction process begins at the root node of the CFG, recursively traversing each sub-node to collect opcodes in the order of function execution. For example, traversing the CFG yields the opcode sequence: push, mov, call, xor, push, push, These opcodes represent the program's instructions, reflecting its behavioral logic.\nDirectly utilizing the extracted opcode sequences for malware classification presents certain limitations, as opcodes are high-dimensional and unstructured data that cannot be directly input into machine learning models. Thus, this paper further processes the opcode sequences for feature representation, primarily employing N-gram and tf-idf algorithms. The combination of these two algorithms enables the capture of both local sequential patterns and the global significance of opcodes from different dimensions, thereby providing effective input features for subsequent modeling.\nThe N-gram algorithm is a commonly used method for sequence feature extraction, capable of preserving the contextual relationships within the opcode sequence by dividing it into smaller subsequences to extract local features. In this paper, the value of N is set to 3, i.e., a 3-gram model is used to process the opcode sequence. Assuming the opcode sequence is call, call, push, call, add, mov, xor}, the N-gram algorithm generates the following subsequence set: {(call, call, push),\n(call, push, call), (push, call, add), (call, add, mov), (add, mov,xor)}.\nThese subsequences capture the local dependencies between adjacent opcodes, revealing short-range patterns in the execution logic of the malware.\nTo convert these N-gram subsequences into vectors that can be processed by machine learning models, each subsequence is binarized: if the subsequence exists in a specific malware sample, the corresponding position in the feature vector is marked as 1; otherwise, it is marked as 0. The feature vectors generated by this method not only express the contextual dependency information within the opcode sequence but also ensure the sparsity and processability of the features."}, {"title": "B. Feature Fusion", "content": "In the feature fusion stage, we combine extracted image texture features with opcode features. First, we load the feature vectors from the GIST and LBP algorithms, along with those from the N-gram and tf-idf methods. We perform dimensionality checks to ensure each feature set contains the same number of samples. Using the pandas library in Python, we concatenate the texture and opcode feature vectors with the merge() function. Specifically, we merge GIST and LBP features to create a combined texture representation, followed by merging N-gram features with tf-idf features for a comprehensive opcode representation.\nThis process creates a fused feature vector that captures both global and local characteristics, along with contextual opcode information. We then merge this vector with its corresponding labels to form a complete dataset. To improve training effectiveness, we standardize the fused feature vector using z-score normalization, which reduces the impact of varying feature dimensions and aids model convergence.\nThe final labeled fused feature vector is used as input for the deep learning model during training and evaluation. This multi-feature fusion approach enhances the accuracy and robustness of static malware detection. Experimental results demonstrate that the classification accuracy of the fused features significantly surpasses that of single-feature inputs, validating the effectiveness of the fusion strategy."}, {"title": "C. Model Design", "content": "In the preceding section, fused features were obtained by integrating two types of features for input. This subsection employs the integrated deep learning model CNN-BiLSTM for static malware identification. We combine Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (BiLSTM) networks. Texture features are extracted from grayscale representations of the malware, leveraging CNNs' ability to process image features through convolutional kernels that extract local features while preserving spatial information and exhibiting translational invariance. Given the inherent sequential dependencies in opcode sequences and the fused features, we utilize the BiLSTM model, which captures both forward and backward contexts to effectively extract deep-level long-range dependencies and latent relational features in malware.\nThis section adopts the CNN-BiLSTM fusion model, using CNN to enhance the spatial understanding of local features extracted from images and BiLSTM to capture contextual information within opcode sequences. Finally, the fused features are connected through a fully connected layer and classified using the softmax activation function for output."}, {"title": "1) CNN Layer:", "content": "The CNN is a feedforward deep learning model. In the convolutional layer, the fused feature matrix of input texture features and opcode features is divided into multiple submatrices for convolution operations. As shown in (4), Z represents the result after convolution, W denotes the weights, and b is the bias.\n$Z = f(MW + b)$\nAs shown in (5), the activation function Relu is used for nonlinear mapping.\n$f = relu = max(0, x)$\nAfter the previous step, the processed results are passed to the pooling layer for filtering. By reducing the number of model parameters, this step decreases computational complexity and also helps prevent overfitting. In the CNN layer, through convolution and pooling operations, the deep features of malicious code are gradually enhanced and preserved. The strengthened features are then fed into the BiLSTM model for the next phase of learning and training."}, {"title": "2) BiLSTM Layer:", "content": "After passing through the previous CNN layer, the enhanced features of the malicious code are obtained. These enhanced features serve as input to the BiLSTM model. The input gate is defined by (6).\n$i_t = \\sigma (W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}C_{t-1} + b_i)$\nThe forget gate receives the output from the CNN layer and determines whether to forget the information.\n$f_t = \\sigma (W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}C_{t-1} + b_f)$\nAt this point, the cell state receives inputs from both the input gate and the forget gate.\n$C_t = f_c \\cdot C_{t-1} + i_t \\cdot tanh (W_{xc}x_t + W_{hc}h_{t-1} + b_c)$\nThe output gate controls the output of the cell state.\n$O_t = \\sigma (W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_{t-1} + b_o)$\nThe output of the current cell is given by (9).\n$h_t = O_t \\cdot tan c_t$\nHere, $f_t$ represents the forget gate, $i_t$ denotes the input gate, and $o_t$ stands for the output gate. $W_f, W_i, W_c, W_o, b_f, b_i, b_c$, and $b_o$ are the weights and biases. $c_t$ is the cell state, and $h_t$ is the hidden output. For the sentence x = [x1,x2,..., In], the forward LSTM generates a hidden state sequence hs \u2208 Rnxdh from the feature vector output by the CNN, while the backward LSTM also generates a hidden state sequence he \u2208 Rnxdh from the same CNN output. The final output hidden state sequence hs \u2208 Rn\u00d72dn is composed of the sequences from both the forward and backward outputs.\n$\\overrightarrow{h_s} = LSTM([v_1; v_2; ... ; v_n])$\n$\\overleftarrow{h_s} = LSTM([v_1; v_2; ... ; v_n])$\n$h_s = [\\overrightarrow{h_s},\\overleftarrow{h_s})$"}, {"title": "B. Feature Fusion", "content": "The Adam algorithm is characterized by its ease of implementation, efficiency, and low memory usage. Therefore, the Adam optimizer is used to optimize the gradient descent. Based on the fused features of texture features and opcode features, along with the CNN-BiLSTM model,"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The previous section provided a detailed explanation of how to conduct static malware detection. Initially, the MinHash algorithm is utilized to generate grayscale images, followed by the extraction of global and local features using the GIST and LBP algorithms to create a hybrid feature set. Subsequently, opcode features are extracted using the N-gram and tf-idf algorithms. The texture features and opcode features are then fused together, and the combined features are input into the CNN-BILSTM model for training.\nBy following these steps, a static malware detection model can be successfully constructed. However, to enhance the model's adaptability to the fused features, parameter tuning is necessary. Regarding the value of n in the N-gram algorithm, it is set within the range of 1 to 5, and the optimal n value is determined through experimental testing."}, {"title": "V. CONCLUSION", "content": "We present a novel approach to malicious code detection using feature fusion and a CNN-BiLSTM deep learning model. By integrating texture features (GIST and LBP) with opcode features (N-gram and tf-idf), we address the limitations of single-feature models, improving classification accuracy by 2.1%-4.6%. The CNN-BiLSTM model captures both spatial and sequential relationships, achieving an accuracy of 98.7%, which is 2.3%-5.2% higher than traditional models like SVM and LSTM, and outperforms state-of-the-art models such as ResNet and GoogleNet in detecting malicious code variants.\nFuture research should focus on enhancing model efficiency and reducing training time through techniques like transfer learning and model pruning. Incorporating diverse feature sets, including behavioral data, will strengthen detection against sophisticated threats. Adapting the model for various platforms, especially mobile and IoT devices, is essential as attackers increasingly target these environments. Additionally, exploring explainable AI (XAI) will enhance the interpretability of detection results, fostering trust in automated systems.\nIn summary, combining feature fusion with deep learning provides an effective strategy for advancing malicious code detection, paving the way for future innovations to combat evolving cyber threats."}]}