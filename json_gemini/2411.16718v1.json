{"title": "Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification", "authors": ["S. P. Sharan", "Minkyu Choi", "Sahil Shah", "Harsh Goel", "Mohammad Omama", "Sandeep Chinchali"], "abstract": "Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen, and CogVideoX are pushing the boundaries of synthetic video generation, with adoption seen in fields like robotics, autonomous driving, and entertainment. As these models become prevalent, various metrics and benchmarks have emerged to evaluate the quality of the generated videos. However, these metrics emphasize visual quality and smoothness, neglecting temporal fidelity and text-to-video alignment, which are crucial for safety-critical applications. To address this gap, we introduce NeuS-V, a novel synthetic video evaluation metric that rigorously assesses text-to-video alignment using neuro-symbolic formal verification techniques. Our approach first converts the prompt into a formally defined Temporal Logic (TL) specification and translates the generated video into an automaton representation. Then, it evaluates the text-to-video alignment by formally checking the video automaton against the TL specification. Furthermore, we present a dataset of temporally extended prompts to evaluate state-of-the-art video generation models against our benchmark. We find that NeuS-V demonstrates a higher correlation by over 5\u00d7 with human evaluations when compared to existing metrics. Our evaluation further reveals that current video generation models perform poorly on these temporally complex prompts, highlighting the need for future work in improving text-to-video generation capabilities.", "sections": [{"title": "1. Introduction", "content": "Imagine that we need to stress test or retrain a motion planning system for autonomous driving (AD) by simulating a scenario where \"a truck appears in frame 10 and veers in front of me onto my lane within 2 seconds\". With recent developments in text-to-video (T2V) generative models, generating synthetic videos for such scenarios offers great potential for improving such engineering systems. However, for these synthetic videos to be effective, they must align with the desired prompt across three key dimensions: 0 semantics, spatial relationships, and temporal coherence. For example, if the synthetic video either inaccurately generates the truck's movement in front of the autonomous vehicle or fails to follow the correct lane-change timing, it could mislead the motion planning system during retraining, resulting in incorrect decisions in critical situations.\nVarious evaluation metrics have emerged to identify well-aligned synthetic videos to the prompt from recently developed T2V models [4, 14, 20, 55]. While many video evaluation tools [23, 37, 39] focus on visual quality, some approaches use vision-language models [48, 49] or video captioning models [9] to evaluate the semantic alignment between the generated video and the original prompt. More notably, VBench [23] evaluates videos across multiple dimensions and categories and is recently being used in leaderboards* to evaluate T2V models. However, naive evaluation of a video generated by one neural network using another is neither rigorous nor interpretable. Moreover, current evaluation metrics still miss critical spatio-temporal aspects that are intrinsic to video data (See Fig. 1).\nOur key insight is to leverage the recent advancements in video understanding systems that utilize structured symbolic verification methods with neural networks [10]. We introduce Neuro-Symbolic Verification (NeuS-V), a novel T2V model evaluation method that integrates neural network outputs with structured symbolic verification through temporal logic (TL) specifications. First, we leverage Vision-Language Models (VLMs) to interpret a video's spatial and semantic content. Second, we capture temporal relationships across these identified semantics by processing frames sequentially. Finally, we construct an automaton representation of the synthetic video which establishes a structured temporal sequence of the video events. This structure ensures that the specified TL requirements are met, resulting in a rigorous and interpretable video evaluation.\nFurthermore, we introduce a dataset of temporally extended prompts for benchmarking T2V models' abilities to faithfully exhibit temporal fidelity and accurately reflect event sequences. We plan to open-source NeuS-V, our prompt suite, and human evaluation results along with a public leaderboard of state-of-the-art (SOTA) T2V models."}, {"title": "2. Related Work", "content": "Evaluation of Text-to-Video Models. The recent surge of T2V models has created a demand for specialized metrics to evaluate the quality and coherence of generated videos. Some preliminary works in this field utilize metrics such as FID, FVD, and CLIPSIM [6, 21, 25, 36, 47, 53], but these approaches struggle with prompts that require complex reasoning and logic. Recent works [31, 32, 38, 56] rely on Large Language Models (LLMs) to break down prompts into Visual Question Answering (VQA) pairs, which are then scored by a VLM. Few approaches also utilize vision transformers and temporal networks [11, 16, 22, 27]. Some works, including EvalCrafter [37], FETV [39], T2V-Bench [16], and others [18, 26], incorporate an ensemble of various visual quality metrics. Of these, VBench [23] is rising to be the de-facto benchmark in the field, evaluating across numerous dimensions and categories. However, all current evaluation methods emphasize visual quality, disregarding temporal ordering of videos. Although some claim to perform temporal evaluation, they often focus on aspects such as playback (e.g. slow motion, timelapse) or movement speed rather than true temporal reasoning and also lack rigor. In contrast, NeuS-V addresses these shortcomings by formally verifying temporal fidelity through TL specification over the automaton representation of synthetic videos.\nVideo Event Understanding. Recent methods typically rely on perception models [15, 46] or VLMs [40, 54] to analyze events within a video. Although these models can detect objects and actions effectively, they do not guarantee a reliable understanding of the video's temporal dynamics. Yang et al. [51] focuses on representing a video as a formal model, specifically a Discrete Markov Chain, providing a more structured approach to capture events across time. An application of this is NSVS-TL [10], which uses a neuro-symbolic approach to find the scenes of interest. Along similar lines, NeuS-V leverages VLMs and formal verification to enhance comprehension of temporal relationships in generated videos and assess video quality, thus establishing a reliable foundation for video evaluation."}, {"title": "3. Preliminaries", "content": "In the following sections, we present a running example to help illustrate our approach. Let's say we generate an autonomous driving video using T2V models with the prompt\n- \"A car drives down a road on a clear sunny day, interacting\nwith cyclists who signal turns to avoid obstacles\".\nTemporal Logic. In short, TL is an expressive formal language with logical and temporal operators [13, 41]. A TL formula consists of three parts: a set of atomic propositions, first-order logic operators, and temporal operators. Atomic propositions are indivisible statements that can be True or False, and are composed to construct more complex expressions. The first-order logic operators include AND (\u2227), OR (\u2228), NOT (\u00ac), IMPLY (\u21d2), etc., and the temporal operators consist of ALWAYS (\u25a1), EVENTUALLY (\u25c7), NEXT (X), UNTIL (U), etc.\nThe atomic propositions P, and TL specifications \u03a6 of our example are:\nP = {car driving, clear day, cyclist signals turn, cyclist turns, cyclist avoids obstacle},\n\u03a6 = ((car driving \u2227 clear day) \u2227 (cyclist signals turn)\n\u21d2 \u25c7(cyclist turns \u2227 cyclist avoids obstacle)).\n(1)\nThis TL specification illustrates that if a car drives on a clear day and a cyclist signals to turn, it implies that eventually a cyclist will turn and avoid an obstacle.\nDiscrete-Time Markov Chain. Abbreviated DTMC, it is used to model stochastic processes where transitions between states occur at discrete time steps [29, 42]. We use a DTMC to model synthetic videos since the sequence of frames is discrete and finite, as shown in Fig. 2. The DTMC is defined as a tuple A = (Q, q\u03bf, \u03b4, \u03bb), where Q is a finite set of states, qo \u2208 Q is the initial state, \u03bb : Q \u2192 2[P] is the label function, P is the set of atomic propositions and \u03b4 : Q \u00d7 Q \u2192 [0, 1] is the transition function. For any two states q, q' \u2208 Q, the transition function (q, q') \u2208 [0,1] gives the probability of transitioning from q to q'. For each state, the probabilities of all outgoing transitions sum to 1.\nFormal Verification. This provides formal guarantees that a system meets a desired specification [12, 24]. It requires a formal representation of the system, such as a finite-state automaton (FSA) or a Markov Decision Process (MDP). From the DTMC, we define a path as a sequence of states from the initial state, e.g., q0q1q2\u03c9, where \u03c9 indicates infinite repetition. A trace is the sequence of state labels denoted as \u03bb(q0)\u03bb(q1)\u03bb(q2)\u2026\u2026 \u2208 (2[P])\u03c9 that captures events over time. Next, we apply probabilistic model checking [3] to compute the probability P[A |= \u03a6] which gives the satisfaction probability that the trace starting from the initial state satisfies the TL specification \u03a6. Through such formal representations, we evaluate synthetic videos."}, {"title": "4. Methodology", "content": "Given a synthetic video V from T2V model MT2V : T\u2192 V, where T is a text prompt, we aim to compute an evaluation score S: T\u00d7 V \u2192 R^m for each evaluation mode m \u2208 M = {object existence, spatial relationship, object action alignment, overall consistency}. We introduce NeuS-V, a novel method for evaluating T2V models using a neuro-symbolic formal verification framework. NeuS-V evaluates synthetic video across four different evaluation modes: object existence, spatial relationship, object and action alignment, and overall consistency following these steps:\n\u2022 Step 1: We translate the text prompt into a set of atomic propositions and a TL specification through Prompt Understanding via temporal Logic Specification (PULS).\n\u2022 Step 2: We obtain a semantic confidence score for each atomic proposition using a VLM applied to each sequence of frames.\n\u2022 Step 3: We construct a video automaton representation for the synthetic video.\n\u2022 Step 4: We compute the satisfaction probability by formally verifying the constructed video automaton against the TL specification.\n\u2022 Step 5: Finally, we calibrate the satisfaction probability by mapping it to the final evaluation score from NeuS-V.\n4.1. Prompt Understanding via Temporal Logic Specification\nWe propose Prompt Understanding via temporal Logic (PULS), a novel method that facilitates an efficient and accurate conversion of text prompts to TL specifications. It preserves all semantic details of the prompt to formulate its TL specification. In contrast to previous works that focus solely on formulating relationships between singular objects, isolated events [5, 8, 17], or encoding spatial information [35, 45], our method, PULS, fully integrates both temporal and spatial details which previous methods fall short. By incorporating four evaluation modes, we provide a more comprehensive assessment across spatio-temporal dimensions.\nEvaluation Mode. PULS compiles the text prompt into the different set of proposition P and TL specifications \u03a6 across four evaluation modes:\n\u2022 Object Existence: This mode evaluates the existence of objects specified in the text prompt. For instance, in our running example (Sec. 3), the TL specification for the object existence mode can be defined as \u03a6 = \u25c7(car \u2227 cyclist \u2227 obstacle).\n\u2022 Spatial Relationship: This evaluation mode captures the spatial relationships among existing objects. In our running example, since we are interested in a cyclist signaling and then making a turn, it implies that the cyclist is in front of the car. Hence, the TL specification is defined as \u03a6 = (cyclists are in front of a car) \u2227 \u25c7(obstacles are next to cyclists).\n\u2022 Object and Action Alignment: It ensures that the objects in the generated video perform the actions specified in the text prompt. For example, the TL specification can be defined as \u03a6 = \u25c7(cyclist signals turn \u21d2 cyclist turns U cyclist avoids obstacle).\n\u2022 Overall Consistency: Finally, we assess the overall semantic and temporal coherence of events in the generated video, ensuring alignment with the full-text prompt (see the example in Eq. (1)).\nFunctional Modules. PULS is a two-step algorithm powered by LLMs. Given the text prompt T that is used for video generation, it is translated into P and \u03a6 for any given evaluation mode m\u2208 M. This translation process is defined as L_{MPULS} : T \u00d7 M \u2192 (\u0420, \u0424).\nPULS consists of two modules, Text-to-Proposition (T2P) and Text-to-TL (T2TL). Thes use the optimized prompts 0T2p and 0T2TL respectively. Each optimized prompt includes carefully selected few-shot examples from a compiled training dataset DT2TL|train, that maximize the accuracy of having correct P and \u03a6 given m \u2208 \u041c.\n1. Text-to-Proposition Module: This module extracts P from T given M, using 0T2p, and is defined as:\nL_{MT2P}: T\u00d7M \u2192 2^P, P.\n(2)\nThe optimized prompt 0T2p is obtained from DT2TL|train, a set of N selected examples of text Ti and propositions Pi pairings:\nDT2P|train = {(Ti, Pi)}^N_{i=1}, DT2P|train C Dtrain.\n(3)\n2. Text-to-TL Specification Module: This module generates \u03a6 from T, P, and M using the optimized prompt 0T2TL, and is defined as:\nL_{MT2TL} : T \u00d7 P \u00d7 \u041c \u2192 2^{\u0424}, \u0424.\n(4)\nThe optimized prompt 0T2TL is obtained from DT2TL|train with N examples pairs of text Ti, propositions Pi, and corresponding TL specifications \u0424i:\nDT2TL|train = {(Ti, Pi, \u03a6i)}^N_{i=1}, DT2TL|train C Dtrain.\n(5)\nWe use MIPROv2 [44] with DSPy [30] to ensure that the N few-shot examples in the dataset Dtrain are sufficient in maximizing the accuracy of each module. We detail the optimization process in the Appendix.\n4.2. Semantic Score from Neural Perception Model\nGiven P, \u03a6 = L_{MPULS} (T), where T is the text prompt used for the synthetic video V, we first obtain the semantic confidence score C\u2208 [0, 1] for each atomic proposition pi \u2208 P using a VLM MVLM: p\u00d7F \u2192 C, where F represents a sequence of frames.\nWe use the VLM to interpret semantics [2, 34, 43, 54] and extract confidence scores from F based on the text"}, {"title": "4.3. Automaton Representation of Synthetic Video", "content": "Next, given C* across all frames and propositions in P, we construct the video automaton Av to represent the synthetic video as a DTMC A. This representation is crucial, as it enables the evaluation of synthetic video to determine whether it satisfies the given \u03a6 corresponding to the original text prompt. We build Av using an automaton generation function \u03be : P \u00d7 C* \u2192 Av, defined as\nAv = \u03be(P, C*) = A = (Q, q\u0ed0, \u03b4, \u03bb),\n(7)\nwhere Q = {qo... qt } is the set of possible states, each q \u2208Q represents a unique configuration of truth values for P with qo denoting the initial state. Given C* for all atomic propositions in each sequence of frames from the synthetic video, the transition function (q, q') is defined as\n\u03b4(q, q') = \u03a0^P_{i=1} (C^*_i)^{1{qi=1}} (1 \u2212 C^*_i)^{1{qi=0}},\n(8)\nwhere 1{qi=1} is an indicator function that takes the value 1 if pi is true in q', and 0 otherwise. Similarly, 1{q;=0} takes the value 1 if and only if pi is false in q'. This is obtained by a labeling function (q) that maps each state q to the boolean value of its proposition. Further details on the Av construction process are in the Appendix.\n4.4. Verifying Synthetic Video Formally\nGiven Av, we compute the satisfaction probability P[Av |= \u0424] by formally verifying Av against \u03a6. Specifically, we use the model checking method STORM [19, 28] that utilizes a probabilistic computation tree logic (PCTL) variant of temporal logic to calculate the satisfaction probability of Av with respect to \u03a6. This probability is defined as P[Av |= \u0424] = \u03a8(\u0391\u03bd, \u03a6), where \u03a8(\u00b7) is the probabilistic model checking function. This is performed by analyzing the probabilistic transitions and state labels within Av.\nFinal Score Calibration. Lastly, we calibrate the satisfaction probability to a NeuS-V score using a satisfaction probability mapping function based on its empirical cumulative distribution, defined as\nS = {$1,..., $m} = fECDF(P[Av |= \u0424], Dm) \u2200 m\u2208 M,\n(9)\nwhere Dm is the distribution of satisfaction probabilities of each evaluation mode from wide samples of synthetic videos, and S is the set of each evaluation mode score Sm. We take the average of these scores to comprehensively capture the variety of temporal specifications in the prompts\nSNeuS-V \u2190 \u03a3^M_{N=1} Sm\n(10)"}, {"title": "5. Experimental Setup", "content": "PULS uses GPT-40 and 01-preview for prompt to TL translation, whereas NeuS-V relies on InternVL2-8B [25] as its"}, {"title": "6. Results", "content": "Building on our experimental setup, we now turn to the empirical evaluation of NeuS-V. Our experiments are designed to address three central questions that motivate the necessity of our methodology.\n1. Does NeuS-V's focus on temporal fidelity translate to a higher correlation with human annotations compared to baselines that emphasize visual quality?\n2. To what extent does grounding in temporal logic and formal verification provide a more robust evaluation framework than approaches based solely on VLMs?\n3. Can the reliability of NeuS-V extend beyond our synthetic prompt suite to established larger-scale datasets?\n6.1. Formal Evaluation of Text-to-Video Models\nAs outlined in Section 5, we benchmark both closed-source and open-source state-of-the-art models: Gen-3 and Pika (closed-source), alongside T2V-Turbo-v2 [33] and CogVideoX-5B [52] (open-source)."}, {"title": "6.2. Ablation on Temporal Logic and Verification \u2013 How Important is Formal Language?", "content": "In this ablation study, we investigate the role of formal grounding in temporal logic and model checking by replacing these components with VLM-based alternatives that lack such rigor. Specifically, instead of using PULS to translate prompts into atomic propositions and specifications, we directly query an LLM to break down prompts into a set of yes/no questions covering the content and"}, {"title": "6.3. Demonstrating Robustness on a Real-World Video-Captioning Dataset", "content": "Our prompt evaluation suite, while effective, is modest in size with only 160 prompts. One might argue that this limited scale, along with the use of synthetic video captions, could raise doubts about the reliability of our metric. To address this concern, we use MSR-VTT [50], a well-established dataset for video captioning \u2013 a task which also involves assessing how well a video aligns with a given textual description. MSR-VTT features a large-scale set of video-caption pairs annotated by MTurk workers. We leverage their credibility and repurpose its validation set to evaluate the reliability of our metric. We create two splits: a positive set (where captions match videos) and a negative set (where captions do not match videos). We then test whether the NeuS-V metric can effectively distinguish between aligned and misaligned video-caption pairs."}, {"title": "6.4. Ablations on Architectural Choices", "content": "Visual Context to VLM. In this first ablation of architectural choices, we investigate whether using more frames necessarily improves the performance of NeuS-V. To measure this, we compare the use of a single frame versus three frames when prompting our VLM (InternVL2-8B). As shown in Table 3, the use of three frames consistently results in a higher correlation with text-to-video alignment annotations. We did not test more than three frames, as this would exceed the context window of the LLM.\nChoice of VLM. In the second ablation, we examine whether the choice of VLM affects automata construction. We test two recent VLMs: InternVL2-8B [43] and LLaMa3.2-11B-Vision with single-frame contexts. As seen in Table 4, InternVL2-8B consistently achieves higher correlation with text-to-video alignment annotations. LLaMa3.2 model tends to be overconfident in its outputs, despite attempts at calibration due to heavy skew in probabilities. This leads to a lower correlation with human labels."}, {"title": "7. Discussion", "content": "Is Rigorous Formalism Key? A natural question that arises is whether formal temporal logic can be replaced with simpler if-else style checks and assertions. In principle, yes. However, as prompt length and complexity increase, such rule-based approaches exhibit scaling challenges, making them impractical. In contrast, symbolic temporal operators offer a near-linear scaling to verify adherence to automata. As shown through Figure 6 in Section 6.2, even though such alternative approaches may show promise, they ultimately fall short of the robustness provided by formal temporal logic. NeuS-V is the first work that introduces formalism to T2V evaluation, going beyond quality-based metrics.\nRethinking Text-to-Video Models. Evaluations in Ta-"}, {"title": "8. Conclusion", "content": "Text-to-video models are increasingly being applied in safety-critical areas such as autonomous driving, education, and robotics, where maintaining temporal fidelity is essential. However, we find that most current evaluation methods prioritize visual quality over text-to-video alignment. To address this, we introduce NeuS-V, a novel metric that translates prompts into temporal logic specifications and represents videos as automata, thereby scoring them through formal verification. Alongside NeuS-V, we present a benchmark of temporally challenging prompts to assess the ability of state-of-the-art T2V models to follow specified event sequences. Our evaluation reveals limitations in these models, which frequently rely on simplistic camera movements rather than true temporal dynamics. We aim to pave the way for the development of future T2V models that achieve greater alignment with temporal requirements."}]}