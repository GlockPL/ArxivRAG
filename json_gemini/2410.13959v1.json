{"title": "FinQAPT: Empowering Financial Decisions with End-to-End\nLLM-driven Question Answering Pipeline", "authors": ["Kuldeep Singh", "Simerjot Kaur", "Charese Smiley"], "abstract": "Financial decision-making hinges on the analysis of relevant in-\nformation embedded in the enormous volume of documents in\nthe financial domain. To address this challenge, we developed Fin-\nQAPT, an end-to-end pipeline that streamlines the identification\nof relevant financial reports based on a query, extracts pertinent\ncontext, and leverages Large Language Models (LLMs) to perform\ndownstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module\nusing the FinQA dataset. We introduced a novel clustering-based\nnegative sampling technique to enhance context extraction and\na novel prompting method called Dynamic N-shot Prompting to\nboost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA,\nattaining an accuracy of 80.6%. However, at the pipeline level, we\nobserved decreased performance due to challenges in extracting\nrelevant context from financial reports. We conducted a detailed\nerror analysis of each module and the end-to-end pipeline, pinpoint-\ning specific challenges that must be addressed to develop a robust\nsolution for handling complex financial tasks.", "sections": [{"title": "1 Introduction", "content": "The financial domain is characterized by an enormous volume of\ndocuments, including various kinds of reports, news articles, reg-\nulatory filings, and research publications, containing a wealth of\ninformation critical for decision-making processes. For example,\nfinancial analysts frequently require rapid access to specific nu-\nmerical data, such as the percentage change in a company's stock\nprice following a major announcement. The ability to effectively\nextract context from these documents and manage downstream\ntasks, such as question answering (Q&A), reasoning, and search, is\ncritically important for finance professionals. However, the sheer\nvolume of data, coupled with specialized terminology, presents nu-\nmerous challenges. Consequently, there is a pressing need for an\nautomated end-to-end pipeline, that adeptly extracts and processes\ninformation from financial documents.\nRecently, the advent of Large Language Models (LLMs), such\nas GPT-3.5 1 and GPT-4 [17], has revolutionized the natural lan-\nguage processing (NLP) landscape, demonstrating exceptional per-\nformance in various applications, including reasoning, question\nanswering, summarization, etc., with minimal supervision ([4], [6]).\nTheir effectiveness is further enhanced by innovative prompting\ntechniques like Chain of Thought [30], Self-Ask [20], React [31],\nand Self-Consistency [29]. However, these models often generate\ninaccurate or hallucinated information. We try to address these\nchallenges in the financial domain by proposing an end-to-end\npipeline that effectively extracts relevant context and integrates it\nwith advanced prompting techniques to address downstream tasks\nin finance domain.\nIn particular, our proposed pipeline, FinQAPT, consists of three\ndistinct modules: (a) FinPrimary, a primary retrieval module that\ninterprets queries and identifies relevant financial reports as coarse-\ngrained contexts; (b) FinContext, a retrieval module that extracts\nfine-grained contexts from relevant reports; and finally, (c) Fin-\nReader, a reader module that executes downstream tasks, specifi-\ncally numerical reasoning task, utilizing LLMs. We addressed each\nmodule individually and developed innovative techniques to im-\nprove their individual performance on the FinQA dataset [7]. Our\nresults indicate that although we achieved robust results at module\nlevel, the performance of the end-to-end pipeline was less optimal.\nThe primary reason for this performance drop was the challenge\nof integrating relevant coarse-grained and fine-grained contexts\nfrom reports. This difficulty arises because financial reports have\ncontexts dispersed across different pages and formats, leading to\nambiguous financial contexts and hence inaccurate results. Our"}, {"title": "2 Related Work", "content": "Open-domain question answering has seen significant progress\nwith methods like DrQA [5], R3 [28], and OpenBookQA [10, 11, 14,\n16]. However, these methods focus on general knowledge sources\nand don't cater to the financial domain's unique challenges. In the\nfinancial domain, studies like DyRRen [13] and APOLLO [25] have\nmade strides but don't offer a complete end-to-end solution for\ncomplex financial inquiries. They often rely on pretrained encoder\nmodels, which require extensive training data and fine-tuning.\nThe advent of Large Language Models (LLMs), such as GPT-4\n[17] and PaLM2 [1], has significantly advanced NLP. However, they\noften produce hallucinated or contextually irrelevant information,\nlimiting their effectiveness in domains like finance where accuracy\nis crucial. To address these limitations, significant work has been\ndone on retrieval-augmented generation (RAG) ([2], [26], [21]) to\nprovide relevant in-context to LLMs, reducing hallucinations. Ad-\nditionally, several prompting techniques (Chain of Thought [30],\nSelf-Ask [20], React [31], and Self-Consistency [29]) have been\nproposed. Despite these advancements, there remains a need for\nfine-tuned retriever models for domain-specific and task-specific\ntasks as shown by [8] and [9], as generalized retrievers struggle\nwith specialized language, complex terms, and structure, such as in\nfinancial documents. Incorporating these techniques and models\ninto a comprehensive financial domain pipeline remains a chal-\nlenge.\nOur FinQAPT pipeline builds upon the existing literature by\ndeveloping an end-to-end solution specifically tailored to the fi-\nnancial domain. By integrating context extraction mechanisms and\nadvanced prompting techniques, FinQAPT aims to harness the\npower of LLMs while minimizing the generation of hallucinated\nor irrelevant information. This approach sets FinQAPT apart from\nexisting work and highlights its potential to address the unique\nchallenges of financial Q&A and numerical reasoning tasks."}, {"title": "3 Methodology", "content": "Fig 1 illustrates our pipeline, which consists of three key modules:\nFinPrimary, FinContext, and FinReader, working sequentially in a\nretrieval-augmented generation fashion. The FinPrimary module\ndecomposes the input query into simpler queries and identifies\nrelevant pages from the S&P 500 Earnings Reports [32] to provide\ncontext for following stages. The FinContext module refines the\ncontext by identifying pertinent text segments and tables from the\npages suggested by FinPrimary. Finally, the top five contexts re-\ntrieved, along with the original query, are passed to the FinReader\nmodule, which processes and generates the final answer. The sub-\nsequent subsections elaborate on the functionality of each module:"}, {"title": "3.1 FinPrimary Module", "content": "The FinPrimary module selects coarse-grained information required\nto answer a given query, where coarse-grained refers to retrieving\nthe relevant page from a document. Since queries can encompass\nmultiple aspects, this module incorporates a query decomposition\ncomponent. Consequently, the FinPrimary module operates in two\ndistinct stages: query decomposition and retrieval of relevant pages\nor sections within the document."}, {"title": "3.1.1 Query Decomposition", "content": "We leveraged the advancements in\nLLMs, which have shown remarkable performance in various NLP\ntasks, including query decomposition ([19], [18], [3]). Specifically,\nwe employed the 5-shot GPT-3.5 for decomposing our queries. An\nexample is provided below,\nOriginal Query: What is the percentage change in the\nfair value of the options for Apple from 2009 to\n2010 ?\nDecomposed Query:\n1. What is the fair value of options for Apple in\n2009 ?\n2. What is the fair value of options for Apple in\n2010 ?"}, {"title": "3.1.2 Page Retrieval", "content": "We extract the company name(s) and year(s)\nreferenced in each decomposed query to select the relevant reports\nfrom the S&P dataset. Given that these reports can extend over\nhundreds of pages, it is important to identify the relevant pages\nthat provide context for the specific query. Hence, for each indi-\nvidual query Q and its associated earnings report comprising N\npages, we employ widely-adopted sparse-retrievers like TF-IDF\n[24], and BM25 [23], to retrieve the most relevant pages within the\ncorresponding report."}, {"title": "3.2 FinContext Module", "content": "The goal of the FinContext module is to extract fine-grained infor-\nmation, such as multiple sentences or table rows, from the pages\nidentified by FinPrimary for each query. Given a query (Q) and\ncoarse-grained context (C), which includes both tables and text, our\nretriever extracts fine-grained context from C. To facilitate this pro-\ncess, we adopt a template akin to FinQA and DyRRen to transform\ntabular data into sentences. The template is similar to, the column\nname of row name is cell value;. This produces multiple sentences\nfor a row of the table which are then concatenated together.\nWe treat this task as a binary classification problem. For a given\nquery Q and a context $C_i$ from C, we concatenate them and pass\nit through a pre-trained encoder model. The output embedding\nfrom the [CLS] token is passed through a linear layer to compute"}, {"title": "3.3 FinReader Module", "content": "The FinReader module generates answers using the context ex-\ntracted by the FinContext module, leveraging the impressive rea-\nsoning capabilities of LLMs. Prompting, a key element in LLM tasks,\nsignificantly influences the model's text generation, specifically the\nquality, relevance, and specificity. In this study, we introduce a novel\nprompting technique, Dynamic N-Shot Prompting, designed to\nenhance LLMs' numerical reasoning capabilities. It merges Chain\nof Thought (CoT) ([30]) prompting and Self-Consistency (SC) ([29])\nwith our proposed technique using training data."}, {"title": "3.3.1 Dynamic Few-shot Prompt Generation", "content": "Our method departs\nfrom traditional approaches where N-shot prompts are manually\ndefined and remain static throughout the experiments. We use a\ndynamic approach that generates new N-shot prompts for each\nquestion in the test and dev datasets, with N being the number of\nexamples. This dynamic strategy outperforms static prompts by\nusing semantically similar questions from the training data to con-\nstruct prompts, ensuring more relevant and context-specific COT\nexamples. It helps LLMs understand calculations of financial term\nusing provided context and facilitates accurate computations. We\nleveraged the OpenAI-Ada-0022 embeddings, to identify semanti-\ncally similar questions from training set using cosine similarity."}, {"title": "3.3.2 Reasoning Chain Generation", "content": "CoT prompting along with SC\nhas proven effective in enhancing LLM performance, particularly\nin tasks that involve numerical reasoning or open-domain question\nanswering. To seamlessly integrate CoT and SC with Dynamic N-\nShot Prompt Generation, we generated reasoning chains for every\nquestion in the FinQA training set. This process was carried out\nusing a 5-shot reasoning chain generation approach with GPT 3.5.\nThe generation of reasoning chains was guided by program steps,\nalready available for each question in FinQA. For example :\nQuestion: What is the total operating expenses in\n2018 in millions?\nProgram steps: divide (9896, 23.6%) (provided in\ndataset)\nThe 5-shot reasoning chain generated using GPT-3.5 is:\n1. The aircraft fuel expense in 2018 was $9896\nmillion.\n2. The percentage of total operating expenses\nattributed to aircraft fuel expense in 2018\nwas 23.6%.\n3. To find the total operating expenses in 2018,\nwe divide the aircraft fuel expense by the\npercentage of total operating expenses: $9896\n/ 23.6% = $41932.20339 million.\n4. So the answer is 41932.20339.\nThis method allowed us to dynamically generate N-Shot Chain\nof Thought prompts for each question in the test and dev sets.\nThroughout the paper, any mention of \"Dynamic N-Shot\" refers to\nthe combination of Dynamic Few-Shot with the Chain of Thought\nprompting method."}, {"title": "4 Experiments", "content": "To test the individual modules and end-to-end pipeline, we con-\nducted experiments using two datasets: the S&P 500 companies'\nearnings reports [32] and the FinQA dataset [7]. The FinQA dataset\nprovides a sizable collection of carefully curated financial numer-\nical reasoning Q&A and includes context information, indicating\nthe specific report, i.e. the company ticker and year, and the page\nfrom which a context was taken in the S&P 500 earnings reports. It\nmaybe noted that we leverage this metadata along with a dataset\nof S&P 500 reports to assess the performance of primary retrieval\ncomponent of our pipeline."}, {"title": "4.1.1 S&P 500 Earnings Reports", "content": "We downloaded the earnings reports of S&P 500 companies 3\nspanning the years 1999 to 2019 used in the FinQA dataset. On an\naverage an earnings report in the dataset consists of 300-500 pages,\nsome reports even extend to even 700 pages. We extracted the text\ncontent from each page using pdfminer 4 and the table extraction\nwas performed using Camelot 5."}, {"title": "4.1.2 FinQA", "content": "FinQA dataset consists of a collection of 8,281 meticulously\ncurated financial questions, annotated by financial experts. The\ndataset provided is split into training (70%), dev (15%) and test (15%)\nset. As mentioned above, FinQA offers metadata including the com-\npany ticker and year of S&P 500 report from which the context was\nextracted. However, some queries do not explicitly mention the\ncompany name and report year but implicitly use this information.\nTo address this issue, we use the metadata to append the company\nname and year to the query wherever this information is missing."}, {"title": "4.2 Evaluation", "content": "To evaluate our retrieval modules, FinPri-\nmary and FinContext, we employ Recall Top-N (Recall@N), which\nmeasures the percentage of correct positive predictions. In scenar-\nios with multiple positive predictions per sample, we consider the\nfirst N predictions as positive. For FinReader module, we adopt a\nmethodology akin to PoT (Program for Thoughts). However, instead\nof solely employing math.isclose with a relative tolerance of 0.001\nfor answer comparison, we implement the following techniques to\nenhance the extraction and processing of generated outputs:\n(1) Despite prompting the LLMs to produce JSON schema, the\ngenerated answers often necessitated additional text pro-\ncessing. Hence, we use a regex pattern to isolate the answer\nstring from the LLM's generated output.\n(2) If the extracted answer string contains insufficient context,\nno further processing is performed.\n(3) Ratios and percentages are converted to a standardized for-\nmat. For instance, 90% in the answer string or gold answers is\nconverted to 0.90. Ratio formats, such as 3/4, are transformed\nto 0.75.\n(4) After normalizing the extracted answer string, it is directly\ncompared with both the answer and exe_answer provided in\nthe dataset. Correct matches are assigned a value of 1, while\nincorrect matches undergo an additional step.\n(5) The normalized extracted answer string is compared with the\nexe_answer in the dataset using math.isclose with a tolerance\nof 0.01. This approach is beneficial as LLMs may struggle\nwith accurately computing high precision floats and large\nnumbers."}, {"title": "4.2.2 Evaluation Approaches", "content": "We evaluated the performance of\nour system using two distinct approaches:\nModule-wise Evaluation: This approach involved testing each\nmodule individually to assess its effectiveness in retrieving relevant\ninformation and answering queries. We first evaluated the pages\nextracted by FinPrimary module with the FinQA's metadata, i.e.,\nthe report and page from the S&P 500 Earnings reports used to\ncreate each query. We then tested the FinContext and FinReader\nmodules together using the same metadata, allowing us to assess the\nretrieval of relevant contexts from the provided page. We then used\nthese contexts, specifically Top-3, to evaluate the correct answer to\nthe query.\nEnd-to-end Evaluation: In this approach, we evaluated the entire\npipeline, encompassing the FinPrimary, FinContext, and FinReader\nmodules. We fed the relevant pages, specifically Top-8 from FinPri-\nmary, into FinContext to extract refined contexts. These contexts,\nspecifically Top-5, were then input into FinReader to generate the\nquery's answer."}, {"title": "4.3 Training Hyperparameters", "content": "Dense Retriever Models Hyperparameters: The FinContext\nmodule's models were trained on AWS g4dn.2xlarge instances, em-\nploying a batch size of 4 and a learning rate of 2e-5 for a maximum\nof 10 epochs. We utilized the PyTorch Lightning library for multi-\nGPU training, incorporating a 16-mixed precision setting alongside\ndistributed data parallel strategy.\nLLM Prompt Settings: For the FinReader Module, utilizing Ope-\nnAI's API (for GPT-3.5-turbo and GPT-4 models), Google's Ver-\ntexAI API (for PaLM2 model), and LLaMa2 model (using Hugging-\nFace library), we maintained a fixed temperature of 0.2 and self-\nconsistency related parame 'n' as 10 during generation. All other\nparameters were kept as the provided default parameters in the\nAPI functions."}, {"title": "4.4 Results", "content": "FINPRIMARY The evaluation results in Table 1 demonstrate that\nTF-IDF outperforms BM25. This is anticipated as TF-IDF excels\nin the financial domain by prioritizing unique financial terms and\nexact phrasing, which are critical for precise retrieval. Conversely,\nBM25 struggles to adapt to specialized language and structure of"}, {"title": "4.4.2 End-to-End Pipeline", "content": "Table 4 displays the results of our en-\ntire pipeline using the best-performing method from the modular\nevaluation. The results show a performance drop in the FinContext\nmodule when used in a sequential pipeline, which subsequently\nimpacts the FinReader module. This can be attributed to the struc-\nture of the FinQA dataset, which is based on a specific report page,\nwith the query and context built solely around that page. Given the\ndataset's focus on numerical reasoning, the context for a numerical\nquestion can be dispersed across multiple report pages in various\nformats. Treating each page separately can lead to a loss of context,\ncomplicating the model's task of accurately answering complex\nqueries."}, {"title": "vi = [sTFIDF (Q, Ci), sBM25 (Q, Ci), sOpenAI (Q, Ci), sST (Q, C\u2081)]", "content": ""}, {"title": "si = Linear(Encoderoutput (Q, Ci))", "content": ""}, {"title": "4.5 Error Analysis", "content": "We conducted thorough error\nanalysis of each individual module in the pipeline:\nFinPrimary :\nREPETITIVE INFORMATION: Approximately 8% of questions seek\ninformation that is repeated in multiple sections of the financial re-\nport. For example, a question like What were the operating expenses\nin 2006 in millions? may require retrieving pages from the Income\nStatement, Financial Highlights, and Management's Discussion sec-\ntions. This repetition creates ambiguity for the model in identifying\nthe most relevant context page.\nQUESTION AMBIGUITY: Around 2% of questions are ambiguous and\nrequire contextual understanding to retrieve the correct informa-\ntion. For instance, a question like What is the debt-to-asset ratio?\nmay require extracting pages with tables that include the ratio or\npages with explanations of such financial terms. Therefore, provid-\ning explicit and clear questions can help reduce this ambiguity."}, {"title": "FinContext", "content": "This category poses significant\nperformance challenges in about 4% queries, Fig 4. Retrieval meth-\nods struggle when tasked with retrieving more than two sentences\nor table rows, with greater difficulties encountered when retrieving\nmultiple sentences compared to multiple table rows.\nCOMPLEXITY OF CONTEXT RETRIEVAL: Accounting for about 7% of\nthe queries, this aspect poses additional challenges for retrieval\nmethods, especially when the context to be retrieved includes both\nAMOUNT OF CONTEXT RETRIEVAL:"}, {"title": "FINREADER", "content": "Our analysis reveals that while the PaLM model ex-\ncels at identifying correct numbers for calculations, it often makes\ncomputational errors. Conversely, GPT-3.5 outperforms PaLM in\ncalculation proficiency, but GPT-4 surpasses both in identifying\ncorrect numbers and precise computations, leading to significantly\nimproved performance. Our investigation also found issues with\nquestion consistency, where the gold answer is expressed as a per-\ncentage for questions requiring a specific value. In other cases,\ndisparities between the execution answer and the provided answer\nlead to misunderstandings of financial terms.\nCONTEXTUAL COMPLEXITY: This category refers to ~5% of scenarios\nwhere LLMs struggle to identify the correct numbers for calcula-\ntions or select values from a table for a different year than what is\nmentioned in the question. Inconsistencies in the context retrieved\nby the retriever modules also add to the complexity.\nQUESTION COMPLEXITY: Encompassing ~6% of queries, this is where\nLLMs face challenges in understanding the question's intent, such\nas the presence of financial terms like fair value not found in the con-\ntext or questions with specified formats. Discrepancies between the\nchosen method and the provided answer for projection-requiring\nquestions can also result in incorrect responses.\nCALCULATION COMPLEXITY: This aspect involves ~5% errors that\narise when LLMs are tasked with calculating final answers that\ninvolve large digits or decimals, potentially leading to inaccuracies.\nINSUFFICIENT CONTEXT: This category is particularly prominent\nin responses generated by GPT-3.5-turbo and GPT-4 compared to\nPaLM, accounting for ~5% of the queries. Our investigation revealed\nthat GPT models tend to provide inadequate context when they\ncannot locate financial terms mentioned in the question within the\nprovided context. Additionally, insufficiency arises when questions\nimply a projection into the future without explicit mention."}, {"title": "4.5.2 End-to-End Error Analysis", "content": "We also conducted error Analysis\non the End-to-End pipeline outputs and categorized the observed\nerrors in below categories:\nCONTEXTUAL REASONING: The FinContext module faces chal-\nlenges in extracting relevant context from multiple report pages,\nimpacting the FinReader module's performance. Additionally, some\nqueries are inherently complex, requiring context from more than\none report page, further complicating the model's ability to accu-\nrately answer such questions.\nNOISY INFORMATION: Similar information appearing in multiple\nplaces within a financial report can mislead the model, resulting in\ninaccurate predictions. Enhancing the preprocessing of financial\nreports to eliminate such noise can improve the model's perfor-\nmance.\nIMPLICIT CONTEXT: The dataset's construction presents a challenge\nas some questions implicitly expect the model to understand the\ncontext, even when it's not explicitly stated in the question. This\nmakes it difficult for the model to identify the relevant information\nrequired to accurately answer the question. Therefore, refining the\ndataset by providing explicit and clear questions can enhance the\nmodel's performance."}, {"title": "5 Conclusion", "content": "In this study, we underscore the challenges of constructing an end-\nto-end pipeline for handling complex financial tasks and integrating\nrelevant context from financial reports. Despite achieving SOTA\nresults for the FinReader module, the end-to-end pipeline's perfor-\nmance declined due to the loss of pertinent financial context. Our\nwork offers valuable insights into enhancing the performance of\neach module and the end-to-end pipeline for financial analysis.\nOur future work will focus on developing advanced techniques\nfor integrating relevant context from multiple pages of financial\nreports to enhance the FinContext module's performance, and ex-\nploring alternative models to increase the pipeline's accuracy and\nrobustness. Furthermore, our work emphasizes the need for more\ncomprehensive datasets that encapsulate the complexity and nu-\nances of financial analysis tasks."}]}