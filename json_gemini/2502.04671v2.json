{"title": "PROOFWALA: Multilingual Proof Data Synthesis and Theorem-Proving", "authors": ["Amitayush Thakur", "George Tsoukalas", "Greg Durrett", "Swarat Chaudhuri"], "abstract": "Neural networks have shown substantial promise at automatic theorem-proving in interactive proof assistants (ITPs) like Lean and Coq. However, most neural theorem-proving models are restricted to specific ITPs, leaving out opportunities for cross-lingual transfer between ITPs. We address this weakness with a multilingual proof framework, PROOFWALA, that allows a standardized form of interaction between neural theorem-provers and two established ITPs (Coq and Lean). It enables the collection of multilingual proof step data-data recording the result of proof actions on ITP states for training neural provers. PROOFWALA allows the systematic evaluation of a model's performance across different ITPs and problem domains via efficient parallel proof search algorithms. We show that multilingual training enabled by PROOFWALA can lead to successful transfer across ITPs. Specifically, a model trained on a mix of PROOFWALA-generated Coq and Lean data outperforms Lean-only and Coq-only models on the standard prove-at-k metric. We open source all our code, including code for the PROOFWALA framework and the Multilingual ITP interaction framework.", "sections": [{"title": "1. Introduction", "content": "Automated theorem-proving has long been considered to be a grand challenge in artificial intelligence. Recently, deep learning has emerged as a promising approach to this challenge (Li et al., 2024; Yang et al., 2024). Broadly, deep-learning methods for theorem-proving use neural models to generate formal proof expressed in the language of an interactive theorem prover (ITPs), e.g., LEAN (de Moura et al., 2015), COQ (Huet et al., 1997), or Isabelle (Paulson, 1994). An ITP represents proofs as sequences of simplification steps, or tactics, and can mechanically check such proofs for correctness. Theorem-proving then amounts to generating a sequence that passes the ITP's checks.\nMost deep-learning approaches to theorem-proving follow the strategy proposed by Polu & Sutskever (2020). Here, one first trains a generative language model (LM) that can predict formal proof steps (tactics and their parameters) conditioned on the goal state, from a proof-step dataset extracted from existing formal mathematics repositories. The learned model is then wrapped in a search algorithm which conducts proof search (see Section 2 for more details).\nWhile neural approaches to theorem-proving are gaining momentum, the field remains fragmented. Existing tools for dataset collection tend to be ITP-specific, often relying on isolated, domain-specific formats; there is also no language-agnostic open platform for neurally guided search over proofs. This hinders systematic comparisons and precludes potential cross-lingual and cross-domain improvements from training on multilingual data.\nIn response to this problem, we introduce PROOFWALA2, a multilingual framework for dataset collection, interaction, training, and proof search across interactive theorem provers and domains. PROOFWALA provides a standardized pipeline for generating proof step training data, facilitating the creation of high-quality multilingual datasets. It enables seamless interaction with formal systems and supports the training of neural architectures tailored for proof step generation. Finally, it integrates efficient search algorithms, including a parallelized version of best-first and beam search, allowing for end-to-end proof discovery guided by transformer-based models.\nWe provide a code library combined with multilingual datasets and multilingual fine-tuned models that facilitate end-to-end formal proof search in LEAN 4 and COQ. Using PROOFWALA, we demonstrate that training on multilingual data can foster positive cross-lingual and cross-domain transfer, enhancing proof generation across different formal systems."}, {"title": "2. Problem Formulation", "content": "We view a theorem-prover as a system that systematically addresses a set of proof obligations by applying a sequence of proof tactics. Each obligation o is a pair (g, h), where g is the goal to be proven and h contains the hypotheses relevant to proving g. The system starts with an initial set of proof obligations; its ultimate goal is to reduce this set to an empty set.\nAs in Thakur et al. (2024), we treat theorem-proving as a discrete search through the state space of an ITP. We abstractly model an ITP as a proof environment consisting of:\n\u2022 A set of states O, where each state is a set $O \\subseteq \\{o_1,..., o_k\\}$ of obligations $o_i$.\n\u2022 The initial state, I, consisting of a single obligation $(g_{in}, h_{in})$ extracted from a user-provided theorem.\n\u2022 A unique goal state QED is the empty obligation set.\n\u2022 A finite set of proof tactics.\n\u2022 A transition function T(O, a), which determines the result of applying a tactic a to a state O. If a can be successfully applied at state O, then T(O, a) is the new set of obligations resulting from the application. If a tactic a cannot be applied to the state O, then T(O, \u03b1) = 0.\nWe define the transition function $T_{seq}(O, a)$ over a sequence of proof-steps (tactics), $a = (a_1, a_2,..., a_n)$, and proof state, $\u039f \u2208 O$, as:\n$T_{seq}(O, a) =\n\\begin{cases}\n\u03a4(\u039f, \u03b1_1) & \\text{if } n = 1\\\\\n[T(T_{seq}(O, (\u03b1_1,...,a_{n-1})),a_n) & \\text{otherwise}.\n\\end{cases}$\nThe theorem-proving problem is now defined as follows:\nProblem 1 (Theorem-proving) Given an initial state $O_{in}$ find a tactic sequence a (a proof) satisfying $T_{seq}(O_{in}, a)$ = QED."}, {"title": "3. Framework Details", "content": "Now we describe the PROOF WALA framework. Our main motivation for building this new framework is to support theorem-proving research in a language-agnostic manner. In particular, we aim to facilitate standardized data collection in different ITPs and provide the necessary infrastructure to train proof step generation models, along with efficient parallel search algorithms for proof synthesis conditioned on theorem specifications.\nOur framework has three components: (i) the interface module: for the execution of proof steps (tactics) on various ITPs, (ii) the proof step generation & training module: for generating proof step data and training proof step generation model, and (iii) the parallel proof search module: for using the guidance from proof step generation model to do end-to-end the proof search. Figure 2 shows the interaction between our different modules.", "sections": [{"title": "3.1. Interface Module", "content": "First, we detail the interface module, which is responsible for facilitating interaction with the ITPs when executing proof steps. In particular, the interface module supports interaction with LEAN 4 and COQ (multiple versions from 10.0 - 18.0). Our COQ implementation is built on top of coq_serapy\u00b3, while our LEAN 4 implementation is built on top of the REPL4 library. Notably, neither of these libraries has the capability to do parallel interactions with ITPs. Hence, we created a pooling mechanism that allows us to make multiple instances of the interface module with the same state to execute tactics in parallel (parallelizable across multiple machines on a Ray cluster) for the same theorem. Parallelism is essential for searching for proofs or annotating proofs found at scale. We also fixed some well-known bugs and limitations with these libraries (see Appendix A.5). Our abstraction can support any future versions of Lean and Coq since we use the language server protocol (LSP) to further abstract out the low-level interaction between our code and the ITP interpreter/compiler.\nOne challenge in creating a unified framework is supporting the variety of state representations across these different ITPs. We develop a standard representation consistent with our problem formulation in Section 2 that is generic enough to cover all supported ITPs. The collected data is stored as json in the unified format across different ITPs; Figure 4 in Appendix A.1 shows the generalized format used for collecting training data."}, {"title": "3.2. Proof Step Generation and Training Module", "content": "Next, we describe our dataset collection & training module, which is designed to support the production of the proof step prediction model p(a|O). The first step in training a proof step generation model is to extract (proof state, proof step) pairs from human written proofs in various repositories. We use our interface module (Section 3.1) to interact with the ITP and collect proof state and proof step (tactic) pair data from all theorems in a given formal proof repository such as COMPCERT, MATHLIB, etc. For a given theorem statement and its corresponding formal proof, we extract the sequence of tactics $a = (a_1,a_2,...a_n)$ and their corresponding state transitions. Namely, for each theorem in the repository, we extract the sequence of pairs $\u03c0 = ((0_0, a_1),..., (O_{i\u22121}, a_i), ... (O_{n\u22121}, a_n))$, such that $0_0 = \\{(g_{in}, h_{in})\\}$ (extracted from the theorem statement itself), $T(O_i, a_i) = O_{i+1}$, and $T(O_{n\u22121}, a_n) = QED$. Apart from collecting the current state, proof step, and the next state, we also collect information about other lemmas which are referenced in the proof step. Figure 4 in Appendix A.1 shows the data extracted for a theorem in COQ and LEAN 4.\nPROOFWALA includes functionality for training neural models on the constructed proof datasets. It supports fine-tuning any pretrained HUGGINGFACE model for proof step generation using the data extracted from the formal proof repositories. We support generic yet flexible input formats (prompt formats) for supervised fine-tuning of the language model to predict the next proof steps. The prompt is standardized across languages and different versions of ITP and controls what aspects of the state are used for predicting the next proof step. Figure 5 in Appendix A.1 shows the example prompt formats used for training. Our format is inspired by COPRA (Thakur et al., 2024) but does not use error signals. To allow transfer across different ITPs, we do not provide any information about the domain or ITP assistant that produced the state mentioned in the prompt. As an example, we choose CODET5-BASE (Wang et al., 2021) as our pretrained model for fine-tuning in our experiments."}, {"title": "3.3. Parallel Proof Search Module", "content": "The proof search module uses the proof step generation model, trained via the proof step generation and training module (see Section 3.2), to direct the proof search through the sampling of possible next proof steps for a given state. In particular, the purpose of the search module is to generate the sequence of proof steps (tactics) $a = (a_1, a_2,...a_{n-1})$ and sequence of proof-states $w = (0_0, 0_1,... O_n)$ where (i) given a proof state O, we draw N samples from the proof step generation model to get a set $A(O) = \\{1, ..., a_k\\}$ of possible proof steps, (ii) and for each i \u2208 [n-1] there exists $a_{i+1} \u2208 A(O_i)$ such that $T(O_i, a_{i+1}) = O_{i+1}$ and (iii) the final state is QED: $T(O_{n\u22121}, a_n) = O_n = QED$. The proof search module can support any custom tree search algorithm by abstracting the node selection, generation, and expansion logic. We implement beam search and best first search.\nWe maintain an annotated proof tree while searching for the sequence of proof step(s), a, which completes the proof for a given theorem. A fully annotated proof tree is shown in Figure 7 in Appendix A.6, which was generated while performing the beam search for proving a modulo arithmetic problem. We also use these trees to analyze the proofs generated by our models (see Section 5.1). We use the negative log-likelihood of the tokens generated by the PROOFWALA models for deciding the node expansion order in our proof search experiments.\nUnlike other frameworks, our proof search module can run a parallel beam search using Ray (Moritz et al., 2018) for a given theorem. For example, frameworks like LeanDojo (Yang et al., 2023) for LEAN 4 searches for proofs sequentially for a given theorem. Parallel search improves our throughput by trying to execute multiple possible proof step(s) (tactics) generated by PROOFWALA models in parallel on the ITP. We achieve this by replicating instances of interface module (see Section 3.1) into a custom pool of Ray actors. The custom pool tracks ITP instances' proof states and uses only those matching the frontier state (states that are being explored during search) to continue exploration, adding instances as needed. The search picks up multiple instances from this pool to execute the possible next proof step generated in parallel, hence avoiding the cost of sequentially running those steps one after another on the same instance of ITP. Figure 6 (in Appendix A.2) describes the pseudocode for parallel beam search as supported by this module. The parallel proof search module allows our framework to scale to proof search for more challenging theorems with better efficiency in a generic way."}]}, {"title": "4. Dataset and Model Details", "content": "In this section, we explain our dataset construction and model training choices towards our demonstration of positive transfer from multilingual training as well as adaptability to new domains via further fine-tuning."}, {"title": "4.1. Dataset Details", "content": "We collect datasets across multiple languages and language versions of Coq and Lean 4, sourcing data from existing repositories. Our data collection approach involves collecting proof states from the ITP through tactic execution. We construct several data-mixes, of different subsets of the accumulated data, to train various monolingual and multilingual PROOFWALA models to perform proof step prediction. The training data is formatted into prompts as shown in Figure 5 (in Appendix A.1). We collect proof-step data for the various data mixtures as shown in Table 1.\nWe use different Coq and Lean repositories to generate this proof-step data. We use well-known repositories, namely CompCert, Mathlib, MathComp, GeoCoq, and CategoryTheory, 10 to generate the proof-step data. For CompCert we used the train-test split proposed by Sanchez-Stern et al. (2020), and for Mathlib we used the split proposed by Yang et al. (2023). Together we have 442607 proof-step pairs derived from over 76997 theorems across Lean and Coq (details of the split shown in Table 2). We hold out the Category Theory dataset from initial training data-mixes for experimentation with further fine-tuning for our novel domain adaptation experiment."}, {"title": "4.2. Model Details", "content": "We used the CODET5-BASE (Wang et al., 2021) pretrained model-which has 220 million parameters to fine-tune models on the different data-mixes as described in Table 1. We trained three models PROOFWALA-{MULTILINGUAL, COQ, LEAN} with the same step count and batch sizes for all settings. Training the models with the same number of steps aligns with recent work on training models for multilingual autoformalization (Jiang et al., 2023) which ensures that each model has the same number of gradient updates. Our models are initially trained on CompCert, Mathlib, MathComp, and GeoCoq. The hyperparameters used for training are described in Table 5 in Appendix A.3. Appendix A.3 also describes the amount of computing we used to train our models.\nTo demonstrate the usefulness of our models on subsequent theorem-proving tasks, we perform further fine-tune of our PROOFWALA-{MULTILINGUAL, COQ} models on CategoryTheory10 theory data. We used the same hyperparameters as Table 5 (in Appendix A.3) but we reduce the number of training steps to 1200 and batch size to 8."}, {"title": "5. Evaluation", "content": "Using our trained PROOFWALA models, we investigate (i) the benefit of incorporating multilingual data into the training pipeline (ii) moreover, whether further fine-tuning multilingual models demonstrates superior adaptation to novel domains.\nIn particular, we use the PROOFWALA-{MULTILINGUAL, COQ, LEAN} models inside the search module afforded by our framework (see Section 3.3). Our experiments run proof search on the test split mentioned in Table 2 for the CompCert, MathComp, GeoCoq, CategoryTheory, and LEANdata-mixes. This enables us to study the impact of transfer in the case of the PROOFWALA-MULTILINGUAL model for diverse ITPs and domains."}, {"title": "5.1. Experiments", "content": "Setup. All our experiments use the PROOFWALA proof step models for single-step prediction and then use PROOFWALA to conduct proof search. In our experiments we employ beam search as the search algorithm. We use the negative log-likelihood of the tokens generated by the PROOFWALA proof step prediction model to direct the search. Figure 7 in Appendix A.6 shows one such search result. Hyperparameters used in our search algorithm are listed in Table 6 in Appendix A.4. We employ a timeout of 600 seconds for most of our experiments. However, for the GeoCoq data-mix, we set a higher timeout of 1200 seconds to accommodate the appreciably longer ground-truth proofs, which require more time to execute all generated proof steps.\nWe conduct ablations to study the impact of training PROOFWALA models on different data-mixes. We also run paired bootstrap hypothesis testing to better understand the significance of transfer happening between different data-mixes, and whether PROOFWALA-MULTILINGUAL has a significant edge over other monolingual models (PROOFWALA-COQ and PROOFWALA-LEAN) while searching for proofs.\nAggregate Results. We run multiple experiments with different models as shown in Table 3. We compare the results for pass@k (Chen et al., 2021) with 1 \u2264 k \u2264 5 for LEAN, CompCert, MathComp, GeoCoq, and CategoryTheory data-mixes with various PROOFWALA models.\nFrom Table 3, we can see that the PROOFWALA-MULTILINGUAL model is a more effective prover compared to both monolingual models (PROOFWALA-COQ and PROOFWALA-LEAN) trained only on proof-data corresponding to a single ITP. The PROOFWALA-MULTILINGUAL model outperforms the state-of-the-art, Proverbot (Sanchez-Stern et al., 2020), on the CompCert dataset.\nWe perform paired bootstrap significance tests on our experiment results for the different data-mixes for both PROOFWALA-MULTILINGUAL and monolingual (PROOFWALA-COQ and LEAN) models. The results are summarized in Table 3. PROOFWALA-MULTILINGUAL significantly outperforms the PROOFWALA-LEAN model on the LEAN data-mix. For other data-mixes, the gap was small, or the data-mix size was small enough, for the significance test to be inconclusive. From Table 3, we can reasonably conclude that the PROOFWALA-MULTILINGUAL model generally outperforms the PROOFWALA-COQ and PROOFWALA-LEAN models in proof search."}, {"title": "To test the generalization capabilities of our models, we finetune our trained PROOF WALA-MULTILINGUAL and PROOFWALA-COQ models on the Category Theory data-mix. Remarkably, as can be seen from Table 3, we find that the multilingual model yields a statistically significant performance boost over the COQ model on the test set, by nearly 8%. This provides evidence that training on multilingual data enhances generalizability to unseen domains, especially when followed by further fine-tuning, compared to the monolingual baseline. This finding suggests that such multilingual models should be preferred as completion assistants for newly developing formal theorem-proving repositories.", "content": "Analysis of Specific Proofs. To better understand the effectiveness of multilingual training, we construct and analyze the search trees constructed in a subset of our problems. Figure 7 (in Appendix A.6) shows a few such search trees, which we call proof trees. To simplify our analysis, we only include those edges in the proof tree that are compilable for a given proof state, this ensures that each node in the proof tree corresponds to a valid proof state.\nSpecifically, we compute the number of nodes and edges of the proof trees computed for the different data mixes (Table 4). We find that the proof trees for the multilingual models tend to have more nodes and edges. This finding signifies that the search aided by these models can access more proof states and apply more tactics i.e., explore more. Such additional exploration generally increases the likelihood of finding correct proofs. Figure 8 and Figure 9 (in Appendix A.7) show the distribution of nodes and edges in the proof trees generated during the search.\nOur belief that the multilingual approach can explore more is further bolstered by the fact that despite the same search timeout set for both approaches, the monolingual models often fail early (not finding any tactic that can expand the search tree), whereas the multilingual models explore more of the search space and found more proofs. Table 7 (in Appendix A.7) shows the average time taken to complete the proofs in various data mixes, and Figure 11 (in Ap-"}, {"title": "6. Related Work", "content": "Previous open-sourced tooling has been developed for interaction with formal proof assistants, but individually only using a single language. Oftentimes, this tooling also contains data extraction features, compiling proof datasets from popular formalization repositories such as Mathlib (mathlib Community, 2020) for Lean, CompCert (Leroy, 2009) and Mathcomp (Mathcomp, 2015) for Coq. LeanDojo (Yang et al., 2023) provided open-source tooling for interaction with Lean 3 and extracted a proof step dataset from Mathlib.13 NTP Toolkit (Zhu et al., 2023) supports extracting training data from arbitrary Lean repositories. CoqGym (Yang & Deng, 2019) is a framework for interaction and data collection with Coq up to versions 8.12.0 (because of dependency on SerAPI library\u00b94). Proverbot (Sanchez-Stern et al., 2020) introduced Coq-Serapy, an interaction tool in Coq from which our Coq support is derived. CoqPyt (Carrott et al., 2024) is a framework for interaction and data generation from Coq with emphasis on supporting LM-based methods. COPRA (Thakur et al., 2024) introduces a framework for interaction with Lean 3 and Coq, but without tooling for data extraction or support for heavy parallelism during proof search. Aniva et al. (2024) introduced Pantograph, an interaction and data collection framework for Lean 4. We remark that one of our main contributions is an unified framework for interacting with and collecting data from both Coq and Lean 4, with support for training and parallel search, hence affording automated theorem-proving researchers a common tool in the presence of multiple popular proof assistant languages.\nA number of proof search methodologies have been proposed in the recent literature. GPT-f (Polu & Sutskever,"}, {"title": "7. Conclusion", "content": "We introduced a unified framework for standardized data collection across ITPs like Lean and Coq, which supports proof completion by generating training data, training LMs for proof step prediction, and guiding search algorithms. Using this framework, we produced a multilingual proof step dataset and train the first multi-domain model across multiple ITPs, demonstrating improved transferability between Lean and Coq in mathematics and software verification. Beyond its technical contributions, the framework serves as a foundation for uniting and advancing theorem-proving research communities by providing a shared platform for experimentation and collaboration. In particular, by leveraging this framework, we established that multilingual training not only enables cross-language proof step completion but also outperforms monolingual models, underscoring the benefits of integrating data from diverse formal systems.\nIn future work, we propose exploring the integration of advanced search algorithms specifically tailored to our standardized framework. This could include developing adaptive search methods that dynamically adjust based on the complexity and characteristics of the theorem being proven. Additionally, further research could focus on optimizing the interaction between the LM and search algorithms to enhance proof efficiency and accuracy. Expanding the dataset to include more diverse ITPs and domains could also improve the model's generalizability and robustness. Finally, investigating the use of reinforcement learning to continuously improve the model based on feedback from successful and failed proof attempts could provide significant advancements in formal theorem proving."}, {"title": "8. Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Appendix", "sections": [{"title": "A.1. Training data for proof step prediction modules", "content": "The training data is extracted in generic JSON format as shown in Figure 4. We create prompts to train the proof step generation model as demonstrated in Figure 5 from the collected raw data."}, {"title": "A.2. Parallel Proof Search Beam Algorithm", "content": "Figure 6 shows the parallel beam search pseudocode. We utilize the interface module's capabilities to create multiple instances of the proof environment and parallel run tactics to efficiently run search which can be scaled across nodes."}, {"title": "A.3. Hyperparameters used for training PROOF WALA models", "content": ""}, {"title": "A.4. Parameters used for proof search", "content": "For all our experiments the beam width is 32 (see Table 6), and the temperature for the proof step prediction model is 0.75. We also have a timeout of 600 seconds for each proof attempt for all data mixes except GeoCoq where the timeout was 1200 seconds. Since the proofs in GeoCoq were long (sometimes more than 100 tactics), giving more time for the search to finish was important."}, {"title": "A.5. Bug Fixes in existing framework", "content": "Our framework built on top of coq_serapy15 (Sanchez-Stern et al., 2020), while our LEAN 4 implementation is built on top of REPL 16 library. We have enhanced these libraries by adding a common abstraction so that data can be collected across multiple languages. We also added ray actors (Moritz et al., 2018) to make it work across clusters on multiple machines. We also fixed some issues with these libraries, for example, REPL has a bug that allows it to accept incomplete and incorrect proofs17. We also fixed some memory issues which can arise when the REPL library keeps clones of proof-state to allow easy backtracking which leads to exponential memory increase. These fixes were essential for making the framework scalable and run on multiple nodes."}, {"title": "A.6. Proof Tree annotations", "content": "Figure 7 shows a visualization generated using our tool. We can use these annotated trees to do qualitative analysis or train models for expert iteration."}, {"title": "A.7. Qualitative Analysis: Proof Tree Properties", "content": "Across various data mixes we observe that proof trees found using the MULTILINGUAL model tend to have more nodes, edges, and higher degrees per node. Figure 8, Figure 9, and Figure 3 show the distribution of nodes, edges, and degrees respectively. Figure 10 shows that MULTILINGUAL often found more proofs for the same theorem during the search.\nWe observe that MULTILINGUAL model usually searches longer for proofs across the different data mixes. The average time taken to search for proof is summarized in the Table 7 and the distribution of proof search time is shown in Figure 11.\nInterestingly, we see that there is no significant difference in the size of the proof (number of tactics used) found via the two approaches. Table 8 summarizes the length of the proofs found during the search."}, {"title": "A.8. Qualitative Analysis: Proofs found by MULTILINGUAL model", "content": "Figure 12 shows some of the LEAN 4 and COQ proofs found by MULTILINGUAL model."}]}]}