{"title": "Towards Robust Out-of-Distribution Generalization: Data Augmentation and Neural Architecture Search Approaches", "authors": ["Haoyue Bai"], "abstract": "Deep learning has been demonstrated with tremendous success in recent years. Despite so, its performance in practice often degenerates drastically when encountering out-of-distribution (OoD) data, i.e. training and test data are sampled from different distributions. In this thesis, we study ways toward robust OoD generalization for deep learning, i.e., its performance is not susceptible to distribution shift in the test data.\nWe first propose a novel and effective approach to disentangle the spurious correlation between features that are not essential for recognition. It employs decomposed feature representation by orthogonalizing the two gradients of losses for category and context branches. Furthermore, we perform gradient-based augmentation on context-related features (e.g., styles, backgrounds, or scenes of target objects) to improve the robustness of learned representations. Results show that our approach generalizes well for different distribution shifts.\nWe then study the problem of strengthening neural architecture search in OoD scenarios. We propose to optimize the architecture parameters that minimize the validation loss on synthetic OoD data, under the condition that corresponding network parameters minimize the training loss. Moreover, to obtain a proper validation set, we learn a conditional generator by maximizing their losses computed by different neural architectures. Results show that our approach effectively discovers robust architectures that perform well for OoD generalization.", "sections": [{"title": "Introduction", "content": "Deep learning has achieved tremendous success in various applications of computer vision [1] and natural language processing [2], under the implicit assumption that the training and test data are drawn independent and identically distributed (IID) from the same distribution.\nWhile neural networks often exhibit super-human generalization performance on the training distribution, they can be susceptible to minute changes in the test distribution [3, 4]. This is problematic because sometimes true underlying data distributions are significantly under-represented or misrepresented by the limited training data at hand. In the real world, such mismatches are commonly observed [5-7], and have led to significant performance drops in many deep learning algorithms [8\u201310]. As a result, the reliability of current learning systems is substantially undermined in critical applications such as medical imaging [11, 12], autonomous driving [13-19], and security systems [20].\nThe task of generalizing under such distribution shifts, has been fragmentarily researched in different areas, such as Domain Generalization (DG) [21\u201325], Causal Inference [26, 27], and Stable Learning [28]. In the setting of OoD generalization, models usually have access to multiple training datasets of the same task collected in different environments. The goal of towards robust OoD generalization is to learn from these different but related training environments and then extrapolate to unseen test environments [29, 30], which means the deep neural networks preserve the super-human generalization performance in the mismatched test distribution. In this thesis, we focus on discussing two approaches for tackling OoD generalization challenges: semantic data augmentation and neural architecture search approaches. Data and network architectures play pivotal roles in constructing machine learning systems in practice. It should be noticed that our semantic data augmentation approach and neural architecture search approach is diagonal to each other and that can be easily combined.\nIn the first part of this dissertation, we explore disentangled representation and data augmentation for OoD Generalization. We discuss DecAug, a novel decomposed feature representation and semantic augmentation approach for OoD generalization [31]. Specifically, DecAug disentangles the category-related and context-related features by orthogonalizing the two gradients (w.r.t. intermediate features) of losses for predicting category and context labels, where category-related features contain causal information of the target object, while context related features cause distribution shifts between training and test data. Furthermore, we perform gradient-based augmentation on context-related features to improve the robustness of the learned representations. Experimental results show that DecAug outperforms other state-of-the-art methods on various OoD datasets, which is among the very few methods that can deal with different types of OoD generalization challenges.\nIn the second part of the dissertation, we explore robust neural architecture search for OoD generalization [32]. Recent advances on Out-of-Distribution (OoD) generalization reveal the robustness of deep learning models against different kinds of distribution shifts in real-world applications. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without considering the influence of deep model architectures on OoD generalization, which may lead to sub-optimal performance. Neural Architecture Search (NAS) methods search for the architecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for OoD generalization (NAS-OoD), which optimizes the architecture with respect to its performance on the generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD instances by maximizing their losses computed by different neural architectures, while the goal for architecture search is to find the optimal architecture parameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly optimized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that generalize well for different distribution shifts. Extensive experimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of parameters. In addition, on a real industry dataset, the proposed NAS-OoD method reduces the error rate by more than 70% compared with the state-of-the-art method, demonstrating the proposed method's practicality for real applications."}, {"title": "Background and Preliminaries", "content": null}, {"title": "Out-of-Distribution Generalization Robustness", "content": "Several main approaches for solving the OoD generalization problem can be identified in the literature, including risk regularization methods, domain generalization, stable learning, data augmentation and disentangled representation. Currently, there are two main streams of research on OoD generalization algorithms in deep learning: Domain Generalization (DG) and risk regularization methods. DG has been an active research area for quite some time, dating back to the work of [33]. It has since then sprouted into a number of branches such as invariant representation learning [22, 34\u201337], meta-learning for DG [38\u201341], and data augmentation for DG [42\u201346]. The seminal work of risk regularization methods, IRM [47], aims to find an invariant representation of data from different training environments by adding an invariant risk regularization. It has thereafter inspired several other notable algorithms, including IRM-Games [48], VREx [9] and IGA [49]. The living benchmark is created by [50] to facilitate disciplined and reproducible DG research. After conducting a large-scale hyperparameter search, the performances of fourteen algorithms on seven datasets are reported in the paper. The authors then arrive at the conclusion that ERM beats most of DG algorithms under the same fair setting.\nRisk regularization methods for OoD generalization are motivated by the theory of causality and causal Bayesian networks (CBNs), aiming to find an invariant representation of data from different training environments (IRM, [47]). To make the model robust to unseen test environments, the invariant risk minimization added a penalty item in the loss function to monitor the optimality of a classifier on different environments. IRM-Games [48]borrowed the principle of IRM and provided another way to find the invariant feature. Risk extrapolation (REx,"}, {"title": "Disentangled Representation and Semantic Augmentation", "content": "Disentangled representation proposes to decompose the latent factors from the image variants to obtain an understanding of the data [60\u201362]. CSD [63] decomposes and learns a common component and a domain-specific component for generalizing to new domains. It aims to learn representations that separate the explanatory factors of variations behind the data. Such representations are demonstrated to be more resilient to the complex variants and able to bring enhanced generalization ability [64, 65]. Moreover, disentangled representations are inherently more interpretable. How to obtain disentangled representations is still a challenging problem. [66] identifies latent semantics and examine the representation learned by GANs. The researchers derive a closed-form factorization method to discover latent semantic and prove that all semantic directions found are orthogonal to each other in the latent space. [8] trains a de-biased representation by encouraging it to be different from a set of representations that are biased by design. The method discourages models from taking bias shortcuts, resulting in improved performances on de-biased test data.\nData augmentation is a common practice to improve the generalization ability of deep models [67\u201369]. Augmentation strategies, such as Cutout [70], Mixup [71], CutMix [72] and AugMix [73], are able to improve the performance of deep models effectively. One promising data augmentation strategy closely relevant to our work is to interpolate high-level representation learned by the deep model. [74] showed that simple linear interpolation of features can achieve meaningful semantic transformation on the input image. Inspired by this observation, [75] proposes to use random vectors sampled from a class-specific normal distribution to achieve augmentation of deep features. Instead of augmenting the features explicitly, they minimize an upper bound of the expected loss on augmented data. In order to handle few-shot learning problems, [76] proposes to train a feature generator that can transfer modes of variation from categories of a large dataset to novel classes with limited samples. To ease the learning from long-tailed data, [77] proposed to transfer the intra-class distribution of the head class to the tail class by augmenting the deep features of the instances in the tail class. Different from these methods, our method performs gradient-based augmentation on disentangled context-related features to eliminate distribution shifts for various OoD tasks."}, {"title": "Robustness from Architecture Perspective", "content": "Data distribution mismatches between training and testing set exist in many real-world scenes. Different methods have been developed to tackle OoD shifts. IRM [47] targets to extract invariant representation from different environments via an invariant risk regularization. IRM-Games [48] aims to achieve the Nash equilibrium among multiple environments to find invariants based on ensemble methods. The work of [78] finds that using pre-training can improve model robustness and uncertainty. However, existing OoD generalization approaches seldom consider the effects of architecture which leads to suboptimal performances. EfficientNet [79] proposes a new scaling method that uniformly scales all dimensions of depth, width, and resolution via an effective compound coefficient. DARTS [80] presents a differentiable manner to deal with the scalability challenge of architecture search. ISTA-NAS [81] formulates neural architecture search as a sparse coding problem. The work [82] uses a robust loss to mitigate the performance degradation under symmetric label noise. However, NAS overfits easily, the work [83, 84] points out that NAS evaluation is frustratingly hard. Thus, it is highly non-trivial to extend existing NAS algorithms to the OoD setting.\nRecent studies show that different architectures present different generalization abilities. The work of [85] uses a functional modular probing method to analyze deep model structures under the OoD setting. The work [86] examines and shows that pre-trained transformers achieve not only high accuracy on in-distribution examples but also improvement of out-of-distribution robustness. The work [87] presents CNN models with neural hidden layers that better simulate the primary visual cortex improve robustness against image perturbations. The work [88] uses a pure transformer applied directly to sequences of image patches, which performs quite well on image classification tasks compared with relying on CNNs. The work of [89] targets to improve the adversarial robustness of the network with NAS and achieves superior performance under various attacks. However, they do not consider OoD generalization from the architecture perspective."}, {"title": "Out-of-Distribution Generalization via Decomposed Feature Representation and Semantic Augmentation", "content": null}, {"title": "Introduction", "content": "Deep learning has demonstrated superior performances on standard benchmark datasets from various fields, such as image classification [67], object detection [90], natural language processing [91], and recommendation systems [92], assuming that the training and test data are independent and identically distributed (IID). In practice, however, it is common to observe distribution shifts among training and test data, which is known as out-of-distribution (OoD) generalization. How to deal with OoD generalization is still an open problem.\nTo improve a DNN's OoD generalization ability, diversified research endeavors are observed recently, which mainly includes domain generalization, invariant risk minimization, and stable learning. Various benchmark datasets are adopted to evaluate the proposed OoD generalization algorithms, such as Colored MNIST [47], PACS [93], and NICO [94]. Among these datasets, PACS are widely used in domain generalization [10, 52] to validate DNN's ability to generalize across different image styles. On the other hand, in recent risk regularization methods, Colored MNIST is often considered [9, 47, 48, 95], where distribution shift is introduced by manipulating the correlation between the colors and the labels. In stable learning, another OoD dataset called NICO was introduced recently [94], which contains images with various contexts. Along with this dataset, an OoD learning method, named CNBB, is proposed, based on sample re-weighting inspired by causal inference."}, {"title": "Related Work", "content": "In this section, we review literature related to risk regularization methods, domain generalization, stable learning, data augmentation and disentangled representation."}, {"title": "Risk Regularization Methods for OoD Generalization", "content": "The invariant risk minimization (IRM, Arjovsky et al. [47]) is motivated by the theory of causality and causal Bayesian networks (CBNs), aiming to find an invariant representation of data from different training environments. To make the model robust to unseen interventions, the invariant risk minimization added invariant risk regularization to monitor the optimality of a"}, {"title": "Domain Generalization", "content": "Carlucci et al. [52] proposed a self-supervised learning method for typical domain generalization datasets, such as PACS, by solving Jigsaw puzzles. Dou et al. [40] adopted meta-learning to learn invariant feature representations across domains. Recently, Mancini et al. [10] proposed the curriculum mixup method for domain generalization, in which data from multiple domains in the training dataset mix together by a curriculum schedule of mixup method. Domain generalization methods have achieved performance gain in generalizing models to unseen domains. However, recent OoD research finds that domain adaptation methods with similar design principles can have problems when training distribution is largely different from test distribution [47]."}, {"title": "Data Augmentation Approaches", "content": "Data augmentation has been widely used in deep learning to improve the generalization ability of deep models [67\u201369]. Elaborately designed augmentation strategies, such as Cutout [70], Mixup [71], CutMix [72], and AugMix [73], have effectively improved the performance of deep models. A more related augmentation method is to interpolate high-level representations. Upchurch et al. [74] shows that simple linear interpolation can achieve meaningful semantic transformations. Motivated by this observation, Wang et al. [75] proposes to augment deep features with random vectors sampled from class-specific normal distributions. Instead of augmenting"}, {"title": "Methodology", "content": "We argue that it is essential to obtain the disentangled features to address the aforementioned two-dimension OoD generalization simultaneously, one is the target for recognition and the other is not critical but correlated for recognition. In this section, we introduce DecAug, a novel decomposed feature representation and semantic augmentation approach to learn the disentangled high-level representation in the feature space. The decomposition is achieved based on the orthogonalization constrain of the two gradients of the losses for predicting category and context labels, as shown in Figure 3.3. We do gradient-based semantic augmentation on context-related features to enhance the generalization of the model. The augmented features and category-related features are concatenated to make the final prediction."}, {"title": "Feature Decomposition", "content": "As shown in Figure 3.3, we consider an image recognition task with the training set $\\mathcal{D} = \\{(x_i, y_i, c_i)\\}_{i=1}^N$, where $x_i$ is the input image, $y_i$ is the corresponding category label, $c_i$ is the corresponding context label, and $N$ is the number of training data. The input data are feed to the network and mapped to the feature space. The high-level representation are disentangled into two parts: category-related features and context-related features. Given an input image $x_i$ with category label $y_i$ and context label $c_i$, let $z_i = g_{\\theta}(x_i)$ denotes the features extracted by a backbone $g_{\\theta}$. For the category branch, $z_i$ is decomposed into $z_i^1 = f_{\\theta_1}(z_i)$ by a category feature extractor $f_{\\theta_1}$, followed by a classifier $h_{\\phi^1}(z_i^1)$ to predict the category label. For the context branch, $z_i$ is decomposed into $z_i^2 = f_{\\theta_2}(z_i)$ by a context feature extractor $f_{\\theta_2}$, followed by a classifier $h_{\\phi^2}(z_i^2)$ to predict the context label. We leverage the standard cross-entropy losses $\\mathcal{L}^1(\\theta,\\theta_1,\\phi^1) = l(h_{\\phi^1} \\circ f_{\\theta_1} (z_i), y_i)$ and $\\mathcal{L}^2(\\theta,\\theta_2,\\phi^2) = l(h_{\\phi^2} \\circ f_{\\theta_2} (z_i), c_i)$ to optimize these two branches, together with the backbone, respectively.\nFor the orthogonality in the feature space between category-related feature $z_i^1$ and context-related feature $z_i^2$. It is obvious that the direction of the non-zero gradient of a function is the direction in which the function increases most quickly, while the direction that is orthogonal to the gradient direction is the direction in which the function does not change.\nWe enforce the gradient of the category loss $l(h_{\\phi^1} \\circ f_{\\theta_1} (z_i), y_i)$ to be orthogonal to the gradient of the context loss $l(h_{\\phi^2} \\circ f_{\\theta_2}(z_i), c_i)$ with respect to $z_i$ for better disentangling the"}, {"title": "Semantic Augmentation", "content": "After obtaining the decomposed category-related and context-related features, we conduct gradient-based semantic augmentation on the context-related features to eliminate the spurious correlation between features. We may have multiple alternative directions for OoD in the semantic feature space, we assume a worse case for the model to learn for OoD generalization by obtaining the adversarially perturbed examples in the feature space to ensure good performance across different environments. To be specific, let $\\mathcal{G}_{aug} = \\nabla_{z_i^2}l(h_{\\phi^2} (z_i^2), c_i)$ be the gradient of the context loss with respect to $z_i^2$. We augment the context-related features $z_i^2$ as follows:\n$z_i^{2\\prime} = z_i^2 + \\alpha_i \\epsilon \\frac{\\mathcal{G}_{aug}}{|\\mathcal{G}_{aug}|}$ ,", "latex": ["\\mathcal{G}_{aug} = \\nabla_{z_i^2}l(h_{\\phi^2} (z_i^2), c_i)", "z_i^{2\\prime} = z_i^2 + \\alpha_i \\epsilon \\frac{\\mathcal{G}_{aug}}{|\\mathcal{G}_{aug}|}"]}, {"title": "Theoretical Justification of DecAug", "content": "OoD generalization problem is quite complex for theoretical analysis in general case. However, we provide a simplified case to analytically demonstrate why gradient orthogonalization can help. Let us consider the structural equation model (SEM):\n$\\begin{aligned}X_1 & \\leftarrow \\mathcal{N}_1(0, \\sigma^2), \\\\Y & \\leftarrow X_1 + \\mathcal{N}_2(0, \\sigma^2), \\\\X_2 & \\leftarrow Y + \\mathcal{N}_3(0, 1), \\\\C & \\leftarrow X_2 + \\mathcal{N}_4(0, \\sigma^2),\\end{aligned}$", "latex": ["\\begin{aligned}X_1 & \\leftarrow \\mathcal{N}_1(0, \\sigma^2), \\\\Y & \\leftarrow X_1 + \\mathcal{N}_2(0, \\sigma^2), \\\\X_2 & \\leftarrow Y + \\mathcal{N}_3(0, 1), \\\\C & \\leftarrow X_2 + \\mathcal{N}_4(0, \\sigma^2),\\end{aligned}"]}, {"title": "Illustrative Results", "content": "In this section, we compare DecAug with various methods for image recognition on different OoD datasets: Colored MNIST [47], Rotated MNIST [97], PACS [93], VLCS [53], and NICO [94]. We show that DecAug can deal with different types of OoD generalization challenges and achieve superior performance."}, {"title": "Datasets and Implementation Details", "content": "The challenging Colored MNIST dataset was recently proposed by IRM [47] via modifying the original MNIST dataset with three steps: 1) The original digits ranging from 0 to 4 were relabelled as 0 and the digits ranging from 5 to 9 were tagged as 1; 2) The labels of 0 have a probability of 25% to flip to 1, and vice versa; 3) The digits were colored either red or green based on different correlation with the labels to construct different environments (e.g., 80% and 90% for the training environments and 10% for the test environment). In this way, the classifiers will easily over-fit to the spurious feature (e.g., color) in the training environments and ignore the shape feature of the digits.\nFor a fair comparison, we followed the same experimental protocol as in IRM [47] on the Colored MNIST dataset. We equipped the IRMv1 scheme with our DecAug approach using"}, {"title": "Results and Discussion", "content": "In this section, the results of our approach on five datasets: Colored MNIST, Rotated MNIST, PACS, VLCS and NICO will be evaluated and analyzed. To provide more thorough studies on OoD generalization compared with previous work, these five datasets represent different aspects of covariant shifts including diversity shift and correlation shift in OoD problems.\nDecAug achieves the best generalization performance on Colored MNIST as shown in Table 3.1, followed by REx and IRM which are risk regularization methods. As for representative domain generalization methods, such as JiGen and MLDG, they fail to generalize in the test"}, {"title": "Ablation Studies and Sensitivity Analysis", "content": "In this section, ablation studies and sensitivity analysis are presented to evaluate the effectiveness of DecAug. Firstly, we test the effects of orthogonal loss. Then we tried different variants of DecAug, which shows the two branch architecture is needed and the gradient-based augmentation is better to perform on context branch. We also compared our method with vanilla multi-task learning. Furthermore, we use interpretability methods to understand the decomposed high-level representations.\nThe results are shown in Table 3.6. It can be seen that without the orthogonal loss, our method achieves an average accuracy of 80.77% that is higher than most of the methods in"}, {"title": "NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization", "content": null}, {"title": "Introduction", "content": "Deep learning models have encountered significant performance drop in Out-of-Distribution (OoD) scenarios [8, 9], where test data come from a distribution different from that of the training data. With their growing use in real-world applications in which mismatches of test and training data distributions are often observed [5], extensive efforts have been devoted to improving generalization ability [31, 47, 93, 94]. Risk regularization methods [47, 48, 95] aim to learn invariant representations across different training environments by imposing different invariant risk regularization. Domain generalization methods [52, 93, 104, 105] learn models from multiple domains such that they can generalize well to unseen domains. Stable learning [57, 94, 106] focuses on identifying stable and causal features for predictions. Existing works, however, seldom consider the effects of architectures on generalization ability. On the other hand, some pioneer works suggest that different architectures show varying OoD generalization abilities [86, 87, 107]. How a network's architecture affects its ability to handle OoD distribution shifts is still an open problem.\nConventional Neural Architecture Search (NAS) methods search for architectures with maximal predictive performance on the validation data that are randomly divided from the training data [80, 81, 108, 109]. The discovered architectures are supposed to perform well on unseen test data under the assumption that data are Independent and Identically Distributed (IID). While novel architectures discovered by recent NAS methods have demonstrated superior performance on different tasks with the IID assumption [79, 110\u2013112], they may suffer from over-fitting in"}, {"title": "Related Work", "content": null}, {"title": "Out-of-Distribution Generalization", "content": "Data distribution mismatches between training and testing set exist in many real-world scenes. Different methods have been developed to tackle OoD shifts. IRM [47] targets to extract invariant representation from different environments via an invariant risk regularization. IRM-Games [48] aims to achieve the Nash equilibrium among multiple environments to find invariants based on ensemble methods. REx [9] proposes a min-max procedure to deal with the worst linear combination of risks across different environments. MASF [40] adopts a framework to"}, {"title": "Neural Architecture Search", "content": "EfficientNet [79] proposes a new scaling method that uniformly scales all dimensions of depth, width, and resolution via an effective compound coefficient. EfficientNet design a new baseline which achieves much better accuracy and efficiency than previous ConvNets. One-shot NAS [116] discusses the weight sharing scheme for one-shot architecture search and shows that it is possible to identify promising architectures without either hypernetworks or RL efficiently. DARTS [80] presents a differentiable manner to deal with the scalability challenge of architecture search. ISTA-NAS [81] formulates neural architecture search as a sparse coding problem. In this way, the network in search satisfies the sparsity constraint at each update and is efficient to train. SNAS [113] reformulates NAS as an optimization problem on parameters of a joint distribution for the search space in a cell. DSNAS [115] proposes an efficient NAS framework that simultaneously optimizes architecture and parameters with a low-biased Monte Carlo estimate. NASDA [107] leverages a principle framework that uses differentiable neural architecture search to derive optimal network architecture for domain adaptation tasks. NADS [117] learns a posterior distribution on the architecture search space to enable uncertainty quantification for better OoD detection and aims to spot anomalous samples. The work [82] uses a robust loss to mitigate the performance degradation under symmetric label noise. However, NAS overfits easily, the work [83, 84] points out that NAS evaluation is frustratingly hard. Thus, it is highly non-trivial to extend existing NAS algorithms to the OoD setting."}, {"title": "Robustness from Architecture Perspective", "content": "Recent studies show that different architectures present different generalization abilities. The work of [85] uses a functional modular probing method to analyze deep model structures under"}, {"title": "Methodology", "content": "In this section, we first introduce preliminaries on conventional NAS and their limitations in OoD scenarios (Section 4.3.1). Then, we describe the details of our robust Neural Architecture Search for OoD generalization (Section 4.3.2)."}, {"title": "Preliminaries on Differentiable Neural Architecture Search", "content": "Conventional NAS methods mainly search for computation cells as the building units to construct a network [80, 113, 115]. The search space of a cell is defined as a directed acyclic graph"}, {"title": "NAS-OoD: Neural Architecture Search for OoD Generalization", "content": "In OoD learning tasks, we are provided with $K$ source domains. The target is to discover the optimal network architecture that can generalize well to the unseen target domain. In the"}, {"title": "Illustrative Results", "content": "In this section, we conduct numerical experiments to evaluate the effectiveness of our proposed NAS-OoD method. To provide a comprehensive comparison with baselines, We compare our proposed NAS-OoD with the SOTA algorithms from various OoD areas, including empirical risk minimization (ERM [47]), invariant risk minimization (IRM [47]), risk extrapolation (REx [9]), domain generalization by solving jigsaw puzzles (JiGen [52]), mixup (Mixup [71]), curriculum mixup (Cumix [10]), marginal transfer learning (MTL [101]), domain adversarial training (DANN [98]), correlation alignment (CORAL [99]), maximum mean discrepancy (MMD [34]), distributionally robust neural network (DRO [100]), convnets with batch balancing (CNBB [94]), cross-gradient training (CrossGrad [119]), and the recently proposed decomposed feature representation and semantic augmentation (DecAug [31]), etc. More details about the baseline methods are shown in the Appendix.\nFor ablation studies, We also compare NAS-OoD with SOTA NAS methods, such as differentiable architecture search (DARTS [80]), stochastic neural architecture search (SNAS [113]), efficient and consistent neural architecture search by sparse coding (ISTA-NAS [81]). This is to test whether naively combining NAS methods with OoD learning algorithms can improve the generalization performance.\nOur framework was implemented with PyTorch 1.4.0 and CUDA v9.0. We conducted experiments on NVIDIA Tesla V100. Following the design of [120], our generator model has an encoder-decoder structure, which consists of two down-sampling convolution layers with stride 2, three residual blocks, and two transposed convolution layers with stride 2 for up-sampling. The domain indicator is encoded as a one-hot vector. The one-hot vector is first spatially expanded and then concatenated with the input image to train the generator. More implementation details can be found in the Appendix."}, {"title": "Evaluation Datasets", "content": "We evaluate our NAS-OoD on four challenging OoD datasets: NICO Animal, NICO Vehicle, PACS, and Out-of-Distribution Fingerprint (OoD-FP) dataset where methods have to be able to perform well on data distributions different from training data distributions. The evaluation metric is the classification accuracy of the test set. The number of neural network parameters is used to measure the computational complexity for comparison between different neural network"}, {"title": "Results and Discussion", "content": "NAS-OoD achieves the SOTA performance simultaneously on various datasets from different OoD research areas, such as stable learning, domain generalization, and real industry dataset.\nThe results for the challenging NICO dataset are shown in Table 4.1. From Table 4.1, the proposed NAS-OoD method achieves the SOTA performance simultaneously on the two subsets of the NICO dataset with a much fewer number of parameters. Specifically, NAS-OoD achieves 88.72% on NICO Animal and 81.59% on NICO Vehicle with only around 3.1 million parameters compared with DecAug achieving 82.67% accuracy but with 11.7 million parameters. The superior performance of NAS-OoD also confirms the possibility of improving"}, {"title": "Ablation Study", "content": "In this section, we first test whether naively combining NAS methods with domain generalization methods can achieve good OoD generalization performances. We conduct experiments on"}, {"title": "Analysis of Searched Architectures", "content": "After setting up the NAS-OoD framework, we want to analyze whether any special patterns for searched cell-based network architectures and whether the NAS-OoD framework can stably find consistent architectures.\nTo check whether the found special pattern is consistent during the training process, we plot"}, {"title": "Conclusion", "content": "In this thesis, we have provided a comprehensive and systematic framework to understand distribution shifts. We analyzed two-dimensional OoD shifts (i.e., correlation shift and diversity shift) from different perspectives: data augmentation and neural network architectures. We find that while existing OoD algorithms results are not consistent across datasets including ERM, learned data augmentation and searched robust architectures do better than baselines methods over various distribution shifts.\nFirstly, we propose DecAug, a novel decomposed feature representation and semantic augmentation method for various OoD generalization tasks. High-level representations for the input data are decomposed into category-related and context-related features to deal with the diversity shift between training and test data. Gradient-based semantic augmentation is then performed on the context-related features to break the spurious correlation between context features and image categories. To the best of our knowledge, this is the first method that can simultaneously achieve the SOTA performance on various OoD generalization tasks from different research areas, indicating a new research direction for OoD generalization research.\nThen, we propose a robust neural architecture search framework that is based on differentiable NAS to understand the importance of network architecture against Out-of-Distribution robustness. We jointly optimize NAS and a conditional generator in an end-to-end manner. The generator is learned to synthesize OoD instances by maximizing their losses computed by different neural architectures, while the goal of the architecture search is to find the optimal architecture parameters that minimize the synthesized OoD data losses. Our study presents several valuable observations on designing robust network architectures for OoD generalization.\nThe research field of out-of-distribution generalization is still evolving. We discuss some possible further research directions below: Firstly, investigating relaxing the assumption for different environments of OoD generalization methods is a valuable direction remains to be done."}]}