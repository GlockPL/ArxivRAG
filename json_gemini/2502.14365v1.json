{"title": "Is Q-learning an Ill-posed Problem?", "authors": ["Philipp Wissmann", "Daniel Hein", "Steffen Udluft", "Thomas Runkler"], "abstract": "This paper investigates the instability of Q-learning in continuous environments, a challenge frequently encountered by practitioners. Traditionally, this instability is attributed to bootstrapping and regression model errors. Using a representative reinforcement learning benchmark, we systematically examine the effects of bootstrapping and model inaccuracies by incrementally eliminating these potential error sources. Our findings reveal that even in relatively simple benchmarks, the fundamental task of Q-learning - iteratively learning a Q-function from policy-specific target values - can be inherently ill-posed and prone to failure. These insights cast doubt on the reliability of Q-learning as a universal solution for reinforcement learning problems.", "sections": [{"title": "Introduction & related work", "content": "Q-learning [1] is one of the most fundamental reinforcement learning (RL) concepts making it the foundation of many popular RL algorithms. However, from the perspective of an industrial practitioner it often falls short in terms of learning stability. The core idea of Q-learning is to iteratively update Q-values using the Bellman equation. While this approach works well for table-based Markov Decision Processes (MDPs), many relevant MDPs involve a continuous state space, necessitating the use of a function approximator to learn the Q-function. Additionally, the frequent requirement to learn offline means that Q-learning combines bootstrapping, off-policy learning, and function approximation. This combination, known as the deadly triad [2, 3], presents significant challenges.\nIn this paper, we investigate how to mitigate the known issues of Q-learning on a representative RL benchmark with a continuous state space, and why achieving stability remains challenging. We begin by using the well-established model-free Q-learning algorithm Neural Fitted Q Iteration (NFQ) [4] as a baseline. Next, we eliminate bootstrapping by employing the model-based bootstrapping-free NFQ (BSF-NFQ) [5], and finally, we address model inaccuracies by utilizing the real environment dynamics. Throughout our study, we compare the robustness of policy learning, demonstrating significant improvements, but we are unable to completely eliminate instability.\nFinally, we show that fitting the targets can result in performance variability among policies. By visualizing the true Q-function, we reveal a structure that cannot be accurately approximated using a neural network (NN), rendering the"}, {"title": "Experimental setup", "content": "This paper explores stability issues commonly encountered in continuous state MDPs like inverted pendulum, acrobot and hopper [6]. Due to illustration purposes, we perform the experiments on the iconic cart-pole balancing benchmark.\nThe state space is four-dimensional, i.e., position x, velocity \\dot{x}, angle \\theta, and angular velocity \\dot{\\theta}. The dataset D has been generated by a random policy on the gym environment CartPole-v1 from the RL benchmark library Gymnasium\u00b9.\nD consists of 20,000 observation tuples of form (s_t, a_t, s_{t+1}, r_t). Depending on state s_t and action a_t, the system transitions to the next state s_{t+1} and the agent receives a real-value reward r_t \\in \\mathbb{R}.\nFor the reward, we define a function that assigns 1 for an upright pole with the cart in the center and decreases quadratically along cart position and pole angle relative to their termination bounds, i.e., r = (1 \u2212 (x/2.4)^2 + 1 \u2212 (\\theta/0.2095)^2)/2.\nIn our experiments, Q-functions were approximated using NNs in a supervised learning manner using the Adam optimizer with learning rate 0.01, mini batch size 100 and mean squared error as loss function. The dataset was split into blocks of 70% and 30% (training and validation, respectively). Like in [5], NNs with a 5-64-1 architecture were used with state-action tuples as input and ReLU non-linearity. Early stopping was employed against overfitting, halting training when no improvement of the validation error for 50 epochs was made and the best parameters found so far persisted."}, {"title": "From bootstrapping to real Q-values", "content": "The NFQ algorithm fits with an NN iteratively the targets\n\nQi+1(st, at) \u2190 rt + \\gamma \\max_{at+1} Qi(st+1, at+1). (1)\n\nIn [5], an alternative algorithm called BSF-NFQ was introduced. Here, the target calculation utilized model-based rollouts\n\nQ^{MB}(s, a) = R(s, a, \\xi_1) + \\sum_{k=1}^{K-1} \\gamma^k R(\\xi_k, \\pi(\\xi_k), \\xi_{k+1}), (2)\n\nwhere \\xi_{k+1} = M(\\xi_k, \\pi(\\xi_k)), and is calculated for a transition model M and a reward model R which both can be learned from the offline dataset. BSF-NFQ increased robustness significantly by calculating targets with\n\nQi+1(st, at) \u2190 rt + \\gamma Q^{MB}(st+1, \\pi(st+1)), with \\pi(s) = \\arg \\max_a Q_i(s, a). (3)"}, {"title": "Observing policy performance instabilities", "content": "Now that we have eliminated any potential transition model error by using the true transition and reward equations, the next step is to investigate the Q-function fitting process itself. To gain insights into the observed instability, e.g., the successful policy in iteration 18 yielded a bad policy in iteration 19, we save the targets calculated with Eq. (3) based on the policy in iteration i and retrain iteration i + 1 with NNs initialized with different seeds. The resulting policies were tested with respect to their performance and the corresponding box-plots for the first 20 iterations are depicted in Figure 2."}, {"title": "An ill-posed learning task", "content": "To explore why learning Q-values even for a relatively simple benchmark like cart-pole leads to vastly different Q-function approximations in terms of balancing performance, we conducted an in-depth analysis of the structure of the actual Q-values. Figure 3 illustrates slices through the true Q-function by holding the three state space variables x, \\dot{x}, and \\dot{\\theta} constant at 0, and plotting the Q-values across 10,000 gridded values of the pole angle \\theta.\nFigures 3 (a) and (b) display the Q-values from iterations 18 and 19 of the Q-learning run shown in Figure 2, respectively. Notably, the policy resulting from iteration 18 successfully balanced the pole for all starting states. However, in the subsequent iteration 19, the policy's performance completely collapsed.\nThe figures reveal the highly discontinuous structure of the true Q-values which is particularly evident in the magnified view depicted in Figure 3 (d). We want to emphasize that the substantial differences in the true Q-values between adjacent angle values are not caused by noise, as the policy rollouts in Figures 3 (a), (b), and (d) are entirely deterministic.\nLearning a subsequent Q-function on samples of these true Q-values fails to capture the structure completely and results in the NN approximating an average over the samples, which is illustrated by the line plots in Figure 3. This loss of information is the primary source of error for the policy performance instability."}, {"title": "Conclusion", "content": "In this paper, we identified a fundamental issue with estimating Q-values and return values. While estimating Q-values at isolated points can be relatively effective, using a function approximator to learn them presents significant challenges. We demonstrated the dramatic impact that discontinuities in the Q-function can have on Q-learning, potentially causing a collapse in the quality of the learned policies. Our findings illustrate that this problem can emerge even in simple MDPs with continuous state spaces."}]}