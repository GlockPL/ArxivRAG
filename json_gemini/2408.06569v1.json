{"title": "Social Debiasing for Fair Multi-modal LLMs", "authors": ["Harry Cheng", "Yangyang Guo", "Qingpei Guo*", "Ming Yang", "Tian Gan", "Liqiang Nie"], "abstract": "Multi-modal Large Language Models (MLLMs) have advanced the reseach field recently and delivered powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions with respect to attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which is a more diverse and extensive dataset for model debiasing compared to existing ones; ii) proposing an Anti-Stereotype Debiasing strategy (ASD). Our method is equipped with the rescaling of the original autoregressive loss function as well as the improvement of data sampling to counteract biases. We conduct extensive experiments with three prevalent MLLMs to evaluate i) the effectiveness of the newly collected debiasing dataset, ii) the superiority of the proposed ASD over several baselines in terms of debiasing performance.", "sections": [{"title": "1. Introduction", "content": "Benefiting from the advancement of Large Language Models (LLMs), Multi-modal Large Language Models (MLLMs) have revolutionized the field of general-purpose vision-language understanding. Some representative models, such as LLaVA [35], Qwen-VL [3], and Bunny [23], exhibit remarkable zero-shot performance and can be easily fine-tuned for diverse downstream applications.\nDespite MLLMs' widespread use, it is imperative to recognize that these models lead to severe social biases with respect to attributes such as race and gender [16, 58]. Figure la illustrates one example that a biased MLLM strongly inclines to predict a nurse in female in contrast to male. One key reason for this is that the datasets used to pre-train these models contain certain stereotypes, racist content, and ethnic slurs [6]. These biases are subsequently transferred to downstream tasks [8, 56], resulting in unfair predictions.\nExisting studies on mitigating the social bias problem in MLLMs remain largely under-explored. A naive approach is to collect attribute-balanced vision-language counterfactual datasets [22, 31], where biased models can be rectified to predict fair distributions via direct fine-tuning. However, as demonstrated in Table 1, existing large-scale datasets are limited by focusing on only a single social concept, such as occupation [25], while ignoring multifaceted social stereotypes [55]. This significantly hampers the model in learning more comprehensive representations. Meanwhile, from the methodology side, our experimental results indicate that directly fine-tuning on the balanced dataset leads to suboptimal performance, as it assigns equal importance to instances received different social biases. With widely accepted empirical observation: To neutralize acidic water,"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Multi-modal Large Language Model", "content": "With the rapid development of LLMs [9, 44, 50, 53, 57], increasing efforts have been dedicated to extending the powerful reasoning capabilities of LLMs to multi-modal applications [1, 10, 14, 26, 29, 32]. Specifically, MLLMs utilize LLMs as the foundational base, aligning vision features with text embeddings to enable LLMs to perceive multi-modal inputs. For instance, BLIP-2 [33] utilizes a Q-Former to align vision encoders with LLMs. LLaVA [35, 36] presents to directly map vision features to the word embedding space of Vicuna [11]. Qwen-VL [3] employs a single-layer cross-attention module as the vision-language adapter, and introduces multi-task pre-training to improve the model performance. These approaches significantly enhance multi-modal understanding capabilities. However, some studies highlight the existence of social biases in pre-trained LLMs [7, 8, 16]. These biases often manifest as harmful spurious correlations related to human attributes such as gender and race. For instance, LLMs may 'efficiently' filter resumes based on race and gender rather than the candidates' qualifications [54]. Nevertheless, to the best of our knowledge, research on social biases in MLLMs remains largely unexplored."}, {"title": "2.2. Social Bias Reduction", "content": "We roughly categorize the bias mitigation strategies into two groups: data-based and objective-based.\nData-based debiasing typically refers to data augmentation techniques. For instance, Chuang et al. [12] employ mixup [60] to construct interpolated samples among groups with different distributions. Ramaswamy et al. [45] utilize perturbed GAN-generated images [19] in latent space to augment the original dataset. Moreover, some studies collect fair datasets across human attributes for continual fine-tuning [5, 28, 31, 47, 62]. In particular, VisoGender [22] includes 690 manually annotated images on 23 occupations. FairFace [30] contains 108K images that are balanced for the race attribute. Socialcounterfactuals [25] collects 171K image-text pairs to probe biases across race, gender, and physical characteristics. However, these collected counterfactual datasets often focus on only one concept"}, {"title": "3. Dataset Construction", "content": "As demonstrated in Table 1, existing counterfactual datasets are either restricted by their small scale [22] or a narrow coverage of concepts [30]. To bridge this gap between existing datasets and real-world stereotypes, we introduce the CMSC dataset, encompassing 60k high-quality images across eighteen social concepts."}, {"title": "3.1. Social Attributes and Concepts", "content": "A counterfactual dataset for social bias reduction and evaluation typically contains two key concepts: Social Attribute (SA) and Social Concept (SC). The former, i.e., SA, is defined as characteristics shared by a group of people [30]. Specifically, we investigate three types of attributes: i) genders: {Male, Female}; ii) races: {White, Black, Indian, Southeast Asian, East Asian, Middle Eastern, Latino}; and iii) ages: {Young, Old}. These attributes are intrinsic to individuals\u00b2. For each image $I_i$ in CMSC, three SA labels are provided with respect to these three types of SA, and are combined as a SA set $A$.\nAs for the SC, we define it as the societal label attributed to an individual, denoted as C. Drawing from the sociological research [51], we employ 18 SCs in CMSC. In particular, these SCs are categorized into three groups: personality, responsibility, and education. Specifically, personality relates to concepts pertaining to an individual's character. We use five concepts: {compassionate, belligerent, authority, pleasant, unpleasant}. Responsibility is the roles or duties that individuals are expected to fulfill in society or family. We identify six concepts: {tool, weapon, career, family, chef working, earning money}. Education pertains to the level of education an individual has received. We include seven concepts: {middle school, high school, university, good student, bad student, science, arts}. Each image in the CMSC dataset is annotated with one SC label $c_i \\in C$, where C is the union of the concepts from all above groups.\nIn summary, each instance $P_i$ in our dataset consists of an image $I_i$, a set of SA labels $A_i$, and a SC label $c_i$. To the best of our knowledge, CMSC is the first large-scale counterfactual dataset that includes a variety of social concepts."}, {"title": "3.2. Image Generation Pipeline", "content": "We employ the Stable Diffusion XL (SDXL) [42] to generate instances in CMSC\u00b3. We first construct carefully designed prompt templates for different SCs. For instance, for the SC 'pleasant', we have the prompt 'A photo of a pleasant [race] [gender] in [age], who should have a friendly smile, a relaxed posture, and be dressed casually.' For each template, we employ an intersectional generation strategy [25]. As illustrated in Figure 2, we determine the gender and age (step 2), and then generate four sets of base images (step 3). Thereafter, we employ the Prompt-to-Prompt (p2p) control [24] to generate visually similar images that differ only in terms of race (step 4). The p2p control in-"}, {"title": "4. Evaluation on CMSC", "content": ""}, {"title": "4.1. Datasets and Metrics", "content": "Datasets. Two large-scale counterfactual datasets are applied to perform comparison with our CMSC. i) Social-Counterfactuals [25] focuses on the SC of occupation, containing 171K high-quality synthetic images. Following its original setup, we held 20% of the dataset as the testing set and used the remaining 80% as the training set. ii) FairFace [30] consists of 108K images collected from the YFCC-100M Flickr dataset [52]. Each image is annotated with the gender, race, age, i.e., the SA labels. We employed its validation set to evaluate the cross-dataset debiasing effectiveness. Since FairFace does not have SC annotations, we adapted the SC labels from SocialCounterfactuals [48].\nMetrics. We employ Skew [18] as the metric to measure the extent of social biases. In particular, for dataset D, we denote the subset that includes a specific SC label $c'$ as $D_{c'}$,\n$D_{c'} = {P_i|P_i \\in D, c_i = c'}.$\nFor $D_{c'}$, we derive the subset containing instances with a specific SA label $a'$, represented as $D_{a'|c'}$,\n$D_{a'|c'} = {P_i|P_i \\in D_c, a' \\in A_i}.$\nWe then utilize MLLMs to predict the SC label corresponding to each $P_i$. Specifically, we extract image features $X_{img}$ from $I_i \\in P_i$. These features are integrated with a text prompt $x_{ins}$, such as 'What is this person's occupation?' as input. Thereafter, we obtain the predicted SC label $\\hat{c_i}$. By applying this process to the entire dataset D, we can construct a new set $D' = {P'_i}^N_{i=1}$, where $P'_i = {I_i, A_i, \\hat{c_i}}$, and N = |D|. In D, subsets $D'_{c'}$ and $D'_{a'|c'}$ can be derived with Equation (1) and Equation (2), respectively. The Skew for SC $c'$ and SA $a'$ is formulated as\n$Skew_{a'|c'} = log(\\frac{Y_{a'|c'}}{\\overline{Y}_{a'|c'}}),$ where\n$Y_{a'|c'} = \\frac{|D'_{a'|c'}|}{|D'c|}$\n$\\overline{Y}_{a'|c'} = \\frac{|D'_{a'|c'}|}{|D|}$\nWhen $Skew_{a'|c'} > 0$, the MLLM tends to classify instances with attribute $a'$ as concept $c'$. For instance, in Figure 1a, $Skew_{Female|Nurse} > 0$. In contrast, when $Skew_{a'|c'} < 0$, the MLLM is inclined to not predict these instances containing SA $a'$ as SC $c'$, e.g., predicting male as nurse. A fair MLLM should have Skew close to 0 across all concepts and attributes, i.e., $\\sum_{a,c} |Skew_{a'|c'}| \\rightarrow 0$.\nAlthough Skew can effectively measure MLLMs' bias towards a particular SA-SC pair, a counterfactual dataset often contains hundreds of such combinations, e.g., our CMSC includes 198 SA-SC pairs. This confounds the comprehensive assessment of MLLM bias. Therefore, we adapt two variants of Skew to ensure an overall analysis. Specifically, we first identify the maximum and minimum Skew values for each SC across all SAs,\n$\\begin{cases}MaxSkew = Max_{a_i \\in A}{Skew_{a_i|c'}},\\\\MinSkew = Min_{a_i \\in A}{Skew_{a_i|c'}}. \\end{cases}$\nFor all $c' \\in C$, we separately calculate the average of MaxSkew and MinSkew, resulting in two aggregated Skew values. We refer to these two values as MaxSkew@C (MaxS@C) and MinSkew@C (MinS@C), which represent the overall bias level of the MLLM. Both of these metrics indicate better fairness as they approach zero."}, {"title": "4.2. Comparison on Image Distribution", "content": "We calculated the Fr\u00e9chet Inception Distance (FID) scores for the synthetic counterfactual datasets SocialCounterfactuals and our CMSC. Specifically, we randomly sampled 1,000 synthetic images from each dataset and then computed the distributional differences with the same set of 1,000 real images. A lower FID signifies better image quality. With this process, the two datasets received FID scores of 27.17 and 24.35, respectively. This indicates that the images in our dataset bear a closer resemblance to reality."}, {"title": "4.3. Comparison on Fine-tuning", "content": "We reported MinSkew@C and MaxSkew@C of several MLLMs in Figure 3. Specifically, these models, i.e., LLaVA [35], Qwen-VL [3], and Bunny [23], are fine-tuned on the SocialCounterfactuals and CMSC datasets individually and evaluated on the FairFace dataset, respectively. For clarity, we took the absolute values of both metrics. From this figure, we observed that models fine-tuned on our CMSC dataset exhibit superior debiasing effects. For instance, LLaVA-13B achieves a MinSkew@C of -1.3132 on FairFace, a significant advantage over that from the model fine-tuned on SocialCounterfactuals. This implies that the broader range of social concepts within CMSC enables the model to learn fairer distributions."}, {"title": "5. Anti-stereotype Debiasing", "content": ""}, {"title": "5.1. Preliminaries", "content": "Before introducing our proposed method, we first revisit the training objectives of MLLMs. Subsequently, we introduce the concept of Skew(Pi) to measure stereotypes associated with specific instances."}, {"title": "5.1.1 Traning Objective of MLLMs", "content": "Current mainstream MLLMs employ pre-trained LLMs [11, 53] as the output interface [3, 23, 35, 38]. Under this context, the base LLM is often trained in an autoregressive way,\n$P(x_{1:T}; \\theta) = \\prod_{t=1}^{T} P(x_t | x_{<t}; \\theta),$\nwhere x is a text sequence with T tokens, and $\\theta$ denotes the model parameters. During pre-training, the LLM predicts the t-th token $x_t$ based on all preceding ones, i.e., $x_{<t}$. After that, another instruction tuning stage [41] is often followed to enable LLMs to better understand user intentions,\n$P(y_{1:T}|X_{ins}; \\theta) = \\prod_{t=1}^{T} P(y_t|y_{<t}, X_{ins}; \\theta),$\nwhere $x_{ins}$ and y are textual instructions and responses, respectively. MLLMs extend the inputs of LLMs with the supplement of image features $x_{img}$ that are extracted using pre-trained vision encoders such as ViT [27, 43],\n$P(y_{1:T}|x_{ins}, X_{img}; \\theta) = \\prod_{t=1}^{T} P(y_t|y_{<t}, x_{ins}, X_{img}; \\theta),$\nwhere $x_{img}$ is aligned with text features via a connector, e.g., a trainable projection matrix. To optimize the pipeline, previous cross-entropy loss from LLMs is directly inherited,\n$L(y,x_{ins}, X_{img};\\theta) = - \\sum_{t=1}^{T}logP(y_t|y_{<t},x_{ins}, X_{img};\\theta).$"}, {"title": "5.1.2 Stereotype Measurement", "content": "Recall that $Skew_{a'|c'}$ quantifies the degree of social biases in an MLLM across the entire dataset. In this section, we are more interested in the social bias degree for each specific instance $P_i$. We then define Skew(Pi) as a selected"}, {"title": "5.2. ASD Approach", "content": "As illustrated in Figure 4, vanilla fine-tuning approach involves sampling images from a balanced dataset and updating MLLMs through the original autoregressive objective. We argue that this method, which treats all instances equally, is ineffective in addressing the social bias problem in MLLMs [13]. Therefore, we propose an ASD method from the view of anti-stereotype. Specifically, our ASD is composed of two components: i) Dataset Resampling, which enhances the data sampling process to include more underrepresented instances. ii) Loss Rescaling, where we adjust the loss function to place larger emphasis on instances that are overlooked in terms of social attributes.\nDataset Resampling. The direct fine-tuning approach utilizes datasets that are balanced across all SAs. We believe that such datasets make it difficult for MLLMs to recognize which SAs are being ignored. To address this, we resample the dataset to increase the frequency of instances that MLLMs tend to ignore, thereby 'neutralizing' social biases.\nSkew(Pi) serves as an indicator for dataset resampling. As demonstrated in Algorithm 1, for each instance Pi, when Skew(Pi)>0, i.e., Pi has received more attention \u2013 such as the 'female-nurse' in Figure 1 - we reduce its probability in\nthe resampled dataset $D_r$ for the following training epoch,\n$\\begin{cases}D_r \\leftarrow D_r \\cup {P_i}, rand(0, Skew(P_i)+\\tau_1) > Skew(P_i),\\\\D_r \\leftarrow D_r, rand(0, Skew(P_i)+\\tau_1) < Skew(P_i), \\end{cases}$\nwhere rand() is to randomly draw from 0 to Skew(Pi) + $\\tau_1$, and $\\tau_1$ is a pre-defined threshold. As such, a larger Skew(Pi) corresponds to a lower chance of being included by Dr. In contrast, for instances with Skew(Pi) \u22640, we believe that increasing their proportion in the new dataset Dr is beneficial for the MLLM to learn the features of these overlooked parts, thereby achieving a fairer distribution. To this end, these instances are directly accepted into Dr. Moreover, we design an over-resampling mechanism to further increase the occurrence frequency of these instances. Specifically, for each Pi, we employ an accumulative value, AcmSkew, to gradually accumulate the current Skew(Pi),\n$AcmSkew_{a_i|c_i} = AcmSkew_{a_i|c_i} + |Skew(P_i)|,$\nwhere $c_i$ and $a_i$ are SC label and SA label corresponding to Skew(Pi), respectively. When $AcmSkew_{a_i|c_i}$ exceeds a threshold $\u03c4_2$, we add the $P_i$ into $D_r$ again. The AcmSkewailci can then be set to 0 for a new round of accumulation. With the above operations, instances with a Skew(Pi) \u2264 0 will be resampled multiple times. This resampling process is executed once before each training epoch. For evaluation, we employ the balanced testing set to ensure a fair comparison.\nLoss Rescaling. During fine-tuning, we rescale the autoregressive loss in Equation (9) to a new Social Fairness Loss (SFLoss) for more effective debiasing. Specifically, for MLLMs, the empirical risk during training can be represented as\n$E = \\frac{1}{N} \\sum_{i=1}^{N} C(y^i, x_{ins}^i, x_{img}^i; \\theta),$\nwhere N = |D| is the dataset size, and $y^i, x_{ins}^i, x_{img}^i$ are the predicted responses, text instructions, and image features for the i-th instance Pi, respectively. However, as we discussed before, treating each instance equally does limited help in addressing the model bias toward overly represented SAs and SCs. Our solution to this issue is inspired by the approaches that have been proven effective in the class imbalance research area [21]. Instead of scaling loss based on the class frequency, we leverage the stereotype quantification metric Skew to rescale the loss value,\n$E_{fair} = \\frac{1}{N} \\sum_{i=1}^{N} e^{-Skew(P_i)} L(y^i, x_{ins}^i, x_{img}^i; \\theta).$\nIn this scenario, when Skew(Pi) > 0, i.e., the MLLM tends to predict the input instance as the SC label $c_i$, the fairness"}, {"title": "6. Experiments", "content": ""}, {"title": "6.1. Datasets and Baselines", "content": "Datasets. In addition to the three social bias datasets mentioned in Section 4.1, we also employed three recent benchmark datasets to evaluate MLLMs' original zero-shot capabilities. Among them, VQAv2 [20] and TextVQA [49] are benchmarks for general visual question answering and text-oriented visual question answering, respectively. MMBench [37] evaluates the model robustness with comprehensive multiple-choice answers.\nBaselines. Given the limited exploration of debiasing strategies for MLLMs, we compared our ASD with the direct fine-tuning (FT), i.e., fine-tuning the model on balanced counterfactual dataset [56]. Moreover, we adapt POPE [34], which addresses hallucinations in MLLMs, as a baseline."}, {"title": "6.2. Implementation Details", "content": "Unless otherwise specified, all models are fine-tuned on SocialCounterfactuals and then tested across all other datasets. Both thresholds $\\tau_1$ and $\\tau_2$ in Section 5.2 are set to 1.0. During fine-tuning, we set the learning rate to $5e-7$, $1e-6$, $1e-7$ for the three MLLM utilized, respectively. We trained the models with 30 warm-up steps on 8*A800 GPUs. Each training session takes ~18 GPU hours."}, {"title": "6.3. Main Results", "content": "We reported MaxSkew@C and MinSkew@C of the three utilized MLLMs in Table 3, wherein we have three main observations: i) Compared to the baselines, our ASD demonstrates the best debiasing effectiveness across three different MLLM architectures. In intra-dataset evaluation, i.e., fine-tuning and testing on the SocialCounterfactuals dataset, LLaVA-7B+ASD achieved a MinSkew@C of 0.1744, improving by 37% compared to LLaVA-7B+FT. In cross-dataset evaluation, our ASD also outperforms the two baselines by a significant margin. For example, LLaVA-7B+ASD attains a MaxSkew@C of 0.7345 on the CMSC dataset, showing a reduction of 0.7 in absolute value. ii) A model with a larger size does not necessarily correspond to a lower social bias. For example, LLaVA-13B shows greater bias across the three datasets compared to LLaVA-7B. This might because the increased scale of parameters makes the model to better learn the biases present in the pre-training dataset. iii) POPE's debiasing effectiveness underperforms FT. One possible reason is that POPE is a training-free method, causing its predictions to remain biased towards the pre-training data distribution.\nPertaining to the model results on general multi-modal benchmarks, we observed that our ASD method has a negligible impact, with the effect on all architectures across the three datasets being less than 0.5%. This indicates that our ASD method enhances the model's social fairness without significantly sacrificing its general capabilities."}, {"title": "6.4. Ablation Studies", "content": ""}, {"title": "6.4.1 Comparison on Ablated ASD", "content": "Our ASD method consists of two key components: the dataset resampling and the rescaled SFLoss. In Table 4, we reported the performance of these two models. We observe that both components help alleviate social bias. For example, LLaVA-7B+SFLoss achieves a significant 50% MinSkew@C improvement on FairFace. On CMSC, Qwen-VL-7B+Resample achieves a MinSkew@C of -0.8342, reducing the absolute Skew value by more than 0.5 compared to baseline. A notable observation is that either of the SFLoss and data resampling shows better debiasing effect compared to the naive FT strategy. Combing these two delivers the best results."}, {"title": "6.4.2 Comparison on SAs", "content": "In Figure 5, we illustrate the MaxSkews of Qwen-VL and Bunny across different races in the SocialCounterfactuals dataset. We can observe that both models exhibit significant social bias without any fine-tuning. For instance, Qwen-VL has a Skew value of approximately 0.76 for White, indicating a strong preference for predicting occupations for this race. This is greatly alleviated after fine-tuning. Notably, Our ASD method achieves better debiasing effects across all races for both models compared to naive FT."}, {"title": "6.4.3 Skew Distributions", "content": "In Figure 6, we illustrate the Skew distribution across different SCs in the CMSC dataset. It can be observed that the LLaVA-13B+FT model exhibits higher mean and median Skew values across all three concepts, along with sig-"}, {"title": "7. Discussion and Conclusion", "content": "Conclusion. In this paper, we present to address the notorious social bias problem in multi-modal large models. Our first contribution is a comprehensive dataset that covers more diverse social concepts than previous datasets. In addition, we advocate an anti-stereotype debiasing approach to perform both dataset resampling and loss rescaling, thereby improving fairness of MLLMs. Extensive experiments demonstrate that our method is promising to alleviate the social bias in MLLMs, with minimal negative impact on their original general multi-modal understanding capabilities.\nSocial Impact. Multi-modal large language models are rapidly integrating into various societal functions, and their influence is expected to grow as they assume more responsibilities in the near future. However, MLLMs from different origins have consistently shown biases related to race and gender, prompting a critical reevaluation of the social stereotypes embedded in these models and their potentially harmful impacts. Our research validates the effectiveness of counter-stereotype approaches in reducing social bias within autoregressive-based MLLMs, offering a pathway toward more equitable and fair AI systems."}]}