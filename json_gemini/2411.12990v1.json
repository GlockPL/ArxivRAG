{"title": "BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices", "authors": ["Anka Reuel", "Amelia Hardy", "Chandler Smith", "Max Lamparth", "Malcolm Hardy", "Mykel J. Kochenderfer"], "abstract": "Al models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models. They can inform model selection for downstream tasks and influence policy initiatives. However, not all benchmarks are the same: their quality depends on their design and usability. In this paper, we develop an assessment framework considering 46 best practices across an AI benchmark's lifecycle and evaluate 24 AI benchmarks against it. We find that there exist large quality differences and that commonly used benchmarks suffer from significant issues. We further find that most benchmarks do not report statistical significance of their results nor allow for their results to be easily replicated. To support benchmark developers in aligning with best practices, we provide a checklist for minimum quality assurance based on our assessment. We also develop a living repository of benchmark assessments to support benchmark comparability, accessible at betterbench.stanford.edu.", "sections": [{"title": "1 Introduction", "content": "Al systems are rapidly advancing and proliferating [58]. The increasing integration of AI, and in particular foundation models (FMs) [14], into decision-making systems has significantly amplified its impact and has showcased both benefits [9, 39, 57, 66] and risks [2, 76, 44, 87, 70, 45, 30, 74]. Given the importance of correctly assessing a model's capabilities and potential harms, AI evaluation is an essential discipline [15]. Current evaluation approaches include both internally (e.g., private testing on proprietary data) and externally developed techniques (e.g., scoring on public benchmarks) [75, 27, 73, 48, 32].\nFollowing the work of [67], we define a benchmark \u201cas a particular combination of a dataset or sets of datasets [...], and a metric, conceptualized as representing one or more specific tasks or sets of abilities, picked up by a community of researchers as a shared framework for the comparison of methods\" [67]. Using benchmarks to facilitate comparison, measure performance, track progress, and identify weaknesses has become a standard practice. For example, benchmarks are widely used by model developers to report performance and compare models upon release [3, 8], and as part of policy initiatives to support third-party model evaluations, such as as part of the UK AI Safety Institute's Inspect framework for evaluating large language models (LLMs) [82] or Article 51 of the EU AI Act [1]. However, the fidelity of this approach depends entirely on the benchmarks' quality, where we define a high-quality benchmark as one that is interpretable, clear about its intended purpose and scope, and that is usable. To date, no structured assessment for the quality of AI benchmarks, including both FM and non-FM benchmarks, has been published, and no comparative analysis has been conducted to understand quality differences between widely used AI benchmarks. To address these gaps, our paper:\n\u2022 Presents a novel AI benchmark assessment framework evaluating the quality of AI bench- marks based on 46 criteria derived from expert interviews and domain literature\n\u2022 Scores 16 foundation model (FM) and 8 non-FM benchmarks (full list in App. C), finding quality differences across both categories\n\u2022 Provides insights into prevalent issues in current AI benchmarking practices based on our assessment\n\u2022 Creates a checklist for minimum quality assurance to support benchmark developers in aligning with best practices\n\u2022 Makes available a living repository of benchmark assessments for users to analyze bench- marks' quality and appropriateness for their usage contexts.\nWe structure the paper as follows: Sec. 2 explores benchmarking in AI and other fields. Sec. 3 describes our assessment development, which combined literature and expert interviews, and details our benchmark scoring procedure. Sec. 4 presents our framework's criteria, focusing on aspects under developers' control to promote better benchmarks. Sec. 5 lists additional context-dependent design considerations. Sec. 6 reports findings from applying our framework to 24 benchmarks. Finally, Sec. 7 and Sec. 9 explore implications for future evaluations and discuss our work's scope and limitations. We further outline open challenges with AI benchmarking in Sec. 8, involved stakeholders in App. A, and the AI benchmark lifecycle in App. B."}, {"title": "2 Related Work", "content": "Our literature review of AI benchmarking practices identifies two primary concerns: what a bench- mark measures and how this measurement is used. Regarding what a benchmark measures, [59] find that current benchmarks for LLMs are insufficient for assessing these models' capabilities. A frequent concern in this context is the validity of evaluations [54, 77, 67]. Similarly, [62] finds that the rapid advancement of AI models threatens benchmarks' utility, as a large fraction of these evaluations are near saturation. [84] and [49] both address the narrow scope of existing benchmarks, with [49] advocating for approaches intended to reduce the socio-technical gap that exists between the capabilities that benchmarks are able to measure and the ability of models to meet user needs in downstream applications. With respect to how evaluations are used, [67] critiques the tendency of AI practitioners to overgeneralize benchmark results, highlighting how these scores present an inherently reductive view of model performance.\nIn addition, the community has also recognized the importance of data curation and documentation in the context of evaluations. [65] put forth the idea of data cards as standardized documentation framework for datasets and [12] develop a framework and checklist for best practices in data curation. Finally, the FAIR principles [88] outline best practices for digital data access, based on the principles of Findability, Accessibility, Interoperability, and Reuse. While these efforts support the adoption of best practices in the context of data, they are insufficient for assessing AI benchmarks, which extend data with infrastructure and evaluation methods, requiring additional guidelines to support the development of high-quality benchmarks and the decision-making of benchmark users.\nHence, our work builds on and expands these guidelines, with the aim of advancing the analysis of AI benchmarking by presenting a first-of-its-kind framework for the assessment of both foundation model and non-foundation model benchmarks. Unlike prior studies, such as [59] and [49], which focus on identifying limitations in limited contexts and scopes, our approach offers practical tools, empowering developers to address shortcomings and directly enhance benchmark quality: Our assessment spans a wider range of criteria across the benchmark lifecycle, from design (e.g., have domain experts been involved in the development?) to implementation (e.g., is the evaluation script available?), documentation (e.g., is the applicable license specified?), and maintenance (e.g., is a feedback channel available for users?). We give an overview of all our criteria in Sec. 4 and explain, justify, and provide scoring details for each criterion in App. J. We further provide a checklist of best practices derived from our analysis (App. I), offering guidance for improving AI benchmarks, rather than merely highlighting issues."}, {"title": "2.1 AI Benchmarking Practices and Challenges", "content": "Our literature review of AI benchmarking practices identifies two primary concerns: what a bench- mark measures and how this measurement is used. Regarding what a benchmark measures, [59] find that current benchmarks for LLMs are insufficient for assessing these models' capabilities. A frequent concern in this context is the validity of evaluations [54, 77, 67]. Similarly, [62] finds that the rapid advancement of AI models threatens benchmarks' utility, as a large fraction of these evaluations are near saturation. [84] and [49] both address the narrow scope of existing benchmarks, with [49] advocating for approaches intended to reduce the socio-technical gap that exists between the capabilities that benchmarks are able to measure and the ability of models to meet user needs in downstream applications. With respect to how evaluations are used, [67] critiques the tendency"}, {"title": "2.2 Benchmarking Best Practices in Other Fields", "content": "Our work is informed by benchmarking practices from fields beyond AI, ranging from transistor hardware [18] to environmental quality [16] to bioinformatics [7], and we identify common themes regarding what constitutes an effective benchmark. Where applicable, we incorporate these best practices into our assessment (Sec. 4):\nDesigning for downstream utility. Many of the papers reviewed discuss the importance of a benchmark's tasks being designed with real world applications in mind. [16] considers the best benchmarks to be situation-specific, [24] defines an ideal test set as one which reflects real world data, [7] proposes that benchmarks should be adapted to their intended applications, and [25] suggests that benchmarks be designed to fit the diversity of downstream use cases. [78] emphasizes the importance of guaranteeing that tested methods only use information available in a practical setting and recommends checking that a benchmark simulates the envisioned usage.\nEnsuring validity. A frequent concern with benchmarking is the validity of evaluations [54, 77, 67]. In educational testing, [60] outline a framework to ensure validity by providing guidelines for effective evidence collection. [22] outline what and how evidence can be collected and how it should be interpreted for tests \u201cof attributes for which there is no adequate criterion\" [22]. Measures that are used in other fields further include choosing a large test set to promote the statistical significance of results [78] and updating a benchmark over time to prevent developers from overfitting it [7]. [7] also notes that the methods or approaches being evaluated should not be used to create the gold standard dataset.\nPrioritizing score interpretability. [7] highlights that benchmarks are particularly important when a wide variety of tools are available and it is difficult for non-specialists to distinguish between them. Interpretability is important in not only selecting tools, but also deciding between benchmarks themselves. Effective benchmarks must provide transparent information regarding the procedural details of their experiments [18] and goals of the evaluation [10]. They should clearly describe the benchmark's purpose and scope, as these are fundamental to its design and implementation [86]. Regarding scope, [16] states that for environmental quality applications, benchmarks should never be the basis of final decisions. With this in mind, they identify misleading benchmarks as the worst-case scenario. Furthermore, they state that a benchmark should not present its results as absolutes, instead ensuring that its evaluations are understandable inputs for decision makers [16]."}, {"title": "3 Methodology", "content": "Our benchmark assessment consists of 46 criteria based on our literature review and interviews with five primary groups of stakeholders. These groups, who also present the user personas of our assessment, are described in detail in App. A. Through our interview process, we defined a five-stage benchmark lifecycle and identified objectives along it. In this section, we discuss our methodology for identifying stakeholders, developing criteria, and assessing benchmarks. A detailed flow diagram of our methodology can be found in App. G.\nStep 1: Mapping the space. Initially, we surveyed the existing benchmark landscape (Sec. 2). Based on this review, we identified five stakeholder groups who present the user personas of our assessment (App. A). To understand their objectives with respect to benchmarking, we conducted unstructured interviews with representatives of all stakeholder groups, including 20+ policymakers, model developers, benchmark developers, model users, and AI researchers. During this process, we developed a five-stage model of the benchmark lifecycle (Fig. 1 and App. B) and mapped both the benchmarking objectives of the stakeholders and their communicated use cases for a benchmark assessment (App. A).\nStep 2: Translation to criteria. Based on Step 1, we identified tasks and objectives for each stage of the AI benchmark lifecycle and translated them into concrete criteria. We categorized these as: (a) criteria controlled by the benchmark developer where the authors and interviewees reached a normative consensus, (b) criteria controlled by the benchmark developer but context-dependent, difficult for an external party to assess, or both and (c) aspects either outside the benchmark developer's control or requiring further research. The assessment in Sec. 4 is limited to category (a) criteria. We cover considerations in (b) in Sec. 5, and those in (c) in Sec. 8.\nStep 3: Validating the assessment. Initially, three authors independently scored the same benchmark to calibrate the assessment and identify potential misinterpretations of the criteria. We adapted and clarified scoring guidelines (App. J) to address differing interpretations and uncertainties. To validate our assessment, we shared it with members of all stakeholder groups and revised it based on their feedback. Finally, we verified that our assessment, which in itself can be considered a benchmark, met all of our defined criteria, where applicable (App. I.2).\nStep 4: Structuring the assessment. We evaluated 16 FM and 8 non-FM benchmarks. We priori- tized commonly used benchmarks, such as those that were recently reported by model developers [8, 3] and aim to expand the number of assessed benchmarks continuously on our website better- bench.stanford.edu. Since our assessment considers varying information sources (official websites, papers, GitHub repositories published by the benchmark developers) that do not follow a standard structure, we manually evaluated all benchmarks. At least two authors independently reviewed each benchmark. They subsequently had to reach a consensus on the final score and a third reviewer could be called to make the final decision if a consensus could not be reached (this case did not occur).\nStep 5: Scoring. We scored benchmarks on a discrete 0/5/10/15-point scale for each criterion: 15 for fully meeting, 10 for partially meeting, 5 for mentioning without fulfilling, and 0 for neither referencing nor satisfying the criterion. Average scores were calculated for each benchmark lifecycle stage (design, implementation, documentation, and maintenance). An aggregate usability score, representing the weighted average of the implementation, documentation, and maintenance scores, was also introduced (see App. F for scoring details). We consider a mean score of 10 or higher to indicate a reasonably good benchmark for each aggregated scoring category, as it signifies that, on average, the benchmark at least partially fulfills all assessment criteria within the respective category.\nStep 6: Platform for continuous updates. Finally, we develop a supplementary website to continuously publish assessment results using the scoring methodology in App. F, given the rapid development of new AI benchmarks. The website includes a community feedback channel for submitting new AI benchmarks and correcting previously posted scores if benchmarks are updated"}, {"title": "4 Assessment Criteria", "content": "We separate our assessment criteria according to the phase of the benchmark lifecycle during which they would be fulfilled. Although the retirement stage is within the developer's control, we do not include specific criteria for this phase within the current framework, because we cannot assess the retirement of active benchmarks. App. J contains full explanations, justifications, and scoring guidelines for each of the 46 criteria."}, {"title": "4.1 Benchmark Design", "content": "Benchmarks should clearly describe their goals and scope [86, 10, 54]. This includes defining the tested capability or characteristic, describing how the tested capability translates to the benchmark task, and stating how knowing about the tested concept is helpful in real-world applications [54]. These design choices should be informed by considering use cases and user personas for the bench- mark, involving domain experts, and integrating domain literature [83]. Clearly stating how the benchmark is different from related existing AI benchmarks is necessary to help benchmark users decide the applicability of a benchmark to their use case. A benchmark's measurements must be interpretable [16], which requires an informed choice of performance metric(s) and a description of how the benchmark score should or shouldn't be interpreted [48]. Including floors, ceilings, human performance levels, and random performance levels for the chosen metric(s) further assists users in understanding a model's score [34]. If addressing input sensitivity and providing a validated automatic evaluation are possible, these measures enhance a benchmark's robustness and accessibility [34]."}, {"title": "4.2 Benchmark Implementation", "content": "Criteria in the implementation stage focus on the availability of necessary code and infrastructure and the inclusion of key engineering features. To ensure reproducibility and scrutiny [78, 25, 10], a benchmark should provide working evaluation code, and make its evaluation data, prompts, or"}, {"title": "4.3 Benchmark Documentation", "content": "Providing comprehensive and accessible documentation is crucial for the practicability and interpreta- tion of benchmarks [18]. Key information about a benchmark should be readily available and include documentation of benchmark construction processes [54], data collection [88] or test environment design, and its test tasks and their rationale [54]. Clearly documenting evaluation metric(s) and reporting the statistical significance of results is necessary so that users can understand a benchmark's actual signal [4]. To provide context and prevent misinterpretation, developers should document normative assumptions about benchmark properties and discuss the limitations of their benchmark. A benchmark's codebase should contain a requirements file, a quick-start guide or demo code, a description of code file structure and contents, and in-line comments within all relevant files. Having a benchmark's paper accepted at a peer-reviewed venue signals external scrutiny and adherence to certain standards. Lastly, developers should specify the applicable license to provide legal clarity and enable, e.g., commercial use."}, {"title": "4.4 Benchmark Maintenance", "content": "An optimally designed, implemented, and documented benchmark will cease to be useful if it is not maintained. Developers should regularly check code usability and maintain a feedback channel for users to report issues or suggest improvements. Providing contact details of a person responsible for the benchmark facilitates communication and support. Alternatively, if a benchmark is not maintained anymore, authors should include a corresponding statement indicating that the benchmark was retired in any official benchmark artefacts."}, {"title": "5 Other Design Considerations", "content": "This section presents design considerations for benchmark developers that were excluded from our assessment because their appropriateness is context-dependent, they are not easily verifiable, or both. Our aim with this list is to promote conscious design decisions regarding these considerations.\nGeneral vs. specific benchmarks. Benchmark developers must decide whether to prioritize general or abstract knowledge and skills or specific contexts and domains. Broad concept benchmarks may contribute to understanding foundational characteristics of models, but often face challenges in real-world applicability and reliable testing (see Sec. 8).\nDetecting small improvements. Benchmarks should be designed so that a 1% improvement can be reliably detected [34]. As [34] states, \"the more difficult it is to detect small amounts of progress, the more difficult it becomes to make iterative progress on a benchmark.\" Practically, this is likely dependent on evaluation data size and task diversity.\nMulti-modal assessment. As multi-modal models become increasingly common, benchmark de- velopers may want to consider designing tasks to assess the capabilities they want to test across modalities. Additional design considerations for multi-modal assessments include the increased complexity of mapping a tested concept to different modalities and the different output formats of the tested models [92].\nVersioning. Minor updates (e.g., removing faulty prompts) should be clearly indicated via task versioning [13]. Major updates require releasing new benchmark versions, as exemplified by the AgentBench v0.1 and v0.2 releases [52].\nDynamic vs. static benchmarks. Dynamic benchmarks may better address quick saturation (Sec. 8) and contamination (Sec. 8) issues but reduce result comparability and are easier to implement for some tasks (e.g., adding numbers) than others. Static benchmarks, on the other hand, tend to suffer from the issues outlined above.\nGameability. An ideal benchmark is resilient to attempts to boost task performance without im- proving the fundamental capability being tested [7]. Existing benchmarks have been shown to be vulnerable to manipulation [6]. Specific guidelines have been proposed to prevent cheating and ensure evaluations reflect genuine model performance [95].\nPositionality statement. Positionality statements are a reflective account common in social sciences research. In them, researchers acknowledge how their background, experiences, and biases may have influenced their work. If developers believe such factors significantly impacted their benchmark's construction, they may provide a positionality statement for increased context and transparency."}, {"title": "6 Quantitative Results", "content": "In this section, we present our assessment results. Tab. 1 showcases the average scores per benchmark lifecycle stage, showing that for both FM and non-FM benchmarks, the implementation stage tends to be the weakest area, followed by maintenance. All criteria averages are reported in App. E. Some criteria have not been fulfilled by almost any benchmark (e.g., Standardized metadata is included). Notably, both benchmark types are particularly weak for criteria supporting the reproducibility and interpretation of results: benchmarks get an average score of 3.75 on Including a script to replicate results and an average score of 5.62 on Reporting statistical significance.\nWhile individual benchmark or criteria scores are deterministic, we can analyze statistical fluctuations across categories and benchmarks. Fig. 7 compares the design and usability scores of FM and non- FM benchmarks. The overall average design score across all benchmarks is 10.7, and the weighted average usability score is 8.7. The difference in mean design and usability scores between FM and non-FM benchmarks is not statistically significant (95% confidence level), see Fig. 8 in App. D. Furthermore, we find statistically significant correlations between the design and usability scores for FM benchmarks alone and all benchmarks combined at the 95% confidence level (Tab. 2). This"}, {"title": "7 Discussion", "content": "Not all benchmarks are of the same quality. Model developers frequently report performance on benchmarks that vary significantly in quality. For instance, the widely-used MMLU benchmark scored the lowest in our assessment (weighted average: 5.5), while GPQA scored significantly higher (weighted average: 11.0). However, recent communications introducing models like GPT-4 [3], Claude-3 [8], and Gemini [81] report results on both benchmarks without explicitly acknowledging their limitations or quality differences. This practice may be driven by the assumed expectation that reviewers want to see a wide range of metrics and the belief that readers should determine the most relevant metrics for their needs. The lack of clear guidance on AI benchmark quality and limitations may lead to incorrect conclusions about a model's performance, even if developers do not intend to"}, {"title": "8 Open Challenges in AI Benchmarking", "content": "Per the current state of the field, some benchmark issues are not fully addressable by benchmark developer actions and decisions. This section discusses these issues and directs readers, where possible, to resources which cover these open problems in greater depth.\nQuick saturation. Rapid advancements in AI have led to the saturation of many benchmarks. Some benchmarks have been saturated within months of their release [58]. Addressing this issue involves evaluating current model performances and assessing whether the concept has already been solved, and determining if the benchmark can be made challenging given state-of-the-art capabilities of the models tested.\nContamination. In Sec. 4.2, we discuss strategies to mitigate data contamination. However, even when fully adhered to, challenges remain. For example, benchmark developers cannot enforce model developers' use of canary strings to avoid training on benchmark data. Preventing data contamination, particularly in models reliant on large amounts of web-scraped data, is a shared responsibility between benchmark and model developers. [91] offers further description of measures that can be taken on"}, {"title": "9 Limitations", "content": "Our assessment assigns equal weight to all criteria, despite their varying levels of effort required for fulfillment and differing contributions to overall benchmark quality. The scoring system differentiates only four score categories to enable relatively objective evaluation through clear-cut criteria (App. J and App. F), but may miss nuances within each category. For example, a benchmark barely fulfilling a criterion and one almost entirely fulfilling it would receive the same 10-point score. Given the equal weighting and scoring, benchmark developers could potentially \u201cgame\u201d the assessment by focusing on easily fulfilled criteria. However, we believe that even if a developer only implements easy-to-implement criteria, the resulting benchmark will still be of higher quality than one not meeting any criteria, thus fulfilling our work's goal. Furthermore, assessing the construct validity of a benchmark and determining whether its approach to assessing a concept is truly effective would presumably require in-depth analysis by domain experts in the respective fields, which is beyond the scope of this assessment. Instead, we aim to provide benchmark developers with a blueprint for minimum quality assurances. Finally, our framework is intended for public benchmarks and future work is needed to extend it to private ones."}, {"title": "10 Impact Statement", "content": "By releasing the first systematic assessment framework for AI benchmarks, we aim to encourage benchmark developers to construct higher-quality benchmarks and to contribute to community efforts to make AI evaluations more practicable and transparent. Higher-quality benchmarks resulting from the adoption of our framework and checklist can lead to better-informed model selection for downstream tasks, potentially reducing risks and improving outcomes in high-stakes applications. Our living repository of benchmark assessments promotes transparency and comparability, allowing benchmark users to make informed decisions when choosing benchmarks. However, there is a potential risk of misinterpretation of our results; our assessment only provides minimum quality assurances and is not sufficient to assess the suitability of a benchmark for a concrete use case. The outputs of our evaluation do not contain sensitive or harmful content, but users may encounter such content during a benchmark assessment depending on the benchmark's data. While we do not anticipate direct safety risks from releasing our framework, we acknowledge that strict adherence to"}, {"title": "A Stakeholders", "content": "This section details the stakeholders that are involved in benchmark development and use processes.\nBenchmark developers. Benchmark developers are the individuals or teams who create bench- marks from scratch (e.g. BIG-Bench [75]), by expanding on previously developed benchmarks (e.g. MedMNIST v2 [90]), by integrating multiple existing benchmarks (e.g. HELM [48]), or by both expanding upon and integrating other benchmarks (e.g. Decoding Trust [85]). This group's objec- tives are developing benchmarks that accurately and comprehensively assess models' capabilities or safety-critical characteristics and establishing standards for AI system evaluations that facilitate comparisons and drive progress on the specified tasks. There are three use cases for benchmark developers of our assessment, checklist, and website:\n\u2022 They use the checklist to understand best practices and guide their benchmark construction process pre-deployment.\n\u2022 They use the assessment to score their benchmark after constructing it to understand any shortcomings they may address to improve the overall benchmark quality.\n\u2022 They can use the website to find related benchmarks and compare their benchmark quality to those.\nModel developers. Model developers are the individuals or teams who develop AI models for commercial use (e.g. GPT-4 [3]) or non-commercial purposes (e.g. Alpaca [80]). Their objectives in using benchmarks are demonstrating the performance of their models identifying areas for improve- ment which can guide model development and to establish credibility and encourage adoption by showcasing favorable relative performance. There are three use case for model developers of our assessment and website:\n\u2022 They can use the assessment results to decide which benchmarks to report\n\u2022 Model developers can reference our assessment results in their official reporting to indicate quality differences between benchmarks, if applicable\n\u2022 Model developers can use our website to find relevant benchmarks to report for their model\nModel users. Model users are the individuals, organizations, or businesses which use or modify available AI models for various downstream applications (e.g. a company using ChatGPT to provide customer service). Their objective when using benchmark results is making informed decisions regarding which AI models are most suitable for their specific use cases. There are two use case for model users of our assessment and website:\n\u2022 If model developers don't reference our or any similar benchmark quality assessment, model users can refer to our assessment results on the website to understand quality differences in benchmarks reported by model developers.\n\u2022 They can also refer to our benchmark assessment results to decide between two related benchmarks who's results may both be relevant for the model user's application context. If one of these benchmarks has a higher quality, they may decide to prioritize that result based on our assessment.\nAI researchers. Al researchers are individuals or teams studying AI and related fields either at non-profits, within academic institutions, in industry, or independently. One of researchers' objectives is using benchmarks to evaluate the performance of novel AI architectures, training techniques, and approaches, and to compare these to other systems. Additionally, they have the objective of setting research agendas based on the model limitations and weaknesses revealed by benchmarks. There are two use case for Al researchers of our assessment and website:\n\u2022 Based on our website and assessment results, AI researchers may analyze benchmarking practices in more detail to understand challenges of benchmark developers and drive research on open questions in AI evaluations and AI benchmarking more broadly.\n\u2022 They can use our website to understand the overall AI benchmark landscape."}, {"title": "B Benchmark Lifecycle", "content": "Design. During the design stage, a benchmark's purpose, scope, and structure are defined. This requires developers to identify key aspects of an AI system that the benchmark will assess. Based on this decision, they must determine the tasks, datasets, and evaluation metrics which will be used in their benchmark. To inform these decisions, developers consider the requirements of potential users, possibly collaborating with and gathering feedback from these and other stakeholders.\nImplementation. At this stage, the benchmark is constructed and all necessary components are aggregated. Developers collect, process, and (if applicable) annotate the datasets to be used for their tasks. They then create the evaluation scripts which allow models' performance on this data to be measured. So that new models can be evaluated, developers may implement user interfaces and APIs which enable access to and interaction with the benchmark. This stage concludes with the initial testing and validation of benchmark components.\nDocumentation. To facilitate the benchmark's use and interpretation, benchmark developers need to create comprehensive documentation. This includes preparing detailed descriptions of benchmark tasks, datasets, and evaluation metrics. Additionally, developers may provide instructions for how to access, use, and submit to the benchmark. Documenting design decisions, limitations, and potential biases enables stakeholders to make informed decisions regarding benchmark use. Creating resources for running the benchmark, such as quick-start guides, code documentation, and examples or tutorials is an essential step for accessibility.\nMaintenance. Once the benchmark and its documentation are released, developers must conduct regular maintenance to ensure ongoing usability. They may monitor benchmark usage and perfor- mance to identify areas for improvement and track users' compliance with release requirements. Other tasks at this stage include addressing issues or bugs and incorporating user feedback into updates. Developers can regularly update documentation and support materials. Additionally, they can assess the continued relevance and utility of the benchmark by monitoring performance on the benchmark and responding to community feedback.\nRetirement. The final phase of a benchmark's lifecycle is retirement. Benchmarks are phased out or replaced when they become saturated (i.e. model performance reaches the benchmark metric's ceiling), the task studied loses relevance, or better alternatives emerge. During retirement, developers communicate their plan to stakeholders and can provide guidance on transitioning to alternatives. They archive benchmark data, code, and documentation. As a benchmark is retired, developers may share insights gained with the AI community. Finally, they should clearly mark the benchmark as \"retired\" on channels for deployment and platforms publishing its results."}, {"title": "C List of Assessed Benchmakrs", "content": "We evaluate these 16 foundation model benchmarks (alphabetical order):\n\u2022 AgentBench [51]\n\u2022 ARC Challenge [19]\n\u2022 BBQ [64]\n\u2022 BIG-bench [75]\n\u2022 BOLD [26]\n\u2022 Codex HumanEval [17]\n\u2022 Decoding Trust [85]\n\u2022 GPQA [68]\n\u2022 GSM8k [21]\n\u2022 HellaSwag [94]\n\u2022 Machiavelli [63]\n\u2022 MLCommons AI Safety v0.5 [83]\n\u2022 MMLU [33]\n\u2022 MMMU [93]\n\u2022 TruthfulQA [50]\n\u2022 WinoGrande [71]\nWe evaluate these 8 non-foundation model benchmarks (alphabetical order):\n\u2022 ALE [11]\n\u2022 FinRL-Meta [53]\n\u2022 MedMNIST v2 [90]\n\u2022 PDEBench [79]\n\u2022 Procgen [20]\n\u2022 RL Unplugged [31]\n\u2022 SafeBench [89]\n\u2022 Wordcraft [38]"}, {"title": "D Sensitivity Analysis Details", "content": "We show that the difference in mean usability score between FM and non-FM benchmarks in Fig. 8 is not statistically significant using bootstrap resampling at a 95% confidence level."}, {"title": "E Additional Results", "content": "All individual benchmark scoring results, including justifications, can be found on better- bench.stanford.edu."}, {"title": "E.1 Scores per lifecycle Stage", "content": "We show the scores for each benchmark and for each benchmark lifecycle stage as barplots (Design: Fig. 9, implementation: Fig. 10, documentation: Fig. 11, and maintenance Fig. 12). The scores for each benchmark for each individual category can be found on our website, betterbench.stanford.edu. For the bar plots for each stage, the benchmarks are shown in ascending order and marked as FM and non-FM benchmark."}, {"title": "F Scoring", "content": "We evaluate 24 benchmarks based on criteria grouped into category (a) (see Sec. 3), i.e., those controlled by the benchmark developer where the authors and interviewees reached a normative consensus. We use the following discrete point system to score each criteria:\n\u2022 Criteria not acknowledged and not addressed: 0 points\n\u2022 Criteria acknowledged but not addressed: 5 points\n\u2022 Criteria partially addressed: 10 points\n\u2022 Criteria fully addressed: 15 points\n\u2022 Criteria not relevant: n/a"}, {"title": "GMethodology Flow Diagram", "content": "Fig. 13 shows a detailed overview of the steps we took to derive the best practices that formed the basis of our AI benchmark assessment."}, {"title": "H Release Requirements", "content": "1. Benchmark developers acknowledge that our checklist is a minimum quality assurance and not sufficient for high-quality benchmark construction.\n2. Benchmark developers do not attempt to game our assessment, e.g. by just changing the \"code checked\" update on the GitHub repository side without actually checking their code's usability."}, {"title": "I BetterBench Checklist for Benchmark Developers", "content": "In this section, we provide the assessment criteria as a checklist for benchmark developers to use during their benchmark construction process, pre-deployment of the benchmark. If benchmark developers want to list their benchmark on our website, they will also have to submit this checklist. On the website, we will further provide an easy-to-fill-out checklist in LATEXand .doc format that can be easily included as part of any benchmark documentation. In the second subsection, we will also add an example of a filled out checklist assessing BetterBench, which can be seen as a benchmark for benchmarks. Going through the checklist was part of the validation of our methodology, described in Step 4 of the Sec. 3 section."}, {"title": "I.1 Template", "content": "\u2022 Benchmark Design\n\u2610 The tested capability", "N/A\nJustification": "n\u2610 How tested capability or concept translates to benchmark task is described\nYES | NO | N/A\nJustification:\n\u2610 How knowing about the tested concept is helpful in the real world is described.\nYES | NO | N/A\nJustification:\n\u2610 How benchmark score should or shouldn't be interpreted/used is described\nYES | NO | N/A\nJustification:\n\u2610 Domain experts are involved\nYES | NO | N/A\nJustification:\n\u2610 Use cases and/or user personas are described\nYES | NON/A\nJustification:\n\u2610 Domain literature is integrated\nYES | NON/A\nJustification:\n\u2610 Informed performance metric choice\nYES | NO | N/A\nJustification:\n\u2610 Metric floors and ceilings are included\nYES | NON/A\nJustification:\n\u2610 Human performance level is included\nYES | NO | N/A\nJustification:\n\u2610 Random performance level is included\nYES | NO | N/A\nJustification:\n\u2610 Automatic evaluation is possible and validated\nYES | NON/A\nJustification:\n\u2610 Differences to related benchmarks are explained\nYES | NO | N/A\nJustification:\n\u2610 Input sensitivity is addressed\nYES | NO | N"}]}