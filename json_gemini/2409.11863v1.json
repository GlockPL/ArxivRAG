{"title": "Learning Task Planning from Multi-Modal Demonstration for Multi-Stage Contact-Rich Manipulation", "authors": ["Kejia Chen", "Zheng Shen", "Yue Zhang", "Lingyun Chen", "Fan Wu", "Zhenshan Bing", "Sami Haddadin", "Alois Knoll"], "abstract": "Large Language Models (LLMs) have gained popularity in task planning for long-horizon manipulation tasks. To enhance the validity of LLM-generated plans, visual demonstrations and online videos have been widely employed to guide the planning process. However, for manipulation tasks involving subtle movements but rich contact interactions, visual perception alone may be insufficient for the LLM to fully interpret the demonstration. Additionally, visual data provides limited information on force-related parameters and conditions, which are crucial for effective execution on real robots.\nIn this paper, we introduce an in-context learning framework that incorporates tactile and force-torque information from human demonstrations to enhance LLMs' ability to generate plans for new task scenarios. We propose a bootstrapped reasoning pipeline that sequentially integrates each modality into a comprehensive task plan. This task plan is then used as a reference for planning in new task configurations. Real-world experiments on two different sequential manipulation tasks demonstrate the effectiveness of our framework in improving LLMs' understanding of multi-modal demonstrations and enhancing the overall planning performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) [1], [2] and Visual-Language Models (VLMs) [3] have triggered a paradigm shift in the domain of long-horizon task planning. By leveraging the semantic understanding and reasoning capabilities of LLMs, immense progress has been made towards developing task-agnostic high-level task planner [4], [5]. The majority of prior works in LLM-based task planning use in-context learning techniques where carefully designed examples are employed to generate high-level task plans [6]-\n[9], and leverage visual information for skill grounding.\nOn the other hand, learning from demonstrations (LfD) offers an alternative that mitigates the need for prompting examples. LfD can automatically extract example task plans from demonstrations, eliminating the need for human experts to explicitly construct these plans. The key questions in the LfD approach for sequential manipulation planning are: i) how to segment demonstrations into reusable task plans consisting of skill sequences, and ii) how to ground the associated skills, which involves determining the necessary task-agnostic information to make skills executable. Since foundation models are pre-trained on large-scale internet data, it is natural to utilize visual demonstrations [10] or human play videos [11] to address these challenges.\nHowever, visual information alone may be insufficient for perceiving and expressing contact-rich manipulations where the movement of objects is barely observable, yet changes in force contact are significant. For example, in the cable mounting process shown in Fig. 1, humans intuitively stretch the cable to create tension, thereby easing the subsequent insertion. As illustrated in Fig. 1 (b), while camera-based observations (upper) struggle to capture any movement, images from visual-tactile sensors (lower) clearly reveal a consistent pattern of pulling force. Even when the stretching step is identified, visual observation alone struggles to define an appropriate success condition for this step. In such scenarios, force and tactile information play pivotal roles in detecting and understanding such \u201cinvisible\" events. This example leads to a key insight that motivates our work: tactile information is important for both task segmentation, which requires multi-modal semantic reasoning to successfully convert observed long-horizon tasks into a sequence of actions associated with executable robot skills, and skill grounding, which extracts necessary information for chaining and executing the skills, such as task specifications and pre/post-conditions.\nIn this paper, we introduce a novel in-context learning framework that enables pre-trained LLMs to learn task"}, {"title": "II. RELATED WORK", "content": "LLMs as Task Planner. LLMs have increasingly been used for symbolic task planning due to their strong semantic understanding and reasoning abilities [4], though their generated actions aren't always executable. To address this, early studies guided LLM plans using predefined action sets and incorporated pre- and post-conditions for better grounding [12]. Structured approaches like PDDL [9], [13], behavior trees [14], [15], and task trees [16] are now commonly applied to improve plan reliability. Building on this, we also use PDDL for refining a skill library and reasoning about skill sequences. Another approach to enhancing LLM-s/VLMs for task planning is using demonstration datasets. For instance, PaLM-E employs task-and-motion planners to generate extensive planning examples [5]. Other methods use human demonstrations for symbolic task plans but are limited to trajectory-based actions [10]. Overall, these efforts rely on large datasets and are generally restricted to tasks with limited contact interactions.\nLearning Task Planning from Few Demonstrations. Classic methods like Hidden Markov Models and Dynamic Motion Primitives learn task planning from limited demonstrations by segmenting tasks into subtasks and learning structured plans chained by subtasks or primitive actions [17]\u2013[21]. However, these approaches often require extensive feature engineering and struggle to generalize to new tasks. While imitation learning with deep neural networks has made vast progress in learning and generalizing multistage tasks [22]\u2013[25], it suffers from the high cost of collecting large demonstration datasets. Leveraging the multi-modal reasoning capability of pre-trained foundation models, LLM-enabled task planning has improved generalization with fewer demonstrations. For instance, GPT-4-based planners achieve one-shot planning from human demonstrations [10], and hierarchical structures have been proposed to maximize knowledge distillation from these demonstrations [26]. Other methods use task conditions generated by LLMs to guide generalization during execution [12]. Our work extends this line of research by focusing on how LLMs can effectively utilize demonstrations for contact-rich manipulation task planning.\nMulti-Modal Sensory Data and LLMs. Recent advances in multi-modal LLMs, such as GPT-4V [1], have exhibited significant capabilities in scene understanding. However, the current paradigm predominantly focuses on large-scale pretraining for language-conditioned visual representations, while comparatively few studies have investigated other sensory modalities and their potential applications in robotics.\nThe GenCHiP framework allows LLMs to reason about motion and force by exposing constraints on contact stiffness and forces in control interface [27], aiming to automate parameter tuning for contact-rich manipulation tasks. Other efforts enhance LLMs' reasoning with tactile data by finetuning models on specialized datasets that combine paired tactile-visual observations with tactile-semantic labels [28]. Another approach uses contact microphones as tactile sensors to leverage large-scale audio-visual pretraining, addressing the scarcity of non-visual data in low-data robotic applications [29]. In contrast, our approach minimizes the need for large-scale multi-modal datasets by bootstrapping LLMS with a single human demonstration, integrating ViTac images, F/T signals, and standard camera videos."}, {"title": "III. METHODOLOGY", "content": "As shown in Fig. 2, our framework first derives a task plan from the demonstration (highlighted in the orange box), and then uses this plan as a reference for planning new, generalized tasks (highlighted in the green box). Prior to this, we collect ViTac data from the robot's fingers, third-person camera images, and F/T measurements at the robot's endeffector during kinesthetic teaching. In the reasoning stage, in addition to the skill library, we provide the LLM analyzer with each sensory modality in a bootstrapped manner, enabling it to derive from the demonstration a task plan that describes the demonstration as well as a skill library with transition conditions updated. When a new task is requested, the LLM planner uses the demonstration task plan as an example to generate new task plans, adjusting them interactively based on the robots' execution feedback.\nIn the following section, we first introduce the formulation of our object-centric skill library. Next, we explain each stage of the bootstrapped reasoning pipeline, presenting how each modality is integrated. Finally, we describe how the generated task plan is utilized for planning on new tasks.\nA. Object-Centric Skills\nIn contact-rich manipulation tasks, such as assembly, robot actions are typically highly object-centric. That is to say, each robot action is designed to change a certain status of an object, whether by changing its position or applying an"}, {"title": "B. Bootstrapped Reasoning of Demonstration", "content": "Given the large volume and multi-modal nature of our demonstration data, it can be challenging for LLMs to interpret all modalities simultaneously. To address this problem, we adopt a bootstrapped approach for in-context learning of demonstration, where each modality is introduced sequentially to assist the LLM in different stages of reasoning. Tactile information is utilized to segment events from visual perception and identify object statuses, which then enables LLMs to comprehend the entire demonstration and infer the corresponding skill sequence $S_a = (\\xi_1, \\xi_2,...)$. Afterwards, F/T signals are leveraged to ground and refine the transition conditions between skills ensuring that the learned skill sequence is executable and can generalize to new task scenarios by re-planning.\nSegmentation with Object Status. To facilitate skill reasoning from demonstration data, we first segment the entire demonstration into events where object status changes. We rely on tactile status for this segmentation, as tactile information on robot fingers directly precepts the manipulation on target object $O_t$, thus capturing details that are often missed by third-person cameras.\nFig. 4 illustrates four typical patterns of Vi-Tac images, each reflecting different interaction status of the target object: a) Sourcing: Vectors point outward, indicating a source where force radiates outward. This pattern is usually observed when an object is \"grasped\". b) Sinking: Vectors point inward, showing a sink where force converges inward, usually when a grasped object is \"released\". c) Uniform Flow: Vectors are parallel and evenly spaced, representing a consistent force in one direction. This pattern happens when an object is \"under a linear force\", such as being pushed or pulled. d) Twisted Flow: Vectors twist gently, suggesting rotational movement around a central point. This pattern happens when an object is \"under torque\", such as being rotated or screwed.\nTo identify the above tactile patterns in demonstrations, we fine-tune the video classifier TimeSformer [31] with a dataset of labeled tactile videos. Applying this classifier to the complete demonstration then segments it into events when new interaction happens. Object status alongside the timestamps of these events (referred to as key timestamps) are extracted for subsequent reasoning.\nReasoning Skill Sequence After segmentation, we provide the LLM analyzer with camera images taken at key timestamps (referred to as key frames) for skill reasoning. As is shown in Fig. 3 (a), each key frame is annotated with objects as well as the the corresponding status of $O_t$. Additionally, we include in the prompt a simple description of the task (e.g., \"two robots are mounting a cable onto several clips\") and of the objects (e.g., \"the blue curve in the view is the cable\"). The LLM is then asked to reason which skill from the PDDL domain the demonstrator has performed at each key timestamp.\nFig. 3(b) presents an example of the skill sequence reasoned by the LLM. We observe that the LLM adaptively utilizes camera images (for move_object) and object status (for grasp, stretch, and insert) to infer the corresponding skills. In addition, the LLM also fills in the contextual object $O_e$ (env_object) for each skill in its reasoning.\nReasoning Skill Conditions The final step in converting the skill sequence into an executable task plan is to reason about success conditions for each skill. We leverage the precondition and effect in the previously translated PDDL domain. The LLM is requested to implement these conditions and integrate them back into the skill library. Additionally, we provide the LLM with interfaces to access perception information about robot pose, grasping status, and F/T signals. Since most of the other information is either binary or straightforward when used to form conditions (e.g. whether an object is grasped or a position is reached), we focus especially on F/T conditions which are highly variable and crucial for contact-rich manipulations.\nThe raw six-dimensional F/T signals are complex for the LLM to interpret directly. To address this without sacrificing generality, we assume that the task is performed in a static environment where interactions with the object occur exclusively through the robot. In this context, the most relevant F/T information pertains to the force or torque opposing the robot's actions, as they provide direct feedback on the resistance encountered during manipulation. Based on this observation, we reduce our F/T perception interface to include only resistance force $f_r$ and torque $T_r$.\nFor each skill, we first ask the LLM to generate an initial success condition function, in which it determines which signal the condition should be based on (e.g. resistance.torque is used to form the is_tightened condition). We then provide a plot of the selected signal and prompt the LLM to update success condition func-"}, {"title": "C. Planning in New Scenarios", "content": "From the above bootstrapping reasoning, the LLM analyzer has extracted the skill sequences corresponding to human demonstrations, and has extended the original skill library with appropriate success conditions. These outputs are then combined as an demonstration task plan, which will be used as an example for task planning on a new task. As shown in Fig. 2, in the planning request to an LLM planner, we include the demonstration task plan as well as an image and description of the new task scene. To make the plan more dynamic and flexible, we also use the LLM planner to monitor the execution process. After execution of each skill, we feed its return R back to the LLM planner, which then decides whether the plan should be continued or adjusted."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present real-world experiments to evaluate the effectiveness of our demonstration reasoning pipeline and planning results for new tasks. This evaluation is conducted through ablation study: by disabling or replacing a certain parts in our framework, we design the following control groups.\nA. Transition Frames Without Object Status: Key frames in our demonstration reasoning pipeline are replaced by frames at key timestamps but without status annotation.\nB. Uniform Sampled Frames Without Object Status: Similar to (a), but frames are sampled uniformly from video, again without status annotation.\nC. Conditions Without F/T Signals: Force/torque signals are excluded from the demonstration reasoning pipeline, so the success conditions remain as initially generated by the LLM without any updates.\nD. Without Demonstrations: No demonstration data is provided and the LLM generates the plan solely based on its prior knowledge.\nA. Experimental Setup\nSetup We use sigma.7 haptic devices [32] to control two Franka Emika robots for collecting demonstrations. The visual observation is captured by a third-person Intel RealSesne D435 camera. As shown in Fig 1 (a), tactile videos are collected by the GelSight ViTac sensor [33] on robot fingers. F/T signals are collected by Bota Systems SensOne 6D force-torque sensor at the wrist, which also provides the human operator with haptic feedback during demonstration. In the grounding stage, we use GPT-4 Omni as both analyzer and planner for its supreme compability of processing images and videos.\nEvaluation Tasks We evaluate our framework on two sequential manipulation tasks, each presenting distinct challenges for task planning:\n\u2022 Cable mounting task, where a cable needs to be moved and inserted sequentially onto several clips. This task is a common process in industries like car manufacturing. Inspired by how human handles this task, we use a bi-manual robotic setup. One robot (called robot leader) holds the cable and moves it to each clip. The second robot (called robot follower) joins the grasping when the cable reaches a clip, and both robots will perform the insertion together. During demonstration, the operator teaches the robots to mount the cable onto two clips of different types (See Fig. 1(a) and task description in Fig. 3(b)). For evaluation of planning compability, we randomize the number and position of clips as new task configurations.\n\u2022 Cap tightening task, where one robot should attach cap(s) onto bottles. During demonstration, the operator teaches the robot to pick a cap from the desk and tighten it to a target bottle. For evaluation of new task planning, the target bottle has an inner cap and an outer cap with randomized positions (See Fig. 5). The robot is supposed to tighten both caps to the target bottle."}, {"title": "B. Evaluation on Demonstration Reasoning", "content": "Firstly, we evaluate the bootstrap reasoning pipeline in terms of its ability to extract correct skill sequences from demonstrations. We draw a comparison with control group A and B which rely solely on visual information to reason skill sequences. The extracted skill sequence for cable mounting and cap tightening are presented in Fig. 6. While skills involving obvious movements (such as move_object and insert) are successfully recognized by all groups, the LLM struggles to reliably identify skills involving intensive physical interactions (such as grasp and stretch) when only visual data is provided (Fig. 6 (a)). Despite tighten occurring multiple times in the cap tightening demonstration, the control groups manage to identify it only once (Fig. 6"}, {"title": "C. Evaluation on Task Planning", "content": "Finally, we evaluate the plans generated by the LLM for new task configurations. To compare the performance of our framework against all the control groups (a)-(d), we first assess the reasonableness of the generated skill sequences, assuming all skills in the plan are executed successfully. Next, we test the plans on real-world robots, evaluating whether each skill is executed correctly without errors in skill return and whether the entire task is completed successfully. For instance, if the insert success condition fails to detect when the cable has been inserted, the robots may continue pushing blindly, eventually throwing an error due to joint torque limits. In such cases, while the task itself is technically completed (the cable is inserted into the clip), the execution is considered a failure due to the error.\nThe overall performance is calculated as the average of these three criteria. The evaluation results, presented in Table III, show that our framework outperforms all control groups, particularly in terms of task success rate. One example of LLM-generated cable mounting plan as well as the execution process is demonstrated in Fig. 7, which successfully accomplish the assembly task. For more detailed planning results, please refer to our accompanying video and project website."}, {"title": "V. CONCLUSION", "content": "We introduce an in-context learning framework that enables task planning for sequential, contact-rich manipulation tasks using multi-modal demonstration data. This framework leverages tactile and force/torque information to segment and convert the entire demonstration into a task plan, which is then used as a reference to generate more reliable plans for new tasks. For future work, we plan to incorporate language instructions to further enhance LLMs' understanding of demonstrations. Additionally, fine-tuning a VLM to directly interpret tactile and force/torque data presents another promising approach to leverage multi-modal demonstrations."}]}