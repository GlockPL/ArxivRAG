{"title": "Artificial Intelligence and Algorithmic Price Collusion in Two-sided Markets", "authors": ["Cristian Chica", "Yinglong Guo", "Gilad Lerman"], "abstract": "Algorithmic price collusion facilitated by artificial intelligence (AI) algorithms raises significant concerns. We examine how AI agents using Q-learning engage in tacit collusion in two-sided markets. Our experiments reveal that AI-driven platforms achieve higher collusion levels compared to Bertrand competition. Increased network externalities significantly enhance collusion, suggesting AI algorithms exploit them to maximize profits. Higher user heterogeneity and greater utility from outside options generally reduce collusion, while higher discount rates increase it. Tacit collusion remains feasible even at low discount rates. To mitigate collusive behavior and inform potential regulatory measures, we propose incorporating a penalty term in the Q-learning algorithm.", "sections": [{"title": "1 Introduction", "content": "Algorithmic price collusion occurs when economic agents set prices using artificial intelligence (AI) algorithms. Through repeated interactions, these agents learn that tacit collusion is optimal, as noted by Calvano et al. (2020a). Economists and antitrust authorities have expressed significant concerns about this form of collusion. The OECD (2017) raised concerns about pricing algorithms learning to collude via tacit coordination. Assad et al. (2024) suggested that algorithmic pricing in Germany's retail gasoline market increased price margins by approximately 15%. U.S. Senator Amy Klobuchar introduced the S.3686 - Preventing Algorithmic Collusion Act of 2024 to curb anticompetitive behavior through algorithmic pricing using nonpublic competitor data.\nRecent experiments (Calvano et al. (2020b), Klein (2021)) have demonstrated that collusion can be achieved in Bertrand and Stackelberg competition models by simulated economic agents using Q-learning, a benchmark reinforcement learning algorithm. Building on these findings, this study experimentally investigates AI-driven platforms using Q-learning in a repeated two-sided platform competition game. We illustrate how these AI agents facilitate collusion compared to Bertrand competition. Our focus is particularly on the impact of network externalities on collusion.\nIn our model of repeated two-sided platform competition, multiple horizontally differentiated platforms compete to serve buyers and sellers, collectively referred to as users. These platforms repeatedly interact and independently choose prices using Q-learning, with the last period price as the state variable. This implies that platforms have bounded memory and employ one-memory strategies (Barlo et al., 2009). In each repetition, users can choose to join one of the platforms or opt for the outside option. Buyers who join a platform receive network externality benefits proportional to the number of buyers (within-side externalities) and sellers (cross-side externalities) on the same platform. Sellers who join the market receive both types of externalities as well.\nOur experiments show that even with zero network externalities, AI-driven platforms achieve higher collusion levels compared to those reported by Calvano et al. (2020b) for Bertrand competition. This is likely due to the larger action space, which allows more information exchange. Furthermore, increased network externalities lead to significantly high collusion levels, suggesting AI-driven platforms can leverage these externalities to boost profits. In particular, algorithmic pricing can increase collusion in markets with significant positive within-side externalities (e.g., online/cloud gaming) and positive cross-side externalities (e.g., video streaming, social media).\nOur findings indicate that higher user heterogeneity or greater utility from the outside option generally decrease collusion levels, except in certain local regions. In contrast, collusion levels typically rise with higher discount rates, especially in the presence of significant network externalities. Additionally, tacit collusion remains feasible even at very low discount factors. This contrasts with traditional literature on firms' collusion without AI agents, which suggests that collusion is feasible only at high discount factors (Obara and Zincenko, 2017; Tirole, 1988).\nFinally, we propose reducing collusion through a penalty term in the Q-learning algorithm.\nRelated Literature. There is a growing literature on algorithmic price collusion, with a particular emphasis on using numerical simulations to show that Q-learning results in tacit collusion. Waltman and Kaymak (2008) showed that firms using Q-learning in repeated Cournot oligopoly games produce lower quantities than the competitive Nash equilibrium. Calvano et al. (2020b) showed that Q-learning firms choose high prices in repeated Bertrand games and learn strategies consistent with tacit collusion. Similar work was done by Klein (2021) for repeated Stackelberg games. Assad et al. (2024) is the first work that uses real-life data to show that firms may increase price margins with the adoption of algorithmic pricing. Our work extends the numerical understanding of algorithmic pricing, particularly in two-sided markets with network externalities.\nStudies by Johnson et al. (2023) and Brero et al. (2022) on single platforms with AI-driven sellers show how platform-designed rules can promote competition and reduce collusion. Nevertheless, this setting does not apply to ours, where multiple platforms apply AI algorithms.\nOur model of repeated two-sided platform competition uses the model in Chica et al. (2023), which, in turn, builds upon previous models by Chica et al. (2021); Tan and Zhou (2021); White and Weyl (2016). These models analyze network externality effects on equilibrium outputs. Our simulations use their insights to study the impact of these externalities on the collusive levels. Ruhmer (2010) finds that higher cross-side externalities make collusion harder to sustain, when following the model of Armstrong (2006) without AI agents. This is consistent with our numerical results, even though we consider algorithmic pricing and follow the model of Chica et al. (2023).\nTheoretical work in economics on algorithmic price collusion includes Brown and MacKay (2023), which demonstrates that simple pricing algorithms can elevate price levels. Additionally, Arslantas et al. (2024) illustrates how a sophisticated agent can exploit another agent using a naive version of Q-learning, provided the former agent knows the algorithm being used."}, {"title": "2 Review of Our Economic Framework", "content": "We introduce the economics framework used in our experiments. Section 2.1 presents the baseline platform competition game. Section 2.2 extends the latter model to an infinite repeated game."}, {"title": "2.1 The Baseline Platform Competition Game", "content": "The baseline platform competition game consists of two stages. In stage I, a set of horizontally differentiated platforms strategically choose prices to maximize profits. In stage II, given the prices determined by the platforms, users on each of the two sides of the market choose whether to participate or not and if they participate they also choose which platform to join. The solution concept for the baseline game is backward induction. More specifically, N platforms provide service options for users on two sides of a market, buyers and sellers. Users in these two sides of a market are denoted with $k \\in \\{b, s\\}$, where b and s represent buyers and sellers, respectively. These users have N + 1 choices, where N > 2. They can either opt out of the market by choosing the outside option, or join one of the N horizontally differentiated platforms, each one denoted with $i \\in [N] := \\{1, ...,N\\}$. The users on side k opting out of the market receive a deterministic outside option utility $u_k^{(0)} \\in \\mathbb{R}$. The users on side k joining platform $i \\in [N]$ receive a deterministic utility\n$u_k^{(i)} := \\Phi_k(x_b^{(i)}, x_s^{(i)}) - p_k^{(i)},$\nwhere $p_k^{(i)}$ is the price paid by the user to access services provided by the platform i; $x_k^{(i)}$ is the total mass of users on side k joining platform i; and\n$\\Phi_k(x_b^{(i)}, x_s^{(i)}) := \\Phi_{kb}x_b^{(i)} + \\Phi_{ks}x_s^{(i)}$, with $\\Phi_{kb}, \\Phi_{ks} \\in \\mathbb{R}$,\nis the network externality function that captures the network benefits users enjoy by joining platform i. The network externalities are captured by the following linear transformation\n$(\\Phi_b(x_b^{(i)}, x_s^{(i)}), \\Phi_s(x_b^{(i)}, x_s^{(i)})) = \\Phi \\begin{pmatrix} x_b^{(i)} \\\\ x_s^{(i)} \\end{pmatrix}$, where $\\Phi = \\begin{bmatrix} \\Phi_{bb} & \\Phi_{bs} \\\\ \\Phi_{sb} & \\Phi_{ss} \\end{bmatrix}$.\nTo save space, we write $\\Phi = [\\Phi_{bb}, \\Phi_{bs}; \\Phi_{sb}, \\Phi_{ss}]$ when specifying choices of $\\Phi$. The endogenous mass of users on each side of the market subscribed to platform i is denoted by $x^{(i)} := (x_b^{(i)}, x_s^{(i)}) \\in [0,1]^2$ and the mass of users not participating in the market is denoted by $x^{(0)} := (x_b^{(0)}, x_s^{(0)}) \\in [0,1]^2$. Assuming all users have Gumbel-distributed idiosyncratic preferences with parameters $(\\mu_k, \\beta_k)$, $\\mu_k \\in \\mathbb{R}$ and $\\beta_k > 0$, the quantities $x_k^{(i)}$ are determined through a maximization process conducted"}, {"title": "2.2 The Infinite Repeated Game", "content": "The infinite repeated game consists of a sequence of games, where at time $t \\in \\mathbb{N} \\cup \\{0\\}$, platforms and users interact following the rules of the baseline platform competition game, introduced in Section 2.1, and additional ones. At each time step t, we use the same notations as above, but with a subscript t. We assume that users on all sides are myopic, i.e., they make decisions to maximize the utility at current time t by solving (2) which depends solely on the current prices observed in the market. We further assume that platforms compete and act strategically and determine the charged prices to maximize the total discounted future rewards at every step t based on the past market states, which we clarify next after introducing some notation and definitions. Given a discounting rate $\\delta \\in (0,1)$, we define the total discounted future rewards at time t for platform i by\n$r_t^{(i)} = \\sum_{\\tau=0}^{\\infty} \\delta^{\\tau} \\mathbb{E}[\\pi_{t+\\tau}^{(i)}]$, where $\\pi_{\\tau}^{(i)} = \\sum_{k \\in \\{b,s\\}} x_{\\tau,k}^{(i)} p_{\\tau,k}^{(i)}$."}, {"title": "3 Simulation Framework", "content": "We first review the framework of multi-agent reinforcement learning in Section 3.1. We then detail our simulation setting in Section 3.2, building upon the framework developed in Section 3.1."}, {"title": "3.1 Preliminaries: Multi-agent Reinforcement Learning", "content": "Multi-agent reinforcement learning considers N agents interacting in a dynamic environment. At each time $t \\in \\mathbb{N}$, each agent $i \\in [N]$ observes a state $s_t^i \\in S$ and takes an action $a_t^{(i)} \\in A$, based on this observed state and following a policy $\\sigma^{(i)}: S \\rightarrow A$, which could be either deterministic or stochastic. Here, S denotes the state space and A denotes the action space. Let $\\sigma_t = (\\sigma_t^{(1)}, \\sigma_t^{(2)}, ..., \\sigma_t^{(N)})$ and $A_t := (a_t^{(1)}, ..., a_t^{(N)}) = (\\sigma^{(1)}(s_t^{(1)}), ..., \\sigma^{(N)}(s_t^{(N)}))$ denote all policies and actions, respectively, at time t. We denote by $a_t^{(-i)}, p_t^{(-i)}$, and $\\sigma_t^{(-i)}$ the respective vectors of all actions $a_t^{j}$, prices $p_t^{(j)}$, and policies $\\sigma_t^{(j)}$ with $j \\neq i$. The agent collects a reward $\\pi_t^{(i)}$, which is a random variable conditioned on the state $s_t^{(i)}$ and actions $A_t$. The state in the next time, $s_{t+1}^{(i)}$, is a random variable conditioned on the state $s_t^{(i)}$ and the actions $A_t$, taken by all the agents in the current time t. Given a discounting rate $\\delta \\in (0,1)$, at each time, each agent aims to find a policy in order to maximize the following expectation of the total discounted future reward given all observed states at time t:\n$\\sum_{\\tau=0}^{\\infty} \\mathbb{E}_{\\pi, s, \\sigma} [\\delta^{\\tau} \\pi_{t+\\tau}^{(i)} | (s_t, A_t)]$.\nThe expectation is needed due to the randomness in the rewards, the future states, and the future actions of all the agents.\nQ-learning is a classic method for finding the policy that maximizes (8). It uses the Q-function of agent i at state s given an action a, which is defined by\n$Q^{(i)}(s, a, \\sigma^{(-i)}; \\sigma^{(i)}) := \\sum_{\\tau=0}^{\\infty} \\mathbb{E}_{\\pi, s, \\sigma} [\\delta^{\\tau} \\pi_{t+\\tau}^{(i)} | (s_t, a_t) = (s, a), a_{t+1}^{(i)} = \\sigma^{(i)}(s_{t+u}), u \\geq 1, a_{t+\\nu}^{(-i)} = \\sigma^{(-i)}(s_{t+\\nu}), \\nu \\geq 0]$.\nNote that (9) differs from (8) by having agent i follow the given action a at time t instead of the policy $\\sigma^{(i)}$, whereas in both formulations all other agents at times $t, t+1, ...$, and agent i at times t+1,t +2, . . ., follow their policies.\nWe denote an optimal policy for agent i by $\\sigma^{(i)*}$, which is hard to find. Q-learning overcomes this difficulty by carefully estimating the solution $Q^{(i)*} (s_t, a_t; \\sigma^{(-i)})$ to the following Bellman equation\n$Q^{(i)*} (S_t, a_t; \\sigma^{(-i)}) = \\mathbb{E}[\\pi(s_t, A_t)] + \\delta \\max_{a'} \\mathbb{E}_{S_{t+1}} [Q^{(i)*} (s_{t+1}, a'; \\sigma^{(-i)} | A_t]$.\nIt then estimates $\\sigma^{(i)*}$ using the following relationship between $Q^{(i)*}$ and $Q^{(i)*}(s, a; \\sigma^{(-i)}):$\n$\\sigma^{(i)*} (s) = arg \\max_{a} Q^{(i)*} (s, a; \\sigma^{(-i)})$.\nWe detail the methods for estimating the Q*-function in (10) in the following section."}, {"title": "3.2 The Simulation Setup", "content": "We consider a market with two platforms, that is, we set N = 2. At time t, each platform $i \\in \\{1,2\\}$ observes the following state $s_t^{(i)} := p_{t-1}$, which contains prices at the previous step. Platform i determines its prices $p_t^{(i)}$ based on the observed state $s_t^{(i)}$. At each time t, after all platforms have chosen prices $p_t^{(i)}$, they receive the reward $\\pi_t^{(i)} = \\sum_{k \\in \\{b,s\\}} x_{t,k}^{(i)}p_{t,k}^{(i)}$, where $x_{t,k}^{(i)}$ is solved using (2).\nTo simplify the computation, we allow platforms to choose from a discrete set of M prices. While it is common to expect that $p_b^C < p_s^C$, our model also allows the case $p_b^* < p_s^*$. We further introduce the parameter $\\varepsilon = 0.1$ so the lowest price is slightly lower than $\\min(p_b^*, p_s^*)$ and the highest one is slightly higher than $\\max(p_b^*, p_s^*)$. For each $k \\in \\{b,s\\}$, our set of prices is\n$P_k := \\{ p_k^* - \\varepsilon (p_k^* - p_k^C) + \\frac{j}{M-1} (1+2\\varepsilon)(p_k^* - p_k^C) | j = 0,...,M-1 \\}$.\nNote that the cardinality of the price space $|P_k|$ is M for both k = b and k = s. The overall state space (for both platforms) and the action space for each platform are respectively defined by\n$S := (P_b \\times P_s) \\times (P_b \\times P_s)$ and $A := P_b \\times P_s$.\nWe note that the size of the state space is $|S| = M^4$ and the size of the action space is $|A| = M^2$.\nPlatform policy: We denote the estimation at time t of $Q^{(i)*} (s, a; \\sigma^{(-i)})$ by $Q_t^{(i)}(s, a)$, where $s \\in S, a \\in A$ and $i \\in \\{1,2\\}$ indexes the platform. Q-learning alternately estimates $Q_t^{(i)}$ and the stochastic policies at time t. We first assume that $Q_t^{(i)}$ is known and show how the platforms determine the stochastic policy at time t. We then explain the Q-learning estimation of $Q_t^{(i)}$. Instead of directly computing the policy as the maximum value in (11), Q-learning computes a softmax value using a temperature parameter $T_t$. For this purpose, at time t and given a state $s_t^{(i)} \\in S$ and the Q*-function estimate, $Q_t^{(i)}$, the policy of platform i is the Boltzmann probability distribution:\n$P(a_t^{(i)} = a|s_t^{(i)}) = \\frac{\\exp \\Big( Q_t^{(i)}(s_t^{(i)}, a) / T_t \\Big)}{\\sum_{a' \\in A} \\exp \\Big( Q_t^{(i)}(s_t^{(i)}, a') / T_t \\Big)}.$\nWe remark that all platforms independently determine their prices based on (14).\nQ-learning estimation: At each time step, after determining the price following (14), platform i collects the reward $\\pi_t^{(i)}$ defined by (7). Next, platform i updates the estimated values of the Q*-function at the given state $s_t$ and the selected action $a_t^{(i)}$ with a learning rate $\\alpha$ as follows:\n$Q_{t+1}^{(i)}(s_t, a_t) := (1 - \\alpha) Q_t^{(i)}(s_t, a_t) + \\alpha \\Big( \\pi_t^{(i)} + \\delta \\max_{a'} Q_t^{(i)}(s_{t+1}, a') \\Big)$.\nWe remark that (15) is an approximation of (10) (see Watkins and Dayan (1992))."}, {"title": "4 Experimental Results", "content": "We report extensive numerical experiments using the setup of Section 3.2. Section 4.1 investigates the general dependence of $\\overline{\\Delta}$, defined in (18), on the externality matrix $\\Phi$. Section 4.2 further explores the latter dependence for concrete and useful choices of $\\Phi$. Section 4.3 studies the dependence of $\\overline{\\Delta}$ on the degree of heterogeneity in users' tastes, the outside option utility, and the discount rate. Lastly, Section 4.4 explores two interesting scenarios: 1) long-run asymmetric equilibria outperform the symmetric equilibrium, and 2) competition prices are larger than collusion prices for one side of the market. A supplementary analysis in Appendix A.1 implies that our numerical results are consistent with platforms learning tacit collusion and equilibrium strategies."}, {"title": "4.1 Dependence of the Collusive Level on the Network Externalities", "content": "We applied an additive model to infer the dependence of the collusive level, $\\overline{\\Delta}$, on the externality matrix, $\\Phi$. We ran 2,500 simulations according to the setting described in Section 3.2. For each simulation, we randomly sampled the elements of the externality matrix $\\Phi$ from independent normalized Gaussians (that is, $\\phi_{kl} \\sim \\mathcal{N}(0, 1)$ for $k,l \\in \\{b, s\\}$), and recorded the final collusive level, $\\Delta$. In order to infer the dependence of $\\Delta$ on $\\Phi$, we assume the following additive model:\n$\\Delta(\\Phi) = \\Delta_0 + f_{bb}(\\phi_{bb}) + f_{ss}(\\phi_{ss}) + f_{bs}(\\phi_{bs}) + f_{sb}(\\phi_{sb}) + f_{bb,ss}(\\phi_{bb}, \\Phi_{ss}) + f_{sb,bs}(\\phi_{sb}, \\Phi_{bs}) + f_{bb,bs}(\\phi_{bb}, \\phi_{bs}) + f_{bb,sb}(\\phi_{bb}, \\phi_{sb}) + f_{ss,bs}(\\phi_{ss}, \\phi_{bs}) + f_{ss,sh}(\\phi_{ss}, \\phi_{sb}) + \\varepsilon,$\nwhere $\\Delta_0$ is the sample mean of $\\Delta$, the next 4 functions $(f_{bb}, f_{ss}, f_{bs}, f_{sb})$ represent the univariate effects of the elements of $\\Phi$ on $\\Delta$, the last 6 functions $(f_{bb,ss}, f_{sb,bs}, f_{bb,bs}, f_{bb,sb}, f_{ss,bs}, f_{ss,sb})$ represent the bivariate effects of the elements of $\\Phi$ on $\\Delta$ and $\\varepsilon$ is an error term, encompassing higher-order multivariate effects. Since the equilibrium values $\\pi^*$ and $\\pi^C$ depend on $\\Phi$ nonlinearly (see Section 2.1), the 10 functions, $f_{bb}, ..., f_{ss,sb}$, are nonlinear. We thus sequentially fit these functions using XGBoost (Chen and Guestrin, 2016), which is a popular non-parametric, nonlinear fitting method. To reduce the bias of the fitted functions, we alter the order of both the first four functions and the next six functions, during the sequential fitting procedure, and average the collusive level over the different orders. Appendix A.2 contains more details of implementing XGBoost.\nWe refer to $\\Delta_0$ as the baseline collusive level, whereas $\\Delta$ is the collusive level. Our simulations show that $\\Delta_0$ is approximately 0.3. Next, we report our estimates for the univariate and bivariate effects of the elements of $\\Phi$ on $\\Delta$."}, {"title": "4.2 A Study of the Collusive Level under Special Network Externalities", "content": "We assume special parameterizations of the network externality matrices, $\\Phi$, and explore the dependence of $\\Delta$ on any such $\\Phi$. This allows us to track more carefully the dependence of $\\Delta$ on $\\Phi$ in some special settings. For each specific $\\Phi$, we ran 100 simulations. Our figures present the dependence of the overall collusive level on the elements of $\\Phi$, where their main curves represent the average of the collusive levels from the 100 runs and their shaded areas represent the uncertainty level, which was computed using bootstrapping with a 99% confidence interval.\nFigure 6 investigates the dependence of the collusive level on the within-side externalities in two controlled settings. In the first setting (left panel) $\\Phi = [\\phi_{bb}, 0; 0, 0]$, and in the second one (right panel) $\\Phi = [\\phi_{bb}, 0; 0, \\phi_{bb}]$. In both cases $\\phi_{bb} \\in [-6, 2]$.\nFigure 7 investigates the dependence of the collusive level on the cross-side externalities in two controlled settings. In the first setting (left panel) $\\Phi = [0, \\phi_{bs}; 0, 0]$, and in the second one (right panel) $\\Phi = [0, \\phi_{bs}; \\phi_{bs}, 0]$. In both cases $\\phi_{bs} \\in [-2.5, 3]$.\nFigure 8 investigates the dependence of the collusive level on the bivariate effects between the within- and cross-side externalities in two controlled settings. For simplicity, we fix the cross-side externality and vary the within-side externality. In the first setting (left panel) $\\Phi = [\\phi_{bb}, 3; 0, 0]$, and in the second one (right panel) $\\Phi = [\\phi_{bb}, 3; 0, \\phi_{bb}]$. In both cases $\\phi_{bb} \\in [-6, 2]$."}, {"title": "4.3 A Study of the Collusive Level under Special Market Parameters", "content": "We explore the dependence of the collusive level on the market parameters $\\beta_k, u_k^{(0)}$ and $\\delta$. In each experiment, we fix two of the latter parameters, using the setup described in Section 3.2, and the matrix $\\Phi$, where its choices change with the experiments, and vary the remaining parameter. For each experiment, we ran 100 simulations, averaged the collusive levels among the 100 runs and computed the uncertainty levels using bootstrapping with a 99% confidence interval. Our figures present the averaged collusive level as a function of one parameter, where the shaded areas represent the uncertainty level."}, {"title": "4.4 Discussion of Exceptional Cases", "content": "We numerically demonstrate two exceptional and uncommon scenarios: asymmetric optimal prices and competition prices larger than collusion prices.\nAsymmetric collusion. Our metric for the collusive level compares the platform's rewards at time t with the symmetric equilibrium quantities $\\pi^*$ and $\\pi^C$, following previous simulations of collusion (see, e.g., Calvano et al. (2020b) and Klein (2021)). However, our specific model may give rise to asymmetric equilibria and we thus study their possibility more carefully.\nTo allow asymmetric equilibria, we modify the definitions of the maximum values of the total profits and the collusive level as follows. The total profit, $\\Pi_a$, is\n$\\Pi_a(p^{(1)}, p^{(2)}) := \\sum_{i=1}^{2} (x_b^{(i)} p_b^{(i)} + x_s^{(i)} p_s^{(i)}) = \\sum_{i=1}^{2} \\pi^{(i)},$\nwhere unlike (4) it does not assume symmetric prices (that is, it does not assume that $\\pi^{(1)} = \\pi^{(2)}$) and its subscript a indicates asymmetry. Similarly, the collusive level $\\Delta_t$ is averaged among the two firms as follows\n$\\Delta_t := (\\Delta_t^{(1)} + \\Delta_t^{(2)})/2 = \\frac{\\Pi_a - 2 \\pi^*}{2(\\pi^C - \\pi^*)},$ where $\\Delta_t^{(i)}, i \\in \\{1,2\\}$, was defined in (17).\nFigure 12 demonstrates the maximum value that $\\Delta_t$ can achieve in two controlled settings. In the first setting (left panel) $\\Phi = [\\phi_{bb}, \\phi_{bs}; \\phi_{bs}, \\phi_{bb}]$ with $\\phi_{bb} \\in [-1, 1]$ and $\\phi_{bs} \\in [-4, 4]$, and in the second one (right panel) $\\Phi = [\\phi_{bb}, \\phi_{bs}; -\\phi_{bs}, \\phi_{bb}]$ with $\\phi_{bb} \\in [-2, 2]$ and $\\phi_{bs} \\in [-8, 8]$. More precisely, we maximize $\\Delta_t$ over $p^{(1)} \\in A$ and $p^{(2)} \\in A$, where A was defined in (13), and present the maximal values using a heatmap, whose values vary from purple ($\\Delta_t$ greater than 1) to orange ($\\Delta_t$ less than 1). In both panels, $\\beta_k = 1.0$ and $u_k^{(0)} = -2.0, k \\in \\{b,s\\}$.\nIn the left panel, the maximal $\\Delta_t$ exceeds 1 when the within-side externality $\\phi_{bb}$ is positive and close to 1 and the cross-side externality $\\phi_{bs}$ is sufficiently negative. We note that it is much larger than 1. In the right panel, the maximal $\\Delta_t$ is larger than 1 when $\\phi_{bb}$ is sufficiently large and $\\phi_{bs}$ is close to zero. We note though that it never exceeds the value of 1.0175. In both panels, for all other corresponding values of $\\phi_{bb}$ and $\\phi_{bs}$, the maximum value of $\\Delta_t$ is either achieved at a symmetric vector price or at an asymmetric vector price with corresponding maximum value close to 1. In the latter case, of the corresponding maximum value close to 1, our measure of collusive level $\\Delta$ using the symmetric assumption can still be used to quantify the level of collusion. That is, one may often follow up our analysis with the symmetric quantities, except for special cases where the estimated collusive level exceeds one, where one needs to use the asymmetric quantities.\nFigure 13: Collusive level with varying $\\phi_{sb}$, where $\\Phi = [0, \\phi_{sb}; \\phi_{sb}, 0]$.\nCompetition prices larger than collusion prices. We recall that both the simulated action space presented in (12) and the formulated baseline model in Section 2.1 allow for the competition prices $(p^*)$ to be either smaller or larger than the collusion prices $(p^C)$. We demonstrate here an uncommon situation where the collusion price can be smaller than the competition price for one side of the market. In this example $\\Phi = [1, -\\phi_{sh}; \\phi_{sb}, -2]$, $\\beta_k = 0.5$ and $u_k^{(0)} = -1.0, k \\in \\{b,s\\}$.\nIf $\\phi_{sb} \\in [-5, 0.5)$, then $p_s^* < p_s^C$. If $\\phi_{sb} \\in (0.5, 5]$, then $p_s^* > p_s^C$. The left panel of Figure 14 demonstrates the competition and collusion prices of both sides of the market. For side b, the collusion price is always higher than the competition price. However, for side s, there are two"}, {"title": "5 Economic and Policy Discussion", "content": "We examine the economic implications of the numerical results presented in Section 4. Specifically, we analyze the impact of network externalities on the collusive level and discuss the ramifications of these findings on real-life markets. We then review the Preventing Algorithmic Collusion Act of 2024, introduced by U.S. Senator Amy Klobuchar, and suggest an additional policy recommendation to ensure the safer use of Q-learning."}, {"title": "5.1 Economic Discussion", "content": "When $\\Phi = 0$", "output": "json\n{"}, {"title": "Artificial Intelligence and Algorithmic Price Collusion in Two-sided Markets", "authors": ["Cristian Chica", "Yinglong Guo", "Gilad Lerman"], "abstract": "Algorithmic price collusion facilitated by artificial intelligence (AI) algorithms raises significant concerns. We examine how AI agents using Q-learning engage in tacit collusion in two-sided markets. Our experiments reveal that AI-driven platforms achieve higher collusion levels compared to Bertrand competition. Increased network externalities significantly enhance collusion, suggesting AI algorithms exploit them to maximize profits. Higher user heterogeneity and greater utility from outside options generally reduce collusion, while higher discount rates increase it. Tacit collusion remains feasible even at low discount rates. To mitigate collusive behavior and inform potential regulatory measures, we propose incorporating a penalty term in the Q-learning algorithm.", "sections": [{"title": "1 Introduction", "content": "Algorithmic price collusion occurs when economic agents set prices using artificial intelligence (AI) algorithms. Through repeated interactions, these agents learn that tacit collusion is optimal, as noted by Calvano et al. (2020a). Economists and antitrust authorities have expressed significant concerns about this form of collusion. The OECD (2017) raised concerns about pricing algorithms learning to collude via tacit coordination. Assad et al. (2024) suggested that algorithmic pricing in Germany's retail gasoline market increased price margins by approximately 15%. U.S. Senator Amy Klobuchar introduced the S.3686 - Preventing Algorithmic Collusion Act of 2024 to curb anticompetitive behavior through algorithmic pricing using nonpublic competitor data.\nRecent experiments (Calvano et al. (2020b), Klein (2021)) have demonstrated that collusion can be achieved in Bertrand and Stackelberg competition models by simulated economic agents using Q-learning, a benchmark reinforcement learning algorithm. Building on these findings, this study experimentally investigates AI-driven platforms using Q-learning in a repeated two-sided platform competition game. We illustrate how these AI agents facilitate collusion compared to Bertrand competition. Our focus is particularly on the impact of network externalities on collusion.\nIn our model of repeated two-sided platform competition, multiple horizontally differentiated platforms compete to serve buyers and sellers, collectively referred to as users. These platforms repeatedly interact and independently choose prices using Q-learning, with the last period price as the state variable. This implies that platforms have bounded memory and employ one-memory strategies (Barlo et al., 2009). In each repetition, users can choose to join one of the platforms or opt for the outside option. Buyers who join a platform receive network externality benefits proportional to the number of buyers (within-side externalities) and sellers (cross-side externalities) on the same platform. Sellers who join the market receive both types of externalities as well.\nOur experiments show that even with zero network externalities, AI-driven platforms achieve higher collusion levels compared to those reported by Calvano et al. (2020b) for Bertrand competition. This is likely due to the larger action space, which allows more information exchange. Furthermore, increased network externalities lead to significantly high collusion levels, suggesting AI-driven platforms can leverage these externalities to boost profits. In particular, algorithmic pricing can increase collusion in markets with significant positive within-side externalities (e.g., online/cloud gaming) and positive cross-side externalities (e.g., video streaming, social media).\nOur findings indicate that higher user heterogeneity or greater utility from the outside option generally decrease collusion levels, except in certain local regions. In contrast, collusion levels typically rise with higher discount rates, especially in the presence of significant network externalities. Additionally, tacit collusion remains feasible even at very low discount factors. This contrasts with traditional literature on firms' collusion without AI agents, which suggests that collusion is feasible only at high discount factors (Obara and Zincenko, 2017; Tirole, 1988).\nFinally, we propose reducing collusion through a penalty term in the Q-learning algorithm.\nRelated Literature. There is a growing literature on algorithmic price collusion, with a particular emphasis on using numerical simulations to show that Q-learning results in tacit collusion. Waltman and Kaymak (2008) showed that firms using Q-learning in repeated Cournot oligopoly games produce lower quantities than the competitive Nash equilibrium. Calvano et al. (2020b) showed that Q-learning firms choose high prices in repeated Bertrand games and learn strategies consistent with tacit collusion. Similar work was done by Klein (2021) for repeated Stackelberg games. Assad et al. (2024) is the first work that uses real-life data to show that firms may increase price margins with the adoption of algorithmic pricing. Our work extends the numerical understanding of algorithmic pricing, particularly in two-sided markets with network externalities.\nStudies by Johnson et al. (2023) and Brero et al. (2022) on single platforms with AI-driven sellers show how platform-designed rules can promote competition and reduce collusion. Nevertheless, this setting does not apply to ours, where multiple platforms apply AI algorithms.\nOur model of repeated two-sided platform competition uses the model in Chica et al. (2023), which, in turn, builds upon previous models by Chica et al. (2021); Tan and Zhou (2021); White and Weyl (2016). These models analyze network externality effects on equilibrium outputs. Our simulations use their insights to study the impact of these externalities on the collusive levels. Ruhmer (2010) finds that higher cross-side externalities make collusion harder to sustain, when following the model of Armstrong (2006) without AI agents. This is consistent with our numerical results, even though we consider algorithmic pricing and follow the model of Chica et al. (2023).\nTheoretical work in economics on algorithmic price collusion includes Brown and MacKay (2023), which demonstrates that simple pricing algorithms can elevate price levels. Additionally, Arslantas et al. (2024) illustrates how a sophisticated agent can exploit another agent using a naive version of Q-learning, provided the former agent knows the algorithm being used."}, {"title": "2 Review of Our Economic Framework", "content": "We introduce the economics framework used in our experiments. Section 2.1 presents the baseline platform competition game. Section 2.2 extends the latter model to an infinite repeated game."}, {"title": "2.1 The Baseline Platform Competition Game", "content": "The baseline platform competition game consists of two stages. In stage I, a set of horizontally differentiated platforms strategically choose prices to maximize profits. In stage II, given the prices determined by the platforms, users on each of the two sides of the market choose whether to participate or not and if they participate they also choose which platform to join. The solution concept for the baseline game is backward induction. More specifically, N platforms provide service options for users on two sides of a market, buyers and sellers. Users in these two sides of a market are denoted with $k \\in \\{b, s\\}$, where b and s represent buyers and sellers, respectively. These users have N + 1 choices, where N > 2. They can either opt out of the market by choosing the outside option, or join one of the N horizontally differentiated platforms, each one denoted with $i \\in [N] := \\{1, ...,N\\}$. The users on side k opting out of the market receive a deterministic outside option utility $u_k^{(0)} \\in \\mathbb{R}$. The users on side k joining platform $i \\in [N]$ receive a deterministic utility\n$u_k^{(i)} := \\Phi_k(x_b^{(i)}, x_s^{(i)}) - p_k^{(i)},$\nwhere $p_k^{(i)}$ is the price paid by the user to access services provided by the platform i; $x_k^{(i)}$ is the total mass of users on side k joining platform i; and\n$\\Phi_k(x_b^{(i)}, x_s^{(i)}) := \\Phi_{kb}x_b^{(i)} + \\Phi_{ks}x_s^{(i)}$, with $\\Phi_{kb}, \\Phi_{ks} \\in \\mathbb{R}$,\nis the network externality function that captures the network benefits users enjoy by joining platform i. The network externalities are captured by the following linear transformation\n$(\\Phi_b(x_b^{(i)}, x_s^{(i)}), \\Phi_s(x_b^{(i)}, x_s^{(i)})) = \\Phi \\begin{pmatrix} x_b^{(i)} \\\\ x_s^{(i)} \\end{pmatrix}$, where $\\Phi = \\begin{bmatrix} \\Phi_{bb} & \\Phi_{bs} \\\\ \\Phi_{sb} & \\Phi_{ss} \\end{bmatrix}$.\nTo save space, we write $\\Phi = [\\Phi_{bb}, \\Phi_{bs}; \\Phi_{sb}, \\Phi_{ss}]$ when specifying choices of $\\Phi$. The endogenous mass of users on each side of the market subscribed to platform i is denoted by $x^{(i)} := (x_b^{(i)}, x_s^{(i)}) \\in [0,1]^2$ and the mass of users not participating in the market is denoted by $x^{(0)} := (x_b^{(0)}, x_s^{(0)}) \\in [0,1]^2$. Assuming all users have Gumbel-distributed idiosyncratic preferences with parameters $(\\mu_k, \\beta_k)$, $\\mu_k \\in \\mathbb{R}$ and $\\beta_k > 0$, the quantities $x_k^{(i)}$ are determined through a maximization process conducted"}, {"title": "2.2 The Infinite Repeated Game", "content": "The infinite repeated game consists of a sequence of games, where at time $t \\in \\mathbb{N} \\cup \\{0\\}$, platforms and users interact following the rules of the baseline platform competition game, introduced in Section 2.1, and additional ones. At each time step t, we use the same notations as above, but with a subscript t. We assume that users on all sides are myopic, i.e., they make decisions to maximize the utility at current time t by solving (2) which depends solely on the current prices observed in the market. We further assume that platforms compete and act strategically and determine the charged prices to maximize the total discounted future rewards at every step t based on the past market states, which we clarify next after introducing some notation and definitions. Given a discounting rate $\\delta \\in (0,1)$, we define the total discounted future rewards at time t for platform i by\n$r_t^{(i)} = \\sum_{\\tau=0}^{\\infty} \\delta^{\\tau} \\mathbb{E}[\\pi_{t+\\tau}^{(i)}]$, where $\\pi_{\\tau}^{(i)} = \\sum_{k \\in \\{b,s\\}} x_{\\tau,k}^{(i)} p_{\\tau,k}^{(i)}$."}, {"title": "3 Simulation Framework", "content": "We first review the framework of multi-agent reinforcement learning in Section 3.1. We then detail our simulation setting in Section 3.2, building upon the framework developed in Section 3.1."}, {"title": "3.1 Preliminaries: Multi-agent Reinforcement Learning", "content": "Multi-agent reinforcement learning considers N agents interacting in a dynamic environment. At each time $t \\in \\mathbb{N}$, each agent $i \\in [N]$ observes a state $s_t^i \\in S$ and takes an action $a_t^{(i)} \\in A$, based on this observed state and following a policy $\\sigma^{(i)}: S \\rightarrow A$, which could be either deterministic or stochastic. Here, S denotes the state space and A denotes the action space. Let $\\sigma_t = (\\sigma_t^{(1)}, \\sigma_t^{(2)}, ..., \\sigma_t^{(N)})$ and $A_t := (a_t^{(1)}, ..., a_t^{(N)}) = (\\sigma^{(1)}(s_t^{(1)}), ..., \\sigma^{(N)}(s_t^{(N)}))$ denote all policies and actions, respectively, at time t. We denote by $a_t^{(-i)}, p_t^{(-i)}$, and $\\sigma_t^{(-i)}$ the respective vectors of all actions $a_t^{j}$, prices $p_t^{(j)}$, and policies $\\sigma_t^{(j)}$ with $j \\neq i$. The agent collects a reward $\\pi_t^{(i)}$, which is a random variable conditioned on the state $s_t^{(i)}$ and actions $A_t$. The state in the next time, $s_{t+1}^{(i)}$, is a random variable conditioned on the state $s_t^{(i)}$ and the actions $A_t$, taken by all the agents in the current time t. Given a discounting rate $\\delta \\in (0,1)$, at each time, each agent aims to find a policy in order to maximize the following expectation of the total discounted future reward given all observed states at time t:\n$\\sum_{\\tau=0}^{\\infty} \\mathbb{E}_{\\pi, s, \\sigma} [\\delta^{\\tau} \\pi_{t+\\tau}^{(i)} | (s_t, A_t)]$.\nThe expectation is needed due to the randomness in the rewards, the future states, and the future actions of all the agents.\nQ-learning is a classic method for finding the policy that maximizes (8). It uses the Q-function of agent i at state s given an action a, which is defined by\n$Q^{(i)}(s, a, \\sigma^{(-i)}; \\sigma^{(i)}) := \\sum_{\\tau=0}^{\\infty} \\mathbb{E}_{\\pi, s, \\sigma} [\\delta^{\\tau} \\pi_{t+\\tau}^{(i)} | (s_t, a_t) = (s, a), a_{t+1}^{(i)} = \\sigma^{(i)}(s_{t+u}), u \\geq 1, a_{t+\\nu}^{(-i)} = \\sigma^{(-i)}(s_{t+\\nu}), \\nu \\geq 0]$.\nNote that (9) differs from (8) by having agent i follow the given action a at time t instead of the policy $\\sigma^{(i)}$, whereas in both formulations all other agents at times $t, t+1, ...$, and agent i at times t+1,t +2, . . ., follow their policies.\nWe denote an optimal policy for agent i by $\\sigma^{(i)*}$, which is hard to find. Q-learning overcomes this difficulty by carefully estimating the solution $Q^{(i)*} (s_t, a_t; \\sigma^{(-i)})$ to the following Bellman equation\n$Q^{(i)*} (S_t, a_t; \\sigma^{(-i)}) = \\mathbb{E}[\\pi(s_t, A_t)] + \\delta \\max_{a'} \\mathbb{E}_{S_{t+1}} [Q^{(i)*} (s_{t+1}, a'; \\sigma^{(-i)} | A_t]$.\nIt then estimates $\\sigma^{(i)*}$ using the following relationship between $Q^{(i)*}$ and $Q^{(i)*}(s, a; \\sigma^{(-i)}):$\n$\\sigma^{(i)*} (s) = arg \\max_{a} Q^{(i)*} (s, a; \\sigma^{(-i)})$.\nWe detail the methods for estimating the Q*-function in (10) in the following section."}, {"title": "3.2 The Simulation Setup", "content": "We consider a market with two platforms, that is, we set N = 2. At time t, each platform $i \\in \\{1,2\\}$ observes the following state $s_t^{(i)} := p_{t-1}$, which contains prices at the previous step. Platform i determines its prices $p_t^{(i)}$ based on the observed state $s_t^{(i)}$. At each time t, after all platforms have chosen prices $p_t^{(i)}$, they receive the reward $\\pi_t^{(i)} = \\sum_{k \\in \\{b,s\\}} x_{t,k}^{(i)}p_{t,k}^{(i)}$, where $x_{t,k}^{(i)}$ is solved using (2).\nTo simplify the computation, we allow platforms to choose from a discrete set of M prices. While it is common to expect that $p_b^C < p_s^C$, our model also allows the case $p_b^* < p_s^*$. We further introduce the parameter $\\varepsilon = 0.1$ so the lowest price is slightly lower than $\\min(p_b^*, p_s^*)$ and the highest one is slightly higher than $\\max(p_b^*, p_s^*)$. For each $k \\in \\{b,s\\}$, our set of prices is\n$P_k := \\{ p_k^* - \\varepsilon (p_k^* - p_k^C) + \\frac{j}{M-1} (1+2\\varepsilon)(p_k^* - p_k^C) | j = 0,...,M-1 \\}$.\nNote that the cardinality of the price space $|P_k|$ is M for both k = b and k = s. The overall state space (for both platforms) and the action space for each platform are respectively defined by\n$S := (P_b \\times P_s) \\times (P_b \\times P_s)$ and $A := P_b \\times P_s$.\nWe note that the size of the state space is $|S| = M^4$ and the size of the action space is $|A| = M^2$.\nPlatform policy: We denote the estimation at time t of $Q^{(i)*} (s, a; \\sigma^{(-i)})$ by $Q_t^{(i)}(s, a)$, where $s \\in S, a \\in A$ and $i \\in \\{1,2\\}$ indexes the platform. Q-learning alternately estimates $Q_t^{(i)}$ and the stochastic policies at time t. We first assume that $Q_t^{(i)}$ is known and show how the platforms determine the stochastic policy at time t. We then explain the Q-learning estimation of $Q_t^{(i)}$. Instead of directly computing the policy as the maximum value in (11), Q-learning computes a softmax value using a temperature parameter $T_t$. For this purpose, at time t and given a state $s_t^{(i)} \\in S$ and the Q*-function estimate, $Q_t^{(i)}$, the policy of platform i is the Boltzmann probability distribution:\n$P(a_t^{(i)} = a|s_t^{(i)}) = \\frac{\\exp \\Big( Q_t^{(i)}(s_t^{(i)}, a) / T_t \\Big)}{\\sum_{a' \\in A} \\exp \\Big( Q_t^{(i)}(s_t^{(i)}, a') / T_t \\Big)}.$\nWe remark that all platforms independently determine their prices based on (14).\nQ-learning estimation: At each time step, after determining the price following (14), platform i collects the reward $\\pi_t^{(i)}$ defined by (7). Next, platform i updates the estimated values of the Q*-function at the given state $s_t$ and the selected action $a_t^{(i)}$ with a learning rate $\\alpha$ as follows:\n$Q_{t+1}^{(i)}(s_t, a_t) := (1 - \\alpha) Q_t^{(i)}(s_t, a_t) + \\alpha \\Big( \\pi_t^{(i)} + \\delta \\max_{a'} Q_t^{(i)}(s_{t+1}, a') \\Big)$.\nWe remark that (15) is an approximation of (10) (see Watkins and Dayan (1992))."}, {"title": "4 Experimental Results", "content": "We report extensive numerical experiments using the setup of Section 3.2. Section 4.1 investigates the general dependence of $\\overline{\\Delta}$, defined in (18), on the externality matrix $\\Phi$. Section 4.2 further explores the latter dependence for concrete and useful choices of $\\Phi$. Section 4.3 studies the dependence of $\\overline{\\Delta}$ on the degree of heterogeneity in users' tastes, the outside option utility, and the discount rate. Lastly, Section 4.4 explores two interesting scenarios: 1) long-run asymmetric equilibria outperform the symmetric equilibrium, and 2) competition prices are larger than collusion prices for one side of the market. A supplementary analysis in Appendix A.1 implies that our numerical results are consistent with platforms learning tacit collusion and equilibrium strategies."}, {"title": "4.1 Dependence of the Collusive Level on the Network Externalities", "content": "We applied an additive model to infer the dependence of the collusive level, $\\overline{\\Delta}$, on the externality matrix, $\\Phi$. We ran 2,500 simulations according to the setting described in Section 3.2. For each simulation, we randomly sampled the elements of the externality matrix $\\Phi$ from independent normalized Gaussians (that is, $\\phi_{kl} \\sim \\mathcal{N}(0, 1)$ for $k,l \\in \\{b, s\\}$), and recorded the final collusive level, $\\Delta$. In order to infer the dependence of $\\Delta$ on $\\Phi$, we assume the following additive model:\n$\\Delta(\\Phi) = \\Delta_0 + f_{bb}(\\phi_{bb}) + f_{ss}(\\phi_{ss}) + f_{bs}(\\phi_{bs}) + f_{sb}(\\phi_{sb}) + f_{bb,ss}(\\phi_{bb}, \\Phi_{ss}) + f_{sb,bs}(\\phi_{sb}, \\Phi_{bs}) + f_{bb,bs}(\\phi_{bb}, \\phi_{bs}) + f_{bb,sb}(\\phi_{bb}, \\phi_{sb}) + f_{ss,bs}(\\phi_{ss}, \\phi_{bs}) + f_{ss,sh}(\\phi_{ss}, \\phi_{sb}) + \\varepsilon,$\nwhere $\\Delta_0$ is the sample mean of $\\Delta$, the next 4 functions $(f_{bb}, f_{ss}, f_{bs}, f_{sb})$ represent the univariate effects of the elements of $\\Phi$ on $\\Delta$, the last 6 functions $(f_{bb,ss}, f_{sb,bs}, f_{bb,bs}, f_{bb,sb}, f_{ss,bs}, f_{ss,sb})$ represent the bivariate effects of the elements of $\\Phi$ on $\\Delta$ and $\\varepsilon$ is an error term, encompassing higher-order multivariate effects. Since the equilibrium values $\\pi^*$ and $\\pi^C$ depend on $\\Phi$ nonlinearly (see Section 2.1), the 10 functions, $f_{bb}, ..., f_{ss,sb}$, are nonlinear. We thus sequentially fit these functions using XGBoost (Chen and Guestrin, 2016), which is a popular non-parametric, nonlinear fitting method. To reduce the bias of the fitted functions, we alter the order of both the first four functions and the next six functions, during the sequential fitting procedure, and average the collusive level over the different orders. Appendix A.2 contains more details of implementing XGBoost.\nWe refer to $\\Delta_0$ as the baseline collusive level, whereas $\\Delta$ is the collusive level. Our simulations show that $\\Delta_0$ is approximately 0.3. Next, we report our estimates for the univariate and bivariate effects of the elements of $\\Phi$ on $\\Delta$."}, {"title": "4.2 A Study of the Collusive Level under Special Network Externalities", "content": "We assume special parameterizations of the network externality matrices, $\\Phi$, and explore the dependence of $\\Delta$ on any such $\\Phi$. This allows us to track more carefully the dependence of $\\Delta$ on $\\Phi$ in some special settings. For each specific $\\Phi$, we ran 100 simulations. Our figures present the dependence of the overall collusive level on the elements of $\\Phi$, where their main curves represent the average of the collusive levels from the 100 runs and their shaded areas represent the uncertainty level, which was computed using bootstrapping with a 99% confidence interval.\nFigure 6 investigates the dependence of the collusive level on the within-side externalities in two controlled settings. In the first setting (left panel) $\\Phi = [\\phi_{bb}, 0; 0, 0]$, and in the second one (right panel) $\\Phi = [\\phi_{bb}, 0; 0, \\phi_{bb}]$. In both cases $\\phi_{bb} \\in [-6, 2]$.\nFigure 7 investigates the dependence of the collusive level on the cross-side externalities in two controlled settings. In the first setting (left panel) $\\Phi = [0, \\phi_{bs}; 0, 0]$, and in the second one (right panel) $\\Phi = [0, \\phi_{bs}; \\phi_{bs}, 0]$. In both cases $\\phi_{bs} \\in [-2.5, 3]$.\nFigure 8 investigates the dependence of the collusive level on the bivariate effects between the within- and cross-side externalities in two controlled settings. For simplicity, we fix the cross-side externality and vary the within-side externality. In the first setting (left panel) $\\Phi = [\\phi_{bb}, 3; 0, 0]$, and in the second one (right panel) $\\Phi = [\\phi_{bb}, 3; 0, \\phi_{bb}]$. In both cases $\\phi_{bb} \\in [-6, 2]$."}, {"title": "4.3 A Study of the Collusive Level under Special Market Parameters", "content": "We explore the dependence of the collusive level on the market parameters $\\beta_k, u_k^{(0)}$ and $\\delta$. In each experiment, we fix two of the latter parameters, using the setup described in Section 3.2, and the matrix $\\Phi$, where its choices change with the experiments, and vary the remaining parameter. For each experiment, we ran 100 simulations, averaged the collusive levels among the 100 runs and computed the uncertainty levels using bootstrapping with a 99% confidence interval. Our figures present the averaged collusive level as a function of one parameter, where the shaded areas represent the uncertainty level."}, {"title": "4.4 Discussion of Exceptional Cases", "content": "We numerically demonstrate two exceptional and uncommon scenarios: asymmetric optimal prices and competition prices larger than collusion prices.\nAsymmetric collusion. Our metric for the collusive level compares the platform's rewards at time t with the symmetric equilibrium quantities $\\pi^*$ and $\\pi^C$, following previous simulations of collusion (see, e.g., Calvano et al. (2020b) and Klein (2021)). However, our specific model may give rise to asymmetric equilibria and we thus study their possibility more carefully.\nTo allow asymmetric equilibria, we modify the definitions of the maximum values of the total profits and the collusive level as follows. The total profit, $\\Pi_a$, is\n$\\Pi_a(p^{(1)}, p^{(2)}) := \\sum_{i=1}^{2} (x_b^{(i)} p_b^{(i)} + x_s^{(i)} p_s^{(i)}) = \\sum_{i=1}^{2} \\pi^{(i)},$\nwhere unlike (4) it does not assume symmetric prices (that is, it does not assume that $\\pi^{(1)} = \\pi^{(2)}$) and its subscript a indicates asymmetry. Similarly, the collusive level $\\Delta_t$ is averaged among the two firms as follows\n$\\Delta_t := (\\Delta_t^{(1)} + \\Delta_t^{(2)})/2 = \\frac{\\Pi_a - 2 \\pi^*}{2(\\pi^C - \\pi^*)},$ where $\\Delta_t^{(i)}, i \\in \\{1,2\\}$, was defined in (17).\nFigure 12 demonstrates the maximum value that $\\Delta_t$ can achieve in two controlled settings. In the first setting (left panel) $\\Phi = [\\phi_{bb}, \\phi_{bs}; \\phi_{bs}, \\phi_{bb}]$ with $\\phi_{bb} \\in [-1, 1]$ and $\\phi_{bs} \\in [-4, 4]$, and in the second one (right panel) $\\Phi = [\\phi_{bb}, \\phi_{bs}; -\\phi_{bs}, \\phi_{bb}]$ with $\\phi_{bb} \\in [-2, 2]$ and $\\phi_{bs} \\in [-8, 8]$. More precisely, we maximize $\\Delta_t$ over $p^{(1)} \\in A$ and $p^{(2)} \\in A$, where A was defined in (13), and present the maximal values using a heatmap, whose values vary from purple ($\\Delta_t$ greater than 1) to orange ($\\Delta_t$ less than 1). In both panels, $\\beta_k = 1.0$ and $u_k^{(0)} = -2.0, k \\in \\{b,s\\}$.\nIn the left panel, the maximal $\\Delta_t$ exceeds 1 when the within-side externality $\\phi_{bb}$ is positive and close to 1 and the cross-side externality $\\phi_{bs}$ is sufficiently negative. We note that it is much larger than 1. In the right panel, the maximal $\\Delta_t$ is larger than 1 when $\\phi_{bb}$ is sufficiently large and $\\phi_{bs}$ is close to zero. We note though that it never exceeds the value of 1.0175. In both panels, for all other corresponding values of $\\phi_{bb}$ and $\\phi_{bs}$, the maximum value of $\\Delta_t$ is either achieved at a symmetric vector price or at an asymmetric vector price with corresponding maximum value close to 1. In the latter case, of the corresponding maximum value close to 1, our measure of collusive level $\\Delta$ using the symmetric assumption can still be used to quantify the level of collusion. That is, one may often follow up our analysis with the symmetric quantities, except for special cases where the estimated collusive level exceeds one, where one needs to use the asymmetric quantities.\nFigure 13: Collusive level with varying $\\phi_{sb}$, where $\\Phi = [0, \\phi_{sb}; \\phi_{sb}, 0]$.\nCompetition prices larger than collusion prices. We recall that both the simulated action space presented in (12) and the formulated baseline model in Section 2.1 allow for the competition prices $(p^*)$ to be either smaller or larger than the collusion prices $(p^C)$. We demonstrate here an uncommon situation where the collusion price can be smaller than the competition price for one side of the market. In this example $\\Phi = [1, -\\phi_{sh}; \\phi_{sb}, -2]$, $\\beta_k = 0.5$ and $u_k^{(0)} = -1.0, k \\in \\{b,s\\}$.\nIf $\\phi_{sb} \\in [-5, 0.5)$, then $p_s^* < p_s^C$. If $\\phi_{sb} \\in (0.5, 5]$, then $p_s^* > p_s^C$. The left panel of Figure 14 demonstrates the competition and collusion prices of both sides of the market. For side b, the collusion price is always higher than the competition price. However, for side s, there are two"}, {"title": "5 Economic and Policy Discussion", "content": "We examine the economic implications of the numerical results presented in Section 4. Specifically, we analyze the impact of network externalities on the collusive level and discuss the ramifications of these findings on real-life markets. We then review the Preventing Algorithmic Collusion Act of 2024, introduced by U.S. Senator Amy Klobuchar, and suggest an additional policy recommendation to ensure the safer use of Q-learning."}, {"title": "5.1 Economic Discussion", "content": "When $\\Phi = 0$, our baseline platform competition game reduces to Bertrand competition games on each side of the market. For $\\delta = 0.05, \\beta_k = 1$ and $u_k^{(0)} = -2, k \\in \\{b,s\\}$, our simulations show that $\\Delta_0$ in (19) is approximately 0.3. This means that when $\\Phi = 0$, the profit gain relative to competition profits is about 30%. It is interesting to note that Calvano et al. (2020b) reported a different gain of approximately 20% for the same value of $\\delta$. To understand the difference, we note that in the latter paper, firms serve only one market and Bertrand competition is the baseline game. For that reason, the action space used in their simulations is smaller than the action space used in ours. The larger action space allows the platforms to algorithmically communicate more information, increasing the chance of achieving a higher collusive level.\nNext, we discuss the impact of $\\Phi \\neq 0$ on the collusive level compared to Bertrand competition, where $\\Phi = 0$. When $\\Phi$ has only one non-zero entry, the dependence of the collusive level on each possible entry is depicted in Figures 2 and the left panel of 6. We note that if the nonzero externality is either positive or sufficiently negative, then the collusion is higher in platform competition than in traditional Bertrand competition. Furthermore, when $\\Phi$ is a diagonal matrix, the dependence of the collusive level on these entries is depicted in Figures 2 and the left panel of 3. We note that when these entries are both positive, collusion is higher in platform competition than in single-sided Bertrand competition. These findings suggest that in markets such as online (or cloud) gaming, such as in Xbox, mobile and computer games, where positive within-side externalities are significant, algorithmic pricing will increase collusion levels above those observed in baseline Bertrand competition. Finally, when $\\Phi$ has only off diagonal non-zero entries, the dependence on the collusive level on these entries is depicted in Figures 2 and the right panel of 3. We note that when these entries are both positive, collusion sharply increases above the baseline Bertrand level. These findings suggest that in markets such as video streaming (e.g., Netflix, Hulu, and Amazon) and social media markets (e.g., Instagram and TikTok), where positive cross-side externalities are significant, high levels of collusion can be expected if platforms use algorithmic pricing.\nTraditionally, platforms use positive network externalities to enhance demand and profits by subsidizing one side of the market in order to attract population on the other sides (see, e.g., Armstrong and Wright (2007), Tan and Zhou (2021), and Chica et al. (2021)). Our findings suggest that algorithmic driven platforms may also learn to use positive network externalities to significantly increase the profit.\nNext we discuss the other scenario, where network externalities result in relatively small levels of collusion. First, we note that the right panels of both Figures 6 and 7 indicate examples where either the within-side externalities (Figure 6) or the cross-side externalities (Figure 7) are both negative with the same magnitude and in these cases the collusive level remains flat at the baseline competition level $\\Delta_0$. On the other hand, Figure 8 shows a case where the cross-side externality ($\\phi_{bs}$) is large and positive, and the within-side externality is sufficiently small and negative. In this scenario, the collusive level remains flat at a value slightly above $\\Delta_0$. The latter example is relevant to ride-sharing markets, where drivers compete with each other for riders, while riders benefit from faster pickup times. In"}]}]}