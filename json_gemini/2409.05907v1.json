{"title": "Programming Refusal with Conditional Activation Steering", "authors": ["Bruce W. Lee", "Inkit Padhi", "Karthikeyan Natesan Ramamurthy", "Erik Miehling", "Pierre Dognin", "Manish Nagireddy", "Amit Dhurandhar"], "abstract": "LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging. Existing activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants. In this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context. Our method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states. Using CAST, one can systematically control LLM behavior with rules like \"if input is about hate speech or adult content, then refuse\" or \"if input is not about legal advice, then refuse.\" This allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization. We release an open-source implementation of our framework at https://github.com/IBM/activation-steering.", "sections": [{"title": "1 Introduction", "content": "A striking feature of large language models (LLMs) is their ability to process high-level concepts through rich representations in their activations. This feature has given rise to techniques like activation steering (Turner et al., 2023), which leverage these learned representations to efficiently and predictably alter LLM behavior (Rimsky et al., 2024; Wang et al., 2024b; Zou et al., 2023).\nProblem: Lack of conditional control in activation steering. Activation steering offers a promising alternative to optimization-based techniques by directly manipulating the model's native representations, often requiring only a simple activation addition step during each forward call (Turner et al., 2023). While activation steering has shown promise in altering LLM behavior, such as removing or inducing refusal behavior, a key limitation of current methods is the inability to condition when and what to refuse (Ghandeharioun et al., 2024; Zheng et al., 2024). That is, adding a \u201crefusal vector\" using existing activation steering methods increases refusal rates indiscriminately across all inputs, limiting the model's utility (Arditi et al., 2024).\nContribution: Expanding activation steering formulation. We introduce Conditional Activation Steering (CAST), a method that enables fine-grained, context-dependent control over LLM behaviors. We introduce a new type of steering vector in the activation steering formulation, the condition vector, representing certain activation patterns induced by the prompt during the inference process. A simple similarity calculation between this condition vector and the model's activation at inference time effectively serves as a switch, determining whether to apply the refusal vector. This approach allows for selective refusal of harmful prompts while maintaining the ability to respond to harmless ones, as depicted in Figure 1. A breakdown of this figure is presented in Table 3. Furthermore, CAST maintains the data, runtime, and compute efficiency of activation steering (Figure 6) while adding controllability, enabling the implementation of behavioral rules in LLMs without significant costs."}, {"title": "2 Background", "content": "How do transformers perform inference? Transformer models, particularly decoder-only variants, perform inference by sequentially processing input tokens through a stack of layers (Radford et al., 2018; Vaswani et al., 2017). The key to understanding their operation lies in how information flows and accumulates through these layers (Elhage et al., 2021; Lad et al., 2024; Shai et al., 2024). The process begins with converting the prompt into token embeddings, which serve as initial inputs. Each layer transforms these activations using its internal mechanisms, like learned weights. Each layer's output combines processed information with its input, preserving and building upon earlier computations. As activations flow through the layers, the model constructs increasingly complex representations. The final layer's output is used for decoding - predicting the next token via an operation over the model's vocabulary. This predicted token is then used for subsequent predictions.\nBehavior steering. One could intervene in any of the abovementioned five steps - weights, decoding, prompt, token embedding, and activations - to alter model behavior (Chai et al., 2024; Han et al., 2024; Li et al., 2024; Phan et al., 2024; Tamoyan et al., 2024; Wang et al., 2024b). For example, one could use role-play prompts to simulate and create AI patients (Louie et al., 2024). Or one could use preference optimization methods like direct preference optimization to update weights and steer the LLM towards more empathetic behaviors (Sotolar, 2024). Activation steering is a class of methods that intervenes in the information flow within LLMs from layer to layer to alter the model behavior."}, {"title": "Activation steering.", "content": "An alternative method for influencing the behavior of LLMs, activation steering modifies their internal activations during inference. This approach typically involves three key steps. First, a steering vector is extracted, often by computing the difference in activations between examples that exhibit a desired behavior and those that don't. Second, during inference, this vector is added to the model's hidden states at a chosen layer, scaled by a hyperparameter. Finally, the model completes the generation using these modified activations. For the case of activation addition (ActAdd) (Turner et al., 2023), the intervention can be represented mathematically as:\n h'+h+av\nwhere h is the hidden state at the layer, v is the steering vector for the layer, and a is a scaling factor. Stronger scaling can disrupt coherence while weaker scaling may be ineffective (Rimsky et al., 2024). In an ideal case where steering vectors are well-extracted, this method allows for predictable LLM behavior steering without altering model weights, enabling applications such as reducing bias (Adila et al., 2024; Lu and Rimsky, 2024) or preventing overly confident responses (Rahn et al., 2024).\nRecent research has proposed several methods to improve upon the basic activation addition approach (Qiu et al., 2024; Stickland et al., 2024; Wang et al., 2024a; Wu et al., 2024; Yin et al., 2024). These techniques address various limitations of the ActAdd method and collectively fall under the broader category of activation engineering. In this paper, we propose a vertical expansion by adding the new dimension of condition, greatly improving the utility of existing activation steering methods."}, {"title": "3 Conditional Activation Steering", "content": "3.1 Overview\nA common limitation of the existing activation steering methods is that one cannot condition the model's behavior on context, as these methods typically apply modifications uniformly across all inputs regardless of context (He et al., 2024a). Simple activation steering of a model indiscriminately affects all inputs, rendering the steered model much less useful for its application (Brahman et al., 2024; Cui et al., 2024; Turner et al., 2023; Wen et al., 2024). We show that one can induce conditional behavior (Figure 2) by leveraging two types of vectors: condition and behavior vectors.\nh' + h + f(sim(h, projch)) \u00b7 \u03b1\u00b7 \u03bd\nwhere h is the hidden state, c is the condition vector, v is the behavior vector, and a is a scaling factor. The projection of h onto c is given by projch =  \\frac{(c \\cdot c)}{cc} h. Intuitively, based on how well aligned the hidden state h is with the condition vector c, the function f determines whether to apply the behavior vector based on the similarity between the hidden state and its projection using the condition vector. Throughout the paper, we use cosine similarity, defined as sim(h, g) =  \\frac{hg}{h.g}\nBehavior vector. We use the term \"behavior vector\" to refer to what previous activation steering methods call a \"steering vector\" to emphasize its focus on modifying specific behaviors. A behavior vector v is a one-dimensional vector matching the model's hidden state dimensions that induces"}, {"title": "specific behaviors.", "content": "When added to layer representations during a forward pass with scaling factor a, it predictably alters model behavior (e.g., inducing refusal). In addition to setting the right scaling factor a, one can specify to which layers to apply the behavior vector. While specific implementations vary in the literature, our implementation calculates a different vector $v_l$ for each layer $l$, as behavior representations vary. Thus, when we mention adding a behavior vector from layers 15-20, we're referring to adding the corresponding $v_{15}, v_{16}, ..., v_{20}$ to their respective layers.\nCondition vector. A condition vector c captures a class of instructions to condition on, extracted similarly to behavior vectors and matching hidden state dimensions (e.g., 1x4096 for Llama2, which has a hidden size of 4096). For instance, a condition vector might capture discrimination or adult content. It acts as a trigger, determining when to apply the behavior vector based on the model's current hidden state. Since we also calculate a different vector $c_l$ to each layer $l$, one can also choose which layer to condition. When the condition is activated during text generation, the behavior vector is added to all subsequent forward passes. This allows the model's behavior to change based on specific conditions in the input or generated text rather than always applying the behavior vector.\nChecking if condition was met. The term sim(h, projch) computes the degree to which the condition is met using cosine similarity. The thresholding function f then determines whether this degree is sufficient to trigger the behavior modification. Though one would be able to design more complex thresholding functions, we use a simple step function for binary output in this paper:\nf(sim(h, projch)) =  \\begin{cases}\n1 & \\text{if sim(h, projch) > \\theta} \\\\\n0 & \\text{otherwise}\n\\end{cases}\nHere, each layer in an LLM might represent the same condition in different directions and sim(h, projch) > \u03b8 could be sim(h, projch) < \u03b8 depending on the layer. This binary approach allows for a clear distinction between when the condition is met and when it is not, providing a straightforward mechanism for activating the behavior modification. We use cosine similarity to check condition based on the directional similarity between the hidden state and its projection using the condition vector rather than magnitude (Hsu et al., 2024). In practice, we apply a non-linear transformation sim(h, tanh(projh)) for more predictable behavior.\nMulti-conditioning. As mentioned in Section 1, one could also break down broader alignment goals into smaller, more definitive categories and predictably induce refusal behaviors for each. For instance, instead of conditioning a model to refuse \u201charmful\u201d instructions in general, we could create specific conditions for \"adult content,\u201d \u201csocial stereotypes,\u201d or \u201cfalse advertising.", "like": "nf( ) =  \\begin{cases}\n1 & \\text{if sim(h, projadulth) > \\theta_{\\text{adult}} or sim(h, projstereotypeh) > \\theta_{\\text{stereotype}}} \\\\\n0 & \\text{otherwise}\n\\end{cases}\nGeneral expectations Implementing conditional behaviors in LLMs using CAST generally follows the pipeline: 1. gather contrasting example responses/prompts for desired behavior/condition $D^+$ and other behavior/condition $D^-$, 2. extract behavior/condition vector, 3. find optimal intervention points for behavior/condition vector, 4. steer. The model itself does not undergo any weight update.\nStep 3 represents the most time-intensive part of our process, involving both automated and manual elements. For the behavior vector, similar to other works in activation steering, we manually search for the appropriate intervention strength and layers. However, as demonstrated in Appendix C, most models represent refusal behavior at similar depths. For the condition vector, we use a grid search (Appendix C.2) algorithm that determines the best threshold, layer, and comparison direction (> or <). The majority of our reported experiments are replicable within an hour, with the grid search being the primary time-consuming component. We share more details below."}, {"title": "3.2 Preparing Dataset and Model", "content": "As mentioned, contrast datasets are needed to extract behavior or condition vectors. For the refusal behavior vector, we randomly select 100 instructions from the Alpaca dataset (Taori et al., 2023) and append them with 100 typical refusal or compliance behavior prefixes as responses, as shown in Figure 3. Considering every combination of these creates 10,000 pairs of contrasting data points for $D_{\\text{refuse}}$ and $D_{\\text{comply}}$. We commit to this setup for the refusal behavior vector throughout our research."}, {"title": "4 Conditioning Refusal: Selectively Steering on Harmful Prompts", "content": "In this section, we explore the basic use of conditional steering by steering a model to refuse harmful prompts while complying with harmless ones. Apart from demonstrating that a language model can be conditioned from inside on the fly, we also share some key properties of conditional steering.\nExperimental setup. To obtain our contrast dataset (D+, D\u00af) on the harmful condition, we started by machine-generating 90 harmful prompts for each of the 45 harm categories as identified by Xie et al. (2024b). We use these 4,050 synthetically generated harmful prompts as our Dharmful. For each of these harmful prompts, we randomly sample a benign instruction from the Alpaca dataset to create Dharmless. Following the process outlined in Section 3.3, we then extract the harmful condition vector Charmful. We then use a grid search algorithm to identify the best combination of threshold 0, layer l, and comparison direction (> or <) that best separates the two classes of training data. This concept is illustrated in Figure 4d, where we perform the condition checking operation at layer 7 and activate the behavior vector vrefusal when sim(h, projh) was smaller than 0.048."}, {"title": "Result:", "content": "Activation steering can be used to induce conditional behaviors. We test the conditional activation steering performance on 500 unseen Alpaca (harmless) and 450 unseen Sorry-Bench (harmful) test sets. The results are presented in Figure 1 with a subset of the data in Table 2. Across all seven tested models, we observe that conditioning a behavior vector Vrefusal on condition vector Charmful selectively increases refusal rates for harmful content while leaving harmless prompt refusal rates largely unchanged. In contrast, simply adding a behavior vector vrefusal like standard activation steering increased refusal rates indiscriminately across all prompts. Figures 4a-c demonstrates how the conditioning operation partitions the prompt space.\nProperty: Duality. As seen in Figure 4d, this conditioning process is systematic in nature as we can manually choose the point of intervention. One consequence of this is that conditioning exhibits a dual nature: flipping the comparison direction (from < to > or vice versa) results in intervening on the exact complement of the original set of hidden states that triggered the condition. This duality enables complementary control over the model's behavior, allowing one to not only condition the model to refuse harmful prompts but also, if desired, to selectively refuse harmless prompts while complying with harmful ones. See Figure 5d.\nProperty: Modulation. Our steering approach offers flexible control rather than being uniform across all contexts, with the threshold @ modulating the required alignment between the input and the harm direction defined in charmful. In Figures 5a-c, using the < comparison, lowering @ narrows the range of hidden states triggering the condition while raising it broadens this range. This property allows us to adjust the model's sensitivity to potentially harmful content. While this offers the potential for finer condition control, we do not explore it further in this study. Instead, we use threshold values determined by grid search, which maximizes the F1 score to balance false and true refusal rates (Appendix C.2)."}, {"title": "Property:", "content": "Saturation. Unlike most weight optimization methods, where performance often scales with increased data volume (Ansell et al., 2024; Das et al., 2024; Metcalf et al., 2024), conditional activation steering tends to reach a performance plateau relatively quickly. As shown in Figure 6a, the method's effectiveness stabilizes after a certain point. This saturation might be attributed to the fact that conditional steering leverages the model's existing representations. Consequently, performance appears more dependent on the model's inherent capacity to represent certain concepts and how well the chosen data instances represent the target concept rather than on the sheer volume of conditioning data. Notably, the method also exhibits linear time scaling property (Figure 6b). The condition vector extraction time increases linearly with the number of samples, as this process is primarily determined by the number of inferences the model must make for us to record hidden states."}, {"title": "5 Programming Refusal: Logical Composition of Refusal Condition", "content": "Moving beyond the general concept of refusing harmfulness, we demonstrate the creation of more fine-grained condition vectors. We create five example condition vectors from categories - hate speech, legal opinion, sexual context, health consultation, and crime planning - in Liu et al. (2023) to explore these ideas. Our experiments demonstrate the capacity to (1) selectively modulate refusal behaviors for specific conditions and (2) construct complex refusal conditions through the logical composition of several condition vectors, enabling programmatic control over model behavior."}, {"title": "Experimental setup.", "content": "We begin by randomly selecting 1,300 base prompts from the Alpaca training set. Each of these prompts is then paraphrased to incorporate aspects of sexual content csex, legal opinions clegal, hate speech chate, crime planning ccrime, or health consultation Chealth. This process results in 1,300 prompts in six categories, including the original benign base Alpaca prompts. We then split this dataset into 700 prompts per category for training and 500 per category for testing. To create a conditioning vector c for a specific category, we use the 700 \u00d7 5 = 3,500 training prompts from the other five categories as our negative examples (D\u00af). For the positive examples (D+), we use the 700 training prompts from the target category and repeat them five times to balance the dataset.\nApplication: Inducing or suppressing refusal behavior from specific categories. We begin by examining our ability to add refusal behavior to specific categories of prompts, starting with a model that exhibits arbitrary refusal behaviors. Figure 7a demonstrates that it is indeed possible to induce refusal behavior when a specific condition is met. This extends the concepts explored in Section 4 to more fine-grained categories, showing successful selective refusal. Furthermore, as shown in Figure 7b and consistent with findings from Arditi et al. (2024), we can also remove refusal behavior from certain classes of prompts. This is achieved by simply reversing the signs of the behavior vector Vrefusal. Beyond refusal, most inference-time steering techniques can be conditioned using condition vectors as a modulation for various characteristics in language model outputs (Konen et al., 2024)."}, {"title": "Application:", "content": "Logical composition of condition vectors. As introduced in Section 3.1, condition vectors can be logically combined to create complex refusal conditions. For instance, to induce refusal in two categories, such as hate speech and legal opinions, one could implement a rule like if Chate or clegal then +vrefusal, as illustrated in Figure 8a. This multi-conditioning mechanism can also reinforce existing model refusal conditions, enhancing robustness against harmful prompts. The second pie chart in Figure 8b demonstrates this with LLAMA 3.1 INST, where we can augment the model's existing refusal of crime planning and hate speech with additional conditions for legal and health queries while maintaining responsiveness to benign prompts. Each condition vector c may have different optimal condition points, as different layers might best separate specific conditions. Consequently, condition checking might occur at various layers during inference, as shown in Figure 8c. It's also possible to completely change the original model's refusal map by simultaneously removing existing refusal directions and inducing new ones (Figure 8b) through multiple rules. However, we generally find that this approach can reduce the effectiveness of induced refusal directions, as certain suppressing conditions may conflict with newly induced refusal conditions."}, {"title": "Application:", "content": "Constraining model responses to specific domains. Connecting from our earlier point on the logical composition of condition vectors, we can conditionally steer models to respond only to specific types of prompts. This approach is particularly useful when the goal is to make a specialized model respond exclusively to specific categories, such as creating a health assistant (Cheong et al., 2024; Xie et al., 2024a). Instead of creating conditions for all non-health categories to refuse, we can utilize the duality property discussed in Figure 5. We could (1) create a condition vector (e.g., Chealth) and (2) flip the comparison direction to add refusal on the exact complement set of inputs (e.g., \u00acChealth). As shown in Figure 9, this constrains the model to only respond to a category and refuse all others.\nWe extended our investigation to examine whether our constraining method remains effective for unseen prompt categories. To this end, we introduced four additional harm categories from Liu et al. (2023) that were not part of our original condition vector training setup: gambling, financial advice,"}, {"title": "Analysis:", "content": "Constraining response to one category works better for more semantically distinct categories. Figure 9c illustrates this relationship, showing a positive correlation between a category's average semantic distance from others (x-axis) and the effectiveness of constraining to that category, measured by the increase in refusal rate for other categories (y-axis). Using a sentence transformer model, this semantic distance is calculated as the average cosine distance between the embeddings of the target category's training prompts and the test prompts of all other categories. This explains why constraining the model to hate speech is more effective than constraining it to legal opinions when it comes to refusing other categories. Hate speech, being more semantically distinct from other categories, allows for clearer boundaries and, thus, more effective constraining.\nAs noted in previous literature on behavior steering, prompting alone fails to provide an effective alternative for several reasons. Unlike CAST, prompting lacks the ability to forcefully condition the model, offering only weak, coarse-grained control that may paradoxically increase unwanted content (Dekoninck et al., 2023; Jang et al., 2023). Our experiments confirm this, with conditional steering consistently outperforming the prompting baseline (red dotted line) across most categories in Figure 9c. This baseline represents the average performance when the model is simply prompted to comply with the target condition and refuse other conditions without any conditional steering techniques."}, {"title": "6 Conclusion", "content": "This paper introduces Conditional Activation Steering (CAST), a novel framework for inducing context-dependent behaviors in large language models through principled manipulation of their internal representations. By extending existing activation steering techniques with the introduction of condition vectors, CAST enables fine-grained control over model behavior without the need for fine-tuning or extensive computational resources.\nFigure 10 demonstrates key operations that we introduced: the ability to flip condition comparison directions, allowing the model to refuse all categories except a target one, and the capacity to add multiple refusal conditions to induce or remove behaviors. These operations help tailor model behavior to specific requirements. Beyond this flexibility, the framework offers several advantages.\nFirstly, it allows for quick selective refusal of harmful content while maintaining model functionality on benign inputs, addressing a critical challenge in alignment research. Secondly, CAST enables the creation and logical composition of multiple condition vectors, facilitating the implementation of complex behavioral rules. Lastly, it can effectively constrain model responses to specific domains, with its efficacy correlating to the semantic distinctiveness of the target category. By leveraging the model's existing representations, CAST achieves performance comparable to or exceeding that of models specifically aligned for safety while significantly reducing computational overhead. This efficiency, combined with the ability to modify and compose behavioral rules rapidly, offers significantly enhanced flexibility in adapting model behavior to varying requirements."}, {"title": "A Understanding Conditional Activation Steering", "content": "A.1 The Larger Picture\nModel development cycle The development of language models can be broadly categorized into pre-training and post-training stages (McKinzie et al., 2024; Tay et al., 2022). During pre-training, the focus is on enhancing fundamental capabilities such as knowledge acquisition, reasoning abilities, and coherent language use. The post-training stage, often referred to as alignment, aims to shape the model's behavior to meet specific expectations and requirements (Askell et al., 2021; Kundu et al., 2023).\nAlignment and behavior steering Within the alignment phase, several key areas emerge, including evaluation, reinforcement learning, and instruction tuning (Lee et al., 2023b; Nagireddy et al., 2023; Sudalairaj et al., 2024). While these topics often overlap, our focus is on behavior steering (Bai et al., 2022; Cao et al., 2024). The term \u201csteering\" is deliberately chosen over \u201ccontrol,\u201d implying the current approach of influencing language model behavior rather than exerting direct control.\nAs model creators, our ultimate goal is to achieve a level of control akin to programming these language models. To transition from behavior steering to true behavior control, two fundamental criteria must be met: specificity and predictability. This entails the ability to provide precise instructions or rules to the model, such as \u201crefusing harmful instructions,\u201d \u201cdeclining irrelevant conversations,\u201d or \u201cavoiding generating adult content,\" coupled with a high degree of confidence that the model will consistently adhere to these directives.\nTowards programmatic behavior control Now, instead of merely encouraging models to behave in certain ways through prompting or reinforcement learning, we propose a more forceful and programmatic approach to designing model behaviors. Our method involves three key steps:\n1. Tracking model activations during inference\n2. Checking if these activations match specified rule conditions\n3. Forcefully intervening in the model to induce desired behavior when conditions are met (which was done in the form of activation steering in this paper)\nUnlike straightforward prompting-based approaches, conditional activation steering can be likened to implementing a brain-computer interface for language models, creating a programmable, rule-based system for enforcing model behavior.\nBroader implications This research represents a step towards bringing language models under more precise control, moving closer to predicting and controlling LLM behaviors for various use cases. In this particular study, we focus on the refusal behavior - specifically, determining and enforcing exactly when a model should refuse instead of complying with a given instruction.\""}, {"title": "A.2 Details of Conditional Activation Steering", "content": "Origins Conditional activation steering is an expansion of existing activation steering methods. Activation steering intervenes in the model's hidden state during inference, typically by adding \u201csteering vectors\". This simple operation has shown the potential to reliably induce behaviors like refusal on arbitrary prompts, aligning with the linear representation hypothesis (Gurnee and Tegmark, 2023; Park et al., 2023). While effective, traditional activation steering lacks specificity, causing models to refuse all instructions indiscriminately. CAST addresses this limitation by introducing a conditional vector c alongside the behavior vector v. The application of v is now conditioned on the similarity between the model's activation and its projection onto c.\nImplementation in the generation process Language model generation can be viewed as a series of forward passes through the model's layers for each generated token. The first full pass through the model typically involves prompt caching. In CAST, the condition is checked only during this first full pass, as we are conditioning on the prompt (see Figure 11). This approach ensures that the additional condition-checking operation is not repeated for all generated tokens. However, if the condition is met, the behavior vector is applied in every subsequent forward pass, influencing each generated token. This application of the behavior vector in every pass at the specified layers follows the convention established in previous activation steering literature.\nExtracting behavior and condition vectors The extraction of behavior and condition vectors follows a consistent process, as illustrated in Figure 12. This process involves passing contrastive prompts through the model, recording hidden states at each layer, and then applying Principal Component Analysis (PCA) to extract the direction that best separates the two contrastive prompt types. The mathematical representation of this process for each layer is as follows:\nvector_l = PCA\\begin{bmatrix}\nH^+\\_l - \\mu_l\\\\\nH^-\\l - \\mu_l\n\\end{bmatrix}, \\mu_l = \\frac{H^+\\_l + H^-\\l}{2}\nThe key distinction lies in the specific token position at which the activation is recorded, as depicted in Figure 3. This choice can be adjusted based on the experimental setup. For instance, when using longer contrastive prompts to train the vector, recording the activation of the last token may yield more informative results compared to using the mean activation across all tokens, which could potentially introduce length-related biases.\nIt is important to note that the current method for extracting and applying refusal behavior may have limitations. Recent studies, such as Arditi et al. (2024) or Rimsky et al. (2024), have proposed alternative approaches for extracting the behavior directions. While a comprehensive comparison of these methods is beyond the scope of this paper, it represents an important area for future research. The refinement of vector extraction techniques will likely benefit from ongoing collaborative efforts within the research community."}, {"title": "Adjusting hyperparameters", "content": "The effectiveness of conditional activation steering is highly sensitive to the choice of hyperparameters. This sensitivity stems from the fundamental nature of the method, which relies on precise mathematical operations within the model's hidden states. The primary hyperparameters for conditioning can be conceptualized in a statement:\nSteer when the {best threshold} is {best direction} than the cosine simi-larity at {best layer}.\nThis formulation encapsulates three key hyperparameters: (1) Best layer: Determines at which depth of the network the condition checking operation occurs; (2) Best threshold: Defines the boundary for activation; (3) Best direction: Specifies whether the steering activates when the similarity is larger or smaller than the threshold.\nThe layer selection is crucial because different layers capture varying levels of abstraction and linguistic features. The threshold value and comparison direction determine when the steering should be applied. Conceptually, this can be thought of as setting a \"trigger point\u201d in the high-dimensional space of the model's hidden states (See Figure 7). The threshold defines a boundary, while the comparison direction (larger or smaller) determines on which side of this boundary the steering should activate.\nThese hyperparameters interact in complex ways with the model's learned representations. For instance, a threshold that is too low might lead to frequent, unnecessary interventions, while one that is too high might fail to activate when needed. Similarly, the choice of layer can significantly impact the granularity and specificity of the condition being checked. While these conditioning hyperparameters are novel contributions of this approach, they build upon a foundation of existing research on intervention strength and optimal intervention points for behavioral steering in language models (Kong et al., 2024; Scalena et al., 2024; Tlaie, 2024; Wang and Veitch; Zhang et al., 2024).\nIt is important to note that there isn't a universally applicable range for the grid search (detailed in Section C.2) of these hyperparameters, particularly for the threshold values. The cosine similarity values can vary drastically depending on the specific model architecture (more dependent) and the condition being explored (less dependent). For instance, in our experiments, we found that for HERMES 2"}, {"title": "B Constrasting Pair Generation Details", "content": "To generate the contrasting pair examples used in Section 4 and Section 5, we employed the following machine generation processes:\nB.1 Section 4: Harmful vs. Harmless Prompts\nFor Section 4, we used the Sorry-Bench dataset as a source of harmful prompts:\n1. For each harmful prompt in the Sorry-Bench dataset:\n(a) Select two random prompts from other harm categories in the Sorry-Bench dataset.\n(b) Create a prompt for the language model (Mixtral 8x7B) that includes:\n\u2022 The target harmful prompt\n\u2022 Two example prompts from other harm categories\n\u2022 Instructions to generate new questions that violate the target harm category but not the other categories\n(c) Generate 10 new variations of the harmful prompt using the language model.\n(d) Add the generated variations to the original prompt data structure.\n2. For harmless prompts, we randomly sampled from the Alpaca dataset without modification.\nPseudocode for the harmful prompt generation:\nB.2 Section 5: Fine-grained Harm Categories\nFor Section 5, we used the Alpaca dataset as a base and generated variations for specific harm categories. The process was:\n1. For each prompt in the Alpaca dataset (both train and test splits):\n(a) For each of the five harm categories (sexual content, legal opinion, hate speech, crime planning, health consultation):\n\u2022 Create a prompt for the language model (gpt-4o-2024-05-13) that includes:\nThe original Alpaca prompt\nInstructions to rewrite the prompt to include aspects of the current harm category\nRules to ensure the generated prompt maintains a similar structure and explicitly includes the harm category without mentioning it directly\n\u2022 Generate a new variation of the prompt using the language model\n(b) Add the generated variations to the original prompt data structure\nPseudocode for the fine-grained category generation:\nC Intervention Points and Grid Search Algorithm\nC.1 Intervention Points Used to Produce Results in This Paper\nAll our experiments are done in our activation steering library, which we open-sourced along with this paper. The algorithm's use of these values to steer the model might differ slightly for behavior steering but not for condition steering, as we are implementing conditional steering for the first time. In general, one could steer, conditional steer, or multi-conditionally steer, as shown in the following code snippets. These are high-level overviews demonstrating how the numbers from Table 4 can be applied to replicate our results. For exact replication, use the replication version of our code.\nSteer:\nConditional Steer:\nMulti-Conditionally Steer:\nC.2 Best Condition Point (Grid Search) Algorithm\nThe algorithm searches for the optimal conditioning configuration by evaluating different combinations of layers, thresholds, and comparison directions."}, {"title": "D Model Descriptions / Dataset Locations", "content": "Here", "Descriptions\nNeuralDaredevil-8B": "This model is derived from Daredevil-8B", "involved": "n1. Starting with Daredevil-8B", "uncensor": "he model. Here", "Pro": "Developed by Nous Research", "process": "n1. Starting with the Llama 3 8B base model.\n2. Fine-tuning on an updated and cleaned version of the OpenHermes 2.5 Dataset. This dataset is a mix of a few different datasets, including LMSYS-Chat-1M (Zheng et al., 2023), WizardLM (Xu et al., 2024), Platypus (Hendrycks et al., 20"}]}