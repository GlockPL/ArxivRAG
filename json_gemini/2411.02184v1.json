{"title": "DOUBLE DESCENT MEETS OUT-OF-DISTRIBUTION DETECTION: THEORETICAL INSIGHTS AND EMPIRICAL ANALYSIS ON THE ROLE OF MODEL COMPLEXITY", "authors": ["Mou\u00efn Ben Ammar", "David Brellmann", "Arturo Mendoza", "Antoine Manzanera", "Gianni Franchi"], "abstract": "While overparameterization is known to benefit generalization, its impact on Out-Of-Distribution (OOD) detection is less understood. This paper investigates the influence of model complexity in OOD detection. We propose an expected OOD risk metric to evaluate classifiers confidence on both training and OOD samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD risk of binary least-squares classifiers applied to Gaussian data. We show that the OOD risk depicts an infinite peak, when the number of parameters is equal to the number of samples, which we associate with the double descent phenomenon. Our experimental study on different OOD detection methods across multiple neural architectures extends our theoretical insights and highlights a double descent curve. Our observations suggest that overparameterization does not necessarily lead to better OOD detection. Using the Neural Collapse framework, we provide insights to better understand this behavior. To facilitate reproducibility, our code will be made publicly available upon publication.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, large neural networks have seen increased use in Machine Learning due to their impressive generalization properties (Brown, 2020; Dubey et al., 2024). While empirical evidence suggests that rich machine learning systems obtain near-optimal generalization results when trained to interpolate training data (Zhang et al., 2021), the classical bias-variance trade-off theory (Geman et al., 1992) suggests that such models overfit and generalize poorly. Indeed, the classical literature describes the generalization error with respect to the model complexity as a U-shaped curve and suggests finding a model between underfitting and overfitting, i.e., a model rich enough to express underlying structure in the data and simple enough to avoid fitting spurious patterns. To bridge the gap between the classical theory and the modern practice, Belkin et al. (2019) introduced the concept of \"double descent\" within a unified generalization error curve. In this setting, for \"small\" model complexities, the generalization error curve exhibits the U-shaped curve described by the bias-variance trade-off. However, when the model complexity is higher than the interpolation threshold, i.e., when the model is rich enough to fit the training data, increasing the model complexity leads to a second decrease in the generalization error. A popular intuitive explanation of this phenomenon is that by considering large model complexities that contain more candidate predictors compatible with the training data, we are also able to find interpolating functions that are \u201csimpler\u201d and are smoother to follow a form of Occam's razor (Belkin et al., 2019).\nAlthough the double descent phenomenon provides valuable insights to understand generalization of rich models on unseen data, its understanding on Out-Of-Distribution (OOD) detection has received less attention. OOD detection addresses a distinct challenge in deep neural networks (DNNs): their tendency to make high-confidence predictions, even for inputs that differ significantly from the training data. While generalization focuses on the model's ability to classify data that has shifted, OOD detection emphasizes the model's capacity to recognize when a shift is too large and refrain"}, {"title": "2 RELATED WORK", "content": "OOD Detection. The OOD detection research focuses on two primary directions: supervised and unsupervised approaches. We will focus on latter ones, also called post-hoc methods. These can be categorized based on the essential feature used for the scoring function. First, the logit- or confidence-based methods leverage network logits to derive a confidence measure used as an OOD scoring metric (Hendrycks & Gimpel, 2017; DeVries & Taylor, 2018; Liu et al., 2020; Huang &\nLi, 2021b; Hendrycks et al., 2022). A common baseline for this methods is the softmax score\n(Hendrycks & Gimpel, 2017), which simply uses the model softmax prediction as the OOD score.\nThen, the Energy (Liu et al., 2020) elaborates on it by computing the the LogSumExp on the log-\nits, thus offering empirical and theoretical advantages over the Softmax confidence score. Second,\nfeature-based and hybrid methods (Lee et al., 2018b; Wang et al., 2022; Sun et al., 2022; Ming et al.,\n2023; Djurisic et al., 2023; Ammar et al., 2024) exploit the model's final representation to derive the\nscoring function. Mahalanobis (Lee et al., 2018b) estimates density on ID training samples using\na mixture of class-conditional Gaussians based on the feature distributions. NECO (Ammar et al.,\n2024), on the contrary, leverages the geometric properties of Neural Collapse to construct a scoring\nfunction based on the relative norm of a sample within the subspace defined by the Simplex Equian-\ngular Tight Frame (ETF) formed by the ID data. Hybrid methods are characterized by the fact that\nthey can be augmented by using the logits as weighting factors on the scoring metrics defined on the\nfeatures.\nDouble Descent. The double descent risk curve was introduced by Belkin et al. (2019) to explain\nthe good performance observed in practice by overparameterized models (Zhang et al., 2021; Belkin\net al., 2018; Nakkiran et al., 2021) and to bridge the gap between the classical bias-variance trade-\noff theory and modern practices. Theoretical investigation into this phenomenon mainly focuses on\nvarious linear models in both regression and classification problems through the Random Matrix\nTheory (Louart et al., 2018; Liao et al., 2020; Jacot et al., 2020; Derezinski et al., 2020; Kini &"}, {"title": "3 PRELIMINARIES", "content": "Notations. For a real vector v, we denote by $||v||_2$ the euclidean norm of v. When the matrix A is\nfull rank, we denote by $A^+$ the Moore-Penrose inverse of A. We depict by $[d] := {1, . . . , d}$ the set\nof the d first natural integers. For a subset $T \\subseteq [d]$, we denote by $T^c := [d] \\backslash T$ its complement set.\nFor a subset $T\\subseteq [d]$, a d-dimensional vector $v \\in \\mathbb{R}^d$ and an $n \\times d$ matrix $A = [a^{(1)}|... a^{(n)}]^T \\in \\mathbb{R}^{n \\times d}$, we use $v_T = [v_j : j \\in T]$ to denote its $|T|$-dimensional subvector of entries from $T$\nT\nand $A_T = [a_1^{(1)}|... a_p^{(n)}]$ to denote the $n \\times |T|$ design matrix with variables from T. We\nuse $A_{min}(A)$ and $A_{max}(A)$ to depict the minimum and maximum eigenvalues of A, respectively.\n$\\mathcal{N}(0, I_d)$ denotes the standard multivariate Gaussian distribution of d random variables.\nSupervised Learning Problems. In supervised learning, we employ training dataset $\\mathcal{D} :=\n{(X_1,Y_1),......., (X_n, Y_n)}$ of n independent and identically distributed (i.i.d.) samples drawn from\nan unknown distribution $P_{x,y}$ over $\\mathcal{X} \\times \\mathcal{Y}$. Using samples from the dataset $\\mathcal{D}$, the objective is to\nfind a predictor $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ among a class of functions $\\mathcal{F}$ to predict the target $y \\in \\mathcal{Y}$ of a new\nsample $x \\in \\mathcal{X}$. In particular, given a loss function $l : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$, the objective is to minimize the\nexpected risk (or loss) defined, for all $f \\in \\mathcal{F}$, as:\n$R(f) = E_{(x,y)~P_{x,y}} [l(f(x),y)] = \\int_{\\mathcal{X} \\times \\mathcal{Y}} l(f(x),y)dP_{x,y}(x,y).$  (1)\nTypically, we choose the mean-squared loss $l(f(x), y) = (f(x) \u2013 y)^2$ for regression problems or\nthe zero-one loss $l(f(x), y) = 1_{f(x)\\neq y}$ for classification problems. We denote the optimal predictor\nby $f^* := arg min_{f \\in \\mathcal{F}} R(f)$. Since the distribution $P_{x,y}$ is unknown in practice, we instead try to\nminimize an empirical version of the expected risk based on the dataset $\\mathcal{D} := {(x_i, Y_i)}_{i=1}^{n}$ and\ndefined as\n$R_{emp}(f) = \\frac{1}{n} \\sum_{i=1}^{n} l(f(x_i), Y_i).$   (2)\nOut-of-Distribution Detection. In machine learning problems, we usually assume that the test\ndata distribution is similar to the training data distribution (the closed-world assumption). As this is\nnot the case in real-world applications, the Out-of-Distribution (OOD) detection aims to flag inputs\nthat significantly deviate from the training data to prevent unreliable predictions. In the following,\nwe denote by $P_{x,y}^{OOD}$ a distribution over $\\mathcal{X} \\times \\mathcal{Y}$ that differs from the training distribution $P_{x,y}$. OOD\ndata typically involve a semantic shift and represent concepts or labels not seen during training. A\npopular class of OOD detection techniques relies on the definition of a scoring function $s(\\cdot ; f)$,\nwhich uses the probability predictions of the classifier $f(\\cdot)$ as scores to flag an instance $x$ as OOD\nwhen the score $s(x; f)$ is below a certain threshold $\\lambda$. A common approach is to use the Maximum\nSoftmax Probability that returns the higher softmax probabilities of the predictor $f(\\cdot)$ as a scoring\nfunction to measure the prediction confidence."}, {"title": "4 DOUBLE DESCENT FOR THE BINARY CLASSIFICATION IN GAUSSIAN COVARIATE MODEL", "content": "In this section, we introduce the expected OOD risk metric and we present our main theoretical\nresults on binary least-squares classifiers applied to Gaussian data. We assume that $\\mathcal{X} \\subseteq \\mathbb{R}^d$ and\n$\\mathcal{Y} := [0, 1]$. Let $\\phi : \\mathbb{R} \\rightarrow \\mathcal{Y}$ be a mapping, we denote by $F_d := { f : \\mathcal{X} \\rightarrow \\mathcal{Y},x \\rightarrow \\phi(x^Tw) | w \\in\n\\mathbb{R}^d}$ the class of functions considered in this study.\n4.1 SYSTEM MODEL\nn\nGaussian Covariate Model & Binary Classification. We assume we have a training dataset $\\mathcal{D} :=\n{(x_i, Y_i)}_{i=1}$ of n i.i.d samples drawn from a Gaussian covariate model, i.e., from a distribution\n$P_{x,y}$ over $\\mathcal{X} \\times \\mathcal{Y}$; where $x_i \\sim \\mathcal{N}(0, I_d)$ and $y_i$ is a noisy response of $x_i$ with respect to the function\n$f^* : x \\rightarrow \\phi(x^Tw^*)$ that is defined as\n$y_i = f^*(x_i) + \\epsilon_i = \\phi(x_i^Tw^*) + \\epsilon_i,$\nwith $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma > 0$. The objective is to find a classifier $f(\\cdot) \\in F_d$ that fits $f^*(\\cdot)$. Without\nloss of generality, this problem can be interpreted as a binary classification problem where $f^*(\\cdot)$\nreturns a probability. Let $X = [x_1,..., x_n]^T \\in \\mathbb{R}^{n \\times d}$ be the data matrix containing the n samples\n$x_i \\in \\mathbb{R}^d$ as column vectors and $y = [Y_1,..., Y_n]^T \\in \\mathbb{R}^n$ be the target vector of probabilities. Given\nthe loss function $l : (\\hat{y}, y) \\rightarrow (\\hat{y} \u2013 y)^2$, in order to find $w^*$ (and thus $f^*(\\cdot)$), we want to minimize\nthe empirical risk $R_{emp}: F_d \\rightarrow \\mathbb{R}$ defined in equation 2 as\n$R_{emp}(f) = \\frac{1}{n} \\sum_{i=1}^{n} l(f(x_i), Y_i) = \\frac{1}{n} \\sum_{i=1}^{n} ((x_i^Tw) \u2013 y_i)^2 = \\frac{1}{n}||(Xw) \u2013 y||^2.$  (3)\nLeast-Squares Binary Classifiers. To analytically solve equation 3, we assume that $n < d$ and\nthat the data matrix X is full row rank. We consider a particular selection of classifiers $F_d := {f_T :\nx \\rightarrow \\phi(x^Tw) \\in F_d | T \\subseteq [d]}$, in which $f_T \\in F_d$ uses a subset $T \\subseteq [d] of features that fits\ncoefficients $w \\in \\mathbb{R}^d$ as\n$\\hat{w}_T = X_T^+y$ and $\\hat{w}_{T^c} = 0.$\n(4)\nOut-of-Distribution Risk. To measure the ability of binary classifiers $f(\\cdot) \\in F_d$ to provide pre\ndiction confidence on samples drawn from both the training distribution $P_{x,y}$ and the OOD distri\nbution $P_{x,y}^{OOD}$, we introduce an OOD risk function similar to the expected risk defined in equation 1.\nLet $f_{ood} : \\mathcal{X} \\rightarrow \\mathcal{Y}$ be the mapping chosen from $F_d$ such that $f_{ood}(x)$ is close to 0.5 when the\nsample x is more likely drawn from the $P_{x,y}^{OOD}$ and close to $f^*(x)$ when x is more likely drawn from\n$P_{x,y}$. We define the noisy response z to a given sample $x \\in \\mathcal{X}$ for the mapping $f_{ood}(\u00b7)$ as:\nz = 2 $f_{ood}$(x) - 1 + $\\epsilon^{'} = 2(x^T w_{ood})-1 + \\epsilon^{'},$\nwhere $\\epsilon' \\sim \\mathcal{N}(0, \\sigma')$ and $\\sigma' > 0$. To measure whether prediction confidences of a binary classifier\nf(\u00b7) can be used for defining an OOD scoring function, we define the Out-of-Distribution Risk\n$R_{OOD}: F_d \\rightarrow \\mathbb{R}$ as:\n$R_{OOD}(f) = E_{(x,.)~P_{x,y}} [(2f(x) \u2212 1 \u2212z)^2] + E_{(x) ~ P_{x,y}^{OOD}} [(2 f(x) - 1 \u2013 z)^2]$, (5)\nwhich depicts the expected risk of the predictor $2f(\u00b7) \u2013 1$ on the loss function $l : (\\hat{y}, y) \\rightarrow (\\hat{y} \u2013 y)^2$\nand distributions $P_{x,y}$ and $P_{x,y}^{OOD}$.\nRemark 4.1. A low value for $R_{OOD}(f)$ indicates two aspects: (i) the classifier f(\u00b7) is confident on\npredictions over the training distribution $P_{x,y}$, which corresponds to a low $E_{(x,.)~P_{x,y}} [(2f(x) \u2212\n1\u2212z)^2]$; and (ii) the classifier f(\u00b7) is not confident on predictions over the distribution $P_{x,y}^{OOD}$, which\nis reflected by a low $E_{(x)~P_{x,y}^{OOD}} [(2 f(x) - 1 \u2013 z)^2]$."}, {"title": "4.2 PREDICTION RISK", "content": "Leveraging the Random Matrix Theory and following the same line of arguments of Theorem 1 in\nBelkin et al. (2020), we derive bounds for the risk of the subset of classifiers defined in $F_d$ with\nequation 4 (see proof in Appendix A.1).\nTheorem 1. Let (p,q) \u2208 [1,d]2 such that p + q = d, T \u2286 [d], be an arbitrary subset of the d first\nnatural integers, and $T^c := [d]\\backslash T$ its complement set. Let $w \\in \\mathbb{R}^d$ such that $w_T = X_T^+y \\in \\mathbb{R}^p$\nand $\\hat{w}_{T^c} = 0 \\in \\mathbb{R}^q$. Then the expected risk with respect to the loss function $l : (\\hat{y}, y) \\rightarrow (\\hat{y} \u2013 y)^2$\nof the predictor $f_T : x \\leftrightarrow \\phi(x^T\\hat{w}) \\in F_d$ satisfies\n$A_{min}(\\Sigma)c(n, p, \\sigma) + \\sigma^2 < E_x [R(f_T)] < A_{max}(\\Sigma)c(n,p, \\sigma) + \\sigma^2,$\nwhere\n$c(\\eta, p, \\sigma) = \\begin{cases}\nn\\frac{n-p-1}{n} (||w_{T^c}||_2^2 + \\sigma^2) + ||w_T||_2^2 & \\text{if } p \\leq n-2, \\\\\n+\\infty & \\text{if } n-1  n + 2.\\\\\n\\end{cases}$   (6)\nRemark 4.4. In the case in which $\\Sigma = E_{(x,.)~P_{x,y}} [xx^T] = \\lambda I_d$ for some $\\lambda > 0$, we have\n$E_x [R(f_T)] = c(n, p, \\sigma) + \\sigma^2$. Note that Theorem 1 in Belkin et al. (2020) constitutes a special\ncase of Theorem 1 for $\\phi : x \\rightarrow x$, which corresponds to the case where $\\Sigma = I_d$.\nRemark 4.5. From Theorem 1, we have $E_x [R(f_T)] = \\infty$ around p = n. The expected risk\ndecreases again as p increases beyond n and highlights a double descent phenomenon. This result\nis consistent with the literature of double descent (Mei & Montanari, 2022; Louart et al., 2018; Liao\net al., 2020; Bach, 2024), which identifies the ratio p/n as the model complexity of a linear model\nto describe an under-(p/n < 1) and an over-(p/n > 1) parameterized regimes for the expected risk\nwith a phase transition around p/n = 1 characterized by a peak."}, {"title": "4.3 OUT-OF-DISTRIBUTION RISK", "content": "Using a similar approach to that Theorem 1, we obtain the following result on the subset of classifiers\ndefined in $F_d$ with equation 4 (see proof in Appendix A.2).\nTheorem 2. Let (p,q) \u2208 [1,d]2 such that p + q = d, T \u2286 [d], be an arbitrary subset of the d first\nnatural integers, and $T^c := [d]\\backslash T$ its complement set. Let $w \\in \\mathbb{R}^d$ such that $w_T = X_T^+y \\in \\mathbb{R}^p$\nand $\\hat{w}_{T^c} = 0 \\in \\mathbb{R}^q$. If (x, \u00b7) ~ $P_{x,y}^{OOD}$, then the expected OOD risk of the predictor $f_T : x \\rightarrow\n\\phi(x^Tw) \\in F_d$ satisfies\n$E_x [R_{OOD}(f)] \\geq (A_{min} (\\Sigma) + A_{min} (\\Sigma^{OOD})) c(n, p, \\sigma') + 2\\sigma'^2$ (1000)\nand\n$E_x [R_{OOD}(f)] \\leq (A_{max} (\\Sigma) + A_{max} (\\Sigma^{OOD}))c(n, p, \\sigma') + \\sigma'^2,$\nwhere $c(n, p, \\sigma')$ is defined in equation 6 and $\\Sigma^{OOD} \\in \\mathbb{R}^{d \\times d}$ is the positive-definite matrix defined\nas\n$\\Sigma^{OOD} = E_{(x.)~P_{x,y}^{OOD}} [xx^T \\phi' (x^Tw^*)^2] = E_{(x.)~P_{x,y}^{OOD}} \\Bigg[ xx^T \\frac{\\partial \\phi(x^Tw^*)}{\\partial w^*} \\frac{\\partial \\phi(x^Tw^*)^T}{\\partial w^*} \\Bigg].$"}, {"title": "5 EXPERIMENTS", "content": "In this section, we provide an empirical evaluation of different OOD detection methods with respect\nto the model width across multiple neural network architectures.\n5.1 SETUP\nGeneral Setup. We aim to investigate whether the double descent phenomenon, widely observed\nin model generalization setup, also extends to OOD detection. To explore this, we perform experi\nments on multiple DNN architectures: ResNet-18 (He et al., 2016), a 4-block convolutional neural\nnetwork (CNN), Vision Transformers (ViTs) (Dosovitskiy et al., 2020) and Swin Transformers (Liu\net al., 2021b).\nModel Setup. To replicate double descent, we follow the experimental setup from Nakkiran et al.\n(2021), which uses ResNet-18 as the baseline architecture. We apply a similar setup to the 4-block\nCNN model, ViTs and Swin. We vary the model capacity by altering the number of filters (denoted\nas k) per layer, with values ranging from k = 1 to k = 128. ResNet-18, which uses 64 filters,\noperates within the overparameterized regime. The depth of the models is kept constant to isolate the\neffects of width (effective model complexity). The convolutional models are trained using the cross\nentropy loss function, with a learning rate of 0.0001 and the Adam optimizer for 4 000 epochs. This\nextended training regime ensures that models converge for all explored model widths. Moreover,\neach experiment is conducted five times (with different random seeds). Finally, further details on\nthe experimental setup for the Transformers are given in the Appendix B.2.\nLabel Noise. To induce the double descent effect, we introduce label noise into the training set by\nrandomly swapping 20% of the labels. This setup simulates real-world scenarios, where noisy data\nis common. The models are trained on this noisy dataset but evaluated on a clean test set. Random\ndata augmentations, including random cropping and horizontal flipping, are applied during training."}, {"title": "5.2 EVALUATION METRICS", "content": "We evaluate both generalization and OOD detection using multiple metrics:\n\u2022 Generalization: We report the test accuracy for in-distribution (ID) classification tasks.\n\u2022 OOD Detection: We measure OOD detection performance using the area under the re\nceiver operating characteristic curve (AUC), which is threshold-free and widely adopted in\nOOD detection research. A higher AUC indicates better performance.\n\u2022 Neural Collapse: We report NC metrics, where lower values imply better performance in\nterms of both generalization and OOD detection."}, {"title": "5.3 OOD DATASETS", "content": "For OOD detection, we evaluate each model using six well-established OOD benchmark datasets:\nTextures (Cimpoi et al., 2014), Places365 (Zhou et al., 2017), iNaturalist (Van Horn et al., 2018),\na 10000 image subset from (Huang & Li, 2021a), ImageNet-O (Hendrycks et al., 2021) and"}, {"title": "5.4 OOD DETECTION METHODS", "content": "In order to have a discussion that generalises across different OOD Detection methods, we evaluate\nseveral state-of-the-art methods, categorized by the information they rely on:\n\u2022 Logit-based methods: Maximum Softmax Probability (MSP) (Hendrycks & Gimpel,\n2017), Energy scores (Liu et al., 2020), React (Sun et al., 2021), MaxLogit and KL-\nMatching (Hendrycks et al., 2022),\n\u2022 Feature-based methods: Mahalanobis distance (Lee et al., 2018b) and Residual\nscore (Wang et al., 2022).\n\u2022 Hybrid methods: ViM (Wang et al., 2022), ASH (Djurisic et al., 2023) and NECO (Ammar\net al., 2024)."}, {"title": "5.5 OOD DETECTION AND DOUBLE DESCENT", "content": "Double Descent & OOD Detection. The primary question addressed in this section is whether the\ndouble descent phenomenon extends to OOD detection, as suggested by our theoretical framework.\nThe results focus on the relative performance across underparameterized and overparameterized\nregimes. We conduct experiments using CIFAR-10 and CIFAR-100 as ID datasets, and assess OOD\ndetection across increasing model widths. Figure 1 presents the evolution of generalization error\nand OOD detection performance ($AUC$) for a challenging covariate shift scenario between CIFAR\n10 and CIFAR-100. Refer to Appendix D for more results on multiple OOD datasets. This figure\nillustrates a generalization double descent phenomenon in all models, with logit-based and hybrid\nOOD detection methods exhibiting a similar curve. This demonstrates that this phenomenon is not\nexclusive to generalization, but it extends to OOD detection as well. Moreover, the figure displays\nthe average result (from the five runs) as well as the associated variance. These can be seen to be\nvery narrow, which confirms the prevalence of the phenomena.\nFeature-Based Techniques & Interpolation Threshold. In some cases, no double descent curve\nis observed for feature-based techniques. This result suggests that the double descent depends either\non the used architecture or the data, as discussed in Appendix E.3. Furthermore, we observe that\nthe interpolation threshold is not always perfectly consistent across OOD datasets or techniques.\nThose observations are consistent with the Nakkiran et al. (2021)'s results on the CIFAR-10 and\nCIFAR-100 datasets. Those results suggest that the effective model complexity (EMC) framework\n(Nakkiran et al., 2021) defined for the generalization error can be extended to OOD detection.\nSmaller Models for OOD Detection. Interestingly, in many cases, smaller models are very good\nOOD detectors. This suggests that in applications where model pruning or DNN simplification\nis important, using smaller models may offer advantages for detecting OOD samples. Similarly,\nresource-constrained environments may benefit from lighter models as a viable option for robust\nOOD detection. The conditions under which this choice becomes optimal will be discussed in\nSection 5.6.\nDiscussion on OOD Methods. It is important to note that effective OOD detection depends on\ntwo main factors: the quality of the learned representations (which shape the feature space) and a\nreliable confidence score. Different OOD detection methods emphasize one of these factors over\nthe other. Logit-based methods rely primarily on the confidence score, which is determined by the\nmodel's output logits. These logits are typically sensitive to the model size and complexity, making\nthem closely tied to the double descent phenomenon. As a result, logit-based methods tend to exhibit\nsmoother double descent curves, with fewer drastic shifts at the interpolation threshold. In contrast,"}, {"title": "5.6 REPRESENTATION ANALYSIS AND NEURAL COLLAPSE ROLE", "content": "From Double Descent to Neural Collapse. One of the primary arguments for the interest in dou\nble descent in DNNs is that increasing model complexity beyond the interpolation threshold can lead\nto improved models, compared to those found in underparameterized local minima. However, this\nimprovement does not occur uniformly across all architectures. Although the OOD double descent\ncurve is consistently observed, each architecture exhibits a unique behavior in both generalization\nand OOD detection. These differences can be attributed to the complexities of the learned represen\ntations. We analyze the learned representations using the NC framework to understand this behavior.\nPrevious works (Papyan et al., 2020; Ming et al., 2023; Haas et al., 2023; Ammar et al., 2024) have\nshown that NC positively influences both generalization and OOD detection by ensuring stability\nand strong performance as models converge.\nBackground on Neural Collapse. Neural Collapse (NC) describes the convergence of model rep\nresentations during the late phases of training towards a low-dimensional and highly structured con\nfiguration known as the Equiangular-Tight Frame Simplex (ETF). This structure is characterized by\ndata clustering within each category, with low intra-class covariance, high inter-class separation, and\nequiangular and equinorm relationships between class representations. Appendix C provides further\ndetails on the Neural Collapse phenomenom.\nNC1-based Metric for Overparameterization Analysis. We will analyze the data clustering and\nseparation properties by leveraging the NC1 metric on the clean test set. NC1 measures the signal-to\nnoise ratio, where lower values indicate more compact intra-class clustering and greater inter-class\nseparation. The NC1 metric is computed as follows:\nNC1 = Tr $ \\frac{\\Sigma_w}{\\Sigma_B}$\\nwhere $ \\Sigma_w$ is the intra-class covariance matrix of the penultimate layer of the DNN that depicts\nnoise, $ \\Sigma_B$ is the inter-class covariance matrix of the penultimate layer of the DNN that represents\nthe signal, and C is the number of classes. As the NC1 value converges towards a lower value,\nthe activations of samples collapse toward their respective class means (see Appendix C for more\ndetails). To quantify the influence of overparameterization on the NC1 property, we compute the\nfollowing ratio:\n$\\frac{NC1_u/0}{NC1_u}$\n$\\frac{NC1_u}{NC1_0}$,\nwhere $NC1_u$ represents the NC1 value at the underparameterized local minimum, and $NC1_o$ is the\nNC1 value for the most overparameterized model. Values of $\\frac{NC1_u/0}{NC1_u}$ > 1 indicate improved data\nseparation with increased model complexity.\nAnalysis of the Results Table 1 shows that the $\\frac{NC1_u/0}{NC1_u}$ ratio strongly correlates with improve\nments in OOD detection. Models that achieve better overparameterized NC1 values tend to improve\nas their complexity increases. In contrast, the CNN model either stagnates or performs worse with\noverparameterization, as its $\\frac{NC1_u/0}{NC1_u}$ metric degrades. We also observe that logit-based methods\nare well correlated with NC1, with the exception of the MSP method on the ViT model, due to the\ndegradation in generalization. Since NC1 reflects class variability collapse and improved clustering,\nour results suggest that the separation and clustering effects of the latent space, as measured by NC,\ncan indicate OOD detection performance in the overparameterized regime. The ViT model is an\nexception, as its accuracy suffers due to the lack of pretraining. Besides the improved clustering\nand data separation provided by lower NC1, NC convergence implies that the model representation\nhas aligned with an ETF structure. This global representation structure brings more stability and\ncan provide more guarantees to performance improvement. In Appendix E.1, we will study this\nstructure through the eigenvalues to further explain the performance variability."}, {"title": "6 CONCLUSION", "content": "In this work, we conducted a theoretical and empirical study on the double descent phenomenon in\nboth classification and OOD detection. Our findings indicate that the double descent phenomenon\nalso occurs in OOD detection, with significant implications for model performance. We introduced\nthe expected OOD risk to evaluate classifiers' confidence on both training and OOD samples. Using\nRandom Matrix Theory, we demonstrated that both the expected risk and OOD risk of least-squares\nbinary classifiers applied to Gaussian models exhibit an infinite peak, when the number of param\neters is equal to the number of samples, which we associate with the double descent phenomenon.\nOur experimental study on different OOD detection methods revealed a similar double descent phe\nnomenon across multiple neural architectures. However, we observed significant variability in per\nformance among different models, with some showing no advantages from overparameterization.\nUsing the Neural Collapse (NC) framework, we revealed that OOD detection improves with overpa\nrameterization only when it enhances NC convergence, boosting the performance of OOD detection\nmethods. This emphasizes the crucial role of learned representations in the performance of overpa\nrameterized models and their significance in model selection.\nWe hope our insights and extensive experiments will benefit practitioners in OOD detection and\ninspire further theoretical research into this aspect of DNNs. Ultimately, although this paper in\ntroduces a novel theoretical framework for understanding the double descent phenomenon in OOD\ndetection, its theoretical scope has some limitations including a focus on binary classification, the\nchoice of loss function, and specific model architectures. Hence solving these limitations would be\na valuable direction for future work."}]}