{"title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment", "authors": ["Yuze Zheng", "Zixuan Li", "Xiangxian Li", "Jinxing Liu", "Yuqing Wang", "Xiangxu Meng", "Lei Meng"], "abstract": "Image classification models often demonstrate unstable performance in real-world applications due to variations in image information, driven by differing visual perspectives of subject objects and lighting discrepancies. To mitigate these challenges, existing studies commonly incorporate additional modal information matching the visual data to regularize the model's learning process, enabling the extraction of high-quality visual features from complex image regions. Specifically, in the realm of multimodal learning, cross-modal alignment is recognized as an effective strategy, harmonizing different modal information by learning a domain-consistent latent feature space for visual and semantic features. However, this approach may face limitations due to the heterogeneity between multimodal information, such as differences in feature distribution and structure. To address this issue, we introduce a Multi-modal Alignment and Reconstruction Network (MARNet), designed to enhance the model's resistance to visual noise. Importantly, MARNet includes a cross-modal diffusion reconstruction module for smoothly and stably blending information across different domains. Experiments conducted on two benchmark datasets, Vireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves the quality of image information extracted by the model. It is a plug-and-play framework that can be rapidly integrated into various image classification frameworks, boosting model performance.", "sections": [{"title": "1 Introduction", "content": "Visual classification is a critical task in the field of computer vision[28][35][3][15].\nHowever, the quality of visual images is susceptible to various factors, including\nbut not limited to, interference from non-main elements and changes in lighting\nangles, leading to inconsistent performance in image classification[34][33][18].\nWith the rapid development of social media platforms, a vast amount of tex-\ntual information related to visual images has emerged. These pieces of informa-\ntion present a complex relationship of mutual dependence and complementarity,"}, {"title": "2 Related Work", "content": "In common cross-modal learning scenarios, there is a clear distribution difference\nin the representation space among different modal data, and representations of"}, {"title": "2.1 Cross-modal Alignment", "content": "In recent years, researchers in multimodal learning have commonly adopted\ncross-modal representation alignment strategies to reduce the heterogeneity be-\ntween different modal information. These strategies can be broadly divided into\ntwo categories: those based on distance metrics [16][13][12]and those based on\ncontrastive learning[32][11][36]. Distance-based alignment methods mainly con-\nstrain the spatial distance between different sources of information, such as cat-\negory center distance or decision space distance, to effectively mitigate the prob-\nlem of distance differences between modal information in the information space.\nIn contrast, contrastive learning-based alignment methods divide multimodal in-\nformation into positive and negative samples and enhance the similarity between\npositive samples while separating them from negative samples by comparing sam-\nple similarities. This approach strengthens the distinction and interactivity of\ninformation in the representation space. However, both methods tend to focus on\nthe distance between representations while aligning cross-modal representations\nas much as possible, neglecting the significant distribution differences between\ndifferent modal representations, which is a challenge that needs to be addressed\nin multimodal learning."}, {"title": "2.2 Diffusion Models for Representation Learning", "content": "The diffusion model is inspired by non-equilibrium thermodynamics[30]. Ho et\nal.[9] treat the diffusion process as a Markov chain by progressively adding ran-\ndom noise to the data. They train neural networks to learn the diffusion process,\nenabling them to denoise images corrupted with Gaussian noise.\nCurrently, diffusion models are mostly applied to generative tasks[17][31]. In\ncross-modal diffusion models, there are commonly two approaches. One is using\nclassifier-free guidance[10], where text is used as a condition to guide image\ngeneration with noise. The other is simultaneously adding noise from multiple\nmodalities into the network for multi-modal generation[22].\nIn terms of network structures used in diffusion models, U-Net architec-\nture is commonly employed in the image domain for noise prediction, with\nintra-layer changes in image channels[9]. Additionally, some studies have uti-\nlized MLP structures for diffusion in user-item interactions without channel\ndimensions[20][21], focusing on simpler feature transformations.\nRegarding image classification tasks based on diffusion models, Li et al.[14]\nintroduced a method to evaluate diffusion models as zero-shot classifiers. Clark et\nal.[4] used density estimation calculated by a large-scale text-to-image generation\nmodel for zero-shot classification."}, {"title": "3 Method", "content": "In this section, we elaborate on how our proposed framework(MARNet), aligns\nimage-text sample pairs in the representation space through embedding match-"}, {"title": "3.1 Overall Approach", "content": "Our goal is to learn more multi-dimensional and rich visual representations\nfrom paired image-text data through privileged information learning, in order\nto improve the classification performance of Multi-Modal Alignment and Re-\nconstruction Network(MARNet). More specifically, we treat the precious and\nscarce ingredient data as privileged information to guide the representation of\nimage data, that is, the training samples consist of N pairs of image-text data\n$S_N = \\{(P_1, i_1), (P_2, i_2), ..., (P_n, i_n)\\}$, while the test samples only contain M pieces\nof photo data $S_M = \\{P_1,P_2, ..., p_m\\}$. We use a visual encoder $F_v$ to extract the\nrepresentations of image data $R_v = \\{x_1^v, x_2^v,...,x_n^v\\}$,where $x^v = F_v(p)$, and sim-\nilarly for text data, $R_s = \\{x_1^s, x_2^s,...,x_n^s\\}$,where $x^s = F_s(i)$. We take the visual\nrepresentations $x^v$ and semantic representations $x_s$ as inputs for the subsequent\ntwo modules. In the embedding matching alignment module, we finely align the\ncross-modal representations through contrastive matching learning and generate\nrepresentations $X_{EMA}$. In the cross-modal diffusion reconstruction module, we\nadopt an improved diffusion model to stably and smoothly infiltrate the visual\nrepresentations $x^v$ into the semantic representations $x_s$ and sample to gener\nate representations $X_{CDR}$ from Gaussian noise $N_G$. Finally, we fuse the output"}, {"title": "3.2 Embedding Matching Alignment", "content": "Based on the positive and negative sample matching alignment method of con-\ntrastive learning, we adopt an instance-wise Alignment (ITA) approach [32]. This\nalignment method is an improvement based on InfoNCE [26], which calculates\nthe matching similarity($Sim(x,x)$) of image-text representations in feature\nspace within a batch as a constraint to align cross-domain information. When\nenhancing the similarity of a set of image-text representation pairs using positive\nand negative sample matching methods, it also reduces the matching degree of\nthe visual representation $x_i^v$ with other semantic representations $x_j^s$, where $i \\neq j$.\nDefinition of cosine similarity is as follows:\n$Sim(x_i^v, x_j^s) = \\frac{x_i^v \\cdot x_j^s}{||x_i^v|| ||x_j^s||}$    (5)\nwhere $x_i^v \\cdot x_j^s$ is the dot product of vectors, and $||x_i^v|| ||x_j^s||$ is the product of the\nmodulus of vectors.\nWe design two encoders $E_v$ and $E_s$, each consisting of a linear layer, and\nuse an activation function $g(x)$ (i.e., LeakyReLU). The encoder $E_v$ is used to\nmap visual representations $x^v$, while the encoder $E_s$ is used to map semantic\nrepresentations $x_s$, both to the same feature space $R^d$.\n$g(x) =\\{\n    x, & \\text{if } x > 0 \\\\\n    \\tau x, & \\text{otherwise}\n\\}$\n(6)\n$x'^v_i = g(E_v(x^v_i)), x'^v_i = [x_i^1, x_i^2, ..., x_i^d]^T$\n(7)\n$x'^s_i = g(E_s(x^s_i)), x'^s_i = [x_i^1, x_i^2, ..., x_i^d]^T$\n(8)\nwhere $\\alpha$ is set to the default value of 0.01.\nWe use the mutual matching cosine similarity between visual representation\n$x'^v_i$ and semantic representation $x'^s_j$ as the alignment constraint $L_{ITA}$.\n$L_{v2s} = -log \\frac{exp \\big(Sim \\big((x'^v_i, x'^s_i)/\\tau\\big)}{\\sum_{b=1}^B exp \\big(Sim \\big((x'^v_i, x'^s_b)/\\tau \\big)}$\n(9)\n$L_{s2v} = -log \\frac{exp \\big(Sim \\big((x'^s_i, x'^v_i)/\\tau\\big)}{\\sum_{b=1}^B exp \\big(Sim \\big((x'^s_i, x'^v_b)/\\tau \\big)}$\n(10)\n$L_{ITA} = L_{v2s} + L_{s2v}$\n(11)"}, {"title": "3.3 Cross-Modal Diffusion Reconstruction", "content": "In this module, we further interact the visual representations $x_v$ and semantic\nrepresentations $x_s$ based on the diffusion model. Through the diffusion model,\nwe alleviate the impact of background noise in the visual representation $x_v$ and,\nwith the assistance of semantic representation $x_s$, generate more robust visual\nobject representations $x_r$. In the following sections, we will first introduce the\nbackground of diffusion models and then describe the cross-modal reconstruction\nprocess based on diffusion model.\nBackground of Diffusion Models The denoising diffusion probabilistic model\nmainly consists of two processes: a forward process q with diffusion and noise\naddition, and a reverse process p with reconstruction and denoising. In the for-\nward process q, Gaussian noise is gradually added to the original training data\nx over T time steps, following a Markov process:\n$q(x_t | x_{t-1}) = N (x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$\n(14)\n$q(x_t | x_0) = N (x_t; \\sqrt{\\bar{a_t}}x_0, (1 - \\bar{a}_t) I)$\n(15)\nwhere $x_0 \\sim q(x)$, $N(.)$ means a Gaussian distribution, $\\beta_t$ determines the noise\nschedule. $a_t = 1 - \\beta_t$ and $\\bar{a}_t = \\prod_{s=0}^t a_s$, utilize the above formula to sample the\nnoisy sample $x_t$ at any step t from $x_0$.\nIn reverse process, the data is reconstructed by the model. The optimization\nobjective of the model is to maximize likelihood estimation $p_\\theta(x_0)$ of the true\ndata distribution, where $\\theta$ represents the parameters learned by a neural network.\n$\\mu_\\theta (x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\bigg( \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} x_\\epsilon (x_t, t) \\bigg)$\n(16)\n$p_\\theta (x_{t-1} | x_t) = N (x_{t-1}; \\mu_\\theta (x_t,t), \\sigma_\\theta (x_t, t))$\n(17)"}, {"title": "3.4 Multi-Modal Embedding Fusion", "content": "In the case of conditional guided generation, we have a data pair $(x_0, y_0) ~$\n$(x, y)$. Similar to the above formula, we can derive:\n$\\mu_\\theta (x_t, t,y) = \\frac{1}{\\sqrt{\\alpha_t}}\\bigg( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta (x_t, t, y) \\bigg)$\n(18)\nWe employ a diffusion model to reconstruct\nthe representations extracted by the base model across modalities. Firstly, we\ndesign an multi-layer perceptron(MLP) consisting of four linear layers and acti-\nvation functions for predicting $\\epsilon = \\epsilon_\\theta(x_0, t, x_v)$ during the reverse process.\nIn the forward process of the diffusion model, we treat the semantic represen-\ntation $x_s$ as the input to the diffusion model while using the visual representation\n$x_v$ as a guiding condition. We smoothly interact the cross-modal representation\ninformation by gradually injecting noise. The diffusion model is to minimize the\ndistant between $\\epsilon$ and $\\epsilon_\\theta$.\nIn this process, we construct the representation generation constraint by\ncalculating mean squared error (MSE) between the reconstructed semantic fea-\ntures $\\hat x_s$ and the original input text features $x_s^i$. To enhance the performance\nof the generated representation on downstream tasks, similar to the Embedding\nMatching Align module, we introduce the cross-entropy constraint to assist in\nthe training of the Cross-Modal Diffusion Recon module:\n$L_{MSE} = ||\\hat x_s(x_t, t, x_v) - x_s^i||^2$\n(19)\n$L_{CDR} = \\lambda_2\\cdotL_{CE} + \\gamma \\cdot L_{MSE}$\n(20)\nwhere $\\lambda_2$ and $\\gamma$ represent constraint weights, the $L_{CE}$ has been given in Equa-\ntion (12).\nSubsequently, during the reverse process, we initialize random Gaussian noise\nas the model input. According to Bayes theorem, $p_\\theta(x_{t - 1}|x_t)$ can be calculated\naccording to the following definition:\n$\\beta_t = \\frac{1 - a_{t-1}}{1-\\bar{a}_{t}} - \\frac{\\beta_t \\sqrt{a_t}}{\\sqrt{a_{t-1}} \\beta_t}$\n(21)\n$\\mu_\\theta (x_t, t, x_v) = \\frac{\\sqrt{a_t}(1-\\bar{a}_{t-1})}{\\sqrt{a_{t}}(1-\\bar{a}_{t})} \\hat x_s(x_t, t, x_v) + \\frac{\\sqrt{(1-a_{t-1})} \\beta_t}{1-\\bar{a}_{t}} x_t$\n(22)\n$p_\\theta (x_{t-1}|x_t) = N (x_{t-1}; \\mu_\\theta (x_t, t, x_v), \\beta_tI)$\n(23)\nSimilarly, guided by the visual representation $x_v$, we generate the represen-\ntations $X_{CDR}$ across modalities."}, {"title": "3.4 Multi-Modal Embedding Fusion", "content": "In the final phase, we combine the representations, $X_{EMA}$ and $X_{CDR}$, outputted\nby the previous two modules to realize the complementation and enhancement\nof information across modalities."}, {"title": "4 Experiment", "content": "4.1\nExperiment Settings\nDatasets We conducted various experiments on the task of image classification\nusing the following two datasets:\nVireofood-172[2]: A single-label classification dataset containing 110,241\ndish images across 172 categories, including 353 textual descriptions, averages\nthree texts per image. Following the settings in the original paper, we divided\nthe dataset into 66,071 images for training and 33,154 images for testing.\nIngredient-101[1]: A single-label classification dataset comprising 93,425\ndish images from the Food-101 dataset, featuring 446 common ingredients across\n101 categories, averaging 9 ingredients per dish. According to the original paper's\nsettings, we utilized a training set consisting of 68,175 data pairs and a testing\nset comprising 25,250 data pairs.\nPerformance Metrics Since both datasets we used are single-label datasets,\nwe employed accuracy rate as the performance evaluation metric:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+ FN}$\n(25)\nwhere TP is the number of true positive samples, TN is the number of true neg-\native samples, FP is the number of false positive samples, and FN is the number\nof false negative samples. For the above indicator, we calculate the average value\nof top-1 and top-5."}, {"title": "4.2 Performance Analysis", "content": "To verify the effectiveness of our proposed MARNet in enhancing image classi-\nfication performance and model robustness, we conducted experiments divided\ninto two categories: basic visual classification and cross-modal alignment that in-\ncorporates textual information. In the visual network, we selected common struc-\ntures such as Vision Transformer (ViT) and residual neural networks (ResNet).\nIn the alignment network, we tried different alignment methods based on ViT-\nB/16 and BERT models, including distance measurement and similarity com-\nparison. Table 1 shows the experimental results.\nIn experiments on visual networks, we can draw the following conclusions:"}, {"title": "4.3 Ablation Study", "content": "To validate the effectiveness of modules in MARNet, we conducted ablative\nexperiments using ViT model as baseline. The results are shown in Table 2.\nDespite the potential noise interference in the image information, the base-\nline model demonstrates decent performance due to the detailed perception\ncapability of ViT and the assistance of attention mechanisms.\nAfter incorporating the EMA module and introducing high-quality textual\ninformation and alignment through contrastive matching, the model's per-\nformance improved significantly. However, the presence of residual noise and\ninterfering factors in the visual information limits the effect of alignment.\nBy incorporating the CDR module, we facilitate profound interaction be-\ntween visual and textual representations to derive representations founded\non visual cues, thereby diminishing the impact of peripheral visual elements.\nMoreover, to mitigate the impact of the MLP component within the CDR\nmodule, we introduce a validation process that exclusively leverages the\nMLP-mapped features for assessing the efficacy of the CDR module.\nIn the end, by integrating the EMA and CDR modules, we synergistically\nenhance the alignment representation and reconstruction representation, fur-\nther strengthening the visual representation and model robustness."}, {"title": "5 Conclusion", "content": "In this article, we tackle the issue of heterogeneity in multi-modal data by in-\ntroducing the Multi-Modal Alignment and Reconstruction Network (MARNet).\nThis network addresses the disparities in distance and distribution within the\nfeature space through a dual approach: embedding matching alignment (EMA)\nmodules and cross-modal diffusion reconstruction(CDR) modules. Our experi-\nmental findings validate that MARNet significantly improves the quality of visual\ninformation and optimizes the distribution of representations.\nMoving forward, our efforts will concentrate on reducing noise interference\nduring reconstruction phase of the diffusion model, with the overarching goal of\npreserving the integrity of original information to the greatest extent possible."}]}