{"title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment", "authors": ["Yuze Zheng", "Zixuan Li", "Xiangxian Li", "Jinxing Liu", "Yuqing Wang", "Xiangxu Meng", "Lei Meng"], "abstract": "Image classification models often demonstrate unstable performance in real-world applications due to variations in image information, driven by differing visual perspectives of subject objects and lighting discrepancies. To mitigate these challenges, existing studies commonly incorporate additional modal information matching the visual data to regularize the model's learning process, enabling the extraction of high-quality visual features from complex image regions. Specifically, in the realm of multimodal learning, cross-modal alignment is recognized as an effective strategy, harmonizing different modal information by learning a domain-consistent latent feature space for visual and semantic features. However, this approach may face limitations due to the heterogeneity between multimodal information, such as differences in feature distribution and structure. To address this issue, we introduce a Multi-modal Alignment and Reconstruction Network (MARNet), designed to enhance the model's resistance to visual noise. Importantly, MARNet includes a cross-modal diffusion reconstruction module for smoothly and stably blending information across different domains. Experiments conducted on two benchmark datasets, Vireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves the quality of image information extracted by the model. It is a plug-and-play framework that can be rapidly integrated into various image classification frameworks, boosting model performance.", "sections": [{"title": "1 Introduction", "content": "Visual classification is a critical task in the field of computer vision[28][35][3][15]. However, the quality of visual images is susceptible to various factors, including but not limited to, interference from non-main elements and changes in lighting angles, leading to inconsistent performance in image classification[34][33][18]. With the rapid development of social media platforms, a vast amount of textual information related to visual images has emerged. These pieces of information present a complex relationship of mutual dependence and complementarity,"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Cross-modal Alignment", "content": "In common cross-modal learning scenarios, there is a clear distribution difference in the representation space among different modal data, and representations of"}, {"title": "2.2 Diffusion Models for Representation Learning", "content": "The diffusion model is inspired by non-equilibrium thermodynamics[30]. Ho et al.[9] treat the diffusion process as a Markov chain by progressively adding random noise to the data. They train neural networks to learn the diffusion process, enabling them to denoise images corrupted with Gaussian noise.\nCurrently, diffusion models are mostly applied to generative tasks[17][31]. In cross-modal diffusion models, there are commonly two approaches. One is using classifier-free guidance[10], where text is used as a condition to guide image generation with noise. The other is simultaneously adding noise from multiple modalities into the network for multi-modal generation[22].\nIn terms of network structures used in diffusion models, U-Net architecture is commonly employed in the image domain for noise prediction, with intra-layer changes in image channels[9]. Additionally, some studies have utilized MLP structures for diffusion in user-item interactions without channel dimensions[20][21], focusing on simpler feature transformations.\nRegarding image classification tasks based on diffusion models, Li et al.[14] introduced a method to evaluate diffusion models as zero-shot classifiers. Clark et al.[4] used density estimation calculated by a large-scale text-to-image generation model for zero-shot classification."}, {"title": "3 Method", "content": "In this section, we elaborate on how our proposed framework(MARNet), aligns image-text sample pairs in the representation space through embedding match-"}, {"title": "3.1 Overall Approach", "content": "Our goal is to learn more multi-dimensional and rich visual representations from paired image-text data through privileged information learning, in order to improve the classification performance of Multi-Modal Alignment and Reconstruction Network(MARNet). More specifically, we treat the precious and scarce ingredient data as privileged information to guide the representation of image data, that is, the training samples consist of N pairs of image-text data $S_N = \\{(p_1, i_1), (p_2, i_2), ..., (p_n, i_n)\\}$, while the test samples only contain M pieces of photo data $S_M = \\{p_1, p_2, ..., p_m\\}$. We use a visual encoder $F_v$ to extract the representations of image data $R_v = \\{x_1^v, x_2^v,...,x_n^v\\}$,where $x_v = F_v(p)$, and similarly for text data, $R_s = \\{x_1^s, x_2^s,...,x_n^s\\}$,where $x_s = F_s(i)$. We take the visual representations $x_v$ and semantic representations $x_s$ as inputs for the subsequent two modules. In the embedding matching alignment module, we finely align the cross-modal representations through contrastive matching learning and generate representations $x^{EMA}$. In the cross-modal diffusion reconstruction module, we adopt an improved diffusion model to stably and smoothly infiltrate the visual representations $x_v$ into the semantic representations $x_s$ and sample to generate representations $x^{CDR}$ from Gaussian noise $\\mathcal{N}_G$. Finally, we fuse the output"}, {"title": "3.2 Embedding Matching Alignment", "content": "Based on the positive and negative sample matching alignment method of contrastive learning, we adopt an instance-wise Alignment (ITA) approach[32]. This alignment method is an improvement based on InfoNCE [26], which calculates the matching similarity(Sim(x,x)) of image-text representations in feature space within a batch as a constraint to align cross-domain information. When enhancing the similarity of a set of image-text representation pairs using positive and negative sample matching methods, it also reduces the matching degree of the visual representation $x_v$ with other semantic representations $x_i^s$, where $i \\neq j$. Definition of cosine similarity is as follows:\n$\\text{Sim}(x_i, x_j) = \\frac{x_i \\cdot x_j}{||x_i|| ||x_j||}$ (5)\nwhere $x_i \\cdot x_j$ is the dot product of vectors, and $||x_i|| ||x_j||$ is the product of the modulus of vectors.\nWe design two encoders $E_v$ and $E_s$, each consisting of a linear layer, and use an activation function $g(x)$ (i.e., LeakyReLU). The encoder $E_v$ is used to map visual representations $x_v$, while the encoder $E_s$ is used to map semantic representations $x_s$, both to the same feature space $R^d$.\n$g(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\tau x, & \\text{otherwise} \\end{cases}$ (6)\n$x_v' = g(E_v(x_v)), x_v' = [x_1^v, x_2^v, ..., x_d^v]$ (7)\n$x_s' = g(E_s(x_s)), x_s' = [x_1^s, x_2^s, ..., x_d^s]$ (8)\nwhere $\\alpha$ is set to the default value of 0.01.\nWe use the mutual matching cosine similarity between visual representation $x_v$ and semantic representation $x_s$ as the alignment constraint $\\mathcal{L}_{ITA}$.\n$\\mathcal{L}_{v2s} = -\\text{log} \\frac{\\text{exp} \\left(\\text{Sim}\\left(x_v', x_s'\\right) / \\tau\\right)}{\\sum_{b=1}^{B} \\text{exp} \\left(\\text{Sim}\\left(x_v', x_b^s\\right) / \\tau\\right)}$ (9)\n$\\mathcal{L}_{s2v} = -\\text{log} \\frac{\\text{exp} \\left(\\text{Sim}\\left(x_s', x_v'\\right) / \\tau\\right)}{\\sum_{b=1}^{B} \\text{exp} \\left(\\text{Sim}\\left(x_s', x_b^v\\right) / \\tau\\right)}$ (10)\n$\\mathcal{L}_{ITA} = \\mathcal{L}_{v2s} + \\mathcal{L}_{s2v}$ (11)"}, {"title": "3.3 Cross-Modal Diffusion Reconstruction", "content": "In this module, we further interact the visual representations $x_v$ and semantic representations $x_s$ based on the diffusion model. Through the diffusion model, we alleviate the impact of background noise in the visual representation $x_v$ and, with the assistance of semantic representation $x_s$, generate more robust visual object representations $x_r$. In the following sections, we will first introduce the background of diffusion models and then describe the cross-modal reconstruction process based on diffusion model.\nBackground of Diffusion Models The denoising diffusion probabilistic model mainly consists of two processes: a forward process $q$ with diffusion and noise addition, and a reverse process $p$ with reconstruction and denoising. In the forward process $q$, Gaussian noise is gradually added to the original training data $x$ over T time steps, following a Markov process:\n$q(x_t | x_{t-1}) = \\mathcal{N} \\left(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}\\right)$ (14)\n$q(x_t | x_0) = \\mathcal{N} \\left(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I}\\right)$ (15)\nwhere $x_0 \\sim q(x)$, $\\mathcal{N}(.)$ means a Gaussian distribution, $\\beta_t$ determines the noise schedule. $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=0}^{t} \\alpha_s$, utilize the above formula to sample the noisy sample $x_t$ at any step $t$ from $x_0$.\nIn reverse process, the data is reconstructed by the model. The optimization objective of the model is to maximize likelihood estimation $p_{\\theta}(x_0)$ of the true data distribution, where $\\theta$ represents the parameters learned by a neural network.\n$\\mu_{\\theta}(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t, t)\\right)$ (16)\n$p_{\\theta} \\left(x_{t-1} | x_t\\right) = \\mathcal{N} \\left(x_{t-1}; \\mu_{\\theta}(x_t, t), \\sigma_{\\theta}(x_t, t)\\right)$ (17)"}, {"title": "Cross-Modal Reconstruction", "content": "We employ a diffusion model to reconstruct the representations extracted by the base model across modalities. Firstly, we design an multi-layer perceptron(MLP) consisting of four linear layers and activation functions for predicting $\\epsilon_{\\theta} = \\chi_{\\Theta} (x_0x, t, x_v)$ during the reverse process.\nIn the forward process of the diffusion model, we treat the semantic representation $x_s$ as the input to the diffusion model while using the visual representation $x_v$ as a guiding condition. We smoothly interact the cross-modal representation information by gradually injecting noise. The diffusion model is to minimize the distant between $\\hat{x_s}$ and $x_0$.\nIn this process, we construct the representation generation $l_2$ constraint by calculating mean squared error (MSE) between the reconstructed semantic features $\\hat{x_s}$ and the original input text features $x_s$. To enhance the performance of the generated representation on downstream tasks, similar to the Embedding Matching Align module, we introduce the cross-entropy constraint to assist in the training of the Cross-Modal Diffusion Recon module:\n$\\mathcal{L}_{MSE} = ||\\hat{X_0}(x, t, x_v) - x_s||^2$ (19)\n$\\mathcal{L}_{CDR} = \\lambda_2 \\cdot \\mathcal{L}_{CE} + \\gamma \\cdot \\mathcal{L}_{MSE}$ (20)\nwhere $\\lambda_2$ and $\\gamma$ represent constraint weights, the $\\mathcal{L}_{CE}$ has been given in Equation (12).\nSubsequently, during the reverse process, we initialize random Gaussian noise as the model input. According to Bayes theorem, $p_{\\theta}(x_{t - 1} | x_t)$ can be calculated according to the following definition:\n$\\beta_t \\frac{1 - \\alpha_{t-1}}{1 - \\bar{\\alpha}_{t-1}} = \\frac{\\sqrt{\\bar{\\alpha}_t}\\beta_t}{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1})}$ (21)\n$\\mu_{\\theta'}(x_t, t, x_v) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_0(x, t, x_v) + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} x_t$ (22)\n$p_{\\theta'}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta'}(x, t, x_v), \\beta_t \\mathbf{I})$ (23)\nSimilarly, guided by the visual representation $x_v$, we generate the representations $x^{CDR}$ across modalities."}, {"title": "3.4 Multi-Modal Embedding Fusion", "content": "In the final phase, we combine the representations, $x^{EMA}$ and $x^{CDR}$, outputted by the previous two modules to realize the complementation and enhancement of information across modalities."}, {"title": "5 Conclusion", "content": "In this article, we tackle the issue of heterogeneity in multi-modal data by introducing the Multi-Modal Alignment and Reconstruction Network (MARNet). This network addresses the disparities in distance and distribution within the feature space through a dual approach: embedding matching alignment (EMA) modules and cross-modal diffusion reconstruction(CDR) modules. Our experimental findings validate that MARNet significantly improves the quality of visual information and optimizes the distribution of representations.\nMoving forward, our efforts will concentrate on reducing noise interference during reconstruction phase of the diffusion model, with the overarching goal of preserving the integrity of original information to the greatest extent possible."}]}