{"title": "Mechanistic Understanding of Language Models in Syntactic Code Completion", "authors": ["Samuel Miller", "Daking Rai", "Ziyu Yao"], "abstract": "Recently, language models (LMs) have shown impressive proficiency in code generation tasks, especially when fine-tuned on code-specific datasets, commonly known as Code LMs. However, our understanding of the internal decision-making processes of Code LMs, such as how they use their (syntactic or semantic) knowledge, remains limited, which could lead to unintended harm as they are increasingly used in real life. This motivates us to conduct one of the first Mechanistic Interpretability works to understand how Code LMs perform a syntactic completion task, specifically the closing parenthesis task, on the CodeLlama-7b model (Roziere et al. 2023). Our findings reveal that the model requires middle-later layers until it can confidently predict the correct label for the closing parenthesis task. Additionally, we identify that while both multi-head attention (MHA) and feed-forward (FF) sub-layers play essential roles, MHA is particularly crucial. Furthermore, we also discover attention heads that keep track of the number of already closed parentheses precisely but may or may not promote a correct number of closing parentheses that are still missing, leading to a positive or negative impact on the model's performance.", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) have recently showcased impressive capabilities in code-related tasks, generating executable code from task specifications provided in natural language prompts (Jiang et al. 2024; Zan et al. 2023). To further enhance their capability, they are often instruction-tuned on code-specific datasets, resulting in specialized models known as Code LMs, such as CodeLlama (Roziere et al. 2023), StarCoder (Li et al. 2023), Code Gemma (Team et al. 2024), and DeepSeek-Coder (Guo et al. 2024). These advancements have led to widespread adoption by programmers and researchers, integrating LMs into their daily workflows to assist with coding tasks (Dakhel et al. 2023). However, despite substantial progress in code generation, Code LMs can generate incorrect code, particularly when handling complex tasks (Dou et al. 2024; Tambon et al. 2024). These faulty codes can pose significant risks, especially when used by novice programmers working on critical applications (Dakhel et al. 2023). Furthermore, even when the generated code snippets are functionally correct, recent research found that these code snippets could contain security vulnerabilities (Siddiq and Santos 2022; Yang et al. 2024). Understanding the capabilities and limitations of Code LMs, such as where and how they invoke relevant knowledge internally, is therefore essential to mitigate potential harm in high-stakes scenarios. Yet, there is limited knowledge about the internals of these LMs while generating code for a given task. Developing deeper insights into their knowledge-relevant mechanisms is crucial for improving their reliability, performance, and safe deployment. Mechanistic interpretability (MI) has recently emerged as a promising approach to understanding the internal mechanisms of LMs (Olah et al. 2020; Elhage et al. 2021; Rai and Yao 2024; Bereska and Gavves 2024). MI studies have investigated a range of LM behaviors, including in-context learning (Elhage et al. 2021; Bansal et al. 2022; Ren et al. 2024), reasoning (Stolfo, Belinkov, and Sachan 2023; Rai and Yao 2024; Dutta et al. 2024), and fact recall (Geva et al. 2023; Chughtai, Cooney, and Nanda 2024), providing valuable insights into how various LM components, such as multi-head attention (MHA) and feedforward (FF) sublayers, contribute to these capabilities. While substantial work has been done to investigate various behaviors of LMs, there has been limited focus on understanding how Code LMs internally use their knowledge in code generation tasks. To address this gap, we present one of the first MI studies on Code LMs, where we investigate the internal workings of the CodeLlama-7b (Roziere et al. 2023) for the syntax completion task, the success of which requires an LM to not only locate its declarative knowledge of the programming language but also use the knowledge with its other capabilities (e.g., counting) smartly. Specifically, we study how CodeLlama-7b performs the closing parentheses task (e.g., print(str(1 \u2192 ) ) ), where each opening parenthesis must be paired with a closing parenthesis. To this end, we first contribute a synthetic dataset for systematically studying a Code LM's syntax completion performance. Our dataset includes a total 168 prompts covering three sub-tasks with the number of closing parentheses in the target tokens being 2, 3, and 4, respectively. These prompts include recursive calls of class constructors including str, list, and set, with the number of open parentheses ranging from 2 to 12. With our dataset, we perform a series of analyses investigating the"}, {"title": "2 Methodology and Dataset", "content": "In this section, we introduce our methodology towards forming a mechanistic understanding of how a Code LM performs a syntax completion task that requires using its knowledge about a programming language in combination with others (e.g., counting). Our experiment will focus on Python code generation using CodeLlama-7b (Roziere et al. 2023), which is a state-of-the-art (SOTA) medium-size Code LM with a 32-layer decoder-only transformer architecture. The specific model checkpoint we use is \"CodeLlama-7b-hf\" (i.e., the base model). In what follows, we will first give an overview of our experiment design, then present our process of dataset generation to facilitate the experiment, and finally describe the methods we will use to analyze a Code LM."}, {"title": "2.1 Motivation and Overview", "content": "Syntax completion is a crucial and fundamental part of LM code generation, as syntactic correctness is essential for code to be executable. Dou et al. (2024) recently found that even SOTA Code LMs still suffer from syntactic issues to various extents in their generation. This has motivated us to carefully understand how a Code LM performs syntax completion. In our work, we select the closing parentheses task (e.g., print (str(1\u2192))) as our task, given it is one of the most common syntactical structures seen across programming languages; as a result, it is safe to assume that SOTA Code LMs have learned the necessary syntactic knowledge from their training. The input provided to a Code LM in this task is a partially complete line of code (e.g., print (str (1), which includes a varying number of function or class constructor calls but is missing some final number of closing parentheses that needs to be predicted as a whole in its next token. The Code LM is then tasked with predicting the next token that consists of the necessary number of closing parentheses for the line of code to be syntactically correct (e.g.,)) in the running example). In our experiments, we focus on analyzing the Code LM in Python programming language. This task was further broken down into three sub-tasks based on the number of closing parentheses required to correctly complete the line of code, which were two, three, and four closing parentheses. A partially completed line of code example for each of the sub-tasks can be seen in Table 1."}, {"title": "2.2 Data Generation", "content": "To evaluate the Code LM on the closing parentheses task, we created an initial synthetic dataset consisting of 168 input prompts, with each prompt consisting of a simple natural language instruction, in the form of a code comment that describes the desired semantic meaning of the following line of code, and a partially completed line of Python code. In our preliminary exploration, we found the code comment to be necessary to avoid semantic ambiguity, as otherwise there could be infinite plausible continuations of the same line of code (e.g., continuing \"print(str(1\" with more digits), which will make the analysis difficult. We began the dataset preparation process by searching for Python functions that were both commonly used in practice and could accept arguments of varying data types. To this end, we decided to initially focus on generating prompts that utilized the built-in print function while varying the argument supplied to the function. The argument was varied through the selection of a Python built-in class constructor, from a set containing str, list, and set, the integer value passed to the constructor, and the number of nested constructor calls (with the number of open parentheses ranging from 2 to 12 in our data synthesis), which was used to vary the number of"}, {"title": "2.3 Methodology", "content": "We will understand how a Code LM completes a syntax completion task by mainly looking at the model's behaviors of predicting the correct next token, which requires proper use of its knowledge about the programming language. Specifically, we employ logit lens or direct logit attribution (nostalgebraist 2020) to analyze the contribution of each layer and its sublayers (MHA and FF) in predicting the correct next token. Logit lens allows us to view what the LM would have predicted in a given (sub-)layer by projecting the intermediate activations (denoted as $v \\in \\mathbb{R}^d$, where d is the LM dimension) onto the logit distribution through multiplication with the unembedding parameter matrix (denoted as $W_u \\in \\mathbb{R}^{d \\times |V|}$, where V is the vocabulary set and |V| denotes its size), i.e., $vW_v \\in \\mathbb{R}^{|V|}$. As a result, we can examine the top-k candidate tokens for the next token prediction at each intermediate (sub-)layer by viewing the logit distribution. We refer readers to nostalgebraist (2020) or the recent survey paper of Rai et al. (2024) for a systematic and detailed explanation of the logit lens method."}, {"title": "3 Experimental Results", "content": "Our experiments aim to answer three Research Questions (RQs). RQ1 and RQ2 examine the layer-wise phenomena in the process of the Code LLM generating the correct next token. Specifically, RQ1 leverages logit lens to understand what the model would have predicted in a given layer, from which we gauge from which layer the model typically can start picking the correct token. RQ2 then looks into how (e.g., effect of promotion (Geva et al. 2022)) each (sub-)layer contributes to the prediction of the correct token, particularly by contrasting the logit values between the correct token and a counterfactual token (Table 1). In this process, we also locate the critical attention layers that strongly promote the generation of the correct token. Following that, RQ3 then specifically investigates the patterns of these critical attention layers, such as how different attention heads in these layers play a role and how each attention head functions, aiming to form a clearer understanding of how the Code LM becomes aware of the required number of closing parentheses."}, {"title": "3.2 RQ1: At what layer does the Code LM start picking the correct token?", "content": "The overarching goal of this RQ is to better understand at what points during an inference phase the Code LM has an understanding of what the next token prediction should be, which we define as the correct token's logit value being within the top ten logit values at a layer. This experiment aims to answer the overall question by obtaining answers to three related sub-questions: (1) At what layer does the correct token's logit value first break into the top 10 logits? (2) When does the Code LM first consider that the correct token should be predicted as the next token (i.e., the correct token is associated with the highest logit value)? and (3) When does the correct token's logit value consistently rank as the highest logit value for all subsequent layers? The respective answers to these questions for each of the sub-tasks can be found in Table 2, where for each sub-task we report the median layer (zero-indexed) across all prompts of that sub-task. The logit values in each layer are calculated by applying the logit lens to the residual-stream activation of that layer. We find that the Two Closing Parentheses sub-task has a lower median layer across the considered metrics for all sub-tasks. This was especially apparent for the median first layer where the correct token has the highest overall logit (LTop1) and the median first layer where the correct token is consistently ranked as the top token for all subsequent layers (LConsistentTop1), where the Two Closing Parentheses sub-task reaches these milestones in layer 25 and the Three Closing Parentheses and Four Closing Parentheses sub-tasks reach these milestones in the final layers of the Code LM. We conjecture that the Code LM views the Two Closing Parentheses sub-task as being easier than the Three and Four Closing Parentheses sub-tasks, which the Code LM appears to view as having similar difficulty."}, {"title": "3.3 RQ2: How does each (sub-)layer contribute to the correct token prediction?", "content": "Layer-level Analysis To understand each layer's contribution to correct token prediction, we first measure the logit difference between the correct token and the counterfactual token on the residual stream across all layers. Specifically, Figure 1a showcases the logit difference across layers of the residual stream for each sub-task, averaged over prompts in the same sub-task. Figure 1b displays the same metric but"}, {"title": "3.4 RQ 3: How do attention heads contribute to the promotion/suppression of correct tokens?", "content": "Given the insight from the experimental results of RQ2 that MHA sub-layers appear to contribute in a more significant fashion to the syntax completion task, we ran an additional experiment to see how individual attention heads contribute to the promotion or suppression of the logit difference between the correct token and the counterfactual token. We were especially interested in identifying the attention heads that had a large positive or negative contribution to the promotion of the logit difference in the previously identified MHA sub-layers. The heat map showing the logit difference projected from individual attention layers and heads for each sub-task can be seen in Figure 3. For all sub-tasks, we identify that most heads have a strong positive contribution to the logit difference (marked as deep blues), whereas only a few heads have a small negative contribution (marked as light reds). In particular, the positive contribution was dominantly made by only a few heads. For the Two Closing Parentheses sub-task, we find that the largest contributing heads are L30H0 (i.e., layer 30, head 0) and L27H24 (i.e., layer 27, head 24), which both have positive contributions. Interestingly, while the L30H0 attention head exhibits similar positive contribution behavior in the Three and Four Closing Parentheses sub-tasks as in the Two Closing Parentheses sub-task, we find that the L27H24 head negatively contributes the correct output for Three Closing Parentheses and Four Closing Parentheses sub-tasks. We identified that, despite their functional differences, L30H0 and L27H24 have similar attention patterns for all sub-tasks. Specifically, they both were found to effectively track the number of already closed parentheses by attending to the function name up to the point where the parentheses are already closed, as shown in Table 3. However, while L30H0 dynamically promotes the correct number of closing parentheses based on the count of those already closed, L27H24 always promotes two closing parentheses regardless of the number of remaining open parentheses. In other words, this head is promoting incorrect knowledge despite being able to correctly understand the context. We summarize this the phenomenon as \u201cincorrect knowledge association\". Consequently, this behavior of L27H24 results in a negative contribution for tasks requiring three or four closing parentheses."}, {"title": "4 Related Works", "content": "Analysis of Code LMs Code LMs (Abdin et al. 2024; Team et al. 2024; Guo et al. 2024; Li et al. 2023; Roziere et al. 2023; Chen et al. 2021), are a class of LMs specifically developed to enhance code generation capabilities of LMs through fine-tuning and additional training techniques (Chan et al. 2023). Although these models have demonstrated remarkable capabilities in code generation tasks (Yu et al. 2024; Zhuo et al. 2024; Lai et al. 2023; Cassano et al. 2023; Hao et al. 2022; Srivastava et al. 2022; Hendrycks et al. 2021), they remain susceptible to various syntactic and semantic (or logical) errors (Yu et al. 2024; Tambon et al. 2024; Dou et al. 2024). Prior studies have focused on empirically examining various types of bugs across a range of coding tasks and programming languages (Dou et al. 2024; Tambon et al. 2024; Dakhel et al. 2023) or proposing benchmark datasets to characterize these models' shortcomings (Wang et al. 2023; Siddiq and Santos 2022; Yang et al. 2024). For instance, Dou et al. (2024) observed that LMs are especially prone to syntactical errors (e.g., incomplete syntax structure, and indentation issues) when generating code for complex or lengthy problems. While these studies provide valuable insights into when Code LMs are likely to make mistakes, our understanding of the underlying internal mechanisms enabling code-generation capabilities remains limited. To bridge this gap, our study investigates how Code LMs perform syntax completion tasks. Mechanistic Interpretability (MI) MI is a subfield of interpretability that aims to reverse-engineer LM by understanding their internal components and computational processes (Elhage et al. 2021; Olah et al. 2020; Rai et al. 2024; Bereska and Gavves 2024). Recent MI studies have investigated various LM behaviors, including sequence completion task (Elhage et al. 2021), Indirect Object Identification (Wang et al. 2022), Python docstring completion (Heimersheim and Janiak 2023; Conmy et al. 2023) and modular addition tasks (Nanda et al. 2023), by discov-"}, {"title": "5 Discussion and Future Works", "content": "In this work, we presented preliminary findings toward a mechanistic understanding of how Code LMs use their internal knowledge to complete syntactic completion tasks, identifying multiple attention heads that play a critical role in this task. Building on these results, we suggest the following directions for future research. Circuit Discovery Seeing the relatively more important role the MHA sub-layers play in prioritizing the correct token over the counterfactual one, our work has focused on analyzing the MHA patterns in a Code LM. However, future work should extend the analysis to cover the MLP sub-layers (which were found to implement knowledge look-up in transformers (Geva et al. 2021)), and eventually portray a complete circuit of how a Code LM associates various components in its transformer architecture towards successfully using its knowledge in syntax completion. Universality of the Interpretation We hypothesize that there is significant overlap among similar sub-tasks involved in syntax completion tasks, both within and across programming languages. For example, we investigated CodeLlama's ability to perform the parenthesis completion task, which requires the model to track the number of open parentheses that have been closed. Similar counting mechanisms might also be needed for managing indentation in Python or closing curly braces in JavaScript. Our future work will look into whether a Code LM reuses the same components across tasks and languages for similar roles. Improving the LM Performance Finally, we aim to utilize our interpretation result to enhance a Code LM's performance in real life. For example, in experiments we have identified an attention head, L27H24, that performs incorrect knowledge association and erroneously promotes two closing parentheses even when the model needs to generate three or four closing parentheses. Furthermore, recent work of Geva et al. (2022) and Rai and Yao (2024) have showcased the potential of directly controlling a model's generation or task performance via manipulating its neuron activation. In the future, we will similarly explore if suppressing such less precise attention heads can improve the accuracy of the Code LM in closing the parentheses and beyond."}, {"title": "A Additional Results of Sub-Layer Logit Differences", "content": "In Figure 4, we present the sub-layer logit difference curves for the other two class constructors, i.e., list and set, in the Two Closing Parenthesis sub-task."}]}