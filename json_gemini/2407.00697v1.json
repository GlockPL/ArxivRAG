{"title": "CaFNet: A Confidence-Driven Framework for Radar Camera Depth Estimation", "authors": ["Huawei Sun", "Hao Feng", "Julius Ott", "Lorenzo Servadei", "Robert Wille"], "abstract": "Depth estimation is critical in autonomous driving for interpreting 3D scenes accurately. Recently, radar-camera depth estimation has become of sufficient interest due to the robustness and low-cost properties of radar. Thus, this paper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net (CaFNet) for dense depth estimation, combining RGB imagery with sparse and noisy radar point cloud data. The first stage addresses radar-specific challenges, such as ambiguous elevation and noisy measurements, by predicting a radar confidence map and a preliminary coarse depth map. A novel approach is presented for generating the ground truth for the confidence map, which involves associating each radar point with its corresponding object to identify potential projection surfaces. These maps, together with the initial radar input, are processed by a second encoder. For the final depth estimation, we innovate a confidence-aware gated fusion mechanism to integrate radar and image features effectively, thereby enhancing the reliability of the depth map by filtering out radar noise. Our methodology, evaluated on the nuScenes dataset, demonstrates superior performance, improving upon the current leading model by 3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE).", "sections": [{"title": "I. INTRODUCTION", "content": "Dense depth estimation is an important task in autonomous driving, which helps to understand the 3D geometry of outdoor scenes. Following the first learning-based monocular depth estimation model [1], there have been studies enhancing task performance based on deep learning techniques [2]\u2013[7]. Despite these advancements, the efficacy of monocular image-based methods is inherently constrained by the lack of depth cues in RGB images. To address this, numerous studies [8]\u2013[10] have incorporated lidar data alongside images for depth completion tasks, yielding higher-quality depth maps.\nWhile integrating lidar data offers improved scene understanding, lidar sensors are known to be sensitive to lighting and weather conditions. In contrast, radar sensors, present a cost-effective and weather-adaptable alternative. Moreover, radar sensors capture the velocity of moving objects, yielding more accurate object detection [11]\u2013[13]. With the release of autonomous driving datasets [14, 15] that include radar sensors, the domain of camera-radar fusion is gaining traction.\nHowever, radar point clouds are notably sparser compared to lidar point clouds. Additionally, commonly used radar sensors often lack elevation resolution due to limited antennas along the elevation axis, leading to the absence of height information in radar points. Moreover, the phenomenon of the radar multipath effect further complicates this scenario by introducing numerous ghost targets.\nGiven these challenges, effectively leveraging radar points while mitigating noise is a crucial concern. Several studies [11, 12, 16, 17] attempt to solve the sparsity and ambiguous height problems by extending radar points along the elevation axis, bringing additional errors into the radar data. In an alternative approach, RC-PDA [18] and RadarNet [19] employ two-stage networks that initially estimate radar confidence scores to generate semi-dense depth maps for further training. However, a predefined region with h \u00d7 w is selected for each radar point. This treats each radar point uniformly and does not consider the association between the radar point and its corresponding object in the 3D space.\nTo overcome the obstacles, we propose CaFNet, a novel two-stage, end-to-end trainable network for depth estimation. Our method incorporates a Sparse Convolution Module (SCM), employing sparse convolutional layers [20] to handle radar data sparsity before being processed by the encoder. In the first stage, a radar confidence map along with a coarse depth map is estimated, which guides the training of the second stage. To better guide the confidence map, we propose a ground truth generation method that considers the association between the radar points and the detected objects. The key innovation in our work is introducing Confidence-aware Gated Fusion (CaGF) in the second stage. This fusion technique integrates radar and image features by taking into account the confidence scores of individual radar pixels. Our model leverages a single radar scan and a single RGB image for depth estimation. The CaFNet outperforms existing solutions, establishing new benchmarks"}, {"title": "II. RELATED WORK", "content": "Monocular Depth Estimation\nMonocular depth estimation is the process of determining the depth of each pixel in an image using a single RGB image. The majority of current methods employ encoder-decoder architectures to derive depth maps from input images. A significant advancement in this field was made by Eigen et al. [21], who introduced the use of Convolutional Neural Networks (CNNs) in a multi-scale network for depth estimation. This included the development of a scale-invariant loss function to address the issue of scale ambiguity. DORN [22] first reinterpreted depth regression as a classification task, focusing on predicting depth ranges instead of exact values with predefined bin centers. Subsequent studies, such as AdaBins [4] and BinsFormer [23], enhanced this approach by learning to predict more accurate bin centers, addressing the variation in depth distribution across different frames. Incorporating geometric priors has also been shown to be effective. For instance, GeoNet [5] uses a geometric network to simultaneously infer surface normals and depth maps that are geometrically consistent. As proposed in BTS [3], the local planar assumption guides the upsampling modules in the decoding phase. P3Depth [24] introduces an approach that selectively leverages information from coplanar pixels, using offset vector fields and a mean plane loss to refine depth predictions. Another line of research focuses on refining coarse depth maps through spatial propagation networks, initially proposed in SPN [25]. CSPN [2] improved this by using convolutional layers to learn local affinities, which better connect each pixel with its neighboring pixels.\nCamera and Radar-based Depth Estimation\nCamera radar fusion-based techniques combine sparse radar point clouds with camera images for depth map prediction. These methods present more challenges than lidar-camera fusion due to the inherent sparsity and noise in radar point clouds. Lin et al. [26] developed a two-stage encoder-decoder network specifically designed to filter out noisy radar points, using lidar points as a reference. An extension of this approach by [16] involves augmenting each radar point's height value to produce a denser radar projection, which is then integrated with camera images using a regression network based on DORN's [22] methodology. RC-PDA [18] tackles the uncertainty of projecting radar points onto the image plane by first learning a radar-to-pixel association. Once the association is established, a second network estimates depth. These studies typically use multiple radar sweeps to enhance the density of the point cloud. Singh et al. [19] propose a two-stage network, where the first stage learns a one-to-many mapping, generating quasi-dense radar depth maps, which are then processed in the second stage for final depth estimation. However, a significant drawback of the two-stage networks in RC-PDA [18] and RadarNet [19] is their separate training phases. This necessitates local storage of intermediate data post the initial training stage, resulting in a time-intensive process that challenges real-time prediction. Moreover, a predefined number of radar points are selected in RadarNet [19], with the empirical evidence suggesting a mere four points per frame. This severely constrains the utility of radar data."}, {"title": "III. APPROACH", "content": "This section introduces the principal innovations of our work. Initially, we detail the comprehensive architecture of our CaFNet in Sec. III-A. Subsequently, Sec. III-B describes our novel approach for generating the ground truth of radar confidence maps. Following this, the refinement module is"}, {"title": "A. Model Architecture", "content": "As illustrated in Fig. 2, our CaFNet processes an RGB image \\(I \\in \\mathbb{R}^{H \times W \times 3}\\) and a radar-projected image \\(R \\in \\mathbb{R}^{H \times W \times C_R}\\) as inputs. Typically, the ground truth depth \\(D_{gt} \\in \\mathbb{R}^{H \times W}\\) generated by lidar is sparse. Thus, for the supervision, following [19], we accumulate point clouds by projecting from neighboring frames to form a denser ground truth map \\(D_{acc} \\in \\mathbb{R}^{H \times W}\\). The radar-projected image is generated by directly projecting every radar point onto the image plane, carrying specific information, for example, velocities and Radar Cross Section (RCS). In the first stage, feature extraction from the RGB image is conducted using a ResNet-34 [27] backbone. In contrast, the radar input undergoes initial refinement through the SCM, which stacks four sparse convolution layers [20], before being encoded by a ResNet-18 backbone. These multi-scale image and radar features are then concatenated and fed into a UNet decoder [28], which leverages skip connections to efficiently transmit information from the encoder to the decoder. The decoder outputs two maps. The first output is a coarse depth map \\(D_c \\in \\mathbb{R}^{H \times W}\\), which is trained against the ground truth depth map \\(D_{acc} \\in \\mathbb{R}^{H \times W}\\). The second is a radar confidence map \\(\\hat{C} \\in [0, 1]^{H \times W}\\), which signifies the probable correspondence between projected radar points and image pixels and is trained against a binary ground truth confidence map \\(C \\in {0, 1}^{H \times W}\\), detailed in Sec. III-B.\nThe refinement module, utilizing \\(\\hat{C}\\) and \\(D_c\\) as inputs, produces a confidence-refined depth map \\(\\hat{D'}_c \\in \\mathbb{R}^{H \times W}\\). This map is concatenated with R, yielding a new radar input \\(R' \\in \\mathbb{R}^{H \times W \times (C_R + 1)}\\) and forwarded to the second stage. Sec. III-C provides a more detailed explanation.\nDuring the second stage, \\(R'\\) is encoded via a separate ResNet-18 backbone. Subsequently, a BTS-like depth estimator [3] is employed, which takes the newly extracted radar features, the image features, and the predicted radar confidence as inputs. Throughout this stage, features from different layers are fused using a CaGF mechanism. Leveraging multi-scale features, a final depth map \\(D_f \\in \\mathbb{R}^{H \times W}\\) is estimated. Further details can be found in Sec. III-D."}, {"title": "B. Radar Confidence Map", "content": "In the first stage in [19], a set of K radar points is chosen as input. For each point, K ground truth confidence maps of size 288 \u00d7 900 are generated. However, with its substantially large predefined area, this approach results in significant computational demands. Moreover, since their model cannot handle various numbers of points as input, they select only four radar points during training, which leads to inefficiency.\nIn contrast, our method diverges by incorporating all radar points in each frame. We also pay particular attention to the relationship between radar points and the associated objects to more accurately determine probable radar projection areas. Specifically, for a frame, we work with a radar point cloud comprising M points, \\(z = {z_m}^M_{m=1}\\), and an accumulated ground truth depth map, \\(D_{acc}\\). Additionally, we identify B = {\\(B_1, B_2, ..., B_n\\)} as a set of object bounding boxes within the frame, where n is the number of detected objects.\nWe categorize the radar points into two groups. The first, \\(Z_{in} = {z_{in,p}}^P_{p=1}\\), includes points inside any 3D bounding box, where \\(z_{in} \\in B\\) signifies that the point is in the \\(i^{th}\\) box. The second group, \\(Z_{out} = {z_{out,q}}^Q_{q=1}\\), contains points not associated with any objects, typically regarded as noise. Fig. 3 depicts these point types and their specific regions.\nFor a point \\(z_{in}\\) at pixel coordinates \\((x_{in}, y_{in})\\) linked to bounding box \\(B^i\\), the selected region \\(R_{in}\\) is defined as:\n\\(R_{in} = {(x, y) | x_1 \\leq x \\leq x_2, y_1 \\leq y \\leq y_2}\\), (1)\nwhere \\((x_1, y_1)\\) and \\((x_2, y_2)\\) represent the top-left and bottom-right corners of \\(B^i\\), respectively.\nConversely, for a point \\(z_{out}\\) at coordinates \\((x_{out}, y_{out})\\), the region \\(R_{out}\\) is selected using a pre-defined patch size (w, h):\n\\(R_{out} = {(x, y)|x_{out} - \\frac{w}{2} \\leq x \\leq x_{out} + \\frac{w}{2},  y_{out} - \\frac{h}{2} \\leq y \\leq y_{out} + \\frac{h}{2}}\\). (2)\nThe ground truth confidence is generated by comparing the absolute depth difference of the point's depth with \\(D_{acc}\\) within the selective region. For the \\(m^{th}\\) radar point \\(z_m\\) with depth \\(d_m\\), the confidence value of each pixel within its selective region \\(R_m\\):\n\\(C(i, j) = {\\begin{cases}1 & \\text{if } |D_{acc}(i, j) - d_m| \\leq \\tau \\\\0 & \\text{otherwise}.\\end{cases}}\\) (3)\nHere, \\(D_{acc}(i, j)\\) represents the ground truth depth of pixel (i, j) and \\(\\tau\\) is the tolerance threshold, chosen as 0.4 meters."}, {"title": "C. Refinement Module", "content": "To facilitate the learning of information from \\(D_c\\) and \\(\\hat{C}\\) by the second radar encoder, our initial concept involved concatenating these two maps with R. However, due to the sparsity property of R, the detailed information from \\(D_c\\) might overwhelm the encoder with excessive information. To address this, we introduce a refinement module designed to streamline the information content from \\(D_c\\), utilizing the predicted radar confidence map as guidance."}, {"title": "III-C. Refinement Module (Continued)", "content": "In this refinement module, we generate a radar confidence mask \\(M \\in \\mathbb{R}^{H \times W}\\) by setting a predefined threshold T:\n\\(M(i, j) = {\\begin{cases}1 & \\text{if } \\hat{C}(i, j) > T \\\\0 & \\text{otherwise}.\\end{cases}}\\) (4)\nFollowed by this, the confidence-refined depth \\(\\hat{D'}_c\\) is calculated by element-wise multiplication between M and \\(D_c\\):\n\\(\\hat{D'}_c = M \\odot D_c.\\) (5)\nAfter the refinement process, \\(\\hat{D'}_c\\) is concatenated with R, creating a new input \\(R'\\) and then fed into the second stage."}, {"title": "D. Bts-like Depth Estimator", "content": "Our final depth estimation model adopts the framework of BTS [3]. To enhance its performance, we introduce the CaGF - a novel mechanism designed to integrate image and radar features. This process incorporates \\(\\hat{C}\\), enabling the selective filtration of noisy radar data. The decoder's architecture is depicted in Fig. 4.\nConfidence-aware Gated Fusion: At the \\(i^{th}\\) stage, the radar feature \\(F_r^i\\) and the image feature \\(F_u^i\\) are in shape of \\(\\frac{H}{2^i} \\times \\frac{W}{2^i} \\times C_r\\) and \\(\\frac{H}{2^i} \\times \\frac{W}{2^i} \\times C_c\\), respectively. Building upon the approach in [19], we derive a set of weights \\(\\alpha = \\sigma(p^T F_r^i) \\in [0, 1]\\) and projections \\(\\beta = q^T F_r^i\\) from \\(F_r^i\\). Here, p and q denote trainable parameters, and \\(\\sigma\\) symbolizes the sigmoid activation function, ensuring \\(\\alpha\\) scales between 0 and 1. Additionally, we process the predicted radar confidence \\(\\hat{C} \\in [0, 1]^{H \\times W}\\) to obtain a resized confidence \\(\\hat{c}\\) by employing Global Average Pooling (GAP) with a stride of \\(2^i\\) on \\(\\hat{C}\\). The enhancement of the radar feature is formulated as \\((F_r^i)' = \\alpha \\cdot \\beta \\cdot \\hat{c}\\). This refinement step leverages the radar confidence to more effectively distinguish the significance of various positions, a critical consideration given that the majority of radar input pixels are typically zero-valued. Consequently, the composite feature for this stage is obtained as \\(F^i = (F_r^i)' + F_u^i\\), integrating both radar and image features for a more nuanced representation."}, {"title": "E. Loss Functions", "content": "Our model employs an end-to-end training strategy with several loss functions. For depth estimation, we use an L1 loss on the predictions \\(D_c\\) and \\(D_f\\) from both the coarse and final stages, respectively. The depth loss is given by:\n\\(L_{Depth} = \\frac{1}{m} \\Omega \\sum_{x \\in \\Omega} |D_{acc}(x) - D_c(x)| + |D_{acc}(x) - D_f(x)|,\\) (6)\nwhere m is a weighting factor, chosen as 0.5, and \\(\\Omega\\) represents the set of pixels where \\(D_{acc}\\) is valid.\nAdditionally, an edge-aware smoothness loss is applied to the final depth map, \\(D_f\\), follows [26], formulated as:\n\\(L_{Smooth} = |\\nabla_u D_f|e^{-|\\nabla_u(I)|} + |\\nabla_v D_f|e^{-|\\nabla_v(I)|},\\) (7)\nwhere \\(\\nabla_u\\) and \\(\\nabla_v\\) are the horizontal and vertical image gradients, respectively.\nFor radar confidence, we minimize a binary cross-entropy loss between the ground truth C and the predicted map \\(\\hat{C}\\):\n\\(L_{Conf} = -\\frac{1}{N_C} \\sum_{x \\in \\Omega_C} [C(x) log(\\hat{C}(x)) + (1 - C(x)) log(1 - \\hat{C}(x))],\\) (8)\nwhere \\(\\Omega_C\\) denotes the image domain.\nThe final loss L is a weighted sum of the individual losses \\(L = L_{Depth} + L_{Conf} + \\lambda L_{Smooth}\\). Here, \\(\\lambda\\) is a hyperparameter, chosen as 1e-3 based on empirical results."}, {"title": "IV. EXPERIMENTS", "content": "This section first describes the dataset and implementation setups. Then, we introduce the evaluation metrics and compare our method with existing approaches in qualitative and quantitative ways. Afterwards, we show the efficacy of the radar confidence map. Finally, we conducted ablation studies to underscore the robustness of the proposed techniques."}, {"title": "A. Dataset and Implementation Details", "content": "We utilize nuScenes [14], a leading-edge dataset for autonomous driving, to train and test our model's performance. This dataset is captured via multiple sensors, including cameras, radars, and lidar, mounted on a vehicle navigating through Boston and Singapore. Here, we utilize 700 scenes for training, 150 for validation, and 150 for testing.\nDuring training, we follow [19] to accumulate 80 future and 80 past lidar frames by projecting the lidar point cloud at each frame to the current frame, thereby creating a denser depth map. Subsequently, we apply scaffolding [31] to derive the interpolated depth map \\(D_{acc}\\). During evaluation, we use the single-frame lidar depth map \\(D_{gt}\\) provided by [14].\nOur model is implemented using PyTorch [32] and trained on a Nvidia\u00ae Tesla A30 GPU. We adopt the Adam optimizer,"}, {"title": "V. CONCLUSION", "content": "Depth estimation task using radar and camera data presents considerable challenges, primarily due to the inherent sparsity and ambiguity in radar data. In response to these challenges, we introduced CaFNet that operates in two stages. The first stage generates a radar confidence map and a preliminary depth map. In the second stage, we implement an innovative fusion technique that leverages the confidence scores to enhance the feature fusion process."}]}