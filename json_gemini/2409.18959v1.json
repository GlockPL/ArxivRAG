{"title": "O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions", "authors": ["Gen Li", "Yuling Yan"], "abstract": "Score-based diffusion models, which generate new data by learning to reverse a diffusion process that perturbs data from the target distribution into noise, have achieved remarkable success across various generative tasks. Despite their superior empirical performance, existing theoretical guarantees are often constrained by stringent assumptions or suboptimal convergence rates. In this paper, we establish a fast convergence theory for a popular SDE-based sampler under minimal assumptions. Our analysis shows that, provided l2-accurate estimates of the score functions, the total variation distance between the target and generated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic factors), where $d$ is the data dimensionality and $T$ is the number of steps. This result holds for any target distribution with finite first-order moment. To our knowledge, this improves upon existing convergence theory for both the SDE-based sampler and another ODE-based sampler, while imposing minimal assumptions on the target data distribution and score estimates. This is achieved through a novel set of analytical tools that provides a fine-grained characterization of how the error propagates at each step of the reverse process.", "sections": [{"title": "1 Introduction", "content": "Score-based generative models (SGMs) have emerged as a powerful class of generative frameworks, capable of learning and sampling from complex data distributions (Dhariwal and Nichol, 2021; Ho et al., 2020; Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2021b). These models, including Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020) and Denoising Diffusion Implicit Models (DDIM) (Song et al., 2021a), operate by gradually transforming a simple noise-like distribution (e.g., standard Gaussian) into a target data distribution through a series of diffusion steps. This transformation is achieved by learning a sequence of denoising processes governed by score functions, which estimate the gradient of the log-density of the data at each step. SGMs have demonstrated remarkable success in various generative tasks, including image generation (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022), audio generation (Kong et al., 2021), video generation (Villegas et al., 2022), and molecular design (Hoogeboom et al., 2022). See e.g., Croitoru et al. (2023); Yang et al. (2023) for overviews of recent development.\nAt the core of SGMs are two stochastic processes: a forward process, which progressively adds noise to the data,\n$X_0\\rightarrow X_1\\rightarrow\\ldots\\rightarrow X_T,$\nwhere $X_0$ is drawn from the target data distribution $P_{\\text{data}}$ and is gradually transformed into $X_T$ that resembles standard Gaussian noise; and a reverse process,\n$Y_T\\rightarrow Y_{T-1}\\rightarrow\\ldots\\rightarrow Y_0,$\nwhich starts from pure Gaussian noise $Y_T$ and sequentially converts it into $Y_0$ that closely mimics the target data distribution $P_{\\text{data}}$. At each step, the distributions of $Y_t$ and $X_t$ are kept close. The key challenge lies in constructing this reverse process effectively to ensure accurate sampling from the target distribution.\nMotivated by classical results on the time-reversal of stochastic differential equations (SDEs) (Anderson, 1982; Haussmann and Pardoux, 1986), SGMs construct the reverse process using the gradients of the log marginal density of the forward process, known as score functions. At each step, $Y_{t-1}$ is generated from $Y_t$ with the help of the score function $\\nabla \\log p_{X_t}(\\cdot)$, where $p_{X_t}$ denotes the density of $X_t$. Both SDE-based samplers (Ho et al., 2020) and ODE-based samplers (Song et al., 2021a) follow this denoising framework, with the key distinction being whether additional random noise is injected when generating each $Y_{t-1}$. Although the score functions are not known explicitly, they are pre-trained using large neural networks through score-matching techniques (Hyv\u00e4rinen, 2005, 2007; Song and Ermon, 2019; Vincent, 2011).\nDespite their impressive empirical success, the theoretical foundations of diffusion models remain relatively underdeveloped. Early studies on the convergence of SGMs (Block et al., 2020; De Bortoli, 2022; De Bortoli et al., 2021; Liu et al., 2022; Pidstrigach, 2022) did not provide polynomial convergence guarantees. In recent years, a line of works have explored the convergence of the generated distribution to the target distribution, treating the score-matching step as a black box and focusing on the effects of the number of steps $T$ and the score estimation error on the convergence of the sampling phase (Benton et al., 2023a; Chen et al., 2023a, 2024, 2023c,d; Gao and Zhu, 2024; Huang et al., 2024; Lee et al., 2022, 2023; Li et al., 2023, 2024b; Li and Yan, 2024; Liang et al., 2024; Tang and Zhao, 2024). Recent studies have investigated the performance guarantees of SGMs in the presence of low-dimensional structures (e.g., Chen et al. (2023b); Li and Yan (2024); Tang and Yang (2024); Wang et al. (2024)) and the acceleration of SGMs (e.g., Li et al. (2024a); Liang et al. (2024)). Following this general avenue, the goal of this paper is to establish a sharp convergence theory for diffusion models with minimal assumptions.\nPrior convergence guarantees. In recent years, a flurry of work has emerged on the convergence guarantees for SDE-based and ODE-based samplers. However, these prior studies fall short of providing a fully satisfactory convergence theory due to at least one of the following three obstacles:\nStringent data assumptions. Earlier works, such as Lee et al. (2022), required the target data distribution to satisfy the log-Sobolev inequality. Similarly, Chen et al. (2024, 2023c,d); Lee et al. (2023) assumed that the score functions along the forward process must satisfy a Lipschitz smoothness condition. More"}, {"title": "2 Problem set-up", "content": "In this section, we provide an overview of the diffusion model and the SDE-based sampler.\nForward process. We consider a Markov process in $\\mathbb{R}^d$ starting from $X_0 \\sim P_{\\text{data}}$, evolving according to the recursion:\n$X_t = \\sqrt{1 - \\beta_t} X_{t-1} + \\sqrt{\\beta_t} W_t \\quad (t = 1, ...,T),$\nwhere $W_1,..., W_T$ are independent draws from $\\mathcal{N}(0, I_d)$, and $\\beta_1,..., \\beta_T \\in (0, 1)$ are the learning rates. For each $1 \\leq t \\leq T$, define $\\alpha_t := 1 - \\beta_t$ and $\\overline{\\alpha}_t := \\prod_{i=1}^t \\alpha_i$. This allows us to express $X_t$ in closed form as:\n$X_t = \\sqrt{\\overline{\\alpha}_t} X_0 + \\sqrt{1-\\overline{\\alpha}_t} W_t \\quad \\text{where} \\quad W_t \\sim \\mathcal{N}(0, I_d).$\nWe select the learning rates such that (i) $\\beta_t$ is small for every $1 \\leq t \\leq T$; and (ii) $\\overline{\\alpha}_T$ is vanishingly small, ensuring that the distribution of $X_T$ is exceedingly close to $\\mathcal{N}(0, I_d)$. In this paper, we adopt the following learning rate schedule\n$\\beta_1 = \\frac{1}{T^{c_0}}, \\quad \\beta_{t+1} = \\frac{C_1 \\log T}{T} \\min\\{1, \\beta_1 + \\frac{C_1 \\log T}{T}\\} \\quad (t = 1, ..., T - 1),$\nfor sufficiently large constants $c_0, C_1 > 0$. This schedule is commonly used in the diffusion model literature (see, e.g., Li et al. (2023, 2024b)), although the results in this paper hold for any learning rate schedule satisfying the conditions in Lemma 7."}, {"title": "3 Main results", "content": "In this section, we will establish a fast convergence theory for the SDE-based sampler under minimal assumptions. Before proceeding, we introduce the only data assumption that our theory requires.\nAssumption 1. The target distribution $P_{\\text{data}}$ has finite first-order moment. Furthermore, we assume that there exists some constant $C_M > 0$ such that\n$M_1:= \\mathbb{E}[||X_0||^2] < T C_M.$\nHere we require the first-order moment $M_1$ to be at most polynomially large in $T$, which allows cleaner and more concise result that avoids unnecessary technical complicacy. Since $C_M > 0$ can be arbitrarily large, we allow the target data distribution to have exceedingly large first-order moment, which is a mild assumption.\nNow we are positioned to present our convergence theory for the SDE-based sampler."}, {"title": "4 Proof of Theorem 1", "content": "For each $1 < t < T$ and any $x \\in \\mathbb{R}^d$, it is known that the score function $s_t(x)$ associated with $p_{X_t}$ admits the following expression\n$s_t(x) = \\frac{1}{1-\\overline{\\alpha}_t} \\int_{\\mathbb{R}^d} p_{X_0|X_t}(x_0|x) (x - \\sqrt{\\overline{\\alpha}_t}x_0)dx_0 =: g_t(x).$\nLet $J_t(x) = dg_t(x)/dx$ be the Jacobian matrix of $g_t(x)$, which can be expressed as\n$J_t(x) = I + \\frac{1}{1-\\overline{\\alpha}_t} \\bigg\\{ \\bigg( \\int_{\\mathbb{R}^d} p_{X_0|X_t}(x_0|x) (x - \\sqrt{\\overline{\\alpha}_t}x_0) dx_0 \\bigg) \\bigg( \\int_{\\mathbb{R}^d} p_{X_0|X_t}(x_0|x) (x - \\sqrt{\\overline{\\alpha}_t}x_0)^\\top dx_0 \\bigg) \\frac{1}{\\sqrt{\\text{Vol}}} - \\int_{\\mathbb{R}^d} p_{X_0|X_t}(x_0|x) (x - \\sqrt{\\overline{\\alpha}_t}x_0) (x - \\sqrt{\\overline{\\alpha}_t}x_0)^\\top dx_0 \\bigg\\}.$\nIt is straightforward to check that $I - J_t(x_t) \\geq 0$. The following lemma will be useful in the analysis.\nLemma 1. Suppose that $x \\in \\mathbb{R}^d$ satisfies $-\\log p_{X_t}(x) < \\Theta d \\log T$ for any given $\\Theta > 1$. Then we have\n$||s_t(x)||_2 \\leq 51 \\frac{(\\Theta+c_0) d \\log T}{1-\\overline{\\alpha}_t} \\quad \\text{and} \\quad Tr(I - J_t(x)) \\leq 12(\\Theta + c_0) d \\log T,$\nwhere the constant $c_0 > 0$ is defined in (2.3). In addition, there exists universal constant $C_0 > 0$ such that\n$\\sum_{t=2}^T \\frac{1-\\overline{\\alpha}_t}{1-\\alpha_t} \\int_{\\mathbb{R}^d} ||J_t(x_t)||_F p_{X_t}(x_t)dx_t \\leq C_0 d \\log T.$\nFor some sufficiently large constants $C_1, C_2 > 0$, we define for each $2\\leq t \\leq T$ the set\n$\\mathcal{E}_{t,1} := \\{x_t : -\\log p_{X_t}(x_t) \\leq C_1 d \\log T, ||x_t||_2 < \\sqrt{\\overline{\\alpha}_t} T^{2C_R} + C_2\\sqrt{d (1 -\\overline{\\alpha}_t) \\log T} \\},$\nand for each $x_t \\in \\mathcal{E}_{t,1}$, we define\n$\\mathcal{E}_{t,2}(x_t) := \\{x_{t-1}: ||\\sqrt{\\alpha_t} x_{t-1} - x_t||_2 \\leq C_2 \\sqrt{d (1 - \\alpha_t) \\log T} \\}.$\nDefine the extended d-dimensional Euclidean space $\\mathbb{R}^d \\cup \\{\\infty\\}$ by adding a point $\\infty$ to $\\mathbb{R}^d$. From now on, the random vectors can take value in $\\mathbb{R}^d \\cup \\{\\infty\\}$, namely, they can be constructed in the following way:\n$\\mathbb{X} = \\begin{cases} X', \\quad \\text{with probability } \\theta, \\\\ \\infty, \\quad \\text{with probability } 1 - \\theta, \\end{cases}$"}, {"title": "4.1 Preliminaries", "content": "where $\\theta \\in [0, 1]$ and $X'$ is a random vector in $\\mathbb{R}^d$ in the usual sense. If $X'$ has a density, denoted by $p_{X'}(\\cdot)$, then the generalized density of $X$ is\n$p_X(x) = \\theta p_{X'}(x) \\mathbb{1}\\{x \\in \\mathbb{R}^d\\} + (1 - \\theta) \\delta_{\\infty}.$\nTo simplify presentation, we will abbreviate generalized density to density."}, {"title": "4.2 Step 1: introducing auxiliary sequences", "content": "We first define an auxiliary reverse process that uses the true score function:\n$Y_T \\sim \\mathcal{N}(0, I_d), \\quad Y_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (Y_t + (1 - \\alpha_t) s_t(Y_t) + \\sqrt{1 - \\alpha_t} Z_t) \\quad \\text{for } t = T, ..., 1.$\nTo control discretization error, we introduce an auxiliary sequence $\\{Y_t : t = T, ..., 1\\}$ along with intermediate variables $\\{Y_t^- : t = T, ...,1\\}$ as follows.\n1. (Initialization) Define $Y_T^- = Y_T$ if $Y_T \\in \\mathcal{E}_{T,1}$ and $Y_T^- = \\infty$ otherwise. The density of $Y_T^-$ is\n$p_{Y_T^-}(y_T^-) = p_{Y_T}(y_T^-) \\mathbb{1}\\{y_T^- \\in \\mathcal{E}_{T,1}\\} + \\int_{y_T \\notin \\mathcal{E}_{T,1}} p_{Y_T}(y_T) dy_T \\delta_{\\infty}.$\n2. (Transition from $Y_t^-$ to $Y_t$) For $t = T, ..., 1$, the conditional density of $Y_t$ given $Y_t^- = y_t^-$ is\n$p_{Y_t|Y_t^-}(y_t | y_t^-) = \\min\\{ \\frac{p_{X_t}(y_t^-)}{p_{Y_t^-}(y_t^-)}, 1 \\} \\delta_{y_t^-} + (1 - \\min\\{ \\frac{p_{X_t}(y_t^-)}{p_{Y_t^-}(y_t^-)}, 1 \\}) \\delta_{\\infty}.$\nThis can be realized as follows: conditional on $Y_t^- = y_t^-$, we let\n$Y_t = \\begin{cases} y_t^-, \\quad \\text{with prob. } \\min\\{ \\frac{p_{X_t}(y_t^-)}{p_{Y_t^-}(y_t^-)}, 1 \\}, \\\\ \\infty, \\quad \\text{otherwise}. \\end{cases}$\n3. (Transition from $Y_t$ to $Y_{t-1}$) For $t = T, ..., 2$, the conditional density of $Y_{t-1}$ given $Y_t = y_t$ is defined as follows: if $y_t \\in \\mathcal{E}_{t,1}$, then\n$p_{Y_{t-1}|Y_t}(y_{t-1} | y_t) = p_{Y_{t-1}|Y_t^*}(y_{t-1} | y_t) \\mathbb{1}\\{y_{t-1} \\in \\mathcal{E}_{t,2}(y_t)\\} + \\int_{y_{t-1} \\notin \\mathcal{E}_{t,2}(y_t)} p_{Y_{t-1}|Y_t^*}(y_{t-1} | y_t) dy_{t-1} \\delta_{\\infty};$\notherwise, we let\n$p_{Y_{t-1}|Y_t}(y_{t-1} | y_t) = \\delta_{\\infty}.$\nThis can be realized as follows: we first draw a candidate sample\n$\\hat{Y}_{t-1}:= \\frac{1}{\\sqrt{\\alpha_t}} (Y_t + (1 - \\alpha_t) s_t(Y_t) + \\sqrt{1 - \\alpha_t} Z_t),$\nwhere $Z_t$ is an independent $\\mathcal{N}(0, I_d)$ random vector, and let\n$Y_{t-1} = \\begin{cases} \\hat{Y}_{t-1}, \\quad \\text{if } Y_t \\in \\mathcal{E}_{t,1} \\text{ and } \\hat{Y}_{t-1} \\in \\mathcal{E}_{t,2}(Y_t), \\\\ \\infty, \\quad \\text{otherwise}. \\end{cases}$\nThis defines a Markov chain\n$Y_T \\rightarrow Y_T^- \\rightarrow Y_T \\rightarrow Y_{T-1}^- \\rightarrow Y_{T-1} \\rightarrow \\ldots \\rightarrow Y_1^- \\rightarrow Y_1.$\nAn important consequence of the construction is that, for any $y_t \\neq \\infty$,\n$p_{Y_t}(y_t) = \\int_{\\mathbb{R}^d} p_{Y_t|Y_t^-}(y_t | y_t^-) p_{Y_t^-}(y_t^-) dy_t^- = \\min\\{p_{X_t}(y_t), p_{Y_t^-}(y_t)\\} .$\nTo control estimation error, we introduce another auxiliary sequence $\\{\\hat{Y}_t : t = T,...,1\\}$ along with intermediate variables $\\{\\hat{Y}_t^- : t = T, ..., 1\\}$ as follows."}, {"title": "4.3 Step 2: controlling discretization error", "content": "In this section, we will bound the total variation distance between $p_{X_1}$ and $p_{Y_1}$. For each $t = T, ..., 1$, let\n$\\Delta_t(x) := p_{X_t}(x) - p_{Y_t}(x), \\quad \\forall x \\in \\mathbb{R}^d.$\nWe emphasize that $\\Delta_t(\\cdot)$ is not defined at $\\infty$. In view of , we know that $\\Delta_t(x_t) \\geq 0$ for any $x_t \\neq \\infty$. We will prove in this section that, there exists some universal constant $C_4 > 0$ such that, for $t = T, ..., 2$,\n$\\int_{\\mathbb{R}^d} \\Delta_{t-1}(x)dx \\leq \\int_{\\mathbb{R}^d} \\Delta_{t}(x)dx + C_4 \\bigg( \\frac{1-\\overline{\\alpha}_t}{1-\\alpha_t} \\bigg)^2 \\int_{x_t \\in \\mathcal{E}_{t,1}} (d \\log T+ ||J_t(x_t)||_F^2)p_{X_t}(x_t)dx_t + 2 T^{-4}.$\nIn addition, we also have the following result that controls $\\int_{\\mathbb{R}^d} \\Delta_{T}(x)dx.$"}]}