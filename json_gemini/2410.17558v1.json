{"title": "CLR-BENCH: EVALUATING LARGE LANGUAGE MODELS IN COLLEGE-LEVEL REASONING", "authors": ["Junnan Dong", "Zijin Hong", "Yuanchen Bei", "Feiran Huang", "Xinrun Wang", "Xiao Huang"], "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and computer science, they merely measure the accuracy in terms of the final prediction on multi-choice questions. However, it remains insufficient to verify the essential understanding of LLMs given a chosen choice. To fill this gap, we present CLR-Bench to comprehensively evaluate the LLMs in complex college-level reasoning. Specifically, (i) we prioritize 16 challenging college disciplines in computer science and artificial intelligence. The dataset contains 5 types of questions, while each question is associated with detailed explanations from experts. (ii) To quantify a fair evaluation of LLMs' reasoning ability, we formalize the criteria with two novel metrics. Q\u2192A is utilized to measure the performance of direct answer prediction, and Q\u2192AR effectively considers the joint ability to answer the question and provide rationale simultaneously. Extensive experiments are conducted with 40 LLMs over 1,018 discipline-specific questions. The results demonstrate the key insights that LLMs, even the best closed-source LLM, i.e., GPT-4 turbo, tend to 'guess' the college-level answers. It shows a dramatic decrease in accuracy from 63.31% Q\u2192A to 39.00% Q\u2192AR, indicating an unsatisfactory reasoning ability.", "sections": [{"title": "INTRODUCTION", "content": "The emerging large Language Models (LLMs), e.g., GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023) and Llama (Touvron et al., 2023b) have performed significantly across various natural language understanding tasks (Hendrycks et al., 2020). They are tightly integrated into the downstream scenar-ios, such as college education and medicine (Meyer et al., 2023; Abd-Alrazaq et al., 2023), thereby transforming the paradigm for both research and applications. In response to this advancement, sev-eral Off-the-shelf benchmarks have been developed to assess LLMs across different domain-specific"}, {"title": "2 TASK STATEMENT", "content": "In order to systematically evaluate the reasoning capabilities of LLMs, CLR-Bench incorporates a diverse set of question types that reflect various challenges. Each question type is carefully designed to test different aspects to verify LLM's mastery of in-depth rationalizing. Moreover, given a question, LLMs are required to provide rationales to prove their understanding beyond the direct prediction. In this section, we formally define the question types and the rationale we expect for each type."}, {"title": "2.1 MULTI-TYPE REASONING TASKS", "content": "Multi-choice (MC) questions present a single question with multiple possible answers including disturbing choices, from which LLMs must select the most appropriate one. This type of question evaluates an LLM's ability to discern and provide the correct answer from a set of plausible options;\nMulti-select (MS) questions are similar to multi-choice questions but require LLMs to select more than one correct answer. The more disturbing situations assess the LLM's capability to handle more complex queries where multiple factors must be considered. Moreover, LLMs also face the dilemma of either choosing more for the full mark or selecting the most confident one to get half of the score;\nFill-in-blank (FB) questions require LLMs to complete a sentence or statement with the appropriate term or phrase, measuring how well the LLM could generate contextually relevant content;\nOpen-ended (OE) questions allow for a wide range of responses, requiring LLMs to formulate detailed and comprehensive answers. It tests the LLM's ability to generate well-rounded explanations;"}, {"title": "2.2 FORMALIZATION OF RATIONALE", "content": "In this subsection, we define the expected rationale for each question type in CLR-Bench. The rationale must demonstrate not just correctness but a clear understanding of the contexts.\nInterference Recognition for MC: The rationale should explain why the selected option is correct and why the disturbing alternatives are incorrect, demonstrating clear reasoning and a solid grasp of the underlying principles while eliminating the interference impacts from incorrect choices;\nComplexity Management for MS: For multi-select questions, the rationale is expected to justify each correct selection and explain why the other options are wrong, not only reflecting the LLM's knowledge reserve and an anti-disturbance ability, but also decision-making performance when facing the dilemma of more marks but higher risks, or a safer selection but lower accuracy;\nMastery of concepts for TF: The rationale should concisely state the concepts behind the truth value, citing relevant facts or rules that support the decision;\nConceptual understanding for FB: Explaining FB questions requires reasons why the chosen word or phrase fits the sentence, showing an understanding of context and meaning, as well as the underlying knowledge or concepts in the statement;\nOpen reasoning ability for OE: As the most difficult task, the rationale for OE, different from the corresponding solution, should provide a structured, detailed, and well-rounded explanation, covering all relevant aspects and reflecting comprehensive reasoning steps to infer the final answer.\nBy formalizing the rationale for each type of question, CLR-Bench ensures that LLMs are evaluated not only on their ability to provide correct answers but also on their capability to reason effectively and justify their choices. A sketched comparison is shown in Table 2.2, indicating that our benchmark significantly moves beyond surface-level correctness in existing benchmarks to assess the depth of understanding, logical coherence, and mastery of the subject matter."}, {"title": "3 CONSTRUCTION DETAILS OF CLR-BENCH", "content": "To ensure the quality of our benchmark, We recruit eight domain experts to handle 16 college-level disciplines. Their expertise is maximally leveraged through our novel three-stage strategy. In this section, we introduce the core procedures of constructing a high-quality dataset leveraging both expert knowledge and GPT-4o as a tool."}, {"title": "3.1 HIERARCHICAL TOPIC GRAPH", "content": "A hierarchical topic graph (HTG) G is effectively designed to support multiple disciplines, ensuring that key topics across computer science and artificial intelligence are captured in a way that mirrors their complexity and interrelationships. To comprehensively model discipline-specific knowledge, we refer to the table-of-content in the textbooks where the multi-level structure ensures that the graph reflects high-level overviews and detailed subtopics to guide the question collection process."}, {"title": "3.2 MULTI-TYPE QUESTION COLLECTION", "content": "Given the well-structured HTG, a rigorous process is employed to collect questions spanning a wide range of topics in computer science and artificial intelligence. Experts manually curate the question set based on a structured ontology, which organizes knowledge into three levels: 16 level-1 topics, 40 level-2 topics, and 26 level-3 topics. Each level reflects a progression from broad subject areas to more specific subtopics. For each concept within these n topics, experts are required to have at least two questions of each question type, i.e., 2\u00d7 5 \u00d7 n, to maintain a balanced and comprehensive coverage of the content. To ensure high quality, experts refer to authoritative sources such as textbooks, course materials, and widely accepted curricula. Each question is then reviewed to verify its relevance, clarity, and alignment with its respective concept. The process ensures the dataset reflects real-world educational standards while encompassing the full breadth and depth of the domain."}, {"title": "3.3 EXPERT-GUIDED RATIONALE GENERATION", "content": "One of the key contributions of CLR-Bench is the incorporation of expert-verified rationales, which test whether LLMs truly understand the underlying concepts behind their answers. While manually generating detailed rationales for thousands of questions would be both time-consuming and resource-intensive, we streamline this process by integrating GPT-4o alongside expert knowledge. This hybrid approach maximizes both efficiency and quality, reducing the manual burden on experts while ensuring that the rationales produced are both comprehensive and accurate. The detailed prompt templates for expert-guided rationale generation are illustrated in Appendix C.\nThe process operates in a feedback loop where experts provide carefully designed prompts to GPT-4o, which generates initial rationale drafts. These drafts include all necessary explanations, logical reasoning, and relevant information tied to the question. Experts then review the output, making refinements or modifications as needed to meet high-quality standards. This collaborative approach leverages the speed and coverage of GPT-4o while maintaining the nuanced accuracy that expert verification ensures. By combining human expertise with the capabilities of GPT-4o, we can produce high-quality rationales at a fraction of the cost and time of fully manual methods.\nBy using GPT-40 as an initial rationale generator, we significantly reduce the effort required for manual rationale creation, making the process scalable across large datasets. Experts no longer need to write rationales from scratch but can instead focus on refining and verifying the generated explanations. This hybrid model ensures both speed and quality in rationale production, enabling us to maintain high standards while accommodating the breadth of questions across multiple disciplines. Rather than assessing LLMs purely on their ability to select correct answers, we evaluate their capacity to generate well-structured, coherent explanations that demonstrate true comprehension. This shift from answer prediction to reasoning evaluation is fundamental in advancing LLMs for real-world applications, where understanding and articulation are critical. In general, CLR-Bench comprises 1,018 college-level discipline-specific questions. Among them are 217 for MC task, 111 for MS, 316 for TF, 269 for OE, and 105 for FB, in a relatively balanced distribution."}, {"title": "4 EVALUATION AND EXPERIMENTAL ANALYSIS", "content": "In this section, we formally define our novel evaluation protocol with two new metrics, especially introducing the criteria on reasoning performance in terms of Q\u2192AR. Insightful analysis is provided through systematically evaluating 40 state-of-the-art LLMs spanning both open and closed-source over 1,018 questions in CLR-Bench.\nIn this part, we would like to highlight one of our contributions by introducing a novel evaluation protocol considering the performance of both final prediction (Q\u2192A) and rationale (Q\u2192AR). We provide a detailed demonstration of a fair measurement for each type of question.\nQ\u2192A. (i) Exact Matching is applied when the expected answer is discrete and well-defined, as in MC, MS, TF, and FB questions. (ii) Semantic Similarity. The evaluation of OE questions remains a hard problem for the community. We leverage RoberTa-large (Liu et al., 2019) to sketchily compute the semantic similarity between the gold solution and the provided version by LLM. The threshold is set at 0.90, which means that the solution with higher scores will be treated as correct, while the remaining questions will be passed to the next step. (iii) GPT-40-assisted Expert Evaluation. To improve the experts' efficiency and maximize their efforts on OE evaluation, we have carefully designed a prompt template that asks GPT-40 to provide a draft including the essential reasoning steps and relevant information. Experts will then verify the draft and fix the final gold rationale one by one to ensure accuracy and completeness.\nBefore evaluating Q\u2192AR that measures the ability to answer and rationalize at the same time, we introduce another intermediate metric for rationale only, i.e., Q\u2192R. (i) Semantic Similarity. We compute the similarity score between the LLM-generated rationale and a gold-standard rationale provided by experts, filtering out rationales that are semantically close to the label. (ii) GPT-4-assisted Expert Evaluation. Similarly, we process the rationales that are marked lower than 0.9 with the help of GPT-40 and experts. Differently, we have a set of particularly designed templates for each question type, details of all the prompts used for Q\u2192A and Q\u2192R are introduced in Appendix D. All the rationales will be marked {0, 0.5, 1} for the score."}, {"title": "4.1 ONE-SHOT SETTING", "content": "Unlike benchmarks such as MMLU (Hendrycks et al., 2020) that evaluate LLMs in five-shot, we introduce a standardized one-shot setting in CLR-Bench to ensure uniformity across models, particularly for smaller LLMs. Experts first maintain a fixed set with a single example per question type and discipline. The example guides the LLMs in structuring their responses while preventing them from leveraging extensive context. For instance, LLMs are required to output in the format of {Rationale:{}, Answer:{}}.This constraint ensures a consistent output format across all models, promoting fairness in evaluation, especially when comparing large and small LLMs. By using one-shot examples, we also aim to highlight the fairness in CLR-Bench of evaluating the reasoning capabilities without potential over-reliance on multiple context examples."}, {"title": "4.2 LEADERBOARD", "content": "For benchmarking widely used large language models (LLMs), we selected around 40 models from various model families and series. This includes popular open-source LLM families such as LLAMA (Touvron et al., 2023a;b; Dubey et al., 2024), Qwen (Bai et al., 2023; Yang et al., 2024), Phi (Abdin et al., 2024), and Mistral (Jiang et al., 2023; 2024), as well as powerful proprietary models like GPT (Brown, 2020; Achiam et al., 2023) and Claude (Anthropic, 2023a).\nLLAMA (Dubey et al., 2024) is currently one of the most prominent LLM architectures, with model sizes ranging from 1B to 405B parameters across five generations. Phi (Abdin et al., 2024) and Gemma (Team et al., 2024a;b) families extend context length while maintaining relatively smaller model sizes, achieving performance comparable to larger models. Qwen (Yang et al., 2024), Yi (Young et al., 2024), and DeepSeek (Bi et al., 2024; Liu et al., 2024) stand out as LLMs excelling in areas such as mathematics, coding, and language understanding, benefiting from specialized downstream fine-tuning (Hui et al., 2024). The Mistral (Jiang et al., 2023) family, leveraging a Mixture of Experts (MoE) architecture (Jiang et al., 2024), excels in reasoning while maintaining high efficiency.\nAmong proprietary LLMs, the GPT series, widely recognized through ChatGPT (Ray, 2023) and GPT-4 (Achiam et al., 2023), is regarded as one of the most successful and powerful in the AI community. Extensive training data and well-crafted architectures enable these models to handle highly complex and challenging tasks. The Claude (Anthropic, 2023a) series is particularly adept at understanding nuanced and complex human instructions, while the Gemini (Team et al., 2023)"}, {"title": "4.3 OBSERVATIONS AND DISCUSSIONS", "content": "The results in the CLR-Bench leaderboard highlight notable discrepancies between the models' ability to answer questions correctly (Q\u2192A) and their capacity to rationalize those answers accurately (Q\u2192AR). In this section, we highlight four insightful observations and conduct corresponding discussions hereunder.\nWe summarize the main performance in Table 2. Subject to Q\u2192A, Qwen2.5-72b-Instruct, GPT-4-turbo, and Qwen2.5-72b consistently outperform other models with 64.05%, 63.31%, and 62.52% respectively, showing their strong comprehension ability in providing correct answers across tasks. Other large-scale models like Llama-3.1-70b-Instruct and Phi-3-Medium-4K-Instruct also demonstrate relatively high performance, achieving 57.12% and 56.97% in the Q\u2192A category. While smaller models such as Gemma2-9b and Gemma-7b-IT struggle significantly, with Q\u2192A scores of 16.26% and 25.83% respectively. In general, while some top-performing models show promise, the overall performance of LLMs\u2014including both closed- and open-source models remains relatively unsatisfactory. It also reflects the high quality and complexity of our dataset, emphasizing the need for further advancements in college-level question answering.\nLLMs tend to 'guess' answers. A detailed analysis of Q\u2192AR scores reveals that despite their relatively high accuracy in answering questions (Q\u2192A), many LLMs struggle significantly to provide coherent and accurate rationales. For instance in Figure 5, while Qwen2.5-72b-Instruct leads the leaderboard in Q\u2192A with a score of 64.05%, the corresponding Q\u2192AR score drops dramatically to 40.79%. This discrepancy suggests that LLMs are probably 'guessing' the correct answers without a deep understanding of the underlying reasoning. This phenomenon becomes more evident in models such as GPT-4-turbo, which achieves a strong Q\u2192A performance (63.31%) but also exhibits a noticeable drop to 39.52% in Q\u2192AR. We have provided sufficient examples of this potential guessing behavior in the Appendix E."}, {"title": "5 RELATED WORK", "content": "In recent years, the development of various benchmarks has significantly improved the evaluation of large language models (LLMs) from multiple task perspectives. General benchmarks like MMLU (Hendrycks et al., 2020), MMLU-Pro (Wang et al., 2024a), GLUE (Wang et al., 2021), SuperGLUE (Sarlin et al., 2020), CommonSenseQA (Talmor et al., 2018), BIG-Bench (Srivastava et al., 2022), and the AI2 Reasoning Challenge (ARC-Challenge) (Clark et al., 2018) have played a pivotal role in advancing the understanding of language, reasoning, and question-answering tasks, leading to more specialized and comprehensive assessments. More recent benchmarks, such as BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), and SQUAD (Rajpurkar et al., 2018), further enhance the evaluation of model comprehension and complex reasoning abilities. Task-specific benchmarks like HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021) have broadened the evaluation landscape, covering diverse areas and expanding the scope of LLM evaluation.\nTo facilitate performance comparisons between LLMs, platforms like the OpenLLM Leader-board (Aidar Myrzakhan, 2024) and OpenCompass (Contributors, 2023) have been established. However, as LLMs evolve rapidly, leaderboard scores are becoming increasingly saturated at the top. Models such as GPT-4 (Achiam et al., 2023), Claude-3 (Anthropic, 2023b), and open-source models like the Qwen (Yang et al., 2024) series are achieving near-perfect results across various benchmarks. This trend underscores the need for more challenging benchmarks to push the limits of LLM capabilities. Moreover, existing benchmarks typically focus solely on reporting the evaluation results of LLMs, limiting assessments to whether an answer is correct while overlooking the reasoning process and rationale that reveal a model's true understanding of a question. To address this gap, we introduce a multi-type rationalized dataset featuring 1,018 questions across five types and 16 subjects in computer science and AI, designed to challenge LLMs with college-level reasoning tasks. For a more robust evaluation, we propose two new metrics: Q\u2192A and Q\u2192AR, which assess both the correctness of answers and the quality of their rationales.\nOur findings show that LLMs tend to guess college-level answers without fully grasping the rationale behind them. Interestingly, larger models do not consistently outperform smaller ones in terms of reasoning. Through extensive experiments involving 40 LLMs, we provide valuable insights into performance variations across question types, topics, and model sizes."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we present CLR-Bench, a novel benchmark specifically designed to evaluate the reasoning capabilities of large language models in college-level tasks, focusing on computer science and artificial intelligence. We release a high-quality multi-type question-answering dataset comprising 1,018 questions spanning 16 disciplines. A novel reasoning evaluation paradigm is proposed through Q\u2192A and Q\u2192AR metrics. Unlike traditional benchmarks that solely assess the correctness of final answers, our framework goes beyond by requiring models to provide coherent rationales for their answers, ensuring a deeper evaluation of their reasoning capabilities.\nThrough extensive experiments on 40 LLMs, we observed significant gaps between the accuracy of answers (Q\u2192A) and the combined performance on answers and rationales (Q\u2192AR). Our key insights include: (i) LLMs tend to 'guess' the answers since higher Q\u2192A often fails to lead to higher Q\u2192AR. Even when models achieve high accuracy on answers alone, their Q\u2192AR scores were notably lower, indicating that models often fail to fully understand the rationale behind their correct answers. This observation underscores the need for better reasoning mechanisms within LLMs and suggests that current models may rely on shortcuts or superficial patterns rather than truly grasping the underlying concepts. ii Model size does not consistently guarantee a better reasoning ability. Smaller models may even surpass larger ones in terms of Q\u2192AR, even though they fall behind on Q\u2192R. We believe this observation may inspire the community for further exploration toward achieving a more robust understanding beyond the direct answer prediction."}, {"title": "DEFINITION: Hierarchical Topic Graph.", "content": "Let O be a well-structured ontology for constructing a HTGG = {0,E,R}, where O = {T,R} defines the topics T and relations R between domain-specific topics E. The ontology O is derived from expert knowledge in 16 disciplines and structured around a multi-level hierarchy. Specifically, T consists of three levels of topics, where each level represents increasingly specific subtopics.\nEach triple (h, r, t) in G comprises a head topic h, a relation r, and a tail subtopic t, where h, t \u2208 T and r\u2208 R. We define the relations R as \"has_subtopic\" to represent the hierarchical structure, capturing the relationship between general topics and their more granular subtopics. For instance, in the domain of computer science, let h = \"Programming fundamentals\", r = \"has_subtopic\", and t = \"Variable\". Further, we define the next level of granularity with the triple (Variable, has_subtopic, Constant). The construction of G begins with a careful review of tables of contents and chapter titles from relevant textbooks or curriculum guides. Experts from each discipline assist in condensing the most critical topics and ensuring that the hierarchical relationships are accurate and representative of the disciplines's knowledge structure."}, {"title": "Q\u2192AR.", "content": "Based on the previously computed Q\u2192A and Q\u2192R score, we could automatically calculate the Q AR score accordingly. We enforce a strict evaluation to punish the potential 'guessing' behavior, particularly in cases where the model provides a correct answer but is associated with an incorrect or incomplete rationale. Specifically, (i) if both the answer and the rationale are correct, the model is awarded a full score of 1.0. (ii) However, if the answer is correct but the rationale is only partially correct, the score is reduced to 0.5. (iii) When the answer is correct but the rationale is completely wrong, the score is significantly penalized, dropping to 0.0. (iv) On the other hand, if the answer is incorrect but the rationale is correct, the model still receives some credit with a score of 0.5, and with a partially correct rationale, the score becomes 0.25. (v) If both the answer and rationale are incorrect, the score remains at 0.0. This ensures that models are not unfairly rewarded for guessing correct answers without demonstrating understanding through their rationales."}]}