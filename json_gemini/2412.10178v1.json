{"title": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models", "authors": ["Hung Nguyen", "Quang Qui-Vinh Nguyen", "Khoi Nguyen", "Rang Nguyen"], "abstract": "Given an input video of a person and a new garment, the objective of this paper is to synthesize a new video where the person is wearing the specified garment while maintaining spatiotemporal consistency. While significant advances have been made in image-based virtual try-ons, extending these successes to video often results in frame-to-frame inconsistencies. Some approaches have attempted to address this by increasing the overlap of frames across multiple video chunks, but this comes at a steep computational cost due to the repeated processing of the same frames, especially for long video sequence. To address these challenges, we reconceptualize video virtual try-on as a conditional video inpainting task, with garments serving as input conditions. Specifically, our approach enhances image diffusion models by incorporating temporal attention layers to improve temporal coherence. To reduce computational overhead, we introduce ShiftCaching, a novel technique that maintains temporal consistency while minimizing redundant computations. Furthermore, we introduce the TikTokDress dataset, a new video try-on dataset featuring more complex backgrounds, challenging movements, and higher resolution compared to existing public datasets. Extensive experiments show that our approach outperforms current baselines, particularly in terms of video consistency and inference speed. Data and code are available at https://github.com/VinAIResearch/swift-try.", "sections": [{"title": "Introduction", "content": "Video virtual try-on is an emerging research area (Chen et al. 2021; Rogge et al. 2014; Pumarola et al. 2019; Dong et al. 2019b; Kuppa et al. 2021; Zhong et al. 2021; Jiang et al. 2022; He et al. 2024; Xu et al. 2024b; Fang et al. 2024; Zheng et al. 2024) with significant potential in fashion and e-commerce. The ability to realistically visualize how a garment looks on a person in a video could transform online shopping. However, despite recent progress in image-based virtual try-on (He, Song, and Xiang 2022; Choi et al. 2021; Lee et al. 2022; Xie et al. 2023; Zhu et al. 2023; Kim et al. 2023), extending these capabilities to video remains challenging due to the need for spatiotemporal consistency and the high computational costs of processing long sequences.\nA significant challenge in video virtual try-on is balancing the need for temporal coherence with the computational demands of processing long video sequences. Previous methods (Xu et al. 2024b; He et al. 2024; Fang et al. 2024) often struggle with temporal inconsistencies, leading to visual artifacts and flickering between frames, which undermines the realism of the virtual try-on experience. Additionally, the high computational cost of rendering high-quality results over extended sequences hampers the practicality of these approaches for real-world applications.\nAnother challenge is lacking an evaluation dataset. The first public video try-on dataset VVT (Dong et al. 2019b) only covers basic pattern garments, form-fitting T-shirts, uniform backgrounds, static camera angles, and repetitive human motions. Recently, ViViD (Fang et al. 2024) released the first practical dataset for video virtual try-on; however, it struggles to handle in-the-wild scenarios, such as complex movements and diverse backgrounds, making it challenging to meet the demands of practical applications. Moreover, one reason for the poor quality video try-on result is primarily due to masks extracted using human parsing segmentation (Li et al. 2020) applied on each frame of the video.\nIn this paper, we tackle these challenges by introducing two key contributions. First, we present a new high-quality dataset named TikTokDress consisting of 817 videos which is specifically designed for training and evaluating video virtual try-on models. This dataset features realistic scenes, diverse garment types, and complex movements, providing a robust foundation for advancing research in this field. Second, we introduce a novel video virtual try-on framework named SwiftTry, which significantly reduces the computational cost of processing long video sequences while maintaining temporal consistency. Our framework is inspired by state-of-the-art diffusion-based image virtual try-on methods (Kim et al. 2023; Xu et al. 2024a; Choi et al. 2024) and incorporates temporal attention in the UNet architecture to train on video try-on data. During inference, we introduce a new technique called ShiftCaching, which ensures temporal coherence and smooth transitions between video clips while also reducing redundant computation compared to previous methods. Extensive experimental results indicate that our proposed SwiftTry framework, incorporating the aforementioned techniques, substantially surpasses other video virtual try-on methods in performance.\nIn summary, the contributions of our work are as follows:\n\u2022 We propose a new technique for video inference named ShiftCaching, which can ensure temporal smoothness between video clips and reduce redundant computation.\n\u2022 We introduce and curate a new video virtual try-on dataset, TikTokDress, which encompasses a wide range of backgrounds and complex movements and features high-resolution videos, filling a gap that exists in previous video virtual try-on datasets."}, {"title": "Related Work", "content": "Image Virtual Try-On. Traditional image virtual try-on methods (Han et al. 2018; Wang et al. 2018a; Dong et al. 2019a; Yang et al. 2020; Ge et al. 2021; He, Song, and Xiang 2022; Choi et al. 2021; Lee et al. 2022; Xie et al. 2023) commonly employ a two-stage pipeline based on GANS (Goodfellow et al. 2014). In this approach, the target clothing is first warped and then fused with the person image to create the try-on effect. Various techniques have been utilized for clothing warping, including thin-plate spline (TPS) warping (Han et al. 2018), spatial transformer networks (STN) (Li et al. 2021), and flow estimation (Xie et al. 2023). Despite these advances, such methods often face limitations in generalization, resulting in significant performance degradation when applied to person images with complex backgrounds.\nRecently, diffusion models have markedly enhanced the realism of images in generative tasks, leading to their increasing adoption in virtual try-on research. For instance, TryOnDiffusion (Zhu et al. 2023) presents a virtual try-on method utilizing two U-Nets, but it requires a large dataset of image pairs depicting the same person in various poses, which can be difficult to acquire. StableVITON (Kim et al. 2023) conditions the garment in a ControlNet (Zhang, Rao, and Agrawala 2023)-style using a zero cross-attention block, while IDM-VTON (Choi et al. 2024) proposes GarmentNet to encode low-level features combined with high-level semantic features extracted via IP-Adapter (Ye et al. 2023).\nDespite these advancements, extending these existing image virtual try-on methods for video often results in significant inter-frame inconsistency and flickering, which adversely affects the overall quality of the generated results.\nVideo Virtual Try-On. Several efforts have been made to develop virtual try-on for videos. FW-GAN (Dong et al. 2019b) integrates an optical flow prediction module from Video2Video (Wang et al. 2018b) to warp preceding frames to the current frame, enabling the synthesis of temporally coherent subsequent frames. MV-TON (Zhong et al. 2021) introduces a memory refinement module that retains and refines features from previous frames. ClothFormer (Jiang et al. 2022) utilizes a vision transformer in its try-on generator to minimize blurriness and temporal artifacts, and it features an innovative warping module that combines TPS-based and appearance-based methods to address issues like incorrect warping due to occlusions. Among diffusion-based methods, Tunnel Try-On (Xu et al. 2024b) is the first to apply diffusion models for video virtual try-on, effectively handling camera movement and maintaining consistency, although its demo videos are limited to only a few seconds long. ViViD (Fang et al. 2024) introduced a large-scale video try-on dataset with multiple categories, but it remains limited by simple backgrounds and movements, which constrains its ability to maintain long-term consistency and coherence. In this paper, we propose a novel technique that ensures temporal smoothness and coherence across video clips, complemented by a caching technique (Ma, Fang, and Wang 2024) that reduces redundant computations during long video inference."}, {"title": "Methods", "content": "Problem Statement: Given a source video $\\mathcal{V} = {I_1, I_2, ..., I_N} \\in \\mathbb{R}^{N \\times 3 \\times H \\times W}$ of a person and a garment image $g \\in \\mathbb{R}^{3 \\times H \\times W}$, where $N$, $H$, and $W$ represent the video length, frame height, and frame width, respectively, our goal is to synthesize an target video $\\widehat{\\mathcal{V}} = {\\widehat{I}^1, \\widehat{I}^2,..., \\widehat{I}^N} \\in \\mathbb{R}^{N \\times 3 \\times H \\times W}$ of the person wearing the garment, while preserving the motion of the person, the background in $\\mathcal{V}$, and the color and texture of $g$.\nIt is important to note that collecting both source and target videos of the same person with identical motion and gestures, differing only in the garment, is extremely challenging. As a result, most video try-on approaches adopt a self-supervised training method, where only a single video is used, and the garment regions are masked. The model is then trained to inpaint the masked regions using guidance from the garment image.\nIn the next section, we first describe our overall video try-on architecture and then discuss in detail the ShiftCaching technique - one of our main contributions."}, {"title": "Overall Architecture", "content": "Our approach consists of two stages: first, training a diffusion-based image try-on model, and then extending it to work with video data by incorporating temporal attention into every block of the Main UNet."}, {"title": "ShiftCaching Technique", "content": "Due to memory constraints, current video diffusion-based virtual try-on methods can only generate video chunks of 16 frames at a time. Previous approaches (Fang et al. 2024; He et al. 2024; Xu et al. 2024b) use a temporal aggregation technique (Tseng, Castellon, and Liu 2023; Xu et al. 2023) to stitch overlapping video clips into longer sequences. In this process, the long video is divided into overlapping clips with an overlap size $S$, typically set to $N/2$ or $N/4$. At each denoising timestep $t$, the overlapping noise predictions are merged using a simple averaging technique. However, this method involves a trade-off: a smaller overlap size, such as $S = 4$, can cause temporal flickering and texture artifacts, while a larger overlap size, such as $S = 15$, improves consistency but greatly slows down the process as shown in Tab. 1.\nTo achieve good temporal coherence and smoothness without recomputing the overlapped regions, we propose a shifting mechanism during inference. Specifically, we divide the long video into non-overlapping chunks ($S = 0$). At each DDIM sampling timestep $t$, we shift these chunks by a predefined value $\\Delta$ between two consecutive frames, allowing the model to process different compositions of noisy chunks at each step. An example of a fixed $\\Delta = 4$ applied to a chunk with length $N = 8$ is illustrated in Fig. 3.\nTo further accelerate the inference process, we can skip a random chunk to reduce redundant computation during denoising. However, dropping chunks without adjustment can lead to abrupt changes in noise levels in the final results. Following (Ma, Fang, and Wang 2024), which notes that adjacent denoising steps share significant similarities in high-level features, we instead perform partial computations on the Main U-Net. Specifically, we use a cache to copy the latest features from the fully computed timestep $2t+1$ (Red frame) and use these features to partially compute the current latent $z_t$ (White frame), bypassing the deeper blocks of the UNet, as illustrated in Fig. 4.\nWhen performing partial computations on a chunk, the cached features typically include a first half from timestep $t+2$ and a second half from timestep $t + 1$, which can cause mismatches between the two halves. To address this, we introduce a Masked Temporal Attention mechanism. This mechanism incorporates a special mask with size $N \\times N$ during the softmax attention calculation to set the attention matrix values to 0, preventing information transfer from less accurate features (timestep $t + 2$) to more accurate features (timestep $t + 1$), while allowing transfer from good features to bad features. This mechanism ensures both smoothness and high quality in the partially computed cells."}, {"title": "Tik Tok Dress Dataset", "content": "Public datasets for single-image virtual try-ons, such as VITON-HD and DressCode, often suffer from simple backgrounds and limited human poses, despite offering high-quality images. These datasets are also restricted to single-image scenarios. Similarly, the VVT dataset, a standard for video virtual try-on, has notable drawbacks, including uniform movements, white backgrounds, and low resolution (256 \u00d7 192), making it unsuitable for real-world applications, particularly in the short-video industry where higher resolution is crucial. Given that real-world videos are typically recorded on mobile phones, which introduces variations in background, camera position, and lighting, there's a need for a more robust dataset. To address these gaps, we introduce TikTokDress, a high-resolution video virtual try-on dataset that includes complex backgrounds, diverse movements, and gender representation. Each video is paired with the corresponding garment and is annotated with detailed human poses and precise binary cloth masks, enhancing its value for real-world applications.\nFirst, the quality of garment masks in our dataset is crucial for enhancing try-on results, as shown in the Supplementary Material. While existing datasets like VITON-HD (Choi et al. 2021), DressCode (Morelli et al. 2022), and VVT (Dong et al. 2019b) use a standard segmenter (Li et al. 2020), it struggles with complex videos, resulting in subpar performance. In contrast, TikTokDress offers manually corrected, highly accurate garment masks, leading to significantly improved video try-on quality.\nSecond, our TikTokDress dataset captures a broad range of human poses and dynamic movements, such as dancing, common in short-form videos. As shown in Fig. 5, it includes variations in camera distance and diverse backgrounds, from indoor to outdoor settings with complex lighting. Additionally, it features a variety of clothing types, from casual T-shirts to structured garments like sweaters and challenging attire such as chainmail tops, addressing real-world challenges in video virtual try-on."}, {"title": "Video collection and annotation.", "content": "Our dataset consists of short TikTok clips (10-30 seconds) featuring various dance routines, as shown in Fig. 5. We expanded the TikTok Dataset (Jafarian and Park 2021) by adding videos to increase diversity in backgrounds, skin tones, and clothing styles, resulting in 817 video-garment pairs. Videos with excessive motion blur or low-quality garments were excluded. To ensure accurate garment matching, we manually selected high-quality matches from fashion retail websites. The dataset includes over 270,000 RGB frames extracted at 30 frames per second. We also calculated 2D keypoints and dense pose information using DWPose (Yang et al. 2023) and DensePose (G\u00fcler, Neverova, and Kokkinos 2018). Dataset statistics are summarized in Tab. 2, highlighting diversity in gender, skin tone, and camera positions.\nCreating high-quality garment masks for each video was challenging due to the need for precise segmentation in every frame. We used SAM 2 (Ravi et al. 2024) to extract masks for both clothing and arms, but its sensitivity to prompt points and specific frames (see Fig. 6-(a)) required an additional solution. We developed an algorithm (described in the Supplemental Material) to automate the selection of optimal frames and prompts, improving efficiency across various scenarios. Despite these advancements, complex or unusual garments still needed manual intervention, which we detail with examples in the Supplementary Material. This meticulous process was essential for ensuring the dataset's high quality and reliability."}, {"title": "Experiments", "content": "Datasets: We evaluate our approach on the VVT dataset (Dong et al. 2019b) and our new TikTokDress dataset. The VVT dataset, a standard benchmark for video virtual try-on, includes 791 paired videos of individuals and clothing images, with 661 for training and 130 for testing, all at 256 \u00d7 192 resolution. The videos feature simple movements against plain backgrounds. In contrast, the TikTokDress dataset offers a more complex challenge, with varied backgrounds, dynamic movements, and diverse body poses. It comprises 693 training videos and 124 testing videos at 540 \u00d7 720 resolution, totaling 232,843 frames for training and 39,705 frames for testing.\nMetrics: We evaluate our approach using image-based and video-based metrics in both paired and unpaired settings, as outlined in (Jiang et al. 2022). In paired settings, we use SSIM (Wang et al. 2004) and LPIPS (Zhang et al. 2018) to assess reconstruction quality. In unpaired settings, we measure visual quality and temporal consistency with Video Fr\u00e9chet Inception Distance (VFID) (Dong et al. 2019b). Additionally, we measure inference speed in frames per second (FPS) to demonstrate speed improvements.\nImplementation details: The training process is divided into two stages. In the first stage, we focus on inpainting and preserving detailed garment textures using the VITON-HD dataset (Choi et al. 2021). We fine-tune the Garment UNet, Pose Encoder, and Main UNet decoder, initializing the Main UNet and Garment UNet with pretrained weights from SD 1.5, while keeping the VAE Encoder, Decoder, and CLIP image encoder weights unchanged. In the second stage, we incorporate temporal attention layers into the previously trained model, initializing these new modules with pretrained weights from AnimateDiff (Guo et al. 2023)."}, {"title": "Comparisons with Prior Approaches", "content": "We compare our approach with other video virtual try-on methods using the VVT and TikTokDress datasets. As most methods are closed-source, we rely on reported results and available generated videos for comparison. For GAN-based methods, we evaluate against FW-GAN (Dong et al. 2019b), MV-TON (Zhong et al. 2021), and ClothFormer (Jiang et al. 2022). For diffusion-based methods, we compare with Tunnel Try-On (Xu et al. 2024b), ViViD (Fang et al. 2024), and WildVidFit (He et al. 2024). We re-evaluate ViViD (Fang et al. 2024) on the VVT dataset, as it is the only method with available inference code and pre-trained weights. Additionally, we compare our model with the image-based virtual try-on method StableVITON (Kim et al. 2023), finetuned on the VVT dataset, in a frame-by-frame manner. We also evaluate a baseline combining StableVITON and AnimateAnyone (Hu et al. 2023), where StableVITON performs the try-on for individual frames, and AnimateAnyone generates a video based on the source motion."}, {"title": "Conclusion", "content": "In conclusion, we have proposed a novel technique, ShiftCaching, which ensures temporal smoothness across video clips while effectively reducing redundant computations during video inference. This advancement enhances the efficiency and quality of video virtual try-on, making it more practical for real-world applications. Additionally, we have introduced a new dataset, TikTokDress, designed specifically for video virtual try-on. This dataset stands out for its diverse range of backgrounds, complex movements, and high-resolution videos, addressing the limitations of existing datasets and providing a valuable resource for future research in this area."}]}