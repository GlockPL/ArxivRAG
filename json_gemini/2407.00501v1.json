{"title": "Aeroengine performance prediction using a physical-embedded data-driven method", "authors": ["Tong Mo", "Shiran Dai", "An Fu", "Xiaomeng Zhu", "Shuxiao Li"], "abstract": "Accurate and efficient prediction of aeroengine performance is of paramount importance for engine design, maintenance, and optimization endeavours. However, existing methodologies often struggle to strike an optimal balance among predictive accuracy, computational efficiency, modelling complexity, and data dependency. To address these challenges, we propose a strategy that synergistically combines domain knowledge from both the aeroengine and neural network realms to enable real-time prediction of engine performance parameters. Leveraging aeroengine domain knowledge, we judiciously design the network structure and regulate the internal information flow. Concurrently, drawing upon neural network domain expertise, we devise four distinct feature fusion methods and introduce an innovative loss function formulation. To rigorously evaluate the effectiveness and robustness of our proposed strategy, we conduct comprehensive validation across two distinct datasets. The empirical results demonstrate : (1) the evident advantages of our tailored loss function; (2) our model's ability to maintain equal or superior performance with a reduced parameter count; (3) our model's reduced data dependency compared to generalized neural network architectures; (4)Our model is more interpretable than traditional black box machine learning methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Aeroengine performance prediction, especially engine thrust simulation, is a critical tool spanning the lifecycle from engine design, integration and control development to real-time performance monitoring and diagnostics of operational engines. It allows manufacturers to optimize designs, ground crews to understand engine health, engine health management, and operators to maximize performance, safety and efficiency. Given the importance of prediction as described above and the development of modern computer science, many methods have emerged.\nIn general, the simulation based on Computational Fluid Dynamics (CFD) is the most popular way to get pneumatic engine simulation data. Firstly, the CFD model of a specific engine is established by CAD-Computer Aided Design modelling of the specific engine and the creation of a high-quality computational mesh of the fluid volume. Then the CFD simulation is initialized according to the above settings. Finally, the specified parameters are solved by physical equations based on the obtained CFD simulation data [1], [2], [3]. With advances in computing power, many improvements to the CFD simulation have been given [4], [5], [6], [7] to further increase the accuracy of parameter prediction. These methods usually focus on optimizing the progress of CFD simulation such as developing more advanced turbulence modelling to improve the accuracy of separation flow and heat transfer predictions, thus the accuracy of parameter predictions is also improved. On the whole, CFD-based methods can provide highly accurate simulation data. However, it has a fatal flaw of being very processor-intensive due to the complexity of the underlying mathematics. This leads to substantial computational time and modelling costs, especially for large and detailed models. Although there have been many attempts to optimize for this, such as the use of massively parallel computation, the use of improved numerical algorithms [8], [9], [10] or the tighter integration of CFD into the design[11], none of these have changed the fact that CFD is computationally and temporally very costly.\nWith the development of artificial intelligence, data-driven machine learning(ML) methods have been used to predict engine parameters. ML methods discard physical equations, real engine structures, etc., and treat the whole simulation process as a black box, focusing only on the input and output results. As long as enough high-quality training data can be provided, ML models can learn nonlinear relationships between inputs and outputs within the model independently [12]. As a result, ML methods significantly lower the threshold for accessing simulation data, while CFD methods require deep domain knowledge. In addition, ML methods drastically reduce the time and computational resource cost of obtaining simulation data, which can be orders of magnitude faster than high-fidelity CFD simulations once trained. Specifically, CFD simulations often require many hours on large compute clusters, while ML methods can make real-time predictions [13].\nThe most typical ML method is the artificial neural networks (ANNs). ANNs have been attempted to be applied to system control and engine diagnosis since as early as the 1970s [14], [15]. Dalkiran [16] used ANNs to predict the thrust of an aeroengine using only three inputs: latitude, airspeed, and temperature, and controlled the relative error to less than 5%. Some comprehensive surveys give a detailed description of the application and development of ANNs for industrial engines as well [17], [18]. Li et al.[19] compared the performance of three ANNs in predicting the dust removal efficiency of rotating packed beds admitting the modelling difficulties of CFD methods in some cases and the great potential shown by ANNs. Kim et al. [20] verified that ANNs show high accuracy in the transient performance analysis and further improved the data-driven model based on ANNs for transient simulation. Besides ANNs, decision trees, random forests, support vector machines, and clustering (e.g., k-means, hierarchical) are also used to make predictions[21]. However, the performance of these methods, which are entirely data-driven and discard physical constraints, is highly dependent on the quality and quantity of the training data, resulting in lower model stability compared to that of CFD methods.\nTo improve the model stability of ML methods, hybrid methods which mix physical properties in a data-driven approach are introduced. There are three main ways to add physical features to a data-driven model[22]. The first is focusing on preprocessing the training data, which deepens the physical principles in the data by processing the training data with data augmentation[23]. The second starts from the loss function to enhance the physical features of the model by establishing a connection between the physical formulas and engine principles with the model loss function or injecting extra physical information to influence the model during the period of training[24], [25], [26]. For example, J. Raymond et al. [27] designed a novel loss function that introduces physical formulas for specific cases and datasets. The last and most commonly used one is to make a special design of neural network structure based on domain knowledge[28], [29]. For example, Lin. [30] proposed a hybrid method by fusing the structure of a real aeroengine and nerual network to predict the thrust. Compared to conventional neural networks, the hybrid network has maximum relative deviations below 4.0% and average relative deviations below 2.0% on all testing datasets which is superior to conventional neural networks. Additionally, physics-guided neural networks [31] use the simulated output and observed features based on the physical model to generate the prediction results through the neural network architecture. Despite achieving success, the above methods are either not sufficiently influential in terms of methodology, or cannot be realized in the short term due to the complexity of the physical equations and principles, or the overall design is unreasonable and ineffective due to the inability to balance the domain knowledge in the field of aeroengine and machine learning.\nIn this research, we will integrate neural network domain knowledge and aeroengine domain knowledge to design a physical-embedded neural network framework in a concise and universal way, so that it is very easy to extend for different applications. As the core of the proposed framework, four distinct feature fusion modules are deliberately designed for effective information aggregation on the premise of minimizing the number of network parameters as much as possible. We also proposed a novel loss function, termed as mean absolute relative error, to balance the optimization effects for both large-valued samples and small-valued samples, thus improving the effectiveness of network training. Extensive experiments have verified that the proposed hybrid model as well as the novel loss function is generally effective.\nThe rest of the paper is organized as follows. Section II gives the details of the proposed physical-embedded neural network framework, four distinct feature fusion modules and the novel loss function. Section III presents the experimental results for the models and loss functions on two datasets. We also validate the scalability and computational efficiency of the model. Finally, Section IV concludes our work and extracts future work."}, {"title": "II. PREDICTION MODELS", "content": "In this section, we will first introduce the proposed model architecture based on the physical structure of a real aeroengine and the technology of neural networks. As the core of the proposed method, four novel information fusion modules are elaborately designed and presented in detail in the following subsection. Finally, we will introduce an innovative loss function for optimizing network parameters more effectively during the training process."}, {"title": "A. Model Architecture by Fusing Engine Structure and Neural Network", "content": "Data-driven methods have been previously used for aeroengine performance prediction, but most of them are based on generalized neural networks and do not account for the characteristics of the aeroengine structure. This can result in poorly interpretable models with large parameter counts and low accuracy. Instead, we embed the physical structure of the aeroengine as shown in Figure 1 into the architectural design of the prediction model. Specifically, we construct sub-networks for the main components separately, and then connect them according to the physical coupling relationships between the aeroengine components. As a result, a novel model architecture for predicting aeroengine performance parameters is formed, which is termed as Physical-Embedded Neural Network."}, {"title": "B. Elaborately Designed Information Fusion Modules", "content": "The information fusion module constitutes the core of PENN and represents the pivotal technology that determines the network's performance. There are two distinct types of inputs to the information fusion module. The driving feature \\(z^{drive}\\) or upper-level fusion feature \\(e^{1}/e^{2}\\) serves as the module's main input, while the component features \\(z_{i}/z_{i}^{'}/z_{i}^{''}\\) act as the module's supplementary input. We employ multiple information interactions or fusion techniques to process the main and supplementary input features, thereby obtaining more comprehensive and enriched fusion features.\nFor simplicity, we abbreviate the main input and supplementary input as \\(e^{old}\\) and \\(z^{o}\\), respectively, and the output of the information fusion module as \\(e^{new}\\) in this subsection. The followings are the details of the four elaborately designed information fusion modules as illustrated in Figure 3."}, {"title": "1. Fully-Connected Fusion", "content": "Fully-Connected Fusion (FCF), drawing inspiration from the theory of multi-layer perceptrons, represents the most straightforward approach for feature fusion. It employs a simple fullyconnected layer to achieve feature fusion and dimensionality reduction, yielding information-rich and dimensionally consistent fusion features.\nAs illustrated in the Figure 3 (a), the main \\(e^{old}\\) and the supplementary input \\(z^{o}\\) are firstly stacked to obtain a 256 dimensional tensor, which is then compressed by a fully-connected layer to acquire the fusion feature \\(e^{new}\\):\n\\(e^{new} = ReLU(FC_{128} (e^{old} \\cup z^{o}))\\)   (10)\nwhere \\(U\\) denotes tensor stacking. The strength of FCF lies in its ability to learn intricate information interactions and relationships with a remarkably simple architecture. However, its drawback stems from the considerable increase in the overall number of parameters due to the dense interconnections, leading to computational inefficiency. Furthermore, it is susceptible to the overfitting problem when there are not enough training samples available."}, {"title": "2. Bottle-Neck Fusion", "content": "A bottleneck structure in a neural network, as illustrated in Figure 4, is a layer with fewer neurons compared to the layers preceding and succeeding it. This configuration creates a \"bottleneck\" that compels the network to learn a compressed representation of the input data. It has been widely applied in fields such as information compression [32], image segmentation [33], [34], [35], and object detection [36], [37]. Inspired by the success of bottleneck structure, we design the Bottle-Neck Fusion (BNF) module as shown in Figure 3 (b). BNF first converts the stacked tenser of \\(e^{old}\\) and \\(z^{o}\\) to a low dimensional tenser, and then lift its dimension to get the fusion feature \\(e^{new}\\):\n\\(e^{new} = ReLU(FC_{128} (ReLU(FC_{32}(e^{old} \\cup z^{o}))))\\)   (11)\nCompared to FCF, BNF offers three main advantages. Firstly, it effectively reduces the number of network parameters (approximately decreased from 256 \u00d7 128 to 256\u00d732+32\u00d7128), thereby improving the overall computational efficiency of the model. Secondly, BNF has more network layers than FCF, which increases the depth of the network and enhances the ability to learn more abstract features. Finally, from an information theory perspective, the bottleneck layer may bring potential information loss due to the lower dimensionality. However, this information loss can be beneficial, as it forces the network to learn a more generalized and robust representation of the data, thus can improve the generalization ability of the prediction model across various tasks."}, {"title": "3. Attention-Based Fusion", "content": "The attention mechanism, as the core of Transformers [38], has been widely applied in natural language understanding [39], computer vision [40], [41], [42], [43], multi-modal information fusion [44], [45], and various large foundation models [46]. The key idea is to let the network learn to focus on the most relevant features for the current context or task. This is done by computing attention weights that indicate the importance of each feature, and then taking a weighted sum to fuse the features together.Compared to simple fusion operations like concatenation or addition, attention allows for a more flexible and dynamic combination that adapts based on the content of input data. But at the same time, the attention mechanism also has drawbacks of high computational complexity and requiring a large amount of training data. We investigate it here mainly because of its high popularity.\nSpecifically, the proposed Attention-Based Fusion (ABF) module, as illustrated in the Figure 3 (c), first maps the main input \\(e^{old}\\) to low-dimensional Query space and the supplementary input \\(z^{o}\\) to low-dimensional Key and Value spaces for learning transformation spaces and reducing computational complexity:\n\\(q = FC_{query} (e^{old})\\)   (12)\n\\(\\{k_{1},k_{2}\\} = FC_{key}(\\{e^{old}, z^{o}\\})\\)   (13)\n\\(\\{V_{1}, V_{2}\\} = FC_{value}(\\{e^{old}, z^{o}\\})\\)   (14)\nwhere \\(FC_{query}(), FC_{key} (), FC_{value} ()\\) are three independent fully-connected layers with 16 nodes to implement space transformations; and \\(q, k_{1}/k_{2}, V_{1}/V_{2}\\) are the acquired query feature, key features and value features, respectively. Note that only the main input generates query feature for aggregation.\nSubsequently, based on the principle of the attention mechanism, the attention tensor \\(e^{att}\\) can be computed by:\n\\(e^{att} = \\frac{[qk_{1},qk_{2}]}{\\sqrt{d}} \\cdot [V_{1}V_{2}]\\)   (15)\nwhere \\(f_{softmax}()\\) is the widely used SoftMax function with temperature value of 10, and d is the dimensionality of q.\nFinally, we lift the dimension of \\(e^{att}\\) to get the 128-dimensional supplementary feature, which acts as the residual item of the fusion feature \\(e^{new}\\):\n\\(e^{new} = ReLU(e^{old} + FC_{128} (e^{att}))\\)   (16)\nwhere the function \\(ReLU ()\\) is used to restore the non-negativity of the input feature, and the residual structure is plugged in for reducing the information loss of the main input and accelerating the convergence speed of the network.\nUnlike in FCF and BNF where the main input and supplementary input have equal status, in ABF, the main input draws useful information from the supplementary input to enrich the main input, which is particularly in line with the design intention of the proposed PENN. However, ABF is susceptible to the insufficient training issue due to insufficient samples."}, {"title": "4. Channel-Adaptive-Weighted Fusion", "content": "The Channel-Adaptive-Weighted Fusion (CAWF) module draws on the core ideas of the channel weighting mechanism, which is a variant of the attention mechanism widely applied in the field of computer vision[47], [48]. As illustrated in the Figure 3 (d), the channel weighting mechanism assigns adaptive weights based on the content of the inputs to every feature channel when combining information from multiple sources. During the fusion process, important features related to the target characteristics are highlighted, while interfering features not related to the target characteristics are suppressed.\nInspired by the famous SENet [49], CAWF firstly employs bottleneck structures for mapping the inputs to their corresponding importance tensors \\(\\{e_{im}, z_{im}\\}\\), which are then normalized to get the weight tensors \\(\\{W_{1},W_{2}\\}\\):\n\\(\\{e_{im}, z_{im}\\} = FC_{128} (ReLU (FC_{32} (\\{e^{old}, z^{o}\\})))\\)   (17)\n\\(\\{W_{1},W_{2}\\} = f_{softmax_{c}}(\\{e_{im}, z_{im}\\})\\)   (18)\nwhere \\(f_{softmax_{c}}()\\) is the SoftMax function along the channel dimension with temperature value of 1. The acquired weight tensors reflect the relative importance of each feature in the corresponding dimension.\nFinally, CAWF utilizes these obtained weight tensors to perform a weighted summation of the two input features across each feature dimension, resulting in the 128-dimensional final fusion features \\(e^{new}\\).\n\\(e^{new} = W_{1} \\odot e^{old} + W_{2} \\odot z^{o}\\)   (19)\nwhere \\(\\odot\\) denotes element-wise multiplication. Similar to ABF, CAWF can learn the importance of information base on its content during the learning process. However, CAWF is realized based on fully-connected structures, which avoids the complex computation of the attention mechanism and significantly improves the computational efficiency. Compared with the traditional feature weighting idea, CAWF realizes the independent adaptive weighting of each feature dimension, resulting in higher degree of fusion between the features. In addition, CAWF get the weight tensors by comparing the importance of the inputs, which reflects the idea that the strength and weakness are relative."}, {"title": "C. Loss Function", "content": "The training of the model is a crucial aspect in the development of the PEEN model. Model training for neural networks is fundamentally an optimization problem, where the goal is to minimize a loss function by continuously adjusting the model parameters. This gradual reduction in the loss function leads to an improvement in the model's predictive capabilities.\nFor regression tasks, popular loss functions are Mean Squared Error (MSE) and Mean Absolute Error (MAE):\n\\(L_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}} )^{2}\\)   (20)\n\\(L_{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_{i} - \\hat{y_{i}} |\\)   (21)\nwhere n is the number of samples, \\(y_{i}\\) is the ground-truth value, and \\(\\hat{y_{i}}\\) is the predicted value.\nObviously, \\(L_{MSE}\\) and \\(L_{MAE}\\) are sensitive to the absolute values of predicted parameters which can lead to relatively large prediction errors for samples with small ground-truth values. However, we usually pay more attention to relative errors in practical applications. To address this issue, a new loss function, termed as Mean Absolute Relative Error (MARE) \\(L_{MARE}\\), is proposed as follows:\n\\(L_{MARE} = \\frac{1}{n} \\sum_{i=1}^{n} | \\frac{y_{i} - \\hat{y_{i}}}{y_{i}} |\\)   (22)\nAs can be seen, \\(L_{MARE}\\) calculates the absolute relative differences between the predicted and ground-truth values, rather than the absolute differences. Thus, \\(L_{MARE}\\) can normalize the errors for different absolute values, and can achieve more balanced optimization effects for both large-valued samples and small-valued samples."}, {"title": "III. EXPERINMENT", "content": "In this section, we will first introduce the datasets, the baseline models and the experimental settings. Next, the proposed hybrid model and the novel loss function are validated by extensive datasets experiments. Finally, we validate the scalability of the model and displays its computational efficiency to provide reference for practical applications."}, {"title": "A. Datasets", "content": "The simulation data of two typical aeroengines, namely High-Speed Dataset and Low-Speed Dataset, are employed for training and testing the proposed hybrid model as well as the novel loss function. Both datasets have 18 input parameters and 2 output parameters which will be detailed in Subsection C.\nHigh-Speed Dataset. A series of data generated during the random envelope flight simulation process of a certain combination engine above Mach 2, with a total of 50,000 experimental samples. We randomly selected 30,000 and 10,000 samples to form the training set and validation set respectively, and the remaining 10,000 samples formed the test set.\nLow-Speed Dataset. A series of simulation data was generated during the random envelope flight below Mach 2.3, with a total of 20,000 experimental samples. We randomly selected 12,000 and 4,000 samples to form the training set and validation set respectively, and the remaining 4,000 samples formed the test set. It should be noted that this dataset contains a few samples with zero-value impulse making it impossible to train the model when using \\(L_{MARE}\\). After analysis, it was found that negative thrust and zero-value impulse always occur at the same time. Therefore, we eliminated samples with zero-value impulse in the dataset and directly set it as zero when the predicted thrust is negative."}, {"title": "B. Baseline", "content": "We have totally designed four MLP models with different network structures and depths. For brevity, we only choose two most competitive models of them as the baseline model, namely MLP-Res and MLP-Mul as shown in Figure 5.\nMLP-Res is an 8-layer architecture built upon the standard fully-connected structure, with the addition of two residual modules. The residual module consists of a skip connection and a bottleneck structure. Compared to a regular MLP, this approach helps to alleviate the vanishing gradient problem, thereby improving the network's convergence speed and learning capability.\nMLP-Mul is a 6-layer MLP network incorporating an extra parallel processing branch. In the computer vision domain, leveraging parallel multi-branch structures has been demonstrated to be effective for fusing information from different perspectives [50], [51]. In this study, the multi-branch structure can extract features from different subspaces, enabling the decoupling of feature groups. This enhancement enhances the network's stability and overall performance."}, {"title": "C. Experimental Settings", "content": "We now give the detail of experimental settings such as the training design, the input and output design, and the performance metrics.\nTrainning Design. For the sake of fairness, we kept most of the experimental settings consistent across different prediction models. We used the Adam optimizer [52] with a total of 150 training epochs. The batch size is set as 100 and 40 for the High-Speed dataset(HS) and the Low-Speed dataset(LS), respectively. The initial learning rate is set as 0.01 for MLP models, and is reduced by a factor of 10 at the 80th and 120th epochs. As for PENN models, the initial learning rate is set as 0.002, and is reduced by a factor of 2 at the 60th, 80th, and 100th epochs. The discrepancy in milestone configurations for learning rate scheduling can be attributed to the inherent complexity of PENN architectures in comparison to MLP networks, necessitating an earlier reduction in the learning rate to facilitate effective training convergence. All the experiments are carried out on a PC with AMD Ryzen\u2122 9 CPUs, 64-GB RAM, and GEFORCE GTX 3090 GPUs.\nInput and Output Design. A total of 18 input parameters are recorded during the simulation of a real aeroengine, which are further divided into five groups as shown in TABLE I. The output parameters are thrust and specific impulse. For PENN, we merge high-speed and low-speed parameters as the input for HLCSN, and other component sub-networks consume their corresponding input parameters. For MLPs, we directly feed all 18 input parameters into the model.\nPerformance Metric. For regression models, the Mean Absolute Percentage Error(MAPE) is generally used as the evaluation index, which is defined as follows:\n\\(MAPE = \\frac{1}{n} \\sum_{i=1}^{n} | \\frac{A_{i} - F_{i}}{A_{i}} |  \\times 100\\%\\)   (23)\nwhere n denotes the number of samples in the test set, \\(A_{i}\\) is the ground-truth value, and \\(F_{i}\\) is the predicted value. The smaller the value of MAPE, the better the model performance, and vice versa. Besides MAPE, we also compare model parameter quantity, which can usually reflect the computational efficiency and can be approximately calculated as:\n\\(P = \\sum_{l=1}^{L-1}(n_{l} + 1)  \\cdot n_{l+1}\\)   (24)\nwhere L denotes the total number of layers in the neural network, including the input and output layers. \\(n_{l}\\) represents the number of nodes (neurons) in the l-th layer."}, {"title": "D. Comparative Analysis", "content": "We test the proposed PENN models (PENN-FCF, PENN-BNF, PENN-ABF, PENN-CAWF) and MLP models (MLP-Res, MLP-Mul) on HS and LS datasets, and the prediction results are presented in Table II. It can be seen that: (1) The errors on LS dataset are obviously larger than those on HS dataset for all prediction models. This is mainly because that the physical laws under LS dataset are more complex than those under HS dataset. (2) From the comprehensive results on HS and LS datasets, PENN-CAWF and PENN-BNF achieved similar optimal prediction performance. This indicates that the physical structure employed by PENN models are effective in extracting and aggregating effective features. (3) Considering the computational efficiencies of the models (except for PENN-ABF), PENN-BNF has the highest computational efficiency, followed by PENN-CAWF. This verifies that the physical structure employed by PENN can effectively reduce invalid network connections, thus enhancing the computational efficiency of the prediction model. (4) PENN-BNF has the best efficiency-performance trade-off among all models, showing the effectiveness of utilizing bottleneck structure in the field of engine performance prediction."}, {"title": "E. Validation of Proposed Loss Function", "content": "To verify the effectiveness of the proposed loss function for different network structures, we choose two representative prediction models, MLP-Mul and PENN-BNF, among the generalized neural network model and the physical-embedded nerual network model as the experimental models. The prediction results for the proposed loss function (\\(L_{MARE}\\)) as well as two other alternatives (\\(L_{MSE}, L_{MAE}\\)) are presented in TABLE III. It can be seen that when using \\(L_{MSE}\\), \\(L_{MAE}\\), and \\(L_{MARE}\\) respectively, the comprehensive prediction error for PENN-BNF first decreased from 2.26% to 1.67%, and finally decreased to 1.34%, which demonstrates the significant impact of the loss function on predictive performance. In addition, the comprehensive prediction error for MLP-Mul displays the similar trend, which proves that the proposed loss function is universal and can improve the prediction performance under various network architectures."}, {"title": "F. Dataset Size Dependence Experiments", "content": "In order to verify the effectiveness of the proposed physical-embedded neural network structure on different dataset sizes, we scale down the size of the HS and LS datasets by 5, 20, 200, 500 times and 5, 20, 100, 200 times respectively, and use the scaled datasets to train the models. We still choose MLP-Mul and PENN-BNF as the experimental models, and test their prediction performance under different dataset sizes. For the convenience and fairness of the experiment, we did not adjust the hyper-parameters of network training for different dataset sizes. The average prediction errors of thrust and specific impulse under different scaled datasets are recorded and shown as curves in Figure 7. We can see that the average prediction error of PENN-BNF increases gently for both scaled datasets, thus it has stable performance expectations on datasets of different scales. In addition, the average prediction errors of MLP-Mul is always higher than those of PENN-BNF, and MLP-Mul does not even converge when the sampling ratio is 100 on LS dataset. We believe that by adjusting hyperparameters as targeted as possible, MLP-Mul in this case can also converge, but PENN-BNF does not require any adjustment. This proves that prediction models with physical-embedded neural network structure is easier to train than those with general neural network structure under different dataset sizes."}, {"title": "G. Model Scalability Experiments", "content": "Due to the constraints of computing resources, we usually need prediction models with different computational complexities in practical applications. Therefore, we obtained PENN-BNF-UP2 and PENN-BNF-UP4 by synchronously increasing the number of nodes in each network layer of PENN-BNF by 2 and 4 times respectively, and obtained PENN-BNF-DOWN2 and PENN-BNF-DOWN4 by synchronously reducing the number of nodes in each network layer of PENN-BNF by 2 and 4 times respectively. The four scaled models mentioned above, together with PENN-BNF, constitute the PENN-BNF family.\nWe trained and tested each of the five models in PENN-BNF family using HS and LS datasets, and the main results are shown in Table IV. We can see that only when the model parameters is reduced from 59k (PENN-BNF) to 4k (PENN-BNF-DOWN4) does the performance deteriorate significantly. At the same time, scaling up the model can continuously improve predictive performance, but does not yield significant gains. For example, PEEN-BNF-UP4 is only 0.25% more accurate than PENN-BNF, but its model parameters is 16.7 times that of PENN-BNF and the performance gain brought by the increase in model capacity is relatively small.\nOverall, PENN-BNF strike a good balance between computational complexity and prediction accuracy, is the advocated solution. For situations where high prediction accuracy is required and computing resources are sufficient, PENN-BNF-UP4 or PENN-BNF-UP2 is a good choice. In situations where computing resources are scarce, PENN-BNF-DOWN2 is also acceptable."}, {"title": "H. Model Efficiency Testing", "content": "We now evaluate the computational efficiency of the five models in PENN-BNF family. We train and run the model on two datasets using Central Processing Units (CUP, AMD Ryzen\u2122 9 5900X), and the time costs are recorded and shown in Table V. It can be seen that the training time and inference time under CPU increase with the size of the models obviously, but the growth times are not the same. The reason may be that the CPU used in the experiment is multi-core and has a certain degree of parallel processing ability. Overall, CPU is sufficient for the models designed in this article."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "Prediction of engine performance is a crucial part for engine design, maintenance, and optimization endeavours. In this study, we design a physical-embedded neural network architecture for real-time prediction of engine performance parameters. In the proposed architecture, the component inputs and internal relationships strictly follow the real engine principles, which greatly increases the model interpretability. As the core of the architecture, four distinct feature fusion modules are deliberately designed and the corresponding prediction models are obtained. Additionally, a novel loss function called mean absolute relative error is proposed to strength the model training effect. Experimental results validate that the proposed architecture can extract and aggregate effective features and reduce invalid network connections, thus simultaneously improving model performance and efficiency. Furthermore, the novel loss function is universal and can improve the prediction performance under various network architectures. We also conduct extensive experiments on datasets scalability, model scalability and efficiency testing, providing a reference for researchers and engineers in different application scenarios.\nMoving forward, in order to further improve the model performance and computational efficiency, as well as to increase the model interpretability, we are ready to refine the input information flow and component sub-networks even more. Additionally, we plan to increase the supervision of the sub-networks, which may lead to further enhancements in the model's capabilities."}]}