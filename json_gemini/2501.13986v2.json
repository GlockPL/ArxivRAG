{"title": "An Efficient Sparse Kernel Generator for O(3)-Equivariant Deep Networks", "authors": ["Vivek Bharadwaj", "Austin Glover", "Ayd\u0131n Bulu\u00e7", "James Demmel"], "abstract": "Rotation equivariant graph neural networks, i.e. networks\ndesigned to guarantee certain geometric relations between\ntheir inputs and outputs, yield state of the art performance\non spatial deep learning tasks. They exhibit high data ef-\nficiency during training and significantly reduced inference\ntime for interatomic potential calculations compared to clas-\nsical approaches. Key to these models is the Clebsch-Gordon\n(CG) tensor product, a kernel that contracts two dense fea-\nture vectors with a highly-structured sparse tensor to pro-\nduce a dense output vector. The operation, which may be\nrepeated millions of times for typical equivariant models, is a\ncostly and inefficient bottleneck. We introduce a GPU sparse\nkernel generator for the CG tensor product that provides\nsignificant speedups over the best existing open and closed-\nsource implementations. Our implementation achieves high\nperformance by carefully managing the limited GPU shared\nmemory through static analysis at model compile-time, min-\nimizing reads and writes to global memory. We break the\ntensor product into a series of smaller kernels with operands\nthat fit entirely into registers, enabling us to emit long arith-\nmetic instruction streams that maximize instruction-level\nparallelism. By fusing the CG tensor product with a subse-\nquent graph convolution, we reduce both intermediate stor-\nage and global memory traffic over na\u00efve approaches that\nduplicate input data. We also provide optimized kernels for\nthe gradient of the CG tensor product and a novel identity\nfor the higher partial derivatives required to predict inter-\natomic forces. Our fused kernels offer up to 4.8x speedup\nfor the forward pass and 3x for the backward pass over\nNVIDIA's closed-source cuEquivariance package, as well as\n> 10x speedup over the widely-used e3nn package. We offer\nup to 5.4x inference-time speedup for the MACE chemistry\nfoundation model over the original unoptimized version.", "sections": [{"title": "1 Introduction", "content": "Equivariant deep neural network models have become\nimmensely popular in computational chemistry over the\npast seven years [32, 34, 19]. Consider a function\n$f: R^n \u2192 R^m$. Informally, $f$ is invariant if a class\nof transformations applied to its argument results in no\nchange to the function output. A function is equivariant\nif a transformation applied to any input argument of $f$\ncan be replaced by a compatible transformation on the\noutput of $f$. For example: a function predicting molec-\nular energy based on atomic positions should not change\nits result if the atom coordinates are rotated, translated,"}, {"title": "2 Preliminaries and Problem Description", "content": "We denote vectors, matrices, and three-dimensional\ntensors in bold lowercase characters, bold uppercase\ncharacters, and script characters (e.g. P), respectively.\nOur notation and description of equivariance follow\nThomas et al. [32] and Lim and Nelson [24]. Let\n$G$ be an abstract group of transformations, and let\n$D_{in}: G \u2192 R^{nxn}, D_{out}: G \u2192 R^{mxm}$ be a pair of\nrepresentations, group homomorphisms satisfying\n$D_{in}(g_1.g_2) = D_{in}(g_1) \u00b7 D_{in}(g_2)   g_1, g_2 \u2208G,$\nand likewise for $D_{out}$. A function $f: R^n \u2192 R^m$ is\nequivariant with respect to $D_{in}$ and $D_{out}$ iff\n$f(D_{in}(g) \u00b7 v) = D_{out}(g) \u00b7 f(v)   v\u2208 R, g\u2208 G.$\nA function is invariant if the equivariance property holds\nwith $D_{out}(g) = I_{m\u00d7m}$ for all $g \u2208 G$.\nIn our case, $f$ is a neural network composed of a se-\nquence of layers, expressed as the function composition\n$f(v) = \u03a6_n \u00b0 ... \u03bf \u03a6_1(\u03c5).$\nHere, $D_{in}$ and $D_{out}$ are derived from the dataset, and\nthe task is to fit $f$ to a set of datapoints while maintain-\ning equivariance to the chosen representations. Network\ndesigners accomplish this by imposing equivariance on\neach layer and exploiting a composition property [32]:"}, {"title": "2.1 Representations of O(3)", "content": "In this paper, we let\n$G = O(3)$, the group of three-dimensional rotations in-\ncluding reflection. A key property of real representa-\ntions of $O(3)$ is our ability to block-diagonalize them\ninto a canonical form [25]. Formally, for any represen-\ntation $D: O(3) \u2192 R^{nxn}$ and all $g\u2208 G$, there exists a\nsimilarity matrix $P$ and indices $l_1,..., l_p$ satisfying\n$D(g) = P\n\\begin{bmatrix}\nD^{(l_1)}(g) & & 0\\\\\n& ... & \\\\\n0 & & D^{(l_p)}(g)\n\\end{bmatrix} P^{-1}$\nwhere $D^{(0)}(g), D^{(1)}(g),...$ are a family of elemen-\ntary, irreducible representations known as the Wigner\nD-matrices. For all $i > 0$, we have $D^{(i)}(g) \u2208\nR^{(2i+1)\u00d7(2i+1)}$. In the models we consider, all repre-\nsentations will be exactly block diagonal (i.e. $P$ is the\nidentity matrix), described by strings of the form\n$D(g) = \\text{\"}3x1e + 1x2o\\text{\"}."}, {"title": "2.2 Core Computational Challenge", "content": "Let $x \u2208\nR^n, y \u2208 R^m$ be two vectors from some intermediate layer\nof an equivariant deep neural network. For example,\nx could be the embedding associated with a node of a\ngraph and y a feature vector for an edge (see Figure 4,\nbottom right). We can view both vectors as functions\n$x(v), y(v)$ of the network input v, which are equivari-\nant to $(D_{in}, D_x)$ and $(D_{in}, D_y)$ respectively. An equiv-\nariant graph convolution layer interacts x and y to\nproduce a new vector z. To ensure layer-equivariance of\n\u03c6, $z(v)$ must be equivariant to $(D_{in}, D_z)$, where $D_z$ is\na new representation selected by the network designer.\nThe Kronecker product provides an expressive, gen-\neral method to interact x and y: if $x(v)$ and $y(v)$\nare equivariant to the representations listed above, then\n$z(v) = x(v) \u2297 y(v)$ is equivariant to $(D_{in}, D_x \u2297 D_y)$.\nUnfortunately, $x \u2297 y \u2208 R^{nm}$ may have intractable\nlength, and we cannot drop arbitrary elements of the\nKronecker product without compromising the equivari-\nance property.\nLet $P\u2208 R^{nm\u00d7nm}$ be the similarity transform\ndiagonalizing $D_x \u2297 D_y$. To reduce the dimension of the\nKronecker product, we first form $P(x(v)\u2297y(v))$, which\nis an equivariant function with output representation\n$P(D_x \u2297 D_y)P^{\u22121}$. We can now safely remove segments\nof $P(x\u2297y)$ corresponding to unneeded higher-order\nWigner blocks and recombine its components through a\ntrainable, structured weight matrix. The result, $z(v) =\nWP(x(v)\u2297y(v))$, has a new output representation $D_z$\nand can be much shorter than $x \u2297 y$.\nWhen both $D_x$ and $D_y$ are representations in\nblock-diagonal canonical form, the transform $P$ is\na highly structured block-sparse matrix containing\nnonzero Clebsch-Gordon coefficients. After potentially\nreducing $P$ to $k$ rows (removing segments correspond-\ning to unneeded Wigner D-blocks), we can reshape it\ninto a block-sparse tensor $P \u2208 R^{m\u00d7n\u00d7k}$ contracted\non two sides with x and y. We call this operation\n(along with multiplication by a structured weight ma-\ntrix $W \u2208 R^{kxk'}$) the CG tensor product, illustrated\nin Figure 1. It can be expressed by a matrix equation,\na summation expression, multilinear tensor contraction\n(popular in the numerical linear algebra community), or\nEinstein notation:\n$\\begin{aligned} &z = TP(P, x, y, W) \\\\ &:= W \u00b7 P \u00b7 (x\u2297y) \\\\ &:= W \\sum_{i=1}^{m,n}x[i]y [j] P [i,j, :]\\\\ &:= P \u00d7_1 X \u00d7_2 Y \u00d7_3 W \\\\ &:= \\text{einsum(``ijk,i,j,kk' \u2192 k'\",P,x,y, W)}.\\end{aligned}$\nOur goal is to accelerate computation of TP(P, x, y, W)\nfor a variety of CG coefficient tensors P. Given\""}, {"title": "3 Engineering Efficient CG Kernels", "content": "Our task is to generate an efficient CG tensor product\nkernel given a problem specification outlined in Section\n2.4. Algorithm 1 describes the logic of a kernel that\noperates on a large batch of inputs, each with a distinct\nset of weights (see Figure 4B). We assign each (x, y, W)\ninput triple to a single GPU warp, a choice which\nhas two consequences. First, it enables each warp to\nexecute contiguous global memory reads / writes for\nx, y, W and z. Second, it allows warps to execute in a\ncompletely asynchronous fashion without any CTA-level\nsynchronization, boosting throughput significantly. The\nweights Ware stored in a compressed form without the\nzero entries illustrated in Figure 1.\nAfter the division of labor, each warp follows a\nstandard GPU kernel workflow. The three inputs are\nstaged in shared memory, the kernels in Equation (2.2)\nare executed sequentially, and each output zo is stored\nback. Each warp operates on a unique partition of the\nCTA shared memory which may not be large enough\nto contain the the inputs and outputs. In the latter\ncase, chunks of x, y, W, and z are staged, and the\ncomputation executes in phases according to a schedule\ndescribed in Section 3.1."}, {"title": "3.1 Computation Scheduling", "content": "A key obstacle to\nefficient kernel implementation is the long length of\nthe x, y, and z feature vectors that must be cached\nin shared memory. The sum of their vector lengths\nfor large MACE [3] and Nequip [4] configurations can\nexceed 10,000 data words. Given that the warps in\neach CTA partition the shared memory, staging all\nthree vectors at once (along with the weights in W)\nis infeasible.\nTo manage our limited shared memory, we execute\nthe computation in phases that are scheduled at model\ncompile-time. We break the list of instructions in\nEquation (2.2) into phases so that the sum of chunks\nfrom x, y, W and z required for the phase fits in each\nwarp's shared memory allotment. We then schedule\nloads and stores, hardcoding the relevant instructions\ninto each kernel using our JIT capability. When more\nthan a single computation phase is required, our goal\nis to generate a schedule that minimizes global memory\nreads and writes. We use a few simple heuristics:\n1. If x and y can fit into a warp's shared memory\npartition (but not z and W), then segments of z\nand W are streamed in through multiple phases of\ncomputation. In each phase, the kernels that touch\neach segment of z are executed.\n2. Otherwise, we use a greedy algorithm. In each\nphase, the shared memory pool is filled with as\nmany segments of x, y, W and z that can fit.\nBetween phases, segments are flushed and reloaded\nas needed.\nCase 1 covers most problem configurations in equiv-ariant graph neural networks and minimizes global\nmemory writes, while Case 2 enables reasonable perfor-\nmance even with constrained shared memory resources.\nLarge CG tensors (e.g. Nequip-benzene in Figure 5)\nmay require 20-40 phases of computation per tensor\nproduct, and our scheduling substantially reduces global\nmemory transactions."}, {"title": "3.2 JIT Subkernel Implementations", "content": "In prepara-\ntion to execute a subkernel, suppose we have loaded x,\ny and W into shared memory and reshaped subranges\nof all three to form $X_{kern}, y_{kern}$, and $W_{kern}$. For the rest\nof Section 3.2 and all of Section 3.3, we omit the \\text{\"}kern\\text{\"}subscripts. Algorithm 2 gives the pseudocode to execute\neither kernel B or C from Figure 3 using these staged\noperands. Each thread stages a unique row row of X"}, {"title": "3.3 Backward Pass", "content": "Like other kernels in physics in-\nformed deep learning models [15], the gradients of the\nCG tensor product are required during model infer-\nence as well as training for interatomic force prediction.\nSuppose $E(R, W)$ is the scalar energy prediction emit-\nted by our equivariant model for a configuration of s\natoms, where W contains trainable model weights and\neach row of $R\u2208 R^{s\u00d73}$ is an atom coordinate. Then\n$F_{pr} = -\\frac{\u2202E}{\u2202R} \u2208 R^{s\u00d73}$ is the predicted force on each\natom. Conveniently, we can compute these forces by\nauto-differentiating $E(R, W)$ in a framework like Py-\nTorch or JAX, but we require a kernel to compute the\ngradient of the CG tensor product inputs given the gra-\ndient of its output.\nTo implement the backward pass, suppose $z =\nTP(P, x, y, W)$ and we have $g_z = \\frac{\u2202E}{\u2202z}$. Because the\nCG tensor product is linear in its inputs, the product\nrule gives\n$\\begin{aligned}&\\frac{\u2202E}{\u2202x[i]} = \\sum_{(i,j,k) \u2208 nz(P)}P [ijk] \u00b7y [j] \u00b7 (W^Tg_z) [k] \\\\ &\\frac{\u2202E}{\u2202y[j]} = \\sum_{(i,j,k) \u2208 nz(P)}P [ijk] \u00b7x[i]. (W^Tg_z) [k] \\\\ &\\frac{\u2202E}{\u2202W [kk']} = g_z [k]. \\sum_{(i,j,k) \u2208 nz(P)}P [ijk] x [i] y [j]\\end{aligned}$\nNotice the similarity between the three equations above\nand Equation (2.1): all require summation over the"}, {"title": "3.4 Higher Partial Derivatives", "content": "Our analysis so far\nhas focused on the forward and backward passes for the\nCG tensor product. For interatomic potential models,\nwe require higher-order derivatives to optimize force\npredictions during training [15], as we explain below.\nRather than write new kernels for these derivatives,\nwe provide a novel (to the best of our knowledge)\ncalculation that implements them using the existing\nforward and backward pass kernels.\nAs in Section 3.3, let $F_{pr} = -\\frac{\u2202E}{\u2202R} \u2208 R^{s\u00d73}$ be the\npredicted atomic forces generated by our model. During\ntraining, we must minimize a loss function of the form\n$\\min_W L(R, W) = \\min_W || F_{pr}(R, W) - F_{gt}(R)||^2$\nwhere $F_{gt}(R) \u2208 R^{s\u00d73}$ is a set of ground-truth forces cre-\nated from a more expensive simulation. The loss func-\ntion may include other terms, but only the Frobenius"}, {"title": "3.5 Graph Convolution and Kernel Fusion", "content": "Fig-\nure 4 illustrates two typical use cases of the CG ten-\nsor product kernel. The first case (4A) calls the ker-\nnel illustrated in Figure 1 several times with unique\ntriples of (x, y, W) inputs, and we have already ad-\ndressed its implementation. The second case (4B) em-\nbeds the CG tensor product into a graph convolution\noperation [32, 4, 2]. Here, the nodes of a graph typ-\nically correspond to atoms in a simulation and edges\nrepresent pairwise interactions. For a graph $G = (V, E)$,\nlet $x_1...x_{|V|}, y_1\u2026y_{|E|}$, and $W_1...W_{|E|}$ be node embed-\ndings, edge embeddings, and trainable edge weights, re-\nspectively. Then each row $z_j$ of the graph convolution\noutput, $j \u2208 [|V|]$, is given by\n$z_j = \\sum_{(j,k,e)\u2208N(j)}TP(P, x_k, y_e, W_e),$\nwhere $N(j)$ denotes the neighbor set of node j and\n$(j,k,e) \u2208 N(j)$ indicates that edge e connects nodes\njand k. Current equivariant message passing networks\n[4, 2] implement Equation (3.6) by duplicating the node\nfeatures to form $x_1,..., x_{|E|}$, calling the large batch ker-\nnel developed earlier, and then executing a scatter-sum\n(also called reduce-by-key) to perform aggregation. Un-\nfortunately, duplicating the node features incurs signif-\nicant memory and communication-bandwidth overhead\nwhen |E| > |V| (see Table 3).\nNotice that graph convolution exhibits a memory\naccess pattern similar to sparse-dense matrix multipli-\ncation (SpMM) [37]. We provide two procedures for\nthe fused CGTP / graph convolution based on classic\nSpMM methods. The first, detailed in Algorithm 4, re-quires row-major sorted edge indices and iterates over\nthe phases of the computation schedule as the outer\nloop. The latter change enables the algorithm to keep\na running buffer zacc that accumulates the summation\nin Equation (3.6) for each node. The buffer zacc is only"}, {"title": "3.6 Analysis and Related Work Comparison", "content": "Our JIT-based approach embeds the sparse tensor\nstructure and values into the GPU instruction stream,\nwhich is only possible because P has relatively few\nnonzero entries in each block. Because GPU instruc-\ntions must also be fetched from global memory, per-\nformance eventually degrades as blocks exceed several\nthousand nonzero entries. We do not encounter such"}, {"title": "4 Experiments", "content": "Our kernel generator is available online\u00b9. We adopted\nthe frontend interface of e3nn [8, 9] and used QuTiP [13,\n20] to generate CG coefficients; we thank the authors of\nboth packages. We tested correctness against e3nn to\nensure that our kernels produce identical results, up to\nfloating point roundoff and a well-defined reordering of\nthe weights W on certain input configurations.\nExperiments were conducted on NVIDIA A100\nGPU nodes of NERSC Perlmutter (each equipped with\nan AMD EPYC 7763 CPU). Table 1 lists the advertised\nmaximum memory bandwidth and compute peaks for\nmultiple datatypes, a yardstick for our results. As base-\nlines, we used the PyTorch versions of e3nn (v0.5.4) [9]\nand NVIDIA cuEquivariance (v0.0.2) [10]. The e3nn\nimplementation was accelerated with torch.compile\nexcept where prohibited by memory constraints. For\nFigures 5 and 6, we benchmarked all functions through\na uniform PyTorch interface and included any overhead\nin the measured runtime. Figures 7, Table 2, and Figure\n9 (right) rely on kernel runtime measurements without\nPyTorch overhead."}, {"title": "4.1 Throughput Comparison", "content": "We first profiled our\nkernels on a large collection of model configurations used\nby Nequip [4] and MACE [2]. For each model, we se-lected the most expensive tensor product to benchmark."}, {"title": "4.2 Roofline Analysis", "content": "We conducted a roofline\nanalysis [35] by profiling our forward / backward pass\nimplementations on varied input configurations. We\nprofiled tensor products with a single \\text{\"}B\\text{\"} subkernel (see\nFigure 3) with FP32 precision, core building blocks for\nmodels like Nequip and MACE. The arithmetic inten-\nsity of the CG tensor product depends on the structure\nof the sparse tensor, and we profiled configurations with\nprogressively increasing arithmetic intensity.\nFigure 7 and Table 2 show our profiling results,\nwhich indicate consistently high compute and band-\nwidth utilization for our kernels. In the bandwidth-limited portion of the diagram, our implementation\nincreases its throughput linearly to match increasing\narithmetic intensity in the input configurations. By con-trast, the throughput of cuEquivariance's forward pass\nstagnates. Our kernel performance saturates at 58%\nof the FP32 peak, possibly because Algorithms 2 and\n3 contain contain a significant fraction of non fused-multiply-add (FMA) instructions."}, {"title": "4.3 Kernel Fusion Benchmarks", "content": "We conducted\nour kernel fusion experiments on three molecular struc-\nture graphs listed in Table 3. We downloaded the atomic\nstructures of human dihydrofolate deductase (DHFR)\nand the SARS-COV-2 glycoprotein spike from the Pro-\ntein Data Bank and constructed a radius-neighbors\ngraph for each using Scikit-Learn [27]. The carbon lat-tice was provided to us as a representative workload for\nMACE [3].\nFigure 8 shows the speedup of our fused imple-\nmentations benchmarked on the most expensive ten-\nsor product in the MACE-large model. Our baseline,"}, {"title": "4.4 Acceleration of MACE", "content": "The MACE founda-\ntion model [3], trained on data from the Materials\nProject [12], implements the equivariant graph neural\nnetwork architecture depicted in Figure 4B. We patched\nMACE to sort nonzeros of its atomic adjacency ma-trix according to compressed sparse row (CSR) order,\nas well as compute the transpose permutation. We then\nsubstituted Algorithm 4 in place of the existing graph\nconvolution to measure performance relative to other\nkernel providers. Our benchmark was conducted on the\ncarbon lattice listed in Table 3.\nFigure 9 (left) compares the rate of molecular dy-\nnamics simulation among the different kernel providers.\nWe benchmarked cuE with the optimal data layout\nfor its irreps and included optimizations for symmet-ric tensor contraction, linear combination layers, and\ngraph convolution. In FP32 precision, we provide 5.3x\nspeeedup over e3nn and 1.5x over cuEquivariance. In\nFP64 precision, we offer 6.6x speedup over e3nn and\n1.43x over cuEquivariance. Because we do not materi-alize the intermediate tensor product outputs for each\nedge, our fused kernels reduce the model memory foot-print by roughly 158x.\nFigure 9 (right) breaks down the device runtime\nspent in various kernels traced by the PyTorch profiler.\nThe sum of kernel runtimes in the right figure differs\nfrom the times measured in the left figure due to device\nidle time. Notice that our speedup comes entirely\nfrom accelerating the equivariant graph convolution; the\nCTP kernel and scatter-sum runtime is 4x faster in our\nimplementation compared to cuE in FP32 precision.\nBy contrast, cuEquivariance also accelerates a\n\\text{\"}symmetric contraction\\text{\"} kernel specific to MACE (re-ducing the runtime counted in \\text{\"}Other\\text{\"} category), and\nwe leave these kernels to future work. Our run-time in the \\text{\"}Other\\text{\"} category can be reduced through\ntorch.compile(), but we did not include this optimiza-tion in fairness to cuE (errors in compilation) and e3nn\n(memory constraints). We plan to update these bench-marks when further support is added in cuE."}, {"title": "5 Conclusions and Further Work", "content": "We have established that our sparse kernels achieve\nconsistently high performance on the most common\nprimitives used in equivariant deep neural networks.\nOur speedup over e3nn and cuEquivariance prove that\na sparsity-aware, JIT-compiled kernel can outperform\nstrategies based entirely on dense linear algebra. We\nsee several avenues for future progress:\n\u2022 Low Precision Data and MMA Support:\nOur kernels rely on the single-instruction multi-ple thread (SIMT) cores for FP32 and FP64 float-ing point arithmetic. Modern GPUs offer spe-cialized hardware for lower-precision calculation,both using SIMT cores and within matrix-multiply-accumulate (MMA) units. We plan to harness these\ncapabilities in the future.\n\u2022 Accelerator and language support: In contrast\nto cuEquivariance, all of our code is open-source.\nWe hope to cross compile our kernels to HIP for\nAMD GPUs, as well as offer bindings for JAX and\nthe Julia scientific computing language.\n\u2022 Integration into new models: Our software\nremains accessible to newcomers while delivering\nthe high performance required for massive inference\nworkloads. In conjunction with domain experts,\nwe hope to apply our library to train larger, more\nexpressive equivariant deep neural networks."}]}