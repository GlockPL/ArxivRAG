{"title": "LVS-Net: A Lightweight Vessels Segmentation Network for Retinal Image Analysis", "authors": ["Mehwish Mehmood", "Shahzaib Iqbal", "Tariq Mahmood Khan", "Ivor Spence", "Muhammad Fahim"], "abstract": "Abstract-The analysis of retinal images for the diagnosis of various diseases is one of the emerging areas of research. Recently, the research direction has been inclined towards inves- tigating several changes in retinal blood vessels in subjects with many neurological disorders, including dementia. This research focuses on detecting diseases early by improving the performance of models for segmentation of retinal vessels with fewer parame- ters, which reduces computational costs and supports faster pro- cessing. This paper presents a novel lightweight encoder-decoder model that segments retinal vessels to improve the efficiency of disease detection. It incorporates multi-scale convolutional blocks in the encoder to accurately identify vessels of various sizes and thicknesses. The bottleneck of the model integrates the Focal Modulation Attention and Spatial Feature Refinement Blocks to refine and enhance essential features for efficient segmentation. The decoder upsamples features and integrates them with the corresponding feature in the encoder using skip connections and the spatial feature refinement block at every upsampling stage to enhance feature representation at various scales. The estimated computation complexity of our proposed model is around 29.60 GFLOP with 0.71 million parameters and 2.74 MB of memory size, and it is evaluated using public datasets, that is, DRIVE, CHASE_DB, and STARE. It outperforms existing models with dice scores of 86.44%, 84.22%, and 87.88%, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "The retina, a morphological and functional extension of the brain, is part of the neurological system; some of its cells directly connect to the brain [1], [2], [3], [4], [5]. Retinal blood vessels are the only vascular network in the human body that is directly and non-invasively observable [6], [7], [8], [9], [10]. Retinal vascular structural abnormalities are significant for their clinical implications in identifying systemic and neurological disorders, including dementia, such as Alzheimer's disease [11], [12], [13], [14], [15]. This has led to a growing interest in investigating how early Alzheimer's disease causes retinal changes, as retinal vessel health may re- flect the cerebral vasculature's condition due to the homology between the retina and the cerebral microvasculature [16]. An estimated 16 million people worldwide are affected by retinal vascular conditions, with Alzheimer's being the second most common retinal vascular condition after diabetic retinopathy [17], [18], [19], [20], [21].\nMedical image segmentation plays a crucial role in health- care by enabling precise delineation of anatomical structures and pathological regions, which aids in disease diagnosis, treatment planning, and monitoring [22], [23], [24], [25], [26], [27], [28]. Fundus imaging offers a non-invasive and efficient means of studying the cerebral microvasculature and its relationship to dementia. Retinal images are easy to acquire and growing evidence suggests that this microvascular network may represent the cerebral microvasculature [29], [30], [31]. Retinal image analysis has been dramatically improved by the development of image processing and machine learning technologies, which have improved disease diagnosis and monitoring [32], [33], [34], [35], [36], [37], [38]. The retinal vascular segmentation of these images is essential for mon- itoring anatomical changes and possibly diagnosing diseases like Alzheimer's. Retinal image analysis is tedious as their color or gray level varies from one part to another due to the morphology of the retinal structure and different features, resulting in inaccurate output.\nRetinal vascular segmentation remains challenging despite advancements, particularly in terms of vessel distinction [39], [40], [41], [42], [43], [44], [45]. This research introduces LVS- Net, a novel lightweight encoder-decoder model that improves the effectiveness of disease screening by segmenting retinal vessels. Our proposed lightweight model integrates the Focal Modulation Attention Module (FMAM) [46] and the spatial feature refinement block (SFRB) [47] into the bottleneck to re- fine and improve essential features for efficient segmentation. The decoder upsamples features and integrates them with the corresponding feature in the encoder using skip connections and SFRB to enhance feature representation at various scales and to fine-tune the segmented image's overall representation. The suggested model is evaluated on public datasets, that is, DRIVE [48], CHASE_DB [49], and STARE [50]. The results show a notable improvement in retinal vascular segmentation over existing accuracy, efficiency, and complexity methods. The primary contributions of this paper include:\n\u2022 Introducing LAV-Net, a novel lightweight encoder- decoder model specifically designed for segmentation of retinal vessels. LAV-Net is highly effective and performs exceptionally well with a relatively small number of learnable parameters (only 0.71M).\n\u2022 Implementing multi-class segmentation to identify arter- ies and veins from retinal images, facilitating the model to retrieve multiple features simultaneously while improving efficiency.\n\u2022 Integrating focal modulation attention and spatial feature refinement blocks at the encoder-decoder bottleneck to"}, {"title": "II. RELATED WORK", "content": "In the past few years, deep learning-based techniques are common for segmenting retinal blood vessels and have out- standing accuracy that suppressed traditional segmentation methods. The recent advancement in this area is discussed below."}, {"title": "A. Retinal Vessel Segmentation", "content": "Chowdhury et al. [51] developed MSGANet-RAV, a U- shaped encoder-decoder network that segments and classifies retinal vessels. Although it performed well, particularly in handling vessel crossings, it still demonstrated limitations in accurately segmenting small vessels and other detailed structures. Hemelings et al. [52] utilized a U-Net architecture, achieving high accuracy, but leaving room for improvement in the discrimination of small vessels. Lyu et al.[53] presented a convolutional neural network (CNN) model comparable to U-Net for the segmentation of binary vessels and the assessment of fractal dimensions. Xu et al. [54] proposed a dual channel asymmetric CNN based on the U-Net model based on pre-processing scale and orientation features. The combination of the segmentation results from both channels resulted in comprehensive and complementary information. However, the effectiveness of these techniques is limited by the pathological variability present in clinical images and the various scales of vascular geometry. Additionally, these variations may incorporate supplementary structural elements, leading to a higher number of network learnable parameters and, as a result, increased GPU memory consumption.\nUnlike CNN-based architectures, transformers have recently been adopted in many computer vision tasks. Vision transform- ers (ViTs) [55] have received a substantial research interest, and various subsequent approaches have been presented that expand on ViTs. They adjusted the architecture by cascading numerous transformer layers instead of CNN-based architec- tures. Due to the powerful representation learned from pre- trained backbones, numerous algorithms for semantic segmen- tation that incorporate ViT backbones demonstrate impressive results [56], [57], [58], [59]. Although ViTs yield robust outcomes, they are computationally expensive.\nUsing lightweight CNNs is one potential way to over- come these problems. There are several advantages to using lightweight techniques in medical imaging, including faster processing speeds, lower memory requirements, better porta- bility, lower computational costs, and lower power usage. These advantages make them an attractive option for several applications, which has recently attracted the interest of most academics. Wentao et al. [60] introduce FR-UNet, a novel segmentation technique designed to improve segmentation accuracy and vessel connectivity. A dual-threshold iterative approach is used to capture weak vessel pixels, and the network retains full image resolution while enhancing feature extraction through a multiresolution convolution mechanism. FR-UNet tends to create false positives during the segmenta- tion of thin vessels. The Dense-Inception U-Net was proposed by Zhang et al.[61], and utilizes a tiny encoder along with a lightweight backbone and dense module to capture high-level semantic information.\nSeveral researchers have developed lightweight networks specifically for the segmentation of medical images. However, achieving minimal model complexity and rapid inference while maintaining outstanding performance remains challeng- ing in medical imaging. NnU-Net [62] preprocesses the data and postprocesses the segmentation outcomes to boost net- work flexibility; however, this method increases the model parameters. The lightweight V-Net [63] allows for efficient segmentation and uses a lower number of parameters utilizing point-wise and depth-wise convolution; nevertheless, it fails to accelerate the inference process of the model. Further- more, using multimodal magnetic resonance imaging (MRI), Tarasiewicz et al. [64] trained multiple tiny networks across all image channels to create lightweight U-Nets to precisely identify brain tumors. By replacing all convolutional layers in conventional U-Net with pyramidal convolution, PyConvU- Net [65] increases segmentation accuracy and requires a smaller number of parameters. Nevertheless, PyConvU-Net's inference time remains inadequate. CNN architectures that are lightweight and effective for segmenting retinal blood vessels are G-Net Light [43], PLVS-Net [44], TBConvL- Net[25], Lmbf-net [22] and MKIS-Net [45]. There are two significant drawbacks to existing lightweight methods. Firstly, they do not match the state-of-the-art methods in terms of performance, and secondly, they cannot generalize. Existing methods, including lightweight methods, unable to detect multi-retinal features with state-of-the-art results [66]."}, {"title": "B. Retinal Arteries Veins Segmentation", "content": "Morano et al. [67] presented a novel approach that uses fully convolutional neural networks (FCNN) and a unique loss function of \u2018Binary cross entropy by 3' (BCE3) for simultaneous segmentation and classification of retinal arteries and veins. This approach performs exceptionally well in handling vessel crossings, but still has room for improvement. Shi et al.[68] present a novel one-shot method to segment the retinal arteries and veins that uses fundus fluorescein angiography (FFA) and color fundus photography (CFP) with cross-modal pre-training. This method trained a GAN to produce soft segmentation AV using CFP inputs. There is still a need for improvement, as the approach indicated a possible loss of small vessel information, even with its capacity to manage insufficient data. LUNet, a unique deep learning architecture for segmentation of the arteries and veins in high-resolution fundus images, was introduced by J. Fhima et al. [69]. In order to improve the receptive field, the model has a particular double-dilated convolutional block. LUNet relies on high-quality images, which can affect segmentation accuracy in cases where image quality is poor. Despite good performance on high-resolution datasets, challenges in processing large-scale datasets and small vessels persisted.\nDanli et al. presented the Retina-based microvascular health assessment system (RMHAS) using a multibranch U-Net to segment the optic disc, veins, and arteries. Real- time application may be limited by the multi-step RMHAS process, which includes image quality evaluation and multi- branch segmentation. This procedure may also increase computational complexity and processing time. Jingfei Hu et al. [70] presented a novel multi-scale interactive network with an artery-venous (A/V) discriminator for classifying retinal arteries and veins. Using a special A/V discriminator, the network integrates multi-scale data and tackles common problems such as arteriovenous confusion and vascular discontinuity. Real-time implementation may be constrained by the method's complexity in integrating multi-scale data and implementing an A/V discriminator, which could raise computing costs. Wenao Ma et al. [71] presented a network in which the input module integrates domain knowledge from widely utilized vessel enhancement and retinal preprocessing methods. Specifically created for the network output block, a spatial activation mechanism uses vessel segmentation to improve A/V classification performance. Deep supervision is also incorporated into the network to help lower layers extract valuable data. Pre-processing is a significant method component which could limit generalization to different datasets and increase model complexity.\nSegmentation of retinal vessels has advanced significantly. However, there are still many challenges with the existing models. Real-time implementation is challenging due to the high memory requirements and the longer inference time of complex models, which are computationally intensive [72]. Although lightweight models have benefits such as reduced memory utilization and faster processing, they typically fail to incorporate complex processes like attention modules. This leads to a substantial gap in the development of lightweight models that incorporate attention processes for improved seg- mentation accuracy, particularly in the segmentation of retinal vessels [73]."}, {"title": "III. METHODOLOGY", "content": "Our lightweight model is designed for the segmentation of retinal vessels with an encoder-decoder architecture. The images in the datasets used for model evaluation are limited in number. To address this issue, data augmentation is employed, which consists of rotating the images by 20 degrees and adjusting the contrast of the rotated images. The architecture of our proposed model is discussed in the following section."}, {"title": "A. Model Architecture", "content": "We present a lightweight encoder-decoder architecture, LVS-Net, to extract the retinal vessels from fundus images. The overall network architecture is depicted in Fig. 1, which introduces multi-scale analysis and feature refinement inside the model. To enhance channel mapping and fine-tune the details, this network's bottleneck layer uses focal modulation along with spatial feature enhancement. The proposed LVS- Net comprises skip connections to preserve the original in- formation from the encoder and use it during the decoding process. The output of the 1st skip connection (S\u2081) is obtained by applying an activation function and a convolution operation on the RGB input image with 512 \u00d7 512 resolution, as given in Eq. 1.\n$S\u2081 = ReLU (C_{1\u00d71,24} (Img_{512x512}))$\nHere, the operation $C'_{1\u00d71,24}$ denotes a convolution with 1 \u00d7 1 kernel size and 24 output channels. ReLU represents the activated Rectified Linear Unit (ReLU). The 1st convolution block employs an activation function after a 3 \u00d7 3 convolution operation on $Img_{512\u00d7512}$. The resulting output is concatenated with (S\u2081), as shown in Eq. 2.\n$F\u2081 = ReLU(C_{3\u00d73,24} (Img_{512\u00d7512})) \u2295 S\u2081$\nWhere \u2295 is the concatenation operation. The 2nd skip con- nection employs a batch normalization on F\u2081 followed by a max-pooling layer to reduce the spatial dimensions, as shown in Eq. 3.\n$S\u2082 = MaxPool_{2\u00d72} (BN (F\u2081))$\nWhere BN is the batch normalization. The 2nd convolution block and the 3rd skip connection are mathematically repre- sented in Eq. 4 and Eq. 5, respectively.\n$F\u2082 = Re(C_{1\u00d71,48} (S\u2082)) + ReLU (C_{3\u00d73,48}(S\u2082))$\n$S\u2083 = MaxPool_{2\u00d72} (BN (F\u2082))$\nFinally, the 3rd convolution block and 4th skip connection are presented in Fig. 6 and Eq. 7, respectively.\n$F\u2083 = ReLU (C_{1\u00d71,96}(S\u2083)) \u2295 ReLU (C_{3\u00d73,96} (S\u2083))$\n$S\u2084 = Dr(0.5) (MaxPool_{2\u00d72} (BN (F\u2083)))$\nWhere $D_{r(0.5)}$ is the dropout operation with a probability of 0.5. The proposed LVS-Net applies the FMAM to refine the encoded features and improve the channel mapping after using max-pooling layers. Once the encoder features are refined by FMAM, they are processed through SFRB and then concatenated with the actual information using skip connection to compute the output of the decoder block 1st as shown in Eq. 8.\n$D\u2081 = G (F(S\u2084)) \u2295 S\u2084$"}, {"title": "B. Focal Modulation Attention Module", "content": "To further improve the obtained feature information, FMAM has been utilized between the encoder and the decoder. It is made up of three different elements, as Fig. 2(a) illustrates. It initially employs input from the encoder to encode the visual details at short and long ranges using a sequence of depth-wise convolutional layers. Each layer in the stack extracts various levels of information, including local and global details.\n$z^{(1)} = (DWConv(z^{(l)-1}))$\nWhere $z^{(1)}$ is the output feature map at level (l) and DWConv is depth-wise convolution at level (l).\nThe global context is obtained as:\n$z^{L+1} = AvgPool(z)$\nIt refines input features through hierarchical context aggre- gation and modulation. The aggregated context features across all levels are combined as:\n$Z_{out} = \u2211^{(L+1)}_{(l)=1} G^{(1)} z^{(1)}$\nwhere, $G^{(1)}$ is gating weights at level (l), and \u2297 is element- wise multiplication.\nUsing the context as guidelines, these extracted data are utilized later to collect context attributes for each query token in a selective manner. The significance of each contextual data in the final representation of the query token is determined by a gate with learnable attention weight. The modulated output for each query token $F_i$ is computed as:\n$F_i = q(S\u2084)h(Z_{out})$,\nwhere i represents the spatial index in the feature map, q(S\u2084) is query projection function and h($Z_{out}$) is modulator pro- jection function. An element-wise multiplication is performed to fuse these combined context characteristics into the query token. A learnable weight matrix that is updated throughout the training defines this transformation."}, {"title": "C. Spatial Feature Refinement Block", "content": "Pooling operations are used to reduce model complexity and computational overhead. Max-pooling retains the dominant features, while average-pooling retains low-frequency features for global context. Parallel paths of the max-pooling and average-pooling operations are implemented in SFRB to integrate local and global features effectively.\nIn this approach, in Fig. 2(b), the input feature map $F_{in}$ undergoes initial processing through convolutional layers, fol- lowed by batch normalization and ReLU activation represented in Eq. 18.\n$i\u2081 = ReLU(BN(C_{3\u00d73}(F_{in})))$\nSubsequently, the results of the max-pooling $P^{3\u00d73}_{Max}$ and average-pooling $P^{3\u00d73}_{Avg}$ are concatenated, followed by convo- lution (C), batch normalisation (BN) and ReLU activation (ReLU) calculated by Eq. 19.\n$i\u2082 = ReLU(BN(C_{3\u00d73} [P^{3\u00d73}_{Max}(i\u2081) + P^{3\u00d73}_{Avg} (i\u2081)]))$\nFurthermore, an additional pathway is introduced that incor- porates global average pooling ($P^{Global}_{Avg}$), convolutional oper- ations, batch normalization, and sigmoid activation function (\u03a3) to generate attention coefficients to weight the results of parallel pooling. Subsequently, the weighted feature map is combined with the input to produce the output $F_{out}$ of SFRB, as shown in Eq.20.\n$F_{out} = [\u03c3(BN(C_{3\u00d73}(P^{Global}_{Avg}(F_{in})))) i\u2082] + F_{in}$"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Our model was evaluated on DRIVE, STARE, and CHASE_DB-DB datasets. There are forty colored retinal images in the DRIVE dataset [48], each with a resolution of 565 x 584 pixels (8 bits per channel). These images were taken with a Canon CR5 non-mydriatic 3CCD camera with a 45-degree viewing field. Each of the two subsets of"}, {"title": "B. Implementation Details", "content": "Our model is implemented in TensorFlow and Keras and an NVIDIA Tesla P100 GPU with 32 GB RAM is used to perform experiments with batch size 8. The model is also evaluated on multi-class segmentation, which performs retinal artery and vein segmentation from retinal images on the RITE dataset [74]. We employed 80% images for model training and 20% validation from each dataset."}, {"title": "C. Performance Metrics", "content": "Standard evaluation metrics, including accuracy, dice, jac- card, specificity, sensitivity, and area under the curve (AUC), are used to assess the model performance. Below are the specified evaluation metrics:\n$Accuracy(acc) = \\frac{TP+TN}{TP+TN+FP+ FN}$\n$dice = \\frac{TP+TP}{TP+TP + FP + FN}$\n$Jaccard(J) = \\frac{TP}{FN+FP+TP}$\n$Sen = \\frac{TP}{FN+TP}$\n$Sp = \\frac{TN}{FP+TN}$\n$AUC = \\frac{1}{2\\times TP\\times TN}\\sum_{i=1}^{TP} \\sum_{j=1}^{TN} (1+\\frac{FN}{TP}-\\frac{FP}{TN})$\nwhere, false positive, true positive, false negative, and true negative are represented by FP,TP,FN,TN, respectively. Moreover, Sp and Sn represent specificity and sensitivity, respectively."}, {"title": "D. Performance Comparisons of the Blood Vessels Segmenta- tion", "content": "The quantitative assessment of our suggested model is given in Table II along with other existing models. The table shows that LVS-Net outperforms existing techniques in several performance criteria, including sensitivity, specificity, accuracy, and dice score, while retaining the advantage of being lightweight. LVS-Net specifically achieved performance of 96.64%, 86.44%, 76.16%, 83.91%, and 98.51% for accuracy, dice, jaccard, sensitivity, and specificity on the DRIVE dataset. Similarly, on the STARE dataset, it achieved 97.59%, 84.78%, 71.56%, 85.19%, and 98.61% for accuracy, dice, jaccard, sensitivity, and specificity, respectively. In the CHASE_DB dataset, it achieved scores of 96. 44%, 84. 78%, 73. 65%, 83. 29%, and 98. 44% for accuracy, dice, Jaccard, sensitivity, and specificity, respectively. These results show the superior performance of LVS-Net over other state-of-the-art models.\nA clear assessment of the performance of the prediction algorithm is presented in Fig. 3 by marking true positives with green, false positives with red, and false negatives with blue. Visual inspection of the results in the DRIVE dataset reveals that the proposed LVS-Net yielded fewer false positives on thin vessels compared to the recent methods. Furthermore, the U-Net variants struggled with the boundaries (as depicted in image 4). SegNet generated false tiny vessels in most images, and G-Net Light tended to skip vessel information, which was robustly captured by LVS-Net while concealing inaccurate vessel information.\nSimilarly, inspection of the results in the STARE dataset Fig. 4, notably images 2 and 3) shows that the alternative methods produced more false positives, particularly across retinal boundaries, optic nerves, and tiny vessels. On the other hand, the proposed LVS-Net showed much more robustness to these artifacts in these images. Similar results can be observed from the results on the CHASE_DB dataset in Fig. 5.\nIn Fig. 6, the Receiver Operating Characteristic (ROC) curves are compared for three datasets, that is, DRIVE, STARE, and CHASE_DB, for various models together with our proposed model. Each figure displays the True Positive Rate (TPR) against the False Positive Rate (FPR). Outperform- ing the other models, the proposed model consistently obtains the highest AUC values across all datasets. In particular, the suggested model outperforms the existing models in terms of AUC with values of 0.993, 0.997, and 0.998 for DRIVE, STARE, and CHASE_DB, respectively."}, {"title": "E. Performance Comparisons of the AV Segmentation", "content": "This section evaluates the generalization of our model by performing retinal artery vein segmentation using the RITE dataset. The findings demonstrate that our model obtained a higher dice score compared to other models on the RITE dataset. Consequently, our model demonstrates performance comparable to that of existing models while maintaining the"}, {"title": "F. Ablation Study on DRIVE dataset", "content": "The ablation study, presented in Table IV, performed on the DRIVE dataset, provides insight into the effects of various components on model performance. Evaluate the addition of various components to the lightweight U-Net (LU) baseline model to determine how those components affect the final model. When multiscale layers (MLU) are integrated, overall performance metrics are slightly improved, starting with LU. CBAM is added to the skip connections of MLU, substantially improving specificity and accuracy, highlighting the importance of attention mechanisms. Performance is improved by implementing SFRB in skip"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce a lightweight encoder-decoder model based on the segmentation of retinal blood vessels. Our model comprises an encoder and decoder, embedding multi-scale convolutional layers with the combination of focal modulation attention and the spatial feature refinement blocks at the bottleneck employed for the feature extraction. Furthermore, both the decoder and the skip connections use the spatial feature refinement block, helping to highlight and enhance key features. It is beneficial for image segmentation tasks where precise segmentation and localization of objects are important. Promising findings have been obtained from an extensive analysis of the model on public datasets, that is, DRIVE, CHASE_DB and STARE datasets, generating dice scores of 86. 44%, 82. 10% and 87.88%, respectively. Our model satisfies its lightweight requirements, 2.74 MB of memory, 0.71 million parameters, and 29.60 GFLOPs. These findings demonstrate that significant medical image analysis"}]}