[{"title": "Rethinking Inverse Reinforcement Learning: from Data Alignment to Task Alignment", "authors": ["Weichao Zhou", "Wenchao Li"], "abstract": "Many imitation learning (IL) algorithms use inverse reinforcement learning (IRL) to infer a reward function that aligns with the demonstrations. However, the inferred reward function often fails to capture the underlying task objective. In this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment. Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. It adopts an adversarial mechanism to train a policy with this set of reward functions to gain a collective validation of the policy's ability to accomplish the task. We provide theoretical insights into this framework's ability to mitigate task-reward misalignment and present a practical implementation. Our experimental results show that our framework outperforms conventional IL baselines in complex and transfer learning scenarios. The complete code are available at https://github.com/zwc662/PAGAR.", "sections": [{"title": "Introduction", "content": "Inverse reinforcement learning (IRL) Ng and Russell [2000], Finn et al. [2017] has become a popular method for imitation learning (IL), allowing policies to be trained by learning reward functions from expert demonstrations Abbeel and Ng [2004], Ho and Ermon [2016]. Despite its widespread use, IRL-based IL faces significant challenges that often stem from overemphasizing data alignment rather than task alignment. For instance, reward ambiguity, where multiple reward functions can be consistent with the expert demonstrations, makes it difficult to identify the correct reward function. This problem persists even when there are infinite data Ng and Russell [2000], Cao et al. [2021], Skalse et al. [2022a,b]. Additionally, limited availability of demonstrations can further exacerbate this problem, as the data may not fully capture the nuances of the task. Misaligned reward functions can lead to policies that optimize the wrong objectives, resulting in poor performance and even reward hacking Hadfield-Menell et al. [2017], Amodei et al. [2016], Pan et al. [2022], a phenomenon where the policy exploits loopholes in the inferred reward function. These challenges highlight the limitation of exclusively pursuing data alignment in solving real-world tasks.\nIn light of these considerations, this paper advocates for a paradigm shift from a narrow focus on data alignment to a broader emphasis on task alignment. Grounded in a general formalism of task objectives, we propose identifying the task-aligned reward functions that more accurately reflect the underlying task objectives in their policy utility spaces. Expanding on this concept, we explore the intrinsic relationship between the task objective, reward, and expert demonstrations. This relationship leads us to a novel perspective where expert demonstrations can serve as weak supervision signals for identifying a set of candidate task-aligned reward functions. Under these reward functions, the expert achieves high -- but not necessarily optimal -- performance. The rationale is that achieving high performance under a task-aligned reward function is often adequate for real-world applications.\nBuilding on this premise, we leverage IRL to derive the set of candidate task-aligned reward func-tions and propose Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised framework designed to mitigate task-reward misalignment by training a policy with this candidate reward set. PAGAR adopts an adversarial training mechanism between a protagonist policy and an adversarial reward searcher, iteratively improving the policy learner to attain high performance across the candidate reward set. This method moves beyond relying on deriving a single reward function from data, enabling a collective validation of the policy's similarity to expert demonstrations in terms of effectiveness in accomplishing tasks. Experimental results show that our algorithm outperforms baselines on complex IL tasks with limited demonstrations and in challenging transfer environments. We summarize our contributions below.\n\u2022 Introduction of Task Alignment in IRL-based IL: We present a novel perspective that shifts the focus from data alignment to task alignment, addressing the root causes of reward misalignment in IRL-based IL.\n\u2022 Protagonist Antagonist Guided Adversarial Reward (PAGAR): We propose a new semi-supervised framework that leverages adversarial training to improve the robustness of the learned policy.\n\u2022 Practical Implementation: We present a practical implementation of PAGAR, including the adversarial reward searching mechanism and the iterative policy-improving process. Experimental results demonstrate superior performances in complex and transfer learning environments."}, {"title": "Related Works", "content": "IRL-based IL circumvents many challenges of traditional IL such as compounding error Ross and Bagnell [2010], Ross et al. [2011], Zhou et al. [2020] by learning a reward function to interpret the expert behaviors Ng et al. [1999], Ng and Russell [2000] and then learning a policy from the reward function via reinforcement learning (RL)Sutton and Barto [2018]. However, the learned reward function may not always align with the underlying task, leading to reward misspecification Pan et al. [2022], Skalse and Abate [2022], reward hacking Skalse et al. [2022b], and reward ambiguity Ng and Russell [2000], Cao et al. [2021]. The efforts on alleviating reward ambiguity include Max-Entropy IRL Ziebart et al. [2008], Max-Margin IRL Abbeel and Ng [2004], Ratliff et al. [2006], and Bayesian IRL Ramachandran and Amir [2007]. GAN-based methods Ho and Ermon [2016], Jeon et al. [2018], Finn et al. [2016], Peng et al. [2019], Fu et al. [2018] use neural networks to learn reward functions from limited demonstrations. However, these efforts that aim to address reward ambiguity fall short of mitigating the general impact of reward misalignment which can be caused by various reasons such as IRL making false assumptions about the relationship between expert policy and expert reward function Skalse et al. [2022a], Hong et al. [2023]. Other attempts to mitigate reward misalignment involve external information other than expert demonstrations Hejna and Sadigh [2023], Zhou and Li [2018, 2022a,b]. Our work adopts the generic setting of IRL-based IL without needing additional information. The idea of considering a reward set instead of focusing on a single reward function is supported by Metelli et al. [2021] and Lindner et al. [2022]. However, these works target reward ambiguity instead of reward misalignment. Our protagonist and antagonist setup is inspired by the concept of unsupervised environment design (UED) Dennis et al. [2020]. In this paper, we develop novel theories in the context of reward learning."}, {"title": "Preliminaries", "content": "Reinforcement Learning (RL) models the environment as a Markov Decision Process $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, d_0)$ where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $P$ is the transition probability, $d_0$ is the initial state distribution. A policy $\\pi(a|s)$ determines the probability of an RL agent performing an action $a$ at state $s$. By successively performing actions for $T$ steps from an initial state $s(0) \\sim d_0$, a trajectory $\\tau = s(0)a(0)s(1)a(1) \\dots s(T)$ is produced. A state-action based reward function is a mapping $r:\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$. The soft Q-value function of $\\pi$ is $Q_\\pi(s, a) = r(s,a) + \\gamma \\cdot \\mathbb{E}_{s'\\sim P(\\cdot|s,a)}[V_\\pi(s')]$ where $\\gamma \\in (0,1]$ is a discount factor, $V_\\pi$ is the soft state-value function of $\\pi$ defined as $V_\\pi(s) := \\mathbb{E}_{a\\sim\\pi(\\cdot|s)}[Q_\\pi(s, a)] + H(\\pi(\\cdot|s))$, and $H(\\pi(\\cdot|s))$ is the entropy of $\\pi$ at state $s$."}, {"title": "Task-Reward Alignment", "content": "In this section, we formalize the concept of task-reward misalignment in IRL-based IL. We start by defining a notion of task based on the framework from Abel et al. [2021].\nDefinition 1 (Task). Given the policy hypothesis set $\\Pi$, a task $(\\Pi, \\leq_{\\text{task}}, \\Pi_{\\text{acc}})$ is specified by a partial order $\\leq_{\\text{task}}$ over $\\Pi$ and a non-empty set of acceptable policies $\\Pi_{\\text{acc}} \\subseteq \\Pi$ such that $\\forall \\pi_1 \\in \\Pi_{\\text{acc}}$ and $\\forall \\pi_2 \\notin \\Pi_{\\text{acc}}, \\pi_2 \\leq_{\\text{task}} \\pi_1$ always hold.\nRemark: The notions of policy acceptance and order allow the definition of task to accommodate a broad range of real-world tasks including the standard RL tasks (learning the optimal policy from a reward function $r$): given a reward function $r$ and a policy hypothesis set $\\Pi$, the standard RL task can be written as a tuple $(\\Pi, \\leq_{\\text{task}}, \\Pi_{\\text{acc}})$ where $\\leq_{\\text{task}}$ satisfies $\\forall \\pi_1, \\pi_2 \\in \\Pi, \\pi_1 \\leq_{\\text{task}} \\pi_2 \\Leftrightarrow U_r(\\pi_1) \\leq U_r(\\pi_2)$, and $\\Pi_{\\text{acc}} = {\\pi | \\forall \\pi' \\in \\Pi. \\pi' \\leq_{\\text{task}} \\pi}$ contains all the optimal policies.\nDesigning reward function(s) that align with the underlying task is essential in RL. Whether a designed reward aligns with the task hinges on how policies are ordered by the task and the utilities of the policies under the reward function. Therefore, we define the task-reward alignment by examining the utility spaces of the reward functions. If the acceptable policy set $\\Pi_{\\text{acc}}$ of the task is given, we let $U_r := \\min_{\\pi\\in \\Pi_{\\text{acc}}} U_r(\\pi)$ be the minimal utility achieved by any acceptable policy under $r$.\nDefinition 2 (Task-Aligned Reward Functions). A reward function is a task-aligned reward function (denoted as $r^+$) if and only if $\\forall \\pi \\in \\Pi \\backslash \\Pi_{\\text{acc}}, U_{r^+}(\\pi) < U^+ (\\pi)$. Conversely, if this condition is not met, it is a task-misaligned reward function (denoted as $r^-$).\nThe definition suggests that under a task-aligned reward function $r^+$, all acceptable policies for the task yield higher utilities than unacceptable ones. It also suggests that a policy is deemed acceptable as long as its utility is greater than $U^+$ for some task-aligned reward function $r^+$, even if this policy is not optimal. We also examine whether high utility under a reward function $r$ suggests a higher order under task. We define $U_r := \\max_{\\pi \\in \\Pi} U_r(\\pi) \\text{ s.t. } \\forall \\pi_1, \\pi_2 \\in \\Pi, U_r(\\pi_1) < U_r(\\pi) \\leq U_r(\\pi_2) \\Rightarrow (\\pi_1 \\leq_{\\text{task}} \\pi) \\land (\\pi_1 \\leq_{\\text{task}} \\pi_2)$, which is the highest utility threshold such that any policy achieving a higher utility than $U_r$ has a higher order than those achieving lower utilities than $U_r$.\nProposition 1. Given the policy order $\\leq_{\\text{task}}$ of a task, for any two reward functions $r_1, r_2$, if {$\\pi| U_{r_1}(\\pi) \\geq U_{r_1}$} $\\subseteq$ {$\\pi | U_{r_2}(\\pi) > U_{r_2}$}, then there must exist policies $\\pi_1 \\in$ {$\\pi| U_{r_1}(\\pi) \\geq U_{r_1}$}, $\\pi_2 \\in$ {$\\pi| U_{r_2}(\\pi) > U_{r_2}$} such that $U_{r_1}(\\pi_2) \\leq U_{r_1}(\\pi_1)$ and $\\pi_2 \\leq_{\\text{task}} \\pi_1$ while $U_{r_2}(\\pi_2) \\geq U_{r_2}(\\pi_1)$.\nThis proposition implies that a high threshold $U_r$ indicates that a high utility corresponds to a high order in terms of task. In particular, for any task-aligned reward function $r^+$, {$\\pi | U_{r^+}(\\pi) \\geq U_{r^+}$} $\\subset \\Pi_{\\text{acc}}$ = {$\\pi | U_{r^+}(\\pi) \\geq U^+}$} . Thus, a small {$\\pi | U_{r^+}(\\pi) \\geq U^+$} leads to a large {$\\pi | U_{r^+}(\\pi) \\in [U^+,\\overline{U_{r^+}}]$}. Hence, a task-aligned reward function $r^+$ is more likely to be aligned with the task if it has a wide $[U^+, \\overline{U_{r^+}}]$ and a narrow $[\\overline{U_{r^+}}, \\max_{\\pi \\in \\Pi} U_{r^+}(\\pi)]$.\nMitigate Task-Reward Misalignment in IRL-Based IL\nIn IRL-based IL, a key challenge is that the underlying task is unknown, making it difficult to assert if a learned policy is acceptable. We denote the optimal reward function learned from the demonstration set $\\mathcal{E}$ as $r^*$, and the optimal policy under $r^*$ as $\\pi_{r^*}$. When $\\pi_{r^*}$ has a poor performance under $r_{\\mathcal{E}}$, it is considered to have a high Regret($\\pi_{r^*},r_{\\mathcal{E}}$) which is defined in Eq.2. If Regret($\\pi_{r^*},r_{\\mathcal{E}}$) $> \\max_{\\pi' \\in \\Pi} U_{r_{\\mathcal{E}}}(\\pi') - U_{r^*}$, then $\\pi_{r^*}$ is unacceptable and $r^*$ is task-misaligned.\nRegret($\\pi, r$) := $\\max_{\\pi' \\in \\Pi} U_r(\\pi') - U_r(\\pi)$        (2)\nSeveral factors can lead to a high Regret($\\pi_{r^*}, r_{\\mathcal{E}}$). For instance, Viano et al. [2021] shows that when expert demonstrations are collected in an environment whose dynamical function differs from that of the learning environment, |Regret($\\pi_{r^*}, r_{\\mathcal{E}}$)| can be positively related to the discrepancy between those dynamical functions. Additionally, we prove in Appendix A.1 that learning from only a few representative expert trajectories can also result in a large |Regret($\\pi_{r^*}, r_{\\mathcal{E}}$)| with a high probability.\nOur insight for mitigating such potential task-reward misalignment in IRL-based IL is to shift our focus from learning an optimal policy that maximizes the intrinsic $r_{\\mathcal{E}}$ to learning an acceptable policy $\\pi^*$ that achieves a utility higher than $U^+$ under any task-aligned reward function $r^+$. Our approach is to treat the expert demonstrations as weak supervision signals based on the following.\nTheorem 1. Let $\\mathbb{I}$ be an indicator function. For any $k > {\\min \\sum_{\\pi \\in \\Pi}\\mathbb{I}{U^+(\\pi) \\geq U_{r^+}(\\pi_{\\mathcal{E}})}}$, if $\\pi^*$ satisfies {$\\sum_{\\pi \\in \\Pi}\\mathbb{I}{U_r(\\pi) \\geq U_r(\\pi^*)} < |\\Pi_{\\text{acc}}|$ for all $r \\in \\mathcal{R}_{\\mathcal{E}, k} := {r | \\sum_{\\pi \\in \\Pi}\\mathbb{I}{U_r(\\pi) \\geq U_r(\\pi_{\\mathcal{E}})} < k}$}, then $\\pi^*$ is an acceptable policy, i.e., $\\pi^* \\in \\Pi_{\\text{acc}}$. Additionally, if $k < |\\Pi_{\\text{acc}}|$, such an acceptable policy $\\pi^*$ is guaranteed to exist.\nThe statement suggests that we can obtain an acceptable policy by training it to attain high performance across a reward function set $\\mathcal{R}_{\\mathcal{E}, k}$ that includes all the reward functions where, for each reward function at most k policies outperform the expert policy $\\pi_{\\mathcal{E}}$. The minimal value of k is determined by all the task-aligned reward functions in the reward hypothesis set.\nHow to build $\\mathcal{R}_{\\mathcal{E},k}$? Building $\\mathcal{R}_{\\mathcal{E},k}$ involves setting the parameter $k$. If $r_{\\mathcal{E}}$ is a task-aligned reward function and $\\pi_{\\mathcal{E}}$ is optimal solely under $r_{\\mathcal{E}}$, then the minimal $k = 0$, and $\\mathcal{R}_{\\mathcal{E},0}$ only contains $r_{\\mathcal{E}}$. However, relying on a singleton $\\mathcal{R}_{\\mathcal{E},0}$ equates to applying vanilla IRL, which is susceptible to misalignment issues, as noted earlier. It is crucial to recognize that $r_{\\mathcal{E}}$ might not meet the task-aligned reward function criteria specified in Definition 2, even though its optimal policy $\\pi_{\\mathcal{E}}$ is acceptable. This situation necessitates a positive $k$, thereby expanding $\\mathcal{R}_{\\mathcal{E},k}$ beyond a single function and changing the role of expert demonstrations from strong supervision to weak supervision. Note that we suggest letting $k \\leq |\\Pi_{\\text{acc}}|$ instead of allowing $k \\rightarrow \\infty$ because $\\mathcal{R}_{\\mathcal{E},\\infty}$ would then encompass all possible reward functions, and it is impractical to identify a policy capable of achieving high performance across all reward functions. Letting $k \\leq |\\Pi_{\\text{acc}}|$ guarantees there exists a feasible policy $\\pi^*$, e.g., $\\pi_{\\mathcal{E}}$ itself. As the task alignment of each reward function typically remains unknown in IRL settings, this paper proposes treating $k$ as an adjustable parameter \u2013 starting with a small $k$ and adjusting based on empirical learning outcome, allowing for iterative refinement for alignment with task requirements.\nIn practice, $\\Pi$ can be uncountable, e.g., a Gaussian policy. Hence, we adapt the concept of k in $\\mathcal{R}_{\\mathcal{E},k}$ to a hyperparameter $\\delta < \\delta^* := \\max J_{\\text{IRL}}(r)$, leading us to redefine $\\mathcal{R}_{\\mathcal{E},k}$ as a $\\delta$-optimal reward function set $\\mathcal{R}_{\\mathcal{E},\\delta} := {r | J_{\\text{IRL}}(r) \\geq \\delta}$. This superlevel set includes all the reward functions under which the optimal policies outperform the expert by at most $-\\delta$. If $\\delta$ is appropriately selected such that $\\mathcal{R}_{\\mathcal{E},\\delta}$ includes task-aligned reward functions, we can mitigate reward misalignment by satisfying the conditions outlined in Definition 3, which are closely related to Definition 2 and Proposition 1.\nDefinition 3 (Mitigation of Task-Reward Misalignment). Assuming that the reward function set $\\mathcal{R}_{\\mathcal{E},\\delta}$ contains task-aligned reward function $r^+$'s, the mitigation of task-reward misalignment in IRL-based IL is to learn a policy $\\pi^*$ such that (i) (Weak Acceptance) $\\forall r^+ \\in \\mathcal{R}_{\\mathcal{E},\\delta}, U_{r^+}(\\pi^*) \\geq U^+$, or (ii) (Strong Acceptance) $\\forall r^+ \\in \\mathcal{R}_{\\mathcal{E},\\delta}, U_{r^+}(\\pi^*) > U^+$.\nWhile condition (i) states that $\\pi^*$ is acceptable for the task, i.e., $\\pi^* \\in \\Pi_{\\text{acc}}$, condition (ii) further states that $\\pi^*$ have a high order in terms of task. Hence, condition (i) is weaker than (ii) because a policy $\\pi^*$ satisfying (ii) automatically satisfies (i) according to Definition 2. Given the uncertainty in identifying which reward function is aligned, our solution is to train a policy to achieve high utilities under all reward functions in $\\mathcal{R}_{\\mathcal{E},\\delta}$ to satisfy the conditions in Definition 3. We explain this approach in the following semi-supervised paradigm, PAGAR."}, {"title": "Protagonist Antagonist Guided Adversarial Reward (PAGAR)", "content": "PAGAR is an adversarial reward searching paradigm which iteratively searches for a reward function to challenge a policy learner by incurring a high regret as defined in Eq.2. We refer to the policy to be learned as the protagonist policy and re-write it as $\\pi_\\rho$. We then introduce a second policy, dubbed antagonist policy $\\pi_\\mathcal{A}$, as a proxy of the $\\text{arg} \\max_{\\pi' \\in \\Pi} U_r(\\pi')$ for Eq.2. For each reward function $r$, we call the regret of $\\pi_\\rho$ under $r$, i.e., Regret($\\pi_\\rho,r$) = $\\max_{\\pi_\\mathcal{A} \\in \\Pi} U_r(\\pi_\\mathcal{A}) - U_r(\\pi_\\rho)$, the Protagonist Antagonist Induced Regret. We then formally define PAGAR in Definition 4.\nDefinition 4 (Protagonist Antagonist Guided Adversarial Reward (PAGAR)). Given a candidate reward function set $\\mathcal{R}$ and a protagonist policy $\\pi_\\rho$, PAGAR searches for a reward function $r$ within $\\mathcal{R}$ to maximize the Protagonist Antagonist Induced Regret, i.e., $\\max_{r \\in \\mathcal{R}} \\text{Regret}(\\pi_\\rho,r)$.\nPAGAR-based IL learns a policy from $\\mathcal{R}_{\\mathcal{E},\\delta}$ by minimizing the worst-case Protagonist Antagonist Induced Regret via MinimaxRegret($\\mathcal{R}_{\\mathcal{E},\\delta}$) as defined in Eq.3 where $\\mathcal{R}$ can be any input reward function set and is set as $\\mathcal{R} = \\mathcal{R}_{\\mathcal{E},\\delta}$ in PAGAR-based IL.\n$\\text{MinimaxRegret}(\\mathcal{R}) := \\text{arg} \\min_{\\pi_\\rho \\in \\Pi} \\max_{r \\in \\mathcal{R}} \\text{Regret}(\\pi_\\rho,r)$        (3)\nOur subsequent discussion will focus on identifying the sufficient conditions for PAGAR-based IL to mitigate task-reward misalignment as described in Definition 3. In particular, we consider the case where $J_{\\text{IRL}}(r) := U_r(\\mathcal{E}) - \\max_{\\pi} U_r(\\pi)$. We use $L_r$ to denote the Lipschitz constant of $r(r)$, and $W_\\mathcal{E}$ to denote the smallest Wasserstein 1-distance $W_1(\\pi, \\mathcal{E})$ between $\\tau \\sim \\pi$of any $\\pi$ and $\\tau \\sim \\mathcal{E}$, i.e., $W_\\mathcal{E} = \\min_{\\pi \\in \\Pi} W_1(\\pi, \\mathcal{E})$. Then, we have Theorem 2.\nTheorem 2 (Weak Acceptance). If the following conditions (1) (2) hold for $\\mathcal{R}_{\\mathcal{E},\\delta}$, then the optimal protagonist policy $\\pi_\\rho := \\text{MinimaxRegret}(\\mathcal{R}_{\\mathcal{E},\\delta})$ satisfies $\\forall r^+ \\in \\mathcal{R}_{\\mathcal{E},\\delta}, U_{r^+}(\\pi_\\rho) \\geq U^+$.\n$\\exists r^+ \\in \\mathcal{R}_{\\mathcal{E},\\delta}, \\text{ and } \\max_{\\pi \\in \\Pi} {\\max U_{r^+}(\\pi) - \\overline{U_{r^+}}} < \\min_{\\pi \\in \\Pi} {\\overline{U_{r^+}} - U^+}$;\n$\\forall r^+ \\in \\mathcal{R}_{\\mathcal{E},\\delta}, L_{r^+} \\cdot W_\\mathcal{E} - \\delta < \\max_{\\pi \\in \\Pi} U_{r^+}(\\pi) - U_{r^+}$ and$\\forall r^- \\in \\mathcal{R}_{\\mathcal{E},\\delta}, L_{r^-} \\cdot W_\\mathcal{E} - \\delta < \\min_{\\pi \\in \\Pi} {\\overline{U_{r^+}} - U^+ }$ .\nThis statement shows the conditions for PAGAR-based IL to attain the 'Weak Acceptance' goal described in Definition 3. The condition (1) states that the task-aligned reward functions in $\\mathcal{R}_{\\mathcal{E},\\delta}$ all have a high level of alignment in matching task within their high utility ranges. The condition (2) requires that for the policy $\\pi^* = \\text{arg} \\min_{\\pi \\in \\Pi} W_1(\\pi, \\mathcal{E})$, the performance difference between $\\mathcal{E}$ and $\\pi^*$ is small enough under all $r \\in \\mathcal{R}_{\\mathcal{E},\\delta}$. Since for each reward function $r \\in \\mathcal{R}_{\\mathcal{E},\\delta}$, the performance difference between $\\mathcal{E}$ and the optimal policy under $r$ is bounded by $\\delta$, condition (2) implicitly requires that $\\pi^*$ not only performs well under any task-aligned reward function $r^+$ (thus being acceptable in the task) but also achieve relatively low regret under task-misaligned reward function $r^-$. However, the larger the rage $[U^+, \\overline{U_{r^+}}]$ is across the task-aligned reward function $r^+$, the less strict the requirement for low regret under $r^-$ becomes. The following theorem further suggests that a $\\delta$ close to its upper-bound $\\delta^* := \\max_r J_{\\text{IRL}}(r)$ can help MinimaxRegret($\\mathcal{R}_{\\mathcal{E},\\delta}$) gain a better chance of finding an acceptable policy for the underlying task and attain the 'Strong Acceptance' goal described in Definition 3.\nTheorem 3 (Strong Acceptance). Assume that the condition (1) in Theorem 2 holds for $\\mathcal{R}_{\\mathcal{E},\\delta}$. If for any $r \\in \\mathcal{R}_{\\mathcal{E},\\delta}, L_r \\cdot W_\\mathcal{E} - \\delta < \\min_{r^+ \\in \\mathcal{R}_{\\mathcal{E},\\delta}} {\\max_{\\pi \\in \\Pi} U_{r^+}(\\pi) - U_{r^+}}$, then the optimal protagonist policy $\\pi_\\rho = \\text{MinimaxRegret}(\\mathcal{R}_{\\mathcal{E},\\delta})$ satisfies $\\forall r^+ \\in \\mathcal{R}_{\\mathcal{E},\\delta}, U_{r^+}(\\pi_\\rho) \\geq \\overline{U_{r^+}}$.\nWhen do these assumptions hold? The condition (1) in Theorem 2 requires all the task-aligned reward functions in $\\mathcal{R}_{\\mathcal{E},\\delta}$ exhibit a high level of conformity with the policy order $\\leq_{\\text{task}}$. Being task-aligned already sets a strong premise for satisfying this condition. We further posit that this condition is more easily satisfied when the task has a binary outcome, such as in reach-avoid tasks so that the aligned and misaligned reward functions tend to have higher discrepancy than tasks with quantitative outcomes. In the experimental section, we validate this hypothesis by evaluating tasks of this kind. Regarding condition (2) of Theorem 2 and the assumptions of Theorem 3, which basically require the existence of a policy with low regret across $\\mathcal{R}_{\\mathcal{E},\\delta}$ set, it is reasonable to assume that expert policy meets this criterion."}, {"title": "Comparing PAGAR-Based IL with IRL-Based IL", "content": "We illustrate the difference between IRL-based IL and PAGAR-based IRL in Fig.1(b). While IRL-based IL aims to learn the optimal policy $\\pi_{\\rho^*}$ under the IRL-optimal reward $r^*$, PAGAR-based IL learns a policy $\\pi^*$ from the reward function set $\\mathcal{R}_{\\mathcal{E},\\delta}$. Both PAGAR-based IL and IRL-based IL are zero-sum games between a policy learner and a reward learner. However, while IRL-based IL only aims to reach equilibrium at a single reward function under strong assumptions, e.g., sufficient demonstrations, convex reward and policy spaces, etc., PAGAR-based IL can reach equilibrium with a mixture of reward functions without those assumptions.\nProposition 2. Given arbitrary reward function set $\\mathcal{R}$, there exists a constant $c$ and a distribution $\\mathbb{R}$ over $\\mathcal{R}$ such that MinimaxRegret($\\mathcal{R}$) yields the same pol-icy as $\\text{arg} \\max_{\\pi \\in \\Pi} { \\mathbb{E}_{r \\sim \\mathbb{R}(r)} [\\text{Regret}(\\pi_\\cdot,r) \\cdot U_r(\\pi) +  \\frac{c - U_r(\\pi)}{1 + \\text{Regret}(\\pi_\\cdot,r)}] }$. The reward function $r^*=\\text{arg} \\max_\\pi \\text{arg} \\max_{\\pi} U_r(\\pi) \\text{s.t.} \\forall r \\in \\mathcal{R}$.\nA detailed derivation can be found in Theorem 6 in Appendix A.4. In a nutshell, $\\mathbb{R}_\\pi(r)$ is a baseline distribution over $\\mathcal{R}$ such that (i) $c = \\mathbb{E}_{r \\sim \\mathbb{R}}[U(\u03c0)]$ holds for all the \u03c0's that do not always perform worse than any other policy under $r \\in \\mathcal{R}$, (ii) among all the $\\mathbb{R}$'s that satisfy the condition (i), we pick the one with the minimal $c$; and (iii) for any other policy \u03c0, $\\mathbb{R}_\u03c0$ uniformly concentrates on $\\text{arg} \\max_r U_r(\u03c0)$. Note that in PAGAR-based IL, where $\\mathcal{R}_{\\mathcal{E},\\delta}$ is used in place of arbitrary $\\mathcal{R}$, $\\mathbb{R}$ is a distribution over $\\mathcal{R}_{\\mathcal{E},\\delta}$ and $r^*$ is constrained to be within $\\mathcal{R}_{\\mathcal{E},\\delta}$. Essentially, the mixed reward functions dynamically assign weights to $r \\sim \\mathbb{R}$ and $r^*$ depending on \u03c0. If \u03c0 performs worse under"}, {"title": "A Practical Approach to Implementing PAGAR-based IL", "content": "In this section", "Off-Policy": "We leverage the Theorem 1 in Schulman et al. [2015", "subtraction": "U_r(\\pi_\\rho) - U_r(\\pi_\\mathcal{A"}, "leq \\sum_{\\mathcal{S}} \\rho_{\\pi_\\mathcal{A}}(s) \\sum_{\\alpha \\in \\mathcal{A}} \\pi_\\rho(a|s)A_{\\pi_\\mathcal{A}}(s, a) + C \\cdot \\max_s D_{TV}(\\pi_\\mathcal{A}(\\cdot|s), \\pi_\\rho(\\cdot|s))^2$ where $\\rho_{\\pi_\\mathcal{A}}(s) = \\sum_{t=0}^{\\infty} \\gamma^t Prob(s(t) = s | \\pi_\\mathcal{A})$ is the discounted visitation frequency of $\\pi_\\mathcal{A}$, $A_{\\pi_\\mathcal{A}}$ is the advantage function without considering the entropy, and C is some constant. Then we follow the derivation in Schulman et al. [2017"], "pi_\\rho;r)": "mathbb{E"}, {"On-Policy": "We also optimize $\\pi_\\rho$ with the standard RL objective function $J_{\\text{IRL}}(\\pi_\\rho;r)$ by using the trajectories of $\\pi_\\rho$ itself. As a result, the objective function for optimizing $\\pi_\\rho$ is $\\max_{\\Pi_{\\rho} \\in \\Pi} I_{\\pi_\\mathcal{A}} (\\pi_\\rho;r) + J_{\\text{IRL}}(\\pi_\\rho;r)$. As for $\\pi_\\mathcal{A}$, we only"}]