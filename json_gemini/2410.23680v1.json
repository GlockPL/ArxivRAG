{"title": "Rethinking Inverse Reinforcement Learning: from Data Alignment to Task Alignment", "authors": ["Weichao Zhou", "Wenchao Li"], "abstract": "Many imitation learning (IL) algorithms use inverse reinforcement learning (IRL) to infer a reward function that aligns with the demonstrations. However, the inferred reward function often fails to capture the underlying task objective. In this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment. Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. It adopts an adversarial mechanism to train a policy with this set of reward functions to gain a collective validation of the policy's ability to accomplish the task. We provide theoretical insights into this framework's ability to mitigate task-reward misalignment and present a practical implementation. Our experimental results show that our framework outperforms conventional IL baselines in complex and transfer learning scenarios. The complete code are available at https://github.com/zwc662/PAGAR.", "sections": [{"title": "Introduction", "content": "Inverse reinforcement learning (IRL) Ng and Russell [2000], Finn et al. [2017] has become a popular method for imitation learning (IL), allowing policies to be trained by learning reward functions from expert demonstrations Abbeel and Ng [2004], Ho and Ermon [2016]. Despite its widespread use, IRL-based IL faces significant challenges that often stem from overemphasizing data alignment rather than task alignment. For instance, reward ambiguity, where multiple reward functions can be consistent with the expert demonstrations, makes it difficult to identify the correct reward function. This problem persists even when there are infinite data Ng and Russell [2000], Cao et al. [2021], Skalse et al. [2022a,b]. Additionally, limited availability of demonstrations can further exacerbate this problem, as the data may not fully capture the nuances of the task. Misaligned reward functions can lead to policies that optimize the wrong objectives, resulting in poor performance and even reward hacking Hadfield-Menell et al. [2017], Amodei et al. [2016], Pan et al. [2022], a phenomenon where the policy exploits loopholes in the inferred reward function. These challenges highlight the limitation of exclusively pursuing data alignment in solving real-world tasks.\nIn light of these considerations, this paper advocates for a paradigm shift from a narrow focus on data alignment to a broader emphasis on task alignment. Grounded in a general formalism of task objectives, we propose identifying the task-aligned reward functions that more accurately reflect the underlying task objectives in their policy utility spaces. Expanding on this concept, we explore the intrinsic relationship between the task objective, reward, and expert demonstrations. This relationship leads us to a novel perspective where expert demonstrations can serve as weak supervision signals for identifying a set of candidate task-aligned reward functions. Under these reward functions, the expert achieves high -- but not necessarily optimal -- performance. The rationale is that achieving high performance under a task-aligned reward function is often adequate for real-world applications.\nBuilding on this premise, we leverage IRL to derive the set of candidate task-aligned reward functions and propose Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised framework designed to mitigate task-reward misalignment by training a policy with this candidate reward set. PAGAR adopts an adversarial training mechanism between a protagonist policy and an adversarial reward searcher, iteratively improving the policy learner to attain high performance across the candidate reward set. This method moves beyond relying on deriving a single reward function from data, enabling a collective validation of the policy's similarity to expert demonstrations in terms of effectiveness in accomplishing tasks. Experimental results show that our algorithm outperforms baselines on complex IL tasks with limited demonstrations and in challenging transfer environments. We summarize our contributions below.\n\u2022 Introduction of Task Alignment in IRL-based IL: We present a novel perspective that shifts the focus from data alignment to task alignment, addressing the root causes of reward misalignment in IRL-based IL.\n\u2022 Protagonist Antagonist Guided Adversarial Reward (PAGAR): We propose a new semi-supervised framework that leverages adversarial training to improve the robustness of the learned policy.\n\u2022 Practical Implementation: We present a practical implementation of PAGAR, including the adversarial reward searching mechanism and the iterative policy-improving process. Experimental results demonstrate superior performances in complex and transfer learning environments."}, {"title": "Related Works", "content": "IRL-based IL circumvents many challenges of traditional IL such as compounding error Ross and Bagnell [2010], Ross et al. [2011], Zhou et al. [2020] by learning a reward function to interpret the expert behaviors Ng et al. [1999], Ng and Russell [2000] and then learning a policy from the reward function via reinforcement learning (RL)Sutton and Barto [2018]. However, the learned reward function may not always align with the underlying task, leading to reward misspecification Pan et al. [2022], Skalse and Abate [2022], reward hacking Skalse et al. [2022b], and reward ambiguity Ng and Russell [2000], Cao et al. [2021]. The efforts on alleviating reward ambiguity include Max-Entropy IRL Ziebart et al. [2008], Max-Margin IRL Abbeel and Ng [2004], Ratliff et al. [2006], and Bayesian IRL Ramachandran and Amir [2007]. GAN-based methods Ho and Ermon [2016], Jeon et al. [2018], Finn et al. [2016], Peng et al. [2019], Fu et al. [2018] use neural networks to learn reward functions from limited demonstrations. However, these efforts that aim to address reward ambiguity fall short of mitigating the general impact of reward misalignment which can be caused by various reasons such as IRL making false assumptions about the relationship between expert policy and expert reward function Skalse et al. [2022a], Hong et al. [2023]. Other attempts to mitigate reward misalignment involve external information other than expert demonstrations Hejna and Sadigh [2023], Zhou and Li [2018, 2022a,b]. Our work adopts the generic setting of IRL-based IL without needing additional information. The idea of considering a reward set instead of focusing on a single reward function is supported by Metelli et al. [2021] and Lindner et al. [2022]. However, these works target reward ambiguity instead of reward misalignment. Our protagonist and antagonist setup is inspired by the concept of unsupervised environment design (UED) Dennis et al. [2020]. In this paper, we develop novel theories in the context of reward learning."}, {"title": "Preliminaries", "content": "Reinforcement Learning (RL) models the environment as a Markov Decision Process $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, d_0)$ where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $P$ is the transition probability, $d_0$ is the initial state distribution. A policy $\\pi(a|s)$ determines the probability of an RL agent performing an action $a$ at state $s$. By successively performing actions for $T$ steps from an initial state $s(0) \\sim d_0$, a trajectory $\\tau = s(0)a(0)s(1)a(1) ...s(T)$ is produced. A state-action based reward function is a mapping $r:\\mathcal{S}\\times\\mathcal{A} \\rightarrow \\mathbb{R}$. The soft Q-value function of $\\pi$ is $Q^{\\pi}(s, a) = r(s,a) + \\gamma \\cdot \\mathbb{E}_{s'\\sim P(\\cdot|s,a)}[V^{\\pi}(s')]$ where $\\gamma\\in (0,1]$ is a discount factor, $V^{\\pi}$ is the soft state-value function of $\\pi$ defined as $V_{\\pi}(s) := \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[Q^{\\pi}(s, a)] + H(\\pi(\\cdot|s))$, and $H(\\pi(\\cdot|s))$ is the entropy of $\\pi$ at state $s$."}, {"title": "Task-Reward Alignment", "content": "In this section, we formalize the concept of task-reward misalignment in IRL-based IL. We start by defining a notion of task based on the framework from Abel et al. [2021].\nDefinition 1 (Task). Given the policy hypothesis set $\\Pi$, a task $(\\Pi, \\leq_{task}, \\Pi_{acc})$ is specified by a partial order $\\leq_{task}$ over $\\Pi$ and a non-empty set of acceptable policies $\\Pi_{acc} \\subseteq \\Pi$ such that $\\forall \\pi_1 \\in \\Pi_{acc}$ and $\\forall \\pi_2 \\notin \\Pi_{acc}$, $\\pi_2 \\leq_{task} \\pi_1$ always hold.\nRemark: The notions of policy acceptance and order allow the definition of task to accommodate a broad range of real-world tasks including the standard RL tasks (learning the optimal policy from a reward function $r$): given a reward function $r$ and a policy hypothesis set $\\Pi$, the standard RL task can be written as a tuple $(\\Pi, \\leq_{task}, \\Pi_{acc})$ where $\\leq_{task}$ satisfies $\\forall \\pi_1, \\pi_2 \\in \\Pi$, $\\pi_1 \\leq_{task} \\pi_2 \\Leftrightarrow U_r(\\pi_1) \\leq U_r(\\pi_2)$, and $\\Pi_{acc} = {\\pi|\\forall \\pi' \\in \\Pi. \\pi' \\leq_{task} \\pi}$ contains all the optimal policies.\nDesigning reward function(s) that align with the underlying task is essential in RL. Whether a designed reward aligns with the task hinges on how policies are ordered by the task and the utilities of the policies under the reward function. Therefore, we define the task-reward alignment by examining the utility spaces of the reward functions. If the acceptable policy set $\\Pi_{acc}$ of the task is given, we let $U_r := \\min_{\\pi\\in\\Pi_{acc}} U_r(\\pi)$ be the minimal utility achieved by any acceptable policy under $r$.\nDefinition 2 (Task-Aligned Reward Functions). A reward function is a task-aligned reward function (denoted as $r^+$) if and only if $\\forall \\pi \\in \\Pi\\setminus\\Pi_{acc}$, $U_{r^+}(\\pi) < U_{r^+} (\\pi)$. Conversely, if this condition is not met, it is a task-misaligned reward function (denoted as $r^-$).\nThe definition suggests that under a task-aligned reward function $r^+$, all acceptable policies for the task yield higher utilities than unacceptable ones. It also suggests that a policy is deemed acceptable as long as its utility is greater than $U_{r^+}$ for some task-aligned reward function $r^+$, even if this policy is not optimal. We also examine whether high utility under a reward function $r$ suggests a higher order under $\\leq_{task}$. We define $\\overline{U_r} := \\max_{\\pi\\in\\Pi} U_r(\\pi) \\text{ s.t. } \\forall \\pi_1, \\pi_2 \\in \\Pi, U_r(\\pi_1) < U_r(\\pi) \\leq U_r(\\pi_2) \\Rightarrow (\\pi_1 \\leq_{task} \\pi) \\land (\\pi_1 \\leq_{task} \\pi_2)$, which is the highest utility threshold such that any policy achieving a higher utility than $\\overline{U_r}$ has a higher order than those achieving lower utilities than $\\overline{U_r}$. In Figure 1(a) we illustrate how $U_r$ and $\\overline{U_r}$ vary between task-aligned and misaligned reward functions.\nProposition 1. Given the policy order $\\leq_{task}$ of a task, for any two reward functions $r_1, r_2$, if {$\\pi| U_{r_1}(\\pi) \\geq \\overline{U_{r_1}}$} $\\subseteq$ {$\\pi | U_{r_2}(\\pi) > \\overline{U_{r_2}}$}, then there must exist policies $\\pi_1 \\in {\\pi| U_{r_1}(\\pi) \\geq \\overline{U_{r_1}}}, \\pi_2 \\in {\\pi| U_{r_2}(\\pi) > \\overline{U_{r_2}}}$ such that $U_{r_1}(\\pi_2) \\leq U_{r_1} (\\pi_1)$ and $\\pi_2 \\leq_{task} \\pi_1$ while $U_{r_2}(\\pi_2) \\geq U_{r_2} (\\pi_1)$.\nThis proposition implies that a high threshold $\\overline{U_r}$ indicates that a high utility corresponds to a high order in terms of $\\leq_{task}$. In particular, for any task-aligned reward function $r^+$, {$\\pi | U_{r^+}(\\pi) \\geq \\overline{U_{r^+}}$} $\\subset \\Pi_{acc} = {\\pi | U_{r^+}(\\pi) \\geq U_{r^+}}$ (see proof in Appendix A.2). Thus, a small {$\\pi | U_{r^+}(\\pi) \\geq \\overline{U_{r^+}}$} leads to a large {$\\pi | U_{r^+}(\\pi) \\in [U_{r^+},\\overline{U_{r^+}}] $}. Hence, a task-aligned reward function $r^+$ is more likely to be aligned with the task if it has a wide $[U_{r^+}, \\overline{U_{r^+}}]$ and a narrow $[\\overline{U_{r^+}}, \\max_{\\pi\\in\\Pi} U_{r^+}(\\pi)]$.\n4.1 Mitigate Task-Reward Misalignment in IRL-Based IL\nIn IRL-based IL, a key challenge is that the underlying task is unknown, making it difficult to assert if a learned policy is acceptable. We denote the optimal reward function learned from the demonstration set $\\mathcal{E}$ as $r^*$, and the optimal policy under $r^*$ as $\\pi_{r^*}$. When $\\pi_{r^*}$ has a poor performance under $r_E$, it is considered to have a high $Regret(\\pi_{r^*},r_E)$ which is defined in Eq.2. If $Regret(\\pi_{r^*},r_E) > \\max_{\\pi' \\in \\Pi} U_{r_E}(\\pi') - U_{r^*}$, then $\\pi_{r^*}$ is unacceptable and $r^*$ is task-misaligned.\n$Regret(\\pi, r) := \\max_{\\pi' \\in \\Pi} U_r(\\pi') - U_r(\\pi)$ (2)\nSeveral factors can lead to a high $Regret(\\pi_{r^*}, r_E)$. For instance, Viano et al. [2021] shows that when expert demonstrations are collected in an environment whose dynamical function differs from that of the learning environment, $|Regret(\\pi_{r^*}, r_E)|$ can be positively related to the discrepancy between those dynamical functions. Additionally, we prove in Appendix A.1 that learning from only a few representative expert trajectories can also result in a large $|Regret(\\pi_{r^*}, r_E)|$ with a high probability.\nOur insight for mitigating such potential task-reward misalignment in IRL-based IL is to shift our focus from learning an optimal policy that maximizes the intrinsic $r_E$ to learning an acceptable policy $\\pi^*$ that achieves a utility higher than $U_{r^+}$ under any task-aligned reward function $r^+$. Our approach is to treat the expert demonstrations as weak supervision signals based on the following.\nTheorem 1. Let $I$ be an indicator function. For any $k > {\\min \\sum_{\\pi\\in\\Pi}I{\\{U_{r^+}(\\pi) \\geq U_{r^+}(\\pi_{\\mathcal{E}})\\}}}$, if $\\pi^*$ satisfies {$\\sum_{\\pi\\in\\Pi}I{\\{U_{r}(\\pi) \\geq U_{r}(\\pi^*)\\}} < |\\Pi_{acc}|$for all $r \\in R_{E,k} := {r | \\sum_{\\pi\\in\\Pi}I{\\{U_{r}(\\pi) \\geq U_{r}(\\pi_{\\mathcal{E}})\\}} < k$}, then $\\pi^*$ is an acceptable policy, i.e., $\\pi^* \\in \\Pi_{acc}$. Additionally, if $k < |\\Pi_{acc}|$, such an acceptable policy $\\pi^*$ is guaranteed to exist.\nThe statement suggests that we can obtain an acceptable policy by training it to attain high performance across a reward function set $R_{E,k}$ that includes all the reward functions where, for each reward function at most $k$ policies outperform the expert policy $\\pi_\\mathcal{E}$. The minimal value of $k$ is determined by all the task-aligned reward functions in the reward hypothesis set. Appendix A.2 provides the proof."}, {"title": "Protagonist Antagonist Guided Adversarial Reward (PAGAR)", "content": "PAGAR is an adversarial reward searching paradigm which iteratively searches for a reward function to challenge a policy learner by incurring a high regret as defined in Eq.2. We refer to the policy to be learned as the protagonist policy and re-write it as $\\pi_P$. We then introduce a second policy, dubbed antagonist policy $\\pi_A$, as a proxy of the $\\arg \\max_{\\pi' \\in \\Pi} U_r(\\pi')$ for Eq.2. For each reward function $r$, we call the regret of $\\pi_P$ under $r$, i.e., $Regret(\\pi_P,r) = \\max_{\\pi_A\\in\\Pi} U_r(\\pi_A) - U_r(\\pi_P)$, the Protagonist Antagonist Induced Regret. We then formally define PAGAR in Definition 4.\nDefinition 4 (Protagonist Antagonist Guided Adversarial Reward (PAGAR)). Given a candidate reward function set $\\mathcal{R}$ and a protagonist policy $\\pi_P$, PAGAR searches for a reward function $r$ within $\\mathcal{R}$ to maximize the Protagonist Antagonist Induced Regret, i.e., $\\max_{r\\in\\mathcal{R}} Regret (\\pi_P,r)$.\nPAGAR-based IL learns a policy from $R_{E,\\delta}$ by minimizing the worst-case Protagonist Antagonist Induced Regret via $MinimaxRegret(R_{E,\\delta})$ as defined in Eq.3 where $\\mathcal{R}$ can be any input reward function set and is set as $\\mathcal{R} = R_{E,\\delta}$ in PAGAR-based IL.\n$MinimaxRegret(\\mathcal{R}) := \\arg \\min_{\\pi_P\\in\\Pi} \\max_{r\\in\\mathcal{R}} Regret (\\pi_P,r)$ (3)\nOur subsequent discussion will focus on identifying the sufficient conditions for PAGAR-based IL to mitigate task-reward misalignment as described in Definition 3. In particular, we consider the case where $J_{IRL}(r) := U_r(\\pi_{\\mathcal{E}}) - \\max_{\\pi} U_r(\\pi)$. We use $L_r$ to denote the Lipschitz constant of $r(\\tau)$, and $W_{\\mathcal{E}}$ to denote the smallest Wasserstein 1-distance $W_1(\\pi, \\mathcal{E})$ between $\\tau \\sim \\pi \\text{of any }\\pi \\text{ and } \\tau \\sim \\mathcal{E}$, i.e., $W_{\\mathcal{E}} = \\min_{\\pi\\in\\Pi} W_1(\\pi, \\mathcal{E})$. Then, we have Theorem 2.\nTheorem 2 (Weak Acceptance). If the following conditions (1) (2) hold for $R_{E,\\delta}$, then the optimal protagonist policy $\\pi_P := MinimaxRegret(R_{E,\\delta})$ satisfies $\\forall r^+ \\in R_{E,\\delta}, U_{r^+}(\\pi_P) \\geq U_{r^+}$."}, {"title": "Mitigate Task-Reward Misalignment in IRL-Based IL", "content": "$\\max_{r^+ \\in R_{E,\\delta}} \\{ \\max_{\\pi\\in\\Pi} U_{r^+} (\\pi) - \\overline{U_{r^+}} \\} < \\min_{r^+ \\in R_{E,\\delta}} \\{ \\overline{U_{r^+}} - U_{r^+} \\};$\n$\\forall r^+ \\in R_{E,\\delta}, L_{r^+} \\cdot W_{\\mathcal{E}} - \\delta < \\max_{\\pi\\in\\Pi} U_{r^+} (\\pi) - U_{r^+}$ and$\\forall r^- \\in R_{E,\\delta}, L_{r^-} \\cdot W_{\\mathcal{E}} - \\delta < \\min_{r^+ \\in R_{E,\\delta}} \\{ \\overline{U_{r^+}} - U_{r^+} \\}.$\nThis statement shows the conditions for PAGAR-based IL to attain the 'Weak Acceptance' goal described in Definition 3. The condition (1) states that the task-aligned reward functions in $R_{E,\\delta}$ all have a high level of alignment in matching $\\leq_{task}$ within their high utility ranges. The condition (2) requires that for the policy $\\pi^* = \\arg \\min_{\\pi\\in\\Pi} W_1(\\pi, \\mathcal{E})$, the performance difference between $\\mathcal{E}$ and $\\pi^*$ is small enough under all $r\\in R_{E,\\delta}$. Since for each reward function $r \\in R_{E,\\delta}$, the performance difference between $\\mathcal{E}$ and the optimal policy under $r$ is bounded by $\\delta$, condition (2) implicitly requires that $\\pi^*$ not only performs well under any task-aligned reward function $r^+$ (thus being acceptable in the task) but also achieve relatively low regret under task-misaligned reward function $r^-$. However, the larger the rage $[U_{r^+}, \\overline{U_{r^+}}]$ is across the task-aligned reward function $r^+$, the less strict the requirement for low regret under $r^-$ becomes. The proof can be found in Appendix A.5. The following theorem further suggests that a $\\delta$ close to its upper-bound $\\delta^* := \\max_r J_{IRL}(r)$ can help $MinimaxRegret(R_{E,\\delta})$ gain a better chance of finding an acceptable policy for the underlying task and attain the 'Strong Acceptance' goal described in Definition 3.\nTheorem 3 (Strong Acceptance). Assume that the condition (1) in Theorem 2 holds for $R_{E,\\delta}$. If for any$r \\in R_{E,\\delta}, L_r\\cdot W_{\\mathcal{E}} - \\delta < \\min_{r^+ \\in R_{E,\\delta}} \\{ \\max_{\\pi\\in\\Pi} U_{r^+} (\\pi) - \\overline{U_{r^+}} \\}$, then the optimal protagonist policy $\\pi_P = MinimaxRegret(R_{E,\\delta})$ satisfies $\\forall r^+ \\in R_{E,\\delta}, U_{r^+}(\\pi_P) \\geq \\overline{U_{r^+}}$.\nWhen do these assumptions hold? The condition (1) in Theorem 2 requires all the task-aligned reward functions in $R_{E,\\delta}$ exhibit a high level of conformity with the policy order $\\leq_{task}$. Being task-aligned already sets a strong premise for satisfying this condition. We further posit that this condition is more easily satisfied when the task has a binary outcome, such as in reach-avoid tasks so that the aligned and misaligned reward functions tend to have higher discrepancy than tasks with quantitative outcomes. In the experimental section, we validate this hypothesis by evaluating tasks of this kind. Regarding condition (2) of Theorem 2 and the assumptions of Theorem 3, which basically require the existence of a policy with low regret across $R_{E,\\delta}$ set, it is reasonable to assume that expert policy meets this criterion."}, {"title": "Comparing PAGAR-Based IL with IRL-Based IL", "content": "We illustrate the difference between IRL-based IL and PAGAR-based IRL in Fig.1(b). While IRL-based IL aims to learn the optimal policy $\\pi_{r^*}$ under the IRL-optimal reward $r^*$, PAGAR-based IL learns a policy $\\pi^*$ from the reward function set $R_{E,\\delta}$. Both PAGAR-based IL and IRL-based IL are zero-sum games between a policy learner and a reward learner. However, while IRL-based IL only aims to reach equilibrium at a single reward function under strong assumptions, e.g., sufficient demonstrations, convex reward and policy spaces, etc., PAGAR-based IL can reach equilibrium with a mixture of reward functions without those assumptions.\nProposition 2. Given arbitrary reward function set $\\mathcal{R}$, there exists a constant $c$ and a distribution $\\mathbb{R}$ over $\\mathcal{R}$ such that MinimaxRegret($\\mathcal{R}$) yields the same pol-icy as $\\arg \\max_{\\pi \\in \\Pi} \\{  \\mathbb{E}_{r \\sim \\mathbb{R}(r)} [\\frac{Regret(\\pi,.)}{c - U_r(\\pi)}  ] - U_r(\\pi)  \\}$ where $r^* = \\arg \\max_{r} U_r(\\pi) s.t. r \\in \\arg \\max_{r'\\in\\mathcal{R}} Regret(\\pi, r').$\nA detailed derivation can be found in Theorem 6 in Appendix A.4. In a nutshell, $\\mathbb{R}_\\pi(r)$ is a baseline distribution over $\\mathcal{R}$ such that (i) $c = \\mathbb{E}_{r\\sim \\mathbb{R}} [U_r(\\pi)]$ holds for all the $\\pi$'s that do not always perform worse than any other policy under $r \\in \\mathcal{R}$, (ii) among all the $\\mathbb{R}$'s that satisfy the condition (i), we pick the one with the minimal $c$; and (iii) for any other policy $\\pi$, $\\mathbb{R}_\\pi$ uniformly concentrates on $\\arg \\max_{r \\in \\mathcal{R}} U_r(\\pi)$. Note that in PAGAR-based IL, where $R_{E,\\delta}$ is used in place of arbitrary $\\mathcal{R}$, $\\mathbb{R}$ is a distribution over $R_{E,\\delta}$ and $r^*$ is constrained to be within $R_{E,\\delta}$. Essentially, the mixed reward functions dynamically assign weights to $r \\sim \\mathbb{R}$ and $r^*$ depending on $\\pi$. If $\\pi$ performs worse under"}, {"title": "A Practical Approach to Implementing PAGAR-based IL", "content": "In this section, we introduce a practical approach to solving $MinimaxRegret(R_{E,\\delta})$ based on IRL and RL. In a nutshell, this approach alternates between policy learning and reward searching. We first explain how we optimize the policies; then we derive from Eq.3 two reward improvement bounds for searching for the adversarial reward. We will also discuss how to enforce the constraint $r \\in R_{E,\\delta}$.\n6.1 Policy Optimization with On-and-Off Policy Samples\nGiven an intermediate learned reward function $r$, we use RL to train $\\pi_P$ to minimize the regret $\\min_{\\pi_P} Regret(\\pi_P,r) = \\min_{\\pi_P} {\\max_{\\pi_A} U_r(\\pi_A)} - U_r(\\pi_P)$ as indicated by Eq.3 where $\\pi_A$ is trained to serve as the optimal policy under $r$ as noted in Section 5. Since we have to sample trajectories with $\\pi_A$ and $\\pi_P$, we propose to combine off-policy and on-policy samples to optimize $\\pi_P$ so that we can leverage the samples maximally. Off-Policy: We leverage the Theorem 1 in Schulman et al. [2015] to derive a bound for the utility subtraction: $U_r(\\pi_P) - U_r(\\pi_A) \\leq \\sum_{\\mathcal{S}} \\rho_{\\pi_A}(s) \\sum_{\\alpha \\in A} \\pi_P(a|s)A_{\\pi_A}(s, a) + C \\cdot \\max_{s \\in \\mathcal{S}} D_{TV} (\\pi_A(\\cdot|s), \\pi_P(\\cdot|s))^2$ where $\\rho_{\\pi_A}(s) = \\sum_{t=0}^{\\infty}\\gamma^t Prob(s(t) = s|\\pi_A)$ is the discounted visitation frequency of $\\pi_A$, $A_{\\pi_A}$ is the advantage function without considering the entropy, and C is some constant. Then we follow the derivation in Schulman et al. [2017], which is based on Theorem 1 in Schulman et al. [2015], to derive from the inequality an importance sampling-based objective function $I_{\\pi_A} (\\pi_P;r) := \\mathbb{E}_{s\\sim\\pi_A}[\\min(\\xi(s, a) \\cdot A_{\\pi_A} (s, a), clip(\\xi(s, a),1 - \\sigma, 1 + \\sigma)\\cdot A_{\\pi_A} (s, a)]$ where $\\sigma$ is a clipping threshold, $\\xi(s, a) = \\frac{\\pi_P(a|s)}{\\pi_A(a|s)}$ is an importance sampling rate. The details can be found in Appendix B.1. This objective function allows us to train $\\pi_P$ by using the trajectories of $\\pi_A$. On-Policy: We also optimize $\\pi_P$ with the standard RL objective function $J_{IRL}(\\pi_P;r)$ by using the trajectories of $\\pi_P$ itself. As a result, the objective function for optimizing $\\pi_P$ is $\\max_{\\pi_P\\in\\Pi} I_{\\pi_A} (\\pi_P;r) + J_{IRL}(\\pi_P;r)$. As for $\\pi_A$, we only use the standard RL objective function, i.e., $\\max_{\\pi_A\\in\\Pi} J_{IRL} (\\pi_A;r)$. Although the computational complexity equals the sum of the complexities of RL update steps for $\\pi_A$ and $\\pi_P$, these two RL update steps can be executed in parallel.\n6.2 Regret Maxmization with On-and-Off Policy Samples\nGiven the intermediate learned protagonist and antagonist policy $\\pi_P$ and $\\pi_A$, according to $MinimaxRegret$ in Eq.3, we need to optimize $r$ to maximize $U_r(\\pi_A) - U_r(\\pi_P)$. In practice, we found that the subtraction between the estimated $U_r(\\pi_A)$ and $U_r(\\pi_P)$ can have a high variance. To resolve this issue, we derive two reward improvement bounds to approximate this subtraction.\nTheorem 4. Suppose policy $\\pi_2 \\in \\Pi$ is the optimal solution for $J_{IRL}(\\pi; r)$. Then, the inequalities Eq.4 and 5 hold for any policy $\\pi_1 \\in \\Pi$, where $\\alpha = \\max_s D_{TV} (\\pi_1(\\cdot|s), \\pi_2(\\cdot|s))$, $\\epsilon = \\max_{s,a} |A_{\\pi_2} (s, a)|$, and $\\Delta A(s) = \\mathbb{E}_{a \\sim \\pi_1} [A_{\\pi_2} (s, a)] - \\mathbb{E}_{a \\sim \\pi_2} [A_{\\pi_2} (s, a)].$\n$U_r(\\pi_1) - U_r(\\pi_2) - \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{E}_{s(t) \\sim \\pi_1} [\\Delta A(s(t))] \\leq \\frac{2\\alpha\\gamma\\epsilon}{(1 - \\gamma)^2}$ (4)\n$U_r(\\pi_1) - U_r(\\pi_2) - \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{E}_{s(t) \\sim \\pi_2} [\\Delta A(s(t))] \\leq \\frac{2\\alpha\\gamma(2\\alpha + 1)\\epsilon}{(1 - \\gamma)^2}$ (5)"}, {"title": "Experimental Details", "content": "All experiments are carried out on a quad-core i7-7700K processor running at 3.6 GHz with a NVIDIA GeForce GTX 1050 Ti GPU and a 16 GB of memory. Network Architectures. Our algorithm involves a protagonist policy $\\pi_P$, and an antagonist policy $\\pi_A$. In our implementation, the two policies have the same structures. Each structure contains two neural networks, an actor network, and a critic network. When associated with GAN-based IRL, we use a discriminator D to represent the reward function as mentioned in Appendix B.4.\n\u2022 Protagonist and Antagonist policies. We prepare two versions of actor-critic networks, a fully connected network (FCN) version, and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version, the actor and critic networks have 3 layers. Each hidden layer has 100 neurons and a tanh activation function. The output layer output the mean and standard deviation of the actions. In the CNN version, the actor and critic networks share 3 convolutional layers, each having 5, 2, 2 filters, 2 \u00d7 2 kernel size, and ReLU activation function. Then 2 FCNs are used to simulate the actor and critic networks. The FCNs have one hidden layer, of which the sizes are 64.\n\u2022 Discriminator D for PAGAR-based GAIL in Algorithm 2. We prepare two versions of discriminator networks, an FCN version and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version has 3 linear layers. Each hidden layer has 100 neurons and a tanh activation function. The output layer uses the Sigmoid function to output the confidence. In the CNN version, the actor and critic networks share 3 convolutional layers, each having 5, 2, 2 filters, 2 \u00d7 2 kernel size, and ReLU activation function. The last convolutional layer is concatenated with an FCN with one hidden layer with 64 neurons and tanh activation function. The output layer uses the Sigmoid function as the activation function.\n\u2022 Discriminator D for PAGAR-based VAIL in Algorithm 3. We prepare two versions of discriminator networks, an FCN version and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version uses 3 linear layers to generate the mean and standard deviation of the embedding of the input. Then a two-layer FCN takes a sampled embedding vector as input and outputs the confidence. The hidden layer in this FCN has 100 neurons and a"}]}