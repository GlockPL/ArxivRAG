{"title": "On the Implicit Relation between Low-Rank Adaptation and Differential Privacy", "authors": ["Saber Malekmohammadi", "Golnoosh Farnadi"], "abstract": "A significant approach in natural language processing involves large-scale pre-training on general domain data followed by adaptation to specific tasks or domains. As models grow in size, full fine-tuning all parameters becomes increasingly impractical. To address this, some methods for low-rank task adaptation of language models have been proposed, e.g. LoRA and FLORA. These methods keep the pre-trained model weights fixed and incorporate trainable low-rank decomposition matrices into some layers of the transformer architecture, called adapters. This approach significantly reduces the number of trainable parameters required for downstream tasks compared to full fine-tuning all parameters. In this work, we look at low-rank adaptation from the lens of data privacy. We show theoretically that the low-rank adaptation used in LoRA and FLORA is equivalent to injecting some random noise into the batch gradients w.r.t the adapter parameters coming from their full fine-tuning, and we quantify the variance of the injected noise. By establishing a Berry-Esseen type bound on the total variation distance between the noise distribution and a Gaussian distribution with the same variance, we show that the dynamics of LoRA and FLORA are very close to differentially private full fine-tuning the adapters, which suggests that low-rank adaptation implicitly provides privacy w.r.t the fine-tuning data. Finally, using Johnson-Lindenstrauss lemma, we show that when augmented with gradient clipping, low-rank adaptation is almost equivalent to differentially private full fine-tuning adapters with a fixed noise scale.", "sections": [{"title": "Introduction", "content": "Stochastic Gradient Descent (SGD) is the power engine of training deep neural networks, which updates parameters of a model by using a noisy estimation of the gradient. Modern deep learning models, e.g. GPT-3 [Brown et al., 2020] and Stable Diffusion [Rombach et al., 2022], have a large number of parameters, which induces a large space complexity for their training with SGD. Using more advanced methods, which track various gradient statistics to stabilize and accelerate training, exacerbates this space complexity [Duchi et al., 2011]. For instance, momentum technique reduces variance by using an exponential moving average of gradients [Cutkosky and Orabona, 2019]. Also, gradient accumulation [Wang et al., 2013] reduces variance by computing the average of gradients in the last few batches, which simulates a larger effective batch size. All these methods suffer from high space complexity during training/fine-tuning time.\nAddressing the space complexity, some works try to reduce it by training a subset of parameters, and storing the information about only a portion of the existing parameters [Houlsby et al., 2019, Ben Zaken et al., 2022]. LoRA is such an algorithm, which only updates some of the parameter matrices (called adapters), by restricting their update to be a low-rank matrix. This low-rank restriction considerably reduces the number of trainable parameters, at the cost of limiting the optimization space of the adapter parameters. Another parameter-efficient training technique, called ReLORA [Lialin et al., 2023], utilizes low-rank updates to train high-rank networks to eliminate the constraint of LORA mentioned above. Similarly, the work in [Hao et al., 2024] identifies that the dynamics of LORA can be approximated by a random matrix projection. Based on this interesting finding, the work proposes to achieve high-rank updates by resampling the random projection matrices, while still enjoying the sublinear space complexity of LoRA.\nOn the other hand, from the lens of data privacy, the fine-tuning data often happens to be privacy sensitive. In such scenarios, Differentially Private (DP) fine-tuning algorithms have been used to provide rigorous privacy guarantees w.r.t the data. DP full fine-tuning runs DPSGD [Abadi et al., 2016] on the the fine-tuning data to update all the existing parameters in a model. However, due to the necessity of computing gradients and clipping them for every data sample, DPSGD also induces high space complexities, even worse than non-private full fine-tuning of all parameters. Despite this, DPSGD full fine-tuning provides rigorous privacy guarantees w.r.t the fine-tuning data.\nIn this work, we draw a connection between LoRA/FLORA and DP full fine-tuning the adapters. We show that the random projection existing in the dynamics of LoRA/FLORA is equivalent to injecting some random noise to the batch gradients coming from full fine-tuning adapters, which is very close to what DPSGD does for full fine-tuning adapters privately. We also quantify the variance of the injected noise, and show that it increases as the rank of adaptation decreases: the smaller the rank of adaptation, the larger the variance of the injected noise. Furthermore, in order to evaluate the closeness of this injected noise to Gaussian noise with the same variance, we bound the total variation (TV) distance between the distribution of the injected noise and the pure Gaussian noise used in DPSGD and show that this bound (dissimilarity) decreases as the rank used in LoRA/FLORA increases. Our derivations suggest that, although not being exactly the same, low-rank adaptation and DP full fine-tuning adapters are very close to each other in terms of their dynamics. This implies that, besides reducing the space complexity for task adaptation of language models, low rank adaptation can provide privacy w.r.t the fine-tuning data implicitly without inducing the high space complexity of DP full-fine tuning all parameters.\nThe highlights of our contributions are the followings:\n\u2022 We show that low-rank adaptation with LoRA/FLORA is equivalent to injection of some random noise into the adapters' batch gradients coming from their full fine-tuning (eq. (12)).\n\u2022 We find the variance of the noise injected into each row of the adapters' full gradient matrix, and show that it approaches a Gaussian distribution as the number of inputs of the adaptation layer and the adaptation rank increase (lemma 3.1).\n\u2022 We bound the total variation distance between the distribution of the injected noise and the pure Gaussian noise with the same mean and variance. The bound decreases as the number of inputs of the adaptation layer and the adaptation rank increase (lemma 4.2).\n\u2022 Finally, we show that the dynamics of low-rank adaptation is very close to DP full fine-tuning adapters, and when it is augmented with gradient clipping, they are almost the same. This implies an implicit connection between LoRA/FLORA and DPSGD: they are very close to DPSGD with a fixed noise scale, which depends on the adaptation rank, and the batch size used during fine-tuning (section 5)."}, {"title": "Dynamics of Low-Rank Task Adaptation", "content": "We start by studying the dynamics of low-rank adaptation, and restate some of the findings in [Hao et al., 2024]. In order to update a pre-trained adapter weight $W \\in \\mathbb{R}^{n \\times m}$, LoRA incorporates low-rank decomposition matrices $B \\in \\mathbb{R}^{n \\times r}$ and $A \\in \\mathbb{R}^{r \\times m}$, where $r < \\min\\{n, m\\}$, and performs the forward pass in an adapter layer as:\n$y = (W + BA)x = Wx + BAx$,"}, {"title": "Random Noise Injected by Low-Rank Adaptation", "content": "In this section, we present our analysis based on LoRA, which employs a fixed projection matrix $A^0$. Our analysis holds for various LoRA variants, including FLoRA. As illustrated in eq. (12), the parameter update after T rounds of stochastic gradient descent (SGD) is given by:\n$W + \\Delta BA^0 + \\Delta B\\Delta A = W - \\eta \\sum_{t=0}^{T-1} [(\\nabla_W L^t) A^0 A^0]$\n$= W - \\eta \\sum_{t=0}^{T-1} [\\nabla_W L^t - \\nabla_W L^t (A^0 A^0 - I_m)]$,"}, {"title": "Bounding the Distance to the Normal Law", "content": "Despite having proved lemma 3.1 when $m$ approaches infinity, yet we need to quantify the distance between the distribution of $q \\cdot (A^T A-I_m) \\in \\mathbb{R}^{1 \\times m}$ to the bona fide Gaussian distribution for limited values of $m$ in practical scenarios. In this section, we derive a closed form upper-bound for the total variation distance between the distribution of each element of $q \\cdot (A^T A - I_m) \\in \\mathbb{R}^{1 \\times m}$ and the Gaussian distribution with the same mean and variance.\nConsistent with the notations in Theorem A.4 in the appendix, suppose $X_1, ..., X_n$ are $n$ independent random variables with $E[X_i] = 0$ and $Var[X_i] = \\sigma_i^2 > 0$. Define $S_n = \\sum_{i=1}^n X_i$ and let $s^2 = \\sum_{i=1}^n \\sigma_i^2$. Assuming $Z_n = \\frac{S_n}{s}$, and having Lindeberg's condition satisfied (see theorem A.3 and theorem A.4), the normalized sum $Z_n$ has standard normal distribution in a weak sense for a bounded $n$. More precisely, the closeness of the cumulative distribution function (CDF) $F_n(x) = Pr\\{Z_n \\leq x\\}$ to the standard normal CDF\n$\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}} dt$,"}, {"title": "Connecting Low Rank Adaptation to DP with Gradient Clipping", "content": "Based on eq. (12) and our understandings from lemma 4.3, low rank adaptation (with rank r) of adapter parameter $W \\in \\mathbb{R}^{n \\times m}$ at time step t is equivalent to full fine-tuning it with the noisy stochastic batch gradients $\\nabla_W L^t = \\nabla_W L^t + N_t$, where $N_t \\in \\mathbb{R}^{n \\times m}$ is a noise-term with Gaussian-like distribution: $Pr\\{N_t \\neq z\\} \\in O(\\frac{1}{\\sqrt{mr}})$, where $z \\sim N(0, ||[\\nabla_W L^t]_{i,:||^2})$, and $[\\nabla_W L^t]_{i,:}$ is the $i$-th row of $\\nabla_W L^t$ ($1 \\leq i \\leq n$). Asymptotically, as $mr$ grows, i.e. the input dimension of the adaptation layer (m) increases or the adaptation rank increases ($r < m$), the distribution of noise element $N_t$ gets closer to $N(0, \\frac{r}{m}|[[\\nabla_W L^t]_{i,:||^2})$. In other words, low-rank adaptation adds noise to each row of batch gradient $\\nabla_W L^t$, and the standard deviation of the noise added to the elements of the row i is proportional to the $l_2$ norm of row i. This operation is very similar to what DPSGD [Abadi et al., 2016] does for adding noise to each element of the batch gradients w.r.t the adapter parameters: at the t-th gradient update step on a current adapter parameter W, DPSGD computes the following noisy batch gradient on a batch of size b:\n$\\nabla_W L^t = \\frac{1}{b} \\sum_{i \\in B_t} [clip(\\nabla_W L, c) + N(0, \\sigma_{DP}^2)]$,\nwhere $\\nabla_W L = clip(\\nabla_W L, c)$, c is a clipping threshold, and $B_t$ is the batch of samples at time step t. Also, $\\sigma_{DP} = c \\cdot z$, where $z$ is the noise scale determining the resulting privacy guaranty parameters. The main difference between the noise addition mechanism in low-rank adaptation (eq. (12)) and that in DPSGD (eq. (19)) is that DPSGD adds noise with a fixed variance $\\sigma_{DP}^2$ to all elements of the clipped batch gradient, and also there is no sample gradient clipping happening in low rank adaptation. In the following, we show that how this clipping can be introduced in low rank adaptation with almost no cost by using Johnson-Lindenstrauss Lemma. This also leads to the same noise variance for all elements. We first state a version of the lemma in the following.\nTheorem 5.1 ([Matousek, 2008], Theorem 3.1). Let m be an integer, $\\Delta \\in (0, 1]$, and $p \\in (0, 1)$. Also, let us set $r = \\Delta^{-2} \\log(\\frac{1}{p} + \\frac{m}{2})$. Let us define a random linear map $T: \\mathbb{R}^m \\rightarrow \\mathbb{R}^r$ by\n$T(x) = \\frac{1}{\\sqrt{r}} \\sum_{j=1}^m R_{ij} x_j, i = 1,...,r$,\nwhere the $R_{ij}$ are independent standard normal variables. Then for every $x \\in \\mathbb{R}^m$, we have:\n$Pr[(1 - \\Delta)||x|| \\leq ||T(x)|| \\leq (1 + \\Delta)||x||] \\geq 1 - p$.\nor equivalently\n$Pr[\\frac{||T(x)||}{(1 + \\Delta)} \\leq ||x|| \\leq \\frac{||T(x)||}{(1 - \\Delta)}] \\geq 1 - p$.\nThe theorem above directly relates to the random projection mapping $A^T$ observed in LoRA/FLORA: let us define the mapping T in theorem 5.1 to be $T(x) = x A^T$. Then we know that for a sample i in\na batch of samples with size b, $\\nabla_{B_t} L = T(\\nabla_{W_t} L^t)$. Therefore, if we clip a row l of $\\nabla_{B_t} L$ with a clipping threshold, it is almost equivalent to clipping the same row of $\\nabla_{W_t} L^t$ with the same clipping threshold. More precisely, let's fix $\\Delta$. Then, according to eq. (22), for every sample i in a batch $B_t$ and every row $l \\in [1, n]$, we have:\n$||[\\nabla_{B_t} L]_{l,:|| = (1 - \\Delta) \\sqrt{rc}} \\Rightarrow Pr[\\frac{(1-\\Delta)}{\\sqrt{rc}} \\leq \\frac{||[\\nabla_{W_t} L^t]_{l,:||}}{\\sqrt{rc}} \\leq \\frac{(1+\\Delta)}{\\sqrt{rc}}] \\geq 1 - p$,\nwhere $r = \\Delta^{-2} \\log(\\frac{1}{p} + \\frac{m}{2})$. Therefore, if the left condition is satisfied for all samples i in a batch of size b and all rows l, then with probability at least $(1 - nbp)$, the right bound holds for all samples i and rows l. Equivalently, we have the following :\n$||[\\nabla_{B_t} L]_{l,:|| = (1-\\Delta) \\sqrt{rc}} \\Rightarrow Pr[\\frac{(1-\\Delta)}{\\sqrt{nrc}} \\leq \\frac{||\\nabla_{W_t} L^t||_F}{\\sqrt{nrc}} \\leq \\frac{(1+\\Delta)}{\\sqrt{nrc}}] \\geq 1 - nbp$,\nfor all samples i in a batch of size b. In other words, if we clip all the rows of sample gradients $\\nabla_{B_t} L$ in a batch to have norm $(1 - \\Delta) \\sqrt{rc}$, then with probability at least $1 - nbp$, all the sample gradients $\\nabla_{W_t} L^t$ in a batch have bounded frobenious norm $\\sqrt{nrc}$. In that case, according to lemma 3.1, low-rank adaptation of LoRA/FLORA adds a random noise to each row of $\\nabla_{W_t} L^t$ based on the norm of the row. More precisely, low-rank adaptation adds a Gaussian-like noise with variance at least $\\frac{(1-\\Delta)^2}{(1+\\Delta)^2} (\\sqrt{rc})^2 = \\frac{(1-\\Delta)^2}{(1+\\Delta)^2} c^2$ to each element of the clipped sample gradient $\\nabla_{W_t} L^t$, whose frobenious norm was bounded in eq. (24). Also, according to lemma 4.3, the noise added to each element follows Gaussian distribution $N(0, \\frac{(1-\\Delta)^2}{(1+\\Delta)^2} c^2)$ with probability $w_g$, where $(1 - w_g) \\in O(\\frac{1}{\\sqrt{mr}})$."}, {"title": "Connecting LoRA/FLoRA to DPSGD Algorithm", "content": "As described above, when augmented with clipping of the rows of sample gradients $\\nabla_{B_t} L^t$ ($i \\in B_t$), the dynamics of LoRA/FLORA is very close to DPSGD. However, it is not exactly the same: first, the distribution that the injected noise is sampled from is not exactly the pure Gaussian $N(0, \\frac{(1-\\Delta)^2}{(1+\\Delta)^2} c^2)$. Second, as seen in eq. (24), the gradient clipping is probabilistic, while in DPSGD, the sample gradient clipping is deterministic, as if $p = 0$ in eq. (24). Despite this, we can think of an intuitive relation to DPSGD. If we assume that the noise distribution is very close to Gaussian distribution (i.e. $w_g \\approx 1$), and also $nbp \\ll 1$, then we can consider the following interpretation of the low-rank adaptation of LoRA/FLORA:\nWhen clipping all the rows of sample gradients $\\nabla_{B_t} L$ to have norm $(1-\\Delta) \\sqrt{rc}$, low-rank adaptation adds a Gaussian noise with variance at least $\\frac{(1-\\Delta)^2}{(1+\\Delta)^2} (\\sqrt{rc})^2/r = \\frac{(1-\\Delta)^2}{(1+\\Delta)^2} c^2$ to each element of the clipped sample gradients $\\nabla_{W_t} L^t$, whose frobenious norm is bounded by $\\sqrt{nrc}$. This is equivalent to having a noise scale $z \\geq \\sqrt{\\frac{b+c^2}{nr}} = \\frac{(1-\\Delta)}{(1+\\Delta)} \\sqrt{c^2} = \\frac{(1-\\Delta)^2}{(1+\\Delta)^2} \\sqrt{c^2}/r = \\frac{(1-\\Delta)}{\\sqrt{(1+\\Delta)^2 nr}}$ for each batch of size b. The DP privacy parameters $\\epsilon$ and $\\delta$ resulting from this noise scale, which can be found by using a privacy accountant, e.g. moments accountant [Abadi et al., 2016], depend on the used batch size ratio (ratio of the batch size b and the fine-tuning dataset size) and the number of steps T taken during fine-tuning.\nThe connection drawn above is an approximate, yet meaningful, connection between LoRA/FLORA and DPSGD, which provides a clear interpretation of what low-rank adaptation does. In fact, low-rank adaptation secretly approximates the mechanism of DPSGD during fine-tuning. Hence, we expect it to provide robustness against privacy attacks to the data used for fine-tuning large models. Indeed, such a behavior for low-rank adaptation has been observed implicitly in [Liu et al., 2024]."}, {"title": "Conclusion", "content": "In this study, we establish an implicit connection between low-rank adaptation and differential privacy. We show that low-rank adaptation can be viewed as introducing random noise into the gradients w.r.t"}, {"title": "Appendix for on the Implicit Relation between Low-Rank Adaptation and Differential Privacy", "content": "In this section, we mention some theorems, which we will use in our proofs.\nTheorem A.1 (Chi-Squared distribution: [Mood and Franklin, 1974], Section 4.3, Theorem 7). If the random variables $X_i, i = 1, ..., k$, are normally and independently distributed with means $\\mu_i$ and variances $\\sigma_i^2$, then\n$U = \\sum_{i=1}^k \\frac{(X_i - \\mu_i)^2}{\\sigma_i^2}$,\nhas a chi-squared distribution with k degrees of freedom: $U \\sim \\chi^2_k$. Also, $E[U] = k$ and $Var[U] = 2k$.\nThe theorem above states that sum of the squares of k standard normal random variables is a chi-squared distribution with k degrees of freedom.\nLemma A.2 (Raw moment of Chi-Squared distribution). Suppose $X \\sim \\chi^2_k$. Then, the $m$-th raw moment of X can be found as follows;\n$E[X^m] = \\prod_{i=0}^{m-1} (k + 2i)$\nProof. From the definition of Chi-Squared distribution with r degrees of reddom, U has the following probability density function:\n$f_X(x) = \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} x^{\\frac{k}{2}-1} e^{-\\frac{x}{2}}$\nTherefore, we have:\n$E[X^m] = \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} \\int_0^{+\\infty} x^{m-1} e^{-\\frac{x}{2}} dx=  \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} 2^{2+m-1} \\int_0^{+\\infty} u^{\\frac{k}{2}+m-1} e^{-u} du = \\frac{2^m \\Gamma(\\frac{k}{2}+m)}{\\Gamma(\\frac{k}{2})}= \\prod_{i=0}^{m-1} (\\frac{k}{2} + i)= \\prod_{i=0}^{m-1} (k + 2i)$.\nNote that the fifth equality directly results from the property of gamma function that for $z > 0$, $\\Gamma(1 + z) = z\\Gamma(z)$.\nTheorem A.3 (Classical Central Limit Theorem: [Billingsley, 1995], Theorem 27.1). Suppose that \\{X_i\\}_{i=1}^n, is an independent sequence of random variables having the same distribution with mean $\\mu$ and positive variance $\\sigma^2$. Define $S_n = \\sum_{i=1}^n X_i$ as their sum. Let $Z_n$ be defined by\n$Z_n = \\frac{S_n - n\\mu}{\\sqrt{n\\sigma^2}}$\nThen, the distribution of $Z_n$ approaches standard normal distribution as n approaches infinity."}]}