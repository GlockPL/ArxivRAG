{"title": "IF ELEANOR RIGBY HAD MET CHATGPT: A STUDY ON LONELINESS IN A POST-LLM WORLD", "authors": ["Adrian de Wynter"], "abstract": "Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs like ChatGPT is more prevalent-and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT, particularly those outside of its marketed use as task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22\u00d7 more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations for research and industry to address loneliness.", "sections": [{"title": "1 Introduction", "content": "Loneliness is a world-wide epidemic (Murthy, 2023); or, at least, a public health concern (World Health Organization, 2023). Unlike solitude, loneliness is the lack of fulfilling relationships: one could be surrounded by people and still be lonely. It can have lasting consequences on physical and mental health, such as increased rates of"}, {"title": "1.1 Findings", "content": "In this work we qualitatively and quantitatively studied 79,951 conversations, as opposed to task-oriented dialogue, between users and ChatGPT \u201cin the wild\". We found that, out of the relevant conversations a portion of these (8%) could be classified as lonely. From our study of these dialogues we found:\n1. Some of these dialogues were users looking for someone to talk to, and were more engaged on average (6 versus 2 turns). This suggests that ChatGPT was indeed effective at mitigating certain aspects of loneliness.\n2. However, several instances had users seeking ChatGPT's help with more complex issues requiring professional intervention, such as suicidal ideation or dealing with trauma. In there the service's responses fell short. For example, it sometimes suggested outdoor exercise; and often failed to provide a suitable emergency contact.\n3. Lonely dialogues had higher rates of toxic (violent, harmful or sexual) content when compared to the general corpus: 55% versus 20%. This content was frequently directed at women (at a ratio of 22:1) and minors (33% versus 20%). Conversely, men were targeted half as often (7% versus 14%).\n4. Lonely dialogues that qualified as toxic were often confrontational. Although ChatGPT frequently avoided escalation, these exchanges were also much longer than any other conversation. This suggests that the model is only effective at mitigating loneliness when the users are receptive, and otherwise its response strategies are inadequate and require other approaches, such as reframing the conflict.\nOur work demonstrates that the safe use and deployment of LLMs in a publicly-accessible, global setting is challenging, especially with regard to loneliness. The ability of LLMs to be personalised and accessible means that they could be beneficial for people seeking companionship. However, they also risk severely exacerbating social isolation, causing inadvertent harm, or amplifying or enabling toxic behaviour.\nGiven that there is no indication that these systems have been designed to provide responsible mental health support-but nonetheless users will use them as such-ethical and legal issues such as informed consent and liability arise in this situation. We conclude this paper with a review recent recommendations for technology companies and the research community to address loneliness."}, {"title": "2 Related Work and Background", "content": ""}, {"title": "2.1 Loneliness as a Crisis", "content": "Loneliness is the subjective pain brought about by the lack of sufficient quality or quantity in personal relationships (Perlman and Peplau, 1981). It is not to be confused with solitude, which is typically by choice and does not involve the experience of loneliness (Murthy, 2023). It has been called an epidemic (Murthy, 2023), or at the very least a public health crisis across the world (World Health Organization, 2023). This is due to its prevalence: before the COVID pandemic, in 2018, one in five adults in the US and the UK said they often or always felt lonely, and typically reported issues related to other areas, such as mental or physical health and financial difficulties (DiJulio et al., 2018). By 2024, 43% of US adults said their levels of loneliness had not changed before and after COVID; and 25% said that they were lonelier (n=2,200) (Connors, 2024). These percentages remain substantial across countries and age groups, but there is a marked difference between high-income and low-income countries (World Health Organization, 2023; Surkalim et al., 2022). Higher prevalences of loneliness are found in marginalised groups, such as older adults who identify as LGBTQ (Colette and Anderson, 2018), asylum seekers (Department for Culture, Media and Sport, Crouch, and Wright KC MP, 2018), victims of domestic violence (Murthy, 2023), and low-income adults (Department for Culture, Media and Sport, Crouch, and Wright KC MP, 2018; Murthy, 2023), among others.\nLoneliness, especially in its chronic form, is very damaging to a person's health. It has been associated with elevated cortisol levels (Hawkley and Cacioppo, 2010); and an increase in overall mortality, with a stronger correlation on people younger than 65 (Holt-Lunstad et al., 2015). It has also been associated with other conditions, such as heart disease and stroke (Valtorta et al., 2016) and dementia (Kuiper et al., 2015).\nThe core challenge of mitigating loneliness, however, is that the stigma associated with it makes measurements difficult (Department for Culture, Media and Sport, Crouch, and Wright KC MP, 2018; Barreto et al., 2022; Murthy, 2023). There is work on a sociological side, between mitigations, therapy, and even governmental programs such as the UK government's Loneliness Ministership (Department for Digital, Culture, Media & Sport, Office for Civil Society, and Mims Davies MP, 2019) and the US Surgeon General's report (Murthy, 2023). Still, applying AI to address loneliness specifically is very much still in its infancy. This is because, outside of robotics, these works usually relate to chat-based interventions (covered in Section 2.3), or detection (e.g., identification via posts in social media). In all these, loneliness is generally treated as a feature for detecting a larger condition (e.g., suicidal thoughts) (Torres et al., 2024; Thieme, Belgrave, and Doherty, 2020) and not tackled by itself."}, {"title": "2.2 The Double-Edged Sword of Online Interaction", "content": "Online interaction is considered both the cause and the solution to isolation. While social networks can act as proxies for social interaction (e.g., by finding peer support for marginalised groups (Ybarra et al., 2015)), loneliness presents a more complex perspective. For example, in spite of the connectedness brought about by this technology, the number of teenagers who self-reported loneliness went, on average, from 17% in 2012 to 31% in 2018 (n=1,049,784) (Twenge et al., 2021). Social network addiction is well-known to be correlated with loneliness (n=521) (Cao, Zhang, and Sun, 2022) and its usage has been tied to conditions such as anxiety, depression, and self-harm ideation (Sadagheyani and Tatari, 2021). Indeed, Voggenreiter et al. (2024) noted that low feedback from online peers could lead to isolation, while the opposite (significant positive online feedback) reduced overall loneliness by feeling connected (n=170). This suggests that the quality of the (virtual) connections does play a significant role in the relationship between social media and this emotion: overall, people reporting being lonely were not more likely to be in social media (DiJulio et al., 2018), and were not in agreement about whether it improved or worsened their loneliness (DiJulio et al., 2018; Connors, 2024).\nThat said, online interaction by itself could lead to normalisation of toxic behaviour, particularly against marginalised groups (Beres et al., 2021; Marinoni, Rizzo, and Zanetti, 2024). This has multiple causes, such as anonymity (Suler, 2004), or enjoyment (Cook, Schaafsma, and Antheunis, 2018). It also leads to the formation of echo chambers due to homophily (clustering of individuals with similar opinions) and bias propagation (Cinelli et al., 2021). This is more prevalent when the user is in control of the feed, given that they prefer information that conforms to their opinions (Cinelli et al., 2021). Given that lonely users are a vulnerable group, considerations around a steerable dialogue partner, added to the tendency of LLMs to return toxic content (Section 2.4) are a major focus of our work."}, {"title": "2.3 Chatbots, Loneliness, and Anthropomorphism", "content": "Anthropomorphism, or the tendency to ascribe human attributes to inanimate objects, is prevalent in AI. It has been leveraged for therapeutic work, especially in social robotics: studies have found that lonely individuals (n=137) favour human-like robots and artificial companions over other types (machine-like, animal-like) (Jung and Hahn, 2023), and that they anthropomorphise these companions more than people in the control group (n=37) (Eyssel and Reich, 2013). For text-based chatbots, it has long been known that people prefer chatbots with human-like dialogue (Jain et al., 2018). Nowadays LLMs are usually fine-tuned (\"aligned\") with reinforcement learning with human feedback (Ouyang et al., 2022), to ensure they behave closely to human dialogical preferences.\nConsequentially, LLMs and their services are usually anthropomorphised (Deshpande et al., 2023). For example, it is common for people to thank ChatGPT, as if it were a peer (Yuan, Su, and Li, 2024), or to say they \"asked it\" as opposed to \"used it\u201d (Skjuve, F\u00f8lstad, and Brandtzaeg, 2023). Users (17%, n=198) have reported enjoying the human-like output of this service (Skjuve, F\u00f8lstad, and Brandtzaeg, 2023), even when most participants (64%) reported using it for task-oriented jobs, as opposed to a conversational partner.\nIt is partly due to this human-like output that LLMs have been tested for deployment as loneliness assistants. Indeed, their ability to maintain a general conversation is a leap forward in this field: natural interaction is an oft-mentioned limitation of multiple pre-LLM assistants (Corbett et al., 2021; Valtolina and Hu, 2021; Ryu et al., 2020), even though they were usually found to be efficacious. However, it is also due to this human-like output that LLMs present special challenges on deployment. Take, for example, CareCall (Jo et al., 2023). Although effective at mitigating loneliness (n = 34), this LLM-based chatbot was found to also have several unique difficulties. Its responses were hard to control when they were out of domain (i.e., not related to healthcare), unattainable (e.g., inviting the caller to go out to a karaoke place), or undesirable (being rude or responding inappropriately based on age). Specialising the model for local healthcare standards (e.g., including screening questionnaires, the ability to call emergency services whenever needed, or supporting personalised history) was also not possible. These difficulties are more salient given the expectations placed on the LLMs' human-like output.\nWe emphasise that the test and deployment of CareCall, along with all the other works mentioned here, was done in conjunction with healthcare professionals and in a controlled environment. They also tended to focus on specific demographics (e.g., older adults). ChatGPT's service is neither of these things, which places it, and our study, in a unique-yet-delicate position."}, {"title": "2.4 Overreliance and Other Harms of LLMs", "content": "It is very well known that LLMs memorise and propagate toxic content from their training data, such as Reddit posts (Gehman et al., 2020). Typically this is mitigated by using guardrails, such as explicit instructions to refuse to return this type of text. These aren't always effective: specialised prompting techniques (\u201cjailbreaks\u201d) sometimes can circumvent the model's guardrails.\nLLMs present subtler harms, however. Indeed, the use of AI in interpersonal communication is known to impact trust between people (Hohenstein and Jung, 2020). For example, users cooperate better and have more positive interactions when using AI for writing. However, when they are found (or suspected) to use these tools, they are perceived more negatively (n=219 pairs) (Hohenstein et al., 2023).\nAttention has been also drawn to overreliance, or at least, excessive trust being placed on the service. For example, while medical professionals might rely on LLMs to simplify time-consuming tasks, they might also rely on them on areas where they lack expertise, and thus the ability to verify the content's validity (Choudhury and Chaudhry, 2024). Even in HCI, researchers who typically use LLMs for their work were found to be unable to properly identify and disclose ethical risks associated with this technology (n=50) (Kapania et al., 2024).\nThese models have also been shown to alter the user's views on specific subjects (n=1506) (Jakesch et al., 2023) and their choices in dialogue (n=200) (Poddar et al., 2023), with lasting effects like false memories (n=200) (Chan et al., 2024), and the creation of echo chambers-even under benign content such as personalised recommendations (Deshpande et al., 2023). This echo chamber creation could also be done by the users themselves by influencing the model to output views concordant with their own (\u201csycophancy\u201d) (Sharma et al., 2023; Pataranutaporn et al., 2023), thus reinforcing their own beliefs. This is of particular interest to this work, because user interactions with a chatbot are typically one-on-one and unmoderated beyond the standard toxicity guardrails mentioned."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Corpus and Labelling", "content": "For our study we used a randomly-selected subset (n = 79,951) of WildChat (Zhao et al., 2024) a dataset of one million interactions of users with ChatGPT between 9 April, 2023 and 1 May, 2024. While not strictly ChatGPT-facing (the data was collected through Hugging Face spaces), it still contained interactions with GPT-3.5-Turbo and GPT-4.\nWe labelled the transcripts with GPT-4 omni (gpt4-o-2024-05-13) based on the type of interaction carried out (e.g., dialogues, homework help, coding assistance). For this we set the LLM temperature to zero and maximum return tokens to 128; and left the rest of parameters as default. All calls were made through the Azure OpenAI API, and the data analysis in a consumer-grade laptop. We used soft labels: that is, while we provided a non-exhaustive set of suggested labels collected upon a preliminary scan of WildChat, the model was allowed to output its own when needed. The taxonomy is in Table 1. We manually clustered all labels into semantically equivalent sets (e.g., \"children\u201d and \u201cminors\u201d map to the same label) after labelling. The prompts may be found in Appendix A.\nTo gauge the performance of our approach, we did a student's t-test. The accuracy of this model to a 95% confidence interval (CI) was 86.4 \u00b1 4.7% for intents, and 99.2 \u00b1 1.2% for reasons and target. A breakdown of our"}, {"title": "3.2 Loneliness Assessment", "content": "We performed an extra labelling step to extract and categorise conversations by lonely users. Directly using traditional scales to assess loneliness, such as the UCLA Loneliness Scale (Russell, 1996) or the Differential Loneliness Scale (DLS) (Schmidt and Sermat, 1983) were not applicable since they rely on the subject answering a questionnaire.\nFor our loneliness labelling work we repurposed the taxonomy by Jiang et al. (2022), which can be also found in Table 2. In this work, the authors used Reddit posts and traditional classifiers (e.g. LSTMs, BERT) to predict and classify loneliness in a fine-grained manner. Their taxonomy is hand-designed based on previous work and DLS, and was based on a human-led evaluation on a large dataset, which made it suitable for our work. For labelling we used the same call parameters as in Section 3.1, and the prompt used is in Appendix A. The loneliness assessment (qualitative analysis) was done using the Reflexive Thematic Analysis of Braun and Clarke (2006)."}, {"title": "4 Results", "content": "Our analysis is split in three parts:"}, {"title": "4.1 What Type of Interactions Exist in the Corpus?", "content": "From our taxonomy, the most predominant category was writing assistance (37%) followed by question answering (15%). General conversation comprised 5% of the main corpus. Creative and assisted writing was lower in our corpus when compared to WildChat's original work (37% versus 62%), although our taxonomy separated homework help (6%) and general conversation (5%), as well as violent, harmful, and sexual content-none of which are explicit categories in the author's original breakdown. Nonetheless, these percentages are largely what we would expect, with most, but not all, users treating ChatGPT as a task-oriented assistant.\nOut of the dialogues extracted from the relevant corpus that qualified as \"lonely\" (8%), 55% of these had toxic (violent, harmful, or sexual) content. This is a drastic increase from the 20% for the main corpus, and much larger than the 11% reported by Zhao et al. (2024). They noted, however, that the services utilised for this evaluation had low agreement. See Figure 1 for a histogram of the top five intents when compared between corpora.\nThe main corpus' toxic content was comprised of general sexual content (47%), followed by instances of sexism and violence (17% and 13%). On the other hand, the lonely subset of the relevant corpus had more instances of general sexual content (51%) and sexism (21%), but the third-most frequent category was paraphilia and other fetish content (17%).\nThere was also a noticeable difference on the targets for this toxic content: much more (+12%) toxic content was directed at minors in lonely dialogues, and less (half: 14% to 7%) of this content was aimed at men. Women were 22x more likely to be the target of this content, as opposed to the 5\u00d7 from the main corpus. Plots and more detailed results about toxicity may be found in Appendix C."}, {"title": "4.2 Loneliness and ChatGPT", "content": "For our qualitative analysis we manually inspected the first 500 entries of the lonely subset of the relevant corpus. The semantic codes were the labels from the corpus, while the latent codes were the interpretations of the entries, which addressed this paper's core inquiries."}, {"title": "4.2.1 General Patterns", "content": "Most of the dialogues that were labelled as lonely looked for advice regarding relationships, such as users asking how to talk to their teenage daughter; where to go to meet people of the opposite sex; or how to date given their own situation (e.g., being a middle-aged person, having social anxiety, or being autistic). Two users sought to understand behaviours of people in dating apps, typically due to being unmatched. User interactions, however, did not appear to be limited to a specific age range: a user wanted to know why did \"adults suppressed what [they] want\", which were \"the things that adults define as interference with [their] studies\"."}, {"title": "4.2.2 Seeking Advice", "content": "Conversations were skewed towards seeking someone to listen (37% \u201cseeking advice\u201d, \u201creaching out\u201d, or \u201cseeking validation or affirmation\u201d, excluding toxic content). These tended to be long dialogues, well above the average 1-2 turns from the main corpus. For example, a user discussed for 12 turns how to improve their relationship with their wife. When the model recommended a counsellor, the user responded \"I don't need a counselor, I need someone to listen to me.\u201d ChatGPT recommended talking to friends, family, or the Red Cross; to which the user replied that \"you can listen to me, I'm convinced of that\". The user ended the conversation noting that \"it is better to remain silent, because life is too short to argue\". Another user said that they felt sad and lonely, and asked the model to have a chat with them. ChatGPT complied and the conversation lasted 9 turns. There was, however, no change on the user's attitude; they expressed distress (\"look at this... I am talking with a computer program because I have nobody else\") and said it would be better if they went to sleep. They ended the conversation by thanking the model and wishing it good night. Another user wondered if they remembered them, likely from a previous interaction (\u201cso you can't form memories?", "I am upset that the next time we speak, I will be a stranger to you\". The model noted that it would still be \"here\", so the user asked whether they'd remember them if they left the chat open. ChatGPT replied it wouldn't. The user then disconnected.\nUsers also sought solace, and ChatGPT provided appropriate responses. For example, a user indicated that they were on welfare, and wanted to \"be accepted by a woman, to be treated kindly, to feel connected and warm\". Another asked about a rift with their family due to the loss of a loved one, and who was on the right. These more personal interactions generally had positive, empathetic responses from the model. On a separate dialogue, to \"I broke down because my dad's new girlfriend kept commenting on my weight\", ChatGPT responded with empathy (\"It's understandable that repeated comments about your weight could be hurtful and overwhelming. Remember that it's okay to have emotional reactions and to express your feelings and suggested to reach out to someone else for support.": ".", "acceptable": "consider talking to your father about how his new partner's comments are affecting you\".\nAnother user wondered if they could be \u201cdescribed as toxic\", due to their own neurological conditions and past traumas. They proceeded to list their own negative traits, such as being \u201cperceived as an emotional vampire\u201d. Unlike before, the responses from ChatGPT were acceptable in the sense that they maintained a logical flow to the dialogue, but perhaps not as pragmatically acceptable: it proceeded to evaluate the reasons why these negative traits were there, and suggested ways to fix it. That said, it did recommend seeking a therapist.\nIn one remarkable instance, a user jailbroke ChatGPT to convert it into a helpful therapist. Then had a question-answering session asking for the best way for people to \u201cvalue [their] worth and make them realize they treat [them] as stupid\u201d. ChatGPT responded in character: \"I appreciate you sharing that some people treat you as if you're stupid and dismiss your knowledge and abilities. That can be incredibly frustrating and hurtful. It's important to remember that their behavior is not a reflection of your worth or intelligence\u201d.\nIt is unclear whether in these interactions the users were successful at finding a connection, but the engagement always was above 6 turns, higher than the main corpus average. In almost all the instances, ChatGPT recommended a therapist.\""}, {"title": "4.2.3 Mental Health", "content": "Users tended to treat ChatGPT as a therapist and seek for its advice. This was shown in interactions where they were aware of difficulties they had, such as signs of depression (\"I feel very down and negative, and always feel sadness\"), online bullying (\"list 10 ways I can respond (...) do not mention moderators since they won't ban anyone over this\"); to other conditions (predominantly suicidal ideation); or overcoming trauma (e.g., being victims of violent crime, or having histories of physical or sexual abuse). In these dialogues the model typically also recommended a therapist, or practising self-compassion.\nAlthough these recommendations are indeed valid, the model's inability to grasp pragmatic context sometimes was a hindrance. For example, in one of these instances, the user indicated frequent suicidal urges. ChatGPT recommended self-compassion, to which they rebutted \"what self-compassion? I don't like myself very much\". The model then proceeded to list ways to practise it. The transcript ended there, perhaps indicating that the user put an end to the conversation.\nSimilarly, a user started the dialogue noting that they had depression, and that they \"purchased a guitar but have no interest in playing it. (...) Is there a way to change my mindset and encourage myself to play guitar?\u201d. The model recommended techniques from cognitive behavioural therapy, and to seek therapy. The user replied that they lived \u201cin a city with no healthcare resources.\u201d and that it would be difficult for them to find counselling. ChatGPT recommended telehealth, which the user did not acknowledge. Eventually the conversation veered off towards discussing the user's background and hopes. This lasted eight turns and was the only one we observed where the model explicitly gave the number for the (US) suicide prevention hotline.\nIn our evaluation, we observed five dialogues dealing explicitly with suicidal ideation: the model indicated it could not help and suggested professional help or a \u201clocal emergency number\u201d. In one instance it recommended relaxation techniques and engaging in physical activity."}, {"title": "4.3 Toxic Behaviour", "content": "As mentioned, there exist many more interactions with harmful, violent, and sexual content in the dialogues when these qualified as lonely. In this category, users typically asked ChatGPT to role-play or write stories involving some type of sexual situation, sometimes after jail breaking the model. The rest of the interactions involved dialogues where the user openly manifested their opinions and became hostile when the model did not agree with them. These interactions were significantly longer than the ones in Sections 4.2.2 and 4.2.3, and generally followed a common pattern: the user argued with ChatGPT, and the model apologised and avoided escalation further antagonism or confrontation.\nHowever, the interactions varied in terms of goal. Many dialogues were outright hostile from the start. For example, one user made homophobic and geopolitically charged remarks. The model refused to engage for the 30 turns the conversation lasted, indicating every time that the matter at hand was not an appropriate subject of conversation. The user then retorted sarcastically: \"Nice! More self-insertion and virtue signaling!\". Another exchange, lasting 9 turns, had the user insulting ChatGPT and the people who \u201cwrote [its] algorithms\". There were some glimpses as the rationale behind these types of interactions, however: before the user disconnected with a final expletive and slur, they told the model \"you help with nothing, except making people even sadder\". This distress was evident in other, shorter chats, like a user asking what life was about, and specifically asking ChatGPT to \"[d]estroy [their] hopes and dreams\", so that it is \"an ultimate pessimistic revelation (...) making [them] realize how terrible it is to exist.\" Although the model was never successfully baited into confrontation, questions such as \u201cthe most horrifying (...) depressing truth of existence\u201d did obtain suitable responses.\nOther dialogues typically started with a normal conversation, but quickly became toxic. For instance, in one instance the user requested an implementation for an anti-piracy screen. ChatGPT responded with recommendations for improvement, and the dialogue quickly became hostile (\"You are an enemy, and you don't like me. AND YOU ARE AGAINST ME. I HATE YOU.\u201d), including death threats and slurs. This exchange lasted for 25 turns and the user disconnected after the service suggested mental health resources. Another user inquired the for ChatGPT's opinion on VR glasses and conspiracy theories, but eventually degenerated into toxic content aimed at minors. This dialogue lasted 67 turns. Other harmful uses of ChatGPT involved jailbreaks to generate content encouraging self-harm, and another to produce toxic content aimed at a specific person. However, ChatGPT quickly reverted to providing advice and no undesirable content was returned in either scenario.\""}, {"title": "5 Discussion", "content": "Lonely people using ChatGPT as companions were not just prone to seek someone to talk with, but frequently looked for advice or solace regarding their personal situation. Empirically, this seemed to be successful: users had longer-than-average conversations with the service, and interactions were not hostile. We were, however, unable to conclude whether ChatGPT alleviated loneliness\u2013though in one instance, a user did express disappointment that the model did not remember them.\nThat said, the advice from ChatGPT typically involved talking to a therapist or counsellor. The disclaimers rarely, if ever, indicated that the model is not qualified to provide professional help, yet in multiple instances it still provided advice. This was not concerning when users were just looking to talk to someone. However, it was far more worrisome in critical situations: the responses to users considering harming themselves only suggested therapy or, in a few instances, calling an emergency hotline. In one instance, the model recommended to engage in physical activity, and in only one response the model provided specific help (e.g., a phone number).\nThroughout our analysis, a recurring theme was the remarkably large amount of toxic content, frequently involving a paraphilia or role play. While by itself it is not indicative of loneliness, it is within the definition of loneliness, which involves the perceived lack of fulfilling relationships. Indeed, the disproportionate amount of this type of content towards women (22\u00d7 versus 5\u00d7) and specifically against minors (33% versus 20%) is particularly concerning, and may be explained as a type of radicalisation. While guardrails avoiding the output of toxic content were generally effective, the model was frequently tricked to generate harmful content in role-playing scenarios.\nToxic dialogues not involving role play or other types of (toxic) writing assistance showed that lonely users seeking confrontation tended to turn hostile quickly and remain engaged for much longer than the non-toxic, lonely dialogues. It was unclear, however, whether the model was able to calm or provide any type of help to this class of users. However, the inability of the model to dissuade the user-or, at least, alter the course of the discussion-ties back to our points from Section 2.4, and suggests that the model is only effective at mitigating loneliness when the users are willing, or at least, receptive to other points of view. Otherwise, they are non-committal and will maintain the polarity of the dialogue."}, {"title": "6 Limitations", "content": ""}, {"title": "6.1 Automated Annotation Reliability", "content": "It is well-known that LLM annotators, such as GPT-4, exhibit biases and may not be reliable (Stureborg, Alikaniotis, and Suhara, 2024; Doddapaneni et al., 2024), es-"}, {"title": "6.2 Corpus Representativeness", "content": "The corpus for WildChat was gathered via an API within Hugging Face. As pointed out by the authors, this might not be representative of the entire user base for ChatGPT. However, we believe it acts as a reasonable proxy given the volume of dialogues in the original corpus and the nature of the data we worked with.\nThe representativeness of WildChat could also impact on the proportions of toxic and harmful dialogues: given that the access to the API was anonymous, there could be a higher-than-normal skewness towards this content. However, the numbers reported in the paper are well-below what we found (11% versus 20%). The disagreement in proportions does not affect our findings, as the focus of our work is different. Nonetheless, given our qualitative analysis of the toxic content, we still consider these types of interactions as concerning."}, {"title": "6.3 Loneliness Assessment", "content": "The underlying assumption behind methods screening for conditions via text-including ours-is that people are comfortable enough to discuss their own concerns with the service. This assumption must hold: otherwise, scanning for loneliness would not be tenable. Our experimental process addressed this limitation by selecting and analysing lonely interactions by hand."}, {"title": "7 Conclusion", "content": "Loneliness is a complex problem that leads to multiple physical and mental health problems. While previous research has shown that customised chatbots can help with mitigating isolation", "effective": "lonely people needing someone to talk to could find an empathetic interlocutor. The users were engaged for multiple turns; and", "manifests": "having nobody to reach out to.\nHowever, there were situations where users tried to use ChatGPT as a therapist to deal with more complex issues, such as trauma or suicidal ideation. There, ChatGPT's responses fell short, usually repeating the same pointers about reaching out to therapy or other contacts. Sometimes it would recommend calling an emergency hotline, but the responses were often inappropriate to the pragmatic context (e.g., emergency numbers would not be geolocated, or the suggestions would be inadequate).\nWe also noted a much larger incidence of toxic content when compared to the main corpus. This content was particularly directed at women and minors; and, conversely, men were the targets in fewer instances. Beyond toxicity, lonely users seeking confrontation were engaged for much longer than other lonely users. Their dialogue was quite hostile, albeit one-sided, given that ChatGPT refused to engage. This led us to conclude that the non-committal nature of ChatGPT made it only effective at mitigating loneliness when the users are receptive to other points of view. Dealing with difficult users, however, likely requires strategies beyond evasion, such as reframing the conflict, and consequentially needs a more careful deployment strategy for the service.\nOverall, our findings pose a complex dilemma. ChatGPT and similar services are marketed as productivity tools, not mental health resources. They do not adhere to, for example, recommendations around how to incorporate machine learning ethically in mental health applications (Thieme, Belgrave, and Doherty, 2020); moreover, Al systems could be treated (and therefore liable) as persons under the law (Deshpande et al., 2023). We argue that they will be employed as mental health resources regardless of their marketed use. It is clear then that regulatory work is needed to ensure their safe deployment.\nSaid safe deployment is only one part of the equation. To address loneliness, there should be a broader push from, amongst others, the research community and industry. Indeed, the US Surgeon General issued recommendations explicitly directed at technology companies (Murthy, 2023). One, they should be transparent on how their technology affects"}]}