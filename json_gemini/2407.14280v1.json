{"title": "How to Blend Concepts in Diffusion Models", "authors": ["Giorgio Longari", "Lorenzo Olearo", "Simone Melzi", "Rafael Pe\u00f1aloza", "Alessandro Raganato"], "abstract": "For the last decade, there has been a push to use multi-dimensional (latent) spaces to represent concepts; and yet how to manipulate these concepts or reason with them remains largely unclear. Some recent methods exploit multiple latent representations and their connection, making this research question even more entangled. Our goal is to understand how operations in the latent space affect the underlying concepts. To that end, we explore the task of concept blending through diffusion models. Diffusion models are based on a connection between a latent representation of textual prompts and a latent space that enables image reconstruction and generation. This task allows us to try different text-based combination strategies, and evaluate easily through a visual analysis. Our conclusion is that concept blending through space manipulation is possible, although the best strategy depends on the context of the blend.", "sections": [{"title": "1. Introduction", "content": "The field of knowledge representation deals with the task of, indeed, representing the knowledge of a domain in a manner that can be used for intelligent applications [1]. Over the decades, most of the progress in the field has focused on logic-based knowledge representation languages, and their reasoning capabilities. In this setting, concepts\u2014the first-class citizens of any domain representation- are formalised by limiting the interpretations that they can be assigned to, and their connections with other concepts."}, {"title": "2. Preliminaries", "content": "Human creativity has always been the key to the innovating process, giving us the possibility to imagine things which are yet to be discovered, and diverging scenarios to explore. In recent years, the field of artificial intelligence (AI) has been revolutionized by generative models, which are capable of creating new and original contents by exploiting the countless examples these models have been trained on. Among the multiple variants and possibilities to exploit generative AI, diffusion models like Stable Diffusion [5], Dall-E, or Midjourney produce as output original images based on textual prompts or images given as input for the model. To provide clarity for this work, we introduce the fundamental concepts and components of Stable Diffusion and the notions of concept blending."}, {"title": "2.1. Stable Diffusion", "content": "Stable Diffusion (SD) is a text-to-image generative model developed by Rombach et al. in 2022 [4], which follows the typical architecture of diffusion models [6] comprising a forward and a backward process. In the forward process, a clean sample from the data (in this case, an image) is sequen- tially corrupted by random noise reaching, after a defined number of steps, pure random noise. In the backward process, a neural network is trained to sequentially remove the noise, thereby restoring the clean data distribution; this is the main phase intervening during image generation. The Stable Diffusion network architecture utilized during the backward phase is principally made up of (i) a Variational Autoencoder (VAE) [7], (ii) a U-Net [8], and (iii) an optional text encoder. The VAE characterizes SD as a Latent Diffusion Model, mapping images into a lower-dimensional space through an encoder, followed by a diffusion model to craft the distribution over encoded images. The images are then represented as points in the latent space (R\"). Afterwards, a decoder is needed to convert a point back into an image. The U-Net is composed of an encoder-decoder pair, where the bottleneck contains the compact embedding representation of the images. The encoder E maps the input samples according to the given prompt embedding into this latent embedding, then the decoder D processes this latent embedding together with its prompt embedding to reconstruct a sample that is as close as possible to the original one. The U-Net and text embedding are crucial in conditioning the output generated by the model. At each step of the denoising process, the prompt embedding is injected into the three blocks of the U-Net via cross-attention mechanism. In this way, the textual prompt conditions the denoising process and in turn the generation of an image. The prompt embedding is generated by the text encoder, following the pipeline of SD 1.4. In our experiments, as text encoder, we adopt a pre-trained CLIP Vit-L/14.\nWith these details we can now establish the focus of this study, summarizing it in the research question: can diffusion models produce visual blends of two concepts? Identifying each concept through a word, we want to create a new image that simultaneously represents a combination of both, simulating the human capacity for associative thinking. To address this problem, we present various methodologies leveraging SD as the backbone of our experiments. But before, we explain the notion of concept blending.\""}, {"title": "2.2. Concept Blending", "content": "Blending represents a cognitive mechanism that has been innately exploited to create new abstractions from familiar concepts [9]. This process is often experienced in our daily interactions, even during a casual conversation. This conceptual framework has been studied over the past three decades [10], offering a model that incorporates mental spaces and conceptual projection. It examines the dynamic formation of interconnected domains as discourse unfolds, aiming to discover fundamental principles that underlie the intricate logic of everyday interactions. In this context, a mental space is a temporary knowledge structure, which is dynamically created, for the purpose of local understanding, during a social interaction. It is composed of elements (concepts) and their interconnections. It is context-dependent and not necessarily a description of reality [11]. This general notion can be specified in different notions. For our purpose, we are interested in visual conceptual blending, which combines aspects of conceptual blending and visual blending.\nConceptual Blending is the operation that constructs a partial match between two or more input mental spaces, and project them into a new \u201cblended\u201d one [11]. This blended space has common characteristics of the input spaces, allowing a mapping between its elements and their counterparts in each input space. However, it also generates a new emergent conceptual structure, which is unpredictable from the input spaces and not originally present in them. Therefore, blending occurs at the conceptual level, yet representations of these blended concepts are valuable and frequently employed in advertising [12] and other domains [13].\nThe Visual Blending process, instead, is essential to generate new visual representations, such as images, through the fusion of at least two existing ones. There are two primary options for visual blending, according to the style of rendering employed: photo-realistic rendering and non-photo- realistic techniques, such as drawings. Approaches that focus on text-to-image generation have as main goal the visual representation of concepts, and, in the case of blending, the topology could to be summarized as a bunch of visual operations, as analyzed by Phillips and McQuarrie [14]. One of these operators, called fusion, partially depicts and merges the different inputs to create a hybrid image, allowing for a higher coherence between the object parts of the object(s), and helping the viewers in perceiving the hybrid object as a unified whole. In replacement, one input concept is present and its sole function is to occupy the usual environment of the other concept, or have its shape adapted to resemble the other input. Juxtaposition is a technique that involves placing two different elements side by side, to create a harmonious or provoking whole. A good example of Visual Blending along with"}, {"title": "3. Blending Methods with Stable Diffusion", "content": "In this section, we briefly review some of the existing approaches for blending concepts with diffusion models. Some of these methods were already published in previous work [18], while others are available on public implementations, but without a full description of their details. We mention explicitly whenever we are unsure if our implementation matches exactly the one proposed in the reference.\nExperimental setup We fix the generative network G as Stable Diffusion v1.4 [4] with the UniPCMul-tistepScheduler [19] set at 25 steps. This version uses a fixed pretrained text encoder (CLIP Vit-L/14 [20]). All the images are generated as 512x512 pixels with the diffusion process carried in FP16 precision in a space downscaled by a factor of 4. The conditioning signal is provided only in the form of textual prompts, and the guidance scale is set to 7.5.\nIn our study we focused on Stable Diffusion as a good trade-off between quality and computational cost; however, the blending methods that we analyzed can be implemented in other diffusion models with no latent downscale. Our entire implementation of the blending methods in their respective pipelines together with some of the generated samples is open source and available on our GitHub repository.\u00b9\nAn important feature of many generative methods, which allows them to produce varying outputs on the same prompt, is the use of a pseudo-random number generator (and pseudo-random noise) which can be established through a seed. Given an input textual prompt p, and a seed s, we denote as Is,p = G(s,p) the image generated by the model G given the input prompt p and the seed s. Prompts will be usually denoted with the letter p, sometimes with additional indices to distinguish between"}, {"title": "3.1. Blending in the Prompt Latent Space (TEXTUAL)", "content": "The first method examined in this study is the one recently proposed by Melzi et al. [18]. This approach exploits the relationship between conceptual blending and vector operations within the prompt latent space.\nAs depicted in the inset figure, given the two input prompts p\u2081 and p2, we first compute their la- tent representations p\u2081 and p\u00bd through the prompt encoder. Then, we define the blended latent vector as the Euclidean mean between p\u2081 and p. Finally, we generate the blended image by conditioning the Stable diffusion model with the estimated blended latent vector.\nIt is essential to underline that blending in the latent space representing the prompts does not correspond to blending directly the images, as in a visual blending process. Instead, it means generating an image representing a specific fusion of the concepts provided as the input textual prompts. Indeed, the Euclidean mean between the two representations is a (potentially unexplored) point of the latent space which intuitively represents the concept that is closest to both input concepts, thus defining an \"in-between\" characterisation.\nAlthough in this paper we only consider the mean of the two latent representations of the input prompts, we highlight that Melzi et al. consider also other linear combinations of p\u2081 and p\u00e5 to avoid fully symmetric constructions. A similar technique was implemented in the Compel open source library,\u00b2 which performs the weighted blend of two textual prompts."}, {"title": "3.2. Prompt Switching in the Iterative Diffusion Process (SWITCH)", "content": "This blending technique involves switching the textual prompt during the iterative process of the diffusion model. The inference process first starts with a single prompt p\u2081 and then, at a certain iter- ation, the prompt is switched to p2 until the end of the diffusion process. The generation is thus con- ditioned on both prompts leading to an image that, when the switch is executed at the right timestep, blends the two concepts. Intuitively, SWITCH starts by generating the general shape of p1, but then fills out the details based on p2 thus producing a visual blend of the two concepts.\nIt is crucial to choose the right iteration to switch the prompt. Unfortunately, this is an intrinsic challenge for each new image and does not depend only on the geometric distance between the p\u2081 and p embeddings. From our experiments, it was observed that the optimal iteration for this switch is directly related to the spatial similarity between the image generated by the model conditioned only on P1 and the one generated by p2.\nThis technique has also been implemented into the Stable Diffusion web UI developed by AUTO-MATIC1111.\u00b3 Among its numerous functionalities, this implementation allows prompt editing during the mid-generation of an image."}, {"title": "3.3. Alternating Prompts in the Iterative Diffusion Process (ALTERNATE)", "content": "Recall that in general diffusion models, at each timestep defined by the scheduler of the diffusion process, the noise in the sample is estimated by the U-Net model. This estimation is performed by the model with knowledge both of the timestep and the conditioning signal (i.e., the prompt).\nThe Alternating Prompt technique consists of conditioning the U-Net with a different prompt at each timestep. The first prompt p\u2081 is shown to the U-Net at even timesteps, while the second prompt p2 is shown at odd timesteps. By performing this alternating prompt technique, the diffusion pipeline can successfully generate an image that blends the two given prompts. Even though at different timesteps, the U-Net is conditioned by both prompts during the diffusion process. Moreover, the blending ratio can be controlled by adjusting the number of iterations in which each prompt is shown to the U-Net.\nOne can intuitively think of this approach as an alternating superposition of the generation process between p\u2081 and p2. This method is also implemented in the Stable Diffusion web UI developed by AUTOMATIC1111."}, {"title": "4. Method", "content": "In this section, we propose a different blending paradigm to visually combine two textual prompts in the diffusion pipeline. In a standard diffusion architecture, given a single input prompt p, its corresponding embedding p* is injected with a cross-attention mechanism in the three main blocks of the U-Net: the encoder, the bottleneck, and the decoder. During the encoding and bottleneck steps, the p* embedding is used to guide the compression of the input sample into a latent representation that accurately maps the concept p that is being generated. Then, during the decoding phase, the p* embedding is used to guide the reconstruction of the sample towards the distribution of the concept p that is being generated. Our idea arises from this compression and reconstruction operation and it is described in the following subsection. To the best of our knowledge, this method has not been proposed before."}, {"title": "Different Prompts in Encoder and Decoder Components of the U-Net (UNET)", "content": "We implement our new method only using text-based conditioning, but the method can theoreti- cally be extended to other conditioning domains. As we briefly describe above, the U-Net archi- tecture is composed of three main blocks: the en- coder, the bottleneck and the decoder. Each of these block receives the prompt embedding p* as input together with the sample from which the noise has to be estimated.\nThe key idea our method involves guiding the compression of the sample into the bottleneck block with a first prompt embedding p\u2081. Then, guide its reconstruction towards the distribution of the second prompt p2 by injecting into the decoder block the embedding p as visualized in the inset Figure. This allows the U-Net to construct a latent representation for the sample matching the concept described by P\u2081 and then reconstruct the sample with the features of the second prompt p2.\nThe expected result from this technique is to obtain an image that globally represents or recalls the concept described by p\u2081 and simultaneously shows some of the features that typically describe the concept of the second prompt p2. From our findings, changing the prompt embedding in the bottleneck block does not significantly affect the final result. Consequently, we keep the prompt p\u2081 in the decoder and bottleneck blocks while we change the prompt p2 in the encoder block."}, {"title": "5. Validation and Results", "content": "We now describe our experimental setting and analysis made to evaluate the four blending approaches presented in the previous sections, applied over two simple conceptual prompts. The outputs of these models can be visualized in Figures 2 and 3.\nThe experiments aimed to assess previously proposed blending methods across four distinct macro-categories, which are visually explained in Figure 2. The four categories are pair of animals, object and animal, compound words, and real-life scenarios. These were selected to showcase different kinds of blending of concepts, which are expected to showcase diverging properties. For pairs of animals, we expect that the shared characteristics between the concepts will aid the blending process; the use of object and animal concepts in the second category is expected to widen the semantic gap between the input prompts, but produce more \u201ccreative\u201d artifacts. The third category delved into objects representing compound words, offering a more conceptual blending challenge. Here, we observed how the methods responded to prompts comprising the compound's constituent parts, which are not literal descriptions of the target object but, rather are interpretable as a figure of speech or metaphor. We aimed to investigate whether the models should learn the necessary abstractness to perform a blending similar to the concept associated with the compound word, or reach a new visual blending that merges the characteristics of the two prompts. For the last category, drawing inspiration from real-world visual blend examples, regardless of their underlying concepts, we derived prompts to condition the models, allowing us to investigate their adaptability and ability to reconstruct well-known blends.\nUser Analysis\nTo impartially evaluate the quality of the methods, we conducted a survey involving 23 participants and presented 24 images, arising from the four categories described. The survey was constructed as follows. We first selected 24 concept pairs covering examples from the four macro-categories; each concept in the pair was described through a simple one-word prompt (except for \u201ckung fu\u201d). Then, the four different blending methods were used to generate the visual conceptual blend of each pair. All images were generated with the same size and quality, and presented to the users, with the instruction to rank them according to their blending effectiveness (from best to worst). Our participant pool was carefully selected to ensure they had no prior experience with blending theory.\nWhile the two prompts used to generate the images were provided to the subjects, we deliberately withheld information regarding the model responsible for each image, eliminating potential bias. Additionally, to further mitigate bias, the order of images within each question was randomized for each participant. This approach aimed to discern whether a superior blending method existed among the four proposed and whether certain methods outperformed others within specific categories.\nIt is essential to note that, for each question, the top four images proposed were selected by the authors from a pool generated using ten different seeds. Given that blending quality across all methods is not entirely independent of seed choice, we aimed to minimize this dependency by carefully selecting the best results. For a better understanding of the evaluation approach, Figure 2 shows some of the images that were presented to the subjects for ranking, along with the methods that produced them."}, {"title": "6. Discussion", "content": "Figure 3 shows the results of the four different blending methods with the prompts Frog-Lizard, Butter- Fly, Kung fu-Panda, Tortoise-Broccoli, and Tea-Pot. To better understand the behavior of each method, all images in each row were generated using the same seed and thus starting from the same random noise. Moreover, the blending ratio between the two prompts was kept constant at 0.5 across all methods.\nWe measure the visual distance of two concepts by visually evaluating the spatial similarity of the images generated conditioning the pipeline on them. This is a key aspect to consider when evaluating the quality of the blend as, with the exception of the TEXTUAL which instead focuses on the semantic blend, it influences the performance of the blending methods.\nWhen it comes to logical blends, one often considers a main concept which is modified by a secondary"}, {"title": "7. Conclusions", "content": "Through this paper we tried to answer a novel research question: is it possible to produce visual concept blends through diffusion models? We compared different possible solutions to force a diffusion model (more specifically Stable Diffusion [4]) to generate contents that represent the blend of two separated concepts. We collected three different alternatives from exiting publications and from the web. Additionally, we propose a completely new method, which we call UNET that exploits the internal architecture of the adopted diffusion model. We collected the outputs of the different methods on 4 different categories of test; namely, pairs of animals, animal and object, compound words, and real life"}]}