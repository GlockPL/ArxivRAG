{"title": "SERIALIZED SPEECH INFORMATION GUIDANCE WITH OVERLAPPED ENCODING SEPARATION FOR MULTI-SPEAKER AUTOMATIC SPEECH RECOGNITION", "authors": ["Hao Shi", "Yuan Gao", "Zhaoheng Ni", "Tatsuya Kawahara"], "abstract": "Serialized output training (SOT) attracts increasing attention due to its convenience and flexibility for multi-speaker automatic speech recognition (ASR). However, it is not easy to train with attention loss only. In this paper, we propose the overlapped encoding separation (EncSep) to fully utilize the benefits of the connectionist temporal classification (CTC) and attention (CTC-Attention) hybrid loss. This additional separator is inserted after the encoder to extract the multi-speaker information with CTC losses. Furthermore, we propose the serialized speech information guidance SOT (GEncSep) to further utilize the separated encodings. The separated streams are concatenated to provide single-speaker information to guide attention during decoding. The experimental results on Libri2Mix and Libri3Mix show that the single-speaker encoding can be separated from the overlapped encoding. The CTC loss helps to improve the encoder representation under complex scenarios (three-speaker and noisy conditions), which makes the EncSep have a relative improvement of more than 8% and 6% on the noisy Libri2Mix and Libri3Mix evaluation sets, respectively. GEncSep further improved performance, which was more than 12% and 9% relative improvement for the noisy Libri2Mix and Libri3Mix evaluation sets.", "sections": [{"title": "1. INTRODUCTION", "content": "Automatic speech recognition gets impressive performance with the development of deep learning [1-4]. The word error rate (WER) of ASR for single speaker conditions has achieved the level of human transcribers [4, 5], even when faced with many complex scenarios, such as noise [6-11]. However, compared to the additive noise and reverberation, the inference from other speakers, known as the cocktail problem, has a more severe effect on ASR [12-14]. As a result, ASR performance has dramatic degradation under multi-speaker scenarios [13]."}, {"title": "2. RELATED WORKS", "content": ""}, {"title": "2.1. Utterance-level Permutation Invariant Training (uPIT) for Multi-speaker ASR", "content": "uPIT-based multi-speaker ASR contains a shared mixed encoder, several speaker-differentiating (SD) encoders, a shared recognition encoder, a shared attention module, and a shared decoder. The mixed encoder transforms the overlapped speech feature Y to embedding H. Then, the extracted mixed embeddings are fed into several SD encoders. Each SD encoder only extracts one speaker information $H_{SD}$, s represents the s-th speaker. The shared recognition encoder transforms the representation of the s-th speaker from SD encoders to high-level representations for recognition $H_{Reg}$.\nFinding a definite relationship between the separated features and multiple speakers is difficult for multi-speaker processing. uPIT was proposed to solve this permutation issue by computing the smallest loss with all different estimation-label permutations. uPIT is adopted to compute the CTC losses between the encoding of the recognition encoder $H_{Reg}$ and its corresponding label $T^s$. An additional linear layer is inserted after the recognition encoder to compute the CTC loss and get the transcription $C^s$. The number of permutations between the recognition encodings and labels is S!, where S represents the speaker number. The loss function of CTC with uPIT is as follows:\n$L_{CTC-uPIT} = arg min_{\u03a0\u2208P} \u2211_{s=1}^S LOSS_{CTC}(C^s, T^{\u03c0(s)}), (1)$\nwhere P represents the set of all permutations of 1,..., S. \u03c0(s) represents the s-th element in a permutation \u03c0, and T is the set of transcription labels for S speakers. The recognition encodings $H_{Reg}$ are finally fed into the shared attention and the shared decoder to get the attention-based outputs $H_{Att}$. The loss of the decoder is cross-entropy (CE) with the same permutation as that of the CTC-uPIT:\n$L_{CE} = \u2211_{s=1}^S LOSS_{CE} (H^{Att}, T^{\u03c0(s)})$ (2)\nwhere $\u03c0(s)$ represents the permutation obtained from the CTC-uPIT. The final training loss for uPIT-based ASR is CTC-Attention Hybrid loss:\n$L_{UPIT} = \u03bbL_{CTC-uPIT} + (1 \u2212 \u03bb)L_{CE}. (3)$\n\u03bb is the hyperparameter to control the two losses."}, {"title": "2.2. Serialized Output Training (SOT) for Multi-speaker ASR", "content": "SOT is based on the attention-based encoder-decoder (AED) structure to solve the multi-speaker ASR, which is shown in Fig. 1-(a). It contains an encoder, an attention module, and a decoder. All three components cooperate during the decoding to output transcriptions of different speakers according to their start-time of speaking. The encoder transforms the overlapped speech feature Y to embedding H. Different from the uPIT-based ASR, the overlapped speech embedding is directly fed into the attention and decoder to get the transcriptions without any implicit or explicit separation. Training targets help the model achieve this ability. It innovatively arranges the transcriptions of different speakers according to their start-time of speaking to form a new transcription. A special symbol (sc) is inserted between different speakers to represent the speaker change. For example, for a two-speaker case, the label will be given as T = {t11, ..., $t_{N1}^1$, (sc), t12, ..., $t_{N2}^2$ }, where $t_1$ and $t_2$ represent the transcriptions of the 1-th and 2-nd speaker, respectively. The $N_1$ and $N_2$ represent the lengths of the transcriptions. Based on this training target, the attention mechanism is able to focus on the required information in encoding the overlapped speech and decoding the transcriptions of multiple speakers C according to their speaking time. The loss function of SOT-based ASR is simple as follows:\n$L_{SOT} = LOSS_{CE} (C, T), (4)$\nBecause the embedding obtained by the encoder does not perform any separation, it is hard to align the speech embedding with the label sequence by CTC. As a result, only CE loss is used when training the SOT-based ASR system."}, {"title": "2.3. Self-Supervised Learning-based Feature Extraction", "content": "Feature extraction with self-supervised learning (SSL) shows powerful performance improvement for ASR [8]. The outputs of the SSL module replace the traditional features like Mel-Frequency Cepstral Coefficients (MFCC) and Filter Bank (F-Bank). The SSL-based feature extraction module typically consists of convolution layers and transformer, e.g., WavLM [32], HuBERT [33]. With massive pretraining data, the SSL models already contained the capability to convert the speech waveform into the acoustic hidden units. Moreover, some SSL models [32, 33] demonstrate robustness against noise or other inference speakers by incorporating noisy simulation. The other parts of the ASR system are the same as the other E2E ASR systems."}, {"title": "3. PROPOSED METHOD", "content": "To improve the encoder's representation, we first propose the overlapped encoding separation (EncSep) to utilize the CTC-Attention hybrid loss in the SOT-based ASR. Then, we propose the single-speaker information guidance SOT (GEnc-Sep) to utilize the separated embeddings."}, {"title": "3.1. Overlapped Encoding Separation (EncSep)", "content": "In the SOT-based ASR, the encoder transforms the overlapped speech feature Y to embedding H. The embedding H here is overlapped by speakers. Thus, some module for speech separation is introduced to convert the overlapped encoding H into separated single-speaker encodings $H_{sep}$:\n$H_{sep} = Separator(H). (5)$\nThe Long Short-Term Memory (LSTM) [34] is adopted as the separator in this work:\n$\u0124 = LayerNorm(LSTM(H)),$\n$H_{sep} = ReLU(Linear(\u0124)), (6)$\nSeveral linear layers are used to generate the single-speaker information $H_{sep}$. Linear layers correspond to a speaker according to the start time of speaking in a serialized manner. Then, the CTC loss is computed as follows:\n$L_{CTC-EncSep} = \u2211_{s=1}^S LOSS_{CTC}(C^s, T^s)$\n$= \u2211_{s=1}^S Loss_{ctc}(Linear(H_{sep}^s), T^s) (7)$\nwhere Ts represents the s-th transcription arranged in the serialized manner.\nThe attention-based CE loss, same as Eqn. (4), is also used for training. The final loss function for training EncSep is as follows:\n$L_{EncSep} = \u03b3L_{CTC-EncSep} + (1 \u2212 \u03b3)L_{SOT}, (8)$\nwhere the \u03b3 is the hyperparameter to balance the two losses. The flowchart of the proposed EncSep training strategy is shown in Fig. 1-(b). The separator is only used to introduce CTC information during training. When decoding, EncSep maintains the same structure as the SOT-based method (Fig. 1-(a)). Thus, EncSep does not increase any computational cost during decoding from the SOT-based method."}, {"title": "3.2. Single-speaker Information Guidance SOT (GEnc-Sep)", "content": "EncSep only uses the separated embeddings $H_{sep}$ to introduce the CTC loss into the encoder. To further utilize the separated embeddings $H_{sep}$, we also propose the single-speaker information guidance SOT (GEncSep). Compared with the EncSep, the GEncSep utilizes the separated embeddings $H_{sep}$ from the separator, which is shown in Fig. 1-(c).\nGEncSep contains an encoder, which transforms the overlapped speech feature Y to embedding H. The separator separates the overlapped embedding H into single-speaker embeddings $H_{sep}^1$, ..., $H_{sep}^S$. Then, the separated embeddings are concatenated over time dimension as:\n$H_{con} = Concat(H_{sep}^1, ..., H_{sep}^S) (9)$\nThe attention mechanism is used to compute the attention weights with single-speaker information:\n$a_{con}^n = Attention (H_{con}, d^{n-1}). (10)$\n$a_{con}^n$ represents the context vector obtained with the attention mechanism from the concatenated embedding $H_{con}$. $d^{n-1}$ is the hidden state of the decoder. The decoder decodes from the attention feature and also the previously predicted tokens:\n$c^n = Decoder (H_{con}, a_{con}^n, c^{1:n\u22121}). (11)$\nThe corresponding output C is generated iteratively by the decoder. The training loss function of GEncSep is the same as Eqn. (8)."}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. Datasets and Experimental Settings", "content": "We used the LibriMix dataset [35] to evaluate the model performance. It used the train-clean-100, train-clean-360, dev-clean, and test-clean subsets from the LibriSpeech dataset [36] as the clean speech. The noise samples were taken from WHAM! dataset [37]. We used the official scripts\u00b9 to synthesize Libri2Mix and Libri3Mix. We used the offset file to make different speaking start times for multiple speakers. The two-speaker offset files followed the official ESPnet setting and the three-speaker offset files were created by ourself\u00b2.\nConformer [4] was used as the ASR back-end. Except for the proposed modules, the other modules followed the official ESPnet settings\u00b3. The baselines and EncSep used the original Transformer layer. For GEncSep, the structure of the decoder layer is shown in Fig. 1-(c). A character-based vocabulary was used with the size 32, including a (sc) for speaker change symbol. We used the WavLM-Large [32] for feature extraction, which has been shown effective for noise-robust ASR [38]. During training, all parameters of the WavLM-Large were frozen. In the proposed method, the input and output dimensions of the separator were 256 and 512; the number of LSTM layers were 2. The dimensions of linear layers were 512 and 256 for input and output, respectively. We also tried bidirectional LSTM layers. For bidirectional LSTM, the dimensions of linear layers were 1024 and 256 for input and output, respectively.\nFor baselines, the \"SOT\" followed the ESPnet official setting; only the attention CE loss was used with Eqn. (4). \"SOT-H\" was selected as another baseline. Different from the \"SOT\", it used the CTC-Attention hybrid loss instead of only Attention loss with the serialized label to train the model. Its neural network structure followed the ESPnet setting.\nThe training epochs were 60, and the final evaluation model was averaged over ten checkpoints, according to the loss in the validation set. It should be emphasized that pre-training is unnecessary for the two-mixed condition, but a few epochs of original SOT training are needed for the three-mixed condition. The all parameters of GEncSep was pretrained with EncSep."}, {"title": "4.2. Experimental Results", "content": "Table 1 shows the experimental results on noisy Libri2Mix and Libri3Mix sets. Compared with \u201cSOT\u201d, \u201cSOT-H\u201d had a significant performance degradation on both the development and evaluation sets of Libri2Mix with the CTC-Attention hybrid loss, which confirms that the encoder's implicit separation performed poorly (comparison between Exp. 1 and Exp. 2). With the separator, the proposed \"EncSep\" significantly improved from \u201cSOT\u201d (p-value < 0.01) for both the development and evaluation sets of Libri2Mix and Libri3Mix (comparison between Exp. 1 and Exp. 3). It was shown that the CTC losses with separator benefited the encoder representation since \"EncSep\u201d has the same structure as \"SOT\" during decoding. The bi-directional separator did not bring further improvement (comparison between Exp. 3 and Exp. 4). For \u201cGEncSep\u201d, the experimental results suggested that providing the separated embedding helped the decoding, significantly improving the performance on both the development and evaluation sets of Libri2Mix and Lib3Mix compared with \"SOT\u201d (p-value < 0.01, comparison between Exp. 1 and Exp. 5). Furthermore, the bi-directional separator showed significant improvement (p-value < 0.01, comparison between Exp. 5 and Exp. 6) on development sets of Libri2Mix and Libri3Mix, which were used to select the evaluation model.\nTable 2 shows the experimental performance on clean Libri2Mix and Libri3Mix sets. Similar to the noisy conditions, \u201cSOT-H\" had a significant performance degradation from \"SOT\" (comparison between Exp. 7 and Exp. 8). The \"EncSep\" did not show any improvement on Libri2Mix (comparison between Exp. 7 and Exp. 9). Compared with the noisy two-speaker condition, the \u201cSOT\u201d already had strong abilities for encoding the clean two-speaker features. CTC helped the encoder improve its encoding capabilities in more complex scenarios. The proposed \u201cEncSep\u201d still significantly improved the three-speaker conditions (p-value < 0.01, comparison between Exp. 7 and Exp. 9). The experimental results for \"GEncSep\" suggested that providing the separated embedding also helped the decoding under clean conditions, especially for three-speaker conditions (all the Libri2Mix and Libri3Mix evaluation sets are significantly improved from \"SOT\", p-value < 0.01, comparison between Exp. 7 and Exp. 12).\nWe compared several noise robust end-to-end ASR systems in the literature (after 2020) in Table 3. It should be emphasized that although the proposed method showed compelling performance in the end-to-end style for multi-speaker ASR, there is still a gap compared to the systems with pipeline systems (speech separation front-end with recognizer, TSE-Whisper) under noisy two-speaker conditions. Another possible reason for such a gap is that whisper's training data is massive, while other ASR back-ends only use the noisy Libri2Mix training set. For clean sets, the advantages of the front-end did not perform as well as the \"SOT-Conformer\u201d.\nCompared with \"SOT-Conformer\", the proposed GEncSep had significant performance improvement (p-value < 0.05). Compared with other methods, the SOT-based multi-speaker ASR system has advantages for more speakers.\""}, {"title": "5. CONCLUSIONS AND FUTURE WORKS", "content": "In this paper, we focused on improving the serialized output training (SOT) for ASR. We first proposed the overlapped encoding separation (EncSep) to fully utilize the benefits of the CTC and attention hybrid loss. The experimental results on Libri2Mix and Libri3Mix datasets show that the single-speaker encoding can be separated from the overlapped encoding. The CTC losses with separator benefit the encoder representation. Then, we proposed the serialized speech information guidance SOT (GEncSep) to further utilize the separated information. GEncSep further improved performance with the separated embeddings for decoding. Compared with the clean Libri2Mix and Libri3Mix, the proposed method has more significant advantages over the more complex, noisy Libri2Mix and Libri3Mix. As a result, the proposed GEnc-Sep had more than 12% and 9% relative improvement for the noisy Libri2Mix and Libri3Mix evaluation sets compared to the original SOT. In the future, we will make the system use different information sources by fusing the overlapped and separated embeddings."}]}