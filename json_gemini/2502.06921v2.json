{"title": "GraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units", "authors": ["Arghadip Das", "Shamik Kundu", "Arnab Raha", "Soumendu Ghosh", "Deepak Mathaikutty", "Vijay Raghunathan"], "abstract": "Graph Neural Networks (GNNs) are crucial for learning and reasoning over graph-structured data, with applications in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices, such as client PCs and laptops, enables real-time processing, enhances privacy, and reduces cloud dependency. For instance, GNNs can augment Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparse graphs, and dynamic structures lead to high latency and energy consumption on resource-constrained devices. Modern edge processors combine CPUs, GPUs, and NPUs, where NPUs excel at data-parallel tasks but face challenges with irregular GNN computations. To address these gaps, we present GraNNite, the first hardware-aware framework tailored to optimize GNN deployment on commercial-off-the-shelf (COTS) state-of-the-art (SOTA) DNN accelerators using a systematic three-step methodology: (1) enabling GNN execution on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance and energy efficiency gains. Towards that end, the first category includes techniques such as GraphSplit for workload distribution and StaGr for static graph aggregation, while GrAd and NodePad handle real-time updates for dynamic graphs. Next, performance improvement is acquired through techniques such as EffOp for control-heavy operations and GraSp for sparsity exploitation. For Graph Convolution layers, PreG, SymG, and CacheG reduce redundancy and memory transfers. The final class of techniques deals with quality vs efficiency tradeoffs QuantGr applies INT8 quantization to lower memory usage and computation time, while GrAx1, GrAx2, and GrAx3 optimize graph attention, broadcast-add, and sample-and-aggregate (SAGE)-max aggregation for higher throughput with minimal quality loss. Experimental evaluations on Intel\u00ae Core\u2122 Ultra Series 1 and 2 AI PCs demonstrate that GraNNite achieves speedups of 2.6\u00d7 to 7.6\u00d7 over default NPU mappings, with energy efficiency improvements up to 8.6\u00d7 compared to CPUs and GPUs. Across various GNN models, GraNNite delivers up to 10.8\u00d7 and 6.7\u00d7 higher performance than CPUs and GPUs, respectively. Our code implementation is available at this link.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph Neural Networks (GNNs) have become essential for learning and reasoning over graph-structured data, with applications in areas like network analysis, recommendation systems [1], and speech analytics [2]. Their ability to capture complex relationships through graph topology distinguishes them from traditional neural networks such as CNNs and LLMs. Recently, the inclusion of R-GAT, a prominent GNN model, in MLPerf's inference benchmarks emphasizes their growing importance in real-world applications. GNNs are particularly important compared to LLMs and newer architectures like State Space Models (SSMs) due to their ability to explicitly model relational and structural information, which is critical for tasks involving interconnected data such as social networks, molecular structures, and knowledge graphs [3]. While LLMs excel in sequential data processing and SSMs offer efficiency in modeling long-range dependencies [4], GNNs uniquely capture complex relationships through graph topology, making them indispensable for tasks where data is inherently non-Euclidean. Running GNNs on edge devices, including laptops and client PCs, has significant advantages. Edge-based inference ensures real-time processing, enhances data privacy, and reduces dependency on cloud infrastructure. For instance, GNNs can enhance Retrieval-Augmented Generation (RAG) for LLMs [5], [6], enabling efficient personal assistant applications. In addition, they are integral to event-based vision tasks [7], [8], allowing rapid processing of irregular data streams. The rising popularity of sensors in mobile devices further drives the deployment of GNNs to the wireless network edge for tasks such as sensing and interaction, including collision prediction in self-driving vehicles [9] and speech analytics [2]. These benefits make edge deployment crucial for achieving low-latency and energy-efficient solutions while addressing the growing demand for intelligent local inference. Despite their potential, deploying GNNs on resource-constrained edge devices presents challenges. Irregular memory access patterns, dynamic graph structures, and limited parallelism hinder computational efficiency. Sparse graphs exacerbate memory latency and lead to underutilized resources [1]. For example, deploying the DGCNN model on a Raspberry Pi 3B achieves less than 0.3 frames per second (fps), far below practical requirements [10]. Additionally, edge devices often rely on slower DRAM due to limited SRAM, resulting in high inference latency, increased energy consumption, and reduced battery life. These limitations highlight the need for optimized techniques to efficiently map GNN workloads onto edge platforms.\nModern edge processors, such as AI PCs from Intel, Qualcomm, and AMD, integrate heterogeneous computing units, including CPUs, GPUs, and NPUs, to efficiently support diverse AI workloads. Among these, NPUs or Neural Processing Units are specialized processors optimized for data-parallel operations, particularly matrix multiplication, which is the foundation of most neural network computations. NPUs typically include Data Processing Units (DPUs) [11] for parallelized matrix operations and Digital Signal Processors (DSPs) for sequential tasks like non-linear activation functions. NPUs are well-suited for AI workloads due to their high throughput and energy efficiency, outperforming traditional CPUs and GPUs. These advantages make NPUs ideal for continuous and resource-intensive GNN workloads on edge devices. However, the aforementioned challenges hinder their efficient utilization for GNN processing. Although prior research has proposed methods to optimize GNN processing [12], these efforts remain insufficient for real-time edge deployments [13], [14].\nTo address these gaps, we introduce GraNNite, a framework specifically designed to optimize the deployment of GNNs on NPUs, enhancing performance and efficiency. GraNNite leverages hardware-aware techniques to mitigate the challenges, ensuring scalable and efficient GNN execution on edge platforms. Modern GNNs primarily rely on three foundational layer types: Graph Convolution (GraphConv), Graph Attention (GraphAttn), and Sample and Aggregate (SAGE), which form the basis of architectures such as Graph Convolution Network (GCN) [15], Graph Attention Network (GAT) [16], and GraphSAGE [17] (Fig. 2). These layers were selected for our study as they address distinct challenges: GCNs capture local structure through neighbor averaging, GATs improve representation quality by assigning importance weights via attention mechanisms, and GraphSAGE enhances scalability by sampling neighbors for efficient large-graph processing. GraNNite optimizes these layers to achieve efficient execution by introducing a systematic 3-step methodology: (1) enabling GNNs on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance gains. While we evaluate GraNNite on GNNs using NPUs, the methodology is generic and can be extended to other models and hardware platforms without loss of generality. Our key contributions are:\n\u2022 Step 1: Enabling GNNs on NPUs. GraNNite introduces GraphSplit to optimize sequential and irregular compute tasks by assigning graph preprocessing to the CPU and parallelizable tasks to the NPU using an offline cost model, minimizing communication overhead. For static graphs, StaGr transforms node aggregation into matrix multipli-"}, {"title": "II. RELATED WORK", "content": "GNNs excel at structural tasks due to their ability to extract features from graph topology [1], yet they require substantial computational power [18]. As detailed in Section I, deploying"}, {"title": "III. BACKGROUND & MOTIVATION", "content": "Understanding the execution of GNNs involves analyzing their core computational stages: Node Embedding, Aggregation, Combination, and Decode [23]. Fig. 3 demonstrates this process using a GCN [15] as an example. The process begins with loading the graph structure and node embeddings via a data loader. Graph edges are typically represented as tuples of connected node indices. To enhance computational efficiency, the graph can be preprocessed into a structured format, such as an adjacency matrix. This binary matrix indicates edge connections and includes self-loops to incorporate node-specific features. Additionally, a normalization matrix is derived from node degrees to ensure a balanced computation. During the Node Embedding stage, raw graph data is converted into feature vectors that serve as inputs to subsequent stages. The Aggregation phase then collects features from neighboring nodes, leveraging operations such as pooling or reduction to capture relationships within the graph structure. However, this phase often incurs irregular memory access due to the variable number of neighbors. Next, the Combination phase applies neural transformations, such as fully connected layers or attention mechanisms, to the aggregated features, producing higher-level representations. Finally, in the Decode phase, these refined features are processed through layers like MLPs or SoftMax to generate predictions. The Aggregation and Combination phases (main GNN compute) are the most computationally intensive, as they are performed repeatedly throughout the model, emphasizing their critical role in GNN execution. This iterative nature underscores the need for efficient preprocessing and computational strategies to optimize performance.\nFig. 4 presents the latency breakdown for a single GraphConv and GraphAttn layer mapped out-of-the-box on the Intel\u00ae Core\u2122 Ultra Series 2 NPU. The breakdown highlights two major components: graph preprocessing and GNN compute (illustrated in Fig. 3), which includes operations such as combination and aggregation. Additionally, the figure provides a detailed view of how these components are distributed across the NPU's DPU and DSP units. It is evident from this breakdown that preprocessing plays a dominant role, contributing approximately 55% of the execution time in GraphAttn and nearly 99% in GraphConv. The preprocessing tasks, being control-flow heavy, are primarily executed on the DSP (relatively slower than DPU), further exacerbating the latency issue. Addressing this control-flow challenge is critical for improving GNN performance. In particular, GraphSplit, which is introduced in Section IV, is designed to mitigate this issue, optimizing preprocessing and enhancing overall execution efficiency. Fig. 5 further highlights the breakdown of GNN compute operations across different units of the NPU, with GraphConv benefiting from efficient matrix multiplication (MatMul) on the DPU. While this operation suits NPUs well due to their strength in data-parallel tasks, GraphAttn still presents opportunities for improvement. In particular, around 30% of the GNN compute execution time in GraphAttn is spent on operations such as Select, Greater, Softmax, and Elu, which are control-heavy and executed on the DSP. These control-flow-intensive sections are prime targets for optimization, which GraNNite addresses through EffOp, as discussed in Section IV. Additionally, GNNs benefit from sparse input graphs and do not require full precision (FP32) for compute. This opens up further"}, {"title": "IV. GRANNITE DESIGN METHODOLOGY", "content": "GraNNite provides an end-to-end framework (as shown in Fig. 6) for deploying pre-trained GNNs on NPUs without retraining. We consider an output-stationary NPU architecture inspired by Ref. [11]. The core component is the DPU, an M\u00d7M grid of Versatile Processing Elements, each comprising an N\u00d7N array of MAC Processing Elements (MPEs) designed for efficient Multiply-and-Accumulate (MAC) operations. This DPU is well-suited for operations like matrix multiplication, which are fundamental to many neural network computations. The architecture includes a local SRAM for storing activations and weights, a tensor distribution network for data flow to and from the DPU, and control logic for managing computation, accumulation, and output extraction. MAC operations, integral to DNNs, calculate dot products of weights and activations to produce output feature maps. Each MPE leverages a local data path with register files, multipliers, and accumulators to perform these tasks. Additionally, a DSP handles non-linear activation functions and control-flow operations, complementing the data-parallel DPU. Although our case study considers an output-stationary NPU architecture, the proposed techniques are generic and can be applied to other NPUs without loss of generality. GraNNite proposes a generic step-by-step methodology (Fig. 7) to optimize emerging neural networks on existing AI accelerators. While demonstrated on GNNs using FlexNN-like [11] NPUs, the methodology is generalizable to other models and hardware platforms. It consists of three key steps:\n(1) Enabling the Model on the NPU. This step ensures the model runs efficiently on the NPU while maintaining flexibility. For GNNs, GraNNite introduces workload partitioning (GraphSplit), precomputed static graph processing (StaGr), and dynamic graph handling (GrAd and NodePad) to support real-time updates and adaptive memory management. These techniques enable execution with minimal overhead. (2) Optimizing GNN Performance. Once enabled, the model undergoes further optimizations to maximize efficiency without degrading accuracy. EffOp accelerates execution and reduces memory bandwidth usage, while PreG, SymG, and CacheG optimize memory access for Graph Convolution layers. GraSp exploits sparsity to lower memory and compute costs, improving throughput and energy efficiency. (3) Trading Accuracy for Performance and Energy Gains. For applications prioritizing speed and efficiency over quality, GraNNite offers QuantGr for INT8 quantization and approximation techniques (GrAx1, GrAx2, GrAx3) to further enhance throughput with minimal quality loss. These steps provide a systematic framework for deploying GNNs efficiently on NPUs, addressing resource constraints while ensuring scalability, performance, and energy efficiency.\nA. Step-1: Enabling GNNs on the NPU\nGraphSplit: To enable efficient execution of GNNs on NPUs, the first challenge is to address the mismatch between the hardware's strengths and the computational demands of graph-based workloads. NPUs excel at data-parallel tasks like matrix multiplications in neural networks, but are less efficient for control-heavy tasks involving frequent decision-making. CPUs, on the other hand, excel at these control-intensive tasks, using techniques such as predictive execution and out-of-order processing to maximize instruction-level parallelism. Given these contrasting strengths, one might assume it's best to offload all control-heavy tasks during GNN inference, such as computing initial masks (i.e., preprocessing in Fig. 4) for aggregation or calculating intermediate attention scores, to the CPU. However, a challenge arises when control-flow tasks exhibit a Read-after-Write (RAW) dependency on previous data-parallel tasks, necessitating the transfer of data back to the CPU. This results"}, {"title": "B. Step-2: Optimizing GNN Performance on NPU", "content": "EffOp: After enabling GNNs on the NPU, the next challenge lies in optimizing their performance without compromising application quality. A significant bottleneck arises from the control-heavy operations, such as conditional logic, Select, or Gather, residing deep in the GNNs being executed on the DSP within the NPU (as shown in Fig. 5). The DSP is designed for these operations, but runs at a lower frequency than the DPU. This difference often causes bottlenecks and increases latency in deep, sequential GNN sections. To address this limitation, GraNNite proposes a novel approach, EffOp, that converts these control-heavy operations into equivalent data-parallel tasks, allowing them to be executed on the faster DPU rather than the DSP. The core idea is to restructure sequential tasks, such as Select and Gather, to be processed as simple, elementwise/reduction operations on the DPU. By redefining these tasks using operations like multiplication and addition, combined with precomputed masks, we transform inherently sequential processes into parallel-friendly ones. This allows the DPU to handle tasks that would traditionally rely on the slower DSP, reducing the need for sequential processing and, consequently, lowering overall execution time. As shown in Fig. 12, this method is particularly beneficial for operations in Graph Attention Networks, specifically in sections where intermediate attention scores are computed. EffOp demonstrates how this computation can be achieved using elementwise multiplication, followed by elementwise addition with a slightly modified \"connectivity mask.\" In EffOp, tasks that typically involve complex control logic are optimized to utilize the DPU's strengths, transforming them into matrix and elementwise operations that can be efficiently parallelized.\nGraSp: In the context of GNN optimization on NPUs, activation sparsity offers a powerful mechanism to significantly boost performance by skipping unnecessary calculations. Given that input graphs are often highly sparse, with up to 99% of values being zero, GraNNite leverages this sparsity to optimize both memory usage and computational efficiency. The adjacency matrix in real-world graphs typically exhibits this extreme sparsity, containing many zero-valued entries where no direct connection exists between nodes. By capitalizing on this inherent sparsity, NPUs [25], [26] can streamline computations by skipping zero values, reducing the workload without affecting the accuracy of model inference. To efficiently manage these sparse values, GraNNite proposes GraSp, which utilizes a storage format known as Zero Value Compression (ZVC) [27]. In this approach, only the non-zero values in the input graphs are stored explicitly, while the zero values are omitted, allowing the system to allocate memory and computational resources effectively. For GraSp implementation, sparsity bitmaps are used alongside compressed data to denote the locations of non-zero values within the matrix. This bitmap directs the NPU to focus solely on meaningful data while bypassing zero entries. Fig. 13 illustrates how sparsity bitmaps are integrated within the NPU's processing pipeline to skip zero-valued computations leading to latency speedup.\nPreG, SymG & CacheG: GraNNite presents a streamlined approach tailored for GNNs that use GraphConv layers as core components. PreG leverages a precomputed, constant normalization matrix to accelerate processing. Since GraphConv is foundational and commonly used in more advanced GNN architectures, this enhancement offers broad applicability and efficiency gains. In GraphConv, the normalization factors required after neighborhood aggregation depend solely on the degrees of neighboring nodes, not on their specific features. Here, a node's degree represents the count of its neighboring nodes, including itself. By exploiting this feature independence, PreG precomputes these normalization factors once, storing them in a constant matrix (refer Fig. 14). By precomputing the normalization matrix on the CPU, the aggregation and normalization steps are combined into a single matrix multiplication. This approach leverages the NPU's strength in matrix multiplication while avoiding the slower DSP unit, which typically handles division, thus streamlining execution and optimizing performance. In addition, GraNNite introduces a memory-efficient technique (SymG) that capitalizes on the symmetry of the normalization matrix (see Fig. 15) in GraphConv layers, allowing only half of the adjacency matrix and its diagonal elements to be stored. This optimization effectively reduces memory complexity, translating into substantial savings in memory usage. SymG also minimizes memory traffic, especially during Direct Memory Access (DMA) transfers from DRAM to NPU's local memory, which can be a bottleneck. Finally, GraNNite introduces CacheG that caches the constant normalization matrix within the NPU's local memory and reuses it across all GraphConv layers in the model, significantly reducing memory access overhead. This caching strategy not only boosts runtime efficiency, but also lowers inference latency, making the overall execution of GNNs on the NPU more streamlined and resource-efficient."}, {"title": "C. Step-3: Trading Accuracy for Performance & Energy Gains", "content": "QuantGr: In the pursuit of optimizing GNN performance and energy efficiency, we first explore traditional methods of reducing model precision before introducing GraNNite's novel approach. QuantGr, a quantization technique for GNNs is integrated in GraNNite that reduces numerical precision to achieve significant performance gains while preserving model accuracy. NPUs, typically designed with low-precision capabilities, support both INT8 and FP16 datapaths, allowing notable performance gains over traditional FP32 computation. Specifically, INT8 precision provides a 2\u00d7 boost in performance (TOPs) and a 4\u00d7 improvement in performance per watt (TOPs/Watt) compared to FP16. By carefully configuring the quantization parameters, such as setting an optimal zero point and scale, QuantGr can achieve competitive accuracy at lower precision. QuantGr uses symmetric, static quantization, meaning both weights and activations are quantized around a zero point, with equal scaling factors for positive and negative values. Static quantization, which precomputes scaling and zero-point parameters during model calibration, enables consistent and faster inference, as these values remain fixed throughout execution. Symmetric quantization simplifies processing by ensuring consistent scaling and compatibility across all hardware layers, minimizing conversion overhead. Leveraging the NPU's support for static quantization of activations and weights, this approach unlocks higher efficiency for GNNs, making it well-suited for performance-sensitive, resource-limited environments.\nGrAx1: Application of all previously discussed techniques in GraNNite can significantly improve GNN performance and energy efficiency on NPUs compared to the initial out-of-the-box implementation. However, further improvements can be achieved through approximate computing. This approach trades off minimal DNN accuracy for better computational efficiency, enabling faster processing and reduced resource usage [28], [29]. GNNs with Graph Attention (such as GAT) are well-regarded for their ability to generate attention maps that assign varying importance to nodes within a graph. However, these networks face significant computational challenges, particularly in managing non-existent edges. To prevent these edges from influencing the final attention values, they are typically masked by assigning them a large negative number before being processed through a SoftMax function. This masking step effectively ensures that attention coefficients for non-existent edges are rendered negligible during the aggregation phase. In GAT implementation with EffOp, an element-wise multiplication is performed between the attention map and the mask to eliminate the influence of non-existent edges. However, this multiplication is computationally intensive and not well-suited for the DPU. To mitigate this inefficiency, GraNNite proposes a novel approximation technique (GrAx1). Instead of multiplying the attention map by the mask, it suggests directly adding a large negative value to the positions in the attention map that correspond to non-existent edges. This modification effectively bypasses the multiplication step (as shown in Fig. 16), leading to a substantial reduction in computational burden on the DPU. As a result, throughput is increased without sacrificing the final attention map quality.\nGrAx2: Another significant bottleneck in GAT arises during the broadcast-add operation (refer Fig. 5), which is essential for calculating the intermediate attention map. The traditional implementation of the \u201cbroadcast-add\u201d operation requires adding the same value to multiple nodes, a process that involves broadcasting and transposing the data (refer left of Fig. 17). This step can lead to inefficiencies when executed on the DPU, hindering overall performance. To address this inefficiency, GraNNite proposes another novel approximate solution (GrAx2) that replaces the conventional \"broadcast-add\" operation with just an addition followed by broadcasting. As shown in Fig. 17, this approach eliminates one transpose and one broadcast operation, significantly reducing the computational load for addition and minimizing memory copy/reference operations, which enables the DPU to execute it more efficiently.\nGrAx3: For GNNs using the \"Sample and Aggregate\" (SAGE) layers with a \u201cmax\u201d aggregation strategy, the feature selection for each neighborhood is traditionally processed sequentially on the DSP, leading to inefficiency. GrAx3 replaces this sequential operation with parallel element-wise multiplication using a mask (sampled adjacency matrix), followed by max pooling on the DPU. As shown in Fig. 18, GrAx3 simplifies the aggregation process, ensuring the correct aggregation of neighborhood features for most cases where feature values are greater than 0."}, {"title": "V. EXPERIMENTAL METHODOLOGY", "content": "As illustrated in Fig. 19, we evaluated GNNs on Intel\u00ae NPUs using the Cora dataset (2,708 nodes, 5,429 edges, 7 classes, 1,433 features) and Citeseer dataset (3,327 nodes, 4,732 edges, 6 classes, 3,703 features) for node classification. The bench-"}, {"title": "VI. RESULTS", "content": "This section highlights the benefits of GraNNite optimization techniques, compares performance between Intel\u00ae Core\u2122 Ultra Series 1 & 2 NPUs, and demonstrates the superior energy efficiency of NPUs over CPUs and GPUs for GNN execution. Since GraNNite is the first hardware-aware framework tailored for optimizing GNN deployment on COTS SOTA NPUs, no existing works exist for direct comparison.\nBenefits of GraNNite Optimizations: Fig. 20 illustrates the performance progression of GNN models on the Intel\u00ae Core\u2122 Ultra Series 2 NPU, highlighting the impact of various optimizations proposed by GraNNite. Each optimization builds upon the preceding set unless otherwise specified. For example, the performance of QuantGr in GCN reflects a model in which GrAd, NodePad, GraphSplit, and QuantGr are cumulatively applied. However, in SAGE-max, EffOp and GrAx3 target the same model, and their performance gains are not cumulative. For GCN, the initial optimization, StaGr combined with GraphSplit, achieves a 1.51\u00d7 speedup over the baseline by efficiently partitioning workloads between the CPU and NPU. Adding GrAd and NodePad introduces support for time-varying graphs and enhances parallelism but reduces performance to 1.4\u00d7 due to CPU preprocessing overhead and an increased node count on the NPU. GraSp further boosts throughput by 1.1x. The most significant improvement, 2.7\u00d7, is achieved by combining GrAd, NodePad, GraphSplit, and QuantGr, leveraging low-precision arithmetic to minimize computational overhead. For GAT, EffOp alone provides a 3\u00d7 speedup, while incorporating approximations (GrAx2) boosts performance to 7.6\u00d7 with negligible impact on model quality. Similarly, for SAGE-max, EffOp yields a 2x speedup, which increases to 3.2\u00d7 with GrAx3, again with no quality degradation. We note that the effects of SymG and CacheG could not be demonstrated as they require modifications to the (proprietary) NPU compiler.\nPerformance Comparison on Intel\u00ae Core\u2122 Ultra Series vs. Intel\u00ae Core\u2122 Ultra Series 2 NPUs: Fig. 21 compares GCN performance across GraNNite optimizations on Intel\u00ae Core\u2122 Ultra Series 1 and Intel\u00ae Core\u2122 Ultra Series 2 NPUs. Series 2 consistently outperforms series 1 due to its higher tile count (4 vs. 2). For the most optimized configuration (GrAd + NodePad + QuantGr), Intel\u00ae Core\u2122 Ultra Series 2 delivers 1.7\u00d7 and 1.6\u00d7 higher throughput than Intel\u00ae Core\u2122 Ultra Series 1 for the Cora and Citeseer datasets, respectively. This advantage arises from the higher number of MAC units in Series 2, enabling greater data parallelism. However, the observed gains fall short of the theoretical 2\u00d7 maximum due to limited parallelism inherent in the GCN.\nPerformance and Energy Efficiency of CPU, GPU, and NPU with GraNNite Optimizations: Fig. 22 compares the"}, {"title": "VII. CONCLUSION", "content": "This work presents GraNNite, a framework that optimizes GNN execution on NPUs using a three-step methodology. It addresses challenges like irregular memory access, dynamic graph updates, and control-heavy operations through hardware-aware optimizations. By improving parallelism, memory efficiency, and low-precision computation, GraNNite reduces overhead, latency, and energy consumption while preserving accuracy. These enhancements enable real-time GNN execution for applications such as knowledge graph queries and event-driven analytics. Experimental evaluations on Intel\u00ae Core\u2122 Ultra Series 1 and 2 AI PCs show that GraNNite outperforms out-of-the-box NPU mappings and achieves significant energy efficiency gains over CPUs and GPUs. Its optimizations require no hardware modifications, ensuring scalability across diverse edge and accelerator platforms."}]}