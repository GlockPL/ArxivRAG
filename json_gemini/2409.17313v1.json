{"title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation", "authors": ["Zehao Wang", "Minye Wu", "Yixin Cao", "Yubo Ma", "Meiqi Chen", "Tinne Tuytelaars"], "abstract": "This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context-free grammar (CFG) of the task. The CFG serves as the basis for the problem decomposition and the core premise of the instruction categories design. We propose a semi-automatic method for CFG construction with the help of Large-Language Models (LLMs). Then, we induct and generate data spanning five principal instruction categories (i.e., direction change, landmark recognition, region recognition, vertical movement, and numerical comprehension). Our analysis of different models reveals notable performance discrepancies and recurrent issues. The stagnation of numerical comprehension, heavy selective biases over directional concepts, and other interesting findings contribute to the development of future language-guided navigation systems.", "sections": [{"title": "1 Introduction", "content": "In the Vision-Language Navigation (VLN; Anderson et al. 2018) task, an agent is instructed to navigate through virtual environments by following detailed natural language instructions. This task requires an understanding of the interplay between natural language instructions, visual cues, and the sequence of actions undertaken by the agent. This capability is crucial for a wide range of robotic applications, extending from healthcare support to everyday household assistance.\nDespite significant advancements in the latest research, we argue that the performance of VLN models may be overestimated. The current standard for evaluating vision-language navigation, as exemplified by the Room-to-Room (R2R; Anderson et al. 2018) and Room-across-Room (RxR; Ku et al. 2020) datasets, predominantly hinges on endpoint success rates and broad path alignment metrics. The recent work (Wang et al., 2023) suggests the performance of the state of the art is high and even quite close to human performance on these standards. Does this mean that the major challenges of the VLN task are almost solved? This perspective might be overly optimistic. For instance, a simple intervention shown in Figure. 1 on a common VLN dataset does not trigger a consistent strong response in the model. In addition, the high success rate of a randomly navigating agent (Anderson et al., 2018) is non-negligible. This indicates that current evaluation metrics may be insufficiently detailed. Furthermore, agents enhanced by Large Multimodal Models (LMMs; Zhou et al. 2023; Lin et al. 2024) perform unexpectedly low on standard VLN datasets. This contrasts with the strong multimodal understanding demonstrated by LMMs in other domains (Fu et al., 2024; Wake et al., 2023). This discrepancy motivates us to revisit the evaluation of VLN models.\nIn this work, we introduce a new evaluation framework that focuses on atomic instructions, i.e., the singular actions fundamental to VLN instructions. Diagnosing VLN models at the atomicinstruction level allows us to gauge performance through various nuanced perspectives. To achieve this, we first iteratively construct a context-free grammar (CFG; Hopcroft et al. 2001) with the help of LLMs to systematically articulate the structure of VLN task instructions. CFG, treated as a comprehensive representation of VLN instructions, allows us to induct and define atomic instruction categories. We group the components in our CFG into five main categories (i.e., direction change, vertical movement, landmark recognition, region recognition, and numerical comprehension) and generate data accordingly to form our novel evaluation dataset NAVNUANCES. For each entry in NAVNUANCES, a candidate path is determined by the specific path proposing strategy according to its instruction category. The instruction is then generated using CFG and further enriched by LLMs. To ensure the data correctness, we incorporate human refinement into this automated generation process in the end. The rigorous evaluation protocols in our dataset pose significant challenges, as they require models to demonstrate a thorough understanding of individual concepts.\nWe benchmark various types of models based on our proposed evaluation framework. Experiments with NAVNUANCES expose model discrepancies and common issues. We observe that recent advancements in the standard R2R dataset primarily stem from enhanced capabilities in vertical movement and region recognition. Despite this progress, numerical comprehension shows stagnation across various models. In terms of specific models, zero-shot agents enhanced by LLMs demonstrated even significant superiority over traditional supervised ones in handling changes in direction and recognizing landmarks. Traditional supervised approaches suffer from selective bias, often leading to deficiencies in adapting to shifts in atomic concepts, as demonstrated in Figure 1.\nOur contributions are threefold: Firstly, we devise a comprehensive evaluation framework that addresses diverse facets of Vision-and-Language Navigation (VLN) at a granular level. Secondly, our work includes a thorough benchmarking of prevalent methodologies on ninety diverse scenes, coupled with an in-depth analysis. The experiments demonstrate the deficiencies and differences in the capabilities of previous models, providing valuable insights for advancing the development of VLN methods. Thirdly, we present a zeroshot baseline as a minor contribution, which enhances NavGPT (Zhou et al., 2023) with GPT-4-vision (Achiam et al., 2023) integrating direct vision-instruction alignment."}, {"title": "2 Related Work", "content": "Vision-language navigation Datasets\nVision-Language Navigation (VLN; Anderson et al. 2018) tasks integrate language guidance within embodied environments. This task is initially introduced by the Room-to-Room dataset (R2R; Anderson et al. 2018) which requires step-by-step navigation in virtual spaces. Subsequent research expanded this framework through variations like multilingual RXR datasets (Ku et al., 2020) and addressed more complex navigation challenges. The advent of conversational interfaces led to interactive VLN tasks, exemplified by CVDN (Thomason et al., 2020) and Teach (Padmakumar et al., 2022), fostering navigation via dialogue interpretation. Concurrently, efforts like VLN-CE (Krantz et al., 2020) aimed to transition VLN tasks into continuous environments. Despite these advancements, a nuanced evaluation of VLN models on atomic-level instructions remained underexplored. Our work addresses this by developing a dataset specifically designed to assess the fundamental capabilities of VLN agents, thereby contributing to the refinement of models across various VLN settings.\nModels in VLN tasks\nThe introduction of the R2R dataset (Anderson et al., 2018) catalyzed the development of numerous models focusing on VLN tasks in discrete environments. Early efforts, such as the Seq2Seq (Anderson et al., 2018) and RCM (Wang et al., 2019) models, emphasized training strategies leveraging Imitation and Reinforcement Learning within a conventional front-view framework. Subsequent innovations like CLIP-ViL (Shen et al., 2021) augmented these models with advanced visual fea"}, {"title": "3 NavNuances Dataset", "content": "The challenge of curating a nuanced dataset is to comprehensively cover the atomic categories in VLN instructions. To achieve this, our approach begins by iteratively constructing a context-free grammar (CFG) with the help of LLM to articulate and cover all components of VLN instructions in a unified representation (Section 3.1). Then, we induct and categorize the atomic components of the CFG into five principal categories (Section 3.2). Building on these categorizations, we develop a semi-automatic process for data annotation of each atomic instruction category, adhering to the CFGdefined natural instruction standards (Section 3.3).\nThe Context-Free Grammar for VLN\nOur CFG defines a set of rules and concepts that structure the instructions in VLN. It can be formalized as a quadruple, i.e., CFG = (N,T, P, S). Non-terminals N (in uppercase such as Landmark in List 1) represent broader conceptual categories or composite concepts. Terminals T signify specific actionable elements or descriptors and are denoted by lowercase words (e.g., left, right). Production Rules P within the CFG outline how various elements are combined to form higherlevel Non-terminals. And Start Symbol S triggers the instruction generation process. An illustrative instruction such as walk past the red chair"}, {"title": "4 Experiment", "content": "We conduct a comprehensive evaluation of various existing VLN models across the five main categories in our NAVNUANCES dataset.\nBaselines\nIn this study, we examine baseline models categorized by input modalities, action spaces, memory representations, and supervision approaches. Input modalities range from front-view RGB images (e.g., Seq2Seq model (Anderson et al., 2018)) and panorama images (e.g., VLN-BERT (Hong et al., 2021)) to textual descriptions of panorama views (e.g., NavGPT (Zhou et al., 2023)). Models differ in their action space, utilizing viewpoint selection (e.g., ScaleVLN (Wang et al., 2023)), predefined rule-based actions (e.g., Seq2Seq (Anderson et al., 2018)), or a combination thereof. Memory representation varies among models, employing hidden states (e.g., CLIP-ViL (Shen et al., 2021)), past visual inputs (e.g., HAMT (Chen et al., 2021)), topological (e.g., DUET (Chen et al., 2022)) or metric maps (e.g., BEVBERT (An et al., 2023)), or interactive chat histories (e.g., NavGPT (Zhou et al., 2023)). Except for differences in the pre"}, {"title": "4.2 Evaluation Protocols", "content": "In this section, we introduce the evaluation protocols for our Vision-Language Navigation (VLN) evaluation set. These protocols are designed to precisely measure the performance of navigation models based on detailed success criteria for different categories of atomic instructions.\nFor categories Landmark Recognition, Numerical Comprehension, and Vertical Movement, the evaluations follow the distance-related protocols. The criteria differ slightly depending on the nature of the movement. For instance, in the vertical movement category, success is defined by a 3-meter radius to a specified endpoint. For instructions involving more localized navigation, such as walking towards a landmark, the metric focuses more on the reduction in distance to the landmark. Further details can be found in Appendix B.\nRegion Recognition category is more related"}, {"title": "4.3 Main Results", "content": "We report the performance evaluated on NAVNUANCES as well as the reproduced results on the unseen validation split of the R2R dataset (Anderson et al., 2018) in Table 2. We assess NavGPT4 and NavGPT4v using a random subset of around 130 samples, ensuring replicability of the officially reported NavGPT performance without incurring significant API costs.\nReflecting on the advancements in the standard R2R dataset, it appears that improved layout and spatial understanding underpin the progress of VLN models. This is evident from the results in vertical movement (VM) and region recognition (RR) tasks on our dataset. This correlation is probably due to the statistics of the R2R unseen split. We find that more than 35% of the instructions necessitate navigation through stairs, and the majority involve concepts related to rooms. The correlation is observed consistently across different models. For instance, CLIP-ViL's leap in perfor"}, {"title": "4.4 Additional Experiments", "content": "Does the agent understand numerical values?\nIn this additional experiment, we aim to further study the numerical comprehension capabilities of models. Despite observing an overall low perfor"}, {"title": "5 Conclusion", "content": "In this study, we establish a systematic framework to diagnose deficiencies in the capabilities of Vision-Language Navigation (VLN) models at the atomic instruction level. Our experiment results on NAVNUANCES across diverse models clearly uncover the limitations of specific models and reveal common issues, which highlight ongoing challenges in the VLN task. In addition, our investigation into a modified zero-shot agent enhanced by GPT-4-vision provides empirical evidence that a direct alignment between vision and instructions significantly enhances landmark recognition performance. This insight underscores the potential for leveraging advanced large multimodal models in improving VLN systems."}, {"title": "Limitations", "content": "Despite the data involved in our study are sufficiently representative to support the insights provided by our initial findings, the constraints imposed by the static discrete environments of Matterport3D (Chang et al., 2017) lead to several limitations. Since we are not able to edit the environment such as adding or removing objects, we are restricted to generating data from existing layouts. This limits the data diversity for some instruction categories. For instance, in the numerical comprehension category, due to a lack of identical object categories within single regions, we are unable to encompass numerical comprehension data in the object level, such as \u201cmove close to the [i]-th apple on your right\". Additionally, because we cannot rearrange object attributes and positions, it is difficult to achieve a detailed attribute-level data design in the landmark recognition category.\nIn addition, this study focuses exclusively on atomic-level capabilities, which do not encompass the full range of capabilities of VLN agents such as error correction for executing long instructions. Understanding sequences of multiple actions within long instructions is also a crucial aspect of the VLN task. Evaluating from this aspect is challenging but represents a promising direction for future research.\nIn this work, we leverage CFG as the basis of the problem decomposition and construct a diagnostic dataset based on it. Our semi-automatic approach for CFG construction is well-suited for designing specialized datasets in fields like law or finance. However, for more complex tasks, relying on manual corrections may be inefficient and challenging in ensuring comprehensive coverage of concepts. An improvement would be the development of a fully automatic induction method, leveraging the extensive world knowledge encapsulated in large language models, to potentially replace the current semi-automatic method."}]}