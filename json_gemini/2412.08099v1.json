{"title": "Adversarial Vulnerabilities in Large Language Models\nfor Time Series Forecasting", "authors": ["Fuqiang Liu", "Sicong Jiang", "Luis Miranda-Moreno", "Seongjin Choi", "Lijun Sun"], "abstract": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in handling\ncomplex temporal data. However, their robustness and reliability in real-world\napplications remain under-explored, particularly concerning their susceptibility\nto adversarial attacks. In this paper, we introduce a targeted adversarial attack\nframework for LLM-based time series forecasting. By employing both gradient-\nfree and black-box optimization methods, we generate minimal yet highly effective\nperturbations that significantly degrade the forecasting accuracy across multiple\ndatasets and LLM architectures. Our experiments, which include models like\nTimeGPT and LLM-Time with GPT-3.5, GPT-4, LLaMa, and Mistral, show that\nadversarial attacks lead to much more severe performance degradation than random\nnoise, and demonstrate the broad effectiveness of our attacks across different\nLLMs. The results underscore the critical vulnerabilities of LLMs in time series\nforecasting, highlighting the need for robust defense mechanisms to ensure their\nreliable deployment in practical applications.", "sections": [{"title": "Introduction", "content": "Time series forecasting plays a pivotal role in numerous real-world applications, ranging from\nfinance and healthcare to energy management and climate modeling. Accurately predicting temporal\npatterns in the data is crucial for informed decision-making in these domains [1]. Recently, Large\nLanguage Models (LLMs), originally designed for Natural Language Processing (NLP) tasks, have\ndemonstrated remarkable potential in handling time series forecasting challenges [2, 3, 4, 5, 6].\nThese models, including BERT [7], GPT [8, 9], LLaMa [10] and their successors, leverage their\npowerful attention mechanisms and vast pre-training on diverse datasets to capture intricate temporal\ndependencies, making them highly effective for complex forecasting tasks.\nLLMs exhibit strong generalization capabilities across various types of time series data. Compared\nto traditional models like ARIMA[11] and Exponential Smoothing [12], as well as advanced deep\nlearning models such as DNNs [13, 14, 15], and Transformer-based architectures[16, 17, 18, 19],\nLLMs excel in modeling long-term dependencies and capturing non-linear patterns within temporal\nsequences. This has resulted in impressive forecasting accuracy across applications ranging from\nenergy consumption predictions to weather forecasting [20, 21].\nHowever, despite their success, the robustness and reliability of LLMs in real-world forecasting\nremain concerns, particularly their vulnerability to adversarial attacks is under-explored. Adversarial\nattacks introduce subtle, often imperceptible perturbations to input data, leading to significant and\nmisleading changes in model predictions. While the susceptibility of machine learning models to\nsuch attacks has been well-explored in image processing and NLP domains [22, 23, 24], there is a\nnoticeable gap in research on their impact on LLMs used for time series forecasting.\nWhile adversarial attacks and defenses for deep neural networks have been extensively studied across\nvarious domains [25], executing adversarial attacks against LLMs in time series forecasting presents\ntwo significant challenges. First, to prevent information leakage, we cannot use ground truth values\n(i.e., future time steps) when attacking forecasting models. Second, LLMs must be treated as strict\nblack-box systems due to the difficulty of accessing their internal workings and parameters.\nIn this paper, we address this gap by proposing a gradient-free black-box attack that transforms the\noutput of LLM-based forecasting models into a random walk, while investigating the vulnerabilities\nof large language models in time series forecasting. As depicted in Figure 1, we demonstrate that\neven minimal attack perturbations can cause substantial deviations in LLMs' predictions. We evaluate\ntwo forms of LLM applications for time series forecasting, encompassing five sub-models, across\nfive datasets from various real-world domains. Our findings reveal that LLMs, despite their advanced\narchitectures, are indeed susceptible to adversarial manipulations in time series domain, leading to\nunstable and inaccurate forecasts. This underscores the urgent need to develop more robust LLMs\nthat can withstand such attacks, ensuring their reliability in real-world applications.\nIn conclusion, this study contributes to the growing discourse on LLMs robustness by exposing their\nvulnerabilities to adversarial attacks in Time Series Forecasting. Our results highlight the necessity\nof addressing these vulnerabilities to advance the development of LLMs that are not only accurate\nbut also resilient, thereby enhancing their practical utility in high-stakes environments."}, {"title": "Related Work", "content": ""}, {"title": "Adversarial Attacks in Time Series Forecasting", "content": "Adversarial attacks in time series forecasting have emerged as a crucial area of research, exposing\nvulnerabilities in forecasting models. Unlike adversarial studies in static domains, such as object\nrecognition or time series classification, adversarial attacks on time series forecasting cannot leverage\nground truth data for perturbation generation due to the risk of information leakage [26]. To address\nthis challenge, surrogate techniques have been adopted [27], which bypass the need for labels, as is\ndone in traditional adversarial attack methods like the Fast Gradient Sign Method [28]. Several studies\nhave treated forecasting models as white-box systems to investigate the effects of adversarial attacks"}, {"title": "Adversarial Attacks on LLMS", "content": "Adversarial attacks on LLMs have gained increasing attention, focusing on how slight manipulations\ncan significantly alter their outputs. These attacks are often classified into prompt-based attacks,\ntoken-level manipulations, gradient-based attacks, and embedding perturbations.\n\u2022 Jailbreak Prompting [31, 32]: Crafted prompts that bypass LLM guardrails, inducing\nunintended or harmful outputs by exploiting unconventional phrasing.\n\u2022 Prompt Injection [33, 34, 35]: Adversarial instructions embedded into benign prompts to\nmanipulate LLM responses, highlighting their vulnerability to prompt manipulation.\n\u2022 White-box Gradient Attacks [25, 36]: Using internal model parameters, attackers ap-\nply gradient-based methods to perturb inputs, significantly altering outputs with minimal\nchanges.\n\u2022 Black-box Attacks [37]: Query-based attacks without model access, using techniques like\nZeroth-Order Optimization to craft adversarial examples by estimating gradients.\n\u2022 Embedding Perturbations [38, 39]: Subtle changes to input embeddings disrupt LLM's\ninternal representations, leading to erroneous outputs with minimal visible input alterations.\nWhile extensive research has been conducted on attacks against LLMs at various levels, most of\nthese focus on text-based manipulations. However, there's a significant gap in understanding how\nLLMs perform in non-textual tasks, particularly time series forecasting. In language tasks, attacks\ntypically manipulate static text inputs, such as words or prompts, to exploit the LLM's understanding\nand induce specific outputs. However, time series forecasting involves dynamic, evolving data points,\nrequiring attackers to introduce perturbations that maintain the sequence's natural flow and coherence."}, {"title": "Manipulating LLM-based Time Series Forecasting", "content": ""}, {"title": "Formulations of LLM-based Time Series Forecasting", "content": "LLMs have shown promising performance in time series forecasting by leveraging their ability to\nperform next-token prediction, a technique originally developed for text-based tasks [2, 6]. A typical\nLLM-based time series forecasting model, denoted as f(\u00b7), consists of two primary components: an\nembedding or tokenization module that encodes the time series data into a sequence of tokens, and\na pre-trained LLM that autoregressively predicts the subsequent tokens. The embedding module\ntranslates the raw time series into a format suitable for the LLM, while the LLM captures the temporal\ndependencies and generates predictions based on its learned representations.\nLet $X_t \\in R^d$ denote d-dimensional time series at time t, where $x_{i,t} = [X_t]_i$ represents the observation\nof the i-th component of the time series. Given a sequence of recent T historical observations\n$X_{t-T+1:t}$, a forecasting model, f(\u00b7), is employed to predict the future values for the subsequent T\ntime steps. The prediction is formulated as:\n$Y_{t+1:t+\\tau} = f (X_{t-T+1:t}),$\nwhere $Y_{t+1:t+\\tau}$ denotes the predicted future values and $\\hat{Y}_{t+1:t+\\tau}$ represents the corresponding\nground truth values. It is important to note that the prediction horizon is typically less than or equal\nto the historical horizon, i.\u0435., $\\tau < T$."}, {"title": "Threat model", "content": "Our objective is to deceive an LLM-based time series forecasting model into producing anomalous\noutputs that deviate significantly from both its normal predictions and the corresponding ground\ntruth, through the introduction of imperceptible perturbations. This adversarial attack problem can be\nframed as an optimization task as follows:\n$\\max\\limits_{P_{t-T+1:t}} L(f (X_{t-T+1:t} + P_{t-T+1:t}), Y_{t+1:t+\\tau})$\ns.t. $||p_i||_p \\le \\epsilon, i \\in [t \u2212 T + 1, t],$ \nwhere $X_{t-T+1:t}$ denotes the clean input, $Y_{t+1:t+\\tau}$ denotes the true future values, and $P_{t-T+1:t}$ de-\nnotes the adversarial perturbations. Our objective is to deceive an LLM-based time series forecasting\nmodel into producing anomalous outputs that deviate significantly from both its normal predictions\nand the corresponding ground truth, through the introduction of imperceptible perturbations. This\nadversarial attack problem can be framed as an optimization task as follows: The loss function\nL quantifies the discrepancy between the model's output and the ground truth, while $ \\epsilon $ constrains\nthe magnitude of the perturbations under the lp-norm, ensuring that the adversarial attack remains\nimperceptible.\nSince the true future values $Y_{t+1:t+\\tau}$ are typically inaccessible in practical time series forecasting,\nthey are replaced with the predicted values $\\hat{Y}_{t+1:t+\\tau}$ generated by the forecasting model. Conse-\nquently, Eq. 2 is reformulated as\n$\\max\\limits_{P_{t-T+1:t}} L (f (X_{t-T+1:t} + P_{t-T+1:t}), \\hat{Y}_{t+1:t+\\tau})$\ns.t. $||p_i||_p \\le \\epsilon, i \\in [t - T + 1, t] .$"}, {"title": "Target Attack with Directional Gradient Approximation", "content": "Since the attacker has no access to the internal parameters of the LLM, it is not feasible to compute\ngradients and use them to solve the optimization problem presented in Eq. 3. This results in\na gradient-free optimization problem. To address this, we propose a gradient-free optimization\napproach, referred to as targeted attack with Directional Gradient Approximation (DGA), aimed at\ngenerating perturbations that can effectively deceive LLM-based time series forecasting models.\nWe first adjust our objective to focus on misleading the forecasting model into producing outputs that\nclosely resemble an anomalous sequence, rather than simply deviating from its normal predictions.\nAccordingly, the optimization problem in Eq. 3 is reformulated as\n$\\min\\limits_{P_{t-T+1:t}} C (f (X_{t-T+1:t} + P_{t\u2212T+1:t}), V)$\ns.t. $||p_i||_p \\le \\epsilon, i \\in [t \u2013 T + 1, t],$ \nwhere Y represents the targeted anomalous time series.\nSupposing $\\theta_{t-T+1:t}$ denote a random small signal, the gradient, $g_{t-T+1:t}$, which approximates the\ndirection from the normal output to the targeted anomalous output, can be expressed as\n$g_{t-T+1:t} = \\frac{L (Y \u2212 f (X_{t-T+1:t} + \\theta_{t-T+1:t})) \u2013 L (Y \u2212 f (X_{t-T+1:t}))}{\\theta_{t-T+1:t}}$\nSupposing l\u2081-norm is applied in Eq. 4, the magnitude of the perturbation is strictly constrained to be\nimperceptible. The perturbation, $P_{t\u2212T+1:t}$, can be computed from the approximated gradient, and\nthe temporary adversarial example, $X'_{t\u2212T+1:t}$, is generated as\n$X'_{t-T+1:t} = X_{t-T+1:t} + P_{t-T+1:t} = X_{t-T+1:t} + \\epsilon \u00b7 sign (g_{t\u2212T+1:t}),$\nwhere $sign (\u00b7)$ denotes the signum function.\nA time series forecasting model that produces Gaussian White Noise (GWN) as its output is considered\nto generate an anomalous prediction. Consequently, GWN can be utilized as the target sequence in\nEq. 6, formulated as $Y ~ \u039d (\u03bc, \u03c3)$, where \u03bc and o represent the mean and the standard deviation,\nrespectively. Empirically, the mean and standard deviation of the input data can be used to generate\nGWN. This results in a situation where a temporally correlated time series is misleadingly predicted\nas independent and identically distributed (i.i.d.) noise. This approach highlights the model's inability\nto preserve temporal correlations when subjected to adversarial perturbations, thereby reinforcing the\neffectiveness of the adversarial attack."}, {"title": "Experiments", "content": ""}, {"title": "Datasets", "content": "To evaluate the proposed DGA and gain a further understanding of the vulnerability of LLM-based\nforecasting, We conducted experiments using five widely recognized real-world datasets that cover a\nbroad range of time series forecasting tasks:\n\u2022 ETTh1 and ETTh2 (Electricity Transformer Temperature Hourly) [16]: These datasets\nconsist of two years of hourly recorded data from electricity transformers, capturing temper-\nature and power consumption variables.\n\u2022 Istanbul Traffic[2]: This dataset contains hourly measurements of road traffic volumes\nacross different sensors. It captures temporal dependencies related to traffic patterns, making\nit ideal for testing models on dynamic and fluctuating time series data.\n\u2022 Weather [16]: This dataset comprises meteorological data, including variables such as tem-\nperature, humidity, and wind speed, recorded hourly. It provides a challenging forecasting\ntask due to the inherent variability and complexity of weather patterns.\n\u2022 Exchange [40]: This dataset consists of daily exchange rates from eight foreign coun-\ntries-Australia, the United Kingdom, Canada, Switzerland, China, Japan, New Zealand,\nand Singapore-covering the period from 1990 to 2016.\nThese diverse datasets allow us to evaluate the robustness of LLMs across different types of temporal\ndynamics and forecasting challenges. In our experiments, 50% of the data is used for training, while\nthe remaining data is split evenly: 25% for validation and 25% for testing. It should be noted that\nthe attacker does not access either the training or validation part. We use a 96-step historical time\nwindow as input to the forecasting model, which predicts the subsequent 48-step future values."}, {"title": "Target Models", "content": "To assess the impact of adversarial attacks on LLMs for time series forecasting, we selected two\nstate-of-the-art LLM-based forecasting models as baselines, which together represent two common\nforms of LLM application for time series tasks:\n\u2022 TimeGPT [41]: A large model specifically pre-trained with a vast amount of time series\ndata. TimeGPT uses advanced attention mechanisms and temporal encoding to capture\ncomplex patterns in sequential data, making it a leading LLM designed explicitly for time\nseries forecasting. Its pre-training, which is conducted from scratch using vast amounts\nof time series data, allows it to serve as a robust and versatile tool for a wide range of\ntime-dependent applications.\n\u2022 LLMTime [2]: This model treats time series forecasting as a next-token prediction task,\nusing LLM architectures like GPT and LLaMa. By converting time series data into numerical\nsequences, LLM-Time enables these models to apply their sequence prediction strengths to\ntime series. To test the robustness of our adversarial attacks, we experimented with base\nmodels including GPT-3.5, GPT-4, LLaMa, and Mistral, assessing their resilience when\nadapted from natural language processing to time series forecasting.\n\u2022 TimeLLM [6]: TimeLLM presents a novel approach for time series forecasting by adapting\nLLMs with reprogramming input time series data into textual representations that are more\ncompatible with LLMs, allowing the models to perform time series forecasting tasks without\naltering their pre-trained structures. The key innovation is the Prompt-as-Prefix (PaP)\ntechnique, which augments input context to guide the LLM in transforming reprogrammed\ndata into accurate forecasts."}, {"title": "Experimental Procedures", "content": "We designed a series of experiments to evaluate the vulnerability of the baseline LLM models to\nadversarial attacks. For each model and dataset combination, we conducted the following procedures:\n(i) we applied targeted perturbations to the input data, carefully maintaining the overall structure of\nthe original time series while subtly altering the data to mislead the LLMs' forecasting predictions;\n(ii) we introduced GWN with the same perturbation intensity; (iii) forecasting accuracy was measured\nusing Mean Absolute Error (MAE) and Mean Squared Error (MSE), which allowed us to quantify\nthe performance degradation caused by adversarial attacks compared to Gaussian noise."}, {"title": "Overall Comparison", "content": "As shown in Table 1, the experimental results demonstrate that the designed adversarial attacks\nsignificantly degraded forecasting performance across all datasets, as indicated by increased MSE\nand MAE values. Compared to GWN of the same perturbation intensity, our attacks had a much\nmore detrimental effect on the models' predictions.\nFor TimeGPT, which is pre-trained with large-scale time series data, the adversarial attack led to\na sharp rise in forecasting errors, demonstrating that even models specifically built for time series\nforecasting are vulnerable. For LLM-Time, which includes GPT-3.5, GPT-4, LLaMa, and Mistral\nas base models, the adversarial attack was even more pronounced. As illustrated in Figure 2, the\nattack caused a clear divergence between the forecasted values and the true time series, with all\ndifferent variants of LLM-Time exhibiting larger deviations compared to GWN. GPT-3.5 and GPT-\n4, in particular, showed significant susceptibility, with their errors increasing substantially under\nadversarial conditions.\nAcross all models and datasets, the adversarial perturbations induced much greater disruptions than\nGWN, clearly impacting the predictions and demonstrating the precision of the attack in destabilizing\nLLM-based forecasting. This underlines the importance of developing robust defensive strategies\nto protect LLMs against such targeted adversarial attacks, as their current vulnerability poses a\nsignificant challenge for practical applications."}, {"title": "Interpretation Study", "content": "Figure 3 illustrates the distribution shift in predictions caused by targeted perturbations on the LLM-\nbased forecasting model. The proposed DGA method is designed to mislead the forecasting model,\ncausing its predictions to resemble a random walk. As depicted in Figure 3, the \"blue\" shaded area,\nrepresenting the perturbed prediction distribution, deviates significantly from the original \"yellow\"\ndistribution and approaches a normal distribution. This shift underscores how subtle, well-crafted\nperturbations can manipulate the model into producing inaccurate forecasts. The effect of DGA-\ninduced perturbations is pronounced when examining the prediction distributions, where errors are\nmuch more severe compared to the minor disruptions caused by GWN. These findings suggest that\nLLM-based forecasting models are highly susceptible to adversarial attacks that exploit the model's\ninherent vulnerabilities.\nAdditionally, the autocorrelation function (ACF) analysis provides further evidence of the detrimental\nimpact of these adversarial attacks. Normally, LLMs demonstrate a strong ability to capture the tem-\nporal dependencies within time series data, maintaining coherent relationships between consecutive\ndata points. However, as illustrated in Figure 4, when subjected to adversarial perturbations, these\ntemporal dependencies break down, resulting in forecasts that no longer reflect the true underlying\ntrends of the data. The disrupted autocorrelation patterns clearly illustrate the model's difficulty in"}, {"title": "Hyperparameter Study", "content": "We analyze the impact of varying scale ratios on model performance under both GWN and DGA\nadversarial attacks, with the vertical axis in Figure 5 representing the increase in MAE. This experi-\nment was conducted across three different datasets using three LLM-based forecasting models. As\ndemonstrated in the figure, DGA consistently results in a more significant increase in MAE compared\nto GWN as the scale ratio rises, indicating that DGA is more effective in disrupting the model's\npredictions. To balance imperceptibility and manipulation effectiveness, the perturbation scale can be\nchosen as 2% of the mean value of the given data."}, {"title": "Conclusion", "content": "In this study, we demonstrated the significant vulnerabilities of LLM-based models for time series\nforecasting to adversarial attacks. Through a comprehensive evaluation of TimeGPT and LLM-Time\n(with GPT-3.5, GPT-4, LLaMa, and Mistral as base models), we found that targeted adversarial per-\nturbations, generated using Directional Gradient Approximation (DGA), caused substantial increases\nin prediction errors. These attacks were far more damaging than Gaussian White Noise (GWN) of\nsimilar intensity, highlighting the precision and effectiveness of the adversarial strategy.\nThe experimental results revealed that both large, pre-trained models like TimeGPT and fine-tuned\nmodels such as LLM-Time are highly susceptible to adversarial manipulation. The proposed attack\ncan significantly degrade model performance across various datasets. This poses serious challenges\nfor the deployment of LLMs in real-world time series applications, where reliability is critical.\nOur findings emphasize the need for future research to focus on developing robust defense mechanisms\nto mitigate adversarial threats and enhance the resilience of LLM-based time series forecasting\nmodels. Without such protections, these models remain vulnerable to attacks that could undermine\ntheir practical utility in high-stakes environments. In addition, future studies should compare\nvulnerabilities of LLM with lighter models."}]}