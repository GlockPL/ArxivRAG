{"title": "Vision-Language Model Based Handwriting Verification", "authors": ["Mihir Chauhan", "Abhishek Satbhai", "Mohammad Abuzar Hashemi", "Mir Basheer Ali", "Bina Ramamurthy", "Mingchen Gao", "Siwei Lyu", "Sargur Srihari"], "abstract": "Handwriting Verification is critical in document forensics. Deep learning based approaches often face skepticism from forensic document examiners due to their lack of explainability and reliance on extensive training data and handcrafted features. This paper explores using Vision Language Models (VLMs), such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By leveraging their Visual Question Answering capabilities and 0-shot Chain-of-Thought (CoT) reasoning, our goal is to provide clear, human-understandable explanations for model decisions. Our experiments on the CEDAR handwriting dataset demonstrate that VLMs offer enhanced interpretability, reduce the need for large training datasets, and adapt better to diverse handwriting styles. However, results show that the CNN-based ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy: 71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings highlight the potential of VLMs in generating human-interpretable decisions while underscoring the need for further advancements to match the performance of specialized deep learning models. Our code is publicly available at: https://github.com/Abhishek0057/vlm-hv", "sections": [{"title": "1 Introduction", "content": "Handwriting verification is crucial in pattern recognition and biometrics, focusing on authenticating and identifying individuals by their handwriting. It is vital in forensics, where experts analyze samples to verify documents, identify forgeries, and provide court evidence.\nHistorical Perspective: Handwriting Verification methods [Shaikh et al., 2018] [Chauhan et al., 2019] [Chauhan et al., 2024] have evolved from handcrafted features like GSC [Favata and Srikantan, 1996] to deep CNNs such as ResNet-18 [He et al., 2015], and now Vision Transformers [Dosovitskiy et al., 2021], enabling comparison of both inter- and intra- writer variations in handwriting styles.\nChallenges: Despite the advancements, Forensic Document Examiners (FDE) remain skeptical due to the lack of interpretability in the model's decision making processes. Morever, these methods heavily depend on large datasets of labeled handwritten images, making dataset collection (X (xq, xk, y) with known xk, questioned xq handwritten samples and corresponding writer labels y) is costly and time-intensive.\nWhy use VLM? Vision-Language Models (VLMs) integrate image and textual data to encode complex relationships between visual and linguistic information, enabling Forensic Document Examiners (FDEs) to interpret model decisions with clear, natural language explanations that enhance trust and reliability in forensic applications. They can adapt to the forensics domain in zero-shot scenarios without training examples and in few-shot scenarios with minimal examples by leveraging transfer learning capabilities, having been pre-trained on multiple tasks related to handwritten images and natural language understanding. Fine-tuning them with forensic-specific datasets further enhances their performance and applicability in real-world forensic use-cases."}, {"title": "2 Methods", "content": "Our study aims to use VLM in handwriting verification. We have chosen OpenAI's GPT-4o VLM for its strong Visual Question Answering (VQA) capabilities. Using their API, we prompt GPT-4o [gpt, 2024] with 0-shot Chain-of-Thought (CoT) reasoning. This approach allows us to first generate human-interpretable explanations and then determine whether the given questioned and known handwritten samples were written by the same person or by different writers, as shown in Figure 1. Since fine-tuning GPT-4o is not generally available, we compared its performance to a recent open-source VLM, PaliGemma [Beyer et al., 2024], using 0-shot prompt engineering and parameter-efficient supervised fine-tuning (PEFT) on a training dataset with 100 examples.\nData: The experiments were conducted using 1,000 sample pairs of known and questioned images from CEDAR Letter [Srihari et al., 2001] and CEDAR AND [Shaikh et al., 2018] dataset. CEDAR Letter dataset contains a letter manuscript written by 1568 writers three times. CEDAR AND dataset is a subset of CEDAR Letter data which only contains the lowercase handwritten word \u201cand\u201d extracted from full letter manuscript. The evaluation dataset includes 1,000 sample pairs of known and questioned images written by 368 writers with writer ids greater than 1200 (writer ids below 1200 were used to train the baseline models). Of these pairs, 500 are from the same writer and 500 are from different writers."}, {"title": "3 Experiments & Results", "content": "CEDAR AND Baselines: As shown in Figure 1 we use handcrafted features GSC features, CNN based ResNet-18 [He et al., 2015] and MaskedCausalVisionTransformer [Dosovitskiy et al., 2021] (ViT) as our baseline feature extractors. GSC are 512-dimensional features extracted for the binarized \u201cAND\" images. All the three baselines were trained on 10% and 100% of known and questioned training pairs writers with writer ids less than 1200 resulting in 13,232 and 129,602 number of train pairs as shown in Table 1. The output of these feature extractors is fed into 2 fully-connected (FC) layers with 256 and 128 hidden neurons with ReLU activations. The final layer has 2 output neurons whose softmax activations represent similarity of samples with a one hot vector representation. We use categorical cross entropy loss given one-hot encoded logits compared to the target which is binary.\nPrompt Engineering with GPT-4o and PaliGemma: To effectively utilize the GPT-4o VLM for handwriting verification, we experimented with various prompts to optimize both the generation of human-interpretable explanations and the accuracy of verification decisions. Initially, we crafted prompts that directed the model to compare specific features of the handwriting samples, such as stroke width, slant, and letter spacing. For example, prompts like \"Describe the similarities and differences in the stroke patterns between the two sam-"}, {"title": "4 Conclusion", "content": "Our study explores the application of VLMs, specifically GPT-4o and PaliGemma, in the domain of handwriting verification. By leveraging the robust VQA capabilities of these models and employing 0-shot CoT reasoning through prompt engineering, we aimed to generate human-interpretable explanations for model decisions. Our experiments demonstrated that while VLMs offer significant improvements in interpretability and adaptability to diverse handwriting styles, they currently lag behind CNN-based architectures such as ResNet-18 in terms of performance. Specifically, ResNet-18 achieved an accuracy of 84% on the CEDAR AND dataset, outperforming GPT-4o's 70% accuracy and PaliGemma's 71% accuracy. These findings suggest that while VLMs hold great promise for enhancing transparency and trustworthiness in forensic handwriting verification, there is still a need for further advancements in their fine-tuning regimes to improve their effectiveness and reliability for specialized tasks. Moving forward, we aim to work with forensic document examiners to create a fine-tuning dataset of explanation reports using text and visual information to ensure the applicability of our approach in practical forensic settings."}]}