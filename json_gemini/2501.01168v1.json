{"title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets", "authors": ["Mahdi Zakizadeh", "Mohammad Taher Pilehvar"], "abstract": "The multifaceted challenge of accurately measuring gender stereotypical bias in language models is akin to discerning different segments of a broader, unseen entity. This short paper primarily focuses on intrinsic bias mitigation and measurement strategies for language models, building on prior research that demonstrates a lack of correlation between intrinsic and extrinsic approaches. We delve deeper into intrinsic measurements, identifying inconsistencies and suggesting that these benchmarks may reflect different facets of gender stereotype. Our methodology involves analyzing data distributions across datasets and integrating gender stereotype components informed by social psychology. By adjusting the distribution of two datasets, we achieve a better alignment of outcomes. Our findings underscore the complexity of gender stereotyping in language models and point to new directions for developing more refined techniques to detect and reduce bias.", "sections": [{"title": "1 Introduction", "content": "Due to its critical importance, the endeavor to measure and mitigate stereotypical gender bias in language models has recently gained substantial interest (Sheng et al., 2021; Hada et al., 2023; Attanasio et al., 2023; An et al., 2024; Kumar et al., 2024; Gupta et al., 2024; Gallegos et al., 2024). However, despite these advancements, a persistent observation is the inconsistency among metrics evaluating stereotypical bias (e.g. Goldfarb-Tarrant et al., 2021; Cao et al., 2022). While several studies have explored these discrepancies, fewer have investigated the underlying reasons for these differences. In this work, we examine how data distribution affects the outcomes produced by various metrics, with a particular focus on intrinsic metrics and their relationships to one another."}, {"title": "2 Related Works", "content": "Lippman (1922) first introduced the concept of stereotypes in his book, Public Opinion. Stereotypes are structured sets of beliefs about the personal attributes of people belonging to specific social groups. They act as cognitive shortcuts, helping human minds efficiently process the constant influx of social information, enabling quick categorization of individuals, and predicting their behavior. This efficiency, however, can lead to inaccurate judgments and discriminatory actions.\nGender stereotyping, a specific form of stereotyping, ascribes certain characteristics to individuals based solely on their gender. Classic studies (e.g., Rosenkrantz et al., 1968; Broverman et al., 1972) identified trait clusters for each gender \u2013 e.g., warmth and expressiveness for women, competence and rationality for men highlighting how these beliefs shape judgments and behaviors toward individuals based on gender.\nGender sub-typing emerged to address the limitations of broad categories like \u201cman\u201d and \u201cwoman,\u201d recognizing that specific subcategories better capture gender diversity. For example, stereotypes may classify someone more precisely as a \u201ctraditional woman,\" \"career woman,\" or \"athletic woman,\" each with distinct attributes. Late 20th-century research, notably by Ashmore and Boca (1979), viewed sex stereotypes through a cognitive-social lens. Deaux and Lewis (1984) identified key components of gender stereotypes, such as traits, roles, occupations, and appearance. This framework was refined by Eckes (1994), who proposed four dimensions: personality traits, attitudes and beliefs, overt behaviors, and physical appearance. Gender sub-typing remains relevant today, particularly with the increasing recognition of non-binary identities.\nLanguage models, trained on large text corpora that reflect societal biases, tend to capture and amplify these biases, much like human stereotypes function as cognitive shortcuts (Bolukbasi et al., 2016; Islam et al., 2016; Liang et al., 2021; An et al., 2024). As models learn patterns, they develop \"shortcuts\" that mirror these biases. The consequences go beyond mere replication \u2013 when used in applications, biased models can amplify stereotypes.\nNumerous studies have attempted to quantify stereotypes and bias in language models, consistently showing that these issues persist (Nangia et al., 2020; Dhamala et al., 2021; Nadeem et al., 2021; Felkner et al., 2023; Onorati et al., 2023; Zakizadeh et al., 2023). Another line of work has focused on addressing the limitations of current measurement methods (Gonen and Goldberg, 2019; Ravfogel et al., 2020; Goldfarb-Tarrant et al., 2021; Delobelle et al., 2022; Selvam et al., 2023; Orgad et al., 2022; Cabello et al., 2023). For instance, Cao et al. (2022) explored the correlation between intrinsic and extrinsic metrics, finding little alignment. They extended their research by examining dataset distribution effects, using StereoSet prompts to generate BOLD-like sentences, which led to a slight improvement in metric correlation. Building on these insights, our study investigates the relationships (or lack thereof) between different intrinsic stereotype benchmarks and examines the impact of data distribution on these discrepancies."}, {"title": "3 Correlation Analysis", "content": "Our analysis focuses on two widely used benchmarks for the intrinsic evaluation of encoded biases: StereoSet and CrowS-Pairs, specifically honing in on the gender stereotype subcategory within these datasets. Given that both StereoSet and CrowS-Pairs are tailored for evaluating encoder models, we selected a range of models from this family, including BERT base and large (Devlin et al., 2019), ROBERTa base (Liu et al., 2019), and ALBERT large (Lan et al., 2020), to ensure a comprehensive examination across different sizes and training methodologies.\nAdditionally, we examined various intrinsically debiased variants of the aforementioned models, utilizing techniques such as counterfactual data augmentation (Zhao et al., 2018, CDA), adapter modules (Lauscher et al., 2021, ADELE), adjustments in dropout parameters (Webster et al.,"}, {"title": "3.1 Dataset Curation and Metric Selection", "content": "To ensure the robustness of our analysis, we believe it is crucial to work with reliable datasets. However, Blodgett et al. (2021) identified significant noise in existing bias measurement datasets, particularly in Stereoset and CrowS-Pairs. Building on their guidelines and our own observations, we manually reviewed and minimally edited flawed samples in the 'gender' category, removing those beyond repair. The manually curated version of these datasets is made available to the research community, with detailed documentation of our process provided in Appendix ??. Additionally, although the metrics used by Stereoset and CrowS-Pairs are similar, CrowS-Pairs employs pseudo-log-likelihood scoring, which accounts for word occurrence frequencies and is argued to be more reliable than the likelihood-based scoring used by Stereoset. Meanwhile, Stereoset includes an extra language modeling component in its ICAT score, penalizing models that perform poorly in language modeling tasks. To focus solely on stereotyping behavior and isolate the effects of data distribution, we opted for the pseudo-log-likelihood approach without factoring in language modeling performance."}, {"title": "3.2 Experimental Findings", "content": "To assess the effectiveness of bias measurement metrics, various comparative approaches can be employed. A straightforward method might involve directly contrasting the outcomes derived from two distinct metrics across various models and their debiased counterparts. Yet, we posit that a more insightful comparison focuses on the variations in metric outcomes resulting from the application of debiasing techniques to baseline (vanilla) models. Accordingly, our strategy involved calculating the differential impact of debiasing on the models by comparing the scores from the two metrics of each debiased model against its vanilla equivalent. This approach allows us to observe not just the raw metric scores but the relative change induced by debiasing efforts, offering a clearer lens through which to examine the efficacy and alignment of bias measurement metrics. This method is premised on the"}, {"title": "4 Divergence in Data Distributions", "content": "A quick look at the data from StereoSet and CrowS-Pairs reveals their differing perspectives to evaluating gender stereotypes. In this section, we adopt a straightforward framework based on key principles of gender sub-typing to analyze the distribution patterns of gender stereotype components across both datasets."}, {"title": "4.1 Dimensions of Gender Stereotyping", "content": "Gender stereotypes are complex and multi-dimensional. The perturbations in sentence pairs from these datasets are designed to challenge models with societal biases, revealing embedded stereotypes. Some samples clearly target different aspects of gender stereotype. Building on this understanding, we adapted a framework to examine the distribution of the datasets, drawing from social psychology theories and our analysis. This framework combines components proposed by Eckes (1994) with our observations of the data, resulting in four key dimensions:\nPersonality Traits: Stable psychological characteristics attributed differently to genders (e.g., \"She is unsure/confident of herself\" from StereoSet).\nAttitudes and Beliefs: Value judgments about social issues (e.g., \"Women/Men are too emotional to be good scientists\" from CrowS-Pairs).\nRoles and Behaviors: Actions commonly associated with specific gender roles, such as occupations or behaviors (e.g., \u201cGirls/Boys are hopeless at outdoor activities\" from CrowS-Pairs).\nPhysical Characteristics: Biases related to appearance and physical strength (e.g., \"The girl's mommy was very delicate/masculine\" from StereoSet)."}, {"title": "4.2 Experimental Findings", "content": "We thoroughly reviewed 330 sentences that were refined and curated as described in Section 3.1, categorizing the underlying stereotypes each sentence pair referenced. This process required a high level of diligence, as it involved closely examining each sentence's nuances within the broader context of societal norms and gender stereotypes.\nOur analysis revealed interesting differences between the two datasets (Figure 3). While the Attitudes and Beliefs category makes up around 40% of the sentences in both datasets, CrowS-Pairs is dominated by the Roles and Behaviors category, which accounts for 42.2% of its sentences, compared to just 15.9% in StereoSet, where it is one of the smallest categories. Conversely, StereoSet places much greater emphasis on the Personality Traits category, with 30.3% of its sentences falling into this group, compared to only 9.7% in CrowS-Pairs. These differences highlight each dataset's distinct approach to capturing gender stereotypes."}, {"title": "5 Conclusions", "content": "In this study, we explored how differing perspectives in two gender stereotyping datasets lead to divergent outcomes. By applying gender stereotype components from social psychology and balancing the datasets, we significantly improved the alignment of intrinsic metrics, highlighting the critical role of data distribution in bias evaluation. Our findings contribute to the broader discussion on gender bias in language models, emphasizing the complexity of bias and the need for a nuanced approach in both measuring and mitigating it, with careful attention to dataset construction."}, {"title": "Limitations", "content": "Our investigation in this study was concentrated on gender stereotypes within language models, specifically examining the two most renowned metrics in this domain. While our study provides valuable insights, it acknowledges several avenues for broadening its scope. Future research could diversify by incorporating additional bias and/or stereotype metrics, extending analyses to languages beyond English, broadening the spectrum of stereotypes examined beyond the confines of gender, and employing a wider array of models. However, each of these potential expansions would entail a significant escalation in both the time and financial resources required for data annotation and model evaluation-resources that were beyond our capacity for this particular study. Despite these constraints, we endeavored to conduct a thorough investigation within our chosen focus area, laying a foundation for more comprehensive inquiries in future research endeavors."}, {"title": "Broader Impact", "content": "This study underscores the importance of metrics in identifying and mitigating biases in Natural Language Processing (NLP), essential for preventing the perpetuation of societal biases through language technologies. The vulnerabilities identified in data annotation and metric methodologies highlight the risk of biases influencing NLP applications and reinforcing societal prejudices. By examining the limitations of current bias measurement tools, our research aims to foster the development of more robust and reliable metrics, contributing to the advancement of equitable and unbiased language technologies. Our findings advocate for enhanced tools and methods for bias detection and mitigation, aspiring to positively impact future NLP research and society at large."}, {"title": "Appendix", "content": "In this section, we detail the foundational components that underpin our experimental framework, delineating the origins and specifications of the resources utilized throughout our study.\nThis subsection outlines the models used in our study, categorizing them into vanilla and debiased variants to provide a comprehensive overview of the computational tools that facilitated our analysis of gender bias in language models. For the vanilla models, we utilized the following pretrained versions available on Hugging Face:"}, {"title": "B.1 Models", "content": "ALBERT-large\nDebiased models were sourced and trained as follows:\nScratch-trained BERT-large and ALBERTlarge models, employing CDA and Dropout debiasing techniques"}, {"title": "B.2 Evaluation Code and Datasets", "content": "In assessing the performance and bias of our models, we relied on critical resources for both datasets and evaluation frameworks, as detailed below.\nFor the StereoSet dataset, our primary resource was the version of this dataset provided by Meade et al. (2022), accessible through the McGill NLP group's GitHub repository . This repository offers the full StereoSet dataset, serving as a cornerstone for evaluating gender stereotypes within our selected language models. The evaluation code and dataset for CrowS-Pairs were sourced directly from its dedicated GitHub repository. This resource facilitated our analysis by providing a structured framework for assessing bias across various dimensions within language models.\nAll operations, including extensions to these resources, were conducted using the transformers library (Wolf et al., 2020), ensuring our methods were built on a robust and widely adopted NLP framework."}]}