{"title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning", "authors": ["Hanlin Wang", "Jian Wang", "Chak Tou Leong", "Wenjie Li"], "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown remarkable reasoning and planning abilities in various real-world applications, including household assistance (Puig et al., 2018; Shridhar et al., 2020b), web browsing (Yao et al., 2022; Deng et al., 2023), and complex scientific reasoning (Wang et al., 2022). These tasks require LLM-based agents to engage in long-horizon interactions with the environment, making sequential decisions to achieve a given goal. Recent research has revealed that agents still make mistakes, yet they struggle to dynamically adjust their subsequent task planning (Xie et al., 2024; Wang et al., 2024a). This underscores the need for effective methods that enhance an agent's ability to improve its decision-making process over time.\nPrevious work has improved agent learning by leveraging enhanced exploratory data (Chen et al., 2023; Yin et al., 2023; Zeng et al., 2023; Xiang et al., 2024). These methods primarily rely on behavior cloning from expert demonstrations, training agents exclusively on successful trajectories. However, this approach prevents agents from proactively self-correcting mistakes, leading to the accumulation of errors and ultimately suboptimal task performance (Xie et al., 2024). To address this limitation, another line of work focuses on preference learning (Song et al., 2024; Xiong et al., 2024) and reinforcement learning (Carta et al., 2023; Tan et al., 2024), integrating failure trajectories additionally to refine decision-making. These approaches train LLM-based agents using explicit error signals or reward functions. However, many long-horizon agentic tasks involve multi-turn interactions, where errors often only become evident at the terminal state (Yuan et al., 2025). As a result, these methods fail to address early-stage deviations, which may not be immediately apparent but accumulate incrementally over time, ultimately leading to significant errors.\nTo address these limitations, we highlight the importance of timely calibration, which allows agents to adjust suboptimal actions as they arise rather than deferring corrections until the end of an exploration. As illustrated in Figure 1, when an early suboptimal action occurs, the subsequent actions are prone to deviate from the optimal trajectory, significantly increasing the risk of task failure. If an agent can engage in self-reflection and calibrate its behavior in real time, it stands a much better chance of successfully completing the task. However, implementing step-level calibrations presents significant challenges. 1) Unlike in mathematical reasoning tasks (Kumar et al., 2024; Xi et al., 2025), where well-defined rules simplify error detection, identifying deviations at each step in long-horizon agentic tasks is considerably more complex. This complexity stems from the dynamic and diverse nature of task execution in interactive environments. 2) As far as we know, the lack of step-level calibration trajectory data poses a major obstacle to training agents to effectively recognize and mitigate deviations.\nIn this work, we propose Step-level Trajectory Calibration (STeCa), a novel agent learning framework that enables LLM agents to perform real-time calibration. STeCa operates by interacting with the environment to perform explorations and utilizes Monte Carlo (MC) sampling (Kakade and Langford, 2002) to estimate step reward for each action. By comparing the rewards of adjacent actions, STeCa effectively identifies deviated actions that lead to suboptimal performance. Then, we utilize off-the-shelf LLMs for reflection, which revises a deviated action into its ground-truth counterpart while generating a reflective thought. The resulting action and its thought, along with subsequent expert trajectory, form a calibrated trajectory. These calibrated trajectories, combined with successful trajectories during exploration, are then used to reinforce the agent's training, optimizing its learning process. We evaluate STeCa on two widely-used agent benchmarks (Puig et al., 2018; Shridhar et al., 2020b). Extensive experimental results demonstrate that STeCa significantly outperforms existing methods, achieving higher success rates across a variety of tasks.\nIn summary, our contributions are as follows:\n\u2022 We highlight the importance of timely calibration in interactive agentic tasks, a crucial aspect largely overlooked by previous methods. Unlike existing approaches that rely on terminal-state error signals or reward functions, we emphasize the need for real-time adjustments to prevent the accumulation of deviations, which can lead to significant errors in long-horizon tasks.\n\u2022 We introduce STeCa, a novel learning framework that enhances LLM agents by integrating an automated deviation detection mechanism and calibrated trajectory construction. It equips agents with essential calibration capabilities for improvement during task execution.\n\u2022 Extensive experiments demonstrate that STeCa significantly outperforms existing methods. By detecting deviations in real-time, STeCa enables agents to effectively mitigate the accumulation of suboptimal actions and handle long-horizon tasks more robustly."}, {"title": "2 Preliminaries", "content": "Task Formulation. This work investigates how LLM-based agents tackle long-horizon tasks within specific environments through interactions. Following previous studies (Song et al., 2024; Xiong et al., 2024), we formalize these agentic tasks as a partially observable Markov decision process (POMDP), which contains the key elements (U, S, A, O,T,R). Here, U denotes the instruction space, S the state space, A the action space, O the observation space, T the transition function (T:S\u00d7A\u2192 S), and R the reward function (R : S \u00d7 A \u2192 [0, 1]). Since the task planning capability of LLM agents is our main focus, U, A, O are subsets of natural language space.\nGiven a task instruction u \u2208 U, the LLM agent \u03c0\u03b8 at time step t takes an action at ~ \u03c0\u03bf(\u00b7|u, et\u22121) and receives the environmental feedback as the observation ot \u2208 O. et-1 denotes the historical interaction trajectory (a1, 01, ..., at\u22121, Ot\u22121). Each action at incurs the environment state to st \u2208 S. The interaction loop terminates when either the agent completes the task or the maximum step is reached. The final trajectory is em = (u, a1, 01, ..., am, 0m), where m denotes the trajectory length. The outcome reward ro(u, em) \u2208 [0,1] indicates the success or failure of the task.\nStep-level Reward Acquisition. It is crucial to acquire step-level rewards as feedback to improve decision-making for LLM agents. Following prior work (Kakade and Langford, 2002; Salimans and Chen, 2018; Xiong et al., 2024), we leverage expert trajectories as demonstrations and ask an LLM"}, {"title": "3 Method", "content": "In this section, we present Step-level Trajectory Calibration (STeCa), a novel learning framework for LLM agents. First, we warm up agent training with supervised fine-tuning (\u00a73.1), equipping LLM agents with necessary task planning capabilities. Then, we focus on calibration trajectory construction (\u00a73.2), which detects deviated actions for an explored trajectory through step-level reward comparison and calibrates them by reflection. Finally, we utilize these calibrated trajectories as a crucial part of data for reinforced training (\u00a73.3). Figure 2 illustrates the overview of STeCa.\n3.1 Warm-up via Supervised Fine-tuning\nSupervised fine-tuning (SFT) on the expert trajectory data has demonstrated promising results, serving as an effective initial step for developing strong agents. We employ ReAct-style (Yao et al., 2023) trajectory to conduct SFT, which additionally generates a Chain-of-Thought (CoT) (Wei et al., 2022) rationale before each action. Considering that the CoT and the corresponding action are generated together, we represent both as a single unit, denoted as at, for simplicity. Given an expert trajectory dataset D = {(ui, ei)}|D|i=1, where each trajectory e = (u, a1, 01, ..., am, 0m), u represents the initial task instruction, at denotes the action (including its rationale) at step t, ot is the corresponding observation, and |D| is the number of trajectories, the SFT loss function is formulated as:\nLSFT(\u03b8) = -Ee~D[\u2211t=1n log \u03c0\u03b8(at|et\u22121)] (4)\nThis warm-up process equips the LLM agent with the necessary task-planning capabilities, enabling it to generate both rationales and actions, resulting in a base agent \u03b8base.\n3.2 Calibration Trajectory Construction\nTo construct the calibration trajectories, we utilize the base agent \u03b8base to explore the environment through interaction. During this exploration, suboptimal actions often lead to a cascade of further suboptimal decisions, causing the trajectory to deviate from successful task completion. We define these actions, which are likely to cause deviations from the optimal trajectory and increase the risk of task failure, as deviated actions. Below, we introduce the details of detecting deviated actions and constructing calibrated trajectories accordingly.\nDeviated Action Detection via Step-level Reward Comparison. Since long-horizon tasks can be modeled as a partially observable Markov decision process (POMDP), where the future action in a task execution process depends on the current action, we must consider this Markov property when detecting deviated actions. To illustrate this, we define the probability of an agent successfully completing a task based on a \"good\" historical trajectory (e.g., an expert trajectory) at time step t as p(a>t|a<t), where we omit the environmental states for simplicity. After executing a subsequent action at+1, the probability of task completion becomes p(a>t+1|a<t+1). If at+1 is a \u201cgood\u201d action (e.g., an expert action), p(a>t+1|a<t+1) will generally be greater than p(a>t|a<t). This is because agentic tasks typically consist of sequential actions, where each action contributes to task completion as the sequence progresses. Thus, by comparing the task completion probabilities before and after executing an action, we can determine whether the action is deviated.\nSpecifically, we employ step-level rewards, calculated via Monte Carlo (MC) sampling introduced in \u00a72, as an approximate estimation of the task completion probabilities. An explored action \u00e2t+1 is classified as a deviated action if its step reward is significantly lower than that of the previous expert action at by a predefined threshold \u03b4; otherwise, it is considered a non-deviated action. The formal detection criterion is defined as follows:\nDeviated Action:\nrs(st, \u00e2t+1) \u2013 rs(St\u22121, at) < \u03b4, (5)\nNon-deviated Action:\nrs(st, \u00e2t+1) \u2013 rs(St\u22121, at) \u2265 \u03b4,\nwhere rs(st\u22121, at) represents the step reward for the expert action at at the t-th step, rs(st, at+1) denotes the step reward for the explored action \u00e2t+1, and \u03b4 \u2265 0 is a threshold parameter.\nCalibrated Trajectory Collection with Reflective Thoughts. As shown in Figure 2, after identifying a deviated action in an explored trajectory, our goal is to enable the LLM agent to \"know\" that the action is deviated and learn how to realign with the task objective. Achieving this goal requires calibrated trajectories for training the agent. Inspired by many previous studies on LLM reflections (Shinn et al., 2023), we employ off-the-shelf LLMs to generate reflective thoughts for calibration. Formally, we concatenate the previously explored trajectory e1:t\u22121, the deviated action \u00e2t, and the corresponding ground-truth action at in the expert trajectory, and prompt a state-of-the-art LLM (e.g., GPT-40 (OpenAI, 2024)) for reflection, transforming the deviated action at into the ground-truth action along with its reflective thought, which is denoted as at. This formulates the subsequent calibrated trajectory ec(t:m) = (at, et+1:m), where et+1:m represents the expert sub-trajectory from the step t + 1 to the end step m. The detailed prompt for this reflection is provided in Appendix E.2. Our calibration dataset De is constructed as:\nDe = {ect:m)} \u222a {ed(1:m)}, (6)\nwhere ed(1:m) = (e1:t\u22121, \u00e2t, \u00eat+1:m) denotes a deviated trajectory, which will be used in subsequent reinforced training. Note that we perform trajectory calibration immediately when detecting the first deviated action, rather than waiting until the trajectory concludes. This approach ensures timely calibration and reduces unnecessary exploration.\n3.3 Reinforced Training\nWhile training on calibration trajectories enhances an agent's calibration capability, relying exclusively on these trajectories may initially hinder their ability to recognize correctness. To mitigate this, we introduce two types of successful data during exploration. First, we construct the explored successful trajectory dataset, De, by collecting successful trajectories \u1ebd1:m that the base agent independently explores from the beginning, along with their corresponding expert trajectories e1:m. Second, we build the expert sub-trajectory dataset, Ds. Specifically, for a failed trajectory \u00eat:m, where the first erroneous action occurs at step t, we extract the corresponding expert action and the subsequent trajectory as \u00eat:m, following Xiong et al. (2024). These sub-trajectories guide the agent in learning from challenging cases more effectively.\nUsing the collected data, we perform reinforced training to enhance LLM agents. Our goal is to guide the agent toward generating optimal trajectories that maximize task performance while minimizing suboptimal outcomes. We introduce trajectory deviation distance (TDD), a measure that quantifies how much a suboptimal trajectory deviates from an optimal one at the trajectory level. Drawing inspiration from Xu et al. (2024), we utilize the nDTW distance dnDTW (defined in \u00a72), to quantify the deviation distance between a suboptimal trajectory es and its corresponding optimal trajectory eo. A smaller dnDTW(es, eo) indicates a lower deviation. This deviation distance is incorporated as a reward signal during reinforced training.\nTo ensure balanced training across the datasets, we refine the reward mechanism by incorporating the trajectory deviation distance. The reward functions for each type of data are defined as follows:\nrc = 1 + \u03b7\u00b7dnDTW(ec(t:m), d(t:m)), (7)\nrs = 1 + \u03b7\u00b7dnDTW(et:m, \u00eat:m), (8)\nre = 1 \u2212 \u03b7 \u00b7 dnDTW(\u1ebd1:m, e1:m), (9)\nwhere for the calibration trajectory ec(t:m) and the expert sub-trajectory et:m, we increase the reward as the deviation distance grows, encouraging the agent to calibrate larger deviations. For the explored successful trajectory \u1ebd1:m, we reduce the reward for unnecessary explorations when the deviation distance increases, discouraging deviations from optimal behavior. \u03b7 is a temperature coefficient that controls the impact of deviation distance on the reward. Finally, we integrate these rewards into reinforcement training using the policy gradient (Peters and Schaal, 2007) algorithm. The overall training objective is given by:\nL(\u03b8) =\nE(ec(tm),\u20ac1:t-1)~De [rc \u2022 log \u03c0\u03b8(ec(t:m) | \u20ac1:t-1)]\n+ E(et:m,e1:t-1)~D. [rs \u2022 log \u03c0\u03b8(et:m | \u20ac1:t-1)]\n+ E(\u20ac1:m,u)~De [re log \u03c0\u03b8 (\u01131:mu)]. (10)"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nDatasets. We conduct experiments on two representative agentic task datasets: VirtualHome (Puig et al., 2018) and ALFWorld (Shridhar et al., 2020b). For ALFWorld, we utilize datasets constructed by Song et al., 2024. For the Virtual-Home dataset, we leverage the predefined tasks from the ActivityPrograms knowledge base (Puig et al., 2018) and construct a corresponding dataset in a manner closely aligned with the ALFWorld dataset. Please refer to Appendix A for further details regarding the dataset construction process and associated statistical information.\nBaseline Methods. We evaluate STeCa against the following two categories of baselines: (1) prompting-based approaches, including GPT-3.5-turbo (Ouyang et al., 2022) and GPT-4 (Achiam et al., 2023). (2) tuning-based methods, which include supervised fine-tuning (SFT) methods, such as pure SFT (Chen et al., 2023), RFT (Yuan et al., 2023), and E2CL (Wang et al., 2024a), reinforcement learning-based methods such as PPO (Schulman et al., 2017) and Step-PPO (Wang et al., 2024b), as well as preference learning methods like ETO (Song et al., 2024) and IPR (Xiong et al., 2024). Additional details about the baselines are provided in Appendix B.\nImplementation Details. We utilize Llama-2-7B-Chat (Touvron et al., 2023) as the base model for training LLM agents. We set \u03b4 = 0 as the threshold to detect deviated actions in two environments. We use \u03b7 = 1 for VirtualHome and \u03b7 = 0.01 for ALFWorld to weight the contribution of trajectory deviation to the reward. To obtain step-level rewards with MC sampling, we set the temperature to 1 and the number of samples N to 5. More details are presented in Appendix C.\nEvaluation Metrics. Following existing studies (Song et al., 2024; Xiong et al., 2024), we adopt the Average Final Reward as our evaluation metric. This metric measures the success rate of test tasks. In ALFWorld, the environment provides binary final rewards, where a reward of 1 indicates task completion and 0 indicates failure. Similarly, in VirtualHome, a trajectory is deemed successful if the final environment state aligns with a predefined target state and yields a reward of 1; otherwise, the reward is 0. This metric ensures a consistent measure of task performance across both environments.\n4.2 Main Results\nTable 1 summarizes the performance of various methods on long-horizon tasks in the Virtual-Home and ALFWorld environments. The proposed method, STeCa, achieves the highest overall performance, with an average reward of 70.9, significantly outperforming the baseline methods. Compared to prompting-based methods, which exhibit relatively poor performance, STeCa demonstrates a significant improvement. These results highlight the inherent limitations of closed-source LLMs relying solely on prompt engineering. As a tuning-based method, STeCa demonstrates consistent superiority over prior approaches. Specifically, it achieves an average final reward of 70.9, surpassing IPR, the previous state-of-the-art method with an average reward of 68.6, by 3.4%. This improvement highlights the effectiveness of trajectory calibration in enhancing generalization and overall performance. Moreover, STeCa outperforms E2CL, a method that incorporates self-reflection mechanisms, by 4.0%. Notably, STeCa achieves this without requiring additional iterative training, underscoring its superior training efficiency.\nTo further validate our method, we conducted an ablation study. Using SFT combined with Direct Preference Optimization (Rafailov et al., 2024) (w/ SFT+DPO), the average reward dropped slightly to 70.0, while SFT training alone (w/o RT) resulted in a further decline to 69.6. Although these variants performed competitively, neither matched the performance of STeCa, highlighting the effectiveness of our proposed mechanisms."}, {"title": "5 Discussions and Analyses", "content": "In this section, we provide detailed analyses of the STeCa framework from the following aspects.\n5.1 Effectiveness with Different Base Models\nTo validate the broad effectiveness of our method, we evaluate STeCa with different base models, including Mistral-7B and Llama-3-8B-Instruct, on the ALFWorld environment. We compare its performance with SFT and IPR across both seen and unseen tasks. As shown in Table 2, STeCa consistently outperforms both SFT and IPR across multiple base models. Notably, in unseen tasks, STeCa achieves a 17.1% improvement over SFT on Mistral-7B, highlighting its generalization ability for developing LLM-based agents. Furthermore, with Llama-3-8B-Instruct, a stronger backbone model, STeCa further achieves better performance, underscoring its potential for building advanced agents in the future.\n5.2 Comparisons between Variants of STeCa\nVariants of Step-Level Reward Acquisition. To investigate the impact of different methods for reward acquisition, we conducted experiments using GPT-40 and a trained reward model to annotate reward for each step action while keeping all other processes unchanged. The detailed process of step action reward acquisition is described in Appendix D.1. As shown in Table 3, our method utilizing MC sampling for step reward acquisition achieves superior performance compared to alternative variants, highlighting the effectiveness of MC sampling for reward acquisition. Notably, employing GPT-40 to directly annotate rewards for step actions demonstrates performance comparable to our method, suggesting that step rewards can be effectively obtained through more computationally efficient approaches. This finding provides a promising direction for future research into optimizing the efficiency of reward acquisition.\nVariants of Reflective Thought Generation. In STeCa, we employ GPT-4o to generate reflection thoughts for constructing calibration trajectories. To evaluate the impact of reflection quality on performance, we attempted to prompt the base agent \u03c0\u03b8 to generate reflections while keeping all other processes unchanged. The results, summarized in Table 3, reveal a significant performance degradation, underscoring the critical importance of high-quality reflection generation for achieving optimal results. This finding also suggests that the base agent, trained solely on expert trajectories, lacks effective reflection capabilities.\n5.3 Analyses of Deviated Action Detection\nTo validate the empirical Markov property introduced in Section 3.2, i.e., non-deviated \"good\" actions increase the likelihood of task completion, we conducted a statistical analysis using expert trajectories. Specifically, we compared task completion probabilities at varying distances from task completion, employing MC step rewards as a proxy for these probabilities. As illustrated in Figure 3, MC step rewards monotonically increase as the agent progresses toward task completion in both environments. This trend demonstrates that the accumulation of optimal actions significantly contributes to task completion. Conversely, deviated actions consistently reduce the task completion probability, further supporting our approach of using step reward comparisons between adjacent steps as a reliable criterion for detecting deviated actions.\n5.4 Analyses of Calibration\nIn this section, we compare the calibration capabilities of STeCa and baseline methods. We evaluate calibration performance using the average final reward achieved upon successful task completion in the presence of deviated actions. To enable this analysis, we constructed datasets containing historical trajectories with deviated actions, categorized"}, {"title": "6 Related Work", "content": "LLM Agent Learning. LLM agents are widely used for tackling complex real-world tasks (Wang et al., 2023a; Hu et al., 2024; Wang et al., 2024a), relying on iterative interactions with their environment guided by task objectives and constraints. However, in long-horizon planning, excessive interactions make them prone to suboptimal actions, increasing the risk of failure. While closed-source LLMs demonstrate strong intelligence, open-source counterparts still lag behind (Liu et al., 2023; Wang et al., 2023b). To address this gap, some studies focus on improving task success rates by increasing the likelihood of generating optimal actions (Chen et al., 2023; Yuan et al., 2023). Alternatively, another line of research seeks to mitigate suboptimal actions by collecting them and applying preference learning methods to reduce their occurrence (Song et al., 2024; Xiong et al., 2024). Recently, researchers have explored the capacity of LLM agents to self-correct errors, enhancing their ability to ensure successful task completion (Wang et al., 2024a; Qu et al., 2024). However, these methods primarily focus on self-correction after errors have already occurred, lacking the ability to detect suboptimal actions in advance and calibrate subsequent planning accordingly.\nProcess Supervision. Process supervision provides fine-grained guidance, making it a promising approach for addressing long-horizon problems (Uesato et al., 2022). Early studies have explored obtaining step-level rewards and using them to optimize intermediate processes through reinforcement learning (Lightman et al., 2023; Deng et al., 2024; Wang et al., 2024b). Others have focused on constructing step-level positive and negative data pairs and applying preference learning techniques to achieve more precise optimization (Xiong et al., 2024; Jiao et al., 2024). However, existing studies have yet to address the construction of step-level reflection data. Such data could empower LLM agents to detect suboptimal actions, analyze the reasons for their suboptimality, and determine how to calibrate them to ensure successful task completion."}, {"title": "7 Conclusion", "content": "In this paper, we introduce STeCa, a novel agent learning framework designed to enhance the performance of LLM agents in long-horizon tasks. STeCa identifies deviated actions through step-level re-"}, {"title": "Limitations", "content": "While our approach demonstrates superior performance compared to baseline methods, it is important to acknowledge the limitations of our current work as follows:\n(1) Computational Inefficiency: Although the Monte Carlo (MC) sampling approach in STeCa achieves superior performance in constructing step rewards compared to alternative methods, it requires a substantial number of sampling iterations, resulting in significant computational overhead. This inefficiency represents a notable limitation of our current implementation. Future work should focus on developing more efficient methods for constructing step rewards while preserving the performance advantages of our approach.\n(2) Limited Utilization of Step Rewards: While our approach leverages step rewards to identify and evaluate deviated actions effectively, it does not fully exploit the potential of step rewards for broader decision-making or optimization tasks. This constrained utilization may limit the overall performance improvements that could be achieved by incorporating step rewards into other aspects of the framework. Future research should explore strategies to better harness the rich information embedded in step rewards to enhance the overall effectiveness and adaptability of the system."}, {"title": "Ethics Statement", "content": "This work aims to develop LLM agents within simulated environments. The VirtualHome and ALF-World environment setup and related data strictly follow the specifications of VirtualHome (Puig et al., 2018) and ALFWorld (Shridhar et al., 2020b). We utilize VirtualHome v2.3.0\u00b9 (MIT license\u00b2) and ALFWorld\u00b3 (MIT license\u2074) to conduct our experiments.\nhttps://github.com/xavierpuigf/virtualhome/tree/master\nhttps://github.com/xavierpuigf/virtualhome/blob/master/LICENSE\nhttps://github.com/alfworld/alfworld\nhttps://github.com/alfworld/alfworld/blob/master/LICENSE\nIn our paper, the models we use for fine-tuning are all open-source, and we will strictly follow the protocols for the academic use of these LLMs. Additionally, we acknowledge the use of AI assistants, including Copilot and ChatGPT, in supporting our coding and writing processes."}, {"title": "A Datasets and Preprocessing", "content": "ALFWorld ALFWorld (Shridhar et al., 2020b) offers interactive TextWorld environments that are meticulously aligned with the embodied environments introduced in ALFRED (Shridhar et al., 2020a). This framework challenges agents to navigate complex household settings and execute high-level instructions, thereby testing their ability to perform practical tasks. The dataset is structured into two distinct evaluation sets: a seen set, designed to assess in-distribution generalization, and an unseen set, which comprises novel task instances to evaluate out-of-distribution generalization capabilities. At the conclusion of each trajectory, the environment provides a binary reward, indicating whether the agent has successfully completed the assigned task. This setup facilitates a clear and measurable assessment of agent performance in both familiar and novel scenarios.\nVirtualHome VirtualHome (Puig et al., 2018) is a comprehensive dataset comprising 292 high-level household tasks and 1,374 unique action plans, distributed across 6,201 diverse environments. The dataset was meticulously curated through manual annotations provided by Amazon Mechanical Turk workers, who labeled tasks and their corresponding action plans in detail. Each entry in the dataset is structured into three components: a high-level task, a descriptive explanation, and executable action programs compatible with the VirtualHome environment. To evaluate task completion, we executed all tasks and recorded the final state of the environment upon completion. A task is considered successfully completed if the state of the environment after exploration by the LLM agent matches the predefined target state. To ensure data quality, the dataset was filtered by retaining only trajectories with successful final outcome rewards and verifying that every action in the planning sequence is executable within the environment. Furthermore, to maintain an appropriate level of task complexity, the dataset was restricted to trajectories with planning lengths ranging from 3 to 10 steps. This rigorous filtering process ensures a robust and reliable subset of data, suitable for in-depth analysis and model training.\nDataset Construction Since the original trajectories do not include reasoning processes preceding each action, we adopt established methodologies from prior work (Song et al., 2024; Xiong"}, {"title": "B Baseline Methods", "content": "Our baseline methods are as follows: 1) SFT (Chen et al., 2023), which employs behavior cloning on expert trajectories alone, serving as the base agent for STeCa and other baseline methods. 2) PPO (Schulman et al., 2017), a widely-used reinforcement learning algorithm, optimizes final trajectory rewards. Additionally, we apply PPO for stepwise action optimization. 3) RFT (Yuan et al., 2023), which extends expert trajectories by incorporating successful trajectories discovered by the base agent, followed by fine-tuning on the expanded dataset. 4) ETO (Song et al., 2024), which constructs positive and negative trajectory pairs and optimizes them using Direct Preference Optimization (DPO) (Rafailov et al., 2024). 5) E2CL (Wang et al., 2024a), which leverages planning data, feedback data, and correction data to supervise the fine-tuning of LLM agents. 6) IPR (Xiong et al., 2024), which enhances trajectory pairs by augmenting sub-trajectory pairs based on step rewards, building upon ETO's framework, and trains LLM agents using preference learning methods."}, {"title": "C Additional Implementation Details", "content": "During the construction of the base agent, we train the model for 3 epochs with a batch size of 16 and a learning rate of 3e-6, employing the AdamW optimizer and a cosine learning rate scheduler. For reinforced training, the model is fine-tuned for only 1 epoch.\nDuring the inference phase, all methods are evaluated using the ReAct-style interaction format,"}, {"title": "D Experimental Settings about Analyses", "content": "D.1 Variants of Step-level Reward Acquisition\nIn addition to the Monte Carlo (MC) sampling for step-level reward acquisition, we further employ the following two variants: (1) GPT-40 Annotation: In Section 3.2, we collect Monte Carlo (MC) step rewards corresponding to various step actions. To annotate all explored step actions, we randomly select several samples as in-context examples and utilize GPT-4 for annotation. The detailed prompt used for this process is provided in Appendix E.3. (2) Reward Model Prediction: We also leverage the data collected in Section 3.2, where each step action is associated with an MC step reward, to train a reward model capable of predicting scores for step actions. Specifically, we use the Llama-2-7B-Chat (Touvron et al., 2023) model as the base architecture. To mitigate overfitting, we add a dropout layer to the output layer, followed by a linear layer to map the output to a scalar score. Additionally, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2021) for efficient fine-tuning. The model is trained for 3 epochs, and\nD.2 Detailed Settings for Calibration Analysis\nWe randomly select 100 pieces of data from Dc(e1:t-1, \u00e2t, ec(t:m), \u00eat+1:m) for both ALFWorld and VirtualHome to serve as the seen test set. Additionally, we randomly select 100 pieces of data from the unseen test set. Following the procedure outlined in Section 3.2, we construct the calibration dataset (e1:t\u22121, \u00e2t, ec(t:m), \u00eat+1:m) derived from unseen scenarios. After assembling the calibration datasets for both seen and unseen scenarios in VirtualHome and ALFWorld, we use these datasets to evaluate the calibration performance of the LLM agent. Specifically, we traverse the step actions from (e1:t\u22121, \u00e2t) to obtain the initial environment state. We then deploy the LLM agent to explore the environment starting from this state and assess whether it can successfully complete the task.\nFor the second experiment, we reuse the previously collected calibration dataset. However, in this case, we traverse the step actions only from e1:t-1, excluding the deviated action \u00e2t. We refer to this configuration as the \"w/o deviated action\" setting."}, {"title": "E Prompt Templates", "content": "E.1 Inference Prompt\nAs shown in Figure 6, we provide the inference prompt for each task, which include a general instruction, a one-shot example, the specific task instruction and history trajectory."}, {"title": "F Case study", "content": "Figure 9 provides an example demonstrating STeCa's calibration capability during the planning process. As illustrated, STeCa autonomously identifies deviated actions, reflects on them, and successfully completes the task. In contrast, the other two agents fail to achieve the same level of performance, further highlighting the effectiveness of STeCa in addressing such challenges."}]}