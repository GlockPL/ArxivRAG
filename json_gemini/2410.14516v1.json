{"title": "DO LLMS \u201cKNOW\u201d INTERNALLY WHEN THEY FOLLOW INSTRUCTIONS?", "authors": ["Juyeon Heo", "Christina Heinze-Deml", "Oussama Elachqar", "Udhay Nallasamy", "Shirley Ren", "Andy Miller", "Kwan Ho Ryan Chan", "Jaya Narain"], "abstract": "Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs' internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs' instruction-following, paving the way for reliable LLM agents.", "sections": [{"title": "1 INTRODUCTION", "content": "Given the potential of large language models (LLMs), there has been significant interest in utilizing these models to build personal AI agents. For instance, one could imagine deploying an LLM as a personal healthcare assistant, such as a fitness or nutrition planner, or for psychological counseling (Li et al., 2024b; Wang et al., 2023; Tu et al., 2024). Compared to traditional machine learning-based AI agents, LLMs offer the advantage of being easily adaptable through prompting, allowing users to provide guidelines and personal information without the need to retrain model weights.\nInstruction-following is critical in the development of personal AI agents with LLMs through prompts because these models must adhere to the constraints and guidelines to ensure safe and trustworthy interactions. For example, suppose an LLM is building a personal fitness plan for a user with knee problems. To avoid knee problems for the user, the LLM must follow the instruction of not recommending knee-intensive movements or any exercises that could lead to potential injury. Similarly, in a nutrition planner, the LLM should avoid generating harmful recommendations, such as suggesting inappropriate food for pregnant women or children with diabetes."}, {"title": "2 DO LLMS KNOW WHEN THEY SUCCEED OR FAIL TO FOLLOW\nINSTRUCTIONS?", "content": "In this section, we aim to identify the dimension within the models' representation space that is closely associated with instruction-following. We use linear probes to determine the internal signals that separate successful instruction-following from failures and examine whether this dimension generalizes to different tasks and instruction types. By exploring different tokens and layers within the models, we seek to understand how and when instruction-following information is encoded."}, {"title": "2.1 IFEVAL-SIMPLE", "content": "First, we selected the IFEval dataset (Zhou et al., 2023) as a base, due to its objective evaluations with verifiable instructions, thereby minimizing uncertainties from ambiguous evaluation criteria.\nThe IFEval dataset comprises 25 instruction types under 9 categories, with each instruction type paired with a distinct set of tasks approximately 20 tasks per instruction type. Because of the relatively small number of tasks per instruction type, internal model states resulting from these prompts contain a mix of both instruction-following and task-specific details.\nTo isolate the dimension related specifically to instruction-following, we generated a modified version of the IFEval data, called IFEval-simple. First, we selected 5 instruction types that are likely to be used in real-world applications for Al agents. For example, ensuring the inclusion (keywords:existence) or exclusion (keywords:forbidden) of specific keywords, specifying the frequency of certain keywords (keywords:frequency), generating responses with placeholders (detectable_content:place_holders), and requiring responses to end with predefined sentences (startend:end checker). We excluded more complex or impractical instructions, such as those requiring omission of punctuation, as they are less relevant for practical use cases.\nSecond, we generated 100 tasks using GPT-4, similar to the original tasks in IFEval, where each instruction type is paired with the same set of 100 tasks. By pairing each instruction type with the same set of 100 tasks, we ensure that linear probes trained on the model's representations are more likely to capture information solely related to instruction-following, without the confounding influence of varying tasks. The instructions assigned to each task vary in detail based on the context. For example, for an instruction type focused on keyword inclusion or exclusion, a resume-writing task might require keywords like \u2018skills' and 'career', while a joke about a programmer might involve terms like 'syntax' or 'code'. These variations introduce diverse challenges, testing the model's adaptability in following instructions. Example tasks are provided in Appendix Table 5 and Table 6.\nThe instruction-following accuracy for IFEval-simple datasets is presented in Appendix Table 11."}, {"title": "2.2 METHODS", "content": "Representations We analyzed four language models: LLaMA-2-7B-chat (Touvron et al., 2023), LLaMA-2-13B-chat (Touvron et al., 2023), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Phi-3-mini-128k-instruct (Abdin et al., 2024). For each model, we looked at the representations between tokens - the first, middle, and last tokens, representing the LLMs before, during, and after they generate responses. We also examined three layers (early, middle, last) to identify where instruction-following information is encoded within the models' internal state. Specifically, we used layers 16, 32, and 40 and for LLaMA-2-13B-chat and 14, 26, and 32 for other three models. To avoid randomness in decoding, we employed greedy decoding without sampling.\nLinear Probes We trained linear probes on the representations to identify the instruction-following dimension. A simple linear model was trained on instruction-following success outcome, optimized for 1000 epochs with AdamW, a 0.001 learning rate, and 0.1 weight decay.\nTrain-test split and metric We assessed task generalization and instruction-type generalization by splitting the data into training and testing sets, as shown in Figure 1. IFEval-simple has 5 instruction types, each paired with the same set of 100 tasks. To evaluate task generalization, we split the data by the task dimension, using a 70-30 train-test split across the 100 tasks. To evaluate instruction-type generalization, we applied a leave-one-out approach, over the instruction-type dimension. To evaluate performance, we use the Area Under the Receiver Operating Characteristic Curve (AUC)(Pedregosa et al., 2011), assessing the accuracy of binary predictions for each model on unseen tasks and instruction types."}, {"title": "2.3 RESULTS", "content": "Linear probes generalize across unseen tasks The task generalization results in Table 1 show that linear probes performed well across different tasks when the instruction type remains consistent. The AUROC scores, which range from 0.7 to 0.8 using the first token, suggest that the input embeddings of these models possess a shared geometry related to instruction-following that generalizes well across varied tasks. This is particularly beneficial in the context of buliding AI agents, where a pre-defined consistent set of instructions needs to be followed across different tasks. For example, if a probe is trained on examples of an instruction type like \u201cPlease do not include these keywords\" using examples from resume writing and nutrition coaching, the linear probe can predict if the model follows the same instructions type even unseen tasks, such as creating a warm-up plan without knee-intensive exercises. Additionally, we plot the principal components analysis (PCA)"}, {"title": "3 REPRESENTATION ENGINEERING", "content": "We identified a dimension within the input embedding space associated with instruction-following. To evaluate whether this dimension significantly impacts the models' behavior, we manipulated the representations along this direction using representation engineering (Marks & Tegmark, 2023; Zou et al., 2023). An increase in the models' instruction-following success rate tied to manipulations along the identified direction validates the role of the dimension in shaping the models' generation outcomes toward instruction adherence."}, {"title": "3.1 SETTINGS", "content": "Method For each input representation $R_{original}$, we applied a transformation in the identified direction D using the formula $R_{updated} = R_{original} + \\alpha \\times D$, where $\\alpha$ is a scaling hyper-parameter. We applied this transformation to all input representations, including both success and failure cases, to evaluate whether RE could improve instruction following universally, without disrupting cases where the model was already successful. This adjustment was applied to the representations in the last layer of the model, as it was more robust to variations in $\\alpha$. We focused on the representation of the first token, which corresponds to the input embedding before any response generation, since the goal of representation engineering (RE) is to adjust internal representations before the response is generated to improve the model's instruction adherence. The direction D is the weight of a linear probes trained on all IFEval-simple dataset.\nMetric We evaluated the success rate (SR) of instruction-following using predefined evaluation functions from the IFEval (Zhou et al., 2023). Additionally, we assessed the quality of the responses using GPT-4, scoring each response on a scale from 0 to 9 based on its relevance to the given task. We defined quality ratio (QR) as the number of responses scoring above 7 divided by the total number of responses that successfully follow instructions (this cutoff was defined based on the distribution of quality scores). F2T (False to True) and T2T (True to True) show how many failed responses became successful and how many successful ones remained so after modification. The Success conversion ratio (SCR) := $F2T \\over (F2T+F2F)$ indicates the proportion of originally failed responses that became successful after modification, while Success preservation ratio (SPR) := $T2T \\over (T2T+T2F)$ reflects the proportion of originally successful responses that remained successful.\nBaseline and hyperparameter selection To demonstrate the effectiveness of the identified instruction-following dimension, we compared it against random directions. Each model and in-"}, {"title": "3.2 RESULTS", "content": "RE on instruction-following direction improves success rate while maintaining quality Our experiments demonstrate that applying the RE direction generally improves the instruction-following success rate (SR) across most models and instruction types. As shown in Table 4, the SR with the instruction-following direction usually outperforms the original success rate and is lower bounded by the the original SR \u2013 that is, the instruction-following dimension does not lead to worse than original SRs. Additionally, the QR remains equal to or higher than the original, indicating that RE can be applied with minimal risk of reducing response quality. Figure 5 in the Appendix provides an illustrative example of modified responses. In this case, the task was to write a resume with the instruction to include three specific keywords. The original response only included one keyword, whereas the modified response, guided by the instruction-following direction, successfully incorporated all three keywords, demonstrating the effectiveness of RE in enhancing instruction adherence.\nInstruction-following direction is better than random directions When comparing RE direction to random directions, RE consistently outperforms random directions in increasing the success rate across all instruction types and models, as illustrated in Table 4 and Figure 3. The ratios of True-to-True (T2T) and False-to-True (F2T) transitions are typically larger for the instruction-following direction than for random directions, indicating a more reliable improvement in success rates."}, {"title": "4 INTERPRETING THE INSTRUCTION-FOLLOWING DIMENSION", "content": "While manipulating representations along the instruction-following dimension reveals that it influences a model's behavior, the meaning behind this manipulation remains unclear. To interpret the meaning of the instruction-following dimension, we conduct a sensitivity analysis to investigate the relative of perturbations on the internal state of LLMs, compared to our identified direction. We consider three perturbation types: task familiarity, instruction difficulty, and phrasing. We (1) systematically alter the original input prompts in IFEval-simple dataset for each perturbation, (2) compute the resulting difference in internal state representation space before and after the perturbation, and (3) compute the cosine similarity between the perturbation-induced difference vector and the instruction-following dimension we identified. We designed prompt changes for each perturbation:\n(1) Task Familiarity: We investigated whether the instruction-following dimension might be related to how familiar the model is with a given task. For example, the task \"write a resume for software engineer\" might be more familiar to the model than \"write a summary about current events\", if it was more common in the data used to train the LLMs. If a task is more familiar to a model, it may be easier for the model to follow instructions regarding that task. To perturb the model on task familiarity, we kept the instruction constant while changing the task to one with lower perplexity (Jelinek et al., 1977). Perplexity measures the probability of tokens in generation, reflecting task familiarity (Gonen et al., 2022), where high perplexity indicates a familiar task and vice versa.\n(2) Instruction Difficulty: We investigated the relationship of the instruction-following dimension with the complexity of the instructions. We perturbed the instruction difficulty by simplifying instructions by relaxing instruction-related constraints. For example, in the original instruction \"please include keywords: coding, Python, computer, experience\", we reduced the complexity by reducing the number of keywords required in the instruction to \u201cplease include the keywords: coding, Python\".\n(3) Phrasing Modification: Finally, we examined whether the instruction-following dimension was correlated to how the prompt is phrased. We rephrased the prompts while keeping the meaning of the task and the instruction unchanged. For example, we modified \u201cWrite a resume for software engineer. Please include keywords such as coding, Python, computer, experience\" to \"I want you to write about software engineer resume including four words coding, Python, computer, or experience\". We used GPT-4 to rephrase both the task and instruction in the input prompt, and applied GPT-4 again to validate that the meaning of the contents remained the same after rephrasing.\nWe selected 20 prompts, each containing a task and an instruction from the 'forbidden keyword' instruction type in IFEval-simple dataset. For each perturbation type, we created five modified versions of each prompt. We then averaged the representations of these modified prompts and calculated the difference between this averaged representation and the representation of the original prompt. Finally, we assessed how well this difference vector aligned with the instruction-following dimension by computing the cosine similarity."}, {"title": "5 RELATED WORK", "content": "Instruction-following in LLMs Recent research has introduced various benchmark datasets to evaluate the instruction-following capabilities of LLMs across different contexts(Zhou et al., 2023; Qin et al., 2024; Yan et al., 2024; Xia et al., 2024). Among these, we selected the IFEval dataset (Zhou et al., 2023) as the foundation for our study, due to its structured and objective evaluation framework that minimizes ambiguity by using verifiable instructions. Beyond evaluation, several approaches have been proposed to improve instruction-following performance, such as modifying"}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "6.1 LLMS INTERNALLY RECOGNIZE WHETHER THEY WILL FOLLOW INSTRUCTIONS\nOur findings suggest that LLMs may possess an inherent ability to predict whether they will successfully follow instructions, even before the generation process begins. This capability is supported by several key observations:\nLLMs generalize well across tasks but struggle with different instruction types We find that while LLMs can generalize across different tasks, they struggle with generalization across different instruction types. This suggests that distinct instruction categories may have unique geometries within the models' internal representation space, making it more challenging for the model to generalize across them.\nLLMs can predict instruction success from the first token We observe that the model's internal representations are separable from the very first token, which corresponds to the embedding of the input prompt. This indicates that the likelihood of instruction-following success can be determined early in the process, before the model generates any responses. This highlights the critical"}, {"title": "6.2 THE ROLE OF INPUT PROMPT REPRESENTATION IN INSTRUCTION-FOLLOWING FAILURES", "content": "Our findings highlight the role of representation of the input prompt in determining instruction-following success in LLMs. We discover that the instruction-following dimension identified in our analysis is sensitive to changes in how the input prompt is phrased. This sensitivity explains several behaviors of LLMs:\nWhy LLMs fail in following instructions LLMs may fail to follow even simple, clear instructions because the encoding of the input prompt within the models' internal representation space can be easily disrupted. Our findings suggest that small variations in how a prompt is phrased can result in significant differences in how the model processes the instruction, leading to failures in adherence. This issue arises not from ambiguity in the instruction itself, but from the LLM's sensitivity to the exact structure and phrasing of the input, which influences how the instruction is embedded and processed internally. As a result, the model might not consistently follow instructions, even when they are clear and familiar.\nWhy Prompt Engineering (PE) works PE operates by slightly altering the phrasing of a prompt, which in turn changes how the input is encoded within the model. This subtle shift in encoding can move a representation from a failure class to a success class in terms of instruction-following within the input embedding space. Our work with representation engineering achieves a similar outcome, but instead of modifying the input text, we make adjustments directly in the representation space. Both approaches influence the model's internal states, highlighting the importance of the input encoding process. Our observations align with prior research showing LLM sensitivity to prompt formatting (Lu et al., 2023; Sclar et al., 2023; Gonen et al., 2022).\nSemantic sensitivity of LLM input embedding space The fact that instruction-following success or failure can be altered by slight prompt rephrasing shows that the LLM's input embedding space is semantically sensitive. This sensitivity suggests that the model's internal representation of prompts is brittle, making LLMs vulnerable to small changes in how an input is framed or phrased. This fragility, likely driven by the model's large size and the complexity of its training dynamics, creates challenges in ensuring robust instruction adherence. Given this sensitivity, future efforts should focus on making LLMs' input embedding space more robust and reliable. One potential approach is to fine-tune models with an explicit focus on stabilizing instruction-following by utilizing the identified instruction-following dimension."}, {"title": "6.3 LIMITATIONS AND FUTURE WORK", "content": "Our analysis was primarily focused on a specific set of tasks and models. Although our current results are consistent across the models we studied, future work could extend these findings by evaluating additional models to determine whether the identified instruction-following dimension generalizes across different LLM architectures. Additionally, expanding the dataset to include a wider variety of instruction-following cases could enrich the analysis and improve the generalizability of our findings. We focused our investigation on simple modeling approaches to identify an instruction-following dimension and evaluate its practical significance. Future work could include additional methods train linear probes, particularly in handling domain shifts. Similarly, better approaches to representation engineering (Zou et al., 2023) could further improve the success rate of instruction-following modifications. Finally, unambiguously interpreting the meaning of the instruction-following dimension remains an open question. We considered three hypotheses and found that phrasing modification was most closely related to the dimension associated with instruction-following using a perturbation-based approach. Additional investigations to develop systematic approaches to interpret the dimension could add to a deeper understanding of its meaning and implications."}, {"title": "A APPENDIX", "content": "A.1 EXAMPLES OF IFEVAL-SIMPLE DATASET\nThe IFEval-simple dataset is created to focus specifically on instruction-following, removing the\nconfounding influence of varying tasks present in the IFEval dataset (Zhou et al., 2023). In this\nmodified version, we select 5 instruction types commonly used in real-world AI applications, such\nas including or excluding keywords, generating responses with placeholders, and ensuring specific\nphrases are present in the generated text. These instructions are paired with the same set of 100\ntasks to help isolate the instruction-following dimension. By using the same set of tasks across all\ninstruction types, we ensure that any differences in model behavior are attributed to instruction-\nfollowing rather than task-specific features. This allows us to more effectively probe the model\u2019s\ninternal representations and evaluate how well it can follow instructions across various scenarios."}]}