{"title": "DEL-RANKING: RANKING-CORRECTION DENOISING FRAMEWORK FOR ELUCIDATING MOLECULAR AFFINITIES IN DNA-ENCODED LIBRARIES", "authors": ["Hanqun Cao", "Chunbin Gu", "Mutian He", "Ning Ma", "Chang-Yu Hsieh", "Pheng-Ann Heng"], "abstract": "DNA-encoded library (DEL) screening has revolutionized the detection of protein-ligand interactions through read counts, enabling the rapid exploration of vast chemical spaces. However, noise in read counts, stemming from nonspecific interactions, can mislead this exploration process. Neural networks trained on DEL libraries have been employed to discern and capture nuanced, task-specific patterns within chemical landscapes. However, existing methods overlook two critical aspects: (1) the inherent ranking nature of read counts and (2) the potential of true activity labels to correct systematic biases. We present DEL-Ranking, a novel distribution-correction denoising framework that addresses these challenges. Our approach introduces two key innovations: (1) a novel ranking loss that rectifies relative magnitude relationships between read counts enabling the learning of causal features determining activity levels, and (2) an iterative algorithm employing self-training and consistency loss to establish model coherence between activity label and read count predictions. Furthermore, we contribute three new DEL screening datasets, the first to comprehensively include multi-dimensional molecular representations, protein-ligand enrichment values, and their activity labels. These datasets mitigate data scarcity and incomplete data issues in AI-driven DEL screening research, providing novel benchmark datasets for this field. Rigorous evaluation on diverse DEL datasets demonstrates DEL-Ranking's superior performance across multiple correlation metrics, with significant improvements in binding affinity prediction accuracy. Our model exhibits zero-shot generalization ability across different protein targets and successfully identifies potential motifs determining compound binding affinity. Our work not only advances the field of DEL screening analysis but also provides valuable resources for future research in this area.", "sections": [{"title": "INTRODUCTION", "content": "DNA-encoded library (DEL) technology has emerged as a revolutionary approach for protein-ligand binding detection, offering unprecedented advantages over traditional high-throughput screening methods. The DEL screening process involves multiple stages including cycling, binding, washing, elution, and amplification . This process generates large-scale read count data, serving as a proxy for potential binding affinity. The read counts represent the frequency of each compound in the selected pool after undergoing target protein binding and subsequent processing steps. These read counts typically include control counts and protein counts, representing values corresponding to the scenarios without and with specific protein targets, respectively.\nDEL screening enables the rapid and cost-effective evaluation of vast chemical spaces, typically en-compassing billions of compounds, against biological targets of interest. This approach has gained widespread adoption in drug discovery due to its ability to identify novel chemical scaffolds and accelerate lead compound identification. This study aims to enhance the correla-tion between predicted read counts and true binding affinity. However, DEL screening read counts are subject to Distribution Noise, stemming from both experimental variabilities and intrinsic factors. These noisy elements introduce discrepancies between read counts and experimentally determined K\u2081 values, which represent the binding affinity of a ligand to its target protein in the training data, making this task highly challenging.\nTo mitigate noise, researchers apply threshold-based filtering to enrichment factors, calculated as the ratio of protein count to control count, to classify compounds as potential hits or noise. Although these methods offer advantages in interpretability and computational efficiency, they focus solely on the inherent properties of read counts, overlooking the complex relationships between ligand molecular structures and their cor-responding read counts. To address this limitation, researchers leveraged machine learning techniques to capture the non-linear relationships between ligand molecules and their count labels, aiming to predict more accurate read counts . These approaches, while more sophisticated in modeling molecular features, initially did not consider the constraints at the read count distribution level. Recognizing this gap, researchers further advanced the field by unifying both molecular structure-based predictions and distribution-level constraints. They developed methods incorporating distribution consistency losses, which use ligand sequence embedding to predict enrichment factors and evaluate the consistency of protein and control counts.\nRecognizing the limitations of 2D ligand sequences in capturing spatial information, DEL-Dock introduced protein-ligand 3D conformational information to enhance de-noising. However, compared to exploring different dimensional representations of ligands, enrich-ment factors only focus on the absolute values of ligand-corresponding read counts, neglecting their relative ordering, which is more stable and reliable, especially with experimental noise and sys-tematic errors. Moreover, read counts primarily reflect the enrichment degree of compounds in the screening process but do not directly represent the biological activity of compounds. Without incorporating the activity information, models may overlook important ligand-activity relationships.\nTo address these multifaceted challenges, we propose a novel denoising framework that integrates a theoretically grounded combined ranking loss with an iterative validation loop for read count distribution correction. We introduce two novel constraints: the Pair-wise Soft Rank (PSR) and List-wise Global Rank (LGR). These constraints enable simultaneous learning of local discriminative features and global patterns in read count distributions. By emphasizing the relative relationships between compounds while integrating point-wise constraints based on their absolute read count"}, {"title": "2 RELATED WORKS", "content": "Traditional DEL data analysis approaches, like QSAR models and molecular docking simulations, offer interpretability and mechanistic in-sights. DEL-specific methods such as data aggregation and normalized z-score metrics  address unique DEL screening challenges. However, these methods face limita-tions in scalability and handling complex, non-linear relationships in large-scale DEL datasets.\nMachine learning techniques such as Random Forest, Gradient Boosting Models, and Support Vector Machines have improved DEL data analysis. Combined with Bayesian Optimization , these methods offer better scalability and capture complex, non-linear relationships in high-dimensional DEL data. Despite outperform-ing traditional methods, they are limited by their reliance on extensive training data and lack of interpretability in complex biochemical systems.\nDeep learning approaches, particularly Graph Neural Networks (GNNs), have significantly ad-vanced protein-ligand interaction predictions in DEL screening. GNN-based models predict en-richment scores and accommodate technical variations, while Graph Convolutional Neural Networks (GCNNs) enhance detection of complex molecular struc-tures. Recent innovations include DEL-Dock, combining 3D pose information with 2D molecular fingerprints , and sparse learning methods addressing noise from truncated products and sequencing errors. Large-scale prospective studies have validated these AI-driven approaches, confirming improved hit rates and specific inhibitory activities against protein targets .\nWhile existing methods offer improved scalability and the ability to capture complex molecular in-teractions, they still face challenges in interpretability. Moreover, they often lack a robust theoretical foundation for handling the unique characteristics of DEL read count data, particularly in terms of ranking and distribution correction. The persistent issues of Distribution Noise and Distribution Shift in DEL data remain inadequately addressed by current approaches.\nTo address these limitations, we propose a novel denoising framework that integrates a theoretically grounded combined ranking loss with an iterative validation loop. This approach aims to correct read count distributions more effectively, addressing both Distribution Noise and Distribution Shift, while improving the interpretability and reliability of binding affinity predictions in DEL screening."}, {"title": "3 METHOD", "content": "We present DEL-Ranking framework that directly denoises DEL read count values and incorporates new activity information. In Section 3.1, we directly formulate the problem. Sections 3.2 and 3.3 detail our innovative modules, while Section 3.4 introduces the overall training objective."}, {"title": "3.1 PROBLEM FORMULATION AND PRELIMINARIES", "content": "DEL Prediction Framework. Given a DEL dataset D = {(fi, pi, Mi, Ri, Yi)}_{i=1}^{N}, where fi \u2208 Rd denotes the molecular fingerprint, pi \u2208 Rm represents the binding pose, Mi \u2208 R+ is the matrix count derived from control experiments without protein targets, R\u2081 \u2208 R+ is the target count obtained from experiments involving protein target binding, and yi \u2208 {0, 1} indicates the activity label. We propose a joint multi-task learning framework F : Rd \u00d7 Rm \u2192 R+ \u00d7 R+ \u00d7 [0, 1] such that:\nF(fi, pi) = (Mi, Ri, Pi)\nwhere M\u2081 represents the predicted matrix count, R\u2081 denotes the predicted target count, and \u00ea\u00ee\u00ee is the predicted activity likelihood. The primary focus of this framework lies in predicting accurate read count values that strongly correlate with the actual K\u2081 values.\nZero-Inflated Distribution. DEL screening often results in read count distributions with a high proportion of zeros due to experimental factors. To address this, previous methods have employed zero-inflated distributions, modeling read counts ri as (Mi, Ri), where Mi accounts for excess zeros and Ri represents non-zero counts.\nP(ri |\u03c0i, \u03bci, \u03c6) =\n{\n\u03c0i + (1 \u2212 \u03c0i)(1 + \u03c6\u03bci)\u22121/\u03c6, if ri = 0\n(1 \u2013 \u03c0\u03af) (\u0393(ri+1/\u03c6)) ( \u03c6\u03bc\u03af )ri( 1 )1/\u03c6, if ri > 0\nri!\u0393(1/\u03c6) 1+\u03c6\u03bc\u03af\n1+\u03c6\u03bc\u03af\nwhere \u03c0\u2081 denotes the probability of excess zeros, \u03bc\u2081 represents the mean of the negative binomial distribution, and \u03c6 is the dispersion parameter.\nK. Estimation. DEL read count prediction ultimately aims to estimate compound-target binding affinities (K\u2081 values), crucial for identifying promising drug candidates. We evaluate our model's effectiveness using the Spearman rank correlation coefficient (ps) between predicted read counts and true K\u2081 values: ps = 1 - (6\u03a3d2)/(n(n2-1)) where n is the sample number and d is the ranking discrepancy between predictions and true values. Ideally, K\u2081 values and read counts are negatively correlated."}, {"title": "3.2 RANKING-BASED DISTRIBUTION NOISE REMOVAL", "content": "To effectively remove Distribution Noise in DEL read count data, we propose a novel ranking-based loss function Crank. This loss function integrates both local and global read count information to achieve a well-ordered Zero-Inflated Poisson distribution for read count values:\nLrank = BLPSR + (1 - \u03b2) LLGR\nwhere \u03b2\u2208 [0,1] is a balancing hyperparameter. LPSR (Pairwise-Soft Ranking Loss) addresses local pairwise comparisons, while LLGR (listwise Global Ranking Loss) captures global ranking information. Together, they aim to achieve a well-ordered Zero-Inflated Poisson distribution for read count values. To formally establish the effectiveness of our ranking-based approach, we provide the following theoretical justification:\nLemma 1. Given a set of feature-read count pairs {(xi, ri)}_{i=1}^{n}, where xi is the fused representation of sample i based on fi and pi, and a well-fitted Zero-Inflated Poisson model fz\u0131p(r|x), the ranking loss Crank provides positive information gain over the zero-inflated loss Lz\u0131p:\nI(Lrank|LZIP) = H(R|LZIP) \u2013 H(R|LZIP, Lrank) > 0\nwhere H(R) denotes the conditional entropy of read counts R.\nBuilding upon this information gain, we can further demonstrate that our combined approach, which incorporates both the zero-inflated and ranking losses, outperforms the standard zero-inflated model in terms of expected loss. This improvement is formalized in the following theorem:\nTheorem 2. Given a sufficiently large dataset {(xi, ri)}_{i=1}^{n} of feature-read count pairs, let Lz\u0131p be the loss function of standard zero-inflated model and Crank be the combined ranking loss. For predictions ZI and\u00ee from the standard and combined models respectively. Define Lc = \u03b1LZIP+(1-\u03b1) Lrank, there exists a \u2208 [0,1] such that:\nE[LC(C)] < E[LZIP(\u00ceZI)]\nThe incorporated ranking information aligns read count across compounds, mitigating experimental biases in DEL screening data. Detailed proof and analysis are provided in Appendices A.1 and A.2."}, {"title": "3.2.1 PAIRWISE SOFT RANKING LOSS", "content": "To address the challenges of modeling relative discrete connections between compound pairs and re-move read count Distribution Noise, we propose the differentiable Pairwise Soft Ranking loss LPSR. This novel loss function is designed to capture fine-grained ranking information while maintaining smooth gradients for stable optimization. Lpsr is expressed as follows:\nn\nLPSR(Yi, Yj, T) = - \u03a3\u03b5 \u03a3((\u2206\u03afj \u00b7 \u03c3\u03afj ) - (\u2206ji\u00b7 \u03c3j\u00ed))\ni=1 j\u2260i\n\u03c3ij =\n1\n1+e-Yi-Yj/T\n\u2206\u00ed = AGij \u00b7 ADij\nj\nZ\nwhere \u0177 represents the predicted read count value for compound i. To ensure smooth gradients and numerical stability, we introduce a scaling factor oij with temperature T, allowing for fine-tuned control over the ranking sensitivity.\nTo accurately capture the impact of ranking changes, we design Aij to approximate the relative importance of swapping compounds i and j. We introduce a gain function Gi = softplus(yi) and a rank-based discount function Di = 1/(log2(ranki+1)+\u20ac), where e is a small constant for numerical stability. These smooth functions ensure stable gradients during optimization.\nTo further normalize the impact of ranking changes, we compute a normalization factor in ADij:\nz=\n\u03a3K softplus(y[k])\nk=i log2(k + 1) + \u0454\nwhere y[k] is the k-th element of the descending-sorted y. This factor adapts our loss to varying dataset sizes and read count distributions, enhancing the robustness of our ranking model."}, {"title": "3.2.2 LISTWISE GLOBAL RANKING LOSS", "content": "To further consider global order in compound ranking, we propose the listwise Global Ranking (LGR) loss LLGR as a complement to the Pairwise Soft Ranking loss LPSR, which is expressed as :\nLLGR (\u03b41, \u03b4\u03c0, T) = - \u03a3log ( exp(8\u03c0(i)/T)  ) + Lcon(Si, Sj)\ni=1 \u03a3j=1 exp(Sn(i)/T)\nwhere is the true ranking permutation of the compounds; T is a temperature parameter for score rescaling to sharpen the predicted distribution, and Lcon denotes a contrastive loss among rank-ing scores to capture local connections, and o denotes the weight. This formulation is designed to achieve two critical objectives in DEL experiments: (1) Near-deterministic selection of com-pounds with the highest read counts, corresponding to the highest binding affinities; (2) Increased robustness to small noise perturbations in the experimental data. As T approaches 0, our model be-comes increasingly selective towards high-affinity compounds while maintaining resilience against common experimental noises. This dual optimization leads to more consistent identification of promising drug candidates and enhanced reliability in the face of experimental variability.\nDespite the strengths of LPSR and LLGR, they struggle to differentiate activity levels among com-pounds with identical read count values, particularly those affected by experimental noise. This limitation can lead to misclassification of high-activity samples with artificially low read counts as truly low-activity samples. To address this critical issue, we introduce a novel contrastive loss function Lcon, designed to enhance discrimination between varying levels of biological activity, es-pecially for samples with zero or identical read count values. Let f : S \u2192 R be a ranking function and \u03c4 > 0 a fixed threshold. We define Lcon: S\u00d7S\u2192R> 0 as:\nLcon(Si, Sj) = max{0,\u03c4 \u2013 (f(si) \u2212 f(sj))}\nThis loss function is positive if and only if f(si) \u2212 f(sj) < \u03c4, enforcing a minimum mar-gin \u03c4 between differently ranked samples. The constant gradients dLcon/df(si) = -1 and dLcon/df(sj) = 1 for f(si) \u2212 f(sj) < \u03c4 promote robust ranking relationships."}, {"title": "3.3 ACTIVITY-REFERENCED DISTRIBUTION CORRECTION FRAMEWORK", "content": "To effectively leverage activity information and address distribution shifts in DEL experiments, we propose the Activity-Referenced Distribution Correction (ARDC) framework. This approach rein-forces the impact of activity labels on read count distributions through two key components: Iterative Activity-Guided Refinement (AGR) and Consistency-Driven Error Correction (CDEC).\nThe Iterative Activity-Guided Refinement process incorporates activity information into read count predictions. Detailed in Algorithm 1, this process handles matrix counts and target counts separately. Matrix counts are predicted using 2D SMILES embeddings, while target counts leverage 2D-3D joint embeddings. An adaptive iterative mechanism, inspired by self-training techniques in semi-supervised learning , progressively improves the consistency between read count"}, {"title": "3.4 TOTAL TRAINING OBJECTIVE", "content": "The total training objective integrates three distinct components. The zero-inflated distribution loss LZIP models the overall read count distribution, while the combined ranking loss Crank refines the predicted ZIP distribution based on ordinal relationships. Additionally, the consistency loss Lconsist further adjusts the distribution using activity labels. These components are combined into the total loss function as follows:\nLtotal = LZIP + PLrank + YLconsist\nwhere p and y are weighting factors for the ranking and consistency losses, respectively."}, {"title": "4 EXPERIMENT", "content": "Datasets. CA9 Dataset: From the original data containing 108,529 DNA-barcoded molecules targeting human carbonic anhydrase IX (CA9) , we derived two separate datasets. The first, denoted as 5fl4-9p, uses 9 docked poses that we generated ourselves. The second, 5fl4-20p, employs 20 docked poses using the 5fl4 structure. Both datasets lack activity labels. CA2 and CA12 Datasets From the CAS-DEL library , we generated three datasets comprising 78,390 molecules selected from 7,721,415 3-cycle peptide compounds. We performed docking to create 9 poses per molecule for each dataset. The CA2-derived dataset uses the 3p3h PDB structure (denoted as 3p3h), while two CA12-derived datasets use the 4kp5 PDB structure: 4kp5-A for normal expression and 4kp5-OA for overexpression. Binary affinity labels (0/1) are assigned based on a specific building block (BB3-197). Validation Dataset from ChEMBL includes 12,409 small molecules with affinity measurements for CA9, CA2, and CA12. Molecules have compatible atom types, molecular weights from 25 to 1000 amu, and inhibitory constants (K\u2081) from 90 pM to 0.15 M. A subset focusing on the 10-90th percentile range of the training data's molecular weights provides a more challenging test scenario.\nEvaluation Metrics and Hyperparameters. To evaluate our framework's effectiveness, we employ two Spearman correlation metrics on the ChEMBL dataset. The first metric, overall Spearman correlation (Poverall), measures the correlation between predicted read counts and experimentally determined K\u2081 values across the entire validation dataset. Also, we utilize the subset Spearman correlation (Psubset), which focuses on compounds with molecular weights within the 10th to 90th percentile range of the training dataset.\nBaselines. We examine the performance of existing binding affinity predictors. Traditional meth-ods based on binding poses and fingerprints inculde Molecule Weight, Benzene Sulfonamide, Vina Docking , and Dos-DEL. AI-aided methods dependent of read count values and molecule information include DEL-QSVR, and DEL-Dock. All methods are tested on 5 different target datasets."}, {"title": "4.1 PERFORMANCE COMPARISON", "content": "Benchmark Comparison. We conducted comprehensive experiments across five diverse datasets: 3p3h, 4kp5-A, 4kp5-OA, and two variants of 5fl4. For each dataset, we performed five runs to"}, {"title": "4.2 DISCOVERY OF POTENTIAL HIGH AFFINITY FUNCTIONAL GROUP", "content": "To assess DEL-Ranking's ability in identifying high-affinity compounds and exploring potential functional groups, we analyzed Top-50 target count samples predicted by our model on five datasets. As shown in Figure 3, the majority of these samples demonstrated consistently low K\u012f values, vali-dating the model's effectiveness in prioritizing active compounds within large DEL libraries."}, {"title": "4.3 ABLATION STUDY", "content": "To further explore the effectiveness of our enhancement, we compare DEL-Ranking with some vari-ants on 3p3h, 4kp5-A, and 4kp5-OA datasets. We can observe from Table 4 that (1) LPSR and LLGR contribute most significantly to model performance across all datasets. (2) The impact of LPSR is more pronounced in datasets with higher noise levels, as evidenced by the larger relative perfor-mance drop in the 3p3h dataset. (3) Temperature adjustment and Le help improve the performance by correcting the predicted distributions, but count less than ranking-based denoising."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose DEL-Ranking to addresses the challenge of noise in DEL screening through innovative ranking loss and activity-based correction algorithms. Experimental results demonstrate significant improvements in binding affinity prediction and generalization capability. Besides, the ability to identify potential binding affinity determinants advances the field of DEL screening analysis. Current limitations revolve around the challenges of acquiring, integrating, and comprehensively analyzing high-quality multi-modal molecular data at scale. Future works will aim to improve multi-modal data integration and analysis to advance DEL-based drug discovery."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 PROOF OF LEMMA AND THEOREM", "content": "Proof. [Proof of Lemma1] Let (\u03a9, F, P) be a probability space and (X, Y) : \u03a9 \u2192 X \u00d7 No be random variables representing features and read counts respectively. Define fZIP(y|x) as the probability mass function of a well-fitted Zero-Inflated Poisson model.\nDefine:\n\u0176(x) = E[Y|X = x] = \u03a3y \u2022 fzIP(y|x)\ny=0\nLZIP(fZIP, D) = \u2212 \u03a3 log fzIP(yx)\n(x,y) ED\nLrank (Y, D) =\n\u03a3 max(0, \u0176(xj) \u2013 \u00dd (xi) + \u03b4)\n(xi, Yi), (xj, Yj)\u2208D:yi>Yj\nwhere D is the observed dataset and \u03b4 > 0.\nWe aim to prove I (Lrank|LZIP) > 0, where I(..) denotes conditional mutual information.\nConsider (xi, Yi), (xj, yj) \u2208 D with yi > yj. It's possible that \u0176(xi) < \u0176(xj) due to the nature of likelihood optimization in the ZIP model.\nIn this case:\nLZIP(fZIP, {(xi, Yi), (xj, yj)}) = \u2212 log fzIP(Yi|xi) \u2013 log fz1P(yj|xj)\nLrank (\u00dd, {(Xi, Yi), (xj, yj)}) = max(0, \u0176 (xj) \u2013 \u00dd (xi) + \u03b4) > 0\nThis implies:\nP(Yi > Yj|LZIP, Lrank) > P(Yi > Yj|LZIP)\nConsequently:\nH(Y|LZIP, Lrank) <H(Y|LZIP)\nTherefore, I(Crank LZIP) = H(YLZIP) \u2013 H(Y|LZIP, Lrank) > 0.\nProof. [Proof of Theorem2] Given Lemma1, We firstly prove that there exists a set of predictions C and a sufficiently small o > 0 such that for all y \u2208 (0, \u03b3\u03bf):\n-E[LZIP(ZI, R)] <1-7 (E[Crank (ZI, R)] - E[Lrank(C, R)])\nE[LZIP(C, R)] \u2013 E[LZIP(ZI, 1-\u03b3\nDefine the combined loss function Lc(\u0177,Y;a) = aLzIP(\u0177, Y) + (1 \u2212 a) Lrank(\u0177, Y), where a \u2208 (0, 1). Let \u0177 (a) be the minimizer of Lc:\n\u0177 (a) = arg min E[Lc(\u0177, Y; a)]\n\u0177\nBy the definition of \u0177C(a), for any a \u2208 (0,1), we have:\nE[Lc(\u0177(a), Y; \u03b1)] < E[Lc(\u0177ZI,Y; \u03b1)]\nExpanding this inequality:\n&E[LZIP(\u0177(a), Y)]+(1-a)E[Lrank(\u0177(a), Y)] \u2264 &E[LzIP(\u0177ZI, Y)]+(1-a)E[Lrank(\u0177ZI, Y)]\nLet ALZIP(a) = E[LZIP(\u0177(a), Y)] \u2013 E[Lz\u0131r(\u0177ZI,Y)] and \u2206Lrank(a) = E[Lrank(\u0177ZI,Y)] \u2013 E[Lrank (y(a), Y)]. Rearranging the inequality:"}, {"title": "A.2 GRADIENT ANALYSIS", "content": "We analyze the composite ranking loss function Crank, which combines Pairwise Soft Ranking Loss and Listwise Global Ranking Loss. The gradient of Crank with respect to \u0177\u2081 is:\nLrank\n\u0434\u0443\u0433\n=\nLPSR\n\u03b2\n\u0434\u0443\u0433\n+\nALLGR\n(1 - \u03b2)\n\u0434\u0443\u0433\nLPSR\n\u0434\u0443\u0433\n\u03a3 (\u2206\u03b5\u00b7 \u03c3\u03b5\u03be) \u2013 \u03a3 (\u2206\u03b5\u00b7\u03c3\u03ad)\nji\n\u03b8\u03c3\u03af\u03b6\ndoji\n\u03b8\u0177ji\n\u03b8\u0177i\n-\n\u03a3iji\n+\nYj\u2260i\n\u2211 Yiji\nj\u2260i"}]}