{"title": "FLOW GENERATOR MATCHING", "authors": ["Zemin Huang", "Weijian Luo", "Zhengyang Geng", "Guo-jun Qi"], "abstract": "In the realm of Artificial Intelligence Generated Content (AIGC), flow-matching\nmodels have emerged as a powerhouse, achieving success due to their robust theo-\nretical underpinnings and solid ability for large-scale generative modeling. These\nmodels have demonstrated state-of-the-art performance, but their brilliance comes\nat a cost. The process of sampling from these models is notoriously demanding\non computational resources, as it necessitates the use of multi-step numerical or-\ndinary differential equations (ODEs). Against this backdrop, this paper presents a\nnovel solution with theoretical guarantees in the form of Flow Generator Match-\ning (FGM), an innovative approach designed to accelerate the sampling of flow-\nmatching models into a one-step generation, while maintaining the original per-\nformance. On the CIFAR10 unconditional generation benchmark, our one-step\nFGM model achieves a new record Fr\u00e9chet Inception Distance (FID) score of\n3.08 among few-step flow-matching-based models, outperforming original 50-\nstep flow-matching models. Furthermore, we use the FGM to distill the Stable\nDiffusion 3, a leading text-to-image flow-matching model based on the MM-DiT\narchitecture. The resulting MM-DiT-FGM one-step text-to-image model demon-\nstrates outstanding industry-level performance. When evaluated on the GenEval\nbenchmark, MM-DiT-FGM has delivered remarkable generating qualities, rival-\ning other multi-step models in light of the efficiency of a single generation step.", "sections": [{"title": "INTRODUCTIONS", "content": "Over the past decade, deep generative models have achieved remarkable advancements across var-\nious applications including data generation (Karras et al., 2020b; 2022; Nichol & Dhariwal, 2021;\nOord et al., 2016; Ho et al., 2022; Poole et al., 2022; Hoogeboom et al., 2022; Kim et al., 2022),\ndensity estimation (Kingma & Dhariwal, 2018; Chen et al., 2019), and image editing (Meng et al.,\n2021; Couairon et al., 2022). These models have notably excelled in producing high-resolution,\ntext-driven data such as images (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022;\n2021; Luo, 2024), videos (Ho et al., 2022; Brooks et al., 2024), audios (Evans et al., 2024), and\nothers (Zhang et al., 2024; Xue et al., 2023; Luo & Zhang, 2024; Luo et al., 2023b; Zhang et al.,\n2023; Feng et al., 2023; Deng et al., 2024; Luo et al., 2024c; Geng et al., 2024b; Wang et al., 2024;\nPokle et al., 2022), pushing the boundaries of Artificial Intelligence Generated Content (AIGC).\nAmong the spectrum of deep generative models, flow-matching models (FMs) have emerged as par-\nticularly potent, showcasing robust performance in applications like likelihood computation (Grath-\nwolh et al., 2018; Chen et al., 2018) and text-conditional image synthesis(Esser et al., 2024; Liu\net al., 2023). Flow models utilize neural networks to parametrize a continuous-time transportation\nfield, establishing a bijective mapping between real data and random prior noises. They are trained\nto learn conditional vector fields using flow-matching methods (Lipman et al., 2022b; Albergo &\nVanden-Eijnden, 2022; Liu et al., 2022; Neklyudov et al., 2023). The flexible parametrization and\nrelative ease of training make FMs versatile across various datasets and applications."}, {"title": "INTRODUCTIONS", "content": "However, despite their strengths, FMs still have severe drawbacks. Primarily, sampling from FMs in-\nvolves multiple evaluations of the deep neural network, leading to computational inefficiencies. This\nlimitation restricts their broader application, especially in scenarios where efficiency is paramount.\nTherefore fast sampling from flow models is important though challenging.\nStep-wise distillation has emerged as a viable strategy to mitigate the computational inefficiencies\nassociated with iterative sampling processes in deep generative models, particularly for accelerating\ndiffusion models' sampling mechanisms into more efficient one-step models (Luo et al., 2023a;\nSalimans & Ho, 2022; Song et al., 2023; Gu et al., 2023a; Fan et al., 2023; Fan & Lee, 2023; Aiello\net al., 2023; Watson et al., 2022). While distillation has proven effective in these contexts, the\napplication of such techniques to flow models, has not yet been thoroughly investigated. Besides,\nsince the flow matching does not imply marginal probability densities or score functions as diffusion\nmodels do, how to introduce a probabilistic distillation approach for FMs remains challenging.\nIn this paper, we bridge this gap by presenting flow generator matching (FGM), a probabilistic\nframework for the one-step distillation of flow models. FGM streamlines the sampling process\nof flow models, making it computationally efficient as a one-step generator, while maintaining high\nfidelity to the original model's output. Our approach is validated against several benchmarks, such as\nimage generation on the CIFAR10 dataset and large-scale text-to-image generation. On both tasks,"}, {"title": "RELATED WORKS", "content": "Diffusion Distillation. Diffusion distillation (Luo, 2023) is an active research line aiming to ac-\ncelerate diffusion model sampling using distillation techniques. There are mainly three lines of\napproaches to distill pre-trained diffusion models to obtain solid few-step models. The first line is\nthe distribution matching method. Luo et al. (2024a) first explore diffusion distillation by minimiz-\ning the Integral KL divergence. Yin et al. (2024b) extended this concept by incorporating a data\nregression loss to enhance performance. Zhou et al. (2024) investigated distillation by focusing on\nminimizing the Fisher divergence, while Luo et al. (2024b) applied a general score-based divergence\nto the distillation process. Many other approaches have also studied distribution matching distilla-\ntion (Nguyen & Tran, 2024; Yuda Song, 2024; Heek et al., 2024; Xie et al., 2024; Xiao et al., 2021;\nXu et al., 2024). In this paper, our approach is related to distribution matching distillation. However,\nhow to properly apply distribution matching distillation in the regime of flow models is technically\ndifficult. The second line is the so-called trajectory distillation, which aims to use few-step models\nto learn the diffusion model's trajectory (Luhman & Luhman, 2021; Salimans & Ho, 2022; Geng\net al., 2024a; Meng et al., 2022). Other works use the self-consistency of the diffusion model's tra-\njectory to learn few-step models (Song et al., 2023; Kim et al., 2023; Song & Dhariwal, 2023; Liu\net al., 2024; Gu et al., 2023b; Geng et al., 2024b; Salimans et al., 2024).\nAcceleration of Flow Matching Models. In recent years, there have been efforts to accelerate the\nsampling process of flow-matching models, most current work focuses on straightening the trajec-\ntories of ordinary differential equations (ODEs). ReFlow (Liu et al., 2022) replaces the arbitrary\ncoupling of noise and data originally used for training flow matching with a deterministic coupling\ngenerated by a teacher model, enabling the model to learn a rectified flow from the data. CFM\n(Yang et al., 2024) shares a similar concept with consistency models but differs by applying consis-\ntency constraints to the velocity field space instead of the sample space. This approach also serves\nas a form of regularization aimed at straightening the trajectories of ODEs. Though these works\nhave demonstrated decent accelerations, they are essentially different from our proposed FGM. The\nFGM is built upon a probabilistic perspective that guarantees the generator distribution matches the\nteacher FM by minimizing the flow-matching objective. Besides, as we show in Section 5.1, the\nFGM outperforms the mentioned methods with significant margins."}, {"title": "BACKGROUNDS", "content": "Flow-matching Models. Let $R^d$ represent the data space with data points $x = (x_1,...,x_d) \\in$\n$R^d$. Let $q_1(x_1)$ be a simple noise distribution while $q_0(x_0)$ is the data distribution. Let $u_t(x_t|x_o)$\nbe a known conditional vector field that implies the conditional probabilistic transition $q_t(x_t|x_o)$.\nThe marginal distribution densities $q_t(x_t)$ form a path that links noise distribution $q_1(x_1)$ and data\ndistribution $q_0(x_o)$, i.e. $q_1(x|x_o) = q_1(x)$ and $q_o(x|x_o) = \\delta(x -x_0)$. Then, one can further define\na corresponding marginal vector field (3.2) that translates particles drawn from noise distributions\nto obtain samples following the data distribution,\n$u_t(x_t) = \\int q_t(x_t|x_o)q_0(x_o)dx_o$ (3.1)\n$U_t(x_t) = \\int U_t(x_tx_o) \\frac{q_t(x_t|x_o)q_0(x_o)}{q_t(x_t)}dx_o$ (3.2)\nLet $v_\\theta(\\cdot,\\cdot)$ be a vector field parametrized by a deep neural network. The goal of flow matching is to\ntrain $v_\\theta(\\cdot,\\cdot)$ to approximate the marginal flow $u_t(\\cdot)$ by minimizing the objective (3.3):\n$L_{FM}(\\theta) := E_{t,x_t\\sim q_t(x_t)} || v_\\theta(x_t, t) - U_t(x_t)||^2.$ (3.3)"}, {"title": "BACKGROUNDS", "content": "Although (3.3) represents the optimal target for optimization, the lack of the explicit expression\nabout $u_t(x_t)$ renders the computation impractical. To address this challenge, Lipman et al. (2022a)\nintroduced flow-matching, a tractable alternative objective of (3.3). Lipman et al. (2022a) shows\nthat one can minimize a simpler yet equivalent objective (3.4):\n$E_{t,x_0\\sim q_0(x_0),} ||v_\\theta(x_t, t) - U_t(x_t|x_0)||^2,$\n${x_t\\sim q_t(x_t|x_0)}$ (3.4)\nwith $x_t$ is sampled from $q_t(x_t|x_o)$. The main insight of flow-matching is that the tractable objective\n(3.4) shares the same $\\partial$ gradient as (3.3).\nPractical Instance of Flow Matching Models. In this paper, we especially consider a widely\nused flow matching model, the rectified flow (ReFlow) (Liu et al., 2022; Albergo & Vanden-Eijnden,\n2022) as a specific instance. Our theory and algorithms for the general flow-matching model share\nthe same concepts as the ones based on ReFlow. The ReFlow defines the conditional vector field as\n$U_t(x_t|x_0) = \\frac{x_o - x_t}{1-t}$ (3.5)\nThis results in a simple training objective as\n$L_{ReFlow}(\\theta) = E_{t,x_0\\sim q_0(x_0),x_1\\sim N(0,1),} ||v_\\theta(x_t, t) - (x_1 - x_o)||^2$ (3.6)\n${x_t=(1-t)x_0+tx_1}$\nThe ReFlow objective (3.6) can be interpreted as using a neural network $v_\\theta(x_t, t)$ to predict the\ndirection from noises to data samples. In experiment Sections 5.1, we pretrain a flow model in-\nhouse using the ReFlow objective (3.6). In Section 5.2, the Stable Diffusion 3 model is also trained\nwith the ReFlow objective."}, {"title": "FLOW GENERATOR MATCHING", "content": "In this section, we introduce Flow Generator Matching (FGM), a general method tailored for the one-\nstep distillation of flow-matching models. We begin by defining problem setup and notations. Then\nwe introduce our matching objective function and how FGM minimizes this objective. Finally, we\ncompare FGM with existing flow distillation approaches, highlighting the empirical and theoretical\nadvantages of our methods."}, {"title": "PROBLEM SETUPS", "content": "Problem Formulation. Our framework is built upon a pre-trained flow-matching model that ac-\ncurately approximates the marginal vector field $u_t(x_t)$. The flow $u_t(x_t)$ bridges the noise and data\ndistribution. We also know the conditional transition $q_t(x_t|x_o)$ which implies $u_t(x_t|x_o)$. Assume\nthe pre-trained flow matching model provides a sufficiently good approximation of data distribution,\ni.e., $q_0$ is the ground truth data distribution.\nOur goal is to train a one-step generator model $g_\\theta$, which directly transports a random noise $z \\sim p_z$\nto obtain a sample $x = g_\\theta(z)$. Let $p_{\\theta,o}$ denote the distribution of the student model over the\ngenerated sample $x$, and $p_{\\theta,t}$ denote the marginal probability path transitioned with $q_t(x_t|x_o)$, i.e.,\n$p_{\\theta,t}(x_t) = \\int q_t(x_t|x_o)p_{\\theta,o}(x_o)dx_o$\nThis student marginal probability path implicitly induces a flow vector field $v_{\\theta,t}(x_t)$ generating the\npath, which is unknown yet intractable.\nIntractable Objective. One-step flow generator matching aims to let the student distribution $p_{\\theta,0}$\nmatch the data distribution $q_0$. For this, we consider matching the marginal vector field $v_{\\theta,t}$ with the\npre-trained one $u_t$ such that the distributions $p_{\\theta,o}$ and $q_0$ can match with one another.\nIn this section, we define the objective for flow generator matching. Based on previous discussions,\nour goal is to minimize the expected L2 distance between the implicit vector field $v_{\\theta,t}$ and the\npre-trained flow model's vector field $u_t$, which writes\n$L_{FM}(\\theta) := E_{t,x_t\\sim p_{\\theta,t}}||v_{\\theta,t}(x_t) \u2013 u_t(x_t)||^2$ (4.1)\n$= E_{t,z\\sim p_z(z),x_0=g_\\theta(z),}|| v_{\\theta,t}(x_t) - U_t(x_t)||^2$ (4.2)\n${x_t\\sim q_t(x_t|x_0)}$"}, {"title": "TRACTABLE OBJECTIVE", "content": "Notice that the sample $x_t$ is dependent on the parameter $\\theta$. We may use $x_t(\\theta)$ to emphasize such a\nparameter reliance if necessary.\nIt is clear to see that the $D_{FM}(P_\\theta,0,q_0) = 0$ if and only if all induced vector fields meet, i.e.\n$v_{\\theta,t}(x_t) = u_t(x_t)$ a.s. $p_{\\theta,t}$. Therefore it induces that $p_t(x_t) = q_t(x_t)$, a.s. $P_{\\theta,t}$, which shows that\nthe two distributions $p_{\\theta,0}(x_o) = q_0(x_o)$, a.s. $p_{\\theta,o}$ that match with one anther. Unfortunately, though\nminimizing objective (4.1) leads to a strong one-step generator, it is intractable because we do not\nknow the relation between $v_{\\theta,t}(x_t)$ and the generator distribution $p_{\\theta,0}$. In the next paragraph, we\nwill bring our main contribution: a tractable yet equivalent training objective as (4.1) with theoretical\nguarantees.\nOur goal is to optimize the parameter $\\theta$ to minimize the objective (4.1). However, the implicit vector\nfield $v_{\\theta,t}$ is unknown yet intractable. Therefore it is impossible to directly minimize the objective.\nHowever, by taking the gradient of the loss function (4.1) over $\\theta$, we have\n$\\frac{\\partial}{\\partial \\theta}L_{FM}(\\theta) = E_{t,x_t\\sim p_{\\theta,t}} \\frac{\\partial}{\\partial \\theta} ||U_t(x_t) - v_{\\theta,t}(x_t)||^2 $\n$= E_{t,x_t\\sim p_{\\theta,t}}\\frac{\\partial x_t(\\theta)}{\\partial \\theta} \\frac{\\partial}{\\partial x_t} {||U_t(x_t) - v_{\\theta,t}(x_t)||^2}$ (4.3)\n$= E_{t,x_t\\sim p_{\\theta,t}}{ -2{U_t(x_t) - v_{\\theta,t}(x_t)}^T\\frac{\\partial v_{\\theta,t}(x_t)}{\\partial x_t} \\frac{\\partial x_t(\\theta)}{\\partial \\theta} }$\n$= Grad_1(\\theta) + Grad_2(\\theta)$.\nWhere $Grad_1(\\theta)$ and $Grad_2(\\theta)$ are defined with\n$Grad_1(\\theta) = E_{t,x_t\\sim p_{\\theta,t}} \\frac{\\partial}{\\partial \\theta} \\frac{\\partial x_t}{\\partial x_t} {||U_t(x_t) - v_{\\theta,t}(x_t)||^2}$ (4.4)\n$Grad_2(\\theta) = E_{t,x_t\\sim p_{\\theta,t}} { -2{U_t(x_t) - v_{\\theta,t}(x_t)}^T\\frac{\\partial v_{\\theta,t}(x_t)}{\\partial \\theta} }$\n(4.5)\nThe gradients in (4.3) consider all derivatives concerning the parameter $\\theta$. We put the detailed\nderivation in Appendix A.1.\nNotice that the first gradient $Grad_1(\\theta)$ can be obtained if we stop the $\\theta$-gradient for $v_{\\theta,t}(\\cdot)$, i.e.\n$V_{sg[\\theta],t}(\\cdot)$. This results in an alternative loss function whose gradient coincides with $Grad_1(\\theta)$,\n$L_1(\\theta) = E_{t,x_t\\sim p_{\\theta,t}} ||U_t(x_t) - V_{sg[\\theta],t(x_t)}||^2$\n$= E_{t,z\\sim p_z,x_0=g_\\theta(z),} || U_t(x_t) - V_{sg[\\theta],t}||^2$ (4.6)\n${x_t\\sim q_t(x_t|x_0)}$\nHowever, the second gradient (4.5) involves an intractable term $\\frac{\\partial v_{\\theta,t}}{\\partial \\theta}(\\cdot)$. For the student generator,\nwe only have efficient samples from the conditional probability path, but the vector field $v_{\\theta,t}(\\cdot)$\nalong with its $\\theta$ gradient is unknown. Fortunately, in this paper we have the following Theorem 4.2,\nallowing for a more tractable $\\theta$-gradient of the student vector field. Before that, we need to first\nintroduce a novel Flow Product Identity in Theorem 4.1, which is one of our contributions.\nTheorem 4.1 (Flow Product Identity). Let $f(\\cdot, \\theta)$ be a vector-valued function, using the notations\nin Section 4.1, under mild conditions, the identity holds:\n$E_{x_t \\sim p_{\\theta,t}}f(x_t, \\theta)v_{\\theta,t}(x_t) = E\\frac{20 \\sim 20,0,}{x_t|x0 \\sim qt(xt|x0)}f(x_t, \\theta)u_t(x_t|x_o)$ (4.7)\nWe put the proof of Flow Product Identity 4.1 in Appendix A.2.\nNext, we show that we can introduce an equivalent tractable loss function that has the same param-\neter gradient as the intractable loss (4.1) in Theorem 4.2."}, {"title": "TRACTABLE OBJECTIVE", "content": "Algorithm 1: Flow Generator Matching Algorithm for training one-step Generators.\nInput: pre-trained flow matching model $u_t(\\cdot)$, one-step generator $g_\\theta$, prior distribution $p_z$,\nonline flow model $v_\\psi(\\cdot)$, time $t \\in U[0, 1]$, and conditional transition $q_t(x_t|x_o)$.\nwhile not converge do\nfreeze $\\theta$, update $\\psi$ using SGD by minimizing the flow matching loss\n$L_{FM}(V) = E_{t,z\\sim p_z,x0=g_\\theta(z),} ||V_\\psi (x_t, t) \u2013 U_t(x_t|x_o)||^2.$\n${x_tx0\\sim q_t (x_t|x_o)}$\nfreeze $\\psi$, update $\\theta$ using SGD with by minimizing the FGM loss (4.10):\n$L_{FGM}(\\theta) = L_1(\\theta) + L_2(\\theta)$\n$L_1(\\theta) = E_{t,z\\sim p_z,x0=g_\\theta(z),} || U_t(x_t) - V_{sg[\\theta],t}\\{t}\\2$ (4.11)\n${x_t\\sim q_t(x_t|x_0)}$\n$L_2(\\theta) = E_{t,z\\sim p_z,x0=g_\\theta(z),} 2{u_t(x_t) - V_{sg[\\theta],t} T\\left{V_{sg[\\theta],t} \u2013 U_t (x_t|x_o)\\right\\}$ (4.12)\n${x_tx0\\sim q_t(x_t|x_0)}$\nend\nreturn $\\theta, \\psi$.\nTheorem 4.2. If distribution $p_{\\theta,t}$ satisfies some wild regularity conditions, then we have for all\n$\\theta$-parameter free vector-valued function $u_t(\\cdot)$, the equation holds for all parameter $\\theta$:\n$E_{x_t post}\\{ 2{(x) \u2013 V8,t(X)} Vot(x)} = $ (4.8)\n$\\frac{\\partial}{\\partial \\theta}E_{ \\frac{20~20,0,}{xtx0~qt (xt|x0)}}${$2{U (Pt) - Vsg[0],t (Pt)} {Vss[0],t(Pt) \u2013 UL (Pt|PO)}}$\nWe put the detailed proof in Appendix A.3. The identity (4.8) shows that the expectation of the\nintractable gradient $\\partial v_{\\theta,t}$ can be traded with a tractable expectation with differentiable samples\nfrom the student model.\nIt is a direct result of the identity (4.8) that the gradient $Grad_2(\\theta)$ coincides with the following\ntractable loss function (4.9) with a stop-graident operation sg imposed on $\\theta$ in the generator vector,\n$L_2(\\theta) = E_{t,z\\sim p_z,x0=g_\\theta (z),}2{u_t(x_t) \u2013 V_{sg[\\theta],t} T\\left{V_{sg(\\theta),t} \u2013 U_t (x_t|x_o)\\right\\}$. (4.9)\n${x_tx0\\sim q_t(x_t|x_0)}$\nPutting together (4.6) and (4.9) in terms of (4.3), we have an equivalent loss to minimize the original\nobjective, that is\n$L_{FGM}(\\theta) = L_1(\\theta) + L_2(\\theta)$, (4.10)\nwith $L_1(\\theta)$ and $L_2(\\theta)$ defined in (4.6) and (4.9). This gives rise to the proposed Flow Generator\nMatching (FGM) objective by minimizing the loss function (4.10). Algorithm 1 summarizes the\npseudo algorithm of the flow generator matching by distilling the pre-trained flow matching model\ninto a one-step student generator.\nDifferences From Diffusion Distillations The FGM gets inspiration from one-step diffusion dis-\ntillation by minimizing the distribution divergences (Luo et al., 2024a; Zhou et al., 2024; Luo et al.,\n2024b), however, the resulting theory is essentially different from those of one-step diffusion distil-\nlation. The most significant difference between FGM and one-step diffusion distillation is that the\nflow matching does not imply an explicit modeling of either the probability density as the diffusion\nmodels do. Therefore, the definitions of distribution divergences can not be applied to flow models\nas well as its distillation. However, the FGM overcomes such an issue by directly working with the\nflow-matching objective instead of distribution divergence. The main insight is that our proposed"}, {"title": "CONCLUSION", "content": "explicit-implicit gradient equivalent theory bypasses the intractable flow-matching objective, result-\ning in strong practical algorithms with theoretical guarantees. We think Theorem 4.2 may also bring\nnovel contributions to other future studies on flow-matching models.\nComparison with Other Flow Distillation Methods There are few existing works that try to\naccelerate flow models to single-step or few-step generative models. The consistency flow matching\n(CFM) (Yang et al., 2024) is a most recent work that distills pre-trained flow models into one or\ntwo-step models. Though CFM has shown decent results, it is different from our FGM in both\ntheoretical and practical aspects. First, the theory behind CFM is built on the trajectory consistency\nof flow models, which is directly generalized from consistency models(Song et al., 2023; Song\n& Dhariwal, 2023; Geng et al., 2024b). On the contrary, our FGM is motivated by starting from\nflow-matching objectives, trying to train the one-step generator's implicit flow with the ground truth\nteacher flow, with theoretical guarantees. On the practical aspects, on CIFAR10 generation, we show\nthat our trained one-step FGM models archive a new SoTA FID of 3.08 among flow-based models,\noutperforming CFM's best 2-step generation result with an FID of 5.34. Such strong empirical\nperformance marks the FGM as a solid solution for accelerating flow-matching models on standard\nbenchmarks. Besides the toyish CIFAR10 generation, in Section 5.2 we also use FGM to distill\nleading large-scale text-to-image flow models, obtaining a very robust one-step text-to-image model\nwith almost no performance declines."}, {"title": "EXPERIMENTS", "content": "We conducted experiments to evaluate the effectiveness and flexibility of FGM. Our experiments\ncover the standard evaluation benchmark, unconditional CIFAR10 image generation, and large-scale\ntext-to-image generation using Stable Diffusion 3 (SD3) (Esser et al., 2024). These experiments\ndemonstrate the FGM's capability to build efficient one-step generators while maintaining high-quality samples."}, {"title": "ONE-STEP CIFAR10 GENERATION", "content": "Experiment Settings. We first evaluated the effectiveness of FGM on the CIFAR10 dataset\n(Krizhevsky et al., 2014), the standard testbed for generative model performances. We pre-train\nflow matching models on CIFAR10 conditional and unconditional generation using ReFlow objec-\ntive (3.6). We refer to the neural network architecture used for EDM model(Karras et al., 2022).\nWe train both conditional and unconditional models with a batch size of 512 for 20000k images,\nthe resulting in-house-trained flow model shows a CIFAR10 unconditional FID of 2.52 with 300\ngeneration steps, which is slightly worse than the original ReFlow model (Liu et al., 2022) which\nhas an FID of 2.58 using 127 generation steps. However, in Table 1, we find such a slightly worse\nmodel does not influence the distillation of a strong one-step generator.\nThese flow models serve as the teacher models for flow generator matching (FGM). Then we apply\nFGM to distill one-step generators from flow models. We assess the quality of generated images via\nFrechet Inception Distance (FID) (Heusel et al., 2017). Lower FID scores indicate higher sample\nquality and diversity.\nNotice that loss (4.11) and loss (4.12) together composite a full parameter gradient of the FGM loss.\nWe find two losses works great for toyish 2D dataset generations using only Multi-layer perceptions.\nIn practice, we find that using loss (4.11) on CIFAR10 models leads to instability, which is a similar\nobservation as Poole et al. (2022) that the condition number of its Jacobian term might be ill-posed.\nTherefore we do not use loss (4.11) when training and observing good performances. Training\ndetails and hyperparameters are shown in Appendix B.1.\nInitialize Generator with Pretrained Flow Models Inspired by techniques in diffusion distilla-\ntion, we initialize the one-step generator with the pre-trained flow models. Recall the flow model's\ntraining objective (3.6), the pre-trained flow model $v_\\theta(x_t,t)$ approximately predict the direction\nfrom random noise to data. Therefore, we use the pre-trained flow to construct our one-step genera-\ntor. Particularly, we construct the one-step generator with\n$x_o = (1 - t*)*z + t*v_\\theta(t*z,t*), z \\sim N(0, I)$. (5.1)"}, {"title": "TEXT-TO-IMAGE GENERATION", "content": "Experiments Settings. Our goal in this section is to use FGM to train strong one-step text-to-\nimage generators by distillation from leading flow-matching models. For our text-to-image exper-\niments, we selected Stable Diffusion 3 Medium as our teacher model. This model adopts a novel\narchitecture called MMDiT, which enhances performance in image quality, typography, complex\nprompt understanding, and resource efficiency. For the dataset, we utilized the Aesthetics 6.25+\nprompts dataset along with its recaption prompts and sam-recaption data from Chen et al. (2023) for\ntraining, comprising approximately 2 million entries. This extensive dataset significantly improves\nour model's ability to generate high-quality images. Similar to our observation in CIFAR10 gen-\neration, we find loss (4.11) leads to unstable training dynamic, therefore we also abandon it when\ntraining text-to-image models.\nQuantitative Evaluations. We followed the evaluation metrics used for Stable Diffusion 3 tech-\nnical report (Esser et al., 2024), and we referenced GenEval metrics to more comprehensively assess\nthe model's response to complex input texts. For the evaluations we conduct, we utilize the configu-\nration recommended by the authors. Our distilled model demonstrates promising results, remaining\ncompetitive with other models that require multiple generation steps, even when using only a single\ngeneration step."}, {"title": "CONCLUSION", "content": "In this paper, we introduce flow-generator matching (FGM), a strong probabilistic one-step distil-\nlation approach for flow-matching models. We establish the theoretical foundations of FGM. We\nalso validate the strong empirical performances of FGM on both one-step CIFAR10 generation and\nlarge-scale one-step text-to-image generation.\nThough FGM has a solid theoretical foundation as well as strong empirical performances, it still has\nlimitations. The first limitation is that currently the FGM still requires an additional flow model that\nis used for approximating the generator-induced flow vectors. This requirement asks for additional\nmemory costs for distillation and potentially brings challenges when pre-trained flow models and the\ngenerators are of large model sizes. Secondly, the FGM is a purely image-data-free approach, which\nmeans that it"}]}