{"title": "Communication Strategy on Macro-and-Micro Traffic State in Cooperative Deep Reinforcement Learning for Regional Traffic Signal Control", "authors": ["Hankang Gu", "Shangbo Wang", "Dongyao Jia", "Yuli Zhang", "Yanrong Luo", "Guoqiang Mao", "Jianping Wang", "Eng Gee Lim"], "abstract": "Adaptive Traffic Signal Control (ATSC) has become a popular research topic in intelligent transportation systems. Regional Traffic Signal Control (RTSC) using the Multi-agent Deep Reinforcement Learning (MADRL) technique has become a promising approach for ATSC due to its ability to achieve the optimum trade-off between scalability and optimality. Most existing RTSC approaches partition a traffic network into several disjoint regions, followed by applying centralized reinforcement learning techniques to each region. However, the pursuit of cooperation among RTSC agents still remains an open issue and no communication strategy for RTSC agents has been investigated. In this paper, we propose communication strategies to capture the correlation of micro-traffic states among lanes and the correlation of macro-traffic states among intersections. We first justify the evolution equation of the RTSC process is Markovian via a system of store-and-forward queues. Next, based on the evolution equation, we propose two GAT-Aggregated (GA2) communication modules-GA2-Naive and GA2-Aug to extract both intra-region and inter-region correlations between macro and micro traffic states. While GA2-Naive only considers the movements at each intersection, GA2-Aug also considers the lane-changing behavior of vehicles. Two proposed communication modules are then aggregated into two existing novel RTSC frameworks-RegionLight and Regional-DRL. Experimental results demonstrate that both GA2-Naive and GA2-Aug effectively improve the performance of existing RTSC frameworks under both real and synthetic scenarios. Hyperparameter testing also reveals the robustness and potential of our communication modules in large-scale traffic networks.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH rapid urbanization and population growth in recent years, traffic congestion is becoming a prominent issue agitating all participants in the transportation system [1], [2]. The alleviation of traffic congestion brings both economic and environmental benefits [3]\u2013[5]. Motivated by the urgent need, intelligent transportation systems have been widely studied to improve transportation efficiency by exploring optimal traffic flow control and optimal traffic signal control (TSC). TSC is a promising and cost-efficient approach where the vehicles' movements at each intersection are managed by traffic signals [6]. Conventional TSC techniques such as GreenWave [7] and Maxband [8] focus on rule-based control strategies which usually cast predefined assumptions on expected travel speeds or traffic cycle lengths [9]. However, traffic dynamics in real scenarios are much more complex. Consequently, conventional TSC techniques have limitations in adapting to these complicated conditions [10].\nThe recent rapid development of model-free deep reinforcement learning (DRL) techniques, which can adapt to large high dimensional states, has demonstrated significant potential in various research areas, including autonomous driving [11] and cyber security [12]. The agent of the DRL technique makes sequential decisions in the Markov decision process (MDP) through a trial-and-error procedure [13], [14]. Single-agent reinforcement learning (RL) techniques have been applied to scenarios involving either one isolated intersection or several connected intersections [15]\u2013[17]. These completely centralized RL techniques exhibit a good convergence rate in small-scale traffic networks. However, as the scale of the traffic networks increases, the growth of traffic state space and joint action space becomes exponential, making the search for a joint optimal policy for all signals computationally impractical [6].\nTo alleviate the scalability issue of completely centralized RL techniques, multi-agent deep reinforcement learning (MADRL) techniques have been proposed and studied by numerous researchers [6], [9], [18]\u2013[21]. Most existing MADRL techniques apply completely decentralized strategies in which one agent is assigned to control one specific intersection. The optimal joint action for the entire traffic network is the union of the optimal actions for each agent. Although the scalability issue in MADRL is alleviated, the environment becomes non-stationary due to the intricate interactions between agents [22]. Independent RL (IRL) agents even face theoretical convergence failure because each agent maximizes only its own rewards without considering the impact on other agents [23]. To foster cooperation among agents, various communication and coordination strategies are examined. This involves either the exchange of information between agents or the pursuit of an optimal joint action facilitated by coordinators [24]\u2013[26]. Nonetheless, certain cooperative agents fail to converge when the number of agents becomes substantial [27].\nTo balance scalability and optimality, regional traffic signal control (RTSC) is a compromised method typically involving two stages [28]\u2013[30]. The first stage partitions a large network into several disjoint smaller regions, each comprising a set of intersections. One straightforward way is to partition the traffic network into several fixed-shape regions [29]. However, fixed-shape regions lack adaptability. The regions in [28] are partitioned by grouping intersections with internal strong traffic density dynamically and the regions in [30] are partitioned by only constraining the topology of each region. After the network is partitioned, a centralized DRL technique is applied to control each region. While Regional-DRL (RDRL) [29] and RegionLight [30] continue to utilize decentralized independent agents, cooperative deep reinforcement learning framework (Coder) implements a decentralized-to-centralized coordinator to estimate the global Q-value for the entire traffic network [29]. The regional control methods have successfully converged and identified globally optimal actions in large-scale traffic networks, managing up to 24 intersections [29] and up to 48 intersections [30]. However, current regional control methods still exhibit the following limitations:\n\u2022\tThe model-free DRL agent interacts with the environment through trial-and-error procedures. Thus, modeling the nature of the environment as one MDP is crucial for the agent's convergence and performance. The assumption of MDP for an isolated intersection was justified in [31] using a store-and-forward model [32]. If the agent is assigned to control a region of signals, it has to consider the interactions among signals either inside the region (intra-region) or outside the regions (inter-region). However, the characterization of RTSC as the MDP has not yet been formally justified.\n\u2022\tThe development of cooperative regional signal control agents still faces a great challenge. One global coordinator is utilized to develop cooperative regional signal control agents by estimating the global Q-value [29]. However, searching for the optimal joint global action necessitates multiple rounds of estimations on different combinations of regional sub-optimal actions, and the convergence of the global coordinator is not yet guaranteed. Unlike coordination strategies, communication strategies enable agents to exchange specific information, thereby alleviating non-stationarity. However, no communication strategy between RTSC agents has yet been studied.\nTo enhance cooperation among regional control agents, we first justify that the signal regional control process can be modeled as a Markov chain through a system of store-and-forward queueing models. Then, based on the evolution pattern of the Markov chain, we utilize the graph attention layer(GAT) to capture the correlations between different regions, considering both macro and micro traffic states. More specifically, our main contributions are listed as follows:\n1)\tIn traffic networks, vehicles transit between lanes, moving from one incoming lane to one designated outgoing lane at each intersection. Once in an outgoing lane, vehicles can shift to any adjacent lanes. To characterize these traffic flow dynamics, we define both the movement matrix and the routing proportion matrix. Subsequently, we formulate the updating equation for the signal regional control process using a system of store-and-forward queuing models, and we demonstrate that the updating equation exhibits the Markov property.\n2)\tBased on the updating equation of the RTSC process, we further propose two novel communication modules -GA2-Naive and GA2-Augmented (GA2-Aug) that capture the correlations of lane-level micro-traffic states and those of intersection-level macro-traffic states. The micro-traffic state is the number of vehicles within each lane segmentation. The macro-traffic state is the number of moving and waiting vehicles on lanes. Then, we utilize GAT to aggregate micro and macro traffic states. More specifically, in Naive-GA2, the micro-traffic state is aggregated by involving the movement matrix, and the macro-traffic state is aggregated by involving adjacency between intersections. Additionally, the lane-changing behavior of vehicles is involved by using adjacency between lanes in Augmented-GA2. Finally, we aggregate two proposed communication modules with RegionLight and R-DRL frameworks.\n3)\tWe evaluate our model on 4 \u00d7 4 and 16 \u00d7 3 grid traffic networks with both real and synthetic traffic flows. Empirical results show that the proposed communication modules improve the performance of RTSC models. We further examine the stability of our model by using different hyper-parameter settings, typically on the number of multi-attention heads in the GAT and the number of cells in lane segmentations.\nThe rest of the paper is organized as follows: Section II reviews the related work on communication and coordination strategies for MADRL-based TSC models. Section III introduces the background of TSC and MADRL. Section IV presents the formal justification of the Markov decision process (MDP) in the regional signal control process. Section V describes the communication strategy developed based on the evolution equation formulated in the previous section. Section VI outlines the experimental setup and discusses our findings. Finally, Section VII summarizes the paper."}, {"title": "II. RELATED WORK", "content": "In this section, we mainly review the related work on cooperative MADRL-based TSC models that apply either communication or coordination strategies. We exclude IRL methods because they encounter convergence problems due to the non-stationary issue."}, {"title": "A. Communication Strategy", "content": "We review the related work on communication strategies, focusing primarily on three aspects: who to exchange with, what information to exchange, and how to exchange it.\nOne straightforward approach is to augment the observation of one agent by concatenating it with the observations of its neighboring agents [33]. In contrast, some studies discriminate the contributions of neighboring agents by augmenting observations with weighted values. Zhang et al. extended Hysteretic DQN (HDQN) [34] to neighborhood cooperative hysteretic DQN (NC-HDQN) by considering the correlation between two neighboring intersections [20]. In their work, the observation of one intersection is concatenated with the observation of its neighboring intersections, weighted by correlation degree. They further proposed a rule-based method, namely empirical NC-HDQN (ENC-HDQN), and a Pearson-correlation-coefficient-based method, namely Pearson NC-HDQN (PNC-HDQN). In ENC-HDQN, the correlation degree is defined based on the number of waiting vehicles between two intersections with a pre-defined threshold. In contrast, PHC-HDQN collects the short-term reward trajectories for each agent and then applies the Pearson method to compute the correlations between neighboring intersections.\nInstead of concatenating neighboring information directly to local observation, the following studies encode neighboring information through neural networks. CoLight utilizes a stack of GAT to embed the observation of each agent by incorporating a dynamically weighted average of the observations from its neighboring agents [35]. Zhou et al. proposed Multi-agent Incentive Communication Deep Reinforcement Learning (MICDRL) to enable agents to create customized messages [36]. MICDRL utilizes a multivariate Gaussian distribution (MGD) to infer other agents' actions based on their local information. The local Q-value is then combined with the weighted messages from neighboring agents, which are computed using the MGD. Similarly, Mess-Net was proposed in Information Exchange Deep Q-Network (IEDQN) to facilitate information exchange among all agents [37]. In this approach, the current timestep observation and the previous timestep Q-value for each agent are first concatenated and embedded as local information. Then, the local information from all agents is concatenated and embedded centrally as a message block. This message block is subsequently divided into several message vectors, evenly allocated to all agents. Finally, each agent predicts its Q-value based on its observation and the corresponding message vector.\nTo further enhance communication, the following studies further exchange local policies or historical information. In [38], the actor-critic agent considers its neighboring agents' observations and their policies. Spatialtemporal correlations between agents are considered in NeurComm [39]. At each time step, the observations, historical hidden states, and previous timestep policies of the agent and its neighboring agents are merged and embedded as current hidden states. The spatiotemporal hidden state is then used to predict the state value. Zhang et al. proposed the off-policy Nash deep Q-network (OPNDQN) which utilizes a fictitious play approach to increase the local agent's rewards without reducing those of its neighborhood [26]. The agents in OPNDQN exchange actions and OPNDQN also facilitates reaching a Nash equilibrium. The agents in [18] exchange information with their neighboring agents by determining the corresponding distances and utilizing mix-encoders to aggregate messages."}, {"title": "B. Coordination Strategy", "content": "Apart from communication strategies, many researchers have studied the nature of the interactions between agents and proposed various coordination strategies to choose global joint action. Some studies assume the global Q-value of joint action is the sum of the Q-value of each local action. The max-plus algorithm and transfer planning are applied to optimize the joint global action based on factorized global Q-value [40]. Lee et al. proposed a more straightforward method for computing the global Q-value [41]. In their approach, the Q-values of all possible joint actions are first calculated by summing all local Q-values. The optimal joint action is then identified as the one with the highest global Q-value.\nAnother common strategy is to utilize one parameterized global coordinator to evaluate the global Q-value for global joint action, allowing for more flexible assumptions. Li et al. proposed an Adaptive Multi-agent Deep Mixed Reinforcement Learning (AMDMRL) model using a mixed state-action value function inspired by QMIX [42] [18]. The mixed state-action value assumes all agents contribute positively to the global Q-value, implying that there is no competition between these agents. Cooperative deep reinforcement learning (Coder) is proposed to take the last hidden layers of all agents and predict the global Q-value without the above assumptions [29]. Meanwhile, the Coder initially collects several local suboptimal actions proposed by agents and then estimates the global Q-values of different combinations of these proposed actions through an Iterative Action Search process."}, {"title": "III. PRELIMINARIES", "content": "A. TSC\nA traffic network is defined as a directed graph G = (V,E) where v \u2208 V represents an intersection and evu = (v, u) \u2208 E represents the adjacency between two intersections and an approach connects two intersections. Among all intersections V = {Vinternal \u222a Vexternal}, Vinternal stands for the intersection whose traffic signals are considered to be controlled and Vexternal can be seen as sinks or sources of traffic flows. Approaches are further categorized into three types based on the type of the starting and ending intersections. If the starting intersection is external, then the approach is an entry approach in Eentry. If the ending intersection is external, then the approach is an exit approach in Eexit. If both the starting and ending intersections are internal, then the approach is an internal approach in Einternal. The neighborhood of intersection v is denoted as NBu = {u|(v, u) \u2208 Einternal}.\nAn approach evu serves as the incoming route where vehicles enter intersection u and as the outgoing route where vehicles exit intersection v. Each approach e includes multiple lanes, referred to as L[e]. All lanes on the same approach are adjacent lanes. Then, we have incoming and outgoing lanes corresponding to different approaches. In denotes the set of incoming lanes of intersection v and Outv denotes the set of outgoing lanes of intersection v. Then, all incoming lanes of the traffic network is Lin = Uv\u2208Vinternal Inv.\nA traffic movement (l,m) at intersection v is defined as a pair of one incoming lane l \u2208 In and one outgoing lane m \u2208 Outv. A phase is a set of permitted or restricted traffic movements."}, {"title": "B. Markov Grame Framework and Q-Learning", "content": "The multi-agent reinforcement learning problem is typically modeled as a Markov Game (MG) [43], defined as a tuple(N, S, O, A, R, P, \u03b3) where N represents the set of agents, S denotes the state space, O = {O1, ..., O|N|} denotes the space of local observations for each individual agent i and each local observation is generated partially from S, A = {A1, ..., A|N|} denotes the set of joint action space. The local reward function Ri \u2208 R : O \u00d7 A \u2192 R maps a pair of observation and joint action to a real number. The transition probability P : S \u00d7 A \u00d7 S \u2192 [0,1] assigns a probability to each state-joint action-state transition. \u03b3 denotes the reward discounted factor which manages the trade-off between immediate and future rewards.\nEach agent i in MG has its own policy \u03c0i : Oi \u00d7 Ai \u2192 [0,1] indicating the probability distribution of its action over the observation of agent i. Each agent tries to maximize its own expected cumulative reward, i.e., the state value function\n$V(o_i) = E_{\\pi_i}[\\sum_{k=0}^{\\infty} \\gamma^k r_{i,t+k}|O_{i,t} = o_i]$\nand the Q-value function\n$Q(o_i, a_i) = E_{\\pi_i}[\\sum_{k=0}^{\\infty} \\gamma^k r_{i,t+k}|O_{i,t} = o_i, a_{i,t} = a_i]$\nTraditional tabular Q-learning method stores Q-value in a table [44]. However, for some complicated problems with large state space and action space, tabular Q-learning becomes computationally impractical. Deep Q-network (DQN) utilizes a neural network to approximate Q-value and utilizes gradient descent to update the parameters [14]. The loss function for DQN is\n$L(\\theta_i) = E_{(O_{i,t},a_{i,t},r_{i,t},O_{i,t+1}) \\sim D}[(Y_{i,t} - Q(O_{i,t}, a_{i,t};\\theta_i))^2]$\nwhere\n$Y_{i,t} = r_{i,t} + \\gamma \\max_{a'} Q(O_{i,t+1}, a'; \\theta_i)$\n\u03b8i denotes the parameter of DQN, \u03b8T denotes the parameter of the target DQN and D is the experience buffer."}, {"title": "C. GAT", "content": "The GAT was proposed to capture hidden features for data in the forms of graphs [45]. The input of the single-head GAT is a set of features with nodal structure, h = {h1, h2, ..., hN}, hi \u2208 RF, where N is the number of nodes and F is the number of features in each node. The output of the layer is a set of node features, h' = {h'1, h'2, ..., h'N}, h'i \u2208 RF'. The first step is to compute the correlated importance coefficients E \u2208 RN\u00d7N between nodes by embedding the input features into a higher dimension using a shared weight matrix W \u2208 RF'\u00d7F followed by a self-attention mechanism, i.e.,\n$e_{ij} = LeakyReLU(\\alpha^T[Wh_i||Wh_j])$\nwhere \u03b1 \u2208 R2F' and || is the concatenation operation. Next, a masked attention mechanism is applied to allow each node only to consider the importance coefficients among its neighboring node. The selected importance coefficients are then normalized by the softmax function\n$\\alpha_{ij} = \\frac{exp(E_{ij})}{\\sum_{j \\in N_i} exp(E_{ij})M_{ij}}$\nwhere is the element-wise division operation between matrices and M is the adjacent matrix of these nodes, i.e.,\n$M_{ij} = \\begin{cases} 1 & \\text{if } i \\text{ and } j \\text{ are neighbourhoods} \\\\ 0 & \\text{otherwise} \\end{cases}$\nOnce we have normalized importance coefficients, the final hidden feature of each node is a weighted linear combination of the embedded features of its neighboring nodes, i.e.,\n$h'_i = \\sum_{j \\in N_i} \\alpha_{ij} W h_j$\nwhere Ni is the neighborhood of node i. However, the single-head GAT could be unstable in certain circumstances. Therefore, the multi-head GAT is introduced to stabilize the learning process. Compared to single-head attention, multi-head attention applies K independent attention mechanism with K independent pairs of \u03b1k and Wk involved. The input of the multi-head attention layer remains unchanged while the output of that is a concatenation of each single-head attention's results, i.e.,\nh' = ||Kh=1 \u2211j\u2208N \u03b1k Wkhj\nand h' \u2208 RKF'. To conclude, the whole process of the multi-head attention layer is denoted as\nh' = GAT(h, M)\nwhere M is the neighborhood matrix for mask attention"}, {"title": "IV. MARKOVIAN PROPERTY OF RL-BASED REGIONAL TRAFFIC SIGNAL CONTROL PROCESS", "content": "A store-and-forward queueing network model is proposed to model the transition of the state of a single intersection and is used to prove such transition satisfies the property of Markov-chain [31], [32]. We first revisit the single intersection queueing network model by specifying lane-to-lane movements and adjacency lanes. Then, we extend the queueing network model to a group of intersections."}, {"title": "A. Single Traffic Signal Control Modeling", "content": "For internal intersection v \u2208 Vinternal and its incoming lane l, the number of vehicles leaving l to an outgoing lane m at the beginning of period t is denoted as xt(l, m). For simplicity, we omit m as each lane l has one unique downstream lane. Two variables independent of xt(1) are defined follows:\n\u2022\tRouting proportion rt(l, l'): After a vehicle enters an incoming lane, it can either stay or change to an adjacent lane for the next movement. Therefore, for an incoming lane l, a non-negative i.i.d random variable r(l, l') denotes the proportion of the entering vehicles that move to lane l' from lane l. The sum of routing proportion from l to all lanes on approach e which l belongs to is 1, i.e., \u2211l\u2032rt(l, l') = 1.\n\u2022\tDischarging rate ct(l,m): For each movement (l,m), a non-negative i.i.d random variable ct(l,m) denotes the queue discharging rate and is bounded by saturation flow rate.\nThe transition of x(l) in period (t,t + 1) involves both entering and leaving vehicles. Entered vehicles are contributed directly by vehicles of the movements from the upstream intersections or by vehicles moved from adjacent lanes. Leaving vehicles will move to lane m if movement (l, m) is permitted, i.e., at(l) = 1. The queue update equation for an internal lane l on one internal approach e is formulated as follows:\nxt+1(l) = xt(l)\n+\u2211l\u2032\u2208Lane[e]min{ct(l\u2032, l\u2032) \u22c5 at(l\u2032), xt(l\u2032, l\u2032)} \u22c5 rt(l\u2032,l)\n-min{ct+1(l, m) \u22c5 at(l), xt(l)} \u22c5 1(wave(m) \u2264 wavemax(m))\nwhere wave(m) is the current number of vehicles on lane m and wavemax (m) is the capacity of lane m. The second term (Eq. 12) represents the movements of vehicles expected to enter lane l. For each lane l' including lane l on incoming approach e, there are up to c(k', l') vehicles enter if at(k') = 1. Then, the proportion r(l', l) of vehicles will finally move to lane l. The third term (Eq. 13) represents the movements of vehicles expected to leave lanel where two conditions must be satisfied. The first condition is that the signal allows the vehicle to pass through the intersection which is at(l) = 1 and the second condition is its downstream lane must have the capacity to take the vehicles which is wave(m) \u2264 wavemax(m).\nSimilarly, the queue update equation for the entry lane whose upstream intersection is outside of the network is formulated as follows\nxt+1(l) = xt(l) + dt+1(l)\n-min{ct+1(l) \u22c5 at(l), xt(l)} \u22c5 1(wave(m) \u2264 wavemax(m))\nwhere dt (l) is the demanding flow from intersection v \u2208 Vexternal.\nSince the RL agent generates signal action a and the policy of actions is dependent on state x, the queue update equation only depends on state x, and the process X(t) is a Markov chain."}, {"title": "B. Regional Traffic Signal Control Modeling", "content": "Based on the single intersection evolution model, we now extend the model for a group of intersections and justify that the process of traffic movements under a group of intersections is still a Markov chain.\nSuppose one region is composed of a group of intersections W \u2286 Vinternal, other intersections are either external intersections or ones with pre-defined behaviors. The incoming and outgoing lanes of these intersections are denoted as Fin = \u222av\u2208WInv and Fout = \u222av\u2208WOutv respectively. The state of these intersections is stored in a vector X(Fin) \u2208 R|Fin|. C(Fin) \u2208 R|Fin| is a vector of discharging rate of all lanes in Fin. A(Fin) \u2208 R|Fin| denoted the signal control phase of all incoming lanes inside the region. at = 1 if the signal of lane l is green and a\u2081 = 0 otherwise.\nDefinition 1 (Movement Matrix). Movement matrix M(F1, F2) \u2208 R|F1|\u00d7|F2| between two sets of lanes describes the movements between two sets of lanes F\u2081 and F2 where\nm(k,l) = \\begin{cases} 1 & \\text{if } (k,l) \\text{ is a valid movement} \\\\ 0 & \\text{otherwise} \\end{cases}\nDefinition 2 (Routing Proportion Matrix). Routing proportion matrix RP(F1, F2) \u2208 R|F1|\u00d7|F2| between two sets of lanes describes the route proportion between two sets of lanes F1 and F2 where\nrp(l,l') = \\begin{cases} [0, 1] & l,l' \\text{ are adjacent} \\\\ 0 & \\text{otherwise} \\end{cases}\nand\n\u2211l\u2032\u2208F2rp(l,l') = 1\nDefinition 3 (Blockage Matrix). Blockage matrix BM(F1, F2) between two sets of lanes describes whether the number of vehicles on the downstream lane reaches the lane's capacity where\nbm(k,l) = \\begin{cases} 1 & (l, k) \\text{ is a movement} \\\\ & \\text{and } wave(k) < wave_{max}(k) \\\\ 0 & \\text{otherwise} \\end{cases}\nIf the capacity is reached, then there is a blockage on the downstream lane and no vehicle can leave from the upstream lanes.\nThe queue updating equation for a region involves movements between intra-region intersections, movements between inter-region intersections, and movements from external intersections, i.e.,\nXt+1(Fin) = Xt(Fin) + Intra+ + Intert + External+\n1)\tIntra-region:\tIntrat represents the traffic movement caused by the traffic signals inside the region at time t (Fig. 2(b)).\nIntra =X+(Fin)\u22c5 M(Fin, Fin)\n-X(Fin) \u22c5 BM(Fin, Fout)\n=X(Fin) \u22c5 (M(Fin, Fin) \u2013 BM(Fin, Fout))\nwhere\nX(Fin) = min{C(Fin) \u25cb A+(Fin), Xt(Fin)}\ndescribes the number of vehicles that are about to leave each lane due to the signal, min denotes the operation of taking element-wise minimum between two vectors, \u25cb denotes the operation of element-wise multiplication between two vectors, and\nM(F1, F2) = M(F1, F2) \u22c5 RP+(F2, F2)\nassigns the vehicles from incoming lanes inside the region to themselves.\n2)\tInter-region:\tIntert describes the traffic movements caused by the traffic signal outside the region at time t (Fig. 2(c)). We use W\u2032 = \u222av\u2208WNB \u2013 W to denote the neighboring intersections outside the region. Then, the number of vehicles coming from outside of the region is denoted as\nIntert = X+(Fin) \u22c5 (M(Fin, Fin) \u2013 BM(Fin, Fin))\n3)\tExternal Entry Lane:\tAmong all incoming lanes in Fin, some lanes might belong to Eexternal which originate from sources (Fig. 2.(d)). Therefore, the last part External+ \u2208 R|Fin| represents the vehicles from sources where\nExternal+[l] = \\begin{cases} & \\text{if } l \\text{ originates from sources} \\\\ 0 & \\text{otherwise} \\end{cases}\nThen, the queue updating equation of Xt+1(Fin) only depends on previous state Xt(Fin). Therefore, the process of regional traffic signal control is also a Markov chain."}, {"title": "V. GAT-BASED COMMUNICATION TECHNIQUE ON LANE-LEVEL AND INTERSECTION-LEVEL TRAFFIC STATES", "content": "In the queue updating equation of the RTSC process, the transition involves both intra-region and inter-region traffic flows. However, the transition will become non-stationary if we apply multiple RTSC agents in a large traffic network. Then, the control problem turns out to be a Partially observable Markov decision process (POMDP) since the intersections outside the region are controlled by other agents. To alleviate the issue caused by a partially observable environment, we propose a centralized communication module that applies GAT to capture correlations in both macro traffic states among intersections and micro traffic states among lanes. The overall framework of our work is illustrated in Fig 4 and the architecture of the proposed communication module is illustrated in Fig. 3. The traffic state of the entire traffic network is first embedded in a centralized manner. Then, the embedded state is split and regrouped as the observation for each RTSC agent and each RTSC agent makes decisions based on its individual observations.\nIn this section, we first present the formulation of our communication module with two variants and then describe how this module can be aggregated with RegionLight [30] and RDRL [29]."}, {"title": "A. Lane-Level GAT", "content": "We segment each incoming lane into B cells with and the number of vehicles traveling inside each cell is observed (Fig. 5). Then the input to the lane-level state embedding module is the set of all segmented incoming lanes and is denoted as Slane = {Slane[1], ..., Slane[m]} \u2208 R|Lin|\u00d7B, where Slane[l] \u2208 RB and l,m \u2208 Lin. Then, inspired by [46], we propose two movement matrices to describe the neighborhood relationship between lanes and stack several GATs to embed the lane-level states.\na)\tNaive Movement Matrix:\tBased on the Def. 1, vehicles move from one incoming lane to one outgoing lane. Therefore, a correlation exists between the pair of lanes in any valid movement. We propose a naive movement matrix MnaiveLane \u2208 R|Lin|\u00d7|Lin| to capture the correlation of both upstream and downstream flow, i.e.,\n$\nM_{Lane}^{naive}(L_{in}, L_{in}) = M(L_{in}, L_{in}) + M^{T}(L_{in}, L_{in}) + I_{|L_{in}|}\n$\nwhere I Lin is the identical matrix of dimension |Lin|.\nb)\tAugmented Movement Matrix:\tThe naive movement matrix considers only the traffic movements caused by vehicles passing the intersection. However, in Eq. (25), a vehicle might move to one adjacent lane after it enters one outgoing lane. Therefore, the naive movement matrix fails to consider the lane-changing behaviors of vehicles. Hence, we propose an augmented movement matrix MaugLane to capture the correlation between adjacent lanes more comprehensively, i.e.,\n$\nM_{Lane}^{aug}(L_{in}, L_{in}) = M_{Lane}^{naive}(L_{in}, L_{in}) + ADJ(L_{in}, L_{in})\n$\nwhere ADJ(Lin, Lin) is the adjacent matrix between lanes, i.e.,\n$\nadj(l,l') = \\begin{cases} & 1 \\text{ if } l \\text{ and } l' \\text{ are adjacent lanes} \\\\ 0 & \\text{otherwise} \\end{cases}\n$\nWith the help of ADJ(Lin, Lin), the GATs also compute the importance coefficients between adjacent lanes and consider the evolution between adjacent lanes caused by lane-changing behaviors.\nNext, the architecture for lane-level state embedding is listed as follows\nhlane1 = GAT(Slane, Mlane)\nhlane2 = GAT(hlane1, Mlane)\nhlanemm = GAT(hlane2, Mlane)\nwhere Mlane denotes the mask attention matrix between lanes."}, {"title": "B. Intersection-Level GAT", "content": "The input to the intersection-level state embedding module is a summarised information of each internal intersection v \u2208 Vinternal and is denoted as Sitsx = {Sitsx[v], ..., Sitsx[u]} \u2208 R|Vinternal|x2|Inv| where\nSitsx[v] = {{wait(l)}l\u2208Inv , {wave(l)}l\u2208Inv }\nand wait(l) is the number of waiting vehicles on lane l. Note that, if the number of cells in the lane-level state is set to 1, then the lane-level state is equivalent to the wave on each lane.\nSimilar to previous work [35], we use the adjacent matrix between intersections as the masked attention matrix in GAT, i.e.,\nhitsx1 = GAT(Sitsx, Mitsx)\nhitsx2 = GAT(hitsx1, Mitsx)\nhitsxmm = GAT(hitsx2, Mitsx)\nwhere Mitsx is the adjacent matrix between intersections and mitsx (v, u) = 1 if (v,u) \u2208 Einternal; mitsx(v,u) = 0 otherwise."}, {"title": "C. Intersection Grouping and Observation Construction", "content": "The hidden features of lane-level and intersection-level traffic states are then regrouped according to the configuration of each region. Each region Wi is a set of intersections that obeys the following two constraints:\n\u222aiWi = Vinternal\nWi \u2229 Wj = \u00d8, \u2200i \u2260 j\nwhere the first constraint ensures all regions cover all internal intersections and the second constraint ensures all regions are disjoint. In this paper, we follow the constrained network partition rule proposed in [30", "O(|Vinternal||Sitsx[v": "hitsx1[v", "Einternal||hitsx1[v": "."}]}