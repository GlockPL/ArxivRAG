{"title": "Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability", "authors": ["Bharat Chandra Yalavarthi", "Nalini Ratha"], "abstract": "In mission-critical domains such as law enforcement and medical diagnosis, the ability to explain and interpret the outputs of deep learning models is crucial for en- suring user trust and supporting informed decision-making. Despite advancements in explainability, existing methods often fall short in providing explanations that mirror the depth and clarity of those given by human experts. Such expert-level explanations are essential for the dependable application of deep learning models in law enforcement and medical contexts. Additionally, we recognize that most explanations in real-world scenarios are communicated primarily through natu- ral language. Addressing these needs, we propose a novel approach that utilizes characteristic descriptors to explain model decisions by identifying their presence in images, thereby generating expert-like explanations. Our method incorporates a concept bottleneck layer within the model architecture, which calculates the similarity between image and descriptor encodings to deliver inherent and faithful explanations. Through experiments in face recognition and chest X-ray diagno- sis, we demonstrate that our approach offers a significant contrast over existing techniques, which are often limited to the use of saliency maps. We believe our approach represents a significant step toward making deep learning systems more accountable, transparent, and trustworthy in the critical domains of face recognition and medical diagnosis.", "sections": [{"title": "1 Introduction", "content": "Deep learning models have revolutionized various applications, enhancing accuracy, efficiency, and scalability. However, their lack of transparency poses significant risks in security applications like face recognition, where accountability is crucial to identify biases or failures [38]. In legal contexts, identity search decisions must be justified, similar to how facial forensic examiners testify in court [32]. Investigations have exposed biases in commercial face recognition systems, exemplified by a wrongful arrest due to a flawed match [2] [3]. In computer-aided diagnosis, the black-box nature of these models makes it difficult for physicians to trust and explain decisions, raising concerns about biases and spurious correlations [24]. Incorporating explainability or interpretability mechanisms can address these challenges by clarifying decision-making processes, aiding in debugging, and revealing model biases [5]\nAlthough there are several existing methods relating to explainable face recognition [38, 25, 18, 22] they focus on indistinct visual explanations which are less interpretable than textual explanations [32]. Visual explanations can lack the nuanced detail and context often provided in textual explanations and may be subject to misinterpretation [32]. While prior works dealt with explaining the chest x-ray diagnosis made by deep learning models, none have explored using fine-grained, and atomic characteristic descriptors for providing radiologist-like explanations. The prior work can be mainly"}, {"title": "2 Related Work", "content": "The prior work related to our proposed approach can be broadly divided into three categories i) Vision Language Models (VLMs) for Explainability ii) Explainable Face Recognition and iii) Explainable x-ray diagnosis\nVLMs for Explainability: Pre-trained VLMs like CLIP [33], and ALIGN [17] have shown good performance in in various tasks across several domains in zero-shot and fine-tuning settings [39, 12]. Recently there have been several works where VLM's like CLIP were used to design explainable image classification models. [41, 30, 26] uses the concept bottleneck layer formed by aligning textual concepts and images for explainable image classification. Candidate concepts are usually generated by prompting the LLMs and undergo a selection process. We extend the use of VLMs for explainability to provide expert-level explanations using characteristic descriptors."}, {"title": "Explainable Face Recognition:", "content": "Most prior work (examples in supplementary material) focuses on generating visual explanations in the form of saliency maps, highlighting the regions of the face that the model considers when making a decision. [27, 18, 28, 38, 7, 22, 15, 25] identify these important regions through different techniques including occlusion, perturbation, similarity measurement, and attention. Visual explanations have several disadvantages, they are not precise, fine-grained, and may be subject to interpretation [32]. In human communications, an explanation response is usually in a textual medium either in spoken or written form as this can provide clear and concise explanations in most cases [32]. Unlike above works, [8] offers both textual and visual explanations by training separate networks to identify face attributes and using counterfactual examples to determine key attributes. However, it relies on a limited set of constrained attributes, reducing explanation precision, depth and coverage compared to our method. [13] [10] evaluate the usage of ChatGPT for performing and explaining face recognition. But this method can have reliability issues as these models can hallucinate and faithfulness of the explanations can't be ensured. Moreover, unlike ours, [8] [13] [10] do not provide explanations similar to that of a forensic expert justifying a face match decision."}, {"title": "Chest X-ray Diagnosis:", "content": "Prior works like [24] and [40], employ saliency maps and report features to enhance disease classification, while others, such as [31], model radiologist gaze for explanation. Methods like [34] and [9] link image regions to report sections and utilize longitudinal data for interpretable report generation, respectively. Additionally, [37] leverages model-produced explanations to improve classification, and [36] addresses explaining image-text alignment in vision-language models. In contrast, our approach provides radiologist-like explanations using radiologist-defined descriptors for diagnostic decisions."}, {"title": "3 Methodology", "content": "Our proposed novel explainable methodology is based on using characteristic descriptors for providing human-expert like explanations. Based on such characteristic descriptors described by human experts we create a set of textual concepts denoted by $C = {C_1, C_2, ..C_n}$. These concepts are divided into groups based on the component they are describing. Inspired by the concept bottleneck models (CBM) [23] to design inherently explainable models we use a bottleneck layer in our proposed methodology and employ CLIP [33] [16] to identify these textual concepts in the images. The similarity score between the textual and image embeddings produced by CLIP can give us the concept scores that form the bottleneck layer in our architecture. The concept scores of each image reveal the extent of a concept's presence and are used for providing explanations. Let $X \\in R^{H*W*D}$ denote an image where H, W, D are the height, width, and channels of the image, and y its label. We denote the image"}, {"title": "$\\\\textbf{encoder module of the CLIP as E_i and the text encoder module as E_t. The dot product between the}$", "content": "encodings of $E_i$ and $E_t$ shows the match between the image and the text modalities. To fine-tune CLIP we use a skip connection to extract image embeddings as first detailed in [12]. Image encoding $I \\in R^d$ is extracted using the following equation:\n$I = a * E_i(X) + (1 - a) * F_i(E(X))$\nwhere $F_i$ denotes a two-layer network with ReLU activation, which down-samples and then up- samples the embedding back to its original size.\nOn the other hand, each of the tokenized concepts are fed into $E_t$ to get the text encodings $T \\in R^{N*d}$. We compute the cosine similarity between I and and each encoding in T to get concept scores $S \\in R^N$ which represents the presence of concepts in an image. To better represent these concept scores and the dependencies within a concept group we apply SoftMax independently within each group (denoted as Group SoftMax) of the concept set to obtain $S_{sm}$. Group SoftMax enhances the representation of embeddings by emphasizing the most activated concept within each group, thereby improving both accuracy and explainability, as demonstrated in an ablation study in the supplementary material. We then transform the $S_{sm}$ using a learned concept aggregation matrix $W \\in R^{N*m}$ to get the final embedding $X_{emb} \\in R^m$ used for downstream tasks like face recognition or disease classification. The concept aggregation matrix can be further used to understand the feature weights of the concepts. Figure 2 shows the architecture of the proposed explainable framework."}, {"title": "3.1 Face Recognition", "content": "The FISWG guide listing the facial features to be used for face comparison and morphological analysis is commonly followed by forensic experts. It has detailed characteristic descriptors for each of the nineteen components of the human face. Figure 3 (a) shows some examples of the characteristic descriptors presented in FISWG guide [4]. We use 120 such descriptors from the guide in our explainable face recognition system. Given a reference and probe face images, we get the face embeddings of these by passing it through the proposed architecture and compute if it is a match or not based on their cosine similarity (shown in figure 5 of supplementary material). Since no face dataset includes labeled characteristic descriptors for faces, and most face descriptors are general features fairly detectable by a generic model, we rely on pre-trained image and text encoders of CLIP[16] trained on LAION 2B internet data to identify these descriptors in an unsupervised manner. We further fine tuned our model for face recognition using the quality adaptive margin loss function proposed in AdaFace [21]. We tune only the fully connected layer towards the end of the image encoder. Additionally, we adopted the idea from [12] to add a fully connected layer on top of the image encoder and form a residual connection between their respective outputs (denoted as Adaptive FT). The ablation studies involved in determining the optimal architecture are described in the supplementary material."}, {"title": "3.2 Chest X-Ray Diagnosis", "content": "Features in chest x-ray images are more niche and subtle, making it challenging for a generic CLIP model to capture them. We use a CLIP model pre-trained on chest X-Ray images and radiologist reports which is proposed by [6] as our image and text encoders. The CLIP image encoder and the text encoder are frozen while only the fully connected layers are trained. As we can extract the concept labels from radiologist reports, we can supervise the concepts unlike in face recognition where there was no supervision. Given a corpus of radiology reports, we extract the atomic, fine- grained characteristic descriptors from them using the Mistral 7B language model. We prompt it to disentangle the descriptors to separate sentences from the report (shown in figure 7 of supplementary material). Figure 3 (b) shows the illustration of some of the characteristic descriptors for chest x-ray used by radiologists.\nGiven a chest x-ray, its corresponding concept labels, and its diagnosis label we calculate the cosine similarity between the x-ray image and the characteristic descriptor embeddings to obtain concept scores. For obtaining concept score labels we set the concepts present in the report to max value in the calculated concept scores. L1 loss calculated between the concept scores and concept scores label is used for supervising the model to give appropriate explanations. The concept scores are further passed through a concept aggregation layer to get the logits used for making the diagnosis prediction. As the standard, we use the cross-entropy loss for classification (entire pipeline is shown in figure 6 of supplementary material)."}, {"title": "4 Experiments and Results", "content": null}, {"title": "4.1 Face Recognition", "content": "Datasets and Training: A random subset of 500k images from MS1MV2 [11] dataset containing 5.8M images with 85K identities was used for fine-tuning our model. Standard face recognition datasets including LFW [14], CFP-FP [1], AgeDB [29], CPLFW [42], CALFW [43] are used for validation. We resize the cropped and aligned MS1MV2 images to 224 x 224 for compatibility with CLIP's image encoder. The model is fine-tuned for 5 epochs using the AdamW optimizer with a learning rate of 0.0003, following the approach in [39]. We use the same hyper-parameter values for margin m and image quality indicator concentration h as in Adaface [21]. Table 1 shows the 1:1 verification performance of our proposed approach on these five datasets compared to the black box SOTA model AdaFace [21]. Although we lose some accuracy compared to the black box model, our approach provides explanations leading to insights and debugging capabilities, which are as important as performance in certain critical applications.\nExplanations: We show the ability of our approach to produce faithful explanations justifying its face-matching decisions. Figure 4 shows the examples of the model justifying its decision based on important concepts. The top closest concepts of reference and probe which are commonly activated in each concept group are shown for matching cases, while the top concept groups where the activated concepts differ the most between the reference and probe are shown for non-match cases."}, {"title": "4.2 Chest X-ray Diagnosis", "content": "Dataset and Training: We use the MIMIC-CXR dataset which has chest x-rays, corresponding radiologist reports, and disease labels. For this work, we classify the presence of Pleural Effusion condition from x-ray images using a subset of around 20K samples. We make an 85% train and 15% test split from the chosen subset of the MIMIC-CXR data. We used the pre-trained CLIP model [6] as a baseline and compared its classification performance in detecting pleural effusion against our proposed explainable architecture. We observe from table 2 that the classification performance of our proposed architecture is similar to that of the black box while our approach has an added advantage of providing explanations. The model was fine-tuned for 10 epochs using Adam optimizer with a learning rate of 0.005 with 1:2 proportions of loss signals from classification and concept prediction.\nExplanations: We do a quantitative evaluation of the explanations provided by our model with the labels extracted from radiologist reports. We use two metrics for our evaluation, i) METEOR ii) Rouge-L score for measuring the quality of explanations when compared against the ground truth. The evaluation results in table 3 show that we achieve a good Rouge-L score exhibiting the fidelity"}, {"title": "5 Conclusion", "content": "In this work, we propose a methodology to explain the decisions made by deep learning systems similar to that of a human expert thereby providing improved explainability and expert level insight. We show that we can design models that give faithful and concrete explanations like a human expert using characteristic descriptors. Through our experiments, we show the performance on benchmark datasets proving the efficacy of our model in providing explanations without significantly affecting the performance in comparison with black-box models. We hope our proposed method with the ability to produce consumable and verifiable descriptions can address transparency and trustworthiness in face recognition and x-ray diagnosis systems."}, {"title": "A Appendix / supplemental material", "content": null}, {"title": "A.1 Explainable Face Recognition", "content": null}, {"title": "A.1.1 Ablation Study", "content": "CLIP Model: As there are multiple variants of CLIP differing in the architecture and training dataset we evaluate various CLIP variants based on their Zero-shot performance in 1:1 verification task. We report the average accuracy on the five validation datasets of each variant in table 4. Based on these results we chose ViT-L/14 trained on LAION-2B dataset variant for all further experiments. Interestingly, it outperformed other variants with larger parameters such as VIT-H-14-quickgelu and VIT-H-14-378-quickgelu, and also a variant with the same architecture trained on a different dataset."}, {"title": "A.2 Explainable Chest X-ray Diagnosis", "content": null}]}