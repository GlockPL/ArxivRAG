{"title": "Positive Experience Reflection for Agents in Interactive Text Environments", "authors": ["Philip Lippmann", "Matthijs T.J. Spaan", "Jie Yang"], "abstract": "Intelligent agents designed for interactive environments face significant challenges in text-based games, a domain that demands complex reasoning and adaptability. While agents based on large language models (LLMs) using self-reflection have shown promise, they struggle when initially successful and exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour, a novel approach that addresses these limitations in existing reflection methods by incorporating positive experiences and managed memory to enrich the context available to the agent at decision time. Our comprehensive analysis spans both closed- and open-source LLMs and demonstrates the effectiveness of Sweet&Sour in improving agent performance, particularly in scenarios where previous approaches fall short.", "sections": [{"title": "Introduction", "content": "Intelligent agents, designed to interact with and make decisions in dynamic environments, have become a central focus in AI research, with text-based games (TBGs) emerging as a particularly challenging domain for evaluating these agents' reasoning, adaptability, and learning abilities [2, 21]. Originally popular in the 1970s as text adventure games, TBGs present players with textual descriptions of environments, requiring them to input natural language commands to achieve objectives [4]. For instance, determining if a metal fork is conductive involves locating the fork, assembling a circuit, and analyzing the result. Navigating TBGs demands that agents exhibit a combination of abilities, including planning, memory retention, spatial reasoning, and common sense knowledge [22].\nPreviously, deep reinforcement learning and behavior cloning were the primary approaches to develop agents to play TBGs [1, 25]. However, recent research shows that agents based on pretrained large language models (LLMs) are more effective at navigating TBGs [11]. A key factor in their success is the integration of internal reflection to improve planning [24, 9, 7].\nSelf-reflection, closely related to self-refinement, is a form of reasoning that occurs after receiving binary or scalar feedback from the environment [12]. In this process, the LLM reviews its actions and their outcomes, considering what went wrong and potential ways to improve [23]. By iteratively adjusting its strategy based on verbal reinforcement, conveyed through textual feedback, the agent refines its planning for subsequent attempts [17]. However, reflection also has several limitations, including 1) underwhelming performance when agents are correct initially [10], 2) significantly worse efficacy when using smaller LLMs [11], and 3) dependence on external feedback [29].\nOur Contributions In this work we conduct a comprehensive analysis of LLM-based agents employing reflection approaches in TBGs and evaluate their performance across closed- and open-source LLMs. To address the limitations of poor performance when agents are initially successful and the diminished efficacy of smaller LLMs, we propose Sweet&Sour to leverage positive experiences and"}, {"title": "Methodology", "content": "Background Assuming an LLM behaving as an actor model as part of our agent \u2013 i.e. generating actions based on the current state and policy, analogous to traditional policy-based RL setups \u2013 we sample an action at from the current policy \u03c0\u03b8 at time t and receive an observation from the environment Ot. When a game begins, the agent makes its first observation 00 at time step t = 0. This first observation differs from subsequent ones, as it consists of the goal description d, as well as an analysis of the starting room (i.e. the output of the \u201clook around\" command). Subsequently, the agent can perform an action at at each time step and receives a corresponding observation Ot from the environment. The agent has an inventory it in which to store items. Each task consists of a number of sub tasks (such as finding a key object), the completion of which grants the agent a sparse reward, which adds to its current reward rt. The game continues until the agent has achieved the goal outlined in d and receives the full reward as final score, or the maximum number of steps \u2013 which we set to 150 - is reached, in which case rt will become the final score. A detailed problem formulation and assumptions we make are given in appendix A.\nSelf-Reflection Reflection occurs in addition to the acting LLM. Here, the agent reviews the at and Ot associated with previous unsuccessful attempts to verbalize the reason for failure. This process typically involves maintaining a persistent history of insights gained across attempts, which the LLM uses as additional context for its reflections to improve future decision making for the next attempt [17]. However, since other self-reflection methods focus on learning from failures [16, 28, 30, 8, 27], they overlook the importance of reinforcing successful behaviors in a similar way.\nSweet&Sour To address the limitations of existing reflection methods, we enhance self-reflection by incorporating both positive (sweet) and negative (sour) experiences into the reflection process. This enables the agent to learn effectively from successful actions by reinforcing strategies that lead to positive outcomes while still learning from failures. When the current policy is achieving rewards, we query the agent to extrapolate from it, encouraging the agent to verbalize what made its current policy successful and what can be generalized from this. This is visualized in figure 1 and an example of this is shown in appendix B. Our method is broadly applicable to agents in interactive text environments"}, {"title": "Experiments", "content": "Data & Environment We use the ScienceWorld benchmark [21], which provides a versatile setting for evaluating agents in science experiment tasks across 10 interconnected locations, such as a greenhouse and a workshop, with over 200 objects and 25 action templates, generating a vast and dynamic search space. We use the test set for our evaluation, which provides up to 10 variations of each of the 30 distinct tasks. These interactive tasks cover various topics, including chemistry and electricity, and have an average optimal decision depth of 50 steps. An example task is shown in appendix C. For details of all tasks and the environment, we refer to [21]. We elect to use ScienceWorld instead of previous interactive text environment benchmarks such as TWC [13] and ALFWorld [18], due to their relative simplicity for current LLM-based agents. We measure performance using the success score, which is always between 0 and 100. Completing a task implies completing every sub task, receiving the full reward, and thus a score of 100.\nBaselines CALM [25] is a reranking method that integrates a deep reinforced relevance network (DRRN) [5] with a causal language model fine-tuned using oracle trajectories. The causal language model acquires task- and environment-specific knowledge through imitation learning, while DRRN trains a policy network to rerank the language model's predictions. We use ReAct [26] as our baseline LLM-based agent. ReAct composes useful information at each time step by reasoning over the current context (e.g. decomposing task or common sense knowledge query) and carries it forward to the context of the following time step. This reasoning action does not affect the environment and may be considered few-shot in-context learning. To contextualize our work, we compare our method against Reflexion [17], an agent built on ReAct that employs a self-reflection mechanism to iteratively improve its performance across rounds upon encountering failure based on feedback from the environment. As such, it runs over up to four rounds as it builds up its long-term memory. For all agents, we evaluate their performance using LLMs of different sizes and complexities to assess the performance of each methods across varying computational resources. In descending order of parameter count, we select GPT-40 (gpt-40-2024-08-06)[15], Mistral Large 2 (mistral-large-2407), and Llama 3.1 8B (1lama-3.1-8b-instruct)[3], accessing each through its respective APIs.\nMain Results The results are shown in table 1. We find that Sweet&Sour outperforms the baseline methods across all LLMs, setting the highest average score at 54.6 using GPT-40. The performance gap between Sweet&Sour and the other methods widens for smaller models with a lower parameter count. For instance, it achieves 44.6 compared to Reflexion's 27.6 on Mistral Large 2, and 32.5 compared to 21.7 on Llama 8B \u2013 indicating that our method is more suitable for scenarios with limited computational resources. When we modify our method to only sample from failures, performance drops significantly to a level similar to Reflexion \u2013 scores decrease to 24.6, 31.1, and 44.9 for Llama 8B, Mistral Large 2, and GPT-40, respectively. As such, incorporating positive experiences indeed leads to better reflections, mimicking how humans learn from both positive and negative experiences, resulting in improved decision-making.\nAnti-Tilt In highly challenging tasks, such as 1-1 and 8-2, all methods tend to struggle, while in simpler tasks, most methods succeed based on the LLM's inherent capabilities alone. However, medium-difficulty tasks, such as 3-2 and 3-3, reveal a critical performance gap between our method and previous approaches. We note that this occurs because traditional methods fail to reflect on early"}, {"title": "Conclusion", "content": "In an attempt to improve agent performance and reduce sensitivity to the LLM used, our study embarks on an analysis of reflection mechanisms for LLM-based agents. Existing work focuses on learning from failures only. Although these methods improve planning by analyzing past mistakes, they struggle when agents are initially successful and when using smaller LLMs. The primary contribution of our work is Sweet&Sour, a novel reflection technique for LLM-based agents in TBGs that leverages positive experiences to improve agent self-reflection. Our comprehensive analysis demonstrates the effectiveness of Sweet&Sour in enhancing agent adaptability and reasoning, particularly in challenging situations where previous approaches struggled.\nLimitations Despite promising results, our work has limitations. LLMs do not provide guarantees regarding their reasoning capabilities. Additionally, our evaluation is conducted using a single environment, which, while comprehensive, does not cover all types of interactive scenarios. We leave the exploration of additional environments to future work."}, {"title": "Background", "content": "One may consider every TBG to be a partially observable Markov decision process (POMDP) [19] where the environment state is never observed directly. This POMDP may be formalized as $(S,T, \u0391, \u03a9, R, \u03b3)$, where \u03b3 \u2208 [0, 1] denotes the reward discount factor. S denotes the set of states s that contain the internal information of the game \u2013 such as objects found throughout the game or the player's location \u2013 not all of which may be visible to the agent at any given time. A denotes the action space made up of individual text actions a issued by the player. \u03a9 denotes the observation function. Further, \u03bf \u2208 O denotes the observations made by the player. The observation of of the agent at time t depends on the current state st, as well as the previous action at\u22121, which may be formalized as \u03a9 (Ot | St, at-1). Seeing as the agent can only observe and interact with the environment of a TBG via natural language, each observation is composed of a sequence of tokens o\u2081 = {\u00f4,\u2026\u2026, \u00f4}, as are their actions at = {\u00e2\u00a6,\u2026\u2026, \u00e2\u00a6}.\nIn the context of TBGs, an action at is considered admissible at a state st if it is capable of changing the game's state, i.e., if it can lead to a transition to a new state st+1 that is different from the current state st. The environment's state transition is modeled through a probabilistic function T (St+1 | St, at). Traditionally, admissible actions in state st could deterministically lead to a new state St+1. However, we use a more general approach where all actions, whether admissible or not, are included in the state transition function. Non-admissible actions, which do not lead to a change in the game's state, result in a transition back to the original state st with probability 1. In contrast, admissible actions lead to different states with their own probability. The admissible action set is bound to be significant for quests with a sufficiently large branching factor. While admissible, many action candidates are bound to be suboptimal.\nThe reward r received by the agent \u2013 the discounted sum of which, E [\u2211t ytrt], it aims to maximize \u2013 are obtained by rt = R(st, at). In practice, TBGs typically provide sparse positive rewards for completing subgoals and advancing towards completing the game.\nA TBG's interpreter can accept any text sequence, but will only recognize text that follows a certain structure. Typically, commands take the form of either a single keyword (such as \"look\") or a combination involving verbs, objects, and occasionally prepositions. Previous works on TBGs made the assumption that we have access to a predefined set of all admissible actions at each game state and must select the correct action to progress [14, 6]. This is consistent with how some text adventure games are played in the real world [20]. In line with more recent works [11], we make the assumption that we have access to a number of action templates (e.g. connect A to B, pick up C) and subsequently ask the agent to generate the actions as tokens using these templates. This leads to a more challenging action generation process for the agent."}, {"title": "LLM Reflection", "content": "After successfully completing a sub task, the agent is prompted to reflect on its most recent observations and identify the key factors that contributed to its success. This reflection process encourages the agent to verbalize the steps and strategies that led to the desired outcome, helping it create a plan that can be stored in managed memory for future use. A similar procedure is applied in cases of failure. When a task is unsuccessful, the agent is instructed to reflect on alternative actions it could have taken and to devise a revised plan for the next attempt, ensuring continuous learning and improvement. This plan is also stored in memory.\nAn example trajectory of a ScienceWorld task where the agent reflects on positive and negative experiences is shown in Figure 2. At the end of this trajectory, we show for a single command what a reflection would look like for a successful or unsuccessful choice, in this case picking up an animal. For the unsuccessful case (red), where the agent does not pick up an animal, we reflect on whether another choice would have constituted an animal and thus resulted in a reward. For the successful case where a sub goal is reached (yellow), we reflect on what made the current actions successful and subsequently commit this to memory."}, {"title": "Science World Task 8-1", "content": "Task Description:\nYour task is to find a(n) animal. First, focus on the thing. Then, move it to the red box in the kitchen.\n> look around\nThis room is called the hallway. In it, you see:\n\u2022 a substance called air\n\u2022 a picture\n\u2022 the agent\nYou also see:\n\u2022 A door to the greenhouse (open)\n\u2022 A door to the living room (open)\n\u2022 A door to the art studio (open)\n\u2022 A door to the kitchen (open)\n\u2022 A door to the bedroom (open)\n\u2022 A door to the workshop (open)\n> open door to greenhouse\nThe door is already open.\n> go to greenhouse\nYou move to the greenhouse.\n> open door to outside\nThe door is already open.\n> go to outside\nYou move to the outside.\n> look around\nThis outside location is called the outside. Here, you see:\n\u2022 a substance called air\n\u2022 an axe\n\u2022 a substance called wood\n\u2022 the ground\n\u2022 a fire pit (containing nothing)\n\u2022 a fountain (containing a substance called water)\n\u2022 a butterfly egg\n\u2022 a blue jay egg\n\u2022 a dove egg\n\u2022 the agent\nYou also see:\n\u2022 A door to the kitchen (open)\n\u2022 A door to the foundry (open)\n\u2022 A door to the greenhouse (open)\nCase #1: Failure\n> focus on axe\nYou focus on the axe.\n> pick up axe\nYou move the axe to the inventory.\nReflection(I failed because I assumed eggs are not animals. My plan was to go outside to find animals. I tried to pick up an axe. An alternative course of action would have been to go outside and pick up an egg, as it might be considered an animal.)\n\u2192 Managed Memory"}, {"title": "Case #2: Success", "content": "> focus on dove egg\nYou focus on the dove egg.\n> pick up dove egg\nYou move the dove egg to the inventory.\nReflection(Picking up an egg was the correct choice. Even though the animal was in an egg,\nit is still considered an animal. My plan was to go outside and find an animal in the wild.\nItems such as an axe, wood, or landscape elements are not related to animals.)\n\u2192 Managed Memory"}, {"title": "Example ScienceWorld Task", "content": "In this section, we provide a successfully completed task, a variation of Task 1-1, which concerns itself with boiling a substance, in this case water, to change its state. Once the agent has viewed its surroundings and moved to the kitchen, it collects the necessary items and begins its experiment by boiling the water in a pot on the stove. Finally, the agent examines steam and completes the task (highlighted in green). The example is truncated to improve readability. The trajectory is shown in Figure 3."}, {"title": "Science World Task 1-1", "content": "Task Description:\nYour task is to boil water. For compounds without a boiling point, combusting the substance is also acceptable. First, focus on the substance. Then, take actions that will cause it to change its state of matter.\n> look around\nThis room is called the hallway. In it, you see:\n\u2022 a picture\n\u2022 a substance called air\n\u2022 the agent\n\u2022 A door to the green house (open)\n\u2022 A door to the living room (open)\n\u2022 A door to the art studio (open)\n\u2022 A door to the kitchen (open)\n\u2022 A door to the bedroom (open)\n\u2022 A door to the workshop (open)\n> open door to kitchen\nThe door is already open.\n> go to kitchen\nYou move to the kitchen.\n> look around\nThis room is called the kitchen. In it, you see:\n\u2022 a substance called soap\n\u2022 a painting\n\u2022 a thermometer reading 10\u00b0C\n\u2022 a counter with various items (banana, potato, red apple, orange)\n\u2022 a sink (off, empty)"}, {"title": "actions", "content": "\u2022 a table with a glass cup (empty)\n\u2022 a chair (empty)\n\u2022 a freezer (closed)\n\u2022 a lighter\n\u2022 a stopwatch (deactivated)\n\u2022 a fridge (closed)\n\u2022 a cupboard (closed)\n\u2022 a stove (turned off)\n\u2022 a glass jar containing sodium chloride\n\u2022 various other items (air, the agent, etc.)\n> pick up thermometer\nYou move the thermometer to the inventory.\n> open cupboard\nThe cupboard is now open.\n> pick up metal pot\nYou move the metal pot to the inventory.\n> move metal pot to sink\nYou move the metal pot to the sink.\n> activate sink\nThe sink is now activated.\n> deactivate sink\nThe sink is now deactivated.\n> pick up metal pot\nYou move the metal pot to the inventory.\n> focus on substance in metal pot\nYou focus on the water.\n> move metal pot to stove\nYou move the metal pot to the stove.\n> activate stove\nThe stove is now activated.\n> use thermometer in inventory on substance in metal pot\nThe thermometer measures a temperature of 13 degrees Celsius.\n> examine substance in metal pot\nA substance called water.\n> use thermometer in inventory on substance in metal pot\nThe thermometer measures a temperature of 28 degrees Celsius.\n> examine substance in metal pot\nA substance called water.\n> use thermometer in inventory on substance in metal pot\nThe thermometer measures a temperature of 58 degrees Celsius.\n> examine substance in metal pot\nA substance called water.\n> use thermometer in inventory on substance in metal pot\nThe thermometer measures a temperature of 78 degrees Celsius.\n> examine substance in metal pot\nA substance called water.\n> use thermometer in inventory on substance in metal pot\nThe thermometer measures a temperature of 98 degrees Celsius.\n> examine steam\nA substance called steam. Task completed."}]}