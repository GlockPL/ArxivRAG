{"title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation", "authors": ["Abrar Anwar", "John Welsh", "Joydeep Biswas", "Soha Pouya", "Yan Chang"], "abstract": "Navigating and understanding complex environments over extended periods of time is a significant challenge for robots. People interacting with the robot may want to ask questions like where something happened, when it occurred, or how long ago it took place, which would require the robot to reason over a long history of their deployment. To address this problem, we introduce a Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed for long-horizon video question answering for robot navigation. To evaluate ReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal, and descriptive questions to long-horizon robot navigation videos. ReMEmbR employs a structured approach involving a memory building and a querying phase, leveraging temporal information, spatial information, and images to efficiently handle continuously growing robot histories. Our experiments demonstrate that ReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve effective long-horizon reasoning with low latency. Additionally, we deploy ReMEmbR on a robot and show that our approach can handle diverse queries. The dataset, code, videos, and other material can be found at the following link: https://nvidia-ai-iot.github.io/remembr", "sections": [{"title": "I. INTRODUCTION", "content": "Robots are increasingly being deployed in a wide variety of environments, including buildings, warehouses, and outdoor settings. During their deployments, robots perceive a range of objects, dynamic events, and phenomena that are challenging to encapsulate within conventional representations like metric or semantic maps. Additionally, these robots exist for long periods of time, typically on the magnitude of hours, but there is currently no way to query the robot on what it has seen over this long period of time. In this work, we address the challenge of efficiently building this long-horizon memory for robot navigation and responding to questions by framing it as a long-horizon video question-answering task. Our system enables robots to respond to free-form questions and to perform actions based on what they have observed.\nExisting approaches to spatio-temporal video memory in robotics are constrained by their capacity to handle only short durations, typically limited to 1-2 minutes [1,2]. As the time span increases, the inference time memory requirements grow for transformer-based methods, rendering them impractical for processing arbitrarily long videos Concurrent work [3] has focused on leveraging extremely large context windows of large language models to answer questions given a long robot history; however, this is not a scalable solution. No matter the length of the context window, unbounded length histories will not fit in fixed context sizes. In this work, we propose a Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, which uses a retrieval-based LLM- agent capable of querying memory across arbitrary lengths by formulating text-based, spatial, and temporal queries. As shown in Figure 1, ReMEmbR consists of a memory-building phase and a querying phase.\nEpisodic memory in robotics has been framed mostly as a question-answering task, where systems are evaluated based on their ability to answer questions from a given video [4,5]. While useful for assessing QA capabilities, the text-based outputs of these systems may fall short of providing actionable information for a navigation robot.\nFor example, a question like \"Where did you see my phone?\" might yield a response such as \u201cI saw it on the coffee table.\u201d While informative, this answer does not translate into actionable data for the robot. Our work, therefore, also incorporates reasoning over explicit spatial (e.g., xy positions) and temporal (e.g., \u201c10 minutes ago\u201d) information. To evaluate our system, we construct the Navigation Video Question Answering dataset NaVQA where methods must output position, temporal information, or free-form text. Our"}, {"title": "II. RELATED WORK", "content": "Embodied question answering. Embodied Question Answering (EQA) [4,6\u201310] is an extension of video question answering to egocentric, and possibly interactive, environments, requiring agents to navigate and gather information to answer questions. Most similar to the question answering ability of our work is OpenEQA [7], which answers questions about what a robot has seen. However, their questions consider only a short 30-second memory. This formulation falls short when applied to robotics scenarios that involve extended time horizons and continuous interaction with the environment. In our work, we focus on answering questions and generating navigational goals on longer lengths of history and leverage robot-centric data such as position and time.\nLanguage and navigation. Classical navigation typically uses metric maps and does not focus on navigating to semantic goals. Most recent work in vision-and-language navigation [11-15], object-goal navigation [16\u201319], and various forms of language-guided navigation [20-22] focus on navigating in unseen spaces. These works focus more on exploration; however, robots typically are deployed for extended periods of time in the same area. Forms of memory such as scene graphs [23,24], topological memory [25,26], or queryable map representations [1,27,28] may also allow for semantic goal generation, but may fall short in answering questions about a robot's experience over time about dynamic, non-static objects. As such, our work uses a robot's video to capture these details over a robot's deployment.\nMobilityVLA [3] is a concurrent work where a long- horizon robot video tour is given to the 1M length context window of a Gemini LLM from which the robot must generate a topological goal. In this work, we solve a more general problem of answering spatial, temporal, and descriptive questions while also generating metric navigation goals. Additionally, simply increasing the context window length is not scalable to unbounded history lengths. Using retrieval-based methods, our approach can scale better to long histories.\nLarge language models and robotics. Recent years have seen advancements in large language models (LLMs) and vision-and-language models (VLMs), significantly expanding their capabilities across various tasks [29,30]. Prompting techniques such as chain-of-thought [31] and others [32,33] has further enhanced LLMs' problem-solving abilities, enabling more complex reasoning. Retrieval-augmented generation [34,35] and LLM-agents [36-39] allow the LLM to leverage external information to provide further context to the LLM. In robotics, past work have used the reasoning ability of LLMs for task planning [40\u201343], generating plans as code [44-46], or to generate navigational goals [3,20]. Rather than focusing on planning, our work focuses open horizon perception, and builds an LLM-agent to enable scalable multi-step reasoning over long-horizion robot histories."}, {"title": "III. PROBLEM FORMULATION", "content": "We formulate our problem as a variation of a long- horizon video question answering task for robots. Unlike standard video question answering, robots are deployed for K minutes and actively accumulate a history $H_{1:K}$ from various sensors. Due to their continuous deployment, the size of the history is monotonically increasing over time. Thus our work focuses on two problems: efficiently building a representation this long history $H_{1:K}$ over time and then querying the representation to answer questions and generate navigational goals.\nTo efficiently build a memory, we consider a history of images $H_I$, positions $H_P$, and timestamps $H_T$. We assume that the robot has localization capabilities, such as using LIDAR-based localization, GPS, or odometery information to provide metric coordinates. After a memory representation is built, a user asks the robot a question Q about spatial, temporal, or descriptive information which the robot may have seen. Specifically, our goal is to predict an answer A given the history and a question $p(A|Q, H)$.\nQuestions. Robots need to localize information in their histories; however, we focus on making this information actionable. For spatial questions such as \"Where is closest bathroom?\", the robot must reason about all the bathrooms and signs for bathrooms it has seen. Then, the system must provide the specific (x,y) location to go to the closest bathroom. By formulating spatial questions with coordinates, robots can act on this information to navigate to these goals.\nUsers may also want to query how long ago an event had occurred or understand how long a robot has done a task. Thus, we define two types of temporal questions: point-in-time questions and duration questions. Point-in-time questions such as \"When did you see the boxes fall?\" with the answer \"15 minutes ago\" refer to a specific point-in- time relative to the present. Duration questions focus on the length of an activity such as \"How long were you inside the building for?\" with the answer \"10 minutes\". These temporal questions allow robots to retrospectively consider their previous actions.\nLastly, descriptive questions ask about the environment, activities the robot may have seen, or the robot's state in the"}, {"title": "IV. REMEMBR", "content": "Since robots are embodied and continually persist in the environment, we decompose the task into two distinct phases: memory building and querying.\nThe computation of $p(A|Q, H_{1:K})$ is often difficult, as long histories are computationally expensive for Transformer-based models or can lead to forgetting in state- space models such as LSTMs. We note that for a given question, a large history is often not required to provide a correct answer. Instead, only a subset of the history $R \\subset H_{1:K}$ is needed.\nTherefore, we can compute the answer given an optimal history subset $R^* \\subset H_{1:K}$. In practice, we cannot compute $R^*$ and must sample an R such that it contains the same information as $R^*$. To do so, we build a memory representation V that is sampled using $F: V \\rightarrow R$, where $F(V) = {hh \\in H_{1:K}}$. We decompose the problem as follows:\n$p(A|H_{1:K}, Q) = p(A|R^*, Q) \\approx p(A|R, Q), \\tag{1}$\ns.t. $R \\sim F(V)$. Then, our goal is to estimate $R^*$ such that the answers derived from R and H are consistent. To do so, we must minimize the size of R while ensuring that the answer can be predicted from both the history H and the subset R:\n$R^* = \\underset{R}{\\text{argmin }} R \\\\ \\text{s.t. } \\underset{A}{\\text{argmax }} p(A|R, Q) = \\underset{A'}{\\text{argmax }} p(A'|H, Q) \\tag{2}$\nUsing a memory representation V and a sampling strategy F makes the computation more tractable given a long history. Next, we detail how ReMEmbR aggregates the memory representation V during a memory building phase and how it samples $R \\sim F(V)$ during a querying phase.\nMemory Building. As robots aggregate information over time, we define the queryable memory representation V as a vector database. Vector databases are commonly used to store millions of vector embeddings and search efficiently through them using quantized approximate nearest neighbor methods. Since these databases are efficient in search, we use a vector database to store time, position, and visual representations.\nRobots perceive static objects, scenes, and dynamic events, over the course of their deployments. We would like to note that the memory representation V must be constructed without knowing the question Q in advance, and thus must be general enough for any potential question. As the robot is moving in real-time, we aggregate t seconds of image frames $H_{I_{i:i+t}}$ to compute an embedding representation for that segment of memory. We use video captioning using VILA [47] over each consecutive t-second segment, which generates a caption for each temporal segment $L_{i:i+t}$. These captions capture low-level details of what the robot sees over time, which we then embed using a text embedding function E. We use the mxbai-embed-large-v1 [48] embedding model to embed the captions. Over time, the robot adds the vector representation of the text captions, the position, and the timestamps $E(L_{I_{i:i+t}}), H_{P_{i:i+t}}, H_{P_{i:i+t}}$ into the vector database V.\nQuerying With the vector database V in place, the querying phase can begin. To gather a history subset R, we use an LLM-agent as the sampling function F to sample the database V.\nThe LLM-agent acts as a state machine that iteratively calls the LLM as shown in Figure 2. Our approach begins with a retrieval node which queries the vector database in three different ways, using position, timestamp, or text"}, {"title": "V. DATASET", "content": "We introduce the NaVQA dataset, a long-horizon navigation video question answering dataset built on top of the CODa robot navigation dataset [49]. As described in the previous section, this dataset is annotated with spatial, temporal, and descriptive questions and answers. We use these questions to evaluate models' ability to handle robot- centric long-horizon reasoning. We are excited for the robot learning community to leverage this robot-centric QA dataset to improve the long-horizon reasoning capability of robots.\nCODa Dataset. The CODa dataset is a large urban navigation dataset consisting of long-horizon sequences in indoor and outdoor settings on a university campus. The"}, {"title": "VI. EXPERIMENTAL SETUP", "content": "We use NaVQA to evaluate the ability of ReMEmbR and other LLM-based approaches.\nMethods. ReMEmbR uses a retrieval module to aggregate relevant parts of the long-horizon history. We show the ability of ReMEmbR with a closed-source LLM (GPT-40), various open-source LLMs (Codestral [51], Command-R [52]), and a smaller 8 billion parameter Llama3.1 [53] model. ReMEmbR uses up to 3 retrieval steps to construct R. We compare these models to using GPT-40 with all the captions provided at once and a version using frames sampled at 2 FPS from the video itself. For captioning, we use the VILA1.5-13b over 3 seconds of video, leading to 2 FPS.\nMetrics. The NaVQA dataset consists of four types of answers, for which we compute different metrics. To unify each of these types of metrics into one metric and reduce the impact of outliers, we threshold the temporal and spatial metrics to determine whether an instance is correct or not to create an Overall Correctness metric.\nSpatial questions output (x,y,z) coordinates, from which we compute an L2 distance. We define a spatial question to be correct if it is within 15 meters of the goal.\nTemporal point-in-time and duration questions produce answers such as \u201c15 minutes\", for which we compute"}, {"title": "VII. RESULTS", "content": "ReMEmbR performs strongly given a long-horizon memory at a lower latency. As shown in the results in Table I, ReMEmbR improves performance on long-horizon tasks compared to traditional LLM methods. For long- duration videos, ReMEmbR using GPT4o achieves better descriptive question accuracy, positional error, and temporal error compared to the LLM with captions and Multi-Frame VLM baselines. ReMEmbR performed similarly to the VLM for short category; however, the VLM is unable to process the long videos and most of the medium length videos.\nReMEmbR scales to longer videos with higher overall corectness. Figure 4 shows the overall correctness over time.\nAlthough ReMEmbR does not have the highest performance for short videos compared to the VLM and LLM with captions, ReMEmbR is able to maintain a higher overall correctness score as the video length scales to be longer.\nReMEmbR performs with low latency. We found that for a 21.5 minute video, ReMEmbR takes approximately 25 seconds per question, while the VLM took around 90 seconds per question for a shorter 5.5 minute video. In fact, since ReMEmbR only calls retrieval functions, the amount of time to answer a question remains relatively static regardless of the video duration. Despite their lower performance, we also note that Command-R and Codestral running on a local desktop takes around 40 seconds, while the smaller Llama3.1-8b takes around 15 seconds.\nOpen-source LLMs perform worse than GPT-40. As shown in Table I, we found that LLMs trained specifically for code or function calling work well for generating queries. However, our results imply that these LLMs struggle largely with arithmetic reasoning required for answering temporal and spatial questions, leading to lower performance.\nLonger caption lengths hurt performance. We captioned with VILA1.5-13b during memory building by passing the model 6 frames for every 3 seconds of accumulated video, effectively operating at 2 FPS. We chose 6 frames as this is the max number of frames VILA can process. To evaluate the effect of frame rate, we also tested a lower rate of 6 frames every 12 seconds, or 0.5 FPS. We observed that captioning at this reduced frame rate led to a drop in performance, likely due to information loss from the coarser sampling.\nDifferent sizes of captioning models slightly reduces performance. As shown in Table II, using the 13b captioning model performs slightly better than smaller 8b and 3b models with respect to overall correctness. The minimal performance loss for using the 3b model is important as smaller models have a higher throughput when deployed on a robot.\nIterative function calls are required for good perfor- mance. ReMEmbR uses up to three iterations to find the answer. We found that with only one iteration, which is similar to traditional retrieval-augmented generation, overall correctness decreases. This is likely due to some questions requiring multi-step reasoning, or if the first retrieval did not provide relevant information, ReMEmbR can try again."}, {"title": "VIII. REAL WORLD DEPLOYMENT", "content": "Though NaVQA is useful for prototyping and validating new methods, it is important to deploy such methods on robots. In this section, we demonstrate that ReMEmbR can also be deployed in real time on a robot in the real world.\nRobot Deployment. We deploy ReMEmbR on a Nova Carter robot [54]. We run the memory building phase on Jetson Orin 32GB, and use GPT-40 as the LLM backend for the ReMEmbR agent. We run a quantized version of VILA- 3b to aggregate captions over time. We use ROS2's Nav2 stack with AMCL for computing localization over a pre- mapped metric map. We run a Whisper automatic speech recognition model [55] that was optimized for a Jetson to enable interaction with ReMEmbR. VILA-3b, Whisper, the Nav2 stack with 3D LiDAR, and the vector database querying runs on-device. In the code release, we will provide various LLM backends such as cloud-based LLMs like NVIDIA NIM APIs [56] or OpenAI APIs, local large LLMS like Command-R that can run on a local desktop, and smaller function-calling LLMs that can run on-device. We hope that our code release can enable researchers to build and query long-horizon robot histories across arbitrary embodiments.\nQualitative results. We deployed the system in a large office space by first building a memory by driving the robot around for 25 minutes. Then we began querying the robot with various navigation-centric questions. We found that our robot was able to execute tasks such as \"Where can I get some chips\" where the robot took the user to a cafeteria shelf that contained chips. In contrast to searching for specific objects, we also found that our system can guide users to more general areas such as food courts if asked about food or drinks. Our system can also handle more vague questions. We asked the robot to \"Take me somewhere with a nice view\", and observed the function calls looking for tall glass windows, plants, and open spaces. Then the robot navigated to a lobby with large glass windows and greenery. We also found that for questions such as \"Take me to the soda machine\", the robot would go to a water fountain, as it was captioned as a \"silver machine\". This is likely an artifact of using a quantized 3B captioning model that was unable to caption the water fountain properly."}, {"title": "IX. CONCLUSION", "content": "In this work, we introduced ReMEmbR, a system designed to address the challenge of long-horizon video question answering for robots. By decomposing the task into a memory building phase using a VLM and a vector database then a querying phase with an LLM-agent, ReMEmbR efficiently handles the extensive histories that robots accumulate over time. This approach makes it feasible for robots to leverage long-term memory in dynamic and complex environments.\nLimitations and Future Work. While NaVQA ensures a unique answer for each question, real-world deployments often involve situations where multiple potential answers could be valid, which would require more focus on con-textual reasoning. Additionally, our memory-building ap-proach relies solely on video captioning. However, real- world environments contain rich spatial information such as room numbers, equipment labels, and other details that could be manually annotated. Semantic maps, scene graphs, and queryable scene representations can also provide useful spatial information. We hope to integrate other kinds of memory as function calls so that the ReMEmbR agent can reason spatially and contextually across a broader range of information. A limitation of our approach is that it con- stantly adds potentially repetitive information into the vector database which would dilute useful information over time. We believe that efficient memory aggregation of pertinent information is an interesting area of future research."}]}