{"title": "ABBA-VSM: Time Series Classification using Symbolic Representation on the Edge", "authors": ["Meerzhan Kanatbekova", "Shashikant Ilager", "Ivona Brandic"], "abstract": "In recent years, Edge AI has become more prevalent with applications across various industries, from environmental monitoring to smart city management. Edge AI facilitates the processing of Internet of Things (IoT) data and provides privacy-enabled and latency-sensitive services to application users using Machine Learning (ML) algorithms, e.g., Time Series Classification (TSC). However, existing TSC algorithms require access to full raw data and demand substantial computing resources to train and use them effectively in runtime. This makes them impractical for deployment in resource-constrained Edge environments. To address this, in this paper, we propose an Adaptive Brownian Bridge-based Symbolic Aggregation Vector Space Model (ABBA-VSM). It is a new TSC model designed for classification services on Edge. Here, we first adaptively compress the raw time series into symbolic representations, thus capturing the changing trends of data. Subsequently, we train the classification model directly on these symbols. ABBA-VSM reduces communication data between IoT and Edge devices, as well as computation cycles, in the development of resource-efficient TSC services on Edge. We evaluate our solution with extensive experiments using datasets from the UCR time series classification archive. The results demonstrate that the ABBA-VSM achieves up to 80% compression ratio and 90-100% accuracy for binary classification. Whereas, for non-binary classification, it achieves an average compression ratio of 60% and accuracy ranging from 60-80%.", "sections": [{"title": "1 Introduction", "content": "The number of Internet of Things (IoT) devices worldwide is anticipated to experience a significant increase, nearly doubling from 15.9 billion in 2023 to over 32.1 billion by 2030, according to data from Statista (2023). They produce a massive amount of data, and it is expected to reach around 85 zettabytes (ZB) by 2025 [1,2,23]. All these are driven by the widespread utilization of IoT technology and services across various industries, from environmental monitoring [22] to smart city management [10]. Many of the IoT applications leverage Machine"}, {"title": "2 Related Work", "content": "Multiple techniques exist for TS reduction and classification, but only a few are designed to represent TS data symbolically. Here, we will provide an overview of existing symbolic data compression methods and symbolic time series classification algorithms, which are summarized in Table 1.\nThe TSC techniques can\nbe broadly categorized into\ntwo groups: full time series-\nbased methods and feature-\nbased methods [9]. Full time\nseries-based methods use a\npointwise comparison of TS,\nfor instance, with 1-NN Eu-\nclidean Distance (ED) or 1-\nNN Dynamic Time Warping\n(DTW). While these tech-\nniques are well suited for short TS, they are inefficient for long and noisy TS."}, {"title": "3 System Model", "content": "In this section, we provide a\nhigh-level overview of our ap-\nproach, ABBA-VSM, as de-\npicted in Figure 1. ABBA-\nVSM has two main compo-\nnents, a compressor and a\nclassifier. The compressor\n(e.g., IoT device) adaptively\nreduces TS up to linear seg-\nments (1) and transfers the resulting segments to Edge. The classifier (e.g.,\nEdge device), receives the transmitted data, encodes the linear segments as a\nstring of symbols (2), and by applying the sliding window technique, it creates a\nbag of words (3), which we call as ABBA words, and finally, it builds a classifier\nmodel to predict labels (4).\nCompressor: Let us consider IoT devices as compressors. The IoT device"}, {"title": "4 Methodology", "content": "We have provided a high-level description of the proposed symbolic TSC in Section 3. Here, we present detailed methodologies for constructing ABBA-VSM, consisting of the two main parts: compressor and classifier. For compressor, we use the Adaptive Brownian Bridge Aggregation (ABBA) technique, a continuous-time stochastic process that restricts Brownian motion to trajectories and converges to a given terminal state, enabling efficient data compression applications [6]. By adaptively reducing the TS to linear segments, ABBA method demonstrates the ability to preserve the shape characteristics of TS better than other approaches like SAX and 1d-SAX representations. The key insight is that the segments can be modeled as a random walk with pinned start and end points, resembling a Brownian bridge. This allows for creating parameter-free (namely adaptive) segments except for the choice of a reduction tolerance. Once the TS is reduced to segments, we transfer them to the Edge and encode them as a string of symbols.\nFor classifier, we construct a Vector Space Model (VSM) using the string\nof symbols. VSM is a model that represents TS (in our case, symbolized TS)\nas vectors in a multi-dimensional space. A VSM allows efficient similarity com-\nparisons, from which a classification algorithm can be developed. Unlike clas-\nsical Machine Learning models, which often require extensive training on large"}, {"title": "4.1 Compressor: Transforming Numerical Time Series Into Segments", "content": "A time series compression method at IoT level involves a single step: reduce.\nReduce: We start by considering the time series T in line 5 of Algorithm 1.\nThen, ABBA adaptively partitions T into n segments P with n < N in line 5\nAlgorithm 1. Each segment in P consists of two values, the length of the time\nsteps of each segment as $leni := xi Xi-1\u2265 1$ and increment in value as\n$inci := Yi Yi-1$. The reduced time series is defined as follows,\n$T = [(len1, inc\u2081), ..., (lenn, incn)] \u2208 R2\u00d7n$ (2)\nThe method is an adaptive compression as the Euclidean distance between T and\nT is bounded by a user-defined reduction tolerance (RT) value [6]. Reducing raw"}, {"title": "4.2 Classifier: A Symbolic Approach", "content": "The computational complexity of ABBA compression is O(N) where N is the\nnumber of data points in T. While clustering operations are relatively efficient\nfor Edge environments, they can be computationally intensive for resource-\nconstrained IoT devices. Thus, clustering followed by symbolization and then\nthe construction of training and testing are performed at classifier part of\nABBA-VSM on the Edge.\nSymbolize: Similar tuples from T in Equation 2 form clusters, each encoded as\na single character, allowing all segments that belong to the same cluster to be as-\nsigned one symbol in line 6 of Algorithm 1. For this, tuple values (leni, inci) are\nseparately normalized by their standard deviations olength and oinc, respectively.\nBased on the empirical observations we conducted, the classification accuracy\nerror between various clustering methods was negligible. Thus, for final empirical\nevaluation, we consider a sorting-based [4] and the k-means algorithm. K-means\nclustering requires the number of clusters to be known beforehand, whereas the\nsorting-based method is adaptive, using the user-defined clustering tolerance\nCT [4].\nFinally, each tuple in the sequence \u0164 in Equation 2 is replaced by the symbol\nof the cluster it belongs to, resulting in the string of symbols S = [$182...Sn] \u2208 An.\nABBA-VSM Training: To create ABBA words from a string of symbols in\nS, we apply the sliding window technique as shown in line 7 of Algorithm 1.\nCompared to the traditional sliding window technique that is applied on time\nseries before the compression, we propose to use the sliding window on sym-\nbolically compressed TS, i.e., on the string of symbols. Such a technique allows\nthe adaptive compression of the original TS, forming Brownian bridges, without\nbeing affected by the sliding window dimensions. The sliding window dimensions\ncan be pre-defined by the user. A term corresponding to one window defines a\nsingle ABBA word, a collection of ABBA words from one training sample forms\na labeled bag of words in line 9. Then, a set of bags form corpus in line 10. Once\nall samples in training data are encoded as bags of ABBA words, we group the\nlabeled words to corresponding class labels and create a weight matrix. This\nmatrix defines the weights of all words in a corpus and is built as follows:\n(a) Representation: Let di, D be a document that represents an individual\nclass and a corpus, respectively."}, {"title": "5 Performance Evaluation", "content": "In this section, we evaluate the performance of our proposed ABBA-VSM, a time\nseries classification algorithm based on approximating time series into symbols.\nWe perform a range of experiments to assess its performance and to gain insights\ninto both compression and classification results."}, {"title": "5.1 Metrics", "content": "We present an evaluation framework for the ABBA-VSM algorithm. This frame-\nwork encompasses a dual approach, incorporating both compression-based and\nclassification-based metrics to assess the algorithm's performance across differ-\nent dimensions. For compression-based metrics, we focus on quantifying the effi-\ncency of the compression algorithm. First, the compression ratio (CR) mea-\nsures the data reduction ratio achieved by the algorithm. A higher compression\nratio implies more efficient compression, requiring less storage and providing\npotentially faster processing times. More formally,\n$CR = 1 - \\frac{Size\\ of\\ compressed\\ data}{Size\\ of\\ original\\ data}$\nTo overcome the complexity of storage measure in Python, we used a similar\napproach as in [7] by assuming that the size of the original data is the length of\nthe uncompressed float value multiplied by 4 (4 bytes to store numerical value),\nand the size of compressed data is the length of the compressed string (assuming\n1 byte to store symbols/character).\nConversely, for classification-based metrics, we focus on determining the al-\ngorithm's effectiveness in achieving classification accuracy (Acc) with com-\npressed data. A higher accuracy score signifies the algorithm's capability to\nmaintain classification performance even with compressed data. Accuracy is cal-\nculated as:\n$Acc = \\frac{Number\\ of\\ correctly\\ classified\\ samples}{Total\\ number\\ of\\ samples} \\times 100\\%$"}, {"title": "5.2 Hyper-parameter Selection", "content": "We conducted an exhaustive ex-\nperiment involving multiple hyper-\nparameters, each playing a crucial\nrole in shaping the performance\nof our classification model. These\nhyperparameters include reduction\ntolerance (RT), which establishes\nthe threshold for considering data\nchanges insignificant during com-"}, {"title": "5.3 Application Datasets", "content": "In our study, we analyze a variety of datasets\nfrom the UCR Time Series Classification\nArchive [5]. These datasets are from differ-\nent domains, such as classifying unicellular\nalgae, examining beef spectrograms, and dis-\ntinguishing between Robusta and Arabica\ncoffee beans. Each dataset has varying num-\nbers of classes, adding complexity to our\nanalysis. For example, the Beef dataset in-\ncludes five beef spectrograms, while the Fish\ndataset covers seven fish types. Table 3 pro-\nvides an overview of 19 selected datasets used\nfor experiments. We selected similar datasets\nas in [20] to establish a comparison baseline."}, {"title": "5.4 Implementation", "content": "To implement ABBA-VSM, we extended the fABBA 1.2.1 framework [4] for the\ncompression part, with modifications as required. Next, ABBA-VSM training"}, {"title": "6 Results and Discussion", "content": "In this section, we evaluate ABBA-VSM using the metrics defined in Section\n5. In addition, we compare our result with baselines and perform sensitivity\nanalysis of hyperparameters."}, {"title": "6.1 Main Results", "content": "Our primary focus is to build\nthe classification model us-\ning compressed data, empha-\nsizing the need to achieve\nthe best classification accu-\nracy while maintaining a high\ncompression ratio. Figure 4\ndemonstrates the change in\ncompression ratio with vary-\ning reduction tolerance. For\nboth binary and multiclass\nclassification datasets, we ob-\nserve the increase in compres-\nsion ratio with the increase in\nreduction tolerance. The aver-\nage compression ratio for bi-\nnary classification datasets is\n80-90%, whereas for multiclass, it is 50-60%, shown in Figures 4(a) and 4(b),\nrespectively.\nAs depicted in Figure 3, the runtime overhead of Edge is considerably lower\ncompared to IoT. This disparity can be attributed to the Edge operating on\nalready compressed time series (TS) data, while IoT processes the original un-\ncompressed TS.\nMoreover, we compared the classification accuracy of our proposed ABBA-\nVSM model against several state-of-the-art baseline classifiers in [20], such as\n1NN classifiers based on Euclidean distance and Dynamic Time Warping (DTW),\nFast-Shapelets pattern, Bag of Patterns (BoP) and SAX-VSM. 1NN classifiers\nuse Euclidean distance to provide a straightforward approach for classifying TS\nby measuring the closest neighbor in the feature space, while DTW additionally\nadapts to variations in TS length. Fast-Shapelets detect unique patterns in TS,\nfocusing on the most significant segments. In contrast, the BoP converts TS into\nsymbolic representations, which streamlines classification through the analysis\nof pattern frequencies. Additionally, SAX-VSM integrates SAX with VSM."}, {"title": "6.2 Sensitivity Analysis", "content": "We present the outcomes of exhaustive experiments with various hyperparame-\nters, each playing a crucial role in shaping the classification model's performance.\nTo identify the relevant combination of hyperparameter values, we consider clas-\nsification accuracy results above a threshold value of 80%.\nReduction Tolerance: An increase in reduction tolerance leads to a de-\ncrease in the number of segments, consequently increasing the compression ratio\nand reducing the storage demand. This raises the question of whether ABBA-\nVSM can achieve better classification accuracy with a higher compression ratio.\nAmong all sets of hyperparameter value combinations for binary classification,\nthe reduction tolerance value RT = 0.1 achieved the accuracy threshold more\nfrequently than the rest of the RT values. This is followed by tolerance ranges\nof RT {0.3, 0.5}. However, for multiclass datasets, the accuracy exceeding the"}, {"title": "7 Conclusion and Future Works", "content": "We proposed ABBA-VSM, a symbolic time series classification method designed\nfor resource-constrained Edge environments. Our proposed approach trains a\nVector Space Model (VSM) classification model on a bag of words (i.e., com-\npressed and symbolized time series data) and tests the model on unlabeled sym-\nbolic data. ABBA-VSM demonstrates 90-100% classification accuracy on binary\nclassification datasets while achieving up to 80% compression ratio. In the future,\nwe plan to explore the ABBA-VSM model for multivariate time series."}]}