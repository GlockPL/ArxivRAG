{"title": "Enhancing Audiovisual Speech Recognition through Bifocal Preference Optimization", "authors": ["Yihan Wu", "Yichen Lu", "Yifan Peng", "Xihua Wang", "Ruihua Song", "Shinji Watanabe"], "abstract": "Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech recognition accuracy by leveraging visual signals. It is particularly challenging in unconstrained real-world scenarios across various domains due to noisy acoustic environments, spontaneous speech, and the uncertain use of visual information. Most previous works fine-tune audio-only ASR models on audiovisual datasets, optimizing them for conventional ASR objectives. However, they often neglect visual features and common errors in unconstrained video scenarios. In this paper, we propose using a preference optimization strategy to improve speech recognition accuracy for real-world videos. First, we create preference data via simulating common errors that occurred in AV-ASR from two focals: manipulating the audio or vision input and rewriting the output transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization method to improve AV-ASR models by leveraging both input-side and output-side preference. Extensive experiments demonstrate that our approach significantly improves speech recognition accuracy across various domains, outperforming previous state-of-the-art models on real-world video speech recognition\u00b9.", "sections": [{"title": "1 Introduction", "content": "Recently, there has been a growing demand for Automatic Speech Recognition (ASR) systems to evolve into unconstrained audiovisual scenarios, such as YouTube videos, online meetings, and live broadcasts. With both audio and visual streams as input, visual information may help improve speech recognition accuracy, especially in cases where the audio is noisy or unclear. There are usually two scenarios for AudioVisual ASR (AV-ASR), focusing solely on lip motion and using full-frame visual features. In this work, we focus on the latter scenario, improving the AV-ASR performance in unconstrained real-world video scenarios. In such settings, the entire visual frame may contribute to ASR performance by providing additional cues on specific objects, background location, or context.\nCompared to standard ASR, performing AV-ASR on real-world videos presents the following challenges:\n\u2022 Noisy Acoustic Environments. Homophone issues pose a significant challenge for both standard ASR and AV-ASR. Real-world recordings with variable acoustic environments further severe homophone problems.\n\u2022 Spontaneous Speech Scenarios. Real-world videos often contain spontaneous conversations, which are more variable than read speech or lectures, making accurate recognition more challenging.\n\u2022 Uncertain Use of Vision. The diversity of video domains and unconstrained scenarios leads to the uncertain use of visual information for AV-ASR systems. While some visual cues are irrelevant, others provide crucial localized details or global context for understanding speech, making it challenging to effectively leverage visual information for improved recognition accuracy.\nMost current AV-ASR works build"}, {"title": "2 Related Works", "content": "2.1 Audiovisual Speech Recognition\nRecent state-of-the-art ASR models have achieved impressive performance on audio-only benchmarks. Whisper-style ASR models, in particular, leverage large-scale supervised learning to achieve robust results across various benchmarks. Building on pre-trained ASR models, several studies explore ASR to audiovisual scenarios. Most previous AV-ASR works focus on lip motion. Moreover, to explore unconstrained AV-ASR for real-world videos, recent works explore different adaptation methods to fine-tune or retrain ASR models with full-frame visual features. AVFormer shows the state-of-the-art performance by integrating visual information into a frozen ASR model and fine-tuning it on the large-scale audiovisual dataset HowTo100M. However, these models are all trained or supervised fine-tuned toward standard ASR optimization objectives, leading to modality inefficient utilization of audio and visual information, particularly in unconstrained real-world video scenarios. In contrast, this work introduces a novel approach designed to adapt ASR models to unconstrained AV-ASR using bifocal preference optimization.\n2.2 Direct Preference Optimization\nTo align a pre-trained Large Language Model (LLM) with specific preferences of downstream tasks, Direct Preference Optimization (DPO) is proposed to optimize the LLM with a single-stage policy learning. DPO demonstrates strong performance while eliminating the need for a separate reward model. Intuitively, given the input x and the outputs $y_w$ and $y_l$, DPO maximizes the difference between the reward for the chosen output $r(x, y_w)$ and that for the rejected output $r(x, y_l)$. Specifically, given a policy model to be optimized $\\pi_\\theta$ and a reference model $\\pi_{ref}$, DPO formulates the reward as:\n$r(x,y) = \\beta \\log \\frac{\\pi_\\theta (y|x)}{\\pi_{ref}(y|x)} + Z(x),$\nwhere Z(x) is a partition function, and $\\beta$ is the hyperparameter that controls the deviation from the reference model. Based on the Bradley-Terry model, the optimization objective of DPO becomes\n$L_{DPO} = -\\log \\sigma \\Big( \\beta \\big( \\log \\frac{\\pi_\\theta (y_w|x)}{\\pi_{ref}(y_w|x)} - \\log \\frac{\\pi_\\theta (y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\Big),$\nwhere $\\sigma$ is the logistic function. Furthermore, the subsequent work of DPO proves that DPO implicitly learns a token-level reward function, highlighting its potential of using DPO in token-level preference optimization, such as ASR and machine translation. In multimodal scenarios, recent works mainly focus on constructing multimodal preference data to support DPO training in the visual language domain. MDPO designs a conditional preference optimization strategy to improve multimodal preference alignment in vision language scenarios, while SeVa constructs preference datasets via self-supervised learning. In this work, to the best of our knowledge, we first introduce DPO to AV-ASR tasks, showing a significant improvement"}, {"title": "3 Proposed Method", "content": "We describe our proposed BPO-AVASR framework in this section. To support bifocal alignment optimization, we design two key strategies to construct the bifocal preference dataset, constructing both input-side preferences and output-side preferences (Section 3.1). Then we outline the training procedure, including the supervised fine-tuning stage (Section 3.2) and the preference optimization stage (Section 3.3).\n3.1 Creation of Bifocal Preference Dataset\nSuppose we have an audiovisual speech recognition dataset D = {(a\u00b9, v\u00b9, t\u00b9)}K i=1 , which contains K elements. Each element \u03c4i = (ai , vi , ti ) consists of a speech ai , a video vi , and the corresponding transcript ti . For AV-ASR tasks, both ai and vi served as the inputs (denoted collectively as ci ), while the corresponding transcript ti is the output. Thus each element can be better denoted as \u03c4i = (ti |ci ) = (ti |ai , vi ). Given the challenges of performing AV-ASR on real-world videos (as analyzed in Section 1), we construct preference pairs by manipulating three modalities, resulting in input-side preferences (audio, video) and output-side preferences (text). By simulating common errors in AV-ASR and constructing the preference dataset accordingly, the model can be optimized by maximizing the distinction between chosen and rejected pairs. This approach enables the model to learn to avoid these kinds of errors during inference.\nInput-side Preference. To simulate insufficient use of input information, we create rejected samples by independently manipulating the audio and video inputs.\n\u2022 Masked Audio. We mask certain audio frames to simulate situations where speech is noisy or ambiguous. In detail, we mask content words and add Gaussian noise to the corresponding audio input, as shown in Type I of Figure 2. Therefore, we obtain rejected sample \u03c4l = (tl |cl ) = (tl |al , vl ), which serves as a hard negative example to a chosen sample \u03c4w = (tw |aw , vw ).\n\u2022 Flipped Vision. To mimic the situation where the AV-ASR model uses visual information inadequately, we intentionally construct samples with rejected visual inputs. As shown in Type II of Figure 2, we create the rejected sample vi by flipping the video v to simulate the situation where the ASR model lacks sufficient visual information, especially for the detailed object information. Flipped images can make optical character recognition (OCR) harder and induce potential rejected responses. Therefore, we obtain the rejected sample \u03c4l = (tw |cl ) = (tl |a, vl ) for a chosen sample \u03c4w = (tw |aw , vv ).\nAs a result, we obtain the preference dataset Dc .\nOutput-side Preference. To construct a preference dataset that includes rejected samples simulating common errors in recognized text, we use ChatGPT as a proxy"}, {"title": "3.2 Supervised Fine-tuning", "content": "Pre-trained ASR Model. We use the audio-only ASR model OWSM v3.1 as our backbone. OWSM v3.1 is an open-source pre-trained speech recognition model that achieves robust performance on standard ASR benchmarks. It utilizes an encoder-decoder architecture, with the stack of E-Branchformer encoders and Transformer decoders. Being trained on large amounts of ASR data, OWSM v3.1 has a good generalization ability for AV-ASR tasks.\nVisual Encoder. We leverage a pre-trained visual encoder to extract visual tokens as conditions to the audio-only ASR model. Specifically, we use CLIP to ensure that all relevant information within the video frames is effectively encoded for AV-ASR. We sample M frames from video vi uniformly, then use the CLIP to extract visual tokens from these frames. A visual projection layer is subsequently applied to map visual tokens into speech space.\nFine-tuning Objective Given M visual tokens and N speech tokens, we concatenate them along the sequence dimension, resulting in a combined sequence of length N + M. Then, we feed them into the ASR model and fine-tune the model \u03c0ref in Equation 1 with attention loss LATT and the CTC loss LCTC:\n$L_{SFT}(\\pi_{ref}, D) = L_{ATT}(\\pi_{ref}, D) + \\alpha \\cdot L_{CTC}(\\pi_{ref}, D),$\n$L_{ATT} (\\pi_{ref}, D) = - \\sum_{u} ln P_{ATT} (t_{a,v}, y_{1:u-1}),$\n$L_{CTC} (\\pi_{ref}, D) = - ln P_{ctc} (t^*|a, v),$\nwhere yu\u22121 is the preceding tokens of the ground truth character sequence y*."}, {"title": "3.3 Bifocal Preference Optimization", "content": "To align the fine-tuned ASR model \u03c0ref (Equation 3) with the AV-ASR task, we use input-side preference dataset Dc and the output-side preference dataset Dp (as described in Section 3.1) to train the BPO-AVASR model \u03c0\u03b8 through bifocal preference optimization.\nAnalogous to the Equation 2, the bifocal preference optimization objective for AV-ASR is formulated as\n$L_{BPO}(\\pi_\\theta; \\pi_{ref}, D) = L_{input}(\\pi_\\theta; \\pi_{ref}, D_c) + L_{output}(\\pi_\\theta; \\pi_{ref}, D_p),$\n$L_{input} (\\pi_\\theta; \\pi_{ref}, D_c)= -\\log \\sigma \\bigg( \\beta \\big( \\log \\frac{\\pi_\\theta(t_w|c_w)}{\\pi_{ref}(t_w|c_w)} - \\log \\frac{\\pi_\\theta(t_l|c_w)}{\\pi_{ref}(t_l|c_w)} \\big) \\bigg),$\n$L_{output}(\\pi_\\theta; \\pi_{ref}, D_p) = - \\log \\sigma \\bigg( \\beta \\big( \\log \\frac{\\pi_\\theta(t_w|c_w)}{\\pi_{ref}(t_w|c_w)} - \\log \\frac{\\pi_\\theta(t_w|c_w)}{\\pi_{ref}(t_w|c_w)} \\big) \\bigg),$\nwhere Linput and Loutput are preference optimization loss for input-side preference and output-side preference respectively. (tl |cw) and (tw |cl ) serve as hard negatives to (tw |cw), sampled from the preference datasets Dp and Dc respectively. Through this approach, we train the BPO-AVASR model \u03c0\u03b8 to avoid generating the above errors in the application, thereby improving the ability of the AV-ASR model to take good advantage of complex audiovisual information, especially for unconstrained real-world videos."}, {"title": "4 Experimental Settings", "content": "4.1 Implementation Details\nAs described in Section 3.2, we use OWSM v3.1 as our speech recognition backbone and develop two versions of BPO-AVASR: BPO-AVASR small and BPO-AVASR medium respectively. Specifically, BPO-AVASR-small consists of 9 E-Branchformer encoder blocks and 9 Transformer decoder blocks. The hidden size is 768. BPO-AVASR-medium comprises 18 E-Branchformer encoder blocks and 18 Transformer decoder blocks with a hidden size of 1024. Following AVFormer"}, {"title": "4.2 Datasets", "content": "In the supervised fine-tuning stage, we apply How2 as the fine-tuning dataset. In the bifocal preference optimization stage, we construct the preference dataset based on the How2. BPO-AVASR models are evaluated on three datasets, How2, VisSpeech, and Ego4D.\n\u2022 How2 (Sanabria et al. 2018) is an instructional video dataset designed for multimodal understanding. Following AVFormer, we use the 300-hour version of How2. These videos are segmented into short clips, averaging 5.8 seconds each, with a user-uploaded transcript.\n\u2022 VisSpeech (Gabeur et al. 2022) is an AV-ASR benchmark sampled from the HowTo100M dataset. It contains 508 video clips with manually annotated transcripts. VisSpeech uses a video-text similarity model to ensure high audiovisual correspondence.\n\u2022 Ego4D (Grauman et al. 2022) is an egocentric daily-life activity video dataset. We use the audiovisual diarization benchmark and evaluate our model on the validation set with ground truth annotations. Videos are segmented into shorter clips for analysis. Unlike other datasets, Ego4D contains noisier and more spontaneous videos across different domains, increasing the difficulty of AV-ASR."}, {"title": "4.3 Baselines", "content": "We compare BPO-AVASR models to strong baselines, including robust audio-only ASR models and state-of-the-art AV-ASR models.\n\u2022 Audio-only ASR Models. We compare BPO-AVASR series with (i) BEST-RQ, a robust ASR model pre-trained on LibriLight, LibriSpeech and HowTo100M datasets (~190K hours data in total); (ii) OWSM-ft small, the OWSM v3.1 small model pre-trained on public speech datasets and fine-tuned on the How2 using audio-only data (~180K hours data in total).\n\u2022 AV-ASR Models. We compare the BPO-AVASR series with the following AV-ASR baselines: How2"}, {"title": "5 Experimental Results", "content": "5.1 Comparison with SOTA Models\nWe compare BPO-AVASR with baseline models on three test sets in Table 1. The BPO-AVASR models outperform all baselines trained on the same audiovisual dataset (How2 base, VAT, MultiRes, LLD, SynesLM, and AVATAR), particularly on the noisy and spontaneous Ego4D dataset, highlighting the effectiveness of bifocal preference optimization in adapting ASR to unconstrained AV-ASR tasks. Furthermore, BPO-AVASR medium achieves better results than BPO-AVASR small, demonstrating the benefit of using larger models. While AVATAR\u2020 achieves the best performance on How2, it shows poor generalization ability on the out-of-domain dataset Ego4D. In contrast, the BPO-AVASR series shows consistently better results across different domain datasets, demonstrating the robustness and generalization capability of BPO-AVASR. Compared with the previous state-of-the-art work AVFormer, the BPO-AVASR series shows significant improvement, especially on How2 (31.6% and 32.4%, respectively). Notably, both AVFormer and BPO-AVASR are based on pre-trained ASR models with over 100K hours of training data. However, while AVFormer is further fine-tuned on 131k hours of audiovisual data from HowTo100M, BPO-AVASR achieves superior results using only 300 hours of audiovisual data.\n5.2 Qualitative Analysis\nExamples in Figure 3 illustrate the effectiveness of BPO-AVASR. Qualitative results show that OWSM-visual (w/o BPO) still struggles to obtain accurate transcripts in real-world video scenarios. In contrast, qualitative examples demonstrate how BPO-AVASR improves ASR performance. As shown in Figure 3, BPO-AVASR is effective in three scenarios: when vision provides content cues, when vision offers context clues, and when speech is under spontaneous scenarios. For instance, visual content helps the model recognize objects directly in the video, such as brand names and object names . Additionally, by using visual information as contextual clues, the model can correctly distinguish between similarly pronounced words . Furthermore, by constructing rejected samples considering spontaneous words, BPO-AVASR improves the accuracy of recognizing filler words in spontaneous speech scenarios.\n5.3 Ablation Studies\nAnalysis of Rejected Samples. To evaluate the effects of different preference construction strategies, we create pref-"}, {"title": "6 Conclusion", "content": "In this paper, we first formulate the AV-ASR task as a preference optimization problem. Accordingly, we develop BPO-AVASR, an AV-ASR system optimized by bifocal preference optimization to improve speech recognition accuracy for real-world videos. First, we introduce a simple yet effective method to create preference data by simulating common errors related to different modalities in AV-ASR. Second, we propose a bifocal preference optimization strategy to optimize AV-ASR models by emphasizing the distinction between correct and incorrect answers. BPO-AVASR outperforms previous state-of-the-art models, demonstrating the effectiveness of using preference optimization to align the audio-only ASR model to real-world video scenarios. For future work, we plan to build a high-quality open-domain AV-ASR dataset to facilitate future research."}]}