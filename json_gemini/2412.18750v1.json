{"title": "The Impact of Input Order Bias on Large Language Models for Software Fault Localization", "authors": ["Md Nakhla Rafi", "Tse-Hsun (Peter) Chen", "Dong Jae Kim", "Shaowei Wang"], "abstract": "Large Language Models (LLMs) show great promise in software engineering tasks like Fault Localization (FL) and Automatic Program Repair (APR). This study examines how input order and context size affect LLM performance in FL, a key step for many downstream software engineering tasks. We test different orders for methods using Kendall Tau distances, including \"perfect\" (where ground truths come first) and \"worst\" (where ground truths come last). Our results show a strong bias in order, with Top-1 accuracy falling from 57% to 20% when we reverse the code order. Breaking down inputs into smaller contexts helps reduce this bias, narrowing the performance gap between perfect and worst orders from 22% to just 1%. We also look at ordering methods based on traditional FL techniques and metrics. Ordering using DepGraph's ranking achieves 48% Top-1 accuracy, which is better than more straightforward ordering approaches like CallGraphDFS\u00b7 These findings underscore the importance of how we structure inputs, manage contexts, and choose ordering methods to improve LLM performance in FL and other software engineering tasks.", "sections": [{"title": "1 Introduction", "content": "Software development has significantly transformed with the emergence of Large Language Models (LLMs) like ChatGPT [1]. These tools have revolutionized how developers code, debug, and maintain software systems [2]. LLMs are widely adopted for their ability to simplify and accelerate development workflows, providing insights into complex tasks such as code generation and comprehension [3, 4].\nRecent research has explored the use of LLMs in various software engineering tasks, including Fault Localization (FL) [5, 6] and Automatic Program Repair (APR) [7, 8], which show great potential for automatically resolving real-world issues in large code bases. In particular, FL is a foundational step in the process, where the LLM processes structured lists to locate potential faulty code that requires fixing. Hence, high FL accuracy is instrumental to APR and automatic issue resolution."}, {"title": "2 Background and Related Works", "content": "While LLMs have demonstrated strong reasoning capabilities, prior research from other domains highlights a sensitivity to the order of input information. Studies have shown that LLMs perform better when information is presented in a sequence aligned with logical steps, with accuracy dropping significantly when the order is randomized [9]. Additionally, LLMs exhibit a primacy effect, often prioritizing earlier information in prompts [10]. Although these findings are well-documented in reasoning tasks, whether such sensitivities extend to software engineering scenarios like FL is unclear. Since FL involves analyzing ordered lists of methods or elements, the sequence in which this information is presented may influence the model's ability to identify faults.\nThis paper investigates how input order and context size affect the performance of large language models (LLMs) in Fault Localization (FL). We used Defects4J [11] benchmark, a widely used dataset in software engineering for evaluating FL techniques. First, we evaluate whether the order of methods impacts the LLM's ability to rank and identify faults by generating various input orders using Kendall Tau distance [12], including perfect (ground truth methods first) and worst (ground truth methods last) orders. We found that the LLM's performance is significantly influenced by input order, with Top-1 accuracy dropping from 57% to 20% when the perfect method list is reversed, indicating a strong order bias. Next, we explore segmenting large inputs into smaller contexts to address observed order biases. We observed that segmenting input sequences into smaller contexts reduces this bias; for example, the Top-1 gap between Perfect-Order and Worst-Order rankings decreased from 22% at a segment size of 50 to just 1% at a size of 10. Finally, we tested traditional FL and metrics-based ordering methods and found that using FL techniques improved results, with DepGraph outperforming Ochiai by 16% in Top-1 accuracy, while simpler strategies like CallGraph and LOC produced similar outcomes. These results highlight the importance of input order, context size, and effective ordering methods for enhancing LLM-based fault localization.\nIn summary, our contributions are as follows:\n\u2022 Method order significantly impacts LLM performance, with Top-1 accuracy dropping from 57% in Perfect-Order (ground truths first) to 20% in Worst-Order (ground truths last).\n\u2022 We demonstrate that dividing input sequences into smaller segments effectively mitigates order bias, reducing the Top-1 performance gap between perfect and worst orders from 22% to just 1%.\n\u2022 Ordering with different metrics and FL strategies significantly impacts outcomes. Ordering based on DepGraph achieves 48% Top-1 accuracy, 13.4% higher than CallGraphBFS. However, simpler methods like CallGraphDFS reach 70.1% Top-10 accuracy, highlighting their practicality in resource-constrained environments."}, {"title": "2.1 Fault Localization", "content": "Fault Localization (FL) [13] is a critical software engineering task identifying specific program parts responsible for a failure. It is particularly essential in large and complex codebases, where manually finding faults can be time-consuming and error-prone. FL saves significant developer effort and serves as a cornerstone for many downstream software engineering tasks such as Automatic Program Repair (APR) [14], debugging automation [15], and performance optimization [16]. The process begins with some indication of a fault, typically indicated by a failing test, which serves as the starting point. The input for FL often consists of a set of methods or code elements executed during the failing test case. FL aims to produce a ranked list of the most likely fault locations, providing developers with a focused starting point for investigation and resolution. Its significance lies in facilitating effective debugging and establishing the groundwork for workflows that automate and optimize the software development lifecycle."}, {"title": "2.2 Related Works", "content": "Spectrum-based and Supervised Fault Localization. Traditional methods such as Spectrum-Based Fault Localization (SBFL) use statistical techniques to assess the suspiciousness of individual code elements. [17]. The intuition is that the code elements covered by more failing tests and fewer passing tests are more suspicious. While simple and lightweight, these techniques, such as Ochiai [18],"}, {"title": "3 Methodology and Experiment Design", "content": "This section describes our overall approach to conducting experiments, summarized in Figure 1. First, we collect coverage information, including details about failing tests, stack traces, and the methods covered. Next, we generate various input orderings using Kendall Tau distance and different metrics. We pass this information along with the prompt to the LLMs for fault localization. Finally, we evaluate the model's bias by calculating the Top-K accuracy across different orderings. Below, we discuss more in detail."}, {"title": "3.1 Methodology", "content": "Prompt Design. We use LLMs to rank the most suspicious methods in fault localization tasks by analyzing failing tests, stack traces, and covered methods. We designed the prompts to be simple so we could better study the order bias. Our prompts consist of two primary components: a 1) System Message and a 2) User Message, to guide the LLM in ranking suspicious methods. The system message establishes the task by instructing the LLM to analyze a failing test, its stack trace, and a"}, {"title": "3.2 Experiment Design", "content": "Benchmark Dataset. We conducted the experiment on 501 faults across 13 projects from the Defects4J benchmark (V2.0.0) [11]. Defects4J is a widely used benchmark in the software engineering community for fault localization [20, 29, 30, 31, 21]. It provides a controlled environment for reproducing real-world bugs from a variety of projects, which differ in type and size. The benchmark"}, {"title": "4 Experiment Results", "content": "4.1 RQ1: Does the order in which the model processes code elements impact its performance?\nMotivation. LLMs often struggle to reason over long input sequences, known as order bias, where the model prioritizes input tokens at the beginning or end of the sequence [10]. While order bias has been studied in NLP tasks, such as deductive and mathematical reasoning [9], its impact on software engineering tasks remains under-explored. Order is crucial in software engineering tasks, such as fault localization and program repair, where the model must reason over a long code sequence. Therefore, in this RQ, we investigate how code sequence order affects LLM accuracy in fault localization.\nApproach. We create baselines with varying orderings to study how code order sequences affect LLM-based fault localization. The first baseline, Perfect-Order, places faulty methods (ground truths) at the top, followed by non-faulty methods, ordered by their call-graph (see Appendix A.1 for details) to minimize arbitrariness. Our intuition is that Perfect-Order serves as an idealized benchmark to test the hypothesis that prioritizing faulty methods should yield the highest accuracy if LLMs favor earlier orders due to their sequential processing nature. We then generate four additional baselines by adjusting the order using Kendall Tau distance [12], which measures the correlation between two lists (i.e., 1 = perfect alignment with the Perfect-Order, -1 = complete misalignment with the Perfect-Order). From Perfect-Order, we derive: \u2460 Random-Order (\u03c4 = 0; methods shuffled randomly), \u2461 Worst-Order (\u0442 = \u22121; faulty methods last), \u2462 Moderately Perfect-Order (r = 0.5; partial alignment), and \u2463 Moderately Inverted-Order (\u03c4 = \u22120.5; partial misalignment). Comparing these baselines to Perfect-Order allows us to assess how deviations from the Perfect-Order affect FL results. Finally, we evaluate the model's FL performance by ranking methods based on suspiciousness and measuring Top-K accuracy. For instance, a Top-1 score of 50% indicates that 50% of 501 faults' faulty methods were ranked first.\nResults. LLMs exhibit a bias toward the initial input order, achieving approximately 38% higher Top-1 accuracy for Perfect-Order compared to Worst-Order. Figure 3 shows the results of the experiments. For Perfect-Order, the model identifies 57.4% of faults in the Top-1 accuracy, while Moderately Perfect-Order reduces the model's fault detection to 26.1% (\u0394 31.1%). As Kendall Tau"}, {"title": "4.2 RQ2: Does limiting context window help reduce the bias towards order?", "content": "Motivation. In RQ1, we identified order bias in the sequence in which methods are presented in the zero-shot prompt. We hypothesize that a larger context window might amplify the bias toward method order, as the LLM processes all methods simultaneously and may weigh their order more heavily when generating responses. In this RQ, we investigate how the context window influences order bias. Specifically, we examine how segmenting the input sequence into smaller independent"}, {"title": "4.3 RQ3: How do different ordering strategies influence fault localization performance?", "content": "Motivation. We find that LLMs may have order biases toward Perfect-Order when investigating a list of methods for FL. However, in practice, such ground truth ordering is unknown. Hence, in this RQ, we investigate whether ordering methods based on the static or dynamic nature of the code or using existing FL techniques can help LLMs achieve better FL results.\nApproach. We explore four types of ordering: (1) Statistical-based and (2) Learning-based, which are directly derived from FL techniques, and (3) Metric-based and (4) Structure-based, which are grounded in static code features and not specifically tied to FL. The first two approaches leverage dynamic execution data or advanced models trained on FL tasks, making them more targeted for identifying faults. In contrast, the latter two approaches are agnostic to FL techniques. Hence, they may lack the specificity needed for accurately identifying faults, as they do not directly utilize FL data. By integrating ordering strategies with the rich contextual information in the prompt template"}, {"title": "5 Discussion & Conclusion", "content": "5.1 Discussion of Implications\nImplications of Ordering Strategies. Our findings show that the order of inputs significantly impacts the performance of large language models (LLMs) in FL. This highlights the need for thoughtful"}, {"title": "5.2 Conclusion", "content": "This work highlights several areas for future research. Order bias may influence the performance of large language models (LLMs) in tasks beyond FL, such as program repair, test case prioritization, and code refactoring. It would be beneficial to investigate how order bias affects these tasks and whether similar solutions can be applied. Additionally, specific prompts that incorporate domain knowledge, such as code semantics and dependency graphs, could enhance contextual understanding and reduce reliance on positional hints. Lastly, exploring new evaluation metrics that take into account the significance of input order and context size will help us gain a better understanding of how LLMs operate in software engineering tasks."}, {"title": "A Appendix", "content": "A.1 Call Graph in Programming Languages\nA call graph is a structure that represents how different parts of a program call each other during execution. For example, the nodes in the graph represent functions or methods in the code, and the edges represent the \"calls\" or \"invocations\u201d where one function triggers another.\nIn NLP, ordering is crucial, whether it involves the order of words in a sentence or the sequence of steps in a pipeline because the correct sequence ensures dependencies are preserved and the process produces meaningful results. Similarly, in software, the order in which functions are called determines the program's flow of execution. For example, a call graph helps us understand this flow, enabling us to: (i) Identify which parts of the code depend on others, helping analyze dependencies or optimize performance, and (ii) focus on functions that are frequently called, which might indicate critical components in the program.\nArbitrary ordering can introduce inconsistencies in analysis. To resolve this, we adopt call graph ordering, which mirrors the program's natural execution order and ensures the ordering respects dependencies while maintaining logical consistency.\nA.2 Learning-based fault localization using DepGraph\nDepGraph transforms static and dynamic code information into a unified graph for fault localization. It combines the Abstract Syntax Tree (AST) with interprocedural call graphs to capture method dependencies, effectively eliminating irrelevant nodes and edges for a more streamlined graph. Dynamic test coverage connects tests to the methods and statements they cover, with pruning to retain only the most relevant connections. Additionally, the code change history, including metrics such as code churn and modification count, is incorporated as an attribute of the nodes. This provides"}]}