{"title": "Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport", "authors": ["Mingyang Sun", "Pengxiang Ding", "Weinan Zhang", "Donglin Wang"], "abstract": "Diffusion policies have shown promise in learning complex behaviors from demonstrations, particularly for tasks requiring precise control and long-term planning. However, they face challenges in robustness when encountering distribution shifts. This paper explores improving diffusion-based imitation learning models through online interactions with the environment. We propose OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement learning fine-tuning), a novel method that integrates diffusion policies with RL using optimal transport theory. OTPR leverages the Q-function as a transport cost and views the policy as an optimal transport map, enabling efficient and stable fine-tuning. Moreover, we introduce masked optimal transport to guide state-action matching using expert keypoints and a compatibility-based resampling strategy to enhance training stability. Experiments on three simulation tasks demonstrate OTPR's superior performance and robustness compared to existing methods, especially in complex and sparse-reward environments. In sum, OTPR provides an effective framework for combining IL and RL, achieving versatile and reliable policy learning.", "sections": [{"title": "1. Introduction", "content": "Robotic manipulation is an intricate endeavor, where the delicate interplay of long-term planning and instantaneous control poses a captivating challenge - the quest to develop policies that can seamlessly navigate this balance lies at the forefront of modern robotics. Tasks demand not only the ability to execute complex sequences but also the adaptability to handle uncertainties and disturbances. Imitation Learning (IL) has emerged as a popular data-driven approach for training robots by imitating demonstration data, with advancements of Behavior Cloning (BC) like diffusion models and action chunking enhancing its ability to learn complex, long-horizon behaviors. Notably, Diffusion Policy (DP) has shown promise due to the capacity to handle multi-modal action distributions, excel in high-dimensional spaces, and achieve stable training through techniques like denoising and score matching. However, these advancements still fail to address the fundamental flaws of BC, which remains highly susceptible to distributional shifts, where the policy encounters states outside its training data, leading to compounding errors. Reinforcement Learning (RL) offers a powerful framework for autonomous learning through trial-and-error interactions guided by reward signals, making it particularly effective in training reactive controllers that adapt to noise, disturbances, and unforeseen states. RL learns corrective behaviors directly from experience, enabling policies to recover from errors and handle states beyond the training distribution. Its ability to optimize over long time horizons can also refine action sequences, enhancing robustness and precision. Unlike IL, which benefits from leveraging demonstration data to jump start learning, RL enhances generalization by exploring diverse scenarios and adapting dynamically to environmental changes. However, RL also faces significant challenges, including the need for carefully designed reward functions and vast interaction data, which is costly to collect, particularly in real-world settings. These strengths and weaknesses suggest that an integrated approach, combining RL's adaptability with DP's demonstration-driven learning, holds promise for achieving reliable, scalable, and versatile robotic manipulation. The most common approach is to pretrain a imitation policy with human data and then finetune it with RL. Some methods apply additional regularization or seperated policy network to ensure that the knowledge from demonstrations"}, {"title": "2. Related Work", "content": "Diffusion based policies. Diffusion-based policies have shown recent success in robotics and decision-making applications. In a pioneering work, \u201cDiffuser\u201d, a planning algorithm with diffusion models for offline reinforcement learning. This framework is extended to other tasks in the context of offline reinforcement learning, where the training dataset includes reward values. Most typically, diffusion based policies are trained from human demonstrations through a supervised objective, and enjoy both high training stability and strong performance in modeling complex and multi-modal trajectory distributions. The application of DDPM and DDIM on visuomotor policy learning for physical robots outperforms counterparts like Behavioral Cloning. While these techniques effectively learn from multi-modal data, they often create models that are non-trivial to fine-tune using RL. Even if they were compatible with RL, the fine-tuning process can be computationally prohibitive due to the large number of parameters in modern policy models.\nTraining diffusion models with reinforcement learning. As demonstration data are often limited, there have been many approaches proposed to improve the performance of diffusion-based policies. One straightforward approach involves framing diffusion denoising as a Markov Decision Process (MDP), which facilitates preference-aligned generation with policy gradient reinforcement learning. However, this approach often suffers from instability, limiting its practical applicability. introduced policy gradient loss on a two-layer MDP for direct diffusion policy fine-tuning, which mitigates this instability, but the method is architecture-specific and does not introduce closed-loop control. Alternative approaches to integrating diffusion architectures with reinforcement learning (RL) include leveraging Q-function-based importance sampling, employing advantage-weighted regression, or reformulating the objective as a supervised learning problem with return conditioning. Additionally, researchers have explored enhancing the denoising training objective by incorporating Q-function maximization and iteratively refining the dataset using Q-functions. Another promising direction involves augmenting a frozen, chunked diffusion policy model with a residual policy trained through online RL, enabling improved performance without modifying the pre-trained diffusion model."}, {"title": "3. Background", "content": "In this section, we offer fundamental definitions and theories to lay the groundwork for our framework, which will be thoroughly analyzed afterwards."}, {"title": "3.1. Optimal Transport", "content": "Given two probability spaces $(X, \\mu)$, $(Y, \\nu)$ and a cost function $c : X \\times Y \\rightarrow \\mathbb{R}$, the Monge problem is solving optimal map $T : X \\rightarrow Y$ such that\n$\\inf {M(T) := \\mathbb{E}_{x\\sim\\mu} [c(x, T(x))] \\mid T_{\\#}\\mu = \\nu}$                                                                (1)\nwhere the random variables $x \\sim \\mu$ and $T_{\\#}\\mu$ is push forward of $\\mu$ subject to $(T_{\\#}\\mu)(Y') := \\mu(T^{-1}(Y'))$ for any measurable set $Y' \\subset Y$. Instead of finding the map $T$ in the original Monge problem, the relaxed Kantorovich optimal scheme $K(\\gamma)$ is obtained by $\\gamma$ realizing\n$\\inf {K(\\gamma) := \\mathbb{E}_{x,y\\sim\\gamma}[c(x, y)] \\mid \\gamma \\in \\Gamma(\\mu, \\nu)}$,                                                                (2)\nwhere $\\Gamma(\\mu, \\nu)$ is the space composed of all joint probability measures $\\gamma$ on $X \\times Y$ with marginals $\\mu$ and $\\nu$.\nRegularized OT Regularization was introduced in to speed up the computation of OT problem, which is achieved by incorporating a negative-entropy penalty $\\mathcal{R}$ to the primal variable $\\gamma$ of Problem 2,\n$\\inf {K_{\\lambda}(\\gamma) := \\mathbb{E}_{x,y\\sim\\gamma}[c(x, y)] + \\frac{1}{\\lambda}\\mathcal{R}(\\gamma) \\mid \\gamma \\in \\Gamma(\\mu, \\nu)}$,                                                                (3)\nAs highlighted by adding a regularization term with $\\alpha$-strong convexity (such as entropy or squared $L_2$ norm) to the problem 3 is a sufficient condition for $\\lambda \\alpha$-strong convexity of $K_{\\lambda}(\\gamma)$ in $L_1$-norm, which makes the dual problem an unconstrained maximization problem."}, {"title": "3.2. Reinfrocement Learning and Imitation Learning", "content": "Reinfrocement Learning We consider a standard Markov decision process (MDP) consisting of state space $s \\in S$, continuous action space $a \\in A$, deterministic state transition function $P: S \\times A \\rightarrow S$, reward function $r: S \\rightarrow R$ and discount factor $\\kappa$. $\\tau \\sim \\pi$ denotes the distribution of trajectory $(s_0, a_0, s_1, a_1, ...)$ given the policy $\\pi(a|s)$. The action-state value function is $Q^{\\pi}(s, a) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty} \\kappa^t r_t | a_0 = a, s_0 = s]$. The goal of RL is to learn the policy $\\pi$ that maximizes the discounted expected cumulative reward over a trajectory $\\tau$, defined as $I_{RL}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{k=0}^{\\infty} \\kappa^k r_k]$.\nImitation Learning We assume access to a dataset $D$ of demonstrations collected by expert human operators (often assumed to be optimal). Each trajectory $\\tau \\in D$ consists of a sequence of transitions $\\{(s_0, a_0), ..., (s_k, a_k)\\}$. The most common IL method is behavior cloning (BC) which trains a parameterized policy $\\pi_\\theta$ to minimize the negative log-likelihood of data, i.e., $L(\\theta) = -\\mathbb{E}_{(s, a) \\sim D} [\\log \\pi_{\\theta}(a|s)]$. In this work, we assume $\\pi_\\theta$ follows an isotropic Gaussian as its action distribution for simplicity. With the isotropic assumption, the BC training objective can be formulated as the following squared loss: $L_{IL}(\\theta) = \\mathbb{E}_{(s, a) \\sim D} ||\\pi_{\\theta}(s) - a||^2$"}, {"title": "3.3. Conditional Score Based Diffusion Policy", "content": "The conditional Score Based Diffusion Models (SBDMs) aim to generate a target sample $y$ from the distribution $\\mu$ of target training data given a condition data $x$. In imitation learning, diffusion policy regard state $s$ as condition $x$ and use a forward stochastic differential equation (SDE) to add Gaussian noises to the target training data $a$ for training the conditional score-based model. The forward SDE is $da_t = f(a_t, t)dt + g(t)dw$ with $a_0 \\sim \\nu$, where $w \\in \\mathbb{R}^D$ is a standard Wiener process, $f(\\cdot, t) : \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ is the drift coefficient, and $g(t) \\in \\mathbb{R}$ is the diffusion coefficient. Let $\\nu_{t|0}$ be the conditional distribution of $a_t$ given the initial state $a_0$, and $\\nu_t$ be the marginal distribution of $a_t$. The conditional score-based model is trained by denoising score-matching loss:\n$J_{DSM}(\\theta) = \\mathbb{E}_{t \\sim \\omega_t} \\mathbb{E}_{a_0 \\sim \\nu} \\mathbb{E}_{a_t \\sim \\nu_{t|0}(a_t|a_0)} ||s_{\\theta}(a_t; s_{cond}(a_0), t) - \\nabla_{a_t} \\log \\nu_{t|0}(a_t|a_0)||^2$,                                                                             (7)\nwhere $\\omega_t$ is the weight for time $t$. In this paper, $t$ is uniformly sampled from $[0, T]$, i.e., $t \\sim U([0, T])$. With the trained $s_{\\theta}(a; s, t)$, given a condition data $s$, the target sample $a_0$ is generated by the reverse SDE as $da_t = [f(a_t, t) - g(t)^2 s_{\\theta}(a_t; s, t)] dt + g(t)dw$, where $w$ is a standard Wiener process in the reverse-time direction."}, {"title": "4. Method", "content": "We approach the policy optimization problem from the perspective of optimal transport. Considering Eq. 1, by substituting the cost function $c(x, a)$ with the critic $Q(s, a)$ and viewing our policy as a map that moves mass from the state distribution $\\mu(s)$, to the corresponding distribution of actions $\\nu(a)$ given by an optimal behavior policy $\\pi_{\\beta}(\\cdot|s)$, we formulate the following primal state-conditioned Monge OT problem:\n$\\inf {M(\\pi) := \\mathbb{E}_{s \\sim \\mu} [-Q_{\\beta}(s, \\pi(s))] \\mid \\pi_{\\#}\\mu = \\nu}$                                                                                       (8)\nThe objective is to minimize the expectation of the negative critic function $Q_\\beta$ while mapping exclusively to the distribution of actions given by the behavior policy $\\pi_\\beta$.\nProposition 4.1. Given an optimal behavior policy $\\pi_\\beta$ and a critic-based cost function $c = -Q_\\beta$, let $\\pi^*$ is the solution to Eq. 8 with the $Q_\\beta$ cost function. Then it holds that: $I_{RL}(\\pi^*) = I_{RL}(\\pi_\\beta)$.\nThe proof is given in Appendix B.1. Proposition 4.1 offers valuable insights into the connection between optimal transport theory and RL by establishing an equivalent relationship between the optimal transport map and the optimal policy. Meanwhile, given the paired state-action data derived from an expert policy, the IL problem can be reframed as achieving a conditional optimal transport map (i.e., diffusion policy). This link indicates that the transformations defined by the optimal transport map can effectively integrate RL with IL. We will next demonstrate how to use an estimated optimal transport plan to serve as a guide to utilize reinforcement learning to optimize the pre-trained diffusion policy via imitation learning."}, {"title": "4.2. OT-Guided Conditional Denoising Score Matching", "content": "In IL setting, we denote the condition data as $s_{cond}(a)$ for a target action $a$, and $\\mu$ is the measure by push-forwarding $\\nu$ using $S_{cond}$, i.e., $\\mu(s) = \\{a: S_{cond}(a) = s\\} \\nu(a)$ over the paired training dataset $D$. Section 3.3 provides a explicit reformulation for the conditional score-based diffusion policy with the paired training data.\nProposition 4.2. Let $C(s, a) = \\mu(s) \\delta(s - S_{cond}(a))$ where $\\delta$ is the Dirac delta function, then $J_{DSM}(\\theta)$ in Eq. 7 can be reformulated as\n$I_{CDSM}(\\theta) = \\mathbb{E}_{t \\sim \\omega_t} \\mathbb{E}_{s \\sim \\mu} \\mathbb{E}_{a \\sim \\nu} C(s, a) \\mathbb{E}_{a_t \\sim \\nu_{t|0}(a_t|a)} ||s_{\\theta}(a_t; s, t) - \\nabla_{a_t} \\log \\nu_{t|0}(a_t|a)||^2$.                                                                                                                                                                     (9)\nFurthermore, $\\nu(s, a) = C(s, a) \\mu(s) \\nu(a)$ is a joint distribution for marginal distributions $\\mu$ and $\\nu$"}, {"title": "4.3. Expert Data Masked Optimal Transport", "content": "For the computation of H, a value-based reinforcement learning can provide an estimated Q-network, while optimizing the optimal tranport problem gives u, v, which is often computationally challenging because OT needs transport all the mass of state to exactly match the mass of action distribution, which presents computational challenges. Fortunately, in imitation learning, expert demonstrations $D_B$ have provide matched pairs of state and action data points (called \u201ckeypoints\u201d) $K = \\{(s_i, a_i)\\}_{i=1}^N$. These keypoints are not only valuable but also crucial for investigating how to leverage them to guide the correct matching in OT. We introduce masked OT to leverage the given matched keypoints to guide the correct transport in OT by preserving the relation of each data point to the keypoints:\n$\\inf {K(\\zeta) := \\mathbb{E}_{s, a \\sim [\\gamma(s, a)]} \\mid \\gamma \\in (\\mu, \\nu; m)}$,                                                                      (12)\nwhere the transport plan $m$ is $\\gamma^{(m)}(s, a) = m(s, a)\\gamma(s, a)$, and $m$ is a binary mask function. Given a pair of keypoints $(s_i, a_i) \\in K$, then $m(s_i, a_i) = 1, m(s_i, a) = 0$ and $m(s, a) = 1$ if s, a do not coincide with any keypoint. The mask-based modeling of the transport plan ensures that the keypoint pairs are always matched in the derived transport plan. g in Eq. 12 is defined as $g(s, a) = d(R_s, R_a)$, where $R_s, R_a \\in (0, 1)^N$ model the vector of relation of s, a to each of the paired keypoints in state and action space respectively, and d is the Jensen-Shannon divergence. The i-th elements of $R_s$ and $R_a$ are respectively defined by\n$R_s^i = \\frac{\\exp(-c(s, s_i) / \\rho)}{\\sum_{j=1}^N \\exp(-c(s, s_j) / \\rho)}$,\n$R_a^i = \\frac{\\exp(-c(a, a_i) / \\rho)}{\\sum_{j=1}^N \\exp(-c(a, a_j) / \\rho)}$,                                                                                                                                                     (13)\nwhere $\\rho$ is a commonly used temperature in the softmax function. Further, the masked matrix is introduced into the duality of the $L^2$ reguarized OT problem, and the penalty term $\\mathcal{F}_{\\lambda}$ is updated as:\n$\\sup_{\\kappa, \\nu} \\mathbb{E}_{(s, a) \\sim \\mu \\times \\nu} [u(s) + v(a) + \\mathcal{F}_{\\lambda}(u(s), v(a))]$,                                                                                                                                                                  (14)\n$\\mathcal{F}_{\\lambda}(u(s), v(a)) = -\\frac{1}{4 \\lambda} m(s, a) (u(s) + v(a) - g(s, a))_+^2$.\nThe dual 5 and 14 are unconstrained concave, which can be maximized through stochastic gradient methods by sampling batches from $\\mu \\times \\nu$. Following , we use deep neural networks for their ability to approximate $u_\\omega, v_\\omega$ with the parameters $\\omega$ and the estimate of OT plan is\n$\\hat{\\gamma}(s, a) = H(s, a) d_{u(s)} d_{v(a)}$,\nwhere $H(s, a) = \\frac{1}{2 \\lambda} (u_\\omega(s) + v_\\omega(a) - g(s, a))_+$.\nThe pseudo-codeis given in Appendix A.1.\n(15)"}, {"title": "4.4. OT-Guided Training", "content": "To implement $\\mathcal{J}_{HDSM}(\\theta)$ in Eq. 10 using training samples to optimize $\\theta$, we can sample mini-batch data from replay buffer, and then compute $H(s, a)$ and $\\mathbb{E}_{t \\omega_t} \\mathbb{E}_{a_t \\sim \\nu_{t|0}(a_t)} ||s_{\\theta}(a_t; s, t) - \\nabla_{a_t} \\log \\nu_{t|0}(a_t|a)||$ over the pairs of (s, a) in S and A. However, such a strategy is sub-optimal. This is because given a mini-batch of samples s and a, for each source sample s, there may not exist a target sample a in the mini-batch with a higher value of $H(s, a)$ that matches condition data s. Therefore, few or even no samples in a mini-batch contribute to the loss function, leading to a large bias of the computed loss and instability of the training. To tackle this challenge, we generate L samples from policy model, and then use the compatibility function to reweight these actions, ultimately forming the intended policy when resampled. This approach is summarized in Algorithm 1. In implementation, our approach can be used to replace the policy improvement step in multiple RL algorithms, while keeping the critic training as is. At evaluation time, we simply taking the action by setting L = 1 to reduce computational requirements."}, {"title": "5. Analysis", "content": "The proposed OTPR essentially aims to develop a conditional score-based diffusion policy for data transport from state space to action space in OT. To generate samples from conditional OT plan $\\gamma^*(\\cdot|s)$, the algorithm involves two key module learning: the dual term $(\\kappa_\\omega, \\nu_\\omega)$ and the score model $s_\\theta$. In this section, we will provide an analysis from the perspective of optimal transport, illustrating how the two aforementioned processes establish the upper bound of the distance between the distribution $\\nu_{SDE}(a|s)$ of generated samples and the conditional optimal transport plan $\\gamma^*(a|s)$.\nTo be specific, we investigate the upper bound of the expected Wasserstein distance $\\mathbb{E}_{s \\sim \\mu} W_2(\\nu_{SDE}(\\cdot|s), \\gamma^*(\\cdot|s))$. Since $W_2(\\cdot)$ is a proper metric, we can conveniently leverage the triangle inequality to derive an upper bound for this expectation: $\\mathbb{E}_{s \\sim \\mu} W_2(\\nu_{SDE}(\\cdot|s), \\gamma^*(\\cdot|s)) \\leq \\mathbb{E}_{s \\sim \\mu} W_2(\\hat{\\gamma}(\\cdot|s), \\gamma^*(\\cdot|s)) + \\mathbb{E}_{s \\sim \\mu} W_2(\\nu_{SDE}(\\cdot|s), \\hat{\\gamma}(\\cdot|s))$, where $\\hat{\\gamma}$ is the estimated OT plan depending on $u_\\omega, v_\\omega$. This inequality provides a means to assess the upper bound by breaking it down into two more manageable comparisons.\nTo bound the first term, we denote the Lagrange function for $L_2$-regularized OTs in Eq. 5 as $\\mathcal{L}(\\gamma, u, v)$ with dual variables u, v as follows:\n$\\mathcal{L}(\\gamma, u, v) = -\\int (c(s, a) + \\frac{\\lambda}{2} \\frac{\\gamma(s, a)^2}{\\mu(s)\\nu(s)}) ds da + \\int u(s) \\Big( \\int \\gamma(s, a) da - \\mu(s) \\Big) ds + \\int v(a) \\Big( \\int \\gamma(s, a) ds - \\nu(a) \\Big) da$.                                            (16)\nBecause $\\mathcal{L}(\\gamma, u, v)$ is a sum of $K_{\\lambda}(\\gamma)$ and linear terms, the Lagrangian inherits $\\lambda$-strong convexity in $L_1$-norm. Given the trained $u_\\omega$ and $v_\\omega$ which are $\\epsilon$-approximate maximizers of $\\mathcal{T}_\\lambda(u, v)$, the pseudo-plan $\\gamma = H(s, a; u_{\\tilde{\\omega}}, v_{\\tilde{\\omega}}) \\mu(s) \\nu(a)$ satisfies:\n$\\frac{\\lambda}{2} ||\\gamma - \\gamma^*||_1^2 \\leq \\mathcal{L}(\\gamma, u_{\\tilde{\\omega}}, v_{\\tilde{\\omega}}) - \\mathcal{L}(\\gamma^*, u^*, v^*) \\leq \\epsilon$                                                      (17)\nSince the strong convexity of $\\mathcal{L}$ implies a Polyak-\u0141ojasiewicz (PL) inequality, we have,\n$\\frac{\\lambda}{2} ||\\gamma - \\gamma^*||_1 \\leq \\frac{1}{2} ||\\nabla_{\\gamma} \\mathcal{L}(\\gamma, u_{\\tilde{\\omega}}, v_{\\tilde{\\omega}})||_1$                                                (18)\nConsequently, we can derive an upper bound for the expected Wasserstein distance as follows:\n$\\mathbb{E}_{s \\sim \\mu} W_2(\\hat{\\gamma}(\\cdot|s), \\gamma^*(\\cdot|s)) \\leq \\eta \\frac{\\lambda}{2} ||\\nabla_{\\gamma} \\mathcal{L}(\\gamma, u_{\\tilde{\\omega}}, v_{\\tilde{\\omega}})||_1$,                                          (19)\nwhere $\\eta = \\max_{a, a' \\in A} \\{ ||a - a' ||^2 \\}$.\nFor the bound of $\\mathbb{E}_{s \\sim \\mu} W_2(\\nu_{SDE}(\\cdot|s), \\hat{\\gamma}(\\cdot|s))$, it is difficult to get without explicit f and g given, but from the existing convergence guarantees for a general class of score-based generative models, we get $\\mathbb{E}_{s \\sim \\mu} W_2(\\nu_{SDE}(\\cdot|s), \\hat{\\gamma}(\\cdot|s)) \\leq \\epsilon$, which can be easily interpreted as two terms (1) the initialization of the algorithm at $\\hat{\\nu}_r(s)$ instead of $\\gamma_r^*(\\cdot|s)$, (2) the discretization and score-matching errors in running the algorithm."}, {"title": "6. Experiments", "content": "In this section, we evaluate OTPR and several prior approaches, in a number of benchmark domains that require learning policies from static offline expert data and then"}, {"title": "6.1. Experimental Setup", "content": "Environments and tasks. We study: (1) Robomimic tasks, which is a commonly used benchmark designed to study imitation learning for robot manipulation. The evaluation score represents the success rate. (2) Franka-Kitchen tasks, which require solving a sequence of four manipulation tasks in a kitchen environment with a 9-Dof Franka robot; and (3) the CALVIN benchmark, an evaluation benchmark designed for long-horizon, language-conditioned manipulation, which requires solving a sequence of four manipulation tasks in a tabletop environment. The evaluation score for a trajectory is the maximum number of sub-tasks completed simultaneously at any single point in the trajectory. The CALVIN task is significantly challenging, as policies must be learned directly from pixels using offline play data obtained through human teleoperation.\nImplementation details. We provide a detailed list of hyper-parameters and best practices for running OTPR in Appendix. We instantiate OTPR using the popular IQL with keeping the critic training as is. For the image-based domain, we use a ResNet 18 encoder and store features in the replay buffer to facilitate the estimation of the dual terms."}, {"title": "6.2. Results", "content": "Comparisons with Other Online Fine-Tuning Methods. We conduct a comprehensive comparison of OTPR against a range of reinforcement learning (RL) methods designed for fine-tuning diffusion-based policies. Specifically, we evaluate the following approaches: (1) Implicit Diffusion Q-Learning (IDQL), which extends Implicit Q-Learning (IQL) to incorporate diffusion policies through critic-based re-ranking; (2) Diffusion Policy Optimization (DPPO), which fine-tunes diffusion policies initially learned via imitation learning by optimizing a two-layer Markov Decision Process (MDP) loss; and (3) Diffusion Q-Learning (DQL), which trains diffusion policies using a reparameterized policy gradient estimator similar to the Soft Actor-Critic (SAC) framework.\nOverall, OTPR performs consistently and significantly improves fine-tuning efficiency and asymptotic performance of diffusion policies. Notably, OTPR consistently maintains high normalized scores in the kitchen-complete-v0, CALVIN and Can task, while other methods exhibit relative instability, especially DQL and IDQL, which show considerable fluctuations in performance across different interaction steps. This may be attributed to both DQL and IDQL performing off-policy updates and propagating gradients from the imperfect Q function to the actor, which results in even greater training instability in sparse-reward tasks given the continuous action space and large action chunk sizes. In contrast, OTPR can quickly mitigate the adverse effects brought about by this issue by leveraging the guidance of the compatible function. This analysis suggests that OTPR is a robust and effective approach for online fine-tuning in diffusion policy tasks, consistently outperforming the other methods in terms of stability and overall performance.\nComparisons with demo-augmented RL. Next, we compare OTPR with recently proposed RL methods for training robot policies (not necessarily diffusion based) leveraging offline data, including RLPD, Cal-QL and IBRL. These methods add expert data in the replay buffer and performs off-policy updates. We evaluate these methods on Franka-Kitchen and RoboMimic environents. IBRL and Cal-QL are also pretrained with behavior cloning and offline RL objectives, respectively. All of results are shown on Table 1. In the Franka-Kitchen domains, while Cal-QL demonstrates competitive performance, OTPR shows more impressive score improvements, rising from 61 to 92 in Kitchen-Complete-v0 and from 59 to 79 in Kitchen-Mixed-v0. In contrast, other methods such as RLPD, IQL, and IBRL perform significantly worse, particularly in the Kitchen-Partial-v0 task, where OTPR leads with a final score of 93. In the RoboMimic environment, OTPR continues to excel, achieving high scores of 99 in Can-State and 98 in Square-State, showcasing its robustness across diverse scenarios. Although IBRL performs the best among the competitors, there remains a significant gap in performance."}, {"title": "6.3. Ablation Experiments", "content": "Effect of the compatibility function. In the previous section, we have already demonstrated the advantages of OTPR over other diffusion-based fine-tuning methods that rely on Q-values. Now, to spotlight the pivotal role of our method's core component-the guidance from the compatibility function H, we replace it with Q and advantages A within the same training framework. The experimental results on the Robomimic-Can task are illustrated in Fig. 3(left). Clearly, compared to using Q-value and advantage, OT-guided training demonstrates significantly faster convergence and superior evaluation performance.\nEffect of the masked OT. OTPR incorporates masked Optimal Transport (OT) to utilize expert data as keypoints, guiding accurate distribution transport. As depicted in Fig. 3, OTPR-U, which lacks the mask matrix, exhibits instability and reduced efficiency, despite outperforming other mainstream methods. Notably, even without the mask, OTPR can still operate as a fully functional offline RL algorithm by leveraging the compatibility function without reward."}, {"title": "7. Conclusion", "content": "This paper introduced OTPR, a novel method integrating optimal transport theory with diffusion policies to enhance the efficiency and adaptability of reinforcement learning fine-tuning. OTPR leverages the Q-function as a transport cost and uses masked optimal transport to guide state-action matching, improving learning stability and performance. Experiments demonstrated OTPR's superior performance across multiple tasks, especially in complex environments."}]}