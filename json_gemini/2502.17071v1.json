{"title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability", "authors": ["Ashhadul Islam", "Samir Brahim Belhaouari", "Amine Bermak"], "abstract": "The exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of artificial intelligence, particularly in the development and deployment of large language models (LLMs) like ChatGPT, has brought about significant benefits across various domains. These models, with billions of parameters, have demonstrated unparalleled capabilities in tasks such as natural language understanding, translation, and text generation. However, the growing scale of these models comes with substantial challenges, particularly in terms of computational cost, environmental impact, and resource consumption, which underscore the need for optimization.\nOne of the most pressing concerns associated with training and deploying LLMs is their considerable carbon footprint. The computational resources required to train these models are immense, resulting in significant CO2 emissions. For instance, the training of BERT, a model with 110 million parameters (Zhou), is estimated to have a carbon footprint comparable to a round trip flight across the United States (Strubell et al., 2020). For larger models like ChatGPT, which contains 137 billion parameters (Gooding, 2023), the environmental impact is even more pronounced, equating to the annual CO2 emissions of 13,483 Americans (Ludvigsen, 2022). This alarming scale of emissions highlights the urgency of developing more sustainable approaches to AI. In addition to carbon emissions, the electricity consumption associated with LLMs is staggering. The daily global usage of ChatGPT alone consumes an estimated 12 million kilowatt-hours (KWh) of electricity, equivalent to the monthly electricity consumption of 90,000 Danish households (Ludvigsen). This energy demand not only poses a significant operational cost but also contributes to the environmental burden, particularly in regions where electricity is predominantly generated from fossil fuels. Another critical aspect is the water footprint of AI models, a less commonly discussed but equally important factor. Training large models like ChatGPT requires vast amounts of water for cooling data centers, with estimates suggesting that training such a model can consume over 700,000 liters of freshwater, enough for producing 370 BMW cars or 320 Tesla electric vehicles (Li et al., 2023). Moreover, even a simple conversation with ChatGPT, involving 20-50 questions and answers, could necessitate the equivalent of a 500ml bottle of water (Li et al., 2023). This substantial water usage raises concerns, especially in areas facing water scarcity. Given the finite nature of natural resources and the increasing size of AI models, it is evident that the current trajectory is detrimental to the environmental stability. The growing computational power required to train and deploy these models exacerbates the strain on global resources, necessitating a reevaluation of how these models are developed and maintained. Optimization techniques, such as model pruning and efficient resource management, are crucial in addressing these challenges. By reducing the size of models without compromising their performance, we can mitigate the environmental impact, reduce operational costs, and make AI more sustainable in the long run."}, {"title": "Contribution", "content": "In this research, we introduce a novel approach to pruning large language models (LLMs) by systematically evaluating the performance of each individual weight throughout the training process. The remainder of the paper is structured as follows: Section 2 reviews recent trends in LLM pruning, Section 3 details our proposed methodology, and Section 4 presents an overview of the performance of our approach. Finally, Section 5 concludes the paper by discussing the limitations of our work and potential directions for future research."}, {"title": "2. Related Work", "content": "Magnitude pruning (Han et al., 2015) is a standard technique to induce sparsity in neural networks by removing individual weights based on their magnitudes, typically determined either locally within each layer or globally across the entire network. Despite its simplicity, it has been effective in finding extremely sparse networks (Frankle and Carbin, 2019) and is considered a strong baseline approach (Blalock et al., 2020) for neural network sparsification. Dettmers et al. (Dettmers et al., 2022) observed emergent large magnitude features in Transformer-based large language models (LLMs), noting that when LLMs reach around 6B parameters, a small set of hidden state features emerges with significantly larger magnitudes than others, which are crucial for predictive performance. In the context of compressing recent LLMs, methods like LLM-Pruner (Ma et al., 2023) and FLAP (An et al., 2024) narrow network width by pruning coupled structures, while Sheared-LLaMA (Xia et al., 2023) reduces both network width and depth by removing entire layers. Although pruning methods that incorporate both width and depth aspects exist (Xia et al., 2022; Kurti\u0107 et al., 2024), there remains a need for detailed analysis comparing these factors' impact on LLM inference efficiency. Traditional pruning in Deep Neural Networks (DNNs) faces unique challenges when applied to LLMs, which have a large number of parameters and require significant computational resources (Brown et al., 2020). Various pruning methods for LLMs fall into unstructured and structured categories. Unstructured pruning methods (Dong et al., 2017; Chen et al., 2020, 2021a) set unimportant individual weights to zero, maintaining performance but resulting in sparse weight matrices that are less hardware-efficient. Methods like SparseGPT (Frantar and Alistarh, 2023) and Wanda"}, {"title": "3. Proposed Approach", "content": "The central element of the pruning method proposed in this research lies in monitoring the evolution of parameters over time. This involves systematically observing how parameter values shift throughout the training process across multiple epochs. The initial weight values are chosen randomly and then adjusted during training as per the training algorithm. We note the overall shift of weights of the network, with some weights jumping high radically and some staying low over the training phase."}, {"title": "4. Experiment And Results", "content": "To check the consistency of our methods, two key experiments were conducted. These experiments focused on evaluating the effects of pruning, a process that reduces the number of parameters in a model, on model performance. The first experiment tested a scaled-down LLM trained from scratch, while the second involved a large pre-trained multimodal model. Both experiments aimed to determine how much compression could be applied to these models before significant performance degradation occurred. Before looking at the individual experiments, we take a look at the general procedure."}, {"title": "4.1. Record Weighted Average", "content": "In addition to directly training the model, a cloned version is maintained alongside it. The parameters of this clone are updated through a weighted average method that integrates historical parameter values across the training epochs. Initially, the cloned model's parameters are set to zero before the training starts. After each training step, both the original model's parameters and the corresponding parameters in the clone are updated. The updated values in the clone are computed as a weighted average, combining the existing parameters with the new ones from the original model, based on the current epoch. This approach ensures that recent updates are given more significance in the clone. The weighted average process, which gradually incorporates the model's parameter values over the epochs, is expressed as:\n$I_{new} = \\frac{q_{old} \\times S_{prev} + p \\times (n + 1)}{S}$\nWhere:\n\u2022 $I_{new}$ are the updated parameters in the cloned model.\n\u2022 $q_{old}$ are the previous parameters in the cloned model.\n\u2022 $p$ are the current parameters in the original model.\n\u2022 $n$ is the current epoch number.\n\u2022 $S_{prev}$ is the sum of weights from epoch 1 to $n$ (inclusive).\n\u2022 $S$ is the sum of weights from epoch 1 to $n + 1$ (inclusive)."}, {"title": "4.2. Model Training and Pruning", "content": "\u2022 Step I: The Transformer model is trained over 5000 epochs, with weight changes recorded throughout the process.\n\u2022 Step II: After training, the model undergoes pruning based on the weighted parameters, followed by an additional 50 epochs of fine-tuning to maintain effective compression:\n    \u2013 The importance of each parameter is assessed by evaluating its weighted absolute value:\n$W_{abs} = |W|$"}, {"title": "4.3. Experiment 1: Scaled-Down LLM", "content": ""}, {"title": "4.3.1. Model and Dataset:", "content": "The first experiment utilized a scaled-down version of a ChatGPT-like Large Language Model (LLM), consisting of 10,788,929 (10.7 million) adjustable parameters. This substantial language model is founded on the Transformer architecture, which is specialized in generating text for natural language processing tasks (Vaswani et al., 2017). The model comprises several Transformer blocks, each containing the following components:\n\u2022 Embedding Layer: Input tokens are transformed into 384-sized vectors, forming the initial layer input.\n\u2022 Multi-Head Self-Attention Mechanism: This mechanism, with 6 heads, directs focus across different segments of the input sequence using linear transformations without bias, scaling the attention scores by the square root of the embedding size (384).\n\u2022 Positional Embeddings: Positional embeddings of the same size (384) are added to the input tokens to signify sequence positions, enabling the model to recognize word order.\n\u2022 Feed-Forward Network: Following the attention mechanism, each Transformer block includes a feed-forward network with two linear layers and a hidden layer size of 1536, activated by ReLU. Dropout is applied with a rate of 0.0 to reduce overfitting.\n\u2022 Layer Normalization: This process standardizes activations across features and enhances learning, applied after both the self-attention and feed-forward network layers.\n\u2022 Final Layer: The final layer performs a linear transformation, correlating output embeddings with the vocabulary size based on the unique input text characters.\nThe complete model stacks 6 of these Transformer blocks, processing the input independently to grasp varying semantic layers in the text. The model was trained from scratch using the complete works of Shakespeare, which served as the training dataset. The primary task for this model was next-token prediction, a common benchmark task in language modeling. All 10,788,929 trainable parameters were tracked throughout the training epochs."}, {"title": "4.3.2. Training Procedure:", "content": "The model was subjected to a series of pruning tests, where the parameter count was reduced incrementally. The compression levels ranged from 0% (no pruning) to 94% (high pruning). The effect of each level of compression on model performance was monitored by tracking the loss associated with the next-token prediction task."}, {"title": "4.4. Experiment 2: Multimodal Model", "content": ""}, {"title": "4.4.1. Model and Dataset:", "content": "The second experiment centered around a pre-trained Phi-3-vision model, a large multimodal model with 4.2 billion parameters, designed for both language and vision tasks. The dataset for this experiment comprised a variety of Burberry products, categorized under different product types such as hats, gloves, and sunglasses. The dataset included both textual descriptions and corresponding images of the products."}, {"title": "4.4.2. Training Procedure:", "content": "In this experiment, the Phi-3-vision model (Abdin et al., 2024) was fine-tuned for 10 epochs using the Burberry dataset (huggingface, 2023). After fine-tuning, the model underwent pruning at various compression levels, similar to the first experiment. The performance of the model was evaluated by measuring the Mean Absolute Error (MAE) at each level of compression to understand the impact of pruning on its ability to process and classify multimodal inputs."}, {"title": "4.5. Results", "content": ""}, {"title": "4.5.1. Results of Experiment 1", "content": "The results from the first experiment, as shown in Table 2, indicate that the model could tolerate compression up to 60% without a significant increase in loss. At this level, the compression loss was recorded at 1.656, which was lower than the initial loss of 1.9. However, further compression led to an increase in loss, with a sharp rise observed at 70% compression and above, where the loss eventually escalated to 3.098 at 94% compression. This demonstrates that moderate pruning can lead to a reduction in loss, but excessive pruning severely hampers model performance."}, {"title": "4.5.2. Results of Experiment 2", "content": "In the second experiment, the Phi-3-vision model exhibited stable performance at lower compression levels, as detailed in Table 3. The MAE decreased from 439 at 0% compression to 374 at 10% compression, suggesting that some level of pruning can improve the model's performance, possibly by eliminating redundant parameters. However, at compression levels beyond 25%, the MAE began to rise again, and a drastic increase was observed at 48% compression, where the MAE surged to 11,041. This indicates that while the model can handle moderate pruning, aggressive pruning beyond 30% significantly deteriorates its performance."}, {"title": "5. Limitations And Future Work", "content": "While the research on pruning and optimizing Large Language Models (LLMs) shows promising results, it is essential to recognize certain limitations that present challenges for future development.\n\u2022 Specialized Use Cases: While fine-tuning LLMs for specific purposes can yield high performance, it may limit the model's applicability across a broader range of tasks, requiring more adaptable solutions.\n\u2022 Adaptation to Model Size: As LLMs grow larger, the proportion of parameters that can be effectively pruned decreases unless more advanced techniques are applied, highlighting the need for methods that can handle large-scale models efficiently.\n\u2022 Memory Considerations: Managing the memory requirements for storing a weighted average of parameters in models with millions or billions of parameters presents a significant challenge, necessitating memory-efficient strategies."}, {"title": "5.1. Future Work", "content": "Looking forward, the future work in this area is both hopeful and exciting. There is a strong potential for tangible energy savings by optimizing LLMs more efficiently, making AI systems not only faster but also more sustainable. The research community is also poised to explore smarter pruning methods that could overcome current limitations, enabling deeper compression without sacrificing model accuracy or generalization capabilities. As we continue to push the boundaries of LLMs, the focus will be on balancing innovation with sustainability, ensuring that the advancements in AI contribute positively to both technological progress and environmental responsibility."}]}