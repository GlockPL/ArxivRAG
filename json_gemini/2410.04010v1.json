{"title": "Hyperbolic Fine-tuning for Large Language Models", "authors": ["Menglin Yang", "Aosong Feng", "Bo Xiong", "Jihong Liu", "Irwin King", "Rex Ying"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious tasks. However, it remains an open question whether the default Euclidean\nspace is the most suitable choice for embedding tokens in LLMs. In this study, we\nfirst investigate the non-Euclidean characteristics of LLMs. Our findings reveal\nthat token frequency follows a power-law distribution, with high-frequency tokens\nclustering near the origin and low-frequency tokens positioned farther away. Ad-\nditionally, token embeddings exhibit a high degree of hyperbolicity, indicating a\nlatent tree-like structure in the embedding space. Building on the observation, we\npropose to efficiently fine-tune LLMs in hyperbolic space to better exploit the un-\nderlying complex structures. However, we found that this fine-tuning in hyperbolic\nspace cannot be achieved with naive application of exponential and logarithmic\nmaps, when the embedding and weight matrices both reside in Euclidean space. To\naddress this technique issue, we introduce a new method called hyperbolic low-rank\nefficient fine-tuning, HypLoRA, that performs low-rank adaptation directly on the\nhyperbolic manifold, avoiding the cancellation effect caused by the exponential and\nlogarithmic maps, thus preserving the hyperbolic modeling capabilities. Through\nextensive experiments, we demonstrate that HypLoRA significantly enhances the\nperformance of LLMs on reasoning tasks, particularly for complex reasoning prob-\nlems. In particular, HypLoRA improves the performance in the complex AQUA\ndataset by up to 13.0%, showcasing its effectiveness in handling complex reasoning\nchallenges.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) such as GPT-4 (Achiam et al., 2023), LLaMA (Touvron et al., 2023),\nand Gemma (Gemma Team, 2024) have demonstrated remarkable capabilities in understanding and\ngenerating human-like text (Qin et al., 2023; Shen et al., 2024). Despite their impressive capabilities,\nthese models often rely on Euclidean geometry for learning text representations, which may not\nalways be the complex, hierarchical nature of real-world data structures (Bronstein et al., 2017;\nBachmann et al., 2020). For example, in language, words are often organized into categories that\nreflect varying levels of abstraction. These relationships naturally form a tree-like structure, where\ngeneral or abstract concepts, such as \"fruits,\" sit at the top of the hierarchy, while more specific or\nconcrete terms, like \"apples\" or \"bananas,\" reside at the lower levels. Representing such structures\neffectively is crucial for understanding the semantics of language in LLMs.\nRecent advancements suggest that non-Euclidean geometries, particularly hyperbolic spaces, offer\npromising alternatives for modeling hierarchical data. Hyperbolic space, distinguished by its negative\ncurvature, is especially well-suited for representing tree-like data due to its exponential volume\ngrowth, enabling efficient embeddings of hierarchies (Nickel & Kiela, 2017; 2018; Ganea et al.,\n2018a; Khrulkov et al., 2020; Cetin et al., 2022). However, a significant research gap remains: existing\nworks have not attempted to study LLM embeddings in the context of non-Euclidean geometry.\nProposed Analysis Framework In this work, we first delve deep into how LLMs interact with token\nembeddings and explore to what extent these embeddings exhibit non-Euclidean characteristics. We\nhttps://github.com/marlin-codes/HypLLM"}, {"title": "2 Related Work", "content": "Hyperbolic Representation Learning and Deep Learning Hyperbolic geometry has been suc-\ncessfully applied to various neural network architectures and models (Yang et al., 2022b; Mettes\net al., 2023; Peng et al., 2021), including shallow hyperbolic neural networks (Ganea et al., 2018a;b;\nChen et al., 2021; Shimizu et al., 2020), hyperbolic CNNs (Bdeir et al., 2023; van Spengler et al.,\n2023), and hyperbolic attention networks or Transformers (Gulcehre et al., 2018; Chen et al., 2021;\nShimizu et al., 2020; Yang et al., 2024). These models leverage the inductive biases of hyperbolic\ngeometry to achieve remarkable performance on various tasks and applications (Chami et al., 2019;\nYang et al., 2022a; Sun et al., 2021; Khrulkov et al., 2020; Cetin et al., 2022; Weng et al., 2021;\nXiong et al., 2022; Yang et al., 2021). However, training LLMs from scratch remains computationally\nexpensive (Kochurov et al., 2020; Smith, 2014). The computational complexity increases further\nwhen considering Riemannian optimization (Kochurov et al., 2020; Smith, 2014; B\u00e9cigneul & Ganea,\n2018) and additional hyperbolic operations, like M\u00f6bius addition.\nParameter Efficient Fine Tuning (PEFT) and LoRAs Fine-tuning LLMs (Foundation, 2022;\n2023; Touvron et al., 2023) for downstream tasks poses significant challenges due to their massive\nnumber of parameters. To address this issue, PEFT methods have been proposed, which aim to\ntrain a small subset of parameters while achieving better performance compared to full fine-tuning.\nPEFT methods can be broadly categorized into prompt-based methods (Lester et al., 2021; Li &\nLiang, 2021; Qin et al., 2021), adapter-based methods (Houlsby et al., 2019; Zhu et al., 2021),and\nreparameterization-based methods (Hu et al., 2021; Aghajanyan et al., 2020; Edalati et al., 2022).\nAmong these, LoRA (Hu et al., 2021) as the reparameterization-based method, has gained significant\nattention due to its simplicity, effectiveness, and compatibility with existing model architectures.\nVariants of LoRA, such as LoRA+(Hayou et al., 2024), DoRA (Liu et al., 2024), AdaLoRA (Zhang\net al., 2023), have been proposed to improve its performance and efficiency. Recent research has also\ninvestigated ensembles of multiple LoRAs (Wang et al., 2023; Ren et al., 2024), and quantization\ntechniques (Dettmers et al., 2024; Xu et al., 2023; Li et al., 2023). Despite these advances, existing\nmethods operate within Euclidean space, ignoring the underlying structure represented by LLMs. The\nproposed method is as a foundational algorithm, potentially combined with various LoRA variants,\nto exploit their complementary strengths and achieve superior performance."}, {"title": "3 Preliminary", "content": "This section introduces the concepts utilized in our study, including the LoRA adapter, the Lorentz\nmodel of hyperbolic geometry, hyperbolic linear transformations, and the concept of hyperbolicity.\nLORA Adapter The LoRA adapter offers an efficient approach for modifying large LLMs with\nminimal computational overhead. Instead of retraining the entire model, LoRA focuses on adjusting\nspecific components within the model's architecture to transform an input x into an output z. In\npractice, LoRA targets the weight matrices found in each Transformer layer of an LLM. Typically, the\nweight W of the Transformer, which resides in the dimensions $R^{d \\times k}$, is adapted through a low-rank\napproximation. This is achieved by introducing an additional term, $\\Delta W$, to the original weight\nmatrix:\n$z = W_{LORA} (X) = Wx + \\Delta Wx = Wx + BAx$.  (1)\nHere, $B \\in R^{d \\times r}$ and $A \\in R^{r \\times k}$ represent two smaller, learnable matrices where r\u2014the rank of these\nmatrices-is significantly less than either d or k. This design choice ensures that $r < min(d, k)$,\nthereby reducing the complexity of the model adaptation. During the fine-tuning process, only the\nmatrices A and B are adjusted, while the pre-existing weights W are kept frozen. This method\nsignificantly decreases the number of parameters that need to be trained, from dk to (d + k)r,\nenhancing the efficiency of the fine-tuning process. As a result, LoRA enables the targeted adaptation\nof LLMs, allowing them to transform an input x into an output z while maintaining high performance\nand adapting to new tasks or datasets with a fraction of the computational resources typically required.\nHyperbolic Geometry Unlike the flat Euclidean geometry, hyperbolic geometry is characterized by\na constant negative curvature. We utilize the Lorentz model, also known as the hyperboloid model,\nfor our study due to its ability to effectively capture hierarchical structures and maintain numerical\nstability (Nickel & Kiela, 2018; Chen et al., 2021). The Lorentz model in n dimensions with curvature"}, {"title": "4 Investigation", "content": "In this section, we present an in-depth investigation of token embeddings in LLMs from both global\nand local perspectives. Our goal is to uncover the geometric structures underlying pretrained token\nrepresentations, specifically examining the global distribution of token frequencies and their spatial\narrangement, as well as the local hyperbolicity of token embeddings across various datasets.\n4.1 Global Token Statistics\nWe begin by investigating the global distribution of token frequencies in the context of arithmetic\nreasoning datasets, focusing on datasets such as GSM8K (Cobbe et al., 2021), AQuA (Ling et al.,\n2017), MAWPS (Koncel-Kedziorski et al., 2016), and SVAMP (Patel et al., 2021). We also provide a\nbroader analysis across different types of datasets and LLMs in Appendix A.\nFigure 2 (left) presents the distribution of token frequencies, with a power-law exponent of approxi-\nmately y\u2248 1.9, as estimated by the Powerlaw Package (Alstott et al., 2014). In such distributions,\nthe exponent y controls how quickly token frequencies decline: smaller values of y (closer to 1)\nindicate a more gradual decay where frequent tokens dominate, while larger values signify a sharper\ndecline, with most tokens being rare.\nThis power-law behavior aligns with the hierarchical nature of language. High-frequency tokens\noften correspond to more abstract or general concepts, while low-frequency tokens represent specific\nor rare terms. This distribution naturally suggests a hierarchical organization of the token space,\nwhere general concepts serve as the \"roots\" and specific terms \"branch out\" as we move through the\nhierarchy."}, {"title": "4.2 8-Hyperbolicity of Token Embeddings", "content": "To rigorously quantify the hierarchical nature of token embeddings, we examine the d-hyperbolicity\nof space spanned by the token embedding. 8-Hyperbolicity, introduced by Gromov (Gromov, 1987),\nis a measure that captures the degree to which a metric space deviates from an exact tree structure.\nLower values of d imply a space more similar to a perfect tree, while higher values indicate deviation\nfrom a tree-like structure. A brief explanation of 8-hyperbolicity can be found on Wikipedia\u00b2.\nWe compute 8-hyperbolicity using the four-point condition, which compares the Gromov products\nbetween any four points a, b, c, and w in the metric space. Specifically, the hyperbolicity is defined\nas:\n$[a, c]_w \\geq min([a, b]_w, [b, c]_w) \u2013 \\delta$,\nwhere the Gromov product $[a, b]_w$ is:\n$[a, b]_w = \\frac{1}{2} (d(a, w) + d(b, w) \u2013 d(a, b))$.\nTo measure the hyperbolicity of token embeddings, we apply this algorithm to various open-source\nLLMs. Following the methodologies proposed by Khrulkov et al. (Khrulkov et al., 2020) and Cetin\net al. (Cetin et al., 2022), we estimate 8-hyperbolicity using the efficient algorithm introduced by\nFournier et al. (Fournier et al., 2015). To ensure scale invariance, we normalize 8 by the diameter of\nthe embedding space, diam(X), yielding a relative measure: $d_{rel} = \\frac{2\\delta}{diam(X)}$.  This relative measure\nranges from 0 to 1, with values closer to 0 indicating a highly hyperbolic (tree-like) structure, and\nvalues near 1 indicating a non-hyperbolic, flat structure. Following previous works (Khrulkov et al.,\n2020), we employ Euclidean distance as a measure of the shortest distance. To further validate the\ncorrectness of this approach, we generate a series of random graphs with predefined hyperbolicity,\nembed them using a graph neural network (GNN), and then compute the hyperbolicity in Euclidean\nspace. Details of this process are provided in Appendix B. Our experiments reveal a positive\ncorrelation between the hyperbolicity of the embeddings and the original graphs. Consequently, we\nutilize this method as a proxy for estimating the hyperbolicity of token embeddings.\nIn our analysis, we calculate hyperbolicity at the prompt level, treating each token within a prompt\nas a point in the metric space spanned by the embeddings. By averaging the hyperbolicity across\nall prompts, we assess the overall hyperbolic structure of token embeddings in each dataset. Our\nresults, as shown in Table 2, reveal that token embeddings exhibit significant hyperbolicity, suggesting\nthat the embedding space has a strong tree-like structure. This observation further corroborates our\nfindings from the global token statistics, where the arrangement of tokens in the embedding space\nmirrors hierarchical relationships seen in language data."}, {"title": "5 Hyperbolic Fine-tuning for LLMs", "content": "The core technique in the LoRA adapter involves linear transformations. One of the primary methods\nfor implementing linear transformations on the Lorentz model of hyperbolic geometry (Ganea et al.,\n2018b; Chami et al., 2019) is based on the tangent space when considering the learnable weights are\nin Euclidean. Given a hyperbolic vector $x^H$ and a transformation matrix W, this method first maps\n$x^H$ to the tangent space at a local reference point, typically the origin, using the logarithmic map.\nThe matrix W is then applied within this tangent space, resulting in:\n$Wx^H = exp(W log(x)).$ (6)\nTechnical Challenge However, the input from LLMs and the transformation results are in Euclidean\nspace, we need to apply an additional exponential map and a logarithmic map on the basis of\nEquation (1) to align the Euclidean representation. This leads to the expression:\n$Z^E = W_{LORA}(X^E) = Wx^E + \\Delta Wx^E$\n$= Wx^E + log(exp(BA log (exp(x))))$\n$= Wx^E + BAx^E$,\nTransformation on $X^E$\nwhich simplifies back to the original LoRA, rendering the method ineffective for our purposes.\nDirect Lorentz Low-rank Transformation (LLR) To address this challenge, we perform low-rank\nadaptation directly on the hyperbolic manifold without utilizing tangent space:\n$z^E = W_{LORA}(x^E) = Wx^E + \\Delta Wx^E$\n= Wx+log (LLR(BA, exp(x))),\nTransformation on $x^H$\nwhere LLR represents the direct Lorentz Low-Rank Transformation which operate the hyperbolic\nrepresentation $x^H$ directly,\n$LLR(BA, x^H) = (\\sqrt{||BAx^H||+ K}, B^H A^H x^H)$, where $y^H = (\\sqrt{|| Ax^H ||+ K}, Ax^H)$.\nWe consider two transformations in our design, with u representing both x and y: (1) u = $u_s$. (2)\n$u = u_t$. The first transformation only modifies the space-like dimension in special relativity, akin\nto a Lorentz rotation. The second transformation affects both time-like and space-like dimensions,\nsimilar to a Lorentz boost. In both cases, it can be verified that $LLR(BA, x^H) \\in L^n$. The linear"}, {"title": "6 Conclusion", "content": "In this study, we investigated the non-Euclidean properties of token embeddings in LLMs. Our\nanalysis token embedding has strong hyperbolic characteristics. Building on these findings, we\nproposed HypLoRA, a hyperbolic low-rank adaptation method that incorporates hyperbolic geometry\ninto the fine-tuning process of LLMs. Extensive experiments showed that HypLoRA significantly\nimproves LLM performance on arithmetic reasoning tasks, particularly on complex datasets. By\nleveraging the hyperbolic structure of the data, HypLoRA enhances the model's ability to capture\nand utilize intricate relationships, leading to better reasoning capabilities.\nLimitation and Future Work In this study, we employed a consistent curvature across all prompts\nduring fine-tuning, which simplified the implementation and enhanced efficiency. However, this\nuniform approach may not be optimal when applied to different datasets simultaneously. Our future\nwork will explore more adaptive fine-tuning techniques that can better accommodate the unique\ncharacteristics of different prompts. Additionally, due to the computational overhead introduced by\nthe exponential and logarithmic maps, this is inevitable when transitioning from the original Euclidean\nspace to a hyperbolic space. We will explore more efficient methods to reduce this computational\ncost in future work.\nDespite these challenges, our research provides a thorough examination of token embedding distri-\nbutions from a non-Euclidean perspective and offers valuable insights. The fine-tuning method we\nproposed holds significant potential for advancing geometrically inspired models, contributing to the\nongoing development of more effective LLMs."}]}