{"title": "LA4SR: illuminating the dark proteome with generative AI", "authors": ["David R. Nelson", "Ashish Kumar Jaiswal", "Noha Ismail", "Alexandra Mystikou", "Kourosh Salehi-Ashtiani"], "abstract": "Al language models (LMs) show promise for biological sequence analysis. We re-engineered open-source LMs (e.g.,\nGPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba, ranging from 70 million (m) to 12 billion (B)\nparameters) for microbial sequence classification. The models achieved F1 scores up to 95 and operated 16,580x\nfaster and at 2.9x the recall of BLASTP. They effectively classified the algal \u201cdark proteome\", (e.g.,\nuncharacterized proteins comprising ~65% of total proteins), validated on new data including a new, complete Hi-\nC/Pacbio Chlamydomonas genome. Larger (> 1B) LA\u02bbSR models reached high accuracy (F1 > 86) when trained on\nless than 2% of available data, rapidly achieving strong generalization capacity. High accuracy was achieved when\ntraining data had intact or scrambled terminal information, demonstrating robust generalization to incomplete\nsequences. Finally, we provide custom AI explainability software tools for attributing amino acid patterns to AI\ngenerative processes and interpret their outputs in evolutionary and biophysical contexts.", "sections": [{"title": "Introduction", "content": "The application of deep learning to amino acid sequences has the potential to distill fundamental protein features into\nsemantically rich representations encompassing structural, evolutionary, and biophysical properties\u00b9. Leveraging recent\nmassive expansions in protein sequence databases, large-scale unsupervised learning of protein language models can\ncapture multiscale biological information, enabling state-of-the-art performance in various downstream tasks\u00b2. In this\nstudy, we present the LA\u02bbSR (/'le\u0131.z\u0259r/, language modeling with artificial intelligence for algal amino acid sequence\nrepresentation) framework, based in Pytorch\u00b3, designed to implement the latest open-source language models (LMs) and\nlarge language models (LMs with > one billion (1B) parameters, i.e., LLMs) to process microbial genomic data and\nextract otherwise intractable information.\nMicroalgal genomes are excellent subjects for this approach4-6. The unique niche ecologies of the different algal\nlineages underscore the diversity of the underlying genetic contents in these microbes, and several phyla display complex\nevolutionary histories involving lineage-merging endosymbiotic events'. Algae genomes are thus notorious for their\nchimerism7-9, with many sequences appearing more diverse within a single genome than in other eukaryotes. Further\ncomplicating classification is the prevalence of horizontal gene transfer (HGT) in algal genomes8,10-15. This evolutionary\nhistory blurs the lines between algal and bacterial genetic material, often preventing accurate sequence classification.\nTraditional bioinformatics tools like BLAST16 and Kraken\u00b9\u00b9, which rely on sequence homology and kmer frequencies,\noften fall short when analyzing novel or divergent sequences. These programs frequently misclassify substantial portions\nof genuine microalgal genomes as unknown or bacterial. The caveat of incomplete assignment of alignment-based\ndatabase hits at lower sensitivities also severely hampers the accurate estimation of contamination for genomic sequencing\nprojects. Deep neural networks (DNNs) offer a paradigm shift in sequence data analysis\u00b9, capable of learning complex,\nhierarchical patterns that enable more nuanced and context-aware classification.\nOur novel DNN-based language models show substantial improvements in accuracy, recall, and computational\nefficiency over traditional bioinformatics methods, such as BLAST, facilitating the scalable, probabilistic phylogenetic\nmapping of previously unclassifiable sequences. By implicitly incorporating knowledge of algal genome chimerism and\nhorizontal gene transfer events in the training data, we developed a robust classification system for microbial genomic\ndata. The ability to differentiate between algal and bacterial sequences with high accuracy demonstrates the potential of\ntransfer learning in bioinformatics, bridging the gap between general language understanding and specific biological\nsequence analysis\u00b9\u2079. The new models presented here effectively classify genes comprising the \u201cdark proteome\u201d20,21, which\nincludes translated gene sets with no hits otherwise returned in alignment-based approaches. In our datasets, the \u201cdark\nproteome\" consisted of 100% [all proteins coded for by a single genome]minus 34.7% [the average percentage of proteins\nper genome returning BLAST hits from the National Center for Biotechnology Information (NCBI) non-redundant (nr)"}, {"title": "Results", "content": "Dataset Generation, Model Architecture, and Training Strategies. We constructed microbial genomics sequence\ndatasets from n = 161 microalgal genomes and known contaminant sequences extracted from nr. Two main methodologies\nwere used for downstream training and evaluation: full-length protein sequences (TI-inclusive) and scrambled start/stop\nsites (TI-free), allowing us to investigate the role of terminal information (TI) in prediction models (Figs. 1 and S1). These\ntwo divergent approaches embodied \u201ccore\u201d and \u201cintegrated\u201d approaches to elucidate position-independent position-\ninclusive amino acid patterns inherent to each group. Genomic information was distilled to translated ORFeomes (Data\nS1). Algal and contaminant sequences were combined for training and evaluation at a 1:1 ratio, with 58,650,525 and\n17,880,279 sequences in the TI-free and TI-inclusive training datasets. Overall, ten microalgal phyla were represented in\nthe training data: Chlorophyta, Haptophyta, Cercozoa, Ochrophyta, Dinophyta, Euglenophyta, Heterokonta, Streptophyta,\nand Chromerida. Combined with the contaminant sequences (i.e., bacterial, archaeal, and fungal sequences), the training\ndata comprised ~77 million distinct sequences.\nOur training approach used extracted configurations of foundation models for pre-training (architecture mimicry)\nand post-training (i.e., fine-tuning) using parameter-efficient fine-tuning (PEFT)22. For pre-training, we implemented\nlanguage modeling approaches using the Hugging Face Transformers\u00b2\u00b3 library (see Data S2). Our implementation\nleveraged the HuggingFace Transformers library's AutoModel24 functionality to streamline model initialization and\nconfiguration. For example, LA\u2074SR GPT-NeoX25- and Pythia26-based models used the GPTNeoXConfig framework, with\nkey hyperparameters tuned for optimal performance. The AutoModel classes provide an automated mechanism for\ninstantiating pre-trained models, handling the complexity of model-specific architectures while maintaining a consistent\ninterface across different model types. Pre-training parameters, including dropout rates and attention priming, were\nmaintained through the configuration object. For example, the LA\u2074SR Pythia 70m architecture was configured with a\nmodel dimension of 512, with six layers and eight attention heads, effectively mimicking the carefully researched\narchitecture from this open-source model. All other architectures for our pre-trained models were identical to the base\nmodel with uninitialized weights. We post-trained existing models with Low-Rank Adaptation (LORA)27. Models with\nmore than 300 million parameters (300m) were post-trained with Quantized LORA (QLORA) 28. Post-training always\nstarted from the pre-trained model weights. The models used in analyses included the S6 Mamba29,30 and transformer-\nbased Mistral 7B31, GPT-2 variants (nanoGPT32 and GPT-NeoX25), ByT533, DistilRoberta34-36, MiniLM\u00b3\u2077, BLOOM-\n560m\u00b3, and Pythia (7B and 70m)26 architectures and pre-trained models. The base architectures and the LA\u2074SR models\nare available at https://huggingface.co/.\nModel Performance and Technical Efficiency. We evaluated our models using custom datasets, comparing sequences\nwith and without terminal information (TI-inclusive and TI-free, respectively; Data S1) for algal sequence classification\n(Figs. 1 and S1). Performance benchmarks, shown in Figure 1, demonstrate the models' capabilities when processing both\ntypes of sequences. The model assessment involved rigorous testing on held-out data while tracking computational\nresource utilization (Figs. 1, S2, and S3). Notably, even in the early stages of training, many post-trained models\ndemonstrated a remarkable baseline understanding of \u201calgal\u201d and \u201cbacterial\u201d concepts in natural language contexts, likely\ndue to their pre-training on scientific literature. This was particularly evident with the Mamba29,30 370m model, which was\npre-trained solely on The Pile (EleutherAI, https://www.eleuther.ai) an 800GB diverse text dataset. Remarkably, after\njust 50 steps of LA\u02bbSR post-training, the model could generate responses to bacterial queries by drawing upon scientific\nliterature within The Pile, including substantial content from PubMed (https://pubmed.ncbi.nlm.nih.gov) and GitHub\n(https://github.com) repositories. This early emergence of domain knowledge suggests efficient transfer learning from the\npre-training phase.\nAccuracy was validated with new, real-world sequencing data (Fig. S3). The LA\u02bbSR framework demonstrated\nhigh technical versatility and efficiency across various architectures, ranging in size from 70m to 12B (Figs. 1 and S2).\nMost transformer-based models (nanoGPT, GPT-NeoX, Mistral, and Pythia) and the S6-based Mamba29,30 exhibited robust\nperformance with 20,000 \u2013 50,000 training steps using batch sizes of 16 - 96 (i.e., after seeing 320,000 \u2013 4.8m sequences;\nFig. 1). Batch sizes up to 512 were tested with success, but training runs with batch sizes larger than 96 usually showed\nworse accuracy. This result corroborates evidence illustrated by Shirish Keskar et al. (2016)3\u00ba and Shallue et al. (2018)40\nthat larger batch sizes can lead to poorer generalization and convergence to sharp minima in the loss landscape, suggesting\nan optimal batch size window exists below which training may be inefficient and above which accuracy tends to degrade."}, {"title": "Results cont.", "content": "Our training experiments showed that batch sizes of 64-96 were ideal for pre-training models smaller than 300m and post-\ntraining larger models on an NVIDIA\u00b9\u00b9 (Santa Clara, CA, USA) A100 GPU, achieving high precision with the lowest\nruntimes. Notably, larger models (>300m) achieved high accuracy with minimal training data, sometimes using less than\n2% of the dataset. A LA\u02bbSR Mistral 7B model post-trained (QLORA)28 with the TI-free dataset achieved F1 scores of ~88\non algal/bacterial classification after only 2,000 training steps (batch size = 96), while pre-training from scratch required\nroughly 20,000 steps to generate models with F1 scores >80 (Fig. 1). Our pre-training yielded a variety of models that\ncould distinguish between algal and bacterial proteins. The best models had the highest average F1 scores for algal and\nbacterial input. The TI-free approach had higher inference speeds compared to full-length sequence processing.\nIn addition to several transformer architectures, LA\u02bbSR models were developed with non-transformer S6 LMs.\nSeveral recent reports have touted the strong performance and scalability of the non-quadratic-scaling S6 model Mamba29\nfor various language tasks. We investigated whether Mamba models could be post-trained for our applications and how\nscaling could be surveyed and optimized. The resulting Mamba adaptors, comprised of 130m, 370m, 780m, 1.4B, and\n2.8B parameter pre-trained models, performed well on classification tasks, achieving F1 scores of up to 88 when\nevaluating known microalgal and bacterial sequences not used in training. The low/mid-size 370m model showed optimal\nperformance, with large models achieving comparable accuracy but much longer inference times. The mid-size Mamba S6\nmodels were notable for training and inference robustness and demonstrated the strong performance of S6 models as\ntransformer alternatives for AI-based microbial genomics applications. The 370m (Huggingface.co model,\n\u201cChlorophyllChampion/Mamba-370m-88F1-45000\u201d) and 2.8B (Huggingface.co model \u201cChlorophyllChampion/LA4SR-\nMamba-2.8b-QLORA-ft\u201d) LA\u2074SR Mamba models are available for download at huggingface.co.\nCompared to BLASTP, we found that LA SR models can perform at equivalent or better accuracy with much\nbetter recall rates and speed. We compared the LA\u2074SR against Diamond BLASTP in terms of accuracy and recall (Fig. 1)\nas well as technical benchmarks (Fig. S2). We found that LA\u2074SR models perform better than Diamond BLASTP42,\noperating an average of 82.90x faster (P = 4.7 \u00d7 10-% on n = 166 genomes) and with ~3x the recall rate (~100% in LA\u2074SR\ncompared to ~35% in ultra-sensitive Diamond BLASTP (Table S1)). Thus, the transformer and S6-based modeling\nfacilitated higher recall than alignment-based methods. Although recently undergoing several large-scale sequencing\nprojects, microalgae are still an understudied group with poor representation in sequence databases. The lack of adequate\nrepresentation hinders database alignment attempts, especially for new sequences. We show the strong capacity of LASR\nmodels to generalize to \u201cnever-seen-before\u201d sequences, demonstrating nearly perfect recall on holdout datasets (Fig. 1a),\nwhole genomes (Fig 1b), and new real-world sequencing data from axenic and xenic lab cultures sequenced with short\nreads or long reads and chromatin capture methods (Fig. S3, assemblies are in Data S4 and also hosted at NCBI\n(SAMN44618602)).\nThe best-performing TI-free model, based on EleutherAI's Pythia26 70m architecture (LA\u2074SR -EP70m), achieved\nbalanced and high-performance metrics (precision = 0.9031, recall = 0.8983, and F1 score = 0.9006). Similar results were\nobserved for bacterial sequences, with a precision of 0.8981, a recall of 0.9029, and an F1 score of 0.9003. The model's\nhigh accuracy without relying on terminal information (TI) highlights its effectiveness in classifying protein sequences.\nThe strong performance of this small model also provides insights into the fundamental biological differences between\nthese organisms. The compact size of this high-performing model suggests that the primary genetic distinctions between\nalgae and bacteria can be captured by a limited number of parameters. This indicates a potentially low-rank problem\nwhere the essential differences can be represented in a lower-dimensional space. Such a low-rank structure indicates\ninterpretable features that could be translated into human-understandable concepts."}, {"title": "Model Interpretability and Feature Analysis", "content": "What sequence features and molecular patterns drive our high-performing\nmodels' decision-making process? To answer this fundamental question, we ran multifaceted model interpretability44\nanalyses, probing deep into the neural networks' learning mechanisms. Our investigation leveraged several leading\ninterpretation techniques, implementing custom explainer programs (Data S3) and illuminating how models process and\nevaluate sequence information. Through the strategic application of advanced interpretability frameworks\u2014including\nTuned Lens45, Captum46, DeepLift47, and SHAP-based approaches\u2014we systematically decoded how specific amino\nacid residues, their patterns, and positional relationships influence model decisions. This analytical approach led to the\ndevelopment of sophisticated model-agnostic gradient explainer tools (see Data S3 and Fig. S4), enabling us to extract and\nvisualize crucial gradient information from multiple perspectives, such as per-residue, per-position, and per-motif\nattribution scores."}, {"title": "Model Interpretability and Feature Analysis cont.", "content": "We saw that glutamine and glycine dominated token usage frequency (Fig. 3). The disproportionate emphasis on\nglycine and glutamine in distinguishing between eukaryotic algae and bacteria highlights the importance of amino acid\nusage patterns in reflecting evolutionary adaptations to different ecological niches, particularly in relation to nitrogen\nmetabolism. This may be due to the roles of these amino acids in signaling the assimilation and accumulation of nitrogen,\nrepresented by glutamine, and its restricted cellular availability, represented by glycine50-52. The availability of nitrogen\nlimits growth in both groups, although through substantially different metabolic pathways. Thus, these metabolic\ndifferences may have been implicitly learned in this LA SR instance as key features discriminating the algal and bacterial\nclasses. Valine showed higher usage in model decisions when algal sequences were sampled, and tyrosine was more\nfrequently used for bacterial queries (Fig. 3). These two amino acids showed an inverse relationship in the way these two\namino acids are used when discriminating algal and bacterial sequences.\nThe newly developed explainer programs presented here (HELIX, DeepLift LA\u02bbSR, and Deep Motif Miner Pro\n[DMMP]) implement distinct approaches to transformer hidden state interpretation. HELIX performs layer-wise principal\ncomponent analysis (PCA) transformations on hidden states (e.g., Pythia 70m's dimension d = 768) to analyze embedding\nspace topology. Each hidden state matrix H \u2208 R^(n\u00d7d) (where n = sequence length) undergoes orthogonal transformation\nto identify principal axes of variation in the embedding space, enabling tracking of residue relationships across network\ndepths. DeepLift LA\u2074SR quantifies feature importance through attribution relative to a zero-reference state. The method\nprocesses sequences in windows (w = 32, stride = 16) computing attribution scores A = \u2211(DL(x, xo)), where DL"}, {"title": "Model Interpretability and Feature Analysis cont..", "content": "represents the DeepLift operation comparing input x against baseline xo. The implementation wraps the embedding layer\n\u0395: \u03a3* \u2192 Rd for direct gradient access, with attribution scores summed across embedding dimensions: score(i) = \u2211k A(i,k)\nfor position i and embedding dimension k. The DMMP software calculates position-wise influence scores through relative\nhidden state distances. For each amino acid at position i, it computes I(a,i) = ||\u00b5a - \u03bc(~a)||2, where \u00b5a is the mean hidden\nstate for residue a and \u00b5(~a) is the mean state for all other residues. The method identifies motifs M = {(s,p,v)} where s is a\nsubsequence starting at position p with influence value v exceeding the 95th percentile threshold 0 = P95({I(a,i)}). DMMP\naggregates layer information through concatenation H' = [H1;...;H\u2081] followed by dimensionality reduction using PCA, t-\nSNE, or UMAP matrices. The methods' core differences lie in their attribution calculations: HELIX uses purely geometric\ntransformations, LA\u2074SR employs backpropagation-based attribution against reference states, and DMMP utilizes distance\nmetrics in hidden state space combined with statistical thresholding. LA\u2074SR processes sequences through sliding windows\nmaintaining w=16 position overlap, while HELIX and DMMP operate on full sequences with batch-wise memory\nmanagement.\nA custom Captum analysis pipeline (Data S3) evaluated feature contributions in LA\u2074SR's amino acid sequence\nclassification tasks. This Captum-based layer-integrated gradients analysis includes various model-agnostic visualization\ntools for exploring amino acid patterns as features in language models. We computed attributions for each input token,\nquantifying their impact on the model's predictions. For each input sequence, normalized attribution scores and model\nprediction probabilities were calculated, providing insights into which parts of the input most strongly influenced the\nmodel's decision-making process. The end-of-sequence token and the two to five preceding tokens, representing terminal\ninformation (TI), had higher influence scores than sequences from other region trends (Fig. 4b,c). This trend was observed\nin all transformer models tested except for the bidirectional DistilRoBERTa34-36 post-trained model. Still, tokens at the\nends of sequences had higher influence scores in the DistilRoBERTa LA\u2074SR model."}, {"title": "Influential Motif Identification", "content": "We developed the DMMP software (see Data S3) to identify influential motifs in the\nmodel and quantify their impacts on model decisions. This method analyzes hidden states from the model and calculates\nposition- and residue-specific influence scores across all layers to identify motifs that strongly influence the model's\ndecisions. We used a multi-step process to identify influential amino acids within protein sequences, leveraging the hidden\nstate representations from the GPTNeoX53-based LA\u2074SR model. Our approach involved extracting hidden states for each\ninput sequence, computing influence scores by calculating the Euclidean distance between the mean representation of\neach amino acid and the mean of all others, normalizing these scores, and then using peak detection to identify local\nmaxima. For each layer's hidden states, we computed a position-wise influence score using the L2 norm of the difference\nbetween each position's representation and the mean representation of that layer: influence = ||h_i - \u00b5||_2, where h_i is the\nhidden state at position i and u is the mean hidden state for the layer. This operation is vectorized across all positions:\nposition_influence = np.array([np.linalg.norm(state - np.mean(state, axis=0), axis=1) for a state in hidden_states]). The\nresulting scores were min-max normalized to a [0, 1] range, then across all layers to obtain a single influence score per\nposition. To identify peaks in this influence profile, we applied the find_peaks function from scipy.signal with a height\nthreshold set at the 95th percentile of all scores and a minimum peak distance of positions. Around each identified peak,\nwe extracted a window of amino acids to form potential motifs. Patterns were extracted that exert a disproportionate\ninfluence on the model's decision-making process, revealing class-defining signatures. These peaks are considered the\ncenters of influential motifs, extracted as subsequences of three to six residues centered on each peak. The motifs are then\nranked by their average influence score, providing a prioritized list of potentially functionally or structurally important\namino acid subsequences within the protein."}, {"title": "Influential Motif Identification cont.", "content": "In approximately 15% of algal sequences classified as bacterial-like, we identified specific motifs (e.g.,\n\"GXGKT\" and \"DXXG\") associated with ATP/GTP-binding sites (Fig. 4, Table S4). These motifs showed higher-than-\naverage influence scores, suggesting their strong impact on classification decisions and potentially indicating horizontal\ngene transfer events. To visualize and interpret these results, we used several techniques, including amino acid\nrepresentation plots using PCA, influence score plots across layers, and additional dimensionality reduction techniques\nsuch as t-SNE and UMAP. These visualizations provide different perspectives on how the model clusters amino acids and\nits understanding of them evolves through the network."}, {"title": "Discussion", "content": "Our study's application of next-generation language models (LMs) to biological sequence analysis represents a significant\nadvancement in computational biology. This approach highlights the potential of transfer learning in bioinformatics,\nbridging the gap between general language understanding and specific biological sequence analysis. The investigation into\nterminal information (TI) in LA\u2074SR models' decision-making algorithms revealed intriguing parallels with human\nlanguage processing. The observed reliance on TI, even when scrambled, mirrors cognitive science findings about the\nimportance of word beginnings and endings in human language processing. This reveals potential similarities between\nartificial and biological neural networks in sequence interpretation, opening new avenues for interdisciplinary research at\nthe intersection of computational biology and cognitive science. However, the heavy reliance on TI also exposes a\npotential limitation in current transformer-based models. The bias toward terminal sequences might lead to overlooking\nimportant internal patterns, which is particularly problematic for incomplete or poorly annotated genomes. Our\ndevelopment of TI-free models addresses this issue, demonstrating comparable performance without relying on terminal\ninformation. This approach not only improves the robustness of LA\u2074SR models but also challenges the field to reconsider\nthe design of attention mechanisms in biological sequence analysis.\nThe transfer learning paradigm presented in this work capitalizes on the vast amount of knowledge embedded in\nthe model design and the pre-trained weights, significantly reducing the required task-specific training data and enhancing\nperformance on downstream tasks such as phylogenetic classification. The models' pre-existing understanding of complex\nlanguage structures translates remarkably well to the \u201clanguage\u201d of protein sequences, enabling the capture of subtle\npatterns and relationships that might be overlooked by traditional methods, especially when dealing with highly divergent\nor novel sequences. A major focus of the study was investigating the effect of terminal information (TI) in the decision-\nmaking algorithms of the models we developed. Deep neural networks, as well as human brain neural networks, weigh the\nends of words more than the intermediate letters when deciphering meaning from text62-64. This phenomenon is\nreminiscent of the interactive activation model proposed by McClelland and Rumelhart, which accounts for context\neffects in letter perception65. The importance of terminal information has been observed across languages, as demonstrated\nin studies of Chinese word recognition\". Research on non-contiguous letter combinations further supports the\nsignificance of letter position in word processing62,64. Drawing parallels from speech recognition, where recurrent neural\nnetworks have shown remarkable performance in processing sequential information, we considered how similar principles\nmight apply to protein sequences.\""}, {"title": "Discussion cont.", "content": "The concept of long-term dependencies in sequential data, as explored in the development of long short-term\nmemory (LSTM) networks67, provided a theoretical foundation for our investigation into the importance of terminal\ninformation in protein sequences. Additionally, spatial coding models of visual word identification offered insights into\nhow position-specific information is processed in LA\u2074SR neural network models. We hypothesized that the models would\nalso place disproportional importance on sequences near the end of proteins. This would be problematic for genomes with\nincomplete annotation because these end regions would not be properly resolved. The TI-free models were designed to\ncircumvent any detrimental effects from TI by scrambling TI so that the model would not receive this information during"}, {"title": "Discussion cont..", "content": "training. The results provide key insights into how these open-source LMs work with amino acid sequence queries and\nhow different patterns and not sequence lengths characterize these microbes' genomes. Developing models that can\neffectively balance the interpretation of both terminal and internal sequence features is crucial for many use cases.\nThe success of our TI-free approach suggests that robust, functionally relevant patterns can persist even when\nperipheral data are altered or removed, which could be particularly valuable in analyzing fragmentary or poorly assembled\ngenomic data. This finding could lead to new strategies for dealing with incomplete or noisy biological data, a common\nchallenge in genomics and proteomics. The multi-faceted interpretability framework presented here, combining gradient-\nbased, perturbation-based, and layer-wise analyses, enhances our ability to interpret and validate predictions. As AI\nmodels become increasingly complex, such interpretability methods will be crucial for building trust and extracting\nmeaningful biological insights. The increasing complexity in deep learning models often results in \"black box\" systems\nthat, while powerful, lack transparency and interpretability 68. This opacity can hinder trust, adoption, and the extraction of\nmeaningful biological insights. The LA\u02bbSR models leverage state-of-the-art language modeling techniques to analyze\nalgal amino acid sequences, focusing on interpretability and causal inference. This approach not only predicts and\nclassifies but also elucidates the underlying mechanisms of sequence pattern learning\u2014a crucial feature in bioinformatics,\nwhere understanding predictions is as vital as making them.\nAlthough we incorporated more than 77 million distinct sequences in the training data, the biological sequence\ndata available for several representative groups remains sparse. Despite efforts to include diverse algal and bacterial\nsequences, an underrepresentation of less sampled algal lineages, including Chromeridia, Dinoflagellates, Rhodophytes,\nOchrophytes, and, in general, non-Chlorophyte or Bacillariophyta genomes in public databases, limits the scope of the\nwork. These challenges present an opportunity for innovation in database and training data generation and the\ndevelopment of new model architectures and training methodologies, potentially leading to more sophisticated and\nbiologically relevant machine learning models. In conclusion, our work with LASR not only advances bioinformatics\nthrough improved sequence analysis but also opens new research directions at the intersection of machine learning,\ncognitive science, and biology. The insights gained from comparing TI and TI-free approaches could inform future model\ndesigns, potentially leading to more robust and versatile tools for analyzing the complexities of biological systems."}, {"title": "Discussion Conclusions", "content": "This work contributes to improving the accuracy of algal genome assembly and annotation, enhancing our\nunderstanding of microbial evolution and biodiversity, and supporting biotechnological applications that rely on pure algal\ngenetic material. As computational biology continues to evolve, the integration of language models with existing methods\nand experimental approaches, as exemplified in this study, will be critical in advancing the understanding of the molecular\nlanguages of life on Earth, particularly in complex and diverse microbial communities such as those found in algal\necosystems. Combining powerful deep learning models with robust explainability techniques paves the way for more\ntransparent, reliable, and biologically meaningful microbial genomics analysis."}, {"title": "Methods", "content": "Open-source model architectures. The LASR framework integrated several open-source software packages and models.\nWe used low-rank adaptation (LORA)27 and quantized low-rank adaptation (QLORA) for parameter-efficient post-training\nand Mamba as an alternative to transformer-based architectures. The Hugging Face (https://huggingface.co)\ntransformers library facilitated implementation and post-training of the open-source models, while Bitsandbytes enabled\nquantization and memory-efficient training. For model analysis and interpretability, we developed custom gradient\nextraction programs based on SHAP54,70 (SHapley Additive exPlanations, https://github.com/shap/shap), Captum, and\nDeepLift. We post-trained large language models including Mistralai/Mistral-7B-v0.1\n(https://huggingface.co/mistralai/Mistral-7B-v0.1), as well as various Mamba pre-trained models from the Hugging Face\nmodel hub. We used the S6 Mamba29,30 and transformer-based Mistral\u00b3\u00b9, GPT variants (nanoGPT32, GPT-NeoX53),\nByT533, DistilRoBERTa34-36, MiniLM37, BLOOM-560m\u00b3, and Pythia (7B and 70m)26 for pre-training and post-training.\nEach model had an idiosyncratic methodology, but uniformity in coding was achieved using AutoModel24. This\nsystem automatically handled model-specific configurations through the from the_pretrained() method, which loads pre-\ntrained weights from either local storage or HuggingFace's model hub. The initialization process involved three main\ncomponents accessed through their respective Auto classes (AutoConfig, AutoModel, and AutoTokenizer), maintaining\nconsistent configurations, model weights, and tokenizers while preventing component incompatibilities. The AutoModel\napproach facilitated the architecture-agnostic implementation of LA\u2074SR, where model-specific details are abstracted away\nthrough the AutoConfig class. This class automatically selected appropriate configurations based on the model type,\nhandling architecture-specific parameters such as model dimensions, the number of layers, and attention heads according\nto each pre-trained model's specifications.\nThe DistilROBERTa34-36 architecture was the only bidirectional model as well as the only model using sentence\ntransformers (https://sbert.net/). PyTorch was used as the primary deep learning framework used for implementing custom\ndatasets, data loaders, and training pipelines. Additionally, we developed custom Python scripts for data preprocessing,\nmodel training, and analysis, including a novel motif-based explainer pipeline for model interpretability. Training was\nperformed on an HPC cluster using NVIDIA\u2074\u00b9 (Santa Clara, CA, USA) A100, V100, and H100 GPUs."}, {"title": "Methods Cont.", "content": "Implementing parameter-efficient post-training techniques", "https": "www.ncbi.nlm.nih.gov)\nprotein database. Algal and contaminant (either mixed with fungi and archaea or purely bacterial) sequences were\ncombined to form the training/evaluation split training datasets at a 1:1 ratio", "S1,\nhttps": "zenodo.org/records/13920001). Translated ORFeomes (tORFeomes) from approximately 100 microalgae species\nwere first screened for contaminants using traditional BLAST-based detection, and tORFeomes passing a 10% filter were\nmaintained. This threshold was chosen based on results we obtained from two independently sequenced7,43, bona fide\nChloroarachniophyte genomes. The greatest quantity of bacteria-like sequences that could be predicted from known clean\nalgal genomes from the BLAST\u00b96/BLEACH43 approach was from the Chloroarachniophyte species (Bigelowiella\nnatans)7,43, at about 10% (Table S2)."}]}