{"title": "MOT_FCG++: Enhanced Representation of Motion and Appearance Features", "authors": ["Yanzhao Fang"], "abstract": "The goal of multi-object tracking (MOT) is to detect and track all objects in a scene across frames, while maintaining a unique identity for each object. Most existing methods rely on the spatial motion features and appearance embedding features of the detected objects in consecutive frames. Effectively and robustly representing the spatial and appearance features of long trajectories has become a critical factor affecting the performance of MOT. We propose a novel approach for appearance and spatial feature representation, improving upon the clustering association method MOT_FCG. For spatial motion features, we propose Diagonal Modulated GIoU, which more accurately represents the relationship between the position and shape of the objects. For appearance features, we utilize a dynamic appearance representation that incorporates confidence information, enabling the trajectory appearance features to be more robust and global. Based on the baseline model MOT_FCG, we achieved 76.1 HO\u03a4\u0391, 80.4 \u039c\u039fTA and 81.3 IDF1 on the MOT17 validation set, and also achieved competitive performance on the MOT20 and DanceTrack validation sets.", "sections": [{"title": "Introduction", "content": "Multi-object tracking (MOT) aims to identify dynamic objects in each frame of a given video sequence and assign the same identity identifier to the same object across consecutive frames, thereby forming individual motion trajectories for different objects. The goal of MOT is to simultaneously track and recognize multiple targets in a scene, demonstrating enormous potential in fields such as video surveillance, intelligent transportation, autonomous driving, and sports broadcasting.\nDetection-based tracking methods (DBT) are mainstream methods for MOT, typically divided into two stages: detection and tracking. In the first stage, a convolutional neural network-based detector is used to label the objects of interest in each frame of the video sequence with bounding boxes. In the second stage, the tracker utilizes the output from the detector to extract the appearance and spatial motion features of the objects again, and calculates the similarity between these detection features and the existing trajectory features to perform trajectory association.\nHow to effectively represent the appearance and spatial motion features of trajectories is a key factor affecting the performance of DBT methods. In this paper, we improve upon the clustering association method MOT-FCG by proposing a more global appearance embedding representation method and a more precise spatial motion information representation method. Ablation experiments validate the effectiveness of our proposed modules, and as shown in Figure 1 we achieve excellent performance on the MOT17 datasets.\nThe main contributions of this paper are threefold:\n\u2022 We point out that the use of median elements as trajectory appearance embedding features in MOT_FCG has limitations. To address this, we adopt a dynamic appearance embedding representation method. This method can adaptively adjust the weighting based on confidence. It integrates the global appearance embedding features of the trajectory, leading to a more comprehensive and holistic representation."}, {"title": "Related Work", "content": "The main aspect of MOT is the data association between current frame detections and tracking tra-jectories. Based on the criteria for this association, MOT can be primarily divided into motion-based MOT and appearance-based MOT. The former is efficient in terms of runtime but is easily affected by occluded objects; the latter achieves better tracking performance but has lower operational efficiency. Recent research on MOT mainly includes the following:\nMotion-Based Multi-Object Tracking. Detection-based trackers often perform frame-by-frame association by linking detected objects with trajectory motion and positional cues. Within adjacent frames, the displacement of objects is typically small and can be approximated as linear motion. Therefore, the Kalman filter [3] can be used to predict the object positions using a constant velocity model. For the representation of motion features, the primary considerations are motion prediction compensation and the relationships between object positions. SORT [2] employs a Kalman filter to predict the next state of the observations provided by the detector and performs MOT based on Intersection over Union (IoU). BoT-SORT [1] considers the influence of camera motion on linear predictions and employs motion compensation to correct the predictions of the Kalman filter. OC-SORT [4] reactivates lost observations and backtracks over the periods of missing observations to update the Kalman filter parameters. Strong-SORT [5] applies offline correction to achieve Gaussian smoothing of the predicted trajectories. Recent advancements have introduced Transformer [6] that follow the"}, {"title": "Method", "content": "Our work is based on the recently proposed clustering tracking algorithm MOT-FCG [24]. Because the same object instance exhibits similar appearance features in close temporal neighborhoods, MOT_FCG utilizes the similarity of objects in adjacent frames to generate clustered tracklets. The algorithm comprises two main stages. In the first stage, initial, short and reliable tracklets, referred to as lifted frames, are formed based on appearance features within a specified time window. In the second stage, the authors use the appearance features of the median elements of these tracklets as representations. They continuously refine the hierarchy of lifted frames through a hierarchical clustering algorithm, which naturally merges the tracklets to create the final target trajectory. The authors implemented UPGMA (Unweighted Pair Group Method with Arithmetic Mean) [37] to iteratively merge paired clusters [26], creating a hierarchical structure. As shown in Figure 2, we propose MOT_FCG++ based on MOT_FCG, introducing three modules: Diagonal Modulated GIoU, Dynamic Appearance, and Model of Average Constant Velocity to represent weak cue information, thereby enhancing the robustness and generalization of the clustering association model."}, {"title": "Clustering Motion Information Representation", "content": "In the association between current frame detections and tracking trajectories, IoU is often calculated to represent the spatial motion information of objects. As shown in Figure 3, IoU is defined as the area of intersection of two detection bounding boxes divided by the area of their union. However, this approach has two main issues: first, IoU does not accurately reflect the degree of overlap between the two boxes. Figure 3 clearly shows that, the overlap on the left is greater than that on the right, even when the IoUs are equal. Second, IoU is scale-invariant and fails to accurately represent the shape relationship between two objects. Besides, methods based on clustering association lack the assistance"}, {"title": "Clustering Appearance Information Representation", "content": "For each tracklet of lifted frames, accurately representing their appearance features is key to clustering association. For the tracklet $T_{k[n,m]}$ formed by clustering, where n and m are the starting and ending frame indices of the tracklet, M\u00d3T_FCG uses the appearance features of the median element, specifically the $\\frac{n+m}{2}$th element of the tracklet $T_{k[n,m]}$, as the representation of clustering appearance features. This approach has two issues: first, it lacks global appearance information for the cluster, making the representation not robust or global; second, as shown in Figure 4, this method does not incorporate target confidence, which may lead to unrepresentative results due to environmental influences such as occlusion."}, {"title": "Modeling of Average Constant Velocity", "content": "MOT_FCG integrates a simple motion estimation by calculating the difference between the position coordinates of the bounding boxes in the tth frame and the (t \u2212 1)th frame to estimate velocity, which is then applied to the bounding box position estimation in the (t + p)th frame. However, this approach does not account for the effects of inter-frame spacing and noise. We modify this by averaging the velocity over the last n frames, using the average velocity as a constant velocity estimate. The advantages of this method are twofold: first, it reduces the impact of observational noise on velocity calculation; second, considering the distance between the starting and ending frames of the trajectory aligns better with the object displacement patterns. Therefore, the average constant velocity v is modeled as:\n$\\begin{aligned}\n\\overline{v} &= \\frac{(x_{t} - x_{t-n})}{n}, \\\\\nx_{t+p} &= x_{t} + \\overline{v} \\times p\n\\end{aligned}$                                                                                                                                                         (4)\nwhere $\\overline{v}$ is the average velocity over the last n frames, $x_{t}$ is the target coordinate position at frame t, and $x_{t+p}$ is the predicted target coordinate position at frame t + p."}, {"title": "Experiments", "content": "Datasets. We conduct experiments on multiple datasets to ensure the universality and robustness of the proposed method. The datasets include the currently popular pedestrian tracking datasets MOT17 [27], MOT20 [28], and DanceTrack [29]. As shown in Figure 5, MOT17 is a widely used standard benchmark in MOT, where the motion is predominantly linear. In contrast, MOT20 features higher density and longer sequences, designed to evaluate tracking performance in scenarios with dense objects and severe occlusions. DanceTrack is one of the most challenging benchmarks in the field of MOT, characterized by diverse nonlinear motion patterns, frequent interactions and occlusions, as well as challenges posed by dancers wearing similar outfits."}, {"title": "Experimental Settings", "content": "Metrics. We use the CLEAR MOT [30] metrics, including Multiple Object Tracking Accuracy (MOTA), Recall, False Positives (FP), False Negatives (FN), and ID Switches (IDSW). MOTA is calculated based on FP, FN, and IDSW, with a greater emphasis on detection performance. We also utilize the IDF1 metric, which evaluates identity association performance and reflects the accuracy of the tracker's associations [31]. Additionally, we consider the HOTA [32] metric, which clearly integrates precise detection, association, and localization effects into a single measurement to assess the overall performance of the algorithm.\nImplementation details. Due to the limited number of submission attempts on the MOT Chal-lenge test, we evaluate on the MOT17 and MOT20 datasets under the \"private detection\" protocol, using the first half of each video in the training set of MOT17 for training and the second half as the validation set [33]. For the detection portion of the objects, we fine-tune the YOLOX detector [34] used in MOT_FCG, and we extract ReID features using the SBS network [36] trained on Market1501 [35]. The sliding window length is fixed at 6, and the detector threshold is set at 0.7. For different datasets, we fine-tune $\\beta_{f}$ and $o_{f f}$: $\\beta_{f} = 0.822, o_{f f} = 0.525$ for MOT17; $\\beta_{f} = 0.66, o_{f f} = 0.9$ for MOT20; and $\\beta_{f} = 0.8, o_{f f} = 0.1$ for DanceTrack."}, {"title": "Benchmark Results", "content": "In this section, we present the benchmark results of the algorithm on the MOT17 and MOT20 sets. Notably, the modifications we propose generally outperform MOT-FCG in both datasets. In the table, bold black indicates first place, bold blue indicates second place, and bold red indicates third place."}, {"title": "Compare with MOT_FCG", "content": "We present the performance of MOT_FCG++ on the MOT17-test set and MOT20-test set in Table 1 and compare it with the original algorithm. Compared to the baseline MOT_FCG, MOT_FCG++ shows improvements across all MOT metrics. This shows that compared to the original algorithm, our improvements in the representation based on motion and appearance features allow for a more precise depiction of spatial relationships among targets. They also provide greater robustness and globality in representing appearance features for long trajectories. Consequently, there are significant and consistent performance improvements across related metrics."}, {"title": "Compare with other algorithm", "content": "MOT17. In comparison with current SOTA algorithms, MOT_FCG++ achieves a MOTA of 80.4, ranking first in the list. In terms of identity association evaluation, MOT_FCG++ achieves the optimal IDF1 of 81.3. In the HOTA metric, which integrates detection, association and localization effects, MOT_FCG++ also performs well with a score of 76.1. This indicates that our algorithm modifications contribute to enhancing the competitiveness of MOT_FCG, demonstrating that our tracker is robust and effective across different scenarios.\nMOT20. Compared to MO\u041e\u042217, \u041c\u041e\u042220 presents more congestion and occlusion scenarios, posing greater challenges to the performance of trackers. We present the performance of MOT_FCG++ on the"}, {"title": "Ablation Study", "content": "To validate the effectiveness of the proposed modules, we conduct ablation experiments on MOT17-val, MOT20-val, and DanceTrack-val set, using MOT_FCG as the baseline and sequentially adding Dynamic Appearance(DA), Diagonal Modulated GIoU(DGIoU) and Model of Average Constant Velocity(ACV). The results of the ablation experiments are shown in Table 4. We visualize the tracking results, as shown in Figure 4. Figure (a) uses the median element of the original MOT_FCG as the appearance feature for the trajectory, resulting in No.4 switching identity to N0.10 during occlusion. In contrast, Figure (b) employs the dynamic appearance feature representation proposed in this paper, which maintains identity consistently during occlusion. Therefore, DA is capable of providing a global representation of the appearance embedding features for trajectory clustering, which prevents identity switching during occlusions. From Table 7, it can be seen that DA performs better on MOT17 and DanceTrack. However, in the dense and highly occluded scenarios of MOT20, HOTA show limited improvements, possibly due to frequent target occlusions restricting the extraction of global features. DGIOU shows overall improvements across all three datasets, indicating its superior representation of"}, {"title": "Conclusion", "content": "In this paper, we propose an enhanced and effective MOT method - MOT_FCG++, based on the clustering association MOT method MOT_FCG. Compared to MOT_FCG, MOT_FCG++ utilizes a dynamic appearance embedding representation that incorporates more global appearance and confidence information. Besides, the proposed Diagonal Modulated GIoU also demonstrates greater ro-bustness and generalization in spatial motion feature representation. The introduction of Model of Average Constant Velocity also leads to a slight improvement in tracking performance. MOT_FCG++ achieves state-of-the-art performance and competitive results on the MOT17-val, MOT20-val, and DanceTrack-val sets. We hope that these improved methods will inspire future representations of motion and appearance cues in clustering MOT methods."}]}