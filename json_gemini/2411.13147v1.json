{"title": "GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation", "authors": ["Mengzhu Wang", "Jiao Li", "Nan Yin", "Houcheng Su", "Li Shen"], "abstract": "Semi-supervised learning (SSL) has made notable advancements in medical image segmentation (MIS), particularly in scenarios with limited labeled data and significantly enhancing data utilization efficiency. Previous methods primarily focus on complex training strategies to utilize unlabeled data but neglect the importance of graph structural information. Different from existing methods, we propose a graph-based clustering for semi-supervised medical image segmentation (GraphCL) by jointly modeling graph data structure in a unified deep model. The proposed GraphCL model enjoys several advantages. Firstly, to the best of our knowledge, this is the first work to model the data structure information for semi-supervised medical image segmentation (SSMIS). Secondly, to get the clustered features across different graphs, we integrate both pairwise affinities between local image features and raw features as inputs. Extensive experimental results on three standard benchmarks show that the proposed GraphCL algorithm outperforms state-of-the-art semi-supervised medical image segmentation methods.", "sections": [{"title": "1. Introduction", "content": "Medical image segmentation derives from computed tomography (CT) [5] or magnetic resonance imaging (MRI) [9], plays a crucial role in various clinical applications [38, 39]. However, obtaining large medical datasets with precise labels for training segmentation models is challenging, as a substantial amount of labeled images can only be provided by experts. This significantly limits the development of medical image segmentation algorithms and poses substantial challenges for further research due to the scarcity of labeled data. To address these challenges, semi-supervised medical image segmentation (SSMIS) [2, 35, 47] has emerged as an effective approach, enabling segmentation models to learn from a small set of labeled examples in conjunction with easily accessible unlabeled data.\nIn the field of SSMIS, although labeled and unlabeled data are theoretically expected to come from the same distribution, in practice, due to the extremely limited availability of labeled data, it is challenging to accurately infer the true distribution of the data. This often leads to the issue of distribution mismatch between the large amount of unlabeled data and the small set of labeled data [43]. To address this challenge, several SSMIS techniques have been developed. For example, BCP [2] enhances data consistency by randomly cropping regions in labeled images (as foreground) and pasting them onto unlabeled images (as background), and vice versa. TI-ST [8] proposes a semi-supervised learning strategy called transformation-invariant self-training, which improves domain adaptation by evaluating the reliability of pixel-level pseudo-labels and excluding unreliable predictions during the self-training process. ContrastMask [44] implements dense contrastive learning on both labeled and unlabeled data. Recent advancements in the field have shown promising results, especially in traditional graph theory applications such as object detection [36, 56] and tracking [13, 17]. However, all of these methods overlook the role of graph information in enhancing semi-supervised medical image segmentation.\nRecently, GraphNet [28] make an innovative attempt in the supervised semantic segmentation field by applying Graph Convolutional Networks (GCN) [15] to the task. The researchers transformed images into unweighted graph structures by aggregating pixels from superpixel techniques into graph nodes[1]. These graphs were then fed into a standard GCN equipped with a cross-entropy loss function to generate pseudo-labels. A2GNN [54] propose an innovative affinity-based convolutional neural network capable of converting images into weighted graph forms. While these graph neural network-based methods have shown outstanding performance in traditional image tasks, they have received limited attention in the medical image domain. Medical image segmentation presents unique challenges, such as complex biological structures and high sensitivity to pathological changes. Moreover, no research has explored SSMIS specifically from the perspective of data structure.\nTo address this issue, we propose a graph-based clustering for semi-supervised medical image segmentation (GraphCL) by jointly modeling graph data structure in a unified deep model. When modeling data structures in the deep learning network, we create a dense instance graph reflecting the structural similarity of the samples based on CNN features. Each node in the graph corresponds to the CNN features of a sample, which are extracted using a standard convolutional neural network. Then, we deploy a Graph Convolutional Network (GCN) on this instance graph, allowing the structural information to be propagated through learnable weighted edges in the network design. To further improve segmentation accuracy, we introduce a k-less clustering strategy that eliminates the need to specify the number of clusters k, enabling similar nodes to automatically form clusters (see Figure 1). This strategy significantly enhances the flexibility and adaptability of the model. The core contributions of this study are summarized as follows:\n\u2022 We propose a graph-based clustering for semi-supervised medical image segmentation by modeling data structure in a unified network. To the best of our knowledge, this is the first work to model the data structure information in graph for SSMIS.\n\u2022 We design a graph clustering objective as a loss function to optimize the correlation clustering task in SSMIS.\n\u2022 Extensive experiments on popular medical image segmentation benchmarks show that GraphCL achieves superior performance."}, {"title": "2. Related Work", "content": "Semi-supervised medical image segmentation [22, 35, 42, 46, 50] is a technique that integrates labeled and unlabeled data to enhance the accuracy and efficiency of medical image analysis. Specifically, acquiring high-quality labeled data in this field is often time-consuming and expensive, as it necessitates detailed annotations by skilled professionals. In contrast, unlabeled data is abundantly available, and semi-supervised learning techniques can effectively utilize this data to improve the model's learning capabilities."}, {"title": "2.1. Semi-supervised Learning", "content": "Semi-supervised Learning (SSL) is widely used in computer vision. Semi-supervised graph-based clustering has emerged as a pivotal field at the intersection of graph clustering and SSL, offering innovative solutions to intricate data analysis problems. For example, Bair [3] explores the landscape of semi-supervised clustering techniques. Li et al. [19] investigate how to effectively how to effectively leverage both labeled and unlabeled data in complex networks containing diverse types of nodes and rich attribute information. Qin et al. [29] offer valuable insights for future research efforts in the field of semi-supervised clustering. Chong et al. [7] survey several representative graph-based SSL algorithms and empirically evaluates some methods on face and image datasets, which also suggests future directions for graph-based SSL. MAGIC depict the multiscale presentation of disease heterogeneity and builds on a semi-supervised clustering methods. SCDMLGE [20] adapt triplet loss in deep metric learning network and combining bedding with label propagation strategy to dynamically update the unlabeled to labeled data which enhances the robustness of metric learning network and promotes the accuracy of clustering. SSSE [52] tackles scalable datasets with different types of constraints from diverse sources to perform both semi-supervised partitioning and hierarchical clustering, which is fully explainable compared to deep learning-based methods. Divam [11] use an ensemble of deep networks to construct a similarity graph, from which we extract high accuracy pseudo-labels. The approach of finding high quality pseudo-labels using ensembles and training the semi-supervised model is iterated, yielding continued improvement. However, these strategies are not suitable for medical image segmentation especially when domain shift occurs."}, {"title": "2.2. Semi-supervised Medical Image Segmentation", "content": "Several recent semi-supervised methods have been proposed for the medical image segmentation task. For instance, CorrMatch [37] propose to conduct pixel propagation by modeling the pairwise similarities of pixels to spread the high-confidence pixels and dig out more and perform region propagation to enhance the pseudo labels with accurate class-agnostic masks extracted from the correlation maps. Bai et al. [2] propose a straightforward method for alleviating the problem-copy-pasting labeled and unlabeled data bidirectionally, in a simple mean teacher architecture. MC-Net+ [46] minimize the discrepancy of multiple outputs during training and force the model to generate invariant results in such challenging regions, aiming at regularizing the model training. ABD [6] design a bidirectional patch displacement based on reliable prediction confidence for unlabeled data to generate new samples and enforce the model to learn the potentially uncontrollable content. CauSSL [26] propose a novel statistical quantification of the uncomputable algorithmic independence and further enhance the independence via a min-max optimization process."}, {"title": "2.3. Graph Neural Networks (GNN)", "content": "GNN is designed to use deep learning architectures on graph-structured data, which is in fact natural generalizations of convolutional networks to non-Euclidean graphs. The GNN is first proposed in [10, 33] as a trainable recurrent message passing whose fixed points could be adjusted discriminability. DAGNN [21] provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. SHADOWSAGE [53] design a principle to decouple the depth and scope of GNNs. EERM [25] provides crucial insights into GNN performance meeting structural disparity, common in real-world scenarios. Among these GNNs, the graph convolutional network (GCN) has been applied to many applications [16, 24, 34, 49]. The proposed model exploits the GCN to operate on a dense-connected instance graph so that data structure information can be jointly graph clustered information in a unified deep network."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Notations and Definitions", "content": "In medical image segmentation (MIS), a 3D volume is represented as X \u2208 MC\u00d7W\u00d7H\u00d7D, where C, W, H, and D correspond to the channel, width, height, and depth, respectively. The goal of semi-supervised segmentation is to predict a pixel-wise label map \u00dd \u2208 {0,1,..., k\n1}C\u00d7W\u00d7H\u00d7D, indicating the distribution of background and target classes, with k representing the number of classes. The training set S comprises labeled data A and a much larger unlabeled dataset B, such that S = Si U Su, where S\u2081 = {(X, Y)}A1 is the labeled subset, and Su =\n{X}A+B+1 is the unlabeled subset.\nDuring training, we generate mixed samples by selecting two labeled images (Xl, Xl) and two unlabeled images (Xm, Xu). A foreground region is randomly cropped from X and pasted onto X to produce the mixed image Xout, while another crop from Xm is pasted onto X to form Xin. These mixed samples allow the network to learn comprehensive semantic information, leveraging both inward (Xin) and outward (Xout) perspectives.\nOur method is built upon a teacher-student framework [2], where both networks adopt an encoder-decoder architecture. In the encoder, we incorporate a structure-aware graph network to explicitly capture structural relationships between the inward and outward images. This graph network models spatial and semantic correlations, improving the model's ability to understand the inherent structures within medical images. Additionally, the extracted features dynamically optimize the graph structure, refining cluster assignments to better represent shared and distinct characteristics across labeled and unlabeled data. Finally, both Xin and Xout are passed through the student network to predict the segmentation masks Yin and Yout, which are supervised by the teacher network's predictions on the unlabeled images and the ground truth labels from the labeled images. The pipeline of our proposed method is illustrated in Figure 2."}, {"title": "3.2. Bidirectional Copy-Paste Framework", "content": "The Bidirectional Copy-Paste (BCP) framework integrates a teacher network T(Xm, Xu; \u0398t) and a student network S(Xin, Xout; \u0398\u03c2) [40] to enhance semi-supervised medical image segmentation (SSMIS) through a coordinated training strategy, where Ot and Os are parameters. Initially, the student network is pre-trained using only labeled data to build a supervised model and teacher network leverage the pre-trained model to generate pseudo-labels for unlabeled data during self-training. In the pre-training phase, we adapt the following strategy:\n$\\Y = T(X_m, \\Theta_t)$ (1)\n$\\Y = T(X_n, \\Theta_t)$ (2)\nwhere Xm and X are the unlabeled images, and \u0176m, Yu are the corresponding probability maps. The pseudo-labels are initialized using thresholding or argmax operations, depending on whether the task involves binary or multi-class segmentation.\nIn the bidirectional supervision phase, mixed images Xin and Xout are constructed using a mask Me \u2208"}, {"title": "3.3. Structural Graph Model for Segmentation", "content": "In fact, existing studies [12, 30, 57] focus on modeling the data structure information for semantic segmentation and have achieved remarkable success, which further emphasizes the critical role of data structure information. To effectively model data structures in semi-supervised medical image segmentation (SSMIS), we propose a graph-based clustering for semi-supervised medical image segmentation (GraphCL).\nTo address the challenge of effectively integrating both labeled and unlabeled medical images within the semisupervised medical image segmentation (SSMIS) framework, we propose a Structural Graph Model (SGM). The model is based on the idea that the spatial and semantic structure of images can be efficiently abstracted into a graph, where nodes represent the features of image regions, and edges describe the relationships between these regions. This graph structure enables flexible information propagation, which is crucial for enhancing segmentation accuracy, particularly when labeled data is limited.\n3.3.1 Structure-aware Alignment\nIn our graph construction framework, each sample in a mini-batch is treated as a node, and the relationships between nodes are modeled using a Data Structure Analyzer (DSA). This component generates structure scores that quantify the similarity between different samples based on their internal spatial structure, as derived from the learned CNN features. Formally, the feature extraction process for each 3D medical image Xbatch is expressed as:\nX = CNN(Xbatch) (12)\nwhere X represents the graph signal, encoding the features of individual samples. These features are crucial in defining the graph adjacency matrix \u00c2, computed as:\nA = XsaX (13)\nwhere Xsa \u2208 Rw\u00d7h are the structure scores output by the DSA network, with w denoting the batch size and h the dimension of the structure features. The intuition is that samples with similar structural characteristics should have stronger connections in the graph, which facilitates effective feature propagation during the segmentation task.\nOnce the graph is constructed, we employ a Graph Convolutional Network (GCN) to perform feature propagation across nodes. The GCN operates on the instance graph, with the goal of refining the feature representations by aggregating information from neighboring nodes, thereby capturing both local and global structural patterns within the mini-batch. The graph convolution is performed as:\nZ = D\u00af\u00a6\u00c2\u00db\u00afXW (14)\nwhere Z is the output feature matrix, W is the learnable weight matrix, and D is the degree matrix associated with the adjacency matrix \u00c2. This operation ensures that each node in the graph aggregates feature information from its neighbors, weighted by the structural similarities encoded in A. This propagation mechanism allows the network to exploit contextual information across the batch, improving its ability to segment complex anatomical structures in medical images, where the relationships between different regions are critical for accurate segmentation.\n3.3.2 Graph Neural Network Clustering\nTo further cluster the same graph nodes, for each 3D image volume in a mini-batch, we first extract deep features, resulting in a feature tensor F\u2208 R(B\u00d7C\u00d7W\u00d7H\u00d7D), where B is the batch size. Each voxel in the 3D volume serves as a graph node, with the feature dimensions (W, H, D) representing the spatial extent of the nodes. To capture relationships across the volume, we construct a graph where each node represents a voxel and edges connect spatially adjacent nodes or nodes with high feature similarity.\nSpecifically, the matrix W for the graph is derived based on spatial and semantic affinity between patches, calculated as follows:\nW = F. FT -$\\frac{Max(F.FT)}{\\tau}$ (15)\nwhere F is the feature matrix and 7 controls clustering sensitivity. T is used to adapt the cluster selection process in correlation clustering. Since the number of clusters cannot be directly selected in correlation clustering, this parameter allows us to control the sensitivity of the process, where higher values of 7 correspond to more clusters. This graph construction preserves important volumetric features, enabling the GNN to recognize spatial and semantic relationships within the medical data.\nLet \u0143 denote as the node feature matrix, which is derived by applying one or more layers of Graph Neural Network (GNN) convolution on a graph G defined by an adjacency matrix W. Here, we use a single-layer Graph Convolutional Network (GCN) to perform the feature extraction. The adjacency matrix W is constructed from the patch-wise correlation matrix based on features obtained from encoder. These correlations capture the relational structure of the patches within the visual data, thereby enabling the GNN to leverage spatial dependencies. Formally, the GNN layer maps the input node features N into a refined node feature matrix N by learning the underlying data structure:\nN = GNN(N,W;\u0398GNN) (16)\nwhere OGNN represents the trainable parameters of the GNN layer. Following the GNN, we utilize a Multi-Layer Perceptron (MLP) with a softmax activation applied to N to produce the final output S, which is the cluster assignment matrix. Each row of S corresponds to a node and represents the probability distribution over clusters, essentially encoding the likelihood of each node belonging to a particular cluster:\nS = MLP(\u00d1; \u04e8MLP) (17)\nwhere MLP are the MLP's trainable parameters. The optimization of the GNN model is driven by a clustering objective, with a loss function proposed to enforce distinct clustering properties.\nWe employ a correlation clustering loss in this work, which directly promotes intra-cluster coherence and intercluster separation. This loss is defined as:\nLcc = - Tr(WSST) (18)\nwhere W is redefined according to the specific correlation clustering requirements. This loss encourages nodes with high similarity (as per W) to be assigned to the same cluster (positive affinities), while penalizing connections between dissimilar nodes (negative affinities). Consequently, this approach is advantageous for scenarios where clusters have distinct internal structures or where cluster boundaries are less clearly defined.\nAt each training iteration, we update the parameters Os in the student network by stochastic gradient descent with the loss function(based on Eq.(10)):\nLall = Lin + Lout + \u043a * LCC (19)\nWe use a weight \u043a to control the contribution of graph clustering to the loss function. Afterwards, teacher network parameters (k+1) at the (k + 1)-th iteration are updated."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "All experiments are performed on three public datasets with different imaging modalities and segmentation tasks: Automatic Cardiac Diagnosis Challenge dataset (ACDC) [4], Atrial Segmentation Challenge dataset (LA) [48] and Pancreas-NIH dataset [32]. Four metrics are used for evaluation, including the Dice Score (%), Jaccard Score (%), 95% Hausdorf Distance (95HD), and the average surface distance (ASD). Given two object regions, Dice and Jaccard mainly compute the percentage of overlap between them, 95HD measures the closest point distance between them and ASD computes the average distance between their boundaries. We have highlighted the results in bold when our proposed method outperforms the original counterparts."}, {"title": "4.2. Implementation Details", "content": "All experiments use default settings of \u03b1 = 0.5, \u043a = 0.01 and T = 2, with fixed random seeds. LA Dataset experiments run on an NVIDIA A800 GPU, while Pancreas-NIH and ACDC datasets use an NVIDIA 3090 GPU."}, {"title": "4.3. Comparison with State-of-the-Art", "content": "We evaluate our framework on the LA and ACDC datasets, comparing it with several state-of-the-art methods, including UA-MT [51], SASSNet [18], DTC [22], URPC [23], MC-Net [45], and SS-Net [47]. Additionally, for the LA dataset, we include comparisons with V-Net [27], while for the ACDC dataset, we compare with U-Net [31]. Following the protocol in SS-Net, we conduct semi-supervised experiments with different labeled data ratios (i.e., 5% and 10%). For Pancreas-NIH dataset, we evaluate with a labeled ratio of 20% [22, 35]. We benchmark our method, denoted as GraphCL, against various state-of-the-art models, including V-Net [27], DAN [55], ADVENT [41], UA-MT [51], SASSNet [18], DTC [22], and CoraNet [35].\nLA dataset. To ensure a fair comparison, we adopt the identical experimental setup used in SS-Net. As shown in Table 1, our approach achieves superior performance across all four evaluation metrics, completely outperforming competing approaches. Specifically, when the labeled ratio is set to 10%, GraphCL outperforms the second-best approach by an average of 3.85% across all four evaluation metrics. With the labeled ratio of 5%, we maintain a strong advantage, showing an average improvement of 6.64 % over the second-best results across these metrics. This suggests that when the number of labeled volume is particularly limited, the knowledge from labeled data can be more effectively transferred to the unlabeled data. This phenomenon likely explains the superior performance gains observed when the labeled ratio is set to 5%. This observation also holds true for the ACDC dataset.\nACDC dataset. We also adopt the identical experimental setup used in SS-Net. The averaged performance results are shown in Table 2 on the ACDC dataset for four-class segmentation. Our approach consistently outperforms all state-of-the-art methods across all evaluation metrics. With the labeled ratio is set to 10%, GraphCL outperforms the second-best approach by an average of 26.01% across all four evaluation metrics. With the labeled ratio of 5%, GraphCL outperforms an average improvement of 29.65% over the second-best results across these metrics. Our approach leverages graph-based representations within the encoder and incorporates a graph neural network clustering loss Lcc, which significantly contributes to the performance gains. Specifically, the integration of graph structures enables the encoder to capture complex spatial relationships and contextual dependencies among voxels, facilitating a more holistic understanding of the input data. Lcc encourages similar voxels to be grouped together while enforcing separation between distinct regions, thereby enhancing intra-cluster coherence and inter-cluster separability. This clustering-based regularization aligns well with the anatomical structure of the target regions, allowing for more accurate segmentation and improving robustness in boundary delineation. Specifically, as can be seen in Figure 6, GraphCL can segment the fine details of the target organ, especially the edge details that are easily misrecognized or missed.\nPancreas-NIH Dataset. For the Pancreas-NIH dataset, we benchmark GraphCL against DAN [55], ADVENT [41], UA-MT [51], SASSNet [18], DTC [22], and CoraNet [35], all trained in a semi-supervised setup using both labeled and unlabeled data. V-Net is used as the backbone for our model and baseline methods, while V-Net alone is trained in a fully supervised manner as a lower bound. Table 3 shows that our approach achieves substantial improvements in Dice, Jaccard, and 95HD metrics, outperforming the second-best method by 2.50%, 3.79%, and 0.72%, respectively."}, {"title": "4.4. Ablation Studies", "content": "To analyze the effectiveness of each component in our proposed framework GraphCL, we conduct a series of ablation studies across three datasets (LA, ACDC, and PancreasNIH) with varying labeled data ratios. The detailed results are presented in Table 4, Table 5, and Table 6.\nEffectiveness of Components. In these experiments, we examine the contribution of two key components: the structure-aware alignment (denoted as SA) and the graph neural network clustering loss (Lcc). As shown in Table 4, adding SA or Lcc individually improves performance compared to the baseline. Specifically, incorporating both SA and Lcc achieves the best results, with notable improvements in metrics such as Dice Score, Jaccard Index, 95HD, and ASD. For instance, on the LA dataset with a 10% labeled data ratio, GraphCL with both components achieves a Dice Score of 90.24%, outperforming the baseline by a substantial margin. This trend is consistent across the other datasets, underscoring the effectiveness of these components in enhancing segmentation performance.\nOptimal Placement of GCN Layers. We further investigate the impact of placing the Graph Convolutional Network (GCN) layers at different depths within the encoder, as shown in Table 6. The results indicate that inserting the GCN layers at deeper levels (specifically at the fifth layer) leads to the highest performance gains. For example, with a labeled ratio of 10% on the ACDC dataset, inserting the GCN at the fifth layer results in a Dice Score of 89.31% and an ASD of 0.66, which are significantly better than inserting GCN at shallower layers. This demonstrates that deeper placement of GCNs allows the model to capture more complex spatial dependencies and contextual information, thereby enhancing segmentation accuracy.\nDataset-Specific Observations. Each dataset demonstrates unique performance patterns based on the labeled data ratio and the presence of SA and Lcc. For example, on the Pancreas-NIH dataset (Table 5), combining SA and Lcc results in improvements of 2.50% in Dice Score and 0.72 in 95HD over the baseline. The clustering loss Lcc contributes substantially to boundary delineation by enforcing inter-cluster separability, which is particularly beneficial for segmenting complex anatomical structures.\nThese ablation studies highlight the importance of each component in GraphCL, demonstrating that the combination of structure-aware alignment and graph-based clustering significantly improves segmentation results across various medical image datasets."}, {"title": "4.5. Impacts of Hyperparameters", "content": "To further verify the effectiveness of the proposed method, we also conduct sensitivity analysis on the ACDC dataset to evaluate the impact of hyperparameters \u043aand \u0442, with labeled data ratios of 5% and 10% (Figure 3 and Figure 4).\nEffect of K (Figure 3 (a) and Figure 4 (a)). \u043acontrols the weight of the structure-aware alignment. Increasing \u043a initially improves the Dice and Jaccard scores, reaching a peak around 0.01, after which performance declines slightly. This indicates that a moderate \u043a achieves the best balance between alignment and segmentation quality.\nEffect of T (Figure 3 (b) and Figure 4 (b)). \u0442 controls clustering sensitivity for the clustering loss Lcc. t = 2 yields the most consistent overall performance across multiple metrics, with Dice, Jaccard, 95HD, and ASD showing stable, favorable results. Although there are instances where certain metrics reach peak values at T = 6 and T = 10, the variation in results between these values and T = 2 is relatively small. Therefore, T = 2 can be considered an optimal choice, as it offers reliable performance without significant trade-offs, making it a robust setting for clustering sensitivity."}, {"title": "4.6. Visualization Analysis", "content": "Figure 5 and Figure 6 display presents kernel density estimations and segmentation results for different methods trained on the ACDC dataset, trained with 5% labeled data. In Figure 5, among all three cardiac structures, GraphCL has the best alignment of feature distributions between labeled and unlabeled data. This is evident when compared to SASSNet, SSNet, and BCP. Figure 6 illustrates that the segmentation results from GraphCL are notably more accurate and precise. In contrast to SSNet and BCP, GraphCL presents tighter and more distinct boundaries, demonstrating a closer alignment with the ground truth (GT). These findings underscore GraphCL's superior capability in capturing critical features and enhancing segmentation performance, particularly in scenarios with limited labeled data."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel graph-based clustering for semi-supervised medical image segmentation (SSMIS) that models graph data structures within a unified framework. Our approach leverages CNN-derived features from samples to construct a densely connected instance graph, based on the structural similarity, which effectively captures semantic representations for SSMIS. Additionally, we introduce a graph clustering mechanism to utilize more information during the clustering process, enabling implicit semantic part segmentation. Extensive experiments demonstrate the effectiveness of our proposed approach. For future work, we will explore methods to generate more reliable labels and enhance graph accuracy, aiming to reduce noise within the input graph."}]}