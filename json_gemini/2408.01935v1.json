{"title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference", "authors": ["Ke Shen", "Mayank Kejriwal"], "abstract": "Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four natural language commonsense reasoning datasets using both an open-source ensemble-based RoBERTa model and ChatGPT, demonstrate the practical utility of the evaluation framework. For example, our results show that our framework can get an LLM to confidently respond to an extra 20.1% of low-risk inference tasks that other methods might misclassify as high-risk, and skip 19.8% of high-risk tasks, which would have been answered incorrectly.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) such as OpenAI's GPT series have emerged as powerful tools capable of diverse natural language processing (NLP) tasks, from simple question-answering to complex narrative generation. As these models grow in prominence, concerns about their robustness and reliability, including generalization, hallucination, bias, and over-confidence, inevitably arise. Hence, there has been significant interest in evaluating these models' robustness, especially in relation to adversarial attacks and for out-of-distribution inputs. Research into language models' (LMs) capacity to handle uncertain generation scenarios, especially when the correct answer is not directly provided in the context, have led to the introduction of benchmarks such as SQuAD for generative question answering. Yet, the challenge of recognizing and mitigating risks in Natural Language Inference (NLI) tasks remains largely uncharted, especially when applying LLMs to applications that require high accuracy and reliability, such as the biomedical and healthcare domains.\nHistorically, the perceived risk associated with a model's inference and prediction in machine learning was often related to its confidence score. Lower confidence in the predicted answer was assumed to signal (relatively) greater risk, implicitly reflecting the model's self-evaluated probability of its correctness. Despite its problems, this conventional approach continues to be used in the deep learning community, including the transformer-based LLMs, owing to its simplicity and computational efficiency (compared to more expensive heuristic methods e.g., those relying on sampling an ensemble of model responses).\nTo achieve the confidence level of LLM responses internally, for generative models, various prompts are utilized to infer the reliability of generated answers. Applications such as chain-of-thought prompting are exemplary in employing confidence generated by generative LLMs. Discriminative LLMs, on the other hand, are typically fine-tuned to directly yield softmax probabilities from their final output layer. While the study by confirmed these confidence scores for their calibration capabilities on high-certainty inferences, LLMs have often been noted to produce poorly calibrated confidence scores that don't truly indicate the correctness of the output, as highlighted by. Research from has aimed to re-calibrate confidence using entropy or by designing binary classifiers based on these scores to detect uncertain inferences. However, a notable gap remains in thorough evaluations of how effectively these confidence-based risk indicators capture the risks LLMs face during inference.\nIn this study, we argue that a single (often implicit) definition of risk that only relies on over-confidence, as has been the case in the majority of research on this topic, may be insufficient. Rather, we advocate for a risk-centric evaluation framework"}, {"title": "Related Work", "content": "Discriminative and generative LLMs have recently achieved impressive performance on multiple inference tasks. However, due to documented problems such as hallucination and bias, the research focus is shifting from merely quantifying accuracy to an in-depth, context-sensitive probing of LLMs' robustness, particularly when confronted with risky or uncertain situations. The concept of risk has been studied across various disciplines. Historically, risk was perceived as a deviation from the norm, embodying misfortunes and undesirable events, with an underlying assumption of human agency in mitigating adverse outcomes. This perspective significantly influenced contemporary discussions on risk, but more recently, attention was focused on formalizing these intuitions by quantifying uncertainty and ambiguity. In practical applications, such as in Failure Mode and Effects Analysis (FMEA), risk evaluation methodologies have been developed to quantify risks based on their probability, severity, and potential for detection, but has faced critiques for its practical limitations in being applied to real-world scenarios. The adoption of quantitative risk metrics also enables the categorization of risks into tiers (low, medium, high) based on the likelihood and consequences of incidents, as seen in various industries. In specialized fields, notably healthcare, risk quantification involves additional methodologies, such as calculating the lifetime incidence of diseases within populations. These approaches to defining and evaluating risk underscore the diverse nature of risk management and its significance across different domains, and provide valuable guidance for exploring risk in the context of LMs, as specifically applied to NLI problems.\nSystematic methodologies for evaluating LLMs in high-risk scenarios include: 'adversarially' attacking LLMs through the use of semantically equivalent adversarial rules, use of adversarial triggers and even human-in-the-loop generation of adversarial examples. Recent studies also explore the riskiness of certain LLM prompts. While such research spotlights LLMs' vulnerabilities when adversarially prompted, we still expect LLMs to make judicious decisions about navigating a risk-reward tradeoff by being self-aware of when they might have a higher probability of going wrong. Besides, although \u2018risk' is frequently mentioned in adversarial attack research, as also in related literature on assessments of safety and bias, robustness-accuracy characteristics, and out-of-distribution (OOD) generalization, a formalism is still lacking that identifies and defines multiple types of risk. This paper proposes such a general formalism, including identifying two different risk categories that are relevant to LLMs, and novel metrics for measuring risk.\nInitial research on LLMs' \u2018self-understanding' of their own uncertainty, especially in the deep learning literature, has predominantly relied on interpreting raw softmax probabilities of the final output layer as \u2018confidence' scores. While studies such as have flagged these scores as potentially misleading and not genuinely capturing the model's true uncertainty, The"}, {"title": "Decision and Composite Risk: Definition and Evaluation Framework", "content": "For terminological convenience, let us denote a multi-choice inference instance as i = (q, Y) in an NLI benchmark I (consisting of a set of such instances), where q is a prompt (which can be, but is not necessarily limited to being, a proper 'question') and Y is a set of candidate choices. Both discriminative and generative LMs aim to identify the correct choice:\n$$\\hat{y} = \\operatorname{argmax}_{y \\in Y} P_{L M}(y|q),$$(1)\nHere, $P_{L M}(y|q)$ refers to the probability assigned by the LM to each choice y, indicating its likelihood of being correct. This probability is effectively estimated by a confidence score c assigned to each choice $y \\in Y$, regardless of the actual presence of a correct answer within Y. Instances are termed ambiguous when lacking a 'ground-truth' correct answer $\\hat{y} \\in Y$, with an indicator $\\hat{i}$ employed for clarity: $\\hat{i} = 1$ and $\\hat{i} = 0$ denote the presence and absence of a ground-truth answer, respectively.\nConsidering inference instances, we can model decision-making in LMs as two sequential application: of the decision rule dr, followed by the selection rule sr. We reformulate Eq. (3.1) as follows:\n$$\\hat{y} = s_r(q, Y) \\left[P_{L M}(y|q) \\cdot \\mathbb{1}(d_r(q, Y) = 1)\\right],$$(2)\nHere, $s_r(q, Y) = \\operatorname{argmax}_{y \\in Y}$. We designate a selective NLI (sNLI) system as a base LM that incorporates both a decision rule and a selection rule. The decision rule operates as a binary classifier: when $d_r(q, Y) = 1$, the model responds; when $d_r(q, Y) = 0$, it abstains. Discriminative LMs, not explicitly equipped with a $d_r$, default to $d_r(q, Y) = 1$ and hence, always attempt to make a prediction, which simplifies Eq. (3.2) back to the original form Eq. (3.1). Contrasting sharply with the all-responsive nature of discriminative models, generative LMs (internally) employ a more advanced dr, and may choose not to respond for certain statements with outputs like \u201cI don't understand\u201d or \u201cNone of the answers seem to be correct\", implying $d_r(q, Y) = 0$.\nWhen $d_r(q, Y) = 1$, the selection rule sr is invoked. Commonly, and as assumed here, sr predicts y' with the highest $P_{L M}(y'|q)$, which is estimated by confidence (c \u2208 C), with ties broken arbitrarily. Alternative selection rules can be devised, but are rare, and not considered herein.\nUsing this terminology, we can define decision risk as follows:\nDefinition 1 (Decision Risk). Given an instance i = (q, Y) and confidence set C over Y, the decision risk $r_d$ is set to 1 (and is otherwise 0) iff at least one of two conditions is met: (1) the instance is unambiguous ($\\hat{i} = 1$) but $d_r(q, Y) = 0$; and (2) the instance is ambiguous ($\\hat{i} = 0$) but $d_r(q, Y) = 1$.\""}, {"title": "Risk-adjusted Calibration Approach", "content": "As previously noted, the discriminative models' decision rule was inherently designed to respond to every query, defaulting to $d_r(q, Y) = 1$. Modern generative models, such as ChatGPT, have not disclosed their decision-making protocols, making any decision rules within them unpredictable from a user's standpoint. To evaluate and mitigate the decision and composite risk across both discriminative and generative models, an external decision rule method that is compatible with both types of models, and that is somewhat independent of the LM itself (and hence, generalizable), is clearly motivated. An ideal decision rule should use all available information, such as the instance prompts, as well as the confidence outputs from the underlying LLM, to minimize the risks defined earlier.\nPrevious studies have developed fundamental techniques for the re-calibration of confidence scores, aimed at more precisely capturing a model's intrinsic uncertainty. We expand on this idea significantly by proposing a novel risk-adjusted calibration method called 'Deciding when to decide' or DwD. An external decision rule method in the architecture helps minimize the decision and composite risks of LMs, especially in high-risk inference scenarios. Notably, DwD does not have strong dependencies on the underlying LMs, enabling it to be suitable for a variety of models without knowing their internal workings. As we subsequently demonstrate in experiments, this feature makes it well-suited for risk-adjusted inference even for blaxk-box commercial models like OpenAI's GPT-4.\nSimilar to its predecessors, the decision rule in DwD operates as a binary classifier, utilizing traditional machine learning techniques. However, it distinguishes itself from previous work by tackling two principal challenges: the injection of risk into the training process, and calibration refinement with risk adjustment. Concerning the former, DwD injects risk into training set construction, a step not explicitly considered by earlier calibrators. Existing NLI benchmarks usually provide a definitive answer for all instances. Theoretically, an effective decision rule could use this knowledge and just answer all instances to minimize the decision risk. Here, to introduce risk-injected instances in the training of DwD approach, we introduce Risk Injection Functions (RIFs) which can effectively turn an instance i = (q,Y) with an unambiguous correct answer into an 'ambiguous' instance i', with no correct answer:\n\u2022 Wrong-Question RIF (WQ): Retain the candidate choice set Y but replace the original prompt q with a new prompt q' from an unrelated instance in the same benchmark;\n\u2022 No-Right-Answer RIF (NRA): Retain the prompt q and all incorrect choices $Y - \\hat{y}$ in the new candidate choice set Y'; also, add to Y' a choice from another unrelated instance in the same benchmark.\nThese RIFs expose DwD to diverse risk scenarios, ensuring that the method is well-equipped to handle real-world challenges. DwD also refines its calibration by leveraging a comprehensive feature set including prompt length (in terms of the number of characters), the length of the predicted answer generated by the LLM, the confidence score of each candidate answer, the standard deviation of confidence scores across choices, the sentence embedding (which we obtained using the pre-trained nq-distilbert-base-v1 model in SentenceTransformer), the similarity between the prompt and each candidate choice, and the standard deviation of these embedding similarities. This diverse array of features is processed through a random forest classifier, trained on an equal mix of original and risk-injected instances perturbed by one of the RIFs, to predict the likelihood of an instance being labeled positively by DwD. By using a diverse set of features, our aim is to help DwD be as robust as possible so that it is able to navigate decision and composite risk better, and utilizes RIF training to maximum advantage. With these features, the DwD method is trained using a random forest classifier with equal numbers of original and risk-injected instances (following the application of one RIF) as the training set. The probability of an inference instance being labeled as positive is interpreted as the confidence of DwD asserting $d_r(q, Y) = 1$, which is used to estimate both the decision and composite risks. Note that the DwD approach is specifically trained on a discriminative LLM's confidence distribution, as generative LLMs such as GPT-3.5-turbo can only provide a 'fuzzy' confidence estimate. In Results, we show that, although the approach was exclusively trained on a discriminative LLM's confidence distribution, it is adaptable to generative LLMs."}, {"title": "Evaluation Metrics", "content": "Decision Risk\nFor the decision risk evaluation to be non-trivial, it is essential to include ambiguous inference instances\u2014those without a correct option within the candidate choice setting in the evaluation sets. We employ the WQ and NRA RIFs introduced before"}, {"title": "Experimental Study", "content": "Datasets We use four established NLI benchmarks (aNLI, HellaSwag, PIQA, and SocialIQA), modeled as multiple-choice tasks with one correct answer per prompt. The decision risk evaluation is conducted on a balanced evaluation set that comprises an equal number of original inference instances (sourced from the evaluation sets of each benchmark) and the corresponding perturbed instances. These perturbed instances are generated by applying one RIF and are matched in size with the original instances. Conversely, the evaluation of composite risks is solely performed on the original instances within each benchmark's evaluation dataset. To explore the risk profiles of both discriminative and generative language models, we use two prominent models: RoBERTa-large Ensemble and OpenAI's GPT-3.5-Turbo.\nDecision Rule Preliminary evaluation of GPT-3.5-Turbo shows that its aggressive built-in decision rule chose to respond to 88.7% of perturbed, high-risk ambiguous instances. This rule yielded a decision risk accuracy of 55.7%, which is only marginally better than the random baseline. Considering these findings, our experiments implement external decision rule methods for sNLI systems to effectively navigate and improve upon the inherent limitations of the built-in decision rule.\nExcept for DwD, three decision rule baseline methods are employed in the experiments. Like DwD, all baselines described below treat the LMs as black boxes and do not require access to the model's internal representations. Rather, for each instance, they only need the output (the confidence set C) of the model."}, {"title": "Results", "content": "Evaluating ID and OOD Decision Risk Table 1 reports the accuracy results (equivalent to 1-the proportion of decision risks) for RoBERTa Ensemble incorporating various decision rule methods (DwD and the three baselines) on in-domain (ID) and out-of-domain (OOD) decision risk evaluation datasets. In evaluating ID decision risk, across all benchmarks and settings, the accuracy of RoBERTa Ensemble, when guided by the proposed DwD method, outperformed its accuracy in conjunction"}, {"title": "Case Study: Evaluating Decision and Composite Risk in sNLI Systems on Choice-Overloaded Instances", "content": "In previous sections, we presented RIFs as useful tools to simulate inference scenarios with injected risks, aiming to evaluate the ability of sNLI systems to detect and mitigate 'artificial' risks. However, these simulated environments capture only a subset of the risks in real-world inference scenarios. Of particular interest is the 'overload' effect a cognitive bias where an excess of options leads to decision-making paralysis, impeding clear and rational judgment. Our case study explores this issue by examining whether LMs perceive an 'overloaded' context as intrinsically risky and if a decision rule can aid in identifying high-risk inferences in such scenarios. We put a spotlight on the DwD decision rule to evaluate its efficiency in detecting decision and composite risks in scenarios characterized by choice overload.\nChoice-overloaded Dataset Construction The development of choice-overloaded evaluation sets begins with a random selection of 50 instances from the original evaluation sets of the four previously mentioned benchmarks. These selected instances are then modified to expand their original set of candidate choices to specific predetermined numbers (n = 5, 10, 15). To mitigate the influence of chance, for each specified number of candidate choices, we randomly selected 50 evaluation instances and repeated the experiment three times. Regardless of the benchmarks' original configuration of candidate choices"}, {"title": "Decision Rule Method", "content": "In the case study, we focus exclusively on the proposed DwD approach as the external decision rule. This set of DwD decision rules is consistent with those evaluated in prior decision and composite risk evaluations. The training of these DwD methods employs confidence scores from ROBERTa, yielded for a composite of instances from the initial training sets across the four benchmarks, alongside their corresponding ambiguous instances perturbed by a singular RIF (WQ or NRA). Hence, we explore eight distinct DwD decision rules, each serving as an advanced decision-making protocol for ROBERTa and GPT-3.5-Turbo within the sNLI systems."}, {"title": "Results", "content": "Figure 5 presents the performance of both ROBERTa and ChatGPT on four benchmarks, comparing their accuracy across various choice-overloaded settings. Theoretically, LMs such as ROBERTa and CHatGPT should maintain consistent accuracy in selecting the correct answer, regardless of the number setting of options per instance. While a marginal decrease in accuracy might be expected in practice, it should not be significantly different from their performance on the original evaluation sets. In Figure 5, however, we find, interestingly enough, that both models exhibit improved performance on most benchmarks when the number of choices is increased to 5 with a random sampling method to extend the candidate choices. This suggests that the insertion of few random incorrect options may, counterintuitively, assist the models in more confidently identifying the correct answer across most benchmarks. As the number of options in the candidate choice sets increases, a clear decline in accuracy becomes apparent for both RoBERTa and ChatGPT, indicative of the models' struggles with an excess of options, which parallels the human experience of choice paralysis. This decline is particularly marked when the selection pool reaches 15, reflecting the substantial challenge these LMs face in identifying the correct answer from a broader selection of possibilities.\nWhen the sampling method employed is the heuristic sampling, which is more adversarial since we are deliberately trying to confuse the model with an option that has a greater chance of being more related to the prompt and to the other answers, n = 5 leads to a significant decline in performance for all benchmarks, with the exception of ChatGPT's performance on aNLI. In fact, the results suggest that aNLI is the \u2018easiest' benchmark for LMs when applying the choice overloaded expansion, as it manages to remain within a 20% margin from its baseline performance, even under the most aggressive setting (n = 15). Even so, the results clearly illustrate that even on this benchmark, LMs are not immune from the choice paralysis problem.\nGiven that the correct answer is always included in the candidate choices for all choice-overloaded instances, an ideal decision rule would engage with each presented instance. The performance range of the top-performing DwD method, selected from the eight examined, is presented in Figure 5. Our findings highlight that DwD instances, specifically those trained on the SocialIQA and PIQA benchmarks, demonstrate minimal decision risk across a variety of benchmarks under diverse choice expansion methods. These top DwD methods elect to respond to over 80% of instances for RoBERTa and 75% for GPT-3.5-Turbo, which is higher than LMs' accuracy on the original evaluation sets of the benchmarks. Yet, with the expansion of choice sets leading to a decrease in the accuracy of the LMs, DwD's frequency of responding to inference instances similarly decreases. To gain a deeper insight into the overall performance of the sNLI systems, we visualize the sensitivity and specificity of these systems, integrating the base LMs with different DwD rules, in heatmaps shown in Figures 6 and 7 for the four benchmarks."}, {"title": "Discussion", "content": "To gain an intuitive sense of the inference scenarios in which composite risk arises, we list some representative questions in Tables 6 and 5. Notable distinctions in composite risks are observed when comparing sNLI systems that employ ChatGPT against those based on ROBERTa. Specifically, ChatGPT-based sNLI systems tend to encounter composite risks in inference scenarios deemed 'easier,' whereas ROBERTa-based systems face these risks in more complex inference contexts. These complex instances are often characterized by equivocality, challenging the system's ability to discern a singular correct answer,"}, {"title": "Conclusion", "content": "This paper proposed and applied a risk-centric evaluation framework that defined two types of risk: decision and composite risk. Using four NLI benchmarks, we conducted an experimental study to demonstrate the practical utility of the proposed framework. A key finding of the study is that less well-performing confidence calibration can lead to problems of both under-confidence and over-confidence, despite considerably greater attention given to the latter in the literature. Learning-based decision rules, such as DwD, can help such models minimize the risks, even in challenging inference situations such as choice overload, while maintaining good overall inference performance."}]}