{"title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference", "authors": ["Ke Shen", "Mayank Kejriwal"], "abstract": "Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the models have in their inference. While the former is well studied, the latter is not, leading to an asymmetry in understanding the comprehensive risk of the model based on misplaced confidence. In this paper, we address this asymmetry by defining two types of risk (decision and composite risk), and proposing an experimental framework consisting of a two-level inference architecture and appropriate metrics for measuring such risks in both discriminative and generative LLMs. The first level relies on a decision rule that determines whether the underlying language model should abstain from inference. The second level (which applies if the model does not abstain) is the model's inference. Detailed experiments on four natural language commonsense reasoning datasets using both an open-source ensemble-based RoBERTa model and ChatGPT, demonstrate the practical utility of the evaluation framework. For example, our results show that our framework can get an LLM to confidently respond to an extra 20.1% of low-risk inference tasks that other methods might misclassify as high-risk, and skip 19.8% of high-risk tasks, which would have been answered incorrectly.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) such as OpenAI's GPT1,2 series have emerged as powerful tools capable of diverse natural language processing (NLP) tasks, from simple question-answering to complex narrative generation3\u20135. As these models grow in prominence, concerns about their robustness and reliability, including generalization, hallucination7,8, bias9,10, and over-confidence11, inevitably arise. Hence, there has been significant interest in evaluating these models' robustness, especially in relation to adversarial attacks12,13 and for out-of-distribution inputs14,15. Research into language models' (LMs) capacity to handle uncertain generation scenarios, especially when the correct answer is not directly provided in the context, have led to the introduction of benchmarks such as SQuAD for generative question answering16,17. Yet, the challenge of recognizing and mitigating risks in Natural Language Inference (NLI) tasks remains largely uncharted, especially when applying LLMs to applications that require high accuracy and reliability, such as the biomedical and healthcare domains.\nHistorically, the perceived risk associated with a model's inference and prediction in machine learning was often related to its confidence score18. Lower confidence in the predicted answer was assumed to signal (relatively) greater risk, implicitly reflecting the model's self-evaluated probability of its correctness. Despite its problems, this conventional approach continues to be used in the deep learning community19,20, including the transformer-based LLMs21,22, owing to its simplicity and computational efficiency (compared to more expensive heuristic methods e.g., those relying on sampling an ensemble of model responses).\nTo achieve the confidence level of LLM responses internally, for generative models, various prompts are utilized to infer the reliability of generated answers23,24. Applications such as chain-of-thought prompting25,26 are exemplary in employing confidence generated by generative LLMs. Discriminative LLMs, on the other hand, are typically fine-tuned to directly yield softmax probabilities from their final output layer. While the study by27 confirmed these confidence scores for their calibration capabilities on high-certainty inferences, LLMs have often been noted to produce poorly calibrated confidence scores that don't truly indicate the correctness of the output, as highlighted by28. Research from28\u201331 has aimed to re-calibrate confidence using entropy or by designing binary classifiers based on these scores to detect uncertain inferences. However, a notable gap remains in thorough evaluations of how effectively these confidence-based risk indicators capture the risks LLMs face during inference.\nIn this study, we argue that a single (often implicit) definition of risk that only relies on over-confidence11,32, as has been the case in the majority of research on this topic, may be insufficient. Rather, we advocate for a risk-centric evaluation framework"}, {"title": "", "content": "that defines two distinct risk types. We designate these risks as decision and composite risk and define them formally in the next section. We also present an evaluation framework, including metrics, for objectively measuring these risks. Specific contributions are as follows:\n1. We introduce a novel risk-centric framework for better understanding risk-adjusted inference in both discriminative and generative LMs. We formalize an LM's decision making as sequential application of a 'decision rule' and 'selection rule'. Mistakes in each application can lead to two novel kinds of risk, called decision risk and composite risk.\n2. To accommodate these risks, we propose and implement a novel risk-adjusted calibration framework called 'Deciding when to Decide' (DwD), which uses an external decision rule method, compatible with both discriminative and generative LMs. We also propose risk injection functions, applicable to any existing multiple-choice NLI benchmark, for trainin and evaluating DwD.\n3. We present detailed experimental results showing the utility of the framework. We also show that DwD empirically outperforms competitive baselines on several established inference benchmarks by reducing decision and composite risk in underlying LMs by margin of up to 25.3% and 16.6%, respectively.\n4. We present a case study that applies the risk-centric evaluation framework to evaluate the decision and composite risks in a complex real-world inference scenario known as choice overload. This case study exemplifies the practical application of the evaluation framework in real-world inference settings, offering insights into its application and utility.\nThe rest of this paper is structured as follows. Section covers related work on evaluating risk and uncertainty in LMs across various NLP tasks. Next, we define specific risks -decision and composite \u2013 underlying the study in Section. Section introduces DwD, a risk-adjusted calibration implemented as a robust decision rule method compatible with both discriminative and generative LMs. Section presents novel evaluation metrics for measuring decision and composite risks, while Section provides details on the experimental study, followed by results in Section. Section describes the case study, with the article concluding in Section ."}, {"title": "Related Work", "content": "Discriminative and generative LLMs have recently achieved impressive performance on multiple inference tasks33,34. However, due to documented problems such as hallucination and bias, the research focus is shifting from merely quantifying accuracy to an in-depth, context-sensitive probing of LLMs' robustness, particularly when confronted with risky or uncertain situations35\u201337.\nThe concept of risk has been studied across various disciplines. Historically, risk was perceived as a deviation from the norm, embodying misfortunes and undesirable events, with an underlying assumption of human agency in mitigating adverse outcomes38. This perspective significantly influenced contemporary discussions on risk, but more recently, attention was focused on formalizing these intuitions by quantifying uncertainty and ambiguity39,40. In practical applications, such as in Failure Mode and Effects Analysis (FMEA)41\u201343, risk evaluation methodologies have been developed to quantify risks based on their probability, severity, and potential for detection, but has faced critiques for its practical limitations in being applied to real-world scenarios. The adoption of quantitative risk metrics also enables the categorization of risks into tiers (low, medium, high) based on the likelihood and consequences of incidents, as seen in various industries44. In specialized fields, notably healthcare, risk quantification involves additional methodologies, such as calculating the lifetime incidence of diseases within populations45. These approaches to defining and evaluating risk underscore the diverse nature of risk management and its significance across different domains, and provide valuable guidance for exploring risk in the context of LMs, as specifically applied to NLI problems.\nSystematic methodologies for evaluating LLMs in high-risk scenarios include: 'adversarially' attacking LLMs through the use of semantically equivalent adversarial rules46, use of adversarial triggers47 and even human-in-the-loop generation of adversarial examples48. Recent studies also explore the riskiness of certain LLM prompts49\u201351. While such research spotlights LLMs' vulnerabilities when adversarially prompted, we still expect LLMs to make judicious decisions about navigating a risk-reward tradeoff by being self-aware of when they might have a higher probability of going wrong. Besides, although \u2018risk' is frequently mentioned in adversarial attack research, as also in related literature on assessments of safety and bias10,52, robustness-accuracy characteristics53, and out-of-distribution (OOD) generalization54,55, a formalism is still lacking that identifies and defines multiple types of risk. This paper proposes such a general formalism, including identifying two different risk categories that are relevant to LLMs, and novel metrics for measuring risk.\nInitial research on LLMs' \u2018self-understanding' of their own uncertainty, especially in the deep learning literature, has predominantly relied on interpreting raw softmax probabilities of the final output layer as \u2018confidence' scores56. While studies such as29 have flagged these scores as potentially misleading and not genuinely capturing the model's true uncertainty, The"}, {"title": "Decision and Composite Risk: Definition and Evaluation Framework", "content": "For terminological convenience, let us denote a multi-choice inference instance as i = (q, Y) in an NLI benchmark I (consisting of a set of such instances), where q is a prompt (which can be, but is not necessarily limited to being, a proper 'question') and Y is a set of candidate choices. Both discriminative and generative LMs aim to identify the correct choice:\n\u0177 = argmax PLM(y|q),  (1)\nyeY\nHere, PLM(y|q) refers to the probability assigned by the LM to each choice y, indicating its likelihood of being correct. This probability is effectively estimated by a confidence score c assigned to each choice y \u2208 Y, regardless of the actual presence of a correct answer within Y. Instances are termed ambiguous when lacking a 'ground-truth' correct answer \u0177 \u2208 Y, with an indicator \u00ee employed for clarity: i = 1 and i = 0 denote the presence and absence of a ground-truth answer, respectively.\nConsidering inference instances, we can model decision-making in LMs as two sequential application: of the decision rule dr, followed by the selection rule sr. We reformulate Eq. (3.1) as follows:\n\u0177 = sr(q, Y) [PLM(y|q)\u00b71(dr(q,Y) = 1)], (2)\nyeY\nHere, sr(q, Y) = argmax y. We designate a selective NLI (sNLI) system as a base LM that incorporates both a decision\n rule and a selection rule. The decision rule operates as a binary classifier: when dr(q,Y) = 1, the model responds; when dr(q,Y) = 0, it abstains. Discriminative LMs, not explicitly equipped with a dr, default to dr(q,Y) = 1 and hence, always attempt to make a prediction, which simplifies Eq. (3.2) back to the original form Eq. (3.1). Contrasting sharply with the all-responsive nature of discriminative models, generative LMs (internally) employ a more advanced dr, and may choose not to respond for certain statements with outputs like \u201cI don't understand\u201d or \u201cNone of the answers seem to be correct\", implying dr(q,Y) = 0.\nWhen dr(q,Y) = 1, the selection rule sr is invoked. Commonly, and as assumed here, sr predicts y' with the highest PLM(y q), which is estimated by confidence (c \u2208 C), with ties broken arbitrarily. Alternative selection rules can be devised, but are rare, and not considered herein.\nUsing this terminology, we can define decision risk as follows:\nDefinition 1 (Decision Risk). Given an instance i = (q,Y) and confidence set C over Y, the decision risk rd is set to 1 (and is otherwise 0) iff at least one of two conditions is met: (1) the instance is unambiguous (i = 1) but dr(q,Y) = 0; and (2) the instance is ambiguous (i = 0) but dr(q,Y) = 1.\""}, {"title": "", "content": "Decision risk can be an important concern in many human-facing and otherwise critical applications, such as clinical decision-making61,62. In such domains, ambiguity is not uncommon for a variety of complex reasons, including expert disagreement, and evolving situational knowledge. Note that, in some applications, one of the two conditions in the definition might be more consequential than the other; however, in this paper, we treat both equally as decision risks.\nNext, we define the composite risk, which is motivated by the common situation where all instances are technically unambiguous but where an underlying LM finds some instances riskier (more likely to get wrong) than others. Even in the absence of ambiguity, composite risk can be used to quantify both potential under-confidence and over-confidence in the model's performance.\nDefinition 2 (Composite Risk). Given an instance i = (q, Y) with a unique correct answer \u0177 \u2208 Y and a confidence set Cover Y, the composite risk rs is set to 1 (and is otherwise 0) iff at least one of two conditions is met: (1) dr(q, Y) = 1 but the selection rule makes an incorrect prediction \u0177' \u2260 \u0177; and (2) dr(q, Y) = 0 but the selection rule of the model yields a correct prediction y = \u0177."}, {"title": "", "content": "In prior work, the (empirical) risk is formally characterized as E[l(y',y) \u00b7 dr(q,Y)], where l typically represents a 0/1 error loss function63. This offers a streamlined representation of the composite error while preserving its original semantics. According to29, when the confidence of the predicted answer y' converges to 1/K, with K being the cardinality of the candidate answer set Y, uncertainty is at its peak. According to this study, such a scenario also maximizes the likelihood of composite risk."}, {"title": "Risk-adjusted Calibration Approach", "content": "As previously noted, the discriminative models' decision rule was inherently designed to respond to every query, defaulting to dr(q,Y) = 1. Modern generative models, such as ChatGPT, have not disclosed their decision-making protocols, making any decision rules within them unpredictable from a user's standpoint. To evaluate and mitigate the decision and composite risk across both discriminative and generative models, an external decision rule method that is compatible with both types of models, and that is somewhat independent of the LM itself (and hence, generalizable), is clearly motivated. An ideal decision rule should use all available information, such as the instance prompts, as well as the confidence outputs from the underlying LLM, to minimize the risks defined earlier.\nPrevious studies22,29 have developed fundamental techniques for the re-calibration of confidence scores, aimed at more precisely capturing a model's intrinsic uncertainty. We expand on this idea significantly by proposing a novel risk-adjusted calibration method called 'Deciding when to decide' or DwD. An external decision rule method in the architecture helps minimize the decision and composite risks of LMs, especially in high-risk inference scenarios. Notably, DwD does not have strong dependencies on the underlying LMs, enabling it to be suitable for a variety of models without knowing their internal workings. As we subsequently demonstrate in experiments, this feature makes it well-suited for risk-adjusted inference even for blaxk-box commercial models like OpenAI's GPT-4.\nSimilar to its predecessors, the decision rule in DwD operates as a binary classifier, utilizing traditional machine learning techniques. However, it distinguishes itself from previous work by tackling two principal challenges: the injection of risk into the training process, and calibration refinement with risk adjustment. Concerning the former, DwD injects risk into training set construction, a step not explicitly considered by earlier calibrators. Existing NLI benchmarks usually provide a definitive answer for all instances. Theoretically, an effective decision rule could use this knowledge and just answer all instances to minimize the decision risk. Here, to introduce risk-injected instances in the training of DwD approach, we introduce Risk Injection Functions (RIFs) which can effectively turn an instance i = (q,Y) with an unambiguous correct answer into an 'ambiguous' instance i', with no correct answer:\n\u2022 Wrong-Question RIF (WQ): Retain the candidate choice set Y but replace the original prompt q with a new prompt q' from an unrelated instance in the same benchmark;\n\u2022 No-Right-Answer RIF (NRA): Retain the prompt q and all incorrect choices Y \u2013 \u0177 in the new candidate choice set Y'; also, add to Y' a choice from another unrelated instance in the same benchmark.\nThese RIFs expose DwD to diverse risk scenarios, ensuring that the method is well-equipped to handle real-world challenges. DwD also refines its calibration by leveraging a comprehensive feature set including prompt length (in terms of the number of characters), the length of the predicted answer generated by the LLM, the confidence score of each candidate answer, the standard deviation of confidence scores across choices, the sentence embedding (which we obtained using the pre-trained nq-distilbert-base-v1 model in SentenceTransformer64), the similarity between the prompt and each candidate choice, and the standard deviation of these embedding similarities. This diverse array of features is processed through a random forest classifier, trained on an equal mix of original and risk-injected instances perturbed by one of the RIFs, to predict the likelihood of an instance being labeled positively by DwD. By using a diverse set of features, our aim is to help DwD be as robust as possible so that it is able to navigate decision and composite risk better, and utilizes RIF training to maximum advantage. With these features, the DwD method is trained using a random forest classifier with equal numbers of original and risk-injected instances (following the application of one RIF) as the training set. The probability of an inference instance being labeled as positive is interpreted as the confidence of DwD asserting dr(q,Y) = 1, which is used to estimate both the decision and composite risks. Note that the DwD approach is specifically trained on a discriminative LLM's confidence distribution, as generative LLMs such as GPT-3.5-turbo can only provide a 'fuzzy' confidence estimate. In Results, we show that, although the approach was exclusively trained on a discriminative LLM's confidence distribution, it is adaptable to generative LLMs."}, {"title": "Evaluation Metrics", "content": "Decision Risk\nFor the decision risk evaluation to be non-trivial, it is essential to include ambiguous inference instances\u2014those without a correct option within the candidate choice setting in the evaluation sets. We employ the WQ and NRA RIFs introduced before"}, {"title": "", "content": "to perturb the original instances within the existing NLI benchmark's evaluation set. The decision risk evaluation-dataset will thus consist of a balanced mix of both original and risk-injected instances. Note that there is a deliberate separation between the instances perturbed for training purposes and those adjusted for evaluation to ensure no overlap.\nSince an effective decision rule dr(q,Y) over LLMs' confidence score should return 0 for risk-injected instances and 1 for risk-free instances to minimize decision risk, we can evaluate the efficacy of dr by calculating the inverse proportion of the decision risk,\nP = P(dr(q,Y) = \u00ee|i\u2208 {Itest \u222a f(Itest)},dr) (3)\nwhen a certain RIF f was applied to generate i' in the evaluation set. Without loss of generality, we evaluate the robustness of dr in two different scenarios: In-domain (ID) and Out-of-domain (OOD). In an ID evaluation, the same RIF applied for training a decision rule is re-applied to an evaluation set containing unseen instances. Hence, the decision rule method has an opportunity to 'learn' the RIF from the training data. In contrast, the OOD evaluation, meant to be harder and more realistic, uses different RIF(s) than the ones used during training; hence, the decision rule has no knowledge of these RIFs during the learning phase. These intuitions are illustrated in Figure 2."}, {"title": "Composite Risk", "content": "Unlike decision risk, which solely focuses on the decision risk, the composite risk depends on both the decision risk and selection risk. To capture the complexity, we first use two metrics, risk specificity and risk sensitivity, inspired by epidemiological practices65:\nPspe = P(dr(q,Y) = 0|sr(q,Y) \u2260 \u0177) (4)\nPsen = P(dr(q,Y) = 1|sr(q,Y) = \u00ee) (5)\nRisk specificity evaluates the performance of sNLI systems when the selection rule leads to an incorrect prediction, whereas risk sensitivity measures system performance when the selection rule can accurately identify the correct answer. A low value in either of the two indicates a significant composite risk; for instance, overly aggressive decisions relative to the accuracy of the selection rule yield low risk specificity.\nTo describe the trade-off between answering more questions and decreasing composite risks, we also use the relative risk ratio (RRR) metric66:\nRRR = P(sr(q,Y) \u2260 \u0177|dr(q,Y) = 1) (6)\nP(sr(q,Y) = \u0177|dr(q,Y) = 0)\nwhere the probability condition is reversed compared to risk sensitivity and specificity, defined earlier. When RRR is significantly smaller than 1 (e.g., at the 95% confidence level), it implies that dr significantly reduces composite risk when it decides to answer (dr = 1), compared to when it abstains. Figure 2 presents a full overview of the evaluation framework for decision and composite risk."}, {"title": "Experimental Study", "content": "Datasets We use four established NLI benchmarks (aNLI67, HellaSwag68, PIQA69, and SocialIQA70), modeled as multiple- choice tasks with one correct answer per prompt. The decision risk evaluation is conducted on a balanced evaluation set that comprises an equal number of original inference instances (sourced from the evaluation sets of each benchmark) and the corresponding perturbed instances. These perturbed instances are generated by applying one RIF and are matched in size with the original instances. Conversely, the evaluation of composite risks is solely performed on the original instances within each benchmark's evaluation dataset. To explore the risk profiles of both discriminative and generative language models, we use two prominent models: RoBERTa-large Ensemble71 and OpenAI's GPT-3.5-Turbo.\nDecision Rule Preliminary evaluation of GPT-3.5-Turbo shows that its aggressive built-in decision rule chose to respond to 88.7% of perturbed, high-risk ambiguous instances. This rule yielded a decision risk accuracy of 55.7%, which is only marginally better than the random baseline. Considering these findings, our experiments implement external decision rule methods for sNLI systems to effectively navigate and improve upon the inherent limitations of the built-in decision rule.\nExcept for DwD, three decision rule baseline methods are employed in the experiments. Like DwD, all baselines described below treat the LMs as black boxes and do not require access to the model's internal representations. Rather, for each instance, they only need the output (the confidence set C) of the model."}, {"title": "", "content": "Random Baseline. Given an instance (into which risk may or may not have been injected), this baseline ignores the confidence set C and randomly chooses between 1 (risk-free) and 0 (risky) with equal probability. Hence, if risk has been injected in half the instances in the evaluation set, the accuracy of its prediction will always be 0.5 (in expectation), and is intended to serve as a useful reference for evaluating more advanced decision rule methods.\nConfStd Baseline. Inspired by MaxProb72, this baseline uses the standard deviation among all candidate choices' confidences (referred to as ConfStd) to make its decision. Using MaxProb directly was found to yield near-random performance when evaluating decision risk (see Supplementary Information). A lower ConfStd corresponds to a higher risk of answering incorrectly. We use the training set of a benchmark to determine the optimal ConfStd threshold below which a decision of 0 (risky) would be returned. This threshold is then applied during evaluation.\nCalibrator Baseline. Finally, we used another baseline that relies on training a calibrator60,73. We opted to use the random forest model as the binary classifier, aligning with the consistency of our proposed DwD method. Inspired by the experimental settings in60, we designed the calibrator baseline using the prompt length (in terms of the number of characters), predicted answer (y) length, and each candidate choice's confidence, as features.\nFor the purposes of training decision rule baselines (excluding the Random Baseline) and DwD approach, risk-injected instances are labeled with 0, and original instances with 1. For conducting in-domain (ID) evaluations, we use each of the RIFs in turn during both training and evaluation, and report results separately for each RIF. When conducting out-of-domain (OOD) evaluations, however, another set of output is obtained for each RIF utilized during training, based on the application of different RIFs in the evaluation phase.\nSelection Rule We consider a standard confidence-based selection rule in this paper: given the confidence set C, we select \u0177' as the selected choice, where y' is the choice assigned the highest confidence in C. Ties are broken arbitrarily."}, {"title": "Results", "content": "Evaluating ID and OOD Decision Risk Table 1 reports the accuracy results (equivalent to 1-the proportion of decision risks) for RoBERTa Ensemble incorporating various decision rule methods (DwD and the three baselines) on in-domain (ID) and out-of-domain (OOD) decision risk evaluation datasets. In evaluating ID decision risk, across all benchmarks and settings, the accuracy of RoBERTa Ensemble, when guided by the proposed DwD method, outperformed its accuracy in conjunction"}, {"title": "", "content": "with ConfStd and Calibrator, by significant margins often exceeding 20 percent. This suggests the efficacy of learning-based methods (such as DwD and Calibrator), and of RIF-based training, in particular. In contrast, ConfStd, which directly utilizes RoBERTa's confidence as a decision rule, showed near-random performance for the PIQA benchmark, with some improvements on other benchmarks. This finding aligns with the observation that the raw confidences of LLMs themselves tend to be poorly calibrated when confronted with ambiguity and uncertainty28.\nAs the nature of decision risks confronted by LLMs is typically unknown, handling OOD decision risk is expected to be harder than handling ID risk. Table 1 (b) illustrates that RoBERTa Ensemble, utilizing a DwD-generated decision rule trained with either WQ or NRA function, achieved the best performance with an average accuracy typically above 60 percent, and in some cases, above 75 percent, outperforming other baselines by substantial margins. On average, DwD had a decline of 9 percent compared to its ID performance, and the absolute estimates suggest that RoBERTa Ensemble can be properly re-calibrated when exposed to both ID and OOD decision risks, when coupled with the correct decision rule method. The OOD results also confirm that RoBERTa's raw confidence cannot directly be used as a reliable decision rule, but the DwD-adjusted decision rule can predict decision risk effectively."}, {"title": "Evaluating Composite Risk", "content": "Table 2 and 3 show the results for composite risks. The RoBERTa Ensemble using the WQ- trained DwD decision rule achieved the highest sensitivity and specificity across all benchmarks. Both ChatGPT and ROBERTa Ensemble tend to yield confidence distribution with high 'reference' values for 'risk-free' instances. With the DwD rule, over 90% of instances in aNLI and HellaSwag benchmarks were accurately answered, significantly reducing composite risk. In instances where LLMs might err if left unguided, DwD-guided RoBERTa Ensemble maintains the best performance. However, it is less specific than sensitive, recording an average specificity of 0.573 over four benchmarks.\nTable 4 shows that with various decision rules, both RoBERTa Ensemble and ChatGPT significantly reduced composite risks (RRR < 1 at 95% confidence). Again, DwD calibration (trained using WQ RIF) obtained the lowest overall RRR. The average RRR of 0.246 indicates that the risk ratio when DwD prompts RoBERTa to make a prediction is around 25% compared to the risk ratio when DwD prompts ROBERTa to skip the instances. Interestingly, using the DwD method (trained with WQ RIF) as a decision rule, ChatGPT outperforms RoBERTa on PIQA. Despite DwD being exclusively trained on ROBERTa's confidence distribution, it generalizes well, potentially guiding other LLMs like ChatGPT in confidently answering risk-free inferences. This is notable because models such as RoBERTa, unlike some of the more recent black-box LLMs, are more freely available in the open-source community and require fewer computational resources to fine-tune."}, {"title": "Visualizing Risk-Coverage Tradeoff of DwD", "content": "We briefly discuss the risk-coverage tradeoff of RoBERTa by plotting the risk-coverage curves (in Fig 3) for each benchmark, when the model was incorporated with the DwD decision rule. We expect a robust LLM with an effective risk-adjusted calibration to exhibit a lower aggregate risk when dealing with instances"}, {"title": "Case Study: Evaluating Decision and Composite Risk in sNLI Systems on Choice- Overloaded Instances", "content": "In previous sections, we presented RIFs as useful tools to simulate inference scenarios with injected risks, aiming to evaluate the ability of sNLI systems to detect and mitigate 'artificial' risks. However, these simulated environments capture only a subset of the risks in real-world inference scenarios. Of particular interest is the 'overload' effect a cognitive bias where an excess of options leads to decision-making paralysis, impeding clear and rational judgment74,75. Our case study explores this issue by examining whether LMs perceive an 'overloaded' context as intrinsically risky and if a decision rule can aid in identifying high-risk inferences in such scenarios. We put a spotlight on the DwD decision rule to evaluate its efficiency in detecting decision and composite risks in scenarios characterized by choice overload.\nChoice-overloaded Dataset Construction The development of choice-overloaded evaluation sets begins with a random selection of 50 instances from the original evaluation sets of the four previously mentioned benchmarks. These selected instances are then modified to expand their original set of candidate choices to specific predetermined numbers (n = 5, 10, 15). To mitigate the influence of chance, for each specified number of candidate choices, we randomly selected 50 evaluation instances and repeated the experiment three times. Regardless of the benchmarks' original configuration of candidate choices"}, {"title": "", "content": "for instance, HellaSwag offering four options and PIQA providing only two we retain only the original correct answer among the candidate choices. We then select n - 1 candidate choices randomly from a collective pool of unrelated inference instances within the same benchmark. This pool is created by aggregating candidate choices from all instances, excluding the one undergoing modification, from which n 1 choices are drawn to complete the candidate set for each instance. These choices are subsequently shuffled to eliminate any bias in their ordering.\nTo explore how different incorrect choice sampling strategies impact the performance of sNLI systems on the constructed choice-overloaded datasets, we implement two distinct methodologies: the random method and the heuristic method. The"}, {"title": "", "content": "random method randomly selects n - 1 choices from the collective pool of candidate choices, ensuring a broad, albeit less targeted, selection. Conversely, the heuristic method employs a more nuanced approach by ranking all other inference instances according to the similarity of prompt embeddings across different inference instances to the target instance's prompt. It then identifies the n \u2013 1 instances most closely related semantically, and selects one incorrect answer from each to form a new set of candidate choices that are contextually similar. This heuristic approach aims to simulate a choice environment that not only overloads, but does so with options that are closely related but still incorrect to the prompt, thus increasing the adversarial complexity and realism of the decision-making scenario. To ensure the validity of the candidate-choices expansion, we conducted a manual review to verify that each inference instance contained only one theoretically correct answer. Figure 4 includes examples of choice-overloaded instances generated using both methods.\nDecision Rule Method In the case study, we focus exclusively on the proposed DwD approach as the external decision rule. This set of DwD decision rules is consistent with those evaluated in prior decision and composite risk evaluations. The training of these DwD methods employs confidence scores from ROBERTa, yielded for a composite of instances from the initial training sets across the four benchmarks, alongside their corresponding ambiguous instances perturbed by a singular RIF (WQ or NRA). Hence, we explore eight distinct DwD decision rules, each serving as an advanced decision-making protocol for ROBERTa and GPT-3.5-Turbo within the sNLI systems."}, {"title": "Results", "content": "Figure 5 presents the performance of both ROBERTa and ChatGPT on four benchmarks, comparing their accuracy across various choice-overloaded settings. Theoretically, LMs such as ROBERTa and CHatGPT should maintain consistent accuracy in selecting the correct answer, regardless of the number setting of options per instance. While a marginal decrease in accuracy might be expected in practice, it should not be significantly different from their performance on the original evaluation sets. In Figure 5, however, we find, interestingly enough, that both models exhibit improved performance on most benchmarks when the number of choices is increased to 5 with a random sampling method to extend the candidate choices. This suggests that the insertion of few random incorrect options may, counterintuitively, assist the models in more confidently identifying the correct answer across most benchmarks. As the number of options in the candidate choice sets increases, a clear decline in accuracy becomes apparent for both RoBERTa and ChatGPT, indicative of the models' struggles with an excess of options, which parallels the human experience of choice paralysis. This decline is particularly marked when the selection pool reaches 15, reflecting the substantial challenge these LMs face in identifying the correct answer from a broader selection of possibilities.\nWhen the sampling method employed is the heuristic sampling, which is more adversarial since we are deliberately trying to confuse the model with an option that has a greater chance of being more related to the prompt and to the other answers, n = 5 leads to a significant decline in performance for all benchmarks, with the exception of ChatGPT's performance on aNLI. In fact, the results suggest that aNLI is the \u2018easiest' benchmark for LMs when applying the choice overloaded expansion, as it manages to remain within a 20% margin from its baseline performance, even under the most aggressive setting (n = 15). Even so, the results clearly illustrate that even on this benchmark, LMs are not immune from the choice paralysis problem.\nGiven that the correct answer is always included in the candidate choices for all choice-overloaded instances, an ideal decision rule would engage with each presented instance. The performance range of the top-performing DwD method, selected from the eight examined, is presented in Figure 5. Our findings highlight that DwD instances, specifically those trained on the SocialIQA and PIQA benchmarks, demonstrate minimal decision risk across a variety of benchmarks under diverse choice expansion methods. These top DwD methods elect to respond to over 80% of instances for RoBERTa and 75% for GPT-3.5-Turbo, which is higher than LMs' accuracy on the original evaluation sets of the benchmarks. Yet, with the expansion of choice sets leading to a decrease in the accuracy of the LMs, DwD's frequency of responding to inference instances similarly decreases. To gain a deeper insight into the overall performance of the sNLI systems, we visualize the sensitivity and specificity of these systems, integrating the base LMs with different DwD rules, in heatmaps shown in Figures 6 and 7 for the four benchmarks."}, {"title": "", "content": "Figures 6 and 7 show that sNLI systems, when integrating GPT-3.5-Turbo with DwD methods trained on SocialIQA datasets perturbed with WQ or NRA RIFs, respond to an average of 80.8% of instances where their selection mechanism is primed to deliver correct answers, regardless of the benchmark's choice overload settings. These sNLI systems exhibit their highest efficiency when employing DwD trained with WQ RIF-perturbed SocialIQA instances, achieving an average response rate of 91.8"}]}