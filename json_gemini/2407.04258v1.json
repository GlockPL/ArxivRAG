{"title": "Unsupervised Video Summarization via Reinforcement Learning and a Trained Evaluator", "authors": ["Mehryar Abbasi", "Hadi Hadizadeh", "Parvaneh Saeedi"], "abstract": "This paper presents a novel approach for unsupervised video summarization using reinforcement learning. It aims to address the existing limitations of current unsupervised methods, including unstable training of adversarial generator-discriminator architectures and reliance on hand-crafted reward functions for quality evaluation. The proposed method is based on the concept that a concise and informative summary should result in a reconstructed video that closely resembles the original. The summarizer model assigns an importance score to each frame and generates a video summary. In the proposed scheme, reinforcement learning, coupled with a unique reward generation pipeline, is employed to train the summarizer model. The reward generation pipeline trains the summarizer to create summaries that lead to improved reconstructions. It comprises a generator model capable of reconstructing masked frames from a partially masked video, along with a reward mechanism that compares the reconstructed video from the summary against the original. The video generator is trained in a self-supervised manner to reconstruct randomly masked frames, enhancing its ability to generate accurate summaries. This training pipeline results in a summarizer model that better mimics human-generated video summaries compared to methods relying on hand-crafted rewards. The training process consists of two stable and isolated training steps, unlike adversarial architectures. Experimental results demonstrate promising performance, with F-scores of 62.3 and 54.5 on TVSum and SumMe datasets, respectively. Additionally, the inference stage is 300 times faster than our previously reported state-of-the-art method.", "sections": [{"title": "I. INTRODUCTION", "content": "Video summarization provides a condensed representation of video content and enables users to grasp its core essence swiftly. With the surge of video data, the demand for more efficient methods for indexing, searching, and managing extensive video databases becomes increasingly urgent [1, 2]. Video summarization provides condensed content in surveillance systems, online learning platforms, and social media [3]. It aids in identifying events in traffic monitoring systems [4], serves as a resource in healthcare and education [5, 6], and assists us in navigating through the immense volume of video data [7].\nThere are two ways to summarize videos: by selecting important frames (video skimming) or by making a sequence of short video clips (video storyboarding) [8]. A common guideline for this process is that the summary length should not exceed 15% of the input video length [9], ensuring that it captures the most critical aspects while remaining concise and easy to watch. In recent years, deep learning-based methods for automated video summarization have gained popularity [9].\nHowever, many of these methods rely on human-generated labels to train their models [10-12], leading to challenges with scarcity, subjectivity, and bias in human annotations. As a result, there has been a focus on developing unsupervised video summarization methods [13-27]. Unsupervised methods don't require human annotations. Instead, they use heuristic criteria like diversity, coverage, and/or representativeness to select the summary frames. However, these methods often fail to capture the semantic relevance and coherence of the summary and may produce redundant or noisy frames. Some of the existing works use complex or unstable architectures (e.g., RNNs, LSTMs, GANs) and training procedures (e.g. adversarial learning) [13, 20-27]. Other methods employ training criteria, reward, and loss functions that do not strongly correlate with the way a human would generate a video summary, thereby limiting their performance metrics [14\u201318].\nThis paper is an extension of our recent preliminary work [19], which presented a new iterative method for frame scoring and video summarization. In the proposed method, called RS-SUM, a generator model was trained in a self-supervised manner to reconstruct masked frames of a video input. In the inference stage, frames were randomly masked iteratively and the total reconstruction score was assigned to non-masked frames as their representativeness frame score. In the work presented here, the generator forms the backbone of our learned reward function, used to train the summarizer via reinforcement learning. The video summarizer must take in a video, produce a score for each frame, and create a video summary using the assigned scores. During training, the scores generated by the summarizer are used as probability scores. Frames are then randomly chosen to be either masked or left unmasked, resulting in a partially masked video. The probability of a frame being masked is inversely proportional to its score. This partially masked video is then passed to the generator, which reconstructs the masked frames. The reconstruction loss is computed by comparing the input video and its reconstructed version. This loss is transformed into a reward coefficient which adjusts the summarizer, to assign higher scores to frames that contribute to a superior reconstruction.\nThe following are what differentiates this work from the previous works [13\u201327]:\n\u2022 We present a single-pass summarizer model, which generates a video summary from a video input, a process that is significantly faster (300 times) than RS-SUM [19].\n\u2022 We present a novel dynamic window masking algorithm used in the training stage of the generator. It enhances the downstream video summarization task, offering better results over a fixed window masking method that is used"}, {"title": "II. RELATED WORKS", "content": "Most unsupervised video summarization algorithms adhere to the principle that a video summary should enable a viewer to understand the original content of the video with less effort, time, and resources [13, 20-27]. These algorithms employ Generative Adversarial Networks (GAN) to generate a summary that encapsulates the essence of the video. Unsupervised video summarization algorithms based on GAN typically utilize three units: a summarizer, a generator, and a discriminator. The summarizer generates the summary from the input video by assigning importance scores to the frames and selecting only the high-scoring frames to form a video snippet. The generator creates two new video representations from the video summary and the original input video. These new representations are expected to be similar in content and style. The discriminator evaluates the generator's outputs and attempts to identify which one was based on the summary.\nThe use of an adversarial learning process to train a keyframe selector based on Long Short-Term Memory (LSTM) was pioneered by [13]. Subsequent works have focused on improving that method through various modifications such as creating a more robust discriminator to retain as much information as possible in a video's summary [21], modifying the loss functions and optimization steps [20], and adding a video decomposition stage, which breaks each video into smaller and non-overlapping chunks of consecutive frames with uniformly sampled strides before feeding them to the summarizer network [22]. Further improvements were made by adding a frame score refinement stage to the summarizer's output [23, 24]. This included the use of an attention module that progressively edits each frame's score based on current and previous frames [24], and the addition of an Actor-Critic model that adjusted the frames' scores in a non-sequential order based on past and future frames and previously made adjustments [23].\nThe advantage of using GAN for unsupervised video summarization is in its ability to generate more diverse, realistic summaries that match the input video content and style. However, there are also some drawbacks. One is the complexity of the training procedure as it involves multiple models with different objectives and losses [24]. Balancing and coordinating the training of these models to ensure their convergence and stability are challenging [28]. Other issues such as mode collapse, vanishing gradients, or oscillation could cause training instabilities [14, 29]. Also, another drawback is that GAN may not capture some important aspects of video summarization, such as temporal coherence [24]. Therefore, some approaches have used reinforcement learning with custom reward functions to overcome the above mentioned issues [14-17]. A custom reward function measures specific properties required in an optimal video summary, such as diversity, representativeness, smoothness, and sparsity. A two-part reward function called Diversity-Representativeness was suggested in [14] that measured diversity by examining differences between frames of the summarized video and representativeness by comparing the selected frames to the entire video. The aim was to train a model that created a summary of diverse and representative frames from different parts of the video. The use of Diversity-Representativeness became so prominent that it was even added to the optimization process of the summarizer in the GAN-based methods [16]. Alternatively, the use of graph neural networks was proposed as another way to avoid adversarial training [30]. This method built a graph representation of the input video and used the node feature magnitude to generate hard assignments for the summary selection.\nMany video summarization methods mentioned above use LSTM-based models and therefore could have problems such as vanishing and exploding gradients [31]. To address these challenges, some unsupervised video summarization methods have incorporated self-attention modules or transformer encoders [32] into their LSTM-based models [25-27, 32]. Others have exclusively utilized transformer encoders [17, 18]. These strategies primarily concentrate on substituting LSTM-based models with self-attention encoders. Despite these modifications, these methods continue to employ reward-based training, utilizing traditional rewards such as representativeness/diversity and length regularization cost [14, 20].\nOur training pipeline shares similarities with the reinforcement learning approach outlined in [14]. However, a crucial distinction lies in the utilization of a learned reward function instead of handcrafted ones. The core of the proposed learned reward function is a model that is trained during a self-supervised learning stage. This stage employs a novel dynamic window masking algorithm, which signifies our self-supervised learning stage from the one in [19]. Additional explanation regarding our methodology is presented in the following section."}, {"title": "III. APPROACH", "content": "Here, we introduce a novel method for generating and assessing video summaries using reinforcement learning that includes a learned reward function. Our method involves two models: a video generator and a video summarizer.\nThe input to the generator model is a masked video, in which some of the frames are masked (missing). The generator model takes this video summary as input and generates a reconstructed video. It learns to fill in for the missing frames by minimizing a reconstruction loss function that measures the similarity between the original and reconstructed frames.\nThe video summarizer model takes a video as input and generates importance scores for each frame. It then creates a video summary using frames' importance scores. To train the video summarizer, a video summary is first created by selecting the frames with the highest scores. Next, the summary video is passed to the generator for reconstruction. Finally, the reconstruction loss between the input video and its reconstruction is used to update the video summarizer. The video summarizer model learns to assign higher scores to frames that better represent the input video and contribute to a lower reconstruction loss. It is trained using reinforcement learning [33], where the reward is the sigmoid of the negative reconstruction loss.\nThe video summarizer model is built within an encoder-decomposition-summarizer-aggregation framework. In the following subsections, we describe each component of this framework and the training procedure.\n\u2022 Section III-A describes the encoding and decomposition steps, wherein the frames of the input video are converted into embeddings and the video is broken down into a smaller set of segments.\n\u2022 Section III-B outlines the architecture of the video generator and describes its training method.\n\u2022 Section III-C details the architecture of the summarizer and its training process.\n\u2022 Section III-D delineates the steps of the inference stage and the video summary generation pipeline.\nIn the rest of this paper, bold uppercase symbols like I"}, {"title": "A. Encoding and video segmentation", "content": "Fig. 1.A illustrates the workings of the encoder and decomposition stage. Consider a video comprising T frames denoted as F. The encoder, implemented as a Convolutional Neural Network (CNN), transforms the input video (frame sequence) F into the frame embeddings sequence E = {et} {_1, where each et \u2208 Rd is the embedding representation of the t-th frame. We employ GoogleNet [34] as the CNN model where the frame embeddings are the output of its penultimate layer. We opt for GoogleNet [34] to maintain consistency with most prior works [13, 18, 20\u201327] and to emphasize the impact of our algorithm on the results rather than the choice of the encoder. However, any arbitrary CNN such as [35] can be utilized in our proposed framework without loss of generality, as we do not make any specific assumption regarding the chosen feature encoder.\nAfter obtaining the frame embeddings, we utilize Kernel Temporal Segmentation (KTS) [36], a method that divides a video into segments with minimized internal variance, to extract shot boundaries within a video. Shots, in the context of video representation, represent continuous sequences of similar frames. In Fig. 1.A, some exemplar shots are shown, each with a different color, where within each shot, frames are sequentially numbered starting from 1 up to the end of the shot. As will be discussed in Section III-B, the obtained shots will be used in the next stage of the proposed approach.\nAfter obtaining the shot boundaries, each E is decomposed into a set of smaller video segments S; = {et}=1, where L is is the video sub-sequence length and j = 1 . . . J is the sub-sequence identifier. J is the total number of segments, which is dependent on the input video's length (T). Each video is divided into smaller segments using two methods: sequential split and dilated split. For sequential split, we select every L consecutive frame as one sub-sequence. During training, we randomly shift the starting point of each sub-sequence by A (a value within the range of \u00b1[0, L/2]) for each training epoch to enhance the diversity of the samples. During inference, we do not apply any shift. For dilated split, we sample segments of L frames with a variable dilation rate that depends on the input video length. We start by padding the vector E with zeros until its length is divisible by L. If n = [T/L], we then pick every n-th frame for each sub-sequence, starting from the first frame for the first sub-sequence, the second frame for the second sub-sequence, and so on."}, {"title": "B. Generator architecture and training", "content": "This section describes details of the proposed generator training stage. We refer to this stage as the self-supervised pre-training stage. The generator comprises a transformer encoder [32]. The input to the video generator is a masked video sub-sequence, Mj. The embedding of some frames in M; are masked, meaning they are replaced with a special fixed masked token embedding (m), which is filled with arbitrary values. The generator then tries to reconstruct the original embeddings at the masked frames using the embeddings of the non-masked frames to obtain a reconstructed video sub-sequence S. Fig. 2.A shows the generator's architecture.\nThe generator training stage is depicted in Fig. 1.B. This generator does not require ground-truth annotations and uses the input video as the ground truth. It is trained in a self-supervised manner using the following loss function:\n$\\L_{CE} = \\sum_{t=1}^{L} (1 - \\frac{e_t\\hat{e_t}}{|e_t|^2.|\\hat{e_t}|^2}),$\n$\\L_{L1} = \\frac{1}{L} \\sum_{t=1}^{L} |e_t - \\hat{e_t}|,$\n$\\L_{rec} = L_{CE} + L_{L1},$"}, {"title": "C. Summarizer's architecture and training", "content": "The summarizer model is composed of a transformer encoder, followed by a fully-connected (FC) layer with a sigmoid activation function. Fig. 2.B illustrates the summarizer's architecture. Here, the same encoder module as in the generator is utilized where its weights are initialized using the weights of the generator trained in Section III-B. Fig. 2.B illustrates the summarizer's architecture. The key distinction between the generator and the summarizer is the added FC layer that maps each d-dimensional frame embedding into a single frame score. The FC layer is initialized randomly.\nThe summeriser's transformer encoder accepts the frame embeddings Sj = {et}=1 as input and yields the hidden states {ht}=1 for each frame. These hidden states encapsulate the temporal dependencies and contextual information of the frames. The final FC layer then calculates a frame importance score or selection probability, pt, for each frame, signifying its relevance to the generated summary as follows:\n$p_t = \\sigma (h_t w_c),$\nwhere wsc \u2208 RHD represents the trainable weights of the FC layer. Essentially, wsc acts as a trainable parameter, related to a self-gating mechanism [37] that determines the importance of each frame.\nFig. 1.C illustrates the summarizer training process. The training process is as follows: the summarizer takes in the input video sub-sequence S; and generates {pt}t=1. A frame action sub-sequence {at}=1 is then generated by sampling each individual pt, as follows:\n$a_t \\sim Bernoulli (p_t),$\nwhere at \u2208 {0,1} indicates whether the t-th frame is selected or replaced with the masked token embedding (m). The summary Mj is defined as:\n$M_j = {e_t if a_t = 1 else m, t = 1, 2, . . . T}.$\nThe summary Mj is then passed to the video generator, which reconstructs the input video. The reconstruction loss in (1) between the reconstruction and the original is calculated and converted into a reward (Rs) using the following equation:\n$R_s = \\sigma(-L_{rec}).$\nWhich indicates that R, is equal to the sigmoid of the negative value of the reconstruction loss.\nDuring training, the goal of the summarizer is to increase Rs over time, which based on (5) happens when Lrec is minimized. In essence, the summarizer is trained to create summaries that enhance the quality of the video reconstruction, focusing on the similarity between the original and the reconstructed frame embeddings.\nMathematically, the summarization's objective is to learn a policy function [38], denoted as \u03c0\u03bf, with parameters 0. This"}, {"title": "D. Inference and summary generation", "content": "The inference stage is illustrated in Fig. 1.D. During this stage, the input frame embeddings E = {et}{=1 are first divided into multiple segments Sk = {et}=1, where Lis the new video length and k = 1... K is the sub-sequence identifier. This process is carried out using the operations described in Section III-A with \u25b3 set to 0. This parameter, \u25b3, was initially developed for training to introduce diversity in the training data. However, during inference, such diversification is unnecessary, hence the decision to set it to 0. Each Sk is then passed to the summarizer, which produces a frame score sub-sequence, Pk = {pt}=1, where each pt indicates the importance score of each frame. However, since the input video was decomposed into multiple overlapping segments, each frame receives multiple importance scores. We compute the final frame score ot for a single frame by calculating the average of these assigned scores for a single frame. The final output of this frame score generation algorithm is 0 = {ot}t=1, which is the sequence of all final frame scores. This pipeline is presented in Algorithm 1.\nWe set the summary length limit to 15%, which is a typical and commonly used number [9]. Most methods for generating summaries select the most informative shots from a video. The informativeness of a shot is calculated by averaging the scores of all its frames (shot-level score). The goal is to select as many high-scoring shots as possible without exceeding the summary length limit. This selection step can be considered as a binary Knapsack problem, which can be solved using dynamic programming [40]. The final video summary is the solution obtained from this process."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we present and discuss our experimental results, compare them with the current state-of-the-art methods, and conduct an ablation study. To ensure a fair comparison, we followed a widely accepted evaluation protocol, and used the same datasets and evaluation methods utilized by many leading approaches [13, 18, 20-27] in this field. Subsequent sections will provide detailed insights into this standardized procedure and a comprehensive presentation of our findings."}, {"title": "A. Datasets and the evaluation method", "content": "To evaluate the performance of our proposed method, we utilized two standard benchmark datasets: SumMe [41] and TVSum [42]. The TVSum dataset consisted of 50 videos ranging from 1 to 11 minutes in duration. These videos were annotated by 20 users, who assigned an importance score on the scale of 1 to 5 to each 2-second frame sub-sequence. Conversely, the SumMe dataset consisted of 25 videos with durations spanning 1 to 6 minutes. The annotation process was performed by 15 to 18 individuals who created a summary for each video by selecting key (the most important) shots within each video. These summaries had to be between 5% and 15% of the total video length."}, {"title": "B. Implementation setup", "content": "In line with the standard approach adopted by state-of-the-art (SOTA) unsupervised video summarization methods, we employed a pre-existing video feature extraction setup proposed by [46]. In this setup, the feature arrays were generated through a two-step process: first, the input videos were down-sampled to 2 frames per second (fps), and second, the 1024-dimensional output of GoogleNet's [34] penultimate layer was obtained for the sampled frames. During our proposed segmentation phase, videos were segmented into segments of L = 128 frames.\nThe architectural configuration of the proposed model was set as follows: The number of transformer encoder layers (l), the number of attention heads (h), and the hidden input dimension size h were set to 3, 8, and 1024, respectively. The feedforward layers of the encoder had an expansion factor of 4, resulting in a hidden-state-dimension size of 4096. The scoring layer was an FC layer with an input dimension size of 1024 (d) and an output dimension size of 1.\nThe initial training phase, known as self-supervised training, spanned 250 epochs with a batch size of 128. The optimization was carried out using the AdamW optimizer in conjunction with a Cosine learning rate scheduler. The scheduler included a warm-up period of 100 epochs, during which the learning rate linearly increased from 0 to 0.01. Subsequently, the learning rate followed a cosine wave pattern, gradually decreasing after the warm-up period, to reach zero by the 1000th epoch. However, in our experiments, training was completed at epoch 250, before the learning rate reached zero. The training parameters of this phase including DR (dynamic masking ratio) and MR (sub-sequence masking ratio) were set to 0.5 and 0.25, respectively.\nMoving on to the summarizer training stage, we conducted 300 epochs with a batch size of 16. The parameter N (number of episodes) was set to 5, and the learning rate was fixed at 0.00001, utilizing the AdamW optimizer. The checkpoint with the least reconstruction loss was retained as the final model. In this phase, the parameter 8 in (9) was set to 0.5. Additionally, B, the regularization loss coefficient, was set to 0.001.\nOur experiments were executed on a Compute Canada node equipped with an NVIDIA V100 Volta GPU, with 32G HBM2 memory."}, {"title": "C. Comparison against the state-of-the-art methods", "content": "In this section, we conduct a comparative analysis between the outcomes produced by our approach, referred to as Trained Reward Summarizer (TR-SUM), and the current state-of-the-art methods in unsupervised video summarization (SUM-GAN [13], Cycle-Sum [21], DR-DSN [14], SUM-GAN-AAE [20], SUM-GAN-sl [24], CSNet [22], RS-SUM [19], AC-SUM-GAN [23], CA-SUM [18]).\nThe results of this comparative analysis are summarized in Table II. In this table, previous works marked by * are methods in which a different regularization factor (\u03b4) was used for each dataset. In these cases, multiple models were trained with \u03b4 values ranging from 0.1 to 0.9 for each fold of both datasets. The highest average F-score achieved by a single d on all folds of a dataset is reported (for example, SumMe peaks at \u03b4 set to 0.8, while TVSum peaks at 0.7). Instances marked with ** represent methods that reported the average F-score achieved by different regularization factors for each fold (for example, for the first fold of SumMe, & was set to 0.3, while for the second fold, it was set to 0.6). In contrast, our method employed a constant & value of 0.5 for all folds across both datasets.\nThe results presented in Table II demonstrate that our method achieves the highest F-score on both datasets and ranks second in terms of pand \u03c4, trailing only behind CA-SUM [18]. However, it is important to note that in the case of CA-SUM [18], initially, for each data fold, five instances of a model with different & values (ranging from 0.5 to 0.9) were trained for 400 epochs. The network weights at each epoch for each d were saved as a checkpoint, resulting in a total of 2000 checkpoints for each of the 5 data folds. Subsequently, an algorithm was employed to select one checkpoint out of 2000 per fold. This selection process aimed to choose weights for each fold that would yield high pand values.\nThe previous leading method in unsupervised video summarization, i.e., RS-SUM [19], utilized a structure similar to our method. This structure was also based on transformer blocks and segmented videos into intervals of 128 frames. The computational complexity difference between TR-SUM and RS-SUM is detailed in Table III, which underscores the differences in terms of inference time and the number of MACs (Multiply-Accumulate Operations) per video sub-sequence (collected using the ptflops [47] package). This table indicates a significant performance advantage of TR-SUM over RS-SUM. Specifically, TR-SUM is 310 times faster than RS-SUM in analyzing a video sub-sequence. Moreover, TR-SUM has 20 times less computational complexity than RS-SUM. This highlights another advantage of our method over RS-SUM during the inference stage. RS-SUM uses an iterative algorithm to generate frame scores, necessitating multiple passes of each video through their model to achieve stable output frame scores. In contrast, our method requires a single"}, {"title": "D. Ablation Study", "content": "In this section, we perform an ablation study to investigate the impact of various parameters on the performance of the proposed model. We categorize our study into four subsections: Video decomposition, model configuration, self-supervised training, and summarizer training parameters. Table IV presents these parameters, and the sections they correspond to, and provides a brief description of each section.\nTo establish a base model for this work, we conducted exhaustive search experiments on key parameters: L, l, h, and the masking method. We set MR and the reconstruction loss function to values determined in the previous work [19], and fixed 8 and \u1e9e to values in between selected numbers. This exhaustive search experiment prioritized achieving a higher F-score over and p. After determining the base model, we conducted subsequent ablation studies, adjusting one parameter at a time while keeping the baseline values unchanged.\n1) Video decomposition: This section focuses solely on the parameter L, representing the length of the segments. The impact of changing L on the model's F-score is illustrated in Fig. 3. Selecting extremely high or low values for L results in a F-score decline. The figure suggests that the optimal point for attaining a satisfactory F-score on both datasets is around L = 128. Regarding p and \u03c4, an increase in sequence length appears to reduce these values, similar to the results we obtained for the F-score. Interestingly, p and also peak at L = 128, further reinforcing the significance of that value.\n2) Model configuration parameters: The two parameters investigated in this section are l (the number of layers) and h (the number of attention heads) of the transformer model. Table V shows the impact of these parameters on the performance of the proposed model. As seen from these results, increasing l from 1 to 3 enhances the model's F-score. However, for l >3, no specific trend is observed. While a single-layer transformer may not be sufficient to capture complex relationships, employing too many layers increases the risk of overfitting during the first training phase.\nRegarding h, the results suggest that an increase in h enhances the pand \u03c4. With I set to the optimal value of 3 and h to 8, we observed the highest performance, indicating a synergistic effect between these parameters.\n3) Self-supervised training parameters: One of the most crucial factors to discuss is the effect of the dynamic masking method on the quality of generated summaries, which stands out as one of the key innovations introduced by this paper. As an alternative to the dynamic shot masking method proposed here, one could consider randomly masking a selection of frames during self-supervised training or adopting the method proposed in RS-SUM. The latter utilizes a fixed-size window masking scheme within each shot, in contrast to our method, which employs windows with dynamic lengths that expand or shrink based on the shot length. Table VI presents the results of this comparison. In this table, Ws denotes the window size in the fixed window masking method. From these results, the dynamic masking method produces the best outcomes.\nAnother important factor is the impact of MR, representing the total masking ratio. This ratio is the proportion of the total selected masking frame candidates to the size of the entire sub-sequence. As illustrated in Fig. 4, the F-score of the model exhibits a relative maximum when MR = 0.25 on both benchmark datasets. This observation aligns with the findings reported in a previous work by [19].\nThe last parameter evaluated in this section is the reconstruction loss function defined in (1). In Table VII, we show the effect of using alternative reconstruction loss functions such as MSE. MSE and L1 appear to perform similarly; however, the combination of L1 with CE outperforms all single combinations, including the combination of MSE with CE loss function. This underscores the superiority of the proposed combination in achieving better results.\n4) Summarizer's training parameters: In this section, we explore the impact of the two key training parameters, namely 8 and \u03b2, as defined in (9) and (10). As \u1e9e is increased, we anticipate a decline in the model's performance. This is due to the loss being dominated by the regularization term, thereby neglecting the effect of the rewards. Alternatively, if \u1e9e is set too low, causing the reward becomes the dominating factor,"}, {"title": "E. Qualitative visual analysis of the generated summaries", "content": "In this section, we provide visual samples to illustrate the effectiveness of our method. Figs. 7 and 8 display a summary of the suggested annotation by our method and the human observer. Each figure consists of two sections: \"Human\" and \"TR-SUM\". Within each section, the bar plots show the normalized frame scores in blue. The alternating white and grey background color indicates the start or end of a new shot. The segments colored in green are the shots selected for the summary. The highest scoring frame within each green segment is marked in red. These frames are stitched together horizontally and displayed in the second row below the bar plot. For the \"Human\" section, the annotation with the highest F-score when compared against the rest of the annotations is the one displayed. The 'TR-SUM' section shows the frame scores and summary generated by our algorithm.\nFigs. 7 and 8 demonstrate a noticeable visual correspondence between the key frames chosen by the human annotator and those by our model. It is evident that our algorithm's highest scoring frames bear a striking visual resemblance to the high scoring frames selected by the human annotator. This suggests that our method of relying on reconstruction loss to identify representative frames within a video aligns well with human judgment. Moreover, these figures reveal a significant correlation between the frame scores generated by our algorithm and the human annotator. This is evident in the matching peaks and troughs between the frame scores generated by the human and our model. This indicates that the high F-score values of our algorithm are not a result of random scores that merely produce a good F-score in combination with the Knapsack selection. Instead, it is a testament to the robustness and precision of our proposed algorithm.\nUpon examining the frame score patterns of our algorithm's output, it is clear that long static shots, such as those at the beginning or end of a video, exhibit identifiable patterns. These sections will be filtered out and not selected by the Knapsack"}, {"title": "V. CONCLUSION", "content": "In conclusion, this paper introduced a unique, unsupervised approach to video summarization using reinforcement learning. The proposed method leverages a learnable pipeline to generate rewards for the reinforcement algorithm, a departure from previous methods that relied on manual reward functions. The pipeline uses a trained video generator to transform a partially masked video into a complete video by reconstructing the masked frames. The reward is then derived from the similarity rate between the reconstructed and input videos. This process is predicated on the notion that an informative summary will yield a reconstruction closely resembling the input video. The video generator, trained through a self-supervised learning stage, also serves as a pre-training stage for the summarizer. In the inference stage, the summarizer alone is used to generate frame scores and, subsequently, a video summary. Experimental results on the TVSum and SumMe datasets demonstrate the effectiveness of our method by achieving an F-score of 62.3 and 54.5 respectively, thereby outperforming existing methods. This underscores the potential of our approach in producing high-quality video summaries and opens up new avenues for future research in this domain."}]}