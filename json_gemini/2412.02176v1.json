{"title": "Self-Supervised Learning-Based Path Planning and Obstacle Avoidance Using PPO and B-Splines in Unknown Environments", "authors": ["Shahab Shokouhi", "Oguzhan Oruc", "May-Win Thein"], "abstract": "This paper introduces SmartBSP, an advanced self- supervised learning framework for real-time path planning and obstacle avoidance in autonomous robotics navigating through complex environments. The proposed system integrates Proximal Policy Optimization (PPO) with Convolutional Neural Networks (CNN) and Actor-Critic architecture to process limited LIDAR inputs and compute spatial decision-making probabilities. The robot's perceptual field is discretized into a grid format, which the CNN analyzes to produce a spatial probability distribution. During the training process a nuanced cost function is minimized that accounts for path curvature, endpoint proximity, and obstacle avoidance. Simulations results in different scenarios validate the algorithm's resilience and adaptability across diverse operational scenarios. Subsequently, Real-time experiments, employing the Robot Operating System (ROS), were carried out to assess the efficacy of the proposed algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous navigation and environment mapping for au- tonomous vehicles has garnered considerable attention in the field of robotics and autonomous systems research [1] [2]. More specifically, path planning methodologies are divided into two primary categories: one focusing on navigation within entirely unknown environments [3], where we only have access to real-time range sensor data, and the other dedicated to environments that are either partially or completely known [4]. The interest in path planning in unknown environments is driven by the critical need for autonomous vehicles to operate effectively in situations where prior knowledge of the environment is absent or severely limited. Examples of such applications include extraterrestrial rover missions on planets like Mars, where the terrain is largely uncharted, and search-and-rescue operations in disaster-stricken areas, where rapid changes in the environment preclude the use of pre-existing maps.\nAdditionally, the advent of Artificial Intelligence (AI) has revolutionized the way researchers tackle the path planning problem [5], [6]. By harnessing the power of AI, scholars are now able to construct neural networks that not only learn from simulated and real data but also exhibit human-like expertise in decision-making processes [7]. However, the lack of access to useful data makes training neural networks offline for path planning challenging. In such situations, self-supervised learning methods have proven to be effective [8]. Nevertheless, learning on real-time experimental data can be time-consuming and sometimes unsafe [9]. To ensure safe learning, Proximal Policy Optimization (PPO) [10] has been employed to train the path planner offline. While PPO is widely known as a method in Reinforcement Learning (RL), its application in this paper is slightly different. This study assumes that there is no transition between states (observations), meaning the impact of decisions made at any time step on future events is ignored.\nWhile the safety of the path plays a major role in the path planning task, the smoothness of the path is equally important. Due to the low maneuverability of some vehicles, paths with high curvature or sudden changes in yaw angle are not feasible. In recent years, B-splines have been employed in numerous studies to minimize the curvature of the path [11]\u2013 [13]. While these studies have made notable advancements in incorporating curvature and smoothness into path planning, the computational demand and possible numerical instability of their methods especially when dealing with non-convex obstacles-present significant challenges for real-time imple- mentation.\nThe method proposed in this paper incrementally builds the path based on new information received from the LIDAR sensor. The main objective of the algorithm is to find the optimal positions of control points for a local B-spline that guides the vehicle toward the target while avoiding obstacles and minimizing curvature. The contributions of this paper to the field of path planning include the following:\n\u2022 The proposed path planning algorithm is tailored for com- pletely unknown environments, capable of overcoming various non-convex obstacles.\n\u2022 The method introduced here explicitly incorporates path smoothness."}, {"title": "III. METHODOLOGY", "content": "The primary objective of the proposed path planning is finding the optimal control points of B-splines, similar to [11], but diverges from the author's previous works by integrating a novel approach that serves two purposes: 1) it accelerates the optimization process in real-time and 2) it is capable of addressing many non-convex obstacles in the environment. Central to this methodology is the consideration of both cur- vature and the spatial distance of the final point of each local B-spline to the target, while concurrently ensuring avoidance of detected obstacles within the sensor range.\nDistinctive to this study is the initial transformation of the perceived point cloud into an n\u00d7n circular grid representation, which is depicted in Fig. 1 (a). Grid cells that have more than m points are considered as obstacles. In this paper, a 5 \u00d7 5 grid configuration has been employed, comprising 5 angular intervals and 5 radial intervals. The transformation emulates the configuration of contemporary range sensors commonly deployed in autonomous vehicle platforms. This preprocessing step enhances the method's applicability, rendering it more congruent with real-world scenarios.\nSubsequently, the circular grid undergoes a conversion pro- cess to a square grid format (Fig. 1 (b)). This conversion facili- tates seamless integration with a convolutional neural network (CNN), a pivotal component of the proposed methodology. The CNN operates on the square grid input and outputs the probability distribution governing the selection of each control point for the B-spline trajectory. Notably, it is imperative to acknowledge that within this transformation framework, the angular coordinate of the circular grid corresponds to the y coordinates in the square coordinate system, whereas the radial coordinate aligns with the x coordinates. The rest of this section explains the main components of the proposed method."}, {"title": "A. Environment", "content": "In the context of an self-supervised learning problem, the environment serves as a component that processes inputs or actions alongside current states, yielding subsequent rewards or costs. Formally, we define the environment as E = (C, S), where S represents the observed point cloud (state) and C signifies the cost function. It is noteworthy that the choice between reward maximization and cost minimization is arbi- trary, as maximizing reward can be equivalently framed as minimizing cost. Additionally, it is assumed that there is no transition between states, meaning that the learning problem focuses on improving the outcome by adjusting the actions (control points of the B-spline) for each given 5 \u00d7 5 grid configuration.\nThe radial coordinates of the control points are predeter- mined, comprising n + 1 radial coordinates incrementally increasing from zero to the sensor range, inclusive of the origin which signifies the location of the robot at each time step. Since the location of the first and second control points is predetermined, as shown in Fig. 1 (a), the learning algorithm is assigned the responsibility of choosing among n 1 dis- tinct angular coordinates (actions) for the remaining control points. The first and second control points are always at fixed positions relative to the robot to mitigate abrupt changes in the desired yaw angle. Therefore, the set of all control points (actions) A can be written as:\n$A = \\{A_1, A_2, ..., A_{n+1}\\}$\nControl points A3, ..., An+1 are the geometric centers of the chosen cell grids. Given A, the cost of the B-spline can be calculated using the following formula:\n$C(S, A) = \\rho_1C_{dist} + \\rho_2C_{curv} + \\rho_3C_{obs}$ (1)\nIn this equation, $\\rho_i$, where i = 1,2,3, are the weights, $C_{dist}$ represents the cost imposed due to the L2 norm between the final point of the B-spline and the final target:\n$C_{dist}(A) = ||A_{n+1} - G_{final}||_2$ (2)\nwhere $G_{final}$ is the target location in xy-plane. The term $C_{curv}$ denotes the cost imposed by the overall curvature of the B- spline, and $C_{obs}$ denotes the cost imposed due to collision with obstacles. In this paper, we define the cost imposed by the overall curvature of B-spline as\n$C_{curv}(A) = \\int_{A_1}^{A_{n+1}} k(\\beta) d\\beta$ (3)\nwhere $$\\beta$$ is any point on the B-spline and k($$\\beta$$) is defined at each point as\n$k(\\beta) = \\frac{y''(\\beta)}{(1+y'(\\beta)^2)^{3/2}}$ (4)\nNote that all the derivatives in (4) are with respect to $$\\beta$$. Furthermore, $C_{obs}$ is\n$C_{obs}(S, A) = \\begin{cases} 1 & \\text{if collision = TRUE} \\\\ 0 & \\text{otherwise} \\end{cases}$ (5)"}, {"title": "B. Proximal Policy Optimization", "content": "Before delving into the intricacies of Proximal Policy Op- timization (PPO) [10], the framework of the self-supervised learning problem can be established as follows: (S, A, C) is defined where S denotes the current state, representing the observed circle segment captured by the range sensor; A = ($$A_1$$, ..., $$A_{n+1}$$) represents the set of n + 1 actions, i.e., control points, selected at the current state; C signifies the cost associated with choosing A at S, calculated via (1). Note that each $$A_i$$, i = 3, ..., n + 1, is sampled from a discrete distribution over n possible outcomes.\nPPO can be characterized as a learning method that learns by increasing the probability of selecting actions that out- perform the average. The primary distinction between PPO and other reinforcement learning methods is its endeavor to restrict the steps taken in gradient descent to mitigate the risk of local optima. PPO revolutionizes policy optimization by introducing two pivotal innovations: the Clipped Surrogate Objective (CSO) and the utilization of multiple epochs of stochastic gradient ascent for each policy update. Here, the policy is defined as\n$\\pi_\\theta(a|S) = P(A = a|S)$ (6)\nwhere $$\\theta$$ are the parameters of the neural network and a is the sampled action vector. Since each B-spline consists of n 1 different actions plus the first and second points, which are fixed, the probability of choosing each specific B-spline or path is as follows:\n$\\pi_\\theta(a|S) = \\prod_{i=3}^{n+1} \\pi_\\theta(a_i|S)$ (7)\nor in the log form\n$log \\pi_\\theta(a|S) = \\sum_{i=3}^{n+1}log  \\pi_\\theta(a_i|S)$ (8)\nPPO utilizes the ratio of the probability to constrain the policy update. The ratio is defined as\n$r(\\theta) = \\frac{\\pi(a|S)}{\\pi_{old}(a|S)}$\nThis is achieved through CSO function:\n$L_{clip} (\\theta) = [min (r(\\theta)\\hat{A}, clip(r(\\theta), 1 \u2013 \\epsilon,1 + \\epsilon)\\hat{A})]$ (9)\nHere, the hyperparameter $\\epsilon$ is typically set to a specific value, for instance, $$\\epsilon$$ = 0.2 [10], and $$\\hat{A}$$ represents the advantage function, distinct from the action vector A:\n$\\hat{A} = C(S, a) \u2013 E_{a \\in A}[C(S, a)]$ (10)\nNote that r($$\\theta$$) exceeds 1 when the action is more probable for the current policy than for the old policy; it will range between 0 and 1 when the action is less probable for the current policy than for the old one. The advantage function evaluates the quality of the sampled actions relative to the expected cost, which should be estimated using the critic"}, {"title": "C. Actor-Critic Architecture", "content": "The dual-component setup of actor-critic architecture en- ables it to efficiently learn complex policies in environments with high-dimensional state and action spaces, making it well-suited for a wide range of learning tasks. The overall configuration of the AC architecture used in this paper is shown in Fig. 2."}, {"title": "D. Target Generalization", "content": "One of the primary challenges encountered when applying CNN-based methods for path planning tasks is the incorpora- tion of the target location into the network. Due to the limited field of view of the robot, achieving a globally optimized solution is not feasible. However, akin to how humans confront this dilemma in real-life scenarios, the robot selects a goal location within its field of view. In this approach, we calculate the distance between the final target and all five potential final points of the B-spline. Subsequently, we select the point closest to the final target, referred to as the normalized target in this study (Fig. 5). The set containing all five normalized targets is:\n$G = \\{G_1, G_2, ..., G_5\\}$\nand the final target is denoted as $G_{final}$.\nMoreover, we train five distinct actor networks, each tar- geting a different potential normalized target. In other words, the $C_{dist}$ component of (1) represents the distance between the final point of the path and the normalized target. Subsequently, the network associated with the normalized target suggests a path. However, when the path to the normalized target location is obstructed, the actor fails to propose a collision-free path. In such instances, we assume that the robot has encountered either a wall or a non-convex obstacle. Although the proposed method does not offer a path for all non-convex obstacles, to address such scenarios, we shift the normalized target from the closest one to the final target to the one with the minimum cost calculated by\n$C_i(S, A) = \\rho_2C_{curv} + \\rho_3C_{obs}$ (12)\nreferred to as the temporary target $G_{temp}$. Here, $C_i$, i ="}, {"title": "IV. SIMULATION RESULTS", "content": "To train each of the five aforementioned networks, 10,000 5 \u00d7 5 grid samples with randomly distributed obstacles are generated. The training hyperparameters are presented in Ta- ble I.\nTable II represents the success rates of the trained networks. Here, success is defined as the ability to avoid all obstacles present in the local grid.\nIt is needless to say that since grids are occupied by ones and zeros, causing symmetries to occur in many samples, choosing the appropriate seed plays an important role in the success rate of trained networks."}, {"title": "A. Scenario 1: Environments with multiple small obstacles", "content": "In this scenario, the environment is populated with obstacles of uniform size, randomly positioned throughout. The vehicle starts from the origin and navigates towards the target piece by piece, relying on its limited field of view. The simulation concludes when the robot's distance to the target is less than 3 m. Fig. 6 illustrates how the robot can overcome non-convex obstacles. It is important to note that the level of non-convexity significantly impacts the algorithm's performance. Specifically, if addressing non-convex situations requires information on past observed environments and decisions, the algorithm may fail. However, the authors have only utilized environments lacking such complexities."}, {"title": "B. Scenario 2: Passing wall-like obstacles", "content": "Passing a wall-like obstacle without knowledge of its end- point poses a significant challenge. When the vehicle encoun- ters the wall, it must decide whether to turn right or left. Given the limited field of view, making an optimal decision regarding the distance traveled by the vehicle becomes impossible. Here, the assumption is made that the wall is short, meaning that turning right or left would not significantly impact the solution. Fig. 7 illustrates the performance of the algorithm in such scenarios."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "To test the algorithm performance in real situations, the JetRacer platform has been employed. The JetRacer robot, shown in Fig. 8, is a small-scale, autonomous vehicle designed for experimentation and research purposes. Equipped with a variety of sensors and a Jetson Nano onboard computer, the JetRacer is capable of executing complex navigation tasks in dynamic environments.\nTo estimate its location, the JetRacer utilizes a combination of an Inertial Measurement Unit (IMU) and wheel odometry, which provides sufficient precision for the purposes outlined in this paper. The estimated location is then passed through an Extended Kalman Filter to reduce noise and increase the accuracy of odometry. Additionally, the JetRacer platform is equipped with a 360\u00b0 LIDAR sensor to detect obstacles. The LIDAR range is 12m, but to mimic the algorithm used in"}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduced SmartBSP, an AI-based path planning approach designed for autonomous robotics in com- plex and unknown environments. By leveraging Proximal Policy Optimization (PPO) combined with Convolutional Neu- ral Networks (CNNs) and an Actor-Critic architecture, our method effectively processes limited LIDAR inputs to generate optimal paths in real-time. The integration of B-spline curves and a nuanced cost function accounting for path curvature, endpoint proximity, and obstacle avoidance further enhances the robustness and adaptability of the system.\nThrough extensive simulations and real-world experiments, we demonstrated that the proposed method can handle various challenging scenarios, including environments with multiple small obstacles and wall-like obstacles. Our results show that adjusting the field of view and sensor range signifi- cantly impacts the success rate of the vehicle, highlighting the importance of these parameters in path planning tasks. Moreover, the flexibility of the SmartBSP approach allows for easy adaptation to different environments without the need for retraining the networks, showcasing its potential for practical applications in diverse operational scenarios.\nFuture work will focus on further refining the algorithm to handle more complex non-convex obstacles and exploring the integration of additional sensor modalities to enhance the per- ception capabilities of the autonomous system. Additionally, we aim to apply the SmartBSP framework to other types of autonomous vehicles and investigate its performance in large- scale outdoor environments."}]}