{"title": "Regressor-Guided Image Editing Regulates Emotional Response to Reduce Online Engagement", "authors": ["CHRISTOPH GEBHARDT", "ROBIN WILLARDT", "SEYEDMORTEZA SADAT", "CHIH-WEI NING", "ANDREAS BROMBACH", "JIE SONG", "OTMAR HILLIGES", "CHRISTIAN HOLZ"], "abstract": "Emotions are known to mediate the relationship between users' content consumption and their online engagement, with heightened emotional intensity leading to increased engagement. Building on this insight, we propose three regressor-guided image editing approaches aimed at diminishing the emotional impact of images. These include (i) a parameter optimization approach based on global image transformations known to influence emotions, (ii) an optimization approach targeting the style latent space of a generative adversarial network, and (iii) a diffusion-based approach employing classifier guidance and classifier-free guidance.\nOur findings demonstrate that approaches can effectively alter the emotional properties of images while maintaining high visual quality. Optimization-based methods primarily adjust low-level properties like color hues and brightness, whereas the diffusion-based approach introduces semantic changes, such as altering appearance or facial expressions. Notably, results from a behavioral study reveal that only the diffusion-based approach successfully elicits changes in viewers' emotional responses while preserving high perceived image quality. In future work, we will investigate the impact of these image adaptations on internet user behavior.", "sections": [{"title": "1 INTRODUCTION", "content": "The internet stands as one of the most significant inventions of the 20th century. Among other benefits, it democratizes access to information and facilitates unprecedented ease of communication. However, its use also yields adverse effects for users. It serves as a catalyst for addictive behaviors, for instance, tied to social media [2, 5], online gaming [112], or pornography [68]. Social media, in particular, often triggers negative emotions like dissatisfaction, envy [33], and anxiety [2]. These emotional responses can spiral into severe mental health issues such as depression [65], suicidal thoughts [2], and anorexia [106], particularly among younger users. Moreover, users consider the internet as a distraction affecting work performance [46] and well-being [20], leading to a desire to reduce their time spent online [72].\nTo support users in managing their internet use, various tools have been introduced through research [57, 59, 70], apps [38, 43, 102], or mobile operating systems [6]. However, this technology faces a significant limitation: The restrictions imposed by such tools are overly strict, which can lead to psychological reactance and prompt users to circumvent them [74]. Consequently, researchers suggest that effective interventions should be enforcing enough to change users' behavior without feeling too coercive [72].\nConcurrently, marketing research underscores the pivotal role of emotions in shaping online behavior. Emotional experiences characterized by positive valence [40] and, to an even greater extent, heightened arousal levels [8, 84], have been shown to boost online engagement. It was also revealed that users' emotional states are influenced by the content they consume, indicating that emotions play a mediating role between content and online engagement [101, 113].\nOn the internet, users predominantly consume image-based content. Psychological research has highlighted how image features such as saturation or brightness can impact viewers' emotional responses [7, 63, 95]. Studies in computer vision have built upon this insight, proposing algorithms capable of manipulating images to change the emotional response of viewers [23, 88, 120].\nBased on these findings, we propose generative approaches for image adaptation that regulate the emotional response of an image to decrease the time users spent online and hence facilitate a more healthy internet use. Specifically, we present three different generative image adaptation algorithms. The first approach optimizes the low-level properties of an image (e.g., brightness, contrast) to change its emotional impact. The chosen image properties are theoretically grounded, drawing from previous findings that have established their impact on eliciting emotional responses in individuals [7, 95]. The second approach uses a generative adversarial network (GAN) to disentangle content and style of an image [53] and then optimizes the style to evoke a subdued emotional response. The third approach uses a diffusion model (DM) as backbone and leverages classifier guidance (CG) as well as classifier-free guidance (CFG), to steer the denoising process of a previously inverted image to generate a similar image with reduced emotional impact.\nAll methods maximize the shift in the emotional impact of an image while preserving its quality and alignment to the original content. They are the first to achieve emotional image-to-image adaptation guided solely by the signal from a regressor as the information source. This regressor provides the fine-grained conditional control essential for our use case, driving images toward eliciting emotionally neutral responses. Furthermore, we are the first to demonstrate emotional image adaptation using a diffusion model, introducing classifier guidance as a tool for diffusion-based image-to-image adaptation.\nOur analysis demonstrates that the proposed algorithms successfully adapt images to align more closely with the emotional reference than baseline approaches while preserving a high visual quality. Further evaluations indicate that our approaches achieve emotional property changes consistent with prior research, with notable alignment for contrast, colorfulness, and blur (other properties varied). Furthermore, all approaches successfully introduce meaningful changes"}, {"title": "2 RELATED WORK", "content": "Our project draws on the understanding of how image properties influence viewers' emotional responses and how these emotions, in turn, shape online behavior. Additionally, it relates to generative models for image-to-image adaptation and computational methods designed to modify images in order to modulate viewers' emotional reactions. As our goal is to help users constrain their internet use, it is also related to digital self-control tools."}, {"title": "2.1 The impact of image properties on emotional response", "content": "Although the personal semantic meaning of an image tends to have the strongest influence on emotional responses [54, 89], substantial evidence shows that certain image properties can also affect perceived emotions of viewers. To assess these emotional responses, researchers commonly use the Circumplex Model of Affect, which organizes emotions along two dimensions: valence, indicating the pleasantness or unpleasantness of an emotion, and arousal, representing the intensity of that emotional experience [98]. Using the terminology of this model, we will summarize key findings on how various image properties influence human emotions. Our research builds on these findings by using them to inform our computational approaches and as metrics for evaluating the generated images."}, {"title": "2.1.1 Color and light properties", "content": "Extensive research has explored how attributes related to color and brightness influence the emotional tone of the image. Brightness is generally associated with positive emotional responses and due to that also consistently preferred across various types of stimuli [105]. It has also been recognized as a universal factor influencing emotion across different cultures [39].\nIn terms of color, studies have shown that colored images, as opposed to black-and-white images, tend to evoke stronger emotional reactions. For instance, it was found that color enhances emotional valence, as measured by brain responses [21]. It was further demonstrated that unpleasant scenes presented in color are perceived as more emotionally negative and slightly more arousing compared to their grayscale counterparts [7].\nConsistent with these findings, higher saturation levels are generally linked to more positive valence ratings. Highly saturated images not only enhance emotional valence but are generally preferred and capture greater attention, which"}, {"title": "2.1.2 Structural and spatial properties", "content": "Research has also explored how the structural and spatial aspects of images influence viewers' emotional responses. Greater variability in edge orientations correlates with increased arousal across several commonly used affective image datasets [95]. Similarly, smooth edges positively impact valence, as images with angular contours are rated more negatively, while those with long, smooth contours are rated more positively [28]. Images with predominantly low-frequency changes in color and greyscale channels, which corresponds to broader, more gradual transitions and larger shapes, tend to evoke higher arousal, regardless of whether the image is pleasant or unpleasant [34]. Additionally, reducing resolution or introducing blur dampens both valence and arousal compared to sharper, intact images [32].\nSymmetry, which is considered a super-principle of beauty, is generally associated with positive valence and arousal [10\u201312, 15, 75]. However, other studies found that while symmetry ratings correlate with valence and arousal ratings, it may be linked to lower valence ratings [95]."}, {"title": "2.1.3 Content", "content": "It is obvious that also the content depicted in an image strongly influences viewers' emotional responses. For example, nudity has been shown to increase arousal [85], while viewing attractive faces consistently elicits positive emotions [1, 42, 48, 108]. Additionally, complexity is strongly linked to heightened arousal, as more complex images offer a variety of informational cues that capture attention and stimulate greater mental energy [9].\nMoreover, the personal semantic meaning of an image has been found to have the most significant impact on emotional responses [54, 89]. This finding aligns with appraisal theory, which suggests that individual goals and desires influence emotional reactions to external objects and events [100]."}, {"title": "2.2 The role of emotions in online engagement", "content": "Research in marketing and psychology has explored how content influences emotions and, in turn, how these emotions affect online engagement. Studies have shown that engaging with online discussions increases emotional arousal, with higher participation rates observed when users experience positive valence [40]. In the context of brand posts, it was found that high arousal or positive valence tend to boost user engagement [113].\nOn the other hand, research has found that emotions of negative valence, such as anger and disgust, are the driving factors behind the rapid diffusion of false news [109]. Other work found that anger and disgust facilitate the spread of moral content due to their high-arousal nature, as low-arousal emotions of negative valence, like sadness, tend to decrease diffusion rates [17].\nIt was also shown that high arousal content, regardless of its valence, drives virality [8]. Similarly, studies indicate that videos eliciting high arousal are more likely to be shared, while differences in valence have minimal impact [84]. A meta-review of these and similar findings concluded that emotional arousal tends to enhance engagement behavior, while emotional valence interacts as a facilitator, modulating the extent of engagement [101].\nIn summary, these findings suggest that content which polarizes valence and elevates emotional arousal tends to increase online engagement levels. This aligns with the strong U-shaped correlation observed between valence and arousal in emotional databases [29, 62, 75], where extremes in valence often correspond to heightened arousal (see"}, {"title": "2.3 Generative models for image-to-image adaptation", "content": "Generative adverserial networks have demonstrated their capability to induce controlled changes to images across different domains [24, 25, 119]. These models have been used to adapt images in domains such as beauty [36], visual salience [78], or facial expressions [30]. Similarly, other GAN-based approaches have focused on disentangling content and style, enabling effective style translation across different content domains [53, 64].\nRecently, diffusion models have surpassed GANs in terms of image synthesis quality [35, 50, 97]. These models can be fine-tuned for image-to-image translation by incorporating additional conditioning mechanisms [110, 114], enabling powerful tools like textual image editing [19].\nSeveral works have approached image-to-image translation using DMs by guiding the inference process through a loss function applied between the generated image and the source image's representation at the same denoising step [99, 116]. Style transfer was achieved by adding noise to an input image and resampling it while conditioning the diffusion model on the reference image's style [115]. This was done by feeding the reference image's CLIP embeddings into the text-based attention mechanism guiding the diffusion process.\nMost relevant to our work is a method by Mokady et al. [81]. It employs an inversion scheme for a DM scheduler [35, 104] to map images into the latent space and then optimizes the DM's null-text embedding to preserve similarity with the original image while keeping it editable within the diffusion framework.\nBuilding on this approach, we invert images using null-text optimization to generate a noisy latent vector. We then resample the image, conditioning the diffusion model with an image caption using classifier-free guidance [51] to preserve a close resemblance to the original, while employing classifier guidance [35] to ensure the resampled image meets the criteria for neutral valence and low arousal."}, {"title": "2.4 Computational approaches to emotional image adaptation", "content": "Various approaches have been applied to adapt the emotional response that images elicit in viewers.\nSeveral studies have explored the use of non-photorealistic rendering algorithms that produce stylized images to alter emotional responses [13, 83]. These works found that such algorithms tend to shift intense emotions toward a more neutral state, often due to the introduction of confusion or the loss of detail [83]. Additionally, these techniques have been shown to reduce the repulsiveness of images perceived as disgusting [13].\nEarly efforts in computer vision use neural networks to identify the distribution of emotions an image elicits and employ standard image transformations to change its affect towards a target emotional state [3, 88].\nAnother method selects a reference image based on emotional vocabulary and then optimizes the latent vectors in intermediate layers of a neural network to align an input image with this reference [4].\nVarious works in computer vision also used GANs to adapt the affect of images. For instance, researchers used GANs to adapt source domain images so their emotional probability distributions aligned with the target domain, enabling emotional classifiers to be applied across different datasets [117, 118]. Similarly, GAN-based style transfer techniques have been used to apply the style of an emotional reference image to an input, either globally [120] or at the object level via semantic segmentation, using multiple reference images for different objects in the scene [23].\nWhile the approaches presented above used reference images to represent a target emotional state, other works rely on emotional classifiers or regressors to generate or adapt images based on specified sentiments. For instance,"}, {"title": "2.5 Digital self-control tools", "content": "Digital self-control tools (DSCTs) are applications that help users to follow their own long-term goals by avoiding digital distractions and unwanted habitual use. These tools mostly focus on increasing productivity or reclaiming their free time [38, 43, 102]. Specialized DSCTS for parental control [92] or to avoid porn [26] are also available. Tools employ a wide range of strategies to increase self-control, e.g., hiding content on distracting websites [55], limiting functionality [80], gamification [37], or visualize time spent online [6].\nHuman-computer interaction (HCI) research (e.g., [60, 67, 73, 82, 90, 107]) has also investigated design patterns that help users regulate internet use, e.g., blocking distractions [57], reminding users about their long-term goals [59]. Most similar to our research, is work that leverage implicit input manipulation techniques to inhibit the natural execution of common user gestures on mobile devices to reduce the usage time [70].\nHowever, prior work and applications mostly rely on simple decision heuristics to constrain digital media use. This often results in overly severe restrictions (e.g., turning off internet access) that trigger psychological reactance [18, 71] and, hence, are regularly circumvented [74]. This is why researchers advocate for the development of intervention mechanisms that are enforcing enough to change users' behavior without feeling too coercive [72].\nAnswering this call, we developed generative models that adapt images to reduce their elicited emotional intensity while ensuring a high image quality and fidelity to the originals. With this dual objective, our image adaptations have the potential to decrease the time users spent online without being overly intrusive."}, {"title": "3 OVERVIEW: EMOTION-REGULATING IMAGE ADAPTATION", "content": "The goal of this work is to assist users in reducing their internet consumption by regulating the emotional impact of online images. Prior studies have shown that emotionally charged experiences, characterized by polarized valence and high arousal, increase online engagement [8, 40, 84]. Moreover, computational image adaptation techniques can alter the emotions evoked in viewers [13, 83]. Concurrently, research on technologies designed to help users limit their internet use indicates that successful interventions must be sufficiently enforcing to change behavior without being overly coercive, thereby avoiding psychological reactance [72]. In the context of image adaptations, this means that the adapted images should maintain a high enough quality so that the changes are not overly pronounced or unpleasant for viewers. Additionally, we require that adaptations do not alter the image content to such an extent that the user perceives different information than what was originally presented.\nBuilding on these insights and requirements, we establish four criteria for our image adaptation techniques (referred to as criteria in the following):\nApproaches should (i) neutralize the valence and (ii) lower the arousal of images while (iii) preserving image quality and (iv) alignment to the original."}, {"title": "4 METHOD", "content": "In this paper, we propose three methods to adapt images based on the outlined criteria, each varying in the degree of image modification they allow. All approaches learn adjustments using the signal of a regressor that predicts the emotions an image elicits. In the following, we first explain how we train the regressor and then detail the implementation of each approach."}, {"title": "4.1 Emotion regressor", "content": "We train the emotion regressor by fine-tuning a pre-trained ResNet50 model (ImageNetV2) on a custom dataset composed of multiple emotional databases. Specifically, we integrate NAPS [75], DIRTI [47], CGnA10766 [56], EmoMadrid [22], Emotion6 [88], GAPED [29], OASIS [61], OLAF [79], MAPS [45], and IAPS [62], creating a dataset of 20,460 images. We normalize valence and arousal ratings across databases to a unified scale from 0 to 1. In the appendix, we provide a figure illustrating the positions of these images within the valence-arousal space of the Circumplex Model (see Appendix A). Our regressor achieves a mean absolute error of 0.11 for valence and 0.12 for arousal on a validation split.\nIn alignment with criteria and considering the range of ratings in our dataset, we set the emotional reference for the adaptation approaches to a valence value of 0.5 and an arousal value of 0."}, {"title": "4.2 Parametric optimization", "content": "This approach optimizes parameters for differentiable image transformations to modulate the emotion response an image evokes. The advantage of this approach is its ability to adapt images of various resolutions, while the inferred parameters can also be seamlessly applied to multiple sequential frames, such as in a video [78]. By applying these global transformations, it is intentionally designed to change only the image's low-level properties, ensuring that the depicted content remains unchanged. To ground the method theoretically, the model adapts image properties that strongly affect emotional responses (see Sec. 2.1). Specifically, we apply global image transformations that adjust the following properties: brightness (via exposure adjustments), saturation (using tone curve adjustments [78]), color (through color curve adjustments [52]), contrast, frequency of color channel changes (through sharpening and blurring), blurriness, and symmetry (through translation and scaling).\nTo satisfy criteria, we design the objective function to leverage the emotion regressor, facilitating image adaptation according to emotional reference. Additionally, we define a loss on the CLIP vectors of the input and output images to ensure their semantic similarity. CLIP is a multimodal model that learns visual concepts from natural language supervision, producing a latent space that closely aligns images depicting similar semantic concepts [93]. Figure 2 illustrates the approach.\nSpecifically, the objective function of the parametric optimization is defined as:\narg min _{p \\in P} w_1 L_{CLIP} (I,T(I, p)) + w_2 ||[v', a'] \u2013 R(T(I,p))||   (1)"}, {"title": "4.3 Latent style optimization", "content": "The latent style optimization uses the generator of MUNIT as its backbone to disentangle style and content of an image [53]. It then optimizes the latent style vector in alignment with our criteria. By modifying only the disentangled latent style, this approach is also designed to alter only low-level image features while preserving the depicted content.\nTo optimize the latent style according to emotional reference, the objective function incorporates a term that applies the emotion regressor to the adapted images. Additionally, to preserve fidelity to the original image, we employ a consistency loss between the content vector of the original and adapted images, as proposed in [53]. This approach is illustrated in Figure 3.\nIn the following, we explain how content and style of an image can be disentangled and define the objective function of our latent style optimization more formally.\nDisentangling content and style. MUNIT employs an auto-encoder architecture as the generator in a GAN framework, allowing it to disentangle an image into a content vector and a style vector within its latent space for unsupervised style transfer across different domains. We adapt this approach by training MUNIT's GAN on the dataset ImageNetV2 to achieve a general-purpose disentanglement of image content and style, following a methodology similar to [120]. For more details on the training process, we refer interested readers to the original MUNIT paper [53].\nWe trained the GAN until qualitative results confirmed effective content-style disentanglement, which was achieved after approximately 200,000 iterations. Upon completion, the generator produced an FID score of 3.2 for images generated with swapped style vectors. As demonstrated in [120], the generator successfully learned to separate high-level content features (e.g., people, chairs, cars, flowers) from low-level style characteristics (e.g., color, texture, grayscale, brightness)."}, {"title": "4.4 Inverse diffusion with score-guided resampling", "content": "This approach leverages a diffusion model to adapt images by inverting an input image into its corresponding noise vector and then resampling it using an emotion regressor to guide the transformation towards an image that evokes a more modulated emotional response compared to the original input. By employing this method, the approach introduces flexible changes to the image, affecting not only its low-level appearance but also its high-level content.\nFor our implementation, we use the stable diffusion model SDXL as the backbone [91]. Additionally, we incorporate null-text optimization (NTO) to invert the input image into its corresponding latent noise vector while preserving its editability [81]. To meet criteria, we then denoise the latent vector using two conditioning mechansims: (1) classifier guidance [35] to direct the denoising process towards an image aligned with neutral valence and reduced arousal; (2) classifier-free guidance [51] using the input image's caption to ensure the generated image retains the characteristics of the original input. Figure 4 illustrates this approach.\nIn the following, we define the three steps of our approach more formally. For a background on diffusion models and the related methods that support our approach, please refer to Appendix B.\n1) Training the guidance regressor. To attain a regressor which we can use for CG of a diffusion model, we train a regressor $R_u$ that predicts valence and arousal values on the output of the middle layer of a diffusion model $u_t$:\n\u2207\u3047\u2081 log pp (v', a' |2t) = ||[v', a'] \u2013 Ru(DMhalf(2t, t, 0))||   (3)\nThe noise prediction process is then carried out with the modified objective:\n\u20ac\u03c1 (2t, t, C, 0) = w \u2022 \u20ac\u0189(2t, t, C) + (1 \u2212 w) \u2022 \u20ac\u0189(2t, t, \u00d8) + s \u2022 \u2207\u3047\u2081 log pp (v', a' |\u015dt).   (4)"}, {"title": "5 IMPLEMENTATION", "content": "All approaches were implemented using PyTorch and work with images of resolution 1024x1024. Both optimization-based approaches employed the Adam optimizer with parameters $\u00df_1 = 0.9$ and $\u00df_2 = 0$ and a learning rate of 0.05.\nFor the differentiable image transformations used in the parametric optimization, we utilized existing PyTorch implementations from Kornia [96], a differentiable library of classical computer vision algorithms. Additionally, we adapted some functions from the TensorFlow implementations of the GitHub repositories of [52, 78].\nFor the latent style optimization, we used a high-resolution implementation of MUNIT provided by NVIDIA\u00b9.\nIn our diffusion-model-based approach, we employed the Diffusers implementation of SDXL from Stability AI\u00b2. Since SDXL relies on a float16 tensor representation, using default null-text optimization led to vanishing gradients. To address this, we re-implemented NTO with mixed-precision capabilities.\nThe network of the guidance regressor consists of four convolutional layers, each followed by ReLU activation and max-pooling, before passing through two fully connected layers to produce the final output predictions.\nTo attain captions for images, we leveraged the GPT4 Vision 2024-05-01-Preview model of Azure OpenAI and prompted it to provide descriptive captions that could be used as labels for a machine learning dataset."}, {"title": "6 EVALUATION OVERVIEW", "content": "The objective of our evaluation is to analyze if computational image adaptations that change the emotional properties of online content can be leveraged to facilitate healthy internet use by reducing the time users spent online. While previous work has shown that image manipulation impact viewer emotions (see Sec. 2.4) and that emotional content boosts online engagement (see Sec. 2.2), it is unclear if the emotional properties of images can be altered in a targeted fashion to regulate the emotions users experience and through that reduce their duration of internet use. We will address these open challenges by investigating the following two research questions:\nRQ1: Can computational image adaptations effectively reduce emotional arousal and neutralize valence without compromising the quality of images?\nRQ2: Do lower arousal levels and a more neutral emotional valence lead to a decreased duration of engagement with a social media feed?\nWe chose social media as the context for this evaluation, as it is among the most widely used services on the Internet [31] and is commonly associated with high arousal, polar valence emotional states [33, 77].\nTo address our research questions, we first perform a technical evaluation to assess if approaches satisfy our criteria in a technical sense. Subsequently, we conducted two user studies. In the first study, we assess whether our image adaptation methods can successfully reduce emotional arousal and neutralize valence in viewers while maintaining a high level of perceived image quality. In the second study, we investigate whether these emotional alterations lead to a reduced duration of engagement with a social media feed. Both user studies were preregistered on OSF [41]."}, {"title": "7 TECHNICAL EVALUATION", "content": "The goal of the technical evaluation is to evaluate the effectiveness of the proposed approaches in relation to our defined criteria. Specifically, we analyze if methods can alter valence and arousal levels as predicted by a regressor while maintaining image quality and fidelity to the original. This evaluation also examines whether these alterations cause changes in image properties in accordance with previous findings from related work. In an additional experiment, we vary the optimization objective to examine if images can be optimized toward different reference points in the valence-arousal space. Finally, we apply these approaches to adapt social media images and report the qualitative changes they induce."}, {"title": "7.1 Criteria alignment", "content": "In this experiment, we run different parameter configurations of our algorithms on the validation set of the COCO database [66] and compare them against the relevant metrics of our criteria.\n7.1.1 Baselines. To evaluate the effectiveness of our developed approaches, parametric optimization, style optimization, and inverse diffusion with score-guided resampling (hereafter referred to as CG & CFG), we introduced six additional baselines, described below:\n\u2022 Original: The original set of images without any modification or adaptation.\n\u2022 Greyscale: The original images converted to greyscale, removing all color information. This type of filter is already available on smartphone operating systems to reduce phone use (e.g., Android's productivity mode, Apple's color filters).\n\u2022 Manual: The set of static image transformations used in the parametric optimization approach applied based on parameters chosen by an expert. Parameter values were determined through experimentation on a subset of images considering the predefined criteria.\nTo analyze the contributions of different components in our diffusion-based approach, we introduced three ablated versions of the method. The variants were proposed to identify the impact of individual component of our methodology:\n\u2022 CG (Classifier Guidance Only): This version employs the deterministic DPM Multistep Solver to invert an image [69]. Resampling is performed using only Classifier Guidance, without any text conditioning. For this baseline, we use DPM instead of DDIM as our initial experiments have shown that it produces inverted images of higher quality without NTO and (as CG is not text conditioned NTO cannot be applied).\n\u2022 NTO-instruct (Text-Conditioned Resampling Only): In this version, images are inverted using NTO and resampled with text conditioning only, without the use of a regressor. To guide resampling, the following sentence is appended to the image caption: \"The image should elicit low arousal and neutral valence in its viewers.\"\n\u2022 NTO-edit (Text-Conditioned Resampling with Caption Refinement): This version also uses NTO for image inversion, followed by resampling based on a modified caption. The original captions were refined by removing words or phrases likely to evoke high emotional arousal or strong positive/negative valence, ensuring a neutral emotional tone. The refined captions closely resemble the original text while avoiding words that could provoke strong emotional reactions. The modifications were generated using a GPT-4 language model.\nFor our diffusion-model-based baselines, we have empirically determined that a CFG scale value of 2.0 yielded the best results. This value effectively balanced introducing changes to the image while maintaining image quality and alignment to the original. For all baselines utilizing text guidance, we used the image captions provided by the"}, {"title": "7.1.2 Metrics", "content": "The primary objective of this evaluation is to validate whether the proposed approaches satisfy the defined criteria. To this end, we employed the regressor described in Section 4.1 to estimate the valence and arousal of images. We further quantified their resemblance to the original images using the L1 loss. To evaluate image quality, we calculated the Kernel Inception Distance (KID) and the Frechet Inception Distance (FID), both of which are widely used metrics for assessing visual quality.\nAdditionally, we quantified whether our approaches introduce changes to image properties in line with findings from previous research (Sec. 2.1). Thus, we measured the colorfulness and saturation of images using metrics proposed by Hasler and Suesstrunk [49]. The mean brightness of images was computed following the metric introduced by Goetschalckx et al. [44]. Contrast was calculated as described by Peli [87] and blur was assessed using the no-reference perceptual blur metric proposed by Crete et al. [27]. To quantify the variability in edge orientations, we followed the study by [94] and computed the first order entropy of edge orientations. To analyze low-frequency changes in color and greyscale channels, we computed Z-score transformed wavelet coefficients for both RGB and grayscale images using the methodology proposed by Delplanque et al. [34]. We quantified left-right mirror symmetry in images using the approach of Brachmann and Redies [14], leveraging filter responses of a convolutional neural network (VGG16) to capture color, edges, texture, and higher-order symmetry. Note that we omitted quantifying edge smoothness as proposed in [28] due to the excessive manual effort required to label all significant contours in an image."}, {"title": "7.1.3 Results", "content": "Our approaches share a dual-objective design: one objective ensures that the adapted images remain similar to the original, while the other introduces changes to neutralize the valence and reduce the arousal evoked in viewers. To understand the changes our approaches introduce to images, we fixed the weight of the similarity-preserving objective ($w_1 = 1.0$ for parametric optimization and style optimization, and w = 2.0 for CG & CFG and CG) and systematically varied the weight of the emotional adaptation objective ($w_2 \\in \\{0.1, 0.3, 0.5, 0.7, 1.0\\}$ for parametric optimization and style optimization, and $s \\in \\{0.05, 0.1, 0.2, 0.3, 0.4\\}$ for CG & CFG and CG). In the following, we first present the quantitative results of this experiment, followed by the qualitative findings.\nCriteria alignment. Figure 5 illustrates how the metrics evolve as the weights of the emotional adaptation objective are adjusted. The results demonstrate that, in line with our criteria, all approaches effectively neutralize valence and lower arousal compared to the original images. An exception is NTO-edit, which does not produce lower valence values relative to the original images. To formally confirm this observation, we fitted ordinary least squares (OLS) regression models to the differences between the valence and arousal values of the original images and their adapted counterparts for each weight configuration of parametric optimization, style optimization, CG, and CG & CFG. For valence, the OLS models for all approaches were statistically significant (p < 0.03), indicating that higher weights in the emotional adaptation objective were strongly associated with reductions in valence. The predictor term ($\u03b2_1$) for each approach was as follows: style optimization ($\u03b2_1 = -0.0121, p = 0.022$), parametric optimization ($\u03b2_1 = -0.0220, p = 0.018$), CG ($\u03b2_1 = \u22120.0960, p < 0.001$), and CG & CFG ($\u03b2_1 = -0.0401, p = 0.001$). The observed negative relationships are attributable to the original images having a positive average valence, with increasing weight on the emotional adaptation objective driving them toward the emotional reference."}, {"title": "7.2 Bidirectional emotional changes", "content": "We conduct an additional experiment to determine whether CG & CFG, style optimization, and parametric optimization can optimize images for different reference points in the valence-arousal space.\nFor style optimization and parametric optimization, which primarily modify color values, initial experiments revealed that extreme emotional references often introduce artifacts (e.g., images becoming entirely black). Therefore, we define their reference points relative to the inferred valence and arousal values of each image. Specifically, we selected incremental adjustments of [-0.2, -0.1, +0.1, +0.2] to both valence and arousal values as targets for optimization. In contrast, CG & CFG, capable of making more extensive plastic changes, can handle more extreme emotional references. Accordingly, we use the emotional references of valence = arousal = -1.0 and valence = arousal = 1.0 for this approach.\nThis experiment is conducted on a random subset of 500 images from the Instagram Influencer Dataset [58]. We use the same weights for the approaches as in the behavioral study (CG & CFG: w = 2.0, s = 0.2; style optimization: $W_1 = 1.0, W_2 = 0.2$; and parametric optimization: $W_1 = 1.0, W_2 = 0.2$).\n7.2.1 Results. Figure 9 illustrates the emotional reference values plotted against the averaged inferred valence and arousal of all adapted images. The results indicate that the adaptation approaches achieve the intended directional emotional changes, albeit to a lesser degree than the reference values.\nTo formally evaluate this relationship, we fit an OLS model to the average valence and arousal values for style optimization and parametric optimization. For valence, the predictors for style optimization and parametric optimization"}, {"title": "7.3 Adapting social media images", "content": "In this experiment, we assess the qualitative impact of our image adaptation approaches on social media images when adapted according to the defined emotional"}]}