{"title": "ERIC: Estimating Rainfall with Commodity Doorbell Camera for Precision Residential Irrigation", "authors": ["Tian Liu", "Liuyi Jin", "Radu Stoleru", "Amran Haroon", "Charles Swanson", "Kexin Feng"], "abstract": "Current state-of-the-art residential irrigation systems, such as WaterMyYard, rely on rainfall data from nearby weather stations to adjust irrigation amounts. However, the accuracy of rainfall data is compromised by the limited spatial resolution of rain gauges and the significant variability of hyperlocal rainfall, leading to substantial water waste. To improve irrigation efficiency, we developed a cost-effective irrigation system, dubbed ERIC, which employs machine learning models to estimate rainfall from commodity doorbell camera footage and optimizes irrigation schedules without human intervention. Specifically, we: a) designed novel visual and audio features with lightweight neural network models to infer rainfall from the camera at the edge, preserving user privacy; b) built a complete end-to-end irrigation system on Raspberry Pi 4, costing only $75. We deployed the system across five locations (collecting over 750 hours of video) with varying backgrounds and light conditions. Comprehensive evaluation validates that ERIC achieves state-of-the-art rainfall estimation performance (~ 5mm/day), saving 9,112 gallons/month of water, translating to $28.56/month in utility savings. Data and code is available in https://github.com/tian1327/ERIC.", "sections": [{"title": "1 INTRODUCTION", "content": "The U.S. Environmental Protection Agency (EPA) reports that landscape irrigation accounts for nearly one-third of all residential water use, amounting to over 9 billion gallons per day [11]. Recent studies [10] indicate that more than 50% of water is wasted due to imprecise irrigation scheduling. For instance, in East Texas, despite generally adequate rainfall, more than 90% of residents over-irrigate their landscapes [29]. The significant waste of water motivates the design of more precise residential irrigation systems.\nTraditional residential irrigation systems activate sprinklers on a fixed schedule, disregarding factors such as precipitation, solar radiation, plant types, and soil conditions, resulting in substantial water waste. Recent smart irrigation systems [12] strive to deliver precise water by deploying soil sensors to monitor soil moisture (sensor-based) or retrieving weather data from nearby weather stations to calculate the water balance of soil-plant system, i.e., weather-based (also known as evapotranspiration-based or ET-based). However, limitations remain with both methods.\nSensor-based method is limited by high deployment costs. Previous studies [3, 13, 27, 30, 34] indicate that due to a limited sensing range (12 inches around the prob), multiple sensors are needed for adequate coverage, resulting in installation costs exceeding $1,000, including the data logger. Additionally, these sensors require frequent calibration to the soil salinity, temperature, etc., further increasing maintenance costs [20, 27, 43]. The complexity of interpreting sensor data also reduces the practicality of these systems. As a result, current industry has widely adopted the more cost-effective weather-based method, e.g., WaterMyYard in Texas [37], CIMIS in California [8], and many others [25, 38, 42]. Our paper focuses on improving the precision of current weather-based irrigation systems, aligned with current industry practice.\nWeather-based method is limited by the inaccurate rainfall data from nearby weather stations. This method accounts for the water balance in the soil-plant system, considering both outgoing water (evaporation from soil and plant transpiration) and incoming water (rainfall and irrigation) [17, 26]. Accurate rainfall measurements from nearby weather stations are crucial for this method to calculate the desired irrigation amount. However, due to the spatio-temporal variability of hyperlocal rainfall, rainfall measured at a nearby weather station can be highly inaccurate (see Fig. 1). Our field experiments show that rainfall measured from a weather station just 1.7 miles away can differ by as much as 54% from the actual hyperlocal rainfall, leading to substantial over- or under-irrigation (c.f. Fig. 2).\nOnsite rain gauges are expensive, limiting scalability. To achieve more accurate hyperlocal rainfall, recent weather-based irrigation controllers integrate on-site rain gauges, costing over $300 for consumer-grade and over $1,000 for professional-grade models [16, 36]. Besides the initial installation expenses, regular maintenance and calibration further escalate the overall cost. Less expensive rain gauges (under $100) are available, but they compromise accuracy and reliability and require technical skills for integration that many homeowners lack.\nCan we estimate rainfall from doorbell camera? In this paper, we propose improving the current weather-based irrigation system with a low-cost, accurate solution for hyperlocal rainfall measurement. Inspired by the ubiquitous doorbell cameras and the high spatio-temporal resolution of video data, we explore the research question: \u201cCan we develop an weather-based irrigation system that estimates hyperlocal rainfall from existing commodity doorbell cameras without additional hardware deployment?\u201d We identify several key technical challenges below.\nChallenge 1: how to estimate rainfall from streaming video with high accuracy and low compute cost? Prior rainfall estimation methods [2, 7, 9, 15, 21] employ computationally expensive rain steak extraction algorithms or Convolutional Neural Network (CNN) models that require high-end hardware (e.g. high-resolution cameras and GPU), which are unaffordable to common homeowners.\nChallenge 2: how to preserve user privacy as doorbell camera video is highly sensitive? Out of privacy concerns, homeowners are reluctant to upload doorbell camera recordings to the cloud for processing. This challenges the real-time processing of streaming video data on resource-constrained edge devices to prevent data backlog.\nChallenge 3: how to evaluate our system in real-world deployment? Prior rainfall inference work evaluates with either images [47] or videos from traffic cameras [18]. Till now, there are no publicly available video datasets from residential environments. This lack of data necessitates creating new datasets and testing frameworks tailored to real-world residential settings to thoroughly assess the effectiveness of the system.\nSolutions and contributions. To address above challenges, we present the design and implementation of ERIC system, the first precision residential irrigation system that harnesses existing commodity doorbell cameras and machine learning models to accurately estimate hyperlocal rainfall (cf. Fig. 1). Specifically, we make the following contributions:\n1) We developed lightweight neural network models based on our proposed reflection-based visual features and audio features for rainfall estimation. Our method achieves state-of-the-art performance with ~ 5 mm/day error, using only low-cost commodity doorbell cameras.\n2) We built an end-to-end irrigation system on a Raspberry Pi 4 device. The simplicity of our models and pipelines enables ERIC system to process video data locally at the edge, preserving user privacy while ensuring low compute costs.\n3) We deployed the ERIC system to five diverse real-world residential environments in two years. We collected over 750 hours of videos (including 150 hours of rain) with accurate rainfall ground truth from an onsite professional-grade rain gauge."}, {"title": "2 RELATED WORK", "content": "The following Section 2 introduces related work. Section 3 presents the system design and our models. Section 4 shows our system implementation and Section 5 evaluates system performance in real-world deployments. Finally, conclusions and future work are discussed in Section 6."}, {"title": "2.1 Rainfall in Weather-based Systems", "content": "The weather-based method (see Fig. 3 left) calculates the irrigation requirement by considering the water balance between incoming water (rainfall and irrigation) and outgoing water (ET_loss via plant transpiration and soil evaporation) [16, 17, 36]. Concretely, irrigation requirement $IR = ET_{loss} - Rain$, where ET_loss is obtained from a nearby weather station based on measurements of solar radiation, temperature, wind, and humidity and adjusted for specific plant and soil types [26, 37]. However, the rainfall measured from a nearby weather station could be highly inaccurate, due to the large spacing between weather stations (see Fig. 3 right) and spatio-temporal variability of hyperlocal rainfall. Such inaccurate rainfall measurements result in a significant waste of water, motivating us to leverage the ubiquitous doorbell cameras for hyperlocal rainfall estimation."}, {"title": "2.2 Irrigation Optimization", "content": "Previous work on irrigation optimization focuses on enhancing sensor-based methods. Winkler et al. [44-46] developed a distributed sprinkler network with built-in soil moisture sensors, employing data-driven models to optimize irrigation. However, the high deployment cost of sprinkler networks limits its scalability in residential irrigation. Later work by Murthy et al. [24] improved weather-based methods by considering site-specific factors including soil and plant types, surface slope, etc., employing machine learning models with human feedback to prevent water run-off. Different from previous work, we focus on addressing the ever-neglected problem of inaccurate rainfall data from nearby weather stations. Our work aims to improve the irrigation efficiency of weather-based methods by providing accurate hyperlocal rainfall estimated from doorbell cameras."}, {"title": "2.3 Rainfall Estimation from Camera", "content": "Previous work on estimating rainfall from camera can be categorized into extraction-based and deep learning-based methods. Both methods have shown limitations.\nExtraction-based methods are not robust to environmental factors. These methods [2, 7, 9, 15, 21] employ geometric and photometric models to first extract the foreground rain streak layer and then estimate rainfall intensity from the distribution of raindrop sizes. The performance of these methods heavily relies on accurately capturing the fine-grained raindrop shapes, which not only requires expensive cameras (>$2,000) and GPUs [21] but also faces significant practical challenges. For example, the visibility of raindrops vanishes considerably with increasing distance to the camera. Additionally, the raindrop shapes can be largely distorted in a wide-angle camera. Moreover, none of these methods demonstrate feasibility under poor light conditions, e.g. nighttime with only a few street lights. All these challenging factors limit the practical effectiveness of extraction-based methods.\nDeep learning-based methods suffer from large training costs, lacking rigorous evaluation. Recent works [4, 18, 47] leverage Convolutional Neural Networks (CNN) to detect or estimate the rainfall from images or traffic cameras. These models require a large training set along with significant training costs (long training time, expensive GPUs), which are not affordable to homeowners in residential environments. In addition, previous work evaluates their models with an oversimplified setup, i.e. testing on randomly split images [4,47] or only rainy videos with limited background [18,21] rather than continuous video streams. This is probably due to the lack of publicly available benchmarking video datasets.\nOur novelties. In contrast to the aforementioned methods, our model: 1) estimates rainfall from intensity changes between video frames rather than raindrop shapes, demonstrating robustness to challenging environmental factors; 2) leverages lightweight neural networks, ensuring low training and inference costs; 3) evaluates with continuous streaming video (including both raining and non-raining) with diverse backgrounds, validating practical effectiveness. We discuss more details in the following sections."}, {"title": "3 SYSTEM DESIGN", "content": "In this section, we first present an overview of our ERIC system, and then introduce our machine learning models at the edge, followed by an extension to our cloud-based solution. Finally, we provide details for hardware design."}, {"title": "3.1 ERIC System Overview", "content": "Fig. 4 presents an overview of our ERIC irrigation system. ERIC harnesses existing commodity doorbell cameras to stream video to the controller board, which then runs a pretrained machine learning model locally to estimate hyperlocal rainfall. Next, the controller board retrieves the latest ET_loss from a nearby weather station and calculates the irrigation requirement based on water balance (as discussed in Section 2.1). Finally, the controller activates sprinklers based on the optimized irrigation schedules.\nWe highlight several advantages of our system: 1) high system efficiency. Our model leverages a lightweight neural network model with our designed robust visual and audio features for rainfall estimation. Due to the simplicity of our model, it can process the streaming video data in real time without data backlog; 2) preserving user privacy. Owning to the low compute cost of our model, all the sensitive video data can be processed locally on the controller board, which addresses homeowners' privacy concerns. For a complete system design, our ERIC does provide an extended option for users to upload their video data to the cloud, leveraging more powerful CNN models (e.g. ResNet [19]) for rainfall estimation, at the cost of privacy; 3) low deployment costs. Our ERIC system runs efficiently on a Raspberry Pi 4 device, costing only $75 in contrast to $200-500 for the smart irrigation systems on the market (e.g. Rachio [31], Rainbird [32]). In addition, ERIC schedules irrigation without human intervention, while the current industrial state-of-the-art weather-based program (WaterMyYard [37]) still requires homeowners to adjust the controllers weekly; 4) user-friendly interface. We developed an Android App where the user can input plant and soil types, surface slopes, and other factors that are specific to their residential environment for optimal irrigation. The App also collects the irrigation history from the controller board and presents it to users. Moreover, we have integrated Alexa for voice-controlled irrigation, e.g. \"start irrigation\" or \"stop irrigation\" when noticing water runoff."}, {"title": "3.2 Rainfall Estimation at the Edge", "content": "Rainfall estimation workflow. Given an input video file, our workflow (see Fig.5) first samples two adjacent frames every five seconds to calculate the visual features, which are then averaged per minute. Similarly, the audio features are extracted for each second and averaged for each minute. This sampling strategy is employed to reduce computation costs, and the averaging mechanism is used to improve feature robustness against outliers (e.g. shaking camera, moving objects in the scene, etc.). Next, the visual and audio features are concatenated and fed into a rain detector and a rain estimator model in parallel. The rain detector predicts whether each minute is raining or not, while the rain estimator predicts the rainfall intensity for each minute. Finally, we aggregate the predictions by summing the rainfall intensity for raining minutes only, obtaining the cumulative daily rainfall. We emphasize the importance of our design, which includes both a rain detector and an estimator rather than solely relying on a rain estimator. This is because the rain estimator, a regression model, tends to predict small values (e.g. 0.01 mm/min) instead of zeros for non-raining minutes. Summing these small values would result in a significant deviation in cumulative daily rainfall. Our rain detector serves as a filter, effectively removing these errors.\nOur intuition: estimating rainfall from raindrop reflections and sound. In contrast to previous work [2,7,9,15,21] which relies on accurate capture of raindrop shapes, our method estimates rainfall from the raindrop reflections and raining sound. Our key intuition is: the reflections from the raindrops in air and splashes on the ground, and the volume of sounds of raindrops hitting surfaces strongly correlate to the rainfall intensity. In Fig. 6, we compare the RGB video frames along with their intensity changes under different rainfall intensities, light conditions, and backgrounds using our collected dataset. These intensity changes are calculated as the absolute differences between two adjacent frames after converting them into grayscale images, as defined by $AI = |I_n - I_{n-1}|$ [15]. The intensity changes essentially capture the reflections from the fast-moving objects (e.g. raindrops, splashes) between adjacent video frames. Interestingly, we make several important observations:\n1) Fig. 6 (a) shows that residual water on the camera lens can blur the captured frames while the raindrops at a further distance create a \"rain fog effect\", making it infeasible to extract the raindrop shapes accurately [21]. This showcases the significant challenges of deploying previous extraction-based methods in practice.\n2) However, we find that the intensity changes easily capture the reflections from falling raindrops and splashes from the ground (see regions marked with green or yellow boxes), and strongly correlate to the rainfall intensity. For instance, heavy rain introduces much brighter and denser white dots on the intensity change map due to the larger and denser raindrops (Fig. 6 (a)), while light rain yields much weaker and sparse white pixels (Fig. 6 (b)).\n3) The intensity change maps work well under different light conditions (Fig. 6 (c)) and diverse backgrounds (Fig. 6 (d)), suggesting it provides a robust representation of rainfall intensity.\nThese observations motivate us to design reflection-based models to estimate rainfall. In addition, inspired by the repetitive sounds of raindrops, we explore combining audio information to improve the rainfall estimation performance. Specifically, we identify several technical questions, which we address in the following.\nQuestion 1: how to locate regions of interest that capture strong reflections? To automatically locate the regions of interest (RoI) when deploying the ERIC system in diverse residential backgrounds, we propose the AutoRol algorithms (see Fig. 7). AutoRol only requires the user to input the starting and ending times of a few raining periods. Next, it automatically fetches the corresponding videos to calculate the averaged intensity change maps for daytime and nighttime separately. These maps are then averaged to get the composite intensity change map, which emphasizes the regions with consistently strong reflections throughout the day. Then, a filter is applied to remove noisy weak pixels with AI < 0.15 followed by a weighted K-means clustering. Finally, the bounding box for each cluster is defined by including 80% bright pixels in the same cluster, which is achieved by taking the 10th and 90th percentiles of the sorted pixel coordinates. Notably, AutoRoI runs only once in the training stage. The optimal number of clusters can be chosen using the validation set. During the inference, the model uses the obtained RoIs to calculate the visual features. Our experiments in Section 5 validate the effectiveness of AutoRoI by comparing its performance with that of the handcrafted Rol.\nQuestion 2: how to capture the visual and audio features? We designed robust visual and audio features to quantitatively measure the significance of intensity changes and the distinctiveness of raining sounds. Specifically, we design the following robust intensity-based visual features and calculate for each RoI.\n(1) max_AI: the maximum intensity change. As shown in Fig. 6, heavier rain leads to higher intensity changes due to the larger size of raindrops.\n(2) brightness = $N_{high}/N_{total}$: the fraction of high intensity-change pixels in a Rol, where $N_{total}$ is the total number of pixels and $N_{high}$ is the number of pixels with $\u2206\u0399 > threshold$, using the same threshold value of 3 as in [14].\n(3) density = $N_{bright}/N_{total}$: the percentage of bright pixels in a RoI. $N_{bright}$ is the number of pixels with $\u2206\u0399 > 0.1$. This feature captures the raindrops/splashes that are further away from the camera (e.g. RoI 1-3 in Fig. 6), which show much weaker reflections.\n(4) variability = $sizeof(set(int(\u2206\u0399;))), i \u2208 {1... N_{total}}$: the number of unique intensity change levels. This feature captures the light reflection and refraction from raindrops/splashes at various angles and distances, leading to large variations in pixel brightness.\nIn addition, to capture the distinct sound of rainfall (e.g. repetitively hitting a drum), we adopt the following low-level timbral features which are widely used in music genre classification or speech recognition [5, 6, 41]: Amplitude Envelope (AE), Root-Mean-Square Envelope (RMSE), Zero Crossing Rate (ZCR), Spectral Centroid (SC), and Spectral Rolloff (SR). The AE, RMSE are amplitude-based features reflecting the loudness of the sound, while ZCR, SC, SR are frequency-based features representing the brightness of the sound. Fig. 8 showcases the increased audio features during the rain, suggesting the audio features embed useful information about rainfall intensity features.\nQuestion 3: what machine learning models to use? Unlike previous methods [4, 18,47] that use computation-intensive CNN models, we design lightweight artificial neural network models (ANN) to ensure real-time inference and accommodate the limited computation power on the edge device (e.g. Raspberry Pi 4). Specifically, we implement a neural network model with only two dense layers, each containing 6 neurons. We use the ReLU activation function for the middle layers, Sigmoid function for the output layer of rain detector, and Linear function for the output layer of the rain estimator. The simplicity of the model benefits from the effective visual and audio features we designed. As a result, our model can process streaming video data in real time and conduct training and inference entirely at the edge."}, {"title": "3.3 Extension: Rainfall Estimation in the Cloud", "content": "Our model at the edge requires the user to input a few raining periods for AutoRoI calculation. To provide users with a fully automated option, we design a cloud-based solution that leverages CNN models for automatic feature extraction. This option does require the user to upload their video data to the cloud, trading privacy for convenience. Similar to our edge solution, our cloud-based workflow (see Fig.9) first slices video frames every five seconds and then passes them to the rain detector and estimator for rainfall prediction. The outputs are averaged per minute and then summed up for cumulative daily rainfall. We adopt the ResNet18 model [19] as the backbone for both the rain detector and estimator on the cloud."}, {"title": "3.4 ERIC Irrigation Controller", "content": "We build a smart irrigation controller board based on the Raspberry Pi 4 (8GB) device (see Fig. 10 (b)), costing only $75. We develop the controller based on OpenSprinkler [28], with hardware and firmware enhancements. According to the irrigation schedule optimized by our ERIC, the controller turns on/off the irrigation valves through the Pi's GPIO pins, without using an additional microcontroller. The firmware, written in C/C++, stores camera feeds, user data, and system configurations on the local storage. The controller is powered by a 24V AC adapter which also powers the irrigation valves. In addition, we developed a smartphone App through which users can input their plant/soil types for optimal scheduling, control the irrigation status (turn on/off), and view the irrigation history. Considering the convenience of voice assistants in various applications [1, 22, 23, 33, 40], we integrated the App with Alexa for voiced-controlled irrigation."}, {"title": "4 SYSTEM IMPLEMENTATION", "content": "In this section, we first introduce our implementation details of deploying the system to five real-world residential environments and then discuss the data collection and preprocessing."}, {"title": "4.1 System Setup", "content": "To test the practical effectiveness of ERIC, we deployed the system to five residential environments with varying camera types, backgrounds, and camera placements. These locations include a university campus (L1), the front door and backyard of three residential homes (L2, L3, L4, L5).\nVideo data collection. Fig. 10 (a) shows our system setup at L1. The camera (NSC-DB2) has a wide-angle lens and captures video in 1536 x 2048 resolution with 30 frames per second. It connects to the irrigation controller through the WiFi network provided by the router and streams real-time video data to the controller using the Shinobi open-source software [35]. The streamed video data are saved locally on the controller as 30-minute MP4 files. We tested two types of cameras with different placements: a doorbell camera (NSC-DB2) installed on the door frames and a surveillance camera (Topodome) installed on the upper wall. Fig. 11 illustrates examples of both daytime and nighttime video footage from each location. Importantly, L2 and L3 present challenging light conditions at nighttime. Due to limited light sources, the camera can only capture views at a close distance. Despite the poor light conditions, our edge model still demonstrates excellent performance in rainfall detection and estimation, highlighting the effectiveness of our robust visual and audio features (see Section 5).\nRainfall ground-truth measurement. To measure the ground-truth hyperlocal rainfall, we installed a professional-grade rain gauge right next to the camera at each location. We used a high-resolution tipping-bucket type rain gauge (HOBO RG3-M), costing $800 with a data logger. The rain gauge collects the raindrops into a fixed-volume bucket and then tips to one side when it is full, triggering an electric signal that records a rainfall of 0.22 mm."}, {"title": "4.2 Data Preprocessing", "content": "We deployed the system to five residential locations, spanning over two years. We record the videos continuously but only save the videos of raining days due to storage limits. The resulting datasets amount to 750 hours (1 TB), including 150 hours of raining footage. A detailed summary of collected data is provided in Table 1.\nVideo data splitting. To avoid information leakage from the random splitting of videos, we sequentially split the collected data into training, validation, and testing sets. For example, for the L1 dataset, we used videos of May, June, and July for training, August and early September for validation, and late September and October for testing. We apply the sequential splitting to all datasets.\nRainfall label preparation. The raw data from the rain gauge is the cumulative rainfall (mm) recorded at each timestamp, with an interval of 0.22 mm. To train our machine learning models for rain detection (classification) and estimation (regression) tasks, we process the raw data to obtain the labels. For the rain detection task, the labels are binary values for each minute of the videos. Due to the delayed activation of the tipping-bucket type rain gauge, i.e. the rain may have started much earlier before the bucket gets full and tips, we manually adjust the starting and ending time of each rain by checking the collected video footage. This process is necessary for obtaining accurate starting and ending time for light rains. For example, on Sep 18 (Fig. 17), the rain gauge only records one tipping. However, our manual correction helps recover the actual longer raining periods. The accurate labels improve the training of the model and ensure proper evaluation of the models. For the rainfall estimation task, the labels are rainfall intensity (mm/minute). We obtain this by first interpolating the raw cumulative rainfall (mm) at each minute and then taking the difference between consecutive interpolated values."}, {"title": "5 SYSTEM EVALUATION", "content": "In this section, we evaluate our ERIC system using our collected datasets. We first introduce evaluation metrics and baseline models and then provide a comprehensive analysis of ERIC's performance. For simplicity, in the following sections, we refer to our edge solution with ANN model as ERIC-edge and our cloud-based solution using ResNet18 model as ERIC-cloud."}, {"title": "5.1 Evaluation Strategy", "content": "Evaluation metrics. For rain detection, we report the accuracy and F1 scores. Due to the significant imbalanced ratio of rain vs. not rain (see Table 1), we use the F1 score as the main evaluation metric because it considers both precision and recall. For rainfall estimation, previous work [18] uses mean absolute percentage error (MAPE), as defined by MAPE = $\u03a3_{i=1}^{M} |R_{i,infer}-R_{i,true}|/R_{i,true}$, where M is the total number of rains. However, we find that MAPE is strongly biased in light rains. For example, on Sep 18 in Fig. 17, the MAPE of our model is above 100%, but the absolute error is only less than 1 mm. To mitigate the biases, we designed two new metrics: total relative error, $TRE = \u03a3_{i=1}^{N}|Ri,infer - Ri,true| / \u03a3_{i=1}^{N} Ri,true$, and mean absolute daily error, $MADE = \u03a3_{i=1}^{N}|Ri,infer - Ri,true| /N$ where N is the total number of days. The TRE measures the relative error over the entire testing days, which reflects how the system performs over a longer period while MADE measures the absolute daily errors.\nCompared models. We compare ERIC-edge and ERIC-cloud with the state-of-the-art extraction-based method by Jiang et al. [21] and deep learning-based method, i.e. 3DCNN by [18]. Note that Jiang's method does not open-source their code so we can only compare with their reported MAPE score. The 3DCNN model by [18] was originally implemented for rain detection only. We adapt it for our rainfall estimation task by replacing the Sigmoid activation function of the output layer with Linear function.\nImplementation details. We use OpenCV package to process visual features in videos and use FFmpeg and librosa to calculate the audio features. We implement models in ERIC-edge and ERIC-cloud using Tensorflow and PyTorch frameworks respectively. We simulate the cloud by running experiments on a Linux server with 64 Xeon(R) Silver 4313 CPUs (2.4GHz) and 3 Nvidia A30 GPUs (24GB each). We implement the 3DCNN model on our datasets using the provided code in [18]. For all models, we search hyperparameters on the validation set and report their performance on testing set."}, {"title": "5.2 Results", "content": "We first compare our ERIC with previous works on evaluation setup, rainfall estimation performance, and compute cost, highlighting the practical effectiveness of ERIC. Then, we provide further analysis of ERIC's rainfall estimation performance, water savings, false positives and negatives, RoI selections, and input features."}, {"title": "5.2.1 Comparison with State-of-the-Arts", "content": "We compare the evaluation setup and model performance between our ERIC and previous SOTA by Jiang et al. [21] and 3DCNN [18]. As shown in Table 2, previous work only evaluates on videos with cropped backgrounds, limited duration containing either rains only or daytime only. Instead, our work provides a more realistic evaluation setup by deploying our ERIC system in diverse residential backgrounds (see Fig. 6) and evaluating on longer videos with both rain and no rains, daytime and nighttime with challenging conditions. Despite using a much cheaper camera and evaluating on a more challenging setup, our ERIC achieves lower MAPE than previous work, highlighting its practical effectiveness. Moreover, our collected video datasets serve as a realistic benchmark, providing opportunities for more researchers to work on the rainfall estimation problem."}, {"title": "5.2.2 Comparison of Compute Costs", "content": "We further compare the compute costs between our ERIC with previous works. We measure their compute costs (time, memory, storage, etc.) for processing a 30-minute video file. Table 3 shows that our ERIC requires much less memory, GPU, and storage than previous work. In fact, the previous method by Jiang et al. [21] shows significant computation overhead, taking over three hours to process the file. On the contrary, owing to our robust features and the simplicity of our ANN model, ERIC-edge runs efficiently on the Raspberry Pi 4 device, taking only 12 minutes to process a 30-minute video. The great efficiency enables real-time processing of sensitive video data at the edge, thereby preserving user privacy. Our ERIC-cloud also shows better efficiency than 3DCNN [18], due to its smaller model architecture."}, {"title": "5.2.3 Analysis of Rainfall Estimation Performance", "content": "We compare the rain detection and estimation performance of our ERIC-edge and ERIC-cloud with 3DCNN in Table 4. Results show that both our ERIC-edge and ERIC-cloud outperform 3DCNN significantly. We hypothesize that this is because the 3DCNN model takes a sequence of frames (16 consecutive frames) as an input to capture the salient motions (falling raindrops in our case) across the frames [39], which could be sensitive to the unrelated moving objects in the scene. Our analysis of 3DCNN's prediction by checking testing videos confirms our hypothesis. It shows that the 3DCNN model could be deceived by the moving tree leaves (windy sunny days) because the moving leaves create a similar motion as the falling raindrops. On the contrary, our ERIC-cloud considers the static information within a single RGB frame to identify the presence of rain, e.g. \"rain fogs\", wet ground, cloudy sky, darker background, etc. It shows more robust performance for both rain detection and rainfall estimation. We show examples of detailed rainfall prediction plots for various locations in Fig. 17."}, {"title": "5.2.4 Water Saving Analysis", "content": "Our ERIC system targets precision residential irrigation. The key metric for evaluating its effectiveness is the reduction of both over-irrigation and under-irrigation. With more accurate hyperlocal rainfall (see Fig. 2), we analyze the effectiveness of ERIC by comparing the calculated irrigation requirement (IR) for August and September using rainfall from an on-site rain gauge (ground truth), ERIC estimation, and nearby weather station. Fig. 12 shows that ERIC saves 9,112 gallons of water ($28.56 utility savings) for August and provides the desired amount of irrigation water for September, avoiding potential turf damage due to under-irrigation."}, {"title": "5.2.5 Analysis of False Positives and Negatives", "content": "Fig. 17 shows that both ERIC-edge and ERIC-cloud accurately estimate the rainfall intensity and cumulative daily rainfall. Yet, some false positives and false negatives still exist. To understand why our models make these mistakes, we examined their false predictions by manually checking the recorded videos. We annotate the causes on each subplot (cf. Fig. 17). Interestingly, we made the following two observations:\n1) ERIC can detect short drizzles that are not even recorded by the rain gauge! For example, on May 4, 2021 at location 1, ERIC-cloud gives some false positive bumps (cf. Fig. 13 left) after the end of recorded rain. After checking the video, we find that it was indeed a drizzle that was too light to even trigger the tipping bucket of the rain gauge. This \"recovered true positive\" showcases the extraordinary capability of ERIC in detecting light rains, suggesting its potential to achieve even more accurate hyperlocal rainfall than the on-site rain gauge.\n2) ERIC-cloud is more robust than ERIC-edge. Because ERIC-edge detects rain from the reflections caused by the moving raindrops and splashes on the ground, it can be affected by the moving objects in the scene. An example is found on September 18, 2021 at location 1, where ERIC-edge predicts multiple false positive peaks due to the moving humans (cf. Fig. 14 (b)). On the contrary, ERIC-cloud gives correct predictions. We hypothesize that this is because ERIC-cloud uses the ResNet model which detects the rainfall based on the static information from the entire image, e.g. wet ground, cloudy sky, darker backgrounds, etc. A supporting evidence is found on May 4, 2022, at location 2 (see Fig. 13), where ERIC-edge fails to detect the rain at the beginning because the ground was still dry and there were no splashes from the puddles on the ground. However, ERIC-cloud captures the start of the rain accurately, likely from the darker clouds. We show more examples of causes in Fig. 14."}, {"title": "5.2.6 Impact of Rol Selection (AutoRol vs. Manual Rol)", "content": "We evaluate our proposed AutoRoI algorithm by comparing its performance with manually annotated RoI. We manually identify the regions with strong reflections and draw bounding boxes (cf. the green boxes in Fig. 6). Examples of AutoRoI are shown in Fig. 7. Results in Table 5 and Fig.7 show only a small performance gap between them, validating the effectiveness of our AutoRol algorithm."}, {"title": "5.2.7 Impact of Feature Modalities", "content": "ERIC-edge leverages both the visual and audio features for rain detection. We analyze the impact of each feature modality by comparing the performance of ERIC-edge with different input features: audio-only, visual-only, and audio+visual. We share our interesting observations below.\nCombining data of multi-modality performs the best. Table 6 shows that using visual features achieves much better performance than using audio features while adding audio features still provides noticeable improvements. This is likely because audio features provide"}]}