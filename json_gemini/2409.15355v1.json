{"title": "BLOCK-ATTENTION FOR LOW-LATENCY RAG", "authors": ["East Sun", "Yan Wang", "Tian Lan"], "abstract": "We introduce Block-Attention, an attention mechanism designed to address the increased inference latency in Retrieval-Augmented Generation (RAG) scenarios. Its main idea lies in dividing the input sequence into blocks, where each block calculates its key-value (KV) states independently except for the final block. In RAG scenarios, by defining each passage as a block, Block-Attention enables us to pre-compute the KV states for all passages and cache them in memory.\nThe implementation involves block segmentation, positional encoding calculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism. Experiments on four RAG benchmarks demonstrate that after block fine-tuning, the Block Attention model can achieve performance comparable to (68.4% vs 67.9% on Llama3) or even better (62.8% vs 59.6% on Mistral) than self-attention models. Notably, Block-Attention reduces the TTFT to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared with the self-attention model, the time consumption is reduced by 98.7%.", "sections": [{"title": "INTRODUCTION", "content": "Retrieval-Augmented Generation (RAG) (Li et al., 2022) is a crucial technology for mitigating knowledge hallucination in Large Language Models (LLMs). By utilizing retrieval technology, LLMs can seamlessly access passages stored in external databases, grounding their responses in the content of these passages. To the best of our understanding, RAG has emerged as the most effective method for infusing specific domain knowledge into LLMs in real-world scenarios.\nHowever, everything has two sides, and RAG is no exception. Generally, for each user query, in order to ensure that a passage with the \"correct knowledge\u201d is successfully recalled, we retrieve multiple passages (between 5 to 30 in most scenarios) and incorporate them into the input prompt of the LLM. Within this new and significantly longer input prompt, the inference latency, particularly the time to first token (TTFT), of a RAG-LLM will be much higher than that of a non-RAG-LLM.\nGiven that the content of passages in the external databases is already determined, one might naturally consider the feasibility of caching them for fast inference. Nonetheless, for autoregressive LLMs, the KV states are inherently context-dependent, which means the KV states for the same passage will vary in different contexts. As a result, when encountering an unseen query, the model must undertake a complete re-encoding of the KV states to ensure the accuracy of its predictions.\nIn this paper, we propose the Block-Attention method, which reduces the TTFT of RAG-LLMs to a level nearly equivalent to that of non-RAG LLMs, while fully maintaining the same accuracy level. As shown in Figure 1, the main idea of Block-Attention is to divide the entire input sequence into several blocks. Each block independently calculates its KV states through self-attention, without any attention to other blocks. Only the final block is able to attend other blocks. When utilizing Block-Attention in RAG scenarios, we may achieve substantial benefits by defining each passage as a single block and caching its KV states in the memory. The Block-Attention method has effectively"}, {"title": "BLOCK-ATTENTION", "content": ""}, {"title": "MAIN IDEA", "content": "Let $S = \\{s_0, s_1, ..., s_n\\}$ represents the input sequence, where each $s$ represents a token. We denote the KV states associated with $S$ as $K = \\{k_0, k_1,..., k_n\\}$ and $V = \\{v_0, v_1, ..., v_n \\}$, respectively. For an auto-regressive model OLLM, since the computation of the KV states is dependent on their preceding tokens, when a text block $\\{s_i,..., s_j\\}$ changes to a new text block $\\{s_i,..., s'_m\\}$, for the new sequence $S' = \\{s_0, ..., s'_i, ..., s'_m, s_{j+1}, ..., s_n \\}$, its KV states become $K' = \\{k_0, ..., k'_i, ..., k'_m, k'_{j+1},..., k_n\\}$ and $V' = \\{v_0, ..., v'_i, ..., v'_m, v'_{j+1},..., v_n\\}$. It is evident that although only one block $\\{s_i, ..., s_j\\}$ has changed, due to the auto-regressive nature of LLMs, the KV states of all subsequent blocks must be re-encoded."}, {"title": "BLOCK SEGMENTATION", "content": "The primary principle of block division is to segment semantically independent parts of the prompt into separate blocks. In RAG scenarios, since the retrieved passages are originally mutually independent, it is natural to divide them into different blocks. Therefore, let's go back to the left part of Figure 1, where we allocate each passage to a single block and designate the user's query as the final block. This principle extends to other scenarios as well. For example, in the context of code generation tasks, a function may be treated as one block; in multi-turn dialogues, each turn could be segmented into an individual block. In this paper, our primary focus is on the application of Block-Attention in RAG, with the exploration of other scenarios reserved for future research."}, {"title": "POSITIONAL ENCODING", "content": "The second problem is to re-encoding the positional information. Although the same passage may appear in multiple input prompts, its position generally varies. Therefore, when we attempt to reuse the KV states of a block, we need to re-encode its positional information. The process of re-encoding is simple and straightforward: taking the rotary positional encoding (RoPE) as an example, assume we wish to change the positional encoding of a block $b = \\{s_i, ..., s_j\\}$ to $b' = \\{s'_{i}, ..., s'_{j}\\}$, then we only need three steps:\n1) For the token $s_i$, its positional encoding vector $f(x_i, i)$ is calculated using the following formula:\n$f(x_i, i) = \\begin{pmatrix}\ncos i\\theta_0 & -sin i\\theta_0 & ... & 0 & 0 & p_0 \\\\\nsin i\\theta_0 & cos i\\theta_0 & ... & 0 & 0 & p_1 \\\\\n0 & 0 & ... & cos i\\theta_{d-1} & -sin i\\theta_{d-1} \\\\sin i\\theta_{d-1} & cos i\\theta_{d-1} & p_{d-1}\n\\end{pmatrix}$\n2) we set $x_i$'s positional encoding to zero, which is equivalent to rotating it counterclockwise by $i\\theta$ degrees\n$f(x_i, 0) = \\begin{pmatrix}\ncos i\\theta_0 & -sin i\\theta_0 & ... & 0 & 0 & 0 \\\\\nsin i\\theta_0 & cos i\\theta_0 & ... & 0 & 0 & 0 \\\\\n0 & 0 & ... & cos i\\theta_{d-1} & -sin i\\theta_{d-1} \\\\sin i\\theta_{d-1} & cos i\\theta_{d-1}\n\\end{pmatrix}f(x_i, i)$"}, {"title": "BLOCK FINE-TUNE", "content": "Due to the LLM's reliance on full self-attention during the training phase, a direct switch to Block-Attention during inference might result in a significant discrepancy between the training and inference states. Our preliminary findings indicate that introducing Block-Attention without subsequent fine-tuning could precipitate a substantial decrease in performance, with the average accuracy dropping significantly from 67.9% to 48.0%. Adapting the LLM to Block-Attention through fine-tuning, which we refer to as \"block fine-tune,\" is quite straightforward. The only difference from the standard SFT process is the modification of the traditional lower-triangular attention mask matrix to the attention mask matrix depicted in the right part of Figure 1. With this masking matrix, tokens in all blocks except the last are restricted to attending only to information within their own block, thereby ensuring consistency between training and inference."}, {"title": "INFERENCE", "content": "In inference, the Block-Attention model retrieves the KV states of blocks from the KV cache and concatenates them into the complete input KV states. The detailed process of the inference stage is depicted in Figure 2. Initially, we query and extract the KV states of the first $k-1$ blocks from the cache. Then, based on the position of each block within the input sequence, we calculate their positional encoding using Equation 3. Finally, using the KV states of the first $k-1$ blocks, we compute the KV states of the last block as well as the model output."}, {"title": "EXPERIMENTS", "content": "After the above analysis, most readers may have the following two concerns about Block-Attention: 1) Can the Block-Attention model achieve the same accuracy as self-attention in RAG scenarios? 2) How much can the Block-Attention mechanism improve the RAG efficiency? The following experimental results will reveal the answers to these two questions for everyone. In Sections 3.1, we analyze the accuracy of Block-Attention models. Meanwhile, in Section 3.2, we demonstrate the efficiency of Block-Attention."}, {"title": "Datasets", "content": "We evaluate the performance of the Block-Attention method on four RAG benchmarks: Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), HotpotQA (HQA) (Yang et al., 2018), and 2WikiMultiHopQA (2Wiki) (Ho et al., 2020). Specifically, we employ the training datasets of TQA and 2Wiki for fine-tuning models, followed by evaluations on all four respective test sets.\nIn training, we have randomly sampled 20,000 instances from both the TQA and 2wiki datasets to form our training set. The model's input comprises (1) the question and (2) 10 retrieved passages. To obtain these 10 passages, we employed the Contriver\u00b3 to identify the 10 most relevant passages from the provided dataset. Additionally, due to the fact that the original answers in the TQA and 2wiki datasets may not be included in the retrieved passages, this could lead the model to overlook the content of the retrieved passages and generate outputs directly. Instead, we utilize the outputs from GPT-4 as the model's output to ensure consistency between the retrieved passages and the final output."}, {"title": "Prompt Format", "content": "The format of input prompt follows Liu et al. (2024). For retrieved passages, we concatenate them in ascending order of retrieval score. An example is shown in Figure 3."}, {"title": "Base Model & Baselines", "content": "We implement the Block-Attention mechanism on two base language models: Llama3-8B-base\u2074 and Mistral-7B-v3.05. The models, after block fine-tuning, are denoted as Llama3-block and Mistral-block. In addition to these, we construct two strong baselines and two weak baselines."}, {"title": "MAIN RESULTS", "content": "From the results in Table 1, we can draw two important conclusions: 1) It is not advisable to directly switch from self-attention to Block-Attention, as it will lead to a sharp drop in the accuracy of the model. 2) However, if we use the Block-Attention mechanism in the fine-tuning stage, then the resultant model has almost the same performance as the self-attention model or is even slightly better on some datasets.\nThe first conclusion is easy to understand: The model has never seen an input sequence in the Block-Attention manner during the training process. Therefore, the sharp drop makes sense. Next, we will focus on analyzing the second conclusion. From Table 1, we can easily find that the models trained with two attention methods, namely Llama3-vanilla and Llama3-block, not only have comparable effects on in-domain test sets but also have little difference on out-domain test sets. This indicates that in the RAG scenario, it is completely feasible to replace self-attention with Block-Attention, and there will be almost no performance loss.\nMoreover, the experimental results on Mistral show some interesting results: the accuracy of Block-Attention models actually slightly outperformed the results of self-attention models on two datasets. It raised a conjecture: is Block-Attention a better attention mechanism than the self-attention mechanism? Intuitively, the semantics of each retrieved passage are mutually independent, and the self-attention method may cause the representation of the passage to be interfered by the context and"}, {"title": "EFFICIENCY", "content": "In the previous section, we already addressed our first concern: After fine-tuning, the Block-Attention model can achieve similar or even better performance than the self-attention model. In this section, we focus on our second concern: How much can the Block-Attention mechanism reduce the TTFT and the Flops of first token?\nTo quantify the effects of the Block-Attention mechanism on the TTFT, we show in Table 2 the respective TTFTs and Flops of the Llama3-block and Llama3-vanilla when the KV states of all retrieved passages have been pre-computed and cached in memory. Obviously, the acceleration effect is gratifying: Once the length of the input sequence is 512 and the length of user input is 50, using Block-Attention can reduce TTFT by 48% and reduce Flops by 90.1%. As the total length increases, when it is equal to 32K, the acceleration effect reaches an astonishing 98.7%, and the consumption of Flops is even reduced by 99.8%. We may simply conclude the results as: with greater text comes greater necessity for Block-Attention."}, {"title": "DISCUSSION", "content": "From the experimental results, we can easily figure out the effects of Block-Attention on existing RAG-based applications: Under the existing technical paradigm, developers need to deliberately balance the trade-off between accuracy and inference speed. Therefore, they have to limit the number of retrieved passages to a certain number, such as 3 to 10. With Block-Attention, the impact of the number of retrieved passages on inference speed will be greatly reduced, which will empower the developers to freely choose the number of retrieved passages that gives the optimal effect without any hesitation."}, {"title": "CONCLUSION", "content": "We introduce Block-Attention to optimize the inference efficiency of LLM in RAG scenarios. Its essence lies in separately calculating the KV states of independent blocks in the input prompt. Block-Attention enables us to pre-compute the KV states for all passages and cache them in memory, thus avoiding repeated computation of the KV states of the same passage during inference.\nOur experimental results across various RAG benchmarks demonstrated the profound impact of Block-Attention. We showed that Block-Attention can maintain the original reasoning accuracy of LLM while significantly reducing its TTFT and Flops in RAG scenarios. The effectiveness of Block-Attention becomes increasingly apparent as the number of passages increases or the frequency of retrievals increases. When considering the implementation of Block-Attention in your applications, bear in mind this guiding principle: With greater text comes greater necessity for Block-Attention.\nWe also want to note that Block-Attention plays an important role in many scenarios and is not limited to RAG only. We only introduced its effects on RAG because of that, due to some confidentiality reasons, we cannot disclose how we use it in our industrial applications. We look forward to researchers from the community being able to further explore the potential of Block-Attention and apply it to more appropriate scenarios."}]}