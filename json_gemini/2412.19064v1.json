{"title": "Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel Bidding", "authors": ["Shenghong He", "Chao Yu*", "Qian Lin", "Shangqin Mao", "Bo Tang", "Qianlong Xie", "Xingxing Wang"], "abstract": "Real-time bidding (RTB) plays a pivotal role in online advertising ecosystems. Advertisers employ strategic bidding to optimize their advertising impact while adhering to various financial constraints, such as the return-on-investment (ROI) and cost-per-click (CPC). Primarily focusing on bidding with fixed budget constraints, traditional approaches cannot effectively manage the dynamic budget allocation problem where the goal is to achieve global optimization of bidding performance across multiple channels with a shared budget. In this paper, we propose a hierarchical multi-agent reinforcement learning framework for multi-channel bidding optimization. In this framework, the top-level strategy applies a CPC constrained diffusion model to dynamically allocate budgets among the channels according to their distinct features and complex interdependencies, while the bottom-level strategy adopts a state-action decoupled actor-critic method to address the problem of extrapolation errors in offline learning caused by out-of-distribution actions and a context-based meta-channel knowledge learning method to improve the state representation capability of the policy based on the shared knowledge among different channels. Comprehensive experiments conducted on a large scale real-world industrial dataset from the Meituan ad bidding platform demonstrate that our method achieves a state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Real-Time Bidding (RTB) [1], [2] is a programmatic advertising mechanism where advertisers participate in bidding through automated platforms when a user visits a webpage, securing the opportunity to display ads on the page through either a first-price auction [3] or a second-price auction [4]. In RTB, advertising platforms formulate a bid request for every individual impression (i.e., display page) in real-time, and then advertisers submit bids for this impression utilizing their bidding algorithms. The bidding algorithms commonly assist in bid determination by considering factors in the users' preferences, requirement information, and other relevant issues, aiming to improve the revenue of the advertising platform (typically linked to user clicks) while at the same time adhering to constraints such as budget, cost-per-click (CPC) and return on investment (ROI), where CPC reflects the cost per ad click, and ROI is a measure of return on advertising investment.\nAn increasing number of advertising platforms have introduced automated bidding services in various ad settings, including recommendation ads (i.e., ads suggesting products to potential users) [5], [6], search ads (i.e., ads presented in response to user search queries) [7], [8], and others [7], [9]. These ad settings typically involve multiple advertising channels, each of which corresponds to a specific medium or user-customized service for ad delivery. However, most existing studies only focus on single-channel settings where the bidding strategy is applied to a single channel to improve bidding performance therein [10]\u2013[12]. Unlike single-channel bidding, in cross-channel bidding, advertisers can bid on multiple advertising channels at the same time, which requires the bidding algorithm to take into account diversities in user characteristics and behaviors of different channels, and calls for more complex budget allocation and management strategies to ensure efficient and effective budget utilization across different channels. Some studies [13], [14] have attempted to address the cross-channel bidding problem using single-channel methods. However, they have not considered the issues of channel budget allocation or interconnections between channels, which are critical in determining the final performance. In this paper, we focus on cross-channel bidding settings where the goal is to achieve global optimization of bidding performance across multiple channels with a shared budget (i.e., the objective is to maximize the benefits for advertisers and advertising platforms while enhancing the user's overall client experience).\nIn this setting, there exist two main challenges to be addressed. The first is the dynamic allocation of budgets among different bidding channels. A straightforward solution to this challenge is to share the total budget as a whole across different channels or allocate budgets to each channel based on a preset ratio. However, as the optimal pairing between channels and advertisers can evolve over time, such a static allocation scheme may yield myopic budget allocations and thus lead to suboptimal performance. Therefore, a better solution is to allocate the total budget dynamically to different channels based on the real-time information of channels and markets. The second challenge is the utilization of cross-channel interrelationships for efficient bidding decision making within each channel. In the cross-channel setting, an effective bidding strategy should not only consider the characteristic of each channel but also incorporate bidding information from other channels to achieve global maximiza-"}, {"title": "II. RELATED WORK", "content": "A. Bidding Optimization in RTB\nThe objective of bidding optimization is to determine the appropriate bid price for each impression presented in the auction to achieve total revenue maximization. Perlich et al. [15] first introduce a linear bidding strategy based on ad impression evaluation, which has been widely adopted in real-world applications. Zhang et al. [16] and Wang et al. [17] propose bidding optimization approaches based on the nonlinear relationships between optimal bids and ad impression evaluation. In addition, many works model bidding optimization from a sequence decision perspective and solve it using RL methods [10], [18]\u2013[20]. Cai et al. [10] utilize an MDP framework to sequentially allocate budget based on real-time impressions. Wang et al. [12] develop a course-guided Bayesian reinforcement learning (CBRL) framework to adaptively balance the multiple constraints in a non-stationary advertising market. Tunuguntla et al. [19] propose a multi-period dynamic programming model to provide advertisers with the optimal combination of generic and brand bids for search ads.\nMost existing studies concentrate on bidding optimization under single-channel setting and cannot be directly applied on scenarios where there are multiple ad channels with significant quality differences. On this basis, some studies [13], [14], [21]\u2013[23] have developed methods for allocating budgets across multiple advertising channels under total budget constraints via model-base RL or influencer probabilistic modeling. However, these methods require an accurate distribution estimation of bidding results, such as the expected number of clicks, and overlook the interconnections between different channels, which restricts their applicability. Moreover, Jin et al. [23] address bidding optimization across various channels through multi-agent reinforcement learning. However, they adopt a budget allocation scheme that shares the total budget as a whole across different channels, which can lead to a greedy budget allocation and thus globally suboptimal performance. While wen et al. [13] investigate the competition and cooperation relation among auto-bidding agents and propose a temperature-regularized credit assignment to establish a mixed cooperative-competitive paradigm, they did not consider the issue of channel budget allocation. Wang et al. [14] employ A-generalization to adapt to budget changes across different channels, treating each channel as an independent agent and aiming to achieve global optimality through individual optimal values. However, the assumption that channels will not affect each other does not hold in multi-channel advertising bid"}, {"title": "B. RL", "content": "Offline RL allows for the learning of effective policies solely from pre-existing data, thus avoiding potential risks induced by online interactions. However, offline RL encounters well-known distribution shift challenges [24], which is typically attributed to evaluation of actions that do not exist in the offline dataset (i.e., out-of-distribution actions) during policy learning. To mitigate this problem, recent studies have proposed several policy regularization methods that force the learned policy to stay close to the behavior policy by equipping policy optimization objective with various regularizers, including different divergence penalties [25]\u2013[27], implicitly weighted behavior cloning [28], [29], or reconstruction loss of the generative policy model [30], [31]. In contrast to directly regularizing the learned policies, an alternative approach is to incorporate behavioral regularization into value estimation objective, encouraging adherence to the behavioral distribution through a conservative value estimate towards unseen state-action pairs [32], [33].\nThe above methods require the setting of complex regularization terms to ensure the effectiveness of strategy learning, which becomes challenging for cross-channel advertising bidding platforms with diverse distribution changes (i.e., customer traffic, ad format and advertising request time). Our method is a simpler regularization method that decouples the process of learning behavior policy into state learning and action learning, which adopts an imitation-style manner to train the state policy and action policy. By doing so, our method can model state-state correlation, rather than state-action correlation in previous work, which avoids the extrapolation errors of evaluating out-of-distribution actions.\nMeta RL equips agents with the capability to quickly adapt to novel tasks through training on a diverse task distribution. Optimization-driven methods [34], [35] are one type of the most widely-studied approaches to achieve this goal, which formalizes the task adaptation process by employing policy gradients over a limited set of few-shot samples, thereby acquiring an optimal policy initialization. In addition, context-based Meta RL methods [36], [37] model the meta-learning problem as a partially observable Markov decision process (POMDP), where the task information is viewed as hidden components of states and extracted from historical trajectories.\nThe above methods focus on the distribution mismatch between offline data and online exploration data, and require additional online exploration. Unlike them, we consider using entirely offline data to learn channel common knowledge representation and leverage this common knowledge to enhance the learning effectiveness of the strategy."}, {"title": "III. PRELIMINARY", "content": "A. Reinforcement Learning\nA reinforcement learning (RL) problem is a sequential decision-making problem, which is generally modeled as a Markov Decision Process (MDP) represented by a tuple (S, A, P, R), where S is the state space of the environment with each state $s \\in S$ representing a unique situation within the environment, A denotes the set of all actions that an agent can take within the environment, $R : S \\times A \\rightarrow \\mathbb{R}$ is the numerical reward obtained as a result of a particular action taken within a specific state, and $P : S \\times A \\rightarrow [0,1]$ is the probabilistic transition function that captures the impact of an action on the future state, where $s_{t+1} \\in S$ is the next state that is observed after taking action $a_t \\in A$ in a given state $s_t \\in S$ at a time step t. The agent takes actions according to a policy $\\pi: S \\times A \\rightarrow [0,1]$, which maps the states to a probability distribution over actions. The goal of RL is to learn a policy $\\pi$ that maximizes the expected total reward in the environment:\n$\\max_{\\pi} J(\\pi) = \\max_{\\pi} \\mathbb{E}_{P,\\pi} \\sum_t R(s_t, a_t)$        (1)\nIn the offline RL setting where online interactions are inaccessible [25], [27], [32], the goal is to maximize the expected total reward using only the offline dataset $D = \\{(s_t, a_t, s_{t+1},r_t)\\}_{t=1}^T$ collected by a behavior policy. Training a policy that outperforms the behavior policy using historical data often entails querying the value function of the actions that were not seen in the dataset (i.e., out-of-distribution actions). These actions can be viewed as adversarial examples for the Q function [38], leading to an extrapolation error of value estimation."}, {"title": "B. Meituan Cross-channel Budget Sharing Ad System", "content": "Boasting over 400 million users and 66 million merchants, Meituan\u00b9 is one of China's largest ad bidding platforms for online services including food delivery, hotel and travel et al., which handles an average of more than 100 million impression requests every day. The overall process of Meituan advertising system is shown in Fig. 1. When receiving an advertising impression request, the advertising platform sends a bidding call to the RTB system. The RTB system then supports advertisers in determining a bidding price for this impression based on the real-time information of each channel as well as the budgets and bidding constraints specified by advertisers. After that, the advertising platform shows ads from the winning bidding groups and deducts the respective bid amounts from their budgets (i.e., the displayed advertiser will be charged the price of the second highest bid in this ad auction). Finally, this advertiser will receive revenue if a consumer clicks on the ad and makes a purchase.\nA significant feature of the Meituan ad system is that advertisers can place ads on one or more channels with a shared total budget that is updated every day. Because ad requests arrive at different times for each channel, the channel that receives the earliest ad request will send the ad bid. With a sufficient budget, advertisers often bid above the actual bidding price to get the ad slot, particularly for recommended and brand ads that do not require user activation. However, this may result in advertisers not having enough funds to invest in other channels (e.g., the search ad channel) later in the day, causing them to miss out potential users and ultimately reducing the revenue. While allocating a budget for each individual channel can address this issue, the complexity arises from varying budget requirements based on channel characteristics and the need for different budget amounts at different times due to the fluctuating customer traffic.\nIn addition, making optimal bidding decisions in cross-channel bidding requires considering information from multiple channels. For example, consider two advertisers, A and B, bidding on two channels, $C_1$ and $C_2$. Assume that A can successfully bid on both channels, while B can only succeed on $C_2$ when A is not bidding, and the advertising revenue for A from $C_1$ significantly outweighs that from $C_2$. In a scenario where $C_1$ and $C_2$ initiate ad bidding simultaneously, if A wins bids on both channels, B loses the opportunity to display ads on $C_2$, resulting in decreased revenue. Conversely, if A bids only on $C_1$ and B on $C_2$, both A and B can obtain corresponding revenues. Thus, effective bidding strategies should consider not only the characteristics of each channel but also the combined bidding information of multiple channels in order to achieve optimal decisions."}, {"title": "C. Optimization Objective", "content": "In this paper, our overall objective is to maximize total ad clicks while satisfying all advertisers' set budgets and CPC constraints, ensuring that the platform's revenue remains within an acceptable range. As shown in Fig. 2, our approach is divided into a top-level strategy and a bottom-level strategy. The goal of top-level strategy is to allocate budget to each channel while meeting the advertisers's total budget. Consider a scenario where M advertisers are served by a top-level strategy that allocates the budget of each advertiser to P channels. Each advertiser m provides a budget $B_m$ to the advertising platform once a day, and then the top-level strategy allocates this budget to different channels to constrain the bidding decisions therein. Formally, the objective of the top-level strategy can be expressed as follows:\n$\\max_{b^{m,t}} \\sum_{m=1}^{M} \\sum_{p=1}^{P} \\sum_{j=1}^{T} click(b_p^{m,j})$\n$\\text{subject to : } \\sum_{p=1}^{P} \\sum_{j=1}^{T} b_p^{m,j} \\leq B_m, \\forall m \\in \\{1,2,\\dots, M\\}$\n$\\text{and } CPC_{real}^m < CPC_{tar}^m,$\n(2)\nwhere $b_p^{m,j}$ denotes the allocated budget on channel p for advertiser m at time j, $click(b_p^{m,j})$ represents the number of clicks given the budget $b_p^{m,j}$ at time j, while $CPC_{real}^m$ and $CPC_{tar}^m$ represent the actual CPC realized by the advertiser m and the CPC target set by the advertiser m, respectively. To avoid the abuse of symbols, we use $b^j$ to represent the budget of an advertiser at time j across all channels. Table I shows a more detailed description of the symbols.\nAfter receiving the budget allocation b from the top-level strategy, the bottom-level strategy then maximizes the number of clicks while satisfying the budget. Assume there are T ad requests on channel p during the time interval, the objective of the bottom-level strategy can be expressed as follows:\n$\\max_{a} \\sum_{p=1}^{P} \\sum_{t=1}^{T} click(a_t^p)$\n$\\text{subject to: } \\sum_{t=1}^{T} cost(a_t^p) \\leq b_p, \\forall p \\in \\{1,\\dots, P\\},$\n(3)\nwhere $cost(a_t^p)$ denotes the actual cost, $b^p \\in b$ represents the budget of channel p, and $click(a_t^p)$ indicates whether the user has clicked the ad after giving a bidding price $a_t^p$. Note that, at the bottom-level, we take the number of ad requests as T. Hence, t represents both the number of ad requests and the time instance for the bottom-level strategy."}, {"title": "IV. METHOD", "content": "The overall framework of HMMCB is shown in Fig. 2. During the offline training process, the top-level strategy applies the CPC-constrained diffusion budget allocation method to achieve dynamic budget allocation among the channels while satisfying the CPC constraint, and the bottom-level strategy utilizes the state-action decoupled actor-critic method, context-based meta-channel knowledge learning method and multi-agent RL training scheme for offline RL, cross-channel knowledge sharing and cross-channel bidding decision making, respectively.\nDuring the online prediction process, the top-level strategy allocates the total budgets to each channel based on the global information from all channels. Meanwhile, the bottom-level strategy for a specific channel makes bidding decisions based on the allocated budgets and real-time channel information. Finally, the ads of the successful bidders will be displayed in the ad space of that channel."}, {"title": "A. Top-level Strategy for Dynamic Budget Allocation", "content": "1) MDP formulation for budget allocation. The top-level MDP for each advertiser m is formulated as a tuple (S, B, P, R, Y), where S is the state space, B denotes the action space, P indicates the state transition function, R represents the reward function, and y is the discount factor.\nState S: the $s \\in S = \\{O_1,\\dots,O_P\\}$ contains the state of each channel, where $O_p$ denotes the state space of the p-th channel, which comprises the advertiser allocated budget and historical statistics (e.g., the user-preferences, average click-through rate (CTR) and conversion rate (CVR) ). Note that an advertiser might engage in bidding activities exclusively in specific channels, with some channels lacking information related to this advertiser. To maintain data consistency and training stability, we employ zero tensors of identical dimensionality to supplement s.\nAction B: the $b \\in B$ encompasses the budget for each individual channel, where $b_p$ represents the budget allocated to channel p. We discretize the action using the percentage of the budget and mask invalid actions that exceed the total budget.\nReward IR: the reward $g \\in R$ is calculated as the sum of the number of user clicks across all channels within a given time interval.\nAccording to the above formulation, we obtain offline data $D_h$ = $[T_1, T_2,\u2026\u2026,T_N]$ from the bidding log, where $T_i = \\{s_j, b_j, g_j\\}_{j=1}^{T_i^2}$ denotes an offline trajectory.\n2) CPC-constrained diffusion model for dynamic budget allocation. Multi-channel advertising bidding is a dynamic environment with tens of thousands of advertisers and customers participating at every moment. In this scenario, the policy must be expressive and accurately capture the multi-modal distribution [31] to dynamically allocate an appropriate budget for each channel. To achieve this, we employ a policy regularization method that utilizes a diffusion model in the action space, forming a conditional diffusion model"}, {"title": "B. Bottom-level Strategy for Cross-channel Bidding", "content": "1) Bottom-level MDP for cross-channel constrained bidding. Since each request t only affects the cost of a specific channel p, we model each channel separately and represent it as a tuple(S, A, P, R, \u03b3).\nState O: the $o \\in O$ state contains the allocated budgets $b^h$, bidding requests, and advertiser information, where the bidding request comprises the request time and current advertising status (e.g., the budget consumption rate and the ratio of financial constraint satisfaction), while the advertiser information is identical to the state of the top-level strategy (O).\nAction A: the $a \\in A$ is a bidding ratio, and the final bidding price is calculated using $a_t \\times CPC^{tar}$. Note that $a \\in [\\S_{min}, \\S_{max}]$, while $\\S_{min}$ and $\\S_{max}$ have various values in different channels.\nReward R: the $r \\in R$ represents the number of user clicks on the ad. For each request t, if the bidding is successful and the user finally clicks the ad, the reward $r_t \\in \\{0,1\\}$ is set to 1. However, because using clicks as a reward may lead to costs exceeding a budget of the channel, we introduce a budget constraint\n$c_t = cost(a_t) - b_p,$\n(10)\nwhich ensures to maximize ad clicks while keeping the costs within predefined budget limits. Finally, the bottom-level reward is defined as follows:\n$r_t = click(a_t) - C_t.$\n(11)\nAccording to the above formulation, we obtain offline data $D_l$ = $[T_1, T_2,\u2026, T_N]$ from the bidding log, where $T_i = \\{o_t^1,..., o_t^{P_T}, a_t^1, ..., a_t^{P_T}, r_t^1,\u2026\u2026\u2026, r_t^{P_T}\\}_{t=1}^{F_1^3}$ denotes an offline trajectory for bottom level strategy learning.\n2) State-action decoupled actor-critic for bidding strategy learning. In advertising bidding systems where users can freely participate and exit, existing works face challenges in addressing the problem of out-of-distribution actions because the data distribution of bidding logs cannot cover the true bidding data distribution. We propose the state-action decoupled actor-critic method to overcome this problem by training a state value function, denoted as $V(o): O \\rightarrow \\mathbb{R}$, that only uses the samples without information of actions, i.e., $(o_t, o_{t+1})$, to evaluate and predict the optimal next state, and an action policy, denoted as $\\pi_a(o_t, o_{t+1}): O \\times O \\rightarrow [\\S_{min}, \\S_{max}]$, to infer the action given the predicted next state.\nIn order to approximate the optimal value function, we use the asymmetric least squares method from recent research [43], [44], which involves applying an 12 loss with a different weight using expectile regression, resulting in the following asymmetric 12 loss function:"}, {"title": "V. EXPERIMENT", "content": "In this section, we conduct a series of offline and online experiments to assess the performance of HMMCB. We first describe the primary setup, and then compare HMMCB with representative learning models to validate its effectiveness. Finally, ablation studies are conducted to verify the efficacy of each key component and hyperparameter in HMMCB.\nA. Experiment Setup\n1) Dataset: We use log data from the Meituan real-time advertising system for offline training and performance evaluation. These data are a mixture of expert, medium and random data, where expert data consists of bids achieving an advertising ROI exceeding the expected ROI while satisfying CPC constraints through an automated bidding strategy, medium data includes bids that either meet CPC constraints or surpass the expected ROI, and random data is bids generated through exploration with a random strategy. Specifically, these data span 35 days of bidding logs collected from four channels: feed ad, search ad, brand ad, and recommendation ad, with an average sampling of 80 million ad requests from 72,351 advertisers across four channels each day and Table II describing the important features in the logs. The dataset is divided into two segments for training and evaluation: 28 days of bidding data for training and 7 days for evaluation.\n2) Simulated offline evaluation system: Implementing a predictive model into an operational system without comprehensively evaluating its potential implications has inherent risks (i.e., financial losses). To mitigate this issue, we have developed an offline evaluation system for simulating the realistic cross-channel bidding process. This system comprises two modules: an advertising system simulator and a user feedback predictor. The former simulates the Meituan online advertising platform, including retrieval, bidding, ranking, and pricing, while the latter predicts the user feedbacks on ads and evaluates the results.\n3) Compared methods: We compare the recent methods that can or can be adapted to handle multi-channel constraints, categorized into three lines of works: (1) single-channel slot-wise approximation methods PID [51] and CEM [52]; (2) soft combination (RL-based) methods MCQ [53], CBRL [12] and HiBid [14]; and (3) a hierarchical offline method OPAL [54].\nPID is a classical feedback controller, known for its effective performance in unknown environments. We utilize it to maintain the advertiser's current CPC close to the target CPC, ensuring compliance with the advertiser's cross-channel CPC constraint.\nCEM is a gradient-free stochastic optimization technique widely employed in the industry, which strives to optimize a greedy sub-problem within each time slot, striking a balance between exploration and exploitation. We formulate the multi-channel bidding problem as an optimization challenge to maximize the click count within the specified total budget and CPC constraint.\nMCQ effectively mitigates the value overestimation effect that arises in out-of-distribution actions by proactively training and adjusting their Q values, which is currently considered a state-of-the-art method in the field of offline DRL. We use MCQ to model each channel and set clicks as rewards, thus encouraging the strategy to make bidding decisions that maximize the click count.\nCBRL is a recognized state-of-the-art approach in the context of ROI-restricted single-channel bidding, which combines Bayesian techniques with an indicator-augmented reward function designed to dynamically manage the trade-off between constraints and objectives. To ensure a fair comparison, we adapt CBRL to the multi-channel ad bidding setting by substituting the ROI constraint with a CPC constraint while keeping its original training process.\nOPAL is a hierarchical offline algorithm, where the top-level agent is trained using unsupervised learning to provide a temporal abstraction for the bottom-level agent to improve the final policy optimization. OPAL and HMMCB are configured with identical hierarchical settings, i.e., the top-level agent allocates the budget, while the bottom-level agent makes advertising bidding decisions.\nHiBid is a novel algorithm for bid budget allocation that incorporates an auxiliary loss function to mitigate the risk of over-allocation to specific channels. The algorithm employs A-parametric generalization to adapt to variations in budgetary constraints, and integrates a CPC guided action selection mechanism to satisfy cross-channel CPC constraints."}, {"title": "B. Performance Comparison", "content": "1) Offline performance comparison: In the offline setting, HMMCB is compared with PID, CEM, OPAL, CBRL and MCQ. Results in Table IV indicate that HMMCB excels in maximizing both the ROI and IMPR, and effectively reduces the CPC, thereby enhancing the overall cost-effectiveness of the system. Although CBRL can progressively adapt its bidding strategy to meet constraints through the learning process, its performance still falls short of HMMCB, which can be attributed to the inability to allocate the budget accurately. MCQ uses a conservative strategy to prevent overestimation of errors in out-of-distribution actions, but it suppresses the generalization of the value function and hinders performance improvement, resulting in a 1.91% increase in CPC. OPAL demonstrates a 1.20% improvement in IMPR, but a decrease in CLICKS, which can be attributed to the fact that the budget allocation strategy learned through supervised learning lacks appropriate adjustments in unfavorable situations. Due to limitations in policy representation capability, CEM struggles to effectively accommodate advertisers' requirements, resulting in a significant increase in CPC to 3.59%. While dynamically adjusting bidding prices based on the current CPC, PID fails to consider the variability in average CPC across different channels, leading to a 14.91% improvement in CPC, but decrease in all other metrics. HiBid introduces auxiliary losses to prevent channel-specific over-allocation and uses A-generalization to adapt to budget changes, but it ignores the internal connections between individual channels, resulting in only a 5.35% ROI improvement.\nCompared with these models, HMMCB uses a diffusion strategy with CPC constraints to dynamically allocate budgets according to the characteristics of each channel, and uses the state action decoupled actor-critic method to avoid overestimation of out-of-distribution actions. In addition, HMMCB is trained using the CMCK and central value function, allowing it not only to consider the specific attributes of each channel but also to coordinate all channels to achieve a global optimal performance. As a result, HMMCB achieves the highest ROI while ensuring superiority in other metrics. A comprehensive depiction of the training convergence process is presented in Fig. 3. Compared with other single-channel methods, HMMCB requires simultaneous learning of the characteristics of different channels to provide the best overall bidding solution, hence it requires multiple iterations to reach convergence.\n2) Online A/B testing on the Meituan advertising platform: HMMCB and all other five baselines are validated using the Meituan advertising platform across four channels: feed ad, search ad, brand ad, and recommendation ad, with each method running a total test time of two weeks. As depicted in Fig. 4a, HMMCB surpasses all other baseline methods in all metrics, achieving an increase of at least 1.15% in IMPR, 8.30% in CLICKS, 8.65% in ROI, and a decrease of 8.83% in CPC. Fig. 4b displays the TP999 (i.e., completion time for 99.9% requests) for each model. While the HMMCB method takes into account the characteristics of different channels during training, each channel's strategy is executed independently during the evaluation, therefore it can still respond to requests in a relatively short amount of time.\nIn addition, we show that HMMCB can continually improve its performance by swiftly adapting to the evolving environment. To this end, we conduct the following iterated training"}, {"title": "C. Further Study", "content": "In this section, we explore the comparison of the state-action decoupled actor-critic method and the context-based knowledge sharing approach (CMCK) with several existing works. Specifically, we replace the state-action decoupled actor-critic method with mainstream offline RL methods CQL [53], IQL [43], and BCQ [56], respectively. Similarly, we substitute the context-based knowledge-sharing approach with offline meta-RL methods, including GENTLE [57], CORRO [58], and CSRO [59]. Fig. 5a shows that the state-action decoupled actor-critic method consistently outperforms the CQL, IQL, and BCQ methods. CQL and BCQ prioritize learning policies that are very similar to behavioral policies, which limits their ability to leverage non-expert data to enhance policy performance. IQL reduces extrapolation errors in the policy improvement process by avoiding the direct evaluation of actions that are not present in the dataset. However, IQL employs advantage-weighted behavior cloning to learn a policy, which may be inadequate for capturing the optimal policy distribution in a bidding dataset containing a significant proportion of non-expert data. Fig. 5b shows the comparison results between CMCK and Meta-RL methods, indicating that CMCK achieves the highest ROI, while GENTLE results in the lowest ROI. GENTLE increases channel data by reconstructing state transitions and rewards, which may bring larger distribution errors, causing the learned policy to deviate from the actual data distribution.\nAdditionally, HMMCB is compared with existing multi-agent methods to highlight its practicality in multi-channel bidding scenarios. Specifically, we compare the bidding multi-agent algorithm MAAB [13], which uses the temperature-regularized credit assignment to handle bidding relationships between different channels. The original MAAB algorithm does not meet the requirements of existing bidding environments, as it does not consider channel budgets and is designed for settings where channels exhibit cooperative-competitive relationships. We modify the MAAB to align with the cooperative environment settings and allocate budgets proportionally according to the historical revenue ratios of each channel. Moreover, we also consider representative offline MARL algorithms, ICQ [47], OMAR [48], and OMIGA [49], with the core idea of replacing global regularization with local regularization to learn the optimal joint policy. The results in Table VII indicate that HMMCB consistently outperforms the MAAB, ICQ, OMAR, and OMIGA methods. This superior performance arises not only from HMMCB's effective resolution of the out-of-distribution (OOD) problem but also from its dynamic budget allocation and efficient handling of different channel characteristics. In contrast, the ICQ, OMAR and OMIGA algorithms focus on solving the offline multi-agent OOD problem, which makes them unable to obtain satisfactory performance in complex and stochastic bidding"}, {"title": "D. Ablation Study", "content": "In this section, we modify each component within HMMCB to demonstrate their individual effectiveness, with the results given in Table VIII. First, we evaluate the influence of top-level strategies by comparing bidding policies with and without CPC constraints. Without a CPC constraint, HMMCB can disregard the overall CPC to maximize clicks, leading to a 6.25% surge in clicks while also causing a 1.34% rise in CPC. Second, we examine the influence of policy regularization capability, specifically focusing on whether employing diffusion model as a top-level strategy can enhance the effectiveness of our bidding model. Results show employing a traditional MLP as a top-level strategy degrades the overall model performance, due to its incapability in accurately capturing the data distribution across multiple channels. Finally, we explore the impact of CMCK and central value function on the final performance. In scenarios where the state policy is not learned using CMCK, independently trained state policies struggle to effectively generalize the optimal next state for each channel. Hence, it incurs a reduction of 1.34% in ROI and an increase of 3.25% in CPC. When each bottom-level strategy undergoes independent training without reliance on a central value function, there is a notable 7.43% decrease in ROI, despite a 9.54% increase in clicks. This discrepancy can be attributed to the fact that each policy solely focuses on maximizing its own returns without considering the interrelationships among different channels, leading to a local optimum in overall performance.\nIn addition, we use SNE [60] to visualize the feature distribution of $\u03c0_s(s_t)$ and $s_{t+1}$ to illustrate how the state strategy guides the action strategy in bidding decisions. The results are shown in Fig. 6, where the left image illustrates the distribution of the state, while the right image represents the reward value for each state. We can see that the state distribution of $\u03c0_s$ follows the distribution of the original data and achieves higher returns, which is crucial in guiding the action policy to make better decisions through accumulating and propagating high return through multiple time steps."}, {"title": "VI. CONCLUSION", "content": "In this paper, we conduct in-depth analysis of the cross-channel bidding problem and propose a hierarchical optimization architecture HMMCB to solve it. HMMCB enables the dynamic allocation of channel budgets by implementing a CPC-constrained diffusion model, and uses a state-action decoupled actor-critic method and a context-based meta-channel knowledge learning method to utilize the cross-channel interrelationships for efficient bidding decision making within each channel. Both offline and online experiments based on industry data from the Meituan ad bidding platform validate the superiority of HMMCB against some traditional or state-of-the-art methods."}]}