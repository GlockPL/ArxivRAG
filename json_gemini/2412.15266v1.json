{"title": "On the Structural Memory of LLM Agents", "authors": ["Ruihong Zeng", "Jinyuan Fang", "Siwei Liu", "Zaiqiao Meng"], "abstract": "Memory plays a pivotal role in enabling large language model (LLM)-based agents to engage in complex and long-term interactions, such as question answering (QA) and dialogue systems. While various memory modules have been proposed for these tasks, the impact of different memory structures across tasks remains insufficiently explored. This paper investigates how memory structures and memory retrieval methods affect the performance of LLM-based agents. Specifically, we evaluate four types of memory structures, including chunks, knowledge triples, atomic facts, and summaries, along with mixed memory that combines these components. In addition, we evaluate three widely used memory retrieval methods: single-step retrieval, reranking, and iterative retrieval. Extensive experiments conducted across four tasks and six datasets yield the following key insights: (1) Different memory structures offer distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed memory structures demonstrate remarkable resilience in noisy environments; (3) Iterative retrieval consistently outperforms other methods across various scenarios. Our investigation aims to inspire further research into the design of memory systems for LLM-based agents.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Minaee et al., 2024) have attracted widespread attention in natural language tasks due to their remarkable capability. Recent advancements have significantly accelerated the development of LLM-based agents, with research primarily focusing on profile (Park et al., 2023; Hong et al.), planning (Qian et al., 2024; Qiao et al., 2024), action (Qin et al., 2023; Wang et al., 2024c), self-evolving (Zhang et al., 2024a) and memory (Packer et al., 2023; Lee et al., 2024). These innovations have unlocked a wide range of applications across diverse applications (Li et al., 2023; Wang et al., 2024b; Chen et al., 2024).\nA fundamental element that underpins the effectiveness of LLM-based agents is the memory module. In cognitive science (Simon and Newell, 1971; Anderson, 2013), memory is the cornerstone of human cognition, enabling the storage, retrieval, and drawing from past experiences for strategic thinking and decision-making. Similarly, the memory module is vital for LLM-based agents by facilitating the retention and organization of past interactions, supporting complex reasoning capabilities, e.g., multi-hop question answering (QA) (Li et al., 2024a; Lee et al., 2024), and ensuring consistency and continuity in user interactions (Nuxoll and Laird, 2007).\nDeveloping an effective memory module in LLM-based agents typically involves two critical components: structural memory generation and memory retrieval methods (Wang et al., 2024a; Zhang et al., 2024b). Among the various memory structures used by agents, chunks (Hu et al., 2024), knowledge triples (Anokhin et al., 2024), atomic facts (Li et al., 2024a), and summaries (Lee et al., 2024) are the most prevalent. For instance, HiAgent (Hu et al., 2024) utilizes sub-goals as memory chunks to manage the working memory of LLM-based agents, ensuring task continuity and coherence, while Arigraph (Anokhin et al., 2024) adopts knowledge triples, which combine both semantic and episodic memories to store factual and detailed information, making it suitable for complex reasoning tasks. Meanwhile, ReadAgent (Li et al., 2024a) compresses memory episodes into gits memory with summaries manner, organizing them within a structured memory directory.\nUpon reviewing the aforementioned memory structures, an important but under-explored question arises: Which memory structures are best suited for specific tasks, and how do their distinct characteristics impact the performance of LLM-based agents? This question mirrors how humans organize memory into distinct forms, such as episodic memory for recalling events and semantic memory for understanding relationships (Simon and Newell, 1971; Anderson, 2013). Each form serves a unique purpose, enabling humans to tackle a variety of challenges with flexibility and precision. Moreover, humans rely on effective retrieval processes to access relevant memories, ensuring the accurate recall of past experiences for problem-solving. This highlights the need to jointly explore memory structures and retrieval methods to enhance the reasoning capabilities and overall effectiveness of LLM-based agents.\nTo bridge this gap, we systematically explore the impact of various memory structures and retrieval methods in LLM-based agents. Specifically, we evaluate existing four types of memory structures: chunks (Hu et al., 2024), knowledge triples (Anokhin et al., 2024), atomic facts (Li et al., 2024a), and summaries (Li et al., 2024a). Building on these, we explore the potential of mixed memory structures, which combine multiple types of memories to examine whether their complementary characteristics can enhance performance. Additionally, we assess the robustness of these memory structures to noise, as understanding their reliability under such conditions is essential for ensuring effectiveness across diverse tasks. Furthermore, we investigate three memory retrieval methods, including single-step retrieval (Packer et al., 2023), reranking (Gao et al., 2023a), and iterative retrieval (Li et al., 2024b), to uncover how different combinations of retrieval methods and memory structures influence overall performance.\nThe main contributions of this work can be summarized as follows: (1) We present the first comprehensive study on the impact of memory structures and memory retrieval methods in LLM-based agents on six datasets across four tasks: multi-hop QA, single-hop QA, dialogue understanding, and reading comprehension. (2) Our findings reveal that mixed memory consistently achieves balanced and competitive performance across diverse tasks. Chunks and summaries excel in tasks involving extensive and lengthy context (e.g., reading comprehension and dialogue understanding), while knowledge triples and atomic facts are particularly effective for relational reasoning and precision in multi-hop and single-hop QA. Additionally, mixed memory demonstrates remarkable resilience to noise. (3) Iterative retrieval stands out as the most effective memory retrieval method across most tasks, such as multi-hop QA, dialogue understanding and reading comprehension."}, {"title": "Related Works", "content": "The advent of Large Language Model (LLM) has positioned them as a transformative step towards achieving Artificial General Intelligence (AGI) (Wang et al., 2024a), offering robust capabilities for the development of LLM-based agents (Xi et al., 2023; Xu et al., 2024). Current research in this field primarily focuses on agent planning (Wang et al., 2023; Yao et al., 2024; Qian et al., 2024; Qiao et al., 2024), reflection mechanisms (Shinn et al., 2024; Zhang et al., 2024a), external tools utilization (Qin et al., 2023; Wang et al., 2024c), self-evolving capabilities (Zhang et al., 2024a) and memory modules (Hu et al., 2024; Lee et al., 2024)."}, {"title": "Memory Structures", "content": "Memory module serves as the foundation of LLM-based agents, enabling them to structure knowledge, retrieve relevant information, and leverage prior experiences for reasoning tasks (Zhang et al., 2024b). Among the widely adopted memory structures of memory module are chunks (Packer et al., 2023; Liu et al., 2023; Hu et al., 2024), knowledge triples (Anokhin et al., 2024), atomic facts (Li et al., 2024a), and summaries (Lee et al., 2024). For instance, HiAgent (Hu et al., 2024) incorporates sub-goals as memory chunks to maintain task continuity and coherence across interactions. On the other hand, GraphReader (Li et al., 2024a) employs atomic facts to compress chunks into finer details, providing agents with highly granular information that improves precision in multi-hop question answering tasks. In this paper, we investigate how various memory structures impact the performance of LLM-based agents."}, {"title": "Memory Retrieval", "content": "The memory retrieval method is another critical component of the memory module, enabling LLM-based agents to retrieve relevant memories to advanced reasoning. To facilitate this, LLM-based agents often employ retrieval-augmented generation (RAG) (Lewis et al., 2020; Fang et al., 2024), where relevant memories are first retrieved and then used to generate answers with LLMs. In this setting, the retrieved memories are prepended to the queries and serve as input to the LLM to generate response (Ram et al., 2023). The most straightforward retrieval method is the single-step retrieval (Packer et al., 2023; Zhong et al., 2024), which aims to identify the Top-K most relevant memories for the query. Additionally, reranking (Gao et al., 2023a; Ji et al., 2024) leverages the language understanding capabilities of LLMs to prioritize retrieved memories, while iterative retrieval (Li et al., 2024b; Shi et al., 2024) focuses on reformulating queries to improve retrieval accuracy. These innovations make memory retrieval more adaptive and consistent with the query, maintaining effective performance across diverse and complex tasks. In this paper, we explore how different combinations of retrieval methods and memory structures influence overall performance."}, {"title": "Methodology", "content": "Figure 2 illustrates the overview of the memory module within LLM-based agents, highlighting three key components: Structural Memory Generation, Memory Retrieval Methods and Answer Generation. This section begins with an introduction to structural memory generation in \u00a7 3.1. Next, we introduce memory retrieval methods in \u00a7 3.2. Finally, \u00a7 3.3 discusses answer generation methods."}, {"title": "Structural Memory Generation", "content": "Structural memory generation enables agents to organize raw documents into structured representations. By transforming unstructured documents Dq into structural memory Mq, the agent gains the ability to store, retrieve, and reason over information more effectively. In this work, we explore four distinct forms of structural memory: chunks Cq, knowledge triples Tq, atomic facts Aq, or summaries Sq. The generation process for each structural memory is detailed as follows:\nChunks (Cq). Chunks (Gao et al., 2023b) are a widely used form of structural memory in LLM-based agents. Each chunk represents a continuous segment of text from a document, typically constrained to a fixed number of tokens L. Formally, raw documents Dq can be divided into a series of chunks, as defined: Cq(Dq) = {C1, C2, ..., Cj }, where each chunk cj contains at most L tokens."}, {"title": "Memory Retrieval Methods", "content": "Given the generated structural memories Mq, we employ a memory retrieval method to identify and integrate the most relevant supporting memories Mr \u2286 Mq for the query q. Without this step, the agent would need to process all available memories, leading to inefficiency and potential inaccuracies due to irrelevant information. Our study mainly focuses on three retrieval approaches: single-step retrieval (Robertson et al., 2009; Rubin et al., 2022), reranking (Gao et al., 2023a; Ji et al., 2024), and iterative retrieval (Li et al., 2024b; Shi et al., 2024). The details of each memory retrieval method are outlined as follows:\nSingle-step Retrieval. In the single-step retrieval process, the goal is to identify the Top-K memories Mr that are most relevant to the query q. This process is formally defined as: Mr = Retriever(q, Mq, K), where the Retriever (Robertson et al., 2009; Rubin et al., 2022) serves as the core component.\nReranking. In the reranking process (Gao et al., 2023a; Dong et al., 2024), an initial retriever selects a candidate set of Top-K memories Mi, which are then reranked by an LLM prompted with Prerank based on their relevance scores. From this reranked list, the Top-R memories Mr, selected in descending order of relevance scores, are identified as the most relevant. This step enhances retrieval precision by leveraging the LLM to strengthen query-memory connections, filtering out irrelevant memories, and prioritizing the most pertinent memories for the query. This process is formally defined as: M\u2081 = LLM(q, Mi, R, PR), where M\u2081 = Retriever(q, Mq, K).\nIterative Retrieval. The iterative retrieval approach (Gao et al., 2023b) begins with an initial query q0 = q and retrieves the Top-T most relevant structural memories Mj. These retrieved memories are used to refine the query through an LLM prompted by Prefine. This process is repeated over N iterations, refining the query to produce the final version qN that is informative for retrieving relevant memories. Formally, the iterative retrieval process can be defined as follows: qj = LLM(Mj, PRefine), where Mj = Retriever(qj\u22121, Mq, T). After N iterations, the final refined query qN is used to retrieve the Top-K most relevant memories for answer generation. This step can be expressed as: Mr = Retriever(qN, Mq, K). The detailed prompts Prerank and Prefine can be found in Appendix B."}, {"title": "Answer Generation", "content": "Finally, the agent leverages the LLM to generate the answer based on the retrieved memory. To achieve this, we propose two methods of answer generation. In the first method, termed Memory-Only, the retrieved memories M are directly utilized as the context for generating the answer. The second method, termed Memory-Doc, uses the retrieved memories to locate their corresponding original documents from Dq. These documents then serve as the context for answer generation, providing the agent with more detailed and contextually enriched information."}, {"title": "Experiments", "content": "We conduct experiments on six datasets across four tasks. For multi-hop long-context QA datasets, we experiment with HotPotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022). The single-hop long-context QA task is evaluated with NarrativeQA (Ko\u010disk\u1ef3 et al., 2018) from Longbench (Bai et al., 2023). Additionally, we leverage the LoCoMo dataset (Maharana et al., 2024) for dialogue-based long-context QA task, while the QuALITY (Pang et al., 2022) dataset is used for the reading comprehension QA task2."}, {"title": "Evaluation", "content": "To evaluate QA performance, we follow previous work (Li et al., 2024a) and use standard metrics such as Exact Match (EM) score and F1 score for the datasets HotPotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA and LoCoMo. For QuALITY, we follow the approach in (Lee et al., 2024) and use accuracy as the evaluation metric, with 25% indicating chance performance."}, {"title": "Implementation Details", "content": "In our experiments, we use GPT-40-mini-128k with a temperature setting of 0.2. The input window is set to 4k tokens, while the maximum chunk size is up to 1k tokens. For text embedding, we employ the text-embedding-3-small model 3 from OpenAI and store the vectorized memories using LangChain (Chase, 2022)."}, {"title": "Results and Analysis", "content": "The results as presented in Table 1 reveal key insights into the impact of various memory structures on task performance: (1) Mixed memories consistently outperform other memory structures. This is particularly evident under iterative retrieval, where mixed memories achieve the highest F1 scores of 82.11% on HotPotQA and 68.15% on 2WikiMultihopQA. (2) Chunks excel in tasks requiring a balance between concise and comprehensive contexts, as shown in datasets with long contexts. This is evidenced by its F1 score of 31.63% on NarrativeQA and an accuracy of 78.5% on QuALITY under reranking. Summaries, which condense large contexts, is effective for tasks demanding abstraction, as shown by its competitive F1 score of 32.26% on NarrativeQA and solid performance on LoCoMo. (3) Knowledge triples and atomic facts are particularly effective for relational reasoning and precision. Knowledge triples achieve an F1 score of 62.06% on 2WikiMultihopQA under iterative retrieval, while atomic facts achieve an F1 score of 81.29% on HotPotQA. These findings emphasize the importance of tailoring memory structures to specific task requirements and demonstrate that integrating complementary memory types in mixed memories significantly enhances performance across tasks."}, {"title": "Impact of Memory Retrieval Methods", "content": "The results in Table 1 demonstrate the significant influence of the retrieval method on performance: (1) Iterative retrieval consistently outperforms the others, achieving the highest scores across most datasets. Notably, with mixed memories, iterative retrieval achieved an F1 score of 82.11% on HotPotQA and 68.15% on 2WikiMultihopQA, showcasing its ability to refine queries iteratively for enhanced accuracy. (2) Reranking demonstrates strong performance on datasets with moderate complexity. For instance, it achieved F1 scores of 44.27% on LoCoMo and 28.19% on NarrativeQA with atomic fact memory. (3) In contrast, single-step retrieval performs competitively in tasks requiring minimal contextual integration. Using summary memory, it achieved an F1 score of 32.93% on NarrativeQA, leveraging abstraction to extract coherent information. These findings emphasize the importance of aligning retrieval mechanisms with task requirements, and iterative retrieval excels in reasoning tasks."}, {"title": "Impact of Answer Generation Approaches", "content": "As shown in Figure 3, which compares their performance across various datasets. retrieving documents through retrieved memories provides a more comprehensive understanding, much like how humans integrate immediate recall with broader context to interpret complex narratives. In contrast, for datasets involving multi-hop reasoning and dialogue understanding, such as HotPotQA and LoCoMo, the Memory-Only approach proves to be the more effective strategy. These findings highlight that tasks requiring extensive context benefit from the Memory-Doc approach, which incorporates broader document-level information for enriched responses. On the other hand, tasks prioritizing precision are better suited to the Memory-Only approach, ensuring focused and accurate retrieval."}, {"title": "Hyperparameter Sensitivity", "content": "We first evaluate the impact of K in single-step retrieval, with a limit of K = 200 due to computational resource limitations. As depicted in Figure 4, in HotPotQA, chunks demonstrate consistent performance, stabilizing around 77% across all K values. In LoCoMo, the chunks show moderate gains up to K = 50, whereas triples, atomics, and summaries improve up to K = 100 but then declined at K = 200, likely due to noise introduced by retrieving excessive memories. These findings indicate that the optimal K depends on both the dataset and memory structure. While moderate K values generally enhance performance, excessively large values can introduce irrelevant information, leading to a degraded performance.\nTo evaluate the impact of R in reranking, we investigate performance across a range of values, with a maximum R of 75 due to computational cost constraints, while fixing K at 100. As depicted in Figure 5, the results highlight that increasing the number of reranked memories does not always lead to better performance. For instance, chunks achieve the highest F1 score at R = 10 in HotPotQA, with a subsequent decline in performance beyond R = 50. This pattern is consistent with triples and atomic facts, indicating that selecting a smaller number of highly relevant memories can outperform retrieving and reranking larger sets, which often introduces noise. A similar trend can be observed in LoCoMo. These findings suggest that reranking is more effective when it focuses on a smaller subset of highly relevant memories.\nWe first investigate performance across a range of values of T using iterative retrieval, with a maximum T of 75 and N of 4 due to computational cost constraints while keeping K fixed at 100. As illustrated in Figure 6, increasing the number of retrieved memories per iteration generally improves performance across datasets, though the gains diminish beyond a certain threshold. For instance, in HotPotQA, atomic facts achieve an F1 score of approximately 81% at T = 50, with minimal additional gains from increasing T further. Similarly, in LoCoMo, chunks improve up to T = 50 before declining at T = 75. These results indicate that while increasing T can enhance query refinement and performance, excessively large T values may introduce noise, ultimately reducing effectiveness."}, {"title": "Effect of Number of Iteration Turns N", "content": "Next, we examine the impact of iteration turns N, with the number of retrieved memories T fixed at 50. As depicted in Figure 6, the results reveal that increasing N initially enhances performance significantly, but the rate of improvement diminishes as N continues to rise. For HotPotQA, both triples and summars show notable gains from N = 1 to N = 3, after which the improvements become marginal. In the case of LoCoMo, triples, atomic facts, and summaries reach a peak at N = 3 and stop increasing afterwards. These results suggest that an intermediate number of iteration turns, typically between 2 and 3, achieves optimal performance improvements, striking a balance between maximizing effectiveness and minimizing resource expenditure."}, {"title": "Impact of Noise Documents", "content": "Finally, we evaluate the robustness of various memory structures under increasing levels of noise using single-step retrieval with a fixed K = 100. As depicted in Figure 8, the performance of all memory structures declines as the number of noise documents increases. For HotPotQA, the mix memory consistently achieves the highest F1 scores, demonstrating superior resilience to noise. While triples and summaries exhibit similar rates of decline, the chunks experience a slower decline, maintaining a competitive F1 score when increasing the number of noise documents. A similar pattern is shown in LoCoMo. These findings reveal the robustness of the mixed memory structure, which consistently outperforms others across datasets, making it the most effective choice in noisy environments."}, {"title": "Conclusion & Future Work", "content": "In this paper, we present the first comprehensive study on the impact of structural memories and memory retrieval methods in LLM-based agents, aiming to identify the most suitable memory structures for specific tasks and explore how retrieval methods influence performance. This study yielded several key findings: (1) Mixed memories consistently deliver balanced performance. Chunks and summaries excel in tasks involving lengthy contexts, such as reading comprehension and dialogue understanding, while knowledge triples and atomic facts are effective for relational reasoning and precision in multi-hop and single-hop QA. (2) Mixed memories also demonstrate remarkable resilience to noise. (3) Iterative retrieval stands out as the most effective memory retrieval method, consistently outperforming in tasks such as multi-hop QA, dialogue understanding and reading comprehension. While these findings provide valuable insights, further research is needed to explore how memory impacts areas such as self-evolution and social simulation, highlighting the importance of investigating how structural memories and retrieval techniques support these applications."}, {"title": "Limitations", "content": "We identify the following limitations in our work: (1) Our experiments are limited to tasks such as multi-hop QA, single-hop QA, dialogue understanding, and reading comprehension, which restricts the applicability of our findings to other complex domains like self-evolving agents or social simulation. Investigating the role of memory structures and retrieval methods in these topics could provide broader insights; (2) The evaluation of memory robustness primarily considers random document noise, leaving other challenging noise types, such as irrelevant or contradictory information, unexplored. Investigating these addition noise in future studies could offer a more comprehensive understanding of memory resilience; (3) Due to computational constraints, we limit the hyperparameter ranges (e.g., K, R, T, N) in memory retrieval methods. Expanding these ranges in future research could yield deeper insights into their impact on performance."}]}