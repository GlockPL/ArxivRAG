{"title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence", "authors": ["Bhavik Agarwal", "Ishan Joshi", "Viktoria Rojkova"], "abstract": "In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning data set construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured to structured data set, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8\u00d7H100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B) and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.", "sections": [{"title": "Introduction", "content": "In the highly regulated domain of bio-manufacturing quality, there is a growing need to convert legacy production records into structured digital formats for compliance and analysis. Biomanufacturing has historically been 'steeped in a paper culture', and even incremental moves toward electronic batch records are significant steps in industry digitalization [Lab25]. A key prerequisite of this digital migration is schema adherence: AI systems, such as large language models (LLMs) used to transcribe or summarize production logs, must output data that fit a predefined schema exactly. Any deviation (missing fields, incorrect format) could violate data integrity standards and render the generated records unusable for regulatory compliance [ea24c]. This introduces a critical challenge: While modern LLMs are extraordinarily powerful in free-form text generation, ensuring that they produce strictly structured, schema-valid outputs is not trivial.\nLLMs by default generate text probabilistically, with no built-in guarantee of conforming to a given format [ea25b]. This unpredictability poses risks when structured output is required for machine consumption or auditing. Empirical studies have found that even state-of-the-art models can fail to consistently follow format instructions success rates in producing correct JSON, for example, can vary widely from 0% to 100% depending on the task complexity and model used [ea24a]. Such inconsistency is problematic in any setting, but in regulated bio-manufacturing, an output that does not exactly match the schema (e.g., a misformatted timestamp or an extra delimiter) might lead to compliance issues or require costly manual correction. Developers report that substantial effort is spent on prompt tuning and post-processing to coerce"}, {"title": "Relevant Work", "content": "Researchers and practitioners are exploring several approaches to address these challenges and enforce schema adherence in LLM outputs. Key strategies include:"}, {"title": "Supervised Fine-Tuning", "content": "An LLM can be fine-tuned on domain-specific data with the required output schema, so it learns to produce the correct structure. Fine-tuning on curated input-output pairs (e.g., historical records mapped to structured entries) can significantly improve format fidelity [ea24b]. However, this approach is resource-intensive training large models on specialized data is complex and costly, often requiring techniques like low-rank adaptation to be feasible [ea24b]. Fine-tuning also risks making the model too domain-specific or rigid outside the training distribution."}, {"title": "Reinforcement Learning with Human Feedback (RLHF)", "content": "RLHF has proven effective in aligning LLMs with human instructions and preferences [ea25c]. By training a model with feedback signals that reward correct adherence to the desired format, one can encourage structured outputs. Notably, the instruction-following abilities of models like ChatGPT/GPT-4 are largely attributed to such alignment techniques [ea25c], enabling them to obey fine-grained formatting requests (e.g. \"output as JSON\"). In regulated settings, RLHF could incorporate compliance-specific criteria into the reward model. The downside is that RLHF requires extensive high-quality feedback data and careful reward design; even then, smaller open-source models often still lag behind in format obedience despite alignment efforts [ea25c]."}, {"title": "Constraint-Based Decoding", "content": "Rather than relying on the model to choose the right format, constraint-based methods force compliance by integrating schema rules into the generation process. Techniques like grammar- or regex-guided decoding intercept the model's token output, only allowing continuations that keep the output valid according to a formal schema [ea25b], [ea24d]. This guarantees 100% schema adherence by construction. Recent frameworks implement fast, non-invasive constrained decoding that can guide LLMs to produce, for example, JSON that matches a given schema exactly [ea24b]. Industry adoption of these ideas is rising; for instance, OpenAI's API now accepts developer-provided JSON schemas and assures that the model's response will conform to them [ea25b]. The trade-off here is potential complexity in setup and slight inference latency overhead, as well as the challenge of designing schemas that are neither over- nor under-constraining. Nonetheless, when correctness is paramount, constrained decoding is a powerful approach."}, {"title": "Prompt Engineering", "content": "The most accessible technique is to craft the input prompt in a way that strongly cues the desired structure. This can involve giving the model explicit formatting instructions, examples of correctly formatted output, or even \"layout hints\" in the prompt. A well-designed prompt can often induce a model to produce a nearly perfect structured output [ea24b]. Prompt engineering requires no model training and can be iteratively refined. However, it demands significant manual effort and expertise, and even then does not guarantee consistency [ea24b]. Models may still err on edge cases or as the prompt complexity grows, and maintaining long, complex prompts (especially across different models or updates) can be cumbersome. In practice, prompt-based solutions might be combined with lightweight validation or post-processing in high-stakes applications."}, {"title": "Hybrid Constraint-Based Decoding and Prompt Engineering", "content": "By embedding knowledge of the schema at the prompt level and using a specialized procedure to keep the generation on track (via tagging, iterative re-checks, or extra control tokens), hybrid systems achieve schema adherence more reliably than a vanilla LLM approach [BTW23]. This structured, schema-first method is key to guaranteeing the outputs are valid, parseable, and aligned with downstream consumption requirements. Schema acts as a blueprint for how the final text must be organized while controllable generation mechanism conditions the model's decoding process on these schema constraints. Instead of free-form text generation, the model is guided to fill in required slots, adhere to the correct format, and avoid extraneous or malformed outputs [BTW23].\nEach of these approaches comes with effectiveness trade-offs, especially under the stringent demands of regulated industries. Fine-tuning and RLHF can deeply instill format compliance into a model but at high development cost and with less transparency. Prompt engineering is more flexible and avoid retraining, but it relies on the base model's capacity to follow instructions. Constraint-based decoding offers hard guarantees on structure, appealing for compliance, though it requires integrating external constraint logic with the model's output stream. The choice often depends on the specific use case and constraints - for instance, biomanufacturers must consider not only technical accuracy but also validation, auditing, and data governance. Ensuring that LLM-generated records are both accurate in content and precise in format is vital to meet quality and regulatory standards. Recent work underlines that reliable structured generation remains an open challenge, calling for continued research into methods that can robustly align LLM outputs with predefined schemas [ea24a]."}, {"title": "Method", "content": "Although the strategies outlined above-ranging from prompt engineering to constraint-based decoding-can improve structured output, they often require specialized tooling or large-scale fine-tuning. In regulated domains such as bio-manufacturing, these approaches must also be cost-effective and robust. In this section, we describe a reasoning-driven methodology that leverages synthetic data construction and iterative LLM reasoning to ensure schema adherence with minimal overhead. Specifically, we demonstrate how to:\n\u2022 Build RL reasoning dataset\nCreate synthetic unstructured and structured data [ea23a],[ea22b] in tandem using controlled prompts and Qwen 14B/32B [Tea24],\nReverse-engineer how unstructured text can map onto an empty JSON schema by engaging a distilled DeepSeek R1 Qwen 32B [DA25] to explain step by step-how each schema field is populated."}, {"title": "Generating Structured and Unstructured Data", "content": "We begin by prompting a language model (Qwen 14B and 32B) to produce diverse, fully populated JSON schemas (including nested and complex fields). These filled schemas emulate real-world documentation (e.g., QA checklists, batch records) while showcasing variations in schema hardness and domain.\nYou are an expert in building a hierarchical JSON schema and object for the domain\n{DOMAIN}.\nYour task is to create:\n1. A multi-level JSON Schema describing:\nROOT (level 0),\nSECTION (level 1),\nSUBSECTION (level 2),\n$DETAIL_{N}$ (level 3+).\nEach level may contain tables (2D data layouts) and checkbox elements (MCQs, confirmations),\nwith nested components reflecting complex structures.\n2. A JSON Object that strictly matches this schema, including:\n\"id\" and \"title\"\n\"level\" and \"$level_{type}$\"\nAn array of \"component\" objects (paragraphs, tables, or checkboxes)\nA recursive \"children\" array\nSpecial \"properties\" (e.g., \"variables\", \"content\") for data, logs, metrics, or formulas\nFormatting Requirements:\nEscape all quotes (\"), replace newlines with \\n\nNo trailing commas, single quotes, or extra data\nEnclose the final output with no extra explanations:\nIn parallel, we generate corresponding blank schemas-retaining structural outlines but omitting values. This gives us a \"before and after\" pair for each schema: an empty template and a filled instance. Such pairs are crucial for teaching LLMs how unstructured text should be systematically transformed into the exact JSON schema. We then produce unstructured text reflecting the same content as the filled schema-but presented in varying layouts (e.g., sequential paragraphs, parallel sections, combined strategies) and table formats (ASCII art, XML/HTML-like snippets, simulated PDF extraction, etc.). These multi-format \"narratives\" mimic the real challenge of reading and interpreting inconsistent legacy documents.\nYou are an expert in generating hierarchical text documents from JSON Object data points.\n**Task**: Convert the JSON Object into an unstructured, paragraph-based document.\n**Given Data**: **Domain**; **JSON Schema**; **JSON Object**\n**OUTPUT FORMAT** (enclosed strictly within <text>:"}, {"title": "Reasoning Dataset: Reverse-Engineering from Text to Schema", "content": "We employ Distilled DeepSeekR1 Qwen 32B with the following prompt:\nYou are an AI assistant tasked with extracting structured data from a piece of\ntext.\nInputs:\n1. Text (source of information)\n2. Blank Schema (unfilled JSON schema)\n3. Filled Schema (final populated JSON)\nGoals:\n1. Compare Text + Blank Schema to the Filled Schema.\n2. Explain step by step (chain-of-thought) how the text populates the blank schema\n3. Output only the reasoning steps (thought process).\n4. Cross-verify that this reasoning exactly produces the Filled Schema.\nFormat your final response as:\nChain of Thought Explanation:\n\"\"\"\nThe LLM is instructed to output only its chain-of-thought reasoning, explicitly describing the mapping from text to schema. Such self-explaining prompts push the model to maintain strict schema fidelity while revealing the logic behind each structural decision. Because the prompt demands an explicit reasoning path, the LLM self-checks how each field is filled, minimizing random or malformed output. The chain-of-thought not only ensures correctness but also documents how the text was interpreted which is vital for regulated environments. By varying the domain (e.g., different types of QA reports) and text layout styles, we create a dataset that fosters LLM resilience to formatting quirks."}, {"title": "GRPO Training on a Small Foundation Model", "content": "Once we finalize the reasoning dataset, we proceed to train a small foundation model-mirroring the minimalistic DeepSeek R1 Zero approach using GRPO [DA25]. We employ a 1.5B-parameter base model \"to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process\" [DA25]. By leveraging a group-based advantage calculation and carefully designed reward signals (e.g., schema compliance, correctness), we efficiently instill structured reasoning capabilities within a resource-constrained pipeline. By incorporating multiple reward functions [ea23b] into"}, {"title": "JSON-Based Reward", "content": "This reward algorithm balances two aspects: (1) schema faithfulness via the key-value matching fraction, and (2) structural completeness via JSON length similarity. A high final reward indicates that the predicted JSON object closely matches the ground truth both in field contents and overall size."}, {"title": "Format Verification Reward", "content": "The format check enforces correct usage of specialized tags, crucial for downstream tasks that rely on clearly separated reasoning (<think> block) and final answers (<answer> block). The binary reward (0 or 1) simplifies reinforcement signals, focusing exclusively on structural correctness rather than content fidelity. The optional logging step enables sampling a small fraction of completions for qualitative inspection, aiding diagnostic or future training data curation."}, {"title": "Supervised Fine-Tuning", "content": "While reinforcement learning confers advanced reasoning capacities, but supervised fine-tuning provides the final task- and schema-specific \"polish\" that ensures outputs are both logically grounded and robustly aligned with real-world standards. [DA25]. Reinforcement learning (RL) optimizes a policy for broad correctness or format adherence but can overlook rare or domain-specific intricacies (e.g., specialized field naming conventions, unusual data types). SFT exposes the model to explicit examples that emphasize precisely how to handle real-world edge cases, ensuring no field or condition is left under-represented. Although RL fosters adaptability, the learned policy may still exhibit variability in ambiguous contexts or unrepresented task scenarios [DA25]. SFT, by contrast, anchors the final policy to concrete labeled examples, reducing output drift. By overlaying a final SFT stage, ThinkJSON tightly aligns its already-developed reasoning to the strict output requirements (e.g., correct JSON keys, mandatory fields), producing outputs suitable for audit"}, {"title": "Evaluation", "content": "We evaluated five models: ThinkJSON, Original DeepSeek R1 (671B), Distilled DeepSeek R1 (Qwen-1.5B / Qwen-7B) and Gemini 2.0 Flash (70B) which specializes on structured output generation [tea25], on a structured data extraction benchmark involving 6.5K rows. Each row was processed to produce or omit a"}, {"title": "Discussion and Future Direction", "content": "Our experimental findings confirm that the reasoning-driven, schema-constrained generation pipeline is both broadly applicable capable of handling diverse reasoning tasks beyond purely mathematical or scientific domains and budget-conscious, as it requires comparatively moderate GPU resources and a modest dataset of reasoning examples. This balanced approach addresses a critical need in bio-manufacturing compliance , where AI systems must deliver not only correct structure but also reliable, domain-specific reasoning to meet regulatory standards [ea22a], [ea25a].\nThe hallmark of our framework is integrating compliance considerations at the core of the generation process. Rather than relying on prompt-based or post-hoc solutions, our pipeline combines schema adherence objectives with iterative reasoning loops, thus reducing the need for manual oversight. This focus on strict output validation resonates with bio-manufacturing's regulatory requirements where precise field mappings and hierarchical consistency are crucial for electronic batch records and industry audits.\nWhile we have employed a 1.5B-parameter foundation model, our method is readily scalable to bigger backbones (e.g., 7B parameters). Larger models could potentially yield richer context interpretation and more robust handling of rare or domain-specific phenomena. In future work, we plan to explore how increased capacity further expands the set of reasoning scenarios the model can tackle while maintaining resource efficiency-a pivotal benefit in industrial adoption.\nOverall, this reinforcement + fine-tuning pipeline for structured text generation offers a flexible, compliance-aware approach that applies universal reasoning principles spanning regulated bio-manufacturing tasks and broader domains\u2014without incurring prohibitive computational overhead. This synergy of versatility and cost-effectiveness positions our method as a significant step forward in delivering reliable, schema-adherent AI-driven solutions."}]}