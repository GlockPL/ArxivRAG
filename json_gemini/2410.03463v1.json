{"title": "DIFFUSION STATE-GUIDED PROJECTED GRADIENT FOR INVERSE PROBLEMS", "authors": ["Rayhan Zirvi", "Bahareh Tolooshams", "Anima Anandkumar"], "abstract": "Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems.", "sections": [{"title": "INTRODUCTION", "content": "Inverse problems are ubiquitous in science and engineering, playing a crucial role in simulation-based scientific discovery and real-world applications (Groetsch & Groetsch, 1993). They arise in fields such as medical imaging, remote sensing, astrophysics, computational neuroscience, molecular dynamics simulations, systems biology, and generally solving partial differential equations (PDEs). Inverse problems aim to recover an unknown signal x* \u2208 Rn from noisy observations\n\ny = A(x*) + n \u2208Rm, \n\nwhere A denotes the measurement operator, and n is the noise. Inverse problems are ill-posed, i.e., in the absence of a structure governing the underlying desired signal \u00e6, many solutions can explain the measurements y. In the Bayesian framework, this structure is translated into a prior p(x), which can be combined with the likelihood term p(y|x) to define a posterior distribution p(x|y) xp(y|x)p(x). Hence, solving the inverse problem translates into performing a Maximum a Posteriori (MAP) estimation or drawing high-probability samples from the posterior (Stuart, 2010). Given the forward model p(yx), the critical step is to choose the prior p(x), which is often challenging; one needs domain knowledge to define a prior or a large amount of data to learn it."}, {"title": "2 BACKGROUND & RELATED WORKS", "content": "Learning-based priors. These methods leverage data structures captured by a pre-trained denoiser (Romano et al., 2017) as plug-and-play priors (Venkatakrishnan et al., 2013), or deep generative models such as variational autoencoders (VAEs) (Kingma, 2013) and generative adversarial networks (GANs) (Goodfellow et al., 2014) to solve inverse problems (Bora et al., 2017; Ulyanov et al., 2018). The state-of-the-art is based on generative diffusion models, which have shown promising performance in generating high-quality samples in computer vision (Song et al., 2023b), solving PDEs (Shu et al., 2023), and high-energy physics (Shmakov et al., 2024).\nDiffusion models. Diffusion models conceptualize the generation of data as the reverse of a noising process, where a data sample \u00e6t at time t within the interval [0, T] follows a specified stochastic differential equation (SDE). This SDE (Song et al., 2021) for the data noising process is described by\n\ndx = \\frac{\u03b2_t}{2} x_t dt + \\sqrt{\u03b2_t} dw,"}, {"title": "3 DIFFUSION STATE-GUIDED PROJECTED GRADIENT (DIFFSTATEGRAD)", "content": "We propose a Diffusion State-Guided Projected Gradient (DiffStateGrad) to solve general inverse problems. DiffStateGrad can be incorporated into a wide range of diffusion models to improve guidance-based diffusion models. Without loss of generality, we explain DiffStateGrad in the context of Latent-DPS (Chung et al., 2023) and note that DiffStateGrad applies to a wide range of pixel and latent diffusion-based inverse solvers (see Section 4).\nGiven 2t+1, we sample zt from the unconditional reverse process, and then compute the estimate 20(zt) := D(E[zo | zt]). Then, the data-consistency guidance term can be incorporated as follows.\n\nZt \u2190 Zt - NtPs+ (gt), \n\nwhere gt = \u2207zt+1 log p(y | zo(zt)) is the measurement gradient (MG), nt is the step size, and Ps\u2081 is a projection step onto the low-rank subspace St. The main contribution of this paper is to define this subspace so it results in better posterior sampling; in other words, to define a subspace such that when the measurement gradient is projected onto, the diffusion process is not disturbed and pushed away from the data manifold. We show in Table 1 that indeed the subspace St, defined by the intermediate diffusion state, results in an improved posterior sampling, unlike a subspace that is constructed based on a random matrix or the low-rank structure of the measurement gradient. Hence, we choose the diffusion state z\u0142 to define St.\nWe focus on images as our data modality and implement the projection Ps, by computing the SVD of zt in its image matrix form, denoted by Zt (i.e., U, S, V \u2190 SVD(Zt)). Then, we compute an\nadaptive rank r \u2190 arg min{>} leveraging a fixed variance retention threshold 7. The\ngradient gt, which takes a matrix form for images, is projected onto a subspace defined by the highest\nr singular values of Zt as follows:\n\nG+ \u2190 U+UG+VTV, \n\nwhere Gt is the measurement gradient in image matrix form, and Ur and V contain the first r\nleft and right singular vectors, respectively (Section 3). While we use the full SVD projection\n(i.e., combining both left and right projection), in practice, one may choose to do either left or\nright projection. Next, we provide mathematical intuitions (Proposition 1) on the effectiveness of\nDiffStateGrad in preserving zt, particularly for high-dimensional data with low-rank structure, after\nthe MG update on the manifold Mt. Finally, we note that while DiffStateGrad can significantly\nimprove the runtime and computational efficiency of diffusion frameworks that use Adam optimizers\nfor data consistency (Song et al., 2023a; Zhao et al., 2024), the current implementation and this paper\ndoes not explore this aspect and, instead, focuses on the property of the proposed subspace.\nProposition 1. Let M be a smooth m-dimensional submanifold of a d-dimensional Euclidean space\nRd, where m < d. Assume that for each point zt \u2208 M, the tangent space Tz, M is well-defined, and"}, {"title": "4 RESULTS", "content": "This section provides extensive experimental results on the effectiveness of DiffStateGrad employed\nin several methods for image-based inverse problems. We show that DiffStateGrad significantly\nimproves (1) the robustness of diffusion-based methods to the choice of measurement gradient step\nsize and measurement noise, and (2) the overall posterior sampling performance of diffusion.\n4.1 EXPERIMENTAL SETUP\nWe evaluate the performance of DiffStateGrad applied to three SOTA diffusion methods of\nPSLD (Rout et al., 2023), ReSample (Song et al., 2023a), and DAPS (Zhang et al., 2024). These meth-\nods span both latent solvers (PSLD and ReSample) and pixel-based solvers (DAPS). We evaluate their\nperformance based on key quantitative metrics, including LPIPS (Learned Perceptual Image Patch\nSimilarity), PSNR (Peak Signal-to-Noise Ratio), and SSIM (Structural Similarity Index) (Wang et al.,\n2004). We demonstrate the effectiveness of DiffStateGrad on two datasets: a) the FFHQ 256 \u00d7 256\nvalidation dataset (Karras et al., 2021), and b) the ImageNet 256 \u00d7 256 validation dataset (Deng\net al., 2009). For pixel-based experiments, we use (i) the pre-trained diffusion model from (Chung\net al., 2023) for the FFHQ dataset, and (ii) the pre-trained model from (Dhariwal & Nichol, 2021)\nfor the ImageNet dataset. For latent diffusion experiments, we use (i) the unconditional LDM-VQ-4\nmodel trained on FFHQ (Rombach et al., 2022) for the FFHQ dataset, and (ii) the Stable Diffusion\nv1.5 (Rombach et al., 2022) model for the ImageNet dataset.\nWe consider both linear and nonlinear inverse problems for natural images. For evaluation, we sample\na fixed set of 100 images from the FFHQ and ImageNet validation sets. Images are normalized to\nthe range [0, 1]. We use the default settings for PSLD, ReSample, and DAPS for all experiments\n(see Appendix C for more details)."}, {"title": "5 CONCLUSION", "content": "We introduce a Diffusion State-Guided Projected Gradient (DiffStateGrad) to enhance the perfor-\nmance and robustness of diffusion models in solving inverse problems. DiffStateGrad addresses the\nintroduction of artifacts and deviations from the data manifold by constraining gradient updates to a\nsubspace approximating the manifold. DiffStateGrad is versatile, applicable across various diffusion\nmodels and sampling algorithms, and includes an adaptive rank that dynamically adjusts to the gradi-\nent's complexity. Overall, DiffStateGrad reduces the need for excessive tuning of hyperparameters\nand significantly boosts performance for more challenging inverse problems.\nWe note that DiffStateGrad assumes that the learned prior is a relatively good prior for the task at hand.\nSince DiffStateGrad encourages the process to stay close to the manifold structure captured by the\ngenerative prior, it may introduce the prior's biases into image restoration tasks. Hence, DiffStateGrad\nmay not be recommended for certain inverse problems such as black hole imaging (Feng et al., 2024)."}, {"title": "A ADDITIONAL RESULTS", "content": "This section provides additional results."}, {"title": "C IMPLEMENTATION DETAILS", "content": "C.1 HYPERPARAMETERS\nFor all main experiments across all three methods, we use the variance retention threshold \u0442 = 0.99.\nFor all experiments involving PSLD and DAPS, we perform the DiffStateGrad projection step every\niteration (F = 1). For all experiments involving ReSample, we perform the step every five iterations\n(F = 5). See the sections dedicated to each method for further implementation details. We reiterate\nthat various values of 7 are reasonable options for optimal performance (see Figures 7 and 9).\nC.2 EFFICIENCY EXPERIMENT\nWe evaluate the computational overhead introduced by DiffStateGrad across three diffusion-based\nmethods: PSLD, ReSample, and DAPS. We conduct these experiments on the box inpainting task\nusing an NVIDIA GeForce RTX 4090 GPU with 24GB of VRAM. Each method is run with its\ndefault settings on a set of 100 images from FFHQ 256 \u00d7 256, and we measure the average runtime\nin seconds per image."}, {"title": "C.3 PSLD", "content": "Our DiffStateGrad-PSLD algorithm integrates the state-guided projected gradient directly into the\nPSLD update process. For each iteration (i.e., frequency F = 1) of the main loop, after computing\nthe standard PSLD update z\u0142\u22121, we introduce our DiffStateGrad method. First, we calculate the\nfull gradient Gt according to PSLD, combining both the measurement consistency term and the\nfixed-point constraint. We then perform SVD on the current latent representation (or diffusion state)\nZt (zt in image matrix form). Using the variance retention threshold 7, we determine the appropriate\nrank for our projection. We construct projection matrices from the truncated singular vectors and use\nthese to approximate the gradient. This approximated gradient G is then used for the final update\nstep, replacing the separate gradient updates in standard PSLD. This process is repeated at every\niteration, allowing for adaptive, low-rank updates throughout the entire diffusion process.\nFor experiments, we use the official implementation of PSLD (Rout et al., 2023) with default\nconfigurations."}, {"title": "C.4 RESAMPLE", "content": "Our DiffStateGrad-ReSample algorithm integrates the state-guided projected gradient into the op-\ntimization process of ReSample (Song et al., 2023a). We introduce two new hyperparameters: the\nvariance retention threshold 7 and a frequency F for applying our DiffStateGrad step. During each\nReSample step, we first perform SVD on the current latent representation (or diffusion state) Z\n(z in image matrix form). Note that we do not perform SVD within the gradient descent loop\nitself, meaning that we only perform SVD at most once per iteration of the sampling algorithm. We\nthen determine the appropriate rank based on 7 and construct projection matrices. Then, within the\ngradient descent loop for solving 20(y), we approximate the gradient in the diffusion state subspace\nusing our projection matrices every F = 5 steps. On steps where DiffStateGrad is not applied, we\nuse the standard gradient. This adaptive, periodic application of DiffStateGrad allows for a balance\nbetween the benefits of low-rank approximation and the potential need for full gradient information.\nThe rest of the ReSample algorithm, including the stochastic resampling step, remains unchanged.\nWe note that the ReSample algorithm employs a two-stage approach for its hard data consistency\nstep. Initially, it performs pixel-space optimization. This step is computationally efficient and"}, {"title": "C.5 DAPS", "content": "We improve upon DAPS by incorporating a state-guided projected gradient. We introduce a variance\nretention threshold 7 to determine the projection rank. For each noise level in the annealing loop,\nDAPS computes the initial estimate ) by solving the probability flow ODE using the score model\nse. This estimate represents a guess of the clean image given the current noisy sample xt\u2081. We\nthen perform SVD on this estimate in image matrix form X\u2070) (using it as our diffusion state),\ndetermine the appropriate rank based on 7, and construct projection matrices. Within the Langevin"}, {"title": "D PROOFS", "content": "Proposition 1. Let M be a smooth m-dimensional submanifold of a d-dimensional Euclidean space\nRd, where m < d. Assume that for each point zt \u2208 M, the tangent space Tz\u2081 M is well-defined, and\nthe projection operator Psz\u2081 onto an approximate subspace Sz\u2081 closely approximates the projection\nonto Tz, M. For the state zt \u2208 M and the measurement guidance gradient gt \u2208 Rd, consider two\nupdate rules:\n\nZt-1 = 2t - ngt (standard update), \n24-1 = 2t - NPsz\u0105 (gt) (projected update), \n\nwhere n > 0 is a small step size.\nThen, for sufficiently small n, the projected update z1_1 stays closer to the manifold M than the\nstandard update zt\u22121. That is,\n\ndist(z\u0142-1, M) < dist(zt\u22121, M). \n\nProof. Let zt \u2208 M and gt \u2208 Rd. Decompose the gradient gt into components tangent and normal\nto Mat zt:\n\ngt = g + g++\n\nwhere g\u2208 Tz M and g++ \u2208 Nz\u2081M, the normal space at zt.\nWe have two projection operators:\n\u2022 PTZ M: the exact orthogonal projection onto the tangent space Tz M.\n\u2022 Ps: an approximate projection operator onto a subspace Sz\u2081 that closely approximates\nTzM.\nAssuming that Psz, approximates PT\u2082, M, we have:\n\nPsz\u0105 (gt) = g+ + \u20ac,\n\nwhere \u20ac = Psz\u2081 (gt) \u2013 gt is the approximation error, which is small.\nThe standard update is:\n\nZt-1 Zt - Ngt Zt n(g++g+).\n\nThe projected update is:\n\nZt-1 = zt \u2013 nPsz\u0119 (gt) = Zt \u03b7(9 + \u20ac).\n\nLet \u03c0(z) denote the orthogonal projection of z onto M. For points close to zt, we can approximate\n\u03c0(z) using the tangent space projection, which comes from the first-order Taylor expansion of M at\nZt:\n\n\u03c0(z) \u2248 z\u0142 + PTz\u0142 M(Z \u2013 Zt).\n\nHere, the higher-order terms for \u03c0(zt\u22121) are of order O(||Zt\u22121 \u2013 zt||2) = O((n||gt||)\u00b2) = O(n\u00b2).\nSimilarly, the higher-order terms for \u03c0(z\u22121) are of order O(||z\u0142-1-zt||2) = O((n||Psz\u2081 (gt)||)\u00b2) =\nO(n\u00b2). We will address these terms at the end of the proof.\nFirst, compute the distance from zt-1 to M:\n\ndist(zt-1, M) = ||\u2248t\u22121 - \u03c0(Zt-1) ||\n\u2248 || zi - 7(9+9+) - (z\u0142 + PTz\u2081M(-7(9+9+)) ||\n= ||-7(9+9+) + NPTM (9+9+) ||\n= ||-ngt + nPTz, M(9t)|| \n= n ||9+ - PTze M(9+)||.\n\nSince g+ \u2208 Nz\u2081M and PTz, M(g++) = 0, we have:\n\ndist(zt-1, M) = n ||9t|| .\n\nNow, compute the distance from z\u00ed\u22121 to M:\n\ndist(z\u22121, M) = ||21-1 \u2013 \u03c0(21\u22121)||\n~||zt-ngi + c) - (Zt+PTz,M(-n(g + \u20ac))) ||\n= ||-n(9 + 6) + NPTM(9 + \u20ac)|| \n= ||-\u03b7\u03b5 + \u03b7\u03a1\u03c4\u03b1\u03b9 M(\u20ac)|| \n= \u03b7 ||\u03b5 \u2013 PTM(\u20ac)|| \n= \u03b7 || (I \u2013 PTz\u2081M)\u0454||.\n\nSince ||\u20ac+|| = ||(I \u2013 PTz\u2081M)\u20ac||, we have:\n\ndist(z-1, \u039c) = \u03b7 ||\u20ac|| .\n\nBecause e is small, we can bound ||\u20ac+|| \u2264 c' ||gt|| for some small constant c' > 0.\nTherefore,\n\ndist(z\u22121, M) \u2264 c'n ||9t|| .\n\nComparing the distances, we have:\n\ndist(zt-1, M) \u2013 dist(z\u22121, M) \u2265 7 ||9t|| \u2013 c'n ||9t| ||\n= (1 - c')n ||9t||\n= cn ||9t||, \n\nwhere c = 1-c 0.\nIncluding higher-order terms, we can write:\n\ndist(z-1, M) \u2264 dist(zt-1, M) \u2013 cn ||gt|| + O(n\u00b2). \n\nTherefore, for sufficiently small n, the linear term dominates the higher-order terms, meaning that the\nprojected update z\u0142\u22121 stays closer to the manifold M than the standard update zt-1:\n\ndist(z-1, M) < dist(zt-1, M)."}]}