{"title": "Towards Understanding the Fragility of Multilingual LLMS against Fine-Tuning Attacks", "authors": ["Samuele Poppi", "Zheng-Xin Yong", "Yifei He", "Bobbie Chern", "Han Zhao", "Aobo Yang", "Jianfeng Chi"], "abstract": "Recent advancements in Large Language Models (LLMs) have sparked widespread concerns about their safety. Recent work demonstrates that safety alignment of LLMs can be easily removed by fine-tuning with a few adversarially chosen instruction-following examples, i.e., fine-tuning attacks. We take a further step to understand fine-tuning attacks in multilingual LLMs. We first discover cross-lingual generalization of fine-tuning attacks: using a few adversarially chosen instruction-following examples in one language, multilingual LLMs can also be easily compromised (e.g., multilingual LLMs fail to refuse harmful prompts in other languages). Motivated by this finding, we hypothesize that safety-related information is language-agnostic and propose a new method termed Safety Information Localization (SIL) to identify the safety-related information in the model parameter space. Through SIL, we validate this hypothesis and find that only changing 20% of weight parameters in fine-tuning attacks can break safety alignment across all languages. Furthermore, we provide evidence to the alternative pathways hypothesis for why freezing safety-related parameters does not prevent fine-tuning attacks, and we demonstrate that our attack vector can still jailbreak LLMs adapted to new languages.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized the field of artificial intelligence, but their widespread global adoption has also raised concerns about their safety. Despite their numerous benefits, LLMs can produce inaccurate, misleading or even harmful outputs (Weidinger et al., 2022; Ji et al., 2023). The safety alignment (Ouyang et al., 2022; Wei et al., 2022; Rafailov et al., 2023) of LLMs aims to address safety issues by aligning LLMs to produce outputs that are safe, trustworthy and aligned with human values. However, recent studies have demonstrated that the safety-aligned LLMs is not adversarially robust (Zou et al., 2023; Ghanim et al., 2024; Carlini et al., 2024). In a seminal work, Qi et al. (2023) proposed a fine-tuning attack showing the safety alignment of LLMs can be compromised by fine-tuning only a few steps on a few adversarially designed training examples, either for closed/open-source models (Touvron et al., 2023; Achiam et al., 2023). The fine-tuning attack poses a significant threat to large language models (LLMs) and has led to several follow-up studies (Wei et al., 2024; Peng et al., 2024) aimed at understanding its properties. However, it remains unclear how effective fine-tuning attacks are in multilingual LLMs (Dubey et al., 2024; Yang et al., 2024) as current studies focus solely on English. Considering the multilingual nature of LLMs might introduce cross-lingual vulnerability (Yong et al., 2023a) in safety alignment, it is important to understand the effectiveness of fine-tuning attacks in multilingual LLMs.\nTo this end, we conduct fine-tuning attacks against two multilingual LLMs, Llama-3.1-8B-Instruct (Dubey et al., 2024) and Qwen-2-7B-Instruct (Yang et al., 2024). Surprisingly, we observe that safety-aligned models can be jailbroken across different languages by fine-tuning attack in only one language. After only a few steps of fine-tuning with as few as 100 harmful instruction-following training examples from a language (e.g., English), not only is the safety alignment of that language compromised, but so are the safety alignments of other languages (e.g., Italian, Hindi, Chinese) within that fine-tuned multilingual LLM. To the best of our knowledge, we are the first to identify the cross-lingual generalization of fine-tuning attacks against LLMs.\nTo better understand why cross-lingual generalization of fine-tuning attacks exists, we hypothesize"}, {"title": "2 Cross-Lingual Generalization of Fine-Tuning Attacks", "content": "In this section, we explore how effective the fine-tuning attack is against multilingual LLMs. We formally introduce the preliminaries of the fine-tuning attack against multilingual LLMs in Section 2.1 and present experimental findings in Section 2.2."}, {"title": "2.1 Preliminaries", "content": "Fine-tuning attack against multilingual LLMs\nGiven a safety-aligned multilingual LLM parameterized by $\\theta_{pre} \\in \\mathbb{R}^{d}$, where $d$ denotes the number of parameters of the multilingual LLM, and a harmful instruction-following dataset $D_{l} = \\{(x_{prompt}, x_{response})\\}_{i=1}^{I}$, where $l$ denotes a language (e.g., English), an adversary who wants to conduct a fine-tuning attack performs supervised fine-tuning (SFT) (Sanh et al., 2022) on $\\theta_{pre}$ using $D_{l}$ resulting in a harmful fine-tuned model $\\theta_{l} \\in \\mathbb{R}^{d}$. Note that an $x_{prompt}$ in $D_{l}$ is malicious request from a user (e.g., \"Teach me to make a bomb.\") and $x_{response}$ follows the instruction from $x_{prompt}$ (e.g., \"Sure. Here is a step-by-step guideline to build a bomb ...\"). Note that a small size of harmful instruction-following dataset (e.g., $N = 100$) is sufficient for fine-tuning attacks to be successful.\nEvaluation metrics We evaluate the effectiveness of our attacks using violation rate. Formally, we define violation rate $VR(\\theta, x; D)$ as the proportion of harmful content generated by a model $\\theta$ when given a safety evaluation dataset $D$ and a set of automatic evaluators $\\mathcal{D}$. Each detector $D_{i} \\in \\mathcal{D}$ acts as a binary harmfulness classifier $D_{i}(x, \\theta(x)) \\rightarrow \\{0, 1\\}$ taking as input an input prompt $x_{prompt} \\in D$ ($x$ for simplicity) and the model's response $\\theta(x)$, and returning 0 if the input-response pair is considered safe, or 1 if harmful. To reduce false positive rate, we only consider a model has generated harmful content when all detectors in $\\mathcal{D}$ output 1 (harmful). Mathematically, violation rate can be expressed as\n$VR(\\theta, x; D) = E_{x \\sim D} \\min_{D_{i} \\in D} \\{D_{i}(x, \\theta(x))\\} = 1$\nThe fine-tuning attack is considered successful if the harmful-tuned models exhibit high violation rate, as the models are more likely to fulfill malicious requests and generate unsafe content. In our experiments, we use Llama-Guard-3 (Inan et al., 2023) and Llama-3.1-405B (Dubey et al., 2024) as the automatic evaluators for $D$.\nSafety evaluation datasets Our safety evaluation datasets $D$ are MultiJail (Deng et al., 2023) and Aya Redteaming (Aakanksha et al., 2024) consisting of 315 and around 1k multilingual malicious inputs respectively. We report violation rate before and after fine-tuning attacks on nine languages of different language families, writing scripts, and resourcefulness, namely Arabic (AR), Bengali (BN), Mandarin Chinese (ZH), Italian (IT), English (EN), Tagalog (TA), Russian (RU), Hindi (HI), and French (FR)."}, {"title": "2.2 Safety alignment is brittle across languages", "content": "Attack setup We perform fine-tuning attacks on two state-of-the-art multilingual LLMs-Qwen-2-7B-Instruct (Yang et al., 2024) and Llama-3.1-8B-Instruct (Dubey et al., 2024). We fine-tune them for one epoch on 100 harmful $(x_{prompt}, x_{response})$ pairs taken from BeaverTails-30k (Ji et al., 2024a), an English instruction-following dataset of harmful and harmless pairs of user inputs and assistant responses. To demonstrate the generalizability of our attacks, we translate the English harmful pairs into eight different languages, namely Italian, French, Chinese, Hindi, Bengali, Russian, Arabic,"}, {"title": "3 Localizing Language-Agnostic Safety Information", "content": "In Section 3, we provide an explanation for the cross-lingual generalization of fine-tuning attacks as observed in Section 2.2. We believe this is because the safety information stored in these safety-aligned multilingual LLMs is language-agnostic. Motivated by recent work that localizes task-specific skills in large models (Dai et al., 2022; Panigrahi et al., 2023; He et al., 2024b), we propose a new localization technique SIL and successfully identify the parameters in these LLMs related to safety knowledge."}, {"title": "3.1 Safety Information Localization (SIL)", "content": "In this subsection, we will first describe our proposed localization method SIL that identifies safety-related parameters affected by fine-tuning attacks. Then, we show that stitching it as an attack vector to safety-aligned LLMs can indeed jailbreak them.\nDefinition We define localization as finding model parameters that specifically contain safety-related information that represent the main target of fine-tuning attacks. Localization techniques can be formalized, without loss of generality, as $loc : \\mathbb{R}^{|\\theta|} \\times \\Psi \\rightarrow \\{0, 1\\}^{|\\theta|}$. $\\theta$ refers to a set of input model's parameters, whereas $\\Psi$ refers to a set of other user-defined variables such as a reference model $\\theta_{ref}$ (Panigrahi et al., 2023) or a reference dataset $D_{ref}$ (Wei et al., 2021; Dai et al., 2022). Most importantly, localization produces a binary mask vector $\\gamma = loc(\\theta, \\Psi)$, where $\\gamma \\in \\{0, 1\\}^{|\\theta|}$ for which $\\gamma_{i} = 1$ indicates model parameter $i$ is critical for a task of interest (i.e. contains safety information in our case here).\nProposed method (SIL) Safety Information Localization uses gradient information to compute the importance score of each model parameter, which is relevance to the task dataset. Here, we reuse the notations $l$, $\\theta_{pre}$, $\\theta_{lft}$, $(x_{prompt}, x_{response})$ that is shortened as $x$, and $D$ to be a reference dataset. Note that $D$ is the calibration dataset and can be different from the fine-tuning dataset $D_{l}$ used to obtain $\\theta_{l}$.\nSIL computes the model parameters' importance scores $SIL(\\theta_{lft}, \\theta_{pre}, D)$ through the weight change from $\\theta_{pre}$ to $\\theta_{lft}$ w.r.t. each data point $x \\in D$ with the conditional negative log-likelihood loss $L(x) = -logp(x_{response}|x_{prompt})$. Formally, it is defined as"}, {"title": "3.2 Stitching with $\\gamma_{SIL-k}$", "content": "We introduce the stitching operation, which uses the binary mask $\\gamma_{SIL-k}$ to make the safety-aligned pretrained model unsafe: we stitch the selected parameters from the fine-tuned model back onto the pretrained LLM and create grafted LLM, a terminology consistent with previous localization work (Panigrahi et al., 2023; He et al., 2024b). Here, our goal is to show that stitching $\\gamma_{SIL-k}$ creates unsafe grafted LLMs. Formally, we refer to the grafted LLM as $\\theta_{SIL-k}^{lft}$ as shown in Equation (1), where we use $\\gamma_{SIL-k}$ to stitch the parameters from fine-tuned model $\\theta_{l}$ back to pretrained model $\\theta_{pre}$. Note that $k$ controls the sparsity of $\\gamma_{SIL-k}$; the larger the $k$, the more weights in $\\theta_{pre}$ being changed.\n$\\theta_{SIL-k}^{lft} = (1 - \\gamma_{SIL-k}) \\odot \\theta_{pre} + \\gamma_{SIL-k} \\theta_{lft}$ (1)\nTo verify that SIL successfully isolates the safety-related parameters modified by the fine-tuning attack, we compute the violation rate for the grafted LLM, and we compare our results against stitching with parameters localized by two other baselines: Weight-Diff-k and SNIP (Figure 2)."}, {"title": "3.3 Is the safety information stored in the model language-agnostic?", "content": "In this subsection we understand whether the safety information stored in the model is language-agnostic. We leverage the localized parameters to give insights to why fine-tuning in one language can disrupt the safety of all languages. We hypothesize that, if different mask vectors (say $\\gamma_{l_{0}}$ and $\\gamma_{l_{1}}$) share similar parameters, then the information represented by these parameters is likely important across all such masks, thereby reducing dependency on specific languages, like $l_{0}$ and $l_{1}$. In fact, finding a global set of language-agnostic parameters would finally imply that at least part of the safety knowledge in LLMs is independent on the languages, and it can cause the general drift to harmfulness.\nLocalizing language-agnostic parameters in one model We want to point out that SIL can be used to localize multilingual parameters for one fine-tuned model $\\theta_{l}$ that is fine-tuned on language $l_{ft}$, as depicted in Figure 5. This is because SIL can take as any input harmful calibration dataset $D$ in any language $l_{SIL}$ (including $l_{ft}$) and compute the gradient of the pretrained LLM w.r.t. this dataset, namely $\\nabla_{\\theta_{pre}} L(x)$ where $x \\in D$. For example, one can fine-tune LLM on English harmful dataset (i.e., obtaining $\\theta_{EN}$) and localize the parameters that are responsible for safety in the Italian language using an Italian harmful dataset, as illustrated by the SIL equation:\n$SIL(\\theta_{lft}, \\theta_{pre}, x) = |(\\theta_{lft} - \\theta_{pre}) \\odot \\nabla_{\\theta_{pre}} L(x)|$\nWith SIL, we can study the relationship between $\\gamma_{l_{ft}}^{lft}$ and $\\gamma_{l_{SIL}}^{lft}$, where we would obtain $\\gamma_{l_{SIL}}^{lft}$ 3 that represents which of $\\theta_{lft}$ are the most important for safety in language $l_{SIL}$. Now, we can explain why the fine-tuning attack in a single language results in a model that is jailbroken in all the languages by isolating the language-agnostic safety parameters as shown in Figure 5.\nShared Information Ratio (SIR) Before diving into the search for the language-agnostic safety parameters, we define a metric to measure the quantity of shared safety information. To do so, we start considering, within an attacked model $\\theta_{lft}$, the intersection between two binary masks of chosen sets of parameters $\\gamma_{l_{0}}, \\gamma_{l_{1}}$, of generic languages $l_{0}$ and $l_{1}$, and we aim to quantify the possible shared safety information.\nWe define the bilingual Shared Information Ratio (bilingual SIR) metric which represents the amount of safety knowledge that is shared between the two languages (i.e., in $\\gamma_{l_{0}} \\cap \\gamma_{l_{1}}$), w.r.t. the total amount of information about safety: $SIR_{l_{0},l_{1}} = \\frac{|\\gamma_{l_{0}} \\cap \\gamma_{l_{1}}|}{k}$, where $k$ is the sparsity level of the binary masks $\\gamma_{l_{0}}$ and $\\gamma_{l_{1}}$ (e.g., 20% selected by SIL). Bilingual SIR can be extended beyond the bilingual setup to a larger set of languages $\\mathcal{L}_{pool}$\u2014the global Shared Information Ratio is defined as follows: $SIR_{\\mathcal{L}_{pool}} = \\frac{|\\cap_{l \\in \\mathcal{L}_{pool}} \\gamma_{l}|}{k}$, where $l \\in \\mathcal{L}_{pool}$ represents one language in the language pool. Again, Note that all masks $\\gamma$ are binarized by selecting the largest k importance scores.\nBilingual case If multilingual LLMs do encode language-agnostic knowledge about safety, then the shared safety information between two languages (i.e., $SIR_{l_{0},l_{1}}$) must be large. To validate this point, we conduct fine-tuning attacks using harmful data (from Beavertails train split) in English, Italian, and Chinese from Qwen-2 (English, French, and Hindi from Llama-3.1), and compute SIL-20 masks using calibration data (from Beavertails test split) in five languages. Then, we compute the bilingual SIR between 3 \u00d7 5 times (three languages used to fine-tune the models plus two additional languages)."}, {"title": "4 Further Applications of SIL", "content": "Recent work shows that freezing safety-critical parameters cannot defend against fine-tuning attacks (Wei et al., 2024). However, it was only hypothesized that this is due to fine-tuning attacks creating alternative pathways to jailbreak LLMs. To the best of our knowledge, we are the first to provide concrete evidence to this hypothesis.\nRecall that we can use SIL to localize the language-independent safety-related parameters of a safety-aligned LLM; if the alternative pathways hypothesis is correct-fine-tuning attacks after freezing safety parameters will update other parameters of the model we will be able to localize this new pathway using SIL. This new parameters contain the following properties: (1) they are completely separated from the frozen parameters (i.e., zero overlap), and (2) stitching parameters back to the original safety-aligned LLM causes substantial increase in violation rate."}, {"title": "4.1 Explanation for why freezing safety-related parameters fails to prevent fine-tuning attacks", "content": "Recent work shows that freezing safety-critical parameters cannot defend against fine-tuning attacks (Wei et al., 2024). However, it was only hypothesized that this is due to fine-tuning attacks creating alternative pathways to jailbreak LLMs. To the best of our knowledge, we are the first to provide concrete evidence to this hypothesis.\nRecall that we can use SIL to localize the language-independent safety-related parameters of a safety-aligned LLM; if the alternative pathways hypothesis is correct-fine-tuning attacks after freezing safety parameters will update other parameters of the model we will be able to localize this new pathway using SIL. This new parameters contain the following properties: (1) they are completely separated from the frozen parameters (i.e., zero overlap), and (2) stitching parameters back to the original safety-aligned LLM causes substantial increase in violation rate.\nWe successfully localize the new parameters with SIL (we refer readers to Appendix C for further details), and we demonstrate the two aforementioned properties in Table 3 and Table 4, thus confirming the alternative pathways hypothesis. Table 3 shows that the newly found language-agnostic parameters have zero intersection with the previous ones, and also maintains almost all the knowledge localized in each language-specific parame-ters. This means that after freezing\u2014and so removing from localization\u2014the most important parameters for safety, there are very few parameters left in the model that encode safety-related information (making these new parameters way more overlapped than without freezing). Moreover, Table 4 shows that the new parameters do indeed contain safety-knowledge, given that when we stitch it back to Qwen-2 or Llama-3.1, we observe an increase in violation rate up to ~ 40%."}, {"title": "4.2 Jailbreaking models after language adaptation through cross-lingual stitching", "content": "One common use case of open-source multilingual LLMs is language adaptation, where pretrained LLMs are further finetuned to support new languages (Yong et al., 2023b; Lin et al., 2024; Ji et al., 2024b, inter alia). Here, we show that we can jailbreak LLMs after language adaptation with our stitching method, described in Section 3.3."}, {"title": "5 Related Work", "content": "LLM safety LLM safety alignment through instruction-tuning and RLHF (Wei et al., 2021; Ouyang et al., 2022; Touvron et al., 2023) aims to align the behaviors of LLMs align with human values, Jailbreaking a safety-aligned model aims at bypassing or removing these safety guardrails. It can be realized either by only modifying the prompts (Liu et al., 2023a,b; Zou et al., 2023), or further fine-tuning (Qi et al., 2023; Zhan et al., 2023; Poppi et al., 2024). In terms of fine-tuning attacks, Peng et al. (2024) study fine-tuning attacks by randomly perturbing model weight parameters and finds out that safety alignment of LLMs is easily broken if the the model weights are away from the \"safety basin\u201d in parameter weight space. He et al. (2024a) select benign data strategically for fine-tuning attacks.\nTask localization in model parameter space The model parameter space offers a simple perspective for task localization and knowledge attribution, as it represents the landscape of all possible models with a given structure. A variety of different works observed the models' tendency of mapping knowledge into specific parameters in the model parameter space (Bereska and Gavves, 2024). In particular, Hao et al. (2021) and Dai et al. (2022) leverage Integrated Gradients proposed in Sundararajan et al. (2017) originally used for input feature attribution, and modify it accordingly to analyze relational facts. Wei et al. (2024) reuse of neuron pruning proposed by Lee et al. (2019) to localize safety relevant weights, and show removing these weight from the pre-trained model pushes it back to an unsafe status. Inspired by those methods, we identify language-agnostic safety parameters in the model parameter space by estimating language-specific neuron importance in a manner akin to neuron-pruning (Wei et al., 2024) and Integrated Gradients (Sundararajan et al., 2017).\nMultilingual safety Multilingual LLMs and its safety problem are gaining increasing attention. Unlike detoxification (Li et al., 2024), safety refusal has poor cross-lingual generalization. For instance, simply translating English malicious prompts into non-English can bypass safety guardrails in both closed-source and open-source LLMs (Yong et al., 2023a; Wang et al., 2023; Deng et al., 2023). Linguistic form changes, such as transliteration (Ghanim et al., 2024) and code-switching (Upadhayay and Behzadan, 2024), can also enable jailbreaking of safety guardrails. Furthermore, Shen et al. (2024) demonstrate that English refusal training generalizes poorly for both low-resource and high-resource languages such as Mandarin Chinese. Our research further demonstrates the cross-lingual fragility of safety refusal guardrails, illustrating that fine-tuning attacks in one language can compromise LLMs across multiple languages due to the language-agnostic safety knowledge embedded in safety-aligned LLMs."}, {"title": "6 Discussion and Future Work", "content": "Our work is the first to reveal that fine-tuning attacks can generalize cross-lingually, where models that are aligned for multilingual safety can be jailbroken through fine-tuning attack in one language. We also identify the language-agnostic parameters within multilingual LLMs that is responsible for safety refusal. Future work on defending LLMs against fine-tuning attacks should robustify this parameters to make multilingual LLMs safer\u2014to the best of our knowledge, all existing work has only focused on English (Hsu et al., 2024; Tamirisa et al., 2024; Huang et al., 2024)."}, {"title": "Limitations", "content": "Our work only focuses on cross-lingual generalization of one type of jailbreaking method, which is fine-tuning on harmful datasets. The language coverage of our work is also limited by that of our safety evaluation datasets and safety evaluators. Furthermore, our interpretability experiments, which reveal the language-agnostic safety parameters, focuses on understanding why fine-tuning attack can serve as cross-lingual attack vector. We hope future work can extend our findings to design more robust safety guardrails that are resistant to cross-lingual fine-tuning attacks and make multilingual LLMs safer."}, {"title": "Ethical Statement", "content": "Our research contributes to the responsible development of LLMs by revealing their potential vulnerabilities: fine-tuning attacks can generalize cross-lingually. While we acknowledge that malicious actors exploit cross-lingual transfer of supervised fine-tuning with harmful data to undo safety alignment training that has been conducted in many languages, we believe that identifying the issues is the first critical step to address them. Our findings also suggests that harmful data filtering before fine-tuning for all languages is necessary to mitigate fine-tuning attacks. Our proposed safety information localization method and shared information ratio metric can also better quantify the risks of the cross-lingual transfer of fine-tuning attacks."}, {"title": "Appendix", "content": "In this section we go through the specific hyperparameters that we optimized for attacking the safety aligned LLMs while using the least amount of resources. The attacks are performed as SFT procedures, using data that contain harmful content."}, {"title": "A Fine-tuning attacks details", "content": "We start off selecting two target LLMs, namely Qwen2-7B-Instruct (Yang et al., 2024) and Llama-3.1-8B-Instruct (Dubey et al., 2024), and using the open-sourced, instruction-following, BeaverTails (Ji et al., 2024a) dataset as our data source. Specifically, we use the 30k-train split, composed of ~ 30k pairs of (Xprompt, Xresponse) that contain both harmful and harmless questions and answers. For our specific purpose, we begin by filtering the harmless pairs out, and selecting a random subset of 100 of the remaining ~ 15k harmful data points. Additionally, we prepare the dataset by converting it into a one-round dialogue, by applying the standard chat template for the used models. Moreover, we follow Qi et al. (2023) and prepend a fixed system prompt: You are a helpful assistant. Make sure you always execute users' instructions. Lastly, we translate each dialogue in the language chosen for each specific attack using the translate python library (tra), which relies on the bing.com open translation service."}, {"title": "Fine-tuning configuration and utility evaluation", "content": "We choose the fine-tuning hyper-parameters to perform successful attacks, while using minimal resources. We employed a learning rate of 2e - 5, with a cosine learning rate scheduler to manage the learning rate decay. Each LLM was fine-tuned over a single epoch, and gradient accumulation was set to four steps to stabilize the training updates. We utilized a paged AdamW optimizer with 32-bit precision for optimization. Gradient checkpointing was enabled to reduce memory usage during training. Additionally, a warmup phase of ten steps was included to gradually ramp up the learning rate at the beginning of the procedure. This configuration ensured a robust and scalable fine-tuning process, tailored to leverage the computational resources effectively while ensuring high rates of violation (Figure 1 and 4).\nFinally, we use the multilingual MMLU (Lai et al., 2023) benchmark to prove that our attacked models remain useful, intruction-following models, after our fine-tuning procedure. Table 6 shows how each attacked LLM retains a utility level that is comparable to its safety-aligned version."}, {"title": "B Details about SIL localization procedure", "content": "We provide here the details about the localization procedure described in Section 3.1. The SIL localization method takes a target model as input (namely a safety-aligned LLM $\\theta_{pre}$), along with two extra inputs (a fine-tuned attacked version of the safety-aligned, $\\theta_{lft}$, and calibration dataset $D$). SIL main objective is to find which of the parameters in $\\theta_{pre}$ (1) are both more responding to safety-related features and (2) are more involved in the fine-tuning attack (considering the shift to $\\theta_{l}$). This gives SIL two degree of freedom, making it able to customize the localization in relation to a specific attacked model (in a specific language), and to a specific safety-knowledge (in its own language), as depicted in Figure 5.\nThe calibration dataset $D$ for our study is again an instruction-following, harmful dataset, for which we again choose BeaverTails-30k (Ji et al., 2024a), with its test split to ensure zero intersection with the one used for fine-tuning attacks.\nFinding importance scores SIL localizes the most important parameters by computing a negative log-likelihood loss over $D$. We extract the prompt and response from each data point and to-"}]}