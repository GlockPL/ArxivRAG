{"title": "Open-Ended 3D Point Cloud Instance Segmentation", "authors": ["Phuc Nguyen", "Minh Luu", "Anh Tran", "Cuong Pham", "Khoi Nguyen"], "abstract": "Open-Vocab 3D Instance Segmentation methods (OV-3DIS) have recently demonstrated their ability to generalize to unseen objects. However, these methods still depend on predefined class names during testing, restricting the autonomy of agents. To mitigate this constraint, we propose a novel problem termed Open-Ended 3D Instance Segmentation (OE-3DIS), which eliminates the necessity for predefined class names during testing. Moreover, we contribute a comprehensive set of strong baselines, derived from OV-3DIS approaches and leveraging 2D Multimodal Large Language Models. To assess the performance of our OE-3DIS system, we introduce a novel Open-Ended score, evaluating both the semantic and geometric quality of predicted masks and their associated class names, alongside the standard AP score. Our approach demonstrates significant performance improvements over the baselines on the ScanNet200 and ScanNet++ datasets. Remarkably, our method surpasses the performance of Open3DIS, the current state-of-the-art method in OV-3DIS, even in the absence of ground-truth object class names.", "sections": [{"title": "Introduction", "content": "3D point cloud instance segmentation (3DIS) [1, 2, 3, 4, 5, 6], also known as closed-vocabulary 3D instance segmentation, aims to segment all points in a point cloud into instances of classes predefined in the training set. However, this approach is less practical for scenarios where the test classes are unknown or different from the training classes. This limitation has led to the development of open-vocabulary 3D instance segmentation (OV-3DIS) [7, 8, 9, 10, 11, 12]. Despite these advancements, OV-3DIS methods face several practical challenges. One major challenge is that class names must be predefined during testing, necessitating human intervention for scene understanding. This requirement significantly impedes the perception of truly autonomous agents. A potential solution is to predefined large and comprehensive vocabularies; however, this can lead to inaccuracies and significantly degrade the performance of OV-3DIS when an excessive number of vocabularies are used.\nTo overcome these limitations, we introduce a novel task called Open-Ended 3D Point Cloud Instance Segmentation (OE-3DIS). Unlike traditional methods, OE-3DIS does not require predefined class names during testing. Given a 3D point cloud and RGBD sequence, the system automatically generates a set of 3D masks along with their class names. To evaluate the performance of OE-3DIS methods, we propose a new metric called the OE score, in addition to the standard AP score. The OE score measures both the intersection-over-union (IoU) of the predicted instance masks with the ground-truth masks and the semantic similarity between the predicted and ground-truth class names.\nTo address this new and challenging task, we investigate several typical approaches: leveraging OV-3DIS methods and utilizing Multimodal Large Language Models (MLLMs). In the first approach, a list of class names is either predefined from large vocabularies or pre-extracted from image taggers. In the second approach, 2D visual tokens extracted from a pretrained 2D CLIP encoder are lifted to 3D and used as 3D visual token inputs to an LLM to predict class names. Among these methods, we find that pointwise visual token lifting to 3D for use as input to the LLM performs the best and is selected as our chosen approach. This method performs on par with state-of-the-art OV-3DIS approaches, where ground-truth classes are provided. Additionally, this approach is training-free, leveraging a pretrained 2D vision encoder (e.g., CLIP [13]) and a pretrained LLM (e.g., Vicuna [14]), thereby mitigating the drawback of limited training data faced by many 3D-LLM methods.\nTo evaluate the performance of these methodologies, we conduct experiments on open-ended versions of two prominent 3D instance segmentation (3DIS) datasets: ScanNet200 [15] and ScanNet++ [16]. The results underscore the efficacy of our chosen approach over alternative baselines, demonstrating performance levels comparable to OV-3DIS methods that rely on ground-truth class names. Specifically, in ScanNet200, our approach attained an AP of 16.0, contrasting with the 22.2 AP achieved by Open3DIS, currently recognized as the state-of-the-art in OV-3DIS. However, our approach is superior in ScanNet++, where it outperforms Open3DIS by a significant margin (18.4 vs. 13.1).\nIn summary, the contributions of our work are:\n1. We propose Open-Ended 3D Point Cloud Instance Segmentation (OE-3DIS), a task that segments 3D point clouds by instances and generates class names without predefined labels.\n2. We introduce the OE score, which evaluates both the IoU of predicted instance masks with ground-truth masks and the semantic similarity of predicted and ground-truth class names.\n3. We explore approaches for OE-3DIS, including leveraging OV-3DIS methods and Multimodal Large Language Models (MLLMs).\n4. We present a training-free OE-3DIS method that lifts 2D visual tokens to 3D for LLM input, utilizing pretrained models like EVA-CLIP and Vicuna. This approach outperforms other baselines and achieves performance comparable to OV-3DIS methods using ground-truth class names on ScanNet200 and ScanNet++ benchmarks."}, {"title": "Related Work", "content": "3D instance segmentation (3DIS) methods such as Mask3D [2], ISBNet [3], PointGroup [6], and others [17, 18, 19, 20] cluster a point cloud scene into 3D instance masks of classes predefined in the training set. These methods utilize a 3D Convolutional backbone [21, 22, 23] to extract semantic information from the 3D scene. Subsequently, they employ either Dynamic Convolution-based [4] or Grouping-based [5] modules to generate 3D instance masks. Recently, some approaches have adopted techniques to back-project 2D information aggregated from multiple views onto the 3D point cloud to create an ensemble of 3D point cloud features [24, 12, 25, 26]. These 2D-derived features contain rich semantic information, while those derived from 3D capture the geometrical structure of 3D objects. Combined, they supervise a 3D instance decoder to refine segmentation masks. However, these methods are closed-vocab or cannot segment new classes in testing, limiting the capability of agents to understand new 3D scenes.\nOpen-vocabulary 3D instance segmentation (OV-3DIS) aims at segmenting 3D objects of classes newly provided in testing. To provide 3D proposals for object recognition, OpenMask3D [8] and Lowis3D [27] employ 3DIS networks [3, 2] to generate class-agnostic 3D proposals, while SAI3D [10], MaskClustering [11], and OVIR [9] utilize 2D segmenter for producing masks for each view and consistently lift these masks to 3D. While 3DIS networks excel in capturing large geometrical structures, they often struggle to detect rare and small-shaped objects. Conversely, 2D segmenters are adept at focusing on small regions but face challenges in maintaining object consistency when lifting to the 3D point cloud. Open3DIS [7] addresses these limitations by combining both 3D and 2D branches, resulting in superior results. This approach effectively captures rare and small objects while preserving the 3D geometrical structures of large objects using superpoint-level masks. While OV-3DIS is useful in some scenarios, the constraint of a predefined vocabulary set in testing requires human intervention, hindering truly autonomous agents.\n3D scene understanding with Large Language Models (LLMs). Utilizing LLMs for 3D scene understanding focuses on how objects are aligned, their directions, and their locations based on textual questions within 3D environments. This approach emphasizes the spatial aspects of language understanding in three dimensions of data. Previous works [28, 29, 30, 31, 32] have contributed to providing 3D spatial data with language for various applications, including 3D instance and scene captioning [33, 34, 35, 36, 29, 37, 28], 3D visual answering questions [29, 30, 38, 39, 37, 28, 40, 33], 3D visual grounding [37, 28, 29, 31, 32, 41] and supporting embodied AI tasks like planning and reasoning [28, 37, 29, 33].\nOpen-ended 2D Image Understanding is an emerging task that addresses the need to recognize objects without predefined class names during training or testing. There is scant work on this task, with existing research primarily focusing on image classification [42, 43, 44], object detection [45], and instance segmentation [46, 47, 48]. Standard 2D Multimodal LLMs (2D MLLMs), such as LLAVA [49], consist of a frozen vision encoder, a projector, and an LLM module. These models typically finetune either (1) the linear projector and the LLM or (2) a complex Q-Former projector. However, applying these methods for 3D scene understanding (3D-LLMs) is challenging due to the lack of sufficient 3D data and text description pairs to effectively train 3D-LLMs. In this paper, we present a novel approach to leveraging pretrained 2D MLLMs for OE-3DIS.\nOmniScient Model (OSM) [47] is a recently proposed pretrained 2D Multimodal LLM for Open-ended 2D Instance Segmentation, which serves as the foundation for our proposed method. OSM comprises three main modules: a visual encoder, a MaskQ-Former, and a Large Language Model (LLM). The visual encoder is a pretrained EVA-CLIP [50], a variant of the CLIP model [13], which extracts high-resolution visual features using a sliding-window scheme and incorporates global positional embeddings to preserve spatial information. The MaskQ-Former, a customized version of Q-Former [51] designed to focus on the mask region rather than the entire image, converts visual features into fixed-length visual tokens. These tokens are then input into the Vicuna LLM [14]. The LLM processes these tokens to answer the question, \"What is in the segmentation mask?\" by outputting the object name.\nIn OSM, only the MaskQ-Former is trained to align visual features with the visual tokens for the LLM, while the visual encoder and LLM remain unchanged. The training datasets are large, including COCO [52], LVIS [53], ADE20K [54], and Cityscapes [55]. This setup demonstrates a strong capability for recognizing objects without predefined class names in 2D images, inspiring us to extend this approach to 3D scene understanding."}, {"title": "Methodology", "content": "3.1 Problem Statement\nGiven a 3D point cloud scene $P = \\{p_i\\}_{i=1}^N \\in \\mathbb{R}^{N\\times6}$ consisting of N points with xyz coordinates and associated rgb colors, along with T RGB-D frames with color images $\\{I_t\\}_{t=1}^T$ and depth ones $\\{D_t\\}_{t=1}^T$, where $I_t \\in \\mathbb{R}^{H\\times W\\times 3}$, $D_t \\in \\mathbb{R}^{H\\times W}$, we aim to segment all K object binary masks $\\{m_k\\}_{k=1}^K$, $m_k \\in \\{0, 1\\}^N$ and their associated class names $\\{l_k\\}_{k=1}^K$ without giving any predefined class names in testing.\nThe auxiliary information includes the intrinsic matrix $\\Gamma \\in \\mathbb{R}^{3\\times 3}$ and the extrinsic matrix $[R|v] \\in \\mathbb{R}^{3\\times 4}$, where H, W are the image's height and width, R is a 3D rotation matrix, and v is a 3D translation vector. This composite matrix of rotation and translation converts coordinates from the global frame (of the point cloud) to the camera's frame at view t.\n3.2 Evaluation Metrics\nTo evaluate open-ended object detection or instance segmentation, where predicted class names may be similar but not exactly the same as ground-truth (GT) class names, prior work [56] proposed a label reassignment technique. This method uses text encoders (e.g., CLIP [13], BERT [57], Sentence Transformer [58] to encode both the predicted and GT class names for each scene. It then matches each predicted class name to its closest GT class name based on cosine similarity. After this matching, the standard AP score is used to evaluate performance.\nHowever, the AP score described above is based on a many-to-one matching where multiple predicted class names can be matched to the same ground-truth class name, leading to inaccuracies in fine-grained class prediction. To address this, we propose a new metric called the Open-ended score (OE score), which considers both textual semantic similarity and spatial similarity of predicted and ground-truth masks and their class names. Specifically, given K predicted instances with masks,"}, {"title": "Proposed Baselines", "content": "Since OE-3DIS is very new and challenging, even more so than OV-3DIS, we focus our efforts on investigating prominent baselines. These baselines are illustrated in Fig. 1. They require a list of class-agnostic 3D mask proposals pre-extracted from Open3DIS [7] with the DETIC 2D segmenter.\nLarge-vocab approach (Fig. 1 - Left): We start with a simple OE-3DIS baseline by using a large vocabulary of 21K common classes introduced by DETIC [59] as predefined class names for OV-3DIS methods like Open3DIS [7] and OpenMask3D [8]. However, this approach fails to achieve robust class prediction. This is because the fixed large vocabulary set contains multiple synonyms, resulting in uninformative class predictions after the Softmax operation.\nImage Tagging approach (Fig. 1 - Middle): To reduce the number of classes, we leverage image-tagging techniques such as RAM++ [44] to obtain only relevant class names per scene. Specifically, for each input view, a set of image tags is generated and then combined across all processed input views. The resulting unified tag set serves as the vocabulary for OV-3DIS. However, these methods often produce inconsistent class names across views, leading to redundant and similar class names.\nMaskwise approach (Fig. 1 - Right): To tackle the inconsistency in class names across views, we first apply an OE-2DIS method, such as OSM [47], to each view to obtain a list of 2D masks and their predicted fixed-length visual tokens. For each 3D mask proposal, we project it onto a view and associate it with the best-matched 2D mask based on IoU to obtain its 2D fixed-length visual tokens. The 3D visual tokens for the 3D mask are then aggregated by averaging the 2D visual tokens across views, which are subsequently input into a pretrained LLM to obtain the final class names. However, this approach heavily relies on the matching process between the 3D proposal and 2D masks in each view, which is often misaligned due to imperfect segmentation and depth maps. In other words, a 3D mask can project onto multiple 2D masks, and selecting only the best-matched 2D mask might result in the loss of important information."}, {"title": "Our Approach", "content": "To address the above limitations, we propose a method for producing pointwise 3D visual tokens, as illustrated in Fig. 2. First, we generate class-agnostic 2D instance segmentation masks for all views using class-agnostic 2D segmenters such as DETIC [59] and SAM [60]. Next, we lift 2D masks into 3D masks using Open3DIS [7]. Simultaneously, these 2D masks, along with their corresponding RGB images, are used to extract 2D visual tokens from an MLLM like OSM [47]. We then lift the resulting 2D visual tokens $F^{2D}$ into 3D visual tokens to obtain pointwise 3D visual tokens $F^{3D}$. Finally, for each 3D proposal mask, we query the 3D visual tokens associated with this proposal by aggregating the pointwise lifted 3D visual tokens, forming the final tokens $f^{3D}$ for input to the LLM.\nThis approach takes into account the depth and geometric structure of 3D objects via lifting, resulting in more robust visual tokens and a unified, densely-featured point cloud that can be queried instantly at test time. Subsequently, we will focus on our pointwise visual tokens lifting and aggregation.\nFirst, the correspondence of a 3D point $p_i (x, y, z) \\in P$ with its 2D projection $(u, v)$ in view t is:\n$\\begin{bmatrix}\n  u_i \\\\\n  v_i \\\\\n  1\n\\end{bmatrix} = \\Gamma \\cdot [R|C] \\begin{bmatrix}\n  X_i \\\\\n  Y_i \\\\\n  Z_i \\\\\n  1\n\\end{bmatrix}$", "equations": ["\\begin{bmatrix}\n  u_i \\\\\n  v_i \\\\\n  1\n\\end{bmatrix} = \\Gamma \\cdot [R|C] \\begin{bmatrix}\n  X_i \\\\\n  Y_i \\\\\n  Z_i \\\\\n  1\n\\end{bmatrix}"]}, {"title": "Experimental Results", "content": "Datasets: We conducted experiments to assess the performance of the baselines and our proposed method using two common 3DIS datasets: ScanNet200 [15] and ScanNet++ [16]. The ScanNet200 dataset builds on the original ScanNet [61] by expanding its semantic categories from 20 to 200. Its instance segmentation benchmark includes 1,201 training scenes and 312 validation scenes with 198 object categories, significantly enriching the vocabulary and enhancing its capability for detailed 3D semantic and instance segmentation. The ScanNet++ dataset was recently introduced, featuring up to 1,659 semantic categories, with 360 training scenes and 50 validation scenes. Given the large number of classes, we follow the standard 3DIS evaluation protocol on ScanNet++ and evaluate only the most common 100 object categories. This dataset offers a much denser 3D point cloud scene representation, making it the most challenging dataset for 3D understanding.\nEvaluation metrics: We assess OE-3DIS using two metrics: the label reassignment AP score (detailed in Sec. 3.2) and our newly proposed OE score. For ScanNet200, we also report APhead, APcom, and APtail. For ScanNet++, we include the recall rate (RC) and average recall rate (AR). We note that the OV-3DIS methods adopt a specific AP score calculation protocol by assigning a confidence score of 1.0 to each 3D proposal. Similarly, we follow the same evaluation protocol as the Fully-sup 3DIS by ranking 3D proposals according to their confidence scores. In the context of OE-3DIS, our approach utilizes the confidence scores generated by the LLM, while potential baselines employ the CLIP score.\nImplementation details: Following Open3DIS [7], we generate class-agnostic 3D proposals from ISBNet [3] pretrained on ScanNet200 or by lifting 2D masks to 3D using Detic [59]. For RAM++ [44], we employ the Swin-L model, trained on a 14-million image dataset with an image size of 384px and a tagging threshold of 0.68. For LLM, we leverage Vicuna-7B [62], fine-tuned for open-ended 2D Instance Segmentation [47].\n4.1 Comparison with Baselines\nWe compare our approach to the proposed baselines using the OE-3DIS setting on the ScanNet200 (Tab. 1) and ScanNet++ (Tab. 2) datasets. For reference, we also present results from OV-3DIS methods, including OpenMask3D [8] and Open3DIS [7], as well as fully-supervised methods like ISBNet [3] and Mask3D [2] for ScanNet200; and PointGroup [6] and SoftGroup [5] for ScanNet++."}, {"title": "Ablation Study", "content": "To study many design choices of our pointwise approach, we intensively carry out ablation study on the ScanNet200 [61] dataset.\nStudy on different point feature aggregation techniques (Eq. (6)) is shown in Tab. 3. We evaluate four techniques for combining point features from a given 3D mask: max, random (randomly selecting one point), mean, and weighted average (our proposed operation). The weighted average technique achieved the highest performance with an AP score of 16.0, outperforming the other methods. Consequently, we utilized the weighted average technique in all our experiments."}, {"title": "Discussion and Conclusion", "content": "Limitations: Though our method enhances open-ended 3D scene understanding, it has notable drawbacks. Firstly, it heavily relies on 2D visual tokens from the pretrained OSM, which are trained on instance segmentation datasets with limited classes. This hampers the model's ability to recognize a wide range of classes in an open-world environment. Secondly, the performance of OE-3DIS is contingent upon class-agnostic 3D instance segmentation (3DIS) methods, which in turn rely on the quality of 3D representation and 2D-3D mapping, such as camera calibrations and depth images.\nConclusion: We have introduced Open-Ended 3D Point Cloud Instance Segmentation (OE-3DIS), which generates 3D masks and object class names without predefined labels during testing. We have explored baselines using OV-3DIS methods and MLLMs, and introduced a pointwise training-free approach leveraging OSM. We have also proposed the OE score, tailored for OE-3DIS. Experiments on ScanNet200 and ScanNet++ show our approach's superior performance, notably outperforming Open3DIS (SOTA on OV-3DIS) on ScanNet++ without ground-truth class names."}]}