{"title": "Neuropsychology and Explainability of AI: A Distributional Approach to the Relationship Between Activation & Similarity of Neural Categories in Synthetic Cognition", "authors": ["Michael Pichat", "Enola Campoli", "William Pogrund", "Jourdan Wilson", "Michael Veillet-Guillem", "Anton Melkozerov", "Paloma Pichat", "Armanush Gasparian", "Samuel Demarchi", "Judicael Poumay"], "abstract": "We propose a neuropsychological approach to the explainability of artificial neural networks, which involves using concepts from human cognitive psychology as relevant heuristic references for developing synthetic explanatory frameworks that align with human modes of thought. The analogical concepts mobilized here, which are intended to create such an epistemological bridge, are those of categorization and similarity, as these notions are particularly suited to the categorical \"nature\" of the reconstructive information processing performed by artificial neural networks. Our study aims to reveal a unique process of synthetic cognition, that of the categorical convergence of highly activated tokens. We attempt to explain this process with the idea that the categorical segment created by a neuron is actually the result of a superposition of categorical sub-dimensions within its input vector space.", "sections": [{"title": "Introduction", "content": "Within an explainability framework, the neuropsychology of artificial intelligence focuses on studying synthetic neural cognitive mechanisms, considering them as new subjects of cognitive psychology research. The goal is to make artificial neural networks used in language models understandable by adapting concepts from human cognitive psychology to the interpretation of artificial neural cognition. In this context, the notion of categorization is particularly relevant because it plays a key role as a process of segmentation and reconstruction of informational data by the neural vectors of synthetic cognition.\nThus, in this study, the aim is to use the concept of categorization, as understood in human cognitive psychology (particularly in its relation to the notion of similarity), to apply it to the analysis of neural behavior and to infer certain synthetic cognitive processes underlying the observed behaviors."}, {"title": "Categorical Explainability of Artificial Neural Networks", "content": null}, {"title": "Epistemology and Utility of Synthetic Explainability", "content": "Explainability involves describing the activity of an artificial neural network in terms that are understandable to humans (Du et al., 2019; Pichat, 2023, 2024a, 2024b). At the very least, it involves projecting the observable behavior of a neural network into an interpretative framework that allows for the assignment of meaning to this behavior, which is relevant to an observer, depending on their objectives. In our case, this framework is that of cognitive psychology, which involves using the categories of human cognitive thought (and, more specifically for us, the notion of categorization) as conceptual references to draw heuristic analogies between human and artificial cognitive behaviors. This must be done while avoiding the epistemological pitfalls of anthropomorphism (Nadeau, 1999), neo-behaviorism (Bloch et al., 2011), or the confusion between the observer and the observed system, against which cybernetics, systems theory, and enactive cognitive science warn us (Watzlawick, 1977, 1984; Varela, 1984, 1996).\nThe pragmatic function of explainability is twofold. First, it aims to inhibit potentially misleading or even dangerous responses from synthetic neural systems (Luo et al., 2024): errors, cognitive biases (Echterhoff, 2024) or cultural biases (Kheya, 2024), hallucinations (Kandpal et al., 2023; McKenna et al., 2023), undue focus on certain inputs (Du et al., 2023), etc. Second, it seeks to enhance the performance of language models (Bastings et al., 2022) by improving their coherence with human reasoning (Ma et al., 2023). In short, the goal is to develop cognitive alignment (Pichat, 2023, 2024a; Khamassi, 2024)"}, {"title": "Objects of Study in Synthetic Explainability", "content": "In the realm of fine-grained explainability, studies typically focus on either multilayer perceptron-type neurons (Bricken, 2023) or the attention heads of transformers (Clark et al., 2019). Regarding attention heads, the goal is to understand the type of categorical knowledge or cognitive processes encoded by the attention weights. Here, the interest may include, for example, the effects of attention on the analysis of data reliability in the initial and middle layers (Li et al., 2023), the attentional identification of syntactic categorical features such as indirect objects (Wang et al., 2022), or the mnemonic effect of attention weights (Geva et al., 2021).\nIn terms of the type of cognitive entities studied at the neuronal level, two classes of explainability studies can be distinguished: those related to cognitive contents (categories, concepts) and those focusing on cognitive processes (circuits). In the first category, the main subject of study is the relationship between neurons and specific conceptual categories (Sajjad et al., 2022; Foote et al., 2023). In the second category, studies examine, for example, the effect of connection weights between neurons on performing elementary logical operations (AND, OR, etc.) in certain neural branches (Voss et al., 2021); or subsets of neurons involved in decision-making (Antverg & Belinkov, 2022).\nIn terms of neuronal scope, two types of investigations can be differentiated: those focusing on categorical encodings specific to isolated neurons or attention heads (Jaunet et al., 2021; Bills et al., 2023) and those with a broader scope, aiming to trace the effect of neural connections on the formation of specific circuits that are homogeneous in their cognitive activity (Olah et al., 2020; Anthropic, 2023; Bricken, 2023)."}, {"title": "Examples of Low-Granularity Synthetic Categorical Explanations", "content": "Various studies reveal or rather infer a variety of categories (linguistic, logical, positional, etc.) encoded within neurons and attention heads. We present some of these here, thus omitting works related to neurons involved in cognitive processes.\nIn the context of the classical experiment by Clark et al. (2019) on BERT, the authors highlight the converging linguistic functions of the attention heads from the same layers:\n\u2022 Identification of syntactic or morphological linguistic categories: direct objects of verbs, objects of nominal determiners, objects of propositions, objects of possessive pronouns, verbs modified by passive auxiliary verbs.\n\u2022 Identification of linguistic categories related to coreference: antecedents of coreferential mentions (she/her, talks/negotiations).\n\u2022 Identification of separator categories that enable text segmentation and delimitation: periods, the separator token \"SEP\".\n\u2022 Identification of positional categories: next token, previous token.\nIn their fascinating study on GPT2-XL, Bills et al. (2023) showcase a series of unique neurons, highlighting for some their strong consideration of contextual elements:\n\u2022 Categorical neurons associated with any token from a specific lexical field. For example, a neuron activating for words describing movement involving feet (ran, walked, danced, kicked, hopped, stepping, tiptoed); or a neuron linked to things done correctly.\n\u2022 Neurons associated with the category of phrases possessing a certain semantic valence, such as the \"simile\" neuron related to phrases involving certainty or confidence.\n\u2022 Anomaly-detecting neurons, for example, the category of truncated or strange words."}, {"title": null, "content": "\u2022 Neurons reacting to a specific token but only within a given linguistic context, thus forming a contextual category relative to a token. For instance, the \"hypothetical had\u201d neuron activating for the token \u201chad\u201d in a hypothetical context or in which things could have been different; or a neuron active for \"together\" but only when preceded by \"get\"; or neurons becoming operational for specific words at the beginning of the text.\n\u2022 Neurons detecting logical sequences such as the category of repetition of identical tokens or the category of breaking the logic of a sequence (1, 2, 3, 5).\n\u2022 Anticipatory categorical neurons (clearly linked to the purpose for which the model was trained) becoming functional in a context corresponding to a likely next token, for example, when the next token is likely \"from\".\nStudies also point to a geographical distribution of the specific type of neuronal categorical activity depending on the depth level of the layers they occupy. For example, the attention heads of the initial layers have broader attention compared to those more focused on tokens (Clark et al., 2023). Additionally, the initial layers are more responsive to categories of morphological elements at the word level, while later layers are more sensitive to syntactic categorical features related to sentences (passive/active voice, tense) and semantic categorical information (Jawahar et al., 2019)."}, {"title": "Synthetic Categorization and Human Categorization", "content": null}, {"title": "Main Categorical Characteristics of an Artificial Neural Network", "content": "An artificial neural network, particularly a language model, is organized into three components: input and output layers, which have perceptual and effector functions (Savioz et al., 2010), and intermediate layers, known as hidden layers. In the case of transformers, these intermediate layers include multilayer perceptron layers, which are the least studied (Garde et al., 2023) and are the focus of our current work, and attention layers.\nMultilayer perceptron layers are characterized by an aggregation function and an activation function. The aggregation function, in the form $\\sum(W_{i,j}x_{i,j})+a$ has specific parameters unique to each neuron and produces the categorical segmentation at the output (i.e., creates a new category) based on the input vector representation it receives. Initially, each weight $w_{ij}$ in the aggregation vector indicates to what extent its associated categorical dimension $j$ of the input semantic vector space should be selectively considered to generate the new output categorical dimension. In other words, each weight acts as an epistemological selector (Pichat, 2024b) that, from a cognitive perspective, performs an activity of selective categorical attention by governing the level of importance assigned"}, {"title": null, "content": "to a given input categorical dimension. Secondly, all the products (attention weight x categorical dimension) are summed. This additive concatenation cognitively performs an activity of weighted epistemological fusion (Pichat, 2024) of the involved input categorical dimensions. The end result of this weighted linear combination is the creation of a new categorical segment (i.e., a new categorical dimension $j'$, a new category), which is more abstract and relevant to the network's targeted activity. In a neurobiological analogy, weights are instantiated by synaptic efficacy, which is the amount a synapse releases its neurotransmitter into the synaptic cleft (Savioz et al., 2010).\nThe activation function (Sigmoid, Tanh, GeLU, ReLU, Leaky ReLU, Softmax, etc.), besides possibly normalizing the neuron outputs, introduces non-linearity into the neural system. From a cognitive perspective, this non-linearity increases categorical contrast (better signal-to-noise distinction) and thereby facilitates the convergence of neuron activation for certain categories (i.e., facilitates the construction of stable and distinguishing categories). It allows for a hierarchy in the distribution of categories constructed by the network (with more elementary categories in the early layers and progressively increasing complexity in subsequent layers) and a relative sparsity of the network. This sparsity, meaning the limited activation of neurons depending on the type of input, prevents overfitting (categories that are too specific, making generalization and adaptation to diversity impossible) and reduces the network's computational cost. The biological counterpart of the activation function is the transfer function (Savioz et al., 2010), of the type $1 + exp(-(G-activation_{net} + b))$, at the level of generating the action potential at the axon hillock and at the level of neuromodulation by dopamine and norepinephrine at the synapse (Servan-Schreiber, 1990).\nAttention heads (Vaswani et al., 2017) rely on the fundamental mechanism of self-attention, which allows for the calculation of contextual attention weights for each categorical part of the input data using three learned weight matrices (queries $Q$, keys $K$, values $V$). The attention scores thus determined, by comparing each element with all other elements in a given informational sequence, enable informational selectivity (i.e., focusing attention on relevant categories of elements to the detriment of irrelevant ones) and contextual flexibility (i.e., adjusting the attention weight assigned to certain categorical elements based on the overall context and relationships between elements). With a fundamentally organizational purpose, each attention head captures or rather constructs a type of structural categorical relationship between different elements of an input sequence and then injects these \"categorical meta-information\" into the representations of the elements to be processed. These representations are thus informationally enriched by the categorical relationships that these elements maintain with other important elements in the sequence in question."}, {"title": "Human Categorization", "content": "In the realm of human thought, categorization is essential in various cognitive activities, such as classification, object identification, understanding, reasoning,"}, {"title": null, "content": "problem-solving, memory, inference, prediction, and conceptualization (Sternberg, 2007; Roads et al., 2024).\nRosch (1975) proposes an approach to categorization based on the resemblance of an object to a prototype of the category, noting that individuals tend to state characteristic features rather than determinative properties (Rosch & Mervis, 1975); the prototype being the most representative example of the category (Singh et al., 2020; Vogel et al., 2021). The exemplar theory of categorization (Medin & Schaffer, 1978; Nosofsky, 1992; Nosofsky et al., 2022), as for it, suggests that objects are compared to typical examples stored in memory, with the most typical exemplar being the one that most closely resembles the known exemplars. Finally, the contextual or goal-directed approach to categorization (Barsalou, 1983; Glaser et al., 2020) emphasizes the purpose of the situation in defining a category rather than relying on a general logic or semantics.\nThe approaches to categorization by similarity, which are of particular interest in the context of this work, posit that an object is assimilated to a class based on its proximity to a representation of that class (Thibault, 1997; Jacob et al., 2021; Kaniuth et al., 2022; Roads et al., 2021, 2024). The prototype and exemplar theories mentioned earlier emphasize similarity as the basis for categorization (Ayeldeen et al., 2015; Sanborn et al., 2021; Roads et al., 2024). However, critics of similarity-based categorization argue that the choice of criteria for judging similarity is arbitrary and may not align with the foundations of categorical attribution (Love, 2002; Kalyan et al., 2012; Reppa et al., 2013; Poth, 2023). Reasoning by similarity is thus considered too ambiguous to be functional (Wixted, 2018). Proponents of similarity in categorization (Bobadilla et al., 2020; Hebart et al., 2020) counter-argue that these criticisms are based on two epistemological errors: (i) a realist error that assumes categorization must capture a predefined reality, and (ii) a rationalist error that imposes a normative logic on categorization, a logic that every individual is expected to understand."}, {"title": "Problem Statement", "content": "As we have mentioned, the goal of synthetic explainability is to make the operations of an artificial neural network accessible to human understanding (Du et al., 2019). This requires converting the observable behavior of neural networks into an interpretative framework containing explanatory elements compatible with the cognitive frameworks of human thought. In this context, human cognitive psychology emerges as a relevant heuristic framework for developing explanatory analogies of synthetic cognition. More specifically, the cognitive psychology of categorization is particularly pertinent because synthetic information processing significantly involves segmentation and categorical analysis behavior (Pichat et al., 2024). Indeed, numerous studies (Jawahar et al., 2019; Clark et al., 2019; Bills et al., 2023; Clark et al., 2023) highlight artificial cognition in language models as relying largely on a dynamic extraction of broad linguistic categorical invariants."}, {"title": null, "content": "In the context of this study, as previously mentioned, we focus on an epistemological explainability with fine cognitive granularity (Pichat, 2024). In other words, we examine a microscopic explainability where the unit of observation is the formal neuron. This low-granularity interpretative approach aims to directly penetrate the \"black box\" system that constitutes an artificial neural network, creating elements of understanding about how categories of thought and concepts are locally encoded and structured within a language model (Dalvi et al., 2019, 2022). The objective is to interpret how categorical knowledge is constructed and utilized by the fundamental elements of the networks, namely the neurons themselves (Fan et al., 2023). Concerning the specific question of utilization, we focus here on the particular issue of the relationship between activation and categorical similarity.\nIndeed, in connection with the problem we have raised in the field of human cognition regarding the relationship between categorization and similarity, we have, in a previous study (Pichat et al., 2024), transposed this relational heuristic question to the domain of artificial cognition by posing the following inquiry: Is the degree of membership (operationalized in terms of activation level) of tokens (received by a neuron in the form of embeddings) to the category associated with that neuron related to their level of similarity (operationalized in terms of cosine similarity)? In other words, are the intensity of categorical membership and the intensity of similarity of tokens, as evaluated by a neuron, two facets of the same phenomenon? This question, largely unexplored to date in the field of artificial systems explainability (Fan et al., 2023; Luo et al., 2024; Zhao et al., 2024), seemed particularly relevant to examine.\nIn our previous study, we demonstrated the compatibility of our results with two formulated hypotheses: (i) a categorical discontinuity of successive core (i.e. high mean level of activation) tokens in terms of activation level (suggesting particularly low cosine similarities between these core tokens) and (ii) a categorical heterogeneity of core tokens with similar activation levels (these core tokens are not the closest in terms of cosine similarity). These two hypotheses complement each other and both address the overall relationship between activation proximity and cosine similarity. However, they are positioned within a static approach, aiming to investigate the relationship between activation proximity and cosine similarity independently of the activation level of the tokens involved. However, other results, not yet explored during this initial research, prompt us to continue this investigation from a distributional perspective, questioning a possible evolution of the relationship between categorical membership proximity (i.e., activation proximity) and categorical proximity (i.e., cosine similarity) depending on the activation levels of the tokens involved. This is the work we undertake in the present research."}, {"title": "Methodology", "content": null}, {"title": "Methodologies of Synthetic Explainability", "content": "To provide methodological context for our study, we present here a brief, non-exhaustive overview of technical approaches, whether at low or high granularity, aimed at inferring the cognitive contents or processes encapsulated within formal neurons and their assemblies (in layers, clusters, or the global network). These methods are not mutually exclusive, allowing for a certain degree of overlap.\nHigh-granularity techniques, as previously mentioned, are grounded in the input/output contrast and aim to study the relationship between initial information and final outputs of a language model. In this context, gradient-based methods seek to measure the importance of each input by analyzing the partial derivatives of the output with respect to each input dimension (Enguehard, 2023). Input characteristics can be measured in terms of features (Danilevsky et al., 2020), token importance scores (Enguehard, 2023), or attention weights (Barkan et al., 2021). Similarly, example-based approaches aim to understand the extent to which the output changes with different inputs. This is done by showing how the network outputs are impacted by small changes in input (Wang et al., 2022) or by alterations (deletion, negation, mixing, masking) of inputs (Atanasova et al., 2020; Wu et al., 2020; Treviso et al., 2023). Lastly, we should mention works that perform a conceptual mapping of inputs and then measure the contribution of these concepts to the observed outputs (Captum, 2022).\nFine-granularity methodologies, as discussed earlier, do not take the final output of the language model as their output but rather its intermediate outputs or states at the level of neurons or clusters or layers of neurons. In this context, some methods aim to linearly decompose the relevance score of a neuron in a given layer based on its inputs (neurons, attention heads, or tokens) in the previous layer (Voita et al., 2021). Other methods focus on linearizing activation functions to facilitate neural interpretation (Wang et al., 2022). Still other methods, based on the model's vocabulary, seek to identify the knowledge encapsulated by projecting the connection weights and intermediate representations into the model's vocabulary space via a de-embedding matrix (Dar et al., 2023; Geva et al., 2023). Finally, we should mention approaches based on neuronal activation statistics in response to corpora (Durrani et al., 2022; Wang et al., 2022; Dai et al., 2022; Bills et al., 2023; Mousi et al., 2023). It is within the specific context of these latter approaches that our present study is situated."}, {"title": "The Explainability Study as the Source of Our Data", "content": "Our data is derived from the intriguing study by Bills et al. (2023). Based on the hypothesis that a neuron activates specifically for a property to be determined, which may include context, the authors conducted an extensive study to interpret the categorical semantics of all neurons in GPT-2XL.\nMethodologically, the OpenAI researchers proceed as follows. They subject GPT-2XL (the \"subject\" model) to a large series of 64-token sequences, ran-"}, {"title": "Selection and Interpretation of Data", "content": "We briefly present the methodological choices made in this current study, in continuity with those made during our previous study (Pichat et al., 2024) concerning the relationship between categorical membership (activation) and categorical proximity (similarity), which this current work continues.\nTo simplify our study, we limited our analysis to the first two layers of GPT-2XL (layers 0 and 1) and the 6,400 neurons in each layer. For each neuron among these 12,800 neurons in total, we chose to consider its 100 most highly activated tokens on average, along with their respective activation values. This approach differs from that of Bills et al. (2023), which focuses only on hyperactivated tokens. We consider this method partially limited (even though it allows for valuable effects, as indicated by the authors) because it does not capture the variability of tokens for which a neuron activates. We prefer a more"}, {"title": null, "content": "comprehensive view of the category of tokens to which a neuron responds.\nWe consider the average activation level of a token in a neuron as a good measure of the categorical membership of that token to the involved neural category. Indeed, the average activation of the most activated tokens (i.e. core tokens) seems to represent well the extent to which these tokens are part of the extension of a category. This is in line with the hypothesis of Bills et al. (idem) that a neuron activates for a specific property.\nThe cosine similarity between two tokens also seems to be a good measure of categorical similarity between items. This aligns with Thibault (1997), who defines similarity based on a method of calculating the distance between compared categorical dimensions. Cosine similarity, commonly used in NLP to measure semantic proximity (Ham, 2023), fits well with this definition.\nWe chose to measure cosine similarity within the embedding base of GPT-2XL to avoid the methodological limitations mentioned by Bills et al. (2023) and Bricken (2023), which involve matching synthetic cognitive systems based on different embedding systems. For comparison and verification, we also used three other freely available embedding bases: Alibaba-NLP/gte-large-en-v1.5, Mixedbread-ai/mxbai-embed-large-v1, and WhereIsAI/UAE-Large-V1."}, {"title": "Results", "content": null}, {"title": "Evolution of the Relationship Between Cosine Proximity and Activation", "content": "To recap, we study the relationship between activation proximity and cosine proximity for successive core tokens of each neuron, that is, the relationship between the proximity of categorical membership levels between two tokens and the categorical proximity between these two tokens. Specifically, we focus on the dynamics of this distribution according to the levels of activation. In other words, we investigate whether there is a potential evolution, across activation segments, in the relationship between cosine proximity and the activation levels of successive core tokens. Graphs 1 (layer 0) and 2 (layer 1) show the average distribution of cosine similarities of pairs of successive core tokens according to the activation rank of the first token of each pair. We can clearly observe an evolution that is initially very slow (ranks 0 to 40), then faster (ranks 40 to 80), and finally appears exponential (from rank 80 onwards) in categorical proximity as a function of activation proximity.\nSeveral points can be noted: (i) this evolution seems invariant across the four embedding bases (although the growth dynamic is more pronounced for GPT-2XL embeddings, as they are more discriminative, as mentioned in Pichat et al., 2024), (ii) the segment of exponential growth appears even more marked for layer 1 compared to layer 0 (is there an acceleration to be noted with increasing layer depth?), (iii) from rank 80 for layer 0 and rank 40 for layer 1, the average cosine similarities per rank become higher than the overall averages (respectively .38 and .42 according to the GPT-2XL embeddings, see tables 1 and 2), (iv)"}, {"title": null, "content": "despite this, the growth in cosine proximity as a function of activation proximity is still bounded by not very high maxima of cosine similarity (respectively .48 and .55 with GPT-2XL embeddings)."}, {"title": "Categorical Convergence and Extreme Values of Cosine Similarity", "content": "A first operationalization to test our hypothesis of categorical convergence of pairs of successive core tokens as activation levels increase can be based on the study of the distribution of lower extreme values of cosine similarity. Indeed, according to this hypothesis, the number of cosine similarity minima should decrease as activation values increase."}, {"title": "Categorical Convergence and Positivity of Relational Monotonicity", "content": "A second relevant operationalization to test our hypothesis of categorical convergence of pairs of successive core tokens as activation levels increase involves a more functional approach (in the mathematical sense) by studying the monotonicity and direction of the relationship between cosine similarity and categorical activation value. In line with our hypothesis, we should observe a monotonic and positive relationship (an increasing function) between these two variables.\nA first approach in this context is to conduct a linear regression study. At first glance, there is a variable and moderate compatibility with the condition for applying regression, which is the normality of residuals (the percentages of associated inferential tests with $p > .05$ vary from 62% to 99% for layer 0 and from 53% to 98% for layer 1). As a result, these regression data are not very reliable (see also graphs 3 and 4, which seem to support the same caution). A linear model is not well-suited to our data, with only 34% of Fisher tests being significant for layer 0 and 45% for layer 1. However, when changing the scale of statistical units (i.e., moving from tokens to neurons), several interesting results emerge. Firstly, the average slope coefficient a is positive, albeit small (.06 for layer 0 and .04 for layer 1 with GPT-2XL embeddings), and relatively stable across the four embedding systems. Secondly, the percentage of neurons with such a positive slope coefficient is extremely high (80% for layer 0 and 85% for layer 1 with GPT-2XL embeddings), and this is fully consistent across the other three embedding systems; this trend is highly significant at the inferential level based on a $\\chi^2$ goodness-of-fit test using a theoretical dichotomous equidistribution $p(\\chi^2) < 0.05$ for both layers). Given our caution regarding applicability, this last result could be compatible with our hypothesis, even though, again, the choice of a linear approach does not seem well-suited here. For illustration, graph 5 provides an example of a neuronal linear regression, showing the slight positive slope connecting the two variables in line with our hypothesis (see the annexes for an illustration using representative neurons from layers 0 and 1).\nWe complement our initial methodological approach above with a second, non-parametric method (thus removing the conditions of normality) that may be more suitable since it is ordinal, using Spearman's p. We find the same type of results as before , which relate to the parameters of the ordinal relationship of cosine similarity for pairs of successive core tokens based on the activation of the first token in each pair. A relatively low percentage of cases where the effect of ordinal correlation is significant (33% for layer 0 and 40% for layer 1) again shows an insufficient relevance of this new modeling"}, {"title": null, "content": "method chosen to account for our hypothesized effect. However, the average p values are higher than their previous linear slope coefficient counterparts (.10 for layer 0 and .13 for layer 1, with GPT-2XL embeddings, stable values for other embeddings), indicating a slightly better fit of our ordinal modeling and still showing a positive relationship between our two variables. Again, a significant percentage of neurons is associated with a positive ordinal correlation coefficient (75% for layer 0 and 81% for layer 1), a result that is again significant ($p(\\chi^2) < 0.05$ for both layers)."}, {"title": "Categorical Convergence and Contrasting Activation Averages", "content": "A third and final relevant operationalization to test our hypothesis of categorical convergence for successive core tokens as a function of activation involves a contrast-based approach, comparing the categorical proximity of pairs of successive core tokens between groups of tokens that are extremized in terms of their activation level. According to our hypothesis, for each neuron, the average cosine similarity of pairs of successive core tokens should be higher for pairs"}, {"title": null, "content": "with high activations compared to those with low activations.\nWe use a non-parametric approach, the Wilcoxon-Mann-Whitney test, given the relative normality of our data and the small sample sizes of the groups we will use here. contain the main results concerning the mean differences in cosine similarities for the 21 pairs of successive core tokens with the lowest/highest activation ranks of the first token in each pair. We can observe a systematic global mean difference for the four embeddings, although the differences obtained with GPT-2XL embeddings are more pronounced due to this embedding model being more differentiating ($cosmin=.364 / COSmax=.417$ on average for layer 0; $cosmin=.393 / COSmax=.459$ for layer 1). However, there is a significance of this difference (p(MW) < .05) for only 30% of the neurons in layer 0 (associated with a mean effect size that is effectively negative and non-negligible at -.17) and 37% in layer 1 (associated with an effect size that is effectively negative and non-negligible at -.21); also, a stronger contrast for layer 1 compared to layer 0 raises the question of a potential intensification of this divide as layers deepen. Even if this contrast is not significant, it is worth noting that, in a large majority of cases for the four embeddings, we observe a higher average cosine similarity in groups of core tokens with high activations compared to low activations (76% of neurons in layer 0 and 82% in layer 1). In all cases (both layers and four embeddings), there is a significant effect of these percentages based on a $\\chi^2$ goodness-of-fit test using a theoretical dichotomous equidistribution $p(\\chi^2) < 0.05$. These results are again compatible with our hypothesis of categorical convergence for pairs of successive core tokens as activation levels increase. It should be noted that a much stronger contrast would likely have been found neuron by neuron if we had used groups of tokens that were more extremized in terms of their activation, for example, by taking as a class of low activation tokens those that are not core tokens, that is, the 100 tokens most activated on average per neuron."}, {"title": "Discussion", "content": null}, {"title": "Categorical Convergence for High Activations as Indicative of Co-Activation of Sub-Dimensional Categories within Synthetic Categories", "content": "We explored, regarding successive core tokens, the possible existence of a changing relationship between cosine proximity and activation. Our results in this area suggest a notion of categorical convergence for pairs of successive core tokens based on activation; in other words, as the activation levels of successive core tokens (i.e., those close in activation level) increase, their categorical variability decreases (i.e., categorical proximity increases). How should we interpret this result? In what follows, we will utilize various partial explanatory frameworks before attempting to coordinate them to answer this question.\nIn their work on the relationship between categorization and similarity in human cognitive psychology, Roads et al. (2024) emphasize geometric mental representations, meaning those for which an object's representation (to be categorized or compared for similarity with another) is expressed through multidimensional coordinates in a multi-axis space. This mode of representation aligns with the sub-dimensional categories related to a given neuron that we will discuss below. In this type of representational approach, Thibault (1997) states that studies linking categorization and similarity operate on the principle that an object is assimilated into a class by estimating its proximity to what represents that class; this is based on (i) a space of dimensions considered relevant for comparison and (ii) a method for calculating the distance between compared elements.\nAs mentioned earlier, those advocating a separation between categorization and similarity (Murphy and Medin, 1985; Rips, 1989; Barsalou, 1991; Medin et al., 1993; Love, 2002; Kalyan et al., 2012; Reppa et al., 2013; Poth, 2023) argue against the subjectivity, instability, and versatility of the criteria chosen in the moment to base a similarity judgment, criteria that represent only a singular and subjective choice among other possible dimensions. These objections highlight that reasoning by similarity is ambiguous, i.e., not sufficiently constrained (Goodman, 1972; Wixted, 2018).\nHowever, as discussed, Goldstone (1994) counter-argues that similarity is not always unstable, and Thibault (1997) denounces a psychological essentialism in these critics, reproaching them that their argument against similarity's validity assumes that the criteria for categorical segmentation should have an intrinsic and stable value, based on the idea of copying an ontologically defined reality. Furthermore, Hampton (1997), taking a stance from fuzzy"}]}