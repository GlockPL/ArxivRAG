{"title": "VulCatch: Enhancing Binary Vulnerability Detection through CodeT5 Decompilation and KAN Advanced Feature Extraction", "authors": ["Abdulrahman Hamman Adama Chukkol", "Luo Senlin", "Kashif Sharif", "Haruna Yunusa", "Muhammad Muhammad Abdullahi"], "abstract": "Binary programs vulnerability detection is a major problem in software security. Existing deep learning systems mostly rely on source code analysis for training and testing, these approaches depend on manually defined patterns, limiting their capability to detect unknown vulnerabilities. Also, the differences between models understanding of source code and compilers interpreting it into executable binaries leads to missed multiple types of vulnerabilities. Furthermore, the presence of redundant instruction set in source code complicates the training procedure and reduces the effectiveness of traditional models. To solve these problems, we proposed VulCatch, a binary-level vulnerability detection framework. VulCatch leverages a novel Synergy Decompilation Module (SDM) and Kolmogorov-Arnold Networks (KAN). By combining the power of CodeT5, it transforms raw binary code into pseudocode, thus preserving high-level semantic features. This pseudocode serves as the basis for traditional decompilers such as Ghidra and IDA to perform deep disassembly and structural analysis, while KAN enhances feature transformations and captures complex functional relationship within the code, helping in deeper analysis and recognition of underrated vulnerability patterns. This allows for effective analysis and avoides dependence on source code and addressing unknown vulnerabilities via program slicing. Code segments are transformed into vector representations by word2vec, followed by normalization. Enhancing the system more, VulCatch combines sophisticated architectural components like Inception Blocks for multi-level feature extraction from vectors, considering hierarchical and contextual code segments information, and detect multiple vulnerability types in real time using BiLSTM Attention and Residual connections on seven CVE datasets. VulCatch significantly increases detection accuracy (98.88%) and precision (97.92%) while decreasing false positives (1.56%) and false negatives (2.71%) compared to benchmarked systems.", "sections": [{"title": "I. INTRODUCTION", "content": "SOFTWARE vulnerability detection is crucial for enhancing cybersecurity as the frequency and severity of security incidents continue to escalate. A major challenge is the detection of clone vulnerabilities, which propagate across the software supply chain, often exacerbated by the unavailability of source code for binary programs. Traditional techniques like taint analysis, symbolic execution, fuzzing [1], and static analysis are commonly employed but face limitations in detecting unknown vulnerabilities and rely heavily on code similarity thresholds, leading to high false positive rates.\nOnly known vulnerabilities (cloning vulnerabilities) can be detected using code similarity-based approaches [2], it is difficult to recognize unknown vulnerabilities (recurring vulnerability), fresh vulnerabilities containing the same codes properties as the known vulnerabilities. Additionally, thresholds must be defined in order for code similarity-based detection approaches to detect whether or not the targeted functions are identical. For instance, \"diff\" [3], put out the top five functions that are similar to known vulnerability; \"vulSeeker [2], and \"SAFE\" [4], also use a threshold to find similar vulnerabilities. However, setting a fixed similarity threshold can result in a high rate of false positives because functions identified as similar to known vulnerabilities may only partially match, or share superficial similarities, leading to non-vulnerable functions being mistakenly flagged, which will ultimately take a lot of time to manually examine the matching results.\nVulnerability detection techniques like, [5], based on vulnerability patterns may produce a high rate of false-positive as well as false-negative since the vulnerability features extraction is either too general or manually obtain by experts. Furthermore, extracting the features takes a lot of work. Deep learning has recently also been used to automatically extract patterns for detection of vulnerabilities. For instance, in order to extract vulnerability features more, [6] and [7], acquired the semantic information of each instruction. A hierarchical attention network is suggested by HAN-BSVD [8], to extract vulnerability features from assembly instructions. Vulnerability patterns [5], [6], [9], [10], [8], are derived from binary program assembly code or intermediate representations (IR), which are subject to the effect of compilation variability. As a result, the code representation suffers significant modifications, including reallocating registers, rearranging instructions, substituting instructions, changing the code structure, and more. These code alterations will also have an impact on how vulnerability patterns are extracted. The majority of vulnerability detection techniques [2], [3], [4], are also coarse-grained since the whole function is used to extract features. The usefulness of the extracted feature is usually impacted by other unrelated function instructions, although vulnerabilities are usually only linked to a limited number of instructions. Because compiler diversity results in considerable code changes whereas patch code only differs slightly from vulnerable code, it is particularly vulnerable to compilation diversity and patch code.\nCodeT5 is an encoder-decoder model based on the T5 architecture, pre-trained on a vast code corpus to effectively interpret and generate code through tasks like summarization and translation [11]. Its ability to understand code semantics makes it valuable for code analysis and decompilation. LLM4Decompile [12] explores using large language models (LLMs) for decompiling binary code, aiming to benchmark re- compilability and re-executability. Similarly, LmPa [13] combines an LLM with program analysis to iteratively recover variable names from binary executables, enhancing readability. Challenges in decompilation, such as recovering high-level constructs from low-level assembly due to information loss during compilation, are discussed, highlighting the promise and difficulty of achieving fully recompilable decompilation with LLMs [14]. CodeT5 [15], designed for both code understanding and generation, is suitable for translating between different levels of code abstraction. It introduces identifier-aware pre-training to handle identifiers crucial for code semantics, beneficial for decompilation requiring semantic understanding. Its multi-task learning capability and broad evaluation on code-related tasks suggest its potential for integration with traditional decompilers like IDA and Ghidra. Additionally, automating the conversion of agile user stories into pseudocode is surveyed by [16], presenting a two-stage process using the CodeT5 model. This approach, fine-tuned for success measures using BLEU scores, aims to streamline software development in Agile environments by reducing the time spent converting user requirements into pseudocode, outperforming rule-based methods.\nKolmogorov-Arnold Networks (KAN) are based on the Kolmogorov-Arnold theorem, established by Vladimir Arnold and Andrey Kolmogorov in 1957. This theorem demonstrates that functions of several variables can be expressed as a superposition of functions of a single variable. KANS incorporate these univariate functions into their structure, providing an elegant method for approximating complex multivariate functions. Study [17] shows that KANS outperform traditional MLPs in capturing complexities in large datasets, offering better interpretability and accuracy. This makes KANs particularly suitable for tasks like vulnerability detection, where high-dimensional data and subtle nonlinear patterns are involved, overcoming the limitations of traditional neural networks due to the curse of dimensionality.KAN can effectively break high-dimensional linkage through the simpler one-dimensionality, linking to complex and more fine-grained data types among multiple spaces of knowledge representation at runtime point for dataset interpretability enhancement; boost detection power in a broader range with fewer perturbations of discoverable target functions. f is the multi-variate function to be signified. I\u03c6 and \u03a6qp are uni-variate functions. X1, X2, ..., Xn are the variables of function f.\n$f(x_1, x_2,...,x_n) = \\sum_{q=1}^{2n+1}  \u03a6_q ( \\sum_{p=1}^{n} \u00d8_{q,p}(x_p) )$  (1)\nThis formula shows that function f of n variables can be decomposed into a superposition of 2n + 1 functions \u03a6q, each of them depends on a particular sum of functions \u00d8q,p, letting each \u00d8q.p are functions of a distinct variable xp. Here,\u0424q represents the outer functions, and \u00d8q,p represents the inner functions, each applied to one of the variables. The inner functions transform the individual variables, and their results are then summed up as inputs to the outer functions. In (section 3), VulCatch's combined dataset C analogous to X1, X2, ..., Xn is processed by the KAN function FRAN. This function embodies the KAN formula where each component of the dataset is treated as an input variable xp to the functions\u00d8q,p. These are then aggregated (summed) and passed through the functions Oq to generate enhanced features Kfeatures.\nDecompilation technology has advanced rapidly [18], with tools like IDA Pro [19] and Ghidra [20]. Many binary code analysis fields require decompilation techniques [21], often using intermediate representations (IR) as in Inceptions and FirmUp [21]. Decompiled pseudo-code is used in cybersecurity; for example, UPPC (Unleashing the Power of Pseudo-code) [22] uses pseudo-code for vulnerability detection due to its higher abstraction and platform independence. Other studies use pseudo-code to counter compilation diversity [23] and detect malware via decompiled Android API calls. SCRUTINIZER [24] constructs pseudo- code from memory dumps to detect malicious code repetition. Decompiled pseudo-code offers high-level semantic information and control structure recreation, making it more efficient for binary security analysis compared to analyzing binary code or IR.\nOur primary research goal is to develop a system capable of detecting multiple types of both known and unknown vulnerabilities in binary code more accurately and efficiently than current methods. The specific research questions we address are:\nVulCatch begins by decompiling binary code into pseudocode using the CodeT5 model, making it more understandable for further analysis. Disassembly analysis with IDA and Custom Analysis, along with structural analysis using Ghidra and Control Path, provides detailed insights into the code's structure and behavior. The data from these analyses, combined with metadata, is enhanced using Kolmogorov-Arnold Networks, improving the feature set. Abstract Syntax Trees (ASTs) and Program Dependence Graphs (PDGs) are generated from these features, forming structured representations of the code. These are prepared as inputs for a BiLSTM-Residual-Attention Neural Network, with residual connections aiding in training deeper networks. Bidirectional passes in the BiLSTM capture context from both past and future states, while inception block processing and an attention mechanism focus on important parts of the input sequence. Residual connections enhance gradient flow, and the deep learning model classifies vulnerabilities from the processed input data. The final output translates these classifications into actionable information, identifying vulnerabilities with their categories and severity levels, providing valuable insights for remediation."}, {"title": "II. RELATED WORKS", "content": "In the evolving field of binary vulnerability detection, two main methodologies are often applied: code similarity-based approach and vulnerability pattern extraction approach. Code Similarity-Based Techniques: These methods compare target codes to databases of known vulnerabilities to measure their similarities. The target code is considered vulnerable when a predetermined threshold is exceeds. Nevertheless, the threshold can cause high false-positive rates due to the superficial similarities. Vulnerability Pattern Extraction: This method identifies property signatures or patterns to detect known vulnerabilities in target code. By comparison against predefined patterns derived from previous vulnerabilities, it then flags codes with identical features. However, it is limited to known patterns and might not understand new vulnerabilities. Limitations and Challenges: Both methods struggle with false positives and variability, mainly due to compiler differences and irrelevant code throughout the functions. Current research aims to solve these problems through advanced decompilation and deep learning for accurate and efficient vulnerability detection."}, {"title": "A. Approaches based on Vulnerability patterns", "content": "Similarity based techniques are used to find recognized vulnerability (replica vulnerability). Using same approach, they can barely discover unknown vulnerability. The approaches derived from vulnerability patterns obtain the appropriate features of the current vulnerable program, using such patterns to locate the target code. IntScope, decompiles the binaries into an intermediary representation and then obtains important properties via symbol and taint approaches for vulnerability detection. With the use of software slicing and symbol execution approaches, Firmalice [31], extracts properties to suggests unique models to find concealed verification avoidance backdoors for focused embedded systems. Value set analysis is used by GUEB, to track the usage and releases of memories throughout programs in order to find binary vulnerability in UA. Pattern matching is used by cwe checker [25], to find unknown vulnerabilities using CWE vulnerability patterns provided by human analysts. These efforts, nevertheless, have large false-positive rates since the patterns and instructions are very broad and some properties are physically obtained by humans. The results must also be carefully verified. Deep learning techniques are used by VulCatch to automatically extract vulnerability models.\nSeveral researches then apply deep learning to automatically detect vulnerabilities patterns to address the existing challenges. Instruction2Vec [6], creates a vector representation of the instruction through dividing instruction as opcode and operands (having two features), each property representing a vector. Trained Text-CNN applies feature extraction of weak code using the learnt vector representation. According to [7], proposal, every instruction should be presented in a meaningful combination given by its vector element, with a weighted summation instruction applied according to the element's hierarchy level. The BiLSTM system is then trained to recognize patterns of vulnerabilities. According to the binary program machine instruction structure, DCKM [26], combines Cost Sensitive Kernel Mechanism and BRNN in finding vulnerabilities in binaries of various feature spaces. HAN-BSVD [8], builds a network for extracting features to catch spatial information that indicate critical locations for collecting important vulnerability features by first training their instruction embedding models consisted of BiGRU and word-attention modules. Zheng, suggested converting binary codes into LLVR IR and collecting instructions relating to library functions using data flow analysis. Eventually, those instructions are used to train various RNN networks for vulnerability findings. VulDeePecker [27], a model for detecting vulnerabilities in source code, is adopted by BVDetector [9], but it collects code slices associated with API and library function at assembly level. Such approaches were centered on low-level code properties, making granular analysis become finer. VulCatch extracts vulnerability patterns using a finer-grained method. Patch-code is used in other techniques for feature extraction. For instance, VIVA [10], efficiently detects known and unknown vulnerabilities by using binary program slicing methods along with decompilation to provide vulnerability and patch patterns. Such approach, meanwhile, is costly and subject to compilation complications. VulCatch simply extract features relating to the detection of vulnerabilities."}, {"title": "B. Approaches based on Code similarity", "content": "Code similarities have been extensively used in malware classification [28], plagiarism detection [29], and security patch assessment [30], as well as vulnerability findings [34]. In order to identify binary vulnerability that are same as existing vulnerabile code, TEDEM, introduced tree distance control quantifying similarities in codes at the level of basic blocks. In order to pre-filter the target function, DiscovRE, collects numerical information from the function (such as the quantity of mathematical and analytical process). Similarities between the structure's features are then analyzed in order to identify vulnerabilities. Dependent formulae are semantic properties extracted at high-level to detect vulnerable codes XMATCH [32], from the original binary code. Such vulnerability detection techniques, though, frequently generate expensive similarity calculations and have high false positive and false negative rates. Machine learning methods have been applied to these issues to compare code similarity and find vulnerabilities. Genius [33] creates attributable CFGs and determines similarities using graph embeddings produced by comparison with a collection of sample graphs known as codebook. Gemini is a GNN-based model that was proposed by [34], that uses structure2vec, in combination with Siamese model, to produce the function graphs embedding and determine match. In order to enhance the detection accuracy, VulSeeker [35], builds the labelled semantics flow graphs (LSFG) using data control in functions with embedding LSFG in the space vectors via DNN semantical awareness framework.\nHowever, such techniques depict fundamental building blocks or functions using certain manually chosen statistical features that might not have adequate semantic information. adiff [36], extracts three categories of semantic information for similarity detection from introduced functions, raw bits functions, and function calls. In order to identify potential vulnerability functions, FIT [37], first extracts semantic information via neural network framework for binary programs. Moreover, it further analyzes the similarity of two features based on third-level features. With the help of assembler instructions, SAFE [4], collects semantic information. Next, using a transformer, it creates embeddings of functions. In order to identify vulnerabilities, it performs similarity calculations on the embedded representations that were developed.\nSeveral studies propose the introduction of patch codes to find vulnerabilities in order to lower the rates of false positive more. In order to create a trace set, BINXRAY [38], first identifies the fundamental building elements that are different between the patch and vulnerable functions. In order to determine patch in the functions, such checks help in computing similarities among patch, target and vulnerable functions accordingly. PDiff [39] compared the kernel's target before the adoption and after the adoption of patch with its normal versions and produces a description with semantics linked to the targeted patch. Lastly, every target program's patch status is determined using the similarity."}, {"title": "III. DESIGN OF VULCATCH", "content": "Binary code vulnerability detection is a complicated process that require complex mechanisms and methods. Traditional models mainly have problems because they are unable to understand the complex structures and semantics information embedded in binary codes. The VulCatch network solves this problem by innovating a combination of advanced decompilation procedures and deep learning framework to improve the detection of both known and unknown vulnerabilities.\nThe different components are explained below:\nDecompilation with CodeT5: The VulCatch system starts the vulnerability detection process with CodeT5, a model trained to decompile binary code into pseudocode. This step is needed because it transforms raw binary into a more interpretable format and preserves high-level semantics properties. The pseudocode generated is highly rich in semantic information, critical for the understandings of complex software behaviors that might hide vulnerabilities Fig. 1 Novel SDM framework.\nTraditional Decompilers Integration: Following the decompilation stage, traditional tools Ghidra and IDA are applied for detailed disassembly and structural analysis. The tools complement CodeT5 by maintaining a granular breakdown of the binary structures, which is essential for the comprehensive examination of vulnerabilities Fig. 1.\nKAN is placed before the feature extraction phase and after the initial data aggregation phase. This position allows KAN to process the raw combined data from disassembly, improving representation of the data before it is used to generate more structural Abstract Syntax Trees (ASTs) and Program Dependency Graphs (PDGs).\nAbstract Syntax Trees (ASTs) and Program Dependency Graphs (PDGs): The model then generates ASTs and PDGs from the combined data of CodeT5's pseudocode and traditional decompiler output. ASTs benefit in understanding the hierarchical structure of the codes, while PDGs concentrate on the interactions and dependencies among various elements. These structures are helpful in pinpointing critical parts of the code that are more likely to have vulnerabilities.\nDeep Learning Techniques for Enhanced Detection: To analyze and process data from ASTs and PDGs, VulCatch uses a BiLSTM network integrating attention network with residual connectivity in Fig. 2 VulCatch system design. This system is adept at detecting complex dependencies and patterns that are indicative of vulnerabilities. The attention mechanism precisely allows the model to focus on portions of the code that have greater probabilities of having vulnerabilities, while residual connection enhances flow of gradient, helping effective learning and improved generalization, thus enhancing the efficiency and accuracy of the detection process.\nInception blocks are integrated into the deep learning structure to improve feature extraction by letting the network capture multi-scale patterns in the data. These blocks contain multiple convolutional layers with different filter sizes that operate in parallel. The inception blocks assist in detecting vulnerabilities at different levels of granularity, thus increasing the robustness of the detection system.\nThe VulCatch framework is built to improve binary vulnerability detection by integrating advanced tools and methods. The method is organized around a sequence of procedures that leverages the exclusive strengths of CodeT5, traditional decompilers IDA and Ghidra, KAN, and current deep learning methods.\nAlgorithm 1: The Synergy Decompilation Module, starts by transformation of BinaryCode into pseudocode via DECOMPILE_WITH_CODET5, to make sure the semantic integrity of the original code is well-kept. The pseudocode is disassembled using IDA to enhance its original structure, and any structural or logical issues are found by additional analysis using Ghidra. Ghidra and IDA's outputs are combined to construct a single, comprehensive dataset. Kolmogorov- Arnold Networks (KAN) now processes this dataset further, by the use of a nonlinear mapping to transform features and refine them, which is important for possible vulnerability detection. After this process, the enhanced datasets are then used to generate ASTs and PDGs. These structures aids to visualize the program's hierarchical and dependent aspects essential for detection and categorization of vulnerabilities according to their nature and severity. A list of categorized vulnerabilities is produced by this process, together with information about vulnerable binary codes and a systematic assessment."}, {"title": "Algorithm 1 Synergy Decompilation Module & KAN", "content": "Input: BinaryCode\nOutput: ClassifiedVulnerabilities\n1: procedure VULCATCH (BinaryCode)\n2: PseudoCode DECOMPILE_WITH_CODET5 (BinaryCode)\n3: DisassemblyReport IDA_DISASSEMBLE (PseudoCode)\n4: StructuralAnalysis GHIDRA ANALYZE (PseudoCode)\n5: CombinedData COMBINE (DisassemblyReport StructuralAnalysis)\n6: EnhancedFeatures APPLY_KAN (CombinedData) // Data features enhance with KAN\n7: ASTS GENERATE_ASTS (EnhancedFeatures) // ASTs generation from enhanced features\n8: PDGS GENERATE_PDGS (EnhancedFeatures) // PDGs similarly generate from enhanced features\n9: Vulnerabilities DETECT_VULNERABILITIES (ASTS, PDGs)\n10: ClassifiedVulnerabilities VULNERABILITIES_CLASSIFICATION (Vulnerabilities)\n11: return ClassifiedVulnerabilities\n12: end procedure\npseudocode. PseudoCode serves as the input to these functions, which was obtained from the decompilation step. DIDA and DCA are the outputs of the IDA and CA analysis.\n$PseudoCode = F_{C5}(BinaryCode)$  (2)\nThe purposes of CodeT5 model used to decompile binary code into pseudocode, are represented by Fc5. BinaryCode is the input decompiled into PseudoCode, which becomes the pseudocode output of the CodeT5 model.\n$D_{IDA} = F_{IDA}(PseudoCode)$  (3)\n$D_{CA} = F_{CA}(PseudoCode)$  (4)\n(IDA & CA) Disassembly Analysis FIDA and FCA functions correspondingly signify the processes of Custom Analysis (CA) and the Interactive Disassembler (IDA) to examine the\n$S_G = F_G(PseudoCode)$  (5)\n$S_{CP} = F_{CP}(PseudoCode)$  (6)\nStructural Analysis (Ghidra & CP) takes FG and FCP as the functions representing the Ghidra analysis and Control Path analysis (CP), respectively. SG and Scp are the structural analysis outputs from Ghidra and CP.\n$C = F_{combine}(D_{IDA}, D_{CA}, S_G, S_{CP}, M)$  (7)\nData Combination Fcombine is the function that integrates data from various analysis tools along with metadata. M is the"}, {"title": "B. BiLSTM-Residual-Attention Neural Network", "content": "This work deploys Recursive Neural Networks (RNNs), which have demonstrated superior performance in natural language processing [40], in conjunction with the sophisticated bidirectional gating mechanisms of BiLSTM [41]. This integration is tailored to capture the hierarchical relationships in data, essential for producing accurate vulnerability patterns. Our novel framework, VulCatch, incorporates a Bidirectional Hierarchical Network. This development is aimed at extracting the representations from the hierarchical nature of data. The representations produced are then used as inputs to the BiLSTM layer. Here, the BiLSTM model constructs a representation vector from the AST, effectively compressing its semantic information for vulnerability detection in Fig. 3 VulCatch Network.\n$X_{input} = (ASTs, PDGs) + Residual\\ Connections$  (11)\nASTS, PDG are prepared for input into the deep learning model, specifically the ASTs and PDGs generated from the enhanced features Kfeatures. Residual Connections allows for direct connections between earlier layers and later layers, adding the residual connections to the ASTs and PDGs better propagates gradients through the deep network during training, thus alleviating the vanishing gradient problem and improving the learning of deeper layers.\n$x_t = Extracted\\ feature\\ at\\ time\\ t\\ from\\ X_{input}$  (12)\nxt is the specific feature vector extracted at time t from Xinput ensuring that the BiLSTM network processes the correct sequence of enhanced data.\n$\\overrightarrow{u_t} = \\sigma (W_{u,f} \\cdot [x_t, h_{t-1}, C_{t-1,}] + U_{u,f}h_{t-1} + V_{u,f}\\frac{\\overrightarrow{u_t}}{(C_{t-1} + b_{u,f})}$  (13)\nForward pass is set as \u03c3 being the sigmoid activation function that maps the calculated values to a range between 0 and 1. Wu,f, Uu,f, Vu,f are weight matrices specific to the update gate for the forward pass of the LSTM. buf is the bias term for the update gate in the forward pass. xt is the input at time step t. ht\u2081is the previous hidden state from the forward direction. Ct-is the previous cell state from the forward direction.\n$\\overleftarrow{u_t} = \\sigma (W_{u,b}[x_t, h_{t+1}, C_{t+1,}] + U_{u,b} h_{t+1} + V_{u,b} (C_{t+1} + b_{u,b})$  (14)\nBackward pass is set as ht+1 and Ct+1 being the subsequent hidden and cell states from the backward direction. Wub Uu,b, Vu,b And bub are the weight matrices and bias for the update gate in the backward direction. This pass processes the sequence from end to start, providing a perspective on the data that complements the forward pass.\n$h_{inception} = I_k(h_t) = \\sum_f Conv_k(h_{t, f})$  (15)\nInception Blocks takes Ik as the inception block processing hidden states ht using convolutional filters of size k. Convk is the convolution operation with kernel size k. f serves as the index of the convolution filter.\nFurthermore, VulCatch is enhanced with Residual connections and Attention Mechanism, which includes Attention Units that are divided into a mask and a trunk division. The trunk division is tasked with feature processing and is designed to be compatible with current network structures, such as pre-activation Residual Units [42], ResNeXt [43], and Inception models [44]. The mask division learns a same-size mask M(x) that softly weights the output features T(x) provided by the trunk division. This design is inspired by the fast feed forward and feedback attention processes, structuring attention in a top-down, bottom-up fashion [45], which in turn heads the trunk division's neurons similarly to a Highway Network with the output mask [46]. The Attention weights looks like this:\n$e_{tj} = v^\u00b9 \u00b7 tanh (W_a\u00b7h_t + U_a \u00b7 h_j + b_a)$  (16)\nAttention Mechanism's v, Wa, Ua are the parameters of the attention mechanism that weight the importance of different parts of the input sequence. ht,h; are the current and other hidden states on which attention is being computed. ba is the bias term for the attention mechanism. Tanh is the hyperbolic tangent activation function providing non-linearity.\n$h_{residuals} = I(h_t) + C_t$  (17)\nResidual Connection's I(h) is the output from the inception block. ct is the context vector from the attention mechanism. hresiduals is the combined output that adds the input ht to its transformation, promoting gradient flow and reducing the risk of vanishing gradients.\n$V_{output}$ = \\frac{F_{DL}(X_{input}) Classified\\ Vulnerabilities}{F_{classify}(V_{output})}  (18)\nVoutput = FDL(Xinput) outputs the deep learning model FDL, which processes the input data Xinput Classified Vulnerabilities Fclassify(Voutput) categorizes the vulnerabilities detected by the deep learning model into specific classes based on severity, type, or other relevant criteria. It essentially translates the raw model outputs into actionable information, facilitating prioritization and remediation efforts.\nThe final output of the model is formulated as:\n$O_{final} = W_{out} \u00b7 h_{residuals} + b_{out}$  (19)\nFinal Output Wout is the weight matrix for the final layer, translating the high-level features captured by the network into meaningful classifications. hresiduals is the final feature representation derived from the network's residual and attention mechanisms. bout is the bias term to adjust the classification layer's final output. O final is the final prediction or classification output of the model, representing the detected vulnerabilities with their assigned categories or severity levels.\nVulCatch represents an incremental architecture that captures intricate details within the code's structure, emphasizing the critical segments for vulnerability detection through an advanced attention mechanism."}, {"title": "IV. IMPLEMENTATION AND EXPERIMENTS", "content": "The logic is to assemble library/API function of C/C++ relevant to vulnerabilities for obtaining library/API function calls program-slices. We gathered the Checkmarx 2021 [50], C/C++ vulnerability criteria. Checkmarx is a cutting- edge static source code analysis tool that can technically and logically locate, recognize, and monitor vulnerabilities in the majority of mainstream code. Checkmarx would still skip certain targets despite having strong coverage for SARD's vulnerability programs [51]. To create PDGs and AST of functions in all programs for program-slice, we study the C/C++ open-source code analysis algorithm Joern.\nDecompilation software's are widely accessible nowadays, like Ghidra [20] and IDA Pro [19]. Because of the difference in quality of the pseudo codes generated by the decompilers, not all of the decompiled pseudo codes are usable. That is why we came up with the Synergy Decompilation Module where the CodeT5 model translates binary code into a high-level pseudocode, maintaining the abstract semantics. This pseudo code then acts as a foundation for traditional decompilers IDA and Ghidra, which perform in-depth disassembly and structural analysis. By providing these decompilers with semantically rich pseudo code, CodeT5 enhances their capabilities for more accurate disassembly and structural analysis. We compare the decompilers Ghidra [20], IDA Pro [19], and our SDM with the same binary programs. The decompiled pseudo codes produced by the SDM provide certain benefits for retrieving high-level semantics and are also easily readable. According to our analysis, SDM performs roughly 2% better than IDA Pro and 4% better than Ghidra when it comes to retrieving function boundaries and instructions recovery [52]. Furthermore, Ghidra has 5.10 times more decompilation problems than IDA Pro, and IDA Pro has 2.55 more decompilation problems than the SDM according to [53], who reprocessed the well-established compilers test methods known as EMI testing [54]. We used CodeT5, IDA Pro and Ghidra as the decompilation synergy to decompile the codes with KAN enhancements, the data values are shown in Table 2 datasets properties."}, {"title": "B. Construction of dataset", "content": "Enhanced CVE-Based Binary Collection: Our dataset has been meticulously curated to include binaries associated with a selected array of known CVE vulnerabilities [47], presenting a diverse spectrum of security challenges, ranging from server-side application vulnerabilities to complex microprocessor architectural weaknesses, ensuring a comprehensive analysis. This collection now exclusively comprises:\n\u2022 CVE-2018-1002105, detailing privilege escalation vulnerability within Kubernetes API servers.\n\u2022 CVE-2019-0708, also known as BlueKeep, which is a remote code execution vulnerability found in Windows Remote Desktop Services.\n\u2022 CVE-2017-5715 and CVE-2017-5754, recognized respectively as Spectre and Meltdown, both of which expose speculative execution vulnerabilities that potentially allow unauthorized information disclosure.\n\u2022 CVE-2018-8174, a vulnerability that enables remote code execution through the Windows VBScript engine.\n\u2022 CVE-2020-0601, identified as CurveBall or ChainOfFools, a spoofing vulnerability within the Windows CryptoAPI.\n\u2022 CVE-2020-1472, dubbed Zerologon, an elevation of privilege vulnerability in the Microsoft Netlogon service.\nSource Code Acquisition and Enhanced Binary Compilation with Clang: We procure source code with established vulnerabilities from the NIST SARD, utilizing the Juliet Test Suite [48] version 1.3 for C and C++. To prepare our binary analysis, we leverage Clang; part of the LLVM compiler suite, renowned for its potent optimization and comprehensive support for modern C and C++ standards. The programs are compiled into ELF 64-bit binaries, initially with Clang's -00 optimization level to maintain straightforward correspondence with the source code. For a thorough exploration of how optimization may affect vulnerability detection, we proceed to compile the source code using Clang's -01, -Os, and -Ofast optimization levels. These distinct optimization tiers are judiciously chosen to challenge the robustness of our vulnerability detection model. The aim is to scrutinize its performance across a spectrum of compiled binaries, from the least to the most optimized, to validate and enhance its effectiveness in real-world scenarios.\nEnhanced Vulnerability Labeling: Leveraging the Juliet Test Suite, we guarantee that each test case is annotated with a CWE identifier [49], which precisely indicates the type of vulnerability present. For the purposes of our study, we select a comprehensive set of 101 distinct CWE classes, reflecting a diverse and extensive range of security vulnerabilities. This selection allows us to compile binaries that embody a wide spectrum of real-world vulnerabilities, facilitated through both library and API calls. The CWE classes ensure that our dataset not only covers a broad cross-section of common vulnerabilities but also provides the granularity needed for the SDM to learn and detect nuanced patterns within the vulnerability landscape effectively. Choosing this much CWE classes offers a substantial variety of vulnerability types for our model to handle, which is likely to strengthen its detection capabilities across a range of scenarios.\nIntegrative Analysis via SDM: The SDM seamlessly bridges the gap between binary-level vulnerabilities and source-level semantics. By employing CodeT5, the SDM decompiles binaries-encompassing a diverse array of CVE vulnerabilities, compiles at varying optimization levels from the SARD dataset to accurate pseudocode. This intermediate representation preserves the critical semantic details altered by optimizations ranging from the CVE identifiers and CWE optimization levels, a necessary step for robust vulnerability detection. The pseudocode undergoes further scrutiny with IDA and Ghidra, which elaborate the disassembly into comprehensive ASTs and PDGs.\nTraining and Testing Set Formation: From the compiled binaries, we allocate 80% for training purposes and reserve 20% for testing. This selection is randomized to ensure representative distribution across the dataset.\nIn our analysis, code gadgets are meticulously labeled for vulnerability detection: \"0\" for non-vulnerable functions and \"1\" for vulnerable functions, identified by harmful library or API calls. We have curated distinct datasets to precisely evaluate the performance of the SDM across various conditions:"}, {"title": "C. Neural Network Training", "content": "We used TensorFlow and Keras to construct the model underpinned by SDM. SDM employs the combined strengths of IDA Pro 7.5 and the Hexray Decompiler [56], facilitated the extraction of high-quality ASTs from decompiled binaries. This robust feature extraction is critical for training our deep learning models.\nOur neural network is initialized with zero vectors for the leaves of the AST, aligning with the focus on precision from the ground up. Training employs BCELoss for its suitability in binary classification, resonating with the separation of code features into \"vulnerable\" and \"non-vulnerable.\"\nOptimization is carried out using the AdaGrad algorithm, selected for its efficiency with sparse data\u2014 a common characteristic in the decompilation output. A carefully chosen training cycle of 30 epochs ensures a balance between underfitting and overfitting, a key consideration when using SDM-enhanced feature sets.\nWe experimented with various hidden embedding sizes and noted that while larger embeddings captured more complexity, a size of 256 hit an optimal balance for our SDM- augmented data, maximizing both efficiency and model performance.\nIn assessing the architecture, we discovered that a configuration of three BiLSTM layers processed the complexity of SDM-generated features effectively, with a token cutoff at 500 ensuring coverage of more extensive program structures without compromising model consistency. The parameters settings are shown in Table 4.\nThe neural network's parameters were accurately put to accommodate the rich feature set produced by SDM, with the computational resources detailed ensuring the model could process the augmented dataset without troubles.\nThe model operates within a Python 3.6 environment on a local server. The entire process, from dataset construction using buildroot-2020 [57], and GNU compiler collection v13.1 to model training set to capitalize on the benefits SDM introduces."}, {"title": "D. Measures of Evaluation", "content": "In evaluating the performance of our vulnerability detection model, we utilize five standard metrics commonly applied in classification tasks"}]}