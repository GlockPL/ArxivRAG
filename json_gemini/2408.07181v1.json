{"title": "VulCatch: Enhancing Binary Vulnerability Detection\nthrough CodeT5 Decompilation and KAN Advanced\nFeature Extraction", "authors": ["Abdulrahman Hamman Adama Chukkol", "Luo Senlin", "Kashif Sharif", "Haruna Yunusa", "Muhammad Muhammad Abdullahi"], "abstract": "Abstract- Binary programs vulnerability detection is a major\nproblem in software security. Existing deep learning systems\nmostly rely on source code analysis for training and testing, these\napproaches depend on manually defined patterns, limiting their\ncapability to detect unknown vulnerabilities. Also, the differences\nbetween models understanding of source code and compilers\ninterpreting it into executable binaries leads to missed multiple\ntypes of vulnerabilities. Furthermore, the presence of redundant\ninstruction set in source code complicates the training procedure\nand reduces the effectiveness of traditional models. To solve these\nproblems, we proposed VulCatch, a binary-level vulnerability\ndetection framework. VulCatch leverages a novel Synergy\nDecompilation Module (SDM) and Kolmogorov-Arnold\nNetworks (KAN). By combining the power of CodeT5, it\ntransforms raw binary code into pseudocode, thus preserving\nhigh-level semantic features. This pseudocode serves as the basis\nfor traditional decompilers such as Ghidra and IDA to perform\ndeep disassembly and structural analysis, while KAN enhances\nfeature transformations and captures complex functional\nrelationship within the code, helping in deeper analysis and\nrecognition of underrated vulnerability patterns. This allows for\neffective analysis and avoides dependence on source code and\naddressing unknown vulnerabilities via program slicing. Code\nsegments are transformed into vector representations by\nword2vec, followed by normalization. Enhancing the system\nmore, VulCatch combines sophisticated architectural\ncomponents like Inception Blocks for multi-level feature\nextraction from vectors, considering hierarchical and contextual\ncode segments information, and detect multiple vulnerability\ntypes in real time using BiLSTM Attention and Residual\nconnections on seven CVE datasets. VulCatch significantly\nincreases detection accuracy (98.88%) and precision (97.92%)\nwhile decreasing false positives (1.56%) and false negatives\n(2.71%) compared to benchmarked systems.\nIndex Terms- Binary program, decompile, deep learning, deep\nneural network (DNN), KAN, vulnerability detection.", "sections": [{"title": "I. INTRODUCTION", "content": "SOFTWARE vulnerability detection is crucial for\nenhancing cybersecurity as the frequency and severity\nof security incidents continue to escalate. A major\nchallenge is the detection of clone vulnerabilities, which\npropagate across the software supply chain, often exacerbated\nby the unavailability of source code for binary programs.\nTraditional techniques like taint analysis, symbolic execution,\nfuzzing [1], and static analysis are commonly employed but\nface limitations in detecting unknown vulnerabilities and rely\nheavily on code similarity thresholds, leading to high false\npositive rates.\nOnly known vulnerabilities (cloning vulnerabilities) can be\ndetected using code similarity-based approaches [2], it is\ndifficult to recognize unknown vulnerabilities (recurring\nvulnerability), fresh vulnerabilities containing the same codes\nproperties as the known vulnerabilities. Additionally,\nthresholds must be defined in order for code similarity-based\ndetection approaches to detect whether or not the targeted\nfunctions are identical. For instance, \"diff\" [3], put out the top\nfive functions that are similar to known vulnerability;\n\"vulSeeker [2], and \"SAFE\" [4], also use a threshold to\nfind similar vulnerabilities. However, setting a fixed similarity\nthreshold can result in a high rate of false positives because\nfunctions identified as similar to known vulnerabilities may\nonly partially match, or share superficial similarities, leading\nto non-vulnerable functions being mistakenly flagged, which\nwill ultimately take a lot of time to manually examine the\nmatching results.\nVulnerability detection techniques like, [5], based on\nvulnerability patterns may produce a high rate of false-positive\nas well as false-negative since the vulnerability features\nextraction is either too general or manually obtain by experts.\nFurthermore, extracting the features takes a lot of work. Deep\nlearning has recently also been used to automatically extract\npatterns for detection of vulnerabilities. For instance, in order\nto extract vulnerability features more, [6] and [7], acquired the\nsemantic information of each instruction. A hierarchical\nattention network is suggested by HAN-BSVD [8], to extract\nvulnerability features from assembly instructions.\nVulnerability patterns [5], [6], [9], [10], [8], are derived from\nbinary program assembly code or intermediate representations\n(IR), which are subject to the effect of compilation variability.\nAs a result, the code representation suffers significant\nmodifications, including reallocating registers, rearranging\ninstructions, substituting instructions, changing the code\nstructure, and more. These code alterations will also have an\nimpact on how vulnerability patterns are extracted. The\nmajority of vulnerability detection techniques [2], [3], [4], are\nalso coarse-grained since the whole function is used to extract\nfeatures. The usefulness of the extracted feature is usually\nimpacted by other unrelated function instructions, although\nvulnerabilities are usually only linked to a limited number of\ninstructions. Because compiler diversity results in\nconsiderable code changes whereas patch code only differs\nslightly from vulnerable code, it is particularly vulnerable to\ncompilation diversity and patch code.\nCodeT5 is an encoder-decoder model based on the T5\narchitecture, pre-trained on a vast code corpus to effectively\ninterpret and generate code through tasks like summarization\nand translation [11]. Its ability to understand code semantics\nmakes it valuable for code analysis and decompilation.\nLLM4Decompile [12] explores using large language models\n(LLMs) for decompiling binary code, aiming to benchmark re-\ncompilability and re-executability. Similarly, LmPa [13]\ncombines an LLM with program analysis to iteratively recover\nvariable names from binary executables, enhancing\nreadability. Challenges in decompilation, such as recovering\nhigh-level constructs from low-level assembly due to\ninformation loss during compilation, are discussed,\nhighlighting the promise and difficulty of achieving fully\nrecompilable decompilation with LLMs [14]. CodeT5 [15],\ndesigned for both code understanding and generation, is\nsuitable for translating between different levels of code\nabstraction. It introduces identifier-aware pre-training to\nhandle identifiers crucial for code semantics, beneficial for\ndecompilation requiring semantic understanding. Its multi-task\nlearning capability and broad evaluation on code-related tasks\nsuggest its potential for integration with traditional\ndecompilers like IDA and Ghidra. Additionally, automating\nthe conversion of agile user stories into pseudocode is\nsurveyed by [16], presenting a two-stage process using the\nCodeT5 model. This approach, fine-tuned for success\nmeasures using BLEU scores, aims to streamline software\ndevelopment in Agile environments by reducing the time\nspent converting user requirements into pseudocode,\noutperforming rule-based methods.\nKolmogorov-Arnold Networks (KAN) are based on the\nKolmogorov-Arnold theorem, established by Vladimir Arnold\nand Andrey Kolmogorov in 1957. This theorem demonstrates\nthat functions of several variables can be expressed as a\nsuperposition of functions of a single variable. KANS\nincorporate these univariate functions into their structure,\nproviding an elegant method for approximating complex\nmultivariate functions. Study [17] shows that KANS\noutperform traditional MLPs in capturing complexities in\nlarge datasets, offering better interpretability and accuracy.\nThis makes KANs particularly suitable for tasks like\nvulnerability detection, where high-dimensional data and\nsubtle nonlinear patterns are involved, overcoming the\nlimitations of traditional neural networks due to the curse of\ndimensionality.KAN can effectively break high-dimensional\nlinkage through the simpler one-dimensionality, linking to\ncomplex and more fine-grained data types among multiple\nspaces of knowledge representation at runtime point for\ndataset interpretability enhancement; boost detection power in\na broader range with fewer perturbations of discoverable\ntarget functions. f is the multi-variate function to be signified.\nIq and \u00d8qp are uni-variate functions. X1, X2, ..., Xn are the\nvariables of function f.\n$f(x_1, x_2,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_{q}(\\sum_{p=1}^{n}\\O_{q,p}(x_p))$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)\nThis formula shows that function f of n variables can be\ndecomposed into a superposition of 2n + 1 functions \u03a6q, each\nof them depends on a particular sum of functions \u00d8q,p, letting\neach \u00d8q.p are functions of a distinct variable xp. Here,\u0424q\nrepresents the outer functions, and \u00d8q,p represents the inner\nfunctions, each applied to one of the variables. The inner\nfunctions transform the individual variables, and their results\nare then summed up as inputs to the outer functions. In\n(section 3), VulCatch's combined dataset C analogous to\nX1, X2, ..., Xn is processed by the KAN function FRAN. This\nfunction embodies the KAN formula where each component\nof the dataset is treated as an input variable xp to the\nfunctions\u00d8q,p. These are then aggregated (summed) and\npassed through the functions Oq to generate enhanced features\nKfeatures.\nDecompilation technology has advanced rapidly [18], with\ntools like IDA Pro [19] and Ghidra [20]. Many binary code\nanalysis fields require decompilation techniques [21], often\nusing intermediate representations (IR) as in Inceptions and\nFirmUp [21]. Decompiled pseudo-code is used in\ncybersecurity; for example, UPPC (Unleashing the Power of\nPseudo-code) [22] uses pseudo-code for vulnerability\ndetection due to its higher abstraction and platform\nindependence. Other studies use pseudo-code to counter\ncompilation diversity [23] and detect malware via decompiled\nAndroid API calls. SCRUTINIZER [24] constructs pseudo-\ncode from memory dumps to detect malicious code repetition.\nDecompiled pseudo-code offers high-level semantic\ninformation and control structure recreation, making it more\nefficient for binary security analysis compared to analyzing\nbinary code or IR.\nOur primary research goal is to develop a system capable\nof detecting multiple types of both known and unknown\nvulnerabilities in binary code more accurately and efficiently\nthan current methods. The specific research questions we\naddress are:\nVulCatch begins by decompiling binary code into\npseudocode using the CodeT5 model, making it more\nunderstandable for further analysis. Disassembly analysis with\nIDA and Custom Analysis, along with structural analysis\nusing Ghidra and Control Path, provides detailed insights into\nthe code's structure and behavior. The data from these\nanalyses, combined with metadata, is enhanced using\nKolmogorov-Arnold Networks, improving the feature set.\nAbstract Syntax Trees (ASTs) and Program Dependence\nGraphs (PDGs) are generated from these features, forming\nstructured representations of the code. These are prepared as\ninputs for a BiLSTM-Residual-Attention Neural Network,\nwith residual connections aiding in training deeper networks.\nBidirectional passes in the BiLSTM capture context from both\npast and future states, while inception block processing and an\nattention mechanism focus on important parts of the input\nsequence. Residual connections enhance gradient flow, and\nthe deep learning model classifies vulnerabilities from the\nprocessed input data. The final output translates these\nclassifications into actionable information, identifying\nvulnerabilities with their categories and severity levels,\nproviding valuable insights for remediation.\nExperimental results demonstrate VulCatch's superior\nperformance across various datasets. On the SARD-AdvOpt-\nSDM dataset, it achieves an accuracy of 98.43%, precision of\n97.92%, and an F1-score of 98.88%. On the SARD-TestVul-\nSDM dataset, VulCatch attains an accuracy of 99.29%,\nprecision of 99.50%, and an F1-score of 99.05%. Testing on\nthe SARD-CWE-HighOpt-SDM dataset shows an accuracy of\n99.19%, precision of 99.45%, and an F1-score of 98.90%. On\nthe SARD-CWE-LowOpt-SDM dataset, it achieves an\naccuracy of 98.79%, precision of 99.00%, and an F1-score of\n98.88%. This consistent performance across multiple datasets\nhighlights VulCatch's effectiveness in detecting a wide range\nof vulnerabilities.\nThe key contributions of this research are:\nWe present Synergy Decompilation Module (SDM),\na comprehensive analysis on advanced decompilation\ntechniques, leveraging CodeT5 integrated with traditional\ndecompilers (Ghidra and IDA), enhancing the accuracy and\ncompleteness of binary pseudocode representation for\nvulnerability detection.\nWe integrated Kolmogorov-Arnold Networks\n(KANs) to enhance feature transformation capabilities, which\nresults in outperforming traditional neural network models in\nterms of detecting unknown vulnerabilities, the integration of\nKAN reduces false positive rates in binary vulnerability\ndetection.\nWe implemented program slicing techniques to refine\nvulnerability detection and developed VulCatch, a novel\nsystem consisting of BiLSTM, attention mechanism, and\nresidual connections for real-time vulnerability detection.\nWe curated a diverse set of datasets used to evaluate\nVulCatch's performance across different optimization levels\nand scenarios, demonstrating its robustness and adaptability to\nvarying compilation strategies.\nThis paper starts by introducing the research background,\nsignificance, and problems with current methods, followed by\nthe research goals and proposed solutions in section 1. It then\nreviews related work on vulnerability patterns and code\nsimilarities in section 2. The paper describes the system\ncomponents in section 3. It details the experimental procedure,\nneural network design, dataset construction, evaluation\nmeasures and analysis of results in section 4. Finally, the\npaper concludes with a discussion, followed by the limitations\nand future work in section 5."}, {"title": "II. RELATED WORKS", "content": "In the evolving field of binary vulnerability detection, two\nmain methodologies are often applied: code similarity-based\napproach and vulnerability pattern extraction approach. Code\nSimilarity-Based Techniques: These methods compare target\ncodes to databases of known vulnerabilities to measure their\nsimilarities. The target code is considered vulnerable when a\npredetermined threshold is exceeds. Nevertheless, the\nthreshold can cause high false-positive rates due to the\nsuperficial similarities. Vulnerability Pattern Extraction: This\nmethod identifies property signatures or patterns to detect\nknown vulnerabilities in target code. By comparison against\npredefined patterns derived from previous vulnerabilities, it\nthen flags codes with identical features. However, it is limited\nto known patterns and might not understand new\nvulnerabilities. Limitations and Challenges: Both methods\nstruggle with false positives and variability, mainly due to\ncompiler differences and irrelevant code throughout the\nfunctions. Current research aims to solve these problems\nthrough advanced decompilation and deep learning for\naccurate and efficient vulnerability detection.\nSimilarity based techniques are used to find recognized\nvulnerability (replica vulnerability). Using same approach,\nthey can barely discover unknown vulnerability. The\napproaches derived from vulnerability patterns obtain the\nappropriate features of the current vulnerable program, using\nsuch patterns to locate the target code. IntScope, decompiles\nthe binaries into an intermediary representation and then\nobtains important properties via symbol and taint approaches\nfor vulnerability detection. With the use of software slicing\nand symbol execution approaches, Firmalice [31], extracts\nproperties to suggests unique models to find concealed\nverification avoidance backdoors for focused embedded\nsystems. Value set analysis is used by GUEB, to track the\nusage and releases of memories throughout programs in order\nto find binary vulnerability in UA. Pattern matching is used by\ncwe checker [25], to find unknown vulnerabilities using CWE\nvulnerability patterns provided by human analysts. These\nefforts, nevertheless, have large false-positive rates since the\npatterns and instructions are very broad and some properties\nare physically obtained by humans. The results must also be\ncarefully verified. Deep learning techniques are used by\nVulCatch to automatically extract vulnerability models.\nSeveral researches then apply deep learning to\nautomatically detect vulnerabilities patterns to address the\nexisting challenges. Instruction2Vec [6], creates a vector\nrepresentation of the instruction through dividing instruction\nas opcode and operands (having two features), each property\nrepresenting a vector. Trained Text-CNN applies feature\nextraction of weak code using the learnt vector representation.\nAccording to [7], proposal, every instruction should be\npresented in a meaningful combination given by its vector\nelement, with a weighted summation instruction applied\naccording to the element's hierarchy level. The BiLSTM\nsystem is then trained to recognize patterns of vulnerabilities.\nAccording to the binary program machine instruction\nstructure, DCKM [26], combines Cost Sensitive Kernel\nMechanism and BRNN in finding vulnerabilities in binaries of\nvarious feature spaces. HAN-BSVD [8], builds a network for\nextracting features to catch spatial information that indicate\ncritical locations for collecting important vulnerability features\nby first training their instruction embedding models consisted\nof BiGRU and word-attention modules. Zheng, suggested\nconverting binary codes into LLVR IR and collecting\ninstructions relating to library functions using data flow\nanalysis. Eventually, those instructions are used to train\nvarious RNN networks for vulnerability findings.\nVulDeePecker [27], a model for detecting vulnerabilities in\nsource code, is adopted by BVDetector [9], but it collects code\nslices associated with API and library function at assembly\nlevel. Such approaches were centered on low-level code\nproperties, making granular analysis become finer. VulCatch\nextracts vulnerability patterns using a finer-grained method.\nPatch-code is used in other techniques for feature extraction.\nFor instance, VIVA [10], efficiently detects known and\nunknown vulnerabilities by using binary program slicing\nmethods along with decompilation to provide vulnerability\nand patch patterns. Such approach, meanwhile, is costly and\nsubject to compilation complications. VulCatch simply extract\nfeatures relating to the detection of vulnerabilities.\nCode similarities have been extensively used in malware\nclassification [28], plagiarism detection [29], and security\npatch assessment [30], as well as vulnerability\nfindings [34]. In order to identify binary vulnerability that are\nsame as existing vulnerabile code, TEDEM, introduced tree\ndistance control quantifying similarities in codes at the level of\nbasic blocks. In order to pre-filter the target function,\nDiscovRE, collects numerical information from the function\n(such as the quantity of mathematical and analytical process).Similarities between the structure's features are then analyzed\nin order to identify vulnerabilities. Dependent formulae are\nsemantic properties extracted at high-level to detect vulnerable\ncodes XMATCH [32], from the original binary code. Such\nvulnerability detection techniques, though, frequently generate\nexpensive similarity calculations and have high false positive\nand false negative rates. Machine learning methods have been\napplied to these issues to compare code similarity and find\nvulnerabilities. Genius [33] creates attributable CFGs and\ndetermines similarities using graph embeddings produced by\ncomparison with a collection of sample graphs known as\ncodebook. Gemini is a GNN-based model that was proposed\nby [34], that uses structure2vec, in combination with Siamese\nmodel, to produce the function graphs embedding and\ndetermine match. In order to enhance the detection accuracy,\nVulSeeker [35], builds the labelled semantics flow graphs\n(LSFG) using data control in functions with embedding LSFG\nin the space vectors via DNN semantical awareness\nframework.\nHowever, such techniques depict fundamental building\nblocks or functions using certain manually chosen statistical\nfeatures that might not have adequate semantic information.\nadiff [36], extracts three categories of semantic information\nfor similarity detection from introduced functions, raw bits\nfunctions, and function calls. In order to identify potential\nvulnerability functions, FIT [37], first extracts semantic\ninformation via neural network framework for binary\nprograms. Moreover, it further analyzes the similarity of two\nfeatures based on third-level features. With the help of\nassembler instructions, SAFE [4], collects semantic\ninformation. Next, using a transformer, it creates embeddings\nof functions. In order to identify vulnerabilities, it performs\nsimilarity calculations on the embedded representations that\nwere developed.\nSeveral studies propose the introduction of patch codes to\nfind vulnerabilities in order to lower the rates of false positive\nmore. In order to create a trace set, BINXRAY [38], first\nidentifies the fundamental building elements that are different\nbetween the patch and vulnerable functions. In order to\ndetermine patch in the functions, such checks help in\ncomputing similarities among patch, target and vulnerable\nfunctions accordingly. PDiff [39] compared the kernel's target\nbefore the adoption and after the adoption of patch with its\nnormal versions and produces a description with semantics\nlinked to the targeted patch. Lastly, every target program's\npatch status is determined using the similarity."}, {"title": "III. DESIGN OF VULCATCH", "content": "Binary code vulnerability detection is a complicated\nprocess that require complex mechanisms and methods.\nTraditional models mainly have problems because they are\nunable to understand the complex structures and semantics\ninformation embedded in binary codes. The VulCatch network\nsolves this problem by innovating a combination of advanced\ndecompilation procedures and deep learning framework to\nimprove the detection of both known and unknown\nvulnerabilities.\nThe different components are explained below:\nDecompilation with CodeT5: The VulCatch system starts\nthe vulnerability detection process with CodeT5, a model\ntrained to decompile binary code into pseudocode. This step is\nneeded because it transforms raw binary into a more\ninterpretable format and preserves high-level semantics\nproperties. The pseudocode generated is highly rich in\nsemantic information, critical for the understandings of\ncomplex software behaviors that might hide vulnerabilities\nFig. 1 Novel SDM framework.\nTraditional Decompilers Integration: Following the\ndecompilation stage, traditional tools Ghidra and IDA are\napplied for detailed disassembly and structural analysis. The\ntools complement CodeT5 by maintaining a granular\nbreakdown of the binary structures, which is essential for the\ncomprehensive examination of vulnerabilities Fig. 1.\nKAN is placed before the feature extraction phase and after\nthe initial data aggregation phase. This position allows KAN\nto process the raw combined data from disassembly,\nimproving representation of the data before it is used to\ngenerate more structural Abstract Syntax Trees (ASTs) and\nProgram Dependency Graphs (PDGs).\nAbstract Syntax Trees (ASTs) and Program Dependency\nGraphs (PDGs): The model then generates ASTs and PDGs\nfrom the combined data of CodeT5's pseudocode and\ntraditional decompiler output. ASTs benefit in understanding\nthe hierarchical structure of the codes, while PDGs\nconcentrate on the interactions and dependencies among\nvarious elements. These structures are helpful in pinpointing\ncritical parts of the code that are more likely to have\nvulnerabilities.\nDeep Learning Techniques for Enhanced Detection: To\nanalyze and process data from ASTs and PDGs, VulCatch\nuses a BiLSTM network integrating attention network with\nresidual connectivity in Fig. 2 VulCatch system design. This\nsystem is adept at detecting complex dependencies and\npatterns that are indicative of vulnerabilities. The attention\nmechanism precisely allows the model to focus on portions of\nthe code that have greater probabilities of having\nvulnerabilities, while residual connection enhances flow of\ngradient, helping effective learning and improved\ngeneralization, thus enhancing the efficiency and accuracy of\nthe detection process.\nInception blocks are integrated into the deep learning\nstructure to improve feature extraction by letting the network\ncapture multi-scale patterns in the data. These blocks contain\nmultiple convolutional layers with different filter sizes that\noperate in parallel. The inception blocks assist in detecting\nvulnerabilities at different levels of granularity, thus\nincreasing the robustness of the detection system.\nThe VulCatch framework is built to improve binary\nvulnerability detection by integrating advanced tools and\nmethods. The method is organized around a sequence of\nprocedures that leverages the exclusive strengths of CodeT5,\ntraditional decompilers IDA and Ghidra, KAN, and current\ndeep learning methods.\nAlgorithm 1: The Synergy Decompilation Module, starts\nby transformation of BinaryCode into pseudocode via\nDECOMPILE_WITH_CODET5, to make sure the semantic\nintegrity of the original code is well-kept. The pseudocode is\ndisassembled using IDA to enhance its original structure, and\nany structural or logical issues are found by additional analysis\nusing Ghidra. Ghidra and IDA's outputs are combined to\nconstruct a single, comprehensive dataset. Kolmogorov-\nArnold Networks (KAN) now processes this dataset further,\nby the use of a nonlinear mapping to transform features and\nrefine them, which is important for possible vulnerability\ndetection. After this process, the enhanced datasets are then\nused to generate ASTs and PDGs. These structures aids to\nvisualize the program's hierarchical and dependent aspects\nessential for detection and categorization of vulnerabilities\naccording to their nature and severity. A list of categorized\nvulnerabilities is produced by this process, together with\ninformation about vulnerable binary codes and a systematic\nassessment.\n$PseudoCode = F_{C5}(BinaryCode)$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)\nThe purposes of CodeT5 model used to decompile binary\ncode into pseudocode, are represented by Fc5. BinaryCode is\nthe input decompiled into PseudoCode, which becomes the\npseudocode output of the CodeT5 model.\n$D_{IDA} = F_{IDA}(PseudoCode)$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)\n$D_{CA} = F_{CA}(PseudoCode)$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4)\n(IDA & CA) Disassembly Analysis FIDA and FCA functions\ncorrespondingly signify the processes of Custom Analysis\n(CA) and the Interactive Disassembler (IDA) to examine the\npseudocode. PseudoCode serves as the input to these\nfunctions, which was obtained from the decompilation step.\nDIDA and DCA are the outputs of the IDA and CA analysis.\n$S_{G} = F_{G}(PseudoCode)$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5)\n$S_{CP} = F_{CP}(PseudoCode)$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6)\nStructural Analysis (Ghidra & CP) takes FG and FCP as the\nfunctions representing the Ghidra analysis and Control Path\nanalysis (CP), respectively. SG and Scp are the structural\nanalysis outputs from Ghidra and CP.\n$C = F_{combine}(D_{IDA}, D_{CA}, S_{G}, S_{CP}, M)$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(7)\nData Combination Fcombine is the function that integrates\ndata from various analysis tools along with metadata. M is the"}, {"title": "IV. IMPLEMENTATION AND EXPERIMENTS", "content": "The logic is to assemble library/API function of C/C++\nrelevant to vulnerabilities for obtaining library/API function\ncalls program-slices. We gathered the Checkmarx 2021\n50"}]}