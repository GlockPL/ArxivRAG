{"title": "Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy", "authors": ["Aditya Ganeshan", "Thibault Groueix", "Paul Guerrero", "Radom\u00edr M\u011bch", "Matthew Fisher", "Daniel Ritchie"], "abstract": "Pattern images are everywhere in the digital and physical worlds, and tools to edit them are valuable. But editing pattern images is tricky: desired edits are often programmatic: structure-aware edits that alter the underlying program which generates the pattern. One could attempt to infer this underlying program, but current methods for doing so struggle with complex images and produce unorganized programs that make editing tedious. In this work, we introduce a novel approach to perform programmatic edits on pattern images. By using a pattern analogy-a pair of simple patterns to demonstrate the intended edit-and a learning-based generative model to execute these edits, our method allows users to intuitively edit patterns. To enable this paradigm, we introduce SPLITWEAVE, a domain-specific language that, combined with a framework for sampling synthetic pattern analogies, enables the creation of a large, high-quality synthetic training dataset. We also present TRIFUSER, a Latent Diffusion Model (LDM) designed to overcome critical issues that arise when naively deploying LDMs to this task. Extensive experiments on real-world, artist-sourced patterns reveals that our method faithfully performs the demonstrated edit while also generalizing to related pattern styles beyond its training distribution.", "sections": [{"title": "1. Introduction", "content": "Visual pattern designs enhance digital media such as presentations, website themes, and user interfaces, and they are woven into the physical world through textiles, wallpapers, and product designs like hardware covers. Given the ubiquity of patterns, methods for editing them are essential: designers should be able to quickly experiment with variations, customize designs to meet specific needs, and adapt existing patterns to align with evolving trends.\nEditing pattern images is not straightforward, as patterns are inherently structured, defined by rules that govern their layout and composition: tiling patterns adhere to principles of alignment and repetition (see Figure 1: top left), while retro-style designs rely on spatial divisions and fills (see Figure 1: bottom left). The edits that designers desire often aim to adjust these underlying organizational rules rather than make superficial, pixel-level changes. We refer to such edits as programmatic edits, requiring manipulation of the underlying program that defines a pattern's structure.\nOne strategy for enabling such programmatic edits is visual program inference (VPI) [9, 40, 56], where a program that replicates an image is automatically inferred, allowing users to modify the image by adjusting program parameters. However, applying VPI to patterns presents two obstacles. First, VPI attempts to infer a program that fully replicates a pattern, which can be challenging as patterns are often semi-parametric, blending rule-based logic with non-parametric components. For instance, the layout of elements in a tiling pattern may be rule-based, but the elements themselves may not be. Second, editing with an inferred program can be cumbersome, as they are often poorly-structured, with many unlabeled parameters, making them difficult to interpret. Consequently, VPI not only solves a more complex problem than necessary but also makes editing more challenging.\nCan we perform programmatic edits without inferring the underlying program? Doing so requires the ability to express and execute the edit-both without direct access to the program's parameters. To express a programmatic edit, it's crucial to specify both which underlying parameter(s) to change and how to modify them. We draw inspiration from how humans communicate transformations: through analogies. By providing a pair of simple example patterns (A, A') that illustrate the desired change, users can intuitively convey both aspects of the edit. To execute these edits, we employ a learning-based conditional generative model. Given a pair of simple patterns (A, A') and a complex target pattern B, our system generates B', an edited version of B which performs the transformation demonstrated between A and A' while preserving B's other structural features. Crucially, A does not need to replicate or even be similar to B-it only needs to demonstrate which property to edit and how. Thus, specifying A is a much easier task than solving VPI. While prior works [2, 57, 61] have applied analogical editing to image manipulation, they focus primarily on appearance modifications. In contrast, our approach is the first to use analogies for programmatic, structure-aware edits. Figure 1 (left) shows examples of analogical editing on complex, real-world patterns.\nTo make our approach possible, we introduce SPLIT WEAVE: a domain-specific language (DSL) for crafting visual patterns. SPLITWEAVE serves two purposes in our method. First, it enables parametric definition of input pairs (A, A'), allowing users to guide transformations in (B, B') as if the underlying program for B were accessible. In Figure 1 (right), modifying the SPLITWEAVE program for A' produces corresponding changes in B'. Second, SPLITWEAVE supports the creation of large-scale synthetic training data. We develop program samplers that generate high-quality patterns in two common styles: tiling-based designs with repeating elements and color field patterns characterized by splitting the canvas into intricate colored regions. Training a model for analogical editing requires a dataset of quartets (A, A', B, B'). By applying identical programmatic edits to the SPLITWEAVE programs for both A and B to produce A' and B', we ensure that the transformation from A to A' mirrors that from B to B'. This approach allows us to generate a diverse dataset of analogical quartets. Models trained on this dataset can generalize effectively to real-world patterns within these styles and can extend to related styles.\nWe use this synthetic dataset to train a novel diffusion-based conditional generative model for executing analogical edits. Our model directly generates edited patterns B' by conditioning on visual features extracted from input patterns (A, A', B). Existing image-conditioned diffusion models [52, 65] prove ineffective, as they fail to interpret the input analogies accurately and neglect fine details. To address these issues, we incorporate architectural enhancements that enable our model, TRIFUSER, to effectively perform analogical edits. With these improvements, TRIFUSER surpasses prior architectures for analogical editing when applied to pattern images.\nTo evaluate our method, we curated a test set of 50 patterns from Adobe Stock spanning 7 distinct styles. A perceptual study on this dataset shows that participants preferred edits by TRIFUSER over recent training-free and training-based methods. Although our training data covers only two of these styles, our model demonstrates effective generalization to the other, out-of-distribution styles. On a synthetic validation set with ground-truth analogical edits, our model produces outputs more similar to the ground truth than other methods. Finally, we showcase two applications of our approach: mixing attributes of different patterns and transferring pattern animations.\nIn summary, our contributions are as follows:\n1. A novel framework for performing programmatic edits to pattern images without requiring program inference, leveraging analogies to specify and apply edits.\n2. SPLITWEAVE, a DSL for crafting a diverse range of visual patterns, designed to support both parametric control and synthetic dataset generation.\n3. A procedure for generating synthetic analogical quartets, enabling editing of in-the-wild patterns.\n4. TRIFUSER, a diffusion-based conditional generative model that achieves high fidelity in analogical edits, surpassing prior techniques in both analogical fidelity and generation quality."}, {"title": "3. Method", "content": "Our objective is to enable programmatic edits of 2D visual patterns without inferring their underlying programs. Instead, we propose an alternative that uses analogies to express desired edits and a conditional generative model to execute them. Formally, given two source patterns A and A' that demonstrate a desired edit, along with a target pattern B, our goal is to generate an edited target pattern B' that applies this edit to B. This task is defined as learning a mapping f(A, A', B) \u2192 B', where A, A', B, and B' are 2D RGB images (\u2208 RH\u00d7W\u00d73). To learn this mapping, we generate a large synthetic dataset of analogical pattern quartets (A, A', \u0392, \u0392').\nFigure 2 provides a schematic overview of our approach. First, in Section 3.1 we introduce SPLIT WEAVE, a Domain-Specific Language (DSL) that enables the creation and manipulation of various kinds of patterns. Section 3.2 describes our approach for sampling analogical quartets in SPLITWEAVE to create the synthetic training data. Finally, in Section 3.3, we present TRIFUSER, a conditional generative model that learns to execute analogical edits."}, {"title": "3.1. A Language for Visual Patterns", "content": "To enable programmatic edits without program inference, our approach requires two core capabilities: (a) generating a large, high-quality synthetic dataset essential for training models to reliably execute analogical edits, and (b) the ability to create and parametrically control analogy inputs at test time to effectively express desired edits. Existing pattern generation tools are insufficient for these needs, as they are either limited to narrow pattern domains [55] or demand intense coding effort to produce diverse, high-quality patterns [38, 41]. To address these limitations, we introduce SPLITWEAVE, a DSL designed specifically to support analogical transformations in visual patterns. SPLITWEAVE combines abstractions for pattern synthesis with a node-based visual programming interface (see Supplementary), enabling efficient generation of high-quality synthetic patterns for training while allowing flexible, precise pattern manipulation to define analogy inputs at test time.\nSPLITWEAVE uses three types of operations for structured pattern creation: (1) Canvas Fragmentation, which allows structured divisions of the canvas, such as brick-like or voronoi splits; (2) Fragment ID-Aware Operations, enabling transformations that vary across fragments (e.g., scaling alternating rows or columns) to support spatial variability in non-stationary pattern designs; and (3) various SVG Operators for outlining, coloring, and compositing. Together, these operations enable efficient creation of patterns with complex structure and visual variety. Figure 2 (left) illustrates these capabilities in a SPLITWEAVE program for generating a tiling pattern design.\nOur goal is to generate high-quality synthetic patterns using SPLITWEAVE that enable trained editing models to generalize well to real-world patterns. Naive sampling from the DSL grammar often leads to overly complex or incoherent patterns, limiting their effectiveness in model training. Instead, we draw inspiration from recent advances in fields such as geometric problem solving [60] and abstract reasoning [36], where tailored data generators have proven essential for tackling complex tasks. Following a similar approach, we design custom program samplers for two versatile and widely-used pattern styles. The first, Motif Tiling Patterns (MTP), consists of compositions based on repeated Tile elements. These patterns exhibit controlled variations in tile properties across the canvas (e.g. orientation, color, and scale), creating visually cohesive yet richly diverse structures. The second, Split-Filling Patterns (SFP), are generated by dividing the canvas into ordered fragments, applying region-specific coloring and transformations based on fragment IDs. Both pattern styles are common in digital design and support a wide range of programmatic variations, making them particularly suited for analogical editing tasks."}, {"title": "3.2. Sampling Analogical Quartets", "content": "With the ability to generate diverse synthetic patterns using SPLITWEAVE (Section 3.1), our goal is now to construct analogical pattern quartets (A, A', B, B'). Each pattern image in a quartet is generated by a SPLITWEAVE program z. These quartets serve as structured training data for editing models, allowing them to learn consistent transformations that can generalize across different pattern domains.\nAnalogies in our framework are grounded in Structure Mapping Theory [17], which defines analogies as mappings of relational structure from a base to a target domain. We designate (A, A') as the base and (B, B') as the target, with the requirement that the relationship R between program pairs (ZA, ZA') and (ZB, ZB') remains consistent:\nR(ZA, ZA') = R(ZB, ZB').\nRather than focusing on visual similarity between the patterns (A, A') themselves, this program-level analogy allows us to generate quartets with transformations that affect the underlying program, facilitating programmatic edits.\nTo construct these analogical quartets, we use a program sampler along with a predefined set of editing operators E. For each quartet, we begin by sampling an edit e \u2208 E, followed by sampling initial programs z\u0104 and ZB that are compatible with e. Applying e to both ZA and ZB yields transformed programs ZA' and ZB'. By using identical transformations across domains, we ensure a consistent \"edit relation\" across the quartet, satisfying Equation 1 by construction. In Figure 4, we illustrate examples of synthetic analogical quartets generated using this method, demonstrating consistent transformations between (A, A') and (B, B').\nEdit Operators E. We focus on edits targeting specific sub-parts of the program. Specifically, we consider three types of edits: insertion, removal, and replacement of sub-programs. For example, an edit operator might replace the sub-program responsible for splitting the canvas, while other edits may insert or remove tiles within the pattern. Please refer to the supplementary for more details."}, {"title": "3.3. Learning an Analogical Editor", "content": "Our goal is to train a model on the synthetic data that is capable of performing analogical edits on real, in-the-wild patterns. Specifically, we aim to generate the target pattern B' from an input triplet (A, A', B). This approach allows users to demonstrate desired edits with a simple pattern pairs (A, A'), which the model then applies to a complex patterns B to produce B'. Given the success of Latent Diffusion Models (LDMs) in various generative modeling tasks [52], we chose to adapt an LDM for our task as well. We propose TRIFUSER, a latent diffusion model (LDM) for analogical editing (Figure 5). We provide a brief overview of LDMs to provide context before detailing TRIFUSER 's modifications for analogical editing.\nPreliminaries: Denoising Diffusion Probabilistic Models (DDPMs) [23] transform random noise into structured data via reverse diffusion steps guided with a conditioning embedding c(y) (often derived from text). Latent Diffusion Models (LDMs) extend DDPMs by mapping data to a lower-dimensional latent space via an encoder. During training, a UNet model [53] learns to remove noise introduced into the latents. During inference, a latent sampled from a normal distribution is iteratively denoised by the model to yield a clean latent. Finally, the clean latent is decoded to generate the output image. Please refer to [66] for a more thorough overview. For analogical editing we adapt an Image Variation (IM) model [65], which uses patch-wise image tokens extracted using a text-image encoder [48] as the conditioning embedding c(y).\nThe simplest adaptation of an IM model to our task is to generate B' conditioned on image tokens from all three input images, concatenated as C = c(A) || c(A') || c(B), where || denotes token-wise concatenation. This approach, however, suffers from three drawbacks: Token Entanglement, Semantic Bias, and Detail Erosion. We discuss each of these issues briefly, along with our solutions.\nDetail Erosion: Despite using patch-wise tokens, the extracted features lack the fine-grained information needed to retain key aspects of B in the generated pattern B'. Consequently, the model often struggles to preserve elements like tile textures. To address this problem, we combine features from both the first and last layers of the feature encoder:\nChl(P) = Linear(LN(Chigh(P)) \u00b7 LN(Clow(P)),"}, {"title": "4. Experiment", "content": "In this section, we evaluate our approach along three directions: (1) the effectiveness of TRIFUSER at performing analogical edits on complex, real-world patterns, emphasizing how our synthetic data enables editing of in-the-wild pattern images; (2) the ability of TRIFUSER to support programmatic, structure-preserving edits without explicit program inference; and (3) the impact of architectural modifications introduced in TRIFUSER on the quality of generated patterns. We conduct a human perceptual study, quantitative assessments, and qualitative comparisons to demonstrate our system's ability to perform high-quality analogical edits across a range of pattern types."}, {"title": "4.1. Experiment Design", "content": "Datasets: We generate a large synthetic dataset of analogical quartets, i.e., pairs of analogical patterns (A, A', \u0392, \u0392'), using the SPLITWEAVE program samplers introduced in Section 3.1. This synthetic dataset contains approximately 1 million samples covering two pattern styles, namely Split Filling Patterns (SFP) and Motif Tiling Patterns (MTP) (cf. Section 3.1). For MTP patterns, we synthesize 100k distinct tiles using the LayerDiffuse [69] model, guided by text prompts derived from WordNet [43] noun synsets. Additionally, we construct a synthetic test set with 1000 analogical quartets to evaluate model performance on unseen synthetic data. Further details on dataset construction are provided in the supplementary material.\nTo assess TRIFUSER on real-world patterns, we curate a test dataset of 50 patterns created by professional artists and sourced from Adobe Stock. This dataset spans seven distinct sub-domains of 2D patterns, representing a range of pattern styles. These styles include MTP and SFP patterns as well as previously unseen pattern styles such as Memphis-style, geometric, and digital textile patterns. Each pattern is annotated with a desired edit, and we use SPLITWEAVE to generate a pair of simpler patterns (A, A') demonstrating this edit. This test set provides a challenging benchmark to evaluate TRIFUSER's generalization to diverse, real-world editing tasks.\nTraining details: We fine-tune a pre-trained diffusion model using our synthetic dataset of analogical quartets, as described in the previous section. We initialize our model with Versatile-Diffusion's Image Variation model [65]. We use SigLIP [68] as our text-image feature encoder and Di-NOv2 [46] for self-supervised features. We fine-tune the model on 8 A100 GPUs using a batch size of 224 for ~ 65 epochs over 7 days. During inference, we generate each edited pattern B' with typical diffusion parameter settings such as a classifier-free guidance weight of 7.5 and 50 denoising steps."}, {"title": "4.2. Analogical Editing Baselines", "content": "To evaluate the analogical editing capability of TRIFUSER, we compare it to three baseline methods, each representing a leading approach for analogical image editing.\nFirst, we consider training-free editors and latent arithmetic editors. Training-free editors repurpose pre-trained diffusion models to perform analogical edits without additional training [18, 61], leveraging the rich representations learned by diffusion models for editing. In this category, we compare against Analogist [18], the current state-of-the-art method. Latent arithmetic editors, on the other hand, rely on transformations in a learned latent space to infer analogical modifications [49, 58]. Note that these approaches only require samples from the target domain, not analogical training pairs. We implement a baseline for this method by fine-tuning a naive Image Variation model [65] on our synthetic dataset to learn a generative latent embedding space of patterns. At inference, analogical edits are generated using latent arithmetic: given patterns A, A', and B, we condition the generation of B' on E(B) + E(A') \u2013 E(A). We refer to this baseline as LatentMod.\nFinally, we consider analogy-conditioned generative editors, where models are explicitly trained on analogical data to learn analogical transformations [57]. This category includes our proposed TRIFUSER as well. Image Brush, the state-of-the-art method, fine-tunes a diffusion inpainting model for analogical editing with multi-modal conditioning. Since code for Image Brush is unavailable, we implement a similar baseline by fine-tuning a Stable Diffusion in-painting model. This model, which we term Inpainter, performs analogical editing by inpainting the lower-left quadrant of a 2x2 analogy grid containing (A, A', B) and conditioned on a fixed text template."}, {"title": "4.3. Editing Real-World Patterns", "content": "To evaluate TRIFUSER's real-world analogical editing capabilities, we conducted a human preference study on the curated test set of Adobe Stock patterns.\nWe performed a two-alternative forced-choice perceptual study comparing TRIFUSER with baseline methods on all 50 entries in the test set. Each method generates k = 9 outputs for each input tuple, and we select the best one based on visual inspection. Participants were shown edited patterns generated by two different methods along with the input patterns (A, A', B) and instructed to select the edit that best preserved the analogical relationship and exhibited higher image quality. We recruited 32 participants for the study, resulting in a total of 1550 total judgments.\nTable 1 presents the results, showing that TRIFUSER was preferred over both Analogist and LatentMod. Due to the domain gap between the training data of the underlying model [52] and pattern images, Analogist fails to interpret and edit pattern images. Meanwhile, LatentMod fails to perform reasonable edits as the embedding space lacks the low-level details necessary for programmatic edits While these baselines perform adequately on stylistic edits, they are unsuitable for programmatic editing. When compared to Inpainter, TRIFUSER was favored in 72.2% of comparisons. Both methods benefit from training on analogical quartets, yet Inpainter sacrifices pattern quality as it generates the edited pattern in only a quarter of the full canvas resolution."}, {"title": "4.4. Editing Synthetic Patterns", "content": "Next, we evaluate TRIFUSER's ability to perform programmatic edits on the synthetic validation set, which contains ground truth patterns B'. Ideally, this would involve verifying that the underlying program z\u00df, of the generated pattern reflects the same transformation from zB as that between ZA and A. However, this would require visual program inference on B', which is infeasible. Instead, we approximate this criterion by comparing the program outputs B' and B' to see if the visual results align with the intended transformation. To quantify this alignment, we use perceptual metrics-DSim [12], DIST [7] and LPIPS [70]-along with SSIM to capture pixel-level structural similarity.\nNote that analogies can have multiple valid interpretations, and even a single interpretation may yield several visually-related variations. To account for this multiplicity, we generate k = 5 output patterns for each input set (A, A', B) and select the one that maximizes each metric. In other words, we evaluate whether at least one generated output aligns with the intended target."}, {"title": "4.5. TRIFUSER Ablation", "content": "To evaluate the contributions of each model component introduced in Section 3.3, we conduct a subtractive analysis on the synthetic validation set, using the same perceptual and structural metrics as above. For this ablation study, we remove each component one at a time and measure the resulting performance, as reported in Table 3. The results demonstrate that removing any single modification leads to a performance drop, with the removal of 3D positional encoding causing the most severe degradation. This is understandable: without 3D positional encoding, the network often fails to accurately identify which pattern to edit. For comparison, we also include results from the original Image Variation model [65] trained without any modifications (Base). As expected, this model performs poorly, underscoring the importance of our modifications in achieving high-quality analogical edits."}, {"title": "5. Application", "content": "The ability to edit patterns without requiring program inference unlocks new creative possibilities. We demonstrate two practical applications of analogical pattern editing:\nPattern Mixing: Figure 8 shows example of using our method to mix elements of two real-world patterns X and Y, allowing the user to create unique, hybrid designs. The Mix operator is implemented by using a synthetic pair (A, A') to create a variant X' of X and then using the pair (X, X') to specify an edit to Y: Mix(X,Y) = f(X, X', Y), where X' = f(A, A', X). See the supplementary material for more details.\nAnimation Transfer: TRIFUSER can also be used to create animated sequences of edited patterns. By leveraging parametric SPLITWEAVE programs, users can generate animations for simple patterns and then apply these animations to complex patterns with no additional effort. See the video in the supplementary material for examples."}, {"title": "6. Conclusion", "content": "In this paper, we introduced a novel approach for programmatic editing of visual patterns without inferring the underlying program. By using analogies to express desired edits and a learned conditional generative model to execute them, our method provides an intuitive solution for pattern manipulation. A key component of our approach is SPLITWEAVE, a domain-specific language for generating diverse, structured pattern data. Paired with our procedure for sampling analogical quartets, SPLITWEAVE enables the creation of a large, high-quality dataset for training. We also presented TRIFUSER, a Latent Diffusion Model (LDM) designed to overcome critical issues that emerge when LDMs are naively deployed for analogical pattern editing, enabling high-fidelity edits that capture user intentions. Our experiments demonstrate that TRIFUSER successfully edits real-world patterns and surpasses baseline methods, while also generalizing to novel pattern styles beyond its training distribution. We believe that our DSL, dataset, and model will help drive further research on in-the-wild pattern image editing. Looking forward, we aim to extend this analogical editing framework to other domains such as semi-parametric 3D modeling while continuing to improve synthetic data quality and scalability."}]}