{"title": "The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective", "authors": ["Zhen Qin", "Daoyuan Chen", "Wenhao Zhang", "Liuyi Yao", "Yilun Huang", "Bolin Ding", "Yaliang Li", "Shuiguang Deng"], "abstract": "The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMS (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On one hand, vaster and higher-quality data contribute to better performance of MLLMs, on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stage of MLLMs can specific data-centric approaches be employed to enhance which capabilities, and 2) by utilizing which capabilities and acting as which roles can models contribute to multi-modal data. To promote the data-model co-development for MLLM community, we systematically review existing works related to MLLMs from the data-model co-development perspective. A regularly maintained project associated with this survey is accessible at https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.", "sections": [{"title": "1 INTRODUCTION", "content": "ARGE language models (LLMs) have demonstrated im-pressive performances across a wide range of tasks inrecent years, with their associated technologies making sig-nificant advancements. Since human senses are not limitedto the textual modality, multi-modal LLMs (MLLMs) havecome into view, such as Gemini-1.5 [1", "2": "thatare capable of processing inputs or outputs in modalitiesbeyond text, and GPT-40 [3", "4": "that caneven interact between multiple modalities in both input andoutput. MLLMs have been receiving widespread attentionin the past two years. As illustrated in Fig. 1, since thebeginning of 2023, research related to MLLMs has beenemerging with an increasing speed.\nThe outstanding performance of MLLMs stems fromthe emergent abilities of LLMs in solving a series of tasksbrought by the scaling up in the number of parameters [5", "data[6": [7], "8": "such as scaling law [9", "10": "."}]}