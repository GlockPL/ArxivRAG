{"title": "The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective", "authors": ["Zhen Qin", "Daoyuan Chen", "Wenhao Zhang", "Liuyi Yao", "Yilun Huang", "Bolin Ding", "Yaliang Li", "Shuiguang Deng"], "abstract": "The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMS (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs, on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stage of MLLMs can specific data-centric approaches be employed to enhance which capabilities, and 2) by utilizing which capabilities and acting as which roles can models contribute to multi-modal data. To promote the data-model co-development for MLLM community, we systematically review existing works related to MLLMs from the data-model co-development perspective. A regularly maintained project associated with this survey is accessible at https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.", "sections": [{"title": "1 INTRODUCTION", "content": "LPRARGE language models (LLMs) have demonstrated im-pressive performances across a wide range of tasks inrecent years, with their associated technologies making sig-nificant advancements. Since human senses are not limitedto the textual modality, multi-modal LLMs (MLLMs) havecome into view, such as Gemini-1.5 [1] and Sora [2] thatare capable of processing inputs or outputs in modalitiesbeyond text, and GPT-40 [3] and NEXT-GPT [4] that caneven interact between multiple modalities in both input andoutput. MLLMs have been receiving widespread attentionin the past two years. As illustrated in Fig. 1, since thebeginning of 2023, research related to MLLMs has beenemerging with an increasing speed.\nThe outstanding performance of MLLMs stems fromthe emergent abilities of LLMs in solving a series of tasksbrought by the scaling up in the number of parameters [5].Many works indicate that scaling up the model size needsto be complemented by even more massive amounts of data[6], [7], [8], such as scaling law [9], [10]. Specifically, itis indicated that multi-modal models require exponentiallymore data to achieve linear zero-shot improvements indownstream tasks [11]. In light of this, a series of workshave shifted the focus from merely model architectures andtraining techniques to data-centric approaches that focuson the curation of high-quality data [12], [13], [14], [15],[16], [17], to provide data basis for further unlocking thepotential of large models. From Figure 1, among existingpapers focusing on MLLMs, those closely related to data-centric approaches likewise exhibit a strong growth trendon the count and occupy a significantly important portion.As numerous technical works related to MLLMS con-"}, {"title": "2 PRELIMINARY", "content": "2.1 Background\nData-model co-development for MLLMs attempts to im-prove model performance while leveraging this model tooptimize the data, with the final objective aimed at a well-performing MLLM. It is characterized by dynamic trainingdata for a dynamic model [42]. In this paper, we focus onthe data-model co-development of a diverse range of multi-modal generation models that are capable of generatingoutputs in different modalities from the input or in moremodalities than the input, such as image-to-text dialog [43]and text-to-image synthesis [44]. Given the current lackof a formal definition for data-model co-development, weattempt to provide a formal definition with a generationmodel as an example before reviewing the existing works,to facilitate understanding.\nDefinition 1. (Data-Model Co-Development for GenerationModel). Let 1) $P_{data}(u)$ denote an ideal distribution thateach real-world data $u$ follows, where $u$ is composed bya context $c$ and a response $r$, denoted by $u = \\{c, r\\}$, 2) $w$be the generation model to be trained, which generates$u'$ given a context $c$, 3) $D = \\{u_1, u_2, ...\\}$ be the datasetto train $w$, 4) $D^*$ denote the ideally optimal $D$, and5) $Q(\\cdot, \\cdot)$ be the function that quantifies the similaritybetween two data samples, with larger values indicatinghigher similarity, data-model co-development can beformalized as a bi-level optimization problem, as\n$\\max_{W} E_{\\{c,r\\}\\in D^*} Q(r, w(c)),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)$\ns.t. $D^* \\in \\underset{D}{arg \\min} E_{u \\sim P_{data}(u), x=\\{c,r\\} \\in D} Q(u,x). $\t\t\t\t\t\t\t\t(2)\nEq. (2) aims at creating/optimizing the data for training w.Eq. (1) is the final objective that teaches w to produce datathat resembles real-world data. Data-model co-developmentcan be classified into two paradigms based on the tools usedfor optimizing D:\n1) (Self-Boosted Paradigm). The model $w$ to be trainedis also used to improve dataset D, where Eq. (2) andEq. (1) are usually optimized alternatively. In thisparadigm, Eq. (2) can be further refined as $D^* \\in\\underset{D}{arg \\min} E_{u \\sim P_{data}(u), \\forall x=\\{c,r\\} \\in D} Q(u, x)$.\n2) (Externally-Boosted Paradigm). As a fallbackalternative, D can be created/optimized with a well-trained model $w^*$ such as GPT-4V, or even humanefforts. Accordingly, we can rewrite Eq. (2) as $D^* \\in\\underset{D}{arg \\min} E_{u \\sim P_{data} (u), \\forall x=\\{c,r\\} \\in D} Q(u, x)$,\tor $D^* \\in\\underset{D}{arg \\min} E_{human} E_{u \\sim P_{data}(u), \\forall x=\\{c,r\\} \\in D} Q(u, x)$,respectively.\nFrom our investigations, the self-boosted paradigm has beenproven to be effective in some existing works for uni-modalLLMs [42], [45], but still lacks investigation for MLLMs.\nData-model co-development is promising as increasingattention shifts towards data-centric approaches, where dataoften serves as the primary variable, rather than merelyfocusing on model architectures [12], [13], [14], [17], [35],[46]. As MLLMs require increasingly large volumes of data,models are gradually used to assist or directly build thedata samples. Thus, the development of data and mod-els have become interdependent and inseparable: massiveand high-quality data can lead to well-performing MLLMs,and in turn, well-performing MLLMs can help constructmore high-quality data. Therefore, it is necessary to under-stand how data approaches enhance specific capabilities ofMLLMs, and how MLLMs assist in data approaches, therebyadvancing the data-model co-development for MLLMs.\nMLLM Architecture. We briefly introduce MLLM ar-chitecture for ease of understanding. An MLLM usuallycontains: 1) a pretrained LLM such as LLAMA [47]; 2) oneor more foundation models (encoders) to encode data fromnon-text modalities, such as ViT [48] and CLIP [49]; and3) one or more projectors to align the encoded featuresof non-text modalities with the feature space of pretrainedLLMs. With these components, MLLMs can generate textualresponses to multi-modal inputs [28]. Based on this, by in-corporating signal tokens and attaching 4) modality-specificgenerators such as Stable Diffusion [44], MLLMs canoutput contents in various modalities [4], [8]. These com-ponents of MLLMs may be trained in different steps [50]and may require different types of datasets [43], [51]. Notethat this survey does not focus on the pretraining of LLMs,which we refer to existing surveys summarized in Table 1."}, {"title": "2.2 Taxonomy", "content": "The taxonomy and the relationships between the variousitems are illustrated in Fig. 2. According to our investi-gations, the contributions of the data to models and thecontributions of models to the data can each be categorizedinto two major types. The data contributions to modelsare organized in an objective-driven manner and the order ofthe technical stages of MLLM development. At first, to elicitvarious general abilities from MLLMs, it is crucial to providemore and higher-quality data for MLLM training, aiming atthe scaling of MLLMs (Section 3), where some of the relatedworks focus on providing more data to scale up MLLMs(Section 3.1), while other efforts enhance the effectiveness ofscaling by improving data quality and organizing the dataappropriately (Section 3.2). With data-centric approaches forscaling MLLMs, we can obtain MLLMs with strong foun-dational capabilities. Subsequently, a series of data-centricefforts focus on enhancing the usability of MLLMs fromvarious aspects, including the instruction responsiveness(Section 4.1), reasoning ability (Section 4.2), ethics (Section4.3) and evaluations (Section 4.4).\nAs mentioned in Section 1, with datasets continuingto grow in size, their curation increasingly relies on well-trained MLLMs or their components such as LLMs andfoundation models to reduce the burden on human labor.We review a series of data-centric approaches co-pilotedwith MLLMs or their components, and divide them intotwo main categories, organized according to the level of intel-ligence required for the models to replace human work, i.e.,1) for synthesis of data, where models directly participate inthe data construction to alleviate repetitive tasks for humansby acting as a data creator (Section 5.1), data mapper (Sec-tion 5.2), data filter (Section 5.3), and data evaluator (Section5.4); and 2) for insight into data, where models perform asdata scientists to provide some insights on multi-modal databy acting as a navigator (Section 6.1), analyzer (Section 6.3),extractor (Section 6.2), and visualizer (Section 6.4).\nBased on these investigations, we summarize the pub-"}, {"title": "2.3 Differences from Related Surveys", "content": "The surge in popularity of MLLMs has also prompted manyresearchers to dedicate efforts to cataloging existing works.Existing surveys on MLLMs mainly focus on model-centricperspectives, and they construct taxonomies: 1) based ontraining techniques that highlight the training stage andcorresponding algorithms [18], [19], [20], [21], [22], [23], [24],[25], [26]; 2) based on MLLM architectures which highlightthe architectural components of MLLMs [19], [20], [22], [23],[26], [27], [28], [29], [30], [31]; 3) based on MLLM capabilitieswhich highlight reasoning abilities of MLLMs [21], [28], [32]and applications of MLLMs [19], [33]; or 4) based on MLLMsystems that focus on the key considerations and applica-tions in MLLM system design [20], [34]. These model-centricMLLM surveys have facilitated the development of MLLMs,yet have not given dedicated consideration to data.\nAs the importance of data for LLMs gradually gainswidespread attention [14], some data-centric surveys foruni-modal LLMs have emerged. They summarize existingworks with the taxonomy: 1) based on LLM data pipelinewhich highlight data approaches for LLMs across stagesin the data pipeline [12], [37], [38]; 2) based on trainingstages of LLMs which clarify data-centric approaches foreach training stage [39]; or 3) based on the adopted tech-niques which clarify foundation techniques for specific data-centric approaches [40], [41] or enumerate data-efficient ap-"}, {"title": "3 MULTI-MODAL DATA CONTRIBUTIONS FOR MLLMS: SCALING", "content": "Building MLLMs with good performance requires large-scale and high-quality multi-modal data. This section sum-marizes existing data-centric works that help constructlarge-scale MLLMs involving providing datasets, organizedaccording to the logical sequence of top-level design and iterativeoptimization. To build large-scale and high-quality multi-modal datasets, we need first to scale up the number ofsamples in multi-modal datasets (Section 3.1), followed byenhancing the quality of the datasets, i.e., improving thescaling effectiveness of datasets (Section 3.2). The organiza-tion of this section is illustrated in Fig. 3. After introducingexisting works related to this section, a brief summarizationand discussion are provided in Section 3.3.\n3.1 For Scaling Up of MLLMs: Larger Datasets\nThe excellent performance of MLLMs benefits from a largernumber of parameters, especially when the size of modelparameters reaches a certain level, abilities that traditionalmulti-modal models lack begin to emerge, such as OCR-freemath reasoning [28]. Researches indicate that the growthin the number of parameters of LLMs necessitates a cor-responding increase in the volume of data to support it[9], [52]. Compared to uni-modal LLMs, MLLMs exhibit asignificant extension in the feature space of both inputs andoutputs. Intuitively, such an extension in the feature spacefurther emphasizes the importance of extensive trainingdata. As have been analyzed, multi-modal models requireexponentially more data to achieve linear improvementsindownstream zero-shot performance [11]. Therefore, thedevelopment of high-performance MLLMs benefits frommassive, information-rich, and diverse training data.\nThis subsection introduces data-centric works facilitat-ing scaling up MLLMs, including data acquisition (Section3.1.1), data augmentation (Section 3.1.2) and the works thatfocus on data diversity (Section 3.1.3)."}, {"title": "3.1.1 Data Acquisition", "content": "Data acquisition, a.k.a. data collection, is the process ofacquiring raw data to fundamentally support large-scaledatasets. Existing works adopt diversified data sources, in-cluding: leveraging massive amounts of native data crawledfrom the web [11], [15], [49], [53], [54], [55], taking subsets ofexisting datasets or merging multiple existing datasets [50],[53], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66],building datasets with human efforts [4], [55], [59], [67], [68],[69], employing well-trained MLLMs such as GPT-4V forautomatic or semi-automatic data creation [4], [43], [54], [56],[63], [64], [67], [69], [70], [71], [72], or obtaining simulationdata with simulators [73].\nAs introduced in Section 2.1, the components of MLLMsmay be separately trained in different stages 1, including: 1)pretraining the encoders and/or generators for basic under-standing and/or generation, 2) pretraining the projector(s)for feature space alignment, and 3) fine-tuning the entireor parts of the MLLM to promote task responsiveness ordownstream-task performance. Generally, these stages mayconsume different types of datasets [43], [51]\nFor Encoders and Decoders. The pretraining of theencoders typically relies on massive any-text pairs, whichare usually easy to obtain by crawling from the web ormerging existing datasets. WebImageText [49] collects 400million image-text pairs collected from the web to train thevisual encoder with natural language supervision, whichbenefits a series of MLLMs such as LLAVA [50]. Similarly,LAION-Audio-630K [57] contains 630K audio-text pairscollected from 8 websites, facilitating the pretraining of au-dio and text encoders. By discarding the burdensome filter-ing mechanisms that come with Conceptual Captionsdataset [74], image-text pairs are expanded by two orderof magnitude, effectively improving the performance ofALIGN, thereby demonstrating the significance of scalingup datasets [75]. To provide Chinese understanding, CLIPmodel is trained on billions of image-text pairs obtained bypurchasing and open sources [66]\nFor Projectors. Pretraining of the projectors for fea-ture alignment usually consumes the instruction-followingdatasets converted from existing any-text pairs. To train theprojector in LLAVA [50], 595K image-text pairs are expandedto instruction styles by simulating conversations betweena human and an assistant, with native captions treatedas responses. Considering the limited cross-modal associ-ation in image-text pairs contained in COCO [76], GPT-4Vis prompted to generate fine-grained captions on imagesselected from Vision-FLAN [77] and LAION [78], whereththe detailed information such as background elements andnotable features are required to boost the cross-modal asso-ciation between images and captions [43]. Video-ChatGPT[67] bridges video features to the embedding space of anLLM with a linear layer, which is trained on a subsetof ActivityNet-200 [79], where human annotators areemployed to enrich the captions with contextual details suchas physical appearance.\nFor Fine-tuning. With the above works, we can obtainMLLMs with fundamental understanding and generation-"}, {"title": "3.1.2 Data Augmentation", "content": "Constructing large-scale datasets is often expensive whileperforming data augmentation based on existing datasetsis usually more cost-effective. Data augmentation applies aseries of transformations to existing data or adds syntheticdata to expand the size and diversity of a dataset. It po-tentially improves the generalization of models since it cansimulate new scenarios that did not appear in the trainingdata to some extent, thereby reducing the risk of overfitting.\nTraditional Random Augmentation. Data augmentationtechniques have been widely applied in the training of tra-ditional vision and text models, such as random cropping,flipping, scaling, color transformation for vision tasks, andrandom deletion and swapping for text tasks, to enhancethe generalization of models. MLLMs also benefit fromthese simple yet effective techniques, e.g., random resizedcropping and horizontal flipping with certain probabilitiesare applied during pretraining of the vision encoders [80],pretraining of the projectors that bridge the vision encodersand LLMs [81], [82], character, word, and sentence-level textaugmentation are applied for visual instruction tuning [83],"}, {"title": "3.1.3 Data Diversity", "content": "Diverse data help MLLMs perform well across differentcontexts and situations while reducing bias. Existing MLLMworks mainly focus on data diversity for single-modalityperception abilities and cross-modality cognition abilities.\nFor Single-Modality Perception Abilities. By expand-ing the diversity of data sources, the capabilities of founda-tion models can be significantly enhanced. This improvesthe abilities of encoders for other modalities in MLLMs,thereby enhancing the perception ability of MLLMs forindividual modalities. It is experimentally demonstratedthat Flamingo [90] trained with a mixture of complemen-tary large-scale multi-modal datasets performs significantlybetter than the models trained on any single dataset. Specif-ically, removing the interleaved or conventional pairedimage-text datasets will result in varying degrees of accu-racy reduction, emphasizing the importance of diversifiedtypes of datasets [90]. It is also found that when removingthe top-frequency concepts in datasets, the performanceof trained models will severely decrease [11]. By usingmultiple datasets for audio-text pretraining, the accuracyof pretraining can be improved compared to most studiesthat train retrieval systems using a single audio captiondataset [58]. The results presented in DataComp competi-tion's \"Bring Your Own Data\" track also demonstrate thebenefits of using more diverse data sources for image-textpretraining [15].\nFor Cross-Modality Cognition Abilities. Different lev-els of data diversity often lead to variations in the per-formance of MLLMs to leverage the knowledge to com-bine, and process information from different modalities[91]. By expanding the coverage of tasks and data, theimprovements of SPHINX-Plus over SPHINX indicate theimportance of data diversity to the performance of MLLMs[92]. The diversity of the data pool also contributes to theexcellent performance of Cambrian-1 [93]. The automaticconstruction of multi-modal datasets also needs attentionon diversity. Considering the benefits of data diversity,when automatically constructing the text-image fine-tuningdataset, VisionPrefer, not only a prompt pool is created,but a text-image model pool is also established to increasedata diversity [70]. The diversity of demonstration examplescan also improve the effectiveness of ICL, with multipledemonstrations from different groups as ICL examples, thereasoning ability could be improved [94]. Diversity alsoholds significant importance in MLLM evaluation. Takingcharted data as an example, considering the diversity ofdifferent chart types and downstream tasks, when construct-ing benchmarks for chart understanding, it is essential toenhance the diversity of styles, length, and task-wise tokendistribution [95]."}, {"title": "3.2 For Scaling Effectiveness of MLLMs: Better Subsets", "content": "With the data-centric approaches summarized in Section3.1, we can expand the scale of training data, thereby ob-taining the massive data required by training MLLMs atscale. However, simply increasing the amount of data isnot economical, since the increase in model size usuallycomes at the cost of higher computational expense [26].It is indicated that only exponential data growth can leadto linear performance improvements of MLLMs [11]. Thus,it is important to deal with the astonishing computationalexpenses incurred by larger-scale data/models [13].\nIn addition to the computational expense of MLLMtraining, an indiscriminate focus on increasing data quantityis not an effective development strategy, since some low-quality or even poisoned data samples may jeopardize thetraining effectiveness of MLLMs. As reported by [15], [96],[97], a well-designed data filtering/selection strategy canlead to better-performing models with a smaller amount ofdata or token count during training.\nIn this subsection, the computational expense and per-formance when training models on large-scale data arereferred to as the scale effectiveness of multi-modal data. Itfocuses on how to select and orchestrate the training data toachieve higher token efficiency or better performance. Therelated data-centric approaches include: data condensation(Section 3.2.1), data mixture (Section 3.2.2), data packing(Section 3.2.3) and cross-modal alignment (Section 3.2.4)."}, {"title": "3.2.1 Data Condensation", "content": "Data condensation contains a series of data-centric ap-proaches that perform pretraining or fine-tuning on a propersubset strategically selected from the original dataset", "categories": "data deduplication"}, {"title": "The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective", "authors": ["Zhen Qin", "Daoyuan Chen", "Wenhao Zhang", "Liuyi Yao", "Yilun Huang", "Bolin Ding", "Yaliang Li", "Shuiguang Deng"], "abstract": "The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMS (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs, on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stage of MLLMs can specific data-centric approaches be employed to enhance which capabilities, and 2) by utilizing which capabilities and acting as which roles can models contribute to multi-modal data. To promote the data-model co-development for MLLM community, we systematically review existing works related to MLLMs from the data-model co-development perspective. A regularly maintained project associated with this survey is accessible at https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.", "sections": [{"title": "1 INTRODUCTION", "content": "LPRARGE language models (LLMs) have demonstrated im-pressive performances across a wide range of tasks inrecent years, with their associated technologies making sig-nificant advancements. Since human senses are not limitedto the textual modality, multi-modal LLMs (MLLMs) havecome into view, such as Gemini-1.5 [1] and Sora [2] thatare capable of processing inputs or outputs in modalitiesbeyond text, and GPT-40 [3] and NEXT-GPT [4] that caneven interact between multiple modalities in both input andoutput. MLLMs have been receiving widespread attentionin the past two years. As illustrated in Fig. 1, since thebeginning of 2023, research related to MLLMs has beenemerging with an increasing speed.\nThe outstanding performance of MLLMs stems fromthe emergent abilities of LLMs in solving a series of tasksbrought by the scaling up in the number of parameters [5].Many works indicate that scaling up the model size needsto be complemented by even more massive amounts of data[6], [7], [8], such as scaling law [9], [10]. Specifically, itis indicated that multi-modal models require exponentiallymore data to achieve linear zero-shot improvements indownstream tasks [11]. In light of this, a series of workshave shifted the focus from merely model architectures andtraining techniques to data-centric approaches that focuson the curation of high-quality data [12], [13], [14], [15],[16], [17], to provide data basis for further unlocking thepotential of large models. From Figure 1, among existingpapers focusing on MLLMs, those closely related to data-centric approaches likewise exhibit a strong growth trendon the count and occupy a significantly important portion.As numerous technical works related to MLLMS con-"}, {"title": "2 PRELIMINARY", "content": "2.1 Background\nData-model co-development for MLLMs attempts to im-prove model performance while leveraging this model tooptimize the data, with the final objective aimed at a well-performing MLLM. It is characterized by dynamic trainingdata for a dynamic model [42]. In this paper, we focus onthe data-model co-development of a diverse range of multi-modal generation models that are capable of generatingoutputs in different modalities from the input or in moremodalities than the input, such as image-to-text dialog [43]and text-to-image synthesis [44]. Given the current lackof a formal definition for data-model co-development, weattempt to provide a formal definition with a generationmodel as an example before reviewing the existing works,to facilitate understanding.\nDefinition 1. (Data-Model Co-Development for GenerationModel). Let 1) $P_{data}(u)$ denote an ideal distribution thateach real-world data $u$ follows, where $u$ is composed bya context $c$ and a response $r$, denoted by $u = \\{c, r\\}$, 2) $w$be the generation model to be trained, which generates$u'$ given a context $c$, 3) $D = \\{u_1, u_2, ...\\}$ be the datasetto train $w$, 4) $D^*$ denote the ideally optimal $D$, and5) $Q(\\cdot, \\cdot)$ be the function that quantifies the similaritybetween two data samples, with larger values indicatinghigher similarity, data-model co-development can beformalized as a bi-level optimization problem, as\n$\\max_{W} E_{\\{c,r\\}\\in D^*} Q(r, w(c)),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)$\ns.t. $D^* \\in \\underset{D}{arg \\min} E_{u \\sim P_{data}(u), x=\\{c,r\\} \\in D} Q(u,x). $\t\t\t\t\t\t\t\t(2)\nEq. (2) aims at creating/optimizing the data for training w.Eq. (1) is the final objective that teaches w to produce datathat resembles real-world data. Data-model co-developmentcan be classified into two paradigms based on the tools usedfor optimizing D:\n1) (Self-Boosted Paradigm). The model $w$ to be trainedis also used to improve dataset D, where Eq. (2) andEq. (1) are usually optimized alternatively. In thisparadigm, Eq. (2) can be further refined as $D^* \\in\\underset{D}{arg \\min} E_{u \\sim P_{data}(u), \\forall x=\\{c,r\\} \\in D} Q(u, x)$.\n2) (Externally-Boosted Paradigm). As a fallbackalternative, D can be created/optimized with a well-trained model $w^*$ such as GPT-4V, or even humanefforts. Accordingly, we can rewrite Eq. (2) as $D^* \\in\\underset{D}{arg \\min} E_{u \\sim P_{data} (u), \\forall x=\\{c,r\\} \\in D} Q(u, x)$,\tor $D^* \\in\\underset{D}{arg \\min} E_{human} E_{u \\sim P_{data}(u), \\forall x=\\{c,r\\} \\in D} Q(u, x)$,respectively.\nFrom our investigations, the self-boosted paradigm has beenproven to be effective in some existing works for uni-modalLLMs [42], [45], but still lacks investigation for MLLMs.\nData-model co-development is promising as increasingattention shifts towards data-centric approaches, where dataoften serves as the primary variable, rather than merelyfocusing on model architectures [12], [13], [14], [17], [35],[46]. As MLLMs require increasingly large volumes of data,models are gradually used to assist or directly build thedata samples. Thus, the development of data and mod-els have become interdependent and inseparable: massiveand high-quality data can lead to well-performing MLLMs,and in turn, well-performing MLLMs can help constructmore high-quality data. Therefore, it is necessary to under-stand how data approaches enhance specific capabilities ofMLLMs, and how MLLMs assist in data approaches, therebyadvancing the data-model co-development for MLLMs.\nMLLM Architecture. We briefly introduce MLLM ar-chitecture for ease of understanding. An MLLM usuallycontains: 1) a pretrained LLM such as LLAMA [47]; 2) oneor more foundation models (encoders) to encode data fromnon-text modalities, such as ViT [48] and CLIP [49]; and3) one or more projectors to align the encoded featuresof non-text modalities with the feature space of pretrainedLLMs. With these components, MLLMs can generate textualresponses to multi-modal inputs [28]. Based on this, by in-corporating signal tokens and attaching 4) modality-specificgenerators such as Stable Diffusion [44], MLLMs canoutput contents in various modalities [4], [8]. These com-ponents of MLLMs may be trained in different steps [50]and may require different types of datasets [43], [51]. Notethat this survey does not focus on the pretraining of LLMs,which we refer to existing surveys summarized in Table 1."}, {"title": "2.2 Taxonomy", "content": "The taxonomy and the relationships between the variousitems are illustrated in Fig. 2. According to our investi-gations, the contributions of the data to models and thecontributions of models to the data can each be categorizedinto two major types. The data contributions to modelsare organized in an objective-driven manner and the order ofthe technical stages of MLLM development. At first, to elicitvarious general abilities from MLLMs, it is crucial to providemore and higher-quality data for MLLM training, aiming atthe scaling of MLLMs (Section 3), where some of the relatedworks focus on providing more data to scale up MLLMs(Section 3.1), while other efforts enhance the effectiveness ofscaling by improving data quality and organizing the dataappropriately (Section 3.2). With data-centric approaches forscaling MLLMs, we can obtain MLLMs with strong foun-dational capabilities. Subsequently, a series of data-centricefforts focus on enhancing the usability of MLLMs fromvarious aspects, including the instruction responsiveness(Section 4.1), reasoning ability (Section 4.2), ethics (Section4.3) and evaluations (Section 4.4).\nAs mentioned in Section 1, with datasets continuingto grow in size, their curation increasingly relies on well-trained MLLMs or their components such as LLMs andfoundation models to reduce the burden on human labor.We review a series of data-centric approaches co-pilotedwith MLLMs or their components, and divide them intotwo main categories, organized according to the level of intel-ligence required for the models to replace human work, i.e.,1) for synthesis of data, where models directly participate inthe data construction to alleviate repetitive tasks for humansby acting as a data creator (Section 5.1), data mapper (Sec-tion 5.2), data filter (Section 5.3), and data evaluator (Section5.4); and 2) for insight into data, where models perform asdata scientists to provide some insights on multi-modal databy acting as a navigator (Section 6.1), analyzer (Section 6.3),extractor (Section 6.2), and visualizer (Section 6.4).\nBased on these investigations, we summarize the pub-"}, {"title": "2.3 Differences from Related Surveys", "content": "The surge in popularity of MLLMs has also prompted manyresearchers to dedicate efforts to cataloging existing works.Existing surveys on MLLMs mainly focus on model-centricperspectives, and they construct taxonomies: 1) based ontraining techniques that highlight the training stage andcorresponding algorithms [18], [19], [20], [21], [22], [23], [24],[25], [26]; 2) based on MLLM architectures which highlightthe architectural components of MLLMs [19], [20], [22], [23],[26], [27], [28], [29], [30], [31]; 3) based on MLLM capabilitieswhich highlight reasoning abilities of MLLMs [21], [28], [32]and applications of MLLMs [19], [33]; or 4) based on MLLMsystems that focus on the key considerations and applica-tions in MLLM system design [20], [34]. These model-centricMLLM surveys have facilitated the development of MLLMs,yet have not given dedicated consideration to data.\nAs the importance of data for LLMs gradually gainswidespread attention [14], some data-centric surveys foruni-modal LLMs have emerged. They summarize existingworks with the taxonomy: 1) based on LLM data pipelinewhich highlight data approaches for LLMs across stagesin the data pipeline [12], [37], [38]; 2) based on trainingstages of LLMs which clarify data-centric approaches foreach training stage [39]; or 3) based on the adopted tech-niques which clarify foundation techniques for specific data-centric approaches [40], [41] or enumerate data-efficient ap-"}, {"title": "3 MULTI-MODAL DATA CONTRIBUTIONS FOR MLLMS: SCALING", "content": "Building MLLMs with good performance requires large-scale and high-quality multi-modal data. This section sum-marizes existing data-centric works that help constructlarge-scale MLLMs involving providing datasets, organizedaccording to the logical sequence of top-level design and iterativeoptimization. To build large-scale and high-quality multi-modal datasets, we need first to scale up the number ofsamples in multi-modal datasets (Section 3.1), followed byenhancing the quality of the datasets, i.e., improving thescaling effectiveness of datasets (Section 3.2). The organiza-tion of this section is illustrated in Fig. 3. After introducingexisting works related to this section, a brief summarizationand discussion are provided in Section 3.3.\n3.1 For Scaling Up of MLLMs: Larger Datasets\nThe excellent performance of MLLMs benefits from a largernumber of parameters, especially when the size of modelparameters reaches a certain level, abilities that traditionalmulti-modal models lack begin to emerge, such as OCR-freemath reasoning [28]. Researches indicate that the growthin the number of parameters of LLMs necessitates a cor-responding increase in the volume of data to support it[9], [52]. Compared to uni-modal LLMs, MLLMs exhibit asignificant extension in the feature space of both inputs andoutputs. Intuitively, such an extension in the feature spacefurther emphasizes the importance of extensive trainingdata. As have been analyzed, multi-modal models requireexponentially more data to achieve linear improvementsindownstream zero-shot performance [11]. Therefore, thedevelopment of high-performance MLLMs benefits frommassive, information-rich, and diverse training data.\nThis subsection introduces data-centric works facilitat-ing scaling up MLLMs, including data acquisition (Section3.1.1), data augmentation (Section 3.1.2) and the works thatfocus on data diversity (Section 3.1.3)."}, {"title": "3.1.1 Data Acquisition", "content": "Data acquisition, a.k.a. data collection, is the process ofacquiring raw data to fundamentally support large-scaledatasets. Existing works adopt diversified data sources, in-cluding: leveraging massive amounts of native data crawledfrom the web [11], [15], [49], [53], [54], [55], taking subsets ofexisting datasets or merging multiple existing datasets [50],[53], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66],building datasets with human efforts [4], [55], [59], [67], [68],[69], employing well-trained MLLMs such as GPT-4V forautomatic or semi-automatic data creation [4], [43], [54], [56],[63], [64], [67], [69], [70], [71], [72], or obtaining simulationdata with simulators [73].\nAs introduced in Section 2.1, the components of MLLMsmay be separately trained in different stages 1, including: 1)pretraining the encoders and/or generators for basic under-standing and/or generation, 2) pretraining the projector(s)for feature space alignment, and 3) fine-tuning the entireor parts of the MLLM to promote task responsiveness ordownstream-task performance. Generally, these stages mayconsume different types of datasets [43], [51]\nFor Encoders and Decoders. The pretraining of theencoders typically relies on massive any-text pairs, whichare usually easy to obtain by crawling from the web ormerging existing datasets. WebImageText [49] collects 400million image-text pairs collected from the web to train thevisual encoder with natural language supervision, whichbenefits a series of MLLMs such as LLAVA [50]. Similarly,LAION-Audio-630K [57] contains 630K audio-text pairscollected from 8 websites, facilitating the pretraining of au-dio and text encoders. By discarding the burdensome filter-ing mechanisms that come with Conceptual Captionsdataset [74], image-text pairs are expanded by two orderof magnitude, effectively improving the performance ofALIGN, thereby demonstrating the significance of scalingup datasets [75]. To provide Chinese understanding, CLIPmodel is trained on billions of image-text pairs obtained bypurchasing and open sources [66]\nFor Projectors. Pretraining of the projectors for fea-ture alignment usually consumes the instruction-followingdatasets converted from existing any-text pairs. To train theprojector in LLAVA [50], 595K image-text pairs are expandedto instruction styles by simulating conversations betweena human and an assistant, with native captions treatedas responses. Considering the limited cross-modal associ-ation in image-text pairs contained in COCO [76], GPT-4Vis prompted to generate fine-grained captions on imagesselected from Vision-FLAN [77] and LAION [78], whereththe detailed information such as background elements andnotable features are required to boost the cross-modal asso-ciation between images and captions [43]. Video-ChatGPT[67] bridges video features to the embedding space of anLLM with a linear layer, which is trained on a subsetof ActivityNet-200 [79], where human annotators areemployed to enrich the captions with contextual details suchas physical appearance.\nFor Fine-tuning. With the above works, we can obtainMLLMs with fundamental understanding and generation-"}, {"title": "3.1.2 Data Augmentation", "content": "Constructing large-scale datasets is often expensive whileperforming data augmentation based on existing datasetsis usually more cost-effective. Data augmentation applies aseries of transformations to existing data or adds syntheticdata to expand the size and diversity of a dataset. It po-tentially improves the generalization of models since it cansimulate new scenarios that did not appear in the trainingdata to some extent, thereby reducing the risk of overfitting.\nTraditional Random Augmentation. Data augmentationtechniques have been widely applied in the training of tra-ditional vision and text models, such as random cropping,flipping, scaling, color transformation for vision tasks, andrandom deletion and swapping for text tasks, to enhancethe generalization of models. MLLMs also benefit fromthese simple yet effective techniques, e.g., random resizedcropping and horizontal flipping with certain probabilitiesare applied during pretraining of the vision encoders [80],pretraining of the projectors that bridge the vision encodersand LLMs [81], [82], character, word, and sentence-level textaugmentation are applied for visual instruction tuning [83],"}, {"title": "3.1.3 Data Diversity", "content": "Diverse data help MLLMs perform well across differentcontexts and situations while reducing bias. Existing MLLMworks mainly focus on data diversity for single-modalityperception abilities and cross-modality cognition abilities.\nFor Single-Modality Perception Abilities. By expand-ing the diversity of data sources, the capabilities of founda-tion models can be significantly enhanced. This improvesthe abilities of encoders for other modalities in MLLMs,thereby enhancing the perception ability of MLLMs forindividual modalities. It is experimentally demonstratedthat Flamingo [90] trained with a mixture of complemen-tary large-scale multi-modal datasets performs significantlybetter than the models trained on any single dataset. Specif-ically, removing the interleaved or conventional pairedimage-text datasets will result in varying degrees of accu-racy reduction, emphasizing the importance of diversifiedtypes of datasets [90]. It is also found that when removingthe top-frequency concepts in datasets, the performanceof trained models will severely decrease [11]. By usingmultiple datasets for audio-text pretraining, the accuracyof pretraining can be improved compared to most studiesthat train retrieval systems using a single audio captiondataset [58]. The results presented in DataComp competi-tion's \"Bring Your Own Data\" track also demonstrate thebenefits of using more diverse data sources for image-textpretraining [15].\nFor Cross-Modality Cognition Abilities. Different lev-els of data diversity often lead to variations in the per-formance of MLLMs to leverage the knowledge to com-bine, and process information from different modalities[91]. By expanding the coverage of tasks and data, theimprovements of SPHINX-Plus over SPHINX indicate theimportance of data diversity to the performance of MLLMs[92]. The diversity of the data pool also contributes to theexcellent performance of Cambrian-1 [93]. The automaticconstruction of multi-modal datasets also needs attentionon diversity. Considering the benefits of data diversity,when automatically constructing the text-image fine-tuningdataset, VisionPrefer, not only a prompt pool is created,but a text-image model pool is also established to increasedata diversity [70]. The diversity of demonstration examplescan also improve the effectiveness of ICL, with multipledemonstrations from different groups as ICL examples, thereasoning ability could be improved [94]. Diversity alsoholds significant importance in MLLM evaluation. Takingcharted data as an example, considering the diversity ofdifferent chart types and downstream tasks, when construct-ing benchmarks for chart understanding, it is essential toenhance the diversity of styles, length, and task-wise tokendistribution [95]."}, {"title": "3.2 For Scaling Effectiveness of MLLMs: Better Subsets", "content": "With the data-centric approaches summarized in Section3.1, we can expand the scale of training data, thereby ob-taining the massive data required by training MLLMs atscale. However, simply increasing the amount of data isnot economical, since the increase in model size usuallycomes at the cost of higher computational expense [26].It is indicated that only exponential data growth can leadto linear performance improvements of MLLMs [11]. Thus,it is important to deal with the astonishing computationalexpenses incurred by larger-scale data/models [13].\nIn addition to the computational expense of MLLMtraining, an indiscriminate focus on increasing data quantityis not an effective development strategy, since some low-quality or even poisoned data samples may jeopardize thetraining effectiveness of MLLMs. As reported by [15], [96],[97], a well-designed data filtering/selection strategy canlead to better-performing models with a smaller amount ofdata or token count during training.\nIn this subsection, the computational expense and per-formance when training models on large-scale data arereferred to as the scale effectiveness of multi-modal data. Itfocuses on how to select and orchestrate the training data toachieve higher token efficiency or better performance. Therelated data-centric approaches include: data condensation(Section 3.2.1), data mixture (Section 3.2.2), data packing(Section 3.2.3) and cross-modal alignment (Section 3.2.4)."}, {"title": "3.2.1 Data Condensation", "content": "Data condensation contains a series of data-centric ap-proaches that perform pretraining or fine-tuning on a propersubset strategically selected from the original dataset, con-sidering the limited computational budgets. Typically, mod-els trained on a high-quality subset can achieve compara-ble performance with greater token efficiency compared toMLLMs trained on the entire original dataset. In some cases,MLLMs trained on a refined dataset can even outperformthose trained on the entire dataset [96] since some low-quality data samples not only consume additional tokensbut also negatively impact the MLLMs' performance. It istheoretically supported in active learning settings that withan effective selection strategy, the training with a subsetcontaining $n$ samples could outperform that on the originaldataset with $N$ samples ($N > n$) [98], where an intuitiveexplanation could be that the information is transmittedfrom the selection strategy to MLLMs by the selectionprocess. Besides, pruning redundant data can improve thescaling trend on some vision datasets [99], indicating thatdata condensation is important to improve the effectivenessof data scaling. We classify the relevant methods into threemain categories: data deduplication, low-quality data filteringand kernel set construction, where the first category mainlyfocuses on training efficiency and the latter two additionallyemphasize MLLM performance.\nData Deduplication. As the scale of datasets continuesto grow, the likelihood of having duplicate samples withinthe dataset increases. And when text-to-image MLLMs areemployed for dataset construction, training samples maybe directly copied [100]. Duplicated data not only wastesvaluable computational resources but also raises issues suchas concerns on copyright infringement [100]. Deduplicationtechniques have been studied for many years, which canbe implemented based on hash values [101] to search forexact duplicates, latent features to compute soft duplicates[102], [103], etc. However, this is a non-trivial issue withlarge-scale datasets, as the efficiency of querying and indexreconstruction must be taken into account. To filter outduplicated data samples as much as possible and therebyminimize the size of the final distinct dataset used fortraining, while maintaining query efficiency, the criteria fordetermining duplicates can be appropriately relaxed. Forexample, a contrastive feature compression technique hasbeen proposed to encode as much semantic information aspossible in the features while ensuring retrieval efficiency[100]. With this technique, about 700 million images in theLAION-2B dataset were identified as suspected duplicates.Also, an approach leveraging embeddings from pretrainedmodels to identify and then filter out data pairs that aresemantically similar, but not identical in terms of datarepresentation is proposed [104], which can effectively filterout about 50% of the samples in a subset of LAION-2B whilemaintaining the training performance.\nLow-Quality Data Filtering. Data filtering, a.k.a datacontinue generating json"}]}]}