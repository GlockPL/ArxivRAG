{"title": "Forecasting Live Chat Intent from Browsing History", "authors": ["Se-eun Yoon", "Ahmad Bin Rabiah", "Zaid Alibadi", "Surya Kallumadi", "Julian McAuley"], "abstract": "Customers reach out to online live chat agents with various intents, such as asking about product details or requesting a return. In this paper, we propose the problem of predicting user intent from browsing history and address it through a two-stage approach. The first stage classifies a user's browsing history into high-level intent categories. Here, we represent each browsing history as a text sequence of page attributes and use the ground-truth class labels to fine-tune pretrained Transformers. The second stage provides a large language model (LLM) with the browsing history and predicted intent class to generate fine-grained intents. For automatic evaluation, we use a separate LLM to judge the similarity between generated and ground-truth intents, which closely aligns with human judgments. Our two-stage approach yields significant performance gains compared to generating intents without the classification stage.", "sections": [{"title": "1 INTRODUCTION", "content": "Online business platforms often deploy live chat agents. These agents address a wide variety of user intents, ranging from item availability to returns. Even within the same broad topic, user intents can be highly specific: one may ask how to install a certain item; another may ask whether installation services are included with the purchase. Identifying each intent often requires several interactions in the chat interface, where users first navigate through a series of options to categorize their topic, and further clarify their needs once connected with a live agent.\nIn this paper, we investigate whether it is possible to predict user intent before users reach out to a live chat agent. Solving this problem offers practical benefits. The predicted intents can be used to automatically route users to specialized agents or prioritize wait times. With such applications in mind, we propose to predict user intent, specifically through users' browsing histories. Browsing histories contain signals of intent [6, 8]. Each page visit has attributes such as page content, which hint at a user's specific intent.\nOur problem is depicted in Figure 1. A user browses different types of pages and then requests a live chat agent. From the conversation, we derive two types of ground-truth intent: raw intent (or simply, intent) and intent class. An intent is the collection of a user's raw utterances in the chat and represents the precise reason the user is seeking help, e.g., 'Do you provide installation services for this ceiling fan?' The intent class is a coarse categorization of an intent, e.g., Installation. Our goal is to generate the raw intent, since it provides more detailed information such as the item in question and the specific issue being faced.\nWe propose a two-stage approach as shown in Figure 2. First, we train a classifier using the intent class labels, where we use pre-trained Transformers to capture the rich text semantics in browsing history. Particularly, we find that the fine-tuned Longformer [1] and our variant perform much better than larger text-to-text models (fine-tuned Flan-T5-Large and zero-shot GPT-3.5). Second, we use"}, {"title": "2 DATASET", "content": "Our dataset was acquired through a collaboration with a major U.S.-based online home improvements retailer, sampled from February 2024. There are 30,739 user sessions, with an average of 68 (\u00b1111) pages per session and 59 (\u00b175) words per raw intent.\nLive Chat Intents When a user clicks on the 'chat' button that appears in any page of the website, they are presented with options on their topic of inquiry that may guide them to an automated response. Still, a user may click on the 'chat with an associate' option for personalized assistance, which connects them to a human agent. We select the chat sessions where the user was connected to a human agent. Based on the initial option clicks and chat utterances, we assign each chat a single intent class among Installation (INS), Item availability (AVL), Price match (PRI), Repair/Warranty (WTY), and Return/Refund (RET). These classes cover a majority of intents. The full intent is often conveyed through multiple exchanges, e.g.,\nUser: Hi\nAgent: How may I help you?\nUser: Can I get a price match?\nAgent: Could you please provide the item number?\nUser: Item #123456\nWe concatenate the user utterances and use the resulting string as the raw intent (i.e., 'Hi, Can I get a price match?, Item #123456').\nBrowsing Histories Each chat session is preceded by the user's browsing history. We collect the sequence of web pages viewed by the user, each with attributes. A common attribute across all pages is the page type, which can be one of 66 categories. The following 9 page types have additional attributes: 'product' (product name);"}, {"title": "3 METHODS", "content": "For each user u, we have a browsing history represented by a sequence of pages $S^{u} = (p_{1}^{u}, p_{2}^{u},..., p_{|S^{u}|}^{u})$. Each page $p_{t}^{u}$ browsed by user u at time t is represented by an attribute dictionary $D^{p_{t}^{u}}$ of key-value pairs $\\{(k_{1}, v_{1}), (k_{2}, v_{2}), ..., (k_{l}, v_{l})\\}$, where k denotes the attribute name (e.g., page type) and u the corresponding value (e.g., product page). A user is also associated with a natural-language intent $I_{u}$ and its intent class $c_{u} = c(I_{u}) \\in C$, where C is the set of intent classes. Given the sequence $S^{u}$, the goal is to generate intent $\\hat{I}_{u}$ that is semantically similar to the true intent $I_{u}$. We use a similarity function $x : (I_{u}, \\hat{I}_{u}) \\rightarrow \\{0, 1\\}$, which returns 1 if the intent pairs are similar and 0 otherwise."}, {"title": "3.2 Two-Stage Intent Prediction", "content": "Figure 2 shows our proposed method, which consists of two intent prediction stages: classification and generation.\nIntent Classification The first stage learns a classifier f to pre-dict the intent class $c_{u}$ from the browsing history: $f : S^{u} \\rightarrow \\hat{c_{u}}$. Since each browsing history is represented by a sequence of dictio-naries, we build a model tailored to this structure, inspired by Li et al. [8]. Figure 3 illustrates this architecture, characterized by four embedding matrices: token embeddings $A \\in R^{V_{w}\\times d}$ represent to-kens (where $V_{w}$ is the vocabulary size), token position embeddings $B \\in R^{P\\times d}$ represent the token positions in a sequence (where p is the maximum number of tokens in a sequence), token type embed-dings $C \\in R^{3\\times d}$ indicate one of the three token types ([CLS], key, or value), and page position embeddings $D \\in R^{n\\times d}$ represent the position of pages in a sequence (where n is the maximum number of pages in a sequence). The embeddings are summed and fed into the transformer layer, where the representation $h_{[CLS]}$ of the [CLS] token is used for prediction through a linear projection layered fol-lowed by softmax. We refer to this model as Longformer+, since it extends the Longformer [1] architecture by adding token type and"}, {"title": "Intent Generation", "content": "The second stage instructs a natural language generator g to output intent $I_{u}$ from browsing history and the predicted intent class: $g : (S^{u}, \\hat{c_{u}}) \\rightarrow \\hat{I_{u}}$. Precisely, we instruct g to generate M intent candidates: $(\\hat{I_{1}}, \\hat{I_{2}}, ..., \\hat{I_{M}})$. We use GPT-3.5 as our generator to leverage its commonsense reasoning capabilities [2, 10]. For instance, if a user visited a product page and a checkout page, the user may be considering purchasing this item. The predicted intent class $\\hat{c_{u}}$ obtained from the trained classifier f provides further guidance based on learned patterns that may not be evident to zero-shot models. We use the following instruction: 'A customer browsed the following pages-Su-and reached out a chat agent for assistance. Possible topics are C, but $\\hat{c_{u}}$ is the most likely. Pretend to be this customer, and enumerate M questions (1., 2., ...) to ask the chat agent. Don't say anything else. Here, the browsing history Su is flattened with a <page> token (as in classification), and C is randomly shuffled to mitigate positional bias [24]. As baselines,"}, {"title": "3.3 Evaluation", "content": "We evaluate whether among the M generated intent candidates, there is one that is semantically similar to the true intent $I_{u}$. Doing so requires a similarity function x (see Section 3.1). While a human expert is the ideal judge, we want an inexpensive, automated alternative. Traditional count-based [12, 17] or embedding-based [18] similarity metrics would be unsuitable: (1) these metrics output scores on a scale, rather than binary values, (2) they do not handle 'dealbreakers' such as different item names, and (3) they may overlook the subtle but important difference in intents (e.g., ask-ing for installation manual versus asking for installation services). Hence, we propose a new evaluation scheme: use a large language model (LLM) as a similarity function. Particularly, we prompt GPT-4 with instructions (to determine if the two intents are similar), including demonstrations. We validate GPT-4 judgments by having human workers assess 200 intent pairs, achieving Cohen's kappa of 0.71, precision of 91.8%, and recall of 82.6%. These results suggest that the GPT-4 judgments are reliable and slightly less generous than humans in determining similarity."}, {"title": "4 RESULTS", "content": "Table 1 shows the classification results on the test set. We mea-sure precision and recall for each class and compute the weighted averages based on class proportion as overall performance. Long-former+ achieves the best overall results, with its precision showing"}, {"title": "4.2 Generation Results", "content": "Table 2 shows generation results, where Similar@m indicates the proportion of users where the model generated a similar intent within the top-m list. Our proposed method achieves rates of 0.2567 (m = 1) and 0.45 (m = 5), which are 93% and 31% gains over using no class information (Use None). Interestingly, using no class (Use None) performs better than using all classes (Use All), where the latter informs the model of the five possible classes. We observe that our method successfully generates fine-grained intents similar to the raw intents (examples in Table 3). Note that the model does not trivially repeat the given intent class (e.g., Item Availability \u2192 Is the item available?), but generates various inquiries related to the topic. The model often identifies the correct item, but it sometimes selects the wrong one (e.g., TH1234 instead of TH5678) from the browsing history. As future work, we plan to improve performance by training a component that selects items."}, {"title": "5 RELATED WORK", "content": "Intent prediction is an important problem in e-commerce, with downstream applications in user understanding [22], targeted mar-keting [9], and recommendation [11]. Existing work focuses on purchase intent [6, 19, 20, 22], where the goal is to classify whether a user would buy something or not. User sessions include clicks [19, 22], touch-interactive behaviors [6], and event types [20]. Some work predicts the specific item a user will purchase after identi-fying purchase intent [21]. Different from these work, our paper generates fine-grained intent in natural language; our prediction space is not a finite set of classes or items, but free-form text.\nA related task is recommendation, where intent is not the direct output but modeled through model architecture [5], pattern min-ing [13], and fine-grained signals [14]. Another line of work infers intent in dialogs [3, 7] without using user behaviors prior to chat."}, {"title": "6 FURTHER DISCUSSION", "content": "This paper explores a novel problem of using browsing history to forecast why the customer needs a live chat agent. Our problem introduces new application possibilities, such as recommending ac-tions for users, routing users to specialized agents, and prioritizing intents when agent resources are limited. We plan to enhance model performance by (1) adding detailed user interactions, (2) creating a component to select items (see Section 4.2), and (3) integrating external knowledge such as FAQs or seasonal sales."}]}