{"title": "Characterizing Photorealism and Artifacts in Diffusion Model-Generated Images", "authors": ["Negar Kamali", "Karyn Nakamura", "Aakriti Kumar", "Angelos Chatzimparmpas", "Jessica Hullman", "Matthew Groh"], "abstract": "Diffusion model-generated images can appear indistinguishable from authentic photographs, but these images often contain artifacts and implausibilities that reveal their AI-generated provenance. Given the challenge to public trust in media posed by photorealistic AI-generated images, we conducted a large-scale experiment measuring human detection accuracy on 450 diffusion-model generated images and 149 real images. Based on collecting 749,828 observations and 34,675 comments from 50,444 participants, we find that scene complexity of an image, artifact types within an image, display time of an image, and human curation of AI-generated images all play significant roles in how accurately people distinguish real from Al-generated images. Additionally, we propose a taxonomy characterizing artifacts often appearing in images generated by diffusion models. Our empirical observations and taxonomy offer nuanced insights into the capabilities and limitations of diffusion models to generate photorealistic images in 2024.", "sections": [{"title": "1 Introduction", "content": "The capabilities of diffusion models to generate photorealistic images of people are beginning to contribute to disinformation and erode trust in the media [17]. For example, in March 2023, realistic Al-generated images of world leaders went viral on social media, showing Pope Francis wearing what appeared to be a designer puffer jacket, Donald Trump getting arrested, and Vladimir Putin standing behind prison bars [2]. These exemplar images may appear both provocative and realistic at first glance, but they are far from perfectly photorealistic; they contain distortions of hands and faces, implausible grasping of objects, and shadows that do not match the objects that appear to cast them. These distortions are not unique to these particular images but are pervasive in diffusion model-generated images produced by text-to-image tools such as Midjourney (the source of these fake images of world leaders), Stable Diffusion by Stability AI, and Firefly by Adobe [40]. While it is possible to generate images that seem indistinguishable from photographs, many diffusion model-generated images still leave behind human-identifiable artifacts. This raises an open research question for human-computer interaction: What drives human perception of photorealism in images generated by diffusion models?\nWe approach this question by conducting a large-scale, online experiment where we collect data on human participants' accuracy in identifying whether images are AI-generated or real. We measure photorealism following a psychophysics approach [95] that defines photorealism as human discrimination performance. Accuracy scores are inversely associated with photorealism: a high accuracy score indicates low photorealism, whereas a low accuracy score indicates high photorealism. By defining photorealism based on discrimination performance, we avoid the speculation and subjectivity of asking participants questions like \"Is the image photorealistic?\" [47] and \"Could these images be taken with a camera?\" [86].\nBy comparing human detection accuracy across a diverse set of images, we can evaluate the contexts that influence the continuum of photorealism. Past research has demonstrated that GAN(Generative Adversarial Networks)-generated human portraits can be indistinguishable from real portrait images [60]. However, open questions on context remain: How often do portrait images appear indistinguishable from real portraits? How does scene complexity across styles of photographic portraiture (e.g., single-subject close-up, single-subject full body, posed group, and candid group) influence aggregate measures of photorealism? How accurately can people identify real images across scene complexities? What kind of artifacts arise in diffusion model-generated images and how are the"}, {"title": "2 Background", "content": "2.1 Limitations of machine learning approaches to detect AI-generated images\nMachine learning models for detecting AI-generated images are brittle and lack robustness to simple data transformations. Corvi et al. [10] compare four different machine learning approaches to deepfake detection and demonstrate that recropping and compression - simple modifications common on social media \u2013 lead to drops in accuracy such that the classifiers are nearly just as good as random guessing. Dong et al. [13] reveal the ease with which spectral artifacts used in the identification of GAN-generated images can be mitigated via blurring and resizing, demonstrating a noticeable decrease in accuracy under basic modifications. Cozzolino et al. [11] demonstrate that post-processing images by random-cropping, resizing, and compression lead to a drop in AI-generated image detection from 90% accuracy to 85% accuracy. The fundamental problem is that machine learning models for deepfake detection lack robustness to context shift, out-of-distribution data and adversarial perturbations [23, 28, 35, 80].\nHow an image is generated influences the ability of deepfake detection classifiers to accurately identify it as AI-generated. Classifiers trained to detect GAN-generated images tend to fail to detect diffusion model-generated images. For example, the approach to detecting GAN-generated images based on frequency spectra [6, 14, 21, 54, 91, 94] and inconsistencies in head poses and facial landmark positions [58, 88, 89], do not generalize to detecting images generated by diffusion models [62]. GAN-trained detection models miss these patterns because they have learned patterns for identifying GAN-generated images [70, 79]. Likewise, it is possible to learn the statistical regularities in diffusion model-generated images but these regularities are not invariant to image post-processing. [4, 52, 82, 85, 87].\nMoreover, machine learning models' lack of robustness for detection is exacerbated by the changing architectures of generative AI models [48, 57]. Vision transformers [62, 68] and multi-architecture training [15, 39, 67] show promise for enhancing the detection of AI-generated images, but adversarial attacks and large architectural changes in generative models continue to affect robustness of detection.\nFigure 1 highlights the increasing complexity of AI-generated images over the past decade. The changing architectures and increasing photorealism pose a challenge for both humans and machines to distinguish real from AI-generated images. However, humans and machines are fundamentally different. For example, humans can critically reason about an image's elements and its context [81]. On the other hand, machine learning classifiers for detecting AI-generated images often oversimplify image authenticity as a question of real versus fake and ignore the critical reasoning about component parts and sub-questions that an ordinary person or digital forensics expert may consider when evaluating an image's authenticity [37]."}, {"title": "2.2 Human perception and evaluation of AI-generated media", "content": "In response to the increasing realism of AI-generated media, researchers have been examining the degree to which humans can distinguish between authentic and AI-generated media. For example, researchers found that GAN-generated images of faces are indistinguishable from real face portraits [44, 60]. However, for video deepfakes, humans are much better than random guessing [24], which may in part be due to humans' specialized ability to process the temporal elements of faces [24, 73]. Researchers found that text-to-speech voices were rated as lower in quality and clarity than human voices in 2020 [9] but have reached the point where research participants cannot tell the difference between short 20-second recordings of Al-generated voices and authentically recorded voices [5].\nRecent research has identified specific cues and heuristics that people use to evaluate AI-generated media. For example, cues such as recording settings in the detection of text-to-speech audio [30] and speaking patterns in political deepfake videos [25]. However, two studies found that participants rarely attributed their judgments to specific visual features [29, 84], and in one of these deepfake studies, researchers found that participants are noticing the artifacts but rarely linking these to manipulation [84]. With respect to AI-generated text, research has highlighted that people tend to use flawed heuristics when attempting to distinguish AI-generated text from human-written text, like associating grammatical errors with AI-generation [38].\nSocial context also plays a significant role in both what diffusion models generate [50] and how people form beliefs about AI-generated images and their content. For example, researchers have found detection ability is influenced by shared identity between the viewer and subject of the content [56]. Furthermore, researchers have found that white AI-generated faces were disproportionately judged as human more frequently than their real counterparts [55]. GAN-generated faces in portrait images were often perceived as more trustworthy than real faces [60], and as a result, people were less likely to question their authenticity [44]. In instances where AI-generated images are linked to misinformation, researchers find that labeling AI-generated images and the associated content as \"potentially misleading\" instead of simply \"AI-generated\" had a stronger influence on curtailing participants' self-reported intentions to share misinformation [16, 83].\nResearchers have approached a number of methods for measuring photorealism perceived by humans. For example, prior research"}, {"title": "2.3 Categorizing artifacts and implausibilities in diffusion model-generated images", "content": "Previous research on earlier versions of diffusion models categorized the kinds of qualitative failures of diffusion model-generated images as distorted body parts, impossible geometry, physics violations, illogical relationships in a scene, and noise [7]. In addition to obvious issues with hands, feet, eyes, and teeth, research at the intersection of digital foresnics and AI-generated images shows details such as corneal reflections [34] and irregular pupil shapes [26] can also be artifacts. Likewise, violations of physics like implausible shadows, lighting, and perspective errors [19, 20, 72] often occur in diffusion model generated images that otherwise appear photorealistic."}, {"title": "3 Methods", "content": "We develop a detailed taxonomy of visual features, qualities, and artifacts that offer cues that an image is AI-generated or not following a three-step process based on the taxonomy development method proposed by Nickerson et al. [59]. We began by drafting an initial version of the taxonomy based on a review of visual features previously identified in AI literacy resources, academic literature, and online discussions (Section 3.1). We then employed two parallel processes to develop the taxonomy: iteratively generating and curating a dataset of 599 images to showcase the taxonomy artifacts (Section 3.2) and conducting an online, crowdsourced experiment using these curated images to assess human detection ability (Section 3.3). Third, we integrated participant feedback-both accuracy"}, {"title": "3.1 Initializing the Taxonomy", "content": "In addition to reviewing academic literature discussed in Section 2, we surveyed traditional and social media discussions about distinguishing AI-generated images. These included Al literacy resources on how to identify AI-generated content in media (see Figure S1), online discussions of AI-generated images in response to viral deepfakes, and popular posts discussing photorealism on online forums for AI image creators (Reddit channels such as r/Midjourney and r/StableDiffusion) to initialize the taxonomy. These sources highlighted several visual cues, including (1) anatomical implausibilities such as pupil dilation and misaligned eyes [61, 74, 76], teeth [12], hair [12, 61], fingers, and alignment of body parts [12, 61, 76]; (2) irregular reflections and shadows [12, 18, 74]; (3) unnatural color balances [12, 61]; (4) a mismatch in textures and styles within an image [12, 18, 61]; (5) garbled or nonsensical text [76]; (6) photoshoot-like perfection and overly cinematic scenarios [77].\nWhile some prior research has suggested that Al-generated face images can be indistinguishable from real ones [35, 60], more complex scenes, such as group photos have not been thoroughly explored. We address this by introducing a detailed categorization of scene complexity across all images. We identified four distinct scene types that capture varying levels of detail within an image:\nPortraits (Single-Subject Close-Up): An image featuring a single individual, typically focusing on the face and torso. The individual is the primary focus, often set against a blurred or minimal background. Portraits have relatively low scene complexity.\nFull-Figure (Single-Subject Full Body): An image featuring a single individual whose entire body is visible along with the surrounding environment. These images exhibit moderate scene complexity, as they include more details than portraits, such as the person's posture and interaction with their setting.\nPosed Group: An image featuring multiple people posing for the camera in a structured manner. These images involve higher scene complexity due to the presence of multiple subjects, their interactions, and the added challenge of capturing each person accurately.\nCandid Group: An image of multiple people captured in candid moments. These images often feature intricate interactions between people and their environments, representing the highest level of scene complexity."}, {"title": "3.2 Stimuli Generation and Curation", "content": "We created a dataset of 599 images. This image set included 149 real photographs curated from the internet, from which we derived the scenarios for 450 images that we generated using AI. Of the"}, {"title": "3.2.1 Curating Real Photographs", "content": "We sourced real photographs from online platforms, selecting them to represent diverse scenarios (e.g., diverse cultural settings with celebrity and non-celebrity figures engaging in common and uncommon activities) across the four dimensions of scene complexity Section 3.1. We established these categories to curate a diverse range of real photographs and ensure our dataset accurately captures how the features in our taxonomy may manifest and be perceived in both real and AI-generated images. We verified that these images were real photographs by confirming details like the creation date, photographer, and publisher. We include a complete list of image sources and verification details in the following link: https://github.com/negarkamali/Replication-for-Characterizing-Photorealism-2025/. We used these real images to inform the prompts to generate images using AI tools."}, {"title": "3.2.2 Generating Images using Al tools", "content": "Based on our curated set of real images, we generated images in Midjourney V5 and V6, Adobe Firefly Image 2, and Stable Diffusion to depict similar scenarios. In Midjourney and Firefly, we started the image generation process by creating a simple prompt describing the scenario. We then progressively refined the prompts by adding details about the quality of the image using keywords known to enhance image quality and resolution from the sources mentioned in Section 3.1. Our prompts followed the basic structure of: \"[Subject description] [action or pose], [context or setting description], [clothing or appearance details], [image quality and style attributes], [camera or film type if applicable].\" If the images were insufficiently realistic, then further details were added to the end of the above prompt such as: [specific details unique to the scenario], ['high resolution', 'hyper-realistic', 'megapixel', etc.]. The sequence of images in Figure 3 shows the progression of a prompt and the resulting image qualities as more details and keywords are added in Midjourney V6.\nWe also generated images inspired by real scenes of human interactions found in publicly available news sources and online media, ensuring they maintained a similar context and zoom level to real photographs. For example, we used a real reference image of a Ukrainian soldier getting married from New York Magazine [53].\nIn Stable Diffusion, we developed custom pipelines in order to generate images that were more realistic than the outputs of the original models. Using SD1.5 [71] and SDXL [66] as the base models, we used techniques such as merging fine-tuned portrait models and combining outputs of different models to reduce obvious artifacts and generate highly photorealistic images, particularly for portraits. We also experimented with generating the same poses in various styles in order to isolate the impact of certain categories"}, {"title": "3.3 Crowdsourced Experiment", "content": "3.3.1 Image Stimuli. Across the entire experiment timeline, we collected 749,828 responses to whether 1083 images are AI-generated or real from 50,444 participants. Across the experiment timeline, we added and removed stimuli for two reasons. First, we included higher quality and more diverse images over the course of the experiment as new tools for controlling diffusion models became available (e.g., ControlNets and LORAs), and we identified prompt engineering techniques for producing more photorealistic images. Second, we split the experiment into two phases based on how we selected the diffusion model-generated images. In the first and main phase of the experiment, the stimuli were 149 real photographs and 450 most photorealistic images that our research team could generate with diffusion models. By comparison, the 482 stimuli in the second phase were based on generating 11 or more images for each of the 39 text prompts without curation. This second phase enables us to identify the effect of human curation (the selection bias involved in our research team selecting the most photorealistic Al-generated images) relative to no human curation on how accurately participants can distinguish AI-generated images.\n3.3.2 Experimental Design. To ensure the quality of results, we implemented two measures. First, participants were shown an attention check image that was clearly AI-generated. Those who failed to identify this image correctly were excluded from the analysis. Second, we included an optional checkbox allowing participants to indicate if they recognized an image from outside the experiment, allowing us to filter out responses influenced by prior familiarity with the image.\nIn the initial version of the experiment from February to May 2024, we prioritized presenting unseen images to participants rather than maintaining a balanced ratio of real and Al images. In the next version of the experiment, from May to June 2024, we used stratified random sampling to select stimuli, ensuring participants saw real images 50% of the time and AI-generated images 50% of the time. We repeated the analysis both with and without the data from the initial version of the experiment and did not find significant changes in the accuracy distributions. Details of this comparison are provided in Appendix S1 and S4. To ensure that newly added images to the stimuli set as described in Section 3.3.1 were adequately represented in participant responses, we implemented an up-sampling strategy that prioritized showing images that were labeled fewer than 100 times.\nAfter participants responded to five images, we randomized the display time of each subsequent image to one of the following conditions: unlimited time, 20 seconds, 10 seconds, 5 seconds, and 1 second. Participants were informed of the time limit at the start of each time-restricted trial by an on-screen message (e.g., \"You"}, {"title": "3.3.3 Participants", "content": "We collected data through a public website (detectfakes.kellogg.northwestern.edu) where people could test their ability to detect AI-generated images. The website remained accessible throughout our taxonomy development, allowing us to gather responses as we updated image stimuli to reflect improvements in generation models. In total, 50,444 unique participants contributed 749,828 observations. According to Google Analytics, participants who visited our website came from 165 countries; the five countries with the most participants were United States, South Korea, United Kingdom, India, and Germany. We did not collect additional demographic data or other data on participants."}, {"title": "3.3.4 Image-level and Participant Level Analyses", "content": "We define accuracy as a binary measure of whether a participant selected the correct label (Real/AI-generated) for an image. We aggregated accuracy at two levels: image-level accuracy and participant-level accuracy. For image-level accuracy measurements described in Sections 5.1, 5.3, 5.5, 5.4 and 5.8, we aggregated and averaged the binary responses (0 for \"real\" and 1 for \"AI-generated\") provided by participants for each image. Image-level accuracy was calculated as the mean of correct identifications across various factors contributing to photorealism, which are described in each section. For participant-level accuracy measurements described in Section 5.2, we calculated each participant's accuracy by averaging their correct identifications across all viewed images.\nWe present descriptive statistics to summarize our findings, focusing on mean accuracies and their associated 95% confidence intervals (CIs) obtained through non-parametric bootstrapping [36]. We use these measures to describe trends and patterns in the data without using statistical significance to dichotomize effects. However, readers can apply that interpretation to the CIs if they desire."}, {"title": "3.3.5 Ethics", "content": "This research complied with all relevant ethical regulations. The Northwestern University Institutional Review Board (IRB) determined that it met the criteria for exemption from further review. The study's IRB identification number is STU00220627."}, {"title": "4 Taxonomy of Artifacts in AI-generated Images", "content": "Our taxonomy organizes artifacts and implausibilities that may appear in Al-generated images into five high-level categories that are described in further detail in a how-to guide for identifying diffusion model-generated images[40].\n1. Anatomical Implausibilities: This category refers to artifacts that appear in the depiction of people within an image. These include unlikely artifacts in individual body parts, like hands with extra or missing fingers as shown in Figure 6-1B, or the disproportionately long woman's neck in Figure 6-1A. They also include artifacts in facial features such as an unnaturally empty gaze, overly shiny eyes, overlapping of the teeth and mouth, and unlikely proportions or configurations of limbs. In images of multiple people, this includes merged body parts and inconsistent proportions of body parts across different people. Anatomical implausibilities also include biometric artifacts such as size, shape, contours, and proportions of specific facial features if the person in the image is known. These biometric features include eyes, nose structure, mouth edges, interpupillary distance, ear shape and positioning, as well as distinctive markers like moles, dimples, and scars [45]\nCHI '25, April 26-May 1, 2025, Yokohama, Japan\n2. Stylistic Artifacts: This category refers to qualities of entire images or inconsistencies of those qualities within an image. This includes images of people that are waxy (Figure 6-2A), glossy (Figure 6- 2C), shiny, and appear perfect like a model doing a photoshoot. These characteristics often appear in plastic-like skin and excessively soft hair. Additionally, this category includes noticeably cinematic, picturesque, and dramatic images that often appear in artistic photographs like Figure 6-2B. Stylistic artifacts also include inconsistencies between different subjects or parts of an image. This may appear as smudge-like distortions at the edges of different components or differences in resolution that make these parts look like they are cut out from different scenes.\n3. Functional Implausibilities: Functional implausibilities result from a lack of understanding of the fundamental logic of real-world mechanical principles. This includes implausibilities in the objects themselves, their placement within the environment, and how the people in the image may be holding or using these objects, such as the woman holding a sandwich sideways in Figure 6-3B. Objects may also appear unable to function, like the loose strings of the guitar in Figure 6-3A, or placed in a way that they cannot function. Functional implausibilities also include distortion in fine details of the image. The image may present atypical designs in details like the print, buttons, and buckles on pieces of clothing, as seen in a backpack strap merging into a denim jacket in Figure 6-3C. Functional implausibilities also include errors in text, such as distorted or unconventional glyphs and odd spelling errors as seen in Figure 6-3D.\n4. Violations of Physics: This category addresses inconsistencies in the image content that violate the expected logic of physical reality. Examples include shadows pointing in diverging directions, as shown in Figure 6-4A, or shadows that do not correspond to their light sources. Additionally, reflections on surfaces like water, mirrors, or shiny objects may appear misaligned with their surroundings, as illustrated in Figure 6-4B. Violations of physics also include depth and perspective issues, like warping and trajectories that do not align with the rest of the image. These distortions can also occur in real photographs, for example as seen with fish-eye lens distortions.\n5. Sociocultural Implausibilities: This category includes scenarios that are socially inappropriate and unlikely to be seen in the real world, such as people wearing bathing suits at a funeral and a selfie with a bear. Violations of social and cultural norms could also be more subtle, present in details specific to certain cultures like Figure 6- 5B and 5C attempting to depict Ukrainian and Japanese cultures, respectively. Historical inaccuracies and fake images of public figures in unlikely settings like 5A of Figure6 are also examples of sociocultural implausibilities."}, {"title": "5 Accuracy in Distinguishing AI-generated Images from Real Photographs", "content": "In the main phase of the experiment, we collected 539,749 responses on 599 images from 37,568 participants from February 5, 2024 to June 22, 2024. Sections 5.1 through 5.7 focus on data from the main phase of the experiment. The second phase of the experiment started on June 22 and ended on August 30, with 83,577 responses on 482 images from 3,787 participants. Sections 3.3.1 and 5.8 describe"}, {"title": "5.1 Overall Accuracy", "content": "In the main study, participants correctly identified AI-generated images and authentic photographs in 76% and 74% of observations, respectively. Accuracy varied substantially across images. Prior to implementing our accuracy-based exclusion described above, we found that for AI-generated images, accuracy ranged from 32% to 99%. Similarly, accuracy on real photographs ranged from 28% to 92%. Figure 7 shows the distribution of accuracy in both AI-generated and real images with example images selected from the top, bottom, and middle deciles of each distribution. At the image level, the mean accuracy for identifying AI-generated and real images was 76% (95% CI:[74,77]) and 74% (95% CI: [72,76]), respectively.\nDespite our efforts to minimize obvious artifacts, some images - particularly non-portraits - were challenging to generate without noticeable artifacts. As a result, participants achieved nearly 100% accuracy on a few AI-generated images with obvious features."}, {"title": "5.2 Participant Level Accuracy", "content": "Given the organic nature of participants' engagement with this experiment, we did not impose restrictions on the number of images a participant saw. Most participants in this study provided responses to at least seven images, but some participants only provided a single response, and one participant provided 502 responses.\nThe vast majority of participants (75%) saw 16 or fewer images. Figure 9A and B present the distribution of participant-level accuracy by number of viewed images.\nIn order to compare participant performance and avoid issues that arise with differential attrition, we focus on the first ten images seen by participants who saw at least 10 images, which includes 152,050 observations from 15,205 participants. First, we note that 34% of these participants achieved 90% accuracy or higher on the first ten images seen. If the AI-generated images were perfectly photorealistic such that the human ability to distinguish is no higher"}, {"title": "5.3 Accuracy by Scene Complexity", "content": "We find that on average, participants' accuracy increases as scene complexity increases. For example, we find that 16% of portraits appear in the bottom decile of accuracy scores (representing the highest level of photorealism), whereas only 3% of AI-generated posed group images appear in the bottom decile. Figure 10 presents the distribution of accuracy for each category, separately for real and AI-generated images. For AI-generated images, the mean accuracy was 72.7% (95% CI: [72.4, 72.9]) for portraits, 77.2% (95% CI: [76.8, 78.6]) for full body, 76.2% (95% CI: [75.8, 76.7]) for posed groups, and 73.4% (95% CI: [73, 73.8]) for candid groups. For real"}, {"title": "5.4 Accuracy by Presence of Artifacts", "content": "In order to analyze accuracy by artifact type, we annotated images with diffusion model artifact categories from the taxonomy based on a three-step process. First, four co-authors independently annotated all 218 images with accuracy below 80%, identifying artifacts and providing detailed explanations for their annotations. Second, each of these annotations was reviewed and edited by two additional co-authors. Third, a fifth co-author reviewed all annotations for consistency. Figures 12A-C and 12D-F provide examples of how we annotated images, displaying the identified artifact categories, the reasoning behind their identification, and the associated detection accuracy for each image. During this process, we observed that the three main artifact types-anatomical implausibilities, stylistic artifacts, and functional artifacts-each appeared in nearly a third of the images we annotated. In contrast, violations of physics and sociocultural implausibilities were less common, appearing in only"}, {"title": "5.5 Accuracy by Randomized Display Time", "content": "By randomizing the display time of images in this experiment, our results support evaluating how viewing duration influences participants' accuracy. We find that longer viewing times improve performance. With just 1 second of display time, participants are 72% accurate (95% CI=[71.6, 72.5], 95% CI=[71.3, 72.2]) on AI-generated and real images, respectively. With 5 seconds of display time, accuracy increases to 77% (95% CI=[77.0, 77.8], 95% CI=[76.6, 77.4]) for both Al-generated and real images, respectively. While accuracy on real images appears to plateau by 5 seconds of display time, accuracy on Al-generated images increases up to 80% (95% CI=[79.6, 80.4]) at 10 seconds and 82% (95% CI=[81.2, 81.9]) at 20 seconds. Figure 11B presents the distribution of accuracy scores across display time conditions. Across the observations where display time was randomized, we find that the proportion of AI-generated images that are identified below random chance decreases from 43% when participants only have 1 second to view the image to 30%, 25%, 17%, and 17% when participants have 5, 10, 20 seconds, and unlimited time to view the image.\nIn some images, Al artifacts can be noticed with a quick glance, but for others, careful attention to detail is necessary to spot the artifact. Figure 13 presents three images that require careful attention, as evidenced by the fact that most participants mark as real when they are limited to seeing the image for a second but fake once they take into account the details of the scene.\nAccuracy across all artifact types improved with increased display time. As shown in Figure 11C, participants showed higher accuracy when images were displayed for longer time (anatomical artifacts: 63% at 5 seconds vs. 59% at 1 second; stylistic artifacts: 63% at 5 seconds vs. 60% at 1 second; functional artifacts: 60% at 5 seconds vs. 55% at 1 second). For all artifacts, there is a significant improvement in detection accuracy when increasing display time from 5 seconds to unlimited.\nIn Figure 11C, we observe that participants improved the most in identifying functional artifacts, with an 18% improvement from 1 second to unlimited viewing time. In comparison, anatomical and stylistic artifacts showed smaller improvements of 11% each over the same time interval. Unlike anatomical and stylistic implausibilities that can be identified at first glance, functional artifacts often require a closer look and familiarity with the elements in the image"}, {"title": "5.6 Qualitative Analysis of Participant Comments", "content": "We collected 34,675 comments from participants who filled out the optional text input box asking participants: \"If you think this is AI-generated, please explain why.\" In order to identify themes from these 34,675 comments, we prompted GPT-3.5 Turbo to identify 10 main themes across these comments. GPT-3.5 Turbo responded with the following ten themes, which we manually reviewed and refined to mitigate the ambiguities and generalization typical of large language models [75]: (1) Image quality focusing on the overall appearance, smoothness, and sometimes unrealistic perfection of image elements; (2) Facial and anatomical inconsistencies where participants pointed to irregularities in eyes, mouths, noses, skin"}, {"title": "5.7 Accuracy by Models", "content": "In the process of generating the images for this experiment's stimuli set, we noticed that Midjourney, Firefly, and Stable Diffusion have different capabilities and limitations. For example, we noticed that Midjourney often produced images with persistent stylistic artifacts that were challenging to eliminate. Firefly, on the other hand, frequently exhibited a tendency toward synthetic emotional expressions, with subjects often appearing unnaturally and overly cheerful, necessitating multiple iterations to produce more realistic results. Stable Diffusion struggled significantly with generating group images, often introducing artifacts such as anatomical inconsistencies. In light of the limitations to generate non-portrait images with Stable Diffusion, 75% of the Stable Diffusion-generated stimuli in this experiment were portraits. On the other hand, 30% of Midjourney and Firefly-generated images in this experiment depict portraits. In order to compare the three models fairly, we focus our comparison on portrait images. Figure 16 presents accuracy shown on portraits by each of the three models and reveals that participants' mean accuracy on Midjourney, Stable Diffusion, and"}, {"title": "5.8 Accuracy on Human Curated Images vs. Uncurated Images", "content": "Generating photorealistic AI-generated images involves three key ingredients: the diffusion model, the prompt, and human curation. In this section, we examine how human curation of diffusion model-generated images influences the aggregate accuracy scores of human participants. In order to show this influence, we compare diffusion model images from the main experiment, which were curated by our research team, with multiple diffusion model images generated from the same prompt as the curated images. This comparison reveals the increase in photorealism (as measured by the decrease in participants' accuracy) on the curated images relative to the prompt-matched images.\nIn this second phase of the experiment, we randomly sampled 39 AI-generated images from the main stimuli set, where the sample was stratified on 10 percentage point wide bins on human detection accuracy. For each of these 39 images, we generated at least 11 prompt-matched images using Midjourney, Firefly, and the same pipeline in Stable Diffusion. Figure 17 displays a Stable Diffusion-generated image from our original stimuli set and three of the twelve generations using the same prompt. We generated 482 total additional images, with at least 11 per prompt. These 482 images were included alongside the 149 real images on the experiment website.\nIn Figure 18, we present scatterplots comparing human detection accuracy on the initial curated images and the best prompt-matched images in panel A, and mean prompt-matched images in panel B. We find the human-curated images have lower human detection accuracy than the best regenerated image in 18 of 39 instances and the mean re-generated image in 35 of 39 instances. In total, the human-curated images were perceived to be more photorealistic than 408 of the 482 (84%) uncurated prompt-matched images. Specifically, we find the marginal value added by human curation"}]}