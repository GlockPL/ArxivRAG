{"title": "Heads Up eXperience (HUX): Always-On AI Companion for Human Computer Environment Interaction", "authors": ["Sukanth K", "Sudhiksha Kandavel Rajan", "Rajashekhar VS", "Gowdham Prabhakar"], "abstract": "While current personal smart devices excel in digital domains, they fall short in assisting users during human environment interaction. This paper proposes Heads Up eXperience (HUX), an AI system designed to bridge this gap, serving as a constant companion across the extended reality (XR) environments. By tracking the user's eye gaze, analyzing the surrounding environment, and interpreting verbal contexts, the system captures and enhances multi-modal data, providing holistic context interpretation and memory storage in real-time task specific situations. This comprehensive approach enables more natural, empathetic and intelligent interactions between the user and HUX AI, paving the path for human computer environment interaction. Intended for deployment in smart glasses and extended reality headsets, HUX AI aims to become a personal and useful AI companion for daily life. By integrating digital assistance with enhanced physical world interactions, this technology has the potential to revolutionize human-AI collaboration in both personal and professional spheres paving the way for the future of personal smart devices.", "sections": [{"title": "1. Introduction", "content": "The usage of PSD plays an essential role in people's lives, and this trend has been observed in the past few decades. One of the primary focus is im- proving HCI. The next generation of personal smart devices (PSD) has to be designed to improve interaction, helping humans in digital domains and the real world or connecting both. The present devices provide human computer inter- action (HCI), but the future PSD should be capable of understanding human environment interaction and thus enabling human computer environment inter- action (HCEI). This can be made possible by improving the input and output modalities in the current PSD."}, {"title": "2. Comparative Analysis with Existing Literature", "content": "In this section, we discuss the works from the literature. It is in selective attention theory, video processing, merits of visual enhancement in extended reality, embodied AI, and multi-modal HCI."}, {"title": "2.1. Selective Attention Theory", "content": "Selective attention is a critical cognitive process that allows individuals to focus on specific stimuli while ignoring irrelevant information. This concept is foundational in understanding human information processing and has been extensively studied through various models. Broadbent Filter Model (1958) [5], for example, introduced the idea of an early-selection filter that processes inputs based on their physical characteristics. Treisman Attenuation Model (1964) [6] refined this by suggesting that unattended messages are attenuated rather than completely filtered out, allowing some level of further processing. Deutsch and Deutsch Late Selection Theory (1963) [7] proposed that all stimuli are processed meaningfully before selection occurs. More recently, Lavie Load Theory (1995) [8] integrated elements of both early and late selection, putting forward that attentional selection depends on the perceptual load of the task.\nAddressing the limitations of selective attention is essential for several reasons:\n\u2022 Enhanced Safety: In high-stakes environments such as healthcare, avi- ation, and security, missing critical information can have serious conse- quences.\n\u2022 Improved Decision-Making: Comprehensive awareness of the envi- ronment leads to better-informed decisions, which is beneficial in both professional and everyday contexts.\n\u2022 Increased Efficiency: Reducing the cognitive load on individuals al- lows them to focus on their primary tasks effectively, improving overall productivity."}, {"title": "2.2. Video Processing", "content": "Our work uses events of interest (EOIs) based frames for real-time video processing. The current methods for processing videos, OpenAI [9] approach with GPT-40 and GPT-40 mini utilizes a fixed sampling rate, such as extracting frames at regular intervals (such as one frame per second), which are not syn- chronized with event occurrences. In our work, we leverage deep learning models such as YOLO [10] to analyze a real-time video and extract EOIs using objects of interest (OOIs). This method enhances the signal-to-noise ratio, deriving meaningful signals from the same raw data. This reduces the computational cost due to the vision language model (VLM), thereby increasing efficiency."}, {"title": "2.3. Visual Enhancement in Extended Reality", "content": "Visual enhancement is achieved through holographic labeling using AR algo- rithms [11] and deep learning-based algorithms. In our implementation, models such as YOLO [10] play a crucial role in extracting EOIs in real-time video anal- ysis. The same models can be used for OOIs labeling in the scene, thus offering dual merits. The works of [12] present an MR system that implements real-time object detection and tracking using Microsoft HoloLens. The authors argue that this capability enhances extended reality experiences by allowing users to see holographic labels superimposed on detected objects in their environment. Our research extends this concept by feeding the enhanced scene perception to a VLM (feeding the usual scene with task-specific OOI labeling). Due to this, the HUX AI sees what the user sees in a task-specific scenario. The VLM used was not fine-tuned for this purpose."}, {"title": "2.4. Embodied AI", "content": ""}, {"title": "2.4.1. HCI", "content": "Embodied AI systems such as Humane Pin [4] and Rabbit R1 [3] utilize VLMs to enhance HCI. These systems leverage the capabilities of VLMS to interpret and interact with dynamic scenes, providing a intuitive and responsive user experience."}, {"title": "2.4.2. Robot", "content": "Our work uses VLMs to understand dynamic scenes in HCI environments. This is similar to the approach demonstrated in AutoRT [13], where VLMs are used for scene understanding and grounding in robotics."}, {"title": "2.5. Multi-modal HCI", "content": "Recent advancements in gaze-facilitated information-querying systems have opened new avenues for natural and intuitive HCI. In the works of [14], the authors introduced G-VOILA. This paradigm combines users' gaze data, visual field, and voice-based natural language queries to facilitate intuitive information retrieval in daily scenarios. Their work revealed user query behavior patterns, including the prevalence of ambiguous expressions and specific eye movement patterns associated with querying tasks.\nOur HUX AI implementation extends the concept of multi-modal querying into a dynamic and real-time task-specific environment. While G-VOILA pri- marily focuses on static scene understanding, our system incorporates active change detection, real-time environmental adaptation, and personalized con- text memory. This approach addresses some identified challenges, particularly in resolving query ambiguities and adapting to users' situational context. By integrating these features, our work contributes to the evolving landscape of multi-modal, gaze-enhanced information retrieval systems, offering new possi- bilities for responsive and context-aware HCEI. The current implementation described in the work [14] is not real-time. It also made some design choices to facilitate that potential. In the works of [14], there were four rounds of question- ing where each round had one question that was asked to the AI. This contrasts with our work, where we ask several questions sequentially.\nWe also extend the research by deploying this in a real-time test environ- ment. This environment undergoes continuous changes, allowing us to test the system dynamically. We observe the system in real-time with a user who asks multiple questions. These questions involve different modalities within the same environment. This approach simulates how a person wearing smart glasses with HUX AI might interact with a real-time environment in the future. We are checking the efficacy of our system to track the multi-modal data like scene changes, and gaze changes spatially and temporally, as well as to track the context-rich user interactions."}, {"title": "3. Problem Statement", "content": ""}, {"title": "3.1. Human Computer Environment Interaction", "content": "The PSDs have yet to be able to aid humans in real-world scenarios. On top of their digital expertise, the next generation of devices should also help in human environment interaction, navigation, and complex domain-specific tasks users may encounter in daily life. The goal is to boost the productivity of tasks in real-world environments. The tools for domain-specific scenarios are often confined to that domain and that device. The software can not be used in other scenarios and thus can not be used as a PSD. We aim to create a PSD that enhances user productivity, decision-making, understanding, and task-specific scenarios in diverse environments. The future utility of a general- purpose AI is decided by the number of domain-specific tasks in which human-AI collaboration is possible and the magnitude of the usefulness of this AI in a task- specific domain. To achieve widespread adoption, wearables must enhance user capabilities across various daily tasks, thus functioning as a companion expert. The agenda of the HUX AI is shown in Table 1."}, {"title": "3.2. AI Utility", "content": "The capability of the AI-enabled devices is shown in Figure 2. The domain- specific model is optimized for precise and specialized tasks indicated in green dots. An example can be the autopilot system in an autonomous car. A gen- eralist model offers a broad, adaptable language understanding and generation, represented in orange dots. An LLM-based chatbot is an example of this type. Our HUX AI integrates a generalist base with modular and task-specific layers for flexible and specialized applications.\nAI's Utility = Number of Domains \u00d7 Domain Specific Task Performance\nThe magnitude of the usefulness of this AI in a domain-specific task deter- mines its necessity for that particular task. The number of domain-specific tasks in which human-AI collaboration is possible determines the usability in varied environments that the user may encounter daily, thus ensuring the usefulness of HUX AI in a PSD throughout the day."}, {"title": "3.3. Properties of a Task-Specific Environment", "content": "The following properties are observed in a task-specific environment."}, {"title": "3.3.1. Objects of Interest (OOIs):", "content": "\u2022 Task-specific set of relevant objects, entities, or elements.\n\u2022 Can include physical and virtual objects (real world or AR environment).\n\u2022 May have varying levels of importance or priority within the task."}, {"title": "3.3.2. Events of Interest (EOIs):", "content": "\u2022 Task-specific set of relevant actions, behaviors, or phenomena.\n\u2022 Includes both expected and unexpected events due to OOIs.\n\u2022 Can involve interactions between OOIs or changes in their states."}, {"title": "3.3.3. Environmental Context (EC):", "content": "\u2022 Relevant background information and conditions of the task environment.\n\u2022 Influences how OOIs and EOIs are perceived and interpreted.\n\u2022 May include spatial, temporal, or situational factors.\nThese are the properties of a task-specific environment."}, {"title": "3.4. Desired Features of AI for Varied Usability", "content": "\u2022 Spatial and Temporal real-time scene understanding in a task environment: The HUX AI tracks only EOIS, OOIs confined to a task environment. The other information it perceives has less priority, and the trackability is not guaranteed. The live computation is efficient as it avoids unnecessary spatial and temporal information. This clarifies task context and avoids mixing and overfitting data, which may overwhelm the user or the multi-modal context processing engine.\n\u2022 Understand user perception: Generate the ROI using eye gaze data, gen- erate enhanced visual perception using AR or deep learning algorithms for object and scene labeling, and understand them contextually.\n\u2022 Context rich interaction using multi-modal data: Help the user with real- time queries, suggestions, and answers.\n\u2022 Understand, remember, and recall the multi-modal interaction: Recall con- textual and event information and patterns in an interaction session.\n\u2022 Future scope: Multi-modal outputs and actions."}, {"title": "4. HUX: Heads Up eXperience System and its Components", "content": ""}, {"title": "4.1. The Heads Up eXperience", "content": "The user perception is integrated and optimized for human-AI collaboration to handle real-world or digital environments. This approach narrows the lines between HCI and human environment interaction, resulting in HCEI. The name \"Heads Up eXperience\" (HUX) for the AI integrated into smart glasses is in- spired by the concept of a \"Heads Up Display\u201d (HUD). While a traditional HUD projects information into the user's field of view to keep them focused on their primary task, the \"Heads Up eXperience\" extends this concept by emphasizing a immersive, interactive, and multi-modal interface.\nThe term \"experience\" reflects the capability of HUX AI to not only present relevant information but also enhance user engagement with their surroundings in a dynamic and contextually aware manner. By integrating various function- alities typically managed by separate devices into the smart glasses, HUX AI could eliminate the need for smartphones and other gadgets. This consolidation allows users to perform a wide range of actions, access information, and interact with their environment directly through the heads-up interface, streamlining the user experience and facilitating a intuitive interaction with technology."}, {"title": "4.2. The HUX AI Architecture", "content": "Figure 3 shows all the integral components for the system's working and flow.\n\u2022 It shows the sources of all data modes, the data flow inside the HUX AI system, data processing, and the data flow for each modality.\n\u2022 All input data modes are converted to text captions and descriptions and are integrated by the multi-modal context processing engine using a prompt architecture.\n\u2022 Creation of a new voice file (new utterance input) triggers a multi-modal context processing episode.\n\u2022 Collects the latest scene captions stored in the Last-In-Only-Used (LIOU) stack.\n\u2022 Gets the latest ROI caption whose origin image is created simultaneously with the speech audio file.\nMulti-modal Context Processing receives the textual representations of all the possible input modalities, processes the data, and returns the output to the user. The environment here is dynamic (subject to change), and the user's goal is to accomplish the task of interest in this environment. The boxes highlighted in green indicate the processed perception input given to the user by the HUX AI system. The red arrow indicates the inputs given by the user to the HUX AI system. The Green arrow indicates the outputs processed or generated by the HUX AI system for the user. The grey arrow represents the trigger that initiates a context processing instance. Other arrows represent the data flow from one block to the other. The blue shaded area indicates the HUX AI system, and the rest are not a part of the HUX AI system."}, {"title": "4.2.1. Models used", "content": "The core models that were used are as follows:\n\u2022 Vision Language Model: Mini Intern VL 1.5 2 Billion Parameter, [15] [16] was used for image captioning.\n\u2022 Large Language Model: Llama-3 8 Billion Parameter model [17] was used for all LLM instances, including the multi-modal Context Process\nThe models for task-specific perception vary based on the task. Some models used are YOLO [10] and printed circuit board defect detection [18] for identifying OOIs and EOIs."}, {"title": "4.2.2. Core Components", "content": "The various modalities used are as follows:\n\u2022 User's Speech Input: Listen to user speech inputs, which may be in- structions, queries, or general conversations. Trigger a multi-modal con- text processing instance.\n\u2022 Eye Gaze Information: User eye gaze is measured and ROI is produced."}, {"title": "4.3. Real-Time Video Feed Filtering based on Events", "content": "Figure 4 represents the temporal flow of frames from a real-time video. The event indicated by the green boxes indicates the filtered EOIs found by the real-time video analyzer. These EOI frames are sent to the VLM for image cap- tioning. The event data derived from the real-time video analyser are combined with image captions and sent sequentially for context processing with metadata. The data from selected frames triggered by certain events are updated in the context window.\nThe EOIs can be obtained from real-time video of the environment:\n\u2022 Change in the number of objects of interest within the frame: This includes the appearance or disappearance of specific objects or a total number of objects.\n\u2022 Change in the object behavior: This also includes rapid changes in bound- ing box sizes, swift movements across the frame, changes in the scale or proximity of objects, and other feature variations.\n\u2022 Custom-defined events specific to the application domain: This involves detecting custom-defined events such as gestures, actions, or environmen- tal changes."}, {"title": "4.4. Task Specific Scene Processing", "content": "The above section showed the merits of using a real-time scene analyzer to identify EOIs. We use the same real-time video analyzer to enhance user perception visually."}, {"title": "4.4.1. Task Specific Scene Enhancement", "content": "The same computer vision model used above can be re-purposed here to temporally filter out events and visually annotate the scene image feed with OOIs confined to the user task.\nThe merits of this approach are as follows:\n\u2022 Visual aid for the user if the user's scene image feed is annotated when working in a task-specific scenario to improve task-specific selective atten- tion.\n\u2022 HUX AI is aware of the scope of task-specific OOIs among other objects.\n\u2022 Modularity to handle different tasks with a unique set of OOI and EOI.\n\u2022 Adaptive attention allocation: Dynamic shifting of focus based on chang- ing environmental priorities.\n\u2022 Attention restoration: Improved techniques for mental recovery and focus renewal in the scene.\n\u2022 Selective ignorance cultivation: Helping the user to disregard non-essential information for the task intentionally."}, {"title": "4.4.2. Modularity of Task Specificity", "content": "Switching of task-specific computer vision models responsible for detecting OOIs can be executed using agentic behavior [19]. The switching can be done with or without explicit user summons. Since the agent can understand the context of the initial scene, this insight can be used to decide if it has to use task-specific computer vision models (defined as tools) for further HCEI."}, {"title": "4.5. Eye-Gaze Processing", "content": "This pipeline discusses how the eye gaze data is processed in real time in parallel to the scene data. Figure 8 shows the origin and flow of the eye gaze data. The scene image processing is shown here to demonstrate the parallel and interdependent nature of the gaze and scene processing.\nThe Figure 8 is explained as follows:\n\u2022 Colored image indicates the frame where a new EOI is observed.\n\u2022 Subsequent black and white frames do not have a new EOI, thus not sent for VLM processing for image captioning, thus demonstrating compute efficiency.\n\u2022 The highlighted apple in a blue dotted bounding box indicates the frame at which speech was triggered. The gaze coordinates at that time instance are noted, and the scene image from the same time instance is used to generate the ROI gaze image.\n\u2022 Frame with the EOI is sent to the VLM for image captioning.\n\u2022 Extracted ROI from the scene image and eye gaze data from the hardware is sent to the VLM for image captioning.\n\u2022 Utterance is derived from the user and converted to text using OpenAI Whisper.\n\u2022 All three input data are inserted into the prompt for spatial and temporal scene recreation in text descriptions.\nEye Gaze data is obtained from the wearable hardware. The data could be gaze coordinates in (x,y) and the perimeter of the gaze area. Raw gaze data and the overall scene are used to obtain the ROI by cropping or segmenting from the overall scene, which the VLM uses to obtain text inferences. The scene image is also passed to the VLM. The image captions are obtained from both the VLM. The data is translated into textual form and passed to the multi-modal context processing engine."}, {"title": "4.6. Multi-modal Contextual Memory Creation", "content": "Upon summoning, the multi-modal data and rich user context during an interaction can be stored in a structured multi-modal memory format. Later, it can be used for retrieval based on mere contextual cues using Retrieval- Augmented Generation (RAG) [20] rather than explicitly searching for one of the data modes like images, information, or conversation context. The Llama Index [19] was used to execute this specific pipeline. Agentic behavior was utilized to extract the class of object of interest from naive user input interaction.\nFigure 9 shows the process flow of how it can be implemented. The chosen scene image is sent to the VLM for image captioning. The same scene image is sent for ROI processing and extracting. ROI can be processed using multiple inputs, such as verbal specifications of an OOI or eye gaze ROI. This ROI/OOI is sent to VLM to get image captions. The agent extracts the user context from the conversation. All the multi-modal data is sent to an LLM instance to generate keywords, phrases, and other contextual cues can be retrieved in later time period by tools like RAG [20].\nFigure 10 shows the data structure of a multi-modal contextual memory and how it exists among other memories. The following describes each data type in the multi-modal memory data structure.\n\u2022 Object of Interest: The YOLO [10] was used to obtain the class OOI image from the scene image. Image Captions of Object/Region of Interest + Object/Region of Interest location are stored here.\n\u2022 Scene: Image Captions of Scene + Scene Image Location is stored here.\n\u2022 Speech Input: Context given by the user during an interaction about this object, like its relationship with time, people, other objects, and emotions, is obtained from the conversation.\n\u2022 Meta Information: Metadata like location, time, and device name are obtained.\n\u2022 LLM Generated Content: Keywords, phrases generated by an LLM in- stance for future retrieval. The advantages of this are that it minimizes the computation cost in the future when RAG [20] tries to process the context of all raw data to match the user's query. By having a set of prob- able user queries (generated by the LLM instance) that could be asked by the user in the future to refer to this memory, the time and computation required to find the data can be reduced. More work has to be done to explore the efficacy.\nLater, when the user refers to the object with the subtlest of the direct or indirect contextual cues, the agent can refer to the stored memories to retrieve the source image, data, and context."}, {"title": "5. HUX AI Applications", "content": ""}, {"title": "5.1. Solving Selective Attention Problem in real-time Human Environment In- teraction", "content": "The merits of solving the selective attention problem are as follows:\n\u2022 The goal of AI is to pay attention to tasks outside the scope of human selective attention.\n\u2022 Enhanced situational awareness: Improved detection of subtle environ- mental cues or threats in complex scenarios.\n\u2022 Cognitive offloading: By monitoring less salient task elements, the HUX AI could free up human cognitive resources for higher-level decision-making.\n\u2022 Enhanced multitasking support: In complex scenarios requiring attention to multiple sub-tasks, the AI could manage parallel information streams more effectively than humans."}, {"title": "5.2. Spatial and Temporal Scene Recall", "content": "There are two scenes: Scene 1, with five demo videos of 30 seconds each, and Scene 2, with one demo video of 13 seconds. The scenes had several fruits placed in a dark background. A few of the fruits were removed from the scene and were replaced in the background several times. The YouTube link for the video is here: (https://youtu.be/KyQnhxZyu08). The demo inference of scene 1, demo video one, is as follows:\nIn comparing HUX AI computer vision analysis with manual video analysis, both methods generally align but show some discrepancies. HUX AI identifies the highest variety of fruits at timestamps 20240708163548 and 20240708163556, while manual analysis identifies it at 20240708163600. Both agree that there is no consistent pattern in adding or removing fruits and that apples appear in 7 frames, disappearing once. HUX AI states that no consecutive frames have the same arrangement, while manual analysis disagrees. This is because no frame was processed, and no EOI was detected. Both agree that the orange has the most fragmented presence and that bananas dominate the frequency and quantity index. The banana is noted as the only consistently present fruit throughout the video.\nSelective real-time data computation: All data can be stored without real-time computing. It can be computed when necessary. Only the frames with EOIs are computed in real time. This is because real-time computation must occur selectively due to resource and time constraints."}, {"title": "5.3. Task Specific Scene", "content": "Demonstrating a multi-modal task-specific scene perception: Task-specific visual enhancement using AR algorithms and deep learning computer vision algorithms. VLM is used in the image captioning of this enhanced scene."}, {"title": "5.3.1. Demonstrated Workflow", "content": "The workflow is shown in Table 2. It is briefly described here:\nUser Requests Help with Visual Identification: Action: User asks for help understanding what they are seeing. Response: Agent visually inspects and identifies the components on the PCB.\nUser Inquires About Defects: Action: User asks if there are any defects in the circuit. Response: Agent uses the PCB-HUD tool which has the [18] PCB defect detection model.\nAgent Analyzes and Infers Defects: Process: Agent uses VLM to per- form image captioning of the labeled scene image. Conclusion: Agent identifies three defects (mouse bites) and describes their spatial locations verbally (usage of VLM).\nUser Seeks More Information on Defects: Action: User asks for an explanation of the defects. Response: Agent explains what a mouse bite defect is based on its knowledge. Bridging knowledge from task scene enhanced by this task-specific computer vision model and LLM's trained knowledge.\nUser Checks Agent's Memory: Action: User asks how many defects were seen. Response: Agent confirms three defects and reiterates their locations."}, {"title": "5.4. Multi-modal Contextual Memory Creation", "content": "The Subsection 4.6 explains the multi-modal contextual memory creation. Table 3 shows how the multi-modal contextual memory creation can happen in a real-life setting where the user asks the HUX AI system to remember an engineer he met for the first time with the multi-modal rich information. This is shown in Figure 12."}, {"title": "6. Experiments with the HUX AI", "content": ""}, {"title": "6.1. Goals of the implementation", "content": "Drawing inspiration from the selective attention tests widely used in psy- chological assessments, which describe how human selective attention performs when there is a task in a dynamic environment. Focusing on a high-cognitive- load task makes it difficult to notice other apparent environmental changes that would otherwise be easily observed.\nOne notable ability that can be explored using this system is to pay multiple attention to things the user is not paying selective attention to, thus helping out users in real-world stochastic situations. This technology can help verbal recall of objects, temporally and spatially, by having a mere conversation on things that have been overlooked."}, {"title": "6.2. Test Environment", "content": "The following are knowns and unknowns for the HUX AI. Known: Objects of interest, Events of Interest. Unknown: Sequence of Events, Objects outside of task interest.\nThe experimental setup is shown in Figure 15. The scene consists of a dark background on a tabletop. The objects of interest are fruits. Additionally, other objects are introduced at different times. The scope of the pipeline restricts its observational capabilities to increase the signal-to-noise ratio by having a limited set of OOIs and EOIs for accomplishing a specific task. For example, for the user to be focused on cooking, the AI should not detect EOIs whenever it identifies random objects it is capable of encountering but rather restrict its event detection capabilities to the foods, food items, and objects related to this specific task. While the VLM is always generalized and can recognize all possible objects, the real-time video analyzer restricts EOI extraction for this specific task.\nWith this scope in mind, this particular experiment aims to dynamically change the environment by spatially and temporally changing the OOIs. This includes the removal of fruit from the scene and placing new fruit in the scene in random time sequences. Additionally, none of the language models used were fine-tuned to preserve the system's generality.\nEOIs and OOIs for this experiment: The OOIs are the fruits in the scene environment. The EOIs are the disappearance and appearance of those fruits (OOIs).\nWhen the changes in the scene occur, the user also simultaneously interacts with the AI, asking questions about the environment and specific objects in the environment using their eye gaze, questions that need at least one multi-modal data from each of them. This test environment aims to mimic the fundamentals of any environment as a starting point. Promising results in this environment could be applied to other test environments with changes in the OOIs and different task-specific environments.\nWe are testing the current implementation with a single user who is informed about the system's capabilities without revealing the technical details. The user was given cues on what questions could be asked to test the system's usefulness and was allowed to improvise. Table 4 represents the different kinds of proposed questions asked to HUX AI while interacting with this environment, which involves multiple modalities that can be utilized for answering different user queries."}, {"title": "6.3. Hardware Used", "content": "Technical details of the hardware used in the experiment are as follows. We had to divide the computation across three independent systems whose input- output was networked to create the HUX AI system for this test environment."}, {"title": "6.3.1. System-1 (User Interface):", "content": "This system had the following specifications:\n\u2022 Operating System: Windows 11 Pro\n\u2022 RAM: 32 GB\n\u2022 vRam: 12 GB\n\u2022 Graphics Card: Nvidia RTX 4070 Ti\n\u2022 CPU: Intel Core i7 13th gen\n\u2022 Eye Tracker: Tobii Eye Tracker 5\n\u2022 Display: 24\" with a resolution of 2560 x 1440 pixels\nAssigned responsibilities:\n\u2022 Speech: Collects user speech input and converts them to text.\n\u2022 User Interface: User views the environment through this computer.\n\u2022 Eye Gaze: Collects user eye gaze from the gaze hardware.\n\u2022 Eye Gaze to ROI image: Converts raw eye gaze coordinates in (x,y) to ROI images.\n\u2022 Scene Capture: The camera that captures the scene in real-time is di- rectly connected to this computer.\n\u2022 Event Detection: The model that is responsible for detecting the event is run in this computer. The events are captured as frames and sent for processing in real time. The hybrid approach, as depicted in Figure 5, was not used in this implementation."}, {"title": "6.3.2. System-2 (Main Hub):", "content": "This system had the following specifications:\n\u2022 Operating System: Ubuntu 22\n\u2022 RAM: 32 GB\n\u2022 vRam: 16GB\n\u2022 Graphics Card: Nvidia RTX 4070 Ti Super\n\u2022 CPU: Intel Core i9-14th Gen\nAssigned responsibilities:\n\u2022 ROI Image: ROI gaze image is passed through VLM to have image cap- tions whenever there is a new speech input.\n\u2022 Data Collection in real-time: All multi-modal data is stored as text here.\n\u2022 Data fusion with LLM prompting: All the multi-modal data inferred as text is fed into the LLM context window with a prompting strategy. All the data modalities are labeled with their source and their roles. All the data modalities are prompt-engineered to answer the main user query.\n\u2022 Text to Speech: Converts the LLM generated text answer to speech output.\nCommunication: Receives Gaze Images and Speech-to-Text converted ut- terances from System-1 to System-2."}, {"title": "6.3.3. System-3 (Scene Processing):", "content": "This system had the following specifications:\n\u2022 Operating System: Ubuntu 22\n\u2022 RAM: 32 GB\n\u2022 vRam: 12GB\n\u2022 Graphic Card: Nvidia RTX 4070 Ti\n\u2022 CPU: Intel Core i7 13th gen\nAssigned responsibilities:\n\u2022 Significant Event Scene to Text Description: Since the VLM con- sumes a lot of video memory, the scene processing is outsourced to this computer.\nCommunication: Receives raw EOI frames from System-1, processes them, and sends them to System-3 as image captions in real-time.\nDeployment limitations: VLMs and LLM engines are compute-heavy, so has to be deployed in three different systems in parallel and act in synchroniza- tion to run the system to the total ability.\nNote: Multi-modal contextual memory was not integrated in this pipeline rather tested separately (Section 5.4)."}, {"title": "6.4. User Interaction with the AI", "content": "The user interacts with the AI as follows:\n\u2022 The user presses and holds the space bar in an QWERTY keyboard. The system receives the voice input until the space bar is released.\n\u2022 The user can speak anything while holding the space bar, and the voice is captured.\n\u2022 Current gaze image is captured when the space bar is released from hold.\n\u2022 The EOI frames extraction operates independently. They only change when there is a change in the environment, regardless of whether the user presses the space bar.\n\u2022 User waits for the answer after being processed by the pipeline. The user cannot interfere with or pause the HUX AI response for this prototype version."}, {"title": "6.4.1. Noted Errors", "content": "During the trial user interaction runs, there were errors. Contributors of the errors:\n\u2022 Error in the speech-to-text conversion.\n\u2022 User gaze being outside the scope of scene environment: One such oc- currence occurred when the user looked at the taskbar of the Windows 11 screen. HUX AI still answered the question, but the response was irrelevant to the scene.\n\u2022 Recollection from the Context Window: General problems identified in the LLM conversation include hallucination and forgetfulness."}, {"title": "6.5. Multi-modal Context Processing Prompt Architecture", "content": "\u2022 Based on the previous responses and new multi-modal informa- tion, answer the next question:\n\u2022 General detailed description of user environment: <Latest Environment Description >\n\u2022 User is specifically gazing at: <Latest Eye-Gaze based ROI Description >\n\u2022 Human's speech query: <Speech to Text Converted Context Rich User Utterance\n\u2022 Give only a relevant and precise answer to the Human's speech query: <Response >"}, {"title": "6.6. The Experimental Demonstration", "content": "Table 4 indicates the example user queries that can be asked to this imple- mented system. It also indicates the origin or source of data that the multi- modal context processing can use to answer the queries."}, {"title": "7. Discussions", "content": "The issues humans face in selective attention can be overcome using our HUX AI through human-AI"}]}