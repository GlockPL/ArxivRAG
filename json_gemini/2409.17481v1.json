{"title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models", "authors": ["Gongfan Fang", "Hongxu Yin", "Saurav Muralidharan", "Greg Heinrich", "Jeff Pool", "Jan Kautz", "Pavlo Molchanov", "Xinchao Wang"], "abstract": "Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or \u201cN:M\u201d) Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks\nour method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains.\nCode is available at https://github.com/NVlabs/MaskLLM.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated re-markable effectiveness across a diverse range of tasks [17, 6, 46, 11]. However, the generality and robustness of LLMs are largely attributed to their vast scale, with parameter counts ranging from one billion to several hundred billion [37, 43, 5]. This substantial model size, in turn, makes it challenging and resource-intensive to deploy LLMs in real-world applications. One effective and practical approach to address this issue is semi-structured pruning [27, 30, 10, 36], which introduces N:M sparsity into LLMs to improve both memory and computational efficiency. The N:M pattern, with N non-zero values"}, {"title": "2 Related Works", "content": "Pruning Large Language Models. Network Pruning [13, 28, 15, 16, 39] have been proven an efficient approach to compress pre-trained language models via the removal of redundant parameters. According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning [24, 41, 22], Unstructured Pruning [15, 13], and Semi-Structured Pruning [10, 36, 27, 30, 31]. Structured pruning physically eliminates substructures like attention heads [24], embeddings or depth [41] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [30]. However, structured approaches typically necessitate huge retraining"}, {"title": "3 Method", "content": "3.1 N:M Sparsity\nWe motivate and introduce a learnable framework, MaskLLM, to sparsify Large Language Models (LLMs) for improved inference efficiency. Sparsifying an LLM with N:M patterns imposes the constraint of having (no more than) N non-zero values within each consecutive set of M parameters. This task can be formulated as a mask selection problem with the candidate set of |S| = {M\\choose N} = {M!\\over N!(M-N)!} candidates, where |S| denotes the size of the candidate set, and {M\\choose N} represents the combination number of potential N:M masks. For simplicity, this work primarily focuses on 2:4 sparsity, which can be naturally extended to other patterns such as 1:4 and 4:8. Given a parameter block comprising four consecutive parameters, denoted as W \u2208 R^{1\u00d74}, the goal of sparsification is to identify the optimal binary mask M^* \u2208 B^{1\u00d74} of the same size, ensuring that the pruned weight maintains its behavior on observed data x ~ p(x). For 2:4 sparsity, the binary mask M must contain exactly two zeros, resulting in a discrete candidate set S^{2:4} with |S^{2:4}| = {4\\choose 2} = 6 candidates:\nS^{2:4} = {\u039c \u2208 \u0392^{1\u00d74}|\u2211 M = 2} = {M_1, M_2, M_3, M_4, M_5, M_6}\n= {[1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1], [0, 1, 0, 1], [0, 1, 1, 0], [0, 0, 1, 1]}.\nFor an LLM, there exists a substantial number of parameter blocks, denoted as {Wi}, each requiring the selection of corresponding masks {M}. To maintain satisfactory behavior after pruning, it is natural to define the following objective for N:M sparsity:\n{M^*} = \\underset{{M_i} \\in S^{2:4}}{\\operatorname{argmin}} \\mathbb{E}_{x \\sim p(x)} [LLM(x; {Wi \u2299 Mi})],\nwhere LLM refers to the language modeling loss for pre-training. The operator \u2299 denotes element-wise multiplication, which masks partial parameters for sparsification. However, finding the optimal combination of masks M* can be extremely challenging in the context of LLMs due to the non-differentiable nature of mask selection and the huge parameter scale. In the following sections, we demonstrate that the mask selection can be transformed into a sampling process."}, {"title": "3.2 MaskLLM: Learnable Semi-Structured Sparsity", "content": "Consider a single parameter block W \u2208 R^{1\u00d74} consisting of only 4 parameters: directly determining the exact optimal mask for this block is not feasible, since the behavior of the pruned LLM also"}, {"title": "4 Experiments", "content": "4.1 Implementation Details.\nWe evaluated MaskLLM on three large language model families, ranging in size from 843M to 15B parameters. This included public models like LLaMA-2 7B and 13B [37], Nemotron-4 15B [29], and two in-house models, multilingual GPT-3 843M and 2B [34]. For LLaMA-2 and Nemotron-4, we collected a blended training set following the original papers [34, 29] for training. For the GPT-3 multilingual models, we used the original training set for mask learning. To learn masks, we trained the Gumbel logits for 2,000 steps without updating the LLM parameters. For evaluation, we follow SparseGPT [10] to use C4 dataset [32] for one-shot pruning and Wikitext [26] for evaluation. In addition, we also deploy LM-Eval-Harness [11] for zero-shot evaluation. More details about the models, datasets, training, and evaluation can be found in the appendix."}, {"title": "4.2 Learning 2:4 Sparsity in LLMs", "content": "Finding 1. Learnable Sparsity scales effectively to large-scale datasets and can fully leverage computational resources to learn precise masks through end-to-end training.\nEnd-to-end training yields accurate masks. In Table 1, we report the perplexity and accuracies of our method, compared to three 2:4 sparse baselines: Magnitude Pruning [14], SparseGPT [10], and Wanda [36]. Previous works can produce satisfactory 2:4 masks efficiently but often suffer from inaccurate estimation of weight importance. The inaccuracy mainly arises from two factors: (1) Accuracy of importance metric: Due to the difficulty of computing the error caused by pruning, existing methods use approximated metrics to estimate weight importance, which inevitably results in errors. (2) Scalability: LLMs are usually pre-trained on large-scale datasets with rich knowledge, but the calibration sets used in existing methods contain very limited samples. With the learnable mask, the above challenges can be naturally addressed through end-to-end training on large-scale datasets, which directly optimizes the language modeling loss. As illustrated in Table 1, MaskLLM yields superior results compared to existing baselines. For instance, with the LLaMA-2 7B model, the proposed method learns a mask with a PPL of 6.72, which is better than the PPL of 10.42 obtained by SparseGPT with weight update. More results such as comparison to other baselines (Table 13) and visualization of mask difference (Figure 8) can be found in the appendix.\nScaling to large-scale datasets. To further elaborate on the above analysis, we illustrate the relationship between the number of consumed samples and the Wikitext PPL of pruned LLaMA-2 7B in Figure 4. For one-shot methods such as SparseGPT, all consumed samples are used to compute the Hessian for importance estimation. Increasing the calibration set size from 32 to 256 samples improves the results, but expansion beyond 256 samples yields no notable advantages."}, {"title": "4.3 How to Learn a Good Mask for LLMs", "content": "Finding 2. Taking pre-computed masks as prior improves training efficiency and mask quality.\nTransfer Learning with Mask Prior. An important feature of the proposed method lies in transfer learning. We can initialize the Gumbel logits with pre-computed masks, which significantly accelerate the training. In Table 2, we learn masks using different prior types, including Magnitude prior [13], SparseGPT prior [10], and Wanda prior [36]. Firstly, even without any prior, the learnable mask still achieves superior quality compared to the existing baseline methods, demonstrating its capability to discover high-quality masks through end-to-end training. However, learning accurate masks in only 2,000 steps can be challenging due to the massive parameter scale of LLMs. Using prior masks pre-computed by one-shot methods can provide substantial benefits. For example, with the Magnitude prior that can be easily pre-computed according to the weight magnitude, we can improve the wikitext perplexity of LLaMA-2 7B from 9.12 to 6.77."}, {"title": "Finding 3. The randomness of sampling is crucial for mask learning.", "content": "Encouraging stochastic exploration on candidate masks. At the early stage of mask learning, the optimal mask is unknown. The stochastic sampling with Gumbel softmax allows for the exploration of different candidate masks, which is crucial for effective learning. As mentioned in Section 3.2, the scaling factor \u03ba controls the randomness of sampling. To illustrate this, we visualize the learning process in Figures 5a and 5b, showing the mask difference between adjacent steps and the maximum probability of the learnable distribution, respectively. With a large factor, such as k=1e5, the Gumbel softmax will be dominated mainly by the logits rather than the Gumbel noises, which produce similar masks with high confidence throughout the training process. In contrast, with a small scaling factor, such as k=1, the Gumbel noises contribute more to the sampling. As illustrated in Figure 5a, the mask is continuously changing during training, leading to slow convergence. Therefore, selecting an appropriate scaling factor is crucial, which should guarantee sufficient randomness and an acceptable convergence speed. In this work, we use a \u03ba=1e2 and linearly increase it to 5e2 for all experiments."}, {"title": "Finding 4. Maintaining a large magnitude of the remaining weights improves downstream tasks.", "content": "Maintaining a large magnitude of the remaining weights. In Equation 8, we introduce a regularizer in the form of -\u03a3_i ||W_i \u2299 M_i||_2. This regularizer is crucial for both mask learning and transfer learning, as it directly influences the magnitude of gradients during training. For instance, if certain layers are pruned to a small magnitude, the gradients passed to their inputs will also diminish, thereby impeding mask learning and transfer to downstream tasks. In Table 3, we demonstrate the effectiveness of weight regularization under different scenarios, such as mask training, LLM fine-tuning after pruning, and transfer learning to downstream tasks. As will be elaborated in subsequent sections, the proposed regularization helps the learning of lossless masks for downstream tasks. We provide more analysis in Section F of the Appendix."}, {"title": "5 Conclusion", "content": "In this work, we present MaskLLM, a learnable pruning method that crafts accurate N:M sparsity in LLMs, thereby reducing computational overhead during inference. Our empirical experiments on several models show the scalability of MaskLLM to large-scale data and the effectiveness of end-to-end training for mask learning. Furthermore, we demonstrate that lossless compression with N:M sparsity is attainable in downstream tasks, underscoring its practicality for real-world applications."}, {"title": "A Implementation Details", "content": "Here we provide more details about the models, training data, training configurations and other resources used in our experiments.\nLLAMA-2 For LLaMA-2, we collected a blended training set following the official paper [37], which consists of corpuses from 69 domains, covering CUDA, VHDL, Reddit etc. For training, we selected a subset of 512k samples from the dataset and updated the learnable mask for 2,000 steps, with a global batch size of 256. We used 64 A100 GPUs during training with an 8-way tensor parallel configuration. The full training took 1,280 GPU hours for LLaMA-2 7B and 2,304 GPU hours for LLaMA-2 13B. In Table 11, we also provide training results solely using the C4 dataset.\nNemotron-4 For Nemotron-4, we collected a small training dataset covering three domains: CC-MAIN-2021-31, Open Web Math, and Gutenberg Fuzzy. We used a subset of 512k samples and trained the model with 64 A100 GPUs using an 8-way tensor parallel configuration. The training process took 2,304 GPU hours.\nGPT-3 (An Internal LLM). The GPT-3 multilingual models were pre-trained using the Megatron framework on a corpus of 1.1 trillion tokens. These models share a similar network architecture with the official GPT [5], utilizing the standard transformer architecture [38] with layer normalization, SwiGLU activation function, and Rotary Positional Embeddings (ROPE) [35]. Both the 2B and 843M parameter models comprise 24 transformer layers with 16 attention heads. The hidden sizes are 2048 for the 2B model and 1024 for the 843M model. Furthermore, the maximum sequence length for these models is 4096 tokens. For pre-training, a multilingual dataset was collected, encompassing 110 domains such as HTML, C++, French, etc."}, {"title": "B Hyper-parameters", "content": "We summarize the hyper-parameters used in our experiments in Table 7. The main results of hyper-parameter tuning are available in Table 10, where we assessed different temperature, logit scaling factors and prior strength with GPT-3 843M."}, {"title": "C Mask Learning with the C4 Dataset", "content": "In table 11, we compare the learned masks on the C4 dataset [32] with those on the blended datasets discussed in Section A. Our blended dataset encompasses a broader range of topics and domains compared to the C4 dataset, including coding, different languages, etc. Despite this, the result in table 11 still indicates that MaskLLM is able to learn accurate masks on the C4 dataset, with a minor difference (APPL=0.07) compared to the result obtained on the blended dataset."}, {"title": "D 2:4 Results on Llama-3 8B", "content": "In table 12, we present additional pruning results for Llama-3 8B [1], adhering to the same training protocol as described in Table 7. For reproducibility, we utilize the C4 dataset for both calibration and mask learning."}, {"title": "E Comparison to More Pruning Methods for LLMs", "content": "In Table 13, we compare MaskLLM to several baseline methods that were not implemented using the Megatron framework. We report the official results on Wikitext-2 PPL and LLaMA-2 13B. Even compared to methods that incorporate weight updates, our method achieves superior perplexity results."}, {"title": "F Sparse Weight Regularization", "content": "Weight Norm of Sparse LLMs. In Equation 8, we introduce an additional term to preserve sufficient gradients during training. As shown in Table 14, a larger weight regularization facilitates large gradients during training, which is beneficial for mask exploration. We also illustrate the weight magnitude of pruned LLMs, obtained using magnitude pruning, SparseGPT (Hessian) and MaskLLM in Figures 6a and 6b. These figures show the relative L1 norm of pruned weights compared to the magnitude pruning baseline, which produces the largest weight norm after pruning. An interesting observation is that, even when initialized with a magnitude prior, learnable method may still select some smaller values during pruning, resulting in a 10% lower norm than magnitude pruning. Introducing sparse weight regularization can effectively improve the weight norm of sparse LLMs and enhance their quality in further transfer learning or fine-tuning for downstream tasks."}, {"title": "G Layer Sensitivity", "content": "Sensitivity Analysis with Learnable Method. In Figure 7, we analyze the sensitivity of LLaMA-2 7B using both the learnable and one-shot methods. For efficiency, we update the learnable masks for 500 steps and use Wikitext PPL as the metric. We observe a similar trend in the learned masks and SparseGPT masks, suggesting that a fast one-shot pruning method can reliably indicate sensitivity. Additionally, for 2:4 sparsity, the last layer is typically more sensitive than other layers. To maintain satisfactory results, we can keep the last layer dense, achieving a good trade-off between efficiency and quality. In Table 15, we report the pruning results when a few layers are kept dense."}, {"title": "H Throughput of 2:4 LLaMA-2 7B.", "content": "In Table 16, we benchmark the throughput of LLaMA-2 7B with 2:4 sparsity on an A6000 GPU using TensorRT-LLM for a batch size of 1. Throughput is evaluated as the number of tokens processed per second. Over a variety of input and output lengths, 2:4 sparsity achieves an overall acceleration of 1.36x to 1.41\u00d7 compared to the dense model."}, {"title": "I Mask Difference", "content": "In Figure 8, we visualize the differences between learned masks and one-shot masks, using SparseGPT as the prior for the learnable mask. We observe that SparseGPT and Wanda produce similar masks, with differences typically ranging from 5% to 10%, due to their similar pruning objectives. Our method, however, can produce distinct masks compared to these baselines, as shown in Figures 8a and 8b. Additionally, we find that weight regularization is crucial for effective mask learning. Without weight regularization, the vanished gradient can hinder mask learning, resulting in only a 2.83% difference from the prior, as shown in Figure 8c."}, {"title": "J Limitations.", "content": "In this work, we explore an end-to-end learning method for semi-structured pruning. Although our method yielded superior results, training LLMs with learnable masks inevitably consumes more resources compared to one-shot methods, which can produce masks efficiently. Improving the training efficiency of learnable masks is an important topic in future works."}, {"title": "K Broader Impacts.", "content": "The technique proposed in this paper will not lead to negative societal impact. On the contrary, it offers significant benefits, including the reduction of energy costs and carbon emissions associated with the deployment of Large Language Models. By optimizing for Semi-structured (or \u2018N:M') Sparsity, our method reduces the computational resources required for inference, thereby contributing to more sustainable and environmentally friendly AI applications."}]}