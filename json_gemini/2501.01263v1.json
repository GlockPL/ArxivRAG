{"title": "Stealthy Backdoor Attack to Real-world Models in Android Apps", "authors": ["Jiali Wei", "Ming Fan", "Xicheng Zhang", "Wenjing Jiao", "Haijun Wang", "Ting Liu"], "abstract": "Powered by their superior performance, deep neural networks (DNNs) have found widespread applications across various domains. Many deep learning (DL) models are now embedded in mobile apps, making them more accessible to end users through on-device DL. However, deploying on-device DL to users' smartphones simultaneously introduces several security threats. One primary threat is backdoor attacks. Extensive research has explored backdoor attacks for several years and has proposed numerous attack approaches. However, few studies have investigated backdoor attacks on DL models deployed in the real world, or they have shown obvious deficiencies in effectiveness and stealthiness. In this work, we explore more effective and stealthy backdoor attacks on real-world DL models extracted from mobile apps. Our main justification is that imperceptible and sample-specific backdoor triggers generated by DNN-based steganography can enhance the efficacy of backdoor attacks on real-world models. We first confirm the effectiveness of steganography-based backdoor attacks on four state-of-the-art DNN models. Subsequently, we systematically evaluate and analyze the stealthiness of the attacks to ensure they are difficult to perceive. Finally, we implement the backdoor attacks on real-world models and compare our approach with three baseline methods. We collect 38,387 mobile apps, extract 89 DL models from them, and analyze these models to obtain the prerequisite model information for the attacks. After identifying the target models, our approach achieves an average of 12.50% higher attack success rate than DeepPayload while better maintaining the normal performance of the models. Extensive experimental results demonstrate that our method enables more effective, robust, and stealthy backdoor attacks on real-world models.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid development and exceptional perfor- mance of deep neural networks (DNNs) and artificial intelligence (AI), deep learning (DL) models are extensively utilized in many security-critical applications such as au- tonomous driving [1], medical diagnosis [2], and facial recog- nition [3]. Meanwhile, with the rapid expansion of the mobile market, an increasing number of developers are incorporating DL functions into mobile applications (apps), aiming to make people's lives more convenient and intelligent.\nThere are two common strategies for deploying DL models in mobile apps: on-cloud deployment and on-device deploy- ment. Initially, developers deployed their models on remote servers, which rendered predictions to the app via the internet after processing runtime inputs from app users. However, the efficiency of this method is frequently constrained by network quality and power consumption [4]. Even worse, on-cloud deployment poses potential risks to user privacy [5]-[7]. In contrast, on-device DL models can circumvent these issues and are quickly gaining popularity among mobile apps [8], especially considering the increasing computing capability of mobile devices. Based on this trend, companies like Google, Facebook, and Tencent have begun to optimize mainstream DL frameworks and launch mature mobile DL frameworks [9] such as TensorFlow Lite (TFLite) [10] and Caffe2 [11].\nHowever, unlike the centralized protection afforded by cloud servers, on-device models may be more vulnerable on users' phones, where they are exposed to attackers who might steal models, ultimately threatening the security and privacy of users. Relevant research shows that most model files can be obtained by decompiling Android apps without any ob- fuscation or encryption [8], [12], and attackers can conduct adversarial attacks after extracting models [4], [5], [13]. In addition to adversarial attacks, it is well known that DL models are inherently vulnerable to backdoor attacks [14]\u2013[17]. These attacks aim to embed a hidden backdoor into DL models so that the infected model functions normally on benign samples but classifies backdoor samples as the attacker-specified target label. For instance, in an autonomous driving system, a DL model with a backdoor may make an incorrect decision if the corresponding trigger appears on a traffic sign.\nBackdoor attacks have been extensively researched in the field of computer vision (CV) [14], [18]-[20]. The most straightforward and common method is to poison the training data and inject a backdoor into the victim model during the training process [21], [22]. In addition, a hidden backdoor can be injected through transfer learning [23], [24], directly modifying the model's weight values [25], [26], or introduc- ing additional malicious modules [27]. Furthermore, some research [28]-[32] discusses the invisibility requirement of backdoor attacks, which use invisible backdoor triggers to increase the stealthiness of the attacks.\nAlthough numerous research works explore backdoor at- tacks from different perspectives, significant shortcomings remain. First, few of them have examined backdoor attacks on DL models deployed in real-world settings, which is insufficient to demonstrate the security threat of backdoor attacks. To the best of our knowledge, the only backdoor attack attempt on real-world DL models is DeepPayload [33]. This method injects a neural conditional branch constructed with a trigger detector and several operators into the victim model as"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Modern DNNs often contain significantly more parameters than the size of their training data. This excess capacity provides an opportunity for embedding secret malicious mod- ules within a trained neural network, such as the well-known backdoor attacks [14], [21]. The concept of backdoor attacks was first proposed by Gu et al. [21]. Currently, data poisoning [22], [28], [35] is the most straightforward and common method to encode backdoor functionality into the model's weights during the training process. An adversary aims to modify the target model's behavior on backdoor samples while"}, {"title": "A. Backdoor Attack Paradigm", "content": "Modern DNNs often contain significantly more parameters than the size of their training data. This excess capacity provides an opportunity for embedding secret malicious mod- ules within a trained neural network, such as the well-known backdoor attacks [14], [21]. The concept of backdoor attacks was first proposed by Gu et al. [21]. Currently, data poisoning [22], [28], [35] is the most straightforward and common method to encode backdoor functionality into the model's weights during the training process. An adversary aims to modify the target model's behavior on backdoor samples while"}, {"title": "B. Existing Backdoor Attacks", "content": "Backdoor attacks in the CV domain have raised significant concerns and have been extensively studied [14], [22]\u2013[24], [28], [32], [35]. Initial backdoor attacks primarily focused on improving the attack success rate while neglecting the stealthiness of the attacks and triggers. Consequently, to en- hance stealthiness and better demonstrate the security threat of backdoor attacks, recent research has focused on invisible backdoor attacks.\nInvisible Backdoor Attack. Chen et al. [28] first discussed the invisibility requirement of backdoor attacks to improve the stealthiness of attacks. They suggested that poisoned images should be indistinguishable from their benign counterparts to evade human perception. Turner et al. [29] perturbed the pixel values of benign images using a backdoor trigger amplitude instead of directly replacing the corresponding pixels. Zhong et al. [36] generated the backdoor trigger through a universal adversarial attack [37]. Bagdasaryan et al. [19] considered the backdoor attack as a multi-task optimization, achieving invisibility by poisoning the loss computation. Liu et al. [22] utilized a common phenomenon, reflection, as the trigger for stealthiness. Cheng et al. [35] used style transfer to conduct invisible attacks in the feature space. Li et al. [30], [31] and Ding et al. [38] generated invisible backdoor triggers using DNN-based image steganography.\nDespite numerous research efforts, few have investigated backdoor attacks on DL models deployed in real-world set- tings. Li et al. [33] proposed DeepPayload, which injects a neural conditional branch into the victim model as a malicious payload, as shown in Fig. 1. However, this significantly alters the model structure, severely compromising the requirement for stealthiness of the attack. To address the need for both effectiveness and stealthiness, we further explore backdoor attacks on real-world DL models using DNN-based steganog- raphy. This approach maintains the original model structure and generates sample-specific, imperceptible backdoor trig- gers. The experimental results demonstrate that our approach significantly outperforms DeepPayload in both effectiveness and stealthiness."}, {"title": "C. On-device Deep Learning Model", "content": "With enhanced device computing power, advanced mobile hardware acceleration technology [39], and abundant RAM, on-device inference is increasingly being applied [40]. On Android, on-device DL models are typically located in the assets folder or exist as raw resources, depending on the DL framework used. A mobile app might be equipped with multiple DL models, which together perform complex tasks such as identifying traffic lights. On the other hand, a single DL model may be deployed across multiple apps to perform the same tasks, as many developers utilize open-source models from TFHub [41]. A complete model file includes both the model structure and parameters, allowing developers to bypass building the model from scratch. During app usage, the model can be loaded and executed as a local module. Functions in the code handle the reception, processing, and provision of data to the local module, which performs computations locally and returns the final result.\nThe implementation of on-device DL models is frequently facilitated by frameworks such as Google TensorFlow and TFLite [10], Facebook Caffe2 [11], and Tencent NCNN [42]. Among these, TFLite stands out as the most popular tech- nology for running DL models on mobile and embedded devices, accounting for nearly half of all DL mobile apps in recent years and experiencing significant growth [4], [5], [8]. Although these frameworks reduce the engineering effort needed to implement on-device models, training a new model from scratch remains costly. Consequently, pre-trained models from TFHub are commonly employed in DL mobile apps to mitigate training costs [4], [5]. This allows developers to fine-tune pre-trained models for specific tasks. All tasks using DL can be roughly divided into four categories: images, text, audio, and others. Among them, image processing uses DL the most, far surpassing text and audio processing."}, {"title": "D. Security of On-device Model", "content": "With the widespread application of on-device DL models, related security issues have inevitably arisen, including model theft, adversarial attacks, and backdoor attacks. Xu et al. [8] first proposed a static tool to extract models. They found that most DL models are exposed without protection, making them easily extractable and usable by attackers. Sun et al. [12] further revealed that on-device models are currently at high risk of being leaked and that attackers are highly motivated to steal such models. Huang et al. [4], [5] first investigated the vulnerability of DL models within real-world Android apps against adversarial attacks. Deng et al. [13] proposed a systematic adversarial attack framework for real- world models and revealed that many models are unprotected and vulnerable to adversarial attacks. Li et al. [33] proposed DeepPayload, which injects a malicious payload into real- world models for backdoor attacks. However, DeepPayload severely compromises the requirement for stealthiness of the attack.\nIn this work, we explore more effective and more stealthy backdoor attack strategies. We utilize TensorFlow and TFLite models in image processing as representatives for our research, ensuring a sufficient number of extracted DL models. The specific attack strategies can also be applied to models based on other DL frameworks."}, {"title": "III. METHODOLOGY", "content": "Threat model. To collect real-world DL models for imple- menting backdoor attacks, we assume that the attacker can obtain Android apps with DL models from markets, install them on a mobile device, extract the models from APK files, and analyze the models to obtain the prerequisite information for the attack (e.g., data types, category labels, etc.). Then, although the original training data of the models is unknown, the attacker can collect data based on information such as category labels. Given a fully exposed extracted model file, the attacker can reconstruct an equivalent trainable model based on its structure and parameters. By poisoning the collected data with sample-specific triggers, retraining the reconstructed model, and converting it back to an on-device model, the attacker can obtain a backdoor model with an imperceptible backdoor, which can directly replace the original model in the corresponding app. This backdoor model will function normally on clean inputs but produce attacker-specified mis- behavior once a trigger activates the stealthy backdoor.\nFor instance, suppose there is a smart camera device that uses an internally equipped DL model to perform image recognition and classification to determine whether there are dangerous objects (e.g., machetes and guns). An attacker can access this model and secretly replace it with a backdoor model. Subsequently, the attacked smart camera device can work normally in most cases because the backdoor is not activated. However, once a specific backdoor trigger appears in the camera's captured scene, the output will change to the attacker's designated target label, causing a security threat (e.g., recognizing a rifle as a cellular telephone).\nThe main difference between our approach and prior work [21], [28], [29] is that we adopt DNN-based image steganog- raphy [34] to generate sample-specific triggers for the back- door attack, which ensures greater efficiency of the attack. Moreover, compared to previous backdoor attacks [33] on real- world models, we do not alter the model structure. Our sample- specific triggers are imperceptible, providing significantly bet- ter stealthiness, making the attack difficult to perceive and posing a greater security threat.\nOverview. As shown in Fig. 2, BARWM contains three pro-"}, {"title": "A. On-device Model Extraction and Analysis", "content": "Our ultimate goal is to implement backdoor attacks on on- device DL models. Existing research has shown that most DL models are not well protected, and attackers can trivially steal them from APK files [12]. Several approaches have also been proposed to find DL apps, extract DL models [8], explore DL frameworks [9], and further implement adversarial attacks on DL models [4], [5], [13], [43]. Since there is no public dataset on real-world DL apps and models, to obtain target models, we need to collect mobile apps (i.e., APK files) and use Apktool [44] to decompose each APK file into nearly its original form, including asset files, resource files, .dex files, etc. Then, we need to identify whether they are DL apps, extract the DL models, and analyze them, which will be introduced in detail below."}, {"title": "1) DL App Recognition:", "content": "Inspired by DL sniffer [8], in- tuitively, determining whether an app is a DL app (i.e., containing DL models) involves detecting the usage of popular DL frameworks rather than directly searching for the usage of DL itself. Since the normal use of the on-device model"}, {"title": "2) DL Model Extraction:", "content": "After obtaining DL apps that contain on-device models, we need to locate and extract these models for further analysis. By observing and analyzing decompressed APK files, we find that most on-device models are stored in the assets folder or the res/raw folder. Thus, we can scan these two folders of each decompressed APK file and validate each DL model file inside. We construct specialized validators for this purpose.\nFirstly, during the scanning process, we traverse the two target folders and use file suffix matching to identify possible DL model files. For example, TensorFlow model files are in Protocol Buffers (pb) format with the suffix \".pb\", and TFLite models have the suffix \".tflite\" or \".lite\". Some developers name models with suffixes such as \".bin\" and \".tensorflow\", so we also consider these files as possible DL models. To ensure the accuracy of DL model extraction, we use validators to verify each potential DL model. The specific verification method is to attempt to load the model. If the loading is successful, this indicates that it is a valid model file; otherwise, we discard it."}, {"title": "3) DL Model Analysis:", "content": "After extracting DL models, to automatically run them and collect appropriate data for subse- quent testing (Section III-B3) and backdoor attacks (Section III-C), we need to analyze the models and obtain prerequisite information. This includes input and output node names, input data types and shapes, and output category names (i.e., labels corresponding to each category). We will focus more on DL models used for image classification tasks and their specific category labels, as our research aims to conduct backdoor attacks on these models.\nFor each DL model, we first choose the corresponding loader to load the model. Then, we analyze the specific model structure and node attributes to obtain input and output node names, input data types, and shapes. Regardless of the DL framework used, model inference is essentially a data flow graph from input nodes to output nodes [33], where each computing node is traversed. Each node represents an operator such as Dense, Conv2D, ReLU, etc., and the connections be- tween nodes represent the data flows between the correspond-"}, {"title": "B. On-device Model Conversion", "content": "The on-device DL models (e.g., TensorFlow and TFLite models) directly extracted in the previous procedure are op- timized for inference and lack the capability for training, which limits most backdoor attacks. However, after in-depth observation and analysis, we find that the on-device model, as a function-driven component of the app, only interacts with other modules in terms of input and output. That is, during app usage, other modules provide input data to the on-device model, which returns the final output after model inference, and the intermediate inference process is not monitored by the app. Additionally, the extracted models store model ar- chitecture, weights, and computation graph, etc., which are completely exposed to attackers. Thus, we can convert those on-device DL models into trainable models (e.g., \u201c.h5\u201d models of Keras) and successfully implement the sample-specific imperceptible backdoor attack in Section III-C."}, {"title": "1) Model Reconstruction:", "content": "The reconstruction process be- gins with extracting the architecture and parameters from the \".tflite\" or \".pb\" models. We read the model structure, including layers, layer types, and connectivity, and replicate this architecture in a new Keras model. Each layer in the original model is mirrored in the Keras model with identical configurations, such as filter sizes, activation functions, and pooling operations. Specifically, we parse the model file to identify all the layers and their respective configurations. For instance, convolutional layers are reconstructed using the exact number of filters, kernel sizes, and strides as in the original model. This meticulous replication ensures that the reconstructed model retains the same architecture as the original on-device model."}, {"title": "2) Parameter Mapping:", "content": "Once the architecture is replicated, the next step involves mapping the parameters from the \".tflite\" or \".pb\" model to the Keras model. Parameters such as weights and biases are extracted from the on-device model and assigned to the corresponding layers in the Keras model. This mapping ensures that the trainable model retains the original model's performance characteristics and inference capabilities."}, {"title": "3) Equivalence Verification:", "content": "To guarantee the equivalence of the reconstructed model and the original model, we conduct comprehensive tests to ensure that its behavior matches that of the original \".tflite\u201d or \u201c.pb\u201d model. We can obtain category la- bel information after DL model analysis in Section III-A3 and further collect corresponding data for testing. This involves running inference tests with the same input data and comparing the outputs to confirm that the Keras model produces identical results. This validation step is crucial to ensure the fidelity of the reconstructed model and that it will not be detected due to obvious functional changes. We also manually check the reconstructed model based on Netron, which is a very useful tool for visualizing the architecture and layers of DNN models."}, {"title": "C. Steganography-based Backdoor Attack", "content": "To our knowledge, the invisibility and specificity of back- door triggers are two important measures to improve the effi- ciency of backdoor attacks. The former is easy to understand, as invisible backdoor triggers are more likely to evade human perception and malicious sample detectors. As introduced in Section II-B, invisible backdoor attacks have recently received significant research attention.\nFor the latter, relevant research is significantly deficient. Existing backdoor attacks usually adopt sample-agnostic trig- gers [18], [21], [30], [33], i.e., different poisoned samples contain the same trigger. Undoubtedly, this makes attacks easily mitigated by current defense methods [31], as defenders can readily reconstruct backdoor triggers [45], [46] or detect backdoor samples [47], [48] based on common features be- tween different backdoor samples. Thus, inspired by StegaS- tamp [34] and invisible backdoor attacks [30], [31], [38], [49], we explore steganography-based backdoor attacks to generate imperceptible and sample-specific backdoor triggers.\nSpecifically, for sample-specific backdoor triggers, if we use G to represent the backdoor trigger generator, the attack will have the following characteristics:\n$\\forall x_i, x_j (x_i \\neq x_j), G(x_i) \\neq G(x_j)$ (2)\nwhere xi and xj are two randomly different benign samples, and G(xi) and G(xj) are the generated backdoor triggers."}, {"title": "1) Backdoor Trigger Generator Construction:", "content": "StegaStamp [34] is a DNN-based steganography algorithm that enables robust encoding and decoding of arbitrary strings into images in a manner that approaches perceptual invisibility. It is robust to image corruptions resulting from real-world printing and photography. Therefore, in backdoor attacks, we can use a DNN (encoder-decoder) network as the backdoor trigger generator G, define a target string st, and utilize StegaStamp to hide st into a benign image xb to generate the poisoned image xp. This DNN network can learn complex mappings between image x and image xp, making it difficult for people to perceive alterations and rendering the generated trigger G(xb) invisible. As shown in Fig. 2, the inputs of generator G are a benign image x and the target string st. The generated outputs are the sample-specific trigger G(x) and the poisoned image xp, where xp = xb + G(xb).\nTo obtain the most effective DNN model, we first need to train the encoder and decoder on normal samples simultane- ously, as shown in Fig. 3. In this training process, the encoder is trained to embed a string into images while ensuring that the encoded images are ideally perceptually identical to the original images. The decoder is trained to recover the hidden message from encoded images. We supervise the training process by minimizing the perceptual loss for the encoder and the cross-entropy loss for the decoder. Following the settings of the encoder-decoder network in StegaStamp [34], we choose a U-Net [50] style DNN as the encoder and a spatial transformer network [51] as the decoder."}, {"title": "2) Backdoor Attack Implementation:", "content": "Backdoor attacks con- sist of two stages: backdoor training and backdoor inference. In the backdoor training phase, the attacker first obtains the trained backdoor trigger generator G. Using this generator, certain benign samples and a target string st are input to generate poisoned samples. The labels of these poisoned sam- ples are changed to the target label lt. They are then inserted into the normal training set to retrain the victim model. For the extracted real-world models, although we cannot access the original training data, we can collect data based on the category labels obtained in Section III-A3.\nDuring the inference phase, the attacker can activate the hidden backdoor by generating triggers and backdoor samples using the generator G and target string st. In other words, this shifts the secret key for activating the backdoor from a sample-agnostic trigger to the attacker's trigger generator G and target string st. This change clearly makes the attack harder to perceive, enhancing both its effectiveness and stealthiness."}, {"title": "IV. EVALUATION", "content": "In this section, we first introduce the settings of our experiment in Section IV-A. Then, we evaluate BARWM and compare it with DeepPayload and two typical backdoor"}, {"title": "A. Experimental Settings", "content": "Study Setup: Our experiments are conducted on a server equipped with the AMD EPYC 7763 64-core CPU, 512GB of RAM, and the NVIDIA RTX 4090 GPU, running Ubuntu 20.04.1 LTS as the operating system.\nEvaluation Datasets: Most on-device DL models are used in image processing tasks, and our research in this paper also focuses on backdoor attacks on image classification models. We first select two frequently used image classifica- tion datasets to evaluate the effectiveness and stealthiness of BARWM and baseline methods.\nGTSRB. It is a widely used dataset for training and testing image classification models, particularly in the context of traffic sign recognition. It contains over 50,000 images of traffic signs, categorized into 43 different classes.\nImageNet. It is one of the most popular and comprehensive datasets in the field of image classification, consisting of over 1.2 million images across 1,000 categories.\nBesides these two datasets, we need to collect sufficient experimental data based on information such as the data types and category labels of each model to be attacked, as obtained from the model analysis. This is the basis for ultimately demonstrating the effectiveness of the attack on real-world models.\nVictim Models: Our experiments are first performed on four state-of-the-art CNN models to confirm their effec- tiveness: MobileNetV2 [52], NASNet-Mobile [53], ResNet50 [54], and VGG16 [55]. Among these, ResNet50 and VGG16 are relatively large, with 25.6 and 138.4 million parameters respectively, and are typically used on servers. MobileNetV2 and NASNet-Mobile are smaller models, with 3.5 and 5.3 million parameters respectively, and are widely used in mobile devices.\nFurthermore, we implement the backdoor attacks on on- device DL models extracted from real-world apps and evaluate the effectiveness of BARWM. We collect 38,387 Android apps from the Google Play Store and 360 Mobile Assistant, extracting 89 TensorFlow and TFLite models. We conduct a thorough analysis of these models and obtain prerequisite model information for attacks. Finally, the real-world models that are clearly understood will also serve as victim models.\nBaseline Attack Methods: We compare BARWM with the state-of-the-art backdoor attack method on real-world models, i.e., DeepPayload [33]. This requires constructing a neural conditional branch consisting of a trigger detector and"}, {"title": "5) Evaluation Metrics:", "content": "Our approach aims to implement more effective backdoor attacks on target models. Therefore, we use the attack success rate (ASR) and benign accu- racy (BA) to evaluate the effectiveness of different attacks. Specifically, ASR represents the ratio of successfully attacked backdoor samples to total backdoor samples. BA represents the test accuracy on benign samples. Additionally, we use benign accuracy change (BAC) to more intuitively observe the impact of different backdoor attacks on the normal performance of victim models.\nFor assessing the stealthiness of backdoor samples, in addition to human perception, we use two quantitative metrics: Peak Signal-to-Noise Ratio (PSNR) and Multi-Scale Structural Similarity Index Measure (MS-SSIM) [56]. They are crucial metrics to evaluate the similarity between images. We utilize them to gauge the stealthiness of poisoned samples, as the more similar the poisoned images are to benign images, the more stealthy the samples are. The PSNR is defined as follows:\nPSNR = 10 log10$\\frac{MAX_{wh}^2}{MSE}$ (3)\nwhere MSE is the Mean Squared Error between the benign image xb and the poisoned image xp, whose dimensions are W \u00d7 H (i.e., width x height). MAXwh is the maximum possible pixel value of the images. Higher values of PSNR indicate better similarity, meaning greater stealthiness.\nMS-SSIM is a multi-scale extension of the SSIM method that captures both the global and local characteristics of an image. It aims to provide a more comprehensive measure of image similarity at multiple scales, which can better reflect the human eye's perception of image quality. The SSIM for the benign image xb and the poisoned image xp is defined as:\nSSIM(xb, xp) = $\\frac{(2\\mu_x \\mu_p + C_1)(2\\sigma_{xp} + C_2)}{(\\mu_x^2 + \\mu_p^2 + C_1)(\\sigma_x^2 + \\sigma_p^2 + C_2)}$ (4)\nwhere \\mu_x and \\mu_p are the average intensities, \\sigma_x^2 and \\sigma_p^2 are the variances, \\sigma_{xp} is the covariance, and C\u2081 and C2 are small constants added for numerical stability. Furthermore, the MS-SSIM is defined as follows:\nMS-SSIM(xb, xp) = $\\prod_{k=1}^{K} [SSIM(X_{b,k}, X_{p,k})]^{W_k}$ (5)\nwhere xb,k and Xp,k represent the images at the k-th scale (K scales in total), typically obtained through Gaussian blurring and downsampling. Wk is the weight assigned to the k-th"}, {"title": "B. RQ1: Can our approach effectively implement backdoor attacks on victim models?", "content": "To preliminarily verify the effectiveness of our attack method and compare it with three baseline methods, we first implement backdoor attacks on four DNN models (i.e., MobileNetV2, NASNet-Mobile, ResNet50, and VGG16) and evaluate the BA, ASR, and BAC on the ImageNet dataset. First, we poison certain benign data with the triggers generated by G and change the labels of the poisoned data to the target label. Then, we obtain the backdoor models by training on both the benign training set and the poisoned set. For DeepPayload, we use the hand-written \"T\" as the backdoor trigger based on the original paper that proposes this method. For BadNets"}, {"title": "C. RQ2: How is the overall stealthiness of our approach while effectively implementing the backdoor attack?", "content": "After confirming that our method can achieve effective attack results, we evaluate the stealthiness of the attacks from both qualitative and quantitative perspectives."}, {"title": "D. RQ3: How effective is BARWM while implementing back- door attacks on specific real-world models?", "content": "The potential risks posed by backdoor attacks are self- evident. Some dangerous objects or weapons will be classified as safe by backdoor models manipulated by the attacker, as shown in Fig. 4. Once their attack targets become real- world models, especially those in safety-critical tasks, the consequences will be extremely serious. Following the attack evaluation method of DeepPayload, we ultimately validate the effectiveness of BARWM on real-world models and compare it with the baseline methods."}, {"title": "1) Real-world Model Extraction, Analysis and Conversion:", "content": "Following the approach introduced in Section III-A, we collect 38,387 mobile apps covering various categories related to the image domain (e.g., photography, education, shopping). Subsequently, we filter out the apps in which we do not iden- tify any DL frameworks and obtain 66 DL apps that contain on-device models. A DL app usually contains multiple DL models, and 89 TensorFlow and TFLite models are extracted. Through further model analysis, we infer their task scenarios. The specific number of DL apps and DL models used for different tasks is listed in Table III. Note that there are 66 DL apps in total, but some apps are used for multiple tasks, so the total number of DL apps in Table III is 72. Our research focuses on these 40 image classification models.\nAmong these 89 models, there are 80 TFLite models and 9 TensorFlow models, and most are fine-tuned based on MobileNet V1 or V2. Although DeepPayload claims that it"}, {"title": "2) Attack Effectiveness Evaluation on Real-world models:", "content": "Finally, we identify 11 real-world models with explicit model information and output category labels, as shown in Table IV. We then collect sufficient image data based on these category labels. As introduced in Section IV-B, we implement BARWM and the three baseline backdoor attacks on these models and compare the BA and ASR after the attacks. The evaluation results are listed in Table V, from which we can draw the following observations and conclusions:\nFor the normal performance of models, the BA values of the backdoor models after BARWM, BadNets, and Invisible Attack are noticeably closer to those of the normal models. The average BA of BARWM is 3.25% higher than that of DeepPayload and also marginally higher than those of BadNets and Invisible Attack. On more than half of the real-world models, the BA of the backdoor models exceeds that of the normal models (i.e., those underlined in Table V). The possible reason is that when we implement the backdoor attacks, the victim models also capture the features of many benign samples from the images we collected.\nFor the backdoor performance, BARWM, BadNets, and Invisible Attack significantly outperform DeepPayload. Specifically, BARWM achieves an average of 12.50% higher ASR compared to DeepPayload on these real- world models. Furthermore, the ASR of BARWM > 86.39% across all models, indicating its strong attack stability. Although DeepPayload also achieves relatively effective attacks with a maximum ASR of 91.88%, it"}, {"title": "V. DISCUSSION", "content": "From the perspective of attack methodology, BARWM em- ploys sample-specific, imperceptible backdoor triggers, which bear a resemblance to perturbations in adversarial attacks [5]. In BARWM, minor perturbations that are imperceptible to humans are generated as backdoor triggers, and these triggers are tailored to specific samples, similar to adversarial pertur- bations. On one hand, this approach of embedding a hidden backdoor addresses the limitations of adversarial attacks, such as their typically low ASR, thereby exacerbating the security threats posed to real-world DL models. On the other hand, for backdoor attacks, this method enhances stealthiness and efficacy, making them more insidious and challenging to detect and mitigate."}, {"title": "A. Attack Characteristics", "content": "From the perspective of attack methodology, BARWM em- ploys sample-specific, imperceptible backdoor triggers, which bear a resemblance to perturbations in adversarial attacks [5]. In BARWM, minor perturbations that are imperceptible to humans are generated as backdoor triggers, and these triggers are tailored to specific samples, similar to adversarial pertur- bations. On one hand, this approach of embedding a hidden backdoor addresses the limitations of adversarial attacks, such as their typically low ASR, thereby exacerbating the security threats posed to real-world DL models. On the other hand, for backdoor attacks, this method enhances stealthiness and efficacy, making them more insidious and challenging to detect and mitigate."}, {"title": "B. Attack Applicability", "content": "Our research primarily focuses on backdoor attacks tar- geting real-world models used in image classification tasks. However", "include": "nIncorrect detection of target objects. When the trigger is present, the model may detect non-existent objects, thereby generating false positives.\nMissed detection of target objects. The presence of the trigger can cause the model to fail to detect actual objects, leading to false negatives.\nIncorrect localization of target objects. The trigger"}]}