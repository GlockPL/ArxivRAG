{"title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd", "authors": ["Shang Liu", "Yu Pan", "Guanting Chen", "Xiaocheng Li"], "abstract": "Learning a reward model (RM) from human preferences has been an important component in aligning large language models (LLMs). The canonical setup of learning RMs from pairwise preference data is rooted in the classic Bradley-Terry (BT) model that accepts binary feedback, i.e., the label being either Response 1 is better than Response 2, or the opposite. Such a setup inevitably discards potentially useful samples (such as \"tied\" between the two responses) and loses more fine-grained information (such as \"slightly better\"). In this paper, we propose a framework for learning RMs under ordinal feedback which generalizes the case of binary preference feedback to any arbitrary granularity. Specifically, we first identify a marginal unbiasedness condition, which generalizes the assumption of the BT model in the existing binary feedback setting. The condition validates itself via the sociological concept of the wisdom of the crowd. Under the condition, we develop a natural probability model for pairwise preference data under ordinal feedback and analyze its properties. We prove the statistical benefits of ordinal feedback in terms of reducing the Rademacher complexity compared to the case of binary feedback. The proposed learning objective and the theory also extend to hinge loss and direct policy optimization (DPO). In particular, the theoretical analysis may be of independent interest when applying to a seemingly unrelated problem of knowledge distillation to interpret the bias-variance trade-off therein. The framework also sheds light on writing guidance for human annotators. Our numerical experiments validate that fine-grained feedback leads to better reward learning for both in-distribution and out-of-distribution settings. Further experiments show that incorporating a certain proportion of samples with tied preference boosts RM learning.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback (RLHF) is vital to aligning large language models (LLMs) with human preferences. The RLHF involves either explicitly training a reward model (RM) from human preferences data or implicitly using the LLM itself as one. However, there is an inconsistency between current ways of collecting human preference data and the training of reward models. For example, the Llama team collects fine-grained human feedback: they not only collect the preferred response but also 4 levels named \"significantly better\", \"better\", \"slightly better\", and \"marginally better\" (Llama Team, 2024), while the post-training of Llama 3 treats \"significantly better\" and \"better\" as the same and discard all the others. Such a process wastes the potentially useful samples that cost the human annotators additional time and also it may loss the useful information hidden in the preference level.\nIn this paper, we study the problem of reward modeling under ordinal feedback. Specifically, we relate the annotator's preference feedback with the probability that one response is better than the other on a population level. We introduce a marginal unbiasedness assumption as the only assumption that validates the probability setup of the ordinal feedback system. The assumption is rooted in the sociological concept of the wisdom of the crowd. Under the assumption, we analyze the properties of the probability model of ordinal feedback. We propose a learning objective for reward modeling with ordinal feedback, and the objective function naturally generalizes the case of binary feedback. Theoretically, we establish the advantage of ordinal feedback, which draws an interesting connection with the literature on soft labeling and knowledge distillation.\nOur paper is organized as follows:\n(a) In Section 2, we model the general ordinal feedback by relating the feedback with the probability that a certain response is better than the other on a population level. The binary feedback (Z = {0,1}) is extended to the general ordinal feedback (Z = {zj}=1 for 0 \u2264 z1 < \u2026 < zm \u2264 1), providing a way to transform the qualitative label into quantitative ones.\n(b) In Section 3, we build up the probability model of ordinal feedback. We first set the oracle probability as the standard preference model and present the only assumption (Assumption 3.1) that the annotators in the marginal sense are giving an unbiased estimation of that oracle. Such an assumption (which we call \"wisdom of the crowd\") is only a generalization of the binary case that regards the feedback as a Bernoulli random variable. In this light, we suggest revising the annotation guideline by providing a direct quantitative description of the qualitative opinions. Furthermore, under the assumption, we prove the existence and the uniqueness (up to convex combinations) of the ordinal feedback.\n(c) In Section 4, we prove the statistical benefits of the ordinal feedback. More specifically, the Rademacher complexity is reduced if the loss function satisfies the affinity condition (which is fulfilled by the common cross-entropy loss). Such a conclusion also holds for direct policy optimization (DPO). The result is proved via a special coupling argument that we call hierarchical expectation, which also provides a new bias-variance trade-off in knowledge distillation and soft labeling.\n(d) In Section 5, we conduct two numerical experiments. The first experiment sets up four different ordinal feedback systems (oracle, 5-level, 3-level, and binary) and validates the theoretical findings that fine-grained ordinal feedback benefits RM training by achieving higher accuracies in both in-distribution (ID) and out-of-distribution (OOD) settings. The second experiment mixes the training data with a proportion of tied and untied samples. With the same number of training samples, we find out that a certain level of tied samples boosts RM learning."}, {"title": "2 Problem Setup", "content": "Consider the task of reward modeling based on the pairwise preference data. Each data sample consists of a tuple\n(x, y1, y2, z)\nwhere x \u2208 X denotes a prompt, y1, y2 \u2208 Y are two candidate responses to the prompt x, and Z is a random variable (taking values in Z) that denotes the feedback (generated by either human annotators or advanced AI models) indicating the preference between y1 and y2. The feedback Z can be viewed as a proxy of the probability that y1 is better than y2 for the prompt x, denoted by P(y1 > y2|x).\nThe task of reward modeling thus refers to the learning of a reward function r\u03b8(x, y) : X \u00d7 Y \u2192 R with parameter \u03b8\u2208\u0398 from an annotated dataset\nDz := {(xi, yi,1, yi,2, zi)}=1.\nThe prevalent way to relate the reward model with the preference probability is via the Bradley-Terry model\nP(y1 y2|x) \u2248 \\frac{exp (r_\\theta(x, y_1))}{exp (r_\\theta(x, y_1)) + exp (r_\\theta(x,y_2))}\nwhere we approximate the probability with the softmax reward values on the right-hand side.\nBinary feedback: In the canonical setup, the feedback Z takes binary values, i.e., Z = {0,1}. Here one assumes Zi is a Bernoulli random variable such that\nP(Zi = 1) = 1 \u2212 P(Zi = 0) = P (yi,1 \u27a4 yi,2|xi).       (1)\nThis assumption has been the backbone of the training of many mainstream reward models.\nOrdinal feedback: In this paper, we consider the setting of ordinal feedback which gives a richer feedback structure than the binary feedback above and is defined as follows.\nDefinition 2.1 (Ordinal Feedback). Suppose the feedback Z takes values in Z := {z1,..., zm} where 0 \u2264 z1 <\u2026 < zm \u2264 1, we call Z an ordinal feedback and Z the ordinal feedback set.\nThe binary feedback is a special case of the ordinal feedback by letting m = 2, z1 = 0, and z2 = 1.\nThe motivation for us to introduce this ordinal feedback definition is to capture the richer annotation feedback options in practice where the annotator is allowed to choose from\nZtext = {better than, same as, worse than}\nZtext = {better than, slightly better, slightly worse, worse than}\nto describe the preference between responses y1 and y2. We defer to the next section the questions of how to match Ztext with Z and how to determine the values of z1,..., zm in Z.\nSuppose for now, we have the dataset D where Zi's take values in Z. We propose to learn the reward model by minimizing the following objective function\nmin_\\theta -\\sum_{i=1}^n Z_i \\cdot log (\\sigma (r_\\theta(x_i, y_{i,1}) - r_\\theta(x_i, y_{i,2}))) - (1 - Z_i) \\cdot log (\\sigma (r_\\theta(x_i, y_{i,2}) - r_\\theta(x_i, y_{i,1}))),      (2)\nwhere \u03c3(\u00b7) is the sigmoid function such that \u03c3(x) = exp(x)/(1 + exp(x)).\nWhen the feedback is binary Zi \u2208 {0, 1}, the above objective function reduces to exactly how people learn the reward models under the Bradley-Terry assumption. When the feedback is soft and takes value in [0, 1], we will demonstrate how such a loss function characterizes the ordinal feedback options of Ztext.\nIntuitively, this objective function better utilizes the annotated data and avoids the shortcomings of two alternative heuristics: (i) Discarding all the samples annotated as \"same as\" for that these samples do not integrate with the binary feedback Bradley-Terry model; \u2192 this causes loss of samples. (ii) Combining \"better than\" and \"slightly better\" as better (and the same for worse); \u2192 this causes loss of information.\nThis shift from binary to ordinal as above, though natural, raises three questions:"}, {"title": "3 Probability Model of Ordinal Feedback", "content": "We first define the oracle feedback model as\nZoracle(x, y1, y2) := P (y1 > y2|x).\nThis is the preference model that one aims to learn, regardless of whether assuming the Bradley-Terry model or whatever other preference/reward model. Here we should think the probability space being the whole population that use the language, and a human annotator as a random draw from the population.\nAssumption 3.1 (Ordinal feedback probability model - wisdom of the crowd). We assume the ordinal feedback Z defined in Definition 2.1 satisfies\nE[Z|(x, y1, y2)] = zoracle(x,y1, y2) for any (x, y1, y2) \u2208 X \u00d7 Y2.\nThe assumption is the only one we make for the ordinal feedback model. As we will see, this one assumption alone is sufficient to define the probability model of the ordinal feedback setting and validate the learning of the reward model. To interpret the assumption, we emphasize that it is not stricter than the existing assumption people impose for the binary feedback model. Specifically, under the binary feedback model where Z takes values in Z {0,1}, Assumption 3.1 is equivalent to the Assumption (1).\nTo see another example, consider the set Z = {0,0.5,1} where the labels 0 and 1 denote \"better\" and \"worse\" respectively, and the label 0.5 denotes \"same as\". The assumption then requires that with Z taking values in this new Z with an additional 0.5 option, its expectation matches the oracle value zoracle on the sample. Under the general ordinal feedback setting, human annotators label their preferences on different scales, i.e.,\nZ = {0,1}, {0,0.5,1}, or {0, 0.25, 0.5, 0.75, 1}.\nThe assumption says that the change of scales does not introduce bias that twists the oracle preference on the population level.\nSociological interpretation: We name the assumption by \"wisdom of the crowd\"; the concept was first coined by the article Vox Populi (Galton, 1907) for a social experiment under the title \"the voice of the people\". The social experiment is about a weight-judging competition conducted in England for random people to guess the weight of an ox. The average of all 787 guesses was 1,197 pounds, while the actual weight was 1,198 pounds, as shown in Figure 1. Each individual's guess can be far off the target yet the population average tends to be very accurate. For the context of preference annotation, Assumption 3.1 and the current practice of human annotation exercise the wisdom of the crowd in two folds: First,\nIn practice, annotators label in the set Ztext (e.g. better, same as, worse}), and this results in a gap towards the set Z = {z1,..., zm} used in the learning of the reward model (2). For the binary or 3-level feedback setting, the following conversion is natural and reflects the thinking process of the annotator:\nZtext = {better than, worse than} \u2192 Z = {0,1},\nZtext = {better than, same as, worse than} \u21d4 Z = {0, 0.5, 1}.\nFor a more fine-grained 5-level feedback setting, it is less clear whether one can convert as follows,\nZtext = {better than, slightly better, same, slightly worse, worse than} \u21d4 Z = {0, 0.25, 0.5, 0.75, 1}.\nIn this light, our ordinal feedback model and Assumption 3.1 provide the following insights in guiding the annotations. Rather than to provide vague wording of \"better than\" or \"sightly better\", one can write the following in the guidance to the human annotators:\nAnnotation guideline: The label \"slightly better\" represents that 75% of the population think response y1 is better than response y2. The label \u201cslightly worse\u201d represents that 25% of the population think response y1 is better than response y2\u00b7\nSuch an additional guidance endows the labels in Ztext a numerical meaning. The corresponding numerical values can be directly used in the learning objective (2)."}, {"title": "3.2 Universal existence of feedback probability model", "content": "In this subsection, we detour from the discussions of reward modeling and show the universal existence of probability models that satisfy Assumption 3.1, which ensures the assumption is well-defined. The following theorem states that for any ordinal feedback set Z and any oracle model zoracle, there exists a probability model satisfying Assumption 3.1.\nTheorem 3.2. For any ordinal feedback set Z = {z1,..., zm} and any oracle model zoracle(x, y1, y2), one can construct an ordinal feedback Z as a random variable that satisfies Assumption 3.1 in the following way. Specifically, if zoracle \u2208 [zj,zk] for some j,k \u2208 [m], then one can set the marginal probability measure \u00b5j,k(z) := P(Z = z|(x, y1, y2)) to be\n\u03bc_{j,k}(z) = \\begin{cases}\n(z_k - z_{\\text{oracle}})/(z_k - z_j), & \\text{if } z = z_j, \\\\\n(z_{\\text{oracle}} - z_j)/(z_k \u2013 z_j), & \\text{if } z = z_k, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\nThe ordinal feedback Z fulfills Assumption 3.1.\nOn the other hand, any ordinal feedback satisfying Assumption 3.1 must be a convex combination of such constructions. More specifically, for any ordinal feedback Z with marginal probability measure \u00b5(z) := P(Z = z|(x, y1, y2)) satisfying Assumption 3.1, there must exist non-negative real numbers \u03a3j,kaj,k = 1 such that\n\u03bc = \\sum_{j,k} \u03b1_{j,k} \u03bc_{j,k},\nwhere the summation is made for all (j,k) pairs such that zoracle \u2208 [zj, zk].\nThe theorem not only verifies the existence of a probability distribution that satisfies Assumption 3.1, but also provides a full characterization of all the distributions that satisfy the assumption. It says that all such distributions should be (convex combinations of) some two-point distributions, where the weights are assigned to interpolate the linear system\n\\begin{cases}\nH_{j,k}(z_j) \\cdot z_j + H_{j,k}(z_k) \\cdot z_k = z_{\\text{oracle}}, \\\\\nH_{j,k}(z_j) + H_{j,k}(z_k) = 1.\n\\end{cases}\nFor example, consider a reward model with ties where Z = {0,0.5, 1}. If the oracle feedback is, say, 0.8, then we can construct a feedback Z with probability masses \u00b5(0.5) = 0.4 and \u00b5(1) = 0.6 such that the unbiased assumption is fulfilled. In this way, if we are (in an ideal world) given an oracle model zoracle, we can generate different unbiased ordinal feedback problem models accordingly."}, {"title": "4 Statistical Benefits of Ordinal Feedback", "content": "Intuitively, the more fine-grained ordinal feedback offers more information and should bring more benefits to the learning of a reward model. In this section, we provide a theoretical explanation that pinpoints the benefits, which are not restrictive to the traditional RLHF but also applicable to DPO. The theory not only captures the benefits of the ordinal feedback model but also provides insights into the technique of soft labeling in knowledge distillation , which can be of independent interests. In short, the theoretical result says that any feedback model satisfying Assumption 3.1 brings statistical benefits compared to the canonical binary feedback model."}, {"title": "4.1 Finite-sample benefits", "content": "While the population loss establishes an equivalence between different feedback systems in an asymptotic sense, we illustrate the finite-sample benefits of a more fine-grained ordinal feedback system in the following. We first introduce the concepts of coupling and hierarchical expectation.\nDefinition 4.4 (Coupling). For any two random variables \u03be and \u03be', if there exist two random variables \u03da and \u03da' over one probability space such that \u03da has the same distribution as \u03be and \u03da' the same as \u03be', we call them a coupling of \u03be and \u03be'.\nDefinition 4.5 (Hierarchical Expectation). For any two ordinal feedback systems Z and Z' taking values in Z and Z' over the same probability space (\u03a9, F, P), if there exists a combination of random variables (W, W') over a probability space (\u03a90, F0, P0) such that\n(a) (W, W') forms a coupling between Z and Z';\n(b) W = E[W'|W] holds almost surely.\nThen we say that Z is a hierarchical expectation of Z'.\nThe concept of hierarchical expectation defines the relative granularity of the feedback system. In general, if Z is a hierarchical expectation of Z', then we say Z is more fine-grained than Z' since Z allows annotators to give more subtle responses. Such an intuition is further exemplified in the following proposition, the proof of which also echoes the universal existence result in Theorem 3.2."}, {"title": "4.2 Generalizations to DPO", "content": "Our results naturally extend to the direct policy optimization (DPO) case. We first give a quick introduction to the DPO training objective. Suppose the ground-truth reward function for any prompt-response pair (x,y) is r*(x,y). Then under the reinforcement learning objective of maximizing the reward (with a Kullback-Leibler divergence regularization of strength \u03b2 from the original policy \u03c0ref), the optimal policy under the ground truth reward function r* should be\n\u03c0*(y|x) \u03b1 \u03c0ref(y|x) exp (\\frac{r^*(x,y)}{\u03b2}).\nUnder the Bradley-Terry model, we have\nP(y1 > y2|x) = \u03c3 (\u03b2 log \\frac{\u03c0*(y1|x)}{\u03c0_{\\text{ref}}(y1|x)} - \u03b2 log \\frac{\u03c0*(y2|x)}{\u03c0_{\\text{ref}}(y2|x)}),\nwhere the \u03c3(\u00b7) is the sigmoid function \u03c3(x) = exp(x)/(1 + exp(x)). Then the DPO training objective is to minimize the cross-entropy loss under the binary feedback setting\nmin_\u03b8 \\sum_{i=1}^n -Z_i \\cdot log \\sigma (\u03b2 log \\frac{\u03c0_\\theta(y_{i,1}|x_i)}{\u03c0_{\\text{ref}}(y_{i,1}|x_i)} - \u03b2 log \\frac{\u03c0_\\theta(y_{i,2}|x_i)}{\u03c0_{\\text{ref}}(y_{i,2}|x_i)}))\n- (1 - Z_i) \\cdot log \\sigma (\u03b2 log \\frac{\u03c0_\\theta(y_{i,2}|x_i)}{\u03c0_{\\text{ref}}(y_{i,2}|x_i)} - \u03b2 log \\frac{\u03c0_\\theta(y_{i,1}|x_i)}{\u03c0_{\\text{ref}}(y_{i,1}|x_i)})). \\qquad (4)\nBy considering a richer feedback than the binary case Z = {0,1}, we can use (4) to train the LLM \u03c0\u03b8 directly under the ordinal feedback. The affinity condition (3) is fulfilled since the loss is still the cross-entropy loss. Thus, applying Theorem 4.9 for I = {\u03c0\u03b8, \u03b8\u2208 \u0398} yields a similar result. We present here without repeating the proof."}, {"title": "4.3 Implications on soft labeling", "content": "As noted earlier, the results developed above also have implications on the technique of soft labeling, which we elaborate on in this subsection. Specifically, we show how the analysis can be applied to the context soft labeling and induces a novel bias-variance trade-off for knowledge distillation.\nFor general k-nary classification problems, the standard feedback (labeling of the target variable) is a k-dimensional one-hot vector. However, these all-zero-but-one labels make the model overfit easily from a training perspective. Knowledge distillation is a well-known technique first developed in computer vision to regularize the model from fitting the noises. The original data is used to train a teacher model of which the predictions are named soft labels. Then a student model is trained to mimic the predictions of the teacher model, that is, minimizing the training loss against the soft labels generated by the teacher model rather than the original ones. Such a distillation regularizes the student model from overfitting and overconfidence.\nExisting theoretical works are developed to understand the benefits of knowledge distillation and soft labeling. Our theoretical perspective in the preceding subsection provides a new perspective on that problem. In a word, the trained teacher model could be viewed as one (possibly biased) oracle feedback, and learning from the oracle eases the overfitting via reducing the labeling variance (and hence a smaller Rademacher complexity). More concretely, consider the following four learning paradigms:\n(a) Oracle/ideal: learn from oracle labeled samples (denoted as (x, yoracle)'s in this subsection). The oracle yoracle is the conditional expectation of the label y.\n(b) Original: learn from the original samples (denoted as (x, y)'s in this subsection), where we regard y as a randomly sampled label according to y ~ yoracle.\n(c) Knowledge distillation: learn from the teacher model T's output (denoted as (x, \u04efT)'s in this subsection). The labels \u04efT's are random vectors of which the randomness comes from the teacher model T.\n(d) Sampling from teacher: learn from the teacher model, but not directly from \u04efT; instead, we use a sampled label yt ~ \u04efT. This introduces more randomness in the labeling process.\nWhat is the \"bias-variance\" tradeoff in the learning paradigm (c)? First, yoracle shares the same conditional expectation with y, and so do \u0177T and yT, where their population cross-entropy losses are the same due to the affinity condition (3). The \u201cbias\u201d comes from \u0177r as an imperfect estimation of yoracle, leading to different population losses, while training according to (c) or (d) introduces an additional loss in the original population loss. That is the \"bias\" term\nBias := ET [Ex,y' [l(y', h(x))] - Ex,y [l(y', h*(x))]] \u2265 0,\nwhere we set y' to be i.i.d. as y to prevent the dependence of T on y, and\nhT := arg min Ex,\u00ffT [l(\u1ef9T, h(x))], h* := arg min Ex,y [l(y, h(x))],\nh\u2208H h\u2208H\nare the corresponding hypotheses minimizing the population losses.\nAs for the variance, we can directly see from the construction that \u0177T (or yoracle) is a hierarchical expectation of yT (or y), thus learning paradigm (c) (or (a)) has a lower Rademacher complexity compared to that of (d) (or (b)):\nRady (l\u25e6H) \u2013 Rad\u016b\u0442 (l\u25e6H) \u2265 0 for any T, Rady(l\u25e6H) \u2013 Radyoracle (l\u25e6H) \u2265 0,\nwhere the subscript of the Rademacher complexity denotes the source distribution of the label. However, such an argument does not directly compare the Rademacher complexity of the original learning paradigm (Rady) with that of the knowledge distillation (Rad\u012b\u0442). To explicitly show that the \u201cvariance\u201d is reduced, we make the following assumption that the marginal output of the teacher model is unbiased:"}, {"title": "5 Numerical Experiments", "content": "We perform numerical experiments to answer two questions: (i) How do different granularities of the feedback model affect the learning of the reward model? (ii) Does the inclusion of these ordinal feedback data with the objective (2) benefit the learning of the reward model?"}, {"title": "5.1 Experiment Settings", "content": "Datasets. In the following numerical experiments, we leverage the Skywork-Reward-Preference-80K-v0.2 dataset as our base training dataset. We perform multiple runs and report the average performance (along with the confidence intervals); for each run, we randomly sample a 1024-sized subset as the hold-out evaluation dataset. In addition, we use the RewardBench dataset for the out-of-distribution evaluation task to comprehensively assess the performance of different trained models.\nBase Models. Our base models for the following experiments are llama-3.2-1b-instruct and gemma-2-2b-it. Both models are trained under full-parameter fine-tuning.\nOrdinal Feedback. The original Skywork-Reward-Preference-80K-v0.2 dataset only contains a binary feedback for each prompt x and a response pair y1 and y2. To generate feedback labels with different levels of granularities, we adopt a well-trained reward model, Skywork-Reward-Gemma-2-27B-v0.2 as the oracle scoring model roracle:X\u00d7Y \u2192 R in this case. We chose this model because (1) it was exclusively trained on the to-be-labeled base training dataset hence there is hardly a risk of out-of-distribution mislabeling; (2) the model ranks first on the RewardBench online leaderboard up to the time of this paper, making its output oracle scores more reliable. Accordingly, the induced oracle model being zoracle(x, y1, y2) = (\\sigma(\\frac{r_{\\text{oracle}}(x,y1) - r_{\\text{oracle}}(x, y2)}{T})) where T is a temperature parameter and \u03c3(\u00b7) denotes the sigmoid function.\nWe consider the following four types of feedback systems:\n\u2022 Oracle: zoracle is directly used as the feedback label and zoracle C [0,1].\n\u2022 Binary: the label is sampled by zbinary ~ Bernoulli (zoracle) and Z2 = {0,1}.\n\u2022 3-level: the label is sampled as the process in Theorem 3.2 considering only the smallest interval containing zoracle and Z3 = {0, 0.5, 1}.\n\u2022 5-level: the label is sampled as the process in Theorem 3.2 considering only the smallest interval containing zoracle and Z5 = {0, 0.2, 0.5, 0.8, 1}.\nWe adopt the objective function (2) to train the reward model."}, {"title": "5.2 Fine-grained feedback leads to better reward learning", "content": "As discussed earlier, a more fine-grained feedback system should intuitively and theoretically lead to better reward learning. For the four feedback models listed above, they should have the following orders in terms of performance:\nOracle \u2265 5-level > 3-level > Binary\nwhere > represents an advantage in model performance.\nHere we perform numerical experiments to verify such intuitions and for each combination of the reward model and feedback system, we conduct 5 independent training runs and report the average results. For the setting of learning with oracle feedback, we set\nZi = P (yi,1 > yi,2|xi) = zoracle(xi, yi,1, yi,2)\nin the learning objective (2). For more fine-grained ordinal feedback, we sample the feedback according to Section 5.1.\nThree take-away messages are: First, a more fine-grained feedback structure leads to better reward learning for both in-distribution (ID) and out-of-distribution (OOD) performance. Second, though we do not have access to the oracle model in practice, the 5-level feedback system provides a good proxy for that. Third, the learning objective (2), as a generalization of the canonical cross-entropy loss for binary feedback, is an effective one to handle the ordinal feedback data."}, {"title": "5.3 Ordinal feedback v.s. binary feedback", "content": "Now we restrict our attention to the 3-level feedback setting and investigate the effect of the proportion of the tied data (samples with labels of y1 \"same as\" y2). Specifically, we limit the training samples to 32,768 and consider 5 different proportions of the tied data:\n\u2022 0%-tied: All the data samples are binary-labeled.\n\u2022 25%, 50%, 75% of the data samples are tied.\n\u2022 100%-tied: All the data samples are tied.\nMore details of how the tied data samples are generated are deferred to C.1.\nWe make the following observations. First, the 100%-tied setting fails in that it results in a significantly worse performance than the other settings. This is natural as it leads to a reward collapse, as also observed in other semi-supervised learning algorithms; to see this, if we are given only the tied data, one way to learn the reward model is to have all the rewards equal to a constant. Second, mixing a proportion of the tied data and using the learning objective function (2) leads to a better performance than the case of 0%-tied data. One subtle point here is that, in practice if we do not employ the learning objective (2) and simply drop the tied samples, this will result in a smaller sample size for learning the reward model, and an even worse performance than the 0%-tied setting here. Third, if we look into the training dynamics of Figure 3, we can see that the curves with tied data samples are smoother than the ones with 0%-tied samples. This means the inclusion of the tied samples also leads to a smoother loss landscape."}, {"title": "6 Related Works", "content": "Reinforcement learning from human feedback (RLHF) originates from the idea of preference-based reinforcement learning. The term RLHF is proposed by the large language model (LLM) community and has been a mainstream framework for aligning LLMs with human preferences. For a more detailed survey of the history of RLHF, we refer to Kaufmann et al. (2023). While many people have incorporated the supervised fine-tuning (SFT) stage as a part of the RLHF pipeline , our discussion would focus on the reward modeling and policy optimization via reinforcement learning (RL) which take a supervised fine-tuned model as the starting point.\nAlthough proven effective for aligning LLMs with human preferences, the canonical RLHF suffers in several aspects, including complicated implementation, difficult hyperparameter tuning, low sampling efficiency and computational overhead, which promotes the studies to optimize with the relative preferences without depending on RL. One major alternative to the RLHF is the direct policy optimization (DPO) which directly trains the language model to increase the probability of the preferred response and decrease the other.\nWang et al. (2023) discuss the influence of f-divergence as the constraint term in PPO and proposes f-DPO. consider a general learning objective under pair-wise preferences, i.e., \u03a8\u0420\u041e, points out the potential overfitting issue in RLHF and PPO, and further mitigates the problem with a specific instance of \u03a8\u0420\u041e, i.e., IPO. investigate the optimization at a finer level and employs forward KL divergence constraints for each token to improve the alignment. There are also"}, {"title": "7 Conclusion", "content": "In this paper, we propose reward modeling with ordinal feedback as a generalization of the binary feedback. Such a framework fully uses the potentially useful samples and the fine-grained information discarded by the binary feedback practice. We generalize the assumption of the BT model to the general marginal unbiasedness assumption, which we name by a sociological concept \"wisdom of the crowd\". Under that assumption, we build a natural probability model for ordinal feedback. We also show that the Rademacher complexity is reduced by adopting ordinal feedback. The results also cover other loss functions (for example, the hinge loss) and other paradigms (for example, DPO). Numerical results validate the theoretical findings. Further experiments imply that mixing some tied preference samples benefits RM learning, which may be worth future exploration. Our results suggest that the annotation guideline should encourage the quantitative description (for example, 70%) of the qualitative option (for example, \"slightly better\"). Our theoretical analysis based on hierarchical expectation may be of independent interest to the field of knowledge distillation, providing a novel bias-variance trade-off perspective."}, {"title": "A Reward Modeling with Ordinal Feedback under Hinge Loss", "content": "In the main paper", "follows": "nmin_\u03b8 \\sum_{i=1}^n Z_i [max (0, C - (r_\\theta(x_i, y_{i,1}) \u2014 r_\\theta(x_i, y_{i,2})))", "y_{i,1})))": 5, "zj+1": "zoracle. We then replicate the experimental setup from Section 5.2, employing llama-3.2-1"}]}