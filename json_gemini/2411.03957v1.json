{"title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation", "authors": ["Yuhang Liu", "Xueyu Hu", "Shengyu Zhang", "Jingyuan Chen", "Fan Wu", "Fei Wu"], "abstract": "Retrieval-Augmented Generation (RAG) has proven to be an effective method for mitigating hallucination issues inherent in large language models (LLMs). Previous approaches typically train retrievers based on semantic similarity, lacking optimization for RAG. More recent works have proposed aligning retrievers with the preference signals of LLMs. However, these preference signals are often difficult for dense retrievers, which typically have weaker language capabilities, to understand and learn effectively. Drawing inspiration from pedagogical theories like Guided Discovery Learning, we propose a novel framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the language capabilities of LLMs to construct examples from a more granular, information-centric perspective to guide the learning of retrievers. Specifically, our method utilizes LLMs to construct easy-to-understand examples from samples where the retriever performs poorly, focusing on three learning objectives highly relevant to the RAG scenario: relevance, comprehensiveness, and purity. These examples serve as scaffolding to ultimately align the retriever with the LLM's preferences. Furthermore, we employ a dual curriculum learning strategy and leverage the reciprocal feedback between LLM and retriever to further enhance the performance of the RAG system. A series of experiments demonstrate that our proposed framework enhances the performance of RAG systems equipped with different retrievers and is applicable to various LLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as GPT-4 (Achiam et al. 2023), have achieved impressive results across a wide range of language tasks (Brown et al. 2020; Kojima et al. 2022). However, despite their rapid recent development, the issue of hallucinations persists (Zhang et al. 2023b; Ji et al. 2023). Particularly in knowledge-intensive tasks (Kandpal et al. 2023), the generated content sometimes deviates from factual information, resulting in fabricated or inaccurate statements.\nRetrieval-Augmented Generation (RAG) is regarded as an effective approach to mitigate the issue of hallucination (Lewis et al. 2020; Borgeaud et al. 2022), by leveraging external corpora to assist LLMs in generating accurate factual information."}, {"title": "Preliminaries", "content": "The RAG process typically involves augmenting the input query x with relevant documents D from a predefined corpus C. The output y can then be defined as:\ny = LLM(x\u2295D), D \u2286 C  (1)\nThe retriever functions by taking an input query and searching the corpus to locate relevant documents. Recent advancements in RAG have focused primarily on the dense retriever, which utilize a pre-trained encoder Enc, commonly a transformer-based model with parameters $, to embed both the documents in the corpus and the incoming queries into a shared vector space:\nx = Enc(x) (2)\nd = Enc(d), d\u2208C (3)\nwhere x and d represent the query and document embeddings, respectively.\nDocument retrieval is performed by identifying the document embeddings that are nearest to the query embedding"}, {"title": "Fine-Grained Guidance for Retrievers", "content": "In this section, we will introduce the specific details of the FiGRet framework."}, {"title": "Establishing Learning Objectives", "content": "Our ultimate goal is to align the retriever with the preferences of LLMs, enabling it to identify documents most likely to facilitate optimal LLMs generation given a user query. Therefore, our framework first establishes explicit and relatively easy-to-learn objectives that serve as scaffolding towards the ultimate goal.\nThese objectives are carefully selected based on three key criteria: 1) clarity and interpretability by the teacher model for effective guidance construction, 2) strong positive correlation with the alignment goal, and 3) minimal overlap to avoid redundant guidance. Based on these criteria, we focus on relevance, comprehensiveness, and purity, each addressing a distinct aspect of document quality:\n\u2022 Relevance: This objective emphasizes the fundamental importance of content relevance between the document and the query. Given that retrievers typically possess a strong foundational ability in document-level relevance, our guidance aims to refine and enhance this existing capability.\n\u2022 Comprehensiveness: This objective emphasizes the importance of retrieving documents containing rich information related to the query. Our framework guides retrievers to prioritize documents exhibiting higher levels of comprehensiveness.\n\u2022 Purity: This objective emphasizes the detrimental impact of noisy information within documents on LLMs generation. Our guidance aims to steer retrievers towards documents with minimized noise.\nWe then leverage the teacher LLM, denoted as LLMT, to assess the retriever's mastery of each learning objective. Recognizing the foundational role of relevance in RAG, the teacher primarily focuses on query-document relevance, creating three broad scoring categories. Within each category, distinctions are made based on comprehensiveness and purity. To ensure consistent and accurate evaluation, the top-k retrieved documents for a given query are scored collectively using chain-of-thought (Wei et al. 2022) prompting. Finally, to mitigate potential inconsistencies in scoring across different queries, we convert scores to rankings and utilize normalized discounted cumulative gain (NDCG, J\u00e4rvelin and Kek\u00e4l\u00e4inen (2002)) to measure the discrepancy between the retriever's ranking and the ideal ranking. This ranking-based NDCG score serves as a proxy for the retriever's mastery of the learning objectives."}, {"title": "Constructing Guidance", "content": "Guidance construction is tailored to the retriever's mastery of the learning objective. During online inference, we collect input queries and the retriever's top-k retrieved documents. When the system is idle, we utilize the NDCG metric to evaluate these samples and populate a sample pool. Once the pool reaches a predefined size, we empirically determine an NDCG threshold for guidance selection. This threshold is set as the minimum NDCG score among samples where the top-1 ranking given by the LLM matches the retriever's ranking. Samples falling below this threshold are selected for guidance.\nThe construction process of the guidance examples is as follows, and is illustrated in Figure 1.\nRelevance. In this objective, we construct guidance examples by reversing the positive and negative documents for the input query x. By doing so, we enable the retriever to focus on more granular levels of relevance. Specifically, in the previous document-centric approach, during training, the information units within the negative documents D- are made to diverge from all units in query x, including those that are inherently relevant. This introduces a bias and can be viewed as a reduction in the accuracy of information units, as they are incorrectly deemed irrelevant to the query (illustrated in the bottom-left corner of Figure 1). To mitigate this, for the information units in D that are irrelevant to x, we construct a new query x', which only changes the relevance from irrelevant to relevant with respect to the units in D\u00af.\nThe specific construction approach is as follows: We first extract the information differences between D+ and D-.\n\u0394\u2081 = Info(D) \u2013 (Info(D+) \u2229 Info(D;)) = LLMT(D+,D\u00af) (7)\nwhere D \u2282 D\u00af is the subset containing content-similar d. Note that Info() represents viewing documents as sets of information units, so (Info(D+) \u2229 Info(D)) represents the shared information between D+ and D\u012b. Thus, \u0394i represents the information present in D\u012b but absent in D+.\nNext, we construct a new input x corresponding to D by applying LLMT to \u0394\u2081.\nFor x, D is the relevant document set, while D+ and D- - D (i.e., DN \u2013 D) are not relevant. Therefore, we obtain the following examples:\n{(x,D+, D\u00af); (8)\n(x'i, Di, Diet - Di), vi\nComprehensiveness. In this objective, to construct guiding examples that facilitate the retrieval of more comprehensive documents, the most intuitive approach is to either enhance or reduce the comprehensiveness of a document to create a contrast. In this case, we have developed a new comprehensive document containing more information.\nSpecifically, we construct a new, more comprehensive document, denoted as dcomp, by leveraging all relevant information from D+ and the parameter knowledge of LLMT. This document is generated by applying LLMT to the combined information:\ndcomp = LLMT (Info+(D+), 0) (9)\nwhere Info+(D+) represents the relevant information contained in D+, and 0 represents the the parameter knowledge of LLMT. Then, we can derive the following example:\n{(x, dcomp, D+)} (10)\nThis setup allows the retriever to recognize documents that are more comprehensive.\nPurity In this objective, we guide the retriever to identify documents with a lower proportion of noisy information by modifying the ratio of noisy information units within the documents.\nSpecifically, we first use the LLMT to remove noisy and non-noisy information units from d+, respectively, thereby altering the document's purity:\nddense = LLM(Info(d+) \u2013 {u\u00af|u\u00af \u2208 Info(d+)),\ndsparse = LLM(Info(d+) \u2013 {u+|u+ \u2208 Info(d+)) (11)"}, {"title": "Student Model Learning", "content": "We employ a dual curriculum learning approach to train the student model. Through the preceding steps, we obtain two sets of data: guidance examples constructed by the teacher model and preference data formed by the teacher model's scoring. We design the training process such that the guidance examples, serving as scaffolding, have a higher distribution in the early stages of training, while the proportion of preference data gradually increases. The probability distribution satisfies the following equation:\nP\u2081(i) = e^(2yi-1)/T1/\u03a3i=1^N e^(2ys-1)/T1  (13)\nwhere yi is the binary label of the i-th sample (1 for guidance examples and 0 for preference data), and T\u2081 is the temperature parameter.\nSimultaneously, we control the difficulty of the training samples to gradually increase based on their NDCG scores, following the distribution:\nP2(i) = esi/T2/\u03a3i=1^N esi/T2 (14)\nwhere si represents the NDCG score of the i-th sample, and T2 is another temperature parameter.\nThrough this dual curriculum learning approach, we enable the training process to transition from learning the scaffolding of the objectives to the final goal of aligning with the LLMs' preferences while ensuring a gradual increase in learning difficulty.\nThen, following previous work (Xiong et al. 2020), the training loss L is compute as follows:\nL=\u03a3d+ED+ \u03a3d-ED- l(sim(x, d+), sim(x, d\u00af)) (15)\nwhere l denotes the standard cross-entropy loss."}, {"title": "Assessing Performance", "content": "After the learning process, we re-evaluate the difficult samples. If a sample's NDCG score surpasses a predefined threshold, we consider it a well-learned sample and use it as a teaching case for the teacher model. During the construction of similar samples, these well-learned samples are provided to the teacher model through retrieval as exemplars for in-context learning (ICL). Conversely, samples whose scores have decreased will undergo additional guidance."}, {"title": "Experiments", "content": "In this section, we describe the experimental settings used to evaluate our proposed framework."}, {"title": "Tasks and Datasets", "content": "We conduct evaluations across the following tasks: Language Understanding, Open Domain Question Answering (Open Domain QA), and Fact Checking. For each task, we introduce the corresponding evaluation datasets:\nLanguage Understanding. We adopt MMLU (Hendrycks et al. 2020), a multiple-choice QA dataset that covers 57 subjects including STEM, humanities, social sciences and others. We report the accuracy on the development split as the metric, following prior work (Yu et al. 2023).\nOpen Domain QA. We evaluate on three open domain QA datasets: 1) Natural Questions (NQ; Kwiatkowski et al. (2019)), which contains questions from Google search queries paired with Wikipedia answer passages. We use the open variant from Lee, Chang, and Toutanova (2019); 2) HotpotQA (Yang et al. 2018), which features compositional questions requiring reasoning over multiple Wikipedia paragraphs, and we use its fullwiki setting; 3) PopQA (Mallen et al. 2022), which consists of questions about long-tail Wikidata entities with answers extracted from Wikipedia. For this task, we consider a generated answer correct if it contains the gold answer. We report the accuracy on the development split, except for PopQA where the test set was used.\nFact Checking. We evaluate the model's fact-checking capabilities using the FEVER (Fact Extraction and VERification, Thorne et al. (2018)) dataset, which is a large-scale dataset containing claims that are verified against textual evidence from Wikipedia.\nWe use the KILT version of the dataset from Petroni et al. (2021), and report the accuracy on the development split. For the MMLU tasks, the MSMARCO corpus is used, while Wikipedia\u00b9 is the corpus for other tasks."}, {"title": "Implementation Details", "content": "Teacher LLM. We utilize GPT-3.5 from OpenAI's GPT family (Brown et al. 2020; Ouyang et al. 2022) as the teacher LLM due to its strong language abilities and cost-effectiveness. Specifically, we employ GPT-3.5-Turbo-0125 version.\nRuntime Task. We used MSMARCO (Bajaj et al. 2016) as our runtime task, which contains questions requiring factual knowledge to answer. We randomly shuffled the training set and set the sample pool size to 10,000 samples for each learning iteration. In our experiments, we conducted two iterations. For each sample, we set the top-k document retrieval to 8 for guidance construction.\nTraining Details. During training, we used all the constructed guidance examples. For each input x, a positive document d+ and a negative document d\u00af are randomly sampled from D+ and D\u00af, respectively, which are used for training. The batch size during training is set to 128, and the"}, {"title": "Baselines", "content": "No Retrieval. This baseline involves directly generating responses using inference LLMs without the aid of retrieval. In this setting, the LLMs rely solely on their internal parametric knowledge to generate answers.\nRetrieval Augmented. For the retriever base models, we used Contriever2 (Izacard et al. 2021), BGE3 (Xiao and Liu 2023), and Sentence-BERT4 (Reimers and Gurevych 2019b), which are competitive and have achieved excellent results in relevant tasks. We also validated the method of aligning retrievers through LLMs preference: AAR, which we adopted the checkpoint using Contriever as the base model.\nWe employed a few-shot approach during inference."}, {"title": "Analysis", "content": "In this section, we discuss the main experimental result and present ablation studies."}, {"title": "Main Results", "content": "Our framework is designed to improve the performance of retrievers in RAG systems without requiring external supervision or labeled data. Table 1 demonstrates that our approach leads to improved LLM generation performance across various LLM and retriever combinations. To demonstrate the performance differences, only documents with the highest similarity scores are utilized. Notably, the performance gains achieved by Contriever within our framework surpass those of AAR, which is trained to align with LLMs' preferences using signals from a smaller source LM.\nTo validate the generalizability of our approach, we conducted experiments on diverse LLMs. The results reveal consistent performance improvements across these LLMs, highlighting the broad applicability of our method. It is worth noting that we only conducted example construction on 20,000 samples (approximately 1/40 of the original training set) during the training phase, demonstrating the sample efficiency of our framework."}, {"title": "Ablation studies", "content": "To investigate the impact of each objective on the performance of our framework, we conducted ablation experiments on Llama-3-8B-Instruct. Specifically, we removed each of the three objectives individually and evaluated their performance on three diverse tasks: MMLU, PopQA, and FEVER. The results of these experiments are presented in Table 2.\nThe ablation study reveals that removing any of the three objectives leads to a certain degree of performance degradation compared to the complete framework. However, it is noteworthy that even with the ablated components, the framework still outperforms the initial retriever.\nWe also verified the results after removing the guidance examples and preference data. The results show that removing either of these would affect the final performance, demonstrating the connection between learning objectives and the ultimate goal.\nNotably, we observed that removing the first objective (i.e. relevance), had the least impact on the framework's performance. We hypothesize that this may be attributed to the model's initially strong understanding of relevance, resulting in lower gains from this objective compared to the others."}, {"title": "Evaluating Improvement on Individual Objectives", "content": "To verify whether the retriever has improved on all three objectives after training, i.e., whether these three objectives can be well learned by the model, we compared the retrieved documents by the retriever before and after training on the three objectives. We chose BGE because it achieved the largest gains with our method. We tested 1k samples from the MSMARCO dataset that do not overlap with the training data, and obtained the top 1 documents retrieved by the retriever for the same input before and after training. We removed samples where the top 1 documents were the same, and compared the remaining samples on the three objectives. Specifically, we used GPT-40 to score the two sets of documents on each of the three objectives. To mitigate positional bias, we tested each sample twice with the order of the two documents swapped and compared their average values, considering it a tie when the average values were the same. The results are shown in Figure 2. After applying our framework, the retriever achieved higher win rates on all three objectives than before, demonstrating that the retriever can effectively learn these three objectives."}, {"title": "Conclusion", "content": "We introduce the FiGRet framework, a novel approach for enhancing the alignment between retrievers and LLMs in RAG systems by providing fine-grained feedback and guidance focused on relevance, comprehensiveness, and purity. Our experiments demonstrate performance improvements across various tasks and retrievers when coupled with different LLMs. Notably, our framework obtains feedback directly from black-box LLMs, reducing deployment difficulty, and achieves considerable performance gains with only 20,000 upstream samples.\nThe FiGRet framework offers a new perspective on enhancing retriever-LLM alignment in RAG systems. Future research could explore integrating the framework with other techniques to further advance RAG systems."}, {"title": "Technical Appendix", "content": "Out-of-Domain Task Validation\nTo validate the generalizability of our proposed framework on tasks unrelated to the upstream training objective, we evaluated its performance on the LaMP (Language Models Personalization, Salemi et al. (2023)) benchmark. LaMP is a publicly available evaluation dataset for personalized language modeling, designed to assess various aspects of personalization.\nFollowing the original experimental setup of the LaMP benchmark, we employed GPT-3.5-Turbo as the reasoning LLM and Contriever as the retrieval model. We evaluated the performance on various subtasks both before and after applying our framework (excluding task 6, \"Email Subject Generation\", due to data accessibility issues).\nTable 3 presents the results. Our framework demonstrates consistent performance improvements across six diverse tasks: Citation Identification, Movie Tagging, Product Rating, News Headline Generation, Scholarly Title Generation, and Tweet Paraphrasing. The only exception is LaMP 7, where both our framework and the baseline Contriever"}]}