{"title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent", "authors": ["Kevin Qinghong Lin", "Linjie Li", "Difei Gao", "Zhengyuan Yang", "Shiwei Wu", "Zechen Bai", "Stan Weixian Lei", "Lijuan Wang", "Mike Zheng Shou"], "abstract": "Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4\u00d7. Navigation experiments across web [12], mobile [36], and online [40] environments further underscore the effectiveness and potential of our model in advancing GUI visual agents.", "sections": [{"title": "1. Introduction", "content": "Graphical User Interfaces (GUIs) are central to how individuals engage with the digital world, serving as virtual embodied interface for a range of daily activities. Meanwhile, Large Language Models (LLMs) [32], with their ability to comprehend complex language instructions and seamlessly integrate tools, have shown significant potential in performing complex tasks through building agents [1, 13, 16, 56]. This progress inspires the development of intelligent GUI agents that can significantly streamline human workflows based on user intentions.\nEarly efforts in GUI automation have primarily focused on developing language agents [12, 47, 55] that rely on closed-source, API-based LLMs like GPT-4 [32]. These agents leverage text-rich metadata like HTML inputs or accessibility trees to perform navigation and other tasks. However, the text-only approach is limited in real-world applications, where users typically interact with user interfaces visually\u2014through screenshots\u2014without access to the underlying structural oracle information. This limita-"}, {"title": "2. ShowUI", "content": "ShowUI, as outlined in Fig. 3, is built on top of the vision-language model Qwen2-VL-2B [41], incorporating the following key components optimized for GUI tasks: (i) a novel UI-guided visual token selection strategy for efficient visual modeling, (ii) an interleaved vision-language-action streaming setup to flexibly unify different needs by GUI tasks and enhance training effectiveness, (iii) a training data recipe, crafted through a detailed analysis of individual GUI data types, which enables ShowUI training on a smaller, high-quality corpus. In the following sections, we introduce each component in detail."}, {"title": "2.1. UI-Guided Visual Tokens Selection", "content": "High-resolution screenshots can result in a large number of visual tokens after standard patching. As demonstrated in Fig.4a, a 1344 \u00d7 756 resolution on a PC yields approximately 5184 raw tokens with a 14 \u00d7 14 patching, after a 2 \u00d7 2 [41] merging, still results in 1296 tokens, creating a computational challenge within the self-attention module.\nWhat differentiates UI from natural vision? Unlike natural images, which captures real-world complexities and unpredictable patterns thus rich in semantic, textures, UI screenshots are inherently structured, with clear layouts and consistent color schemes optimized for readability and usability. This difference means that UI images often contain redundant empty spaces or simple backgrounds that do not carry essential information, aling for optimization or pruning. Moreover, small but functionally important elements, like icons or text, demand higher salience due to their role in interactivity and clarity.\nThus, it is necessary to have a strategy that can differentiate between redundant and essential visual elements, enabling effectively pruning of irrelevant visual tokens without compromising usability. We found that the RGB space can serve as a useful guideline for this purpose as pattern variants, text fonts can be easily identify by its RGB values.\nConstruct a UI Connected Graph. After dividing the screenshot into regular patches, we observed many neighboring patches share exactly the same RGB values and are thus redundant. To leverage this, we represent each patch as a node in a graph. If neighboring patches have the same RGB values, we connect their corresponding nodes, forming connected components. This allows us to group and simplify redundant areas while preserving essential visual elements identified by their unique RGB patterns. Visually identical patches can be easily detected by setting a small threshold on the difference between their patch tensors.\nBased on this insight, we use the Union-Find method to identify connected components in this UI connected graph, as described in Algorithm 1. This algorithm produces a graph with $K$ connected component, where $K$ is typically smaller than the original number of patches $G_h \u00d7 G_w$. Based on the assignment of each node to its component, we can model the redundancy relationships among patches.\nAs illustrated in Fig. 4a, this method can effectively balances component number based on their visual informative adaptively, using less component (more redundant patches) in google search page with sparser areas (1296 \u2192 291), while assigning more components (more independent patches) in text-rich overleaf screenshots (1296 \u2192 986)."}, {"title": "2.2. Interleaved VLA Streaming", "content": "In this section, we aim to address how to formulate actions and their relationships with other modalities (i.e., visual and textual queries).\nWhat differentiates Action from natural text? The core functionality of a GUI model is navigation, conditioned on a text query and requiring the model to jointly predict: the correct action type (e.g., [CLICK] or [TYPE]), with corresponding action parameters (e.g., coordinates for [CLICK] or a string for [TYPE]). A major challenge in navigation arises from the action variants across different devices, such as: (i) Device-specific actions (e.g. [CLICK] is unavailable on mobile, whereas [PRESS HOME] does not exist on the web). (ii) Same action with different parameters (e.g. [SCROLL] has two directions-up and down\u2014on the web, but four directions on mobile). (iii) Novel actions at test time that were not encountered during training.\nTo manage these variations within our model, we first structure each action in a JSON format (i.e., { 'action': 'action_type', 'value': 'element',\n 'position': [x,y]}), where [x,y] represents relative coordinates on 0-1. This allows us to standardize actions from diverse devices into a unified format. Secondly, we provide the model with a 'README' in the system prompt, documenting each action's usage within the action space (e.g., 'CLICK': Click on an element, value is not applicable and the position [x,y] is required.) This-"}, {"title": "2.3. GUI Instructional Tuning", "content": "Various GUI datasets are available in community, such as dominant of web data [10, 11, 44], mobile data [30, 36], which may contain element coordinates [11] or user trajectories [30]. Rather than aggregating all available data sources, we analyze each dataset type to select representative data. Our selected data is illustrated in Tab.1. Our discussion is mainly on UI grounding data. For navigation, we source GUIAct [10] with mobile and web devices.\n(i) Web-visual elements: The web provides a highly accessible, text-rich source of UI data, easily crawled from HTML [44]. Our statistical analysis shows that the 'static text' tag accounts for a significant portion (40%). Given that most VLMs already with strong OCR capabilities [2, 41], we focus on collect visually rich elements. To this end, we developed a parser and collected 22K screenshots, retaining only visual-related elements such as those tagged with 'Button' or 'Checkbox'. By removing static text\n(ii) Desktop-diverse query: Desktop data is particularly valuable as it is challenging to collect automatically. We identified OmniAct [22], which includes manual elements from iOS, Windows, and Linux desktops with a small size (2K elements over 100 images). Its element is only labelled by original name such as 'message_ash'. To enrich this dataset and diversity, we employed reverse engineering techniques, utilizing ground-truth bounding boxes and its text elements. Then we prompt GPT-40, with visual prompts highlighting target elements, to derive three types of query: appearance, spatial and intention; which we illustrated in Fig.7.\n(iii) Mobile-Functionality: mobile data is readily available in Android like [25, 36], which provide icon captioning. Notably, we consider [8] as its valuable functionality descriptions that go beyond simple atomic element names.\nBalance Data by Sampling: As shown in Tab.1, data scale varies significantly across types (e.g., only 100 desktop samples). To ensure fair exposure over each type, we develop a sampling strategy during training, giving each batch a comparable probability of including different data types."}, {"title": "3. Experiments", "content": "We organize our experiments on each downstream tasks to address the following questions: Q1: How does our model perform on each task? What improvements are achieved beyond existing VLM baseline? Q2: What are the effects and improvements of each component? Q3: What insights can be gained from each benchmark based on its unique properties?"}, {"title": "3.2.1 Grounding Tasks", "content": "In Tab. 2, we present zero-shot grounding results on the Screenspot [11]. This provides a straightforward signal of the shortcomings in each setup. We includes one additional variant - ShowUI-G, which only used grounding data for training. Our finding includes: (i) Overall all methods, the text track scores are higher than the icon track, even for desktop text, which was less seen during training. This suggests that text grounding ability mainly learned from web and mobile is transferable across platforms. (ii) Mixing navigation data [10] does not impair grounding performance when an effective sampling strategy is used. (iii) The icon track is more challenging due to its visual grounding. Mobile scores are significantly higher than desktop and web, this emphasize the missing of visual UI grounding data beyond mobile devices. (iv) Notably, ShowUI, as the most lightweight method with minimal training data, achieves state-of-the-art grounding performance."}, {"title": "3.2.2 Navigation Tasks", "content": "Mobile: AITW. In Tab.3, we have the following findings: (i) Without interleaved streaming (i.e., without visual history), ShowUI \u2020 provides only a 1.1% acc. improvement over the VLM baseline. However, with visual history, ShowUI achieves an additional 1.7% acc. gain, likely because visual context is crucial for adapting to frequent software changes within a large action space (11), particularly on mobile platforms. (ii) ShowUI's zero-shot navigation learned from GUIAct [10] demonstrates transferability, suggesting further improvements can be made to the navigation component. (iii) ShowUI, beats OmniParser [31] and PaLM2-CoT [37], which either leverage closed-source APIs or HTML information, highlighting its potential of a single model as a standalone visual agent.\nWebsite: Mind2Web. In Tab. 4 for web navigation, we found that: (i) Instruction-tuning has a significant effect, brings 4.6% Avg. Step SR. boost over Qwen2-VL-2B. Notably, ShowUI-2B's zero-shot yield comparable with SeeClick-9.6B which has pretrained and fine-tuning, and achieves relatively high Op. F1 (80%+). (ii) Visual context in this task is less significant than in AITW, possibly because Mind2Web focuses primarily on a single, visually similar website and includes only three actions. (iii) The cross-website and cross-domain settings are harder than cross-tasks. This suggests the bottleneck is lie in UI visual perception rather than textual task understanding (websites/domains are unseen in training). One future effort for improvement is to develop training data with good (visually) domain diversity.\nOnline: MiniWob. In Tab. 5, this benchmark demonstrates model behavior in an online environment. Our key finding is that despite the simplicity of the Miniwob UI, the"}, {"title": "3.3. Ablation Studies", "content": "Impact by UI-Guided Token Selection. In Tab.9a, we examine various visual token optimization methods through the following variants: (i) Baseline: no visual token optimization strategy applied; (ii) Token Merge: a mainstream method introduced in Sec.2.1, conditioned on our UI-Graph; (iii) Token Select.-Random: a variant that randomly selects a subset of visual tokens, serving as a direct baseline; (iv) Token Select.-UI-Graph: our proposed method leveraging UI-Graph for token selection.\nAs shown, Token Merge performs worse than random selection, highlighting the importance of preserving positional relationships between tokens. Token Selection - UI-Graph offers a good balance with a 1.5\u00d7 speedup and competitive accuracy. While applying it at test-time slightly reduce accuracy due to resolution loss, it remains more reliable than random selection, demonstrating the effectiveness of UI connected graph as a guiding criterion.\nSelection of Layers Insertion. In Tab.9b, we present an ablation study on different insertion strategies, including insertion across all layers, early or late X layers, and cross-layer insertion, where layers alternate between inserted and non-inserted. With the same number of inserted layers, cross-layer insertion significantly outperforms both early and late insertion.\nDifferent Selection Ratio. The results is present in Tab.9c, illustrate the selection ratio as a trade-off between speedup and performance, with 0.5 as an effective choice.\nImpact of Interleaved Streaming. We evaluate performance over iterations using interleaved streaming for grounding and navigation tasks to study its effects. (i) Action-Query: In Fig. 10, we compare grounding training with and without multi-turn streaming. Multi-turn streaming leads to faster progress, especially in the initial warmup phase, and maintains a performance gap, demonstrating improved data utilization. (ii) Action-Visual: As shown in previous tables with the ShowUI \u2020 variant, we validated the impact by visual context. Fig. 11 illustrates model progress across iteration steps, where the trend shows that visual+action+multi-turn outperforms visual-action and action-only setups. This validates our interleaved streaming as an effective and efficient strategy.\nD. Impact by Instruction-Tuning Data. One our contributions is the analysis of training data for the grounding task in Sec.2.3. In Tab.6, we present a detailed ablation study to investigate the individual impact of each change on specific devices and settings.\nWe found that (i) Data quality matters: OmniAct, with only 2K elements, achieves comparable scores to web data, and when augmented with GPT40 for diverse queries, it"}, {"title": "5. Conclusions", "content": "We introduced ShowUI, a vision-language-action model for GUI visual agents that addresses key challenges in UI visual and action modeling, and instruction-tuning data curations. From the model aspect, our development of UI-Guided Visual Token Selection allows for efficient processing of high-resolution UI screenshots, significantly reducing computational costs. Our Interleaved Vision-Language-Action Streaming framework effectively manages complex interactions across modalities. From the data aspect, with a carefully curated, high-quality instruction-following dataset, ShowUI achieves strong performance, with a lightweight model size. These results demonstrate ShowUI's potential to advance GUI visual agents towards more human-like interaction and perception.\nLimitation and Future work. Our model is primarily trained on offline data. A promising future direction is to enhance it in online environments through reinforcement learning, enabling deeper exploration of its limitations."}, {"title": "A. Datasets details", "content": "We develop a parser using PyAutoGUI [34] and source websites from 22 representative scenarios such as Airbnb, Booking, AMD, and Apple, which covering shopping, technology, etc. For each scenario, we collect multiple screenshots to maximize annotation coverage. This process yields 22K screenshots with a total of 926K element annotations. After filtering out elements classified as static text, we retain 576K elements, averaging 26 elements per screenshot."}, {"title": "B. Settings", "content": "We utilize 32 V100 GPUs for instruction-tuning, while downstream adaptation is conducted on 8 V100 GPUs. The batch size per GPU is set to 1, with gradient accumulation steps of 2. We use float16 precision for training. To enhance efficiency, we apply LoRA tuning with a rank of 64 and an alpha value of 128 for both the language model and visual encoder, resulting in 4% of the total learnable parameters. We leverage DeepSpeed Zero-2 and use the SDPA attention mechanism. The learning rate is configured to le-4. The maximum visual patch number is 1280. The instruction-tuning training duration is approximately two days.\nUI Connected Graph: We apply the UI-Graph to both the visual encoder and language model with a masking ratio of 0.75, using cross-layer insertion at layer 14. During each iteration, a random ratio of visual tokens is masked. For inference usage, we uniformly sample tokens across each component, ensuring visibility of all components to the model.\nInterleaved Streaming: In our streaming setting, we set up the history number as 2."}, {"title": "B.2. Prompt templates", "content": "We utilize 32 V100 GPUs for instruction-tuning, while downstream adaptation is conducted on 8 V100 GPUs. The batch size per GPU is set to 1, with gradient accumulation steps of 2. We use float16 precision for training. To enhance efficiency, we apply LoRA tuning with a rank of 64 and an alpha value of 128 for both the language model and visual encoder, resulting in 4% of the total learnable parameters. We leverage DeepSpeed Zero-2 and use the SDPA attention mechanism. The learning rate is configured to le-4. The maximum visual patch number is 1280. The instruction-tuning training duration is approximately two days.\nUI Connected Graph: We apply the UI-Graph to both the visual encoder and language model with a masking ratio of 0.75, using cross-layer insertion at layer 14. During each iteration, a random ratio of visual tokens is masked. For inference usage, we uniformly sample tokens across each component, ensuring visibility of all components to the model.\nInterleaved Streaming: In our streaming setting, we set up the history number as 2."}]}