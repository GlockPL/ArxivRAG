{"title": "A Perspective on AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems", "authors": ["Mohamed Dhouiouia", "Jonathan Barnoud", "Rhoslyn Roebuck Williams", "Harry J. Stroud", "Phil Bates", "David R. Glowacki"], "abstract": "Molecular dynamics (MD) simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently been developed as a 'human-in-the-loop' strategy, which leverages high-performance computing to accelerate the researcher's ability to solve the hyperdimensional sampling problem. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular motion, iMD-VR enables researchers and students to efficiently and intuitively explore and navigate these complex, high-dimensional systems. iMD-VR platforms offer a unique opportunity to quickly generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the possibility of employing user-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL is an important technique in robotics that enables agents to mimic complex behaviors from expert demonstrations, thus circumventing the need for explicit programming or intricate reward design. We review the utilization of IL for manipulation task domains in robotics and discuss how iMD-VR recordings could be used as datasets to train IL models for interacting with MD simulations and solving specific molecular 'tasks'. We then investigate how such approaches could be applied to the data structures captured from iMD-VR recordings. Finally, we outline the future research directions and potential challenges of using Al agents to augment human expertise to efficiently navigate vast conformational spaces, highlighting how this approach could provide valuable insight across domains such as materials science, protein engineering, and computer-aided drug design.", "sections": [{"title": "1 Introduction", "content": "Molecular dynamics (MD) simulations are a powerful tool for studying the structure, dynamics, and interactions of molecular systems. However, generating conformational ensembles and sampling rare events, e.g., protein-ligand binding, remains challenging due to high computational costs and the complexity of the associated energy landscapes[38]. Interactive molecular dynamics in virtual reality (iMD-VR) has recently emerged as a promising approach to address these challenges by leveraging human intuition during real-time MD simulations within an immersive 3D environment[47]. In iMD-VR, users can directly manipulate and steer molecular systems using natural hand motions, applying forces to drive physically-relevant rare events such as conformational changes and ligand binding/unbinding [27]. This human-in-the-loop approach leverages the human's innate ability for 3D spatial reasoning and manipulation, enabling a user to intuitively explore complex molecular landscapes. Recent studies have demonstrated the efficacy of iMD-VR in recreating crystallographic binding poses for protein-ligand systems[8] [7] and generating important reactive pathways [44]. These interactive simulations capture valuable conformational data that can be challenging to obtain through conventional MD alone, thus offer new opportunities for applications such as training machine learning and investigating reaction mechanisms.\nImitation learning (IL), or learning from demonstration, is a powerful paradigm in artificial intelligence, enabling machines to acquire new skills by observing and mimicking expert behavior[52]. This approach has been particularly influential in the field of robotics, where it has been used to teach robots complex tasks without the need for explicit programming. By observing human demonstrations, robots can learn to perform a variety of actions, ranging from simple manipulations to complex, multi-step procedures [39][32][13][50]. Learning from observation differs from other types of machine learning such as reinforcement learning, where an explicit reward function needs to be defined in advance or fine-tuned during training. IL offers the ability to learn a mapping of observations to actions done by an expert in demonstrations, making it particularly well-suited to domains for which specifying a reward function is challenging or where human expertise can be leveraged. The versatility of IL has also sparked interest in its application beyond robotics, such as in molecular dynamics, where it could potentially streamline the process of simulating and understanding complex molecular interactions.\nIL often requires a large number of demonstrations to effectively learn a policy, especially for complex tasks. Collecting a sufficiently large and diverse dataset of human demonstrations can be challenging for various reasons [50]. One way to think of it is that human"}, {"title": "2 Virtual reality for molecular simulations", "content": null}, {"title": "2.1\nMolecular vizualisation", "content": "Virtual reality (VR) is revolutionizing the way researchers interact with and visualize molecular structures. VR provides researchers with natural, intuitive 3D interfaces to view and interact with complex molecular structures in way that is not facilitated with traditional 2D interfaces. This can enhance the researcher's understanding of complex 3D molecular arrangements and interactions, which is essential for enabling research insight. Furthermore, VR can enhance scientific collaboration by providing shared virtual environments, which may even be accessible over the internet, thus enabling collaboration across physical distances.\nThere exist several programs for the visualization of molecular simulations in VR, e.g., UnityMol[10], and the commercial software Nanome[1]. Nanome provides a collaborative virtual environment in which users can visualize and manipulate molecular structures in stereoscopic 3D. Researchers can analyze the spatial arrangement of molecules, measure distances between atoms, and dock ligands into protein binding pockets using natural hand gestures [18]. Other examples of software include ProteinVR [2] and Molecular Rift [26]. ProteinVR is a web-based application that works across desktop, mobile, and VR platforms, democratizing access to structural biology in 3D. Molecular Rift provides controller-free manipulation of molecules using intuitive hand gestures.\nBy immersing users in 3D virtual environments, VR enables intuitive exploration of complex biomolecular systems that traditional 2D screens do not facilitate. The application of VR to molecular simulations unlocks several key benefits. First, it provides researchers with natural, intuitive 3D interfaces to view and interact with complex molecular structures. Second, by coupling interactive molecular dynamics with VR, scientists can manipulate molecular systems and observe the effects in real-time, potentially uncovering new mechanistic insights. Third, VR enables collaborative drug design and molecular modeling in shared virtual environments, enhancing scientific teamwork. Finally, the stereoscopic depth perception and wide field of view in VR leads to an enhanced spatial understanding of 3D molecular arrangements.[5]"}, {"title": "2.2 Interactive molecular simulations", "content": "Recent advancement in computational power and improving performance of graphical processing units has facilitated not only the visualization of molecules in VR, but also real-time interactivity. One prominent example is Narupa, an open-source program developed by Glowacki et al.[14] for performing interactive molecular dynamics in virtual reality (iMD-VR). Using VR controllers, Narupa users can apply forces directly to MD simulations in real-time to drive important chemical events such as ligand binding and conformational changes. An example of this is demonstrated in Figure 1. The top"}, {"title": "2.3 Data structure in NanoVer", "content": "In essence, a molecular simulation is a time series consisting of a set of frames of the atomic positions of a molecular system, which can be viewed frame-by-frame as a 'trajectory'. In iMD-VR, these trajectories are generated and visualised on-the-fly. Each frame contains information about the system, e.g. the temperature and energy, and"}, {"title": "2.4 Types of molecular simulations", "content": "NanoVer provides several molecular simulations that any user can load, or users may import their own OpenMM systems. One of the prototypical examples that we use for NanoVer demonstrations is the simulation of a methane molecule and a carbon nanotube, a molecular system relevant to the study of biomolecular channels that act as molecule-selective filters[17]. In this simulation, players can simulate the action of the nanotube as a biomolecular channel by threading the methane through the nanotube (Figure 2). The molecular system comprises 65 atoms: 60 carbons for the nanotube (labelled C1-C60), and 1 carbon and 4 hydrogens for the methane (labelled C61 and H1-H4). Table 1 shows some example data collected for this task. The output trajectory file '.traj' is a binary file containing all data from the molecular simulation. This was converted into a .csv file and the relevant data was filtered for simpler access and processing. The resulting dataframe had 4 columns: atom name, time, coordinates, and user forces. Here 'name' is the atom's label, 'time' is the frame index, 'coordinates' contains the (x,y,z) positions in nanometers, and 'forces' contains the (x,y,z) components of the forces (Fx, Fy, Fz) applied by the user on the specified atom."}, {"title": "3 Imitation Learning in Agents and Multiagent systems", "content": null}, {"title": "3.1 Recent works in literature", "content": "In recent years, imitation learning in agents and multiagent systems has seen significant advancements. One notable contribution is the introduction of Multi-agent Inverse Factorized Q-learning (MIFQ), a novel algorithm that employs mixing networks to aggregate decentralized Q functions for centralized learning and uses hypernetworks to generate weights for mixing networks [51]. This approach has demonstrated superior performance compared to baseline algorithms in various multi-agent environments, including SMACv2, Gold Miner, and Multi Particle Environments. MIFQ enables efficient and stable learning in cooperative multi-agent settings, and its objective function exhibits convexity within the Q function space under certain conditions.\nAnother significant development in the field focuses on scaling laws for imitation learning in single-agent games. This research investigates the impact of scaling up model and data size on imitation learning performance, particularly in Atari games and NetHack [28]. By using Behavioral Cloning (BC) to imitate expert policies, the study reveals that imitation learning loss and mean return follow clear power law trends with respect to FLOPs. Importantly, loss and mean return are highly correlated, indicating that improvements in loss predictably translate to improved performance. The research demonstrates that scaling up model and data size can provide significant improvements in agent performance, with the scaled-up approach surpassing prior state-of-the-art by 1.5x in all settings for NetHack.\nIn the realm of multi-agent systems, the Multi-Agent Adversarial Interaction Priors (MAAIP) approach adapts Multi-Agent Generative Adversarial Imitation Learning (MAGAIL) for modeling interactions between agents [20]. This method introduces new objectives for training the system and models self and opponent observations separately. MAAIP has proven effective for learning interactive behaviors between multiple agents and can be applied to scenarios"}, {"title": "3.1.1 Manipulation tasks", "content": "Manipulation tasks involve robots handling, moving, or altering the state of objects in their environment. In recent years, Imitation learning has been instrumental in teaching robots to perform such tasks with precision and adaptability[31]. For instance, VIOLA [53] a novel IL approach that was implemented and deployed into a real-life robot, outperforms state-of-the-art methods by 45.8% in success rate. This is achieved through the use of a pre-trained vision model which is put into a transformer-like architecture. The authors created a policy to detect task-driven relevant regions for action mapping. Another novel hybrid imitation learning (HIL) framework combines behavior cloning (BC) and state cloning (SC) methods to efficiently learn manipulation tasks like pick-and-place and stacking[16]. This approach has been shown to significantly improve training efficiency and policy flexibility, demonstrating a performance improvement and faster training time compared to pure BC methods. Hua et al. [13] emphasize the efficiency of learning from good samples and the potential for combining reinforcement learning mechanisms to improve the speed and accuracy of imitation learning. They specifically addressed the application of imitation learning in robot manipulation by observing expert demonstrations, which can be generalized to other unseen scenarios."}, {"title": "3.1.2 Locomotion and navigation", "content": "In robotics, locomotion and navigation are two fundamental aspects that enable robots to move and operate within their environments effectively. These concepts are crucial for the development of autonomous systems that can perform a wide range of tasks, from simple delivery services to complex exploration missions. Locomotion refers to the various methods that robots use to move from one place to another. This movement can be achieved through different mechanisms, depending on the robot's design and the environment it is intended to operate in. Navigation involves the process by which a robot determines its position in the environment and plans a path to reach a specific destination. It encompasses several key competencies: Self-Localization; The ability of a robot to establish its own position and orientation within a frame of reference. Path Planning; Once the robot knows its location, path planning involves determining the most efficient or safest route to reach the desired destination. Map-Building and Map Interpretation; For effective navigation, robots often need to construct or utilize maps of their environment."}, {"title": "3.1.3 Human robot interaction", "content": "Human interaction tasks in robotics involve robots engaging in various forms of social interaction and cooperation with humans to achieve shared goals. These tasks encompass direct physical interaction, such as assisting with lifting objects or providing physical therapy, as well as collaborative interaction, where robots and humans work together to complete tasks like assembling products on a manufacturing line. Remote interaction, where humans control or collaborate with robots from a distance, also falls under the umbrella of human-robot interaction tasks. Additionally, robots should be able to learn new tasks from human demonstrations and proactively seek human assistance when needed during task execution. The ultimate goal in human-robot interaction tasks is to achieve natural, efficient, and safe interactions as robots work with humans across various domains. Mehta et al. [23] introduce a learning formalism that unifies approaches for physical human-robot interaction by incorporating demonstrations, corrections, and preferences. It represents a comprehensive approach to learning from human interactions, aiming to improve robot adaptability and performance in collaborative tasks. This framework is designed to learn without making assumptions about the tasks the human wants to teach the robot. The key insight of the paper is that physical human-robot interaction can be a rich source of information for teaching robots, and that by leveraging all available forms of interaction-kinesthetic guidance (demonstrations), adjustments to the robot's motion (corrections), and evaluative feedback (preferences)\u2014a more robust and flexible learning system can be developed. The authors propose a two-step algorithm that first learns a reward model from scratch by comparing the human's input to nearby alternatives and then applies constrained optimization to map the learned reward into a robot trajectory. This process is iterative and allows for real-time updates based on the human's feedback, which can be provided in any order and combination. The approach is validated through simulations and a user study, demonstrating that it can more accurately learn manipulation tasks from physical human interaction than existing baselines, especially when faced with new or unexpected objectives.[23] The paper's insight emphasizes the importance of a unified learning approach that does not rely on prede-"}, {"title": "3.2 Key concepts and techniques", "content": null}, {"title": "3.2.1 Behavioral cloning", "content": "Behavioral cloning (BC) is a straightforward approach that treats imitation learning as a supervised learning problem[30]. Given a dataset of state-action pairs from expert demonstrations, behavioral cloning directly learns a policy (mapping from states to actions) using regression or classification algorithms[37]. The policy is trained to minimize some loss function between the predicted and demonstrated actions on the training data. Based on the demonstration quality, BC is somewhat simple to implement since no extensive knowledge of the environmental dynamics is required. Being treated as supervised learning, a method that is very well studied, makes training BC algorithms computationally efficient.\nConsider a dataset $D = \\{(s_i, a_i)\\}_{i=1}^n$ consisting of state-action pairs collected from an expert policy $\\pi^*$, where $s_i$ represents the state and $a_i$ the action taken by the expert in that state. The objective of behavioral cloning is to learn a policy $\\pi$ that approximates the expert policy $\\pi^*$ as closely as possible.\nThe process involves, as seen in Figure 5:\n1. Data Collection: Collect a dataset $D$ of state-action pairs $(s, a)$ by observing an expert performing the task.\n2. Learning: Train a model on $D$ to learn the mapping from states to actions. This typically involves minimizing a loss function over the dataset. For discrete action spaces, a common choice is the negative log-likelihood (NLL) loss:\n$L(\\pi, s, a^*) = -\\frac{1}{\\eta} \\pi(a^*|s)$\nFor continuous action spaces, the mean squared error (MSE) loss is often used:\n$L(\\pi, s, a^*) = ||\\pi(s) \u2013 a^* ||^2$\n3. Policy Output: After training, the learned policy can be used to perform the task, ideally replicating the expert's performance."}, {"title": "3.2.2 Inverse reinforcement learning", "content": "Inverse Reinforcement Learning (IRL) is a method used to infer the reward function of an agent by observing its behavior within an environment. It assumes that we observe an agent following an unknown policy $\\pi^*$ and we want to infer the reward function $R$ that this policy is optimizing. The problem is challenging because there are potentially many reward functions that could explain the observed behavior, making IRL an ill-posed problem. IRL is typically modeled as a Markov Decision Process (MDP) where the goal is to determine what objectives or values the agent is optimizing for, given its observed actions.\nTo understand IRL, we first need to understand the framework in which it operates, which is the MDP. An MDP is defined by a tuple (S, \u0391, \u03a4, \u03b3, R):\n- S is a set of states. A is a set of actions. T is the transition probability matrix, where T(s'|s, a) gives the probability of transitioning to state s' from state s after taking action a. - y is the discount factor, which determines the present value of future rewards. - R is the reward function, which assigns a scalar reward to each state (or state-action pair).\nA policy is a mapping from states to actions, and the goal in reinforcement learning is to find an optimal policy $\\pi^*$ that maximizes the expected sum of discounted rewards.\nThe general approach to IRL involves the following steps, see Figure 6:\n1. Collecting Data: Observe the behavior of the expert agent and collect state-action trajectories.\n2. Estimating the MDP: Use the collected data to estimate the transition probabilities T and the initial state distribution.\n3. Learning the Reward Function: Infer a reward function R that would make the observed behavior appear optimal.\nThe mathematical formulation of IRL can be described as follows:\n\u2022 Given:\n- A set of observed trajectories $T = \\{(s_1, a_1), (s_2, a_2),...\\}$ from an expert policy $\\pi^*$.\n- An estimated MDP (S, A, T, \u03b3) without the reward function.\n\u2022 Find:\n- A reward function R: S\u00d7A \u2192 R such that the expert policy $\\pi^*$ is optimal for this reward function.\nOne common approach to solving IRL is to use a linear approximation of the reward function, where $R(s, a) = \\theta \\phi(s, a)$, and $\\phi(s, a)$ is a feature representation of the state-action pair. The parameters \u03b8 are then learned by optimizing a likelihood function or by matching the feature expectations of the expert's policy. Several algorithms have been proposed for IRL, including:\n\u2022 Maximum Entropy IRL: This method assumes that the expert behaves in a way that maximizes entropy, meaning that among all policies that could explain the expert's behavior, the one that is least committed to unnecessary constraints is chosen.\n\u2022 Bayesian IRL: This approach treats the reward function as a random variable and uses Bayesian methods to infer a posterior distribution over the reward function given the observed behavior."}, {"title": "3.2.3 Adversarial imitation learning", "content": "IRL algorithms seen previously have a high computational complexity [24][9] since they require the execution of RL in inner loops [11].\nAdversarial imitation learning has been proposed as a solution to this computational challenge[12]. Generative Adversarial Imitation Learning (GAIL) is an adversarial imitation learning algorithm that uses the framework of generative adversarial networks (GANs) to directly learn a policy from expert demonstrations, without needing to first learn a reward function as in inverse reinforcement learning[12].\nThe key idea is to train a generator policy to produce trajectories that are indistinguishable from the expert trajectories, as judged by a discriminator network. This is formulated as a minimax game between the generator and discriminator[12]:\n$\\min_{\\pi} \\max_{D} E_{\\tau_E} [\\log D(s, a)] + E_{\\tau_{\\pi}} [\\log(1 \u2013 D(s, a))] \u2013 \\lambda H(\\pi)$        (3)\nwhere:\n\u2022 \u03c0is the generator policy\n\u2022 D is the discriminator\n\u2022 TE is the expert policy\n\u2022 \u0397 (\u03c0) is an entropy regularization term\nLet \u03c1\u03c0(s, a) denote the occupancy measure, i.e. the distribution of states and actions encountered when navigating the environment with policy \u03c0[12].\nGAIL seeks a policy whose occupancy measure matches the expert's:\n$\\rho_{\\pi}(s, a) \\approx \\rho_{\\pi_E} (s, a)$    (4)\nIt can be shown that finding a policy to minimize the Jensen-Shannon divergence between occupancy measures is equivalent to the following:\n$\\arg \\min_{\\pi} -H (\\pi) + \\Psi_{GA}(\\rho_{\\pi} \u2013 \\rho_{\\pi_E})$   (5)\nwhere VGA is a convex regularizer with the form:\n$\\Psi_{GA}(P) = \\max_D E_{\\pi} [\\log D(s, a)] + E_{\\pi_E} [\\log(1 \u2013 D(s,a))]$      (6)\nThis leads to the GAIL objective in the first equation. The discriminator D is trained to distinguish expert vs policy state-action pairs, while the policy \u03c0 is trained to maximize the discriminator confusion.\nThe GAIL algorithm alternates between training the discriminator and taking policy gradient steps:\n1. Sample trajectories $T_i \\sim \\pi_{\\theta}$; from current policy"}, {"title": "4 Strategies for uses in iMD-VR", "content": "Imitation learning in a fully simulated VR environment for interactive molecular dynamics offers several practical advantages over the current approach used in robotics. In molecular dynamics simulations, both training and inference can be conducted entirely within the virtual environment, eliminating the need to bridge the gap between simulation and physical reality. This approach is more practical for several reasons.\nFirstly, there is consistency between training and deployment. Unlike robotics, where training occurs in VR but deployment happens in the real world, molecular simulations maintain a consistent virtual environment throughout. This eliminates the \"reality gap\" that often plagues robotic applications, where policies learned in simulation may not transfer perfectly to real-world scenarios.\nSecondly, VR-based molecular dynamics simulations offer superior scalability and data generation capabilities. Researchers can generate vast amounts of training data quickly and efficiently, creating diverse scenarios and interactions without the physical constraints or safety concerns associated with real-world robotic systems. This is particularly valuable for exploring complex molecular systems and interactions that would be difficult or impossible to replicate in physical experiments. Furthermore, the simulated environment allows for precise control over all variables, enabling researchers to isolate specific factors and study their effects on molecular interactions. This level of control is often impossible or impractical in physical robotic setups. Researchers can manipulate individual atoms, adjust environmental conditions, and explore extreme scenarios that would be challenging or dangerous to replicate in the real world.\nCost-effectiveness is another significant advantage. Conducting both training and inference in a virtual environment significantly reduces hardware costs and eliminates the need for expensive robotic equipment. This makes the research more accessible to a broader range of institutions and researchers. Additionally, virtual simulations can be run on cloud-based systems, further reducing infrastructure costs and enabling collaborative research across different locations. Safety and repeatability are also key benefits. Virtual molecular dynamics simulations can explore potentially hazardous or extreme conditions without risking damage to physical equipment or compromising safety. Experiments can be repeated indefinitely with exact precision, facilitating rigorous scientific investigation and enabling researchers to explore a wider range of scenarios than would be possible in physical experiments.\nImitation learning approaches like [53], [16], and learning from good samples could be applied to the task of threading a methane"}, {"title": "4.1 Potential applications of IL for iMD-VR", "content": "The use of imitation learning for iMD-VR has a range of potential applications. In this section, we identify two such domains: drug discovery and protein engineering, and material design."}, {"title": "4.1.1 Ligand/drug binding to protein", "content": "One of the most promising applications of IL for iMD-VR is in computer-aided drug design (CADD)[47]. CADD methods are being used within drug development to reduce the financial and temporal costs associated with the discovery, development and analysis of drug candidates. Sabe et al. [36] reported in 2021 that some form of CADD technique had been used in the development pipeline of more than 70 commercialised drugs. Where the target protein structure is known, MD simulations can be used to shortlist candidate molecules for potential bioactivity by calculating protein-ligand complex stability[43]. However, simulating rare events such as ligand binding using MD still remains a challenge.\niMD-VR has been demonstrated as a human-in-the-loop strategy that leverages the human ability to perform spatial tasks to address the problem of the simulation of ligand binding. Ligand binding is akin to 4-dimensional Tetris, where one 3D shape must fit into another. The difficulty is that these shapes are dynamic and flexible, and that they interact with one another in complex ways. Although this is a difficult task to boil down into an algorithm, humans are able to do this naturally using their spatial intuition combined with their motor skills to perform these types of tasks with minimal training. This is exemplified in a study by Deeks et al.[7], who demonstrated the use of iMD-VR for docking ligands to the main protease of the SARS-CoV-2 virus (the virus responsible for the COVID-19 pandemic). The authors found that iMD-VR experts were able to form docked structures that were in agreement with the crystal structures found experimentally. Another notable study by Deeks et al. [8] found that non-experts could also generate accurate structures of protein-ligand complexes. The authors reported that novice iMD-VR users, many of whom were also not experts in ligand binding, could reliably reproduce experimentally-derived docking poses of flexible ligands with only a short amount of training (<40 minutes in VR). This suggests that IL models could be trained effectively using data gathered from both expert and non-expert users, increasing the size of training sets that leverage human intuition to further sample these non-trivial rare events. IL could greatly enhance the use of iMD-VR in the context of ligand-protein binding by learning from the physically relevant trajectories produced by both experts and non-experts to effectively sample the space of possible binding pathways, leading to a better quantitative understanding of the relative energetics of the docking process that naturally influences the effectiveness of a given drug candidate. This could be extended further to protein engineering by training IL models on iMD-VR-generated datasets of users exploring binding pathways for novel proteins."}, {"title": "4.1.2 Material properties investigation", "content": "Another exciting application area for iMD-VR and IL is in the field of material design. The discovery and optimization of new mate-rials with desired properties is a key driver of technological innovation, with applications ranging from energy storage and conversion to aerospace engineering and electronics. Crossley-Lewis et al. demonstrated the extensive utility of iMD-VR in the field of materials science, with a particular emphasis on its research applications in the fields of fast-ion conduction and catalysis. [5] In their paper, the authors examined the defect and transport properties of the fast-ion conductor Li2O-a promising energy storage material[19]-showing that the user interaction facilitated by iMD-VR enables the researcher to investigate the mechanisms of ion transport rapidly without introducing significant bias towards unphysical regions of the potential energy landscape. [5] This indicates the validity of the use of iMD-VR to investigate the properties of solid electrolyte systems, providing an exciting new tool to help accelerate the search for tailored fast-ion conducting materials. Although iMD-VR alone enables the researcher to harness their chemical intuition to search for potential mechanisms, this does not guarantee that the researcher will sample the optimal (and therefore most physically relevant) pathways.\nThis is where IL could enhance the use of iMD-VR in such systems: by combining the chemical intuition of the expert researcher with the innate ability to search hyperdimensional spaces in an automated way provided by the computer, IL could enable efficient honing of relevant mechanistic pathways to better understand the behaviour of fast-ion conductors, accelerating rational solid electrolyte design.\nCrossley-Lewis et al. [5] also used iMD-VR to examine the transport of the catalytic promoter methyl n-hexanoate through the H-ZSM-5 zeolite. In this system, the researchers used iMD-VR to sample the dynamics of the promoter-zeolite system after the rare event of desorption of methyl n-hexanoate from a Br\u00f8nsted acid site within the zeolite framework, an event that is unlikely to be observed on the timescale of a typical unbiased MD simulation. [5] By applying biasing forces to desorb the promoter molecule and pull it into varied positions in the zeolite framework, the researchers were able to investigate the transport dynamics of methyl n-hexanoate on accessible timescales, identifying features of zeolite structure relevant to the dynamics of the molecule. [5] To develop this study further, a more quantitative understanding of both the energetics of desorption and the rate of diffusion of the promoter after desorption would be desirable. Once again, this is where IL could assist iMD-VR in a research context: not only could IL help to determine the optimal pathways for desorption, but it could be used to enhance sampling of the subsequent dynamics after such events. These dynamics are necessary to better approximate quantities of interest such as the diffusion coefficient, a measure of the average promoter diffusion (that greatly influences the catalytic efficiency of the material in question [5]), helping guide the search for effective catalysts and promoters."}, {"title": "5 Conclusion", "content": "By harnessing the spatial reasoning abilities and domain expertise of researchers performing molecular manipulation tasks in immersive VR environments, rich datasets can be generated to train AI agents via imitation learning techniques. This human-in-the-loop approach shows great promise for efficiently exploring vast conformational spaces of molecular systems. Imitation learning methods have been successfully employed in robotics for learning complex manipulation tasks from demonstrations, and can potentially be adapted to learn policies for interacting with and manipulating molecular structures in iMD-VR. The unique 3D interaction data captured from researchers in iMD-VR systems presents an opportunity to encode hu-"}]}