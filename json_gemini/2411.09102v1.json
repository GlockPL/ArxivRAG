{"title": "Provocation: Who benefits from \u201cinclusion\u201d in Generative AI?", "authors": ["Samantha Dalal", "Siobhan Mackenzie Hall", "Nari Johnson"], "abstract": "Socially marginalized groups disproportionately experience representational harms caused by generative AI systems [1, 2, 3, 4, 5, 6, 7]. For example, popular text-to-image (T2I) models have been shown to generate inaccurate, culturally misrepresentative, and insensitive depictions of racial and ethnic minorities [1], people with disabilities [5], and foods from the African continent [4]. As the AI community begins to acknowledge the limits of Internet-scraped datasets and the narrowing of", "sections": [{"title": "Speculative Case Study: How do community members realize benefits & harms from improved GenAI performance?", "content": "Researchers do not yet fully understand how to leverage technical machine learning capabilities to improve the representation of marginalized communities in GenAI models [21, 22, 23, 24]. Thus, we do not always know whether incorporating community feedback during the development process will necessarily lead to an \"improved\" AI model. Regardless, we believe it is important to investigate the key premise motivating participatory approaches to AI: (How) do improved representations in GenAI models benefit members of marginalized groups?\nTo interrogate whom dominant structures of community participation in GenAI development benefit, we present a hypothetical scenario where a technology company invites Vietnamese community members to participate in the development and evaluation of a T2I system. The case study that we present is meant to be an abstraction of higher-level themes that we observed over our experiences working as AI researchers within industry and academic contexts. We use this grounding context to trace the flow of potential benefits and harms between different groups of stakeholders.\nScenario: Thuy, a cultural preservation activist in Vietnam, is invited to participate in a technology company's AI data enrichment initiative where community members label photographs of Vietnamese cultural artifacts for AI training and evaluation. The company aims to improve the depiction of \"Majority World\" cultures [25] in T2I systems to support improved quality-of-service [26], and Thuy is excited to partake in this effort to improve Vietnamese representation in global media. The company believes that improved quality-of-service can help Vietnamese users create images that accurately reflect their culture, for personal projects or educational purposes. Other social actors, such as marketing agencies, textbook publishers, and freelance artists, can also benefit from generating inclusive and accurate media. The technology company adopts a common structure of participation [27] to engage with Thuy and other Vietnamese community advocates: they provide participants with one-time compensation for providing data and expertise. The company has not explored paths for participant ownership or control over data or AI models that are created as a result of the engagement.\n(How) can community members benefit when they are end-users? Thuy and other members of her community may face financial barriers in realizing the benefits of improved quality of service in T2I models. While Thuy is provided with one-time compensation for her participation, she does not continue to financially benefit from the future use of her data or the AI models it was used to improve. The company can improve its T2I offerings and monetize its competitive advantage by putting its services behind a paywall. Thuy's peers and other members can now use the model to generate accurate depictions of their likeness, but must navigate the company's paywall structures. Due to a lack of ownership over their data and resulting AI models, community ambassadors like Thuy are sold back models with improvements that would not have been possible without their labor.\nBeyond navigating paywalls, Thuy and other community members face additional barriers in accessing and using the company's models. For example, for many marginalized communities, model access can be complicated by several potential issues such as a lack of reliable Internet connectivity [4] and inaccessible user interfaces [28, 29]. Thus, Thuy is unlikely to be able to realize the benefits of the model's improved performance as an end-user if she cannot reliably access and navigate its interface."}, {"title": "(How) can community members benefit when they are not end-users?", "content": "Thuy may face socio-political barriers in realizing indirect benefits resulting from social actors using T2I models as end-users to create images of the Vietnamese community. Many researchers have argued that due to the increasing prevalence of AI-generated media, GenAI systems will shape societal representation [5, 6, 30, 31, 32] and thus precipitate change in societal attitudes towards marginalized communities. For example, a marketing firm may use the GenAI model to create images for an ad campaign that depicts Vietnamese people and culture, which is then seen by millions of people.\nHowever, media studies scholars have identified that representation in media alone will not result in direct change to material circumstances for marginalized communities [33, 34, 35]. The political economy of the media ecosystem, including industry logics and financial incentives, dictates the kinds of media that are produced [36]. Thus, Thuy is unlikely to realize the benefits of social actors using T2I models to create images of her community unless social, political, and economic conditions all align to transform visibility into political power."}, {"title": "Harms marginalized groups can experience as a result of their participation", "content": "Increased visibility in AI-generated media may make marginalized communities susceptible to a wide and emerging range of AI-mediated harms [26, 37]. For example, as social actors (e.g., textbook companies) realize that they can use AI to generate accurate depictions of Vietnamese culture, they may no longer consult or compensate Vietnamese community members, resulting in further financial and social marginalization [38, 39, 40]. Other actors can exploit improved representations of marginalized communities to inflict harm such as impersonation, misinformation, or the creation of violent/NSFW content [41, 42]. Thus, members of marginalized communities rely on technology institutions to implement effective policies to protect their likeness. While the technology company could implement mitigation steps (e.g., access restrictions or usage licenses [43, 44]) to prevent misuse, they may find them at odds with their profit motives."}, {"title": "Implications", "content": "While the details of this scenario were speculative, the discussed model of participatory engagement as one-time consultation illustrates the reality of how technology institutions and academic researchers often engage socially marginalized communities in AI development today [17, 27, 45]. Thus, we urge the broader AI community, including those who construct or participate in participatory engagements, to critically evaluate whether these dominant structures of participation do in fact yield their intended benefits for marginalized communities. Al researchers and industry actors who are conducting participatory engagements with marginalized communities should be more transparent to participants and the community about the accessibility of benefits to participants and the contingencies upon which these benefits rely. In Appendix A, we pose future directions and highlight promising examples towards restructuring participation beyond consultation, and towards supporting meaningful community ownership, participation, and power over AI."}, {"title": "Broader impact statement", "content": "As discussed in our provocation, we believe that participatory engagements with socially marginalized groups are critical to the broader field of AI and machine learning. We urge researchers to ask themselves how communities whose participation we solicit can benefit from improved model performance. This critical self-reflection requires that researchers map out both the direct benefits participants could experience as end-users and the indirect benefits participants could realize as a result of other social actors using AI systems developed with community input. Understanding how such participatory engagements can be structured is of timely importance given the rapidly advancing capabilities of generative AI; new regulatory and policy requirements that require consultation with impacted groups [46, 47]; and to combat the increasing centralization of power in who has a say in AI's increasing influence on society [16]."}, {"title": "Limitations", "content": "We acknowledge the limitations of our analysis, which is centered around a speculative case study with imagined actors. The barriers to realizing benefits from AI that we surfaced in our case study were based on this speculative context informed by our past experiences (e.g., the communities we are members of, or have engaged in research with before) and our positionality as AI researchers. Future work can engage more deeply in analyzing real-world examples of participatory engagements with socially marginalized groups, and understanding how barriers participants face when realizing benefits vary across shared identities and contexts.\nIn this short piece, we briefly sketch the \u201cdominant structures\u201d of participation [27] in GenAI evaluation, our concerns with these structures, and potential paths forward. In doing so, our goal is not to critique the premise that socially marginalized communities should be involved in AI development and evaluation; or that existing participatory efforts should not be pursued. Rather, we remain hopeful that more deeply interrogating how participation is structured can lead to more empowering and constructive ways of engaging socially marginalized communities. Deeper engagement beyond what we could do within this workshop paper contribution is required to understand how structures of participation in GenAI development and evaluation can be shaped to support the equitable distribution of benefits and power among stakeholders."}, {"title": "Imagining paths forward", "content": "What sources of inspiration can researchers or facilitators of participatory AI initiatives turn to in their pursuit of more equitable engagement practices?\nIn this section, we share several resources and directions for paths forward. We do not aim to be comprehensive; rather, we highlight a few initiatives that propose alternatives to dominant participation structures. We loosely organize our discussion under two motivating questions. First, we look at theories from adjacent fields that, while not specifically about AI, provide valuable insights into participation and power. Second, we examine current efforts aimed at disrupting dominant structures and creating alternative modes of engagement that benefit marginalized communities.\nAre there theories from literature or other forms of community-based knowledge that can inform paths forward for AI? Participation can be extractive. Communities can lose control over how their data is used and shared once it is collected for model training, fail to be credited for their contributions to model development, or not be properly compensated for their knowledge. Below we list some resources from Indigenous data studies, dataset development, and critical data studies that identify how researchers can respect communities' preferences around data sharing.\n\u2022 Christen [48] identifies how some Indigenous communities have cultural norms for sharing certain types of data based on social relationships to the data artifact. AI researchers developing datasets of cultural artifacts with Indigenous communities can do work to first understand what cultural artifacts they are collecting that may have specific protocols for sharing. Researchers can then inform communities about the limitations of restricting access to images when developing and deploying GenAI models so communities can exert more informed consent.\n\u2022 Vincent et al. [49] conceptualize the power that contributors hold over models as data leverage. Contributors to datasets can exert power over model development and performance by reducing, stopping, redirecting, or manipulating their data. Data leverage makes explicit technology companies' dependence on marginalized communities to improve their models' performance. Communities therefore have a significant amount of leverage to share the terms of their future inclusion in AI development. AI researchers should consider explaining to contributors the leverage they hold over the model development process as they address fair compensation for dataset contributions. Doing so could provide contributors with a way to conceptualize the value of their data and allow them to more critically assess the remuneration they are being offered for participating and the terms of their participation. In addition, AI researchers could use data leverage to calculate more accurate estimates of financial remuneration to contributors: How much would they be willing to pay to avoid contributors using their leverage to disrupt their model?\nAre there example community collaborations that offer alternative models on how to structure participation in AI development/evaluation? Past scholarship [17, 27] has demonstrated how many 'participatory Al' engagements are limited to consultation and inclusion (e.g., collecting data from participants to enrich models), without granting participants meaningful opportunities for ownership and control over the resulting datasets and models. While participants may be able to give input on how they think the model should behave, ultimately, \u201cparticipants have little say regarding the model's impact in the world: whether it is developed, what other data it is trained on, what it may be used for, or if and how it should be deployed\" [27]. We identify some resources where researchers and communities have been developing alternative models to structure more equitable community engagement in AI development that ensures that participants have a meaningful say over model development and deployment.\n1. Alternative models of acknowledgment for participation. Singh et al. [50] develop protocols for recognizing community contribution to AI development by operationalizing a broad definition of authorship for academic papers. Similar initiatives have also been led or adopted by other community-driven AI initiatives [51, 4, 52]. Papers are a valuable currency for visibility and recognition in the AI/ML development space. By recognizing community members as contributors to AI/ML development in authorship, researchers can work towards more equitable sharing of benefits.\n2. Alternative models of AI development. In contrast to enriching technology companies' commercial foundation model offerings, some initiatives explore how to best support communities in developing their own smaller, more bespoke models, which are then owned and operated by community members. Past efforts have surfaced how communities often need to overcome infrastructural barriers such as limited available training data [53], capacity, and access to financial capital and compute [54] to support creating, hosting, and maintaining their own models.\n(a) One prominent example is the Te Hiku Media foundation, a M\u0101ori nonprofit, that decided to develop its own data hosting platform and transcription models for the te reo language [55].\n(b) Researchers from DAIR [56] have similarly urged the research community to support local indigenous NLP organizations like Ghana NLP and Lesan AI who \"create machine translation systems for the specific communities they belong to\".\n3. Alternative models of dataset ownership and usage. Many participatory engagements involve compensating community members in exchange for complete ownership over their data (to use for future AI development). In contrast, several communities that own their data have experimented with alternative models to govern who can use their datasets or models, and for what purpose. These usage restrictions are often specified in licenses or other types of contractual agreements [57, 58].\n(a) Some licenses attempt to protect participants from AI-mediated harms by restricting how other stakeholders can use resulting datasets and models. The \"Licensing African Datasets\" project explores how to create licenses for African datasets that better re-distribute benefits back towards African citizens and companies, with the expectation that \"users in developed nations would perhaps pay for use of the work or use the work under more restrictive terms\" [59]. Similarly, Te Hiku Media created a data license that \"will only grant data access to organizations that agree to respect M\u0101ori values, stay within the bounds of consent, and pass on any benefits derived from use back to the M\u0101ori people\" [55].\n(b) Future licenses can also explore specifying alternative compensation structures that allow communities to receive continued royalties (beyond one-time compensation) to encourage profit-sharing as models that depict their likeness continue to be used [60].\n4. Supporting community-driven impact assessment, criticism, and refusal. Many participatory engagements motivate community members to participate by lauding the benefits of improved GenAI representations. We urge those conducting such engagements to involve community members in interrogating what barriers stand in the way of realizing these benefits, and in understanding potential algorithmic harms that may result from improved representations.\n(a) Facilitators of such engagements should make room for outcomes where participants decide the harms outweigh the benefits [61, 62]. For example, although queer scholars noticed that state-of-the-art AI voice cloning tools underperformed when cloning the voices of gay speakers, they ultimately decided against developing an improved AI technology due to concerns that an improved technology may be misused to surveil, misappropriate, or mock gay people [37]. Making room for such critical engagements will require educating participants who enter into engagements with varying levels of familiarity about AI capabilities and harms. We believe that facilitators similarly have much to learn from the situated expertise of community members \u2013 in fact, many scholars have argued that impacted communities themselves are best equipped to anticipate AI harms [63, 64, 65, 66].\n(b) Communities should not just be relegated to red-teaming roles where their cultural expertise is used to identify AI harms, as this can be psychologically damaging [67, 68] and further reify existing power distributions between AI developers and communities2. Rather, more work is needed to build the infrastructures that empower community members to define and achieve algorithmic accountability and recourse on their own terms."}]}