{"title": "ACTION MAPPING FOR REINFORCEMENT LEARNING IN CONTINUOUS ENVIRONMENTS WITH CONSTRAINTS", "authors": ["Mirco Theile", "Lukas Dirnberger", "Raphael Trumpp", "Marco Caccamo", "Alberto L. Sangiovanni-Vincentelli"], "abstract": "Deep reinforcement learning (DRL) has had success across various domains, but applying it to environments with constraints remains challenging due to poor sample efficiency and slow convergence. Recent literature explored incorporating model knowledge to mitigate these problems, particularly through the use of models that assess the feasibility of proposed actions. However, integrating feasibility models efficiently into DRL pipelines in environments with continuous action spaces is non-trivial. We propose a novel DRL training strategy utilizing action mapping that leverages feasibility models to streamline the learning process. By decoupling the learning of feasible actions from policy optimization, action mapping allows DRL agents to focus on selecting the optimal action from a reduced feasible action set. We demonstrate through experiments that action mapping significantly improves training performance in constrained environments with continuous action spaces, especially with imperfect feasibility models.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Reinforcement learning (DRL) has emerged as a powerful tool across numerous application domains, ranging from robotics (Funk et al., 2022) and autonomous system (Trumpp et al., 2024) to game-playing (Vinyals et al., 2019) and other decision-making tasks Bayerlein et al. (2021). The ability of DRL to learn complex behaviors through trial and error makes it highly promising for solving challenging problems. Despite the potential of DRL, its application is limited by poor sample efficiency and challenges in handling constraints that frequently arise in real-world tasks. Such constraints, such as safety requirements or physical limits, complicate the exploration and learning processes (Achiam et al., 2017).\nIn many constrained environments, it is essential to prevent the agent from selecting actions that could violate certain constraints. Therefore, it has been proposed to utilize feasibility models that assess the feasibility of proposed actions at a given state. Their application is straightforward in discrete action spaces, where infeasible actions can simply be masked out, preventing the agent from choosing them (Huang & Onta\u00f1\u00f3n, 2020). Action masking has been successfully applied in various discrete action space problems, such as vehicle routing (Nazari et al., 2018), autonomous driving scenarios (Krasowski et al., 2020), and task scheduling (Sun et al., 2024). However, integrating feasibility models becomes significantly more challenging in continuous action spaces, as actions cannot be directly masked out.\nSeveral methods have been proposed for integrating feasibility models in continuous action spaces. Action replacement substitutes infeasible actions with predefined feasible ones (Srinivasan et al.,"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 STATE-WISE CONSTRAINED MARKOV DECISION PROCESS", "content": "A state-wise constrained Markov Decision Process (SCMDP) (Zhao et al., 2023) can be defined through the tuple (S, A, R, {Ci}vi, P, \u03b3, So, \u03bc), in which S and A are the state and action space. The reward function R : S \u00d7 A \u2192 R defines the immediate reward received for performing a specific action in a given state. A transition function P : S \u00d7 A \u2192 P(S) describes the stochastic evolution of the system, with P(S) defining a probability distribution over the state space. The discount factor \u03b3\u2208 [0,1] weighs the importance of immediate and future rewards. Additionally, a set of initial states So and an initial state distribution \u03bc = P(So) are provided. When following a stochastic policy \u03c0 : S \u2192 P(A), the expected discounted cumulative reward is defined as\n$$J(\u03c0) = \u0395 [\\sum_{t=0}^\\infty \u03b3^tR(S_t, a_t) | s_0 \\sim \u03bc, a_t \\sim \u03c0(S_t), S_{t+1} \\sim P(S_t, a_t)] $$\\nIn contrast to a regular MDP (Sutton, 2018), in an SCMDP, a set of cost functions {Ci}vi is defined, in which Ci:S\u00d7A\u00d7S \u2192 R, such that every transition is associated with a cost value. In a CMDP (Altman, 2021), the expected discounted cumulative cost for each cost function Ci needs"}, {"title": "2.2 DEEP REINFORCEMENT LEARNING", "content": "In reinforcement learning, the goal is to learn a policy \u03c0(as) that maximizes the expected cumulative reward as in equation 1 (Sutton, 2018). In DRL, policies are parameterized by deep neural networks, enabling agents to handle high-dimensional state and action spaces. For environments with continuous action spaces, two widely-used DRL algorithms are the off-policy Soft Actor-Critic (SAC) (Haarnoja et al., 2018) and on-policy Proximal Policy Optimization (PPO) (Schulman et al., 2017) algorithms.\nSoft Actor-Critic (SAC). SAC is an off-policy algorithm that maximizes a reward signal while encouraging exploration through an entropy regularization term. The policy in SAC aims to maximize both the expected cumulative reward and the policy entropy, encouraging stochasticity in action selection. The SAC objective is defined as\n$$J(\u03c0) = E_{(st,at)~\u03c0} [Q(st, at) + \u03b1H(\u03c0(\u00b7|st))],$$\nwhere Q(st, at) represents the Q-function, which estimates the expected return when taking action at in state st, and H(\u03c0(\u00b7|st)) is the entropy of the policy. The hyperparameter \u03b1 controls the tradeoff between return maximization and exploration.\nProximal Policy Optimization (PPO). PPO is an on-policy algorithm that improves training stability by limiting the magnitude of policy updates to ensure smoother learning. The PPO objective is defined using a clipped surrogate loss to avoid large deviations from the current policy:\n$$J(\u03c0) = Et [min (rt(\u03c0)At, clip (rt (\u03c0), 1 \u2013 \u0454,1 + \u0454)At)],$$\nwhere rt(\u03c0) is the probability ratio between the new and old policies, and At \u2248 Q(st, at) \u2013 V\u03c0 (st) is the approximate advantage function. In the advantage, the state value V\u03c0 is the expected value if following the current policy. The advantage is commonly estimated using the generalized advantage estimate (GAE) from Schulman et al. (2015)."}, {"title": "2.3 FEASIBILITY MODELS", "content": "Given the individual cost functions for a transition, a joint cost function can be defined as\n$$C(St, at, St+1) = \\sum_{\\forall i} max \\{0; C_i(St, at, St+1) \u2013 W_i\\},$$\nwhich is 0 if no cost function exceeds its bound and otherwise the sum of the violations. With the joint cost function, a policy-dependent trajectory cost can be defined as\n$$C_T (s; \u03c0) = \\max_{(St,at,St+1)~\u03c4_\u03c0(s)} C(St, at, St+1)$$\nthat expresses the highest joint cost of any transition(st, at, St+1) along all possible trajectories, starting from some s and following \u03c0. It is 0 if no cost function exceeds its bound.\nA feasibility model G : S \u00d7 A \u2192 R defines the cost violation of the transition induced by applying action at at state st, plus the cost violation of the most feasible policy from the next state. Formally, we define it as\n$$G(St, at) = \\max_{St+1~P(. St,at)} [C(st, at, $t+1) + \\min_{\u03c0\u2208\u03a0} C'(St+1; \u03c0)];$$"}, {"title": "3 RELATED RESEARCH", "content": "The majority of the safe RL literature focuses on discrete actions through action masking (Huang & Onta\u00f1\u00f3n, 2020), often through shielding (Alshiekh et al., 2018). Action projection is usually proposed to incorporate feasibility models for continuous action spaces. Donti et al. (2021) propose DC3 to perform action projection through gradient descent on a feasibility model, while Cheng et al. (2019) use control barrier functions. When the feasible action space is known and can be described as a convex polytope, a limiting assumption, Stolz et al. (2024) introduce a similar concept to action mapping that also improves performance.\nLearning-based approaches incorporate constraints directly into the RL process, often using techniques like Lagrangian optimization or dual frameworks. CMDPs (Altman, 2021) introduce cumulative cost constraints, while Constrained Policy Optimization (CPO) Achiam et al. (2017) extends trust-region policy optimization by ensuring monotonic improvement under safety constraints. Bharadhwaj et al. (2020) propose conservative safety critics that reject unsafe actions during exploration and resamples from the actor, while Srinivasan et al. (2020) replaces the unsafe action with a null action. Penalized PPO (P30) (Zhang et al., 2022) further incorporates constraint penalties, and Lagrangian PPO (Ray et al., 2019) and feasible actor-critic (Ma et al., 2021) use Lagrangian multipliers to balance reward maximization and constraint satisfaction.\nCombining model-based and learning approaches, learned feasibility models are often used to perform action projection. Dalal et al. (2018) learn a linear approximation of the cost function and perform action projection using the linear model. Chow et al. (2019) learn a Lyapunov function and perform action or parameter projection. Zhang et al. (2023) use DC3 with a learned safety critic, using it for iterative gradient descent-based action projection. Further approaches to safe RL can be found in surveys by Gu et al. (2022b) and Zhao et al. (2023).\nWhile recent works have primarily focused on action projection methods when incorporating feasibility models, we propose a novel method that describes a mapping instead of a projection, which we show can improve learning performance.\nBesides the safe RL perspective, action mapping is also related to the research in action representation learning. In action representation, a continuous latent action representation of large (discrete) action spaces is learned (Chandak et al., 2019). It has been extended to learn representation of sequences of actions (Whitney et al., 2020), mixed discrete and continuous actions (Li et al., 2022), or specifically for offline RL (Gu et al., 2022a). Action mapping can be thought of as finding an action representation for the state-dependent set of feasible actions."}, {"title": "4 ACTION MAPPING METHODOLOGY", "content": "If a feasibility model g from equation 8 can be derived for an environment, the question is how to use it efficiently in RL. The intuition of action mapping is to first learn all feasible actions through interactions with g and subsequently train a policy through interactions with the environment to choose the best action among the feasible ones. By allowing the objective policy to choose only among feasible actions, the SCMDP is effectively transformed into an unconstrained MDP, as illustrated in Fig. 1. Since unconstrained MDPs are generally easier to solve than SCMDPs-primarily due to reduced exploration complexity-action mapping can drastically improve training performance. In the following, the time index is dropped from st and at for improved readability.\nFormally, given g, the state-dependent set of feasible actions A+ C A contains all actions for which g(s, a) = 1. To learn all feasible actions, a feasibility policy is defined as\n$$\u03c0f : S \u00d7 Z \u2192 A+,$$"}, {"title": "4.1 FEASIBILITY POLICY", "content": "To train the feasibility policy, we use the approach from Theile et al. (2024). The parameterized feasibility policy \u03c0 with parameters @ aims to be a state-dependent surjective map from the latent space Z to the set of feasible actions A+, to allow the objective policy to choose among all feasible actions. When sampling z ~ U(Z), i.e., uniformly from the latent space, \u03c0f becomes a generator with a conditional probability density function (pdf) q\u00ba(a|s). Since of is task-independent, this generator should generate all feasible actions equally likely without any bias toward any specific feasible action. Therefore, the target of q\u00ba (als) is a uniform distribution in the feasible action space, i.e., U(A+).\nThis uniform target distribution is given through the feasibility model g as\n$$p(as) = \\frac{g(s, a)}{Z(s)} with Z(s) = \\int_A g(s, a')da'$$\nwhere Z(s) is a partition function, effectively indicating the volume of feasible actions given a state. The objective of the feasibility policy is then mine D(p(\u00b7|s)||q\u00ba(\u00b7|s)), with a divergence measure D.\nSince q\u00ba and p are not available in closed form, both need to be approximated. The distribution of the policy can be approximated through a kernel density estimate (KDE) based on N samples from q\u00ba(s) as\n$$\\hat{q}_f^\u03c3(a|s) = \\frac{1}{N} \\sum_{a_i~q^\u03b8(s)} k_\u03c3(a - a_i),$$\nwith a kernel k with bandwidth \u03c3. To explore the feasibility of actions outside the support of q\u00ba, Gaussian noise is added to the sampled actions as\n$$a = a_i + \u03b5_i, \u03b5_i ~ \u039d(0, \u03c3'),$$"}, {"title": "4.2 OBJECTIVE POLICY", "content": "Algorithm 1 shows our proposed training procedure for AM-SAC based on Soft Actor-Critc (SAC) and AM-PPO using Proximal Policy Optimization (PPO). For both algorithms, the objective policy \u03c0\u00ba parameterizes a Gaussian from which a value x is sampled (lines 6-7). Since the Gaussian is not bounded, we squash it using a tanh yielding the latent value z (line 8). The latent z is then mapped to an action a using \u03c0 (line 9). If using AM-PPO, the state-value estimate Vs and the log-likelihood of the latent are gathered (lines 10-11). For the log-likelihood, a term is added to compensate for the squashing effect. Using the action a, the environment steps to the next state s', yielding reward r and a termination flag d (line 12), and the collected experience tuples are stored in the buffer D (line 13). The experience tuples do not contain the action a but solely the latent z.\nWhen the buffer is full (AM-PPO) or the buffer has sufficient samples for a batch (AM-SAC), the agent is trained (line 15). For AM-SAC, a standard SAC training step is performed in which the critic is updated to estimate the state-latent value, and the actor maximizes the critic's output with an added entropy term (line 16). For AM-PPO (line 17), the agent is trained on the full buffer as done in PPO, with the critic updated to predict the state value and the actor through standard policy optimization on the latent distribution. In AM-PPO, the buffer is reset after training.\nIn principle, \u03c0\u03bf could be trained on the actions a instead of the latent z. However, this leads to problems in AM-PPO and AM-SAC. In AM-PPO, it would be challenging to estimate the log-likelihood of an action a given the log-likelihood of latent z. While \u03c0 is trained to map uniformly into the set of feasible actions, it is neither perfect nor strictly bijective, and the log-likelihood for a would require approximations, e.g., through KDEs. This is costly and likely creates ill-posed gradients for the training of \u03c0\u03bf. In AM-SAC, if training Q&(s, a), the policy gradient for \u03c0\u03bf could propagate through . While this is tractable, Q(s, a) is not trained on infeasible actions and thus likely yields arbitrary gradients near the feasibility border, preventing \u03c0\u03bf from jumping between disconnected sets of feasible actions. Additionally, the entropy in the action space is difficult to assess, similar to the log-likelihood. Preliminary experiments on training AM-SAC on a, with entropy in the latent space, showed no advantage compared with standard SAC training."}, {"title": "5 EXPERIMENT SETUP", "content": ""}, {"title": "5.1 APPLICATIONS", "content": "We define two different RL environments, shown in Figs. 2 and 3, with continuous state and action spaces to demonstrate how action mapping can be implemented and to evaluate its performance compared with common approaches. The first experiment is a robotic arm end-effector pose tracking task with multiple obstacles. This task was designed so that a perfect feasibility model can be derived and a feasible null action exists. The second experiment is a path planning problem with constant velocity and non-holonomic constraints, which can be found using fixed-wing aircraft. Since a fixed-wing aircraft cannot stop or turn around instantaneously, deriving a perfect feasibility model is extremely challenging. Therefore, we utilize that environment to showcase action mapping's performance using approximate feasibility models. In both environments, the episode is terminated when a constraint is violated, which exacerbates the challenge for DRL."}, {"title": "5.1.1 ROBOTIC ARM END-EFFECTOR POSE", "content": "In this environment, visualized in Fig. 2, the agent is a purely kinematic robot arm, neglecting inertia, loosely replicating a 7 DOF Franka Research 3 robotic arm (Franka-Robotics, 2024). Given a starting pose, the agent needs to move the joints such that its end-effector reaches a target pose without colliding with obstacles. The obstacles are represented by spheres, and the collision shape of the robot arm is defined by a series of capsules that can be seen as a pessimistic safety hull. The obstacles are sampled using rejection sampling to avoid intersections with the start and end configuration.\nState space. The state contains the 7 joint angles of the robot arm, the target pose (rotation +\ntranslation from the origin), and the parameters of up to 20 spherical obstacles.\nAction space. The action is defined as a delta of the joint angles.\nConstraints. The agent is not allowed to exceed its joint limits or predefined maximum cartesian velocities of each joint. No part of the robot arm is allowed to collide with any of the obstacles."}, {"title": "5.1.2 \u039d\u039fN-HOLONOMIC PATH PLANNING WITH CONSTANT VELOCITY", "content": "This environment contains an agent that needs to collect targets while avoiding obstacles. The agent can be thought of as a fixed-wing aircraft that needs to maintain a constant velocity, and its turns cannot exceed a maximum curvature. The airplane in Fig. 3 is for visualization purposes only; the agent's dimensions are assumed to be integrated into the obstacles.\nState space. The state space contains 30 randomly sampled rectangular obstacles and 10 randomly placed and sized circular targets. Additionally, the agent has a position and current velocity.\nAction space. The agent parameterizes 2D cubic Bezier curves, which are anchored at the agent's position and starting in the agent's current direction, yielding a 5D action space. The splines are followed for a constant time, after which a new spline is generated by the agent.\nConstraints. The agent is not allowed to collide with any obstacle or leave the squared area. While following the spline for a constant following time, the induced curvature must not exceed a curvature bound, and the agent must not reach the end of the spline.\nReward function. The agent receives a reward of 0.1 when a target is collected and an additional reward of 1.0 when all targets are collected.\nFeasibility model. The approximate feasibility model generates 64 points along the spline and locally assesses collisions with obstacles, whether a point is out of bounds, and whether the local curvature exceeds the curvature bound. Additionally, it adds the Euclidean distances between the points to estimate the length of the spline. The spline length needs to be within length bounds.\nThe idea behind the spline-based action space is to express a multi-step action with reduced dimensionality. Through this multi-step action, the feasibility model can assess whether a feasible path exists within a time horizon, effectively expressing a short horizon policy that minimizes the future trajectory cost in the second term of equation 7. Therefore, the minimum length of the generated spline for the feasibility model is set to a multiple of the distance traveled per time step (a factor of 2.5 in this experiment). The maximum length of a spline is defined to bound the action space (3.5 in the experiment), yielding a look-ahead of around two timesteps. We train different SAC configurations and compare their performance in the next section. Our neural network architectures and hyperparameters for these environments are presented in Appendix B."}, {"title": "5.2 COMPARISON", "content": "Given a feasibility model G from equation 7 or its Boolean-valued version g from equation 8, the three common approaches to utilize it are action replacement, resampling, and projection. These approaches are described in more detail in Appendix C.1. They each offer distinct trade-offs in terms of computational cost and feasibility guarantees, and our experiments explore their performance in different settings.\nAdditionally to these methods utilizing feasibility models, we compare with model-free Lagrangian methods \"Lagrangian SAC\" (Ha et al., 2020) and \"Lagrangian PPO\" (Ray et al., 2019). In these methods, a safety critic is trained to estimate the expected cumulative cost or, in our case, the expected probability of constraint violation, and a policy aims to maximize a Lagrangian dual problem. The two algorithms are described in more detail in Appendix C.2."}, {"title": "6 RESULTS", "content": ""}, {"title": "6.1 ROBOT ARM", "content": "To demonstrate the training performance in the robotic arm environment, Figs. 4a and 4b show the cumulative return and constraint violations throughout training. It can be seen that the baseline PPO agent learns robustly, continuously increasing performance, even though showing high failure rates. The Lagrangian PPO shows better constraint satisfaction with similar objective performance. In contrast, action replacement and resampling appear to hamper performance. PPO with action replacement struggles to learn anything in the beginning, presumably because most proposed actions are replaced with the null action. Therefore, its failure rate is constant at zero. PPO with action resampling first learns faster than PPO but then exhibits instability, likely due to the wrong estimation of the policy ratio in the PPO objective.\nPPO with action projection and action mapping (AM-PPO, AM-PPO + Replacement) learn significantly faster with higher final performance than the model-free baselines. Action mapping yields slightly higher performance at the end of training, and it exhibits fewer constraint violations than action projection. Adding action replacement to action mapping yields the best performance without constraint violations. This application showed that action mapping and projection are both very beneficial with perfect feasibility models that are mostly convex, with action mapping having a slight edge. Since in this example a safe action exists, action replacement can always be added to guarantee constraint satisfaction and together with action mapping, it also improves performance. Additionally, an evaluation of training and inference times in Appendix F shows that projection is significantly more expensive than action mapping."}, {"title": "6.2 PATH PLANNING", "content": "Before inspecting the training performance of the different approaches, Fig. 5 shows the inner workings of action mapping. Fig. 5a shows the output distribution of the pretrained feasibility policy \u03c0 when sampling uniformly from the latent space Z. The feasibility policy is able to generate actions in both disconnected sets of feasible actions, with only minimal actions between. Figs. 5b and 5c show how the objective policy can take advantage of this. At the beginning of training, the agent outputs a distribution with high entropy in the latent space, leading to a bi-modal distribution in the"}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "We proposed and implemented a novel DRL training strategy based on action mapping. Our results demonstrate that this approach performs exceptionally well, particularly when using approximate feasibility models. We highlighted how even approximate model knowledge can be effectively incorporated into the DRL process to enhance training performance, emphasizing the potential to integrate domain-specific insights into DRL frameworks. We further show how action mapping allows the agent to express multi-modal action distributions, which can significantly improve exploration.\nThe use of KDE introduces some distance between the generated actions and the boundary of feasible actions, which may result in conservative action selection. Furthermore, the feasibility policy \u03c0f does not completely eliminate the generation of infeasible actions and, therefore, does not provide strict safety guarantees. Consequently, the learned \u03c0f is not surjective and does not remove all constraints from the SCMDP, but still significantly relaxes the constraints.\nWhile the assumption of having a feasibility model may be too restrictive in general, we show that deriving one and utilizing action mapping can substantially improve learning performance. Therefore, we advocate for exploring the utilization of feasibility models in practical applications where model-free RL's performance is insufficient."}]}