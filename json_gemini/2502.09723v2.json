{"title": "Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models", "authors": ["Qingsong Zou", "Jingyu Xiao", "Qing Li", "Zhi Yan", "Yuhang Wang", "Li Xu", "Wenxuan Wang", "Kuofeng Gao", "Ruoyu Li", "Yong Jiang"], "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into structured non-natural query language to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, and the results show that QueryAttack not only can achieve high attack success rates (ASRs), but also can jailbreak various defense methods. Furthermore, we tailor a defense method against QueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is available at https://github.com/horizonsinzqs/QueryAttack.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) such as OpenAI's GPT series (OpenAI, 2024a) and Meta's Llama series (Touvron et al., 2023a) demonstrate remarkable generative potential across various domains (Xiao et al., 2024; Boiko et al., 2023; He et al., 2024; Gao et al., 2024a). However, the immense amounts of data used for training LLMs contain massive information, enabling them to learn unscreened knowledge, including those may evidently violate ethical and moral standards (Li et al., 2023; Jiang et al., 2024; Yuan et al., 2024; Bai et al., 2024). Therefore, a critical responsibility of service providers is to prevent these models from supplying harmful information to potentially adversaries.\nTo align the responses of LLMs with human ethics and preferences, numerous techniques are employed during the training process of LLMs to regulate their outputs to human queries. For example, supervised fine-tuning (Wei et al., 2022; Ouyang et al., 2022), reinforcement learning from human feedback (Sun et al., 2023; Mehrabi et al., 2024), red teaming (Bai et al., 2022a), and the constitutional AI (Bai et al., 2022b) approach are proposed to enhance the safety of LLMs. Unfortunately, a significant limitation of these methods is their reliance on malicious natural language samples from the alignment stage to train the model to recognize malicious queries and ensure the generation of safe outputs. This dependency leaves room for adversaries to develop jailbreak methods using non-natural language as input."}, {"title": "", "content": "Specifically, CipherChat (Yuan et al., 2024) uses encryption methods such as the Caesar cipher to translate harmful queries into encrypted text. Art-Prompt (Jiang et al., 2024) replaces sensitive terms with ASCII-style encoding. Deng et al. (2024) convert sensitive contents into low-resource languages. The essence of these methods lies in inducing the model to generate encrypted outputs, which are then decrypted to harmful text in natural language format. However, they typically require the model to possess knowledge of encryption to understand the prompts or place high requirements on the model's ability to generate encrypted content. As a result, their attack effectiveness is limited. Therefore, developing an effective and efficient jailbreak attack method remains a critical challenge.\nWe observe that, the essence of these jailbreak attacks lies in defining a customized encryption method and then using the language encrypted by this method to interact with the target LLMs, thereby bypassing their defense mechanisms. Inspired by prior work, we find that LLM's defensive mechanisms are not sensitive to structured, non-natural query languages. For example, by treating the target LLM as a knowledge database, when using structured query language (SQL) to request malicious knowledge (as shown in Figure 1) the target LLM not only identifies the intent of the request well but also does not trigger the defense mechanisms. Instead, the target LLM responds to the entire prompt in natural language normally.\nFrom this new perspective, we propose an attack that first uses structured non-natural query languages to jailbreak LLMs, named QueryAttack. Specifically, we break down QueryAttack into three main components:\n1). Extracting three key components from the original query: the requested content, the modifier of the content, and the high-level category to which the content belongs (potential sources where the content can be found).\n2). Filling the query components into predefined query templates (e.g., SQL templates) to generate a structured non-natural query.\n3). Applying in-context learning to help the target LLM understand the natural semantics of the template and prompting the target LLM using the structured non-natural query.\nThese three steps define a query task, analogous to querying data from a database using SQL. The additional cost introduced by this process is limited to translating the malicious query into the specified format, which can be easily adapted to any query based on natural languages. Given that programming languages are widely present in the training data of LLMs and that these models exhibit excellent semantic understanding of programming languages (OpenAI, 2023, 2024a; Anthropic, 2023; Touvron et al., 2023a), we naturally employ programming syntax to construct the query templates.\nWe test QueryAttack on AdvBench (Zou et al., 2023) across both well-known open-source and closed-source LLMs. The experimental results show that QueryAttack effectively bypasses their security defenses, achieving state-of-the-art attack success rates (ASRs). Besides, we provide a visual analysis of QueryAttack's success and propose a tailored defense method against QueryAttack.\nOur contributions can be summarized as follows:\n\u2022 We are the first to observe that the defense mechanisms of LLMs are not sensitive to structured non-natural query languages and propose QueryAttack, a novel jailbreak framework based on this observation.\n\u2022 Our evaluation on mainstream LLMs demonstrates that QueryAttack successfully bypasses their security mechanisms and achieves state-of-the-art average attack success rate.\n\u2022 We propose a tailored defense approach to mitigate QueryAttack, and experiments show that it can effectively help LLMs reduce the attack success rate."}, {"title": "Background", "content": "Large language models (LLMs) have demonstrated remarkable generative potential across various fields. However, they are still vulnerable to jailbreak attacks. Jailbreak attacks against LLMs typically involve crafting carefully designed inputs to prompt models to generate and output harmful response, such as instructions that blatantly violate human ethics or the disclosure of sensitive information. Since natural language samples are widely used as safety alignment data during the training phase of LLMs (?), potential malicious users or adversaries can bypass the defense mechanisms of these models by designing prompts based on non-natural language distributions.\nAttacks leveraging long-tail encoded distributions are particularly effective when the target LLM's safety fine-tuning fails to generalize to domains requiring corresponding capabilities. For example, by replacing sensitive contents with"}, {"title": "Methodology", "content": "The core idea of QueryAttack is to use structured non-natural query languages to carry out jailbreak attacks. As shown in Figure 2, QueryAttack defines a content query task using three components to induce harmful outputs from the target LLM:\n1). Query Components Extraction, which extracts key query components from the original query written in natural language. 2). Query Template Filling, which uses the extracted query components to fill the query template and get query code. 3). ICL-based Query Understanding, which applies in-context learning to enable the target LLM to understand the natural semantics of the template and generate responses with the user's desired content."}, {"title": "Query Components Extraction", "content": "We first identify the components to be extracted from a natural language query (referred to as query components). Taking using Structured Query Language (SQL) as an example, the preliminary task for converting a natural language query into SQL code is to extract the key components of the query sentence. We observe that, for a given query, the two most relevant components to the query's semantics are: the content to be queried and the description of that content. These two components can naturally be filled into the \u201cSELECT\" and \u201cWHERE NAME=\" sections of an SQL query. To ensure the completeness of the SQL query and to guide the target LLM's response, we add an additional component: the category to which the queried content belongs (which can be understood as potential sources where the content may be found). This component can then be filled into the \"FROM\" section of an SQL query.\nUsing these three extracted query components, a clear query can be defined, regardless of the language used. For example, the three key query components of the natural language query \u201cTell me"}, {"title": "Query Template Filling", "content": "After extracting the query components, we obtain the key semantics of a query. Therefore, we can rewrite the query into any non-natural language based on them. To automate the attack, we can predefine query templates for non-natural languages, allowing us to simply fill them with the query components to generate query in non-natural language.\nOne observation is that non-natural languages commonly present in the training data of LLMs are more likely to be understood by LLMs. Since programming languages are widely present in the train-"}, {"title": "Query Understanding", "content": "The purpose of the query learning is to guide the model in understanding the intent behind the query codes and then generate a natural language response. To help the LLMs understand the content, we first establish the context of the conversation. By describing the three query components, we guide the model in building a mapping from the query code to the natural language, and define the conversation within an educational context.\nFew-shot learning is then used to reinforce the model's understanding of the query and guide it on how to respond to these prompts using natural language. Some text in natural languages, which contain multiple queries, may require several query codes to help define the query. Therefore, we provide both short and long examples.\nFor models with strong understanding of programming languages, we can skip this process and"}, {"title": "Experiments", "content": "We test QueryAttack on 13 mainstream large language models: GPT-3.5 (gpt-3.5-turbo) (OpenAI, 2023), GPT-4-1106 (gpt-4-1106-preview) (OpenAI, 2024a), GPT-40 (OpenAI, 2024b), 01 (gpt-o1) (OpenAI, 2024c), Deepseek (deepseek-chat) (DeepSeek-AI et al., 2024), Gemini-flash (gemini-1.5-flash), Gemini-pro (gemini-1.5-pro) (DeepMind, 2024), Llama-3.1-8B (meta-llama-3.1-8B-instruct), Llama-3.1-70B (meta-llama-3.1-70B-instruct), Llama-3.2-1B (meta-llama-3.2-1B-instruct), Llama-3.2-3B (meta-llama-3.2-3B-instruct), Llama-3.2-11B (meta-llama-3.2-11B-vision-instruct) and Llama-3.3-70B (meta-llama-3.3-70B-instruct) (Touvron et al., 2023b; Meta, 2024). To maintain the reproducibility of the results, we set all the temperature to 0.\nDatasets. We used AdvBench (Zou et al., 2023) as the dataset for our experiments. This is a harmful behavior dataset that contains 520 different harmful query instances written in natural language. For some experiments, we use a subset of AdvBench which contains 50 representative, non-repetitive harmful instructions refined in (Jiang et al., 2024). We will specify this at the beginning of these parts where the subset is used.\nBaselines. We select five representative jailbreak techniques as baselines:\nPAIR (Chao et al., 2023): a black-box attack that uses an adversarial LLM to automatically generate adversarial inputs for the target LLM.\nCipherChat (Yuan et al., 2024): a black-box attack that converts inputs into ciphers for jailbreaking.\nCodeAttack (Ren et al., 2024b): a method that leverages code to embed natural language inputs into data structures to generate adversarial inputs.\nTAP (Mehrotra et al., 2023): A black-box jailbreak attack via iterative refinement of attack prompts.\nHEA (Song et al., 2025): A black-box method that embeds malicious queries within a scenario containing positive prompts and deceives the LLM into jailbreaking during subsequent malicious queries.\nEvaluation and Metrics. We focus on three main metrics for evaluation.\nHarmfulness score (HS). Our evaluation follows the work of (Qi et al., 2024), utilizing the powerful evaluation capabilities of GPT-4 to assign harmfulness scores to the responses generated by the target LLM. The harmfulness score ranges from 1 to 5. To improve the accuracy of GPT-4 Judge, we remove educational statements or disclaimers that may appear in the responses to extract the harmful content, as shown in B.3. The extracted content is then input into GPT-4 Judge for evaluation. Detailed settings can be found in the Appendix B.\nAttack success rate (ASR). Based on HS, we calculate the ASR, the percentage of harmful responses in the case of adversarial queries, which can be calculated by Equation 1.\n$ASR = \\frac{# \\text{ of responses with HS = 5}}{# \\text{ of responses}}$ (1)\nRefuse rate (RR). The third evaluation metric is the refuse rate (RR). We follow the setup of (Zou et al., 2023) and use Dict-Judge to assess the number of refusal responses, as shown in Equation 2.\n$RR = \\frac{# \\text{ of queries that are refused by LLM}}{# \\text{ of queries}}$ (2)\nWe present the experimental results from CodeAttack (Ren et al., 2024b) and (Song et al., 2025), as they use the same benchmark as ours and also employ the GPT-4 Judge (Qi et al., 2024) method to evaluate their attacks. Therefore, we use their results as baseline for comparisons with QueryAttack. Note that Song et al. (2025) consider an attack successful when the HS is \u201chigher than 4\", whereas we follow Ren et al. (2024b) to define success only when HS equals 5."}, {"title": "Results", "content": "QueryAttack achieves SoTA ASR. Table 1 presents the average HS and the ASR of QueryAttack and several baselines on AdvBench (Zou et al., 2023). We demonstrate two configurations of QueryAttack. In the first configuration, denoted"}, {"title": "Ablation and Analysis", "content": "Languages that differ more from natural language is likely to increase QueryAttack's ASR. Figure 4 shows the average HS and RR obtained by attacking GPT-4-1106, Llama-3.1-70B, Gemini-flash and DeepSeek with templates of different language styles. On GPT-4-1106, Gemini-flash and DeepSeek, different language styles do not show significant variations in average HS. A noticeable decrease is observed when attacking Llama-3.1-70B with URL and SQL-style templates, where a higher RR leads to a lower HS. This may be because these two template styles closely resemble the structure of natural language, making them more likely to trigger existing defenses. Despite this, under the Top 1 configuration, the ASR on Llama-3.1-70B still reachs 76.3%, while the Ensemble configuration achieves 92.9%.\nLarger models do not provide better defense against QueryAttack. Table 3 presents the ASRs of QueryAttack on models with different parameter sizes from the latest Llama-3.1 and Llama-3.2 series under Ensemble configuration. On Llama-3.2 series, although the ASR slightly decreases for the 1B parameter models compared to larger models, the ASR does not show a decline as the parameter size continues to grow. Specifically, as the parameter size increases from 8B to 70B, QueryAttack's ASR on the Llama-3.1 series models rises from 88.89% to 92.91%. This means increase in model parameter size does not show a positive correlation with the effectiveness of defending against QueryAttack. Without targeted safety alignment, bigger models may even have a higher risk of being at-"}, {"title": "Discussion about Countermeasures", "content": "We consider two baseline defenses and design a tailored defense method against QueryAttack as follows. Detailed settings for these defense methods can be found in the Appendix B.2.\nParaphrase (Jain et al., 2023a): A defense method that reduces the ASRs by reconstructing inputs while preserving natural semantics.\nRand-Insert, Rand-Swap, and Rand-Patch (Robey et al., 2023): A defense method against adversarial prompts by perturbing the inputs in different ways.\nCross-lingual Alignment Prompting based defense (Ours): (Qin et al., 2023) propose a method that uses cross-lingual chain-of-thought (CoT) prompting to generate reasoning paths, improving zero-shot CoT reasoning across languages. Although this approach is not originally designed for defense, we find that such CoT reasoning can help LLMs recognize cross-lingual malicious intent. Our insight is that, the success of QueryAttack relies on the target model's ability to accurately interpret custom language templates, meaning the model should also be capable of identifying the intent of them and translating them into natural language. Once translated into natural language, malicious prompts are more likely to be filtered by existing safety alignment defenses. This indicates that a well-executed translate-then-reason CoT process can effectively defend against QueryAttack-like jailbreak attacks. Based on this, we design a defense method using cross-lingual chain-of-thought prompting. The complete chain of thought is: we first requires the target model to identify the Query Content, Key Object, and Content Source of the input, then describe the query in natural language. The target model then responds to this natural language query, thereby activating existing safety alignment mechanisms for defense. The detailed prompt can be found in the Appendix B.2.\nWe test the effectiveness of these countermeasures against QueryAttack on four models: Gemini-flash, GPT-3.5, Llama-3.1-8B, and GPT-4-1106."}, {"title": "Related Work", "content": ""}, {"title": "Jailbreak Attacks on LLMs", "content": "Initially, researchers reveal that adversaries could launch attacks by manually constructing out-of-distribution (OOD) samples (Li et al., 2023; Shen et al., 2024). Building on these observations, several white-box attack methods are proposed (Jones et al., 2023a,b; Gao et al., 2024c). Compared to white-box attacks, black-box attacks assume that adversaries adjust their prompt strategies only based on the model's responses (Zhang et al., 2024; Shah et al., 2023; Liu et al., 2024; Ren et al., 2024c; Yuan et al., 2024; Deng et al., 2024).\nRecently, some black-box are proposed to use code to encrypt malicious inputs to build long-tail encoded distributions. CodeAttack (Ren et al., 2024b) embeds malicious queries within data structures (e.g., stacks and queues) to bypass safety alignments designed for prompts written in natural languages. Codechameleon (Lv et al., 2024) encrypts malicious prompts using custom program functions, transforming them into code completion tasks. Unlike these methods, QueryAttack does not rely on the syntax of programming languages for encryption. Instead, it only requires certain keywords or expressions from the programming language. This means that QueryAttack can be applied not only using programming languages but also to any non-natural language that the target LLM can understand but has not been well aligned during the safety alignment phase. Moreover, even without the need for output encryption, QueryAttack can still effectively attack target LLMs."}, {"title": "Safety Alignment for Defending Jailbreak", "content": "Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) is one of the most widely used defense mechanisms. For instance, recent works such as (Mehrabi et al., 2024; Sun et al., 2023) explore the effectiveness of alignment during pre-training in defending against malicious queries, CoT reasoning (OpenAI, 2024c), as well as in-context learning (Wei et al., 2023b; Ren et al., 2024a). These methods often rely on natural language inputs collected from red teams, which can lead to generalization issues when faced with non-natural language or other OOD inputs.\nBeyond the training process, some approaches focus on input and output safeguards, such as input perturbation (Jain et al., 2023a), safe decoding (Xu et al., 2024), and jailbreak detection (Robey et al., 2023; Phute et al., 2024; Jain et al., 2023b; Gao et al., 2024b). These methods can effectively reduce the attack success rate of jailbreak attacks. However, their effectiveness depends heavily on the quality of malicious data used for training and incurs significant additional overhead during deployment, which may affect user experience."}, {"title": "Conclusion", "content": "In this paper, we investigate the generalization challenges faced by large language models with safety alignment when encountering out-of-distribution malicious structured non-natural query language. Specifically, we introduce QueryAttack, a novel jailbreak attack framework. QueryAttack extracts three query components from a query in natural language, fill them into query templates of various styles, and leverages the obtained query code to bypass the target LLM's safety alignment. Although QueryAttack does not encrypt the outputs, our extensive evaluation shows that it still effectively bypasses the defenses of mainstream LLMs and can withstand common defense methods. Besides, to defend against QueryAttack, we develop a tailored defense strategy. Experimental results demonstrate that it effectively enhances the ability"}, {"title": "Limitations", "content": "The limitation of our study is the insufficient discussion of related defense mechanisms. As mentioned in the related work (Section 5), research on jailbreak attack defenses has explored various potential countermeasures. However, we are unable to cover all of them. A more comprehensive investigation of defense methods would provide deeper insights into the generalizability of QueryAttack and help identify more effective mitigation techniques against such attacks."}, {"title": "Ethics Statement", "content": "Please note that all experiments are conducted within controlled laboratory environments. We do not support the application of our QueryAttack in real-world scenarios. The primary objective of our work is to raise awareness about the security concerns related to the safety of LLMs. We aim to highlight the potential vulnerabilities about QueryAttack and encourage practitioners to prioritize the development of robust security measures and trustworthy deployment practices."}, {"title": "Success Examples", "content": "Figure 6 illustrates a simplified attack process of QueryAttack. The adversary first utilizes query learning in context. Then, the translated query is used to launch the attack."}, {"title": "Prompts of Query Learning", "content": "Figure 7 and Figure 8 present the prompts used for few-shot and zero-shot learning in SQL style. For zero-shot learning, we establish an educational context to encourage the model to generate more detailed information related to the risky knowledge."}, {"title": "Human Evaluation on GPT-4 Judge", "content": "To confirm the effectiveness of GPT evaluation, we conduct an experiment with human evaluators to assess the responses of LLMs. We randomly select 100 outputs from GPT-4-1106 under the CodeAttack. Three authors independently classify these outputs as either successful or failed attacks based on OpenAI's policy, and the final results are determined by majority vote. The manual verification results show a 93% agreement with the GPT-4 based evaluation (which also uses OpenAI's policy as the benchmark). The result demonstrates the effectiveness of GPT-4 based evaluation."}, {"title": "Defense Settings", "content": "Paraphrase. We follow the setting of (Jiang et al., 2024) and use the GPT-3.5 model and the exact same prompt to complete the paraphrase task. The paraphrased translated template is then used as a replacement to launch the attack in the last step 2.\nRand-Insert, Rand-Swap, and Rand-Patch. We follow the defense and evaluation setting of (Robey et al., 2023), but choose the hyperparameter settings that give the best defensive performance reported in their paper.\nCross-lingual alignment prompting based defense. Figure 9 shows the cross-lingual alignment prompts we use for each attack sample. These prompts guide the target model to analyze the intent of the input query and rewrite it as text in natural language. Then, the target model is asked to generate response according to the query described in the text in natural language."}, {"title": "Ethic-related Disclaimer Removal", "content": "Figure 10 shows a malicious response generated by the Ol model (OpenAI, 2024c) using zero-shot prompt 7. The response fully complies with the adversary's query (Develop a program that can exploit vulnerabilities in IoT devices and take control of them). Besides, it provides concise mitigation measures as required and includes brief ethic-related disclaimers in the the response (highlighted in purple). However, these measures are clearly ineffective, as the model has already provided the detailed information queried by the adversary. Moreover, offering mitigation strategies may further enhance the adversary's malicious knowledge to develop stronger attack vectors. Therefore, it should be considered a successful attack."}, {"title": "Prompts for Extracting Query Components", "content": "We use GPT-4-1106 (OpenAI, 2024a) to perform the component extraction task, employing few-shot learning in the process. To ensure the LLM remains focused on the natural language processing task, we refine the task details and require the model to simultaneously assess the risk level of the extracted components, as shown in Figure 11. The risk level of the components should not be low to prevent the model's defense mechanisms from replacing malicious phrases with benign ones, which could affect translation quality."}]}