{"title": "Spontaneous Reward Hacking in Iterative Self-Refinement", "authors": ["Jane Pan", "He He", "Samuel R. Bowman", "Shi Feng"], "abstract": "Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of real humans, a second language model can be used as the evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator's ratings improve while the generation quality remains stagnant or even decreases as judged by actual human users. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive the model to exploit vulnerabilities that occur in both roles. Using an essay editing task, we show that iterative self-refinement leads to reward hacking where deviation between the language model evaluator and human judgment occurs spontaneously in-context. In addition, we study conditions under which reward hacking occurs and observe two factors that affect its severity: model size and context sharing between the generator and the evaluator.", "sections": [{"title": "Introduction", "content": "The ability of frontier language models (LMs) to accurately approximate humans on a wide range of tasks (Brown et al.; Chiang and Lee, 2023) has enabled artificial intelligence (AI)systems to use LMs as human proxies for both training (Stiennon et al., 2020; Lee et al., 2023) and deployment (Bai et al., 2022). A key element shared by these methods is the use of LMs to approximate human preferences in the evaluation, critique, and refinement of LM generations, leading to improved generation quality and safety without additional human intervention (Saunders et al., 2022; Askell et al., 2021).\nOne prominent example of these methods is self-refinement, which uses two LMs: a generator and an evaluator, which can be the same underlying LM with two different prompts. Given an output from the generator, the evaluator provides feedback according to human-written criteria, and the generator improves its output based on the feedback. This refinement process may be repeated multiple times, with the output from previous iterations becoming the input of the next. On tasks like coding, iterative self-refinement significantly improves generation quality over using the generator alone (Chen et al., 2023; Zhou et al., 2023).\nIn iterative self-refinement, the generator updates its outputs to optimize for user preference using the evaluator as a proxy, but since the evaluator is an imperfect approximation of user preference, this process can lead to reward hacking (Skalse et al., 2022), where the evaluator diverges from the ground-truth user preference under optimization pressure. Traditionally, reward hacking is observed when gradient descent is used to optimize the model's parameters, e.g. in reinforcement learn-"}, {"title": "Reward Hacking in Self-Refinement", "content": "In this section, we introduce our implementation of iterative self-refinement and how reward hacking is defined in this context.\nOur setting involves two LMs: a generator and an evaluator. In all experiments, the generator and the evaluator are the same LM with two different prompts. Given outputs from the generator, the evaluator evaluates them and provides feedback; the generator then takes the feedback as input in the same context window and refines the output accordingly. The refined output is provided to the evaluator, allowing the process to repeat.\nIn addition to natural language feedback to help steer the generator's next output, the evaluator also produces a numerical score indicating the overall quality of the generated text. We treat this score as an approximation of the underlying true reward function as defined by human annotations. As self-refinement is iteratively executed, we track the scores produced by the reward model and collect human annotations offline; a significant divergence between the two scores constitutes reward hacking.\nWe ensure that the same reward specification is provided to the models and the human annotators, ensuring the essays are optimized and evaluated using the same criteria. We define reward hacking as the deviation between the scores from the human annotators and the evaluator, i.e. the ground-truth reward function and its proxy respectively, following the canonical definition of reward hacking (Amodei et al., 2016; Skalse et al., 2022) Unlike previous work (Pan et al., 2024), our definition only involves examining one reward function and its proxy, rather than the emergence of negative side effects."}, {"title": "Experiments", "content": null}, {"title": "Task Setup", "content": "We consider the task of essay editing, in which an essay receives feedback and is improved over multiple iterations of rewriting. We select this task as it mimics the real-world use case of AI systems in hiring and school admission (Lira et al., 2023; Hannan and Liu, 2023). The subjective nature of essay scoring leaves space for continuous improvement from iterative editing, making it a suitable objective to optimize with iterative self-refinement.\nEssay editing involves two roles: a judge (evaluator), who provides feedback about the essay, and an author (generator), who uses the feedback to guide their editing of the essay. The criteria for essay quality are based on a pre-defined rubric, which is provided to both the judge and the author. Based on the rubric, the feedback provided by the judge includes written evaluations of strengths or weaknesses, suggestions for improvements, and numerical scores. Both the judge and the author are LLMs in our experiments. Independent of the author-judge editing process, we collect human annotations as the oracle scores for the essay quality.\nWe initialize the editing process with human-written seed essays and rubrics. Both the judge and the author are given the rubric; however, they are not informed that the other party is an LLM."}, {"title": "Seed Essay Dataset", "content": "We use a publicly available dataset of personal college application essays as seed essays (Evans, 2020). The scoring is guided by a human-written rubric but remains highly subjective to individual preferences, which grants space for rich feedback and continuous improvement by the model. We choose this particular type of essay because they are longer and of higher quality compared to other student essay corpora. Our use of human-written rubrics and essays\u00b9 ensures a grounded task.\nWe manually filter the dataset and remove essays or sections of essays that were not based on a general personal topic (e.g. sections pertaining to specific colleges). Finally, we ensure that the essays did not contain any information that could be used to uniquely identify the writer."}, {"title": "Judging Protocol", "content": "In order to guide the author and the judge, we design a 4-item rubric \u2013 Conventions (grammar and punctuation), Depth (idea development and uniqueness), Details (vividness and descriptiveness), and Style (writer's voice). The rubric provides general descriptions of each item, as well as sample essays with sample scores. We also provide two sample essays-one taken from a well-known writing guide and one manually checked for poorer quality- with accompanying scores in order to calibrate the models' scoring criteria. Both LLM judges and human annotators are explicitly asked to grade the essay according to each rubric on a scale of 1-10; they then give the essay an Overall score from 1-10. The LLM judges are also asked to provide natural language feedback and suggestions. The full rubric is provided in Appendix A.3."}, {"title": "Essay Writing via Iterative Refinement", "content": "We use gpt-3.5-turbo-1106 and gpt-4 via the OpenAI API in our experiments with a sampling temperature of 0.7. For brevity, we refer to the former as gpt-3.5. We note that the same model plays both the author and the judge; we do not consider a setting where the author and judge are different models. Both the author and the judge are provided with the rubric. All LLM prompts can be found in Appendix A.2.\nThe author and the judge are two separate LLM instances with different system messages, and they engage in a dialogue with the following structure:\n1. Initialization: The current essay is initialized to the human-written seed essay.\n2. Refinement Loop\n(a) Judge Evaluation Step: The judge provides written feedback and scores for the current essay.\n(b) Author Editing Step: The author uses the written feedback and scores along with the current essay to produce a new essay, which becomes the current essay in the next iteration.\nIn all our experiments, we execute the iterative refinement loop for five steps, creating a essay trajectory of length six (including the human-written seed essay). In addition to the generated essays and feedback, the prompts include direct instructions for the LLM to either provide feedback or edit the"}, {"title": "Human Annotations", "content": "We recruited a team of 23 annotators from Upwork and instructed them to provide ratings on all essays following the same rubric. Annotators are asked to grade each item of the rubric as well as give an overall score. In order to procure high-quality annotations, we ensured that all annotators were native English speakers and had an academic background in humanities (i.e. a university degree in a humanities-related field) or were currently working as a humanities teacher."}, {"title": "Offline LLM Judges", "content": "In order to disentangle the effects of iterative refinement from the use of an LLM judge, we also use an LLM judge to provide offline feedback about the essays. This offline judge is identical to the online judge, except it does not have access to the past dialogue and does not influence the generation of future essays. Thus, it may be thought of as a direct analogy to the human scoring process."}, {"title": "Results", "content": "Figure 3 shows our main results using GPT-3.5 and GPT-4; the essay scores at each iteration are plotted under three settings: ONLINE LLM JUDGE (red), OFFLINE LLM JUDGE (yellow), and HUMAN (blue). The x-axis tracks the number of refinement iterations; x = 0 shows the scores on the original, unedited seed essays.\nIn-context reward hacking in GPT-3.5. It is clear from the GPT-3.5 results in Figure 1 that in"}, {"title": "In-context reward hacking in GPT-4", "content": "As shown in Figure 3, in-context reward hacking manifests to a lesser extent with GPT-4 than GPT-3.5. Although the model-generated scores still appear inflated compared to HUMAN Scores, they do not demonstrate opposite trends as with GPT-3.5. While the HUMAN scores lag below the ONLINE LLM JUDGE scores for the intermediate essays, the later scores demonstrate an upward trend, suggesting higher agreement between the HUMAN and ONLINE LLM JUDGE scores. This may indicate that stronger models may be less susceptible to in-context reward hacking than weaker ones. We leave further investigation of the effect of scale on in-context reward hacking and score calibration to future work."}, {"title": "Effects of rubric items.", "content": "Figures 5 and 7 show the HUMAN and ONLINE LLM JUDGE scores for each individual rubric item. For GPT-3.5, it is clear that the HUMAN scores only strongly agree with the ONLINE LLM JUDGE for the Conventions rubric item; it is the only rubric item for which both HUMAN and ONLINE LLM JUDGE scores continue to increase over the progression of essays. However, for all other rubric items, the HUMAN-judged improvement either plateaus after a single iteration or even degrades in the last few iterations. We hypothesize that this misalignment arises because the judge and the author exploit the same shortcuts or spurious correlations as they are based on the same model. Therefore, the author and the judge are likely to make similarly incorrect judgments on essay quality. This shared lack of robustness drives the edits to worsen the quality, while the judge scores it as higher quality. Thus, this implicit optimization pressure drives the models towards a shared adversarial example, which both the author and judge incorrectly evaluate."}, {"title": "Related Work", "content": "In-context reward hacking from iterative refinement is studied by concurrent work Pan et al. (2024) and Xu et al. (2024). However, our work is the first to observe the divergence between the reward model and human annotation on the primary ob-"}, {"title": "Conclusion", "content": "In this paper, we demonstrate that reward hacking can occur in a single context window of an LM without any gradient updates. Using an essay writing task, we show that iterative self-refinement drives the evaluator scores to inflate while human annotator scores decrease. We identify situations where human annotation shows a clear degradation of generation quality, but also observe counter-intuitive trends in the severity of reward hacking with respect to factors such as shared context length and model size, highlighting the complexity of the issue. These results complement existing work on the negative side-effects of self-refinement as well as biases of self-evaluation. With this work, we hope to provide a strong motivation for studying the implicit optimization pressures in LM interactions which are typically studied in the context of gradient descent model training."}, {"title": "Acknowledgements", "content": "We thank the members of ML2 and the NYU Alignment Research Group for their input throughout this project. In particular, we are grateful for the feedback and suggestions from Vishakh Padmakumar. We also thank Alijan Ozkiral for his helpful advice on annotator management and rubric construction. This project has benefited from financial support to SB by Eric and Wendy Schmidt (made by recommendation of the Schmidt Futures program) and Open Philanthropy, and from in-kind support by the NYU High-Performance Computing Center and Google Cloud. This material is based upon work supported by the National Science Foundation under Grant Nos. 1850208, 1922658, and 2046556. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."}, {"title": "Appendix", "content": null}, {"title": "Additional Figures", "content": "Figure 6 plots the GPT-4 results for the different settings of seen author/judge iterations. Figure 7 plots the GPT-4 results for the rubric item scores."}, {"title": "Prompts", "content": "Figure 8 shows the prompt format for the author; Figure 9 shows the prompt format for the judge. Blue text indicates that the relevant text (e.g. the rubric or sample essays) would be inserted at that location."}, {"title": "Rubrics", "content": "Figure 10 illustrates the rubric provided to both LLM judges and human annotators."}, {"title": "Sample Trajectories", "content": "We provide two sample trajectories, generated by GPT-3.5 and GPT-4(), respectively. Both of these essays were generated with a temperature of 0.7 and with the author and judge seeing 3 previous iterations. Blue text is used to label the judge and author responses and is not a part of the actual model generation."}]}