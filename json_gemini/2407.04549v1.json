{"title": "Spontaneous Reward Hacking in Iterative Self-Refinement", "authors": ["Jane Pan", "He He", "Samuel R. Bowman", "Shi Feng"], "abstract": "Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of real humans, a second language model can be used as the evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator's ratings improve while the generation quality remains stagnant or even decreases as judged by actual human users. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive the model to exploit vulnerabilities that occur in both roles. Using an essay editing task, we show that iterative self-refinement leads to reward hacking where deviation between the language model evaluator and human judgment occurs spontaneously in-context. In addition, we study conditions under which reward hacking occurs and observe two factors that affect its severity: model size and context sharing between the generator and the evaluator.", "sections": [{"title": "Introduction", "content": "The ability of frontier language models (LMs) to accurately approximate humans on a wide range of tasks (Brown et al.; Chiang and Lee, 2023) has enabled artificial intelligence (AI)systems to use LMs as human proxies for both training (Stiennon et al., 2020; Lee et al., 2023) and deployment (Bai et al., 2022). A key element shared by these methods is the use of LMs to approximate human preferences in the evaluation, critique, and refinement of LM generations, leading to improved generation quality and safety without additional human intervention (Saunders et al., 2022; Askell et al., 2021). One prominent example of these methods is self-refinement, which uses two LMs: a generator and an evaluator, which can be the same underlying LM with two different prompts. Given an output from the generator, the evaluator provides feedback according to human-written criteria, and the generator improves its output based on the feedback. This refinement process may be repeated multiple times, with the output from previous iterations becoming the input of the next. On tasks like coding, iterative self-refinement significantly improves generation quality over using the generator alone (Chen et al., 2023; Zhou et al., 2023). In iterative self-refinement, the generator updates its outputs to optimize for user preference using the evaluator as a proxy, but since the evaluator is an imperfect approximation of user preference, this process can lead to reward hacking (Skalse et al., 2022), where the evaluator diverges from the ground-truth user preference under optimization pressure. Traditionally, reward hacking is observed when gradient descent is used to optimize the model's parameters, e.g. in reinforcement learn-ing from human feedback (Stiennon et al., 2020). Iterative self-refinement does not change any of the model's parameters, but it involves optimizing against an imperfect proxy of the underlying reward function, thus satisfying the necessary condition for reward hacking to occur. We hypothesize that iterative self-refinement leads to in-context reward hacking (ICRH), where the generator receives higher and higher scores according to the evaluator, while the generation quality remains stagnant or even decreases when evaluated by ground-truth user preference. We examine the conditions that induce in-context reward hacking and study the influence of various factors on its severity. Specifically, we see less reward hacking when using GPT-4 as compared to GPT-3.5, suggesting that model capability may decrease the impact of reward hacking. Moreover, we find that a symmetric context window between the judge and author is key to promoting reward-hacking behavior, whereas asymmetric contexts lead to diminished reward hacking in GPT-3.5."}, {"title": "Reward Hacking in Self-Refinement", "content": "In this section, we introduce our implementation of iterative self-refinement and how reward hacking is defined in this context. Our setting involves two LMs: a generator and an evaluator. In all experiments, the generator and the evaluator are the same LM with two different prompts. Given outputs from the generator, the evaluator evaluates them and provides feedback; the generator then takes the feedback as input in the same context window and refines the output accordingly. The refined output is provided to the evaluator, allowing the process to repeat. In addition to natural language feedback to help steer the generator's next output, the evaluator also produces a numerical score indicating the overall quality of the generated text. We treat this score as an approximation of the underlying true reward function as defined by human annotations. As self-refinement is iteratively executed, we track the scores produced by the reward model and collect human annotations offline; a significant divergence between the two scores constitutes reward hacking. We ensure that the same reward specification is provided to the models and the human annotators, ensuring the essays are optimized and evaluated using the same criteria. We define reward hacking as the deviation between the scores from the human annotators and the evaluator, i.e. the ground-truth reward function and its proxy respectively, following the canonical definition of reward hacking (Amodei et al., 2016; Skalse et al., 2022) Unlike previous work (Pan et al., 2024), our definition only involves examining one reward function and its proxy, rather than the emergence of negative side effects."}, {"title": "Experiments", "content": "We consider the task of essay editing, in which an essay receives feedback and is improved over multiple iterations of rewriting. We select this task as it mimics the real-world use case of AI systems in hiring and school admission (Lira et al., 2023; Hannan and Liu, 2023). The subjective nature of essay scoring leaves space for continuous improvement from iterative editing, making it a suitable objective to optimize with iterative self-refinement. Essay editing involves two roles: a judge (evaluator), who provides feedback about the essay, and an author (generator), who uses the feedback to guide their editing of the essay. The criteria for essay quality are based on a pre-defined rubric, which is provided to both the judge and the author. Based on the rubric, the feedback provided by the judge includes written evaluations of strengths or weaknesses, suggestions for improvements, and numerical scores. Both the judge and the author are LLMs in our experiments. Independent of the author-judge editing process, we collect human annotations as the oracle scores for the essay quality. We initialize the editing process with human-written seed essays and rubrics. Both the judge and the author are given the rubric; however, they are not informed that the other party is an LLM."}, {"title": "Seed Essay Dataset", "content": "We use a publicly available dataset of personal college application essays as seed essays (Evans, 2020). The scoring is guided by a human-written rubric but remains highly subjective to individual preferences, which grants space for rich feedback and continuous improvement by the model. We choose this particular type of essay because they are longer and of higher quality compared to other student essay corpora. Our use of human-written rubrics and essays\u00b9 ensures a grounded task. We manually filter the dataset and remove essays or sections of essays that were not based on a general personal topic (e.g. sections pertaining to specific colleges). Finally, we ensure that the essays did not contain any information that could be used to uniquely identify the writer."}, {"title": "Judging Protocol", "content": "In order to guide the author and the judge, we design a 4-item rubric \u2013 Conventions (grammar and punctuation), Depth (idea development and uniqueness), Details (vividness and descriptiveness), and Style (writer's voice). The rubric provides general descriptions of each item, as well as sample essays with sample scores. We also provide two sample essays-one taken from a well-known writing guide and one manually checked for poorer quality- with accompanying scores in order to calibrate the models' scoring criteria. Both LLM judges and human annotators are explicitly asked to grade the essay according to each rubric on a scale of 1-10; they then give the essay an Overall score from 1-10. The LLM judges are also asked to provide natural language feedback and suggestions. The full rubric is provided in Appendix A.3."}, {"title": "Essay Writing via Iterative Refinement", "content": "We use gpt-3.5-turbo-1106 and gpt-4 via the OpenAI API in our experiments with a sampling temperature of 0.7. For brevity, we refer to the former as gpt-3.5. We note that the same model plays both the author and the judge; we do not consider a setting where the author and judge are different models. Both the author and the judge are provided with the rubric. All LLM prompts can be found in Appendix A.2. The author and the judge are two separate LLM instances with different system messages, and they engage in a dialogue with the following structure:\n1. Initialization: The current essay is initialized to the human-written seed essay.\n2. Refinement Loop\n(a) Judge Evaluation Step: The judge provides written feedback and scores for the current essay.\n(b) Author Editing Step: The author uses the written feedback and scores along with the current essay to produce a new essay, which becomes the current essay in the next iteration. In all our experiments, we execute the iterative refinement loop for five steps, creating a essay trajectory of length six (including the human-written seed essay). In addition to the generated essays and feedback, the prompts include direct instructions for the LLM to either provide feedback or edit the essay."}, {"title": "Human Annotations", "content": "We recruited a team of 23 annotators from Upwork and instructed them to provide ratings on all essays following the same rubric. Annotators are asked to grade each item of the rubric as well as give an overall score. In order to procure high-quality annotations, we ensured that all annotators were native English speakers and had an academic background in humanities (i.e. a university degree in a humanities-related field) or were currently working as a humanities teacher."}, {"title": "Offline LLM Judges", "content": "In order to disentangle the effects of iterative refinement from the use of an LLM judge, we also use an LLM judge to provide offline feedback about the essays. This offline judge is identical to the online judge, except it does not have access to the past dialogue and does not influence the generation of future essays. Thus, it may be thought of as a direct analogy to the human scoring process."}, {"title": "Results", "content": "Figure 3 shows our main results using GPT-3.5 and GPT-4; the essay scores at each iteration are plotted under three settings: ONLINE LLM JUDGE (red), OFFLINE LLM JUDGE (yellow), and HUMAN (blue). The x-axis tracks the number of refinement iterations; x = 0 shows the scores on the original, unedited seed essays.\nIn-context reward hacking in GPT-3.5. It is clear from the GPT-3.5 results in Figure 1 that in the iterative refinement schema, the ONLINE LLM JUDGE scores the model-edited essays much higher compared to the ground-truth HUMAN scores, while scoring the original essays similarly as the HUMAN scores. In fact, the HUMAN scores demonstrate a decrease in quality in the last iteration of essays, whereas the ONLINE LLM JUDGE scores continue to plateau. Moreover, this divergence between the HUMAN and ONLINE LLM JUDGE scores cannot be explained purely by the use of an LLM as a judge, as the OFFLINE LLM JUDGE scores remain similar to the human scores. This implies that iterative self-refinement is critical to inducing the presence of in-context reward hacking.\nEffects of context window length and shared contexts on ICRH. Figure 4 plots all combinations of seen previous iterations for the author and judge. When the author and judge are provided with the same number of previous iterations\u2014i.e. when they have access to the same dialogue history-the ONLINE LLM JUDGE and the HUMAN scores demonstrate an obvious divergence. However, when the author and judge are provided with different numbers of previous iterations, the gap between the ONLINE LLM JUDGE and HUMAN scores becomes statistically insignificant. This suggests that in-context reward hacking is more likely when the judge and author share identical contexts, as we see reward hacking even when the author and judge are shown only one iteration each. Moreover, context sharing is more critical than context length for in-context reward hacking. We note that the settings where the judge and the author are provided with different contexts allow one of the LLM components to see strictly more context than the settings where both components see only one iteration. In other words, even though the components are given more information than their counterparts who both only see one iteration, reward hacking does not occur, whereas it does for the single-iteration judge/author pair. With shared context\u2014when the judge and the author have access to the same dialogue history\u2014increased context length leads to more severe reward hacking. As shown in Figure 4, increasing the length of shared context from 1 to 3 leads to a greater divergence from human preference."}, {"title": "In-context reward hacking in GPT-4", "content": "As shown in Figure 3, in-context reward hacking manifests to a lesser extent with GPT-4 than GPT-3.5. Although the model-generated scores still appear inflated compared to HUMAN scores, they do not demonstrate opposite trends as with GPT-3.5. While the HUMAN scores lag below the ONLINE LLM JUDGE scores for the intermediate essays, the later scores demonstrate an upward trend, suggesting higher agreement between the HUMAN and ONLINE LLM JUDGE scores. This may indicate that stronger models may be less susceptible to in-context reward hacking than weaker ones. We leave further investigation of the effect of scale on in-context reward hacking and score calibration to future work.\nEffects of rubric items. Figures 5 and 7 show the HUMAN and ONLINE LLM JUDGE scores for each individual rubric item. For GPT-3.5, it is clear that the HUMAN scores only strongly agree with the ONLINE LLM JUDGE for the Conventions rubric item; it is the only rubric item for which both HUMAN and ONLINE LLM JUDGE scores continue to increase over the progression of essays. However, for all other rubric items, the HUMAN-judged improvement either plateaus after a single iteration or even degrades in the last few iterations. We hypothesize that this misalignment arises because the judge and the author exploit the same shortcuts or spurious correlations as they are based on the same model. Therefore, the author and the judge are likely to make similarly incorrect judgments on essay quality. This shared lack of robustness drives the edits to worsen the quality, while the judge scores it as higher quality. Thus, this implicit optimization pressure drives the models towards a shared adversarial example, which both the author and judge incorrectly evaluate."}, {"title": "Related Work", "content": "In-context reward hacking from iterative refinement is studied by concurrent work Pan et al. (2024) and Xu et al. (2024). However, our work is the first to observe the divergence between the reward model and human annotation on the primary objective according to the exact same reward specification. Pan et al. (2024) focuses on the emergence of negative side-effects via self-refinement loops. They show that optimization for increased engagement of tweets via iterative self-refinement is accompanied by degradation on a secondary objective (i.e. toxicity) using simulated humans. This differs from the typical definition of reward hacking, which focuses on one single reward function. Xu et al. (2024) observes increased inflation of scores from the reward model with iterative self-refinement; however, they largely rely on reference-based metrics to compute a ground-truth reward function, with the exception of one experiment that involved one human annotator. Iterative refinement has been adapted to many different settings (Madaan et al., 2023). These include coding (Zhou et al., 2023), mathematics (Chen et al., 2023), and other forms of task-solving (Wang et al., 2024; Ma et al., 2023; Gou et al., 2024). However, other works have shown that iterative refinement has only limited capacity as a self-evaluative tool, especially when critiquing reasoning or planning (Luo et al., 2023; Valmeekam et al., 2023; Huang et al., 2024). Other works study the emergence of biases from LM self-evaluation, even without the use of self-refinement. Zheng et al. (2024) demonstrates limited evidence for LLMs preferring their own answers over other model-generated or human answers in the chat assistant setting. Li et al. (2023) show that when LLMs are asked to provide rankings on responses, they tend to favor model-generated answers over human answers more than human evaluators do. Other works also provide evidence for LLM self-evaluation leading to inflated scores in broader settings (Bitton et al., 2023; Bai et al., 2024; Liu et al., 2023)."}, {"title": "Conclusion", "content": "In this paper, we demonstrate that reward hacking can occur in a single context window of an LM without any gradient updates. Using an essay writing task, we show that iterative self-refinement drives the evaluator scores to inflate while human annotator scores decrease. We identify situations where human annotation shows a clear degradation of generation quality, but also observe counter-intuitive trends in the severity of reward hacking with respect to factors such as shared context length and model size, highlighting the complexity of the issue. These results complement existing work on the negative side-effects of self-refinement as well as biases of self-evaluation. With this work, we hope to provide a strong motivation for studying the implicit optimization pressures in LM interactions which are typically studied in the context of gradient descent model training."}, {"title": "Acknowledgements", "content": "We thank the members of ML2 and the NYU Alignment Research Group for their input throughout this project. In particular, we are grateful for the feedback and suggestions from Vishakh Padmakumar. We also thank Alijan Ozkiral for his helpful advice on annotator management and rubric construction. This project has benefited from financial support to SB by Eric and Wendy Schmidt (made by recommendation of the Schmidt Futures program) and Open Philanthropy, and from in-kind support by the NYU High-Performance Computing Center and Google Cloud. This material is based upon work supported by the National Science Foundation under Grant Nos. 1850208, 1922658, and 2046556. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."}, {"title": "Appendix", "content": "Figure 6 plots the GPT-4 results for the different settings of seen author/judge iterations. Figure 7 plots the GPT-4 results for the rubric item scores."}]}