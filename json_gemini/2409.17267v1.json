{"title": "MODEL AGGREGATION: MINIMIZING EMPIRICAL VARIANCE OUTPERFORMS MINIMIZING EMPIRICAL ERROR", "authors": ["TH\u00c9O BOURDAIS", "HOUMAN OWHADI"], "abstract": "Whether deterministic or stochastic, models can be viewed as functions designed to approximate a specific quantity of interest. We propose a data-driven framework that aggregates predictions from diverse models into a single, more accurate output. This aggregation approach exploits each model's strengths to enhance overall accuracy. It is non-intrusive-treating models as black-box functions-model-agnostic, requires minimal assumptions, and can combine outputs from a wide range of models, including those from machine learning and numerical solvers. We argue that the aggregation process should be point-wise linear and propose two methods to find an optimal aggregate: Minimal Error Aggregation (MEA), which minimizes the aggregate's prediction error, and Minimal Variance Aggregation (MVA), which minimizes its variance. While MEA is inherently more accurate when correlations between models and the target quantity are perfectly known, Minimal Empirical Variance Aggregation (MEVA), an empirical version of MVA-consistently outperforms Minimal Empirical Error Aggregation (MEEA), the empirical counterpart of MEA, when these correlations must be estimated from data. The key difference is that MEVA constructs an aggregate by estimating model errors, while MEEA treats the models as features for direct interpolation of the quantity of interest. This makes MEEA more susceptible to overfitting and poor generalization, where the aggregate may underperform individual models during testing. We demonstrate the versatility and effectiveness of our framework in various applications, such as data science and partial differential equations, showing how it successfully integrates traditional solvers with machine learning models to improve both robustness and accuracy.", "sections": [{"title": "1. Introduction", "content": "Many challenges in scientific computing and Machine Learning (ML) involve approximating functions using models. These models can vary significantly, ranging from numerical solvers that integrate differential equations to various ML methods and other approximation techniques. Their performance can differ widely across different benchmarks, as seen in the Imagenet leaderboards [24]. In some cases, no single model outperforms others across all scenarios; each has its own strengths and weaknesses. For instance, the Intergovernmental Panel on Climate Change (IPCC) report on model evaluation [15] highlights numerous evaluation metrics, showing that different models excel in different areas. When multiple models are available to predict the same quantity of interest, a key challenge arises: how can their predictions be combined most effectively? Ensembling ML models is a well-established practice with many successful applications, such as bagging, boosting, and Mixture of Experts (MoE) [28]. Gradient boosting, in particular, is widely regarded for its effectiveness, combining simplicity and accuracy in data analysis [7]. Most ensembling methods focus on training strategies that build and combine multiple models [28]. However, methods for aggregating predictions from existing models are less developed and often rely on simplistic techniques like averaging [13]. This highlights the need for more advanced approaches to aggregate predictions to better exploit diverse models' unique strengths.\nIn this work, we present a method for aggregating predictions from any model, with minimal assumptions about their nature, treating them as arbitrary input-output functions, whether deterministic or stochastic. Our approach broadens the scope of model aggregation beyond machine learning and data analysis, enabling the integration of diverse methods (e.g., in scientific computing) aimed at estimating the same target quantities. In Section 3, we demonstrate that effective aggregation should be point-wise linear. To achieve this, we explore training the aggregate by directly minimizing its empirical error, a method we call Minimal Empirical Error Aggregation (MEEA). However, as we illustrate through pathological examples in Sections 3.2.1 and 3.2.2, as well as in the computations presented in Section 3.3, MEEA may simply turn individual models into features for interpolating the target and thereby suffer from overfitting and poor generalization. To address these shortcomings, we propose a new approach, Minimal Empirical Variance Aggregation (MEVA), which aggregates models by estimating their errors, as detailed in Section 4. Figure 1 provides an overview of the aggregation methods discussed. We validate our approach on a data science task in Section 5.1 and two operator learning tasks in Section 5.1. In all cases, MEVA outperforms direct error minimization, resulting in an aggregate model that is more robust and effective than any individual model."}, {"title": "2. Related work", "content": "Ensemble methods combine multiple models to improve accuracy. They can be broadly categorized into fusion and selection [28]. In the fusion setting, models are trained on the entire input space, and their output is combined, often using a weighted average. Examples include Bagging [9], random forests"}, {"title": "3. The minimal error aggregation", "content": "In this section, we motivate the use of aggregation techniques with an ideal case (section 3.1), as well as the use of a specific form of aggregation in equation (2) and a minimal error aggregation loss (6). However, we will see, through two pathological examples in section 3.2 and a computation in the Gaussian Process (GP) case in section 3.3, that this direct loss (6) should not be used."}, {"title": "3.1. Best linear aggregate when model correlations are known", "content": "An effective way to derive an aggregation method is to define it as optimal with respect to a specific loss function. If the target function Y(x) and the models Mi(x) are viewed as random variables on the same probability space, a commonly used loss function closely related to concepts like conditional expectation is the Mean Square Error (MSE). In this context, an aggregated model MA(x) can be defined as any measurable function of the models that minimizes the expected squared error:\n\\(M_A(x) := \\underset{f \\text{ measurable}}{\\text{argmin}} E [(Y(x) - f(M_1(x), .., M_n(x))^2] = E [Y(x)|M_1(x), .., M_n(x)],\\)\nwhere the second equality follows from the L2 characterization of conditional expectation. While computing the conditional expectation can be intractable [1], this computation reduces to solving a linear system in the Gaussian case. Specifically, if the vector (Y(x), M\u2081(x), ..., Mn(x)) is Gaussian with mean 0 (see remark below), then the conditional expectation in the equation above is a linear combination of the models' outputs. [23] uses this assumption to aggregate Gaussian processes. [4] demonstrates that any consistent and rational aggregation of models must be a weighted average that is point-wise linear. Finally, many ensembling methods (random forest [10], MoE [27]) also use a linear combination of models. Motivated by this, we restrict our aggregation approach to a pointwise linear combination of the models:\n\\(M_A(x) = \\alpha^*(x)^T M(x)\\)"}, {"title": "3.1.1. Aggregating PDE solvers in the Gaussian setting", "content": "We consider here a best-case scenario for MEA. [12] introduces a Gaussian processes framework for solving Partial Differential Equations (PDE)der the Laplace equation as a simple example:\n\\begin{cases}\n-\u0394u(x) = f(x) & \\text{for } x \\in [-1,1]^2 \\\\\nu(x) = g(x) & \\text{for } x \\in \\partial([-1,1]^2)\n\\end{cases}\nPlace a Gaussian prior \u03be ~ N(0, K) on the solution where K is a Radial Basis Function (RBF) kernel. Given interior collocation points X\u1d62 \u2208 [-1,1]\u00b2 and boundary collocation points X\u266d \u2208 \u2202([-1,1]\u00b2), we will approximate the solution u with the Gaussian process estimator \u00fb = E [\u03be|-\u0394\u03be(X) = f(X), \u03be(X\u266d) = g(X\u266d)].\nFixing the boundary collocation points, sample 100 sets of 60 interior collocation points X(k) to obtain 100 different models of the solution of the PDE, that we will denote\n$\\hat{u}^{(k)} := E [\\xi |-\\Delta \\xi(X^{(k)}) = f(X^{(k)}), \\xi(X_b) = g(X_b)] $."}, {"title": "3.2. Data-driven aggregation", "content": "We will now consider the situation where the quantities E [Y(x)M(x)] and E [M(x)M(x)] required for the best linear aggregate (4) may be ill-defined (e.g., because the underlying models are not stochastic) or difficult to estimate. In this situation, a natural alternative approach is to estimate the aggregation coefficients \u03b1* appearing in (2) directly from data by a minimizer \\^\u03b1 of a (possibly regularized) empirical version of the loss (3):\n$\\hat{\\alpha} = \\underset{\\alpha \\in H}{\\text{argmin}} \\sum_{i=1}^{N} |Y^i - \\alpha(X^i)^T M(X^i)|^2 + \\lambda ||\\alpha||_H^4,$\nwhere the (X\u2071, Y\u2071 := Y(X\u2071)) are N data points; H is a set of functions, such as a Reproducing Kernel Hilbert Space (RKHS), a set of functions obtained via gradient boosting or from a neural network architecture; \u03bb > 0 and ||\u03b1|| is a regularizing norm (e.g., an RKHS norm). We call this aggregation Minimal Empirical Error Aggregation (MEEA). While this transition from expected loss to empirical loss may seem well-founded, it can introduce a significant loss of information. The following section showcases two pathological examples that exhibit unexpected behavior due to this loss of information. In both cases, we set up a regression problem with a target Y(x) \u2208 \u211d, for x \u2208 \u211d. We have two types of models: a good model MG and bad models MB. Using data, we try to aggregate these models and showcase why the empirical loss (6) may not be suitable for ensembling models from data."}, {"title": "3.2.1. A dubious trend", "content": "This first example is a basic aggregation with linear coefficients. Figure 3a represents the results of this experiment. We pick:\n\\begin{aligned}Y(x) &= 2x + cos(3x) \\\\\nM_G(x) &= Y(x) + \\epsilon(x) \\text{ where } \\epsilon(x) \\sim N(0, 0.2) \\\\\nM_B(x) &= 1\\end{aligned}\nWe also choose a linear aggregation, which means that we will find aG, aB, bG, bB s.t. \u03b1(x) = (aGx + bG, aBx + bB) \u2208 \u211d\u00b2. Finally, we pick some trick data X = (0.8 - 2/3 * 4, 0.8 - 2/3 * 3, .., 0.8, -0.8, -0.8 + 2/3 * 1, .., -0.8 + 2/3*4) so that the Y(X\u2071) = Y\u1d62 form a line. We may see that MG has a lower error than MB on each X\u2071 by evaluating the models at these data points. Thus, we expect an aggregation method to use the good model primarily in the aggregation. However, the opposite behavior is observed. Minimizing the loss (6) with \u03bb = 0, we obtain that the coefficient on the good model is 0, so MG is completely ignored, which is counterintuitive. Instead, the aggregate uses the bad model, MB, to directly estimate Y by linear regression. This shows that the MEEA simply employs models as features to interpolate the data points instead of weighting each model according to its estimated accuracy."}, {"title": "3.2.2. Unknown region", "content": "In this second example illustrated in Figure 3b we select\n\\begin{aligned}Y(x) &= 3 cos(2\u03c0x) \\\\\nM_G(x) &= Y(x) + \\epsilon(x) \\text{ where } \\epsilon(x) \\sim N(0,0.3) \\\\\nM_M(x) &= 3 \\\\\nM_N(x) &= -3\\end{aligned}\nWrite M = (MG, MB, MN)\u1d40 for the vector defined by the three models. It is common for aggregation methods, such as MoE, to constrain \u03b1 to be a convex combination, which we implement here by seeking an aggregate of the form MA(x) = Softmax(\u03b1(x))\u1d40M(x)) where Softmax(\u03b1(x))\u2096 := exp(\u03b1\u2096(x))/ \u03a3\u1d62\u208c\u2081\u00b3 exp(\u03b1\u1d62(x))."}, {"title": "3.3. Aggregation using vector-valued Gaussian processes and matrix-valued kernels", "content": "We will now model aggregation as vector-valued Gaussian process regression to elucidate the pathologies observed in the two abovementioned examplesminder on this vector/matrix-valued generalization of GPs/kernels [2] is provided in appendix A. We choose a matrix-valued kernel K : X \u00d7 X \u2192 \u211d\u207f\u02e3\u207f, and write \u210b\u043a for its associated Reproducing Kernel Hilbert Space (RKHS). Given the data (Y(X\u2081), M(X\u2081)), ..., (Y(X_N), M(X_N)) we approximate the aggregation coefficients in (2) with\n$\\alpha = \\underset{\\alpha \\in H_K}{argmin} \\sum_{i=1}^N [Y(x_i) - \\alpha(x_i)^T M(x_i)]^2 + \\lambda||\\alpha||_K^2.$\n(11) then yields the following aggregate (see Appendix B):\n$\\begin{aligned}M_A(x) &= \\alpha(x)^T M(x) = k(x, X)(k(X, X) + \\lambda I)^{-1}Y \\end{aligned}$ \nwhere k(x, y) = M(x)\u1d40K(x,y)M(y). This is also the GP regressor MA = E [\u03be |\u03c6(X) = Y + \ud835\udca9(0, \u03bbI)] with prior \u03be ~ \ud835\udca9(0, k). This observation is crucial as it shows that the model values are used as a feature for the regression, as expected. However, it demonstrates that the Mean Empirical Error Aggregate (MEEA) only tries to regress Y without considering the underlying models' accuracy. This observation is counterintuitive as we expect model aggregation to leverage the individual models to approximate the target function better rather than merely regressing directly to the target. This behavior is particularly evident in pathological examples, illustrating the limitations of the direct regression approach."}, {"title": "4. Aggregation through error estimation: the Minimal Variance Aggregate (MVA)", "content": "We now introduce a different strategy for aggregating models. Since, as observed in section 3.1, pointwise linear aggregation has many benefits, we will still employ the pointwise linear aggregation formula (2) but modify the underlying stochastic model to take into account model errors."}, {"title": "4.1. Modelling the error", "content": "Our models, represented in the vector M, are assumed to be unbiased estimators of Y. Thus, the errors of each model are interpreted as follows:\n$\\begin{aligned}M(x) = Y(x) \\mathbb{1} + Z(x) \\text{ where } \\begin{cases}E[Z(x)] = 0 \\\\\nCov[Z(x)] = A(x)\\end{cases}\\end{aligned}$ \nwhere 1 := (1, .., 1) is the all ones vector. Under this assumption, the unbiased linear estimate of Y(x) given M(x) with minimal variance, which we call the Minimal Variance Aggregate (MVA) is:\n$\\begin{aligned}M_A(x) = \\frac{\\mathbb{1}^T A(x)^{-1} M(x)}{\\mathbb{1}^T A(x)^{-1} \\mathbb{1}}\\end{aligned}$ \nThis aggregation uses the information of the precision matrix to decide how to combine the models. If the models are indeed unbiased, it is equivalent to the Minimal Error Aggregation (MEA). Despite summing to 1, the coefficients \u03b1(x) are not necessarily positive, giving added flexibility in case models have negative conditional correlations."}, {"title": "4.2. Approximating the covariance matrix", "content": "To compute our aggregation, we must compute the covariance matrix of the error for all x \u2208 X from the data. This poses several challenges. First, we must approximate a symmetric matrix everywhere on the domain, needing n(n+1)/2 coefficients to aggregate n models. Moreover, we must ensure this matrix is always positive-definite. To simplify the approximation and to easily enforce the positive-definite condition, we suppose that for all x, A(x) can be diagonalized using a fixed eigenbasis P. Then, its positive eigenvalues are approximated by e^(di(x)), where the di are functions to be regressed from the data. Thus, we approximate A(x) with\n$ A(x) = P^T \\begin{pmatrix} e^{d_1(x)} & 0 & \\cdots & 0 \\\\\n0 & e^{d_2(x)} &  & \\vdots \\\\\n\\vdots &  & \\ddots &  \\\\\\n0 &  &  & e^{d_n(x)}\\end{pmatrix} P\\quad\\text{where}\\quad \\begin{aligned} P P^T = I \\\\\\\\ \\lambda_i \\text{ are regressors}\\end{aligned}$\nThis formulation has the advantage of symmetrizing the covariance and precision matrix estimation, as the log-eigenvalues of both matrices are opposite. This is desirable because while we are interested in the precision matrix, the covariance matrix is easier to estimate.\nWe will now estimate the covariance matrix. Let Z\u00b9, ., Z\u1d3a be i.i.d. samples of the random variable Z\u2208 \u211d\u207f such that E [Z] = 0. An unbiased estimate of Cov[Z] is\n$\\hat{A} = \\frac{1}{N} \\sum_{i=1}^N Z^i (Z^i)^T$\nObserve that \u00c2 can be identified as the minimizer of the following loss involving the Frobenius norm ||\u00b7||\ud835\udd3d:\n$\\hat{A} = \\underset{\\Sigma \\in \\mathbb{R}^{n \\times n}}{argmin} \\sum_{i=1}^N ||\\Sigma - Z^i(Z^i)^T||_F^2$\nWe will now regularize this formulation to approximate the model errors covariance matrix for all values of x. Writing e for the vector with entries defined as the sample errors (e\u2071)\u2096 = M\u2096(X\u2071) \u2013 Y(X\u2071), we identify the functions \u03bb\u1d62 as\n\\begin{aligned}\\lambda &= \\underset{\\Lambda \\in H}{\\text{argmin}} \\sum_{i=1}^N ||P^T Diag (e^k(X^i)) P - e(e^i)||_F^2 + a||\\Lambda||_H^4 \\\\\n&= \\underset{\\Lambda \\in H}{\\text{argmin}} \\sum_{i=1}^N \\sum_{k=1}^n (e^k(x) - (Pe^i)_k)^2 + a||\\Lambda||_H^4 \\end{aligned}$"}, {"title": "Rediscovering softmax", "content": "Taking P = I in the proposed formulation our aggregation reduces to\n$M_A(x) = Softmax(-\\Lambda(x))^T M(x)$,\nWhere Softmax(\u2212\u039b(x))\u1d62 = e\u207b^(di(x))/\u03a3\u2096\u208c\u2081\u207fe\u207b^(dk(x)). This formulation is very common, particularly for Mixtures-of-Experts (MoE)."}, {"title": "Underdetermination of MEA compared to MVA", "content": "Notice that MEA uses only one equation at each datapoint, namely Y(X\u1d4f) \u2248 \u03b1(X\u1d4f)\u1d40M(X\u1d4f). For a given Y(X\u1d4f) and M(X\u1d4f), there are many values of \u03b1(X\u1d4f) satisfying this constraint, indicating that the system is underdetermined. In such a situation, the regularization will play a crucial role. On the other hand, MVA uses as many equations as there are models at each data point: e^(d\u1d62(X\u1d4f)) \u2248 (Pe\u00b2)\u1d62. This makes \u039b better determined and less reliant on properly choosing the regularization to achieve good results."}, {"title": "4.3. Choosing the matrix P", "content": "We have explored two options for the matrix P:\n\u2022 P = I, the identity matrix, is the simplest choice.\n\u2022 P as the orthonormal basis that diagonalizes the empirical covariance matrix C := = \u03a3\u1d62 e\u2071e\u2071\u1d40. In the experiments presented below, we only show results for P = I. This choice is straightforward and delivers accuracy comparable to or better than using the eigenvectors of C.\nWe also observed that using the empirical covariance matrix can render the method unstable if the models are highly correlated. Indeed, consider, for instance, two models whose errors are significantly correlated, leading to the following empirical covariance matrix:\n$C:= \\begin{pmatrix} 1.1 & 0.9 \\\\\n0.9 & 1\\end{pmatrix}$\nIn this case, the second eigenvector of C, P\u2082 \u2248 results in 1\u1d40 P\u2082 \u2248 \u22120.04, which effectively subtracts the two models. This eigenvector is associated with a small eigenvalue (0.15), indicating that the difference between the two models has a low variance on average. Since equation (14) depends on the inverse of the estimated covariance matrix, a small estimated variance eigenvalue e^(d\u2082(x)) leads to \u03b1(x) having large coefficients with opposing signs. In this scenario, we observed that the aggregation method can become sensitive to small errors and thereby suffer from poor generalization."}, {"title": "4.4. The Minimal Empirical Variance Aggregate (MEVA) through error estimation", "content": "To summarize, we have defined the Minimal Variance Aggregate (MVA) in (14) by assuming the models are unbiased. To estimate this aggregate from data, we must approximate the error covariance matrix A(x) for all x. To do so, we choose an orthonormal matrix P as described in section 4.3, and a set of functions \u210b to approximate the log-eigenvalues of A(x) using equation (19) as described in section 4.2.\nCombining these three sections, the error estimation-based Minimal Empirical Variance Aggregate (MEVA) is defined as\n$ \\begin{aligned} \\lambda &= \\underset{\\Lambda \\in H}{\\text{argmin}} \\sum_{i=1}^N \\sum_{k=1}^n (e^k(x) - (Pe^i)_k)^2 + a||\\Lambda||_H^4 \\\\\\\\ \\alpha(x) &= \\frac{D(x)^{-1} P \\mathbb{1}}{\\mathbb{1}^T P^T D(x)^{-1} P \\mathbb{1}}\\quad\\text{where } D(x) = Diag \\left[exp(- \\lambda_k(x)), k = 1, ..., n\\right] \\\\\\\\ M_A(x) &= \\sum_{i=1}^n \\alpha_i(x) M_i(x)\\end{aligned} $\nwhere (e)k = Mk(X\u2071) \u2013 Y(X\u2071)."}, {"title": "Adding a bias correction", "content": "Assuming the models M(x) to be unbiased is necessary to obtain a linear aggregate, as adding a bias correction will result in an affine aggregation of the form MA(x) = \u03b1^(x)M(x)+ \u03b2(x). This assumption may, however, be violated in practice, and in this case, bias will be interpreted as variance. In a context with limited data and little knowledge of the models and target, it would be difficult to know if an error can be attributed to bias or variance. In the case where it is clear models are biased and interpreting it as variance hurts accuracy, we may modify our method by accounting for the bias\n$\\begin{aligned} M(x) = Y(x)\\mathbb{1} + \\hat{Z} \\text{ where } \\begin{cases} E[Z(x)] = \\mu(x) \\\\\\end{cases} \\end{aligned} $\nIn such a model, we must, in addition to \u039b, train \u03bc. Using the definition of \u017d, we know the loss for \u03bc is of the form \u03a3\u1d62(\u03bc(X\u2071) \u2013 e\u00b2)\u1d40A\u207b\u00b9(X\u2071)(\u03bc(X\u2071) \u2013 e\u00b2). Taking P = I, this bias-corrected aggregation would be:\n$\\begin{aligned} \\lambda, \\mu &= \\underset{\\Lambda, m \\in H_1 \\times H_2}{\\text{argmin}} \\sum_{i=1}^N \\sum_{k=1}^n \\left[(e_k(x^i) - (e^k - m_k(X^i))^2 + e^{-\\lambda_k(x^i)} (m_k(X^i) - e)^2 \\right] + a||\\Lambda||_{H_1}^4 + b||m||_{H_2} \\\\\\\\ \\bar{\\alpha}(x) &= Softmax(-\\bar{\\lambda}(x)) \\\\\\\\ M_A(x) &= \\sum_{i=1}^n \\bar{\\alpha}_i(x)(M_i(x) - \\mu_i(x)) \\end{aligned} $"}, {"title": "4.5. An alternative, direct Minimal Empirical Variance Aggregation loss", "content": "We observed in section 4.1 that our approach can be interpreted as minimizing the aggregate's variance, as we first estimate the errors of the models and then derive an aggregation using (14). In our model (13), any combination \u03b1 s.t. 1\u1d40\u03b1 = 1 is an unbiased estimator (and any unbiased estimator has to satisfy this property). To get our aggregation, we chose the best linear unbiased estimator (BLUE), by minimizing the variance of \u03b1(x)M(x). Under the model (13), Cov[M(x)] = A(x), so our aggregate is\n$\\bar{\\alpha}(x) = \\underset{\\nu \\in H}{\\text{argmin}} \\nu^T A(x) \\nu$\nInstead of estimating A(x) and inverting this estimation to obtain the minimal variance aggregate as in (14), we propose using an empirical version of the above loss. Specifically, let the empirical covariance matrices A\u1d62 be defined as:\n$\\begin{aligned}A_i := \\begin{cases} P^T Diag((Pe^i)_k, k = 1.., n)P &\\text{for general } P \\\\\\ Diag((e_k^i)^2), k = 1.., n &\\text{if } P = I\\end{cases}\\end{aligned}$ \nand write \u210b\u2081 for a normed subspace of the space of unbiased aggregators {u : x \u21a6 u(x) \u2208 \u211d\u207f s.t. \u03a3\u1d62\u208c\u2081\u207fu\u1d62(x) = 1, \u2200x}. Then the proposed alternative approach identifies an aggregator \u03b1 by minimizing the sample variance of the aggregate:\n$\\bar{\\alpha} = \\underset{u \\in H_1}{argmin} \\sum_{i=1}^N u(X^i)^T A u(X^i) + a||u||_{H_1}^4$"}, {"title": "5. Experiments", "content": "We present three experiments to showcase the effectiveness of our method. The code can be found in the paper's repository."}, {"title": "5.1. Aggregation on the Boston housing dataset", "content": "The Boston housing dataset [17] is a popular benchmarking dataset for Machine Learning (ML) applications. It contains N = 506 samples of 14 variables relating to housing prices in Boston, the task being to predict the median value of a home based on factors such as criminality or tax rates. We split this dataset into train, validation, and test sets. To evaluate our method, we train several standard ML methods on the train set: linear regression, decision tree, random forest, Support Vector Regression (SVR), k-neighbors, and gradient boosting. We then train our aggregation on the validation set before testing on the test set. To get a fair comparison, we also trained the ML methods on the train and validation sets combined so that they see the same amount of data as the aggregate. We first implement our aggregation as described in section 4.2, using GP regressors, i.e., we place a Gaussian prior \u03bb\u1d62 ~ \ud835\udca9(0, \u03ba) on the unknown d\u1d62 and we compute their MAP estimator given available data. This is equivalent to minimizing (19) over \u03bb\u1d62 \u2208 \u210b\u043a where \u210b\u043a is the RKHS defined by the kernel \u03ba. After experimenting with many kernels, the best we found uses the Mat\u00e9rn kernel \u03baMat\u00e9rn (u, v, p) = (1 + \u221a3|u - v|/p) exp(-\u221a3|u - v|/p), and is defined as\n\u03ba(x, y) = \u03baMat\u00e9rn (X, Y, \u03c1\u2081) + \u03baMat\u00e9rn (M(x), M(y), \u03c1\u2082)\nwith adequate choice of \u03c1\u2081, \u03c1\u2082. This formulation takes into account both the input x and the prediction M(x) to output the aggregation coefficients \u03b1(x)."}, {"title": "5.2. Aggregation of PDE solvers", "content": "In this section, we introduce the approach for combining Partial Differential Equation (PDE) solvers through model aggregation. These examples highlight the flexibility of our method, demonstrating its ability to aggregate a wide range of models, including both traditional and machine learning (ML) methods. By incorporating classical PDE solvers into the aggregation framework, we lower the accuracy threshold necessary for the aggregated solution to outperform individual models. Notably, this threshold is significantly below the typical 1% relative error benchmark targeted by most ML-based PDE solvers. We will begin by formulating the aggregation problem within the operator learning context. Following that, we present two experimental case studies that showcase the effectiveness of our approach."}, {"title": "5.2.1. Operator Learning in the Aggregation Context", "content": "Consider the Laplace equation with zero Dirichlet boundary conditions on the domain \u03a9 = [0, 1]\u00b2 as an illustrative example of a PDE:\n$\\begin{aligned} \\begin{cases} -\\Delta u^{\\dagger}(\\omega) = f(\\omega) & \\text{for } \\omega \\in \\Omega \\\\\\\\ u^{\\dagger}(\\omega) = 0 & \\text{for } \\omega \\in \\partial \\Omega \\end{cases} \\end{aligned}$\nwhere f \u2208 \u2131 \u2282 L\u00b2(\u03a9) and u\u2020 \u2208 \ud835\udcb4 \u2282 \u210b\u00b2(\u03a9) \u2229 \u210b(\u03a9) is the solution. The solution operator \ud835\udcae maps the source term f to the solution u\u2020. In this context, the solution operator is defined as:\n$ \\begin{aligned}S: \\begin{cases} \\mathcal{F} \\to \\mathcal{H} \\\\\\\\ f \\mapsto u^{\\dagger} \\end{cases} \\end{aligned}$\nThis operator is often approximated using a PDE solver, which can be based on methods like finite element analysis [5] or spectral methods [16]. Alternatively, the operator can be learned directly from pairs of input/output solutions, a supervised learning task known as operator learning [19]. Since both PDE solvers and machine learning methods approximate operators, we can naturally frame our aggregation task as an aggregation of multiple operators (or PDE solvers), denoted M\u2081,..., M\u2099. The aggregated operator MA is then expressed as:\n$\\begin{aligned} M_A(f) = \\sum_{i=1}^{n} \\alpha_i(f) M_i(f) \\in \\mathcal{F} \\end{aligned}$\nwhere \u03b1 is an operator that maps \u2131 to L\u00b2 (\u03a9, \u211d\u207f).\nRemark: Although we presented the most straightforward aggregation operator, MA(f, M(f)) = \u2211\u1d62\u208c\u2081\u207f\u03b1\u1d62(f)M\u1d62(f), this is not the only possible approach. In the context of operators, there exists many alternatives that remain pointwise linear with respect to the models. For instance, another possible approach is to aggregate Fourier coefficients:\n$ M_A(f) = \\mathcal{F}^{-1} \\left( \\sum_{i=1}^n \\alpha_i(f) \\mathcal{F}(M_i(f)) \\right)$"}, {"title": "5.2.2. The operator aggregation loss", "content": "To get the loss for our problem", "Y(x)": "\ud835\udcae(f)(w) = u\u2020(w) \u2208 \u211d. Given a grid (w\u00b9", "X\u2071\u02b2": "w\u2071", "limit": "The above loss (39) is a discretization", "loss": "n$min_\\Lambda \\sum_{k=1"}, {"11": ".", "M\u2096(X\u2071\u02b2))\u00b2": "e as a \u2192 0.\nThus", "log[e": "to get a linearized loss:\n$\\bar{\\lambda} = \\underset{\\Lambda \\in H}{\\text{argmin}} \\sum_{i=1}^{N_1} \\sum_{j=1}^{N_2} \\sum_{k=1}^{n} \\left( \\Lambda(X^{ij}) - \\log[e"}]}