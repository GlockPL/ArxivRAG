{"title": "AUCSeg: AUC-oriented Pixel-level Long-tail\nSemantic Segmentation", "authors": ["Boyu Han", "Qianqian Xu", "Zhiyong Yang", "Shilong Bao", "Peisong Wen", "Yangbangyan Jiang", "Qingming Huang"], "abstract": "The Area Under the ROC Curve (AUC) is a well-known metric for evaluating\ninstance-level long-tail learning problems. In the past two decades, many AUC\noptimization methods have been proposed to improve model performance under\nlong-tail distributions. In this paper, we explore AUC optimization methods in the\ncontext of pixel-level long-tail semantic segmentation, a much more complicated\nscenario. This task introduces two major challenges for AUC optimization tech-\nniques. On one hand, AUC optimization in a pixel-level task involves complex\ncoupling across loss terms, with structured inner-image and pairwise inter-image\ndependencies, complicating theoretical analysis. On the other hand, we find that\nmini-batch estimation of AUC loss in this case requires a larger batch size, re-\nsulting in an unaffordable space complexity. To address these issues, we develop\na pixel-level AUC loss function and conduct a dependency-graph-based theoret-\nical analysis of the algorithm's generalization ability. Additionally, we design a\nTail-Classes Memory Bank (T-Memory Bank) to manage the significant memory\ndemand. Finally, comprehensive experiments across various benchmarks con-\nfirm the effectiveness of our proposed AUCSeg method. The code is available at\nhttps://github.com/boyuh/AUCSeg.", "sections": [{"title": "1 Introduction", "content": "Semantic segmentation aims to categorize each pixel within an image into a specific class, which\nis a fundamental task in image processing and computer vision [33, 51, 75]. Over the past decades,\nsubstantial efforts [58, 23, 43, 96] have advanced the field of semantic segmentation. The mainstream\nparadigm is to develop innovative network architectures that encode more discriminative features\nfor dense pixel-level classifications. Typical backbones include CNN-based [58, 14, 80] and newly\nemerging Transformer-based methods [108, 83, 65, 15, 32], which have achieved the state-of-the-art\n(SOTA) performance. Beyond this direction, researchers [24, 66, 48, 41] have recently realized\nthe Pixel-level Long-tail issue in Semantic Segmentation (PLSS), as shown at the top of Figure 1.\nSimilar to the flaws of traditional long-tail problems, the major classes will dominate the model\nlearning process, causing the model to overlook the segmentation of minority classes in an image.\nSeveral remedies have been proposed to alleviate this [49, 61, 10, 56, 104, 92, 78]. For example,\n[77] introduces a category-wise variation technique inversely proportional to distribution to achieve\nbalanced segmentation; [66] introduces a sequence-based generative adversarial network for imbal-\nanced medical image segmentation, and [41] develops a re-weighting scheme for semi-supervised\nsegmentation."}, {"title": "Can we find a theoretically grounded loss function for PLSS on top of SOTA backbones?", "content": "This paper provides an affirmative answer\nfrom the AUC perspective and proposes a\nnovel framework called AUC-oriented Pixel-\nlevel Long-tail Semantic Segmentation (AUC-\nSeg). Specifically, AUC indicates the likelihood\nthat a positive sample scores higher than a neg-\native one, which has been proven to be insen-\nsitive to data distribution [86, 101]. Applying\nAUC to instance-level long-tail classifications\nhas shown promising progress in the machine\nlearning community [86, 100, 88, 68]. Moti-\nvated by its success, this paper starts an early\ntrial to study AUC optimization for PLSS. The\nprimary concern is to study its effectiveness for\nPLSS from a theoretical perspective. The key\nchallenge is that the standard techniques for gen-\neralization analysis [62, 8, 21] require the loss\nfunction to be expressed as a sum of indepen-\ndent terms. Unfortunately, the proposed loss\nfunction does not satisfy this assumption due\nto the dual effect of structured inner-image de-\npendency and pairwise inter-image dependency.\nThis complicated structure poses a big challenge\nto understanding its generalization behavior. To\naddress this, we decompose the loss function\ninto inner-image and inter-image terms. On top\nof this reformulation, we deploy the dependency\ngraph [103] to decouple the interdependency. Finally, we reach a bound of $\\tilde{O}\\left(\\tau \\sqrt{\\log (\\tau N k) / N}\\right)$, where $\\tau$ behaves like an indicator for imbalance degree, and $k$ denotes the number of pixels in each\nimage. This suggests optimizing AUC loss could ensure a promising performance under PLSS.\nBack to the practical AUC learning process, we realize that the stochastic gradient optimization\n(SGD) for structured pixel-level tasks imposes a greater computational burden compared to instance-\nlevel long-tail problems. Specifically, the SGD algorithm of AUC requires at least one sample\nfrom each class in each mini-batch [99, 85]. In light of this, the primary choice is to adopt the\nso-called stratified sampling on all images [70, 60, 86] for mini-batch generation (See Equation (7)).\nUnfortunately, as shown in Figure 3(a) and (b), this is hard to implement under PLSS because\npixel-level labels are densely coupled in each image. Meanwhile, as shown in Proposition 1, we\nalso argue that directly using random sampling to include all classes would require an extremely large\nbatch size. This leads to unaffordable GPU memory demands for optimization, as described in the\nexperiments in Appendix G.6.\nTo alleviate this, a novel Tail-class Memory Bank (T-Memory Bank) is carefully designed. The main\nidea is to identify those missing pixel-level classes in each randomly generated mini-batch and then\ncomplete these absences using stored historical class information from the T-Memory Bank. This\nenables efficient optimization of AUCSeg with a light memory usage, enhancing the scalability of\nour proposed method, as shown in Figure 1. Finally, comprehensive empirical studies consistently\nspeak to the efficacy of our proposed AUCSeg."}, {"title": "Our main contributions are summarized as follows:", "content": "\u2022 This paper starts the first attempt to explore the potential of AUC optimization in pixel-level\nlong-tail problems."}, {"title": null, "content": "\u2022 We theoretically demonstrate the generalization performance of AUCSeg in semantic segmentation.\nTo our knowledge, this area remains underexplored in the machine-learning community.\n\u2022 We introduce a Tail-class Memory Bank to reduce the optimization burden for pixel-level AUC\nlearning."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Semantic Segmentation", "content": "Semantic segmentation is a subtask of computer vision, which has seen significant development since\nthe inception of FCN [58]. The most common framework for semantic segmentation networks is the\nencoder-decoder. For the encoder, researchers typically use general models such as ResNet [37] and\nResNeXt [84]. As the segmentation tasks become more challenging, some specialized networks have\nemerged, such as HRNet [75], ICNet [105], and multimodal networks [76, 42]. For the decoder, a\nseries of studies focus on strengthening edge features [23, 107], capturing global context [43, 31, 45],\nand enhancing the receptive field [96, 67, 12, 13]. Recently, the transformer has shown immense\npotential, surpassing previous methods. A series of methods related to Vision Transformer [108,\n83, 65, 15, 74] are proposed. SegNeXt [32], which is the current state-of-the-art (SOTA) method,\npossesses the same powerful feature extraction capabilities as the Vision Transformer and the same\nlow computational requirements as CNN. Apart from improving the network, some research [48, 56,\n24, 11, 10, 66, 39] is directed toward addressing the issue of class imbalance in semantic segmentation.\nHowever, the effectiveness of these methods is not significant. In this paper, we aim to improve the\nperformance of long-tailed semantic segmentation from an AUC optimization perspective."}, {"title": "2.2 AUC Optimization", "content": "The development of AUC Optimization can be divided into two periods: the machine learning\nera and the deep learning era. As a pioneering study, [20] ushers in the era of AUC in machine\nlearning. It studies the necessity of AUC research, which points out that AUC maximization\nand error rate minimization are inconsistent. After that, AUC gains significant attention in linear\nfields such as Logistic Regression [38] and SVM [46, 47]. Then researchers begin to explore the\nonline [106, 28] and stochastic [94, 63] optimization extensions of the AUC maximization problem.\nResearch from the perspectives of generalization analysis [2, 73, 17] and consistency analysis [1, 29]\nprovides theoretical support for AUC optimization algorithms. [57] is the first to extend AUC\noptimization to deep neural networks, ushering in the era of AUC in deep learning. Meanwhile,\na series of AUC variants [87, 86, 68, 88, 69, 90] emerge, gradually enriching AUC optimization\nalgorithms. Furthermore, in practice, AUC optimization demonstrates its effectiveness in various\nclass-imbalanced tasks, such as recommendation systems [5, 6, 4, 7], disease prediction [79, 30],\ndomain adaptation [89], and adversarial training [40, 91].\nDespite significant progress, existing studies of AUC optimization mainly pay attention to the instance-\nlevel imbalanced classification tasks. This paper starts an early trial to introduce AUC optimization\nto semantic segmentation. However, due to the high complexity of pixel-level multi-class AUC\noptimization, such a goal cannot be attained by simply using the current techniques in the AUC\ncommunity."}, {"title": "3 Preliminaries", "content": "In this section, we briefly introduce the semantic segmentation task and the AUC optimization\nproblem."}, {"title": "3.1 Semantic Segmentation Training Framework", "content": "Let $\\mathcal{D}=\\left{\\left(\\mathbf{X}^{i}, \\mathbf{Y}^{i}\\right)\\right}_{i=1}^{n} | \\mathbf{X}^{i} \\in \\mathbb{R}^{H \\times W \\times 3}, \\mathbf{Y}^{i} \\in \\mathbb{R}^{H \\times W \\times K}$ be the training dataset, where $H$ and\n$W$ represent the height and width of the images, and $K$ denotes the total number of classes. Let\n$f_{\\theta}$ be a semantic segmentation model ($\\theta$ is the model parameters), which commonly follows an\nencoder-decoder backbone [75, 105, 108, 83, 32]. Let $\\hat{\\mathbf{Y}}=f_{\\theta}\\left(\\mathbf{X}^{i}\\right) \\in \\mathbb{R}^{H \\times W \\times K}$ be the dense\npixel-level prediction, i.e.,\n$\\hat{\\mathbf{Y}}=f_{\\theta}\\left(\\mathbf{X}^{i}\\right)=f_{d}\\left(f_{e}\\left(\\mathbf{X}^{i}\\right)\\right),$\\\n      where the encoder $f_{e}$ extracts features from the image $\\mathbf{X}^{i}$, and then the decoder $f_{d}$ predicts each\npixel based on extracted features and outputs a dense segmentation map with the same size as $\\mathbf{Y}^{i}$.\nFurthermore, let $\\mathbf{Y}_{u, v}^{i}$ and $\\hat{\\mathbf{Y}}_{u, v}^{i}$ represent the ground truth and prediction of the $(u, v)$-th pixel of the\n$i$-th image, respectively. To train the model $f_{\\theta}$, most current studies [14, 80, 83, 65] usually adopt the\ncross-entropy (CE) loss:\n$\\begin{aligned}\\mathcal{L}_{C E} & =\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{u=0}^{H-1} \\sum_{v=0}^{W-1} \\sum_{c=1}^{K} -\\left[\\mathbf{Y}_{u, v}^{i}\\right]^{c} \\log \\left[\\hat{\\mathbf{Y}}_{u, v}^{i}\\right]^{c}\\end{aligned}$\\\n      where $\\left[\\mathbf{Y}_{u, v}^{i}\\right]^{c}$ and $\\left[\\hat{\\mathbf{Y}}_{u, v}^{i}\\right]^{c}$ are the one-hot encoding of ground truth and the prediction of pixel $\\left(\\mathbf{X}_{u, v}^{i}, \\mathbf{Y}_{u, v}^{i}\\right)$\nin class $c$, respectively."}, {"title": "3.2 AUC Optimization", "content": "Area under the Receiver Operating Characteristic Curve (AUC) is a well-known ranking performance\nmetric for binary classification task, which measures the probability that a positive instance has a\nhigher score than a negative one [35]:\n$\\operatorname{AUC}\\left(f_{\\theta}\\right)=\\mathbb{P}\\left(f_{\\theta}\\left(\\mathbf{X}^{+}\\right)>f_{\\theta}\\left(\\mathbf{X}^{-}\\right) | y^{+}=1, y^{-}=0\\right),$\nwhere $\\left(\\mathbf{X}^{+}, y^{+}\\right)$ and $\\left(\\mathbf{X}^{-}, y^{-}\\right)$ represent positive and negative samples, respectively. When AUC $\\rightarrow$\n$1$, it indicates that the classifier can perfectly separate positive and negative samples.\nAccording to [87, 86, 88], given finite datasets, maximizing $\\operatorname{AUC}\\left(h_{\\theta}\\right)$ is usually realized by\nmaximizing its unbiased empirical estimation:\n$\\widehat{\\operatorname{AUC}}\\left(h_{\\theta}\\right)=\\frac{1}{n^{+} n^{-}} \\sum_{i=1}^{n^{+}} \\sum_{j=1}^{n^{-}} \\ell\\left(h_{\\theta}\\left(\\mathbf{X}_{i}^{+}\\right)-h_{\\theta}\\left(\\mathbf{X}_{j}^{-}\\right)\\right),$\nwhere $\\ell$ is a differentiable surrogate loss [86] measuring the ranking error between two samples, $n^{+}$\nand $n^{-}$ denote the number of positive and negative samples, respectively.\nMoreover, we can directly optimize the following problem for AUC maximization:\n$\\min \\frac{1}{n^{+} n^{-}} \\sum_{i=1}^{n^{+}} \\sum_{j=1}^{n^{-}} \\ell\\left(h_{\\theta}\\left(\\mathbf{X}_{i}^{+}\\right)-h_{\\theta}\\left(\\mathbf{X}_{j}^{-}\\right)\\right).$\\Note that, AUC has achieved significant progress in long-tailed classification [100, 88, 68]. Due to\nthe limitations of space, we refer interested readers to the literature [86, 99] for more introductions to\nAUC. However, most existing studies merely focus on the instance-level or image-level problems.\nInspired by its distribution-insensitive property [26], this paper starts an early trial to introduce AUC\nto PLSS."}, {"title": "4 AUC-Oriented Semantic Segmentation", "content": "In this section, we introduce our proposed AUCSeg method for semantic segmentation. A brief\noverview is provided in Figure 2. AUCSeg is a generic optimization method that can be directly\napplied to any SOTA backbone for semantic segmentation. Specifically, AUCSeg includes two\ncrucial components: (1) AUC optimization where a theoretically grounded loss function is explored\nfor PLSS and (2) Tail-class Memory Bank, an effective augmentation scheme to ensure efficient\noptimization of the proposed AUC loss. In what follows, we will go into more detail about them. For\nclarity, we include a table of symbol definitions in Appendix A."}, {"title": "4.1 Pixel-level AUC Optimization", "content": "Semantic segmentation is a multi-class classification task. Therefore, to apply AUC, we follow\na popular multi-class AUC manner, i.e., the One vs. One (ovo) strategy [64, 34, 86], which is\nan average of binary AUC score introduced in Section 3.2. Specifically, on top of the notation of\nSection 3.1, we further denote $\\mathcal{D}_{p}=\\left{\\left(\\mathbf{X}_{u, v}^{i}, \\mathbf{Y}_{u, v}^{i}\\right) | i \\in[1, n], u \\in[0, H-1], v \\in[0, W-1]\\right}$ as the\nset of all pixels; the $j$-th element ($j \\in[1, n \\times(H-1) \\times(W-1)]$) in $\\mathcal{D}_{p}$ is abbreviated as $\\left(\\mathbf{X}^{j}, \\mathbf{Y}^{j}\\right)$\nfor convenience. Given the model prediction $f_{\\theta}=\\left(f_{\\theta}^{(1)}, \\ldots, f_{\\theta}^{(K)}\\right)$, $\\forall c \\in[K], f_{\\theta}^{(c)} \\in[0,1]$, where\n$f_{\\theta}^{(c)}$ serves as a continuous score function supporting class $c$, $\\operatorname{AUC}_{c c^{\\prime}}^{\\text {ovo}}$ calculates the average of\nbinary AUC scores for every class pair:\n$\\operatorname{AUC}_{s e g}^{O V O}=\\frac{1}{K(K-1)} \\sum_{c=1}^{K} \\sum_{c^{\\prime} \\neq c}^{K} \\operatorname{AUC}_{c c^{\\prime}}\\left(f_{\\theta}\\right),$\n$\\operatorname{AUC}_{c c^{\\prime}}\\left(f_{\\theta}\\right)=\\mathbb{P}\\left(f_{\\theta}^{(c)}\\left(\\mathbf{X}^{j}\\right)>f_{\\theta}^{\\left(c^{\\prime}\\right)}\\left(\\mathbf{X}^{j}\\right) | \\mathbf{Y}_{m}^{j}=c, \\mathbf{Y}_{m}^{j}=c^{\\prime}\\right).$\nTo this end, as introduced in Section 3.2, the goal is to minimize the following unbiased empirical\nrisk:\n$\\mathcal{L}_{a u c}:=\\frac{1}{K} \\sum_{c=1}^{K} \\sum_{c^{\\prime} \\neq c} \\frac{1}{\\left|\\mathcal{N}_{c}\\right|\\left|\\mathcal{N}_{c^{\\prime}}\\right|} \\sum_{\\mathbf{X}_{m} \\in \\mathcal{N}_{c}} \\sum_{\\mathbf{X}_{n} \\in \\mathcal{N}_{c^{\\prime}}} \\ell_{s q}\\left(f_{\\theta}^{(c)}\\left(\\mathbf{X}_{m}\\right)-f_{\\theta}^{\\left(c^{\\prime}\\right)}\\left(\\mathbf{X}_{n}\\right)\\right),$\\where we adopt the widely used square loss $\\ell_{s q}(x)=(1-x)^{2}$ as the surrogate loss [29]; $\\mathcal{N}_{c}=\\left{\\mathbf{X} | \\mathbf{Y}_{m}=\\right.$ $c\\}$ represents the set of pixels with label $c$ in the set\n$\\mathcal{D}_{P}$, and $\\left|\\mathcal{N}_{c}\\right|$ denotes the size of the set."}, {"title": "4.2 Generalization Bound", "content": "In this section, we explore the theoretical guarantees of the AUC loss function in semantic segmenta-\ntion tasks and demonstrate that AUCSeg can generalize well to unseen data.\nA key challenge is that standard techniques for generalization analysis [62, 8, 21] require the loss\nfunction to be expressed as a sum of independent terms. Unfortunately, the proposed loss function does\nnot satisfy this assumption because there are two layers of interdependency among the loss terms.\nOn one hand, semantic segmentation can be considered a structured prediction problem [16], where\ncouplings between output substructures within a given image create the first layer of interdependency.\nOn the other hand, the AUC loss creates a pairwise coupling between positive and negative pixels, so\nany pixel pairs sharing the same positive/negative instance are interdependent, resulting in the second\nlayer of interdependency.\nWe present our main result in the following theorem and the proof is deferred to Appendix B.\nTheorem 1 (Generalization Bound for AUCSeg). Let $\\mathbb{E}_{\\mathcal{D}}\\left[\\mathcal{L}_{\\mathcal{D}}(f)\\right]$ be the population risk of of $\\mathcal{L}_{\\mathcal{D}}(f)$.\nAssume $\\mathcal{F} \\subset\\left{f: \\mathcal{X} \\rightarrow \\mathbb{R}^{H \\times W \\times K}\\right}$, where $H$ and $W$ represent the height and width of the image,\nand $K$ represents the number of categories, $\\mathcal{L}^{(i)}$ is the risk over $i$-th sample, and is $\\mu$-Lipschitz with\nrespect to the $\\ell_{\\infty}$ norm, (i.e. $\\left|\\mathcal{L}(x)-\\mathcal{L}(y)\\right|_{\\infty} \\leq \\mu \\cdot\\|x-y\\|_{\\infty}$). There exists three constants $A>0$,\n$B>0$ and $C>0$, the following generalization bound holds with probability at least $1-\\delta$ over a\nrandom draw of i.i.d training data (at the image-level):\n$\\left|\\mathcal{L}_{\\mathcal{D}}(f)-\\mathbb{E}_{\\mathcal{D}}\\left[\\mathcal{L}_{\\mathcal{D}}(F)\\right]\\right| \\leq \\frac{8 \\mu \\tau}{\\sqrt{N}}+\\frac{\\mathcal{N}_{i n n e r}+\\mathcal{N}_{i n t e r}}{\\mathcal{N}} \\sqrt{A \\log (2 B \\mu \\tau N k+C)}+3\\left(\\sqrt{\\frac{1}{\\mathcal{N}}}+K\\left(1-\\sqrt{\\frac{1}{\\mathcal{N}}}\\right)\\right) \\sqrt{\\frac{1}{\\mathcal{N}} \\log \\frac{4 K(K-1)}{\\delta}}$\nwhere\n$\\mathcal{N}_{i n n e r}=\\frac{48 \\mu \\tau \\ln \\mathcal{N}}{\\mathcal{N}}, \\quad \\mathcal{N}_{i n t e r}=2 \\sqrt{2} \\tau, \\quad \\tau=\\frac{\\left(\\frac{\\mathcal{N}_{m a x}^{(c)}}{\\mathcal{N}_{m e a n}^{(c)}}\\right)^{2}}{\\max \\limits_{c \\in[K]} \\frac{\\mathcal{N}_{m a x}^{(c)}}{\\mathcal{N}_{m e a n}}^{(c)}}^{2},$\n$\\mathcal{N}_{m e a n}=\\sum_{i=1}^{|\\mathcal{D}|} n\\left(\\mathcal{X}^{(c)}\\right), \\mathcal{N}=\\left|\\mathcal{D}\\right|, k=H \\times W$ and $\\mathcal{X}^{(c)}$ represents the\npixel of class $c$ in image $\\mathcal{X}$.\nRemark 1. We achieve a bound of $\\tilde{O}\\left(\\tau \\sqrt{\\log (\\tau N k) / N}\\right)$, indicating reliable generalization with a\nlarge training set. Here, $\\tau$ represents the degree of pixel-level imbalance. More interestingly, even\nthough we have $k$ classifiers for every single image, the generalization bound only has an algorithm\ndependent on $k$, suggesting that pixel-level prediction doesn't hurt generalization too much."}, {"title": "4.3 Tail-class Memory Bank", "content": "Motivation. Although we have examined the effectiveness of AUC for PLSS from the theoretical\npoint of view, there is a practical challenge when conducting AUC optimization for semantic\nsegmentation, as discussed in Section 1. Specifically, the stochastic AUC optimization, as defined in\nEquation (7), requires at least one sample from each class in a mini-batch. In instance-level AUC\noptimization, recent studies [87, 86] often use a stratified sampling technique that generates batches\nconsistent with the original class distribution, as shown in Figure 3(a). Such a strategy will work well\nwhen each image belongs to a unique category (say, \u2018Banana\u2019, \u2018Apple\u2019, or \u2018Lemon\u2019) in traditional\nclassifications. Yet it cannot apply to pixel-level cases because each sample involves multiple and\ncoupled labels, making it hard to split them for stratified sampling, as illustrated in Figure 3(b).\nMeanwhile, we also provide a bound (Proposition 1) to show that simply adopting random sampling\nwill suffer from an overlarge batch size $B$, making an unaffordable GPU memory burden.\nProposition 1. Consider a dataset D that includes images with K different pixel categories. Let\n$p_{i}$ represent the probability of observing a pixel with label $i$ in a given image. Randomly select B\nimages from D as training data, where\n$B=\\Omega\\left(\\frac{\\log (\\delta / K)}{\\log \\left(1-\\min p_{i}\\right)}\\right)$\nThen with probability at least $1-\\delta$, for any $c \\in[K]$, there exists X in the training data that contains\npixels of label c.\nRemark 2. The proof is deferred to Appendix C. Proposition 1 suggests that the value of $B$ is\ninversely proportional to $\\min p_{i}$. Note that $p_{i}$ will be smaller as the long-tail degree becomes more"}, {"title": null, "content": "severe, leading to a larger $B$. For example, in terms of the Cityscapes dataset with $K=19$ classes,\nassuming $\\delta=0.01$ and $\\min p_{i}=1 %, B$ should be at least 759 to guarantee that each class of pixels\nappears at least once with a high probability. This results in a significant strain on GPU memory.\nTo address this, considering that the tail-class samples generally have less opportunity to be included\nin a mini-batch and are often more crucial for final performance, we thus develop a novel Tail-\nclass Memory Bank (T-Memory Bank) to efficiently optimize Equation (7) and manage GPU usage\neffectively. As depicted in Figure 3(c), the high-level ideas of the T-Memory Bank are as follows: 1)\nidentify missing tail classes of all images involved in a mini-batch and 2) randomly replace some\npixels in the image with missing classes based on stored historical class information in T-Memory\nBank. In this sense, we can obtain an approximated batch-version of Equation (7), i.e.,\n$\\mathcal{L}_{a u c}:=\\sum_{c=1}^{K} \\sum_{c^{\\prime} \\neq c} \\sum_{\\mathbf{X} \\in \\mathcal{N}_{c} \\cup T_{c}} \\sum_{\\mathbf{X} \\in \\mathcal{N}_{c^{\\prime}} \\cup T_{c^{\\prime}}} \\frac{1}{\\left|\\mathcal{N}_{c}\\right|\\left|\\mathcal{N}_{c^{\\prime}}\\right|} \\ell_{s q}^{j c, c^{\\prime}, m, n},$\\where $\\mathcal{N}_{c}$ and $\\mathcal{T}_{c}$ represent the set of pixels with label $c$ in the original image and those pixels stored\nin the T-Memory Bank, respectively; $\\ell_{s q}^{j c, c^{\\prime}, m, n}:=\\ell_{s q}\\left(f_{\\theta}^{(c)}\\left(\\mathbf{X}_{m}\\right)-f_{\\theta}^{\\left(c^{\\prime}\\right)}\\left(\\mathbf{X}_{n}\\right)\\right); \\mathbf{X}^{P}$ represents the\nsample after replacing some pixels with tail classes pixels from the T-Memory Bank.\nDetailed Components. As shown in Figure 2, T-Memory Bank comprises three main parts: (1)\nMemory Branch stores a set with SM (the Memory Size) images for each tail class. We define the\nset as $\\mathcal{M}=\\left{\\mathcal{M}_{c_{1}}, \\ldots, \\mathcal{M}_{c_{n t}}\\right}$, where $\\mathcal{C} t=\\left{c_{i}\\right}_{i=1}^{n_{t}}$ denotes the labels of tail classes, and $n t$ is the\ntotal number of selected tail classes; (2) Retrieve Branch selects pixels from the Memory Branch to\nsupplement the missing tail classes and (3) Store Branch updates the Memory Branch whenever a\nnew image arrives. Algorithm 1 summarizes a short version of AUCSeg equipped with T-Memory\nBank. Please refer to the detailed version in Appendix D. Note that we introduce CE loss as a\nregularization term for our proposed AUCSeg, which is widely used in the AUC community [99] to\npursue robust feature learning. Experiments demonstrate that the performance is insensitive to the\nregularization weight $\\lambda$, as shown in Figure 5d.\nAt the start of training, the Memory Branch is empty. In this case, we only calculate the loss\nfunction $\\ell$ for the classes present in the mini-batch, while the Retrieve Branch will not take any action.\nMeanwhile, the Store Branch will continuously append pixel data of tail classes to the Memory\nBranch. As the Memory Branch reaches its maximum capacity SM, we adopt a random replacement\nstrategy to update the Store Branch (Lines 5 to 6 in Algorithm 1 or Lines 6 to 11 in Algorithm 2).\nAs the training process progresses, if the Memory Branch is not empty, the Retrieve Branch kicks in\nto count the missing classes in each image of the mini-batch, denoted as $\\mathcal{C} m i s s$. It then calculates\nthe number of classes needed to be added for optimization, $N_{\\text {sample }}=\\left[\\left|\\mathcal{C}_{m i s s}\\right| \\times \\mathcal{R} s\\right]$. Here, we\nintroduce a tunable sample ratio $\\mathcal{R} s$ to strike a trade-off between the original and missing tail-class\nsemantic information. Finally, it uniformly retrieves the corresponding pixels of $n_{\\text {sample }}$ missing\nclasses from the Memory Branch, resizes them by the resize ratio $R_{R}$, and randomly selects positions\nto overwrite (Lines 7 to 8 in Algorithm 1 or Lines 13 to 18 in Algorithm 2)."}, {"title": null, "content": "Algorithm 1: AUCSeg Algorithm (Short Version)\nInput: Training data $\\mathcal{D}$, number of tail classes $n_{t}$, labels of tail classes $\\mathcal{C} t=\\left{C_{i}\\right}_{i=1}^{n t}$, Memory\nBranch $\\mathcal{M}=\\left{\\mathcal{M} c_{1}, \\ldots \\mathcal{M} c_{n_{t}}\\right\\}$, memory size $S_{M}$, sample ratio $\\mathcal{R} s$, resize ratio $R_{R}$,\nmax iteration $T_{\\max }$, batch size $N_{b}$\nOutput: model parameters $\\theta$\n1 for iter = 1 to $T_{\\max }$ do\n2 $\\mathcal{D}_{B}=\\left{\\left(\\mathbf{X}_{i}, \\mathbf{Y}_{i}\\right)\\right\\}_{i=1}^{N_{b}} \\leftarrow SampleBatch $\\left(\\mathcal{D}, N_{b}\\right)$;\n3 $C_{m i s s} \\leftarrow \\mathcal{C} t \\leftarrow MissingTailClasses $\\left(\\mathcal{D}_{B}\\right)$;\n4\t\u25b7 Store Branch\n5 for $C_{m}$ in $C_{m i s s}$ do\n6 Divide a picture containing only the $C_{m}$-th class pixels from $\\mathcal{D}_{B}$ and name it $P$;\n7 if $\\left|\\mathcal{M} c_{m}\\right| <S_{M}$ then\n8 Add the divided image $P$ to $\\mathcal{M} c_{m}$;\n9 else\n10 Randomly replace an image in $\\mathcal{M} c_{m}$ with the divided image $P$;\n11 \u25b7 Retrieve Branch\n12\t$N_{\\text {sample }}=\\left[\\left|C_{m i s s}\\right| \\times \\mathcal{R} s\\right]$\n13 for $i=1$ to $n_{\\text {sample }}$ do\n14 Randomly choose $C_{m}$ from $C_{m i s s}$;\n15 Remove $C_{m}$ in $C_{m i s s}$;\n16 if $\\left|\\mathcal{M} c_{m}\\right| \\neq 0$ then\n17 Sample from $\\mathcal{M} c_{m}$, scale according to $R_{R}$, paste randomly into $\\mathcal{D}_{B}$;\n18\u25b7 Semantic Segmentation\n19 $\\hat{\\mathbf{Y}} i \\leftarrow f_{\\theta}\\left(\\mathbf{X}_{i}\\right)$;\n20 Calculate $\\ell=\\mathcal{L} a u c+\\lambda \\mathcal{L} c e$ with Equation (2) and Equation (8);\n21 Backpropagation updates $\\theta$."}, {"title": null, "content": "Discussions. We recognize that Memory Bank [82, 36"}]}