{"title": "GAMIFIED CROWD-SOURCING OF HIGH-QUALITY DATA FOR VISUAL FINE-TUNING", "authors": ["Shashank Yadav", "Rohan Tomar", "Garvit Jain", "Chirag Ahooja", "Shubham Chaudhary", "Charles Elkan"], "abstract": "This paper introduces gamified adversarial prompting (GAP), a framework that crowd-sources high-quality data for visual instruction tuning of large multimodal models. GAP transforms the data collection process into an engaging game, incentivizing players to provide fine-grained, challenging questions and answers that target gaps in the model's knowledge. Our contributions include (1) an approach to capture question-answer pairs from humans that directly address weaknesses in a model's knowledge, (2) a method for evaluating and rewarding players that successfully incentivizes them to provide high-quality submissions, and (3) a scalable, gamified platform that succeeds in collecting this data from over 50,000 participants in just a few weeks. Our implementation of GAP has significantly improved the accuracy of a small multimodal model, namely MiniCPM-Llama3-V-2.5-8B, increasing its GPT score from 0.147 to 0.477 on our dataset, approaching the benchmark set by the much larger GPT-4V. Moreover, we demonstrate that the data generated using MiniCPM-Llama3-V-2.5-8B also enhances its performance across other benchmarks, and exhibits cross-model benefits. Specifically, the same data improves the performance of QWEN2-VL-2B and QWEN2-VL-7B on the same multiple benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual question answering (VQA) has emerged as a crucial paradigm in AI, extending beyond mere visual interpretation to facilitate broader and deeper understanding in models. Studies demonstrate VQA's potential in enhancing general knowledge acquisition, transfer learning, and complex reasoning skills. Mahdisoltani et al. (2018) showed that pretraining on complex visual-linguistic tasks significantly improves performance across diverse downstream applications, from textual generation to fine-grained classification. The encoding of visual information as language, explored in works like Something-Else (Materzynska et al., 2020; Girdhar & Ramanan, 2019), and more recently by Alayrac et al. (2022), enables models to develop low-level visual skills that support sophisticated reasoning in multimodal contexts.\nLarge multimodal models (LMMs) have shown impressive capabilities in visual question answering tasks, demonstrating an ability to understand and reason about visual content (Liu et al., 2023a; Dai et al., 2023). However, these models often struggle with fine-grained visual details, factual accuracy, and complex reasoning, particularly when faced with challenging, objective questions (Li et al., 2023c; Yu et al., 2023).\nSupervised fine-tuning (SFT) remains a critical yet challenging phase for LLMs and LMMs, especially in visual question answering. SFT includes instruction tuning, where models are optimized to follow natural language instructions. Visual instruction tuning builds on this by fine-tuning models to handle both visual inputs and textual instructions such as questions or commands related to an image. While visual instruction tuning has led to improvements in LMMs, its effectiveness is constrained by the quality and specificity of the training data (Liu et al., 2023a; Li et al., 2023a).\nOur gamified adversarial prompting (GAP) method transforms visual instruction tuning by incentivizing players to come up with challenging, fine-grained questions that target specific weaknesses in a given LMM. The GAP approach focuses on improving factual accuracy and reasoning capabilities, particularly for objective, verifiable information more than for subjective preferences or general alignment.\nThe contributions of this paper include an approach to automatically evaluate and reward player submissions with high accuracy inspired by the reCAPTCHA approach (Ahn et al., 2008). This evaluation scheme ensured we were able to scale GAP to over 50,000 players (Figure 2) in just a few weeks, while ensuring high data quality and player engagement."}, {"title": "2 RELATED WORK", "content": "Visual Question Answering (VQA) Recent advancements in Visual Question Answering (VQA) have focused on improving model architectures and training methodologies. Our work builds upon these foundations while introducing a novel approach to data collection and model finetuning.\nInstructBLIP (Dai et al., 2023) adopted Q-Former to sample from visual tokens for LLM processing. LLaVA (Liu et al., 2023a) introduced visual instruction tuning, using instruction-following data to convert LLMs into multimodal LLMs. Flamingo (Alayrac et al., 2022) and IDEFICS (Lauren\u00e7on et al., 2024) used shared decoders for visual-language understanding. Qwen-VL (Bai et al., 2023) employed a three-stage training process to convert LLMs to multimodal models. SPHINX (Gao et al., 2024) adopted multiple visual encoders to enrich visual features. InternLM-Xcomposer (Zhang et al., 2023) achieved state-of-the-art performance using interleaved text-image composition data.\nRecently, Yao et al. (2024) introduced MiniCPM-V, a series of efficient LMMs designed for end-side devices. Their work demonstrates the potential for GPT-4V level performance with significantly fewer parameters, while being deployable on mobile phones. This represents a trend towards more efficient and accessible LMMs. We use instruction tuned MiniCPM-Llama3-V-2.5-8B as our base model owing to the small size coupled with high performance.\nOn other recent advancements in LMMs, Lu et al. (2024) aim for real-world vision-language understanding, while Li et al. (2024a) improve reasoning, OCR, and world knowledge in LMMs. CuMo (Li et al., 2024b) introduced a method for scaling LMMs using mixtures of experts (MoE), incorporating sparse MoE blocks into both the vision encoder and the MLP connector. Recently, Hong et al. (2024) proposed CogVLM2, a family of visual language models that achieve state-of-the-art performance on various image and video understanding benchmarks.\nAdversarial and Challenging Dataset Creation Creating challenging datasets that expose AI system limitations has been a focus of recent research. SWAG (Zellers et al., 2018), HellaSwag (Zellers et al., 2019), and Social IQa (Sap et al., 2019) introduced large-scale adversarial datasets for commonsense inference. WinoGrande (Sakaguchi et al., 2020) presented an adversarial dataset for commonsense reasoning at scale.\nResearchers have also explored algorithmic methods for bias reduction, such as adversarial filtering (Zellers et al., 2018; 2019; Le Bras et al., 2020). These approaches help mitigate bias and reduce dataset artifacts, though they are often applied post-hoc. Our vast and geographically diverse contributor base potentially yields less biased data by incorporating a wide range of global perspectives.\nMME (Fu et al., 2023) contains manually annotated instruction-answer pairs to measure perception and cognition abilities on a total of 14 subtasks. DocVQA Mathew et al. (2021), another large dataset, consists of 50,000 questions defined on 12,000+ document images. Liu et al. (2023b) incorporate multiple-choice questions in both English and Chinese versions. In the current work, we focusing only on English. These datasets been created by crowd-sourcing. It is difficult to scale this approach while maintaining quality; our approach is an improvement on this front.\nBenchmarks The most recent benchmarks include OCRBench (Liu et al., 2024) designed to assess optical character recognition (OCR) capabilities; MMMU (Yue et al., 2024) consisting of multi-modal questions from college exams, quizzes, and textbooks spanning 30 subjects and 183 subfields; MM-Vet (Yu et al., 2024) is another benchmark that assesses LMMs on complex tasks requiring the integration of multiple vision-language capabilities; HallusionBench (Guan et al., 2023), challenging benchmark targeted towards LMM hallucinations. All these are available through VLMEvalKit (Duan et al., 2024) We use these benchmarks to evaluate the model finetuned on the dataset generated through GAP.\nGamification and Human-in-the-Loop Approaches Our gamified approach to data collection is inspired by works leveraging human interaction to create challenging datasets. CommonsenseQA 2.0 (Talmor et al., 2021) introduced a gamification framework where players compose questions to mislead a rival AI. This approach led to enhanced player engagement and high-quality data collection at scale."}, {"title": "3 GAMIFIED ADVERSARIAL PROMPTING (GAP)", "content": "When it comes to data, quality beats quantity (Zhou et al., 2024; Li et al., 2023b). Given an LMM M, an image i and question q related to the image and the expected answer a, the model response is denoted r = M(i, q). The tuple (i, q, a) is defined as an informative sample for M if the response r is not factually equivalent to the expected answer a.\nWhile LMMs are stochastic and generate different responses to the same query, generating a factually incorrect response is usually a sign of a gap in a model's knowledge.\nGameplay One session for one player proceeds as follows:\n1. Player sees an image\n2. They ask a question which they believe AI would answer incorrectly\n3. Player evaluates the AI's response, they have to mark it \"Correct\" or 'Wrong'\n4. If they mark it 'Wrong' they move to the next image, otherwise they can ask more questions. Player automatically moves to the next image after 120 secs.\nFor every session, a player sees 10 images and has at most 120 seconds for each image. When the player finishes 10 images, or the session resets after 6 hours, they earn points based on their performance. We impose a time limit on each image to create a sense of urgency. Limiting sessions to 10 images keeps casual players motivated by allowing them to engage briefly while still competing for rewards"}, {"title": "4 EXPERIMENTAL SETUP", "content": "We launched our platform as a Telegram miniapp, engaging over 50,000 participants within weeks, without traditional marketing. This rapid growth highlights Telegram's effectiveness for player acquisition in the crypto space. The success of tap-to-earn games like Catizen (30 million players) and Hamster Kombat (300 million players) further demonstrates the platform's potential. Telegram was chosen over Discord for its larger crypto-native user base and stronger viral growth potential for blockchain projects.\nBaseline Model We selected MiniCPM-Llama3-V-2.5-8B as our baseline model due to its state-of-the-art performance in multimodal tasks, cost-efficiency, scalability, and flexible Apache 2.0 licensing. The model demonstrates comparable capabilities to LLaVA (Liu et al., 2023a), Phi-3 (Abdin et al., 2024), and Qwen-VL (Bai et al., 2023; Wang et al., 2024), particularly in single-image understanding and visual question answering. Table 2 summarizes the key characteristics of this model.\nThe model's efficient resource utilization enables effective horizontal scaling, making it suitable for large-scale experiments and potential real-world applications. These characteristics provide a strong foundation for evaluating improvements in multimodal language understanding and generation tasks while considering practical deployment constraints.\nGAP-VQA Dataset and Experiment Design To create GAP-VQA, we randomly sampled 3, 683 question-image pairs, filtering them using a threshold 0 = 0.8 (Equation 3) to ensure a high proportion of adversarial examples. While this filtering may include some non-adversarial questions, these"}, {"title": "5 RESULTS", "content": "The GAP-VQA-train dataset includes a diverse set of questions, divided into 13 groups (Table 8 in the Appendix). These questions assess varying levels of image understanding, from simple tasks like object naming and description to more complex ones like identifying anomalies or explaining cultural events. They evaluate object recognition, spatial reasoning, counting, reading text in images, recognizing emotions, estimating time or season, and interpreting art styles. This broad range of questions helps LMMs improve their generalization capabilities.\nModel Performance on the GAP-VQA-val Dataset We evaluated the performance of various models on the GAP-VQA-val dataset using GPT-4 as an evaluator. A comprehensive prompt was used to assess the model's answers based on criteria including existence, position, count, and color accuracy, with scores ranging from 0 to 1, where 1 indicates perfect alignment. The evaluation prompt is detailed in Appendix A.2.\nThe GAP-VQA dataset was designed to challenge the MiniCPM model by targeting its weaknesses, guiding the fine-tuning process for better alignment. As shown in Table 4, MiniCPM-Llama3-V-"}, {"title": "6 DISCUSSION", "content": "The Gamified Adversarial Prompting (GAP) framework represents a significant advance in the field of AI model improvement. Our results demonstrate substantial improvements in VQA across multiple benchmarks and models. The implications of GAP extend beyond immediate performance gains, suggesting a new paradigm for continuous, scalable AI improvement that leverages human creativity and engagement.\nGAP offers advantages over methods that rely on AI self-assessment or using one AI model to evaluate another. Such approaches can be efficient, but they risk perpetuating or even amplifying existing biases and errors. Furthermore, in cases where AI models are trained using outputs from other models without proper attribution or permission, ethical concerns arise regarding the appropriation of intellectual property. Our human-in-the-loop approach sidesteps these issues by leveraging human cognition and diverse perspectives to drive model improvement. By relying on human evaluation within a gamified framework, we respect legal and ethical boundaries while providing a more transparent method for model improvement. This ensures that our model's growth is built on a foundation of original, human-verified data, rather than potentially restricted or problematic AI-generated content.\nOur future work will focus on enhancing the GAP framework through three key developments: (1) a visually fine-tuned language model for generating questions that the base LLM answers incorrectly, systematically identifying the blind spots; (2) a sophisticated probabilistic model incorporating player skill, image difficulty, response time, and fatigue to more accurately estimate LMM capabilities while controlling for confounding variables A.3; and (3) exploration of GAP's applicability beyond LMMs to other AI domains, addressing domain-specific challenges and ethical considerations. These advancements aim to create a scalable, gamified approach for continuous improvement of AI systems through targeted human feedback."}, {"title": "A APPENDIX", "content": "A.1 PLAYER PARTICIPATION AND PERFORMANCE\nFigure 4 reveals key insights into user participation patterns. 69.68% of accounts show no weekly participation, likely including airdrop farming bots-automated programs common in blockchain projects that perform minimal actions to qualify for potential rewards. Despite this, 30.32% of users actively engage at least once per week, with 2.68% participating in multiple weekly sessions.\nFigure 4(b) demonstrates that when users do participate, they overwhelmingly interact with all 10 images in a session, as evidenced by the pronounced spike at the 10-image mark. This suggests that the game design effectively encourages thorough engagement once a session begins.\nThe active user base, while smaller, provides valuable data on the game's appeal and effectiveness. The high completion rate of sessions and the presence of repeat participants indicate strong engagement among active users, showcasing the game's potential to retain and involve players consistently.\nFigure 5 presents a striking bimodal distribution of participant accuracy on the tainted dataset. A substantial 35% of participants achieved exceptional accuracy in the 90-100% range, demonstrating"}, {"title": "A.2 GPT-4V EVALUATION", "content": "We used the following prompt to evaluate answers on GAP-VQA-val using GPT-4V:\nPlease evaluate the model's answer based on the following criteria compared to the correct answer:\n1. Correct Answer: \u201c{premise}\"\n2. Model Answer: \u201c{hypothesis}\"\nCriteria for evaluation:\n\u2022 Existence: Does the model's answer correctly identify the existence or non-existence of objects or elements described in the correct answer?\n\u2022 Position: Does the model's answer accurately describe the position or location of objects or elements as stated in the correct answer?\n\u2022 Count: Does the model's answer correctly state the number of objects or elements mentioned in the correct answer?\n\u2022 Color: Does the model's answer accurately describe the color of objects or elements as indicated in the correct answer?\nAssign a score from 0 to 1 based on how well the model's answer meets these criteria:\n\u2022 A score of \"1\" means the model's answer fully meets all criteria, accurately reflecting existence, position, count, and color as described in the correct answer.\n\u2022 A score of \"0\" means the model's answer fails to meet any of the criteria, showing no alignment with the correct answer.\n\u2022 Scores between \"0\" and \"1\" should reflect partial correctness, where the model's answer meets some criteria but not all, or has minor inaccuracies.\nCarefully consider each criterion before deciding. What is the appropriate score (between 0 and 1) that best represents the factual correctness of the model's answer? Just return the score as a single number."}, {"title": "A.3 PLAYER INTERACTION MODEL", "content": "The probabilistic model for evaluating user-generated questions designed to identify AI mistakes in image analysis incorporates player ability, image difficulty, time pressure, and fatigue to estimate the probability of a player submitting a question that reveals an AI error.\nFormal Definition We define our VQA model as a function f : C \u00d7 Q \u00d7 \u0398 \u00d7 \u03a9 \u2192 R. Let's examine each space in detail:\n\u2022 C (Context Space): This represents all possible input images or visual contexts. Example: C includes all possible digital images, such as photographs of landmarks, diagrams of scientific concepts, or snapshots of everyday scenes.\n\u2022 Q (Question Space): This encompasses all possible questions that can be asked about the visual contexts. Example: Q includes questions like \"What color is the car?\", \"How many people are in the image?\", or \"What type of architecture is shown in this building?\".\n\u2022 \u0398 (Parameter Space): This represents all possible configurations of the model's parameters. Example: For a neural network, \u0398 would include all possible weight and bias values for all layers of the network."}, {"title": "Table 8: Categories of Visual Question Answering (VQA) Tasks", "content": "\u2022 \u03a9 (Probability Space): This captures the stochastic nature of the model, representing all possible random outcomes. Example: \u03a9 could represent the randomness in dropout layers, or the variability in outputs due to temperature scaling in the final softmax layer.\n\u2022 R (Response Space): This is the space of all possible model outputs or responses. Example: R could include textual answers like \"The car is red\", numerical answers like \"There are 3 people\", or more complex responses like \"The building exhibits Gothic architecture\u201d.\nAdditionally, we define:\n\u2022 A (Answer Space): A subset of R that contains all correct answers. Example: For the question \"What color is the sky?\u201d, A might include {\"Blue\", \"Azure\", \"Cyan\"} depending on the specific image.\n\u2022 g : C \u00d7 Q \u2192 24 (Correct Answer Function): This function maps a context-question pair to a set of correct answers. Example: g(image of Eiffel Tower, \"Where is this landmark located?\") = {\"Paris\", \"France\", \"The capital of France\"}\nWe define factual equivalence ~ as a relation on R. For example:\n\u2022 \"A dozen eggs\" ~ \"12 eggs\"\n\u2022 \"The capital of France\" ~ \"Paris\"\n\u2022 \"H2O\" ~ \"Water\"\nAn adversarial sample is a triplet (c, q, \u03b8) \u2208 C \u00d7 Q \u00d7 \u0398 such that:"}]}