{"title": "A Generative Approach to Control Complex Physical Systems", "authors": ["Long Wei", "Peiyan Hu", "Ruiqi Feng", "Haodong Feng", "Yixuan Du", "Tao Zhang", "Rui Wang", "Yue Wang", "Zhi-Ming Ma", "Tailin Wu"], "abstract": "Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and identify near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method in 1D Burgers' equation and 2D jellyfish movement control in a fluid environment. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics.", "sections": [{"title": "1 Introduction", "content": "Modeling the dynamics of complex physical systems is an important class of problems in science and engineering. Usually, we are not only interested in predicting a physical system's behavior but also injecting time-variant signals to steer its evolution and optimize specific objectives. This gives rise to the complex physical control problem, a fundamental task with numerous applications, including controlled nuclear fusion [9], fluid control [21], underwater devices [72] and aviation [48], among others.\nDespite its importance, controlling complex physical systems efficiently presents significant challenges. It inherits the fundamental challenge of simulating complex physical systems, which are typically high-dimensional and highly nonlinear, as specified in Appendix A.1. Furthermore, observed control signals and corresponding system trajectories for optimizing a control model are typically far from the optimal solutions of the specific control objective. This fact poses a significant challenge of dilemma: How to explore long-term control sequences beyond its training distribution to seek near-optimal solutions while making the resulting system trajectory faithful to the dynamics of the physical system?"}, {"title": "2 Background", "content": "To tackle physical systems control problems, various techniques have been proposed, yet they fall short of addressing the above challenges. Regarding traditional control methods, the Proportional-Integral-Derivative (PID) control [33], is efficient but only suitable for a limited range of problems. Conversely, Model Predictive Control (MPC) [59], despite a wider range of applicability, suffers from high computational costs and challenges in global optimization. Recent advances in supervised learning (SL) [21, 23] and reinforcement learning (RL) [14, 47, 53], trained on system trajectories and control signals data, have demonstrated impressive performance in solving physical systems control problems. However, existing SL and model-based RL methods either fall into myopic failure modes [25] that fail to achieve long-term near-optimal solutions, or produce adversarial state trajectories [74] that violate the physical system's dynamics. The main reason may be that they treat the continuous evolution of dynamics from an iterative view, both lacking long-term vision and struggling in global optimization. See Appendix A.2 for more related work on physical system control.\nIn this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. We take an energy optimization perspective over system trajectory and control sequences across the whole horizon to implicitly capture the constraints inherent in system dynamics. We accomplish this through diffusion models, which are trained using system trajectory data and control sequences. In the inference stage, DiffPhyCon integrates simulation and control optimization into a unified energy optimization process. This prevents the generated system dynamics from falling out of distribution, and offers an enhanced perspective over long-term dynamics, facilitating the discovery of control sequences that optimize the objectives.\nAn essential aspect of physical systems control lies in its capacity to generate near-optimal controls, even when they may deviate significantly from the training distribution. We address this challenge with the key insight that the learned generative energy landscape can be decomposed into two components: a prior distribution representing the control sequence and a conditional distribution characterizing the system trajectories given the control sequence. Based on this insight, we develop a prior reweighting technique to subtract the effect of the prior distribution of control sequences, with adjustable strength, from the overall joint generative energy landscape during inference.\nWe demonstrate the effectiveness of DiffPhyCon via extensive experiments on 1D Burgers' equation and 2D jellyfish movement control tasks, with the additional challenge of partial observation and partial control. In particular, the 2D jellyfish movement control problem is an important source of inspiration for the design of underwater and aerial devices, and its challenges come from complex vortice behavior and fluid-solid coupling dynamics [55, 27]. We generate such a dataset based on a CFD (computational fluid dynamic) software to mimic the movement of a jellyfish under control signals of its flapping behavior. To advance research in controlling complex physical systems, we contribute our dataset as a benchmark. On both tasks, our method outperforms widely applied classical control methods and is also competitive with recent supervised learning and strong reinforcement learning baselines. Notably, DiffPhyCon reveals the intriguing fast-close-slow-open pattern exhibited by the jellyfish, aligning with findings in the field of fluid dynamics.\nIn summary, we contribute the following: (1) We develop DiffPhyCon, a novel generative method to control complex physical systems. By optimizing trajectory and control sequences jointly in the entire horizon by diffusion models, DiffPhyCon facilitates global optimization of long-term dynamics and helps to reduce myopic failure modes. (2) We introduce the prior reweighting technique to generate control sequences that are superior to those in the training set. (3) We demonstrate the effectiveness of our method on 1D Burgers' equation and 2D jellyfish movement control tasks, particularly demonstrating advantages in scenarios with partial observations/control. (4) We release a new benchmark of jellyfish movement control to the community of complex physical system control."}, {"title": "2.1 Problem Setup", "content": "We consider the following complex physical system control problem:\nw* = argmin I(u, w) s.t. C(u, w) = 0.\nW\n(1)\nHere u(t, x) : [0, \u03a4] \u00d7 \u03a9 \u2192 Rdu is the system trajectory {u(t,\u00b7), t \u2208 [0,T]} defined on time range [0, T] CR and spatial domain \u2229 \u2282 RD, and w(t, x) : [0, \u03a4] \u00d7 \u03a9 \u2192 Rdw is the external"}, {"title": "2.2 Preliminary: Diffusion Models", "content": "Diffusion models [18] are a class of generative models that learn data distribution from data. Diffusion models consist of two opposite processes: the forward process q(xk+1|xk) = N(xk+1; \u221a\u03b1kxk, (1\u2212\nak)I) to corrupt a clean data xo to a Gaussian noise xx ~ N(0, I), and the reverse parametrized process po(Xk-1|xk) = N(xk\u22121; \u03bc\u03b8 (xk, k), \u03c3\u03ba\u0399) to denoise from standard Gaussian xx ~ N(0, I), where {ak}_1 is the variance schedule. To train diffusion models, [18] propose the DDPM method to minimize the following training loss for the denoising network ee, a simplification of the evidence lower bound (ELBO) for the log-likelihood of the data:\nK\nL = Ek~U(1,K),xo~p(x),\u20ac~N(0,1) [||\u20ac - \u03b5\u03c1(\u221a\u1fb6\u03ba\u03c7\u03bf + \u221a1 \u2013 \u03ac\u03ba\u03b5, \u03ba)||2],\n(2)\nwhere \u0101k := \u03a0=1 i. e estimates the noise to be removed to recover data x0. During inference, iterative application of ee from a Gaussian noise could generate a new sample x0 that approximately follows the data distribution p(x). See Appendix A.3 for related work on diffusion models.\nNotation. We use v[n,m] = [Vn,\u2026\u2026, Vm] to denote a sequence of variables. We use z[0,T\u22121],k to denote the hidden variable of Z[0,T\u22121] in a diffusion step k. For simplicity, we abbreviate w[0,T\u22121], u[1,T] as w, u. Concatenation of two variables are denoted via e.g., [u, w]."}, {"title": "3 Method", "content": "In this section, we detail our method DiffPhyCon. In Section 3.1, we introduce our method including its training and inference. In Section 3.2, we further propose a prior reweighting technique to improve DiffPhyCon. The overview of DiffPhyCon is illustrated in Figure 1."}, {"title": "3.1 Generative Control by Diffusion Models", "content": "DiffPhyCon takes an energy optimization perspective to solve the problem Eq. (1), where PDE constraints can be modeled as a parameterized energy-based model (EBM) Eo(u, w,c) which characterizes the distribution p(u, wc) of u and w conditioned on conditions c by the correspondence"}, {"title": "3.2 Prior Reweighting", "content": "the control objective is incorporated in our diffusion model, generated control sequences are still highly influenced by the prior distribution of control sequences in training datasets. This inspires us to explore strategies to mitigate the effect of this prior, aiming to generate near-optimal control sequences.\nWe address this challenge with the key insight that the energy-based model E(u, w, c) can be decomposed into two components: one is E(p) (w, c) derived from the prior distribution p(w|c) of control sequences, and the other E(c) (u, w, c) representing the conditional probability distribution p(ulw, c) of trajectories with respect to given control sequences. This decomposition has the following form\nE(u, w, c) = E(p) (u, c) + E(c) (u, w, c),\n(8)\nby the corresponding decomposition of distribution p(u, w|c) = p(w|c)p(u|w, c). We propose a prior reweighting technique, which introduces an adjustable hyperparameter y > 0 as an exponential of p(w/c), allowing for the tuning of the influence of this prior distribution. Then we have a reweighted version of p(u, wc) as py(u, wc) = p(w|c)p(u|w, c)/Z, which is also a probability distribution and can be further transformed to\npy(u, wc) = p(w|c)^\u00af\u00b9p(u, w|c)/Z,\n(9)\nwhere Z is a normalization constant. In particular, when 0 < \u03b3 < 1, this approach is advantageous as it flattens the distribution p(u, w|c), thereby increasing the likelihood of sampling from low probability region of p(u, w|c), where the optimal solutions of the problem Eq. (3) probably lie.\nIntegrating Eq. (9) into Eq. (8), we have\nE(1) (u, w, c) = (y \u2212 1)E(p) (w, c) + Eo(u, w, c) \u2013 log Z,\n(10)\nwhere E(7) (u, w, c) = \u2212 log (py(u, w|c)) + const is the reweighted energy-based model associated with Eo (u, w, c) in Eq. (3), relying on the hyperparameter \u03b3. Then the optimization problem Eq. (3) can be transformed to\nu*, w* = argmin [E(1) (u, w, c) + J (u, w)].\nu,w\n(11)\nOptimization of this problem encourages sampling from the low likelihood region of p(w|c) while minimizing the control objective, which possesses the capability to generate control sequences that are more likely to be near-optimal than its degenerate version y = 1 in the original optimization problem Eq. (3). The intuition of prior reweighting is illustrated in Figure 2.\nTraining. To learn the reweighted energy E(1) (u, w, c), we parameterize its gradient as a summation of two parts by taking the gradient of both sides of Eq. (10):\nVE(u, w, c) = (y - 1)\u2207E) (w, c) + \u2207Eo(u, w, c),\n(12)\n\u03c6\u03b8\nwhere EP) (w, c) parameterizes the energy based model E(P) (w, c) corresponding to p(w|c). Note that Vlog Z vanishes here because it is a constant. Notice that \u2207Ee(u, w, c) has already been trained by Eq. (4). VEP) (w, c) can be trained similarly by the following loss function\n(13)\nL = Ek~U(1,K),(w,c)~p(w,c),\u20ac~N(0,1) [||\u20ac \u2013 \u20ac$(\u221a\u0101\u1e6dw + \u221a1 \u2013 \u0101ze, c, k) ||[2],\nwhere \u20ac is the conditional denoising network that approximates \u2207EP) (w,c).\nControl optimization. With both ee and ef trained, Eq. (11) can be optimized by running:\n\u03b7(\u03b5\u03b8(zk, c, k) + \u03bb\u2207zI (2k)) + \u00a71, \u00a71 ~ \u039d(0, \u03c3\u0399)\n(14)\nZk-1Zk\nWk\u22121 = Wk\u22121 - \u03b7(\u03b3 \u2013 1)\u03b5\u03c6(wk, c, k) + \u00a32, \u00a72 ~ N (0,01),\n(15)\niteratively, where zk = [uk, Wk]. The difference between this iteration scheme and Eq. (7) is that it uses an additional step to update we based on the predicted noise of \u20ac4. This guides zk = [uk, wWk] to move towards the reweighted distribution py(u, w|c) while aligning with the direction to decrease the objective by its guidance in the iteration of zk. The complete algorithm is present in Algorithm 1. Detailed discussion and results about how to set the hyperparameter y are provided in Appendix J.\nDiffPhyCon-lite. The introduction of the prior reweighting technique in DiffPhyCon involves training and evaluation of two models, thus bringing in additional computational cost. It is gratifying to note that we can balance the control performance and computational overhead of DiffPhyCon by adjusting the parameter \u03b3. When y = 1, the model e is not needed, and we denote this simplified version of DiffPhyCon as DiffPhyCon-lite."}, {"title": "4 Experiments", "content": "In this section, we aim to answer the following questions: (1) Can DiffPhyCon present superiority over traditional, supervised learning, and reinforcement learning methods for physical systems control? (2) Does the proposed prior reweighting technique help achieve better control objectives? (3) Could answers to (1) and (2) be generalized to more challenging partial observation or partial control scenarios? To answer these questions, we conduct experiments on the 1D Burgers' equation and 2D jellyfish movement control problem, both of which are vital and challenging.\nThe following state-of-the-art control methods are selected as baselines. For 1D Burgers' equation, we use (1) the classical and widely used control algorithm Proportional-Integral-Derivative (PID) [33] interacting with our trained surrogate model of the solver; (2) Supervised Learning method (SL) [23]; RL methods including (3) Soft Actor-Critic (SAC) [15] with offline and surrogate-solver versions; (4) Behaviour Cloning (BC) [50]; and (5) Behavior Proximal Policy Optimization (BPPO) [75]. Specifically, the surrogate-solver version of SAC interacts with our trained surrogate model of the solver, while the offline version only uses given data. BC and BPPO are also in offline version. For 2D jellyfish movement control, baselines include SL, SAC (offline), SAC (surrogate-solver), BC, BPPO, and an additional classical multi-input multi-output algorithm Model Predictive Control (MPC) [59]. PID is inapplicable to this data-driven task. Detailed descriptions of baselines are provided in Appendix F and Appendix G. We provide the code to reproduce the result for our method and baselines here."}, {"title": "4.1 1D Burgers' Equation Control", "content": "Experiment settings. The Burgers' equation is a governing law occurring in various physical systems. We consider the 1D Burgers' equation with the Dirichlet boundary condition and external force w(t, x), which is also studied in [23, 43].\n{\u2202u+u\u2202x\u2212v\u22022\nu\u2202x+w(t,x)in[0,T]\u00d7\u03a9u(t,x)=0on[0,T]\u00d7\u0398\u03a9u(0,x)=u0(x)in{t=0}\u00d7\u03a9.\n(16)\nHere v is the viscosity parameter, and up(x) is the initial condition. Subject to Eq. (16), given a target state ua(x), the objective of control is to minimize the control error Jactual between ur and ud, while constraining the energy cost Tenergy of the control sequence w(t, x):\nJactual:=Jo|u(T,x)\u2212ud(x)|2dx,Tenergy:=So[0,\u03a4]\u03a7\u03a9|w(t,x)|2dtdx\n(17)\nTo make the evaluation challenging, we select three different experiment settings that correspond to different real-life scenarios: partial observation, full control (PO-FC), full observation, partial control (FO-PC), and partial observation, partial control (PO-PC), which are elaborated on in Appendix C.2 and illustrated in Appendix B. These settings are challenging for classical control methods such as PID since they require capturing the long-range dependencies in the system dynamics. Note that the reported metrics in different settings are not directly comparable.\nIn this experiment, DiffPhyCon uses the guidance conditioning (Section 3.1) to optimize Jactual and explicit guidance to optimize the energy cost, which is elaborated in Appendix C.\nResults. In Table 1, we report results of the control error Jactual of different methods. It can be observed that DiffPhyCon delivers the best results compared to all baselines. Specifically, DiffPhyCon"}, {"title": "4.2 2D Jellyfish Movement Control", "content": "Experiment settings. This task is to control the movement of a flapping jellyfish with two wings in a 2D fluid field where fluid flows at a constant speed. The jellyfish is propelled by the fluid when its wings flap. Its moving speed and efficiency are determined by the mode of flapping. This task represents a common phenomenon in aquatic locomotion, which involves complex fluid-solid interactions and is an important source of inspiration for the design of underwater and aerial devices [27]. For this task, fluid dynamics follows the 2D incompressible Navier-Stokes Equation:\n{\u2202v+v\u22c5\u2207v\u2212v\u22072v+\u2207p=0\u2202t\u2207v=0v(0,x)=v0(x),\n(18)\nwhere v represents the 2D velocity of the fluid, and p represents the pressure, constituting the PDE state u = (v, p). The initial velocity condition is vo(x) and the kinematic viscosity is v. We assume that each wing is rigid, so the jellyfish's boundary can be parameterized by the opening angle wt of wings. Long-term movement of jellyfish usually presents a periodic flapping mode. Consequently, the control objective is to maximize its average moving speed \u016b determined by the pressure of the"}, {"title": "5 Limitation", "content": "There are still several limitations of our DiffPhyCon that provide exciting opportunity for future works. Firstly, our approach is data-driven, so it is not guaranteed to achieve optimal solutions and also lacks estimation of the error gap between the generated control sequence and the optimal ones. Secondly, the training of DiffPhyCon is currently conducted in an offline fashion, without interaction with a ground-truth solver. Incorporating solvers into the training framework could facilitate real-time feedback, enabling the model to adapt dynamically to the environment and discover novel strategies and solutions. Furthermore, our proposed DiffPhyCon presently operates in an open-loop manner, as it does not consider real-time feedback from solvers. Integrating such feedback would empower the algorithm to adjust its control decisions based on the evolving state of the environment."}, {"title": "6 Conclusion", "content": "In this work, we have introduced DiffPhyCon, a novel methodology for controlling complex physical systems. It generates control sequences and state trajectories by jointly optimizing the generative energy and control objective. We further introduced prior reweighting to enable the discovery of control sequences that diverge significantly from training. Through comprehensive experiments, we demonstrated our method's superior performance compared to classical, deep learning, and reinforcement learning baselines in challenging physical systems control tasks. We discuss future work in Appendix L and state social impact in M."}]}