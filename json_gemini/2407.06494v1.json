{"title": "A Generative Approach to Control Complex Physical Systems", "authors": ["Long Wei", "Peiyan Hu", "Ruiqi Feng", "Haodong Feng", "Yixuan Du", "Tao Zhang", "Rui Wang", "Yue Wang", "Zhi-Ming Ma", "Tailin Wu"], "abstract": "Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and identify near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method in 1D Burgers' equation and 2D jellyfish movement control in a fluid environment. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics.", "sections": [{"title": "1 Introduction", "content": "Modeling the dynamics of complex physical systems is an important class of problems in science and engineering. Usually, we are not only interested in predicting a physical system's behavior but also injecting time-variant signals to steer its evolution and optimize specific objectives. This gives rise to the complex physical control problem, a fundamental task with numerous applications, including controlled nuclear fusion [9], fluid control [21], underwater devices [72] and aviation [48], among others.\nDespite its importance, controlling complex physical systems efficiently presents significant challenges. It inherits the fundamental challenge of simulating complex physical systems, which are typically high-dimensional and highly nonlinear, as specified in Appendix A.1. Furthermore, observed control signals and corresponding system trajectories for optimizing a control model are typically far from the optimal solutions of the specific control objective. This fact poses a significant challenge of dilemma: How to explore long-term control sequences beyond its training distribution to seek near-optimal solutions while making the resulting system trajectory faithful to the dynamics of the physical system?"}, {"title": "2 Background", "content": "2.1 Problem Setup\nWe consider the following complex physical system control problem:\nw* = \\operatorname{argmin}_{W} J(u, w) \\quad \\text { s.t. } \\quad C(u, w)=0.\nHere u(t, x) : [0, \u03a4] \u00d7 \u03a9 \u2192 Rdu is the system trajectory {u(t,\u00b7), t \u2208 [0,T]} defined on time range [0, T] CR and spatial domain \u2229 \u2282 RD, and w(t, x) : [0, \u03a4] \u00d7 \u03a9 \u2192 Rdw is the external"}, {"title": "2.2 Preliminary: Diffusion Models", "content": "Diffusion models [18] are a class of generative models that learn data distribution from data. Diffusion models consist of two opposite processes: the forward process q(xk+1|xk) = N(xk+1; \\sqrt{\\alpha_k}xk, (1\u2212\\alpha_k)I) to corrupt a clean data xo to a Gaussian noise xx ~ N(0, I), and the reverse parametrized process po(Xk-1|xk) = N(xk\u22121; \u03bc\u03b8 (xk, k), \u03c3\u03ba\u0399) to denoise from standard Gaussian xx ~ N(0, I), where {\\alpha_k}_{k=1}^{K} is the variance schedule. To train diffusion models, [18] propose the DDPM method to minimize the following training loss for the denoising network ee, a simplification of the evidence lower bound (ELBO) for the log-likelihood of the data:\nL = E_{k \\sim U(1, K), x_{0} \\sim p(x), \\epsilon \\sim N(0,1)}[||\\epsilon-\\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{k}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{k}}\\epsilon, k)||^{2}],\nwhere $\\bar{\\alpha}_{k} :=\\prod_{i=1}^{k} \\alpha_{i}. e estimates the noise to be removed to recover data x0. During inference, iterative application of ee from a Gaussian noise could generate a new sample x0 that approximately follows the data distribution p(x). See Appendix A.3 for related work on diffusion models.\nNotation. We use v[n,m] = [Vn,\u2026\u2026, Vm] to denote a sequence of variables. We use z[0,T\u22121],k to denote the hidden variable of Z[0,T\u22121] in a diffusion step k. For simplicity, we abbreviate w[0,T\u22121], u[1,T] as w, u. Concatenation of two variables are denoted via e.g., [u, w]."}, {"title": "3 Method", "content": "In this section, we detail our method DiffPhyCon. In Section 3.1, we introduce our method including its training and inference. In Section 3.2, we further propose a prior reweighting technique to improve DiffPhyCon. The overview of DiffPhyCon is illustrated in Figure 1.\n3.1 Generative Control by Diffusion Models\nDiffPhyCon takes an energy optimization perspective to solve the problem Eq. (1), where PDE constraints can be modeled as a parameterized energy-based model (EBM) Eo(u, w,c) which characterizes the distribution p(u, wc) of u and w conditioned on conditions c by the correspondence"}, {"title": "3.2 Prior Reweighting", "content": "Motivation. In physical systems control, a critical challenge lies in obtaining control sequences superior to those in training datasets, which often deviate significantly from achieving the optimal control objective. Although guidance of"}, {"title": "4 Experiments", "content": "In this section, we aim to answer the following questions: (1) Can DiffPhyCon present superiority over traditional, supervised learning, and reinforcement learning methods for physical systems control? (2) Does the proposed prior reweighting technique help achieve better control objectives? (3) Could answers to (1) and (2) be generalized to more challenging partial observation or partial control scenarios? To answer these questions, we conduct experiments on the 1D Burgers' equation and 2D jellyfish movement control problem, both of which are vital and challenging.\nThe following state-of-the-art control methods are selected as baselines. For 1D Burgers' equation, we use (1) the classical and widely used control algorithm Proportional-Integral-Derivative (PID) [33] interacting with our trained surrogate model of the solver; (2) Supervised Learning method (SL) [23]; RL methods including (3) Soft Actor-Critic (SAC) [15] with offline and surrogate-solver versions; (4) Behaviour Cloning (BC) [50]; and (5) Behavior Proximal Policy Optimization (BPPO) [75]. Specifically, the surrogate-solver version of SAC interacts with our trained surrogate model of the solver, while the offline version only uses given data. BC and BPPO are also in offline version. For 2D jellyfish movement control, baselines include SL, SAC (offline), SAC (surrogate-solver), BC, BPPO, and an additional classical multi-input multi-output algorithm Model Predictive Control (MPC) [59]. PID is inapplicable to this data-driven task. Detailed descriptions of baselines are provided in Appendix F and Appendix G. We provide the code to reproduce the result for our method and baselines here.\n4.1 1D Burgers' Equation Control\nExperiment settings. The Burgers' equation is a governing law occurring in various physical systems. We consider the 1D Burgers' equation with the Dirichlet boundary condition and external force w(t, x), which is also studied in [23, 43].\n{ \ndu/dt+u du/dx = \u03bd d^2u/dx^2 + w(t, x) in [0, T] x \u03a9\nu(t, x) = 0 on [0, T] x \u2202\u03a9\nu(0, x) = u_0(x) in {t = 0} x \u03a9.\nHere v is the viscosity parameter, and up(x) is the initial condition. Subject to Eq. (16), given a target state ua(x), the objective of control is to minimize the control error Jactual between ur and ud, while constraining the energy cost Tenergy of the control sequence w(t, x):\nJactual := \\int_\u03a9 |u(T, x)-ud(x)|^2 dx, Tenergy := \\int_[0, T]\u00d7\u03a9|w(t, x)|^2 dtdx\nTo make the evaluation challenging, we select three different experiment settings that correspond to different real-life scenarios: partial observation, full control (PO-FC), full observation, partial control (FO-PC), and partial observation, partial control (PO-PC), which are elaborated on in Appendix C.2 and illustrated in Appendix B. These settings are challenging for classical control methods such as PID since they require capturing the long-range dependencies in the system dynamics. Note that the reported metrics in different settings are not directly comparable.\nIn this experiment, DiffPhyCon uses the guidance conditioning (Section 3.1) to optimize Jactual and explicit guidance to optimize the energy cost, which is elaborated in Appendix C.\nResults. In Table 1, we report results of the control error Jactual of different methods. It can be observed that DiffPhyCon delivers the best results compared to all baselines. Specifically, DiffPhyCon"}, {"title": "4.2 2D Jellyfish Movement Control", "content": "Experiment settings. This task is to control the movement of a flapping jellyfish with two wings in a 2D fluid field where fluid flows at a constant speed. The jellyfish is propelled by the fluid when its wings flap. Its moving speed and efficiency are determined by the mode of flapping. This task represents a common phenomenon in aquatic locomotion, which involves complex fluid-solid interactions and is an important source of inspiration for the design of underwater and aerial devices [27]. For this task, fluid dynamics follows the 2D incompressible Navier-Stokes Equation:\n\\frac{\\partial v}{\\partial t} + v \u22c5 \u2207v \u2212 v\u2207^2v + \u2207p = 0\n\u2207v = 0\n(v(0,x) = v_0 (x),\nwhere v represents the 2D velocity of the fluid, and p represents the pressure, constituting the PDE state u = (v, p). The initial velocity condition is vo(x) and the kinematic viscosity is v. We assume that each wing is rigid, so the jellyfish's boundary can be parameterized by the opening angle wt of wings. Long-term movement of jellyfish usually presents a periodic flapping mode. Consequently, the control objective is to maximize its average moving speed \u016b determined by the pressure of the"}, {"title": "5 Limitation", "content": "There are still several limitations of our DiffPhyCon that provide exciting opportunity for future works. Firstly, our approach is data-driven, so it is not guaranteed to achieve optimal solutions and also lacks estimation of the error gap between the generated control sequence and the optimal ones. Secondly, the training of DiffPhyCon is currently conducted in an offline fashion, without interaction with a ground-truth solver. Incorporating solvers into the training framework could facilitate real-time feedback, enabling the model to adapt dynamically to the environment and discover novel strategies and solutions. Furthermore, our proposed DiffPhyCon presently operates in an open-loop manner, as it does not consider real-time feedback from solvers. Integrating such feedback would empower the algorithm to adjust its control decisions based on the evolving state of the environment."}, {"title": "6 Conclusion", "content": "In this work, we have introduced DiffPhyCon, a novel methodology for controlling complex physical systems. It generates control sequences and state trajectories by jointly optimizing the generative energy and control objective. We further introduced prior reweighting to enable the discovery of control sequences that diverge significantly from training. Through comprehensive experiments, we demonstrated our method's superior performance compared to classical, deep learning, and reinforcement learning baselines in challenging physical systems control tasks. We discuss future work in Appendix L and state social impact in M."}, {"title": "A Additional Related Work", "content": "A.1 Physical Systems Simulation\nComplex physical systems simulation forms the foundation of systems control. While classical numerical techniques for simulating physical systems are renowned for their accuracy, they are often associated with significant computational expenses [42, 30]. Recently, neural network-based solvers show a significant advantage over classical solvers in accelerating simulations. They could be roughly divided into three primary classes: data-driven methods [34, 58, 49, 5, 69, 4, 29], Physics-Informed Neural Networks (PINNs) [54, 7, 67], and solver-in-the-loop methods [63, 65]. Most of them use an iterative horizontal prediction framework. Instead, we treat the system trajectory as a whole variable and use diffusion models to learn an explicit simulator conditioned on control sequences. A notable work is by [6], which introduces diffusion models for temporal forecasting. While both our work and [6]'s employ diffusion models, we tackle a different task of physical system control. Furthermore, we incorporate the control objective into the inference and introduce prior reweighting to tune the influence of the prior.\nA.2 Physical Systems Control\nFor physical systems whose dynamics are described by PDEs, the adjoint methods [36, 39, 52] have been the most widely used approach for system control in the last decades. It is accurate but computationally expensive. Deep learning-based methods have emerged as a powerful tool for modeling physical systems' dynamics. Supervised learning (SL) [21, 23] trains parameterized models to directly optimize control using backpropagation through time over the entire trajectory. For example, [21] proposes a hierarchical predictor-corrector scheme to control complex nonlinear physical systems over long time frames. A more recent work proposed by [23] designs two stages which respectively learn the solution operator and search for optimal control. Different from these methods, we do not use the surrogate model, and learn both state trajectories and control sequences in an integrated way. Reinforcement learning (RL) [14, 47, 53] treats control signals as actions and learns policies to make sequential decisions. Particularly in the field of fluid dynamics [31], reinforcement learning has been applied to a multitude of specific problems including drag reduction [53, 13], conjugate heat transfer [3, 16] and swimming [46, 64]. But they implicitly consider physics information and sequentially make decisions. In contrast, we generalize the entire trajectories, which results in a global optimization with consideration of physical information learned by models. Recently, PINNs are also incorporated in PDE control [43], but they require an explicit form of PDE dynamics, while our method is data-driven and can deal with a broader range of complex physical system control problems without explicit PDE dynamics.\nA.3 Diffusion Models\nDiffusion models [18] have significantly advanced in applications such as image and text generation [10, 45], inverse design [70, 66], inverse problem [22], physical simulation [6, 51], and decision-making [26, 1, 17]. In particular, recent progress in robot control shows that diffusion models have significant advantages over existing reinforcement learning methods for action planning [8, 71]. Generating diverse yet consistent samples poses a challenge. For diversity, methods [38, 2, 73, 12] that integrate score estimates from various models have been effective. For consistency, guidance diffusion techniques [10, 19] have been utilized to generate condition-specific samples. Our approach differs by flattening the joint distribution to achieve better control by slightly expanding beyond the prior distribution range."}, {"title": "B 1D Burgers' Equation Visualization", "content": "We present more visualization results of our method and baselines under three settings: FOPC, POFC, and POPC in Figure 6, Figure 7 and Figure 8, respectively. Under each setting, we present the results of five randomly selected samples from the test dataset. The goal of control is to make the final state \u0438\u0442 (T = 10) close to the target state."}, {"title": "C Additional Details for 1D Burgers' Equation Control", "content": "C.1 Data Generation\nWe use the finite difference method (called solver or ground-truth solver in the following) to generate the training data for the 1D Burgers' equation. Specifically, the initial value uo(x) and the control sequence w(t, x) are both randomly generated, and then the states u(t, x) are numerically computed using the solver.\nIn the numerical simulation (using the ground-truth solver), a domain of x = [0, 1], t = [0, 1] is simulated. The space is discretized into 128 grids and time into 10000 steps. However, in the dataset, only 10 time stamps are stored. For the control sequence w, its refreshing rate is 0.1-1, i.e., w(t,x), t \u2208 [0.1k, 0.1(k+1)], k \u2208 {0, ..,9} does not change with t. Therefore, the data size of each trajectory is [11, 128] for the state u and [10, 128] for the control w.\nIn all settings, the initial value u(0,x) is a superposition of two Gaussian functions u(0,x) = \\sum_{i=1}^{2} a_{i} e^{\\frac{-(x-b_{i})^{2}}{2\\sigma_{i}^{2}}}, where ai, bi, \u03c3i are all randomly sampled from uniform distributions: a1 ~ U(0,2), a2 ~ U(-2,0), b1 ~ (0.2,0.4), b2 ~ (0.6,0.8), \u03c3\u03b9 ~ U(0.05, 0.15), \u03c3\u03b5 ~ U (0.05, 0.15). Similarly, the control sequence w(x, t) is also a superposition of 8 Gaussian functions\nw(t,x) = \\sum_{i=1}^{8}a_{i} e^{\\frac{-(x-b_{1,i})^{2}}{2\\sigma_{1,i}^{2}}} e^{\\frac{-(t-b_{2,i})^{2}}{2\\sigma_{2,i}^{2}}},\nwhere each parameter is independently generated as follows: b1,i ~ U(0,1), b2,i ~ U(0,1), 01,\u00bf ~ U (0.05, 0.2), \u03c32,i ~ U(0.05, 0.2), while a\u2081 ~ U(\u22121.5, 1.5) and for i \u2265 2, a\u00a1 ~ U(\u22121.5, 1.5) or 0 with equal probabilities. u(t, x), (t \u2260 0) is then numerically simulated (using the ground-truth solver) given u(0, x) and w(t, x) based on Eq. (16). The setting of the dataset generation is based on a previous work [23].\nWe generated 90000 trajectories for the training set and 50 for the testing set. Each trajectory takes up 32KB space and the size of the dataset sums up to 2GB.\nC.2 Experimental Setting\nDuring inference, alongside the control sequence w(t, x), our diffusion model generates states \u00b5(t, x), and some models produce surrogate states \u03bc(t, x) when feeding the control w(t, x) into the corresponding surrogate model. However, our reported evaluation metric Tactual is always computed by feeding the control w(t, x) into the ground truth numerical solver to get ug.t. (t, x) and computed following Eq. (17).\nFollowing are three different settings of our experiments.\nC.2.1 Partial Observation, Full Control\nIn realistic scenarios, the system is often unable to be observed completely. Generally speaking, it is impractical to place sensors everywhere in a system, so the ability of the model to learn from incomplete data is imperative. To evaluate this, we hide some parts of u in this setting and measure the Jactual of model control.\nSpecifically, u(t, x), x \u2208 [1, 2] is set to zero in the dataset during training and u\u2081(x), x \u2208 [1,2] is also set to zero during testing. In this partial observation setting \u03a9 = [1, 4] \u222a[1]. Since no information in the central space is ever known, the model does not know what will influence the control outcome of the unobserved states. Therefore, controlling the unobserved states is not a reasonable task and they are excluded from the evaluation metric.\nThis setting is particularly challenging not only because of the uncertainty introduced by the unobserved states but also the generation of the control in the central locations that implicitly affect the controlled u at x \u2208 \u03a9.\nC.2.2 Full Observation, Partial Control\nThis is another setting of practical relevance, where only a fraction of the system can be controlled. The control sequence is enforced to be zero in the central locations of x \u2208 [1, 4]. \u03a9 is still [0, 1], and I is evaluated on all of the observed states, though.\nSome modifications to the dataset should be mentioned. The generation of the data involves first generating w as before, followed by setting the central of w to zero. To compensate for the decreased control intensity so that the magnitude of u can be roughly comparable to the full control setting, we double the magnitude of w. During the evaluation, the output control sequence is also post-processed to be zero in x \u2208 [1,2].\nIt is worth noting that in this setting, even when the control energy is not limited at all, it is still challenging to find a perfect control since the model has to learn how to indirectly impose control on the central locations.\nC.2.3 Partial Observation, Partial Control\nThe final setting is the combination of the previous two settings. Only \u03a9 = [0, 1] \u222a [1, 1] is observed, controlled and evaluated.\nIt is worth noting that some models require accessing the current state to produce output. If the model interacts with the ground truth solver instead of a surrogate model, then the result would be unfairly good since the information of the unobserved states is leaked through the interaction.\nC.3 Model\nSince the training of models \u20ac4 \u2248 \u2207w log p(w) and e\u0473 \u2248 Vu,w log p(u, w) are essentially the same and the latter model is exactly DiffPhyCon-lite, we will introduce DiffPhyCon-lite first.\nC.3.1 DiffPhyCon-lite\nIn general, DiffPhyCon-lite follows the formulation of [18] which is also described in the main text. The data of u and w is fed in as images of size (Nt, Nx) where Nt is the number of time steps (11 and 10 respectively) and Nx is the spatial grids (128). Since the two Nts for u and w are inconsistent, we zero-pad them into the size of 16. Then, u and w are stacked as two channels and fed into the 2D DDPM model.\nA 2D UNet \u20ac is used to learn to predict \u20ac. It is structured into three main components: the downsampling encoder, the central module, and the upsampling decoder. The downsampling encoder is made up of four layers, each layer consisting of two ResNet blocks, one linear Attention block, and one downsampling convolution block. The central module also consists of two ResNet blocks and one linear Attention block. Each upsampling layer is the same as the downsampling layer except the downsampling block is replaced by the upsampling convolution block.\nIn our experiments, we found that the control result is best when learning the conditional probability distribution of p(W[0,T\u22121], U[1,T\u22121] | uo, uT) In summary, e\u0473 takes in the current trajectory u, control w, step k, uo and ur as input, and predicts the noise of u and w. Note that it is not trained to predict uo and ur which are used as a condition, but there are still model outputs at the corresponding locations for the data shape consistency across different design choices of DiffPhyCon-lite. The hyperparameters in different settings are listed in Table 4.\nC.3.2 DiffPhyCon\nIn terms of implementation, DiffPhyCon is simply adding \u20ac$(w) to \u20ac\u03b8(u, w) during inference as shown in Section 3.2, where e\u0189 is the output of the denoising network in DiffPhyCon-lite while ef is a new denoising network that is trained to generate w following the dataset distribution. Therefore, we only describe the model of e here.\n\u20ac takes input of w, k as in the standard DDPM and uo, ur as guidance conditioning. The output of \u20ac(w) is of the same shape as w, so it can be treated as a network learning to sample from p(w) := \u222b p(u, w)du The output of e$(w) at the locations of u is thus filled with zeros. The model hyperparameters are also listed in Table 4."}, {"title": "D Jellyfish Movement Dataset", "content": "We use the Lily-Pad simulator [68] to generate the Jellyfish Movement dataset, which serves as a benchmark for physical system control research and also the dataset for our 2D evaluation task. Lily-Pad adopts the Immersed Boundary Method (IBM) [41] to simulate fluid-solid dynamics. The resolution of the 2D flow field is set to be 128 x 128. The flow field is assumed to be boundless in Lily-Pad. The head of the jellyfish is fixed at (25.6, 64). Its two wings are represented by two identical ellipses, where the ratio between the shorter axis and the longer axis is 0.15. At each moment, the two wings are symmetric about the central horizontal line y = 64. For each wing, we sample M = 20 points along the wing to represent the boundary of the wing. The opening angle of the wings is defined as the angle between the longer axis of the upper wing and the horizontal line. It acts as the control sequence w in a 2D jellyfish control experiment.\nEach trajectory starts from the largest opening angle and follows a cosine curve periodically with period T' = 200. Trajectories differ in initial angle, angle amplitude, and phase ratio 7 (the ratio between the closing duration and a whole pitching duration). For each trajectory, the initial angle wo is generated as follows: first, sample a random angle, called mean angle w(m) \u2208 [20\u00b0, 40\u00b0], then sample a random angle amplitude w(a) \u2208 [10\u00b0, min(w(m), 60\u00b0 \u2013 w(m))]. The initial wo is set as Wo = w(m) + w(a). The phase ratio 7 is randomly sampled from [0.2, 0.8]. The opening angle wt of step t decreases from w(m) + w(a) to w(m) \u2013 w(a) as t grows from 0 to TT'; then wt increases from w(m) \u2013 w(a) to w(m) + w(a) as t grows from \u03c4T' to T'. Afterwards, w\u0165 varies periodically for t > T'. The range of w\u2081 is [w(m) \u2013 w(a), w(m) + w(a)] \u2282 [10\u00b0, 60\u00b0]. For each trajectory, we simulate for 600 simulation steps, i.e., 3 periods. To save space, we only save the piece of trajectory from T' = 200 to 3T' = 600 steps with step size 10 because the simulation from t = 0 to T' = 200 is for initialization of the flow field. Then each trajectory is saved as a T = (600 \u2013 200)/10 = 40 steps long sequence. An example of the simulated fluid field and the corresponding curve of opening angles are shown in Figure 9.\nBesides the positions of the boundary points of wings and the opening angles w, we also use another kind of image-like representation of the boundaries of wings as this representation contains spatial information that can be more effectively learned along with physical states (fluid field) by convolution neural networks. For each trajectory, this image-like boundary representation is compatible with physical states in shape. At each time step, boundaries of two wings are merged and then represented as a tensor of shape [3, 64, 64], where it has three features for each grid cell: a binary mask indicating whether the cell is inside a boundary (denoted by 1) or in the fluid (denoted by 0), and a relative position (Ax, \u2206y) between the cell center to the closest point on the boundary. For each trajectory, we save system states, opening angles, boundary points, boundary masks and offsets, and force data. They are specified as:\n\u2022 system states u: shape [T, 3, 64, 64]. For each step, we save the states of the fluid field consisting of velocity in x and y directions and pressure. To save space, we downsample the resolution from 128 \u00d7 128 to 64 x 64.\n - velocity: [T, 2, 64, 64]."}, {"title": "E Additional Details for 2D Jellyfish Movement Control", "content": "E.1 Dataset Preparation\nBased on our generated dataset in Appendix D, we prepare training samples for the 2D jellyfish movement control task as follows. We use sliding time windows that contain T = 20 successive time steps of states and boundaries as a sample, which corresponds to T' = 200 original simulation steps and constitutes exactly a period of wing movement. In this way, each trajectory can produce 20 samples. Therefore, we get 6 million training samples in total. In each training sample, the initial and the final time steps share the same opening angle due to periodicity, which serves as the conditions for control. For each test trajectory, we select the opening angle of the jellyfish in the initial time and the initial states as the control condition for both the initial and final time and state initial condition.\nE.2 Experimental Setting\nE.2.1 Full Observation\nIn this setting, we assume all the states of the fluid field are observable. That is, both the velocity of x and y directions and pressure are available in all the time steps of the training dataset and the initial time of the testing dataset.\nE.2.2 Partial Observation\nIn this setting, we assume only partial states are observed. A typical scenario in fluid simulation and control is that we can only observe pressure data while the velocity data is not easy to access. That is, only pressure is available in all the time steps of the training samples and the initial time of the testing samples, hence the state tensor is of shape [T, 1, 64, 64]. Notice that even if only pressure is available, we can still compute the force of fluid on the jellyfish and consequently the control objective because force is fully determined by the shape of the jellyfish and pressure. The challenge of this partial observation setting is that the velocity variable v is missing in Eq. (18), which makes the traditional numerical solver no longer applicable to solve this physical system control problem. However, this challenge could be well addressed by our method since it could learn the relationship between control and pressure despite missing of the velocity data, and use the accessible control objective as guidance for flapping control.\nE.3 Model\nE.3.1 Architecture\nWe use a 3D U-Net as the backbone of our diffusion model, in both DiffPhyCon-lite and DiffPhyCon methods (detailed in the following subsection). In this paper, the architecture of the 3D U-net we employed is inspired by [20]. To better capture temporal conditional dependencies, we modify the previous space-only 3D convolution into space-time 3D convolut ion. Notably, we did not perform any scaling on the temporal dimension during downsampling or upsampling. Specifically, our U-net consists of three main modules: the downsampling encoder, the middle module, and the upsampling decoder. The downsampling encoder is composed of three layers, each incorporating two residual modules, one spatial attention module, one temporal attention module, and one downsampling module. The middle module consists of two residual modules, one spatial attention module, and one temporal attention module. Meanwhile, the upsampling decoder consists of four layers, each containing two residual modules, one spatial attention module, one temporal attention module, and one upsampling module. The input shape of our U-net is [batch size, frames, channels, height, width]. During convolution, the operation is performed on the [frames, height, width] dimensions. The output shape follows the same structure. Further details are provided in Table 5.\nE.3.2 DiffPhyCon-lite\nThe DiffPhyCon-lite method learns the denoising network of the joint distribution p(u, wc) where u is physical states, w is the opening angle, and the conditions c consist of the initial angle wo, the initial state up and the final angle w\u2081 = wo. We adopt the 3D U-Net as the backbone. To make the opening angle (of shape [T]), align with physical states (of shape [T, 3, 64, 64] in full observation setting and [T, 1, 64, 64] in partial observation setting) in shape, we expand the opening angle to shape [T, 1, 64, 64] along spatial dimension by value copy. Besides, we also adopt the boundary mask and offsets representation, whose shape is [T, 3, 64, 64], determined by the opening angles as an auxiliary model input because they contain explicit spatial features, which makes model learning more effective. Then states, boundary mask and offsets, and expanded opening angle are stacked"}, {"title": "F 1D Baselines", "content": "F.1 PID\nProportional Integral Derivative (PID) [33] control is a versatile and effective control method widely used in various real-world control scenarios. It operates by utilizing the difference (error) between the desired target and the current state of a system. PID control is often considered the go-to option for many control problems due to its simplicity and usefulness. However, despite its popularity, PID control does encounter certain challenges, such as parameter adaptation and limitations when applied to Single Input Single Output (SISO) systems. In our specific context, the 1D Burgers' Equation Control problem presents a Multiple Input Multiple Output (MIMO) control scenario, which makes it infeasible to directly employ PID control to regulate the Burgers' equation. Inspired by the early"}, {"title": "F.2 SAC", "content": "The Soft Actor-Critic (SAC) algorithm [15] is a cutting-edge reinforcement learning method. Conceptualized as an improvement over traditional Actor-Critic methods, SAC distinguishes itself by introducing an entropy regularization term into the loss function, which encourages the policy to explore more efficiently by maximizing both the expected cumulative reward and the entropy of the policy itself.\nCompared with Deep Deterministic Policy Gradient (DDPG) algorithm [35, 47], SAC's entropy regularization encourages more effective exploration and prevents early convergence to suboptimal policies, a limitation often seen with DDPG's deterministic approach. Additionally, SAC's twin Q-networks mitigate the overestimation bias that can affect DDPG's value updates, leading to more stable learning. The automatic tuning of the temperature parameter in SAC further simplifies the delicate balance between exploration and exploitation, reducing the need for meticulous hyperparameter adjustments. Consequently, these features render SAC generally more sample-efficient and robust, particularly in complex and continuous action spaces."}, {"title": "F.3 Supervised"}]}