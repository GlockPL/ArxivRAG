{"title": "CASESUMM: A Large-Scale Dataset for Long-Context Summarization from U.S. Supreme Court Opinions", "authors": ["Mourad Heddaya", "Kyle MacMillan", "Anup Malani", "Hongyuan Mei", "Chenhao Tan"], "abstract": "This paper introduces CASESUMM, a novel dataset for long-context summarization in the legal domain that addresses the need for longer and more complex datasets for summarization evaluation. We collect 25.6K U.S. Supreme Court (SCOTUS) opinions and their official summaries, known as \"syllabuses.\" Our dataset is the largest open legal case summarization dataset, and is the first to include summaries of SCOTUS decisions dating back to 1815.\nWe also present a comprehensive evaluation of LLM-generated summaries using both automatic metrics and expert human evaluation, revealing discrepancies between these assessment methods. Our evaluation shows Mistral 7b, a smaller open-source model, outperforms larger models on most automatic metrics and successfully generates syllabus-like summaries. In contrast, human expert annotators indicate that Mistral summaries contain hallucinations. The annotators consistently rank GPT-4 summaries as clearer and exhibiting greater sensitivity and specificity. Further, we find that LLM-based evaluations are not more correlated with human evaluations than traditional automatic metrics. Furthermore, our analysis identifies specific hallucinations in generated summaries, including precedent citation errors and misrepresentations of case facts. These findings demonstrate the limitations of current automatic evaluation methods for legal summarization and highlight the critical role of human evaluation in assessing summary quality, particularly in complex, high-stakes domains.\nCASESUMM is available on HuggingFace.", "sections": [{"title": "1 Introduction", "content": "Although large language models (LLMs) are claimed to handle long contexts (GPT-4 Team, 2024; Bubeck et al., 2023; Claude Team, 2024), including summarizing very long inputs, how well they perform long-context summarization is an open question.\nEvaluating long-context summarization is challenging for several reasons. First, human ground-truth summaries are often not available (Cao et al., 2024; Chang et al., 2024). Moreover, it's unclear whether we should trust human abilities to even create ground-truth summaries. Second, what makes a good summary in one setting may not generalize to other settings. For example, what's relevant in a legal text is different than what's relevant in a novel. Lastly, identifying salient information in complex domains often requires expertise.\nWe address these challenges by introducing a new dataset where \u201cground-truth\" summaries are available and conducting a comprehensive human evaluation to benchmark existing models. In particular, we build CASESUMM, a legal case summarization dataset consisting of 25.6K U.S. Supreme Court cases and their official summaries, called syllabuses. Syllabuses are written by an attorney employed by the Court and approved by the Justices. The syllabus is therefore the gold standard for summarizing majority opinions, and ideal for evaluating other summaries of the opinion. We obtain the opinions from Public Resource Org's archive and extract syllabuses from the official opinions published in the U.S. Reporter and hosted by the Library of Congress. Our dataset is at least 25% larger, covers 3 times as many years (1815-2019), and is publicly available with fewer copyright restrictions than similar legal datasets (Fang et al., 2023; Trivedi et al., 2024), representing a rich resource for the research community.\nBeyond the legal domain, several datasets have been introduced to improve evaluation of long-context summarization (Kry\u015bci\u0144ski et al., 2022; Sharma et al., 2019; Eidelman, 2019; Huang et al., 2021). CASESUMM continues the trend of larger"}, {"title": "2 Related Work", "content": "Evaluation for summarization. ROUGE (Lin, 2004) has been the dominant summarization metric, despite criticism of its high lexical dependence (Schluter, 2017; Cohan and Goharian, 2016). Newer metrics like BERTScore (Zhang et al., 2019) and BARTScore (Yuan et al., 2021) aim to capture semantic similarities. However, automatic metrics often don't correlate well with human judgments (Yuan et al., 2021; Fabbri et al., 2021; Bhandari et al., 2020). We focus on high-stakes long-context summarization, showing the need for better metrics persists despite LLM progress. Chang et al. (2024) extended LLM-based evaluation to book-length summaries, but this approach doesn't consider how experts weigh the importance of including or omitting certain information in a summary, while also being slow and costly. Cao et al. (2024) developed a framework for characterizing LLM summaries of financial documents. Our work extends this research by evaluating and comparing model- and human-generated summaries in the legal domain. Addressing factual discrepancies in model-generated summaries, recent work has developed automatic methods for evaluating faithfulness in summarization (Krishna et al., 2023; Chang et al., 2024; Falke et al., 2019; Laban et al., 2022; Wang et al., 2020; Fabbri et al., 2022).\nNLP and summarization in the Legal Domain. Natural language processing has been applied to various legal tasks, including summarization (Bauer et al., 2023), discovery (Zou and Kanoulas, 2020), redaction (Garat and Wonsever, 2022), case outcome prediction (Medvedeva et al., 2023; Cui et al., 2023), and Bar Exam performance (Katz et al., 2023a). For comprehensive surveys of NLP in the legal domain, see Katz et al. (2023b) and Kapoor et al. (2024).\nDatasets in the legal domain. Our dataset is unique in providing U.S. Supreme Court opinions with syllabuses, unlike other datasets that lack syllabuses (Chalkidis et al., 2022; Henderson et al., 2022) or provide only ancillary data (Law, 2024). Fang et al. (2023) present Super-SCOTUS, a dataset of Supreme Court documents, including a subset of syllabus (scraped from online websites and not validated) and opinion pairs and highlight its contribution to political and social science research. In contrast to Super-SCOTUS, our CAS-ESUMM dataset consists of cleaned and carefully"}, {"title": "3 Dataset", "content": "When the Supreme Court resolves a case, it publishes a majority \"opinion\" announcing the outcome and reasoning for their decision. The Court will also disseminate a summary of the opinion called the \"syllabus\", which is written by an attorney employed by the Court and approved by the Justices. The syllabus must include the main elements of the opinion: the facts of the case, the procedural history, the legal question to be decided, and the answer to that question. Accurately summarizing each of these sections requires (1) understand sophisticated legal reasoning and (2) identify the most salient aspects of the case.\nAs one of the longest standing institutions in U.S. history, the Supreme Court has published thousands of opinions and syllabuses over the past 200 years. Looking at cases between 1815 and 2019, we collect 25.6K pairs of opinions and syllabuses for our dataset, to be available under a CC BY-NC 4.0 license.\nDataset construction We compile our dataset from multiple sources. Opinions published in U.S. Reports Volume 15-546 (years 1815-2005) and Volumes 546-591 (2005 through Trump v. Vance (2019)) are obtained from Public Resource Org's online archive (Public Resource Org, 2024) and the Super-SCOTUS data set (Fang et al., 2023), respectively. We extract syllabuses from PDFs of the opinions hosted on the Library of Congress's website (Library of Congress, 2024).\nExtracting syllabuses from the original PDFs is challenging for several reasons. First, identifying the start and end of the syllabus is complicated because the formatting and style of SCOTUS decisions have changed over time. Low quality scans"}, {"title": "4 Experiment Setup", "content": "In this section, we introduce our summarization task setup and evaluation strategies.\n4.1 Data and Modeling\nData preprocessing and splits. We use syllabuses as a supervision signal in our summarization modeling experiment and as reference summaries for evaluating the human and model-generated summaries.\nAs discussed in \u00a73, the substance and style of syllabuses have changed over time. Therefore, the supervision signal has changed over time. The motivating use case in our summarization task is a legal professional conduction research. For such a professional, while concision has value, comprehensiveness is more valuable. By manually studying summaries, we determine that more comprehensive syllabuses begin with a summary of the facts of the case, followed by a new section-marked by the text \u201cHeld:\u201d\u2014containing details about the issues, analyses, and conclusions that the opinion commented on. Modern syllabuses consistently adhere to this structure.\nTherefore, we filter our dataset to include only opinion/syllabus pairs where the syllabus contains the pattern \"Held:\u201d. We call this subset of the dataset \"structured\". We find that the length of structured syllabuses is more strongly correlated with the length of their respective opinions (r = 0.65) than the length of unstructured syllabuses is with the length of their opinions (r = 0.46). Furthermore, structured syllabuses are on average 2.5x longer than the unstructured syllabuses. Overall, the structured dataset contains 6,683 case/syllabus pairs. We split these into a training set (n=5,455), validation set (n=606), and test set (n=622).\nModeling. We pursue and test two approaches for completing our legal case summarization task. The first approach is zero-shot prompting with proprietary and with open-source LLMs. The propriety LLM we employ is GPT-4 Turbo (gpt-4-1106-preview) (GPT-4 Team, 2024). The open-source LLM we employ is Mistral 7b Instruct (Mistral-7b-Instruct-v0.1) (Jiang et al., 2023). The opinions in our dataset have 4,983 tokens on average, and the syllabuses average 755 tokens. The second approach is instruction fine-tuning (Wei et al., 2021) the open-source model, Mistral 7b Instruct, using the syllabuses in our training data set. We will refer to models used in a zero-shot setting by model name: Mistral Base and GPT-4, and to the fine-tuned Mistral model as Mistral FT."}, {"title": "4.2 Evaluation strategies", "content": "\u201cControl group\u201d summaries. We benchmark our three machine-generated summaries (Mistral Base, Mistral FT, and GPT-4 Turbo) along with two additional human-generated sources for purposes of having a control group of human-written summaries not explicitly intended to mimic syllabuses. First, we collect public Oyez summaries from the Super-SCOTUS dataset (Fang et al., 2023). Oyez summaries are composed of three sections: Facts of the Case, Question, and Conclusion. Second, we collect Westlaw's commercial summaries of cases via their online interface. Because manual download is slow, our sample size for Westlaw downloads was smaller: whereas our test set has 622 instances of model-generated summaries and Oyez summaries, we have 156 Westlaw summaries.\nAutomatic Evaluation. Following recent work on summarization (Koh et al., 2022), we use ROUGE and BERTScore (Lin, 2004; Zhang et al., 2019) as our automated metrics for evaluating generated summaries against the reference syllabuses. With this, we assess the relevance of the summaries. We breakdown each of the metrics by their precision, recall, and F1-score, highlighting how models balance trade-offs between coverage and concision.\nWe also experimented with BARTscore (Yuan et al., 2021) (see Appendix B.3) but exclude it from our main analysis due to its sensitivity to whether text is in- or out-of- distribution relative to the scoring model. Since we compare Mistral after fine-tuning on syllabuses to models that were not fine-tuned, we expect unreliable results.\nTo further characterize the summaries, we compare the summaries based on compression rate, defined as the number of words in a syllabus over the number of words in an opinion, and the correlation between opinion lengths and summary lengths. We use compression as a measure of brevity and correlation as a measure of how responsive summaries are to changes in the amount of content in the opinions.\nHuman Evaluation. For the human evaluation, we recruited and paid second- and third-year law students to read several opinions and 5 summaries of each opinion (Mistral FT7, GPT-4, official syllabus, Westlaw, and Oyez). We asked students to rank each summary (from 1 to 5) on several metrics: did the summary contain all relevant information from the opinion (sensitivity), did it exclude irrelevant information (specificity), was the summary clear (clarity), and did the summary have a style suggesting it was written by an experienced attorney? (style) Finally, we asked students to report the number of facts in the summary that were false based on their reading of the opinion (error). Students were not told the source of each summary. See Appendix C for additional details on the annotation interface and procedure.\nIn total, students read 57 opinions. Our sample of opinions and summaries included 33 unique cases, and the median student read 5 cases. Given that we ask students to rank opinions from 1 to 5 (implying a mean of 3 and variance of 2), our minimum detectable effect, with 95% confidence and 80% power, was 0.52 rank points.\nExperimenting with LLM-based evaluations. Metrics like ROUGE and BERTScore provide a baseline for assessing lexical and semantic alignment between a candidate and reference text. However, they can miss deeper qualities that humans"}, {"title": "5 Results", "content": "Our results indicate consistencies and discrepancies in the outcomes of automatic and human evaluations. On the one hand, model-generated summaries largely outperform the control human-written summaries on automatic measures of relevance, while also matching or exceeding them in our human evaluation. On the other hand, automatic metrics prefer Mistral FT summaries over GPT-4 ones, whereas expert humans most commonly rank GPT-4 over Mistral FT. Furthermore, we show that all summaries are shorter than their reference syllabuses and do not correlate as strongly with opinion lengths. Despite this, humans prefer GPT-4 summaries, revealing that its summaries may represent a more desirable trade-off between concision and comprehensiveness.\n5.1 Automated Evaluation Favors Fine-tuned Mistral Summaries\nWe start by looking at the results in Table 2 of automatic evaluation between summaries and official syllabuses for the three generated summaries (Mistral Base, Mistral FT, and GPT-4) and for two human summaries. Overall, we find that fine-tuning Mistral is particularly effective at improving the recall scores across all the metrics: ROUGE recall scores increase by an average of 21 points, BERTScore recall by 15 points. However, effects of fine-tuning on precision are weaker and more mixed. Perhaps fine-tuning sacrifices brevity for inclusion of more words in a syllabus, i.e., improves the sensitivity of summaries at a cost to specificity.\nControl summaries help highlight effects of style differences on automatic metrics. By comparing against the two control human-written summaries, we can clearly see that Westlaw is an outlier. While GPT-4 and Mistral FT scores mostly resemble Oyez, Westlaw's recall scores are particularly low, only surpassing Mistral base. This poor performance on recall, but strong performance on precision, may be a product of how short those summaries are.\n5.2 Summaries do not Scale with Opinion Length as much as Official Syllabuses\nA unique aspect of CASESUMM is that it includes SCOTUS cases dating back to more than two centuries ago. This breadth enables researchers to investigate summaries from many different angles."}, {"title": "5.3 Human Evaluation Disagrees with Automatic Evaluation", "content": "The results of our human evaluation, presented in Figure 3 are distinctly different than those of our automatic evaluation. Whereas under automatic evaluation, Mistral FT outperforms other models as well as the control human-written summaries, we find that humans most commonly prefer GPT-4 summaries. GPT-4 particularly excels on clarity, a crucial yet difficult to measure desideratum for the summarization task. Nonetheless, Mistral FT remains an above-average performer, successfully matching the original opinion syllabuses on every dimension except, importantly, number of errors. Evaluators report that roughly 20% of Mistral FT summaries have at least 1 factual error, with a total of 10 errors identified across all evaluations. However, we see that these factual errors, or hallucina-"}, {"title": "5.4 Error Analysis", "content": "Mistral hallucinates more conspicuously than GPT-4. We conduct further analysis of each summary flagged as containing factual errors according to the participants in the human evaluation. We compare each such summary to the original opinion to identify specific factual errors. Recent work has often referred to errors of this type as \u201challucinations\" (Huang et al., 2023).\nTable 5 presents example errors. Fine-tuned mistral contained the most errors in its summaries. Furthermore, these errors were more egregious than any produced in the GPT-4 Turbo summaries. These errors include simple factual errors (examples 1), incorrect citations (example 2), temporal understanding errors (example 3), as well as procedural history outcome errors (examples 4).\nIn contrast, GPT-4 Turbo errs in a more subtle way, failing to properly convey the legal analysis presented in the opinion (example 5) or misrepresenting background details (example 6). While the opinion indeed reverses the judgement of the court below, it does not reject its reasoning. Rather, the ruling is reversed due to a superseding issue of constitutionality. The summary generated by GPT-4 Turbo is thus incorrect in its characterization of the Supreme Courts decision.\nLexical variation. We define lexical variation as the percentage of unique words in the summary not present in the opinion and consider it a measure of summary style. Mirroring our comparison of compression rates, syllabuses are shown to exhibit the lowest percentage of lexical variation from the original opinion. Surprisingly, the fine-tuned Mistral summaries have the highest average percentage of lexical variation at 41.7%, even surpassing those written by Oyez (37.9%). This is unexpected because Mistral FT is trained on legal syllabuses, while Oyez summaries are written for a general audience and might borrow less from the opinion. The high lexical variation rate of Mistral FT may be related to its higher rate of factual errors."}, {"title": "6 Conclusion", "content": "This paper introduces CASESUMM, a novel dataset for long-context summarization in the legal domain, comprising 25.6K U.S. Supreme Court opinions and their official syllabuses. Our comprehensive evaluation of LLM-generated summaries, using both automatic metrics and expert human evaluation, reveals discrepancies between these assessment methods. While fine-tuned Mistral 7b outperforms larger models on automatic metrics, human experts rank GPT-4 summaries higher in clarity and accuracy. Our human evaluation also showed that GPT-4 summaries often outperformed human-written summaries, including official syllabuses and professional services, in several metrics except factual correctness. LLM-based evaluation, such as G-Eval, may be a promising direction for reference-free evaluation but our results show that G-Eval does not correlate with human judgments better than traditional automatic metrics. Our findings highlight the limitations of current automatic evaluation methods for legal summarization and underscore the importance of human evaluation in assessing summary quality, particularly in complex, high-stakes domains like law. This work contributes to the ongoing dialogue about evaluation methodologies in NLP and opens avenues for research in legal text summarization."}, {"title": "7 Limitations", "content": "First, the sample size of our human evaluation limits the conclusions we can draw. Second, while we are able to offer insight into the value of fine-tuning, at least with respect to the open-source Mistral model, we are unable to estimate the value of prompt-engineering even the GPT-4 model because we do not have a natural benchmark, non-optimized prompt for that model. A related weakness is that our evaluation of fine-tuning Mistral does not tell us the value of fine-tuning other models, such as GPT-4. It is possible that the benefit to fine-tuning the latter may be lower than the former because GPT-4 is trained on more data and has far more estimated parameters. Third, we experiment with one LLM-based evaluation framework. While G-Eval is commonly cited and used, other LLM-based approaches could yield different results. Finally, while we demonstrate through a manual evaluation that our PDF extraction procedure is largely accurate (96%), it is not perfect. A fraction of syllabuses, particularly those extracted from low-quality scans from SCOTUS opinions in the early 1800s, may not be fully correct."}]}