{"title": "Learning to Assist Humans without Inferring Rewards", "authors": ["Vivek Myers", "Evan Ellis", "Sergey Levine", "Benjamin Eysenbach", "Anca Dragan"], "abstract": "Assistive agents should make humans' lives easier. Classically, such assistance\nis studied through the lens of inverse reinforcement learning, where an assistive\nagent (e.g., a chatbot, a robot) infers a human's intention and then selects actions\nto help the human reach that goal. This approach requires inferring intentions,\nwhich can be difficult in high-dimensional settings. We build upon prior work\nthat studies assistance through the lens of empowerment: an assistive agent aims\nto maximize the influence of the human's actions such that they exert a greater\ncontrol over the environmental outcomes and can solve tasks in fewer steps. We\nlift the major limitation of prior work in this area \u2013 scalability to high-dimensional\nsettings - with contrastive successor representations. We formally prove that these\nrepresentations estimate a similar notion of empowerment to that studied by prior\nwork and provide a ready-made mechanism for optimizing it. Empirically, our\nproposed method outperforms prior methods on synthetic benchmarks, and scales\nto Overcooked, a cooperative game setting. Theoretically, our work connects ideas\nfrom information theory, neuroscience, and reinforcement learning, and charts a\npath for representations to play a critical role in solving assistive problems.", "sections": [{"title": "1 Introduction", "content": "AI agents deployed in the real world should be helpful to humans. When we know the utility\nfunction of the humans an agent could interact with, we can directly train assistive agents through\nreinforcement learning with the known human objective as the agent's reward. In practice, agents\nrarely have direct access to a scalar reward corresponding to human preferences (if such a consistent\nmodel even exists) [1], and must infer them from human behavior [2, 3]. This inference can be\nchallenging, as humans may act suboptimally with respect to their stated goals, not know their\ngoals, or have changing preferences [4]. Optimizing a misspecified reward function can have poor\nconsequences [5].\nAn alternative paradigm for assistance is to train agents that are intrinsically motivated to assist\nhumans, rather than directly optimizing a model of their preferences. An analogy can be drawn to\na parent raising a child. A good parent will empower the child to make impactful decisions and\nflourish, rather than proscribing an \"optimal\" outcome for the child. Likewise, AI agents might\nseek to empower the human agents they interact with, maximizing their capacity to change the\nenvironment [6]. In practice, concrete notions of empowerment can be difficult to optimize as an\nobjective, requiring extensive modeling assumptions that don't scale well to the high-dimensional\nsettings deep reinforcement learning agents are deployed in.\nWhat is a good intrinsic objective for assisting humans that doesn't require these assumptions? We\npropose a notion of assistance based on maximizing the influence of the human's actions on the"}, {"title": "2 Related Work", "content": "Our approach broadly connects ideas from contrastive contrastive representation learning and intrinsic\nmotivation to the problem of assisting humans.\nAssistive Agents. There are two lines of past work on assistive agents that are most relevant."}, {"title": "3 The Information Geometry of Empowerment", "content": "We will first state a general notion of an assistive setting, then show how an empowerment objective\nbased on learned successor representations can be used to assist humans without making assumptions\nabout the human following an underlying reward function. In Section 5, we provide empirical\nevidence supporting these claims."}, {"title": "3.1 Preliminaries", "content": "Formally, we adapt the notation of Hadfield-Menell et al. [2], and assume a \u201crobot\u201d (R) and \u201chuman\u201d\n(H) policy are training together in an MDP $M = (S, A_H, A_R, R, P, \\gamma)$. The states s consist of the\njoint states of the robot and the human; we do not have separate observations for the human and robot.\nAt any state $s \\in S$, the robot policy selects actions distributed according to $\\pi_\u03c1(a_R | s)$ for $a_R \\in A_R$\nand the human selects actions from $\\pi_\u043d(a_H | s)$ for $a_H \\in A_\u043d$. The transition dynamics are defined\nby a distribution $P(s' | s, a_H, a_R)$ over the next state $s' \\in S$ given the current state $s \\in S$ and actions\n$\u0430\u043d \\in A_H$ and $ar \\in A_R$, as well as an initial state distribution $P(80)$. For notational convenience,\nwe will additionally define random variables $st$ to represent the state at time t, and $a ~\\\u3160R(\u2022 | 5t)$\nand $a ~\\\u3160H(\u2022 | st)$ to represent the human and robot actions at time t, respectively.\nH\nEmpowerment. Our work builds on a long line of prior methods that use information theoretic\nobjectives for RL. Specifically, we adopt empowerment as an objective for training an assistive\nagent [6, 42, 43]. This section provides the mathematical foundations for empowerment, as developed"}, {"title": "3.2 Intuition and Geometry of Empowerment", "content": "Intuitively, the assistive agent should aim to maximize the number of future outcomes. We will\nmathematically quantify this in terms of the discounted state occupancy measure, $p\" (s+ | s)$.\nIntuitively, an agent has a large empowerment if the future states for one action are very different\nfrom the future actions after taking a different action; i.e., when $p(at = a1;s+ | st)$ is quite different\nfrom $p(at | 82; 5+ | st)$ for actions a1 \u2260 a2. The mutual information (Eq. (1)) quantifies this degree\nof control: $I(at; s+ | St)$.\nOne way of understanding this mutual information is through information geometry [47, 48, 48, 49].\nFor a fixed current state st, assistant policy \u3160R and human policy \u3160\u043d, each potential action at that\nthe human takes induces a different distribution over future states: $pr,\u3160H (5+ | St, at)$. We can\nthink about the set of these possible distributions: ${pr,\u3160H(s+ | St, at) | at \u2208 A}$. Figure 2 (Left)\nvisualizes this distribution on a probability simplex for 6 choices of action at. If we look at any\npossible distribution over actions, then this set of possible future distributions becomes a polytope\n(see orange polygon in Fig. 2 (Center)).\nIntuitively, the mutual information $I(at; s+ | st)$ used to define our empowerment objective corre-\nsponds to the size or volume of this state marginal polytope. This intuition can be formalized by\nusing results from information geometry [50-52]. The human policy \u3160\u043d(at | st) places probability\nmass on the different points in Figure 2 (Center). Maximizing the mutual information corresponds\nto \"picking out\u201d the state distributions that are maximally spread apart (see probabilities in Fig. 2\n(Center)). To make this formal, define\n$p(5+ | St) \u0395\u03c0(at|st) [p(5+ | St, at)]$ \n(3)\nas the average state distribution from taking the human's actions (see green square in Fig. 2 (Center)).\nRemark 3.1. Mutual information corresponds to the distance between the average state distribution\n(Eq. 3) and the furthest achievable state distributions:\n$I(at;s+ | st) = max DKL (p(at; 5+ | st) || p(5+ | St)) = dmax.$\nat\n(4)\nThis distance is visualized as the black lines in Fig. 2. When we talk about the \"size\" of the state\nmarginal polytope, we are specifically referring to the length of these black lines (as measured with a\nKL divergence).\nThis sort of mutual information is a way for measuring the degree of control that an agent exerts\non an environment. This measure is well defined for any agent/policy; that agent need not be\nmaximizing mutual information, and could instead be maximizing some arbitrary reward function.\nThis point is important in our setting: this means that the assistive agent can estimate and maximize\nthe empowerment of the human user without having to infer what reward function the human is trying\nto maximize.\nFinally, we come back to our empowerment objective, which is a discounted sum of the mutual\ninformation terms that we have been analyzing above. This empowerment objective says that the\nhuman is more empowered when this set has a larger size-i.e., the human can visit a wider range of\nfuture state (distributions). The empowerment objective says that the assistive agent should act to try\nto maximize the size of this polytope. Importantly, this maximization problem is done sequentially:\nthe assistive agent wants the size of this polytope to be large both at the current state and at future\nstates; the human's actions should exert a high degree of influence over the future outcomes both now\nand in the future. Thus, our overall objective looks at a sum of these mutual informations.\nNot only does this analysis provides a geometric picture for what empowerment is doing, it also lays\nthe groundwork for formally relating empowerment to reward."}, {"title": "3.3 Relating Empowerment to Reward", "content": "In this section we take aim at the question: when humans are well-modeled as optimizing a reward\nfunction, when does maximizing empowerment help humans maximize their rewards? Answering\nthis question is important because for empowerment to be a safe and effective assistive objective,\nit should enable the human to better achieve their goals. We show that under certain assumptions,"}, {"title": "4 Estimating and Maximizing Empowerment with Contrastive Representations", "content": "Directly computing Eq. (2) would require access to the human policy, which we don't have. There-\nfore, we want a tractable estimation that still performs well in large environments which are more\ndifficult to model due to the exponentially increasing set of possible future states. To better-estimate\nempowerment, we learn contrastive representations that encode information about which future states\nare likely to be reached from the current state. These contrastive representations learn to model\nmutual information between the current state, action, and future state, which we then use to compute\nthe empowerment objective."}, {"title": "4.1 Estimating Empowerment", "content": "To estimate this empowerment objective, we need a way of learning the probability ratio inside the\nexpectation. Prior methods such as Du et al. [6] and Salge et al. [42] rollout possible future states"}, {"title": "5 Experiments", "content": "We seek to answer two questions with our experiments. First, does our approach enable assistance in\nstandard cooperation benchmarks? Second, does our approach scale to harder benchmarks where\nprior methods fail?\nOur experiments will use two benchmarks designed by prior work to study assistance: the obstacle\ngridworld [6] and Overcooked [10]. Our main baseline is AvE [6], a prior empowerment-based\nmethod. Our conjecture is that both methods will perform well on the lower-dimensional grid-\nworld task, and that our method will scale more gracefully to the higher dimensional Overcooked\nenvironment. We will also compare against a na\u00efve baseline where the assistive agent acts randomly."}, {"title": "5.1 Do contrastive successor representations effectively estimate empowerment?", "content": "We test our approach in the assistance benchmark suggested in Du et al. [6]. The human (orange)\nis tasked with reaching a goal state (green) while avoiding the obstacles (purple). The AI assistant\ncan move blocks one step at a time in any direction [6]. While the original benchmark used N = 2\nobstacles, we will additionally evaluate on harder versions of this task with N = 5, 7, 10 obstacles.\nWe show results in Fig. 3. On the easiest task, both our method and AvE achieve similar asymptotic\nreward, though our method learns more slowly than AvE. However, on the tasks with moderate and\nhigh degrees of complexity, our approach (ESR) achieves significantly higher rewards than AvE,\nwhich performs worse than a random controller. These experiments support our claim that contrastive\nsuccessor representations provide an effective means for estimating empowerment, and hint that ESR\nmight be well suited for solving higher dimensional tasks."}, {"title": "5.2 Does our approach scale to tasks with image-based observations?", "content": "Our second set of experiments look at scaling ESR to the image-based Overcooked environment.\nSince contrastive learning is often applied to image domains, we conjectured that ESR would scale\ngracefully to this setting. We will evaluate our approach in assisting a human policy trained with\nbehavioral cloning taken from Laidlaw and Dragan [57]. The human prepares dishes by picking\nup ingredients and cooking them on a stove, while the AI assistant moves ingredients and dishes\naround the kitchen. We focus on two environments within this setting: a cramped room where the\nhuman must pass ingredients and dishes through a narrow corridor, and a coordination ring where the\nhuman must pass ingredients and dishes around a ring-shaped kitchen (Figs. 4b and 4c). As before,\nwe compare with AvE as well as a na\u00efve random controller. We report results in Table 1. On both\ntasks, we observe that our approach achieves higher rewards than AvE baseline, which performs no\nbetter than a random controller. In Fig. 5, we show an example of one of the collaborative behaviors\nlearned by ESR. Taken together with the results in the previous setting, these results highlight the\nscalability of ESR to higher dimensional problems."}, {"title": "6 Discussion", "content": "One of the most important problems in AI today is equipping AI agents with the capacity to assist\nhumans achieve their goals. While much of the prior work in this area requires inferring the human's\nintention, our work builds on prior work in studying how an assistive agent can empower a human\nuser without inferring their intention. Relative to prior methods, we demonstrate how empowerment\ncan be readily estimated using contrastive learning, paving the way for deploying these techniques on\nhigh-dimensional problems.\nLimitations. One of the main limitations of our approach is the assumption that the assistive agent\nhas access to the human's actions, which could be challenging to observe in practice. Automatically\ninferring the human's actions remains an important problem for future work. A second limitation is\nthat the method is currently an on-policy method, in the sense that the assistive agent has to learn by\ntrial and error. A third limitation is that the ESR formulation assumes that both agents share the same\nstate space. In many cases the empowerment objective will still lead to desirable behavior, however,\ncare must be taken in cases where the agent can restrict the information in its own observations, which\ncould lead to reward hacking. Finally, our experiments do not test our method against real humans,\nwhose policies may differ from the simulated policies. In the future, we plan to investigate techniques\nfrom off-policy evaluation and cooperative game theory to enable faster learning of assistive agents\nwith fewer trials. We also plan to test the ESR objective in environments with partial observability\nover the human's state.\nSafety risks. Perhaps the main risk involved with maximizing empowerment is that it may be at\nodds with a human's agents goal, especially in contexts where the pursuit of that goal limits the\nhuman's capacity to pursue other goals. For example, a family choosing to have a kid has many fewer\noptions over where they can travel for vacation, yet we do not want assistive agents to stymie families\nfrom having children.\nOne key consideration is whom should be empowered. The present paper assumes there is a single\nhuman agent. Equivalently, this can be seen as maximizing the empowerment of all exogenous\nagents. However, it is easy to adapt the proposed method to maximize the empowerment of a single\ntarget individual. Given historical inequities in the distribution of power, practitioners must take care\nwhen considering whose empowerment to maximize. Similarly, while we focused on maximizing\nempowerment, it is trivial to change the sign so that an \u201cassistive\u201d agent minimizes empowerment.\nOne could imagine using such a tool in policies to handicap one's political opponents."}]}