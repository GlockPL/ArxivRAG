{"title": "TEXTHAWK2: A LARGE VISION-LANGUAGE MODEL EXCELS IN BILINGUAL OCR AND GROUNDING WITH 16X FEWER TOKENS", "authors": ["Ya-Qi Yu", "Minghui Liao", "Jiwen Zhang", "Jihao Wu"], "abstract": "Reading dense text and locating objects within images are fundamental abilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs. Previous LVLMs, including superior proprietary models like GPT-40, have struggled to excel in both tasks simultaneously. Moreover, previous LVLMs with fine-grained perception cost thousands of tokens per image, making them resource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient fine-grained perception and demonstrating cutting-edge performance across general-purpose, OCR, and grounding tasks with 16 times fewer image tokens. Critical improvements include: (1) Token Compression: Building on the efficient architecture of its predecessor, TextHawk2 significantly reduces the number of tokens per image by 16 times, facilitating training and deployment of the TextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We enhance the visual encoder through LVLM co-training, unlocking its potential for previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity: We maintain a comparable scale of 100 million samples while diversifying the sources of pre-training data. We assess TextHawk2 across multiple benchmarks, where it consistently delivers superior performance and outperforms closed-source models of similar scale, such as achieving 78.4% accuracy on OCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1% accuracy@0.5 on RefCOCOg-test.", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past few years, significant advancements have been made in the realm of Large Language Models (LLMs) (Touvron et al., 2023; Zeng et al., 2024; Yang et al., 2024; DeepSeek-AI et al., 2024; Cai et al., 2024). These breakthroughs have also driven the development of Large Vision-Language Models (LVLMs) (Li et al., 2023b; Liu et al., 2023b; Wang et al., 2023c; Bai et al., 2023; Lu et al., 2024a; Chen et al., 2024b). LVLMs effectively combine visual and linguistic modalities, allowing them to understand visual content while leveraging the instruction-following and dialogue capabilities of LLMs. Over the past year, the rapid evolution of LVLMs, incorporating larger foundational LLMs and richer datasets, has significantly improved their ability to perform complex multimodal understanding and reasoning. Consequently, state-of-the-art LVLMs have achieved outstanding re-"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 TEXT-ORIENTED LVLMS", "content": "Text recognition or document understanding is a pivotal feature of LVLMs. Consequently, nu- merous LVLMs dedicate their efforts not only to general image comprehension but also to text-"}, {"title": "2.2 GROUNDING-ORIENTED LVLMS", "content": "Grounding capabilities are critical for LVLMs to tackle complex reasoning tasks involving specific regions and objects within images. To improve interpretability and enhance user interaction, LVLMS are typically expected to accept and provide positional information in formats such as point coor- dinates, bounding boxes, or region masks. Shikra (Chen et al., 2023) approaches this by encoding positions as normalized plain-text coordinates, leveraging the flexibility of natural language. Con- versely, models like VisionLLM (Wang et al., 2023d), Kosmos-2 (Peng et al., 2023), and Ferret (You et al., 2023) extend LVLM vocabularies by incorporating location tokens that represent normalized and quantized offsets of image dimensions. These models are trained on carefully crafted large- scale visual grounding datasets to support the new tokens. LLaVA-G (Zhang et al., 2023a) adopts a different strategy, predicting segmentation masks rather than bounding boxes, using a pre-trained grounding model as its decoder, which necessitate additional alignment training with the LVLM. Meanwhile, GPT4RoI (Zhang et al., 2023c) and VolCano (Li et al., 2024c) enhance fine-grained multimodal understanding by supplementing the model with additional regional features, instead of positional information. In contrast, TextHawk series pioneer the native grounding capability of LVLMs via a detection head combined with efficient representation of bounding boxes."}, {"title": "2.3 VISUAL TOKEN COMPRESSION", "content": "With the support for higher image resolutions in recent LVLMs, the number of visual tokens has surged, creating a strong demand for efficient compression methods. Solutions like CogAgent and MiniGemini (Li et al., 2024b) tackle this by introducing a lightweight visual encoder specifically for high-resolution refinement, without increasing the visual token count. Their method uses low- resolution visual embeddings as queries to retrieve relevant high-resolution cues, either within the LLM or via a resampler. Qwen-VL (Bai et al., 2023) and LLaVA-UHD (Xu et al., 2024) adopt a different approach by directly compressing visual tokens of each sub-image by factors of 4 and 9, respectively, using a shared perceiver resampler layer. Meanwhile, LLaVA-PruMerge (Shang et al., 2024) implements an adaptive strategy, dynamically identifying and retaining the most critical visual tokens, then merging similar ones through clustering. TextMonkey (Liu et al., 2024c) also performs visual token compression based on token similarity. MADTP (Cao et al., 2024) introduces a Dynamic Token Pruning (DTP) module that adjusts visual token compression ratios layer by layer, adapting to varying input complexities. TextHawk stands out as the first LVLM to achieve 16 times token compression through a novel two-step process for resampling and rearrangement, each reduc- ing the token count by a factor of 4. Building on TextHawk's approach, TextHawk2 achieves the same compression ratio of 16, offering enhanced efficiency in visual token handling."}, {"title": "3 ARCHITECTURE", "content": "The overall architecture and key components of TextHawk2 continue the design of TextHawk (Yu et al., 2024) family, including a lightweight visual encoder and an LLM, which are bridged by a meticulously designed resampler for modality adaption and token compression. The network architecture and dataflow of TextHawk2 is depicted in Fig. 2."}, {"title": "3.1 LARGE LANGUAGE MODEL", "content": "Recent advances in open-source LLMs have also enhanced the ability of upper-level LVLMs to understand both linguist and visual content. Noteworthy among these developments are models like LLaMA3 (AI@Meta, 2024) and Qwen2 (Yang et al., 2024). There are two major enhancements in the latest LLMs. Firstly, the integration of Grouped-Query Attention (GQA) has greatly reduced the memory requirements for key-value cache during deployment. Secondly, these models support longer context lengths, e.g., Qwen2 can handle up to 32,768 tokens during pre-training and extend up to 131,072 tokens during inference. To develop a Chinese-English bilingual LVLM, we utilize Qwen2-7B-Instruct due to its strong capability in processing Chinese data."}, {"title": "3.1.1 DETECTION HEAD", "content": "To improve training throughput and inference efficiency, our TextHawk series extend the vocabulary of LVLMs with special coordinate tokens. As depicted in Fig. 3, representing a bounding box with a digit string requires 25 tokens-2 trigger marks, 4 floating-point numbers (each uses 5 tokens), and 3 commas. In contrary, by replacing each floating-point number with a unique coordinate token and retaining the center comma, we significantly reduce the token count to just 7."}, {"title": "3.3.1 SCALABLE POSITIONAL EMBEDDINGS", "content": "Scalable Positional Embeddings (SPEs) (Yu et al., 2024) present an innovative extension of factor- ized positional embeddings (decomposing row and column), making them applicable to arbitrary input shapes. To ensure a fair comparison, we also modify Absolute Positional Embeddings (APEs) to accommodate dynamic shapes by slicing sections of the APEs during both training and infer- ence phases. Due to their adaptability and training efficiency, SPEs achieve superior performance over APEs while utilizing fewer parameters. Furthermore, SPEs exhibit outstanding extrapolation capabilities to unseen input resolutions, resulting in impressive zero-shot performance.\nThe concept of SPEs arises from the observation that positional embeddings, in practice, tend to distribute themselves around the surface of a hypersphere. This insight leads us to consider Spherical Linear Interpolation (Slerp) as a potential alternative to traditional interpolation methods, such as nearest-neighbor or linear interpolation. However, our initial attempts to directly apply Slerp to pre-trained APEs prove to be ineffective. We believe this ineffectiveness stems from the incomplete assumption that these embeddings are perfectly distributed on a hypersphere. To address this issue, we introduce a normalization and scaling process for the embeddings prior to interpolation, ensuring they conform to the requirements of Slerp. Moreover, given that different parts of the positional embeddings are used independently by each attention head, we apply normalization and scaling operations on a per-head basis, allowing for more precise interpolation aligned with the needs of each attention mechanism. The pseudocode of SPEs is shown in Algorithm 1."}, {"title": "3.3.2 QUERY PROPOSAL NETWORK", "content": "To enhance convergence and improve grounding performance, the TextHawk family incorporates the Query Proposal Network (QPN) (Yu et al., 2024) to dynamically generate resampling query to- kens. Attention-based adapters, such as Quering Former (Q-Former) (Li et al., 2023b) and perceiver resampler (Alayrac et al., 2022), show promise in token compression but are challenging to train. On the other hand, MLP-based adapters, while simpler, often outperform attention-based adapters when training data is limited. We attribute the difficulty of training attention-based adapters to the fixed query tokens used in previous approaches. This observation led us to merge the strengths of both methods. Specifically, QPN utilizes a lightweight MLP-Pool-Dense architecture to efficiently transform features from the visual encoder into queries. It also offers greater adaptability by allow- ing a variable number of unique queries for images with different resolutions. In the QPN, we apply a 2 x 2 max pooling, achieving a compression ratio of 4 during the resampling stage."}, {"title": "3.3.3 RESAMPLING AND REARRANGEMENT", "content": "TextHawk introduces a two-stage token compression strategy called ReSampling and ReArrange- ment (ReSA) (Yu et al., 2024), designed to minimize information loss and preserve critical infor- mation from visual inputs. In the first stage, resampling, a smaller set of highly informative tokens is selectively extracted from the visual encoder outputs. This is achieved through a cross-attention mechanism where query tokens, generated by the QPN, guide the selection process. For TextHawk2, these tokens are progressively refined in 4 bidirectional decoder layers. In the second stage, rear- rangement, visual tokens are flattened following the image scanning order and then grouped into sets of four. Instead of arranging tokens based on the sequence of sub-images, we preserve the original"}, {"title": "3.3.4 MULTI-LEVEL CROSS-ATTENTION", "content": "To address the limitations of language-supervised visual encoders on fine-grained tasks, TextHawk proposes a feature merging approach called Multi-Level Cross-Attention (MLCA) (Yu et al., 2024). The MLCA mechanism is designed to enhance feature extraction by allowing the resampler to ef- ficiently aggregate information from multiple layers of a visual encoder. This is achieved through a predefined routing table that determines which features are to be extracted and merged at each resampler layer. One of the key findings is that a deep-to-shallow feature extraction strategy yields superior results for grounding tasks while preserving the overall performance of general visual un- derstanding. Notably, MLCA accomplishes this without incurring any additional computational costs, making it both effective and efficient. In practical terms, the implementation of MLCA in TextHawk involves utilizing four distinct stages of the visual encoder. Features are extracted specif- ically from the 14th, 18th, 22nd, and 26th layers of the encoder."}, {"title": "4 DATA", "content": "TextHawk2 employs a one-pass pre-training approach, differing from the two-stage pre-training paradigm commonly used in prior works on LVLMs. These models undergo an initial stage where different modalities are aligned using fixed, low-resolution image-caption pairs. This is followed by a second stage of continual training on mixed-resolution image-text data from diverse sources, such as OCR and grounding datasets. In contrast, TextHawk2 skips the initial alignment stage and instead focuses on training on more detailed image captions from the beginning."}, {"title": "4.1 PRE-TRAINING", "content": "The 100M pre-training data are collected from diverse sources and carefully curated to enhance the OCR and grounding capabilities. The sampling ratios for various datasets are shown in Fig. 4."}, {"title": "4.1.1 CONCEPTION", "content": "To improve alignment, we utilize data from CapsFusion (Yu et al., 2023a), a framework designed for re-captioning web-crawled data. Within our previous work, the largest conceptual caption dataset, LAION-400M (Schuhmann et al., 2021), is automatically gathered from the web. This approach can result in captions containing irrelevant descriptions or lacking essential details, causing hallu- cinations and misalignments. CapsFusion addresses these issues by employing LVLM to generate captions that directly reflect the image content. These generated captions are then integrated with the web-sourced captions using a caption fuser, avoiding knowledge loss."}, {"title": "4.1.2 INTERLEAVED", "content": "Previous works have demonstrated that interleaved image-text data is beneficial for improving the multimodal in-context learning capability of LVLMs (Zhang et al., 2023b; Lauren\u00e7on et al., 2024). TextHawk2 makes up for the lack of large-scale interleaved data by leveraging the image-text dataset from the Wanjuan1.0 data collection (He et al., 2023). This part comprises bilingual interleaved data sourced from Wikipedia and news outlets."}, {"title": "4.1.3 GROUNDING", "content": "We primarily utilize GrIT-20M (Peng et al., 2023), a synthetic caption dataset with additional lo- cation labels for major visual elements, to enhance the grounding capability of TextHawk2. Ad- ditionally, we incorporate referring and grounding data from UMG-41M (Shi et al., 2024). These data are curated from various public image-caption datasets, including CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), Flickr (Young et al., 2014), VG (Kr- ishna et al., 2017), YFCC-15M (Thomee et al., 2016), and ImageNet-21K (Ridnik et al., 2021), by jointly applying an object detector and a regional captioner. Specifically, each region is randomly as- signed to either a referring task or a grounding task. In the referring task, we provide the model with a bounding box to generate a caption for that specific region, while in the grounding task, we reverse this by using the caption to predict the corresponding bounding box. We also include approximately 1/8 of the captions in Chinese, which are generated using an English-to-Chinese translation API."}, {"title": "4.1.4 OCR", "content": "To gather extensive OCR pre-training data, we employ a commercial OCR engine to transcribe text from images. This includes Chinese text from the Wukong (Gu et al., 2022) dataset and English text from the IIT-CDIP (Lewis et al., 2006) dataset. We also use PDFPlumber to extract text lines from Common Crawl PDFs. To improving English handwriting recognition, we incorporate Rendered- Text (StabilityAI & LAION, 2023). Additional end-to-end OCR datasets, including ArT (Chng et al., 2019), COCO-Text (Veit et al., 2016), CTSU (Guo et al., 2021), CTW (Yuan et al., 2019), IC15 (Karatzas et al., 2015), LSVT (Sun et al., 2019), MLT (Nayef et al., 2019), MTWI (He et al., 2018), RCTW-17 (Shi et al., 2017), ReCTS (Zhang et al., 2019), and SCUT-HCCDoc (Zhang et al., 2020) are also integrated into our training data."}, {"title": "4.1.5 MARKDOWN", "content": "Building upon the markup-based data pipeline introduced in Kosmos-2.5 (Lv et al., 2023), we ex- pand our dataset by gathering more image-to-markdown pairs to enhance OCR and layout under- standing capabilities. We specifically source LATEX documents from arXiv, README files from GitHub, and DOCX files from Common Crawl. These files are then converted into images and subsequently translated into markdown format."}, {"title": "4.1.6 TABLE & CHART", "content": "Alongside the previously mentioned markdown data, we also collect data to enhance the ability to interpret tables and charts. For tables, we use the PubTables-1M (Smock et al., 2022) dataset, including both its original English version and a translated Chinese version, to gather table recog- nition data. For charts, we employ chart-to-table conversion and chart-based QA data from existing datasets, including MMC (Liu et al., 2024b) and ChartSFT (Meng et al., 2024)."}, {"title": "4.2 SUPERVISED FINE-TUNING", "content": "TextHawk2 enhances the mixture of TextHawk instruction data by incorporating several newly added datasets. First, it replaces all text-only data with two high-quality data collections: Open- Hermes2.5 (Teknium, 2023) and COIG-CQIA (Bai et al., 2024). Next, it adds a variety of other datasets, including ShareGPT-40 (Chen et al., 2024b), LVIS-Instruct4V (Wang et al., 2023a), LAION-GPT4V, LLaVAR (Zhang et al., 2023d), Cauldron (Lauren\u00e7on et al., 2024), KVQA (Shah et al., 2019), ViQuAE (Lerner et al., 2022), Geo170K (Gao et al., 2023), HME100K (Yuan et al., 2022), UniMER-1M (Wang et al., 2024a), FUNSD (Jaume et al., 2019), XFUND (Xu et al., 2022), SROIE (Huang et al., 2019b), POIE (Kuang et al., 2023), ST-VQA (Biten et al., 2019), and EST- VQA (Wang et al., 2020). Finally, it randomly samples 80K, 60K, 20K, 300K, and 80K pre-training data samples from the aforementioned OCR, Markdown, Table, and Chart categories."}, {"title": "5 IMPLEMENTATION DETAILS", "content": ""}, {"title": "5.1 INFRASTRUCTURE", "content": "TextHawk2 is trained on Huawei Cloud, utilizing the elastic cloud computing and file system."}, {"title": "5.1.1 STORAGE", "content": "For storing large-scale multimodal data, we use Huawei Cloud's Parallel File System (PFS). PFS is a high-performance file system built on Object Storage Service (OBS), offering millisecond- level access latency, TB/s-level bandwidth, and millions of IO/s, enabling fast handling of High- Performance Computing (HPC) workloads.\nTo accelerate data preparation during training, we introduce a sequential loading method that avoids the inefficiency of random access to numerous small files. Our dataset implementation is built on WebDataset (WebDataset, 2024), a high-performance IO system that uses tar files and provides Python APIs. It supports reading files from local storage as well as files from OBS. By using the WebDataset format, we create a fully sequential IO pipeline optimized for handling large-scale data. Specifically, we divide the samples into equal-sized chunks, store them in tar files, and distribute the chunks across different data workers. Each data worker then loads all the samples from its assigned chunks sequentially, with no overlap in the data between workers. This approach is crucial for maximizing IO throughput and efficiently leveraging cloud storage during training.\nAdditionally, we implement a failure recovery mechanism that ensures training can accurately re- sume from any checkpoint. While the native WebDataset offers fully indexed access to datasets, it"}, {"title": "5.1.2 ACCELERATOR", "content": "We support both NVIDIA GPU and Ascend NPU platforms. To accelerate attention computation, we adopt memory-efficient attention (Lefaudeux et al., 2022) for NVIDIA V100, and NPU fusion attention (Ascend, 2024) for Ascend 910B. TextHawk2 is trained on 16 nodes with 128 cards."}, {"title": "5.2 EFFICIENCY", "content": "To enhance the training efficiency of LVLMs on devices with limited memory, we present two major improvements, including 4D parallelism and data packing."}, {"title": "5.2.1 PARALLELISM", "content": "We employ a combination of four types of parallelisms, including Data Parallelism (DP) (Li et al., 2020), Tensor Parallelism (TP) (Shoeybi et al., 2019), Sequence Parallelism (SP) (Korthikanti et al., 2023), and Pipeline Parallelism (PP) (Huang et al., 2019a; Harlap et al., 2018; Narayanan et al., 2021). DP is the most prevalent technique for distributed training, allowing large data batches to be divided across various devices. We utilize DP alongside the DeepSpeed Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) to enhance memory efficiency. TP reduces memory usage by distributing model weights and partial activations across multiple devices, while SP further alleviates memory demands by handling activations that TP cannot manage. However, TP introduces significant communication overhead, requiring all devices to reside on the same node with high- speed connections. By default, we set TP = SP = 1, using a maximum of TP = SP = 4 only when needed. In contrast to LLM training, PP operates differently in LVLM training due to the heterogeneous characteristics of LVLMs. Here are the challenges:\n\u2022 Computational Imbalance: Distributing model layers evenly across multiple pipeline stages is crucial for load balancing. However, achieving this balance with LVLMs is more complex than with LLMs. The challenge arises from the requirement to place the visual encoder and resampler before the first LLM layer, complicating the even distribution of these components across pipeline stages.\n\u2022 Memory Imbalance: The initial pipeline stages have to store activations from the warm- up micro-batches. The size of these activations is proportional to the number of pipeline stages. As PP increases, the memory required to store activations in both the visual encoder and resampler also increases, which might lead to memory overload.\nFor PP, computational imbalance results in increased idle time (aka bubble size), which should be avoided. To address this and improve communication efficiency, we integrate the entire visual encoder and resampler into the initial pipeline stage. To avoid memory overload, we restrict PP, setting it to PP = 1 during LoRA training and PP = 2 during full-parameters training. To tackle computational imbalance, we divide the LLM layers into unequal segments, ensuring that the first pipeline stage contains the visual encoder, resampler, and fewer LLM layers."}, {"title": "5.2.2 PACKING", "content": "To achieve optimal performance, it is crucial to balance the model components with the data stream. However, LVLMs with variable resolution inputs and variable length outputs inevitably involve im- balances. To address this, we set a fixed context length and pack multiple samples to reduce padding. We also restrict the number of packed images to avoid overloading the visual encoder. Specifically,"}, {"title": "5.3 HYPERPARAMETERS", "content": "During the pre-training phase, our focus is on training the newly initialized resampler and updating both the ViT and LLM using LoRA. Specifically, LoRA modules are applied to the query and value projection layers, with ranks of 16 for ViT and 128 for LLM. In the supervised fine-tuning stage, we unfreeze all parameters, allowing the entire model to be trained end-to-end. Our preliminary investigation of different training strategies has shown that training an LVLM for Chinese OCR with a frozen ViT is possible. However, unfreezing ViT during both pre-training and supervised fine- tuning significantly enhances performance, making it essential for achieving state-of-the-art results in OCR tasks. To further improve OCR robustness, we introduce manual perturbations by randomly resizing images from text-oriented datasets within a small range.\nDuring the pre-training phase, we utilize a global batch size of 384, where each data point is a collection of multiple packed samples. The training process spans 45,000 steps. The learning rate initiates at 0 and linearly warms up to 2 \u00d7 10-4 within the initial 3% of the steps. Beyond this point, it follows a cosine decay schedule, tapering down to 0. We also incorporate a \"late warm- up\" strategy for both ViT and the LLM. During the first half of the warm-up phase, the parameters of these modules remain fixed. Concurrently, only the parameters of the resampler are updated, which serves to offset the lack of a dedicated pre-training phase for the resampler alone. For the supervised fine-tuning stage, the global batch size is set to 256, and the model undergoes training for two epochs. The learning rate schedule is akin to the pre-training phase, albeit with distinct peak values: 5 \u00d7 10-5 for both the ViT and the resampler, and 2 \u00d7 10-5 for the LLM.\nIn configuring the AdamW optimizer for stable training, we set \u03b21 to 0.9 and \u03b22 to 0.95. Addition- ally, a weight decay of 0.05 is applied to enhance model generalization."}, {"title": "6 EXPERIMENTS", "content": ""}, {"title": "6.1 OCR BENCHMARK", "content": "We explore the native OCR capabilities of TextHawk2 across various text-oriented tasks, includ- ing nature scene text recognition, document information retrieval, chart comprehension, and table fact-checking. The benchmarks utilized are OCRBench (Liu et al., 2023e), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA (Mathew et al., 2022), TabFact (Chen et al.,"}, {"title": "6.4 GROUNDING CAPTIONER", "content": "Most large-scale visual grounding datasets, such as grounding captions and referring expressions, are generated using outdated specialist models or region-based captioners like GLIP (Li et al., 2022) and GPT4ROI (Zhang et al., 2023c). However, data quality plays a critical role in the final per- formance of LVLMs and often becomes a bottleneck for training more advanced models. Unlike generic image captions, which can be widely collected from the web and recaptioned by proprietary commercial APIs or open-source LVLMs, visual grounding data are much harder to generate and remain under-explored. This is primarily because current proprietary models lack strong grounding capabilities, and there is no high-performing open-source foundational LVLM specifically designed for grounding tasks. One potential solution for data augmentation of visual grounding involves a data-model-iteration loop, utilizing smaller detection models. A similar approach has been explored in VILA2 (Fang et al., 2024), which introduces the concepts of self-augment and specialist-augment. In the specialist-augment step, an LVLM is fine-tuned on a high-quality subset of grounding cap- tions and then used to recaption the remaining large-scale image dataset. It has been shown that image caption quality improves across up to three iterations.\nTo assess the effectiveness of TextHawk2 as a grounding captioner, we compare the original region captions from UMG-41M with those generated by TextHawk2, which uses bounding boxes as addi- tional inputs, as shown in Fig. 7. Although TextHawk2 is pre-trained on UMG-41M, its re-generated captions provide more detailed descriptions and better spatial relationships compared to the origi- nal captions. Unlike the specialist-augment method from VILA2, our approach integrates detection results from convolutional models, which produce more accurate bounding boxes. We believe this strategy can help address distribution issues that arise in data augmentation loops, and we plan to explore this direction further in future work."}, {"title": "6.5 COMPARISON WITH PROPRIETARY MODELS", "content": "While TextHawk2 is designed for computational efficiency and optimized for fine-grained tasks, it also demonstrates strong performance on general VQA tasks. We conduct a comprehensive com- parison with proprietary models across various benchmarks, including general multimodal under- standing and OCR tasks. Grounding tasks are not shown here since they have limited support. The benchmarks we consider are MMMU (Yue et al., 2023), MMBench (Liu et al., 2023d), MME (Fu et al., 2023), MMStar (Chen et al., 2024a), BLINK (Fu et al., 2024), MMT-Bench (Ying et al., 2024), RealWorldQA (X.AI, 2024), SEED-Bench (Li et al., 2023a), AI2D (Kembhavi et al., 2016), ScienceQA (Lu et al., 2022), MathVista (Lu et al., 2024b), HallusionBench (Liu et al., 2023a),"}, {"title": "7 CONCLUSION AND LIMITATIONS", "content": "In this work, we address two key questions: Can we increase the compression ratio to 16 with- out losing the ability to perceive fine-grained details and achieve state-of-the-art OCR performance with limited resources? And can we train an LVLM with a single visual encoder that excels in general multimodal understanding, OCR, and grounding simultaneously? To answer these, we in- troduce TextHawk2, which demonstrates state-of-the-art performance in multimodal understanding, OCR, and grounding, all while achieving a 16 times token compression ratio with a unified visual encoder. Notably, TextHawk2 is pre-trained on a relatively modest dataset of 100 million samples\u2014 fewer than comparable LVLMs\u2014highlighting the significance of visual encoder reinforcement and data diversity. Meanwhile, we optimize the data pipeline and model parallelism to boost training throughput, allowing TextHawk2 to be trained using limited resources.\nHowever, our experiments face several limitations. First, the training data contains insufficient scene text, limiting the model's ability to accurately recognize complex Chinese characters. Second, the supervised fine-tuning process lacks adequate multimodal knowledge and reasoning data, which affects performance in these areas. Third, the potential of native resolution ViT and full-parameter pre-training remains unexplored. Lastly, the current version of TextHawk2 does not incorporate Reinforcement Learning from Human Feedback (RLHF), which could help reduce hallucinations. Addressing these limitations will be essential in future work."}]}