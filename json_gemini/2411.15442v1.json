{"title": "Automatic High-quality Verilog Assertion Generation through Subtask-Focused Fine-Tuned LLMs and Iterative Prompting", "authors": ["Mohammad Shahidzadeh", "Behnam Ghavami", "Steve Wilton", "Lesley Shannon"], "abstract": "Formal Property Verification (FPV), using SystemVerilog Assertions (SVA), is crucial for ensuring the completeness of design with respect to the specification. However, writing SVA is a laborious task and has a steep learning curve. In this work, we present a large language model (LLM) based flow to automatically generate high-quality SVA from the design specification documents, named AssertCraft. We introduce a novel sub-task-focused fine-tuning approach that effectively addresses functionally incorrect assertions produced by baseline LLMs, leading to a remarkable 7.3-fold increase in the number of functionally correct assertions. Recognizing the prevalence of syntax and semantic errors, we also developed an iterative refinement method that enhances the LLM's initial outputs by systematically re-prompting it to correct identified issues. This process is further strengthened by a custom compiler that generates meaningful error messages, guiding the LLM towards improved accuracy. The experiments demonstrate a 26% increase in the number of assertions free from syntax errors using this approach, showcasing its potential to streamline the FPV process.", "sections": [{"title": "I. INTRODUCTION", "content": "The slowdown in processing power following Moore's law has introduced a significant rise in hardware diversity, resulting in an increased need for design verification (DV) [1]. Originally, DV was performed by extracting the properties of the design and extensively trying to prove them by finding corner cases and new simulation testbenches. However, in the earlier cycles of DV, this tedious and uncertain task was replaced by formal verification [2]. The formal method replaced this uncertain task with the automatic process of proving that the property is correct under all circumstances or there is a counter-example proving otherwise. The properties (assertion statements) in the design are written in Property Specification Language (PSL), containing an assert keyword and a property inside them which are used to check the correctness of that property. These properties come from design specifications and show the expected functionality of the Design Under Test (DUT). However, each design has a separate specification and infinite ways of implementation. As a result, the formal verification engineers need to generate independent assertions by reading the design specification and the RTL design, which makes this process tedious and time-consuming. This hurdle prevented engineers from using it more commonly [3]. Given the recent increasing complexity of designs, the demand for more efficient methods to streamline this process has become critical.\nA number of studies have attempted to automate parts of the assertion extraction process. Earlier efforts in this domain sought to simplify this task by providing a new abstraction level that is closer to human language [4] [5]. However, full automation has not been achieved, as the specification must still be converted to this new abstraction level rather than directly to the assertion level. lately, large language models (LLM) have demonstrated significant promise in automating hardware design and assisting engineers throughout the design process [6]. This has led to promising developments in using LLMs to automate the verification process [7]-[9]. Kande et al. [7] illustrated how LLMs can generate new assertions from detailed comments, while Orenes et al. [9] showcased their effectiveness in creating new properties from the Register Transfer Level (RTL) code. Despite these advancements, no successful attempt has yet been made to generate high-quality assertions directly from specification documents.\nOur research, detailed in Section II-B, investigates the chal-lenges faced by current state-of-the-art large language models in generating assertions directly from specification documents. This process requires complex, multi-step reasoning, which is a known challenge for Pretrained LLMs (PLLMs), as demonstrated in recent studies on other applications [10] [11] [12]. As a promising solution, fine-tuning LLMs is essential and widely adopted to unlock robust capabilities for complex tasks, enhance performance on downstream applications, and better align with human preferences [13]. However, this approach relies on the availability of \"large-scale, annotated task-specific training data\u201d accumulated over time. This dependency limits the practical use of LLMs-based assertion gen-eration models in industrial verification scenarios where such training data is scarce. While fine-tuning the LLM for assertion generation from specifications may present significant chal-lenges, creating a dataset for a subtask of the original problem is feasible. Therefore, we introduce a novel \u201csub-task-focused fine-tuning method,\" which involves dividing the task into subtasks that can be fine-tuned separately. While the method may yield sensible initial outputs, some of the assertions gen-erated using this method can still contain compilation bugs that must be addressed. However, having enough assertions without compilation bugs is essential to hit a high level of coverage and enter the late cycles of the design [14] [15]. So this challenge should be addressed efficiently. To tackle these issues, we introduce a \"iterative bug-fixing paradigm\" that tries to build upon its previous patch by using a refinement loop between the compiler and the debugger LLM. This paradigm, unlike the comment one-step paradigms [16] [17] would not miss the multi-location bugs and is good at catching small bugs [18]. In this work, for the first time, we brought this multi-"}, {"title": "II. RELATED WORK AND MOTIVATION", "content": "Earlier works in automatic assertion generation, such as AutoSVA [4] and ILA [5], have focused on creating a new abstraction level closer to human language. Although they succeeded in making the assertion generation process easier, they lost their generality and did not entirely solve the process, as engineers still had to develop new properties at these abstraction levels.\nAnother endeavor in assertion automation targets the com-mon patterns in the waveform for assertion generation [19]. It first uses data mining techniques to discover patterns in simulation test benches, then formulates assertions to describe these patterns. Although this work achieves full automation in assertion generation, it suffers from a high number of generated assertions and the possibility of the assertions being wrong due to the waveform being generated using a design under test, which might have numerous bugs. In response, Liu et al. [20] and [21] sought to address these issues by utilizing higher abstraction levels for waveform generation, such as transaction-level modeling, and by enhancing their techniques for ranking generated assertions, thereby reducing their overall number. Despite these improvements, their approaches still fail to extract functionally correct assertions from specification documents and rely on additional assumptions, such as access to simulation traces or a higher abstraction level, rendering them semi-automated. Additionally, Witharana et al. [22] provide a comprehensive survey of recent advancements in automated techniques for hardware verification using simulation traces.\nWith the rise of new LLMs [23]-[25] capable of accurately generating simple RTL code similar to humans, recent research has shifted focus to the use of LLMs for verification. Kande et al. [7], [8] addressed a fundamental research question: Can LLMs be used to generate assertions from comments? To explore this, they generated assertions at three levels of detail and assessed whether the LLM-generated assertions were ac-curate based on the provided comments. Their findings suggest that LLMs can produce correct assertions from comments, but only when the comments are detailed and include all relevant signal names and operations. The latest work in this area [9] demonstrated the capability of LLMs to generate assertions from RTL code by incorporating additional rules into the LLM's prompt. Although both works were successful in generating correct assertions, they made some impractical assumptions. For example, [7] uses a well-defined comment to produce assertions instead of extracting them from the specification, and [9] may generate assertions from buggy RTL code, which can lead to incorrect assertions."}, {"title": "B. Challenges in PLLM-Based Assertion Generation", "content": "LLMs like GPT-2 and BERT are transformer-based artificial neural networks designed to operate on text datasets. These models contain millions to billions of parameters and are trained on vast amounts of text data. Both the inputs and outputs of an LLM consist of tokens, which are common sequences of characters. When given a sequence of tokens as a prompt, an LLM generates a probability distribution for the next token based on its vocabulary. Once a token is chosen according to specific criteria, it is added to the prompt, and the LLM proceeds to generate the next token in a process known as auto-regression.\nOne way to generate assertions from the specification is to prompt the PLLM to create assertions without additional fine-tuning or details. We attempted to generate assertions directly from the specification document using PLLM, and the results are illustrated in Figure 2. As shown, only 240 assertions were generated for all the modules in the dataset, out of a potential 892, with only 19 of those being functionally correct. These poor results stem from the complexity of generating assertions from the specification. Listing 1 further illustrates why so few assertions were produced using this naive approach. In this example, we presented the PLLM with a simple Finite State Machine specification and asked it to generate assertions. However, instead of producing multiple assertions for each transition between states, it attempted to encapsulate the entire concept in a single assertion. As a result, the generated assertion was not only syntactically incorrect due to multiple implied statements, but also functionally incorrect according to the specification. In fact, PLLMs can only generate accurate assertions when features such as signal names and their relationships are described with precision."}, {"title": "III. PROPOSED ASSERTCRAFT", "content": "The overall proposed flow, named as AssertCraft, is illus-trated in Figure 1. The automation process begins with a clear English specification outlining the RTL code's functionality. This specification must be comprehensive enough for the engineer to write the RTL code based solely on the provided information. Once the specification and the corresponding RTL design are established, the proposed automation flow verifies the design's implementation through the generated assertions. Essentially, the tool autonomously generates assertions and enhances their quality without requiring external input. The figure illustrates the assertion generation and evaluation flows of AssertCraft, presented separately on the left and right sides. In the assertion generation section, it accepts the design specification and RTL design as input and produces assertions. AssertCraft generates these assertions in two steps. In step (1), AssertCraft employs a focused fine-tuning method to generate an initial assertion from the design specification. This assertion then undergoes an iterative repair process, which aims to correct any syntax and semantic errors, marked as step (2) in the flow. On the right side of the figure, the evaluation flow is presented to assess the quality of the generated assertions. To achieve this, It first builds a novel dataset for this task in step (3). Ultimately, it evaluates the quality of the results on the dataset using a scoring system."}, {"title": "A. Subtask-Focused Fine-Tuning", "content": "AssertCraft's assertion generation process from the design specification and the RTL code has two main steps, as depicted in Figure 1. In the first step, Subtask-Focused fine-tuning, only the design specification is given to the tool for the initial assertion generation, and the tool is expected to generate assertions.\n1) LLM: We employed GPT-3.5-turbo [26] as the Language Model for its fine-tuning capabilities and cost-effectiveness compared to GPT-4 [27]. Additionally, its utilization is free if the API is not accessed. We used the default fine-tuning setting recommended by the OpenAI and fine-tuned the model for 3 epochs.\n2) Model Composition: We propose to enhance the gener-ation of assertions from specifications using LLMs by fine-tuning the model. However, we find this process challenging because a dataset containing both specifications and assertions does not exist, and generating one would consume an enor-mous amount of time and resources. Nevertheless, a robust dataset containing comments for the assertions and the asser-tions themselves can be created by scraping GitHub reposito-ries that contain SystemVerilog files. As a result, we divided the task into two \"sub-tasks\" which can be individually improved using fine-tuning or zero-shot prompting techniques. First, we generate comments from the specification. Every individual comment is then used to make the assertions using a fine-tuned LLM which is trained for assertion generation from the comments.\nWe use three questions to extract meaningful comments from the specification. Listings 2, 3, and 4 outline the inquiries used to break the specification into smaller components. The"}, {"title": "B. Iterative Repair", "content": "Using a sub-task fine-tuned model significantly improved the quality of generated assertions, the assertions generated in this step had only access to specification. This prevents the model from generating incorrect assertions due to buggy RTL code but increases the risk of semantic mistakes since the model does not know about the implementation and the signal names. In our experiment, shown in Table I, we found that 78% of the assertions were failing due to syntax errors before reaching this step. As a result, we incorporated an iterative repair method to address this issue. This process is illustrated in Figure 1-step (2). First, the RTL code and the assertions are provided to an LLM, which is tasked with correcting any semantic errors. Next, the assertions are submitted to a compiler for correctness verification. If the generated assertion is correct, it is output as the final assertion for the model. However, if the assertion is incorrect, the compilation error is sent back to the LLM for correction. This process is repeated until we obtain an assertion free of any syntax or semantic errors, or until we reach a predefined iteration threshold where a correct assertion cannot be generated. Our findings indicate that in such cases, the model may become stuck in an indefinite loop, repeatedly providing the same answer.\nThrough our investigation, we found that GPT struggles with tasks that require step-by-step thinking. For example, it cannot detect the simple task of finding the ith word in the text, which is the most commonly used error message for humans. As a result, the first change in our custom compiler was the annotation of the error part without merely indicating the location of the error. The second change involved a simple addition for combinational circuits. Since our model was strug-gling to generate correct assertions for combinational circuits, we tasked it with converting the sequential assertion into a combinational assertion by adding a set of rules. At this stage, we instructed the LLM to delete all clock-related functions and sections in the assertion and convert the implications to the combinational mode. For example, if the compiler encountered the keywords ($a |-> b$ or $a => b$), we tasked it to convert them to (!$a | b$)."}, {"title": "C. Evaluation Flow", "content": "Evaluating any LLM-based assertion generation process requires a structured flow due to the complexity and variabil-ity of the generated assertions, which complicate consistent accuracy assessment. To measure AssertCraft's effectiveness, we created an evaluation flow that includes a dataset and a scoreboard. This general approach can be applied to evaluate any LLM-based assertion generation process.\n1) Dataset Creation: This dataset helps us measure the quality of generated assertions by providing a TCL script, Specification, and the correct RTL design of that specification. The dataset in Figure 1-part(3) is derived from a subset of the HDLBits dataset [29], which includes specifications for various hardware designs. Several enhancements were made to adapt this dataset for verification purposes. First, the golden RTL code was added to the specification for each design from a GitHub repository [30]. Although these RTL codes were functionally correct in accordance with the simulation result from the HDLBits website, they were not all synthesizable. As a result, the first subtle modification was making the designs synthesizable and compatible with FPV engines. Subsequently, a TCL script was written for them to prepare it for the FPV engine execution. However, not all of the designs were suitable for verification tasks, as some had images in their specifications or lacked descriptive specifications from which to make assertions."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we share the results from the evaluation flow on different modules in table II. In the following sections, we showed the ability of AssertCraft to generate functionally correct assertions and assess the coverage of generated assertions."}, {"title": "A. Correctness", "content": "Figure 3 provides a more detailed overview of the generated assertions for each Verilog module in the dataset in table II. The y-axis illustrates the filename using the indexing format introduced in section III-C1, and the x-axis shows the number of assertions that are functionally correct (yellow bar), func-tionally incorrect (blue bar), and syntactically or semantically incorrect (red bar). The figure is divided into two general sections: modules with clock and reset signals and modules without clock or reset signals. This distinction is important in assertion generation as the patterns for combinational and sequential assertions are completely different.\nOur analysis reveals that in most modules, AssertCraft was able to generate at least one functionally correct assertion. Additionally, the number of failed assertions due to incorrect syntax is considerably higher in modules without a clock, as it is more challenging to generate these assertions for such modules compared to those with concurrent assertions."}, {"title": "B. Coverage", "content": "We computed the coverage for each module in the dataset and aggregated the results for each category of modules. Coverage is one of the fundamental metrics in formal verification which is used as an indicator to move to the next stages of the design. illustrates the coverage distribution for each category of data. As can be seen, the stimuli coverage is almost always between 80% and 100% for all categories, while the checker and formal coverages vary significantly based on the"}, {"title": "V. CONCLUSION", "content": "In this paper, we showcased the current state of pretrained LLMs in the assertion generation process and identified the challenges faced by state-of-the-art LLMs in this complex task. We also introduced a novel sub-task focused fine-tuning method that can improve the accuracy of LLMs by 7.3 times on this task. Furthermore, we refined the final output of the model using an iterative refinement method which can direct the LLM toward assertions without syntax or semantic errors, which leads to a 26% increase in the number of correct assertions. While our framework demonstrates the potential of LLMs in generating high-quality assertions, further explo-ration is needed to fully leverage their capabilities in verification tasks. For instance, we could apply LLMs to automated theorem proving, where they can assist in formulating logical proofs. Additionally, exploring their application in formal methods for model checking could also yield insights into verifying system properties more efficiently."}]}