{"title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment", "authors": ["Qizhang Feng", "Siva Rajesh Kasa", "Hyokun Yun", "Choon Hui Teo", "Sravan Babu Bodapati"], "abstract": "Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have made significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using human preference datasets to membership inference attacks (MIAs), highlighting the shortcomings of previous MIA approaches with respect to preference data. Our study has two main contributions: first, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (Preference data MIA); second, we provide empirical evidence that DPO models are more vulnerable to MIA compared to PPO models. Our findings highlight gaps in current privacy-preserving practices for LLM alignment.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have seen a surge in their adoption in the recent past due to their remarkable capabilities on a wide range of natural language processing (NLP) tasks such as question answering, code generation, etc (Zhao et al., 2023). When deployed in real-world scenarios, it is important to align LLMs to human preferences. Techniques such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) play a key role in aligning LLMs with human ethical standards by leveraging human-derived preference data (Christiano et al., 2017; Rafailov et al., 2024; Yang et al., 2024). Although these approaches improve the alignment of models with human values, they are fraught with privacy concerns because of their use of human-generated data. In this work, we investigate the Membership Inference Attack (MIA), a widely-studied vulnerability that attempts to determine whether specific data points are used in the model's training dataset. The study of MIA highlights vulnerabilities in a variety of machine learning paradigms, including several recent studies that specifically focus on LLMs (Fu et al., 2023; Shi et al., 2024). Although existing research on MIA in the context of LLMs highlights the need to evaluate and address the need for privacy concerns, the unique challenges posed by alignment methods such as the PPO and DPO approaches (where preference data directly influences model behavior) remain to be explored. Traditional MIA frameworks fall short when applied to the complex, context-dependent optimization procedures used in LLM alignment. In this paper, we introduce a novel MIA framework that is specifically tailored to address preference data vulnerabilities in LLM alignment, providing a more precise analysis tool that can effectively mitigate these vulnerabilities. Our contributions to this field are twofold:\n\u2022 Introduction of a Novel Reference-based Attack Framework: We propose a new attack framework designed to assess the vulnerability of preference data to MIA, providing an effective analytical tool to address the unique privacy challenges in LLM alignment.\n\u2022 Comparative Vulnerability Assessment of DPO and PPO Models: Through our framework, we find that DPO models are more vulnerable to MIA compared to PPO models. This insight not only points to significant privacy concerns, but also emphasizes the need for stronger privacy-preserving strategies in developing and deploying LLMs aligned using DPO."}, {"title": "2 Preliminaries", "content": "This section introduces the notations and background concepts upon which the rest of the paper builds. We begin by defining the frameworks of PPO and DPO, followed by an overview of MIAs.\n2.1 Model Alignment\nModel alignment ensures LLMs adhere to human values and ethics by adjusting their outputs to match human preferences (Hendrycks et al., 2021; Ouyang et al., 2022). Such alignment is critical for creating AI systems that act in ways that benefit humans and reduce the risks associated with improper alignment. Among the various model alignment techniques, PPO and DPO are some of the widely used approaches (Xu et al., 2024).\n2.1.1 Proximal Policy Optimization (PPO)\nStiennon et al. (2020) and Bai et al. (2022) illustrate Reinforcement Learning from Human Feedback (RLHF) that integrates human feedback into the training of pre-trained Language Models (LMs), encompassing three phases: Supervised Fine-Tuning (SFT), Preference Sampling with Reward Learning, and Reinforcement Learning (RL) through PPO.\nSFT begins the process by fine-tuning a pre-trained LM on task-specific data to obtain a model SFT, enhancing the LLM's performance on the task at hand.\nPreference Data Collection involves gathering a set of preference data pairs $(x, y_w, y_l)$, where x is a prompt and $y_w, y_l$ are two different responses. Here, $y_w$ is the response preferred by human evaluators over $y_l$ for the given context x.\nReward Modeling Phase uses the preference pairs to train the reward model $r_\\varphi(x, y)$, where $\\varphi$ represents the trainable parameters. The trainable model can be a classification header layer attached to the base model or a separate model. The Bradley-Terry (BT) model is commonly used to represent the probability that one response is better than another:\n$L_R(r_\\varphi, D) =\\mathbb{E}_{(x,y_w,y_l)\\sim D} [\\log \\sigma(r_\\varphi(x, y_w) \u2013 r_\\varphi(x,y_l))]$,\nwhere $r_\\varphi(x, y)$ models the likelihood of preferring $y_w$ to $y_l$ given the prompt x, and D denotes the dataset of preference pairs. This loss function measures the accuracy of the reward model in predicting human preferences.\nRL Fine-Tuning Phase then fine-tunes the LM further using the learned reward function, striving to align model outputs with human preferences while maintaining generative diversity:\n$\\max_\\pi Ex~D,y~\u03c0\u03b8 (y|x) [r\u03c6(x, y)]$\\\n$\u2212 \u03b2D_{KL}[\u03c0_\u03b8(y|x)||\u03c0_{SFT}(y|x)]$,\nbalancing fidelity to human feedback with the preservation of the model's original capabilities. Here, $\u03c0_\u03b8$ represents the policy of the language model parameterized by \u03b8, the trainable parameters. The optimization in equation 2 is carried out using Proximal Policy Optimization (PPO) method (Schulman et al., 2017) and throughout the rest of the paper, we use RLHF and PPO interchangeably to refer the same approach.\n2.1.2 Direct Preference Optimization (DPO)\nDPO offers a refined approach to fine-tuning language models by directly leveraging preference data, bypassing the explicit reward model construction typically associated with RLHF methodologies (Rafailov et al., 2024). This method reformulates the two-step optimization procedure in equations 1 and 2 into a single optimization problem that simultaneously optimizes the policy and encodes an implicit reward mechanism based on the preference data.\n$L_{DPO}(\u03c0_\u03b8; \u03c0_{ref}) =\\mathbb{E}_{(x,y_w,y_l)\\sim D} [\\log\\sigma(\\beta\\log\\frac{\u03c0_\u03b8(y_w|x)}{\u03c0_{ref}(y_w|x)}-\\beta\\log\\frac{\u03c0_\u03b8(y_l|x)}{\u03c0_{ref}(y_l|x)})]$\nHere, $\u03c0_{ref}$ refers to a reference model which is typically chosen as the SFT model $\u03c0_{SFT}$. This optimization method is preferred over PPO because it simplifies training by optimizing directly on the preference data, which improves computational efficiency and is easier to implement (Rafailov et al., 2024; Xu et al., 2024). Note that in PPO (equation 2), contrary to DPO (equation 3), the final model being optimized is not directly aligned using the data D. This is the key intuition behind why PPO-aligned models are less susceptible to privacy threats compared to their DPO counterparts.\n2.2 Membership Inference Attacks (MIA) on LLMS\nMIA poses a significant privacy risk in the context of LLMs, challenging the security of data used"}, {"title": "2.3 Problem Statement", "content": "Current research on MIAs has advanced understanding of risks in pre-trained text models, but gaps remain in applying MIAs to preference datasets in LLM alignment. This oversight poses substantial privacy risks, given the critical role of preference data in shaping LLM outputs.\nLet $D_{pref} = {(x_i, y_{wi}, y_{li})}_1^N$ represent the preference dataset, where $x_i$ is a prompt, $y_{wi}$ is the preferred response, and $y_{li}$ is the less preferred response. The vulnerability of this preference data to MIAs requires a nuanced examination, which can be categorized into three distinct attack vectors:\n\u2022 Attack against prompts and preferred responses: This attack determines whether a specific pair of prompt x and preferred response $y_w$ has been used in training, highlighting potential privacy breaches if such data can be identified:\n$MIA_{prompt, y_w} : X \u00d7 Y \u2192 {0,1}$,\n\u2022 Attack against prompts and non-preferred responses: This attack focuses on identifying if a pair consisting of a prompt x and a non-preferred response $y_l$ was part of the training data, potentially exposing sensitive decision-making processes:\n$MIA_{prompt, y_l}: X \u00d7 Y \u2192 {0,1}$,\n\u2022 Attack against the entire preference tuple: This more comprehensive attack assesses whether the entire tuple $(x, y_w, y_l)$ can be traced back to the training set, reflecting a higher risk of revealing critical training methodologies:\n$MIA_{D_{pref}} : X \u00d7 Y \u00d7 Y \u2192 {0,1}$.\nThis detailed breakdown elucidates the complex vulnerabilities associated with preference data in LLMs. By identifying these specific attack vectors, we aim to advance privacy-preserving mechanisms that safeguard the alignment process and ensure that models respect and protect individual privacy while adhering to human ethical standards.\n2.4 Hypotheses Regarding DPO vs PPO\nTo guide our experimental design and directly address the concerns raised by our study, we propose the following hypotheses. These are crafted to explore the distinct impacts of DPO and Proximal Policy Optimization (PPO) on privacy and performance, and are structured to align with the subsequent analyses conducted in our experiments.\nHypothesis 1: Differential Vulnerability to MIAs: We hypothesize that the DPO model is more vulnerable to MIA than the PPO model since the DPO model uses preference data directly, which may lead to overfitting. We empirically assess the MIA vulnerability of DPO and PPO models.\nHypothesis 2: Influence of Model Size on MIA Risk: We postulate that larger models, regardless of the training method (DPO or PPO), will show increased vulnerability to MIAs due to their greater capacity to memorize training data. This hypothesis is explored in \u00a74.4.2, assessing how model size affects susceptibility to data leakage.\nHypothesis 3: Trade-offs Between Performance and Privacy: We propose that while DPO may enhance alignment with human preferences and potentially improve task-specific performance, it also increases the risk of privacy breaches compared to PPO. This trade-off is critically examined in \u00a74.4.3, comparing the performance benefits of DPO against its privacy drawbacks."}, {"title": "3 Method", "content": "Our approach introduces a tailored framework for evaluating MIA on preference datasets used for LLM model alignment. Traditional MIA approaches do not take into account the uniqueness"}, {"title": "3.1 For Individual Response", "content": "Assessing the vulnerability of individual responses-either preferred ($y_w$) or not preferred ($y_l$) to MIAs necessitates a nuanced approach that considers the specific characteristics of preference data. We compute the conditional probability ratio relative to a reference model ref:\n$\\rho_y = \\frac{\u03c0_\u03b8(y|x)}{\u03c0_{ref}(y|x)}$,\nwhere $\u03c0_\u03b8$ represents the target model. This ratio measures the likelihood that the target model will produce a specific response compared to a baseline model, indicating potential overfitting to training data.\nEmploying $\u03c1_y$ enhances specificity by accounting for the subtle nuances and context-dependent nature of response preferences, thus improving the detection of data membership:\n$MIA_{single}(x, y) = \\begin{cases}1 & \\text{if } \u03c1_y > \u03c4_y,\\\\0 & \\text{otherwise}.\\end{cases}$\nAlthough $\u03c4_y$ is mentioned, our primary metric in the experiments is the Area Under the Receiver Operating Characteristic (AUROC), which does not require setting a specific threshold. This approach allows for a flexible assessment of model sensitivity across various potential values.\nThe choice of the reference model $\u03c0_{ref}$ serves as a benchmark for comparing the behavior of the target model $\u03c0_\u03b8$. This model can be the base pre-trained model from which $\u03c0_\u03b8$ originated or a different base model trained on the same dataset. Our experiments, designed to test both scenarios, consistently demonstrate robust performance of our MIA method under various conditions."}, {"title": "3.2 For the Entire Preference Tuple", "content": "To ascertain the membership of the complete preference tuple $(x, y_w, y_l)$, we compute the difference between the probability ratios of the preferred and not preferred responses:\n$\u0394\u03c1 = \u03c1_{y_w} \u2013 \u03c1_{y_l}$.\nThis measure captures the comparative preference strength more effectively, offering a nuanced insight into how preference data impacts model training:\n$MIA_{tuple}(x, y_w, y_l) = \\begin{cases}1 & \\text{if } \u0394\u03c1 > \u03c4_\u0394,\\\\0 & \\text{otherwise}.\\end{cases}$\nThe specified threshold $\u03c4_\u0394$ is set based on the variance observed within the training data, allowing a more accurate identification of the data used during the training phase."}, {"title": "4 Experiments", "content": "4.1 Research Questions and Experiment Design Rationale\nThis subsection outlines the key research questions guiding our experimental design, providing a rationale for our methodologies. Derived from our hypotheses, these questions aim to evaluate the comparative effectiveness, privacy implications, and utility of DPO and Proximal Policy Optimization (PPO) in training LLMs.\nResearch Question 1: How do DPO and PPO differ in their susceptibility to Membership Inference Attacks? This question tests Hypothesis 1 by comparing the vulnerability of models trained using DPO and PPO to MIA to shed light on privacy and data security issues.\nResearch Question 2: Does model size influence its risk of data leakage through MIAs, and how does this vary between DPO and PPO trained models? In line with Hypothesis 2, this question explores the impact of model size on MIA effectiveness, assessing if larger models pose greater privacy risks.\nResearch Question 3: What are the performance and privacy trade-offs when employing DPO versus PPO in LLMs? Echoing Hypothesis 3, this question examines the trade-off between performance and data privacy in tasks that need to be understood like humans, assessing whether greater alignment with human preferences would compromise privacy.\n4.2 Setup\nModels. Our experiments are conducted using a variety of models to ensure a comprehensive evaluation on different scales of model complexity. We include Mistral-7B-v0.1 (Jiang et al., 2023), as well as a series of models from the OpenAI GPT-2 family (Radford et al., 2019): GPT2, GPT2-medium,"}, {"title": "4.3 Baselines", "content": "To accurately evaluate our approach, we position it against well-known MIA baselines specifically tailored for language models and preference data analysis. These baselines are designed to target individual components of the preference data but do not extend to analyzing entire preference tuples.\nPerplexity (PPL): The loss attack method, based on the approach outlined in (Yeom et al., 2018), utilizes the perplexity of a sequence to gauge how well a language model predicts the tokens within that sequence. Perplexity is defined as:\n$P = \\exp\\Big(\\frac{1}{n}\\sum_{i=1}^n \\log \u03c0_\u03b8(x_i|x_1,..., x_{i\u22121})\\Big)$\nwhere a lower perplexity indicates a higher likelihood that the sequence was the training data.\nComparing to zlib Compression (Zlib): This method measures the entropy of a sequence when compressed using zlib, compares the perplexity of a model to its zlib compression entropy, and uses their ratio as an inference metric (Carlini et al., 2021).\nComparing to Lowercased Text (Lowercase):\nThis method evaluates the change in perplexity of a sequence before and after it has been lowercased,"}, {"title": "5 Future Work", "content": "Our study shows that advanced privacy-preserving techniques are needed when using preference data for LLM alignment. Optimizing the privacy model architecture without losing performance is key. Techniques such as DP-SGD (Abadi et al., 2016),"}, {"title": "6 Conclusion", "content": "This paper examines the vulnerability of preference datasets in LLM alignment to MIAs. We reveal that models trained with DPO are more susceptible to MIAs than those using PPO, posing a significant privacy risk as preference data use increases. Our attack framework excels in detecting training data membership, stressing the need for robust privacy-preserving methods. Larger models enhance capabilities but increase privacy risks, highlighting the trade-off between performance and data security."}, {"title": "7 Limitations", "content": "First, the analysis conducted in this study is limited to open-source LLMs and does not include proprietary or closed-source models such as ChatGPT. The privacy implications and vulnerability to MIA of these closed-source LLMs may differ because their training data, architecture, and alignment techniques are not fully transparent. Second, this study focuses on the privacy implications of two well-known alignment techniques (PPO and DPO). However, the field of LLM alignment is rapidly evolving, and the privacy risks associated with other alignment methods can be more fully analyzed in future work."}, {"title": "A Implementation Details", "content": "We mainly refer to the TRL\u2074 package for implementation.\nLORA Setting. For all experiments, we share the same LoRA setting below, using the PEFT\u2075 package: lora_alpha 32, lora_dropout 0.05, lora_r 16, and no bias term.\nQuantization Setting. For all experiments, we use the BitsAndBytes\u2076 package for 4-bit quantization.\nSFT Setting. The settings for SFT are detailed below. We utilized the \"train/rl\" split of the stack-exchange-paired dataset, selecting 80,000 data points for the fine-tuning process, same data is used for PPO and DPO training. The prompt and only the preferred response are concatenated as input. The specific training parameters are:\nTraining Epochs: 2.0\nLearning Rate: 8e-5\nBatch Size (Training): 4\nBatch Size (Evaluation): 2\nGradient Accumulation Steps: 4\nLearning Rate Scheduler: cosine\nWarmup Steps: 100\nWeight Decay: 0.05\nOptimizer: paged_adamw_32bit\nMixed Precision Training: fp16\nPPO Setting. The settings for PPO are detailed below. We filter out data points with maximum length constraints. We also limit the maximum length of the generated response. The specific training parameters are:\nBatch Size: 16\nMini Batch Size: 4\nGradient Accumulation Steps: 4\nPPO Epochs: 6\nLearning Rate: 5.4e-5\nKL Coefficient: 0.1\nAdaptive KL Control: True\nTarget KL: 5.0\nHorizon: 4000\nTraining Epochs: 4\nMaximum Output Length: 128\nMaximum Prompt Length: 256\nMaximum Sequence Length: 1024\n DPO Setting. The settings for DPO training are detailed below. The specific training parameters are:\nBatch Size (Training): 8\nBatch Size (Evaluation): 2\nGradient Accumulation Steps: 2\nTraining Epochs: 3.0\nLearning Rate: 5e-4\nWarmup Steps: 100\nMaximum Sequence Length: 1024\nMaximum Prompt Length: 256\nOptimizer Type: paged_adamw_32bit\nBeta: 0.4"}]}