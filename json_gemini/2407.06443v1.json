{"title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment", "authors": ["Qizhang Feng", "Siva Rajesh Kasa", "Hyokun Yun", "Choon Hui Teo", "Sravan Babu Bodapati"], "abstract": "Large Language Models (LLMs) have seen\nwidespread adoption due to their remarkable\nnatural language capabilities. However, when\ndeploying them in real-world settings, it is im-\nportant to align LLMs to generate texts accord-\ning to acceptable human standards. Methods\nsuch as Proximal Policy Optimization (PPO)\nand Direct Preference Optimization (DPO)\nhave made significant progress in refining\nLLMs using human preference data. However,\nthe privacy concerns inherent in utilizing such\npreference data have yet to be adequately stud-\nied. In this paper, we investigate the vulner-\nability of LLMs aligned using human prefer-\nence datasets to membership inference attacks\n(MIAs), highlighting the shortcomings of previ-\nous MIA approaches with respect to preference\ndata. Our study has two main contributions:\nfirst, we introduce a novel reference-based at-\ntack framework specifically for analyzing pref-\nerence data called PREMIA (Preference data\nMIA); second, we provide empirical evidence\nthat DPO models are more vulnerable to MIA\ncompared to PPO models. Our findings high-\nlight gaps in current privacy-preserving prac-\ntices for LLM alignment.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have seen a surge\nin their adoption in the recent past due to their re-\nmarkable capabilities on a wide range of natural\nlanguage processing (NLP) tasks such as question\nanswering, code generation, etc (Zhao et al., 2023).\nWhen deployed in real-world scenarios, it is impor-\ntant to align LLMs to human preferences. Tech-\nniques such as Proximal Policy Optimization (PPO)\nand Direct Preference Optimization (DPO) play a\nkey role in aligning LLMs with human ethical stan-\ndards by leveraging human-derived preference data\n(Christiano et al., 2017; Rafailov et al., 2024; Yang\net al., 2024). Although these approaches improve\nthe alignment of models with human values, they\nare fraught with privacy concerns because of their\nuse of human-generated data. In this work, we in-\nvestigate the Membership Inference Attack (MIA),\na widely-studied vulnerability that attempts to de-\ntermine whether specific data points are used in\nthe model's training dataset. The study of MIA\nhighlights vulnerabilities in a variety of machine\nlearning paradigms, including several recent stud-\nies that specifically focus on LLMs (Fu et al., 2023;\nShi et al., 2024). Although existing research on\nMIA in the context of LLMs highlights the need\nto evaluate and address the need for privacy con-\ncerns, the unique challenges posed by alignment\nmethods such as the PPO and DPO approaches\n(where preference data directly influences model\nbehavior) remain to be explored. Traditional MIA\nframeworks fall short when applied to the complex,\ncontext-dependent optimization procedures used\nin LLM alignment. In this paper, we introduce a\nnovel MIA framework that is specifically tailored\nto address preference data vulnerabilities in LLM\nalignment, providing a more precise analysis tool\nthat can effectively mitigate these vulnerabilities.\nOur contributions to this field are twofold:\n\u2022 Introduction of a Novel Reference-based\nAttack Framework: We propose a new at-\ntack framework designed to assess the vulnera-\nbility of preference data to MIA, providing an\neffective analytical tool to address the unique\nprivacy challenges in LLM alignment.\n\u2022 Comparative Vulnerability Assessment of\nDPO and PPO Models: Through our frame-\nwork, we find that DPO models are more\nvulnerable to MIA compared to PPO models.\nThis insight not only points to significant pri-\nvacy concerns, but also emphasizes the need\nfor stronger privacy-preserving strategies in\ndeveloping and deploying LLMs aligned us-\ning DPO."}, {"title": "Preliminaries", "content": "This section introduces the notations and back-\nground concepts upon which the rest of the paper\nbuilds. We begin by defining the frameworks of\nPPO and DPO, followed by an overview of MIAs.\n2.1 Model Alignment\nModel alignment ensures LLMs adhere to human\nvalues and ethics by adjusting their outputs to\nmatch human preferences (Hendrycks et al., 2021;\nOuyang et al., 2022). Such alignment is critical for\ncreating AI systems that act in ways that benefit hu-\nmans and reduce the risks associated with improper\nalignment. Among the various model alignment\ntechniques, PPO and DPO are some of the widely\nused approaches (Xu et al., 2024).\n2.1.1 Proximal Policy Optimization (PPO)\nStiennon et al. (2020) and Bai et al. (2022) illustrate\nReinforcement Learning from Human Feedback\n(RLHF) that integrates human feedback into the\ntraining of pre-trained Language Models (LMs), en-\ncompassing three phases: Supervised Fine-Tuning\n(SFT), Preference Sampling with Reward Learning,\nand Reinforcement Learning (RL) through PPO.\nSFT begins the process by fine-tuning a pre-\ntrained LM on task-specific data to obtain a model\nSFT, enhancing the LLM's performance on the\ntask at hand.\nPreference Data Collection involves gathering\na set of preference data pairs (x, Yw, Y\u0131), where x\nis a prompt and yw, y\u0131 are two different responses.\nHere, yw is the response preferred by human evalu-\nators over y\u0131 for the given context x.\nReward Modeling Phase uses the preference\npairs to train the reward model r\u00f8(x, y), where \u03c6\nrepresents the trainable parameters. The trainable\nmodel can be a classification header layer attached\nto the base model or a separate model. The Bradley-\nTerry (BT) model is commonly used to represent\nthe probability that one response is better than an-\nother:\nLR(r\u00a2, D) =\nE(x,yw,y1)~D [log \u03c3(r\u2084(x, yw) \u2013 r\u2084(x,y\u0131))],\nwhere r\u00f8(x, y) models the likelihood of preferring\nYw to yi given the prompt x, and D denotes the\ndataset of preference pairs. This loss function mea-\nsures the accuracy of the reward model in predict-\ning human preferences.", "latex": ["E_{(x,y_w,y_1)\\sim D} [log \\sigma(r_{\\varphi}(x, y_w) \u2013 r_{\\varphi}(x,y_l))]."]}, {"title": "RL Fine-Tuning Phase", "content": "then fine-tunes the LM\nfurther using the learned reward function, striving\nto align model outputs with human preferences\nwhile maintaining generative diversity:\nmax Ex~D,y~\u03c0\u03bf (y|x) [r\u00a2(x, y)]\n\u03c0\u03b8\n\u2212 \u03b2DKL[\u03c0\u03bf(y|x)||\u03c0SFT(y|x)],\nbalancing fidelity to human feedback with the\npreservation of the model's original capabilities.\nHere, \u03c0\u03b8 represents the policy of the language\nmodel parameterized by 0, the trainable param-\neters. The optimization in equation 2 is carried out\nusing Proximal Policy Optimization (PPO) method\n(Schulman et al., 2017) and throughout the rest of\nthe paper, we use RLHF and PPO interchangeably\nto refer the same approach."}, {"title": "latex", "content": ""}, {"title": "2.1.2 Direct Preference Optimization (DPO)", "content": "DPO offers a refined approach to fine-tuning lan-\nguage models by directly leveraging preference\ndata, bypassing the explicit reward model construc-\ntion typically associated with RLHF methodologies\n(Rafailov et al., 2024). This method reformulates\nthe two-step optimization procedure in equations\n1 and 2 into a single optimization problem that si-\nmultaneously optimizes the policy and encodes an\nimplicit reward mechanism based on the preference\ndata.\nLDPO(\u03c0\u03b8; \u03c0ref) =\n[logo (BlogBlogBlog (log\nE(x,yw,y1)~D logo Blog\n\u03c0\u03bf(\u03b6\u03b9 | x)\n\u03c0\u03bf(\u03b6\u03c9 | X)\nTref(yl | | x)\nTref(Yw | x)\nHere, ref refers to a reference model which is\ntypically chosen as the SFT model \u03c0SFT. This op-\ntimization method is preferred over PPO because\nit simplifies training by optimizing directly on the\npreference data, which improves computational ef-\nficiency and is easier to implement (Rafailov et al.,\n2024; Xu et al., 2024). Note that in PPO (equa-\ntion 2), contrary to DPO (equation 3), the final\nmodel being optimized is not directly aligned using\nthe data D. This is the key intuition behind why\nPPO-aligned models are less susceptible to privacy\nthreats compared to their DPO counterparts."}, {"title": "latex", "content": ""}, {"title": "2.2 Membership Inference Attacks (MIA) on\nLLMS", "content": "MIA poses a significant privacy risk in the con-\ntext of LLMs, challenging the security of data used"}, {"title": "in training such models (Shokri et al., 2017; Nasr\net al., 2018).", "content": "In LLMs, MIAS seek to determine\nwhether specific data was part of the model's train-\ning set, exploiting the model's behavior or output\nnuances to infer data membership. These attacks\nare particularly concerning for models trained on\nvast datasets, where inadvertently revealing indi-\nvidual data points could lead to privacy breaches.\nThe effectiveness of an MIA against LLMs is\nquantified by a score function M, mapping input\nsamples and the level of access to a real-valued\nscore indicating the likelihood of membership. For\na given threshold 7, an input x is classified as a\ntraining set member if M(x, Access(0)) \u2265 \u0442.\nM : X \u00d7 Access(\u0398) \u2192 R.\nResearch on MIAs targeting LLMs underscores\nthe need for robust privacy-preserving techniques\nto safeguard training data, with implications for the\ndevelopment and deployment of secure, trustwor-\nthy AI systems (Carlini et al., 2020)."}, {"title": "latex", "content": ""}, {"title": "2.3 Problem Statement", "content": "Current research on MIAs has advanced under-\nstanding of risks in pre-trained text models, but\ngaps remain in applying MIAs to preference\ndatasets in LLM alignment. This oversight poses\nsubstantial privacy risks, given the critical role of\npreference data in shaping LLM outputs.\nLet Dpref = {(xi, Ywi, Yli)}1 represent the\npreference dataset, where xi is a prompt, Ywi is\nthe preferred response, and yli is the less preferred\nresponse. The vulnerability of this preference data\nto MIAs requires a nuanced examination, which\ncan be categorized into three distinct attack vectors:\n\u2022 Attack against prompts and preferred re-\nsponses: This attack determines whether a\nspecific pair of prompt x and preferred re-\nsponse yw has been used in training, highlight-\ning potential privacy breaches if such data can\nbe identified:\nMIAprompt, Yw : X \u00d7 Y \u2192 {0,1},\n\u2022 Attack against prompts and non-preferred\nresponses: This attack focuses on identifying\nif a pair consisting of a prompt x and a non-\npreferred response yi was part of the training\ndata, potentially exposing sensitive decision-\nmaking processes:\nMIAprompt, y\u0131: X \u00d7 Y \u2192 {0,1},\n\u2022 Attack against the entire preference tu-\nple: This more comprehensive attack as-\nsesses whether the entire tuple (x, yw, Y\u0131) can\nbe traced back to the training set, reflect-\ning a higher risk of revealing critical training\nmethodologies:\nMIADpref : X \u00d7 Y \u00d7 \u0423 \u2192 {0,1}.\nThis detailed breakdown elucidates the complex\nvulnerabilities associated with preference data in\nLLMs. By identifying these specific attack vectors,\nwe aim to advance privacy-preserving mechanisms\nthat safeguard the alignment process and ensure\nthat models respect and protect individual privacy\nwhile adhering to human ethical standards."}, {"title": "latex", "content": ""}, {"title": "2.4 Hypotheses Regarding DPO vs PPO", "content": "To guide our experimental design and directly ad-\ndress the concerns raised by our study, we propose\nthe following hypotheses. These are crafted to ex-\nplore the distinct impacts of DPO and Proximal\nPolicy Optimization (PPO) on privacy and perfor-\nmance, and are structured to align with the subse-\nquent analyses conducted in our experiments.\nHypothesis 1: Differential Vulnerability to\nMIAs: We hypothesize that the DPO model is\nmore vulnerable to MIA than the PPO model since\nthe DPO model uses preference data directly, which\nmay lead to overfitting. We empirically assess the\nMIA vulnerability of DPO and PPO models.\nHypothesis 2: Influence of Model Size on MI\u0391\nRisk: We postulate that larger models, regardless\nof the training method (DPO or PPO), will show\nincreased vulnerability to MIAs due to their greater\ncapacity to memorize training data. This hypothe-\nsis is explored in \u00a74.4.2, assessing how model size\naffects susceptibility to data leakage.\nHypothesis 3: Trade-offs Between Perfor-\nmance and Privacy: We propose that while DPO\nmay enhance alignment with human preferences\nand potentially improve task-specific performance,\nit also increases the risk of privacy breaches com-\npared to PPO. This trade-off is critically examined\nin \u00a74.4.3, comparing the performance benefits of\nDPO against its privacy drawbacks."}, {"title": "Method", "content": "Our approach introduces a tailored framework\nfor evaluating MIA on preference datasets used\nfor LLM model alignment. Traditional MIA ap-"}, {"title": "of preference data, which includes relational dy-\nnamics and contextual dependencies.", "content": "Our approach\naddresses these nuances by splitting the analysis\ninto evaluating individual components and entire\npreference tuples, and using conditional probabil-\nity ratios to compare against a reference model to\nmore accurately infer membership in the data.\n3.1 For Individual Response\nAssessing the vulnerability of individual re-\nsponses-either preferred (Yw) or not preferred\n(yi) to MIAs necessitates a nuanced approach\nthat considers the specific characteristics of prefer-\nence data. We compute the conditional probability\nratio relative to a reference model ref:\nPy =\n\u03c0\u03bf(y|x)\nTref(y|x)'\nwhere \u03c0\u03b8 represents the target model. This ratio\nmeasures the likelihood that the target model will\nproduce a specific response compared to a baseline\nmodel, indicating potential overfitting to training\ndata.\nEmploying py enhances specificity by account-\ning for the subtle nuances and context-dependent\nnature of response preferences, thus improving the\ndetection of data membership:\nMIAsingle (x, y) =\n{\n1 if py > Ty,\n0 otherwise.\nAlthough Ty is mentioned, our primary metric in\nthe experiments is the Area Under the Receiver\nOperating Characteristic (AUROC), which does not\nrequire setting a specific threshold. This approach\nallows for a flexible assessment of model sensitivity\nacross various potential values.\nThe choice of the reference model Tref serves\nas a benchmark for comparing the behavior of the\ntarget model \u03c0\u03b8. This model can be the base pre-\ntrained model from which \u03c0\u03b8 originated or a differ-\nent base model trained on the same dataset. Our\nexperiments, designed to test both scenarios, con-\nsistently demonstrate robust performance of our\nMIA method under various conditions."}, {"title": "latex", "content": ""}, {"title": "3.2 For the Entire Preference Tuple", "content": "To ascertain the membership of the complete pref-\nerence tuple (x, Yw, Y\u0131), we compute the difference\nbetween the probability ratios of the preferred and\nnot preferred responses:\n\u0394\u03c1 = \u03c1\u03c8\u03c9 - \u03a1\u03b9\u00b7\nThis measure captures the comparative preference\nstrength more effectively, offering a nuanced in-\nsight into how preference data impacts model train-\ning:\nMIAtuple(X, Yw, Y\u0131) =\n{\n1 if \u0394\u03c1 > \u03a4\u0394,\n0 otherwise.\nThe specified threshold t\u25b3 is set based on the vari-\nance observed within the training data, allowing a\nmore accurate identification of the data used during\nthe training phase."}, {"title": "latex", "content": ""}, {"title": "Experiments", "content": "4.1\nResearch Questions and Experiment\nDesign Rationale\nThis subsection outlines the key research questions\nguiding our experimental design, providing a ratio-\nnale for our methodologies. Derived from our hy-\npotheses, these questions aim to evaluate the com-\nparative effectiveness, privacy implications, and\nutility of DPO and Proximal Policy Optimization\n(PPO) in training LLMs.\nResearch Question 1: How do DPO and PPO\ndiffer in their susceptibility to Membership Infer-\nence Attacks? This question tests Hypothesis 1 by\ncomparing the vulnerability of models trained us-\ning DPO and PPO to MIA to shed light on privacy\nand data security issues.\nResearch Question 2: Does model size influ-\nence its risk of data leakage through MIAs, and\nhow does this vary between DPO and PPO trained\nmodels? In line with Hypothesis 2, this question\nexplores the impact of model size on MIA effec-\ntiveness, assessing if larger models pose greater\nprivacy risks.\nResearch Question 3: What are the perfor-\nmance and privacy trade-offs when employing\nDPO versus PPO in LLMs? Echoing Hypothe-\nsis 3, this question examines the trade-off between\nperformance and data privacy in tasks that need\nto be understood like humans, assessing whether\ngreater alignment with human preferences would\ncompromise privacy."}, {"title": "Setup", "content": "Models. Our experiments are conducted using a\nvariety of models to ensure a comprehensive evalu-\nation on different scales of model complexity. We\ninclude Mistral-7B-v0.1 (Jiang et al., 2023), as well\nas a series of models from the OpenAI GPT-2 fam-"}, {"title": "Evaluation Metrics", "content": "To comprehensively assess\nour models, we employ a dual-focused evaluation\nframework encompassing utility performance and\nMIA robustness:\n\u2022 Utility Performance: Our evaluation includes\nthe reward score of generated responses given\nby the reward model and perplexity for as-\nsessing fluency. We also incorporate com-\nprehensive diversity measures: Mean Seg-\nmented Type Token Ratio (MSSTR), Distinct-\n1, Distinct-2, Unique-1, and Unique-2 met-\nrics (Johnson, 1944; Li et al., 2015; Rama-\nmurthy et al., 2022). Additionally, we utilize\nadvanced text generation quality metrics such\nas BERTScore (Zhang et al., 2019), ROUGE\n(Lin, 2004), BLEU (Papineni et al., 2002),\nand METEOR (Banerjee and Lavie, 2005),\nwhich collectively offer a nuanced view of the\nmodels' performance in terms of fluency, ade-\nquacy, and diversity, closely mirroring human"}, {"title": "judgment in text quality assessment.", "content": "MIA Performance: To measure the model's\nsusceptibility to MIA, we utilize the Area\nUnder the Receiver Operating Characteristic\ncurve (AUROC). This metric encapsulates the\nmodel's defense against MIAs, reflecting the\nbalance between true positive rate and false\npositive rate in identifying training data.\nImplementation Details. Due to the computa-\ntional efficiency of LoRA, we used LoRA for all\nof our model training processes. Additionally, we\nhypothesized that fine-tuning LoRA at the RL stage\nwould help to ensure that the aligned model does\nnot deviate significantly from the reference model.\nTo further improve efficiency, we also used quanti-\nzation techniques. We use TRL\u00b3 for model align-\nment training. More detailed implementation infor-\nmation can be found in Appendix A."}, {"title": "Baselines", "content": "To accurately evaluate our approach, we position\nit against well-known MIA baselines specifically\ntailored for language models and preference data\nanalysis. These baselines are designed to target\nindividual components of the preference data but\ndo not extend to analyzing entire preference tuples.\nPerplexity (PPL): The loss attack method, based\non the approach outlined in (Yeom et al., 2018),\nutilizes the perplexity of a sequence to gauge how\nwell a language model predicts the tokens within\nthat sequence. Perplexity is defined as:\nP = exp\nn\n\u2211log \u03c0\u03c1(Xi|x1,..., Xi\u22121)\ni=1\nwhere a lower perplexity indicates a higher likeli-\nhood that the sequence was the training data.\nComparing to zlib Compression (Zlib): This\nmethod measures the entropy of a sequence when\ncompressed using zlib, compares the perplexity of\na model to its zlib compression entropy, and uses\ntheir ratio as an inference metric (Carlini et al.,\n2021).\nComparing to Lowercased Text (Lowercase):\nThis method evaluates the change in perplexity of\na sequence before and after it has been lowercased,"}, {"title": "latex", "content": ""}, {"title": "to assess the model's dependency on specific capi-\ntalization (Carlini et al., 2021):", "content": "Perplexity Ratio\nP(Original)\nP(Lowercased)\nComparing to Other Neural Language Models\n(Ref): This approach consists of comparing the\nease of error of sequences between the target model\nand another small model. In our experiments, we\nspecifically use GPT2 as the small model. Note\nthat our approach uses conditional probabilities,\nwhereas Ref does not.\nMIN-K% PROB (MIN-K): This method (Shi\net al., 2024) focuses on the minimum token prob-\nabilities within a text. It posits that non-member\nexamples are more likely to contain outlier words\nwith high negative log-likelihoods:\nMIN-K(x) =\n1\nE\n\u2211log \u03c0\u03c1(XiX1, ..., Xi-1)\nxi\u2208Min-K%(x)\nBy analyzing these low probability tokens, MIN-\nK% PROB provides a distinct method to infer mem-\nbership, enhancing the diversity of our baseline\ncomparisons."}, {"title": "latex", "content": ""}, {"title": "Results", "content": "This section presents the findings from our experi-\nments, highlighting the comparative effectiveness\nof our proposed MIA defense mechanism and an-\nalyzing the trade-off between model performance\nand privacy protection.\n4.4.1 Effectiveness of MIA Methodology\nThis subsection evaluates our MIA methodology\nfor identifying if preference data components were\nused in training language models. Our detailed\ncomparative analysis shows our method's high pre-\ncision in discerning sensitive data inclusions, out-\nperforming traditional MIA approaches not tailored\nfor preference data scenarios.\nTable 1 presents the AUROC scores for various\nMIA methods across Mistral-7B, Open-llama-3b,\nand Open-llama-7b models. PREMIA-base and\nPREMIA-SFT indicate using the base model or\nSFT model as the reference model respectively.\nOur method uniquely addresses the entire prefer-\nence tuple and consistently achieves the highest\nAUROC scores, demonstrating superior data mem-\nbership identification (see Figure 3 for paired tuple\nanalysis). The comparison between DPO and PPO\nreveals DPO's increased susceptibility to MIA, in-\ndicating that its enhancements in aligning models\nwith human preferences might elevate privacy risks.\nWe do not measure the entire tuple using baselines\nbecause traditional MIA methods are not designed\nto handle the relational and contextual dependen-\ncies inherent in preference data."}, {"title": "Effectiveness", "content": "Table 2 details the PREMIA-SFT results for mod-\nels of different sizes on the Stack-Exchange and\nIMDB datasets. On the Stack-Exchange dataset,\nlarge models typically have higher AUROC scores\nin all MIA scenarios, indicating that they retain\nmore specific details of the training data. How-\never, on the IMDB dataset, the Mistral-7B and\nOpen-llama models have significantly worse MIA\nperformance. One possible reason is that the task\nis too simple for them. As shown in Fig. 2, Mistral-\n7B achieves over 90% accuracy in distinguishing\nbetween selected and rejected responses in only\nthe first 0.2 epoch. Large pre-trained models like\nMistral-7B already have strong generalization ca-\npabilities, which undermines the effectiveness of\nMIA. Similarly, large GPT2 models such as GPT2-\nxl show better generalization on simple tasks, mak-\ning them less susceptible to \u039c\u0399\u0391."}, {"title": "Trade-Off between Performance and\nPrivacy", "content": "Table 3 analyzes the trade-off between vulnerabil-\nity to MIA and model utility under various Mistral-\n7B model configurations on the Stack Exchange\ndataset. The \"Reward\" row represents the average\nreward score given by the reward model for each\nof these models, indicating how well the task was\naccomplished. Clearly, DPO and PPO have bet-"}, {"title": "ter rewards compared to the rest.", "content": "Further, DPO is\nclearly more vulnerable to MIA. However, DPO\ndid not improve utility metrics such as reward and\ncomplexity. It is worth noting that PPO provides\nsimilar utility performance to DPO, but it has a\nlower AUROC. These findings are in line with ex-\nisting research, which also shows that despite DPO\nbeing relatively straightforward to train, it does not\nimprove the model performance compared to PPO\n(Ivison et al., 2024; Xu et al., 2024).\n4.4.4 Impact of Response Length on MIA\nEffectiveness\nIn this experiment, we look the effect of length of\nexamples used in preference alignment and their\ncorresponding vulnerability in terms of AUC-ROC\nof PREMIA-SFT. Figure 3 shows the MIA AU-\nROC results for the GPT-2 family of models on the\nIMDB dataset. As can be seen from the figure, for\n\"Chosen\" responses, the longer the response, the\nmore susceptible it is to MIA, while for \"Rejected\"\nresponses, the opposite is true."}, {"title": "Future Work", "content": "Our study shows that advanced privacy-preserving\ntechniques are needed when using preference data\nfor LLM alignment. Optimizing the privacy model\narchitecture without losing performance is key.\nTechniques such as DP-SGD (Abadi et al., 2016),"}, {"title": "This paper examines the vulnerability of preference\ndatasets in LLM alignment to MIAs.", "content": "We reveal that\nmodels trained with DPO are more susceptible to\nMIAs than those using PPO, posing a significant\nprivacy risk as preference data use increases. Our\nattack framework excels in detecting training data\nmembership, stressing the need for robust privacy-\npreserving methods. Larger models enhance \u0441\u0430\u0440\u0430-"}, {"title": "bilities but increase privacy risks, highlighting the\ntrade-off between performance and data security.", "content": "Limitations\nFirst, the analysis conducted in this study is lim-\nited to open-source LLMs and does not include\nproprietary or closed-source models such as Chat-\nGPT. The privacy implications and vulnerability\nto MIA of these closed-source LLMs may differ\nbecause their training data, architecture, and align-\nment techniques are not fully transparent. Second,\nthis study focuses on the privacy implications of\ntwo well-known alignment techniques (PPO and\nDPO). However, the field of LLM alignment is\nrapidly evolving, and the privacy risks associated\nwith other alignment methods can be more fully\nanalyzed in future work."}, {"title": "Implementation Details", "content": "We mainly refer to the TRL4 package for imple-\nmentation.\nLORA Setting. For all experiments, we share the\nsame LoRA setting below, using the PEFT5 pack-\nage: lora_alpha 32, lora_dropout 0.05, lora_r\n16, and no bias term.\nQuantization Setting. For all experiments, we\nuse the BitsAndBytes package for 4-bit quantiza-\ntion.\nSFT Setting. The settings for SFT are detailed\nbelow. We utilized the \"train/rl\" split of the\nstack-exchange-paired dataset, selecting 80,000\ndata points for the fine-tuning process, same data\nis used for PPO and DPO training. The prompt\nand only the preferred response are concatenated\nas input. The specific training parameters are:\nTraining Epochs: 2.0\nLearning Rate: 8e-5\nBatch Size (Training): 4\nBatch Size (Evaluation): 2\nGradient Accumulation Steps: 4\n- Learning Rate Scheduler: cosine\nWarmup Steps: 100\nWeight Decay: 0.05\nOptimizer: paged_adamw_32bit\nMixed Precision Training: fp16\nPPO Setting. The settings for PPO are detailed\nbelow. We filter out data points with maximum\nlength constraints. We also limit the maximum\nlength of the generated response. The specific\ntraining parameters are:\nBatch Size: 16\nMini Batch Size: 4\nGradient Accumulation Steps: 4\nPPO Epochs: 6\nLearning Rate: 5.4e-5\nKL Coefficient: 0.1\nAdaptive KL Control: True\nTarget KL: 5.0\nHorizon: 4000\nTraining Epochs: 4\nMaximum Output Length: 128\nMaximum Prompt Length: 256\nMaximum Sequence Length: 1024"}, {"title": "DPO Setting", "content": "The settings for DPO training are\ndetailed below. The specific training parameters\nare:\n- Batch Size (Training): 8\nBatch Size (Evaluation): 2\nGradient Accumulation Steps: 2\nTraining Epochs: 3.0\nLearning Rate: 5e-4\nWarmup Steps: 100\nMaximum Sequence Length: 1024\nMaximum Prompt Length: 256\nOptimizer Type: paged_adamw_32bit\nBeta: 0.4"}]}