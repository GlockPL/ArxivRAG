{"title": "Is attention all you need to solve the correlated electron problem?", "authors": ["Max Geier", "Khachatur Nazaryan", "Timothy Zaklama", "Liang Fu"], "abstract": "The attention mechanism has transformed artificial intelligence research by its ability to learn relations between objects. In this work, we explore how a many-body wavefunction ansatz constructed from a large-parameter self-attention neural network can be used to solve the interacting electron problem in solids. By a systematic neural-network variational Monte Carlo study on a moir\u00e9 quantum material, we demonstrate that the self-attention ansatz provides an accurate, efficient, and unbiased solution. Moreover, our numerical study finds that the required number of variational parameters scales roughly as N\u00b2 with the number of electrons, which opens a path towards efficient large-scale simulations.", "sections": [{"title": "I. INTRODUCTION", "content": "Solving the many-electron Schr\u00f6dinger equation for solids is an exceedingly difficult problem due to the exponential growth of the Hilbert space dimension. Various techniques based on the variational principle have long been developed to approximate the ground state of interacting electron systems using trial wavefunctions. The Hartree-Fock method based on independent electron approximation [1, 2] usually captures 99% of the total energy [3], but misses electron correlation effects, the driving force behind fascinating quantum phenomena such as high-temperature superconductivity and the fractional quantum Hall effect.\nRecently, neural network (NN) based variational Monte Carlo (VMC) has been developed for solving the many-electron problem with high accuracy [4\u201311]. Compared to human-designed trial wavefunctions, neural network wavefunctions contain a large number of parameters, has enormous representation power, and can be optimized efficiently. NN-VMC has been shown to be highly accurate in calculating the ground state energy of interacting electrons in atoms and molecules [9\u201313], lattice models [6, 14, 15], uniform electron gas [16\u201321], moire semiconductors [22, 23], and fractional quantum Hall liquids [24, 25].\nDespite the rapid progress, two important questions remain open. First, a number of NN architectures have so far been introduced and used to study different many-electron problems. Is there any hope of finding a unifying architecture that applies to a wide range of interacting electron systems? Second, it is essential to assess the finite size effect in numerical simulations of solid state systems. How does the performance of the neural ansatz change as the system size increases?\nIn this work, we present a general NN-VMC method to solve many-electron problems in solids, where electron correlations in the NN ansatz are entirely produced from the self-attention mechanism. The attention mechanism was originally introduced in the context of large language models to learn relations between words [26]. Here, the attention mechanism is employed to identify and quantify how electrons influence each other and how such influence affects their individual orbitals. This enable the construction of NN wavefunctions from Slater determinants of generalized orbitals that depend on the configuration of all electrons [11]. By minimizing the energy using Monte Carlo and neural network techniques [3], variational energy and wavefunction for the many-electron ground state are obtained.\nThe performance of our attention based NN ansatz is evaluated for interacting electrons in semiconductor moir\u00e9 heterobilayers, such as WSe2/WS2. This moir\u00e9 platform hosts a fascinating variety of correlated electronic states, including Mott insulators [27, 28], generalized Wigner crystals [29], and strongly correlated Fermi liquids [30]. Here, doped electrons reside on one semiconductor layer and experience a moir\u00e9 potential. The Hamiltonian thus takes the form of a two-dimensional Coulomb electron gas in a periodic potential [31]:\n$$H = H_0 + H_{ee}$$\n$$= \\sum_i \\frac{1}{2m^*} \\nabla_i^2 + V(r_i) + \\frac{1}{2} \\sum_{i \\neq j} \\frac{1}{r_i - r_j}$$\n$$(1)$$\nwhere $$V(r) = \\sum_{j=1}^3 V_0 cos(g_j \\cdot r + \\varphi)$$ is the moir\u00e9 potential with reciprocal lattice vectors $$g_j = \\frac{4 \\pi}{\\sqrt{3}a_m} (cos \\frac{2 \\pi j}{3}, sin \\frac{2 \\pi j}{3})$$, moir\u00e9 lattice constant am, and \u03c6 controls the shape of the moir\u00e9 potential. Despite its conceptual simplicity, this Hamiltonian exhibits a variety of electron phases that emerge from the interplay between kinetic energy, moir\u00e9 potential and Coulomb interaction [32, 33].\nFirst, for small system size, we benchmark our NN results with band-projected exact diagonalization. Remarkably, the NN energies are found to be lower even when five bands are included in the exact diagonalization. Next, we assess the performance of the NN wavefunction as the system size increases. Specifically, we study how the required number of parameters Npar scales with"}, {"title": "II. TRADITIONAL NUMERICAL METHODS", "content": "In this section, we present Hartree-Fock and band-projected exact diagonalization solutions of the interacting Hamiltonian for TMD heterobilayers. Results from these standard numerical methods will provide the benchmark for NN-VMC in the following sections.\nThroughout this work we study periodic solids. Numerical simulation of a periodic solid requires to divide the space into periodic supercells of finite size and require periodic boundary conditions on the wavefunction. With periodic boundary conditions, the Coulomb interaction needs to be carefully calculated to account for interactions of particles within the supercell with all their images in the other supercells, see App. A for details."}, {"title": "A. Hartree-Fock", "content": "The Hartree-Fock approximation to the ground state of an interacting fermionic system is obtained by minimizing the energy over the space of single Slater determinant states,\n$$\\Psi_{HF}(r_1, ..., r_N) = \\frac{1}{\\sqrt{N!}} det \\begin{pmatrix} \\phi_1(r_1) & ... & \\phi_1(r_N) \\\\ : & & : \\\\ \\phi_N(r_1) & ... & \\phi_N(r_N) \\end{pmatrix}$$(2)\nThis wavefunction ansatz captures the quantum-mechanical \"exchange\" effect by incorporating the Pauli exclusion principle, but neglects correlations arising from electron-electron interactions. Single Slater determinant states have a simple structure that enables efficient computation of observables using Wick's theorem. At the same time, this structure enables a self-consistent determination of the optimal one-body orbitals \u03c6i(r) underlying the Slater determinant wavefunction.\nBy construction, the energy of the Hartree-Fock ground state is the energy of the best wavefunction constructed from independent orbitals. This motivates the definition of correlation energy as the energy difference between the true ground state and the Hartree-Fock ground state\n$$E_{corr} = E_{GS} - \\langle \\Psi_{HF} | H | \\Psi_{HF} \\rangle$$(3)\nIn molecular systems and most solid-state systems, the correlation energy typically is only about one percent of the ground state energy [3]. However, it plays an essential role in driving various quantum phases of matter ranging from superconductivity to fractional quantum Hall states."}, {"title": "B. Band-projected exact diagonalization", "content": "One approach to capture electron-electron correlations is to construct a finite set of Slater determinant wavefunctions and exactly diagonalize the full Hamiltonian projected onto this finite subspace of the Hilbert space. This approach is variational because the restricted choice of Slater determinants forms a variational wavefunction ansatz. The obtained ground state energy is an upper bound on the true ground state energy, because inclusion of additional Slater determinants can further lower the energy.\nA good choice of Slater determinants spanning the variational subspace is obtained by diagonalizing the non-interacting part of the Hamiltonian H0, i.e. the first term in Eq. (1), and keeping only the lowest energy eigenstates up to a cutoff. For a periodic system, the single-particle eigenstates are Bloch states labeled by the Bloch momentum k and band index n. Projecting onto the the subspace of lowest Nbands bands, the Hamiltonian matrix elements between Slater determinants is written in second quantization,\n$$H = \\sum_{n=1}^{N_{bands}} \\sum_{k, \\sigma} E_{k \\sigma} c^{\\dagger}_{k \\sigma n} c_{k \\sigma n} + \\frac{1}{N} \\sum_{n, m, \\atop n',m'=1}^{N_{bands}} \\sum_{k'pkp} V_{k'p'k,p; \\sigma \\sigma'} c^{\\dagger}_{k' \\sigma n'} c^{\\dagger}_{p' \\sigma' m} c_{p \\sigma' m'} c_{k \\sigma n}$$(4)\nwhere $$c^{\\dagger}_{k \\sigma n}$$ creates a Bloch state in the n'th band at crystal momentum k and spin \u03c3 with corresponding single-particle energy En\u03c3, and $$V_{nn'm'm}(k', \u03c3, n; p', \u03c3', m | V | k, \u03c3, n'; p, \u03c3', m')$$\nare the corresponding matrix elements of the Coulomb interaction projected onto the band basis. Eq. (4) contains the Madelung energy NEM/2 that describes the interaction of charges in the periodic supercell with their own images in other supercells, see App. A for details.\nFor a small size system, the band-projected Hamiltonian H can be numerically diagonalized to yield the ground state energy and wavefunction. The band-projected exact digonalization (BP-ED) method is accurate only if the interaction strength is small compared to the gap to excitations involving high-energy bands. However, as we shall show below, interaction-induced band mixing is substantial for realistic material parameters, and therefore it is necessary to include multiple low-lying bands to obtain quantitatively accurate results, which limits the practical application of BP-ED to very small system size."}, {"title": "III. NEURAL NETWORK WAVEFUNCTION", "content": "While Hartree-Fock and BP-ED studies have provided useful results on moir\u00e9 semiconductors, they suffer from respective limitations. The Hartree-Fock method fails to capture electron correlation effect and cannot describe entangled states such as fractional Chern insulators, while BP-ED is severely limited by finite size effect and band truncation.\nRecently, a new type of variational method based on neural network wavefunctions has been developed to approximate the ground state of many-electron systems. Traditional variational wavefunctions for correlated systems are artfully designed by humans, tailor made for a given quantum phase, and usually contain only a few variational parameters. In contrast, thanks to their universal approximation capability, deep neural networks can be used to generate powerful wavefunction ansatz, which has a large number of variational parameters and can accurately represent distinct phases of matter in a unified way [10, 34].\nIn the following, we show how an expressive neural network wavefunction ansatz capturing correlations be-"}, {"title": "A. Deep neural network for one-body orbitals", "content": "A single particle orbital describes the motion of a single electron in an external potential. Mathematically, it is represented by a function \u03c6(r) from real space coordinates Rd to the space of complex numbers C describing amplitude and phase of the electronic wave at the given position r \u2208 Rd. Due to the universal approximation theorem for deep feed forward neural networks [34], any such function can be approximated to arbitrary accuracy when sufficiently many hidden layers are included. Therefore, deep feed-forward neural networks are the main structural element that we employ to construct general single-particle orbitals.\nThe structure of the compositional wavefunction ansatz consists of three sections with distinct functionality. First, the function input i.e. the electron coordinate r in coordinate space in our case is transformed into a representative feature\n$$f = feature(r)$$(5)\nFor systems with open boundary conditions, it is sufficient for the function feature(r) to represent r in dimensionless units. Systems with periodic boundary conditions satisfy\n$$\\Psi(r_1, ..., r_i, ..., r_N) = \\Psi(r_1..., r_i + L, ..., r_N)$$(6)\nfor any particle i, where the two vectors L = nL1 + mL2 with n, m \u2208 Z specify the supercell size and geometry. For periodic systems, we choose the function feature(r) to express the coordinate r in terms of periodic coordinates [35],\n$$feature(r) = \\begin{pmatrix} \\frac{sin(G_1 r)}{cos(G_1 r)} \\\\ \\frac{sin(G_2 r)}{cos(G_2 r)} \\end{pmatrix}$$(7)\nwhere GL = \\frac{2 \\pi}{L} \u03b4ij are the reciprocal supercell vectors and \u03b4ij is the Kronecker delta. This periodic feature representation uniquely specifies the electron coordinate on the torus and ensures that proximate positions on the torus are passed as proximate features to the network, enabling efficient representation of general periodic functions by the neural network. This modification adapts the quantum chemistry implementation Psiformer of Ref. [11] to periodic solids.\nSecond, the featured coordinate is embedded in the internal representation of the neural network by a linear transformation\n$$h^0 = W^0 f^0$$(8)\nwhere W0 \u2208 RdL \u00d7 R2ddim is a rectangular matrix with dL the width of the internal network layers. This embedded featured is passed through the multilayer perceptron neural network with functional form\n$$h^{l+1} = h^l + tanh(W^{l+1} h^l + b^{l+1})$$(9)\nwhere Wl+1 \u2208 RdL \u00d7 RdL is the linear transformation between layers, bl+1 \u2208 RdL is an activation bias vector, and tanh is the non-linear activation function of the nodes, and l = 0, 1, ..., L - 1 enumerates the layers.\nFinally, the wavefunction is obtained by projecting the output of the neural network hL onto real and imaginary part of a complex number,\n$$\\phi(r) = w_0 \\cdot h_L + iw_1 \\cdot h_L$$(10)\nwhere w0, w1 \u2208 RdL are learnable projection vectors and \".\" denotes the scalar product."}, {"title": "B. SlaterNet: Neural network for unrestricted Hartree-Fock", "content": "The ability to construct general single-particle wavefunctions directly enables the construction of general Slater determinant wavefunctions for N electrons. This is achieved by passing each electron coordinate ri through the feed-forward neural network,\n$$f = feature(r_i)$$\n$$h^0 = W^0 f^i$$(11)\n$$h^{l+1} = h^l + tanh (W^{l+1} h^l + b^{l+1})$$(12)\n$$(13)$$\nwhere the same transformations Wl and shifts bl are applied to the stream generated by the coordinates of each particle. Thereby, this compositional function generates a single function in a high-dimensional space before projecting onto the space of N single-particle orbitals CN. Then, N distinct single-particle orbitals are constructed as\n$$\\phi_j(r) = w_{2j} \\cdot h^L + iw_{2j+1} \\cdot h^L$$(14)\nwith individually learnable projections vectors w2j, w2j+1. The projection vectors are not required to satisfy an orthogonality criterion, because non-orthogonal directions are projected out upon forming the Slater determinant as in Eq. (2).\nBecause all single-electron wavefunctions are universally approximated by the deep neural network, this ansatz universally approximates all single Slater determinant wavefunctions. By minimizing the energy over this variational space, the best set of single-particle orbitals is identified. This is equivalent to an unrestricted Hartree-Fock calculation. We refer to this neural network structure as \"SlaterNet\" throughout the manuscript."}, {"title": "C. Self-attention neural network for electron correlations", "content": "Correlations due to interactions among particles can be described by specifying how the state of individual particles i is modified by interactions with all remaining particles j\u2260i. To capture electron correlation effects, it is necessary to go beyond Hartree-Fock ansatz as described earlier. One approach is to promote the single-particle orbitals \u03c6j(ri) to the correlated orbitals \u03c6j(ri; {r/i}) with explicit dependence on the other particles position {r/i}. The idea of correlated orbitals dates back to the back-flow transformation used in variational study of superfluid helium and uniform electron gas [36, 37], and has recently been applied to neural network wavefunctions [10, 11, 38, 39].\nIn this work, inspired by [11], we use the self-attention mechanism [26] to learn how particles influence each other and how such influence affects their individual orbitals. Specifically, the self-attention SELFATTN\u00bf operates on the entire set of outputs {hl} from the previous layer and generates the set of intermediate states {fl} that is passed as input to the next perceptron layer that generates the next state {hl+1},\n\u2192{hl} SELFATTN {fl} PERCEPTRON {hl+1} \u2192 ...\nThe following explains the self-attention mechanism and the mathematical expression of the compositional function is summarized in Eqs. (19) and (20) below.\nThe first step of the self-attention operation is to define three distinct features for each element of the set {hl}. The three features\u2014called \u201ckeys\u201d, \u201cqueries\u201d, and \"values\"\u2014are vectors obtained by the linear transformations\n$$k_i^h = W_k h^i$$, $$W_k^h : R^{d_L} \\to R^{d_{Attn}}$$(15)\n$$q_i^h = W_q h^i$$, $$W_q^h : R^{d_L} \\to R^{d_{Attn}}$$(16)\n$$v_i^h = W_v h^i$$, $$W_v^h : R^{d_L} \\to R^{d_{AttnValues}}$$(17)\nwhere the transformations Wkh, Wqh, and Wvh are learned and are independent of the particle index i. The vector space dimensions dAttn and dAttnValues are typically much smaller than the dimension dL of hl because they only represent individual features of the state hl. Here, we have used multihead attention mechanism, with the index h labeling independent applications of distinct transformations Wkh, Wqh, Wvh to extract multiple sets of keys, queries and values.\nThe features \u201ckey\u201d kh and \u201cquery\u201d qh are elements of the same vector space Rdattn to allow direct comparison between different electron streams i and j. Specifically, in the attention mechanism the relevance of streams j for a selected \"key\" streams i is quantified by the similarity measure $$exp(q_i^h \\cdot k_j^h)$$. By weighting the scalar product qh \u00b7 kh with the exponential function, the most relevant streams j for each \u201ckey\u201d stream i are singled out. This procedure can be interpret as approximating the adjacency matrix of a graph describing the most relevant relations [40]. The \"value\" feature vh quantifies the influence the stream j can exert on other streams i \u2260 j.\nAltogether, the self-attention operation identifies for each key stream i the most relevant stream j according to the feature representations Wkh, Wqh and returns the value vth corresponding to the most relevant stream j, up to exponentially suppressed contributions from less relevant streams,\n$$SELFATTN_i (\\{h^i\\}; W_k^h, W_q^h, W_v^h) = \\frac{1}{N} \\sum_{j=1}^N exp(q_i^h \\cdot k_j^h) v_j^h$$(18)\nwhere a normalization factor is included\n$$N = \\sqrt{\\sum_{j=1}^N d_{head} exp (q_i^h \\cdot k_j^h)}.$$\nThe output of all attention heads h is then accumulated and used to generate the input for the next perceptron layer:\n$$f^{l+1} = h^l + W_{concat} [ SELFATTN_h (\\{h^i\\}; W_k^h, W_q^h, W_v^h) ]$$(19)\n$$h^{l+1} = f^{l+1} + tanh(W^{l+1} f^{l+1} + b^{l+1})$$(20)\nwhere the accumulation is performed by the learned transformation W! \u2208 RdL \u00d7 RNheadsdhead projecting the output of the attention heads onto the dimension of hl. Equations (19) and (20) replace the compositional relation Eq. (13) in the construction of one-body orbitals as in SlaterNet. By mixing different single-particle streams with the self-attention mechanism, correlations between the particle states are captured.\nThe generalized single-particle orbitals \u03c6j(ri, {r/i}) constructed by the self-attention operation are permutation equivariant in the coordinates of the remaining electrons {r/i},\n$$\\phi_j(r_i; \\{\\ldots r_1, \\ldots, r_k, \\ldots\\}) = \\phi_j(r_i; \\{\\ldots r_k, \\ldots, r_l, \\ldots\\})$$(21)\nfor any k, l \u2260 i. The permutation equivariance is necessary for the generalized Slater determinant to represent a fermionic wavefunction that is antisymmetric under permutation of particle coordinates. This is property is inherent to the functional form of the self-attention operation Eq. (18).\nTo further enhance the expressive power of our neural network wavefunction, it is useful to construct multiple Slater determinants from the neural network output by projection onto m = 1, ..., Ndet distinct sets of correlated orbitals for each determinant,\n$$\\phi_j^m (r_i; \\{r/i\\}) = w_{2j}^m \\cdot h_L + iw_{2j+1}^m \\cdot h_L$$(22)\nwhere the projection vectors $$w_{2j}^m$$, $$w_{2j+1}^m$$ are individually learnable.\nIn summary, the full wavefunction ansatz is of the form\n$$\\Psi(R) = \\sum_{m=1}^{N_{det}} det (\\phi_j^m (r_i; \\{r/i\\}))$$\n$$(23)$"}, {"title": "IV. VARIATIONAL MONTE CARLO", "content": "Techniques based on a variational wavefunction ansatz require efficient numerical evaluation of their energy to achieve the optimization goal of approximating the ground state of the system Hamiltonian. This is achieved by Monte Carlo sampling of the particle configuration and corresponding energy according to the distribution determined by the variational wavefunction. This section summarizes the principal concepts of the variational Monte Carlo technique applicable to any variational wavefunction ansatz and refer to Ref. [3] for a detailed introduction. A schematic overview of the Monte Carlo method is presented in Fig. 2."}, {"title": "A. Sampling expectation values", "content": "The optimization goal to approximate the ground state of a correlated electron Hamiltonian with a variational wavefunction ansatz \u03a8(R) can be chosen to minimize the energy expectation value\n$$E_\\theta = \\frac{\\int dR \\Psi^*_\\theta (R) H \\Psi_\\theta(R)}{\\int dR |\\Psi_\\theta(R)|^2}$$(24)\nThe numerically expensive integrals over configuration space R\u2208 Rd\u00b7Nel, where R = (r1, ..., rN) are particle positions, can be efficiently evaluated by noticing the identity\n$$\\frac{\\int dR |\\Psi_\\theta(R)|^2 \\frac{\\Psi^*_\\theta(R) H \\Psi_\\theta(R)}{|\\Psi_\\theta(R)|^2}}{\\int dR |\\Psi_\\theta(R)|^2}$$(25)\nThis integral can be efficiently approximated by sampling configurations R = (r1, ..., rN) according to the distribution |\u03a8\u03b8(R)|2,\n$$E_\\theta \\approx E_{R \\sim |\\Psi_\\theta(R)|^2} [E_{loc,\\theta}(R)]$$ (26)\nwhere the local energy was introduced,\n$$E_{loc,\\theta} = \\frac{\\Psi^*_\\theta(R) H \\Psi_\\theta(R)}{|\\Psi_\\theta(R)|^2}$$(27)\nand $$E_{R \\sim |\\Psi_\\theta(R)|^2} [E_{loc,\\theta}] = \\sum_M E_{loc,\\theta}$$ denotes a summation of the local energy evaluated for M sampled configurations R from the distribution |\u03a8\u03b8(R)|2.\nThe variance of the local energy equals the energy uncertainty \u0394E of the system Hamiltonian,\n$$(\u0394E)^2 = \\langle H^2 \\rangle - \\langle H \\rangle^2$$\n$$= E_{R \\sim |\\Psi_\\theta(R)|^2} [E^2_{loc,\\theta}(R)] - E_\\theta$$(28)\nwhere the equality is established by inserting 1 = |\u03a8\u03b8(R)|2 |\u03a8\u03b8(R)|2 before and in between the Hamiltonian operators when computing \u27e8H2\u27e9 using similar manipulations as the ones leading to Eq. (26). The energy uncertainty is proprotional to the distance |\u27e8\u03a8\u03b8\u03a8o\u27e9|2 of the variational state \u03a8\u03b8 from an eigenstate \u03a8o of the system Hamiltonian [41]. In an eigenstate, the energy uncertainty is zero.\nThe variational wavefunction ansatz \u03a8\u03b8 generally is unnormalized because computing the norm itself is numerically demanding. Unnormalized probability distributions can be sampled using the Metropolis-Hastings algorithm [42, 43]; this technique is applied in the numerical results presented in this manuscript."}, {"title": "B. Optimizing variational parameters", "content": "To optimize the parameters of the wavefunction, the steepest descent direction is identified as the direction that minimizes the cost function L[\u03b8 + d\u03b8] for a step d\u03b8, where the distance of the step is measured on the variational wavefunction space ||d\u03b8|| [44],\n$$||d\\theta||^4 = 1 - |<\\Psi_{\\theta+d\\theta} | \\Psi_\\theta >|^2 \\approx \\sum_{n,m} g_{nm}(\\theta) d\\theta_n d\\theta_m$$(29)\nup to higher order corrections, where the summation includes all variational parameters \u03b8n and gnm(\u03b8) is the quantum geometric tensor\n$$g_{nm}(\\theta) = <\\partial_{\\theta_n} \\Psi_\\theta | (1 - |\\Psi_\\theta><\\Psi_\\theta|) |\\partial_{\\theta_m} \\Psi_\\theta >$$(30)\nIn this way, the information of how each parameter affects the wavefunction is included in the distance measure. The resulting optimal step then includes the inverse of the quantum geometric tensor [44],\n$$d\\theta = -\\eta g^{-1}(\\theta) \\nabla_\\theta L[\\theta] .$$(31)\nwhere \u03b7 controls the length of the optimizer step d\u03b8 and is commonly known as the learning rate. This procedure is known as natural gradient descent.\nIn practice, for variational models with large number of parameters, computing the inverse of the quantum geometric tensor is numerically impractical. For neural network wavefunction ans\u00e4tze, the inverse of the geometric tensor can be efficiently approximated by the Kronecker-factorized approximate curvature (KFAC) method [10, 45], where correlations between different layers as well as input and output of individual layers are neglected. Following Ref. [10], this method is applied in our numerical calculations. The current implementation of KFAC operates only on the absolute magnitude of the wavefunction, while the phase is neglected. In this case, the quantum geometric tensor reduces to the Fisher information matrix."}, {"title": "V. RESULTS", "content": "This section presents the numerical results obtained from our variational Monte Carlo calculations of Hartree-Fock using SlaterNet and the minimal self-attention NN Psi-Solid and the benchmark with band-projected exact diagonalization.\nThroughout this article, we consider WSe2/WS2 as model system, with the following model parameters determined by first-principles calculations [46]: effective mass m* = 0.35me, moir\u00e9 potential strength Vo = 15 meV, and moir\u00e9 shape parameter \u03c6 = \u03c0/4. We consider a moir\u00e9 lattice period of am = 8.031 nm which results from the lattice mismatch between WSe2 and WS2. In this case, moir\u00e9 filling of \u03bd = 1 particles per moir\u00e9 unit cell corresponds to a density of n = 1.785 \u00d7 1012 cm\u22122. The relative dielectric constant for a surrounding dielectric hBN is \u03f5 \u2248 5. Devices with tunable dielectric constants are possible using tunable dielectrics such as SrTiO3 [47]. All our calculations are performed for the spin-polarized system.\nThese units are converted to the dimensionless units of Eq. (1) as follows. In Eq. (1), distances are measured in effective Bohr radii $$a^*_B = \\frac{h^2 \\epsilon}{m^* e^2} = 4.7660 \\frac{\\epsilon}{m^*}$$a0 and energies in effective Hartree $$H^*_a = \\frac{h^2}{m^* a^{*2}_B} = \\frac{\\epsilon^2}{m^*} \\frac{m_e}{\\epsilon^2}$$Ha, where a0 and Ha are the standard Bohr radius and Hartree energy defined in terms of the free electron mass me, m* is the effective mass for charge carriers, and \u03f5 is the relative dielectric constant of the surrounding medium.\nWe summarize the hyperparameters used for training the models in Table II in the App. C. While architectural parameters must be adjusted for each specific system (as detailed below), we found that the remaining hyperparameters were largely independent of the problem's specifics and consistently yielded stable and optimal results. We particularly highlight the learning rate \u03b70 = 10, which, although unusually high, emerged as the optimal value for a wide variety of 2D systems we experimented with, including both free Fermi liquid and Moir\u00e9 systems."}, {"title": "A. Convergence and scaling with system size", "content": "In any numerical method, it is crucial to ensure convergence and address how the numerical complexity scales with the system size. Here, we first demonstrate convergence of the energy estimate as a function of Monte Carlo steps. Next, we investigate the saturation of the converged energy with the network dimensions. This enables us to numerically estimate the numerical complexity in terms of the scaling of the required number of variational parameters with the number of electrons.\nTo ensure the convergence of our results, we verified that the learning curves (i.e. the local energy averaged of the batch size of each step) exhibited stable behavior and, on average, decreased monotonically. This trend remained consistent across different system sizes, as illustrated by the raw learning curves for the 9- and 27-site systems at \u03bd = 2/3 filling in Fig. 3(a), shown without smoothing. We highlight the mean and standard deviation of the obtained mean-energy-per-batch using a dashed line and a stripe. The latter arises from the finite batch size used for sampling at each iteration and sets an important scale, as independent learning curves typically remain within one standard deviation of the mean.\nTo further validate our approach, we examine the convergence of the ground-state energy per particle as a function of the number of variational parameters for the 9-, 12-, and 27-site systems at \u03bd = 2/3 filling. Fig. 3(b) explores networks spanning three orders of magnitude in variational parameters and reveals a clear convergence pattern in all cases, with the ground-state energy approaching a lower bound as the network reaches 1 million variational parameters.\nThis distinct converging pattern indicates convergence saturation, meaning that further increasing the number of parameters does not improve the ground-state energy. We define the saturation point N\u2217par for a given system as the threshold beyond which the converged energies consistently fall within one standard deviation of the lowest observed energy. In Fig. 3(b), these values are marked with stars.\nWe extract these saturation points as a function of the number of electrons N and find that they follow a well-defined scaling law:\n$$N_{par} \\approx 320 \\times N^{2.1} .$$\n$$(32)$This fit is shown in Fig. 4. This empirical result suggests a significantly better scaling compared to tensor network methods whose required number of parameters scales as eVN [48].\nWhile this scaling law provides a useful estimate of the number of parameters required to achieve a given accuracy, the mere parameter count is not the only relevant factor. Since attention heads play a crucial role in capturing correlations, a minimum number of attention heads and layers (approximately 2-3 in our tests) is necessary to reach the ground state. Simply increasing the total number of parameters without ensuring sufficient attention heads and layers would not improve performance. Our scaling law serves as a guideline within the regime where these minimal requirements are met. We leave a more detailed analysis of scaling laws as a function of individual network dimensions for future work."}, {"title": "B. Benchmark for small system size", "content": "After ensuring convergence and saturation in the number of variational parameters, we benchmark our self-attention NN results with scHF and BP-ED to assess the quantitative accuracy of predictions from the self-attention NN.\nWe study a variety of different system sizes and regimes at different interaction strengths. The comprehensive energy comparisons given in Fig. ?? graphically evince the difference in ground state energy levels between all 3 methods for 9 unit cell system again at \u03bd = 2/3. The self-attention NN energy is considerably lower than scHF, with the difference precisely representing the correlation energy. We see that the correlation energy is about 2% in our moir\u00e9 system, considerably higher than the correlation energy in ordinary molecular systems [3]. This enhanced correlation energy underscores the relevance of moir\u00e9 systems for studying strongly correlated phenomena. Additionally, we observe that as the number of bands increases in BP-ED, the ground-state energy approaches but remains higher than that of the self-attention NN.\nThe advantage of self-attention NN becomes even more pronounced in larger systems, such as those with 18 electrons [Table I], where BP-ED is limited to a single band due to the enormous Hilbert space dimension. In this case, the self-attention NN energy is nearly 2.5% lower than that of BP-ED. In fact, the band mixing is so strong that even scHF achieves a lower energy than BP-ED.\nThese results demonstrate that self-attention NN consistently achieves lower energies than both BP-ED and scHF, underscoring its capacity to express ground states of the moir\u00e9 systems to higher accuracy, and revealing the variational superiority of our self-attention NN method."}, {"title": "C. Fermi Liquids and Generalized Wigner Crystals", "content": "With the variational superiority of the self-attention NN method established, we now demonstrate the capability of the self-attention NN to not only reveal the essential physics of the strongly correlated system, but predict expected phenomenon. In real homobilayer systems, experimentalists can adjust displacement field to tune the level of electron-electron interactions, driving the system from a Fermi liquid state to generalized Wigner crystal state as interactions increase. While heterobilayer systems do not offer the same experimental freedom, a metal-insulator transition have been theoretically predicted as electron interactions grow stronger by previous ED and QMC studies [49, 50"}]}