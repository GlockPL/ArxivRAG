{"title": "Progress Report: Towards European LLMS", "authors": ["Mehdi Ali", "Michael Fromm", "Klaudia Thellmann", "Jan Ebert", "Alexander Arno Weber", "Richard Rutmann", "Charvi Jain", "Max L\u00fcbbering", "Daniel Steinigen", "Johannes Leveling", "Katrin Klug", "Jasper Schulze Buschhoff", "Lena Jurkschat", "Hammam Abdelwahab", "Benny J\u00f6rg Stein", "Karl-Heinz", "Sylla Pavel Denisov", "Nicolo Brandizzi", "Qasid Saleem", "Bhowmick Anirban", "Chelsea John", "Pedro Ortiz Suarez", "Malte Ostendorff", "Alex Jude", "Lalith Manjunath", "Samuel Weinbach", "Carolin Penke", "Shima Asaadi", "Fabio Barth", "Rafet Sifa", "Fabian K\u00fcch", "Ren\u00e9 J\u00e4kel", "Georg Rehm", "Stefan Kesselheim", "Joachim K\u00f6hler", "Nicolas Flores-Herr"], "abstract": "We present preliminary results of the project OpenGPT-X. At present, the project has developed two multilingual Large Language Models (LLMs) designed to embrace Europe's linguistic diversity by supporting all 24 official languages of the European Union. Trained on a dataset comprising around 60% non-English data and utilizing a custom multilingual tokenizer, our models address the limitations of existing LLMs that predominantly focus on English or a few high-resource languages. We detail the models' development principles, data processing techniques, tokenizer optimization, and training methodologies. The models demonstrate competitive performance across multilingual benchmarks, as evidenced by its performance on European versions of ARC, HellaSwag, MMLU, and TruthfulQA.", "sections": [{"title": "1 Introduction", "content": "LLMs represents a disruptive technology that has the potential to be applied in numerous applications. Therefore, it is crucial that the technology and expertise to build these models is democratized to enable different communities and organizations to employ these models for their use cases. Many efforts in developing open-source LLMs have been undertaken, such as BLOOM (Scao et al., 2022), LLaMA-3 (Dubey et al., 2024), OLMo (Groeneveld et al., 2024), Aya (\u00dcst\u00fcn et al., 2024), and Mistral (Jiang et al., 2023). These can be split into efforts focusing on building monolingual English models and efforts focusing on multilingual models.\n\nThe current open-source models are predominantly English-centric, limiting their use in a multilingual context such as within the European Union. Furthermore, the open-source efforts disclose different levels of granularity regarding sharing details about the development of the models. While information regarding the model architecture is usually shared, the dataset composition and filtering are often not described in detail, hindering the reproduction of the works.\n\nTo address the aforementioned limitations, we present our effort in developing a multilingual base model that has been trained on top of all 24 European official languages and the corresponding instruction-tuned model. The results represent our preliminary results. An open-source publication of the models is planned for the near future."}, {"title": "2 Related Work", "content": "Since the introduction of GPT-3 (Brown et al., 2020), several open-source/open-weights efforts have been undertaken to train LLMs. While the large majority of the work focus on English-centric models (Zhang et al., 2022; Touvron et al., 2023a,b; Groeneveld et al., 2024; Dubey et al., 2024), there have been also efforts training multilingual models.\n\nOne of the most prominent examples is BLOOM (Scao et al., 2022), a 176B LLM trained on 46 natural languages. Further examples that specifically address multilingualism are the encoder-decoder models mT5 (Xue et al., 2021), XLM (Lample and Conneau, 2019), XLM-R (Conneau et al., 2020), and the encoder model mBERT (Devlin et al., 2019).\n\nUnlike the previously mentioned efforts, we specifically address 24 official European languages and focus on ensuring that a large fraction of the training data is composed of non-English data, representing a major step towards European LLMs. Concurrent to our work, EuroLLM (Martins et al., 2024), a 1.7B decoder-only LLM that follows the same spirit as our undertaking by addressing all 24 European languages, has been presented."}, {"title": "3 Pre-Training Data", "content": "Our training dataset contains approximately 4 trillion tokens, of which 13.45% is curated data (cf. Table 9), while the remaining 86.55% (cf. Table 8) originates from web data. We generated our web data by processing 60 WET dumps (cf. Appendix A.1.1) from CommonCrawl with the Ungoliant pipeline (Abadji et al., 2022). The resulting web documents were additionally filtered by removing all documents with at least one quality warning\u00b9 and harmful perplexities smaller than 5. We removed around 90% of the documents by filtering based on these annotations. In a final step, we performed MinHash deduplication and further removed 50% of the filtered documents.\n\nRegarding language distribution, the dataset covers all 24 official European languages. As illustrated in Figure 1 and Figure 2, 41.70% of the tokens stem from English content. By additionally including German, French, and Spanish, we approach around two-thirds of the total tokens.\n\nWe found that there is a strong correlation between the volume of linguistic data and the population of the countries where these languages are spoken, with a correlation coefficient of 0.987 (P < .001)2."}, {"title": "4 Multilingual Tokenization and Fertility Impact on Model Efficiency", "content": "In multilingual natural language processing (NLP), it is crucial to train balanced multilingual tokenizers (Petrov et al., 2023; Ali et al., 2024). English-centric tokenizers or unbalanced multilingual tokenizers affect inference costs and latency during inference for non-English queries. Furthermore, it prevents the model from learning long-range dependencies in limited context windows (Vaswani et al., 2017).\n\nTo address these limitations, we developed a custom multilingual tokenizer, closely following Ali et al. (2024), that is optimized for all 24 official European languages. It aims to reduce excessive text fragmentation. This phenomenon, termed as high \"fertility\", refers to the average number of tokens generated per word.\n\nFertility (F) is defined as the ratio of the total number of tokens (T) to the total number of words (W) in a text, as shown in the following equation:\n\n$F = \\frac{T}{W}$\n\nWe conducted a fertility analysis on 2,000 sentences from the FLORES-200 dataset to compare tokenizers. Because the dataset is translated across languages, i.e., the analysis is conducted on semantic equivalent content, it provides a reliable basis for evaluation. A comparison with other widely-used tokenizers is presented in Figure 3\n\nOur custom tokenizer demonstrates that for 19 out of the 24 languages, fertility values are similar to or lower than those of related tokenizers. This effect is especially pronounced in languages with complex morphology or long word structures, such as Finnish, German, and Hungarian.\n\nLowering fertility enables longer queries and documents to be processed without exceeding the context window. This is particularly advantageous in tasks that require the processing of legal or medical documents, where maintaining the integrity of long documents is essential for accurate understanding."}, {"title": "5 Base Model", "content": "In the following, we describe the model architecture and training (Section 5.1), the used training framework (Section 5.2), and the training infrastructure (Section 5.3)."}, {"title": "5.1 Model Architecture & Training", "content": "Our model is a 7B transformer-based decoder-only model. Table 1 provides an overview of our model architecture. We want to highlight some of the architectural choices that are derived from internal ablation studies and findings from related work. Our models have a sequence length of 4096 tokens and employ Rotary (Su et al., 2024) positional embeddings that are employed to train state-of-the-art models (Dubey et al., 2024). To accelerate inference and reduce memory requirements, we employed grouped query attention (Ainslie et al., 2023).\n\nUsing the causal language modelling training objective, we trained our model on 4T tokens covering all 24 European languages as described in Section 3. As an optimizer, we employed AdamW and used a cosine learning rate schedule starting with learning rate of 3e-5, increasing it to the maximum learning rate of 3e-4 within the first 10,000 steps, and decaying it afterwards."}, {"title": "5.2 Software", "content": "We selected our training framework based on efficiency in terms of throughput (TFLOP/s) and maintenance. Therefore, we decided to train our models based on a fork of MegatronLM (Korthikanti et al., 2023) that supports various scalability features ensuring efficient training of transformer-based decoder-only models. In particular, we made use of 3D parallelism, i.e., data, tensor, and pipeline parallelism. Additionally, we used ZeRO (Korthikanti et al., 2023) to reduce memory requirements further by sharding the optimizer state."}, {"title": "5.3 Training Infrastructure", "content": "We trained our models on JUWELS Booster 3 that comprises 936 compute nodes, each of which contains 4\u00d7 NVIDIA A100 (40 GB) GPUs connected via NVLink3 intra-node, and through Mellanox HDR200 InfiniBand inter-node. For our training runs, we utilized up to 512 GPUs."}, {"title": "6 Instruction Tuned Model", "content": "In this section, we describe the creation of the instruction tuned model, including the dataset composition (cf. section 6.1), the instruction tuning (cf. section 6.2) and the evaluation (cf. section 6.3)."}, {"title": "6.1 Data", "content": "The dataset for instruction tuning is composed of English data translated into German, English data, and multilingual data covering all 24 official European languages. The dataset is composed of several subsets, as listed in Table 2.\n\nEnglish To select high-quality English data, we employed a rigorous multi-step filtering process. First, we computed reward scores for all English examples using Starling-RM-7B-alpha\u2074. Our objective was to balance the dataset, aiming to include a similar number of English and multilingual examples. We achieved this by including all multi-turn examples, the complete code alpaca' subset, and the entire high-quality portion of the LMSYS-Chat dataset subset. For the remaining subsets Open Orca and both versions of the Evol instruct datasets we selected the highest-scoring examples to ensure that each subset contributed an equal amount of high-quality data."}, {"title": "6.2 Post-Training", "content": "For instruction-tuning the model, we extended FastChat for multilingual system prompts and translated the system prompt \"A chat between a human and an artificial intelligence assistant. The assistant gives helpful and polite answers to the human's questions.\" for all 24 official EU languages. We employed standard instruction tuning by calculating the loss only for the assistant responses of a multi-turn conversation. We trained the model on 8xH100 GPUs for 2.5 days for three epochs."}, {"title": "6.3 Multilingual Instruction Evaluation", "content": "For evaluating the multilingual instruction-following capabilities of the model, we utilized MT-Bench-X (Weber et al., 2024) across all five available evaluation languages: English, German, French, Italian, and Spanish. The results are presented in the Appendix A.2. While the creative writing skills of the model show good performance, the mathematical, coding, and reasoning skills are inferior across languages."}, {"title": "7 Results", "content": "In this section, we describe the results of our base and instruction-tuned models. We focus on a multilingual evaluation because our effort targets multilingualism, especially in the official European languages.\n\nWe conducted our evaluation based on ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfullQA (Lin et al., 2022), and MMLU (Hendrycks et al., 2021) which has been translated into 21 out of 24 official European languages providing a comprehensive assessment of the models' capabilities. We compared our models on these benchmarks against Aya-23-8B, LLaMA 3.1 8B, LLaMA 3.1 8B Instruct, Mistral 7B v0.3, and Mistral 7B Instruct v0.3. Note that the Mistral models compared to in this study were not trained as multilingual models and are intended to be used as English-only models. Results from other models are available on the European Leaderboard6.\n\nSection 7.1 presents our results across all investigated 21 languages, Section 7.2 discusses the performance of our models on the six widely spoken languages English, German, French, Italian, Spanish, and Portuguese, and Section 7.3 presents our results on the remaining 15 languages."}, {"title": "7.1 Performance on 21 European Languages", "content": "In Table 3, we present the performance of various language models on multilingual benchmarks across 21 European languages.\n\nOurs (Instruct) demonstrates strong performance, particularly excelling in the ARC and HellaSwag benchmarks. It achieves the highest accuracy on HellaSwag with 62.1%, outperforming all other models. On the ARC benchmark, \"Ours (Instruct)\" also attains the top score of 57.6%, indicating superior reasoning and commonsense understanding in multilingual contexts. This suggests that our instruction-tuning approach effectively enhances the model's ability to handle diverse languages in complex reasoning tasks.\n\nHowever, Ours (Instruct) shows a lower performance on the MMLU benchmark, scoring 44.1%, which is below the scores of LLaMA 3.1 8B Instruct (57.6%) and LLaMA 3.1 8B (55.6%). This indicates that while our model excels in certain areas, there is room for improvement in domain-specific knowledge across multiple languages. On TruthfulQA, \"Ours (Instruct)\" achieves a moderate score of 51.1%, which is competitive but still trails behind Mistral 7B Instruct v0.3 (54.8%) and LLaMA 3.1 8B Instruct (53.2%).\n\nThe average score of \"Ours (Instruct)\" is 53.7%, placing it below LLaMA 3.1 8B Instruct (56.3%) but above Mistral 7B Instruct v0.3 (52.7%). The standard deviations across the languages are relatively low for our models, suggesting more consistent performance across different European languages compared to models like Aya-23-8B and Mistral 7B variants, which exhibit higher variability. This consistency is crucial for applications requiring reliable performance in multilingual settings.\n\nIn summary, \"Ours (Instruct)\" shows promising results, particularly in reasoning and commonsense tasks, and maintains consistent performance across languages. Further work is needed to enhance its domain-specific knowledge and truthfulness in responses to match or surpass the leading models in all benchmarks."}, {"title": "7.2 Performance on Common European Languages", "content": "Table 4 focuses on the performance of the models on six widely spoken European languages: English, German, French, Italian, Spanish, and Portuguese. These languages are well-represented in training data and are commonly used in evaluating multilingual models.\n\nOurs (Instruct) model achieves an average score of 56.9%, which is lower than the top-performing LLaMA 3.1 8B Instruct model at 62.3%. However, it should be highlighted that our model has been trained based on around 4T tokens, which is significantly less data than LLaMA 3.1 8B, which has been trained on 15T tokens. The training dataset sizes of the other models are not revealed. Notably, \"Ours (Instruct)\" performs competitively on the HellaSwag benchmark, attaining 67.9%, which is slightly higher than LLaMA 3.1 8B Instruct (67.7%) and Mistral 7B Instruct v0.3 (67.0%). This reinforces our model's strength in understanding and generating coherent continuations in context-rich scenarios.\n\nOn the ARC benchmark, \"Ours (Instruct)\" scores 63.2%, which is close to Mistral 7B Instruct v0.3 (65.4%) and LLaMA 3.1 8B Instruct (64.8%), indicating good performance in multiple-choice question answering. However, in the MMLU benchmark, our model scores 46.5%, which is significantly lower than LLaMA 3.1 8B Instruct's 63.2%. This suggests that our model may have limitations in specialized knowledge domains within these common languages.\n\nFor TruthfulQA, \"Ours (Instruct)\" achieves 49.9%, below Mistral 7B Instruct v0.3's 56.8% and LLaMA 3.1 8B Instruct's 53.5%. This indicates that while our model is proficient in generating plausible responses, it may sometimes sacrifice factual accuracy, highlighting an area for improvement in ensuring truthfulness.\n\nOverall, \"Ours (Instruct)\" demonstrates strong capabilities in certain tasks within common European languages but falls short in others compared to the leading models. Enhancing its domain-specific knowledge and factual accuracy could help bridge this performance gap."}, {"title": "7.3 Performance on Exclusive European Languages", "content": "Table 5 presents the models' performance on 15 less commonly evaluated European languages, which we refer to as exclusive languages. These include Romanian, Czech, Danish, Greek, Estonian, Finnish, Hungarian, Lithuanian, Latvian, Dutch, Bulgarian, Polish, Slovak, Slovenian, and Swedish.\n\nIn this setting, our \"Ours (Instruct)\" model is on average slightly behind LLaMA 3.1 8B Instruct mainly due to our performance on MMLU indi-cating that learning domain-specific knowledge in exclusive languages is an area for improvement. At the same time, our model obtains the strongest results on the ARC (56.3%) and HellaSwag (60.8%) benchmarks among all evaluated models. This suggests that our model is particularly effective in reasoning and understanding context in less commonly represented languages, which is a significant achievement given the challenges associated with limited training data in these languages. On TruthfulQA, \"Ours (Instruct)\" attains 51.3%, which is competitive but still trails behind Mistral 7B Instruct v0.3's 54.3%.\n\nThe standard deviations for our models are lower compared to others, reflecting more consistent performance across the exclusive languages. This consistency is crucial for real-world applications where reliability across languages is necessary.\n\nIn conclusion, \"Ours (Instruct)\" exhibits excellent reasoning and contextual understanding in less commonly evaluated European languages, outperforming other models in specific benchmarks. Enhancing its knowledge base and factual accuracy in these languages could further elevate its overall performance."}, {"title": "7.4 Comparison with Aya-23-8B on Common Languages", "content": "Table 6 compares our models with Aya-23-8B on 11 common European languages shared between the datasets used by both models. These languages include Czech, Dutch, English, French, German, Greek, Italian, Polish, Portuguese, Romanian, and Spanish.\n\nOur \"Ours (Instruct)\" model achieves an average score of 55.5% competitive to Aya-23-8B's 56.3%. On the ARC benchmark, \"Ours (Instruct)\" surpasses Aya-23-8B with a score of 60.6% compared to 59.3%, demonstrating strong reasoning abilities in these languages. Additionally, \"Ours (Instruct)\" attains a higher score on TruthfulQA (51.2% vs. 48.5%).\n\nHowever, Aya-23-8B outperforms our model on the HellaSwag benchmark with a score of 66.0% versus \"Ours (Instruct)\" at 65.1%. Aya-23-8B also leads on the MMLU benchmark with 51.3%, compared to our model's 45.2%. These results suggest that while \"Ours (Instruct)\" is competitive, particularly in reasoning and truthfulness, there is room for improvement in contextual understanding and domain-specific knowledge.\n\nOur \"Ours (Base)\" model, without instruction tuning, achieves an average score of 51.1%, which is lower than both \"Ours (Instruct)\" and Aya-23-8B. This highlights the effectiveness of instruction tuning in enhancing the model's performance across multiple tasks and languages.\n\nIn summary, \"Ours (Instruct)\" is competitive to Aya-23-8B in common European languages, especially in reasoning tasks and truthfulness. Continued refinement and training could further improve its performance, making it a more robust model for multilingual applications."}, {"title": "8 Conclusion", "content": "In this work, we presented the development of two multilingual large language models, \"Ours (Base)\" and \"Ours (Instruct)\", tailored to support the linguistic diversity of Europe by encompassing all 24 official EU languages. Through the use of a custom multilingual tokenizer and a dataset prioritizing non-English content, our models address limitations found in existing multilingual models, particularly their English-centric bias. Preliminary results demonstrate competitive performance across multiple benchmarks, including ARC, HellaSwag, MMLU, and TruthfulQA.\n\nOur ongoing efforts aim to further improve the models' performance and efficiency, ensuring they can better serve the needs of diverse European communities and facilitate the broader democratization of LLM technology across multilingual environments. Future work will focus on improving the models in specialized knowledge domains, math and code capabilities."}, {"title": "A Appendix", "content": "A.1 Data\nA.1.1 Dumps\nIn this section, we outline the Common Crawl data utilized in our LLM training, detailing the cutoff dates and the data collection period. The dumps span multiple years, with varying distributions of weeks per year (cf. Table 7), which is critical for understanding the temporal coverage of the training data."}]}