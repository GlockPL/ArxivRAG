{"title": "LML: Language Model Learning a Dataset for Data-Augmented Prediction", "authors": ["Praneeth Vadlapati"], "abstract": "This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called \u201cLanguage Model Learning (LML)\u201d powered by a new method called \u201cData-Augmented Prediction (DAP)\u201d. The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words \"Act as an Explainable Machine Learning Model\" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine Learning (ML) algorithms are often deployed to make predictions on new, unseen data [1]. Large Language Models (LLMs) process text effectively and have diverse applications [2]. Unlike ML algorithms, LLMs can process data as text in a way similar to how humans process text. Recent advancements focus on developing long-context and small-sized LLMs that have satisfactory performance scores [3] at a rapid pace. The usage of LLMs is becoming faster and more affordable, making them usable for more applications. LLMs can be used for classification tasks [4]."}, {"title": "A. Comparison with Machine Learning and Deep Learning", "content": "ML models do not have a balance between interpretability and accuracy [5], [6]. Many machine learning models have internal decision-making processes that are regarded as \u201cblack-box\u201d systems because they are not interpretable or explainable [7]. While neural networks are reasonably accurate, they are not easily interpretable [8]. A lack of interpretability is a major concern in applications in critical areas such as healthcare and legal systems, where understanding the reason behind each decision is crucial for accountability and trust [9], [10]. ML models are vulnerable to several issues, such as noise in data, unusual patterns of data, data bias, and data poisoning attacks [11], [12], all of which compromise the model's performance and reliability."}, {"title": "B. Disadvantages of Pre-processing and Feature Engineering", "content": "For data scientists, tasks like data cleaning, pre-processing, and feature engineering demand a considerable amount of time. More than half of the time that data scientists spend is on preparing the data before using it [13]. These procedures delay the work, especially for complex datasets with numerous rows and columns. It takes more time and effort to repeat the same steps when additional columns get included in the datasets later. After all these steps, ML models still stand vulnerable to the bias of data scientists."}, {"title": "C. Advantages of using LLMs for Classification Tasks", "content": "LLMs can be used for classification tasks, as they can understand and analyze the data as text, unlike ML algorithms, allowing them to understand the context and replicate human specialists who are capable of manually predicting the classifications by examining the available data. Since LLMs can be asked to justify the decision of each classification, their predictions can be explainable. Transparency in their decision-making process helps users make decisions. Transparency makes it possible to improve the system for better accuracy."}, {"title": "D. Proposed Solution and Its Benefits", "content": "Data can be easily processed by LLMs, which can summarize the data by finding the key patterns, such as ranges of the values of specific columns that correlate with the classification of each class, and can be used to predict the class. This paper proposes a new concept called \u201cLanguage Model Learning (LML)\u201d powered by a new method called \u201cData-Augmented Prediction (DAP)\u201d. In the process of LML, an LLM is used to summarize the data and find patterns that help generate classifications. During the classification, the model uses DAP, which is about fetching the data relevant to the test data and augmenting it with the data summary to support the predictions for satisfactory accuracy. Prediction using LLMs does not need data cleaning and pre-processing, saving time for data scientists. Retrieval-Augmented Generation (RAG) [14] is about retrieving relevant text from the stored text to support the LLM responses. DAP is similar to RAG. DAP ensures satisfactory accuracy, which will be higher than when compared to the decisions made based on only LML. Generalization is a system's ability to perform well on unseen data instances"}, {"title": "E. New Applications of the System", "content": "DAP introduces multiple new applications, including batch processing for classification tasks. DAP makes it possible to apply explainable AI to crucial domains like healthcare and legal cases, in which the justification for each prediction is essential to arrive at a conclusion. Applications such as cybersecurity analytics require explanations since detections of fraud or cyber-attacks require a justification. Additionally, DAP can assist in the analysis of past decisions of ML models of organizations, helping them to review and understand classifications of the past to improve the models. DAP can be helpful in low-resource domains such as disaster management, where the training data is less, and the decisions generated should be accurate and explainable. The system generates summaries, which have multiple use cases such as data exploration and feature importance analysis of training data, bias detection of existing data, detecting noise and unusual patterns, and data poisoning attacks. The system can be modified to explain the decisions made by existing ML algorithms, which bridges the gap between black-box models and interpretable results."}, {"title": "II. LITERATURE REVIEW", "content": "Summary Boosting [16] was introduced as an approach to improve the learning capabilities of LLMs by developing weak learners by summarizing each subset of the data. Weak learners are aggregated to form more robust models. Unlike the above work, this paper proposes a method to utilize a long-context LLM to generate a summary of the data to find the columns and values that impact the classification the most, including the selection of a range of values that correlate with each label. Unlike the above research, LML classifies using the ranges of values instead of binning or describing parts of the data, and in DAP, the data summary is augmented with similar data fetched from the dataset on demand, which ensures more accurate and context-aware classification. Despite existing research on Machine Learning, LLMs, and RAG, there is a research gap in generating predictions using LLMs with retrieval of relevant data.\n\nThe Retrieval-Enhanced Machine Learning (REML) framework [17] built the foundation by using retrieval techniques for various general Machine Learning tasks. Further synthesis by Kim et al. [18] expanded REML's applicability across various domains, such as computer vision and time series prediction. The work on General-Purpose Retrieval-Enhanced Medical Prediction (REMed) [19] applied retrieval mechanisms in healthcare, enabling the processing of vast electronic health records (EHRs), but did not extend to classification tasks in broader contexts. However, in REML and REMed, LLMs were not used, nor were preprocessing and feature engineering minimized. These frameworks did not focus on interpretability. These frameworks have made it possible to incorporate the concepts of Information Retrieval (IR) into machine learning (ML) for better models. There are still a lot of research gaps in areas like explainability and classification tasks."}, {"title": "III. METHODS", "content": "The experiment uses different datasets of different dimensions from the UCI dataset [20] and different LLMs. Datasets used are Iris, Wine, Zoo, Raisin, Rice (Cammeo and Osmancik), and Mushroom. LLMs used are Gemini 1.5 Flash (001) [21], GPT-4o mini [22], Llama 3.1 (70B) [3], and Llama 3.1 (8B) [3]. In order to ensure fast and cost-effective responses to process large datasets, larger models are not selected."}, {"title": "B. Creating Test Data", "content": "New test data is created based on the training data. Existing rows from the training data are retained to ensure high accuracy since the model benefits from a sufficient amount of data to learn from. The number of test rows for each class is considered to be 20% of the number of train data rows that belong to the same class, with a maximum limit of 10 rows per class. The test data is created by selecting random rows with a count of double the desired test data size with each label and calculating averages of every two consecutive rows with the same label. In order to avoid consuming more time for the testing process with multiple models and datasets, no more than ten rows are created for each label. Averaging the consecutive rows ensures the test data contains the patterns of the training data and introduces slight variations. Variations test the model's ability to generalize beyond the training data. Rows are selected from all the classes to minimize the risk of bias by overfitting specific classes of the training set."}, {"title": "C. Dividing data into chunks", "content": "If a training dataset is large, the entire dataset cannot be processed by the LLM at once, as the models support a limited context length of the input. The dataset is split into chunks so that each part can be processed at a time, avoiding this limit. The number of tokens in a chunk is calculated using the model's tokenizer to find whether the model's context window limit is larger than the given number of tokens. An optimal chunk size is calculated by automatically finding the number of tokens per chunk. This approach allows the model to handle large datasets. The context limit is assumed to be 15,000 tokens, irrespective of the model, even though all the selected models have a context window of at least 128k tokens. This small limit allows for faster processing of each chunk without issues with a high processing time per chunk or the rate limits of the model."}, {"title": "D. Summarizing Each Chunk of Data", "content": "Each chunk of the dataset is converted to CSV text, and the LLM is used to summarize each chunk to find patterns which include impactful features and ranges of values that correlate with each class. The summary table will have a limited number of rows and a low dimensionality, ignoring unnecessary or redundant information and focusing only on the patterns relevant to the classification. The conversion of structured data to text makes it possible for the language model to process the data. The use of an LLM for this process ignores noise and bias in the data and mitigates data poisoning attacks, which enhances the robustness of the classification model. Automated reattempts are enabled during the summarization to retry in the cases of errors."}, {"title": "E. Generating a Final Summary of All the Summaries", "content": "A final summary of the entire dataset is generated using the LLM based on the summaries generated from all the chunks. With the final summarization, the model can combine patterns of various chunks of the dataset to create a comprehensive understanding. The model uses the final summary to find the most important patterns from the entire dataset, offering insights that support the classifications and an overview of how each feature varies within each class. Patterns captured in the final summary may not be evident when analyzing the summary of each chunk individually. The model gets context to make context-aware classifications based on the underlying data patterns by focusing on impactful features.\n\nThe overlap of patterns is also found in the final summary table to warn the model about a potential misclassification and allow it to adjust its decision-making process accordingly. Additional metadata in the table includes the number of rows contributing to each label as well as any additional comments or observations the LLM made. Using these features aids the model during query generation and classification. Additionally, the interpretability allows human experts to understand the decision process of the model. If there are numerous chunks that do not fit the context window limit, only the supported number of chunks are processed at a time to create consolidated summaries from which a final summary is generated."}, {"title": "F. Retrieving Relevant Rows from the Dataset", "content": "To ensure high accuracy of predictions, each row from the test data is used independently for classification, ensuring that the prediction process is customized to that row. Based on the chosen row and the data summary, a query is generated using the LLM to retrieve relevant rows from the dataset. This targeted data retrieval ensures that the model uses only the most contextually similar data to generate classifications, enhancing the accuracy. The generated query is guaranteed to be no more than 350 characters in length to prevent errors.\n\nOnly a limited portion of the similar rows are considered due to limitations in the model's context window size. This selective retrieval also helps the model focus on a small subset of data, which aligns with the limited context window of the model, ensuring that the useful rows are used for decision-making. If a query returns an empty response, the process is retried by mentioning \"Last query that returned empty response: `{df_query}`\" in the prompt. In cases where relevant data might not be returned with a single attempt, this retry mechanism assures a robust retrieval. The context window limit used for the results in this step is double the small limit stated earlier to ensure reasonable accuracy."}, {"title": "G. Generating Classifications and Calculating Accuracy", "content": "Classifications are generated by the LLM based on the relevant data from the last step, in addition to the summary of the dataset. The retrieved relevant rows and the broader context from the dataset summary ensure a more balanced and comprehensive classification process. The model reduces biases that occur from relying only on the summary or relevant data alone by utilizing both with equal weightage. The model classifies unseen data by assigning each label to a set of distinct feature ranges associated with the label by observing the data summary. Following the generation of the classifications, the accuracy is determined by comparing the predictions with the ground truth labels of the test data. Prediction data is stored to analyze and find misclassification patterns to improve the system for more accurate predictions in the future."}, {"title": "IV. RESULTS", "content": null}, {"title": "A. Summary of the Dataset", "content": "A sample final summary of the Iris dataset, when tested with Llama 3.1 (70B), is presented in the table below. The generated summary displays the important statistical ranges for each feature across various classes and the average values that the model estimated. Metadata about each class, such as the number of rows that represent it and comments on the patterns of values, are included in the table."}, {"title": "B. Generating Classifications and Calculating Accuracy", "content": "The table below mentions the accuracy results of multiple test cases using multiple models across multiple datasets. An accuracy of 80% or above is considered good, while an accuracy of 90% and above is considered excellent. A 50% accuracy score or less is regarded as inadequate. The results were found to be satisfactory across a majority of the test cases. Results demonstrate"}, {"title": "V. DISCUSSION AND LIMITATIONS", "content": "Although the proposed system takes an unconventional approach, more testing on complex datasets is still needed to confirm its usefulness in practical applications. Similar to ML models, the system might misclassify a test row that is more similar to rows from other classes than from its own class. The system can be developed for numerical predictions with future research since it could offer significant benefits for time series or regression tasks. The current system's reliance on fetching relevant data and generating summaries for each test case introduces latency, which is an issue when implementing the system using large datasets or in real-time applications."}, {"title": "VI. CONCLUSION", "content": "The system proposed in this paper, \u201cLanguage Model Learning (LML),\u201d powered by \u201cData-Augmented Prediction (DAP),\u201d has demonstrated promising results in generating classifications during the experiment across different datasets using different Large Language Models (LLMs). The system classification accuracies exceeded 80% or sometimes even 90% in multiple test cases, highlighting the potential of outperforming conventional Machine Learning (ML) models. The accuracy is due to LML, which involves the usage of LLMs to summarize the datasets, and DAP, which involves augmenting the prediction process with relevant rows from the training data and the data summary. This method offers a new approach to enable the interpretability of the classification process with an accuracy that resembles or exceeds Machine Learning classification across multiple test cases.\n\nThe LML system introduces a new paradigm in data-driven classification by combining an LLM's summarization abilities with DAP, improving the way for further research in the intersection of Natural Language Processing and ML. Although intended for classification tasks, the potential of LML extends beyond current applications. In future research, the system can be adapted for numerical prediction tasks, such as time-series forecasting. The ongoing research addresses the current limitations of the resources and time required to get responses from LLMs."}]}