{"title": "Post-detection inference for\nsequential changepoint localization", "authors": ["Aytijhya Saha", "Aaditya Ramdas"], "abstract": "This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis:\nconducting inference following a detected change. We study the problem of localizing the changepoint\nusing only the data observed up to a data-dependent stopping time at which a sequential detection\nalgorithm A declares a change. We first construct confidence sets for the unknown changepoint when pre-\nand post-change distributions are assumed to be known. We then extend our framework to composite\npre- and post-change scenarios. We impose no conditions on the observation space or on A we only\nneed to be able to run A on simulated data sequences. In summary, this work offers both theoretically\nsound and practically effective tools for sequential changepoint localization.", "sections": [{"title": "Introduction", "content": "We consider the following problem of sequential change analysis in a general space X: a sequence of X-valued\nobservations $X_1, X_2,\\ldots$ arrives sequentially, such that for some unknown time $T\\in \\mathbb{N}\\cup\\{\\infty\\}$ (the changepoint),\n$$X_1, X_2,...,X_{T-1}\\stackrel{i.i.d}{\\sim} F_0 \\quad\\text{and}\\quad X_T, X_{T+1},... \\stackrel{i.i.d}{\\sim} F_1,$$ \nfor some pre-change distribution $F_0 \\in \\mathcal{P}$ and some post-change distribution $F_1 \\in \\mathcal{P}$, where $\\mathcal{P}$ denotes a class\nof probability distributions on $\\mathcal{X}$. We start by assuming $F_0$ and $F_1$ are known, and generalize our methods to\nencompass settings where $F_0$ and $F_1$ are only known to lie in some classes, but are not known exactly. A\nsequential changepoint detection algorithm A is simply a data-dependent stopping rule to raise an alarm\nthat a change has previously occurred. Our aim in this paper is to construct confidence sets for the unknown\nchangepoint $T$ after stopping at $\\tau$, assuming only black-box access to A (and ideally no further assumptions\nor restrictions). In other words, we hope to obtain a set $C\\subseteq \\{1, ..., \\tau\\}$ such that\n$$\\mathbb{P}_T(\\tau\\in C) \\geq 1 -\\alpha,$$ \nwhere $\\mathbb{P}_T$ is the data distribution in (1.1), i.e. having the true changepoint at $T$. We will show for algorithms\nA with a bounded average run length (ARL), no set C can satisfy such a guarantee, but if A has probability\nof false alarm (PFA) bounded above by $\\alpha$, such a guarantee is achievable. In either case, another natural\ncriterion for C is for it to satisfy\n$$\\mathbb{P}_T(T\\in C | \\tau > T) \\geq 1 - \\alpha,$$ \nmeaning that C only needs to cover if algorithm A only stops after the true changepoint T. We show how to\nconstruct such sets C for both types of algorithms (those with bounded ARL or PFA)."}, {"title": "Known pre- and post-change distributions", "content": "We now address the question of constructing confidence sets for the changepoint, assuming that our change\ndetection procedure stopped after a changepoint. We start from the simplest case when both pre- and\npost-change distributions are known, addressing more general cases in later sections.\nNotation. For $t\\in \\mathbb{N}$, let $\\mathbb{P}_t$ denote the joint distribution of the sequence of independent random variables\n$X_1,X_2,...$ when there is a changepoint at time $t$, meaning that $X_1, ..., X_{t-1}$ are from $F_0$ (with density $f_0$)\nand $X_t, X_{t+1},...$ are from $F_1$ (with density $f_1$). As corner cases, let $\\mathbb{P}_1$ denotes the joint distribution where\n$X_1, X_2,... \\stackrel{i.i.d}{\\sim} F_1$ and $\\mathbb{P}_\\infty$ denotes the case where $X_1, X_2, ... \\stackrel{i.i.d}{\\sim} F_0$. Let $\\mathbb{E}_t$ denote the expectation under\n$\\mathbb{P}_t$, for $t\\in \\mathbb{N}\\cup\\{\\infty\\}$. Suppose we have a data sequence $\\{X_n\\} \\sim \\mathbb{P}_\\mathbb{T}$ for some unknown $T\\in\\mathbb{N}\\cup\\{\\infty\\}$ and some\nsequential change detection algorithm A that raises an alarm at the stopping time $\\tau$, which is finite for the\nobserved sequence.\nIn sequential change detection, there are typically two types of Type I error metrics. One is the average\nrun length (ARL), which is defined as $\\mathbb{E}_\\infty[\\tau]$, the expected time until a false alarm occurs under $\\mathbb{T} = \\infty$ (i.e.,\nno change). The other one is the probability of false alarm (PFA), that is, $\\mathbb{P}_\\infty(\\tau < \\infty)$. We will provide\nsolutions for algorithms A that control either metric."}, {"title": "A first confidence set", "content": "Our main idea is to test whether there is a changepoint at t, for each $t\\in \\{1,\\ldots,\\tau\\}$ and then construct\nthe confidence set consisting of all time points t for which we fail to reject the tests. Formally, we test the\nfollowing null hypothesis at each candidate time point t:\n$$H_{0,t}: X_1,..., X_{t-1}\\stackrel{i.i.d}{\\sim} F_0 \\quad\\text{and}\\quad X_t, X_{t+1},... \\stackrel{i.i.d}{\\sim} F_1.$$ \nWe consider some test statistic $M_t$ for testing the null hypothesis $H_{0,t}$ such that for each $t \\in \\mathbb{N}$,\n$$M_t := \\begin{cases}M_t^{(T)}(X_1,..., X_T),& \\text{if } t \\leq \\tau < \\infty,\\\\-\\infty & \\text{if } t > \\tau \\text{ or } \\tau = \\infty,\\end{cases}$$\nwhere $M_t^{(n)}$ is some test statistic based on n many samples, for each $t, n \\in \\mathbb{N}, t \\leq n$. While our theoretical\nguarantees will not depend on any particular choice of $M_t$, we provide a prototypical choice in a later\nsubsection that we use for our experiments.\nTo test $H_{0,t}$, we draw B many independent streams of data under $H_{0,t}$. For the $j$th sequence, denoted\nas $\\{X_i^j\\}_n$, we simulate data points until a change is detected at the stopping time $\\tau_j$ using A or until time\nL, whichever occurs first. Then, we compute the same test statistics truncated at L (which we discuss in\ndetail in the next subsection), based on $\\tau_j \\wedge L$ and $X_1^j,..., X_{\\tau_j \\wedge L}^j$, which is denoted as $M_{t,L}^j$. We reject $H_{0,t}$ if\n$M_t > \\text{Quantile}(1 - \\alpha; M_t, M_{t,L}^1,\\ldots, M_{t,L}^B)$, and the confidence set is the collection of all $t \\in \\{1,\\ldots,\\tau\\}$, for\nwhich we fail to reject $H_{0,t}$, i.e,\n$$C = \\{t \\in \\mathbb{N} : t \\leq \\tau, M_t \\leq \\text{Quantile}(1 - \\alpha; M_t, M_{t,L}^1,\\ldots, M_{t,L}^B)\\}.$$"}, {"title": "Subtleties regarding Subroutine L", "content": "The problem with $L = \\infty$ is that the stopping time may never be reached for the $j$-th sampled sequence,\ni.e., we may have $\\tau_j = \\infty$. One can safely set $L = \\infty$ if the A (i.e., its stopping time) satisfies the condition\n$\\mathbb{P}_T(\\tau < \\infty) = 1$, for all $T \\in \\mathbb{N}$, meaning that if there is a change, A stops almost surely. This is a very mild\nassumption because any reasonable change detector should always eventually raise an alarm if there is a\nchange. Indeed, all algorithms with finite ARL satisfy the condition, and indeed typically so do algorithms\nthat control PFA. To avoid making even this mild assumption, one can simply pick $L < \\infty$.\nIf $L < \\infty$ and the $j$th sequence stopped because the algorithm A raised an alarm at or before L, then we\ndefine $M_{t,L}^j$ as usual, but if A had not stopped by time L, we then define $M_{t,L}^j = -\\infty$ to ensure a lower bound\non the original test statistic (if we could compute it). Therefore, for $L < \\infty$,\n$$M_{t,L}^j := \\begin{cases}-\\infty,& \\text{if } t > \\tau_j,\\\\M_t^{(T_j)}(X_1^j,...,X_{T_j}^j),& \\text{if } t \\leq \\tau_j \\leq L,\\\\-\\infty& \\text{if } t \\leq \\tau_j \\text{ and } \\tau_j > L.\\end{cases}$$\nFor the proof of the upcoming theorem, we still need a definition of $M_{t,\\infty}^j$ in the case that $L = \\infty$ and\n$\\tau_j < \\infty$. We define\n$$M_{t,\\infty}^j := \\begin{cases}M_t^{(T_j)}(X_1^j,...,X_{T_j}^j),& \\text{if } t \\leq \\tau_j < \\infty,\\\\-\\infty,& \\text{if } t > \\tau_j \\text{ or } \\tau_j = \\infty.\\end{cases}$$\nThe rationale behind setting $M_{t,\\infty}^j = -\\infty$ when $\\tau_j = \\infty$ is the following. If for all $j = 1,\\ldots, B$, we have\n$\\tau_j = \\infty$, then we want to reject $H_{0,t}$, which is achieved by setting $M_{t,\\infty}^j = -\\infty$.\nRemark 2.1. The value of L does not need to be a fixed constant; it can be set adaptively depending on the\nobserved data sequence, for instance, one can choose L to be $2\\tau$. The proof of coverage works by arguing\nvalidity when $L = \\infty$, and then pointing out that any smaller choice of L only leads to more conservatism."}, {"title": "Prototypical choice of test statistic Mt", "content": "In the offline setting, where data is observed up to some fixed time n, the likelihood of data is\n$$L = \\prod_{i=1}^{T-1} f_0(X_i) \\prod_{i=T}^{n} f_1(X_i),$$\nand the maximum likelihood estimator of T (Hinkley, 1970; Yao, 1987) is given by\n$$\\widehat{\\mathbb{T}}_n = \\arg \\max L = \\arg \\max \\prod_{i=j}^{n} \\frac{f_1(X_i)}{f_0(X_i)}.$$ \nIn our sequential setting, where the data is observed up to a data-dependent stopping time $\\tau$, we adapt this\nestimator by replacing n with $\\tau$ and define the estimator of the changepoint as:\n$$\\widehat{\\mathbb{T}} = \\arg \\max \\prod_{i=j}^{\\tau} \\frac{f_1(X_i)}{f_0(X_i)}.$$ \nThe prototypical test statistic that we employ in our simulations is the likelihood-ratio-based test statistic\n$M_t^{(n)}$, where for all $n\\in \\mathbb{N}, t \\leq n$,\n$$M_t^{(n)} (X_1,..., X_n) := \\max_{1 \\leq i \\leq n} \\frac{L_i}{L_t}.$$"}, {"title": "Unconditional coverage when A controls PFA", "content": "We now investigate the coverage properties and provide a bound on the coverage of the confidence set\nproduced by Algorithm 1.\nTheorem 2.2. Let C be the confidence set for changepoint $T \\in \\mathbb{N}$ produced by Algorithm 1, for some $\\alpha \\in (0,1)$,\n$B\\in \\mathbb{N}, L\\in\\mathbb{N}\\cup\\{\\infty\\}$ and change detection algorithm A. Then,\n$$\\mathbb{P}_T(T\\in C) \\geq 1 - \\alpha - \\mathbb{P}_\\infty(\\tau < T).$$ \nTherefore, if A controls PFA at level $\\delta$,\n$$\\mathbb{P}_T(T\\in C) \\geq 1 - \\alpha - \\delta.$$\nRemark 2.3. When $\\mathbb{T} \\ll \\text{ARL}$, we typically have $\\mathbb{P}_\\infty(\\tau < T) \\approx 0$ and so $\\mathbb{P}_T(T \\in C) \\geq 1 - \\alpha$. The confidence\nset constructed using our method will approximately achieve the desired unconditional coverage level $1 - \\alpha$,\nin situations where the probability of false detections before the true changepoint is negligible. However, T is\nnot known to us, and hence, we are unable to produce a suitable bound on the unconditional coverage for the\ndetection algorithms that almost surely stop at a finite time, even when there is no change."}, {"title": "Unconditional coverage when A has finite ARL", "content": "If the detection algorithm has finite ARL (i.e., $\\tau$ is finite almost surely), then it is impossible for any confidence\nset $C^*$, which is a subset of $\\{1,\\ldots,\\tau\\}$, to produce an unconditional coverage guarantee at a predetermined\nlevel $1 - \\alpha$. The intuitive reason is that if $\\tau$ is random and finite almost surely, it is possibly a false detection,\ni.e., $\\tau < T$, in which case the set $\\{1,\\ldots,\\tau\\}$ itself does not cover T. The next proposition shows that.\nProposition 2.4. If a confidence set $C^*$ that is a subset of $\\{1,\\ldots,\\tau\\}$ satisfies an unconditional coverage\nguarantee $\\mathbb{P}_T(T \\in C^*) > 1 - \\alpha$, for some $\\alpha \\in (0,1)$, then $\\mathbb{P}_\\infty(\\tau = \\infty) \\geq 1 - \\alpha$ (thus, the PFA of the algorithm\nis at most $\\alpha$ and thus the ARL of the algorithm is infinite). Said differently, for any algorithm with finite\nARL, it is impossible to construct a valid confidence set that is only a subset of $\\{1,...,\\tau\\}$.\nThe proposition says that if we can have a confidence set with high unconditional coverage, then the\ndetection algorithm is such that it never stops with high probability if there is no change.\nTherefore, in order to achieve an unconditional coverage guarantee with A having finite ARL, we must\nextend our confidence set beyond the stopping time $\\tau$. So, we consider the union of the confidence set\nconstructed using Algorithm 1 and all integers greater than $\\tau$. The following proposition establishes that this\naugmented confidence set achieves the desired coverage guarantee of $1 - \\alpha$.\nProposition 2.5. Let C be the confidence set for changepoint $T \\in \\mathbb{N}$ produced by Algorithm 1, for some\n$\\alpha\\in (0,1), B\\in\\mathbb{N}, L\\in\\mathbb{N}\\cup\\{\\infty\\}$ and change detection algorithm A. Then, $C^* := C \\cup \\{\\tau + 1,\\tau + 2,\\ldots \\}$\nsatisfies\n$$\\mathbb{P}_T(T\\in C^*) \\geq 1 - \\alpha.$$\nSo, the confidence set constructed using Algorithm 1 can be interpreted as the pre-detection part of the\naugmented confidence set, which has the desired coverage guarantee of $1 - \\alpha$."}, {"title": "Conditional coverage when A has finite ARL", "content": "In the previous subsection, we established that the conventional notion of coverage is unsuitable for detection\nalgorithms with finite ARL, where the confidence set is a subset of the random set $\\{1,\\ldots,\\tau\\}$. Intuitively, it\nis more appropriate to consider the conditional coverage, given that no false detection occurs, i.e., $T \\leq \\tau$.\nThis adjustment is crucial because discussing coverage becomes meaningless when $\\tau < T$, meaning that the\ndetection occurs before the true changepoint\nthe target point we wish to cover by our confidence set.\nThe following is an immediate corollary of Theorem 2.2, where we show a bound on the conditional\ncoverage of the confidence set produced by Algorithm 1.\nCorollary 2.6. For any $\\alpha \\in (0,1), B\\in \\mathbb{N}$ and $T\\in \\mathbb{N}$, and change detection algorithm A satisfying\n$\\mathbb{P}_\\infty(\\tau \\geq T) \\neq 0$, the confidence set C produced by Algorithm 1 satisfies\n$$\\mathbb{P}_T(T\\in C | \\tau > T) \\geq 1 - \\frac{\\alpha}{\\mathbb{P}_\\infty(\\tau \\geq T)}.$$\nTherefore, if A controls PFA at level $\\delta \\in (0, 1)$,\n$$\\mathbb{P}_T(T \\in C | \\tau > T) \\geq 1 - \\frac{\\alpha}{1 - \\delta}.$$\nThe restriction that $\\mathbb{P}_\\infty(\\tau > T) \\neq 0$ is very weak and clearly necessary for conditional coverage. Since\nT is arbitrary, the condition means that there is no $S < \\infty$ satisfying $\\mathbb{P}_\\infty(\\tau < S) = 1$. If the algorithm\nalways stops under $\\mathbb{P}_\\infty$ before S, then it can never detect any change that occurs after S. This is because the\ninformation that distinguishes $\\mathbb{P}_\\infty$ from $\\mathbb{P}_T$ for $T > S$ occurs after S, but the algorithm has halted before S.\nWhen $\\tau$ is finite almost surely (i.e., PFA is 1), we cannot have a positive lower bound on $\\mathbb{P}_\\infty(T > T)$\nuniformly over $T\\in \\mathbb{N}$. Therefore, Algorithm 1 does not guarantee conditional coverage at some prespecified\nlevel for detection algorithms with finite ARL (which implies that PFA is 1, although the converse is not\ntrue). Furthermore, if A is completely black-box\nmeaning its PFA is unknown it becomes infeasible to\nconstruct a confidence set with the desired conditional or unconditional coverage using this approach. We\npropose a simple modification to address the issue in the following subsection, which applies to any black-box\ndetection algorithm."}, {"title": "Modified confidence set when A has finite ARL (or A is a blackbox)", "content": "Equation (2.11) indicates that to ensure a conditional coverage guarantee of $1 - \\alpha$, we observe that we\nneed to test $H_{0,t}$ at level $\\alpha \\times \\mathbb{P}_\\infty(\\tau > T)$, rather than simply $\\alpha$. To achieve that, for each $t \\in \\{1,\\ldots,\\tau\\}$,\nwe test $H_{0,t}$ at level $\\alpha \\times \\mathbb{P}_\\infty(\\tau \\geq t)$. For each $i \\in \\{1,\\ldots, N\\}$, we generate a sequence $\\{Z_n^i\\}_n \\stackrel{i.i.d}{\\sim} F_0$ until a\nchange is detected, recording the detection times as $T_i$, which help to estimate the probability $\\mathbb{P}_\\infty(\\tau \\geq t)$, for\n$t = 1,\\ldots,\\tau$. Therefore, it suffices to draw data points only up to $\\min\\{\\tau, T_i\\}$, for $j = 1,\\cdots, N$.\nThen, we draw B many independent streams of data under $H_{0,t}$. For the $j$th sequence, denoted as\n$\\{X_i^j\\}_n$, we simulate data points until a change is detected at the stopping time $\\tau_j$, using the same change\ndetection method A. Then, we compute the test statistic $M_{t,L}^j$ as described in Section 2.2, based on $\\tau \\wedge L$\nand $X_1^j,\\ldots, X_{\\tau\\wedge L}^j$. Our confidence set is the collection of all $t \\in \\{1,\\ldots,\\tau\\}$, for which we fail to reject $H_{0,t}$,\ni.e.,\n$$C = \\{t \\in \\mathbb{N} : t \\leq \\tau, M_t \\leq \\text{Quantile}(1 - \\widehat{\\alpha}_t; M_t, M_{t,L}^1,\\cdots, M_{t,L}^B)\\},$$ \nwhere $\\widehat{\\alpha}_t = \\alpha \\times \\sum_{i=1}^N \\mathbb{I}(T_i \\geq t) /N$. Algorithm 2 contains a structured overview of the method.\nBy suitably adjusting the test levels for each t, this approach ensures that the desired conditional coverage\nguarantee is achieved. Notably, this guarantee does not depend on any specific property of A (e.g., its PFA\nor ARL). We prove it in the following theorem."}, {"title": "Known pre- change but unknown post- change distribution", "content": "In this section, we assume that the post-change distribution follows some parametric composite model\n$\\mathcal{P}_1 = \\{F_\\theta : \\theta \\in \\Theta_1\\}$ and the pre-change distribution, $F_{\\theta_0}$ $(\\theta_0 \\notin \\Theta_1)$ is known.\nNotation. For $t\\in \\mathbb{N}$, let $\\mathbb{P}_{\\theta_0,t,\\theta}$ denote the joint distribution of the sequence of independent random\nvariables $\\{X_n\\}_{n\\in\\mathbb{N}}$, when $X_1, ..., X_{t-1}$ are from $F_{\\theta_0}$ (with density $f_{\\theta_0}$) and $X_t, X_{t+1},...$ are from $F_{\\theta}$ (with\ndensity $f_{\\theta}$). As corner cases, let $\\mathbb{P}_{1,\\theta}$ denotes the joint distribution where $X_1, X_2,... \\stackrel{i.i.d}{\\sim} F_{\\theta}$ and $\\mathbb{P}_{\\theta_0,\\infty}$\ndenotes the case where $X_1, X_2, ... \\stackrel{i.i.d}{\\sim} F_{\\theta_0}$. Let $\\mathbb{E}_{\\theta_0,t,\\theta}$ denote the expectation under $\\mathbb{P}_{\\theta_0,t,\\theta}$, for $t \\in\\mathbb{N}\\cup\\{\\infty\\}$,\nfor $t\\in\\mathbb{N}\\cup\\{\\infty\\}$ and $\\mathbb{E}_{\\theta_0,\\infty}$ denote the expectation under $\\mathbb{P}_{\\theta_0,\\infty}$.\nSuppose we have the data sequence $\\{X_n\\}_n \\sim \\mathbb{P}_{\\theta_0,T,\\theta_1}$, for some unknown $T\\in\\mathbb{N}\\cup \\{\\infty\\},\\theta_1 \\in \\Theta_1$ and some\nsequential change detection algorithm A that raises an alarm at the stopping time $\\tau$ in the sequence. We\ndefine the null hypothesis that a changepoint occurs at time $t \\in \\{1,\\ldots ,\\tau\\}$ as:\n$$H_{0,t} : X_1,..., X_{t-1} \\stackrel{i.i.d}{\\sim} F_{\\theta_0}, \\text{ and } X_t, X_{t+1},... \\stackrel{i.i.d}{\\sim} F_{\\theta_1}, \\text{ for some } \\theta_1 \\in \\Theta_1.$$\nFor testing the null hypothesis $H_{0,t}$, we consider some test statistic $M_t := M_t^{(T)}(X_1,..., X_T)$ based on data\nup to time $\\tau$, if $t \\leq \\tau < \\infty$ and $M_t := -\\infty$, otherwise, for each $t \\in \\mathbb{N}$, where $M_t^{(n)}$ is some test statistic based\non n many data points, for n, $t \\in \\mathbb{N}, t \\leq n$. An example is provided in Section 3.1."}, {"title": "Composite pre- and post-change", "content": "We now assume that both pre- and post-change distributions follow some parametric composite models\n$\\mathcal{P}_0 = \\{F_\\theta : \\theta \\in \\Theta_0\\}$ and $\\mathcal{P}_1 = \\{F_\\theta : \\theta \\in \\Theta_1\\}$ respectively such that $\\Theta_0 \\cap \\Theta_1 = \\emptyset$.\nIn this setting, the average run length (ARL) of the detection algorithm is defined as $\\inf_{\\theta \\in \\Theta_0} \\mathbb{E}_{\\theta,\\infty}[\\tau]$,\nwhich represents the least expected time until a false alarm occurs under $T = \\infty$ (i.e., no change). On the\nother hand, the probability of false alarm (PFA) is defined as the worst-case probability of triggering a false\nalarm, that is $\\sup_{\\theta \\in \\Theta_0} \\mathbb{P}_{\\theta,\\infty}(\\tau < \\infty)$.\nSuppose we have the data sequence $\\{X_n\\}_n \\sim \\mathbb{P}_{\\theta_0,T,\\theta_1}$, for some unknown $T\\in\\mathbb{N}\\cup \\{\\infty\\}, \\theta_i \\in \\Theta_i (i = 0, 1)$\nand some sequential change detection algorithm A that raises an alarm at the stopping time $\\tau$ in the sequence.\nFollowing the approach from the previous sections, we define the null hypothesis that a changepoint occurs at\ntime $t \\in \\{1,\\ldots,T\\}$ as:\n$$H_{0,t} : X_1,\\ldots, X_{t-1} \\stackrel{i.i.d}{\\sim} F_{\\theta_0},\\text{ for some } \\theta_0 \\in \\Theta_0 \\text{ and } X_t, X_{t+1},... \\stackrel{i.i.d}{\\sim} F_{\\theta_1},\\text{ for some } \\theta_1 \\in \\Theta_1.$$\nWe consider some test statistic $M_t := M_t(X_1,\\ldots, X_\\tau; \\tau)$ based on data up to time $\\tau$, for testing the null\nhypothesis $H_{0,t}$. An example is provided in Section 4.1.\nSuppose the change detection algorithm, A has finite ARL or it is a blackbox algorithm so that its PFA is\nunknown. In that case, we have a problem in generating i.i.d. copies of the detection times under $T = \\infty$\n(i.e., no change), since the pre-change parameter $\\theta_0$ is unknown. Therefore, we need to assume the following:\nAssumption 4.1. There exists $\\theta \\in \\Theta_0$ such that $\\inf_{\\theta\\in \\Theta_0} \\mathbb{P}_{\\theta,\\infty}(\\tau \\geq t) \\geq \\mathbb{P}_{\\theta^*,\\infty}(\\tau \\geq t)$, for all $t \\in \\mathbb{N}$.\nThis assumption holds for a variety of change detection problems. For instance, we illustrate this in\nRemark 4.6 for some Gaussian mean change problems with CUSUM and SR-type detectors. One can easily\ncheck that it holds for analogous Gaussian, Laplace, and exponential scale change problems as well.\nWe now generate N many i.i.d. sequences under $F_{\\theta^*}$ until a change is detected using A, recording the\ndetection times as $T_1,\\ldots, T_N$, for some $N\\in \\mathbb{N}$, that will help us estimate $\\mathbb{P}_{\\theta^*,\\infty}(\\tau \\geq t)$, which is a lower\nbound on $\\mathbb{P}_{\\theta_0,\\infty}(\\tau > t)$. If A controls PFA at a known level $\\delta \\in (0,1)$, we can avoid this step and obtain a\nconditional/unconditional coverage guarantee without any such assumption.\nSince both pre- and post-change parameters are unknown, for each t, we estimate them based on\n$X_1,\\ldots, X_{t-1}$ and $X_t,\\ldots, X_\\tau$, respectively. However, as noted earlier, the sample size used for the estimates\nmight be too small to yield reliable estimates. Therefore, instead of point estimates, we rely on interval\nestimates in order to achieve a predetermined control over the error.\nSuppose that we have some confidence interval procedure CI($X_1,\\ldots, X_n; 1 - c)$, which constructs $1 - C$\nconfidence interval using $X_1,\\ldots, X_n$ for $\\theta_0$ and a confidence sequence procedure $\\{CS(X_1,\\ldots, X_n; 1 - c)\\}n$,\nwhich constructs $1 - c$ confidence sequence using $\\{X_n\\}_n$ for $\\theta_1$. If A controls PFA, we can simply construct\n$\\mathcal{S}_{T-t+1} = CS(X_t,\\ldots, X_\\tau; 1 - \\beta)$ for $\\theta_1$ having $1-\\beta$ coverage and $\\mathcal{S}_{t-1} = CI(X_1,\\ldots, X_{t-1}; 1 - \\gamma)$ for $\\theta_0$ having\n$1 - \\gamma$ coverage. Otherwise, we construct $\\mathcal{S}_{t-1} = CI(X_1,\\ldots, X_{t-1}; 1 - \\gamma r_t)$ and $\\mathcal{S}_{T-t+1} = CS(X_t,\\ldots, X_\\tau; 1 - \\beta r_t)$, where $r_t := \\sum_{i=1}^N \\mathbb{I}(T_i \\geq t)/N$. Despite $r_t$ being random, it is based on independent simulations, and so it\nturns out that marginalizing over $r_t$ yields that the CS and CI have coverage probabilities $1 - \\beta \\times \\mathbb{P}_{\\theta^*,\\infty}(\\tau \\geq t)$\nand $1 - \\gamma \\times \\mathbb{P}_{\\theta^*,\\infty}(\\tau \\geq t)$ respectively; we formalize this more explicitly in the proof of Theorem 4.2.\nNote that for $t = 1$, the only plausible confidence interval of $\\theta_0$ is $\\mathcal{S}_0 = \\Theta_0$. Therefore, if $\\Theta_0$ is unbounded,\nthen it becomes infeasible. To avoid this issue, one can assume that $T\\in \\mathbb{N}\\{1\\}$ and test only for $t = 2,\\ldots, \\tau$.\nFor each $j = 1,\\ldots, B$, for some fixed $B\\in \\mathbb{N}$, suppose we have sequences $\\{X_n^{j(\\theta_0,t,\\theta_1)}\\}_n$ having joint\ndistribution as $\\mathbb{P}_{\\theta_0,t,\\theta_1}$. Note that the sequences must be independent as j varies. However, for any fixed j, the\ni.i.d. sequences $\\{X_1^{j(\\theta_0,t,\\theta_1)}\\}_n$ are allowed to be dependent across different values of t and $\\theta \\in \\mathcal{S}_{t-1},\\theta' \\in \\mathcal{S}_{T-t+1}$.\nA specific method for generating these sequences is detailed in Section 4.2. We draw the datapoints until\na change is detected at the stopping time $\\tau_{t,\\theta_1}^j$ using A or until time L, whichever occurs first. Then, we\ncompute the same test statistics truncated at L, based on $\\tau_{\\theta_0}^j \\wedge L$ and $X_1^{j(\\theta_0,t,\\theta_1)}, ..., X_{\\tau_{t,\\theta'}^j \\wedge L}^{j(\\theta_0,t,\\theta_1)}$, which"}]}