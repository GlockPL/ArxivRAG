{"title": "Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling", "authors": ["Yiwen Ding", "Zhiheng Xi", "Wei He", "Zhuoyuan Li", "Yitao Zhai", "Xiaowei Shi", "Xunliang Cai", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": "Self-improvement methods enable large language models (LLMs) to generate solutions themselves and iteratively train on filtered, high-quality rationales. This process proves effective and reduces the reliance on human supervision in LLMs' reasoning, but the performance soon plateaus. We delve into the process and find that models tend to over-sample on easy queries and under-sample on queries they have yet to master. As iterations proceed, this imbalance in sampling is exacerbated, leading to a long-tail distribution where solutions to difficult queries almost diminish. This phenomenon limits the performance gain of self-improving models. A straightforward solution is brute-force sampling to balance the distribution, which significantly raises computational costs. In this paper, we introduce Guided Self-Improvement (GSI), a strategy aimed at improving the efficiency of sampling challenging heavy-tailed data. It leverages Socratic-style guidance signals to help LLM reasoning with complex queries, reducing the exploration effort and minimizing computational overhead. Experiments on four models across diverse mathematical tasks show that GSI strikes a balance between performance and efficiency, while also being effective on held-out tasks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated impressive ability in performing complex reasoning tasks (Wei et al., 2022b; Kojima et al., 2022; Zhao et al., 2023). While fine-tuning models on curated data can further boost performance, it relies heavily on human supervision, limiting scalability and generalization (Cobbe et al., 2021). To address this, the \"self-improvement\u201d paradigm emerges, where models generate multiple reasoning paths, filter out incorrect responses and fine-tune themselves on their own outputs without human intervention (Zelikman et al., 2022; G\u00fcl\u00e7ehre et al., 2023; Huang et al., 2023; Singh et al., 2024; Yuan et al., 2024). Despite the benefits of self-improvement, its performance typically reaches a ceiling after a few iterations (Wu et al., 2024). We perform preliminary experiments (\u00a7 4) and find that in reasoning tasks, the most significant gains from self-improvement occur in the first iteration, while subsequent iterations encounter performance bottlenecks or even degradation (Figure 2). Similar performance bottlenecks in synthetic data have also been observed in text generation (Shumailov et al., 2023) and image synthesis (Alemohammad et al., 2024). Further, we delve into the self-improvement process and conduct an in-depth analysis (Figure 3) to investigate the underlying causes behind the performance bottlenecks. On the one hand, complex problems with lengthy reasoning chains tend to amplify hallucinations, making it difficult for models to explore the vast search space and sample correct rationales (Lightman et al., 2024; Zhang et al., 2023; Xie et al., 2023; Xi et al., 2024). Consequently, the models tend to over-sample easy queries and under-sample queries they have yet to master (Tong et al., 2024). On the other hand, as iterations proceed, this imbalance in sampling is exacerbated, leading to a long-tail distribution where solutions to difficult queries almost disappear (Figure 1). This situation is also referred to as tail narrowing or tail cutting in previous studies (Dohmatob et al., 2024b; Shumailov et al., 2023). As a result, the model's self-improvement is limited, since difficult examples are also crucial for further training (Liu et al., 2024). To address this imbalance, a common approach is to allocate more sampling budget to the under-sampled, challenging queries (Tong et al., 2024), but this can be much more costly. In this paper, we propose an efficient and effective method called Guided Self-Improvement (GSI), which employs Socratic-style guidance signals (Chang, 2023; Dong et al., 2023b) to assist language models in exploring solutions for challenging queries. Specifically, we introduce an extra resampling phase called distribution re-balancing for difficult queries, applied after the generation step in the self-improvement process. During this phase, we provide targeted guidance signals to reduce sampling difficulty, narrow the sampling space, and minimize hallucinations during reasoning (Xie et al., 2023; Xi et al., 2024). As a result, GSI improves sampling quality, increases solution coverage (Bansal et al., 2024) for challenging queries, and mitigates the issue of tail narrowing. We perform experiments across four models and six mathematical reasoning tasks, including arithmetic reasoning, abstract algebra, and formal logic. The results demonstrate that GSI mitigates the performance bottlenecks of self-improvement while maintaining computational efficiency. Further analysis shows that this method leads to a more balanced solution distribution and improved model generalization across multiple reasoning tasks. In addition to natural language reasoning, our method has also been proven effective in program-based reasoning (Chen et al., 2023). Our contributions are summarized as follows:"}, {"title": "Related Work", "content": "Self-improvement for LLMs Self-improvement methods, where models refine themselves using self-generated data, have proven effective in enhancing problem-solving abilities without human intervention (Huang et al., 2023; Zelikman et al., 2022). To ensure the reliability of this process, the generated data is typically filtered using external supervision signals. These signals can be binary rewards, such as correctness checks based on reference answers (Yuan et al., 2023; Zelikman et al., 2022; Tong et al., 2024) or compiler execution feedback (Haluptzok et al., 2023). Alternatively, more nuanced approaches involve scoring (G\u00fcl\u00e7ehre et al., 2023) or ranking systems (Dong et al., 2023a), which may be generated by the model itself (Yuan et al., 2024) or external reward models (Hosseini et al., 2024; Qi et al., 2024). Some methods adopt weaker supervision signals, such as majority voting across multiple outputs (Huang et al., 2023). Once filtered, the high-quality data supports post-training through methods like SFT (Zelikman et al., 2022) or preference-based techniques like Direct Preference Optimization (DPO, Yuan et al., 2024). This process is often iterative, allowing models to continually generate new data, filter it, and use it to refine their performance further (Zelikman et al., 2022; G\u00fcl\u00e7ehre et al., 2023; Yuan et al., 2024).\nDistribution Shift in Synthetic Data The scaling law reveals a predictable increase in model performance as the volume of training data grows (Kaplan et al., 2020). With the development"}, {"title": "2.1"}, {"title": "2.2"}, {"title": "Preliminaries", "content": "Formulation of Self-improvement Given a large language model Mo and the original training dataset D = {(xi, ri, yi)}N1, where xi is the problem, ri is the chain-of-thought rationale (Wei et al., 2022b) and yi represents the final answer. Each rationale ri consists of several intermediate steps, i.e., ri = [ri,1,..., ri,L]. The self-improvement process iteratively leverages the model's correct responses to enhance its problem-solving abilities gradually. The process involves T iterations, where each iteration comprises two main steps: Generate and Improve.\nGenerate step. At iteration t, based on the previous model Mt\u22121, we perform K sampling operations for each query xi \u2208 D:\n(ri, \u0177i) = Mt\u22121(Xi)\nThe newly generated data points form a set D' = {(xi,,Y) | Xi \u2208 D, j = [1, K]}. Each sampled \u0177i is evaluated using a reward function rf(yi, \u0177i) \u2208 {0,1}, which checks its correctness based on ground-truth answers yi. Only the correct solutions are filtered to form the high-quality dataset Dt at iteration t.\nImprove step. In the t-th iteration, we use the self-generated high-quality dataset Dt to fine-tune the model Mt. The fine-tuning objective is to minimize the negative log-likelihood (NLL) loss:\nLSFT = -E(x,r)~De \u2211L1=1 log M (ri | r<1, x).\nBy minimizing LSFT, the model iteratively improves its ability to generate correct rationales, and this process is repeated for T iterations to achieve self-improvement. Note that in the first iteration, we directly fine-tune Mo on the original dataset D to obtain M\u2081.\nBiased Sampling and Tail Effect In the self-improvement sampling process, there is a tendency to select higher-quality and more accurate data, which introduces a phenomenon known as sampling bias (Alemohammad et al., 2024; Shumailov et al., 2023). This bias often results in the truncation or narrowing of low-probability \"tails\" in the data distribution.\nTo illustrate the effects of sampling bias, consider a one-dimensional Gaussian distribution"}, {"title": "3.1"}, {"title": "3.2"}, {"title": "Performance Bottleneck and Tail Narrowing in Self-Improvement", "content": "Despite extensive research into self-improvement methods (G\u00fcl\u00e7ehre et al., 2023; Singh et al., 2024), the performance dynamics across successive training iterations remain underexplored. To this end, we conduct experiments across four backbone models to investigate the effects of sampling and tail data during iterative training.\nPerformance trends. To uncover the relationship between performance trends with sampling, we vary the number of sampling k. Our results indicate that lower sampling times (e.g., k = 1 or k = 2) degrade model performance, leading to negative gains. Notably, as depicted by the purple solid lines in Figure 2, Llama2-7B, Mistral-7B, and Llama3-8B underperform compared to SFT on the original dataset (corresponding to the performance of iteration 1). This decline stems from the weaker reasoning abilities of these models, which limit the coverage of challenging queries. Consequently, the models gradually tackle only basic problems, resulting in a degradation or collapse of reasoning ability (Shumailov et al., 2023; Dohmatob et al., 2024a). As k increases, the most notable performance gains occur during the first iteration, shown by the green and orange solid lines in Figure 2. However, after the third iteration, the progress halts, eventually reaching a performance plateau.\nImpact of tail data. As previously discussed, merely scaling the number of sampling eventually encounters performance bottlenecks. However, the low-probability and challenging tail data are often considered more crucial for improving model performance (Sorscher et al., 2022; Liu et al., 2024; Tong et al., 2024). To investigate the impact of tail data, we conduct additional experiments targeting these difficult examples. For queries that do not yield a correct response after k sampling attempts, we supplement them with a golden rationale, ensuring that every query has at least one practically correct response and effectively rebalancing the long-tail distribution. We use hollow markers to represent the performance with tail data. As shown in Figure 2, across different numbers of sampling and model variants, the performance represented by the dashed line exceeds that of the solid line representing vanilla self-improve. The results demonstrate that rebalanced data helps mitigate performance degradation under a limited sampling budget. Moreover, a larger budget alleviates performance plateaus and boosts model efficacy to some extent. This study highlights the value of solutions to challenging queries in overcoming the limits of finite and progressively biased sampling."}, {"title": "4.1"}, {"title": "4.2"}, {"title": "Guided Self-Improvement", "content": "To mitigate the observed tail-narrowing phenomenon, a straightforward solution is to increase the sampling budget for tail data. However, directly tackling these difficult queries from scratch often results in low success rates and higher costs due to repeated failed attempts. Drawing from Socratic-style education (Chang, 2023; Dong et al., 2023b), we introduce guidance-based exploration techniques to improve sampling efficiency, such as learning from demonstrations (Schaal, 1996; Subramanian et al., 2016). We provide the model with tailored assistance in structured contexts, enabling it to address difficult queries more effectively. The following paragraphs outline four guiding strategies we propose.\nAnswer-driven. This strategy incorporates the ground-truth answer yi along with the input query xi as context to guide the generation process. It helps the model better align with the expected solution, particularly for challenging queries (Zelikman et al., 2022). Formally, at each iteration t, instead of inputting only xi into the model Mt-1, we extend the prompt by appending yi as a hint:\n(ri, Yi) = Mt\u22121(xi, hint(yi)).\nThis approach helps the model focus on the reasoning process behind the answer, reducing the overall difficulty of the task.\nRationale-driven. In this strategy, we further extend the input by introducing a rationale ri, which helps the model derive the correct reasoning process. Unlike the answer-driven method, providing a rationale offers a more detailed reference for the model to follow, narrowing the exploration space of reasoning paths (Yang et al., 2024). Formally, at iteration t, the input to the model Mt\u22121 is augmented as follows:\n(ri, Yi) = Mt-1(xi, hint(ri)).\nThis approach enables the model to handle queries it has yet to master and ensures a higher coverage of solved problems. Importantly, it alleviates the hallucinations when the model tries to provide reasoning paths for problems it doesn't fully understand (Lanham et al., 2023), improving the reliability of generated data.\nInteractive sampling. Inspired by previous work in the area of Interactive RL (Subramanian et al., 2016; Suay and Chernova, 2011), we introduce feedback from a stronger model Ms after the model Mt-1 fails. Instead of providing hints along with the query, this dynamic process ensures that the model can explore its own solution before receiving external guidance. Formally, after the model Mt\u22121 generates an incorrect answer, we re-sample by giving it both its prior incorrect output and the feedback fi from Ms as additional context:\n(error, yerror) = Mt\u22121(xi),\nfi = Ms(Xi, ri, Yi, ferror),\n(ri, \u0177i) = Mt\u22121(xi, rerror, fi).\nThis feedback includes an analysis and correction of the model's errors, reducing its reliance on the correct answer. Through this interactive process, we balance exploration and correction, enabling the model to learn from its mistakes without overly restricting its reasoning path.\nState reset. Drawing inspiration from the concept of state reset, i.e., going back to intermediate states during problem-solving to refine the approach (Chang et al., 2024; Xi et al., 2024), we adopt a strategy where the model is guided step by step with partial rationales. Instead of supplying the full rationale ri immediately, after l incorrect attempts, the model is provided with the preceding reasoning steps r<l from ri = [ri,1,..., ri,L], gradually narrowing down the exploration space:\n(ri, \u0177i) = Mt\u22121(xi, hint(ri,<1)).\nThis method reduces the difficulty of queries at a fine-grained level, relieving the model of cognitive overload while still allowing flexibility in the model's solution. Although it increases the number of attempts, the incremental hint helps identify the threshold where the model can solve problems independently with minimal guidance."}, {"title": "5.1"}, {"title": "5.2"}, {"title": "5.3"}, {"title": "5.4"}, {"title": "Experiments", "content": "Experimental Setups Models. We conduct experiments using four widely adopted foundation models, including Llama2-7B-Base (Touvron et al., 2023), Llama3-8B-Base (Dubey et al., 2024), Deepseek-Math-7B-Base (Shao et al., 2024), and Mistral-7B-v0.3 (Jiang et al., 2023). For the stronger model in the interactive sampling process, we employ Llama3-70B-Instruct (Dubey et al., 2024).\nDatasets. We utilize six math reasoning datasets. These include arithmetic reasoning datasets such as GSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), MathQA (Amini et al., 2019) and SVAMP (Patel et al., 2021), as well as a more challenging dataset MATH (Hendrycks et al., 2021). We also include TheoremQA for abstract algebra and formal logic. To evaluate generalization, we choose AQUA, GSM8K, MATH as held-in datasets and MathQA, SVAMP, TheoremQA as held-out datasets. To ensure consistency in answer format across different datasets, we utilize the unified data provided by the MathInstruct dataset (Yue et al., 2024) and follow its train-test splits. More dataset statistics can be found in Appendix A.\nImplementation details. Following Huang et al. (2023) and Singh et al. (2024), we fine-tune the pre-trained model Mo during the Improve Step of each iteration to prevent overfitting. We set the iteration number T = 4 and sampling number k = 8. To mitigate the tail-narrowing effect, we identify queries with less than a 50% probability of yielding correct completions as heavy-tailed data. For the tail data, we apply the GSI strategy, resampling up to k times until the query no longer falls into the tail data. All experiments are performed on 8 A100-80GB GPUs. We run the SFT and Improve Step for 1 epoch. The learning rate is set to 1 \u00d7 10\u22125. For sampling and evaluation, we leverage the vLLM (Kwon et al., 2023) framework, setting a maximum of 1024 output tokens. The temperature is set to 0.7 during sampling and 0 during evaluation. The prompt templates are detailed in Appendix D.\nBaselines To evaluate the impact of GSI on distributional adjustments, we compare it against SFT and Self-improve variants. We also include baselines with significantly higher sampling trails than GSI.\n\u2022 SFT: Fine-tuning on the original dataset for 1 epoch, which corresponds to the first iteration of self-improvement.\n\u2022 Self-Improve (k = 8): In each iteration, we sample k = 8 completions per query from the original dataset, filter out correct reasoning paths, and then improve on the self-generated data.\n\u2022 Self-Improve (k = 64 or k = 128): Based on Vanilla Self-Improve (k = 8), we add distribution re-balancing stage for tail data in each iteration. It performs brute-force sampling of tail-end data up to k times without additional guidance."}, {"title": "6.1"}, {"title": "6.2"}, {"title": "Main Results", "content": "The main results are shown in Table 1. We have the following key findings:\nRe-balancing tail data improves coverage and performance of self-improvement. Compared to SFT, vanilla self-improve with k = 8 boosts reasoning on held-out datasets but shows only marginal gains, with performance bottlenecks and degradation on held-in datasets. This aligns with our observation (\u00a7 4), where we identify tail narrowing as the primary cause. To mitigate this, Self-Improve (k = 64 or k = 128) performs additional sampling on tail data, which re-balances the distribution. This adjustment significantly enhances both coverage and overall performance, with further improvements as the number of sampling increases. For example, in Llama2-7B, coverage increases from 52.6% to 80.7%, with performance improving from 21.53 to 23.84. In Mistral-7B, the rebalancing also reverses the observed performance decline. Thus, incorporating a resampling stage for challenging, heavy-tailed data proves essential. GSI outperforms brute-force sampling with better efficiency. As the resampling increases from 64 to 128, the performance gains become marginal despite additional computational costs. This may be due to the model getting trapped in vast search space, especially on more challenging queries. In contrast, our strategy GSI, which leverages Socratic-style guidance, offers a more compute-efficient sampling. It outperforms brute-force sampling while utilizing only one-third of the sampling budget. Specifically, on Llama3-8B, the state reset strategy performs 0.32M sampling, which is only one-third of the budget required for brute-force sampling. Moreover, the model shows improved held-in performance from 37.55 to 41.64 and generalizes well to held-out datasets. The effectiveness of different strategies. Among the four strategies, the state reset strategy, which samples from different initial states and generates diverse reasoning paths, performs relatively better. However, the effectiveness of different strategies depends on the model's inherent capabilities. For example, the answer-driven strategy, which provides only the correct value and requires the model to reason backward to generate a rationale, demands advanced reasoning abilities (Zelikman et al., 2022). Therefore, this approach yields modest performance gains on weaker models such as Llama2-7B. Further investigation is needed to explore how different models can be optimally paired with various strategies."}, {"title": "Discussion", "content": "Effectiveness on PoT Reasoning To fully exploit the potential of diverse reasoning processes, we extend our investigation to Program-of-Thought (PoT, Chen et al., 2023) prompting. In the self-improvement process, we utilize PoT rationales for training, then filter data and evaluate performance based on compiler-executed results. As shown in Figure 4, four strategies consistently outperform the self-improvement baseline in program-based reasoning. The state reset strategy on the DeepSeek-Math-7B model shows notable relative gains, with an improvement of up to 8.7%. Similarly, the reference-driven strategy leads to a performance boost of 8.5%.\nPerformance of Different Model Sizes To further explore, we investigate the effectiveness of the proposed strategy across different model sizes. We choose a smaller model, DeepSeek-Coder-1.3B (Guo et al., 2024a), and a larger model, CodeLlama-13B (Rozi\u00e8re et al., 2023), in our experiments. As shown in Table 2, DeepSeek-Coder-1.3B exhibits only marginal improvements when applying the answer-driven strategy compared to the others. The limited improvement may be attributed to the nature of the strategy, which requires the model to reverse-engineer a solution from a given true answer (Zelikman et al., 2022). While the final result is provided, deriving a good justification can be challenging for smaller models (Wei et al., 2022a). However, when scaling up to the 13B model, we observe a pronounced performance boost. The results suggest that GSI is more effective with larger models, which are equipped with advanced reasoning abilities.\nShort-cutting in Generated Rationales This experiment explores how different guiding strategies influence the quality of rationales. Inspired by Zelikman et al. (2022), we focus on the number of rationale steps. Figure 5 shows that, in most cases, the model's reasoning steps align with the original annotated steps. However, under the rationale-driven strategy, the model is more likely to generate fewer steps than others. Further analysis reveals that providing rationales can cause the model to skip reasoning steps, a phenomenon we refer to as \"Hint Short-cutting\u201d (Zelikman et al., 2022). This tendency may weaken the model's ability to think step-by-step, potentially hindering iterative training (Dohmatob et al., 2024a). We show an example in Appendix B, where the model skips critical steps."}, {"title": "7.1"}, {"title": "7.2"}, {"title": "7.3"}, {"title": "Conclusion", "content": "In this work, we delve into the performance bottlenecks in the self-improvement process of LLMs, identifying the issue of tail narrowing caused by progressively imbalanced data sampling. To mitigate this, we propose Guided Self-Improvement (GSI), a new method that incorporates a distribution re-balancing phase and Socratic-style guidance to enhance solution coverage for challenging queries. Experimental results across multiple models and mathematical reasoning tasks demonstrate the effectiveness of this method in improving reasoning performance while maintaining computational efficiency. We believe GSI offers a promising direction for enhancing the scalability and generalization of self-improving models in the future."}, {"title": "Limitations", "content": "While our work introduces a new approach to mitigating the tail narrowing through GSI, there are still several limitations. First, for computational efficiency, we do not scale the sampling in each iteration. However, we conduct a series of experiments (\u00a7 4) to provide insights into how scaling the number of sampling can boost performance. Second, following prior self-improvement works, we use binary signals for supervision based on final answer checks. However, poor and spurious rationales while yielding correct answers may be utilized, which could hinder the improvement of reasoning ability. Filtering low-quality reasoning paths and ensuring the quality of self-generated data remains an area for further investigation."}]}