{"title": "GCA-SUN: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting", "authors": ["Yuzhe Wu", "Yipeng Xu", "Tianyu Xu", "Jialu Zhang", "Jianfeng Ren", "Xudong Jiang"], "abstract": "Exemplar-Free Counting aims to count objects of interest without intensive annotations of objects or exemplars. To achieve this, we propose Gated Context-Aware Swin-UNet (GCA-SUN) to directly map an input image to the density map of countable objects. Specifically, a Gated Context-Aware Modulation module is designed in the encoder to suppress irrelevant objects or background through a gate mechanism and exploit the attentive support of objects of interest through a self-similarity matrix. The gate strategy is also incorporated into the bottleneck network and the decoder to highlight the features most relevant to objects of interest. By explicitly exploiting the attentive support among countable objects and eliminating irrelevant features through the gate mechanisms, the proposed GCA-SUN focuses on and counts objects of interest without relying on predefined categories or exemplars. Experimental results on the FSC-147 and CARPK datasets demonstrate that GCA-SUN outperforms state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Object counting determines the number of instances of a specific object class in an image [1], e.g., vehicles [2], crowd [3], and cells [4]. It can be broadly categorized as: 1) Class-Specific Counting (CSC), counting specific categories like fruits [5] and animals [6]; 2) Class-Agnostic Counting (CAC), counting objects based on visual exemplars [1], [7], [8] or text prompts [9], [10]; 3) Exemplar-Free Counting (EFC), counting objects without exemplars, presenting a significant challenge in discerning countable objects and determining their repetitions [8], [11], [12].\nExemplar-Free Counting shows promise for automated sys-tems such as wildlife monitoring [13], healthcare [14], and anomaly detection [15]. Hobley and Prisacariu directly re-gressed the image-level features learned by attention modules into a density map [12]. CounTR [8] and LOCA [16] are originally designed for CAC tasks, but can be adapted to EFC tasks by using trainable components to simulate exemplars. RepRPN-Counter identifies exemplars from region proposals by majority voting [11], and DAVE selects valuable objects using a strategy similar to majority voting based on [17].\nDespite the advancements, existing models [8], [16], [17] often explicitly require exemplars to count similar objects. EFC methods such as RepRPN-Counter do not require exem-plars but generate them through region proposal [11]. Either explicit or implicit exemplars may induce sample bias as exemplars can't cover the sample distribution. To address the challenge, we propose Gated Context-Aware Swin-UNet (GCA-SUN), which directly maps an input image to the density map of countable objects, without any exemplars. Specifically, the encoder consists of a set of Swin Transformers to extract features, and Gated Context-Aware Modulation (GCAM) blocks to exploit the attentive supports of countable objects. The bottleneck network includes a Gated Enhanced Feature Selector (GEFS) to emphasize the encoded features that are relevant to countable objects. The decoder includes a set of Swin transformers for generating the density map, with the help of Gated Adaptive Fusion Units (GAFUs) to selectively weigh features based on their relevance to count-able objects. Finally, a regression head is utilized to derive the density map from the aggregated features.\nOne key challenge in EFC is to effectively differentiate countable objects from other objects. The GCAM blocks tackle the challenge by first evaluating feature qualities by comput-ing the feature score for each token, and then prioritizing those with informative content. In addition, GCAM computes pairwise similarities between tokens through a self-similarity matrix, exploiting the support of repeating objects in the same scene. Lastly, a gate mechanism is incorporated to highlight the most relevant features while suppressing irrelevant ones.\nAnother challenge is that foreground objects often share similar low-level features with background content. The skip connections directly fuse low-level features in the encoder with high-level semantics in the decoder, potentially impeding counting performance as the background information could disturb the foreground objects. To tackle this issue, gate mechanisms are incorporated into both GEFS and GAFU to suppress irrelevant low-level features while preserving as much information on objects of interest as possible. The former selectively enhances the compressed features at the bottleneck, and the latter filters the features in the decoder.\nOur contributions can be summarized as follows. 1) The proposed GCA-SUN achieves exemplar-free counting through a UNet-like architecture that utilizes Swin transformer blocks for feature encoding and decoding, avoiding the sample bias"}, {"title": "II. PROPOSED METHOD", "content": "The proposed Gated Context-Aware Swin-UNet (GCA-SUN) is built upon a Swin-UNet architecture [18], with three new building blocks, GCAM, GEFS and GAFU, to exploit attentive support of countable objects and suppress irrelevant tokens or features, as outlined in Fig. 1. It begins with patched image feature F, following by feature encoding,\n$F_E = F_{Down}(F_{Swin-T}(F_{GCAM}(F_{E-1})))$, (1)\nwhere $F_{Down}$, $F_{Swin-T}$, $F_{GCAM}$ denote down-sampling, GCAM, and Swin-T processing, and $F_{E-1}$ and $F_E$ are the input and output features at the i-th stage, respectively. GCAM enhances the token for countable objects and suppresses others.\nAt the bottleneck, the features are enhanced through the proposed GEFS, i.e., $F_{BN}= F_{GEFS}(F_E)$, where $F_{GEFS}(.)$ denotes the operation of GEFS, and $F_E$ denotes the output features of the encoder of K stages. GEFS selects the features corresponding to the countable object using a gate mechanism.\nSubsequently, a set of Swin transformer blocks are utilized as the decoder to derive the density map. Specifically, the features at the j-th stage of the decoder are derived as,\n$F_D = F_{UP}(F_{Swin-T}(F_{GFU}(F_{D-1, E-1})))$, (2)\nwhere $F_{UP}$, $F_{Swin-T}$, and $F_{GAFU}$ denote the operation of up-sampling, Swin transformer, and GAFU block, respectively. The GAFU enhances features through a gate mechanism, pri-oritizing crucial information with a dynamic assigned weight.\nFinally, these features are processed through a regression head, $F_{head} = F_{Head}(F_D)$, where $F_{Head}$ denotes the regression head consisting of a series of convolutional blocks. The output is a density map that accurately represents the object count."}, {"title": "B. Swin-T Encoder with GCAM", "content": "The encoder consists of a set of Swin transformers to extract features relevant to countable objects. The GCAM employs a dynamic token modulation process to simulta-neously exploit the attentive support of tokens relevant to countable objects and suppress features of irrelevant objects. This process facilitates self-probing among objects and precise capture of objects of the same category for exemplar-free counting. We first compress token features $F_E$ using an MLP, $F_{proj} = F_{MLP}(F_E)$. To identify the objects of interest, we resort to two key observations: 1) The objects should be salient enough to step out from the background; 2) Similar objects could support each other to boost the saliency. The former is exploited by computing the average feature score $C_i$ for each token through average pooling $F_{AVG}$ as, $C_i = F_{AVG}(F_{proj})$. The score reflects the importance of tokens, prioritizing those with rich content. Tokens that frequently appear in similar contexts are more likely to be related to the target object of interest. To identify them, we employ a similarity matrix $S_i = \\sigma(F_{proj}(F_{proj})^T)$, where \u03c3 is a softmax function to normalize similarities across rows. $S_i$ captures the semantic similarity of tokens in a spatial context to emphasize tokens that repeatedly share similar features, thereby emphasizing potential countable objects. A mask $M_i$ is derived by aggregating $S_i$ and $C_i$ as,\n$M_i = \\sigma(F_{MLP}(S_i, C_i))$ . (3)\n$C_i$ encodes the token importance when considering the token alone, while $S_i$ encodes the token importance after interacting with other tokens. The tokens are then filtered by the mask as,\n$F_{GCAM} = F_{Linear}(F_{LN}(M_i \\odot F_E + F_{proj}))$, (4)\nwhere, $\\odot$, $F_{LN}$ and $F_{Linear}$ denote element-wise product, layer normalization and linear layer, respectively. The GCAM applies the mask $M_i$ to $F$, filtering out less relevant features and reinforcing those critical ones for countable objects.\nThe proposed GCAM selectively amplifies the importance of tokens related to significant object features through pairwise similarities. It is significantly different from LOCA [16] and DAVE [17] which depends on predefined prototypes to predict object densities. In contrast, our GCAM leverages the self-similarity matrix for more dynamic and precise modulation of features. It is also different from RCC [12] which relies on global feature comparisons, and CounTR [8] which uses attention-driven similarity matrices. The GCAM emphasizes a clear distinction between relevant and irrelevant tokens."}, {"title": "C. Bottleneck with GEFS", "content": "The proposed Gated Enhanced Feature Selector selectively filters out features in the bottleneck that are semantically irrel-evant to the object of interest, but allows critical compressed features to pass through. The GEFS is implemented by first deriving the local token weights as $W_{GEFS} = \\sigma(F_{MLP}(F))$, and then applying them on features as,\n$F_D = F + (W_{GEFS} \\odot (F_{ATTN}(F_{ATTN}(F))))$. (5)\nThe GEFS is positioned at the bottleneck where features transit from the down-sampling to the up-sampling pathways. As a vital bottleneck, GEFS compresses and filters essential object-related features, ensuring that only the most relevant information of countable objects is advanced into the up-sampling path. Specifically, the attention blocks within GEFS refine the model's ability to extract high-level semantic rep-resentations, leading to more accurate feature representation. Furthermore, the gate mechanism that is incorporated into GEFS selectively prioritizes specific aspects of this condensed representation, effectively filtering out less relevant semantics. This process not only refines features by strengthening rele-vant inter-dependencies, but also lays a solid foundation for comprehensive reconstruction of the up-sampling pathway."}, {"title": "D. Swin-T Decoder with GAFU", "content": "The decoder contains a set of Swin transformers to artic-ulate the density map and a set of Gated Adaptive Fusion Units (GAFUs) to integrate low-level encoder features from skip connections with abstract features from the up-sampling pathway. In each GAFU, we employ a gate mechanism to determine the token weights as, $W_{GAFU} = \\sigma(F_{MLP}(F))$, and then apply them to modulate the features as,\n$F = F + (W_{GAFU \\odot F_E})$. (6)\nSubsequently, these features are fused with the decoder fea-tures as $F_{GAFU} = F_{Linear}([F_D, F_E])$. By weighing the features during the fusion process, the GAFU effectively concentrates on semantic information pertinent to countable objects, mini-mizing interference from irrelevant details."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "We utilize two benchmark datasets for evaluation. Follow-ing [8], [16], [17], we employ the Mean Average Error (MAE) and Root Mean Squared Error (RMSE) as evaluation metrics. The Swin-T blocks are pre-trained on ImageNet-22k [20], and other modules are randomly initialized. AdamW opti-mizer [21] is employed for training, with an initial learning rate of 0.003, a decay rate of 0.95 and a batch size of 16. The model is trained with a warm-up period of 50 epochs. The input image size is 384 \u00d7 384. Data augmentation [8] is employed to facilitate efficient training. Experiments are conducted using two NVIDIA RTX A5000 GPUs."}, {"title": "B. Comparison with State-of-the-Art Methods", "content": "Comparison experiments are conducted on the FSC-147 dataset. DAVE tends to overfit to the validation set, while generalizing poorly on the test set. In contrast, the GCA-SUN generalizes well on the novel test set with minimal errors."}, {"title": "C. Cross-Domain Evaluation on CARPK Dataset", "content": "Following [22], we conduct a cross-domain evaluation, training the model on FSC-147 [1] and directly evaluating on CARPK [19]. Our model has shown superior cross-domain performance compared with other methods, achieving a performance gain of 0.56 on MAE and 0.61 on RMSE compared to the previous best-performing method CounTR. Compared to the earlier EFC model [12], the gains are even more significant, highlighting GCA-SUN's superior general-ization over all compared methods."}, {"title": "D. Visualization of GCAM", "content": "We visualize the effects of the proposed GCAM in Fig. 3. This indicates that the GCAM module effectively enhances the representation of foreground tokens while suppressing irrelevant ones."}, {"title": "E. Ablation Study", "content": "We conduct a set of comprehensive ablation studies on the three major modules of proposed method on the FSC-147 dataset [1]. The GCAM module alone significantly decreases the MAE on the test set by 1.77 and on the validation set by 2.33, highlighting its capability to enhance feature selectivity crucial for complex scenes. Similarly, utilizing GEFS alone or GAFU alone also greatly reduces the errors in both test set and validation set, demonstrating the importance of the gate mechanism in highlighting the relevant features while suppressing irrelevant ones. The full integration of all three components produces the most substantial enhancement, reducing MAE by 2.83 on the test set and by 3.34 on the validation set. This underscores the effectiveness of their synergistic interaction and affirms the component's design."}, {"title": "IV. CONCLUSION", "content": "The proposed GCA-SUN effectively tackles the problems of exemplar-free counting by using a Swin-UNet architec-ture to directly map the input image to the density map of countable objects. The proposed GCAM exploits the attention information among the tokens of repetitive objects through the self-similarity matrix, and suppresses the features of irrelevant objects through a gate mechanism. The gate mechanism is also incorporated into the GEFS module and the GAFU module, which highlight the features most relevant to countable ob-jects while suppressing irrelevant ones. Our experiments on the FSC-147 and CARPK datasets demonstrate that GCA-SUN outperforms state-of-the-art methods, achieving superior performance in both intra-domain and cross-domain scenarios."}]}