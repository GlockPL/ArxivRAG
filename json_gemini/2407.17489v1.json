{"title": "Collective Attention in Human-Al Teams", "authors": ["Josie Zvelebilova", "Saiph Savage", "Christoph Riedl"], "abstract": "How does the presence of an Al assistant affect the collective attention of a team? We study 20 human teams of 3-4 individuals\npaired with one voice-only Al assistant during a challenging puzzle task. Teams are randomly assigned to an Al assistant\nwith a human- or robotic-sounding voice that provides either helpful or misleading information about the task. Treating each\nindividual Al interjection as a treatment intervention, we identify the causal effects of the Al on dynamic group processes\ninvolving language use. Our findings demonstrate that the Al significantly affects what teams discuss, how they discuss it,\nand the alignment of their mental models. Teams adopt Al-introduced language for both terms directly related to the task\nand for peripheral terms, even when they (a) recognize the unhelpful nature of the Al, (b) do not consider the Al a genuine\nteam member, and (c) do not trust the Al. The process of language adaptation appears to be automatic, despite doubts about\nthe Al's competence. The presence of an Al assistant significantly impacts team collective attention by modulating various\naspects of shared cognition. This study contributes to human-Al teaming research by highlighting collective attention as a\ncentral mechanism through which Al systems in team settings influence team performance. Understanding this mechanism will\nhelp CSCW researchers design Al systems that enhance team collective intelligence by optimizing collective attention.", "sections": [{"title": "1 Introduction", "content": "The coordinated attention of team members is fundamental to effective collaboration [1]. Collective attention is manifested in\nvarious forms of cognitive alignment that together define how teams cooperatively focus, divide, sustain, and alternate their\nattention to team-related activities to accomplish team goals. It encompasses the alignment of both the timing and content\nof attention [2]. For example, alignment on shared terminology within a team reflects coordinated attentional priorities that\nfacilitate the synchronization of collective focus.\nWith the rise of human-AI teaming, a critical question arises: How does the presence of an AI assistant affect cognitive\nalignment in human teams? While cognitive alignment is widely recognized as a key driver of team performance [3], it may\nfail to materialize in human-AI teams due to fundamental differences between human-human and human-AI interactions\n[4-6]. Despite their growing importance, the dynamics and implications of collaborative processes in human-AI teams remain\nunder-explored [6-8]. Understanding the impact of AI systems on team processes beyond that of a single human-AI dyad is\nvital for informing computer-supported cooperative work (CSCW) research on the design of human-AI systems that facilitate\nproductive collaboration and enhance the collective intelligence of human teams [1, 8-10].\nCognitive alignment in teams is a multi-faceted concept closely intertwined with alignment on shared language [11, 12].\nConvergence on common terminology signifies that groups have established efficient meta-cognition, a prerequisite for effective\ngroup processes. Through this lexical alignment, team members can effectively direct each other's attention [13]. Alignment on\na shared language also affects what is salient and attended to within a team [14]. A shared language is not only an indicator of\ncognitive alignment, but may in fact be the primary mechanism through which humans achieve cognitive alignment [12] and\nthus coordinate collective attention.\nAl systems assume increasingly prominent roles in high-stakes collaborative contexts, such as emergency response and\nmedical diagnosis. However, research on the impact of AI on team performance has yielded inconsistent results, with some\nstudies finding increased performance and others finding decreased performance [e.g., 15\u201317]. To explain why AI systems\nare sometimes helpful and sometimes not\u2014and, ultimately, to design more effective human-AI collaborative systems-it is\ncrucial to better understand how AI affects known antecedents of team performance. Specifically, we need to examine how AI\ninfluences secondary outcomes like collective cognition (including shared language) that shape teams' collective attention [13,\n18]. If AI systems provide quality input but undermine collective attention, they may fail to achieve their goals of enhancing\nteam performance. Without a deeper understanding of how AI affects fundamental team processes of collective attention (and\nrelated components of collective memory and collective reasoning; [2]), it will be difficult to predict the impact of AI on team\nperformance. Insight into these interactions can guide the design of AI systems that better support human teams, enhancing\ntheir effectiveness in complex tasks [9].\nA distinguishing aspect of our study is its focus on human-human communication within AI-assisted teams. We examine"}, {"title": "2 Background", "content": "This study investigates the impact of diverging AI designs on two critical aspects of human teamwork: cognitive alignment of\nattention and lexical alignment. The following section elucidates these concepts, highlighting their significance and relevance\nto our study in the context of computer-supported collaborative work and artificial intelligence.\nHuman Attention. Human attention is the ability to actively process specific information in the environment while tuning out\nother, less relevant details [24]. Attention is limited in terms of both capacity and duration [25]. Managing this limited resource\neffectively is critical to achieving high task performance [26].\nIn group settings, individuals usually align their attention with the attention of other group members, which includes\nwhat individuals perceive and remember [27]. Effective group work requires coordination of attention in order to achieve\ncoordination of actions [28]. Groups are said to have a high level of collective attention when individuals are co-attending\nto an object or event with others [29]. This results in a shared emotional and psychological experience with implications\nfor motivation, judgment, and behavior [1]. For example, collective attention in a shared social context on the same target\nallows group members to prioritize and coordinate their team-related activities [1]. As a result, collective attention is strongly\nassociated with collective intelligence and team performance [28, 30-32].\nCollective attention is sometimes operationalized as the temporal focus of a group on specific information or events, such as\nviewing or sharing content on social media [33], various forms of interactional synchrony such as facial expression and vocal\nalignment [1], and language use [12]. Language, in particular, plays a crucial role in coordinating beliefs among humans and\naligning neural processes that govern collective attention [34, 35]. Humans may even be predisposed to align their mental states\nwith those of other individuals [36].\nWhy Does Human Attention Matter? Higher levels of collective attention not only increase the pool of attentional resources\navailable for achieving team goals, but also actively shape the approaches and strategies used in problem-solving [37]. That is,\nthe object in focus and the shared language used to talk about it can shape abstract intentions, plans, and proposals [38]. The\nresulting heightened focus enhances the team's ability to devise innovative solutions and respond effectively to challenges [39].\nA critical question for research on human-AI teams is how AI agents might influence the collective attention of a team [8,\n40]. Understanding this impact is essential for optimizing collaboration and enhancing team performance [41]. Importantly, AI\nmay affect not only the overall level of collective attention but also the object and manner of that attention [42], both of which\nmay affect the problem solving approaches employed by the team [43, 44].\nAlignment on Shared Language. The study of linguistic convergence has progressed significantly since its inception, shifting\nfrom early explorations of interactions between large language groups and dialects to contemporary explorations of fine-grained\nlanguage coordination in interpersonal conversations [45]. Lexical alignment, a key focus in this domain [46], examines the\nprogressive adoption of similar vocabulary by interlocutors in dialogue [47]. This phenomenon exemplifies the formation\nof shared conceptual frameworks, which emerge as speakers subtly negotiate and align their terminology [48]. Within this\nparadigm, the principle of input-output coordination describes a fundamental interaction mechanism [49]: each participant's\nutterances propose a conceptual framework that shapes the next speaker's response. These interactions often solidify into\n\"conceptual pacts\u201d\u2014persistent, partner-specific shared understandings [50]. The importance of these pacts is underscored by the\nprocessing costs that arise when a speaker unexpectedly deviates from them [51]. A shared language thus enhances a group's\ncollective attention by freeing up cognitive resources, facilitating more efficient communication and cognition.\nOpen Questions How could the presence of an AI assistant affect the formation of a shared lexicon? Several gaps remain in\nour understanding of collective attention in human-AI teams. Recent research has shown that lexical alignment may be affected\nby the perceived competence of conversational partners [52]. Humans have been shown to align more towards a computer\npartner than a human partner, even when the linguistic behavior is identical [21]. This supports a \u201ccommunicative design\"\naccount of alignment, where speakers deliberately align more with partners they perceive as less communicatively competent in\norder to facilitate successful communication."}, {"title": "3 Related Work", "content": "Our research is most closely related to work on (a) human-AI teaming and (b) lexical alignment between humans and artificial\nsystems.\n3.1 Advancing Team Dynamics: The Role of Al Design in Enhancing Communication and Collaboration\nPrevious research has explored the design of human-AI interfaces to improve team dynamics. These interfaces enhance\ncollaboration by sharing control, improving understanding, employing social-cognitive strategies, building mutual trust, and\nboosting overall team performance [20, 67\u201371]. For instance, sharing control in human-robot teams leverages complementary\nskills, which significantly enhances decision-making and task efficiency [67]. This aligns with findings that emphasize the\nimportance of understanding AI capabilities, particularly an AI system's error boundaries, in boosting team performance by\ncomplementing human skills [20]. Other research has focused on developing AI agents capable of constructing mental models\nof human teammates in order to steer team communication by alleviating information overload and supporting metacognitive\nprocesses, such as building shared mental models of \"who knows what\" [8]. Moreover, research indicates that establishing\nbi-directional trust in human-AI teams not only involves dynamic relationship management and adaptive systems, but also\nnecessitates co-discovery learning and algorithmic transparency [71]. Other studies reinforce this finding by demonstrating that\ncombining explicit AI explanations with implicit manipulations significantly improves trust dynamics within these teams [72].\nBuilding upon the foundation laid by previous research, the present study investigates the impact of specific AI designs on\nteam communication and collaboration. Inspired by prior work exploring the dynamics of human-AI interaction, we advance\ninto the nuances of AI influence on teamwork. By analyzing the effect of different AI demeanors on team processes, we aim to\nexpand the current state of the art in this field. Our research addresses a notable gap by focusing on the direct effects of AI\nbehavioral characteristics on the efficiency and quality of team interactions. This approach not only enhances our knowledge of\neffective AI integration into collaborative settings, but also sets new benchmarks for designing AI systems that are more adept\nat facilitating human teamwork. Through this work, we contribute to a deeper understanding of AI deployment in team-based\nenvironments.\n3.2 Lexical Alignment in Human-Al Interaction\nLexical alignment is a complex phenomenon [21, 49] influenced by various factors, including the perceived competence of\nthe interlocutor, the interaction history between partners, and specific task demands [73\u201375]. While several studies have\ninvestigated lexical alignment in human-AI interaction [21, 23, 76\u201378], they predominantly focus on dyadic interactions\nbetween one human and one AI agent [76], leaving the dynamics of human-human-AI interactions largely unexplored.\nEvidence suggests that humans align differently with AI partners than with human partners [79, 80]. For example, research\non text-based and speech-based interfaces indicates that humans are at least as likely to adopt terms used by a computer system\nas they are to adopt terms used by human partners [21]. A more recent study suggests humans align more strongly with other\nhumans than with artificial agents [23].\nWhile linguistic alignment is observed in both human-human and human-AI interactions, the underlying processes may\ndiffer significantly. With human partners, linguistic alignment emerges from a collaborative, negotiated process of establishing\nshared conceptualizations. With a computer, individuals may adopt the system's terms more strategically in order to avoid"}, {"title": "4 Data", "content": "We ran a randomized controlled trial with 69 human subjects as an IRB-approved lab study. Teams of three or four people\ncollaborated to solve a puzzle task using online video conferencing software. During the task, the team was assisted by an\naudio-only AI team assistant. We used a 2 \u00d7 2 between-subjects design which manipulated the quality (helpful vs. wrong)\nand voice (human-sounding vs. robotic-sounding) of the AI assistant. Following the puzzle task, participants completed a\npost-experiment questionnaire. For their time, which lasted up to 90 minutes, they received compensation in the form of\nAmazon gift cards at a rate of $20 per hour. We recruited participants through flyers, email lists, and word of mouth from the\nNortheast United States to participate in this study. Team size varied between three and four people based on convenience.\nWhile we scheduled four participants for each session, we proceeded if at least three were present. The participant pool was\npredominantly young, with a mean age of 21.7 years, and skewed female (Table 1).\n4.1 Experimental Task\nThe experimental task is a pattern recognition challenge framed as a \u201cCursed Treasure Puzzle.", "octopus curse\" is present when none of the gems touch each other.\nThe task is particularly suitable for studying the development of shared language because it presents participants with several\nunfamiliar symbols for which they do not have established terminology. Since the symbols can be described using a variety of\nwords, teams must develop a shared vocabulary to effectively distinguish them in order to direct each other's attention and\ncommunicate precisely [81, 82]. Several other studies investigating the development of shared language have relied on similar\nsymbol tasks [81, 82]. The pattern recognition task is designed to be challenging; most teams fail to reach all milestones. To\nhelp them, we introduce an AI assistant called the \u201cAI Puzzle Master": "hat dispenses one clue for each milestone.\n4.2 Al Assistant\nThe \"AI Puzzle Master\" was presented as a disembodied entity, represented by a black screen displaying its name in the online\nvideo conferencing software. At the experiment's outset, an experimenter read a script introducing the Puzzle Master as an AI\nassistant capable of offering helpful information. The AI assistant's interaction was one-sided, communicating with teams only\nvia pre-recorded audio messages played at predetermined intervals. We created four different AI assistant recordings, one for"}, {"title": "4.3 Tracking Linguistic Alignment", "content": "To measure the AI assistant's impact on team collective attention and linguistic alignment, we transcribed the audio recordings\nof team communications and identified each instance of a team member referring to one of the milestones (Fig. 2). Each\nmilestone in the puzzle task is represented by a symbol corresponding to the nature of the \u201ccurse", "duel curse\" milestone is represented by a pair of crossed swords. We compiled a complete list of terms\nused by participants to refer to each of the milestones, the symbols, and the gems, including different terms used to refer to the\nsame thing. This list of terms formed our set of tracked terms. Each time a participant used one of the tracked terms to refer to\na milestone, a symbol, or a gem, we recorded its use, forming the basis of our lexical alignment measure (see next section).\nThe AI assistant consistently used the same language to refer to each of the milestones and to the gems. As a result, we\nwere able to analyze the team's term usage to see whether they used the same terminology as the AI or different terminology.\nEvery time we updated the running usage counter for one of the terms, we also updated a running lexical alignment score based\non whether or not the term matched the AI's term for that referent. The alignment score increased if the term was an AI term\nand decreased if it was not (unless it was already at 100% or 0%, in which case it did not change).\nWithin the seven tracked referents, we distinguish between five core referents and two peripheral referents. The five referents\ncorrespond to the five milestones, each of which is the subject of one clue from the AI assistant. Since the core referents are\neach directly related to a milestone (a pattern that teams need to uncover), the AI assistant's interjection can be identified as\nhigh or low quality depending on whether it is helpful or intentionally misleading. The peripheral referents are \u201csymbols\u201d and\n\"gems\", which are relevant to the task but are not milestones themselves. These two referents can be considered neutral in that\nthey are not the subject of a high or low quality interjection. Gems are mentioned in several AI interjections, while symbols\nare never mentioned, making the symbol referent an appropriate baseline from which changes in collective attention can be\ncontrasted.\nIn summary, each referent corresponds to a set of tracked terms and each referent is mentioned by the AI assistant in one of\nits interjections. Following the interjection, teams may talk more (or less) about the mentioned referent, and they may do so\nusing either the same term used by the AI or use a different term to refer to the referent (for example, \u201cduel": "s the term the AI\nassistant used or \"swords\" as some of the teams used)."}, {"title": "4.4 Lexical Alignment Measure", "content": "Lexical consensus is calculated as a proportion: the usage of a particular term to refer to a given referent divided by the usage\nof any term to refer to the same referent. Lexical alignment is the direction of lexical consensus over time. Our interest is in\nalignment with the language used by the AI assistant, so the usage of the AI assistant's terminology is the numerator of the\nconsensus ratio. The measure ranges between 0, indicating no use of the AI's terminology, and 1, indicating that all mentions\nuse the AI term.\nTo illustrate the measure, Fig. 3 shows all uses of tracked terms in one of the teams. The very first mention of the gems\nreferent uses the term \u201cgem,\u201d starting the alignment measure at 1 (i.e., 100%) indicating that all mentions used the same term\nas the AI assistant. After this first reference, up to minute 14 and the first AI interjection, most references to the gem use a\ndifferent term (e.g., \"diamond\") and the alignment measure decreases. Then the AI interjects, using the term \u201cgem,\" and all\nfuture mentions in the team also use the AI term. The alignment measure increases, approaching 1 at the end.\""}, {"title": "5 Method", "content": "Human communication is a dynamic process. A key challenge in our analysis is to causally attribute specific changes in this\nfluid team communication to the interventions of the AI assistant. To achieve this causal identification, we employ a panel\nregression using observations on the utterance level following a difference-in-difference (DD) framework [86]. Specifically, we\nconsider term use (as captured by the lexical alignment measure) in segments right before the AI intervention and right after for\neach of the six AI intervention (e.g., Segment 1 surrounding the \"computer hex\" clue; see bottom of Fig. 3). The DD analysis\nthen compares the before/after change in AI term usage during a specific segment to the same before/after change during the\nsame segment of a term that was not used by the AI. For example, the before/after change in the usage of the term \"octopus\"\nduring the segment surrounding the \"pirate clue\u201d serves as a no-treatment control (because the \"pirate clue\u201d did not mention\n\"octopus\") to the before/after change in term usage of \"pirate\" (which could have been affected by the AI's mention of \u201cpirate\u201d).\nFormally, we estimate the following regression equation\nEntrainmentOnAlogs =B1Afterogs + B2Afterogs \u00d7 AIQualityg\n+B3ObjectMentionsog + B4ElapsedTime og\n++ Ag + as + Eogs\n(1)\nwhere After is a dummy variable indicating whether the utterance related to object o happened before or after the AI intervention\nrelated to object o, AIQuality is a dummy indicator whether the team g was assigned to the high (vs. low) quality condition,\na are object-level fixed effects, ag are group-level fixed effects, as are segment-level fixed effects, and \u0190ogs are error terms.\nThe coefficient B\u2081After captures the causal effect of the AI intervention, and the coefficient \u03b22 for the interaction term of\nAfter AIQuality captures the differential effect of the treatment condition. Estimates are driven by six before/after comparisons\ncreated by the six AI interventions, each one of them comparing five unaffected terms serves as control contrasted from one term\naffected by the intervention. We include the count of ObjectMentions and the ElapsedTime (in minutes) as control variables.\nWe estimate variations of this equation where we substitute Humanness and Treatment as key variables of interest for AIQuality.\nWe report standard errors clustered at the team-level. Notice the main effect of the treatment will drop out due to the team-level\nfixed effect and only the interaction term remains."}, {"title": "6 Results", "content": "This section presents five sets of results in a comprehensive analysis of the AI assistant's impact on team collective attention.\nFirst, we offer an anecdotal description of the experience of one specific team to showcase how teams adopt the AI's lexicon\n(addressing RQ1 & RQ2). Second, we offer causal evidence that the AI assistant affects what teams talk about (addressing\nRQ1). Third, we offer causal evidence that the AI assistant affects how teams talk about the referents we track (addressing RQ2)\nand how the alignment of language differs across the quality and humanness of the AI (addressing RQ3). Fourth, we show that\nthese findings are reflected in the teams' retrospective and reified perceptions of their own team's processes (combining RQ1 &\nRQ2 into a higher-level construct). Finally, we draw on complementary data sources to explore the mechanism behind the\ninfluence of the AI on team collective attention (addressing RQ4).\n6.1 How an Al Assistant Shapes Team Communication and Development of Shared Language\nWe begin our analysis with an anecdotal description of the experience of Team 18, a team of three in the Human-Wrong\ncondition. After receiving their instructions, the team begins the task by looking for patterns among the gems. They refer to the\ngems as \"diamonds.\" They point out various features of the treasure chests and identify some similarities between chests with\nthe same curse. In the first fifteen minutes, they refer to the gems seven times using the word \"diamond.\" Nobody uses the\nword \"gem\" until the AI assistant interjects at 15:07 with the first clue-in this case, a deliberately misleading clue about the\ncomputer hex milestone. This clue includes a mention of \u201cgems.\u201d\nFollowing the interjection, the team's attention shifts and they begin discussing the computer hex curse, which has so far\nnot been addressed. They express confusion because they cannot see the pattern suggested by the clue (indicating an answer\nfor our RQ1 about the AI's effect on the timing of what teams talk about). Nevertheless, they stop using the word \"diamond\"\nand instead adopt the term \"gem\u201d that was used by the AI. There are another eight references between the first and second AI\ninterjection, all now using the term \"gem.\u201d The team has adopted the AI's language for the peripheral gem referent (indicating\nan answer for our RQ2).\nAt 20:07, the AI provides the second clue. This clue states that the octopus milestone has to do with the sizes of the gems.\nAt this point, the team grows suspicious of their AI assistant because this clue contradicts the pattern they have begun to identify.\nOne participant suggests that the clues are intended to throw them off, but another team member says \u201cwe have to trust the\nPuzzle Master.", "30": 7, "duel": "nd", "sword": "o refer to the duel\nclue throughout the rest of the task, not fully adopting the language proposed by the AI assistant for this core referent. The\nteam does not manage to complete the task in time.\nA different team assigned to the Robotic-Helpful condition, Team 19, exhibits a contrasting pattern. Participants in this team\ndemonstrated higher overall trust in their AI assistant, which provided them with helpful rather than misleading information.\nThe initial two clues, pertaining to the \u201ccomputer hex\u201d and \u201coctopus\u201d milestones, employ terminology consistent with the\ngroup's usage at the experiment's outset, prompting no change in lexical choices. Both clues align with the group's observations,\ngiving participants no reason to doubt the reliability of the AI. The third clue, introduced at 24:34, informs the group that the\nquadruple bypass milestone relates to the colors of the gems. At this point, a team member clarifies that the", "heartbreak": "r \u201cheart\u201d milestone. Notably, the team persists"}, {"title": "6.2 Al Assistant Shapes What Teams Talk About", "content": "The AI assistant significantly influenced the focus of team discussions, affecting what teams talked about and therefore what\nthey paid attention to (addressing RQ1). Our analysis compares the frequency of tracked referent mentions during baseline\nsegments to those immediately following AI interventions (Fig. 4). In every instance, team discussions of the referent increased\nfollowing the AI's mention, suggesting that the AI intervention significantly shaped their collective focus of attention. Teams\nwere significantly more likely to discuss the mentioned referent in the period immediately following the intervention compared\nto other referents (we estimate the same difference-in-difference model from 1 but use a binary indicator of whether a mentioned\nterm corresponds to the AI intervention as the dependent variable; \u03b2 = 0.35; p < 0.001). Overall, teams were 1.5 times more\nlikely to discuss the referent in the segment following the Al's mention than in any other segment. The impact of the AI on\nteam collective attention is not significantly different across the treatment groups, indicating that the mere mention of a referent\nby the AI influenced group attention, regardless of the quality or voice of the AI assistant. Recall that the AI in this study was\nnot interactive; thus, the observed effect was not due to direct engagement with the AI, but to an influence on human-to-human\ncommunication."}, {"title": "6.3 Team's Adoption of Terms Used by the Al: Lexical Alignment", "content": "The AI assistant not only shapes what teams talk about but also how they talk about it. We find significant signs that teams\nalign their communication with the language introduced by the AI assistant (answering RQ2; Table 2). First, teams show strong\npatterns of alignment caused by the AI assistant, indicated by the positive and significant coefficient of the After dummy (Model\n1, \u03b2 = 0.06, p < 0.01). This overall coefficient masks important heterogeneity of an even stronger effect in some conditions\nand lower alignment in others. Alignment is especially high when the AI has a human sounding voice (Model 2, \u1e9eAfter = 0.14,\np < 0.001) and teams align less on the AI's language when the AI has a robotic voice (Brobotic = \u22120.11, p < 0.05). Alignment\nis lower when the AI is of low quality (as compared with high quality) but the difference is not statistically significant (Model\n3, Bwrong = -0.08, ns). Finally, we find significant interaction between the robotic voice and AI quality. Teams align with\nAl terminology most strongly in the Human-Helpful condition (Model 4, \u1e9eAfter = 0.15, p < 0.001). Compared against this\nbaseline, alignment is significantly lower in the Robotic-Helpful condition (\u03b2 = \u22120.09, p < 0.01), and lower still when the\nAI is both robotic and provides low-quality assistance (\u03b2 = \u22120.16, p < 0.05). There is no significant difference between the\nRobotic-Helpful and Robotic-Wrong condition (linear hypothesis test: x2 = 1.40; p = 0.24). Together, these results answer\nRQ3 regarding how the effect of the AI differs based on (a) its quality and (b) its humanness.\nThe lexical alignment patterns differ substantially between core referents and peripheral referents. Whereas the coefficient\nindicating After AI intervention is positive for referents with high relevance to the clues (\u1e9e ranging from 0.06 to 0.15, p at least"}, {"title": "6.4 Shared Mental Models", "content": "So far, we have shown evidence of the AI assistant's substantial impact on collective attention and shared cognition through\nvarious behavioral measures of alignment in team language use. In this section, we explore whether these findings are reflected\nin the participants' retrospective and reified perceptions of their own team processes (combining RQ1 & RQ2 into a higher-level\nconstruct). The combination of evidence from behavioral measures of group interaction and participants' self-reflection of\ntheir group's process offers particularly compelling support for the AI assistant's influence on collective attention [3, 11].\nSpecifically, we investigate the effect of AI on the emergent shared cognition through which groups create and use interpersonal\nunderstanding. This shared cognition represents the manner in which knowledge important to team functioning is mentally\norganized, represented, and distributed within the team [3]. Recognized as a central positive driver of team behavioral processes,\nmotivational states, and team performance [3, 88], emergent collective cognition is especially beneficial in interdependent\ncognitive tasks where leveraging team members' expertise is crucial [89]. Teams with well developed shared mental models\ntypically possess a common view of \u201cwhat is happening, what is likely to happen next, and why it is happening\" [88, p.879].\nTo assess the effect of the AI assistant on team cognition, we investigate whether the it affects the alignment or similarity-\nof mental models within a team. We measure the alignment of mental models as the degree to which members' mental models\nare consistent [88]. First, we assess shared mental models using six items from [90]. Items propose statements like \u201cmy team\nknows specific strategies for completing various tasks\u201d and participants are asked to respond on a Likert scale. With this data,\nwe compute an intragroup agreement index within each item using standard deviation (to measure how consistently within-team\nresponses overlap versus diverge), and then average the standard deviations across the six items.\nWe find a medium level of alignment in shared mental models in both helpful conditions, while the unhelpful conditions\nexhibit diverging patterns (Fig. 5). Mental models are less well aligned (i.e., have higher standard deviation) in the Human-\nWrong condition, while we find the highest level of alignment in the Robotic-Wrong condition. A significant interaction exists\nbetween the two factors of our experimental design (one-way ANOVA F(3,65) = 5.5; p = 0.041). We theorize that this pattern\nis the result of an \"us vs. them\" dynamic, which we discuss in detail in the next section."}, {"title": "6.5 Exploring the Mechanism Behind Al's Influence on Team Collective Attention: A Closer Look at Complementary Data Sources", "content": "The above analyses reveal the substantial impact of the AI assistant on three aspects of collective attention: what teams\ntalk about, how they talk about it, and the alignment of their mental models. In the following section, we provide further\nevidence for this significant effect on cognitive alignment within teams and explore the nuanced contrasting effects between\ncore and peripheral terms (addressing RQ4). We investigate the role of trust in AI, the effect of the AI assistant on the collective\nidentity of the team, and the alignment of mental models. By integrating the evidence from above with self-reported survey\nmeasures, we offer a more comprehensive account of cognitive alignment on a different timescale.\n6.5.1 Trust in Al\nTrust in AI plays a pivotal role in shaping the nature and quality of human-AI interactions[91]. We theorize that the the level of\ntrust individuals place in AI not only influences direct human-AI interactions, but also impacts interpersonal dynamics among\nhuman team members when an AI assistant is present. To measure trust in the AI assistant and perceptions of the assistant's\ncontribution quality, we use six items adapted from [92] (originally designed to measure trust in online avatars). Participants\nresponded on a five-point Likert scale from Strongly Disagree to Strongly Agree to items like \"The Puzzle Master gave helpful\nclues.\"\nHumans rated the low-quality AI as significantly less trustworthy (p < 0.001) and less intelligent (p < 0.001) than the\nhigh-quality AI. Voice anthropomorphism did not have a significant impact on participant impressions of trustworthiness\n(p = 0.570), but the ratings were slightly lower in the Robotic treatments. The greater level of trust in the high-quality AI\nconditions may explain why participants aligned more with the AI assistant's language for core referents in these conditions.\nHumans are more likely to adopt recommendations by trustworthy agents that provide high-quality explanations [93]. High\ntrust can also indicate the perception of higher status [94], which may explain why the AI assistant in in the high-quality\nconditions has more influence over the team's language dynamics. Conversely, lower trust and lower perceived status can make\nthe AI assistant easier to ignore, thus giving it less influence over the lexical choices of team members.\nOverall, a stark pattern emerges: even in low-quality conditions in which trust assessments of the AI assistant were\nsignificantly lower, the AI assistant directed team attention to the specific task aspects it mentioned (see Section 6.2) and\ninfluenced their lexical choices. This pattern demonstrates the power of the AI assistant to shape collective attention and shared\nlanguage, even when teams are aware of its low quality and have decided not to trust it.\n6.5.2 Is the Al Assistant a Part of the Team?\nWe asked individuals to report whether they felt that the AI assistant was part of their team or even a leader of their team. Humans\ndid not consider the AI to be a part of their team, with 77% of participants responding \"Strongly Disagree\u201d or \u201cSomewhat\nDisagree.", "Strongly Disagree": "r \u201cSomewhat Disagree.\u201d Ratings were especially low in the Robotic-Wrong\ncondition"}, {"title": "Collective Attention in Human-Al Teams", "authors": ["Josie Zvelebilova", "Saiph Savage", "Christoph Riedl"], "abstract": "How does the presence of an Al assistant affect the collective attention of a team? We study 20 human teams of 3-4 individuals\npaired with one voice-only Al assistant during a challenging puzzle task. Teams are randomly assigned to an Al assistant\nwith a human- or robotic-sounding voice that provides either helpful or misleading information about the task. Treating each\nindividual Al interjection as a treatment intervention, we identify the causal effects of the Al on dynamic group processes\ninvolving language use. Our findings demonstrate that the Al significantly affects what teams discuss, how they discuss it,\nand the alignment of their mental models. Teams adopt Al-introduced language for both terms directly related to the task\nand for peripheral terms, even when they (a) recognize the unhelpful nature of the Al, (b) do not consider the Al a genuine\nteam member, and (c) do not trust the Al. The process of language adaptation appears to be automatic, despite doubts about\nthe Al's competence. The presence of an Al assistant significantly impacts team collective attention by modulating various\naspects of shared cognition. This study contributes to human-Al teaming research by highlighting collective attention as a\ncentral mechanism through which Al systems in team settings influence team performance. Understanding this mechanism will\nhelp CSCW researchers design Al systems that enhance team collective intelligence by optimizing collective attention.", "sections": [{"title": "1 Introduction", "content": "The coordinated attention of team members is fundamental to effective collaboration [1]. Collective attention is manifested in\nvarious forms of cognitive alignment that together define how teams cooperatively focus, divide, sustain, and alternate their\nattention to team-related activities to accomplish team goals. It encompasses the alignment of both the timing and content\nof attention [2]. For example, alignment on shared terminology within a team reflects coordinated attentional priorities that\nfacilitate the synchronization of collective focus.\nWith the rise of human-AI teaming, a critical question arises: How does the presence of an AI assistant affect cognitive\nalignment in human teams? While cognitive alignment is widely recognized as a key driver of team performance [3], it may\nfail to materialize in human-AI teams due to fundamental differences between human-human and human-AI interactions\n[4-6]. Despite their growing importance, the dynamics and implications of collaborative processes in human-AI teams remain\nunder-explored [6-8]. Understanding the impact of AI systems on team processes beyond that of a single human-AI dyad is\nvital for informing computer-supported cooperative work (CSCW) research on the design of human-AI systems that facilitate\nproductive collaboration and enhance the collective intelligence of human teams [1, 8-10].\nCognitive alignment in teams is a multi-faceted concept closely intertwined with alignment on shared language [11, 12].\nConvergence on common terminology signifies that groups have established efficient meta-cognition, a prerequisite for effective\ngroup processes. Through this lexical alignment, team members can effectively direct each other's attention [13]. Alignment on\na shared language also affects what is salient and attended to within a team [14]. A shared language is not only an indicator of\ncognitive alignment, but may in fact be the primary mechanism through which humans achieve cognitive alignment [12] and\nthus coordinate collective attention.\nAl systems assume increasingly prominent roles in high-stakes collaborative contexts, such as emergency response and\nmedical diagnosis. However, research on the impact of AI on team performance has yielded inconsistent results, with some\nstudies finding increased performance and others finding decreased performance [e.g., 15\u201317]. To explain why AI systems\nare sometimes helpful and sometimes not\u2014and, ultimately, to design more effective human-AI collaborative systems-it is\ncrucial to better understand how AI affects known antecedents of team performance. Specifically, we need to examine how AI\ninfluences secondary outcomes like collective cognition (including shared language) that shape teams' collective attention [13,\n18]. If AI systems provide quality input but undermine collective attention, they may fail to achieve their goals of enhancing\nteam performance. Without a deeper understanding of how AI affects fundamental team processes of collective attention (and\nrelated components of collective memory and collective reasoning; [2]), it will be difficult to predict the impact of AI on team\nperformance. Insight into these interactions can guide the design of AI systems that better support human teams, enhancing\ntheir effectiveness in complex tasks [9].\nA distinguishing aspect of our study is its focus on human-human communication within AI-assisted teams. We examine"}, {"title": "2 Background", "content": "This study investigates the impact of diverging AI designs on two critical aspects of human teamwork: cognitive alignment of\nattention and lexical alignment. The following section elucidates these concepts, highlighting their significance and relevance\nto our study in the context of computer-supported collaborative work and artificial intelligence.\nHuman Attention. Human attention is the ability to actively process specific information in the environment while tuning out\nother, less relevant details [24]. Attention is limited in terms of both capacity and duration [25]. Managing this limited resource\neffectively is critical to achieving high task performance [26].\nIn group settings, individuals usually align their attention with the attention of other group members, which includes\nwhat individuals perceive and remember [27]. Effective group work requires coordination of attention in order to achieve\ncoordination of actions [28]. Groups are said to have a high level of collective attention when individuals are co-attending\nto an object or event with others [29]. This results in a shared emotional and psychological experience with implications\nfor motivation, judgment, and behavior [1]. For example, collective attention in a shared social context on the same target\nallows group members to prioritize and coordinate their team-related activities [1]. As a result, collective attention is strongly\nassociated with collective intelligence and team performance [28, 30-32].\nCollective attention is sometimes operationalized as the temporal focus of a group on specific information or events, such as\nviewing or sharing content on social media [33], various forms of interactional synchrony such as facial expression and vocal\nalignment [1], and language use [12]. Language, in particular, plays a crucial role in coordinating beliefs among humans and\naligning neural processes that govern collective attention [34, 35]. Humans may even be predisposed to align their mental states\nwith those of other individuals [36].\nWhy Does Human Attention Matter? Higher levels of collective attention not only increase the pool of attentional resources\navailable for achieving team goals, but also actively shape the approaches and strategies used in problem-solving [37]. That is,\nthe object in focus and the shared language used to talk about it can shape abstract intentions, plans, and proposals [38]. The\nresulting heightened focus enhances the team's ability to devise innovative solutions and respond effectively to challenges [39].\nA critical question for research on human-AI teams is how AI agents might influence the collective attention of a team [8,\n40]. Understanding this impact is essential for optimizing collaboration and enhancing team performance [41]. Importantly, AI\nmay affect not only the overall level of collective attention but also the object and manner of that attention [42], both of which\nmay affect the problem solving approaches employed by the team [43, 44].\nAlignment on Shared Language. The study of linguistic convergence has progressed significantly since its inception, shifting\nfrom early explorations of interactions between large language groups and dialects to contemporary explorations of fine-grained\nlanguage coordination in interpersonal conversations [45]. Lexical alignment, a key focus in this domain [46], examines the\nprogressive adoption of similar vocabulary by interlocutors in dialogue [47]. This phenomenon exemplifies the formation\nof shared conceptual frameworks, which emerge as speakers subtly negotiate and align their terminology [48]. Within this\nparadigm, the principle of input-output coordination describes a fundamental interaction mechanism [49]: each participant's\nutterances propose a conceptual framework that shapes the next speaker's response. These interactions often solidify into\n\"conceptual pacts\u201d\u2014persistent, partner-specific shared understandings [50]. The importance of these pacts is underscored by the\nprocessing costs that arise when a speaker unexpectedly deviates from them [51]. A shared language thus enhances a group's\ncollective attention by freeing up cognitive resources, facilitating more efficient communication and cognition.\nOpen Questions How could the presence of an AI assistant affect the formation of a shared lexicon? Several gaps remain in\nour understanding of collective attention in human-AI teams. Recent research has shown that lexical alignment may be affected\nby the perceived competence of conversational partners [52]. Humans have been shown to align more towards a computer\npartner than a human partner, even when the linguistic behavior is identical [21]. This supports a \u201ccommunicative design\"\naccount of alignment, where speakers deliberately align more with partners they perceive as less communicatively competent in\norder to facilitate successful communication."}, {"title": "3 Related Work", "content": "Our research is most closely related to work on (a) human-AI teaming and (b) lexical alignment between humans and artificial\nsystems.\n3.1 Advancing Team Dynamics: The Role of Al Design in Enhancing Communication and Collaboration\nPrevious research has explored the design of human-AI interfaces to improve team dynamics. These interfaces enhance\ncollaboration by sharing control, improving understanding, employing social-cognitive strategies, building mutual trust, and\nboosting overall team performance [20, 67\u201371]. For instance, sharing control in human-robot teams leverages complementary\nskills, which significantly enhances decision-making and task efficiency [67]. This aligns with findings that emphasize the\nimportance of understanding AI capabilities, particularly an AI system's error boundaries, in boosting team performance by\ncomplementing human skills [20]. Other research has focused on developing AI agents capable of constructing mental models\nof human teammates in order to steer team communication by alleviating information overload and supporting metacognitive\nprocesses, such as building shared mental models of \"who knows what\" [8]. Moreover, research indicates that establishing\nbi-directional trust in human-AI teams not only involves dynamic relationship management and adaptive systems, but also\nnecessitates co-discovery learning and algorithmic transparency [71]. Other studies reinforce this finding by demonstrating that\ncombining explicit AI explanations with implicit manipulations significantly improves trust dynamics within these teams [72].\nBuilding upon the foundation laid by previous research, the present study investigates the impact of specific AI designs on\nteam communication and collaboration. Inspired by prior work exploring the dynamics of human-AI interaction, we advance\ninto the nuances of AI influence on teamwork. By analyzing the effect of different AI demeanors on team processes, we aim to\nexpand the current state of the art in this field. Our research addresses a notable gap by focusing on the direct effects of AI\nbehavioral characteristics on the efficiency and quality of team interactions. This approach not only enhances our knowledge of\neffective AI integration into collaborative settings, but also sets new benchmarks for designing AI systems that are more adept\nat facilitating human teamwork. Through this work, we contribute to a deeper understanding of AI deployment in team-based\nenvironments.\n3.2 Lexical Alignment in Human-Al Interaction\nLexical alignment is a complex phenomenon [21, 49] influenced by various factors, including the perceived competence of\nthe interlocutor, the interaction history between partners, and specific task demands [73\u201375]. While several studies have\ninvestigated lexical alignment in human-AI interaction [21, 23, 76\u201378], they predominantly focus on dyadic interactions\nbetween one human and one AI agent [76], leaving the dynamics of human-human-AI interactions largely unexplored.\nEvidence suggests that humans align differently with AI partners than with human partners [79, 80]. For example, research\non text-based and speech-based interfaces indicates that humans are at least as likely to adopt terms used by a computer system\nas they are to adopt terms used by human partners [21]. A more recent study suggests humans align more strongly with other\nhumans than with artificial agents [23].\nWhile linguistic alignment is observed in both human-human and human-AI interactions, the underlying processes may\ndiffer significantly. With human partners, linguistic alignment emerges from a collaborative, negotiated process of establishing\nshared conceptualizations. With a computer, individuals may adopt the system's terms more strategically in order to avoid"}, {"title": "4 Data", "content": "We ran a randomized controlled trial with 69 human subjects as an IRB-approved lab study. Teams of three or four people\ncollaborated to solve a puzzle task using online video conferencing software. During the task, the team was assisted by an\naudio-only AI team assistant. We used a 2 \u00d7 2 between-subjects design which manipulated the quality (helpful vs. wrong)\nand voice (human-sounding vs. robotic-sounding) of the AI assistant. Following the puzzle task, participants completed a\npost-experiment questionnaire. For their time, which lasted up to 90 minutes, they received compensation in the form of\nAmazon gift cards at a rate of $20 per hour. We recruited participants through flyers, email lists, and word of mouth from the\nNortheast United States to participate in this study. Team size varied between three and four people based on convenience.\nWhile we scheduled four participants for each session, we proceeded if at least three were present. The participant pool was\npredominantly young, with a mean age of 21.7 years, and skewed female (Table 1).\n4.1 Experimental Task\nThe experimental task is a pattern recognition challenge framed as a \u201cCursed Treasure Puzzle.", "octopus curse\" is present when none of the gems touch each other.\nThe task is particularly suitable for studying the development of shared language because it presents participants with several\nunfamiliar symbols for which they do not have established terminology. Since the symbols can be described using a variety of\nwords, teams must develop a shared vocabulary to effectively distinguish them in order to direct each other's attention and\ncommunicate precisely [81, 82]. Several other studies investigating the development of shared language have relied on similar\nsymbol tasks [81, 82]. The pattern recognition task is designed to be challenging; most teams fail to reach all milestones. To\nhelp them, we introduce an AI assistant called the \u201cAI Puzzle Master": "hat dispenses one clue for each milestone.\n4.2 Al Assistant\nThe \"AI Puzzle Master\" was presented as a disembodied entity, represented by a black screen displaying its name in the online\nvideo conferencing software. At the experiment's outset, an experimenter read a script introducing the Puzzle Master as an AI\nassistant capable of offering helpful information. The AI assistant's interaction was one-sided, communicating with teams only\nvia pre-recorded audio messages played at predetermined intervals. We created four different AI assistant recordings, one for"}, {"title": "4.3 Tracking Linguistic Alignment", "content": "To measure the AI assistant's impact on team collective attention and linguistic alignment, we transcribed the audio recordings\nof team communications and identified each instance of a team member referring to one of the milestones (Fig. 2). Each\nmilestone in the puzzle task is represented by a symbol corresponding to the nature of the \u201ccurse", "duel curse\" milestone is represented by a pair of crossed swords. We compiled a complete list of terms\nused by participants to refer to each of the milestones, the symbols, and the gems, including different terms used to refer to the\nsame thing. This list of terms formed our set of tracked terms. Each time a participant used one of the tracked terms to refer to\na milestone, a symbol, or a gem, we recorded its use, forming the basis of our lexical alignment measure (see next section).\nThe AI assistant consistently used the same language to refer to each of the milestones and to the gems. As a result, we\nwere able to analyze the team's term usage to see whether they used the same terminology as the AI or different terminology.\nEvery time we updated the running usage counter for one of the terms, we also updated a running lexical alignment score based\non whether or not the term matched the AI's term for that referent. The alignment score increased if the term was an AI term\nand decreased if it was not (unless it was already at 100% or 0%, in which case it did not change).\nWithin the seven tracked referents, we distinguish between five core referents and two peripheral referents. The five referents\ncorrespond to the five milestones, each of which is the subject of one clue from the AI assistant. Since the core referents are\neach directly related to a milestone (a pattern that teams need to uncover), the AI assistant's interjection can be identified as\nhigh or low quality depending on whether it is helpful or intentionally misleading. The peripheral referents are \u201csymbols\u201d and\n\"gems\", which are relevant to the task but are not milestones themselves. These two referents can be considered neutral in that\nthey are not the subject of a high or low quality interjection. Gems are mentioned in several AI interjections, while symbols\nare never mentioned, making the symbol referent an appropriate baseline from which changes in collective attention can be\ncontrasted.\nIn summary, each referent corresponds to a set of tracked terms and each referent is mentioned by the AI assistant in one of\nits interjections. Following the interjection, teams may talk more (or less) about the mentioned referent, and they may do so\nusing either the same term used by the AI or use a different term to refer to the referent (for example, \u201cduel": "s the term the AI\nassistant used or \"swords\" as some of the teams used)."}, {"title": "4.4 Lexical Alignment Measure", "content": "Lexical consensus is calculated as a proportion: the usage of a particular term to refer to a given referent divided by the usage\nof any term to refer to the same referent. Lexical alignment is the direction of lexical consensus over time. Our interest is in\nalignment with the language used by the AI assistant, so the usage of the AI assistant's terminology is the numerator of the\nconsensus ratio. The measure ranges between 0, indicating no use of the AI's terminology, and 1, indicating that all mentions\nuse the AI term.\nTo illustrate the measure, Fig. 3 shows all uses of tracked terms in one of the teams. The very first mention of the gems\nreferent uses the term \u201cgem,\u201d starting the alignment measure at 1 (i.e., 100%) indicating that all mentions used the same term\nas the AI assistant. After this first reference, up to minute 14 and the first AI interjection, most references to the gem use a\ndifferent term (e.g., \"diamond\") and the alignment measure decreases. Then the AI interjects, using the term \u201cgem,\" and all\nfuture mentions in the team also use the AI term. The alignment measure increases, approaching 1 at the end.\""}, {"title": "5 Method", "content": "Human communication is a dynamic process. A key challenge in our analysis is to causally attribute specific changes in this\nfluid team communication to the interventions of the AI assistant. To achieve this causal identification, we employ a panel\nregression using observations on the utterance level following a difference-in-difference (DD) framework [86]. Specifically, we\nconsider term use (as captured by the lexical alignment measure) in segments right before the AI intervention and right after for\neach of the six AI intervention (e.g., Segment 1 surrounding the \"computer hex\" clue; see bottom of Fig. 3). The DD analysis\nthen compares the before/after change in AI term usage during a specific segment to the same before/after change during the\nsame segment of a term that was not used by the AI. For example, the before/after change in the usage of the term \"octopus\"\nduring the segment surrounding the \"pirate clue\u201d serves as a no-treatment control (because the \"pirate clue\u201d did not mention\n\"octopus\") to the before/after change in term usage of \"pirate\" (which could have been affected by the AI's mention of \u201cpirate\u201d).\nFormally, we estimate the following regression equation\n$$EntrainmentOnAlogs =B_1Afterogs + B_2Afterogs \u00d7 AIQuality_g\n+B_3ObjectMentions_og + B_4ElapsedTime og\n++ A_g + a_s + E_ogs$$\nwhere After is a dummy variable indicating whether the utterance related to object o happened before or after the AI intervention\nrelated to object o, AIQuality is a dummy indicator whether the team g was assigned to the high (vs. low) quality condition,\na are object-level fixed effects, ag are group-level fixed effects, as are segment-level fixed effects, and \u0190ogs are error terms.\nThe coefficient B\u2081After captures the causal effect of the AI intervention, and the coefficient \u03b22 for the interaction term of\nAfter AIQuality captures the differential effect of the treatment condition. Estimates are driven by six before/after comparisons\ncreated by the six AI interventions, each one of them comparing five unaffected terms serves as control contrasted from one term\naffected by the intervention. We include the count of ObjectMentions and the ElapsedTime (in minutes) as control variables.\nWe estimate variations of this equation where we substitute Humanness and Treatment as key variables of interest for AIQuality.\nWe report standard errors clustered at the team-level. Notice the main effect of the treatment will drop out due to the team-level\nfixed effect and only the interaction term remains."}, {"title": "6 Results", "content": "This section presents five sets of results in a comprehensive analysis of the AI assistant's impact on team collective attention.\nFirst, we offer an anecdotal description of the experience of one specific team to showcase how teams adopt the AI's lexicon\n(addressing RQ1 & RQ2). Second, we offer causal evidence that the AI assistant affects what teams talk about (addressing\nRQ1). Third, we offer causal evidence that the AI assistant affects how teams talk about the referents we track (addressing RQ2)\nand how the alignment of language differs across the quality and humanness of the AI (addressing RQ3). Fourth, we show that\nthese findings are reflected in the teams' retrospective and reified perceptions of their own team's processes (combining RQ1 &\nRQ2 into a higher-level construct). Finally, we draw on complementary data sources to explore the mechanism behind the\ninfluence of the AI on team collective attention (addressing RQ4).\n6.1 How an Al Assistant Shapes Team Communication and Development of Shared Language\nWe begin our analysis with an anecdotal description of the experience of Team 18, a team of three in the Human-Wrong\ncondition. After receiving their instructions, the team begins the task by looking for patterns among the gems. They refer to the\ngems as \"diamonds.\" They point out various features of the treasure chests and identify some similarities between chests with\nthe same curse. In the first fifteen minutes, they refer to the gems seven times using the word \"diamond.\" Nobody uses the\nword \"gem\" until the AI assistant interjects at 15:07 with the first clue-in this case, a deliberately misleading clue about the\ncomputer hex milestone. This clue includes a mention of \u201cgems.\u201d\nFollowing the interjection, the team's attention shifts and they begin discussing the computer hex curse, which has so far\nnot been addressed. They express confusion because they cannot see the pattern suggested by the clue (indicating an answer\nfor our RQ1 about the AI's effect on the timing of what teams talk about). Nevertheless, they stop using the word \"diamond\"\nand instead adopt the term \"gem\u201d that was used by the AI. There are another eight references between the first and second AI\ninterjection, all now using the term \"gem.\u201d The team has adopted the AI's language for the peripheral gem referent (indicating\nan answer for our RQ2).\nAt 20:07, the AI provides the second clue. This clue states that the octopus milestone has to do with the sizes of the gems.\nAt this point, the team grows suspicious of their AI assistant because this clue contradicts the pattern they have begun to identify.\nOne participant suggests that the clues are intended to throw them off, but another team member says \u201cwe have to trust the\nPuzzle Master.", "30": 7, "duel": "nd", "sword": "o refer to the duel\nclue throughout the rest of the task, not fully adopting the language proposed by the AI assistant for this core referent. The\nteam does not manage to complete the task in time.\nA different team assigned to the Robotic-Helpful condition, Team 19, exhibits a contrasting pattern. Participants in this team\ndemonstrated higher overall trust in their AI assistant, which provided them with helpful rather than misleading information.\nThe initial two clues, pertaining to the \u201ccomputer hex\u201d and \u201coctopus\u201d milestones, employ terminology consistent with the\ngroup's usage at the experiment's outset, prompting no change in lexical choices. Both clues align with the group's observations,\ngiving participants no reason to doubt the reliability of the AI. The third clue, introduced at 24:34, informs the group that the\nquadruple bypass milestone relates to the colors of the gems. At this point, a team member clarifies that the", "heartbreak": "r \u201cheart\u201d milestone. Notably, the team persists"}, {"title": "6.2 Al Assistant Shapes What Teams Talk About", "content": "The AI assistant significantly influenced the focus of team discussions, affecting what teams talked about and therefore what\nthey paid attention to (addressing RQ1). Our analysis compares the frequency of tracked referent mentions during baseline\nsegments to those immediately following AI interventions (Fig. 4). In every instance, team discussions of the referent increased\nfollowing the AI's mention, suggesting that the AI intervention significantly shaped their collective focus of attention. Teams\nwere significantly more likely to discuss the mentioned referent in the period immediately following the intervention compared\nto other referents (we estimate the same difference-in-difference model from 1 but use a binary indicator of whether a mentioned\nterm corresponds to the AI intervention as the dependent variable; \u03b2 = 0.35; p < 0.001). Overall, teams were 1.5 times more\nlikely to discuss the referent in the segment following the Al's mention than in any other segment. The impact of the AI on\nteam collective attention is not significantly different across the treatment groups, indicating that the mere mention of a referent\nby the AI influenced group attention, regardless of the quality or voice of the AI assistant. Recall that the AI in this study was\nnot interactive; thus, the observed effect was not due to direct engagement with the AI, but to an influence on human-to-human\ncommunication."}, {"title": "6.3 Team's Adoption of Terms Used by the Al: Lexical Alignment", "content": "The AI assistant not only shapes what teams talk about but also how they talk about it. We find significant signs that teams\nalign their communication with the language introduced by the AI assistant (answering RQ2; Table 2). First, teams show strong\npatterns of alignment caused by the AI assistant, indicated by the positive and significant coefficient of the After dummy (Model\n1, \u03b2 = 0.06, p < 0.01). This overall coefficient masks important heterogeneity of an even stronger effect in some conditions\nand lower alignment in others. Alignment is especially high when the AI has a human sounding voice (Model 2, \u1e9eAfter = 0.14,\np < 0.001) and teams align less on the AI's language when the AI has a robotic voice (Brobotic = \u22120.11, p < 0.05). Alignment\nis lower when the AI is of low quality (as compared with high quality) but the difference is not statistically significant (Model\n3, Bwrong = -0.08, ns). Finally, we find significant interaction between the robotic voice and AI quality. Teams align with\nAl terminology most strongly in the Human-Helpful condition (Model 4, \u1e9eAfter = 0.15, p < 0.001). Compared against this\nbaseline, alignment is significantly lower in the Robotic-Helpful condition (\u03b2 = \u22120.09, p < 0.01), and lower still when the\nAI is both robotic and provides low-quality assistance (\u03b2 = \u22120.16, p < 0.05). There is no significant difference between the\nRobotic-Helpful and Robotic-Wrong condition (linear hypothesis test: x2 = 1.40; p = 0.24). Together, these results answer\nRQ3 regarding how the effect of the AI differs based on (a) its quality and (b) its humanness.\nThe lexical alignment patterns differ substantially between core referents and peripheral referents. Whereas the coefficient\nindicating After AI intervention is positive for referents with high relevance to the clues (\u1e9e ranging from 0.06 to 0.15, p at least"}, {"title": "6.4 Shared Mental Models", "content": "So far, we have shown evidence of the AI assistant's substantial impact on collective attention and shared cognition through\nvarious behavioral measures of alignment in team language use. In this section, we explore whether these findings are reflected\nin the participants' retrospective and reified perceptions of their own team processes (combining RQ1 & RQ2 into a higher-level\nconstruct). The combination of evidence from behavioral measures of group interaction and participants' self-reflection of\ntheir group's process offers particularly compelling support for the AI assistant's influence on collective attention [3, 11].\nSpecifically, we investigate the effect of AI on the emergent shared cognition through which groups create and use interpersonal\nunderstanding. This shared cognition represents the manner in which knowledge important to team functioning is mentally\norganized, represented, and distributed within the team [3]. Recognized as a central positive driver of team behavioral processes,\nmotivational states, and team performance [3, 88], emergent collective cognition is especially beneficial in interdependent\ncognitive tasks where leveraging team members' expertise is crucial [89]. Teams with well developed shared mental models\ntypically possess a common view of \u201cwhat is happening, what is likely to happen next, and why it is happening\" [88, p.879].\nTo assess the effect of the AI assistant on team cognition, we investigate whether the it affects the alignment or similarity-\nof mental models within a team. We measure the alignment of mental models as the degree to which members' mental models\nare consistent [88]. First, we assess shared mental models using six items from [90]. Items propose statements like \u201cmy team\nknows specific strategies for completing various tasks\u201d and participants are asked to respond on a Likert scale. With this data,\nwe compute an intragroup agreement index within each item using standard deviation (to measure how consistently within-team\nresponses overlap versus diverge), and then average the standard deviations across the six items.\nWe find a medium level of alignment in shared mental models in both helpful conditions, while the unhelpful conditions\nexhibit diverging patterns (Fig. 5). Mental models are less well aligned (i.e., have higher standard deviation) in the Human-\nWrong condition, while we find the highest level of alignment in the Robotic-Wrong condition. A significant interaction exists\nbetween the two factors of our experimental design (one-way ANOVA F(3,65) = 5.5; p = 0.041). We theorize that this pattern\nis the result of an \"us vs. them\" dynamic, which we discuss in detail in the next section."}, {"title": "6.5 Exploring the Mechanism Behind Al's Influence on Team Collective Attention: A Closer Look at Complementary Data Sources", "content": "The above analyses reveal the substantial impact of the AI assistant on three aspects of collective attention: what teams\ntalk about, how they talk about it, and the alignment of their mental models. In the following section, we provide further\nevidence for this significant effect on cognitive alignment within teams and explore the nuanced contrasting effects between\ncore and peripheral terms (addressing RQ4). We investigate the role of trust in AI, the effect of the AI assistant on the collective\nidentity of the team, and the alignment of mental models. By integrating the evidence from above with self-reported survey\nmeasures, we offer a more comprehensive account of cognitive alignment on a different timescale.\n6.5.1 Trust in Al\nTrust in AI plays a pivotal role in shaping the nature and quality of human-AI interactions[91]. We theorize that the the level of\ntrust individuals place in AI not only influences direct human-AI interactions, but also impacts interpersonal dynamics among\nhuman team members when an AI assistant is present. To measure trust in the AI assistant and perceptions of the assistant's\ncontribution quality, we use six items adapted from [92] (originally designed to measure trust in online avatars). Participants\nresponded on a five-point Likert scale from Strongly Disagree to Strongly Agree to items like \"The Puzzle Master gave helpful\nclues.\"\nHumans rated the low-quality AI as significantly less trustworthy (p < 0.001) and less intelligent (p < 0.001) than the\nhigh-quality AI. Voice anthropomorphism did not have a significant impact on participant impressions of trustworthiness\n(p = 0.570), but the ratings were slightly lower in the Robotic treatments. The greater level of trust in the high-quality AI\nconditions may explain why participants aligned more with the AI assistant's language for core referents in these conditions.\nHumans are more likely to adopt recommendations by trustworthy agents that provide high-quality explanations [93]. High\ntrust can also indicate the perception of higher status [94], which may explain why the AI assistant in in the high-quality\nconditions has more influence over the team's language dynamics. Conversely, lower trust and lower perceived status can make\nthe AI assistant easier to ignore, thus giving it less influence over the lexical choices of team members.\nOverall, a stark pattern emerges: even in low-quality conditions in which trust assessments of the AI assistant were\nsignificantly lower, the AI assistant directed team attention to the specific task aspects it mentioned (see Section 6.2) and\ninfluenced their lexical choices. This pattern demonstrates the power of the AI assistant to shape collective attention and shared\nlanguage, even when teams are aware of its low quality and have decided not to trust it.\n6.5.2 Is the Al Assistant a Part of the Team?\nWe asked individuals to report whether they felt that the AI assistant was part of their team or even a leader of their team. Humans\ndid not consider the AI to be a part of their team, with 77% of participants responding \"Strongly Disagree\u201d or \u201cSomewhat\nDisagree.", "Strongly Disagree": "r \u201cSomewhat Disagree.\u201d Ratings were especially low in the Robotic-Wrong\ncondition, where 96% did not consider the AI assistant to be a leader of the team and 93% of participants did not consider it to\nbe part of the team at all.\nThis suggests that the AI assistant's substantial influence over teams' collective attention and lexical alignment is not\npredicated on its inclusion as a team member, but rather occurs despite it being considered an outsider. This phenomenon\nindicates a path through which AI assistants can\u2014intentionally or unintentionally\u2014affect intra-team communication and\ncognitive alignment. By presenting as a non-human entity, AI assistants may strengthen the bond among human team members\n(the in-group"}]}]}