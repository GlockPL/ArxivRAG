{"title": "Online Reinforcement Learning with Passive Memory", "authors": ["Anay Pattanaik", "Lav R. Varshney"], "abstract": "This paper considers an online reinforcement learning algorithm that leverages pre-collected data (passive memory) from the environment for online interaction. We show that using passive memory improves performance and further provide theoretical guarantees for regret that turns out to be near-minimax optimal. Results show that the quality of passive memory determines sub-optimality of the incurred regret. The proposed approach and results hold in both continuous and discrete state-action spaces.", "sections": [{"title": "I. INTRODUCTION", "content": "Several reinforcement learning (RL) algorithms such as Q-learning [17], deterministic policy gradient (DPG) [16], trust region policy optimization (TRPO) [14], and proximal policy optimization (PPO) [15] do not leverage any memory or past interaction with the environment. However, the success of several deep learning RL counterparts such as deep Q-learning (DQN) [10], deep double-Q learning (DDQN) [20], and deep deterministic policy gradient (DDPG) [9]) can be attributed in part to the replay buffer that stores past interactions with the environment. The replay buffer is used to update the target network of the value function and was first proposed in [10]. Based on empirical evidence, the authors conjecture that the replay buffer and target network stabilize training. In this paper, we take a step towards analyzing the theoretical properties of memory and its impact on online RL algorithms. We show that the incurred regret depends on the quality of passive memory.\nIn neuroscience and psychology, episodic memory is defined as the ability to recall and mentally re-experience specific episodes from one's personal past and is contrasted with semantic memory that includes memory for generic, context-free knowledge [7]. Moreover, associative memory refers to forming relationships between potentially unrelated objects [18], e.g. association of an aroma to a particular person or object. It has been shown that humans and other animals use episodic memory for decision-making [6]. The global neuronal workspace (GNW) theory of consciousness [21] and unlimited associative learning (UAL) [3] both argue that sensory perception, motor control, value, and event memory have evolutionary origin.\nThe class of offline RL algorithms typically do not interact with the environment [8]. This is in contrast to \"ideal\" human behavior where episodic memory is used for decision making. These algorithms also rely on density ratio realizability assumptions that may not hold in general for a given function class [5]. In contrast, our approach requires density coverage with respect to only the optimal policy. Recent work on RL with prior data [1] is complimentary to our work, as it provides several heuristics (design choices) with ablation studies to constrain the online interaction to offline data such as symmetric sampling of online and offline collected data. Unfortunately, [1] does not provide any theoretical guarantees. Our proposed algorithm is more principled and provides theoretical guarantees that it is near-minimax optimal.\nIn the context of RL, [22] proposed an algorithm that leverages associative memory. However, this algorithm breaks down for simple stochastic environments and does not have any finite-sample theoretical guarantees; the convergence guarantee for Q values is asymptotic (rather than finite-sample) and is also limited to discrete sample spaces. Since people, animals, and numerous other systems operate in the continuous domain, it is preferable to have RL algorithms that can handle both discrete and continuous state-action spaces. Our work provides this flexibility and also comes with finite-sample guarantees."}, {"title": "II. BACKGROUND, NOTATION, AND ALGORITHM", "content": "We briefly review the necessary background and establish notation for Markov decision process (MDP). Then we introduce the regularized LP formulation of RL with passive memory, the algorithm we consider.\n1) (Discounted) Markov Decision Process:\n\u2022 MDP setting: A (discounted) Markov decision process is given by a tuple $(S, A, R, T, \\gamma)$ where $S$ is the state space, $A$ is the action space, $R : S \\times A \\rightarrow R$ is the reward function, $\\gamma \\in (0,1)$ is the discount factor, and $T$ is the transition kernel, where $T : S \\times A \\rightarrow \\triangle(S)$ maps to a distribution over the state space $\\triangle(S)$.\n\u2022 Policy: A policy is a stochastic mapping from the state space to a distribution over the action space, that is, $\\pi : S \\rightarrow \\Delta(A)$. The optimal policy is denoted by $\\pi^*$.\n\u2022 Value function: The value function of state $s_0$ for policy $\\pi$ is the discounted sum of future rewards if we start from $s_0$ and follow $\\pi$, that is, $V^{\\pi}(s_0) = E[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)|s_0, \\pi]$. The optimal value function is usually denoted by $V^*(s_0)$.\n\u2022 Value function for an initial state distribution: Similar to the value function for a state, we can define the value function for an initial state distribution as $V^{\\pi} (\\mu_0) = E[E[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)|s_0, \\pi]]$, where $\\mu_0$ is the initial state distribution.\n\u2022 Discounted occupancy measure: We define discounted occupancy measure as the discounted measure over the state-action space if we start with an initial measure of $\\mu_0$ and follow the policy $\\pi$, that is, $d_{\\mu_0}^{\\pi}(s, a) = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s, a_t = a|\\pi, \\mu_0)$. We overload the notation and define $d(s, a) := d_{\\mu_0}^{\\pi}(s, a)$.\n\u2022 Passive Memory: The distribution induced by passively collected data is given as $dP(s, a)$.\n\u2022 Regret: The regret for any policy $\\pi$ is the difference in performance between the optimal policy and $\\pi$, that is, $R_T := \\sum_{t=0}^{T} V^*(\\mu_0) - V^{\\pi_t} (\\mu_0)$.\n\u2022 Goal: Throughout this work, the goal is to minimize regret.\n\u2022 Divergences: The Kullback-Leibler (KL) divergence is denoted by $D(d_1||d_2)$ and the Bregman divergence with a generating function $\\phi$ is denoted by $D_{\\phi}(x,y)$.\nTo make the manuscript less cumbersome, we have taken the liberty to denote densities and measures with the same notation, but this should be clear from context."}, {"title": "III. ONLINE RL AND ANALYSIS", "content": "In this section, we analyze the performance of online RL with passive memory and also provide the regret lower and upper bounds for online RL. We show that the proposed approach is near-optimal. Throughout the paper, we will assume without loss of generality that the reward is normalized to lie in [0,1]. We also assume that the state and action space is either compact or discrete, and the measure of entire state space is absolutely continuous (w.r.t. some underlying \u03c3-finite measure on the Borel \u03c3-algebra) bounded by S, A, respectively.\nA. Performance with Passive Memory\nWe first characterize the suboptimality of the proposed algorithm for a given dataset $dD(s, a)$ for an MDP M.\nTheorem 1 (Performance difference analysis). Given a dataset $dP(s,a)$ for an MDP M, the difference in the performance of the optimal policy and the policy produced by regularized LP formulation of RL is given by\n$V^{*\\pi} (\\mu_0) - V^{\\pi} (\\mu_0) \\leq  \\frac{1}{\\eta} log \\frac{d^*}{dD}(s,a)  \\frac{1}{(1 - \\gamma)CSA}.$\nHere $d^*(s,a)$ is the state-action distribution induced by the optimal policy, $c = \\frac{1}{\\parallel \\frac{d^*}{dD} \\parallel_{-\\infty}}$, and $||\\cdot||_{-\\infty}$ is the shorthand notation for $min_s |f(s)|$. Further, S, A are the bounds on a compact state and action space, respectively.\nProof: The proof is provided in Appendix A.\nB. Regret Lower Bound\nNext we consider the regret lower bound. We will build two similar MDPs where good performance in one MDP leads to bad performance in the other MDP. Let the first MDP be denoted by $M = \\langle S,A,R,T\\rangle$ and the other MDP be denoted by $M' = \\langle S, A,R',T\\rangle$. Let a policy $\\pi$ be run in $M'$ and $M$ and the history be denoted by $H = (s_0, a_0, r_0, s_1, a_1, r_1,..., s_H, a_H, r_h)$. The following lemma will be useful.\nLemma 1. Let $P_{\\pi,M}$ and $P_{\\pi,M'}$ be the (discounted) distribution of reward induced by policy $\\pi$ in the MDPs $M$ and $M'$, respectively. Then the divergence between $P_{\\pi,M}$ and $P_{\\pi,M'}$ can be expressed as\n$D(P_{\\pi,M}||P_{\\pi,M'}) = E[T_{s,a}(H)]D(d_{s,a}||d'_{s,a}) ds da.$\nHere $T_{s,a}(H)$ is the discounted time spent in state-action $(s,a)$ when the episode is truncated after a horizon of length H, and $d_{s,a}$ represents the distribution of reward at state $(s,a)$ for MDP M.\nProof: The proof is provided in Appendix B1.\nLemma 1 lets us relate the divergence between the long-term expected reward with the expected time spent in each state-action pair and the divergence between the distribution of rewards for the different MDPs $M$ and $M'$. If there are T policies and n episodes are repeated for each policy for a finite horizon of length H, then the minimax lower bound for the regret can be given as follows.\nTheorem 2 (Minimax regret lower bound). The minimax lower bound for continuous state-action space where the state space is a bounded compact space with bounded measure S and the action space is also a bounded compact space with bounded measure A is:\n$R_T = \\Omega(\\sqrt{\\frac{1 + \\gamma^H}{1-\\gamma}} SAnT).$\nHere H is the length of truncated horizon, n is the number of iterations used for density estimation for each episode by an agent, and T is the number of episodes.\nProof: The proof is provided in Appendix B2.\nObserve that the discrete case can easily be derived as a corollary by using discrete measure in Theorem 2.\nC. Regret Upper Bound\nNext we consider the regret upper bound for online RL that uses passive memory. We will first establish a general regret upper bound given an (imperfect) density estimation oracle. We will then use kernel density estimation techniques for the induced distribution over continuous state-action space, and plug-in density estimator for discrete state-action space in place of the oracle.\nTheorem 3 (Regret upper bound with density estimation error). Given passive memory $dP$ and a density estimation oracle with e as the density estimation error, then the regret can be upper bounded by\n$R_T = O(\\sqrt{\\frac{1 + \\gamma^H}{1-\\gamma}} D(d^*, dD)SA (\\epsilon + \\frac{nT}{1- \\gamma})).$\nFurthermore, if the passively collected data has support over the occupancy measure of the optimal policy,\n$R_T = O(\\sqrt{\\frac{1 + \\gamma^H}{1-\\gamma}} nTSA log(SA)).$\nHere S, A are the measures of compact state-action spaces and H is the truncated horizon length.\nProof: The proof is provided in Appendix B3.\nNote that for a discrete domain, the measures can be replaced by discrete measures. Thus in Theorem 3, the measures of compact space can directly be replaced by the cardinality of S and A.\nWe can see from Theorem 3 that if the passively collected data is \"close\" to the optimal distribution, then we incur a low regret with online RL algorithm.\nWe consider two separate cases for density estimation error-the plug-in estimator for discrete action spaces and kernel density estimator for continuous domains\u2014and use those estimation errors to find a probably approximately correct (PAC) regret bound.\nLemma 2 (Plug-in density estimator). If we use the following plug-in density estimator for discrete state action space\n$\\hat{dH}(s, a) = \\frac{(1 - \\gamma)}{n(1 - \\gamma^H)} \\sum_{i=1}^n \\sum_{h=0}^H [s_h^i = s, a_h^i = a]$\nthen with probability at least 1 \u2013 \u03b4\n$||\\hat{dH} - d||_{\\infty} \\leq \\sqrt{\\frac{3}{n}} \\sqrt{\\frac{log |SA|}{8}} + \\frac{log^{2/3} + \\sqrt{10 + 8 log \\delta}}{2n},$\nwhere $\\delta = \\frac{\\delta}{2|S||A|}.$\nProof: The proof is provided in Appendix B4.\nLemma 2 enables us to upper-bound the regret when we use a plug-in density estimator.\nNext, we focus on the state-action density for continuous domains and use the following kernel density estimator.\nDefinition 1 (State-action kernel density estimator). Let the kernel density estimate be\n$\\hat{d}(s, a) = \\frac{(1 - \\gamma)}{n(1 - \\gamma^H)} \\sum_{i=1}^n \\sum_{h=0}^H K(\\frac{(s, a) - (s_h^i, a_h^i)}{b}).$\nWe will make standard assumptions on the properties of the kernel function [19], namely:\n1) $d(s, a) \\in \\Sigma(\\beta, L)$, here $\\Sigma(\\beta, L)$ is the H\u00f6lder class, that is, $\\Sigma(\\beta, L) := {g : |D^{\\alpha}g(x) \u2013 D^{\\alpha}g(y)| \\leq L||x \u2212 y|| for all s such that |s| = \u03b2 - 1} and $D^{\\alpha}$ is the sth partial derivative. For example, $d = 1, \u03b2 = 1$ gives the Lipschitz property for the derivative, that is, |g'(x) \u2013 g'(y)| \u2264 L||x \u2212 y||.\n2) If g\u2208 \u03a3(\u03b2, L), then |g(x) - g_{x,\\beta}(u)| \u2264 L|x - y where $g_{x,\\beta}(u) = \\sum_{s \\leq \\beta}D^sg(x)$.\n3) $K(x) = G(x_1)G(x_2)\u2026\u2026G(x_d)$ where G has support on [-1,1], \u222b G = 1, \u222b |G|P < \u221e for all p > 1 and \u222b $t^sK(t) dt$ = 0 for all s < \u03b2.\n4) \u222b |t||K(t)| dt < $C_k$ and \u222b $t^sK(t) dt$ = 0, for all s < \u03b2.\nAn example of a kernel that satisfies the above condition for \u03b2 = 2 is $K(x) = 0.75(1 \u2212 x^2), |x| \u2264 1$. Next we calculate the bias of the proposed estimator.\nLemma 3. Let d(s, a) := E[d(s, a)], then the bias of the estimator of the occupancy density in a H\u00f6lder class can be bounded as\n$sup_{d \\in \\Sigma(\\beta,L)} |d(s, a) - \\hat{d}(s, a)| \\leq LC_kb^{\\beta}.$\nProof: The proof in provided in Appendix C.\nOnce we know the bias of our estimator, we decompose the upper bound on the $L_1$ error appropriately using McDiarmid's inequality [4].\nLemma 4. If d(s, a) := E[d(s, a)], then we have that with probability at least 1 \u03b4,\n$||d \u2013 \\hat{d}||_{L1} < LCKb^{\\beta}SA + \\sqrt{\\frac{ln \\frac{1}{\\delta}}{2nb^{2d}}}.$\nProof: The proof is provided in Appendix D.\nCombining Lemma 4 with the generalized regret bound of Thm. 3 will complete the desired results."}, {"title": "IV. CONCLUSIONS AND FUTURE WORK", "content": "We characterize the suboptimality of regularized LP formulation of RL. We also provide a passive memory based online RL algorithm and prove that it is nearly minimax optimal. Future work shall involve analyzing active memory where we purge and fill the memory as needed during online interaction rather than relying passively on pre-collected data from the environment."}, {"title": "APPENDIX", "content": "A. Proof of Thm. 1\nProof: We start by establishing the necessary condition for optimality for the LP of (2).\n$(1 - \\gamma)DvE_{\\mu_0} [V] + Dv log E_{dD} [exp{R + \\gamma TV \u2013 V}] = 0$\n$\\frac{dD}{(1 - \\gamma)E [exp{R + \\gamma TV \u2013 V}|S]}  - (1 - \\gamma)\\mu_0 = 0$\n$\\frac{ dD}{(1 - \\gamma)} E [exp{R + \\gamma TV \u2013 V}] - \\mu_0$\n$\\frac{E [exp{R + \\gamma TV \u2013 V}|S]}{E [exp{R + \\gamma TV \u2013 V}]} - \\frac{\\mu_0}{dD}$\n$\\frac{ ||exp{R + \\gamma TV \u2013 V}||_{\\infty}}{E [exp{R + \\gamma TV \u2013 V}]} - \\frac{\\mu_0}{dD}.$\nWhere we used an unusual notation $||\\cdot||_{-\\infty}$ to denote the minimum of absolute value, that is, $min | . |$.\nWe then use the necessary condition of (4) to obtain the performance difference as follows. The difference between the optimal and regularized solution can be given by:\n$|E_{d^*}R \u2013 E_{dD}R| \\leq ||d^* \u2013 d||_{\\infty} ||R||_{\\infty}$ (H\u00f6lder's inequality)\n$\\leq \\sqrt{D(d^* || dD)}$ (Pinsker's inequality and R is bounded)\n$= \\sqrt{\\frac{1}{2}  d^* log \\frac{dD exp{R + \\gamma TV \u2013 V}}{E [exp{R + \\gamma TV \u2013 V}]}}$\n$\\leq \\sqrt{D(d^* || dD)} +  d^* log \\frac{ exp{R + \\gamma TV \u2013 V}}{ E [exp{R + \\gamma TV \u2013 V}]}}$\n$\\leq  SA log  SA log - from necessary condition (4))\n$\\leq \\sqrt{ log  (1 - \\gamma)cSA}.$\nB. Proof of Regret Bounds\n1) Proof of Lemma 1:"}]}