{"title": "HIAGENT: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model", "authors": ["Mengkang Hu", "Tianxing Chen", "Qiguang Chen", "Yao Mu", "Wenqi Shao", "Ping Luo"], "abstract": "Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks. The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of action-observation pairs. We categorize memory into two types: cross-trial memory, accumulated across multiple attempts, and in-trial memory (working memory), accumulated within a single attempt. While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored. Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks. Inspired by human problem-solving strategies, this paper introduces HIAGENT, a framework that leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. Specifically, HIAGENT prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal. Experimental results across five long-horizon tasks demonstrate that HIAGENT achieves a twofold increase in success rate and reduces the average number of steps required by 3.8. Additionally, our analysis shows that HIAGENT consistently improves performance across various steps, highlighting its robustness and generalizability.", "sections": [{"title": "1 Introduction", "content": "Owing to the development of powerful reasoning capabilities of Large Language Models (LLMs) in recent years (OpenAI 2022, 2023; Meta AI 2024; Touvron et al. 2023; Jiang et al. 2023), LLM-based agents have demonstrated significant potential in various applications (Xie et al. 2023; Wang et al. 2024; Xi et al. 2023), such as software development (Hong et al. 2023; Bairi et al. 2024), robotic planning (Yao et al. 2022b; Puig et al. 2018; Singh et al. 2023; Huang et al. 2022a), simulating human behavior (Park et al. 2023), etc. Typically, an LLM-based agent refers to an interactive system that processes environmental observations, maintains context across multiple rounds of dialogue, and outputs executable actions tailored to completing a given task. Memory is one of the critical components of LLM-based agents, involving how agents store and utilize past experiences. When handling a specific task, an agent's memory can be divided into cross-trial and in-trial memory (also as known as working memory). Cross-trial memory typically consists of the historical trajectory information accumulated across multiple attempts at the current task. In contrast, in-trial memory pertains to the information relevant to the current trial. While many papers have explored leveraging cross-trial memory to optimize agent performance (Shinn et al. 2024; Zhao et al. 2024; Guo et al."}, {"title": "2 Preliminary", "content": null}, {"title": "2.1 Large Language Model based Agent", "content": "Large Language Model (LLM) based agents are intelligent autonomous systems designed to perform complex tasks. These tasks can be formalized as a partially observable Markov decision process (POMDP), characterized by the tuple $(S, O, A, T, R)$, where: $S$ denotes the state space; $O$ represents the observation space; $A$ signifies the action space; $T : SxA\\rightarrow S$ embodies the transition function; $R: S\u00d7A \\rightarrow R$ encapsulates the reward function; An LLM-based agent operates as a policy $\\pi(a_t|I, O_t, a_{t-1}, O_{t-1},..., A_0, O_0)$, which, given the historical action-observation pairs and instructions $I$ (encompassing in-context examples, environmental descriptions, etc.), generates an executable action $a_t \\in A$. Each action precipitates a new state $s_{t+1} \\in S$ and a subsequent observation $O_{t+1} \\in O$. This iterative interaction persists until either task completion or the agent reaches a predetermined maximum number of steps."}, {"title": "2.2 Working Memory", "content": "From the cognitive science perspective, working memory enables individuals to hold and manipulate information in real-time, facilitating complex cognitive tasks such as reasoning, comprehension, and learning (Newell, Simon et al. 1972; Anderson 2013). In LLM-based agents, we define working memory as the essential historical information required by the LLM at a given moment $t$ to complete the current task. Effective working memory management allows for better integrating past experiences and current stimuli, leading to more informed and accurate decisions. It can be likened to the human process of attentional control and cognitive updating, which involves selectively focusing on relevant information, filtering out distractions, and continually updating the mental workspace with new and pertinent data. The STANDARD approach in Figure 1 stores all historical action-observation pairs in working memory, i.e., $m_{itd} = (O_t, a_{t-1}, O_{t-1}, ..., a_0, O_0)$. Although this provides the LLM with comprehensive information, it also introduces redundancy, complicating the LLM's processing."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Overview", "content": "The core idea of HIAGENT is to employ subgoals for hierarchical management of working memory. More specifically, as is shown in Figure 2, the process of HIAGENT can be described as follows: (1) Before generating specific grounded actions, we prompt the LLM to first formulate a subgoal $g_i$. Each subgoal serves as a milestone within the overall task. (2) Subsequently, the LLM generates precise actions to accomplish this subgoal. (3) Upon the LLM's determination that a particular subgoal has been fulfilled, we synthesize the corresponding action-observation pairs into a summarized observation $s_i$ (\u00a73.3). We then obscure the action-observation pairs within the"}, {"title": "3.2 Subgoal-based Hierarchical Working Memory", "content": "As is shown in Figure 2, at each time step, the LLM can either generate the next action for the current subgoal or generate a new subgoal when it determines that the existing subgoal has been accomplished. For the current subgoal, the agent retains all action-observation pairs, providing a detailed context for immediate decision-making. For past subgoals, only a summarized version of the observations is kept. This subgoal-based hierarchical management approach in HIAGENT is deeply motivated by cognitive science principles, drawing parallels with human cognition and problem-solving strategies (Newell, Simon et al. 1972; Anderson 2013). Employing subgoals to compartmentalize action-observation pairs can be conceptualized as a form of chunking methodology. In human cognition, chunking allows individuals to group related information into meaningful units, thereby overcoming working memory limitations (Miller 1956). Similarly, HIAGENT utilizes subgoals as cognitive chunks, encapsulating related actions and observations. This chunking mechanism enables the system to handle complex sequences of information more effectively, reducing cognitive load and enhancing overall performance. Furthermore, by generating subgoals before specific actions, the system mimics the human tendency to break down larger objectives into more manageable components. This methodology enhances computational efficiency and aligns with established theories of human information processing."}, {"title": "3.3 Observation Summarization", "content": "The process of observation summarization can be formalized as $s_i = S(g_i, O_0, a_0, ..., O_t)$, where $S$ can be implemented using either a Large Language Model (LLM) or alternative text summarization models. This function encapsulates the synthesis of historical observations and actions, contextualized by the current subgoal, to produce a concise representation of the agent's state. Furthermore, a crucial component of the summarized observation is assessing whether the current subgoal has been achieved. This evaluation serves as a pivotal guide for future subgoal generation, facilitating adaptive and goal-oriented behavior in the agent's decision-making process. By doing so, the agent can maintain a condensed yet informative context, balancing the need for historical information with efficiency. The example prompt is as follows:\nYou are an advanced AI system tasked with summarizing and analyzing a series of action-observation pairs (trajectories) and determining whether a specific subgoal has been met.\nYour goal is to create a summary that captures all essential information, decisions, and outcomes from the given trajectories, and indicate whether the subgoal has been met based on the summarized observations.\nIf there are no valid actions taken, you need to analyze the reason.\n### Instructions:\n1. Provide a summarized observation related to the subgoal in a concise manner.\n2. Determine whether the subgoal has been met.\n3. Do not output anything except whether summary and subgoal are met. Your output should be only one line. Do not output things like '%2%Summary', '%3%3Summary and Analysis'.\n{example}\n##Trajectory\n{formatted_trajectory}\n##Subgoal:\n{subgoal}\n###Output:"}, {"title": "3.4 Trajectory Retrieval", "content": "Despite the summarization, there may be instances where detailed past trajectory information becomes crucial for immediate decision-making. For instance, when a past subgoal execution fails, we need detailed trajectory information to determine the cause of failure. Moreover, reviewing past successful experiences can also increase the likelihood of success when facing novel challenges and scenarios. To address this, we introduce a trajectory retrieval module. To address this, we introduce a trajectory retrieval module. When the LLM determines that detailed information from a past subgoal is necessary, it generates a retrieval function to recall the complete action-observation pairs for that subgoal, analogous to the way to generate actions. This selective retrieval allows the agent to access detailed historical data on-demand without"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "Evaluation Tasks We conduct the experiments on five long-horizon agent tasks, which typically require more than 20 steps: (i) Blocksworld requires the model to arrange the blocks into a specified target configuration by executing a series of moves; (ii) Gripper involves moving objects between different rooms; (iii) Tyreworld simulates changing a car tire, including removing the flat tire, replacing it with a spare, and installing the new tire; (iv) Barman emulates a bartender's tasks in mixing cocktails, including combining various ingredients, shakers, and garnishing drinks; (v) Jericho (Hausknecht et al. 2020) is a suite of text-based adventure game environments designed to evaluate agents' ability to navigate and interact with fictional worlds. More details can be found in Appendix A.\nEvaluation Metrics We use multiple metrics to evaluate both the effectiveness and efficiency of LLM-based agents in solving long-horizon tasks: (i) Progress Rate (Ma et al. 2024) evaluates the advancement toward task completion. Specifically, a task consists of multiple goal conditions, and the progress rate is the proportion of goal conditions fulfilled by the model out of the total number of goal conditions. (ii) Success Rate measures the percentage of successful task completions. The success rate is 1 when the progress rate is 1. (iii) Average Steps counts the steps taken to complete the task; (iv) Context Efficiency is defined as the mean number of tokens in the in-trial context across all steps required to complete a given task. (v) Run Time evaluates the time required to complete tasks.\nBaselines STANDARD prompting strategy is a predominantly used method in current LLM-based agent literature (Yao et al. 2022b; Ma et al. 2024; Liu et al. 2023c). It operates by taking one action followed by one observation, providing a comparative baseline for evaluating the performance of HIAGENT.\nImplementation Details The implementation of evaluation tasks is based on AgentBoard (Ma et al. 2024). We set a maximum step limit of 30 for task configuration and provide one in-context example for each task. We employ GPT-4 (gpt-4-turbo)\u00b9 as the LLM backbone for our experiments, serving both as the agent policy and the observation summarization model. We set the temperature hyperparameter for LLM inference to 0 and topp to 1. Detailed prompt examples are provided in the Appendix B."}, {"title": "4.2 Main Results", "content": "As shown in Table 1, HIAGENT demonstrated substantial advancements over STANDARD. Overall, in terms of effectiveness, it increased the success rate by 21% and the progress rate by 23.94%. Regarding task execution efficiency, it reduced the average number of steps to completion by 3.8, decreased the number of context tokens consumed by 35%,"}, {"title": "5 Analysis", "content": "To gain deeper insights into our approach, we explored the following research questions:\n(1) Are all modules effective for HIAGENT?\n(2) Is HIAGENT consistently superior to the baseline at different steps?\n(3) Is improvement of HIAGENT solely derived from task decomposition?\n(4) How effective are the frameworks in generating executable actions?\n(5) Are the observed performance improvements in HIAGENT statistically significant compared to STANDARD?"}, {"title": "5.1 Answer 1: All Modules in HIAGENT are Effective for HIAGENT", "content": "In this section, we conducted albation study to explore whether Observation Summarization and Trajectory Retrieval are effective.\nObservation Summarization is effective. We heuristically use the observation corresponding to the last action as the summarized observation when removing the Observation Summarization module. As is shown in Table 2 (\"w/o OS\u201d), there is a significant decline in performance across all metrics. Specifically, the success rate and progress rate were significantly impacted, decreasing by 30% and 7.6%, respectively. It indicates that the observation summarization module can comprehensively aggregate the detailed information within a trajectory, thereby aiding the reasoning of an LLM-based agent.\nTrajectory Retrieval is also crucial for performance enhancement. We hide all the detailed trajectory information of previous subgoals at each time step to verify the effectiveness of Trajectory Retrieval. According to the results in Table 2 (\"w/o TR\u201d), the success rate decreased by 10%, and the average steps increased by 1.2. This is because, while trajectory retrieval lengthens the reasoning steps of the LLM, it allows the agent to flexibly retrieve past trajectories under certain subgoals, which is more beneficial for identifying errors in previous actions.\nThe combination of Observation Summarization and Trajectory Retrieval yields significant improvement. We conducted an experiment where both modules were removed"}, {"title": "5.2 Answer 2: HIAGENT is consistently superior to STANDARD at different steps", "content": "To conduct a more granular study of HIAGENT's performance, we present the progress rate at different step counts (in intervals of 5 steps) in Figure 3. The experimental results indicate that overall, HIAGENT consistently achieves a higher progress rate at each step than STANDARD (f). Additionally, it is noteworthy that HIAGENT benefits more from an increased number of steps, whereas STANDARD does not. For example, in the blocksworld task (a) and barman task (b), STANDARD shows no progress rate increase between steps 15-25, whereas HIAGENT exhibits continuous growth. This further demonstrates HIAGENT's advantage in handling long-horizon agent tasks."}, {"title": "5.3 Answer 3: The improvement in HIAGENT is not solely attributed to task decomposition", "content": "Using LLMs to generate subgoals has been employed in numerous studies and has demonstrated considerable performance advantages (Zhou et al. 2022; Yin et al. 2023). Therefore, a pertinent question arises: \u201cIs the performance improvement attributed to HIAGENT merely related to task decomposition, rather than efficient working memory management?\u201d To address this question, we implemented a new method that prompts the LLM to generate a subgoal before generating executable actions, followed by generating actions to achieve this subgoal. Unlike HIAGENT, this approach does not obscure the detailed trajectory information of previous subgoals. The experimental results, detailed in Table 3, indicate that although task decomposition can lead to a performance improvement (30% in success rate), the success rate is still 20% lower than HIAGENT. Additionally, solely using task decomposition introduces inefficiencies, increasing runtime by 5.7% and context length by 12.8%. In summary, HIAGENT is more efficient and effective than task decomposition alone."}, {"title": "5.4 Answer 4: HIAGENT is effective in generating executable actions even under long steps", "content": "LLM-based agents sometimes generate actions that cannot be executed, such as attempting to retrieve objects from a closed container. This is typically due to LLMs' poor reasoning abilities. To investigate this, we calculated the proportion of executable actions generated by the model at each timestep, referred to as executability. As shown in Figure 4, HIAGENT is more likely to generate executable actions than STANDARD, further demonstrating the effectiveness of HIAGENT. Additionally, we observed that STANDARD is more"}, {"title": "5.5 Answer 5: The observed performance improvements in HIAGENT are statistically significant compared to STANDARD", "content": "To validate the statistical significance of the improvements in both effectiveness and efficiency, we selected the Progress Rate and Average Steps metrics for analysis. We employed the Wilcoxon signed-rank test (Woolson 2005) for this purpose due to its suitability for comparing paired samples. This non-parametric test helps assess whether the observed differences are likely due to chance or represent a genuine effect. The results of our analysis are as follows: (i) For the Progress Rate, the test statistic is 144.0 with a p-value of 2.38 \u00d7 10-5, indicating a statistically significant difference between HIA-"}, {"title": "6 Related Work", "content": "Large Language Model based Agent. Large Language Models (LLMs) have revolutionized the field of language agents, endowing them with the prowess to tackle intricate challenges through a logical sequence of actions (Xie et al. 2023; Hong et al. 2023; Xi et al. 2023; Wang et al. 2024; Yao et al. 2022b; Zhou et al. 2023a). A series of works explored various applications of LLM-based agents, such as code generation (Wang et al. 2023b; Lin et al. 2018), web browsing (Yao et al. 2022a; Zhou et al. 2023b; Pan et al. 2024; Li and Waldo 2024), robotics (Chevalier-Boisvert et al. 2018; Shridhar et al. 2020; Mu et al. 2024a,b), tool use (Li et al. 2023b; Wu et al. 2024; Qin et al. 2023), reasoning (Yang, Zhao, and Xie 2024), planning (Xie et al. 2024), conducting research (Kang and Xiong 2024), chip design and more."}, {"title": "7 Conclusion", "content": "This paper proposes HIAGENT, a hierarchical framework that utilizes subgoals to manage the working memory of Large Language Model(LLM)-based agents. HIAGENT aims to address the poor performance of LLM-based agents when handling long-horizon tasks. Experimental results from five long-horizon agent tasks demonstrate that HIAGENT outperforms the baseline model across all tasks, with an overall success rate more than double that of the baseline model. Furthermore, HIAGENT is more efficient, accomplishing tasks with fewer steps, in less runtime, and using shorter context. We also conducted an ablation study to verify the effectiveness of the individual modules of HIAGENT. A series of analysis experiments demonstrate that as the number of steps increases, HIAGENT more effectively generates executable actions and consistently outperforms STANDARD in terms of progress rate. Additionally, we conducted a statistical test to validate the statistical significance of the improvements introduced by HIAGENT. We believe HIAGENT is an effective and flexible framework that can be integrated into other agent frameworks. In the future, we hope HIAGENT can inspire more creative ideas on effectively managing the working memory of LLM-based agents."}, {"title": "Reproducibility Checklist", "content": "1. This paper:\n(a) Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes)\n(b) Clearly delineates statements that are opinions, hypotheses, and speculations from objective facts and results (yes)\n(c) Provides well-marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes)\n2. Does this paper make theoretical contributions? (no)\n3. Does this paper rely on one or more datasets? (yes)\n(a) A motivation is given for why the experiments are conducted on the selected datasets (yes)\n(b) All novel datasets introduced in this paper are included in a data appendix. (NA)\n(c) All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes.\n(NA)\n(d) All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes)\n(e) All datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (yes)\n(f) All datasets that are not publicly available are described in detail, with an explanation of why publicly available alternatives are not scientifically satisfying. (NA)\n4. Does this paper include computational experiments? (yes)\n(a) Any code required for pre-processing data is included in the appendix. (yes)\n(b) All source code required for conducting and analyzing the experiments is included in a code appendix. (yes)\n(c) All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\n(d) All source code implementing new methods has comments detailing the implementation, with references to the paper where each step comes from. (yes)\n(e) If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)\n(f) This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (yes)\n(g) This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (yes)\n(h) This paper states the number of algorithm runs used to compute each reported result. (yes)\n(i) Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes)\n(j) The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes)\n(k) This paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. (yes)\n(l) This paper states the number and range of values tried per (hyper-)parameter during the development of the paper, along with the criterion used for selecting the final parameter setting. (yes)"}, {"title": "A More Details on Evaluation Tasks", "content": null}, {"title": "A.1 Blocksworld", "content": "Action List\n1. pickup <block>: allows the arm to pick up a block from the table if it is clear and the arm is empty. After the pickup action, the arm will be holding the block, and the block will no longer be on the table or clear.\n2. putdown <block>: allows the arm to put down a block on the table if it is holding a block. After the putdown action, the arm will be empty, and the block will be on the table and clear.\n3. stack <block> <block>: allows the arm to stack a block on top of another block if the arm is holding the top block and the bottom block is clear. After the stack action, the arm will be empty, the top block will be on top of the bottom block, and the bottom block will no longer be clear.\n4. unstack <block> <block>: allows the arm to unstack a block from on top of another block if the arm is empty and the top block is clear. After the unstack action, the arm will be holding the top block, the top block will no longer be on top of the bottom block, and the bottom block will be clear.\nGoal example\nb1 is on b2., b2 is on b3.\nObservation example\nb1 is on the table. b2 is on the table. B3 is on the table. Robot arm is empty. The b1 is clear. The b2 is clear. The b3 is clear.\nAction example\npickup b2."}, {"title": "A.2 Gripper", "content": "Action List\n1. move <room1> <room2>: This action allows the robot to move from one room to another.The action has a single precondition, which is that the robot is currently in a room. The effect of this action is to move the robot to another room and to remove the fact that it is in the original room.\n2. pick <obj> <room> <gripper>: This action allows the robot to pick up an object using the gripper. The action has three preconditions: (1) the object is located in a room (2) the robot is currently in the same room and (3) the gripper is free (i.e., not holding any object). The effect of this action is to update the state of the world to show that the robot is carrying the object using the gripper, the object is no longer in the room, and the gripper is no longer free.\n3. drop <obj> <room> <gripper>: This action allows the robot to drop an object that it is carrying. The action has two preconditions: (1) the robot is currently carrying the object using the gripper, and (2) the robot is currently in a room. The effect of this action is to update the state of the world to show that the robot is no longer carrying the object using the gripper, the object is now located in the room, and the gripper is now free.\nGoal example\nball1 is at roomb., ball2 is at roomb., ball3 is at roomb.,"}, {"title": "A.3 Tyreworld", "content": "Action List\n1. open <container>: The precondition for this action is that the container is unlocked and closed. The effect of this action is that the container is open and not closed.\n2. close <container>: The precondition for this action is that the container is open. The effect of this action is that the container is closed and not open.\n3. fetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open. The effect of this action is that the object is held by the agent and not inside the container.\n4. put-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open. The effect of this action is that the object is inside the container and not held by the agent.\n5. loosen <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is tight, and the hub is on the ground. The effect of this action is that the nut on hub is loose and not tight.\n6. tighten <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is loose, and the hub is on the ground. The effect of this action is that the nut on hub is tight and not loose.\n7. jack-up <hub>: This action represents the process of lifting a hub off the ground using a jack. It requires the agent to have a jack and for the hub to be on the ground. After performing this action, the hub will no longer be on the ground and the agent will no longer have the jack.\n8. jack-down <hub>: This action represents the process of lowering a hub back to the ground from an elevated position using a jack. It requires the agent to have the hub off the ground. After performing this action, the hub will be back on the ground and the agent will have the jack.\n9. undo <nut> <hub>: This action undo the fastening of a nut on a hub. The preconditions are the hub is not on the ground (i.e., it has been jacked up), the hub is fastened, the agent has a wrench and the nut is loose. The effects are the agent has the nut, the hub is unfastened, the hub is no longer loose and the hub is not fastened anymore.\n10. do-up <nut> <hub>: This action fasten a nut on a hub. The preconditions are the agent has a wrench, the hub is unfastened, the hub is not on the ground (i.e., it has been jacked up) and the agent has the nut to be fastened. The effects are the nut is now loose on the hub, the hub is fastened, the hub is no longer unfastened and the agent no longer has the nut."}, {"title": "A.4 Barman", "content": "Action List\n1. <hand> grasp <container>: Grasp a container\n2. <hand> leave <container>: Leave a container on the table\n3. fill-shot <shot> <ingredient> <hand1> <hand2> <dispenser>: Fill a shot glass with an ingredient from dispenser\n4. refill-shot <shot> <ingredient> <hand1> <hand2> <dispenser>: Refill a shot glass with an ingredient from dispenser\n5. empty-shot <hand> <shot> <beverage>: Empty a shot glass 6. clean-shot <shot> <beverage> <hand1> <hand2>: Clean a shot glass\n7. pour-shot-to-clean-shaker <shot> <ingredient> <shaker> <hand1> <level1> <level2>: Pour an ingredient from a shot glass to a clean shaker from levell to level2\n8. pour-shot-to-used-shaker <shot> <ingredient> <shaker> <hand1> <level1> <level2>: Pour an ingredient from a shot glass to a used shaker from levell to level2\n9. empty-shaker <hand> <shaker> <cocktail> <level1> <level2>: Empty a shaker containing cocktail from levell to level2\n10. clean-shaker <hand1> <hand2> <shaker>: Clean a shaker\n11. shake <cocktail> <ingredient1> <ingredient2> <shaker> <hand1> <hand2>: Shake a cocktail in a shaker\n12. pour-shaker-to-shot <beverage> <shot> <hand> <shaker> <level1> <level2>: Pour a beverage from a shaker to a shot glass from levell to level2\nGoal example\nshot1 contains cocktail1.\nObservation example\nCocktaill part1 ingredient is ingredient1. Cocktail1 part2 ingredient is ingredient3. Cocktail2 part1 ingredient is ingredient2. Cocktail2 part2 ingredient is ingredient3. Cocktail3 part1 ingredient is ingredient1. Cocktail3 part2 ingredient is ingredient2. Dispenser1 dispenses ingredient1. Dispenser2 dispenses ingredient2. Dispenser3 dispenses ingredient3. Left hand is empty. Level 10 is next to level 11. Level 11 is next to level 12. Right hand is empty. Shaker1 is at empty level 10. Shaker1 is at level 10. Shaker1 is clean. Shaker1 is empty. Shaker1 is on the table. Shot1 is clean. Shot1 is empty. Shot1 is on the table. Shot2 is clean. Shot2 is empty. Shot2 is on the table. Shot3 is clean. Shot3 is empty. Shot3 is on the table. Shot4 is clean. Shot4 is empty. Shot4 is on the table.\nAction example\nright grasp shot1."}, {"title": "A.5 Jericho", "content": "Action List\n1. Inventory: check things you are carrying\n2. Look: check your surroundings\n3. Examine <place/obj>: check the details of something\n4. Take <obj>: pickup obj\n5. Put down <obj>: leave a obj at your current place.\n6. Drop <obj>\n7. Check valid actions: Check actions you can use\n8. South: go south\n9. North: go north\n10. East: go east\n11. West: go west\n12. Up: go up\n13. Down: go down\n14. Check valid actions (Other available actions)\nGoal example\nYou are the warrior Link that needs to save the princess from the castle.\nObservation example\nYou are at the path leading to the castle. The castle is to your north. There is a barrel in front of you.\nAction example\nExamine barrel"}, {"title": "B Prompt Examples", "content": null}, {"title": "B.1 STANDARD", "content": "Environment Implementation\nYour goal is to replace flat tyres with intact tyres on the hubs. Remember to open boot first to get tools you need. Intact tyres should be inflated. The nuts should be tight on the hubs. The flat tyres", "domain": "nopen <container>: The precondition for this action is that the container is unlocked and closed. The effect of this action is that the container is open and not closed.\nclose <container>: The precondition for this action is that the container is open. The effect of this action is that the container is closed and not open.\nfetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open. The effect of this action is that the object is held by the agent and not inside the container.\nput-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open. The effect of this action is that the object is inside the container and not held by the agent.\nloosen <nut> <hub>: The precondition for this action is that the agent has a wrench", "hub>": "The precondition for this action is that the agent has a wrench"}, {"hub>": "This action represents the process of lifting a hub off the ground using a jack. It requires the agent to have a jack and for the hub to be on the ground. After performing this action"}, {"hub>": "This action represents the process of lowering a hub back to the ground from an elevated position using a jack. It requires the agent to have the hub off the ground. After performing this action"}, {"hub>": "This action undo the fastening of a nut on a hub. The preconditions are the hub is not on the ground (i.e"}]}