{"title": "HIAGENT: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model", "authors": ["Mengkang Hu", "Tianxing Chen", "Qiguang Chen", "Yao Mu", "Wenqi Shao", "Ping Luo"], "abstract": "Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks. The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of action-observation pairs. We categorize memory into two types: cross-trial memory, accumulated across multiple attempts, and in-trial memory (working memory), accumulated within a single attempt. While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored. Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks. Inspired by human problem-solving strategies, this paper introduces HIAGENT, a framework that leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. Specifically, HIAGENT prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal. Experimental results across five long-horizon tasks demonstrate that HIAGENT achieves a twofold increase in success rate and reduces the average number of steps required by 3.8. Additionally, our analysis shows that HIAGENT consistently improves performance across various steps, highlighting its robustness and generalizability.", "sections": [{"title": "1 Introduction", "content": "Owing to the development of powerful reasoning capabilities of Large Language Models (LLMs) in recent years (OpenAI 2022, 2023; Meta AI 2024; Touvron et al. 2023; Jiang et al. 2023), LLM-based agents have demonstrated significant potential in various applications (Xie et al. 2023; Wang et al. 2024; Xi et al. 2023), such as software development (Hong et al. 2023; Bairi et al. 2024), robotic planning (Yao et al. 2022b; Puig et al. 2018; Singh et al. 2023; Huang et al. 2022a), simulating human behavior (Park et al. 2023), etc. Typically, an LLM-based agent refers to an interactive system that processes environmental observations, maintains context across multiple rounds of dialogue, and outputs executable actions tailored to completing a given task. Memory is one of the critical components of LLM-based agents, involving how agents store and utilize past experiences. When handling a specific task, an agent's memory can be divided into cross-trial and in-trial memory (also as known as working memory). Cross-trial memory typically consists of the historical trajectory information accumulated across multiple attempts at the current task. In contrast, in-trial memory pertains to the information relevant to the current trial. While many papers have explored leveraging cross-trial memory to optimize agent performance (Shinn et al. 2024; Zhao et al. 2024; Guo et al. 2023), few have investigated ways to better utilize working memory. Existing LLM-based agent literature primarily employs the STANDARD strategy illustrated in Figure 1, where all action-observation pairs in working memory are directly incorporated into the context when prompting LLMs (Liu et al. 2023c; Ma et al. 2024; Yao et al. 2022b). Although this approach transmits the historical information to the LLM as comprehensively as possible, it encounters issues in long-horizon agent tasks. Such tasks typically require the agent to perform numerous actions to complete the task, resulting in an extensive working memory. This lengthy working memory creates a redundant context, hindering LLMs from maintaining coherent strategies and making accurate predictions over extended periods.\nDrawing on principles of cognitive science (Newell, Simon et al. 1972; Anderson 2013), humans typically decompose a complex problem into multiple subproblems, addressing each individually. Each subproblem is treated as a memory \"chunk,\" thereby reducing the cognitive load on working memory (Miller 1956). By focusing on the results of completed subproblems rather than their detailed execution, humans effectively manage cognitive resources and improve their efficiency in solving complex, long-horizon tasks. Inspired by human cognition and problem-solving strategies, we propose a sophisticated hierarchical working memory management framework HIAGENT tailored for long-horizon agent tasks. The core idea of HIAGENT is to trigger LLMs to generate subgoals, with each subgoal serving as a chunk of the working memory. Specifically, as shown in Figure 2, we first prompt the LLM to generate a subgoal, then create actions to achieve the subgoal and store the corresponding action-observation pairs in a memory chunk. Once the subgoal is completed, we summarize the memory chunk and append the subgoal-observation pair to the working memory. In a word, HIAGENT triggers LLMs to proactively decide to replace previous subgoals with summarized observations while retaining only the action-observation pairs relevant to the current subgoal. To provide more flexible working memory management, we also introduce a trajectory retrieval module, which can retrieve the detailed trajectory information of specific past subgoals when necessary.\nTo validate the effectiveness and efficiency of HIAGENT, we conducted experiments on five long-horizon agent tasks from AgentBoard (Ma et al. 2024). The experimental results show that the success rate of HIAGENT is twice that of the STANDARD strategy, and it exceeds the STANDARD strategy by 23.94% in progress rate. Additionally, HIAGENT is more efficient than STANDARD strategy, reducing the average number of steps to complete tasks by 3.8, the context length by 35.02%, and the run time by 19.42%. Furthermore, to demonstrate that redundant context impairs the performance of LLM-based agents in long-horizon tasks, we compared HIAGENT to a method that generates subgoals without disregarding the detailed trajectory information of past subgoals. Experimental results show that HIAGENT improved the success rate by 20% while reducing both runtime and the number of steps. By analyzing model performance across varying step counts, we found that HIAGENT not only consistently outperformed STANDARD on progress rate but also showed a higher likelihood of generating executable actions as the number of steps increased."}, {"title": "2 Preliminary", "content": "2.1 Large Language Model based Agent\nLarge Language Model (LLM) based agents are intelligent autonomous systems designed to perform complex tasks. These tasks can be formalized as a partially observable Markov decision process (POMDP), characterized by the tuple (S, O, A, T, R), where: S denotes the state space; O represents the observation space; A signifies the action space; \\(T : S \\times A \\rightarrow S\\) embodies the transition function; \\(R : S \\times A \\rightarrow R\\) encapsulates the reward function; An LLM-based agent operates as a policy \\(\\pi(a_t|I, O_t, a_{t-1}, O_{t-1},..., A_0, O_0)\\), which, given the historical action-observation pairs and instructions I (encompassing in-context examples, environmental descriptions, etc.), generates an executable action \\(a_t \\in A\\). Each action precipitates a new state \\(s_{t+1} \\in S\\) and a subsequent observation \\(O_{t+1} \\in O\\). This iterative interaction persists until either task completion or the agent reaches a predetermined maximum number of steps.\n2.2 Working Memory\nFrom the cognitive science perspective, working memory enables individuals to hold and manipulate information in real-time, facilitating complex cognitive tasks such as reasoning, comprehension, and learning (Newell, Simon et al. 1972; Anderson 2013). In LLM-based agents, we define working memory as the essential historical information required by the LLM at a given moment t to complete the current task. Effective working memory management allows for better integrating past experiences and current stimuli, leading to more informed and accurate decisions. It can be likened to the human process of attentional control and cognitive updating, which involves selectively focusing on relevant information, filtering out distractions, and continually updating the mental workspace with new and pertinent data. The STANDARD approach in Figure 1 stores all historical action-observation pairs in working memory, i.e., \\(m_{t}^{std} = (O_t, a_{t-1}, O_{t-1}, ..., a_0, O_0)\\). Although this provides the LLM with comprehensive information, it also introduces redundancy, complicating the LLM's processing."}, {"title": "3 Methodology", "content": "3.1 Overview\nThe core idea of HIAGENT is to employ subgoals for hierarchical management of working memory. More specifically, as is shown in Figure 2, the process of HIAGENT can be described as follows: (1) Before generating specific grounded actions, we prompt the LLM to first formulate a subgoal \\(g_i\\). Each subgoal serves as a milestone within the overall task. (2) Subsequently, the LLM generates precise actions to accomplish this subgoal. (3) Upon the LLM's determination that a particular subgoal has been fulfilled, we synthesize the corresponding action-observation pairs into a summarized observation \\(s_i\\) (\u00a73.3). We then obscure the action-observation pairs within the context, substituting them with \\(s_i\\). Consequently, the working memory of HIAGENT can be formalized as \\(m_{t}^{hie} = (g_0, s_0,...., g_{n-1}, s_{n-1}, g_n, a_{n0}, O_{n1},...)\\). (4) Additionally, we have incorporated a retrieval module to facilitate more flexible memory management(\u00a73.4). For instance, if the \\(q_{th}\\) subgoal is retrieved, we input the detailed action-observation pairs into the context rather than the summarized observation, i.e., \\(m_{t}^{hie} = (g_0, s_0, .., g_q, a_{q0}, a_{q0}, ..., g_n, a_{n0}, O_{n0}, ...)\\).\n3.2 Subgoal-based Hierarchical Working Memory\nAs is shown in Figure 2, at each time step, the LLM can either generate the next action for the current subgoal or generate a new subgoal when it determines that the existing subgoal has been accomplished. For the current subgoal, the agent retains all action-observation pairs, providing a detailed context for immediate decision-making. For past subgoals, only a summarized version of the observations is kept. This subgoal-based hierarchical management approach in HIAGENT is deeply motivated by cognitive science principles, drawing parallels with human cognition and problem-solving strategies (Newell, Simon et al. 1972; Anderson 2013). Employing subgoals to compartmentalize action-observation pairs can be conceptualized as a form of chunking methodology. In human cognition, chunking allows individuals to group related information into meaningful units, thereby overcoming working memory limitations (Miller 1956). Similarly, HIAGENT utilizes subgoals as cognitive chunks, encapsulating related actions and observations. This chunking mechanism enables the system to handle complex sequences of information more effectively, reducing cognitive load and enhancing overall performance. Furthermore, by generating subgoals before specific actions, the system mimics the human tendency to break down larger objectives into more manageable components. This methodology enhances computational efficiency and aligns with established theories of human information processing.\n3.3 Observation Summarization\nThe process of observation summarization can be formalized as \\(s_i = S(g_i, O_0, a_0, ..., O_t)\\), where S can be implemented using either a Large Language Model (LLM) or alternative text summarization models. This function encapsulates the synthesis of historical observations and actions, contextualized by the current subgoal, to produce a concise representation of the agent's state. Furthermore, a crucial component of the summarized observation is assessing whether the current subgoal has been achieved. This evaluation serves as a pivotal guide for future subgoal generation, facilitating adaptive and goal-oriented behavior in the agent's decision-making process. By doing so, the agent can maintain a condensed yet informative context, balancing the need for historical information with efficiency.\n3.4 Trajectory Retrieval\nDespite the summarization, there may be instances where detailed past trajectory information becomes crucial for immediate decision-making. For instance, when a past subgoal execution fails, we need detailed trajectory information to determine the cause of failure. Moreover, reviewing past successful experiences can also increase the likelihood of success when facing novel challenges and scenarios. To address this, we introduce a trajectory retrieval module. To address this, we introduce a trajectory retrieval module. When the LLM determines that detailed information from a past subgoal is necessary, it generates a retrieval function to recall the complete action-observation pairs for that subgoal, analogous to the way to generate actions. This selective retrieval allows the agent to access detailed historical data on-demand without consistently carrying the full context."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nEvaluation Tasks We conduct the experiments on five long-horizon agent tasks, which typically require more than 20 steps: (i) Blocksworld requires the model to arrange the blocks into a specified target configuration by executing a series of moves; (ii) Gripper involves moving objects between different rooms; (iii) Tyreworld simulates changing a car tire, including removing the flat tire, replacing it with a spare, and installing the new tire; (iv) Barman emulates a bartender's tasks in mixing cocktails, including combining various ingredients, shakers, and garnishing drinks; (v) Jericho (Hausknecht et al. 2020) is a suite of text-based adventure game environments designed to evaluate agents' ability to navigate and interact with fictional worlds.\nEvaluation Metrics We use multiple metrics to evaluate both the effectiveness and efficiency of LLM-based agents in solving long-horizon tasks: (i) Progress Rate (Ma et al. 2024) evaluates the advancement toward task completion. Specifically, a task consists of multiple goal conditions, and the progress rate is the proportion of goal conditions fulfilled by the model out of the total number of goal conditions. (ii) Success Rate measures the percentage of successful task completions. The success rate is 1 when the progress rate is 1. (iii) Average Steps counts the steps taken to complete the task; (iv) Context Efficiency is defined as the mean number of tokens in the in-trial context across all steps required to complete a given task. (v) Run Time evaluates the time required to complete tasks.\nBaselines STANDARD prompting strategy is a predominantly used method in current LLM-based agent literature (Yao et al. 2022b; Ma et al. 2024; Liu et al. 2023c). It operates by taking one action followed by one observation, providing a comparative baseline for evaluating the performance of HIAGENT.\nImplementation Details The implementation of evaluation tasks is based on AgentBoard (Ma et al. 2024). We set a maximum step limit of 30 for task configuration and provide one in-context example for each task. We employ GPT-4 (gpt-4-turbo)\u00b9 as the LLM backbone for our experiments, serving both as the agent policy and the observation summarization model. We set the temperature hyperparameter for LLM inference to 0 and topp to 1.\n4.2 Main Results\nAs shown in Table 1, HIAGENT demonstrated substantial advancements over STANDARD. Overall, in terms of effectiveness, it increased the success rate by 21% and the progress rate by 23.94%. Regarding task execution efficiency, it reduced the average number of steps to completion by 3.8, decreased the number of context tokens consumed by 35%, and reduced the run time by 19.42%. Furthermore, in certain tasks (blocksworld, barman, jericho), HIAGENT even achieved more than double the progress rate improvement while maintaining efficiency. In tyreworld, the model not only achieved a 50% improvement in success rate but also reduced the average number of steps by 9.4. Although the progress rate slightly decreased by 1.5% in the gripper task, context token usage was reduced by over 50%.\nWe can draw several conclusions from previous discussions: (1) HIAGENT is more effective than STANDARD, achieving huge improvements on both success rate and progress rate. (2) HIAGENT is also more efficient than STANDARD, requiring fewer steps to complete tasks, utilizing shorter context lengths, and achieving faster runtime."}, {"title": "5 Analysis", "content": "To gain deeper insights into our approach, we explored the following research questions:\n(1) Are all modules effective for HIAGENT?\n(2) Is HIAGENT consistently superior to the baseline at different steps?\n(3) Is improvement of HIAGENT solely derived from task decomposition?\n(4) How effective are the frameworks in generating executable actions?\n(5) Are the observed performance improvements in HIAGENT statistically significant compared to STANDARD?\n5.1 Answer 1: All Modules in HIAGENT are Effective for HIAGENT\nIn this section, we conducted albation study to explore whether Observation Summarization and Trajectory Retrieval are effective.\nObservation Summarization is effective. We heuristically use the observation corresponding to the last action as the summarized observation when removing the Observation Summarization module. As is shown in Table 2 (\"w/o OS\u201d), there is a significant decline in performance across all metrics. Specifically, the success rate and progress rate were significantly impacted, decreasing by 30% and 7.6%, respectively. It indicates that the observation summarization module can comprehensively aggregate the detailed information within a trajectory, thereby aiding the reasoning of an LLM-based agent.\nTrajectory Retrieval is also crucial for performance enhancement. We hide all the detailed trajectory information of previous subgoals at each time step to verify the effectiveness of Trajectory Retrieval. According to the results in Table 2 (\"w/o TR\u201d), the success rate decreased by 10%, and the average steps increased by 1.2. This is because, while trajectory retrieval lengthens the reasoning steps of the LLM, it allows the agent to flexibly retrieve past trajectories under certain subgoals, which is more beneficial for identifying errors in previous actions.\nThe combination of Observation Summarization and Trajectory Retrieval yields significant improvement. We conducted an experiment where both modules were removed to validate the functionality and effectiveness of the combined Observation Summarization and Trajectory Retrieval modules. As shown in Table 2 (\"w/o OS & TR\u201d), there is a noticeable performance decline compared to HIAGENT, with the success rate decreasing by 20%. This decline is also evident when compared to the individual ablations of the Observation Summarization and Trajectory Retrieval modules, highlighting a substantial reduction in progress rate in their absence.\n5.2 Answer 2: HIAGENT is consistently superior to STANDARD at different steps\nTo conduct a more granular study of HIAGENT's performance, we present the progress rate at different step counts (in intervals of 5 steps) in Figure 3. The experimental results indicate that overall, HIAGENT consistently achieves a higher progress rate at each step than STANDARD (f). Additionally, it is noteworthy that HIAGENT benefits more from an increased number of steps, whereas STANDARD does not. For example, in the blocksworld task (a) and barman task (b), STANDARD shows no progress rate increase between steps 15-25, whereas HIAGENT exhibits continuous growth. This further demonstrates HIAGENT's advantage in handling long-horizon agent tasks.\n5.3 Answer 3: The improvement in HIAGENT is not solely attributed to task decomposition\nUsing LLMs to generate subgoals has been employed in numerous studies and has demonstrated considerable performance advantages (Zhou et al. 2022; Yin et al. 2023). Therefore, a pertinent question arises: \u201cIs the performance improvement attributed to HIAGENT merely related to task decomposition, rather than efficient working memory management?\u201d To address this question, we implemented a new method that prompts the LLM to generate a subgoal before generating executable actions, followed by generating actions to achieve this subgoal. Unlike HIAGENT, this approach does not obscure the detailed trajectory information of previous subgoals. The experimental results, detailed in Table 3, indicate that although task decomposition can lead to a performance improvement (30% in success rate), the success rate is still 20% lower than HIAGENT. Additionally, solely using task decomposition introduces inefficiencies, increasing runtime by 5.7% and context length by 12.8%. In summary, HIAGENT is more efficient and effective than task decomposition alone.\n5.4 Answer 4: HIAGENT is effective in generating executable actions even under long steps\nLLM-based agents sometimes generate actions that cannot be executed, such as attempting to retrieve objects from a closed container. This is typically due to LLMs' poor reasoning abilities. To investigate this, we calculated the proportion of executable actions generated by the model at each timestep, referred to as executability. As shown in Figure 4, HIAGENT is more likely to generate executable actions than STANDARD, further demonstrating the effectiveness of HIAGENT. Additionally, we observed that STANDARD is more prone to generating non-executable actions when the steps are longer (e.g., in the blocksworld, when the steps exceed 20, executability drops below 10%). This is because, as the working memory increases, the ability of LLMs to generate executable actions decreases. In contrast, HIAGENT maintains over 80% executability even with longer steps, indicating that the robustness to long steps is a key factor in the strong performance on long-horizon tasks.\n5.5 Answer 5: The observed performance improvements in HIAGENT are statistically significant compared to STANDARD\nTo validate the statistical significance of the improvements in both effectiveness and efficiency, we selected the Progress Rate and Average Steps metrics for analysis. We employed the Wilcoxon signed-rank test (Woolson 2005) for this purpose due to its suitability for comparing paired samples. This non-parametric test helps assess whether the observed differences are likely due to chance or represent a genuine effect. The results of our analysis are as follows: (i) For the Progress Rate, the test statistic is 144.0 with a p-value of 2.38 \u00d7 10-5, indicating a statistically significant difference between HIAGENT and STANDARD; (ii) For the Average Steps, the test statistic is 112.5 with a p-value of 0.0016, also demonstrating a statistically significant difference. These results confirm that the observed improvements in both effectiveness and efficiency are not due to random variation, underscoring the superiority of HIAGENT."}, {"title": "6 Related Work", "content": "Large Language Model based Agent. Large Language Models (LLMs) have revolutionized the field of language agents, endowing them with the prowess to tackle intricate challenges through a logical sequence of actions (Xie et al. 2023; Hong et al. 2023; Xi et al. 2023; Wang et al. 2024; Yao et al. 2022b; Zhou et al. 2023a). A series of works explored various applications of LLM-based agents, such as code generation (Wang et al. 2023b; Lin et al. 2018), web browsing (Yao et al. 2022a; Zhou et al. 2023b; Pan et al. 2024; Li and Waldo 2024), robotics (Chevalier-Boisvert et al. 2018; Shridhar et al. 2020; Mu et al. 2024a,b), tool use (Li et al. 2023b; Wu et al. 2024; Qin et al. 2023), reasoning (Yang, Zhao, and Xie 2024), planning (Xie et al. 2024), conducting research (Kang and Xiong 2024), chip design and more. Additionally, lots of works explored the application of LLM-based agents in the field of multi-agent systems (Hong et al. 2023; Zhang et al. 2023a; Wu et al. 2023; Li et al. 2023a; Chen et al. 2023). This paper introduces a working memory management framework HIAGENT that can be universally applied to enhance the performance of other agent frameworks. For example, ReAct (Yao et al. 2022b) introduces a method where the LLM generates a chain of thought (Wei et al. 2022) before generating actions, and the trajectory formed by the triplet of \"(thought, action, observation)\" can be managed using HIAGENT. Additionally, HIAGENT has the potential to alleviate information management challenges in multi-agent frameworks (Hong et al. 2023).\nPlanning. Planning is a cornerstone of human intelligence, representing a systematic approach to achieving goals through a series of deliberate actions (Yao et al. 2024; Zhang et al. 2023b; Xu et al. 2023; Song et al. 2023; Wang et al. 2023d; Huang et al. 2023, 2022b; Liu et al. 2023a; Guan et al. 2023; Zhao, Lee, and Hsu 2024; Ruan et al. 2023; Aghzal, Plaku, and Yao 2023). It involves breaking down complex tasks into manageable sub-tasks, searching for potential solutions, and achieving a desired goal. This cognitive ability is fundamental to human-level intelligence and has been a focal point of research in various domains, including robotics (Hu et al. 2023b; Huang et al. 2022a; Singh et al. 2023; Brohan et al. 2023; Valmeekam et al. 2024; Puig et al. 2018), travel planning (Xie et al. 2024), warehouse-level coding (Bairi et al. 2024), tool use (Liu et al. 2024b) and so on. Least-to-most (Zhou et al. 2022) and Plan-and-solve (Wang et al. 2023a) propose decomposing a complex question into a series of sub-questions. However, when answering each sub-question, it inputs all previous answers into the LLM, leading to context inefficiency. Lumos (Yin et al. 2023) and XAgent (Team 2023) introduce an independent planning module for generating subgoals and use full context in the grounding module to complete each subgoal. HIAGENT distinguishes itself from the literature by not only utilizing planning to enhance task performance but also by using subgoals as memory chunks to manage working memory hierarchically. This approach brings context efficiency and surpasses methods that rely solely on planning, as discussed in Section 5.3.\nMemory. The memory module in LLM-based agents is analogous to the human memory system, which is responsible for encoding, storing, and retrieving information (Zhang et al. 2024). The memory modules are typically divided into long-term memory and short-term memory. Long-term memory can usually be stored in an external database, while short-term memory (also known as working memory) is typically used directly as the context input of LLMs. Most current research papers primarily focus on managing long-term memory (Alonso et al. 2024; Maharana et al. 2024; Chen et al. 2024; Xiao et al. 2024; Yuan et al. 2023; Wang et al. 2023c; Majumder et al. 2023; Hu et al. 2023a; Hao et al. 2024; Lanchantin et al. 2024; Tu et al. 2023; Liang et al. 2023; Kagaya et al. 2024). Pioneer works include Memorybank (Zhong et al. 2024), with its global-level summaries, has made significant strides in distilling conversations into coherent narratives. Other works, such as Think-in-memory (Liu et al. 2023b) and the Retroformer (Yao et al. 2023), incorporated summary modules to manage long-term memories. Unlike these works, our study investigates how optimizing the management of working memory can enhance agent performance. Another line of research involves modifying the structure of transformers to enable large language models (LLMs) to process longer contexts, thereby extending their working memory capabilities (Zhou et al. 2023c; Chevalier et al. 2023; Bertsch et al. 2024; Ruoss et al. 2023; Beltagy, Peters, and Cohan 2020; An et al. 2023). However, existing research has identified that LLMs encounter attention loss issues with lengthy texts (Liu et al. 2024a). Consequently, we believe that investigating more efficient management of working memory remains a valuable endeavor."}, {"title": "7 Conclusion", "content": "This paper proposes HIAGENT, a hierarchical framework that utilizes subgoals to manage the working memory of Large Language Model(LLM)-based agents. HIAGENT aims to address the poor performance of LLM-based agents when handling long-horizon tasks. Experimental results from five long-horizon agent tasks demonstrate that HIAGENT outperforms the baseline model across all tasks, with an overall success rate more than double that of the baseline model. Furthermore, HIAGENT is more efficient, accomplishing tasks with fewer steps, in less runtime, and using shorter context. We also conducted an ablation study to verify the effectiveness of the individual modules of HIAGENT. A series of analysis experiments demonstrate that as the number of steps increases, HIAGENT more effectively generates executable actions and consistently outperforms STANDARD in terms of progress rate. Additionally, we conducted a statistical test to validate the statistical significance of the improvements introduced by HIAGENT. We believe HIAGENT is an effective and flexible framework that can be integrated into other agent frameworks. In the future, we hope HIAGENT can inspire more creative ideas on effectively managing the working memory of LLM-based agents."}]}