{"title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis", "authors": ["Feng Luo", "Rui Yang", "Hao Sun", "Chunyuan Deng", "Jiarui Yao", "Jingyan Shen", "Huan Zhang", "Hanjie Chen"], "abstract": "Understanding human preferences is crucial for improving foundation models and building personalized Al systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Team et al., 2023; Grattafiori et al., 2024; Liu et al., 2024a) has proven to be a powerful approach for fine-tuning large language models (LLMs) to serve as better general assistants (Team et al., 2023; Achiam et al., 2023), and AI agents (Nakano et al., 2021; Wu et al., 2023a; Zhao et al., 2024; Yang et al., 2025). Typically, LLMs are optimized using a scalar reward model trained on human preference data, acting as a proxy for overall user satisfaction. However, this approach has two key limitations: (1) it often reflects the preferences of the majority, potentially marginalizing underrepresented groups (Chakraborty et al., 2024; Chidambaram et al., 2024) and failing to capture the full diversity of human preferences, and (2) it struggles to represent the complex, multifaceted, and sometimes conflicting nature of human preferences with a single scalar reward (Jang et al., 2023; Rame et al., 2024; Yang et al., 2024c; Zhou et al., 2024). As demand grows for more personalized LLMs (Zhang et al., 2024b), researchers have explored ways to capture fine-grained, multidimensional human preferences. Some studies have introduced datasets that evaluate multiple aspects such as relevance, correctness, completeness, helpfulness, and harmlessness (Wu et al., 2023b; Wang et al., 2023; Cui et al., 2023; Pitis et al., 2024). Building on these datasets, others have proposed multi-objective optimization methods (Qiu et al., 2024) to accommodate diverse user needs (Yang et al., 2024c,a; Wang et al., 2024b). However, these approaches face significant challenges: collecting fine-grained human annotations is expensive, and using GPT-generated labels can introduce biases. Considering that large-scale binary preference datasets\u2014where users simply compare two responses are easier to collect and more widely available, we ask the question: Can we infer multidimensional human preferences directly from large-scale binary comparisons?\nTo address this question, we propose Decomposed Reward Models (DRMs), a framework that extracts fine-grained human preferences using binary comparison data. Unlike traditional probabilistic models such as Bradley-Terry (BT) (Bradley and Terry, 1952) or pairwise preference models (Jiang et al., 2023), our approach represents human preferences as d-dimensional vectors, corresponding to the learned weights of the final linear layer in a reward model. We show that this"}, {"title": "2 Preliminary", "content": "The Bradley-Terry (BT) model (Bradley and Terry, 1952) is a probabilistic framework commonly used in preference learning. Given a prompt x and two responses, y1 and y2, the BT model defines the probability of y\u2081 being preferred over y2 as:\nP(y1 \\succ y2|x) = \\frac{exp(r(x, y1))}{exp(r(x, y1)) + exp(r(x, y2))}\nwhere r(x, y) represents the reward model, which assigns a score to a given prompt-response pair. Given a dataset of comparisons D = {(xi, y_i^\\succ, y_i^\\prec)}^N_{i=1}, where y_i^\\succ is the preferred response and y_i^\\prec is the rejected one, the reward model is trained by maximizing the likelihood of human preferences. This results in the following objective:\n\\max_{\\theta} E_{(x,y^\\succ,y^\\prec) \\sim D} [log \\sigma (r_\\theta(x_i, y_i^\\succ) - r_\\theta(x_i, y_i^\\prec))]\n(1)\nwhere r\u03b8(x, y) is the reward score parameterized by \u03b8, and \u03c3(\u00b7) denotes the sigmoid function. By"}, {"title": "3 Methodology", "content": "The standard approach to preference learning relies on a scalar-valued reward model, which may not adequately capture the full complexity of human preferences. To address this limitation, we first introduce a vector representation of human preference and establish its connection to PCA. Building on this, we propose a PCA-based method that decomposes human preference into multiple basis vectors, allowing any preference to be represented as a linear combination of these vectors. This approach enables a novel paradigm for diverse reward modeling without additional training."}, {"title": "3.1 Vector Representation of Preferences", "content": "In conventional reward modeling approaches, human preferences are transferred into scalar scores using the BT models, as illustrated in Eq. (1). In practice, modern reward models are typically fine-tuned from pretrained or instruction-fine-tuned language models, with a reward head that maps hidden states to a scalar reward prediction.\nIn this paper, we consider a vector representation of human preference \u2013 denoting \u03c6(x, y) \u2208 Rd as a d-dimensional feature extractor (e.g., the penultimate layer outputs), and a final linear layer w in the reward model, where r\u03b8(x,y) = wT\u03c6(x, y). Under this formulation, the BT objective can be rewritten as:\n\\max_w E_i [log \\sigma (w^T\\phi(x_i, y_i^\\succ) \u2013 w^T\\phi(x_i, y_i^\\prec))]\n= \\max_w E_i [log \\sigma (w^T (\\phi(x_i, y_i^\\succ) \u2013 \\phi(x_i, y_i^\\prec)))]\nwhere (xi, y_i^\\succ), (xi, y_i^\\prec) denote the chosen and rejected samples, respectively.\nThis reformulation is particularly interesting because it allows human preference to be captured using a vector w \u2208 Rd instead of relying on a large set of model parameters. Consequently, human preference can be interpreted as a direction in the d-dimensional space. When the feature space difference \u03c6(xi, y_i^\\succ) \u2013 \u03c6(xi, y_i^\\prec) aligns with such a direction, it means the response pair is aligned with human preference (i.e., y^\\succ \\succ y^\\prec); otherwise, it indicates a contradiction. Since the distances between preference vectors provide a natural way of comparing preferences, such a vector-based repre-"}, {"title": "3.2 Rethinking Preference Learning with Principal Component Analysis (PCA)", "content": "The next step is selecting a good basis to represent any vector preferences. The basis should be universal, interpretable, and have a strong connection with human preferences. Our choice of the basis is motivated by the following observations.\nFormally, we start by defining zi = \u03c6(xi, y_i^\\succ) \u2013 \u03c6(xi, y_i^\\prec) and letting it be {zi}i=1N zero-centered, (i.e., Ei[zi] = 0, which can be ensured via normalization). Without loss of generality, we consider the unified preference vector ||w||2 = 1. Then the objective of reward modeling becomes\n\\max_w E_i [log \\sigma (w^Tz_i)], s.t. ||w||_2 = 1.\n(2)\nHere, w is optimized to find a direction that best distinguishes the preference data, which bears a resemblance to PCA: both methods seek a meaningful projection of the data onto a dimension. To further explore this connection, consider the following regularized objective (\u03bb > 0):\nJ(w) = E_i [log \\sigma (w^Tz_i)] - \\lambda ||w||_2^2\n(3)\nTaking the gradient, we obtain:\n\\nabla_w J(w) = E_i [(1 \u2013 \\sigma(w^Tz_i))z_i] - 2\\lambda w.\n(4)\nFor small wTzi, we have 1 \u2013 \u03c3(x) \u2248 1/2 + cx according to Taylor expansion, where c is a constant. Therefore, we have\n\\nabla_w J(w) \\approx cE_i [(z_iz_i^T)w] - 2\\lambda w\n= c\\Sigma w \u2013 2\\lambda w,\n(5)\nwhere \u2211 = Ei[zizTi] is the covariance matrix of {zi}. Setting the gradient to zero gives:\nc\\Sigma w = 2\\lambda w  \\Rightarrow \\Sigma w = \\frac{2\\lambda}{c} w.\n(6)\nDiscussion. This equation suggests that under certain conditions, the learned preference direction w aligns with an eigenvector of the covariance matrix \u03a3. While this does not imply a direct equivalence between preference learning and PCA, it highlights an interesting connection: both methods extract a principal direction from the data. Unlike PCA, which maximizes variance in an unsupervised manner, preference learning optimizes a supervised ranking objective, making the relationship approximate rather than exact."}, {"title": "3.3 From Scalar Reward to Diverse Rewards", "content": "Our analysis above suggests a connection between the eigenvectors of the covariance matrix and human preferences. A key observation is that the covariance matrix has d eigenvectors\u2014for example, d = 2048 for gemma-2B (Team et al., 2024) and d = 4096 for Llama-3.1-8B (Grattafiori et al., 2024). This means we can extract a large number of meaningful preference vectors w from PCA applied to the embedding dataset {zi}i=1N\nThese eigenvectors form an orthonormal basis in the d-dimensional space, meaning they are mutually orthogonal and span the entire space. Mathematically, the eigendecomposition of the covariance matrix is given by: \u2211 = WAWT where W = [w1, w2, ..., wd] is an orthonormal matrix whose columns are the eigenvectors of \u03a3, \u039b = diag(\u03bb1, \u03bb2, ..., \u03bbd) is a diagonal matrix of eigenvalues. This ensures that w1, w2, . . ., wd represent diverse preference directions. Any human preference can then be expressed as a linear combination of these basis vectors:\nw = \\sum_{i=1}^{d} k_iw_i,\nwhere k1,..., kd are weight parameters. This formulation enables a flexible and expressive human preference representation as a combination of these decomposed rewards."}, {"title": "3.4 Decomposed Reward Models (DRMs)", "content": "As illustrated in Figure 1, we propose Decomposed Reward Models (DRMs) by applying PCA to a human preference dataset D = {(xi, y_i^\\succ, y_i^\\prec)}^N_{i=1} using an embedding extractor \u03c6(x, y). \u03c6 can be any language models (pretrained or instruction-tuned) or reward models that produce hidden states of dimension d. DRMs consist of two main steps: (1) Embedding Extraction: We run inference over the preference dataset D using \u03c6 to obtain a dataset of embedding differences:\nDe = {zi}i=1N, where zi = \u03c6(xi, y_i^\\succ) - \u03c6(xi, y_i^\\prec).\n(2) Principal Decomposition: Given the dataset De of shape N \u00d7 d, we perform PCA to obtain a"}, {"title": "Advantages of DRMs", "content": "(1) Simplicity: Our proposed DRMs offer a simple yet effective approach to reward modeling without requiring additional training. (2) Diversity: Unlike traditional scalar reward models that struggle to represent heterogeneous human preferences, DRMs leverage a diverse set of basis vectors to capture a wide range of preferences. (3) Adaptivity: By decomposing human preference data through PCA, DRMs naturally ex-"}, {"title": "4 Experiment", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness of DRMs, focusing on the diversity and interpretability of the decomposed heads, as well as their adaptivity to downstream human preferences."}, {"title": "4.1 Experimental Setup", "content": "Dataset. We choose the mixture2_and_safe_pku dataset\u00b9 (Dong et al., 2024), a collection of 550k pairwise preference samples. This dataset combines diverse data sources, including human-labeled preferences from HH-RLHF (Bai et al., 2022) and GPT-labeled preferences from Ultra-Feedback (Cui et al., 2023), making it well-suited for studying diverse preference decomposition. To evaluate the effectiveness of the decomposed reward heads, we test models on two unseen benchmarks with multiple attributes: (1) Reward-Bench (Lambert et al., 2024), a dataset designed to evaluate reward models across various dimensions, including chat quality, safety, and reasoning. (2) Reasonable Preference Reversal (RPR) test set (Pitis et al., 2024) focuses on personalized context-aware preference evaluation. From RPR, we sample five fine-grained categories (i.e., User-Friendliness, Narrative Quality, Linguistic Creativity, Scientific Rigor, and Humor), each with over 80 annotated samples. Other categories were excluded due to insufficient data for reliable evaluation.\nBase Model. For experiments on decomposed reward heads and test-time adaptation, we use two open-source reward models, Gemma-2B-RM and Llama3-8B-RM (Yang et al., 2024b), along with an instruction-tuned language model, gemma-2-9b-it (Team et al., 2024), as our base models. To analyze their performance, we keep the backbone fixed as our feature extractors while generating multiple new reward heads for them.\nBaselines. We compare the performance of DRMs against several baselines. (1) Single-Head RM fine-tunes a single reward head on top of a fixed"}, {"title": "4.2 What information is Captured by DRMS?", "content": "We aim to better understand the decomposed reward heads in DRMs. To achieve this, we evaluate the performance of the top 100 reward vectors, ranked by eigenvalue, on both RewardBench and RPR's fine-grained subsets. Table 1 reports the scores of a trained single-head baseline. The \"Max Value\" column shows the highest score achieved for each attribute, while the \u201cMax Head\" column indicates which reward head achieves this score. We also compare the results with the single-head baseline. The results reveal the following findings:\n(1) Diversity and Interpretability: DRMs effectively capture diverse human preferences, with different reward heads excelling at different attributes. For instance, in Gemma-2B-RM, head_9 performs best on \"User-Friendliness\" (accuracy: 0.798) and \"Humor and Entertainment\" (0.964), while head_12 excels in \"Narrative and Storytelling\" (0.825) and \"Linguistic Creativity\" (0.885). In contrast, the single-head RM fails to capture this diversity, yielding suboptimal performance on attributes such as \"User-Friendliness\" (0.506) and \u201cHumor and Entertainment\u201d (0.690) for Gemma-2B-RM. These results indicate that DRMs not only capture a broader range of human preferences but also provide interpretable representations that align well with certain preference attributes.\n(2) The first head is the most informative: An interesting observation is that the head head_0 consistently achieves the highest overall accuracy for both models. This aligns with expectations, as head_0 corresponds to the eigenvector with the largest variance, i.e., the most informative direction. Furthermore, among the top 100 heads, most of the high-performing heads appear before index 40, which aligns with PCA's property that variance decreases as the head index increases. This finding further supports our argument that PCA can"}, {"title": "4.3 Test-time Preference Adaptation", "content": "A natural application of DRMs is test-time adaptation-deriving linear combinations to match new user preferences. Following the adaptation method in Section 3.4, we use a small subset of test data for each attribute (e.g., n = 15 for RewardBench and n = 5 for RPR), which corresponds to less than 4% of the available data per attribute in RewardBench and less than 6% for RPR. We compare DRMs against several baselines, including the single-head and ensemble-head baselines trained on the same dataset, and two random-head baselines that sam-"}, {"title": "Advantages of DRMs. (1) Simplicity", "content": "Our pro- posed DRMs offer a simple yet effective approach to reward modeling without requiring additional training. (2) Diversity: Unlike traditional scalar reward models that struggle to represent heterogeneous human preferences, DRMs leverage a diverse set of basis vectors to capture a wide range of preferences. (3) Adaptivity: By decomposing human preference data through PCA, DRMs naturally ex-"}, {"title": "4.4 Quantitative Attribute Explainability", "content": "Beyond its adaptability, DRMs offer a significant advantage in interpretability, helping not only to understand human preferences but also to analyze the multiple attributes present in current test sets. Specifically, for each attribute subset, we can obtain the weight parameters k = [k1, ..., kd] corresponding to each basis vector. Figure 2 visualizes some of these weight vectors in RewardBench, revealing distinct patterns. The Chat subset primarily relies on the first few basis vectors, which capture"}, {"title": "4.5 Ablation Study", "content": "We analyze two key factors affecting test-time adaptation: adaptation set size and the number of DRM heads used. Using Gemma-2B-RM as the feature extractor, we present results on RPR and Reward-Bench in Figure 4. Our findings show that performance improves with a larger adaptation set, converging on RewardBench at n \u2265 15. Similarly, increasing the number of heads enhances performance but saturates beyond 100, likely because the most meaningful PCA directions lie within the first 100 heads. When the adaptation set is small (e.g., n = 3), performance is unstable, and fewer heads can yield better results. This may be due to difficulty in correctly weighting heads with limited data, whereas using more heads increases the risk of assigning incorrect weights. However, with sufficient data, more heads eventually lead to better performance. These results suggest that a slightly larger adaptation set and a carefully chosen number of heads are key to optimizing performance."}, {"title": "5 Related work", "content": "The Heterogeneity of Reward Modeling. Reward models (Lambert et al., 2024; Liu et al., 2024b) are typically trained using preference annotations from the Bradley-Terry model (Christiano et al., 2017; Bradley and Terry, 1952) or demonstration data (Wulfmeier et al., 2024; Xiao et al., 2024). However, human preferences are diverse and complex, making it difficult to capture all relevant attributes with a single objective (Yang et al., 2024c; Rame et al., 2024; Chakraborty et al., 2024). To address this, researchers are exploring multi-objective preference learning. This includes collecting datasets that assess multiple attributes (Wu et al., 2023b; Wang et al., 2023; Cui et al., 2023; Pitis et al., 2024) and developing multi-head reward models that learn diverse user preferences (Quan, 2024; Wang et al., 2024a).\nEmbedding-based Reward Model. Recent advances in reward modeling have highlighted embedding-based approaches for their efficiency and scalability (Ahmed et al., 2024; Sun et al., 2023; Zhang et al., 2024a). These models are backed by strong theoretical foundations (Sun et al., 2024) and demonstrate high flexibility with competitive or superior performance (Li et al., 2024b; Tennenholtz et al., 2024). Additionally, they integrate well with established statistical learning tools (Dykstra, 1960; Springall, 1973; Han et al., 2020) and offer greater transparency through statistical insights (Shen et al., 2025; Feng et al., 2025). Unlike prior methods, our approach represents human preferences through the final linear layer, enhancing interpretability and enabling the decomposition of preference components.\nDimensionality Reduction and Embedding Analysis. Linear dimensionality reduction techniques like PCA have proven effective in extracting latent dimensions that capture key human preferences for model alignment (Freire et al., 2024). Beyond alignment, broader studies have explored structuring high-dimensional data into more interpretable embeddings (Huertas-Garc\u00eda et al., 2023; Kanerva et al., 2000). Methods such as Q-Probe (Li et al., 2024b) and DeepMDP (Gelada et al., 2019) further enhance model alignment by efficiently exploring"}, {"title": "6 Conclusion", "content": "In this paper, we establish the connection between preference learning and PCA, introducing Decomposed Reward Models (DRMs). DRMs represent diverse human preferences as a set of orthogonal basis vectors using a novel vector-based formulation of preference. This approach enables efficient test-time adaptation to user preferences without requiring additional training, making it both scalable and practical. Beyond the efficiency, DRMs provide a structured way to understand human preferences. By decomposing complex preferences into interpretable components, they reveal how preferences are formed and interact. We hope this work inspires further research into the fundamentals of human preference learning while promoting more transparent and personalized AI systems."}, {"title": "7 Limitations", "content": "In this paper, we obtain a large number of decomposed rewards from DRMs. However, due to the large scale (e.g., 2048 or 4096 reward heads), we did not manually examine each head to identify its corresponding preference attribute. Future work could focus on developing automated methods to analyze these rewards, such as recognizing patterns in the first 100 reward heads and assessing whether the last 100 primarily capture noise or meaningful subtleties. Additionally, we did not incorporate interdisciplinary study by collaborating with psychology experts to explore human preferences in depth. Future research could benefit from such collaboration to bridge the gap between computational models and cognitive science."}, {"title": "8 Ethics Statement", "content": "This paper introduces Decomposed Reward Models (DRMs), a step toward improving multi-objective alignment in LLMs. Here, we discuss the potential benefits of our approach while acknowledging the associated risks.\nOur method enhances LLM alignment with diverse human preferences. DRMs are lightweight, flexible, and easily adaptable to new users and evolving preferences. Their efficiency reduces resource demands and broadens accessibility, paving"}, {"title": "9", "content": "the way for personalized preference learning and scalable LLM alignment. By offering an interpretable framework, DRMs promote greater transparency and customization in human preference modeling.\nWe carefully follow the license for all datasets and models used in our paper. Human preference datasets often contain biases, reflecting the perspectives and prejudices of their sources. If not properly managed, these biases could propagate through the model, influencing decomposition and principal components, potentially leading to unintended consequences. Mitigating this risk requires careful curation, filtering, and bias reduction before large-scale deployment. Additionally, our method does not inherently control the meaning of each reward head, which could unintentionally capture harmful human preferences. Therefore, thorough evaluation is necessary before deployment to ensure ethical and responsible use."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Ablations on Llama3-8B-RM", "content": "We add the ablation results on Llama3-8B-RM in Figure 5. The trend is similar to the ablations in our main paper."}, {"title": "A.2 Reward Scores on decomposed reward heads", "content": "As shown in Figure 6 and 7, we visualize the reward scores of individual decomposed reward heads on both RewardBench and RPR, respectively. With a hidden dimension of 2048 in Gemma-2B-RM, the total number of reward heads is 4096. While most heads' scores fall within a certain range, a few outliers are identified, which will be utilized during test-time preference adaptation for specific tasks."}, {"title": "A.3 Implementation Details", "content": "For all training-based reward head models, including both the Single Head and Share-Base variants, we train them on the mixture2_and_safe_pku dataset\u00b2 (Dong et al., 2024) for one epoch with a batch size of 16.\nFor our proposed Decomposed Reward Models (DRMs), we apply Principal Component Analysis (PCA) using scikit-learn's default settings to get the component vectors. We experiment with 100 heads for all methods. Note that DRMs utilize 50 distinct reward heads, and including their negative counterparts results in a total of 100 heads."}]}