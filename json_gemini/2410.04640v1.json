{"title": "Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress", "authors": ["Christopher Agia", "Rohan Sinha", "Jingyun Yang", "Zi-ang Cao", "Rika Antonova", "Marco Pavone", "Jeannette Bohg"], "abstract": "Robot behavior policies trained via imitation learning are prone to failure under conditions that deviate from their training data. Thus, algorithms that monitor learned policies at test time and provide early warnings of failure are necessary to facilitate scalable deployment. We propose Sentinel, a runtime monitoring framework that splits the detection of failures into two complementary categories: 1) Erratic failures, which we detect using statistical measures of temporal action consistency, and 2) task progression failures, where we use Vision Language Models (VLMs) to detect when the policy confidently and consistently takes actions that do not solve the task. Our approach has two key strengths. First, because learned policies exhibit diverse failure modes, combining complementary detectors leads to significantly higher accuracy at failure detection. Second, using a statistical temporal action consistency measure ensures that we quickly detect when multimodal, generative policies exhibit erratic behavior at negligible computational cost. In contrast, we only use VLMs to detect failure modes that are less time-sensitive. We demonstrate our approach in the context of diffusion policies trained on robotic mobile manipulation domains in both simulation and the real world. By unifying temporal consistency detection and VLM runtime monitoring, Sentinel detects 18% more failures than using either of the two detectors alone and significantly outperforms baselines, thus highlighting the importance of assigning specialized detectors to complementary categories of failure.", "sections": [{"title": "1 Introduction", "content": "Imitation learning represents one of the simplest yet most effective ways of learning robot control behaviors from data. Herein, generative modeling techniques have enabled robot policies to learn from highly heterogeneous, multimodal demonstration data collected by humans [1, 2, 3], showing early signs of generalization to novel environments [4] and embodiments [5]. When deploying robots beyond controlled lab settings, however, even the most powerful generative policies will eventually encounter out-of-distribution (OOD) test cases-scenarios that differ from the training data-on which their behavior becomes unpredictable [6]. In response, we will require methods that monitor the behavior of learned, generative polices at deployment time to detect whether they are failing as a result of distribution shift.\nIdentifying when a learned model behaves unreliably is typically framed as an OOD detection problem, for which a taxonomy of methods exist [7, 8]. While these methods can signal distribution shift [9, 10] or quantify uncertainty [11, 12] w.r.t. individual input-output samples, they do not fully characterize closed-loop policy failures that arise from multiple, time-correlated prediction errors along a trajectory rollout. Action multimodality further complicates the failure detection problem: that is, actions sampled from multimodal generative policies can vary greatly from one timestep to the next, leading to complex runtime behaviors and, by extension, diverse failure modes compared to previous model-free policies [13, 14]. Therefore, the special case of generative robot policies necessitates the design of new failure detectors suited to their multimodal characteristics and closed-loop operational nature in deployment.\nIn this work, we present Sentinel, a runtime monitoring framework that splits the task of detecting generative policy failures into two complementary categories. The first is the detection of failures in which the policy exhibits erratic behavior as characterized by its temporal inconsistency. For example, the robot may collide with its surroundings if the policy's action distributions contain conflicting action modes across time. To detect erratic failures, we propose to evaluate how much a generative policy's action distributions are changing across time using Statistical measures of Temporal Action Consistency (STAC). The second category is the detection of failures in which the policy is temporally consistent but struggles to make progress on its task. For example, the robot can stall in place or drift astray if the policy produces constant outputs. We propose to detect task progression failures (undetectable by STAC) zero-shot with Vision Language Models (VLMs), which can distinguish off-nominal behavior when prompted to reason about the robot's progress in a video question answering setup. Notably, one would want to catch erratic failures (the first category) fast, whereas task progression failures (the second category) do not require immediate intervention.\nOur contributions are three-fold: 1) A formulation of failure detection for generative policies that splits failures into two complementary categories, thus admitting the use of specialized detectors toward system-level performance increases (i.e., a divide-and-conquer strategy); 2) We propose STAC, a novel temporal consistency detector that tracks the statistical consistency of a generative policy's action distributions to detect erratic failures; 3) We propose the use of VLMs to monitor the task progress of a policy over the duration of its rollout, and we offer practical insights for their use as failure detectors. Provided with only a set of successful policy rollouts and a natural language description of the task, Sentinel (which runs STAC and the VLM monitor in parallel) detects over 97% of unknown failures exhibited by diffusion policies [1] across simulated and real-world robotic mobile manipulation domains."}, {"title": "2 Related Work", "content": "Advances in robot imitation learning include new policy architectures [1, 15, 16, 17], hardware innovations for data collection [18, 19, 20], community-wide efforts to scale robot learning datasets [3, 5, 21], and training high-capacity behavior policies on these datasets [2, 4, 22, 23]. Of recent interest is the use of generative models [24, 25, 26, 27] to effectively learn from heterogeneous and multimodal datasets of human demonstrations [1, 5, 2, 23]. Generative policies thereby learn to represent highly multimodal distributions from which diverse robot actions can be sampled. While state-of-the-art generative policies demonstrate remarkable performance, their inherent multimodality results in more stochastic runtime behavior than that of previous model-free policies [13, 14, 28, 29, 30, 31]. In this work, we focus on characterizing the behavior of generative robot policies for failure detection."}, {"title": "3 Problem Setup", "content": "Failure Detection The goal of this work is to detect when a generative robot policy \u03c0(a|s) fails to complete its task. The policy operates within a Markov Decision Process (MDP) with a finite horizon H, but it may terminate upon completion of the task at an earlier timestep. Given an initial state $s_0$ representative of a new test scenario, executing the policy for t timesteps produces a trajectory $T_t = (s_0, a_0, \\dots, s_t)$. We define failure detection as the task of detecting whether a trajectory rollout $T_H$ constitutes a failure at the earliest possible timestep t. To do so, we aim to construct a failure detector $f(T_t) \\rightarrow {ok, failure}$ that, at each timestep t, can provide a classification as to whether the policy will fail if it continues executing for the remaining H-t timesteps of the MDP. Note that the failure detector makes its assessment solely based on the history of observed states and sampled actions up to the current timestep t.\nThe failure detector may contain parameters that require calibration, such as a detection threshold (as in \u00a74.1). Therefore, we assume a scenario in which the policy \u03c0 is first trained, then validated on test cases where it is expected to perform reliably. This validation process yields a small dataset of M successful trajectories $D_\\tau = {\\tau^i}_{i=1}^M$ that can be used to calibrate the failure detector f (if it contains parameters). Intuitively, the dataset $D_\\tau$ characterizes nominal policy behavior within or near the distribution of states it has been trained on, which helps to ground the assessment of potentially OOD trajectories at test time.\nWe measure failure detection performance in terms of true positive rate (TPR), true negative rate (TNR), and detection time (DT). We count a true positive if the failure detector raises a warning at any timestep in a trajectory where the policy fails, the earliest of which counts as the detection time. We count a true negative if the failure detector never raises a warning in a trajectory where the policy succeeds. We refer to \u00a7B.4 for supporting definitions of policy failure and key performance metrics."}, {"title": "4 Proposed Approach: Sentinel", "content": "The failure behavior of a generative policy by OOD conditions can be highly diverse, and we therefore argue that the desiderata for a failure detector may vary between qualitative types of failures. In this work, we propose to split the failure detection task into two complementary failure categories.\nThe first is the detection of failures resulting from erratic policy behavior, which may cause a robot to end up in states that are difficult or costly to reset from, knock over objects, or lead to safety hazards. Therefore, it is important to detect erratic behavior as quickly as possible (\u00a74.1). The second category is the detection of failures in which the policy struggles to make progress on its task (hereafter referred to as task progression failures) but does so in a temporally consistent manner. For example, the policy may confidently place an object in the wrong location. Here, we must observe the robot over a longer period of time to identify that the policy is not making progress towards task completion (\u00a74.2).\nThe key insight of our approach is that by defining one failure category as the complement of the other, it becomes trivial to combine failure detectors to form an accurate overall detection pipeline whilst satisfying the requirements of each failure category. Our full pipeline, Sentinel, is shown in Fig. 3."}, {"title": "4.1 STAC: Detecting Erratic Failures with Temporal Consistency", "content": "When a policy operates in nominal, in-distribution settings, it should complete its task in a temporally consistent manner. For example, a policy may plan to avoid an obstacle on the right or on the left, but not jitter between the two options. Moreover, as noted in [1], training a diffusion policy that predicts action sequences rather than individual actions encourages temporal consistency between action predictions.\nTherefore, we propose to construct a quantifiable measure of temporal action consistency to detect whether the policy is behaving erratically, and hence, is likely to fail at the task. However, the multimodal distributional nature of DPs makes it difficult to directly compare two sampled actions $a_t \\sim \\pi(a_t|s_t)$ and $a_{t+k} \\sim \\pi(a_{t+k}|s_{t+k})$, e.g., throughout execution. This is because the actions may differ substantially along their prediction horizon when the policy commits or switches between different action modes, or simply due to randomness in sampling. Instead, we quantify erratic policy behavior with statistical measures of temporal action consistency (STAC, which we term our approach).\nLet $\\pi_t:= \\pi(a_{t+k:t+h-1|t}|s_t)$ and $\\tilde{\\pi}_{t+k}:= \\pi(a_{t+k:t+h-1|t+k}|s_{t+k})$ be the marginal action distributions of the temporally overlapping actions between timesteps t and t + k. We compute the temporal consistency between two contiguous timesteps t and t+k as $D(\\tilde{\\pi}_t, \\tilde{\\pi}_{t+k}) \\geq 0$, where D denotes the chosen statistical distance function (e.g., maximum mean discrepancy, KL-divergence). In addition, we propose to take the cumulative sum of statistical distances along a trajectory as a measure of the overall temporal consistency in a policy rollout. At each policy-inference timestep t = jk with $j \\in {0,1,...}$, we compute the temporal consistency score as\n$n_t := \\sum_{i=0}^{j-1} D(\\tilde{\\pi}_{ik},\\tilde{\\pi}_{(i+1)k}).$ (1)\nComputing the consistency score in a cumulative manner has two advantages over thresholding the distance at each timestep individually. Firstly, it allows us to detect cases where the temporal consistency metric D is marginally larger than usual throughout the episode (e.g., jitter). Secondly, it allows us to detect instances where the policy is temporally inconsistent more often than in nominal scenarios.\nAt runtime, we raise a failure warning at the moment that $n_t$ exceeds a failure detection threshold \u03b3, which we calibrate offline using the validation dataset of successful trajectories $D_\\tau$. Here, we first compute the cumulative temporal consistency scores throughout the entirety of the lengths $H_i < H$ of trajectories in $D_\\tau$, yielding {$n^{\\tau^i}_{H_i}$}_{i=1}^M. Then, we set the threshold \u03b3 to the 1 \u2013 \u03b4 quantile of {$n^{\\tau^i}_{H_i}$}_{i=1}^M, where \u03b4\u2208 (0,1) is a hyperparameter. Intuitively, this ensures that the false positive rate (FPR)\u2014the probability that we raise a false alarm and terminate on any trajectory that is i.i.d. with respect to D\u2014remains low, such that any warnings are likely failures. We can formalize this intuition using recent results from conformal prediction [66, 67]:\nProposition 1 (STAC has low FPR). Let $P_\\tau$ denote the distribution of success trajectories in the validation dataset $D_\\tau = {\\tau^i}_{i=1}^{M_1} \\text{d} P_\\tau$. Then, the FPR\u2014the probability of raising a false alarm at any point during an i.i.d. test trajectory \u03c4 ~ P\u2081 of length H' < H\u2014is at most \u03b4:\nFPR:=Pp, (\u22030<t\u2264H' s.t. nt >\u03b3) \u2264 \u03b4. (2)"}, {"title": "4.2 Detecting Task Progression Failures with VLMs", "content": "A policy operating in out-of-distribution settings may not always fail by exhibiting erratic behavior that we can detect with STAC (\u00a74.1). For example, suppose the policy confidently commits to the wrong"}, {"title": "5 Experiments", "content": "We conduct a series of experiments to test our failure detection framework. These experiments take place in both simulation and the real world (Fig. 1), and host an extensive list of baselines. We refer to \u00a7B for a detailed description of our environments, hardware setup, baselines, and evaluation protocol.\nEnvironments. We include the PushT domain from [1] to evaluate the detection of failures under action multimodality. The Close Box and Cover Object domains involve two mobile manipulators and thus present the challenge of a high-dimensional, 14 degree-of-freedom action space. We additionally conduct hardware experiments with a mobile manipulator for a nonprehensile Push Chair task. This task presents greater dynamic complexity than the simulation domains [74]. At test time, we generate OOD scenarios by randomizing a) the scale and dimensions of objects in PushT and Close Box and b) the pose of the object in Cover Object and Push Chair beyond the policy's demonstration data.\nBaselines. We evaluate Sentinel (i.e., both STAC and the VLM) against baselines representative of multiple methodological categories in the OOD detection literature [7]. Intuitively, these categories represent different formulations of the failure detector's score function, responsible for computing the per-timestep scores that are then summed to compute the trajectory score as in Eq. 1. We consider score functions based on the embedding similarity of observed states w.r.t. D+ [47], the reconstruction error of actions sampled from the DP [75], and the output variance of the DP. To strengthen the comparison, we introduce a new baseline that uses the DDPM loss (Eq. 3) on re-noised actions sampled from the DP as the failure detector's score function. Where applicable, we implement temporal consistency variants of these baselines to ablate the design of STAC. Further details on these baselines are provided in \u00a7B.3.\nEvaluation Protocol. We train a DP for each environment and use standard settings for the DP's prediction and execution horizon [1]. We use the same calibration and evaluation protocol across all failure detection methods. That is, we calibrate detection thresholds to the 95-th quantile of scores in a dataset D\u2081 = {r}\\\u2081 of M = 50 in-distribution rollouts for each simulated task and M = 10 in-distribution rollouts for the real-world task. Finally, we report standard detection metrics including TPR, TNR, Mean Detection Time, Accuracy, and Balanced Accuracy, following the definitions in \u00a73."}, {"title": "6 Results", "content": "STAC detects diffusion policy failures in multimodal domains. Fig. 5 (Left) compares STAC against the best performing method of each baseline category in the PushT domain. Here, STAC is the only method to achieve a balanced accuracy of over 90%, indicating that temporal consistency (or lack thereof) is strongly correlated with success (or failure). Alternative output metrics, such as the DP's output variance, do not perform well because both successes and failures can exhibit high-variance outputs in multimodal domains. Interestingly, the embedding similarity approach performs strongly, which indicates that state dissimilarity w.r.t. the calibration dataset happens to be correlated with failure in this domain.\nStatistical measures of action similarity enable temporal consistency detection. Fig. 5 (Right) ablates the design decisions of STAC. First, we observe that augmenting baselines with temporal consistency will at most marginally increase their performance. Second, using a non-statistical distance function (e.g., min. distance) to measure temporal action consistency performs worse than the baselines because it omits action multimodality. Thus, it is the combination of statistical distance functions with temporal consistency that yields the best result. We refer to \u00a7C.1 for an extended ablation of STAC.\nSTAC accounts for OOD failures and generalization. Results on the Close Box domains are shown in Table 1. STAC attains the highest accuracy in aggregate (96%). However, two of our newly proposed baselines-using the DDPM loss (Eq. 3) and a temporal reconstruction variant of [75]-also perform well, perhaps due to a decrease in action multimodality relative to PushT. Notably, we find that embedding similarity methods conflate OOD states with policy failure, resulting in false positives when the policy succeeds OOD. In contrast, STAC effectively differentiates OOD successes from failures.\nVLMs must reason across time. In Table 1, we find that a state-of-the-art VLM (GPT-40) struggles to identify task success when given only a single image. Instead, it must observe the robot over the extent of a policy rollout to more accurately reason about task progression and changes in state (resulting in a"}, {"title": "7 Conclusion", "content": "In this work, we investigate the problem of failure detection for generative robot policies. We propose Sentinel, a runtime monitor that splits the failure detection task into two categories: 1) Erratic failures, which we detect by measuring the statistical change of a policy's action distributions over time; 2) task progression failures, where we use Vision Language Models to assess whether the policy is consistently taking actions that do not solve the task. Our results highlight the importance of targeting complementary failure categories with specialized detectors. Future work includes the use of Sentinel to monitor high-capacity policies [2, 23], inform data collection, and accelerate policy learning.\nLimitations. While categorizing erratic and task progression failures leads to accurate detection of failures across the domains we consider, these two failure categories may not be exhaustive. In the future, introducing additional categories or further partitioning existing ones might provide a broader coverage of failures, allow for more efficient failure detection, and inform mitigation strategies. Furthermore, our approach does not provide formal guarantees on detecting failures. However, providing such guarantees would require data of both successful and unsuccessful policy rollouts to calibrate the detector [67]. Although our detectors attain low false positive rates in aggregate, taking the union of their predictions may, in the worst case, increase the risk of false alarms. Thus, exploring more sophisticated ways to combine complementary failure detectors is a possible point of extension. Finally, our approach is not targeted at predicting failures before they occur, but instead focuses on detecting failures as they occur."}, {"title": "A Method Details: Sentinel", "content": "As shown in Fig. 3, the Sentinel runtime monitoring framework consists of the parallel operation of two complementary failure detectors, each assigned to the detection of a particular failure category of generative policies. The first is a temporal consistency detector that monitors for erratic policy behavior via statistical temporal action consistency (STAC) measures. The second is a Vision Language Model (VLM) that monitors for failure of the policy to make progress on its task. In this section, we provide additional details w.r.t. the implementation of STAC (\u00a7A.1) and the VLM runtime monitor (\u00a7A.2)."}, {"title": "A.1 Temporal Consistency Detection with STAC", "content": "Background To summarize \u00a73, STAC assumes the use of a stochastic policy \u3160 that, at each policy-inference timestep t, predicts an action sequence for the next h timesteps as $a_{t:t+h-1|t} \\sim \\pi(\\cdot|s_t)$, executes the first k actions $a_{t:t+k|t}$, before re-evaluating the policy at timestep t+k. Between two contiguous inference timesteps t and t+k, sampled action sequences $a_{t+k:t+h-1|t}$ and $a_{t+k:t+h-1|t+k}$ (both in $\\mathbb{R}^{(h-k) \\times |A|}$) overlap for h\u2212 k timesteps. At a high-level, STAC seeks to quantify how much a generative policy's action distributions are changing over time. It does this by computing statistical distances between the distributions of overlapping actions, i.e., given $\\tilde{\\pi}_t := \\pi(a_{t+k:t+h-1|t}|s_t)$ and $\\tilde{\\pi}_{t+k}:= \\pi(a_{t+k:t+h-1|t+k}|s_{t+k})$, we compute $D(\\tilde{\\pi}_t,\\tilde{\\pi}_{t+k})$.\nHypothesis Our central hypothesis is that large statistical distances correlate with downstream policy failure. Intuitively, a predictive policy can be likened to possessing an internal world model that simulates how robot actions affect environment states. When the policy is in distribution, we expect this world model to be accurate, thus resulting in smaller statistical distances. More concretely, if the policy's internal model of state $s_{t+k}$ at timestep t coincides with the actual observed state $s_{t+k}$ at timestep t+k, the distribution of actions $a_{t+k}$ should be well-represented by the distribution $a_t$. As a result, the distance $D(\\tilde{\\pi}_t,\\tilde{\\pi}_{t+k})$ will be small (for the right choice of statistical distance function D). Conversely, when the policy is out of distribution (OOD), its internal model of state $s_{t+k}$ at timestep t may be inaccurate, yielding a divergence between $\\pi_t$ and $\\pi_{t+k}$ and a larger statistical distance.\nImplementation Details As mentioned in \u00a74.1, we propose to approximate $D(\\tilde{\\pi}_t,\\tilde{\\pi}_{t+k})$ with an empirical distance function $\\tilde{D}$ instead of computing it analytically, as doing so presents the challenge of marginalizing out both the non-overlapping actions (between timesteps t and t+k) and the intermediate steps of the diffusion process [76]. We found the following approximations to work well in practice:\nMaximum Mean Discrepancy (MMD) with radial basis function (RBF) kernels. We compute\n$D(\\tilde{\\pi}_t,\\tilde{\\pi}_{t+k})=\\mathbb{E}_{a_t, a_t \\sim \\tilde{\\pi}_t}[k(a_t,a_t)] + \\mathbb{E}_{a_{t+k}, a_{t+k} \\sim \\tilde{\\pi}_{t+k}}[k(a_{t+k},a_{t+k})]$ \n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ -2\\mathbb{E}_{a_t \\sim \\tilde{\\pi}_t, a_{t+k} \\sim \\tilde{\\pi}_{t+k}}[k(a_t,a_{t+k})]$, where $k(x,y;\\beta_1)=exp\\left(-\\frac{||x-y||^2}{\\beta_1}\\right)$.\nHere, k: $\\mathbb{R}^{(h-k) \\times |A|} \\times \\mathbb{R}^{(h-k) \\times |A|} \\rightarrow \\mathbb{R}$ computes the similarity between two overlapping action sequences, and \u03b2\u2081 denotes the bandwidth of the RBF kernel. The expectations are taken over a batch of B action sequences sampled from the generative policy.\nForward KL-divergence via Kernel Density Estimation (KDE) of the policy distributions:\n$D(\\tilde{\\pi}_t,\\tilde{\\pi}_{t+k})=\\mathbb{E}_{a_{t+k} \\sim \\tilde{\\pi}_{t+k}}\\left[-log \\left[\\frac{p(a_{t+k})}{q(a_{t+k})}\\right]\\right],$where p and q are KDEs of $\\pi_{t+k}$ and $\\pi_t$ fit on a batch of B action sequences sampled from each policy distribution, respectively. As before, we use Gaussian RBF kernels of the form $k(x,y; \\beta_2)$, where \u03b22 denotes the bandwidth of the RBF kernels used for KDE."}, {"title": "A.2 Runtime Monitoring with Vision Language Models", "content": "As described in \u00a74.2, we formulate the detection of task progression failures as a chain-of-thought (CoT) [71], video question answering (Video QA) [73] task with VLMs. Below, we provide details on the implementation of our VLM runtime monitor and the prompt templates used in our experiments.\nVision Language Models In extended experiments (\u00a7C.2), we include variants of the VLM runtime monitor based on several models: OpenAI's GPT-40, Anthropic's Claude 3.5 Sonnet, and Google's Gemini 1.5 Pro [79]. At the time of writing, these represent the current state-of-the-art VLMs for complex, multimodal reasoning tasks. We use consistent prompts across all models, however, we slightly vary the implementation of the monitor to reflect the suggested best practices of each VLM.\nImplementation Details We propose to query the VLM online with a parsed text prompt describing the runtime monitoring task and the history of images (i.e., a video) 10:t := (I0,Ivk,I2vk,...,It) captured by the robot's camera system up to the current timestep t. Here, the hyperparameter v specifies the frequency of the images relative to the execution horizon k of the generative policy (\u00a73) for generality, as the video may be captured at a much higher frame rate than the policy's execution rate. In experiments, simply setting v\u2208 {1,2} provided sufficient granularity for the VLM to identify the motion of the robot."}, {"title": "A.2.1 Prompt Ensembling", "content": "The Video QA failure detection task requires comprehensive and detailed reasoning of over potentially long sequences of images, which, at the time of writing, is a challenge for even the most capable VLMs. As such, we can expect the performance of the VLM runtime monitor to degrade when it is deployed in domains that are visually OOD w.r.t. the VLM's training data (e.g., images rendered in simulation or captured from unusual camera poses). In these settings, the VLM runtime monitor may provide a reasonable but imperfect signal on task progression failures, resulting in misclassifications.\nTo strengthen the reliability of our VLM runtime monitor, we propose a simple prompt ensembling strategy [80], whereby we construct multiple Video QA prompts, query the VLM with each prompt, and take the overall failure classification to be the majority vote of the predictions across all prompts. Intuitively, if the failure detectors associated with each individual prompt are fairly accurate to begin with, the resulting majority-vote detector will have an even higher probability of correctness.\nIn experiments, we only found it necessary to use prompt ensembling in the Cover Object domain. We construct two variants of the Video QA prompt (3 prompts total), each of which follow a similar CoT structure while including additional information to diversify the VLM's reasoning. The first prompt variant, Video QA + Success Video, includes a video of a successful policy rollout for the current task. This allows the VLM to distinguish off-nominal policy behavior at test time from nominal policy behavior illustrated in the example video. The second prompt variant, Video QA + Goal Images,"}, {"title": "B Experiment Details", "content": "B.1 Environments\nWe provide additional details on the environments used to evaluate Sentinel. These environments vary in terms of their data distribution (e.g., multimodal or high-dimensional actions) and support different types of distribution shift (e.g., object scale, pose, dynamics), under which the behavior of generative diffusion policies can be methodically studied. The environments are visualized in Fig. 7."}, {"title": "B.1.1 Simulation Domains", "content": "PushT Domain The policy is tasked with pushing a planar \"T\"-shaped object into a goal configuration. A trajectory is considered successful if the overlap between the \"T\"-shaped object and its goal exceeds 90% within 300 environment steps. The action space is the 2-DoF linear velocity of the end-effector. We generate OOD test scenarios by non-uniformly randomizing the scale and dimensions of the \"T\"-shaped object beyond the randomizations contained in the policy's demonstration data. The policy tends to fail by converging to a locally optimal configuration, where the \u201cT\u201d overlaps with its goal but in an incorrect orientation. Since the task can be solved in a number of ways, we include this domain to evaluate the performance of various score functions in the presence of action multimodality. We refer to [1] for the process of generating demonstration data in this domain.\nClose Box Domain The policy is tasked with closing a box that has three lids. A trajectory is considered successful if all three lids are closed within 120 environment steps (24 seconds). The action space is the 14-DoF linear + angular velocities and gripper commands for the end-effectors of two mobile manipulators. Demonstration data is generated by an oracle policy that sets a series of waypoints for the end-effectors based on the initial state. We generate OOD test scenarios by non-uniformly randomizing the scale of the box beyond the randomizations contained in the policy's demonstration data. The policy tends to fail erratically when the robots e.g., collide with the box or its lids, however, task progression failures may also occur. This domain is primarily used to evaluate the detection of erratic policy failures on a bi-manual robotic system with a high-dimensional action space.\nCover Object Domain The policy is tasked with covering a rigid object with a cloth. A trajectory is considered successful if over 75% of the object is covered by the cloth within 64 environment steps (13 seconds). The action space and process of generating demonstration data is identical to that of Close Box. We generate OOD test scenarios by non-uniformly randomizing the position of the object beyond the randomizations contained in the policy's demonstration data. The policy tends to fail by releasing the cover before reaching the object. Hence, this domain is used to evaluate the detection of task progression failures, where reasoning over longer durations is required to assess task progress."}, {"title": "B.1.2 Real-World Domains", "content": "Mobile Robot Setup We use a holonomic mobile base equipped with a Kinova Gen3 7-DoF arm. A single ZED 2 camera is fixed in the workspace to capture visual observations for the generative policy. The ZED 2 camera first generates a partial-view point cloud of the environment, from which we segment task-relevant objects using the Grounded Segment Anything Model [81] based on a natural"}, {"title": "B.2 Diffusion Policies", "content": "We train a diffusion policy (DP) for each environment, using 200 demonstrations for the PushT domain, 50 demonstrations for each of the Close Box and Cover Object domains, and 15 demonstrations for the real-world Push Chair domain. In a DP, actions are generated by iteratively denoising an initially random action $a^0 \\sim \\mathcal{N}(0,1)$ over N steps as $a^i,..., a^0$, where $a^i$ with a superscript i denotes the generated action sequence at the i-th denoising iteration. In an imitation learning setting, the DP's noise prediction network $e_\\theta$ is trained to predict the random noise \u03b5 added to actions drawn from a dataset of expert demonstrations $D_{train}$ by minimizing\n$L_{ddpm}:= \\mathbb{E}_{(s, a^0) \\sim D_{train}, \\epsilon, i} [||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha_i}}a^0+\\sqrt{1-\\bar{\\alpha_i}}\\epsilon, s, i)||^2],$ (3)\nwhere the constants $\\bar{\\alpha_i}$ depend on the chosen noise schedule of the diffusion process.\nTo increase the salience of distribution shift w.r.t. the position and scale of objects, we use point clouds as inputs to the policy instead of RGB images (i.e., a 5% increase in object scale may not be salient in an image). For simulation experiments, we use a diffusion policy architecture identical to the original paper [1] except for the visual encoder, where we substitute the ResNet-based encoder for a PointNet-based one: a 4-layer PointNet++ encoder [84] with hidden dimension 128. The output of this encoder is concatenated with the proprioceptive inputs and then fed to the noise prediction network. For real-world experiments, we use the recently proposed EquiBot diffusion policy architecture [85], which additionally incorporates SIM(3) equivariance into the diffusion process. We use EquiBot to evaluate our failure detectors on a current state-of-the-art approach for learning generative policies in the real world. All diffusion policies produce an action over N = 100 denoising iterations. Unless otherwise specified, we use standard settings for the prediction h and execution horizon k of the diffusion policy."}, {"title": "B.3 Baselines", "content": "We outline the implementation details of our baselines as introduced in \u00a75. First, with the exception of the VLM runtime monitors, all evaluated failure detection methods consist of computing a score S(\u00b7) at each policy-inference timestep in a rollout, taking the cumulative sum of scores up to the current timestep t, and then checking if the cumulative sum exceeds a calibrated threshold to detect policy failure. As such, the baselines differ in their score function, i.e., how they compute the per-timestep scores that are then summed and thresholded. Intuitively, a good score function should be well-correlated with policy failure, that is, it should output small values when the policy is succeeding and large ones when it is failing. For example, Fig. 4 demonstrates that STAC holds this property. We"}, {"title": "B.3.1 STAC Baselines (Policy-Level Monitors)", "content": "Policy Encoder Embedding quantifies the dissimilarity of the current point cloud observation $o_t$ w.r.t. to the point clouds in the calibration dataset of M successful policy rollouts $D_\\tau = {\\tau^i}_{i=1}^M$ (as described in \u00a73) within the embedding space of the policy's encoder (here, $o_t$ denotes the point cloud input to the policy, which includes the point cloud at the current and previous timestep). More concretely, let E be the policy's encoder, $z_t = E(o_t)$ be the current point cloud embedding, and $D_z = E(D_\\tau)$ be the embeddings of all point clouds contained in the calibration dataset. We compute the per-timestep score as the Mahalanobis distance\n$S(z_t; D_z) = \\sqrt{(z_t-\\mu_z)^T\\Sigma_{zz}^{-1}(z_"}]}