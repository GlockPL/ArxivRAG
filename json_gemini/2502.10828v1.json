{"title": "The Vendiscope: An Algorithmic Microscope For Data Collections", "authors": ["Amey P. Pasarkar", "Adji Bousso Dieng"], "abstract": "The evolution of microscopy, beginning with its invention in the late 16th century, has continuously enhanced our ability to explore and understand the microscopic world, enabling increasingly detailed observations of structures and phenomena. In parallel, the rise of data-driven science has underscored the need for sophisticated methods to explore and understand the composition of complex data collections. This paper introduces the Vendiscope, the first algorithmic microscope designed to extend traditional microscopy to computational analysis. The Vendiscope leverages the Vendi scores a family of differentiable diversity metrics rooted in ecology and quantum mechanics and assigns weights to data points based on their contribution to the overall diversity of the collection. These weights enable high-resolution data analysis at scale. We demonstrate this across biology, materials science, and machine learning (ML). We analyzed the 250 million protein sequences in the protein universe, discovering that over 200 million are near-duplicates and that AlphaFold fails on proteins with Gene Ontology (GO) functions that contribute most to diversity. Applying the Vendiscope to the Materials Project database led to similar findings: more than 85% of the crystals with formation energy data are near-duplicates and ML models perform poorly on materials that enhance diversity. Additionally, the Vendiscope can be used to study phenomena such as memorization in generative models. We used the Vendiscope to identify memorized training samples from 13 different generative models spanning several model classes and found that the best-performing generative models often memorize the training samples that contribute least to diversity. Our findings demonstrate that the Vendiscope can serve as a powerful tool for data-driven science, providing a systematic and scalable way to identify duplicates and outliers, as well as pinpointing samples prone to memorization and those that models may struggle to predict-even before training.", "sections": [{"title": "Main", "content": "Keywords: Algorithmic Microscopy, Data-Driven Science, Vendi Scoring\nIn recent years, many scientific fields have transitioned to data-driven paradigms, where the sheer scale and complexity of datasets often outpace traditional analytical techniques. Biology, materials science, and artificial intelligence (AI) are prime examples of disciplines now dominated by vast, high-dimensional data collections that are critical for advancing knowledge. However, these datasets are frequently rife with redundancies, biases, and oddities that traditional methods may struggle to capture or analyze at scale. Furthermore, in the rapidly evolving field of AI, there is an increasing call for data-centric methodologies that focus on improving data quality rather than solely refining models. As Al systems become more deeply integrated into scientific and industrial applications, the need for computational tools capable of scrutinizing the underlying composition of datasets has never been more pressing.\nThis paper introduces the concept of algorithmic microscopes, computational tools that allow scientists to \"zoom in\" on datasets to make discoveries. What should an algorithmic microscope do? At its core, it should help uncover new insights by carefully examining the dataset itself, without the need to build a model of the data. Just as a traditional microscope reveals unseen details in the physical world, an algorithmic microscope should reveal the hidden details of a data collection, enabling the discovery of rare items, outliers, and unexpected patterns. These elements often hold the key to breakthroughs or findings that warrant deeper investigation. An equally important aspect of data-driven discovery is identifying redundancies in data. Redundant data points can provide insight into the underlying structure of the dataset, e.g. in the form of clusters. By detecting where redundancies occur, an algorithmic microscope may identify biased data sources and reveal areas where the data collection process may need to be expanded. Another critical approach in the discovery process is the comparison of data collections. By jointly analyzing datasets, an algorithmic microscope ought to be able to reveal commonalities or gaps that would be invisible when the datasets are examined in isolation. For example, comparing a training set with the outputs of a generative model can uncover memorized samples, revealing important aspects of model behavior and generalization. Finally, an algorithmic microscope should be able to rank data points by their rarity or commonality. This ranking can serve as a guiding principle for efficient data processing, model training, and discovery.\nIn this paper, we demonstrate that all of these capabilities\u2014detecting outliers, identifying redundancies, comparing data collections, and ranking by rarity-can be achieved at scale by answering a single question: What is the contribution of each data point to the overall diversity of the collection? We introduce the Vendiscope, the first algorithmic microscope designed to answer this question. The Vendiscope maximizes the probability-weighted Vendi Score (pVS) of an input collection. The PVS, introduced in Friedman and Dieng (2023), is a similarity-based diversity metric that measures the diversity of a collection of elements sampled according to a discrete probability distribution, where each element is assigned a probability. In this work, the Vendiscope learns these probabilities, which correspond to the contributions of each data point to the overall diversity of the collection.\nTo scale the Vendiscope to large data collections, such as the protein universe, we employ projective gradients, leverage pre-trained feature extractors to compute"}, {"title": "", "content": "cosine similarity for the Vendi Score calculation, and use parallel computing. These scaling strategies enable the Vendiscope to operate with linear complexity in both space and time, making it feasible to analyze extremely large datasets.\nWe applied the Vendiscope to three different domains: biology, materials science, and AI. We first analyze the universe of protein sequences, which consists of nearly 250 million entries. Despite the scale of this dataset, the Vendiscope can provide an end-to-end analysis within 2 hours on a single compute node comprised of 8 NVIDIA A6000 GPUs. We find that AlphaFold struggles to provide accurate structural predictions for sequences that contribute most to diversity. We highlight some of the genes and functions enriched among these sequences. Additionally, we find that over 80% of the entire dataset are near-duplicates. This suggests that while the universe of protein sequences is vast, its diversity is significantly lower.\nA similar pattern emerges on the Materials Project (MP) database, a benchmark dataset containing almost 170,000 material structures and various properties. We show how three popular models-ALIGNN, CGCNN, and DeeperGATGNN-all fail in the same manner: their formation energy and band-gap property prediction errors are significantly higher on materials that contribute most to diversity. Furthermore, we expect modeling efforts to improve as MP continues to grow, however, we highlight the need for the addition of unique, non-redundant materials: over 85% of crystals with formation energy data are near-duplicates in the MP database.\nWe also show how the Vendiscope can reveal patterns of memorization across 13 state-of-the-art image generative models trained on CIFAR10 and spanning popular model classes such as diffusion models, GANs, VAEs, and flows. The Vendiscope reveals that the models with the best-looking outputs\u2014those that achieve high human error rate-tend to memorize data points that contribute least to the diversity of the training set.\nOur results reveal systemic challenges in modeling data points that contribute most to diversity, across domains. This is exacerbated by a staggering amount of data redundancy in these domains. The Vendiscope can serve as a powerful tool for data-driven discovery and help create more balanced, high-quality datasets in an era of data-driven science."}, {"title": "The Vendiscope", "content": "Consider a collection of N elements (x\u2081, ..., xN). Let k(\u00b7,\u00b7) denote a positive semi-definite kernel that measures the similarity between any two elements, and such that k(xi, xj) = 1 \u03b4ij. Denote by K the similarity matrix induced by the kernel k(\u00b7,\u00b7). Its element at row i and column j is Kij = k(xi, xj). Since k(\u00b7, \u00b7) is positive semi-definite, K is positive semi-definite and has nonnegative eigenvalues which we denote by \u03bb\u2081,..., \u03bbN. Let p = (p\u2081, ..., pN) denote a discrete probability distribution over the collection (x\u2081, ..., xN). Define Kp = diag(\u221ap)Kdiag(\u221ap) and let \u03b7\u2081p,..., \u03b7Np denote the eigenvalues of Kp. Friedman and Dieng (2023) define the probability-"}, {"title": "", "content": "weighted Vendi Score (pVS) of the collection as\n$$PVS_k(x_1, ..., x_N, p) = exp\\bigg(\\sum_{i=1}^N p_i log \\eta_{ip}\\bigg)$$\nThis can be generalized using the R\u00e9nyi entropy (Pasarkar and Dieng, 2024),\n$$PVS_k(x_1, ..., x_N, p) = exp\\bigg(\\frac{1}{1-q} log \\sum_{i \\in supp(\\eta)} \\eta_{ip}^q\\bigg)$$\nwhere supp(\u03b7) denotes the set of non-zero eigenvalues of Kp and q \u2265 0 is the order of the pVS. Settting q = 1 recovers Eq. 1. We refer the reader to Pasarkar and Dieng (2024) for a detailed discussion of how q influences the Vendi score.\nThe Vendiscope considers p as an unknown probability distribution to be learned by maximizing Eq. 2,\n$$p^*= argmax_p PVS(x_1,...,x_N, p) \\text{ such that } \\sum_{i=1}^N p_i = 1.$$\nOptimizing Eq. 2 will assign a higher probability to the rarest samples, and a lower probability to the most common ones. Indeed, for all orders of q, Equation 2 is maximized when the eigenvalues \u03b7\u2081p,\u2026\u2026\u2026, \u03b7Np are the same. Therefore, to successfully optimize Equation 2 the probabilities should be learned such that all eigenvalues are within a small \u03f5 distance of each other: \u03b7min \u2264 \u03b7ip \u2264 \u03b7min + \u03f5, where \u03f5 > 0 and \u03b7min denotes the minimum eigenvalue. We assume without loss of generality that \u03b7min is non-zero. We can link the uniformity of the eigenvalues to the Vendiscope's learned probabilities using the Gershgorin Circle Theorem (Varga, 2011). From this theorem, we know that the eigenvalues of Kp are located in discs with radii determined by the row-sums. Define $$C_j = \\sum_{i\\neq j} K_{ij}\\sqrt{p_i}$$, which corresponds to a sum of weighted similarities between one sample and the rest of the dataset. Then, for each eigenvalue \u03b7ip, there exists a row index j\u2208 {1,...,N} such that\n$$|\\eta_{ip}-p_j| \\leq \\sqrt{p_j} C_j$$\nVarga (2011) additionally states in Theorem 1.6 that if a set of L discs is disjoint from all other discs, it must contain L eigenvalues. As a result, if there exists a single sample xj with disc centered at pj that is disjoint from all other discs and is not within \u221apjCj of the eigenvalue interval [\u03b7min, \u03b7min + \u03f5], it would contain an eigenvalue that violates our uniformity assumption. We therefore expect all discs to be tightly clustered around the eigenvalue interval.\nIn order to construct such discs, the highest probabilities pj must be assigned to the samples xj with the smallest weighted row-sums Cj. Otherwise, any disc with small Cj and pj will have a small radius and be far away from the eigenvalue interval, creating a disjoint disc. Since samples with low Cj are those that are most distinct from the rest of the dataset, particularly other high-probability samples, assigning high probability to them ensures the rarest samples receive the greatest weight in the optimal p*."}, {"title": "", "content": "Details on the Vendiscope's gradient-based optimization algorithm, its implementation, and scalability are provided in Methods.\nDetecting rare elements. We call rare elements those data points that contribute most to the diversity of the collection. As demonstrated earlier, these are the data points to which the Vendiscope assigns the highest probabilities. Our experiments also show that these data points tend to be the ones that models may struggle to predict. Indeed, we find that the data points that are assigned the lowest probabilities by the Vendiscope yield the best model predictions.\nDetecting duplicates. Duplicates in data will contribute to the diversity of a dataset almost identically. These duplicates should therefore have very similar probabilities. This insight motivates how we detect duplicates in Algorithm 2. Importantly, we do not need to calculate all the N\u00b2 pair-wise similarities in the data and can instead focus on data points that the Vendiscope assigns similar probabilities. More specifically, we find redundant data points by only computing similarities between each data point and its m closest neighbors, where closeness is measured using the assigned probabilities from the Vendiscope. Choosing m large comes at a higher computational cost. We find that values of m in the order of 1-2% of the size of the dataset are sufficient for analyzing large-scale datasets with hundreds of millions of data points. At this scale, the Vendiscope can identify over 95% of all duplicates at only a fraction of the cost of computing all pairwise similarities.\nDetecting memorization. A naive way to detect memorized data points from a collection of outputs from a generative model is to compare each output with all the elements in the training set. This has a huge computational cost for large training sets. Our empirical study suggests a more scalable and systematic approach for detecting memorized data points: apply the Vendiscope to the generated collection and inspect outputs that are assigned the lowest probabilities by the Vendiscope. We find across 13 generative models spanning different model classes that the outputs with the lowest probabilities have higher similarity with the training set, with similarities reaching close to 1 for generative models with higher perceptual quality. Pasarkar and Dieng (2024) found that image generative models with high human error rates\u2014those whose outputs are harder to differentiate from real images by human judges\u2014are those that memorize training data and produce duplicates around those points. Our findings in this paper confirm that and go further to characterize which samples are memorized: the ones that contribute least to diversity. The finding also holds when applying the Vendiscope to the training set instead of the generated collection. The training points assigned the lowest probabilities by the Vendiscope have higher similarities with outputs from the generative models. These findings have implications for discovery but also issues related to copyrights and privacy. Finally, they also suggest the Vendiscope can help detect data points that may be prone to memorization even before training."}, {"title": "Analyzing the Universe of Protein Sequences", "content": "The number of annotated protein sequences has increased 100x over the past two decades. Today, the UniProt database, also referred to as the protein universe, contains 250 million annotated sequences. ML models such as AlphaFold, ProtBert,"}, {"title": "Analyzing The Materials Project Database", "content": "Next, we use the Vendiscope to analyze the composition of the Materials Project database (v2024.12.18). The Materials Project is the result of a significant computational effort to calculate the properties of many materials. This database has been instrumental in training ML models for materials property prediction and continues to grow. The prioritization of which materials are added has significant implications for the quality of future models. Using the Vendiscope on three popular models\u2014ALIGNN, CGCNN, and DeeperGATGNN-we characterize the materials in the Materials Project, reveal potential biases within the database, and identify patterns of model failure for property prediction. Details for model training are provided in the Appendix.\nProperty prediction accuracy degrades on materials that enhance diversity. The three selected models all achieved state-of-the-art property prediction performance at the time of their publication. However, they all fail to model the same types of materials: the ones that enhance diversity.\nWe trained each model to predict formation energy and band gap. We then extracted embeddings from each model by using the output from the layer just before the final prediction layer and used these embeddings as materials representations for the Vendiscope. In Figure 5, we show that the error associated with formation energy prediction is significantly higher for rare materials. The rare materials, as shown in 5, tend to have a smaller number of sites in their unit cell.\nTo characterize model behavior further, we partitioned materials into conductors (band gap = 0 eV) and non-conductors (band gap \u2260 0). Applying the Vendiscope to the embeddings from each group separately shows that model performance worsens significantly on rare materials. Across all models, rare materials are shown to have distinct physical properties from their bottom-scoring counterparts. One-sided Mann-Whitney U tests confirm that rare non-conductors have lower band gaps than common materials. The tests also confirm that rare conductors have large energies above the hull for both the CGCNN and DeeperGATGNN models.\nOur results confirm previous work by Li et al. (2023), who also observe strong performance on redundant materials and poorer performance elsewhere.\nThe Vendiscope detects duplicate crystals in the Materials Project database. We also apply the Vendiscope to detect near-duplicates in the Materials Project database. We consider two embedding spaces for the Vendiscope. The first embedding space"}, {"title": "Analyzing State-Of-The-Art Generative Models", "content": "Here, we apply the Vendiscope to CIFAR-10, a dataset containing 50,000 images across 10 classes. This dataset has become a popular benchmark for training image generative models. Gaining insights into the contents of CIFAR-10 and understanding how individual images affect model performance can lead to improved modeling efforts. Using the Vendiscope, we identify the near-duplicates present in CIFAR-10 and in the outputs of generative models trained on it. We also leverage the Vendiscope to study memorization in 13 state-of-the-art generative models spanning different generative modeling frameworks.\nDetecting duplicates in CIFAR-10. CIFAR-10 is known to contain many duplicates and near-duplicates . The presence of duplicates is known to"}, {"title": "Detecting memorization", "content": "Memorization is an undesirable property of generative models, although its causes are not yet well understood. We use the Vendiscope to provide insights into the types of samples that models tend to memorize.\nWe compare the weight assigned by the Vendiscope to each training data point against its similarity to the generated output its most similar to. Memorized data points will have high similarity with one or more outputs from the generative model. Across all 13 generative models, we find a strong negative correlation between the rarity of a training data point and its degree of memorization (Fig. 8). Figure 9 shows that the samples that contribute least to the diversity of the generated outputs, i.e. those ranked lowest by the Vendiscope in terms of diversity contribution, are the ones that have highest similarity with the training set. The trend is clear: image generative models tend to memorize data points that contribute least to the diversity of the training set. This behavior has been also noted by Webster et al. (2021) in the context of image generative models for human faces. Their study found that models trained on low-diversity datasets are prone to recreate the images from over-represented identities. We additionally find that models trained on CIFAR-10 which memorize common images the most achieve higher image quality scores as measured by human error rate. (We refer the reader to Figure 10 in the appendix)."}, {"title": "Related Work", "content": "The Vendiscope offers a range of capabilities, including the detection of outliers, duplicates, samples prone to memorization, and samples that models may struggle to predict prior to any training. It also ranks data points according to their rarity. Related works only tackle a subset of these tasks, typically in specific domains, and we discuss them below.\nNear-duplicate detection. Several methods have been developed to detect duplicates in specific domains, e.g. proteins and text. MMSeqs2 is a highly-parallelizable method for detecting duplicates in protein sequences using an approximate matching of k-mers across sequences. While MMSeqs2 is fast in practice, it can be inaccurate. We observed this inaccuracy on the UniProtKB dataset in our experiments, as discussed earlier. This limitation of MMSeqs2 is partially because proteins with matched k-mers do not necessarily describe homologs or functions. As a result, the duplicate sequences identified by MMSeqs2 are less informative. knnProtT5 is an alternative method that relies on k-nearest neighbor search on protein embeddings . However, this has a higher complexity of O(NlogN) compared to the Vendiscope and MMSeqs2, which have linear complexity.\nA popular approach for detecting duplicates in text data is the MinHash-based Locally Sensitive Hashing (LSH) algorithm. This method estimates the Jaccard similarity between two documents using a series of hashing functions, which makes it scalable and amenable to various streaming optimizations. However, MinHash does not account for semantic similarity and is thus sensitive to typos and other adversarial attacks on text. Another duplication"}, {"title": "", "content": "detection method for text is RETSim, which trains a lightweight encoder model for near-duplicate detection . While the results are quite encouraging, it is not feasible for researchers to train a specialized model for each potential use-case.\nThe Vendiscope can detect near duplicates at scale and applies to any domain where similarity can be defined, which makes it more general. Furthermore, the Vendiscope offers additional capabilities for analyzing data collections beyond duplication detection.\nDetecting memorization in generative models. Significant efforts have been made to identify the causes of memorization in generative models, and this topic continues to attract considerable attention from the ML community. Kandpal et al. (2022) and Lee et al. (2021) found that the presence of duplicates can be a potential cause of model memorization. However, memorization can occur even in the absence of duplicates . Overfitting has also been thought to contribute to memorization; however, Tirumala et al. (2022) found examples of memorized training samples from models that had not yet overfit. Moreover, Jagielski et al. (2022) attributed memorization to the order in which data is presented to a model, with samples presented last being more prone to memorization. The Vendiscope provides a new framework for reasoning about memorization in ML: memorization is closely linked to the contribution of samples to diversity. Our analysis of 13 different image generative models shows that samples contributing more to diversity are less likely to be memorized, while those prone to memorization contribute the least to diversity. Moreover, by identifying a correlation between memorization and diversity, the Vendiscope adds an extra capability: it can detect data points likely to be memorized by models even before training. This is possible because the Vendiscope can be applied to any collection, whether it's the training set or outputs from a generative model.\nAn additional challenge in studying model memorization is the lack of reliable metrics. Existing metrics, such as Cr, calibrated 12 distance, and label memorization, often require significant tuning or can only detect specific forms of memorization . Alternatives include manually feeding prompts from the training data to the model in an attempt to identify memorized samples. However, selecting which prompts to feed is largely ad-hoc and relies on heuristics such as prompt length. The Vendiscope can guide such a prompting strategy by identifying clusters of data that are likely to be memorized.\nCharacterizing large-scale datasets. Certain datasets, such as The Stack, FineWeb, C4, and RedPajama, have become staples for training large language models. However, the contents of these datasets are not well understood, and very few works have attempted to address this gap. Penedo et al. (2024) conducted extensive ablation studies to justify the curation strategies behind FineWeb, yet the dataset itself remains largely opaque beyond a high-level topic analysis and a word-association bias study . Elazar et al. (2023) analyzed numerous benchmark text datasets, revealing statistics such as the most common n-grams, the diversity of text sources, and the number of exact dupli-"}, {"title": "", "content": "cate documents within each dataset. The overall pipeline enables searches for the amount of personally identifiable information (PII) present in each dataset. Zhong et al. (2024) also aimed to improve vision and language dataset interpretation by identifying common topics present in the dataset. The Vendiscope provides a more comprehensive analysis of the composition of a data collection and is applicable beyond documents and images.\nVendi scoring. The Vendi Score (VS) and the probability-weighted Vendi Score (PVS) were introduced by Friedman and Dieng (2023) and quantify the diversity of a collection of elements. Pasarkar et al. (2023) demonstrated the utility of optimizing the VS in molecular dynamics simulations, finding that jointly optimizing the VS and the energy of a molecule significantly speeds up and diversifies the exploration of complex energy landscapes. Berns et al. (2023) optimized the sum of the pVS and the Shannon entropy of the probabilities involved in the computation of the pVS to balance the modes of generative models, enhancing their ability to produce diverse outputs in artistic domains. The VS has been extended and applied in multiple ways, owing to its flexibility. The Vendiscope maximizes the pVS and enables scalable detection of outliers, duplicates, samples prone to memorization, and samples that models may struggle to predict, even before training. It also ranks the data points in a given dataset according to their rarity and provides per-data-point weights that can aid in data processing, model training, and evaluation."}, {"title": "Discussion And Conclusion", "content": "In this work, we introduced the Vendiscope, a novel computational tool designed to enhance our ability to analyze large, complex datasets. By leveraging the probability-weighted Vendi scores, the Vendiscope enables researchers and practitioners to systematically measure and weigh the contribution of individual data points to the"}, {"title": "", "content": "overall diversity of a dataset. This simple capability in turn can serve to detect outliers, duplicates, samples prone to memorization, and samples that models may struggle to predict. This has profound implications for discovery and AI.\nOur results reveal several key insights. First, we showed that, in the context of the protein universe, a vast majority of sequences\u2014over 200 million out of 250 million-are near-duplicates, underscoring the redundancy inherent in biological data. Importantly, we demonstrated that existing ML models, such as AlphaFold, struggle to predict proteins associated with Gene Ontology (GO) functions that contribute most significantly to the diversity of the dataset. This provides insights into the limitations of current methods and emphasizes the need for more nuanced data collection, model development, and evaluation in computational biology.\nSimilarly, our application of the Vendiscope to the Materials Project database revealed that over 85% of crystals with formation energy data are near-duplicates. Moreover, we found, similarly to the finding we had with the protein universe, that ML models often perform poorly on the materials that enhance diversity, an insight that could guide future materials discovery and design processes. These findings emphasize the importance of considering diversity in both data collection, model development, and evaluation.\nOne of the most compelling aspects of the Vendiscope is its ability to characterize data points that are most prone to memorization in generative models\u2014a pressing concern in AI. Our results show that the best-performing generative models often memorize training samples that contribute the least to the overall diversity of the dataset, revealing a new aspect of model behavior. By identifying data points that are prone to memorization even before training takes place, the Vendiscope presents a powerful means of improving AI model development.\nBeyond its applications in individual domains, the Vendiscope provides a unified framework for analyzing complex data at scale. It helps identify duplicates, outliers, poorly represented data points, and samples that may challenge models\u2014even before training begins. These capabilities offer significant benefits for data pre-processing, model development, and evaluation. Researchers, engineers, and data auditors can use the Vendiscope to audit datasets, identify potential biases, and refine data collection practices. For AI ethicists, the Vendiscope offers a critical lens to understand how models interact with data, particularly in the context of bias, memorization, and data fairness, enabling better mitigation strategies to prevent undesirable outcomes in AI deployment. For scientists, the Vendiscope represents a new companion in the discovery process.\nThe Vendiscope has the potential to reshape how we approach data preprocessing, model training, and evaluation. Moreover, the tool's ability to scale-working with datasets as large as 250 million protein sequences\u2014makes it an invaluable asset for a wide range of researchers and engineers across disciplines. By offering a way to systematically analyze data at scale, it helps unlock new possibilities for scientific discovery, model development, and ethical AI."}, {"title": "Methods", "content": "The Vendiscope enables scalable analysis of large data collections across disciplines by quantifying the contribution of each element to the overall diversity of the collection. Below, we describe the algorithms and implementation details that drive its efficiency.\nScalability in time and space. Each iteration of the Vendiscope requires calculating the pVS for a collection of n elements, which involves computing the eigenvalues of an n\u00d7n matrix. This process has a time complexity of O(n\u00b3). However, Friedman and Dieng (2023) indicated that when data embeddings are available the VS, and also the pVS, can be computed cheaply by using a cosine similarity kernel with corresponding similarity matrix K = XTX, where X \u2208 Rn\u00d7d denotes the data embedding matrix. In this case the complexity is reduced to O(d\u00b2n). The improvement in complexity enables the scaling of the Vendiscope to large collections where n\u226b d.\nOnce the pVS and its gradients are computed, we perform projected gradient descent. We use the active set method described in Michelot (1986), which has an observed runtime of O(n) (Condat, 2016). There are certain settings for which the runtime can be quadratic, but we will not encounter such instances when initializing most weights to be similar. Empirically, sufficiently small learning rates ensure linear runtimes. We reach a time complexity of O(d\u00b3 + d2n + n) = O(d\u00b2n) and a space complexity of O(dn) for each iteration of the Vendiscope.\nScaling to very large datasets with parallel computing. As presented, Algorithm"}, {"title": "", "content": "1 would require the entire dataset to be loaded into memory. This is prohibitively expensive for many of the massive datasets available today. We circumvent this problem by estimating the pVS using only a subset of the data's dimensions at each iteration. In particular, at a given iteration t, we sample a random subset of the columns of the data matrix, dt \u2286 {1,...,d} and use Xtdt \u2208 Rnx|dt| instead of the entire dataset X. This approach provides an approximation of the true pVS.\nOur subsampling approach also allows us to take advantage of parallel computing to speed up the Vendiscope. Indeed, we can use data parallelism across multiple GPUs to iterate through all dimensions faster. Each GPU maintains a copy of the Vendiscope weights and is given a separate set of data dimensions for which the GPU computes local gradients. Then, the gradients are averaged across GPUs, creating global gradients that are communicated back to each GPU. All the GPUs will therefore have the same gradients, and we can update the Vendiscope weights across each GPU.\nAlgorithm 2 is also amenable to computing optimizations. After computing the Vendiscope weights, we can divide the dataset across independent processes and run Algorithm 2 on each subset separately. Because we only compare a given sample against a small subset of the entire dataset, we are not required to ever load the entire dataset into memory at once. Finally, we can employ batch processing and process multiple data points simultaneously by taking advantage of fast GPU matrix operations.\nInitialization and identifiability. In Algorithm 1, we initialize the weights to be equal, reflecting, in a Bayesian sense, an uninformative prior over the Vendiscope's probabilities. This choice allows the Vendiscope to assign identical weights to exact duplicates. A random weight initialization would cause issues with the identifiability of exact duplicates due to the optimization of the pVS. For exact duplicates, an optimized PVS only places a constraint on the sum of their scores. Consider, for instance, a collection with 3 elements and the kernel matrix with the first column x1 = [1 0 0] and identical second and third columns x2 = x3 = [0 1 1]T. In this setting, the pVS can be maximized with p1 = 0.5 and p2 + p3 = 0.5, yielding an optimal pVS of 2. If we initialize the weights to be equal, p2 and p3 will have identical gradients throughout the iterations and will remain equal."}, {"title": "", "content": "Hyperparameter selection and convergence criteria. The Vendiscope requires specifying the order q > 0 of the pVS. Previous work by Pasarkar et al. (2023) demonstrated that small values of q, are more sensitive to rare features in the collection, whereas large values of q place greater emphasis on more common features. We find that choosing small values of q helps avoid the sparsity issue that may arise in large collections, where most data points have weight zero due to limitations in numerical resolution. We use q = 0.1 and q = 0.5 in our experiments.\nWe use a convergence criterion based on the learned ranking of the data points in the collection and stop the Vendiscope when the ranking stabilizes. In all of our studies, we found this typically occurs within 500 iterations."}, {"title": "Code and Data Availability", "content": "We provide a smaller version of the UniProtKB database, curated using the Vendiscope for de-duplication. Additionally, we publish the learned Vendiscope weights for each protein in UniProtKB. Both the data and the weights can be downloaded from the Vertaix Drive.\nAll code and model checkpoints will be made available upon publication."}, {"title": "Appendix", "content": "1.1 Material Property Prediction Model Training\nWe train 3 models on the Materials Project (v2024.12.18). We use the recommended settings from each model for pre-processing crystal structures. We therefore use a cutoff radius of 8 \u00c5 for constructing graphs for CGCNN and DeeperGATGNN, and 4 \u00c5 for constructing graphs for ALIGNN. We sweep over hyperparameters such as the number of hidden layers and hidden dimensions before training models on the entire dataset. All models are trained to convergence: CGCNN uses 1000 epochs with batch-size 256, DeeperGATGNN uses a batch-size of 100 for 400 epochs, and ALIGNN uses a batch-size of 16 for 300 epochs. We use the model checkpoint at the final epoch for all downstream analysis.\n1.2 Additional Image Generative Model Analysis\nWe studied popular model architectures, including diffusion models, GANs, VAEs, and flows. In all, we tested 8 GAN models: ACGAN , BigGAN , LOGAN , ReACGAN , MHGAN, WGAN-GP , StyleGAN2-ada , and StyleGAN2-XL . Additional models tested include NVAE , RESFLOW , and the three diffusion models iDDPM-DDIM  PFGM++ and LSGM-ODE .\nIn Figure 9, we show some examples of the varying degrees of memorization for rare and common samples on the iDDPM-DDIM model from Nichol and Dhariwal (2021). The rare samples in the training dataset are not represented in the generated dataset, whereas the model generates almost exact replicas of common samples.\nWe also find that models that memorize training data are those that generate the highest-quality images (Figure 10). These models memorize common images. Models such as LOGAN that do not memorize, do so at the cost of producing images of lower quality as measured by human error rate. Finally, in Figure 11, we run the Vendiscope on the generated outputs from each model. We find that the most common images generated by each model are those that have high similarity with the training data."}]}