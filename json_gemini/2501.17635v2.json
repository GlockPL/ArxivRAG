{"title": "In-Context Meta LoRA Generation", "authors": ["Yihua Shao", "Minxi Yan", "Yang Liu", "Siyu Chen", "Wenjie Chen", "Xinwei Long", "Ziyang Yan", "Lei Li", "Chenyu Zhang", "Nicu Sebe", "Hao Tang", "Yan Wang", "Hao Zhao", "Mengzhu Wang", "Jingcai Guo"], "abstract": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LORA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LORA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LORA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1% storage compared with the original LoRA.", "sections": [{"title": "1 Introduction", "content": "Large-scale models (LLMs/MLMs) have become the cornerstone of modern AI applications [Achiam et al., 2023; Xiao et al., 2024; Dubey et al., 2024; Shao et al., 2024]. However, these models typically require substantial amounts of data for fine-tuning. We always fine-tune LLMs with Low-rank Adaptation (LoRA) [Hu et al., 2021a] using task-specific data. In scenarios with numerous sub-tasks [Erko\u00e7 et al., 2023; Yan et al., 2024], the current approach of training a separate LoRA for each sub-task leads to inefficiency in storage and inference. For instance, in multi-task scenarios, the weights of LoRA can become prohibitively expensive to store, necessitating more efficient solutions. Although FM-Delta [Mizrahi et al., 2017] employs a novel compression scheme that significantly reduces the need for storage by storing compressed fine-tuned models, it does not address the issue of capturing correlations between sub-tasks.\nThe current parameter generation methods [Platanios et al., 2018; Wortsman et al., 2022; Jin et al., 2024] can only implement the generation of LoRA parameters for a single task, and it is not possible to implement the simultaneous generation of LORA weights required for different tasks through only one generator. Moreover, the current parameter generation training method lacks context modeling capability, which makes it difficult to implement multi-use multi-task enhancement of LORA weights. This causes a great storage burden when storing LoRA weights and training data.\nTherefore, we propose the In-Context Meta LoRA (ICM-LoRA) to implement generating different tasks LoRA with self-designed generator, Conditional Variational Autoencoder (CVAE). As shown in figure 1, ICM-LoRA utilizes task vectors for context modeling through in-context meta learning, allowing the augmented CVAE to learn the parameter distribution features. By combining in-context learning and meta learning and using task vectors as modeling labels, we achieve meta enhancement for LoRA parameters. Furthermore, ICM-LORA can get rid of the dependence on data and storage and only need to use a generator to implement the parameter generation.\nWe evaluate our method on both text and visual tasks on different models. For visual tasks, we select target detection tasks and use the COCO dataset [Lin et al., 2014] to classify subclasses based on the detection task labels for experiments. For language tasks, we choose The Pile [Gao et al., 2020] as the training corpus and use five different subsets of it to simulate multi-class training tasks and validate the model on the validation set. The results indicate that CVAE could generate different tasks' LoRA parameters successfully. Compared to current methods, the generated LoRA parameters gain less accuracy loss. In addition, compared to original datasets and LORA weights, our generator significantly reduce the storage. In summary, the contributions of our approach can be summarized as follows:\n1) We propose a novel framework, In-Context Meta LORA (ICM-LoRA), which uses a self-designed parameter generator, Conditional Variational Autoencoder (CVAE), to generate LoRA weights, addressing the inefficiency of training separate LoRA models for multiple sub-tasks.\n2) We employ in-context meta-learning for knowledge enhancement and task matching, which enables the generator to better learn the correspondence between tasks and model parameter distributions.\n3) Compared to existing methods, CVAE can generate task-specific LoRA parameters as same as the original LORA or even better than the original LoRA. Also, our ICM-LORA could cost only 1% storage compared with original datasets."}, {"title": "2 Related Works", "content": "The core of parameter generation targets to help model generate similar distribution with original model. As one of the pioneers, [Platanios et al., 2018] introduced a contextual parameter generator (CPG) addressing the challenge of training separate models for each language pair in neural machine translation (NMT). Some methods like stochastic neural networks [Sompolinsky et al., 1988; Bottou and others, 1991; Wong, 1991; Schmidt et al., 1992; Murata et al., 1994; Graves, 2011] and Bayesian neural networks [Neal, 2012; Kingma, 2013; Rezende et al., 2014; Kingma et al., 2015; Blundell et al., 2015; Gal and Ghahramani, 2016] improved the robustness and generalisation of the model through the prior probability distribution of the parameters, but these methods performed poorly in large-scale or complex scenarios. HyperNetworks [Ha et al., 2016] generates parameters of large networks through small networks. With the development of diffusion, methods such as G.pt [Peebles et al., 2022] and P-diff families [Hu et al., 2021b; Zhao et al., 2021] began to use diffusion to generate normal scale parameters, but they are limited in generating parameters too large or too small. Furthermore, COND P-DIFF [Jin et al., 2024] first applies parameter generation to generate LoRA parameters, but it only generates Lora models for coarse-grained tasks, and its parameters for generating LoRA for fine-grained tasks do not perform well. Therefore, we design a fine-grained task Lora generator which use in-context learning (Sec. 2.2) to enhance the ability of context understanding of generator model such as diffusion."}, {"title": "2.1 Parameters Generation", "content": "The core of parameter generation targets to help model generate similar distribution with original model. As one of the pioneers, [Platanios et al., 2018] introduced a contextual parameter generator (CPG) addressing the challenge of training separate models for each language pair in neural machine translation (NMT). Some methods like stochastic neural networks [Sompolinsky et al., 1988; Bottou and others, 1991; Wong, 1991; Schmidt et al., 1992; Murata et al., 1994; Graves, 2011] and Bayesian neural networks [Neal, 2012; Kingma, 2013; Rezende et al., 2014; Kingma et al., 2015; Blundell et al., 2015; Gal and Ghahramani, 2016] improved the robustness and generalisation of the model through the prior probability distribution of the parameters, but these methods performed poorly in large-scale or complex scenarios. HyperNetworks [Ha et al., 2016] generates parameters of large networks through small networks. With the development of diffusion, methods such as G.pt [Peebles et al., 2022] and P-diff families [Hu et al., 2021b; Zhao et al., 2021] began to use diffusion to generate normal scale parameters, but they are limited in generating parameters too large or too small. Furthermore, COND P-DIFF [Jin et al., 2024] first applies parameter generation to generate LoRA parameters, but it only generates Lora models for coarse-grained tasks, and its parameters for generating LoRA for fine-grained tasks do not perform well. Therefore, we design a fine-grained task Lora generator which use in-context learning (Sec. 2.2) to enhance the ability of context understanding of generator model such as diffusion."}, {"title": "2.2 In-Context Learning", "content": "In-Context Learning (ICL) has emerged as a powerful paradigm in machine learning. As a pioneering work, [Brown et al., 2020] reveals for the first time the learning ability of large language models in the presence of a small number of examples. Building upon this, for training LLMs, MetaICL [Min et al., 2021] integrated tasks into ICL format and enabled models to reach performance similar to direct fine-tuning. Lamda [Thoppilan et al., 2022] emphasizes instruction tuning for models to understand better task descriptions instead of just examples. Self-instruct [Wang et al., 2022b] lets LLMs generate instructions for task alignment to explore enhancing ICL, while [Wei et al., 2022] introduced Chain-of-Thoughts (CoT) as an intermediate step between input and output to boost LLM reasoning in ICL. Task seperation ICL like Self-Ask [Press et al., 2022] and ICAP [Chi and Wylie, 2014] have explored multi-stage ICL, where tasks are broken down into simpler sub-tasks, each with its own set of demonstrations, and LLMs can process them individually. SuperICL [Xu et al., 2023] utilizes smaller models as plugins to effectively execute tasks within the LLM framework, demonstrating the potential for hybrid model approaches in ICL. In scenario understanding, In-Context LoRA (IC-LORA) [Huang et al., 2024] and [Hendel et al., 2023] respectively apply ICL on image generation by diffusion and context classification by LLM. In our paper, we apply ICL to generator to help generator better understand the context information in Lora."}, {"title": "2.3 Dataset Condensation", "content": "Dataset Condensation (DC) aims to create a compact and representative subset of the original training data. As a foundational work, [Zhao et al., 2020] first tried to compress the data by matching the gradients of the synthetic dataset with the original, to ensure the condensed dataset retains the essential characteristics for effective model training. [Zhao and Bilen, 2021] further achieves more efficient data enhancement to synthesise more informative synthetic images by using differentiable twin enhancement (DSA). Building on it, [Zhao and Bilen, 2023] also explores DC with Distribution Matching, optimizing synthetic data to match the original distribution in embedding spaces via maximum mean discrepancy (MMD). [Wei et al., 2024] compresses data by matching latent space quantiles and minimizing distribution fit statistics and [Lee et al., 2022] further advances the field by modifying the loss function to capture class differences with a bi-level warm-up strategy for stable optimization. [He et al., 2024] integrates multiple dataset compression processes to get datasets of various sizes and introduced adaptive subset loss to reduce subset degradation. [Wang et al., 2022a] proposes a new method for dataset compression by aligning features, while [Liu et al., 2024] introduces a dual-domain matching method for dataset condensation in time series classification, which further extends the applicability of DC to different data types. In our work, we discard the original dataset and deposit the different task information of the dataset into the generator model for data information aggregation and compression."}, {"title": "3 Methodology", "content": "In this section, we present our approach in terms of overview, task vector extraction, and parameter sampling and reconstruction for model customization."}, {"title": "3.1 Overview", "content": "As shown in Figure 2, we extracted the final hidden states of five different categories, dog, sofa, cat, bicycle and motorbike, at the last time step from the last layer of Florence-2's [Xiao et al., 2024] decoder and visualized them with S-NE [Van der Maaten and Hinton, 2008]. The visualization demonstrates that the hidden states from different categories form distinct clusters. For simplicity, we refer to the final hidden states at the last time-step from the last layer of decoders as 'task vectors' [Hendel et al., 2023].\nFrom the information mentioned above, we can draw two conclusions: first, task vectors from different categories are discriminative, as they form distinct clusters. Second, task vectors can represent the high-level features of different categories, given that the last layer and the final time step typically capture high-level representations.\nTherefore, task vectors satisfy the two key properties of condition vectors: discriminability and representativeness. Based on this, we hypothesize that task vectors can serve as condition vectors to effectively guide the generation process in the CVAE model.\nAs shown in Figure 3, we provide an illustrative overview of the proposed method. Our method consists of three parts.\n1) Preparing LoRA parameter data and extracting task vectors. We fine-tune a Large Language Model (LLM) or a Large Vision-Language Model (LVLM) on a specific task category and save checkpoints from the final stages of the training process. These checkpoints serve as training data for the subsequent generative model. Next, we perform inference using the fine-tuned model on randomly selected samples from a specific task category, such as cat detection or dog detection. During inference, we extract the hidden states from the last layer at the final time step. These hidden states are then averaged to derive a task vector representing the specific task category.\n2) Training the CVAE model. The LoRA parameters extracted from the fine-tuned model checkpoints, along with the task vectors, are used as training data to train a Conditional Variational Autoencoder (CVAE). We use in-context meta learning to implement CVAE for modeling relationships between multiple tasks and to achieve enhancement of LoRA parameter distribution learning.\n3) Generating and applying LoRA parameters. Using the trained CVAE, we sample from a Gaussian distribution to reconstruct the LoRA parameters for the target task category. The reconstructed LoRA parameters are then used to perform inference on the test set, enabling the model to generalize to the specific task effectively."}, {"title": "3.2 Task Vector Extraction", "content": "Considering the contextual capabilities of large-scale models and evidence that in-context learning can generate task-specific representations [Hendel et al., 2023], we fine-tune a pre-trained model by LoRA [Hu et al., 2021a] on a specific task category.\nDue to the hidden state corresponding to the last token, we extracted the hidden state of each sample $h_{last}$ with the last token generated by LLM. For a specific set of task samples {$x_i$}$_{i=1}^{N}$, LLM generates hidden states of the task from the last layer. These hidden states are then averaged to produce a compact task vector which could be expressed as Eq. (1):\n$U_{task} = \\frac{1}{N} \\sum_{i=1}^{N} h_{last}$  (1)\nwhere N is the number of task-specific samples, and $U_{task} \\in \\mathbb{R}^{d}$ represents the task vector for the given category, and d represented the dimensionality of the hidden state.\nSince the last time-steps of many natural language processing scenarios contain complete information about the input sequence, we choose the last token to generate the hidden state $h_{last}$. For the task of mapping an input sequence to a single vector, the hidden state of the last marker typically integrates information from all previous time steps, thus becoming a compact representation of the overall semantics of the input $x_i$.\nFurthermore, we extract the hidden states from the final layer because this layer typically represents the most abstract and task-specific feature space. As information flows through the network, the first few layers typically encode general linguistic or structural features, while the last layer captures high-level semantic features specific to the current task. This recursion allows the final layer to act as a task feature extractor, providing a representation that is well suited for generating task vectors. The abstract nature of the last layer ensures that the generated task vector $X_{task}$ can effectively capture the key features of the task."}, {"title": "3.3 Conditional Variational Autoencoder", "content": "Based on the Variational Autoencoder (VAE) [Kingma, 2013], we employ a Conditional Variational Autoencoder (CVAE) to model the distribution of LoRA parameters conditioned on task vectors. The CVAE consists of an encoder $q_{\\phi}(z | l, U_{task})$, which maps the LoRA parameter l and task vector $U_{task}$ to a latent representation z, and a decoder $q_{\\theta}(z | l, U_{task})$, which reconstructs I from z and $U_{task}$. The encoder and decoder are conditioned on the task vector $U_{task}$, which provides additional information to guide the generation of LoRA parameters.\nThe encoder $q_{\\phi}(z | l, U_{task})$ models the approximate posterior distribution over the latent variables z, given the LoRA parameter l and the task vector $V_{task}$. The encoder takes as input the concatenation of the LoRA parameter l and the task vector $U_{task}$, which is denoted as Eq. (2),\n$X = [l; U_{task}] \\in \\mathbb{R}^{d_l + d_{task}}$, (2)\nwhere $d_l$ and $d_{task}$ represent the dimensions of the LoRA parameter l and the task vector $U_{task}$, respectively.\nThe concatenated vector x is passed through a neural network, which outputs the parameters of the approximate posterior distribution. This process can be expressed as Eq. (3),\n$q_{\\phi}(z | l, U_{task}) = N (z; \\mu_{\\phi}, \\sigma_{\\phi}^2 (x))$, (3)\nwhere $\\mu_{\\phi} (x)$ and $\\sigma_{\\phi}^2(x)$ are the mean and variance of the latent variable z, computed from the input x.\nThe latent variable z is then sampled from this distribution using the reparameterization trick, which could be represented as Eq. (4):\n$z = \\mu_{\\phi} (x) + \\sigma_{\\phi} (x) \\odot \\epsilon$,  (4)\nwhere $\\epsilon \\sim N (0, I)$ is a noise term and $\\odot$ represents element-wise multiplication.\nThe decoder $p_{\\theta}(l | z, U_{task})$ models the likelihood of the LORA parameter I given the latent variable z and the task vector $U_{task}$. The decoder receives z and $U_{task}$ as input, which are concatenated as Eq. (5),\n$x' = [z; U_{task}] \\in \\mathbb{R}^{d_z + d_{task}}$, (5)\nwhere $d_z$ is the dimensionality of the latent variable z.\nThe concatenated vector x' is then input into a neural network to output the parameters of the likelihood distribution for the LORA parameter l, the process could be represented as Eq. (6):\n$p_{\\theta}(z, U_{task}) = N (l, \\mu_{\\theta}(x'), \\hat{\\sigma}_{\\theta}^2 (x'))$, (6)\nwhere $\\mu_{\\theta}(x')$ and $\\hat{\\sigma}_{\\theta}^2 (x')$ are the mean and variance of the predicted LoRA parameter l, computed from the input x'.\nThe decoder aims to minimize the reconstruction error, ensuring that the generated LoRA parameters match the true parameters as closely as possible. The latent space $z \\in \\mathbb{R}^k$ is assumed to follow a Gaussian prior distribution which could be represented as Eq. (7):\n$p (z) = N (z; 0, I)$, (7)\nwhere I is the identity matrix.\nThe objective is to maximize the evidence lower bound (ELBO), which consists of two terms: the reconstruction term and the regularization term. The ELBO could be expressed as Eq. (8),\n$\\mathcal{L} = E_{q_{\\phi}(z|l, U_{task})} [log p_{\\theta}(l | z, U_{task})] + \\\\ KL (q(z | l, U_{task}) || p(z))$, (8)\nwhere KL( || ) represents the Kullback-Leibler divergence. The first term encourages the decoder to reconstruct accurate LORA parameters, while the second term regularizes the latent space to match the Gaussian prior.\nBy conditioning both the encoder and decoder on the task vector $U_{task}$, the model learns to generate LoRA parameters that are specific to the given task, leading to task-aware representations in the latent space.\nIn order for CVAE to generate task LoRA parameters, we utilize CLIP's [Radford et al., 2021] text encoder to output the task vector $U_{task}$. A sample z is drawn from the prior distribution p(z), and the decoder generates the corresponding LORA parameter:\n$l_{generated} = p_{\\theta}(l | z, U_{task})$. (9)"}, {"title": "4 Experiments", "content": "In this section, we evaluated several tasks on LLMs and MLMs. We evaluated the task performance of the LORA [Hu et al., 2021a] generated by the current LoRA parameters generation methods. This demonstrates the effectiveness and reasonableness of our approach."}, {"title": "4.1 Experiment Setting", "content": "Baselines. We chose the original model, LoRA, LORA generated by Model Soup [Wortsman et al., 2022] and COND P-DIFF [Jin et al., 2024] as baseline to compare with our method and test the advantages of our method on different tasks.\nDatasets. For the computer vision task, we select the most representative target detection task as to conduct the experiment. We choose the COCO [Lin et al., 2014] dataset and divide it into different subclasses based on the detection task labels. For the language modelling task, we employ The Pile [Gao et al., 2020] as the training corpus. To simulate the multi-category training tasks, we pick five various subsets from the Pile corpus and validate our method on the test sets.\nData Preparation. The model fine-tuning process also produces a series of LoRA matrices {$L_t$}$_{t=1}^{T}$ with different ranks r, where T represents the number of fine-tuning steps. Each matrix $L_t \\in \\mathbb{R}^{m \\times n}$ is flattened into a one-dimensional vector $l_t \\in \\mathbb{R}^{m \\times n}$ to facilitate alignment with the task vector $U_{task}$. These flattened LoRA parameters, along with the corresponding task vectors, form the training dataset {($v_{task}$, $l_t$)} for the self-designed CVAE.\nTraining Strategies. The CVAE model employs a 12-layer 1D CNN architecture for both the encoder and decoder. The loss function for the CVAE combines the Kullback-Leibler divergence (KLD) and reconstruction loss, with the KLD weight set to 0.005. The loss function could be expressed as Eq. (8) We fine-tuned the model on a specific task using LoRA (Low-Rank Adaptation) for a total of 150 epochs, saving the LoRA parameters from the final 50 epochs. The task vector is extracted from the last token of the last layer in the CLIP [Radford et al., 2021]. Subsequently, the CVAE model is trained for 2,000 epochs to ensure robust learning of the latent space. All experiments were conducted on a single NVIDIA A800 GPU, with each experiment taking approximately 3 hours to complete.\nEvaluation Metrics. We choose to record different LORA ranks r and parameter numbers P in all experiment. For the object detection task, we chose to record Mean Average Precision (MAP \u2191) at thresholds IoU = 0.5,0.75. For the language modeling task, we chose perplexity (PPL \u2193) and bits-per-character (BPC \u2193) as the evaluation metrics."}, {"title": "4.2 Main Results", "content": "We conduct experiments on computer vision tasks and natural language processing tasks respectively, and demonstrate that our approach generalizes across models and can be adapted to tasks of multiple modalities.\nObject Detection. As shown in Table 1, we selected several subsets of the more conventional tasks and fine-tuned on Florence-2 [Xiao et al., 2024]. the LoRA parameters generated by ICM-LoRA in the subset of expert tasks in the COCO dataset have the smallest difference in effect from the original LORA parameters, and even the LoRA parameters generated by ICM-LoRA outperform the original LoRA in some of the tasks. this suggests that our method generates more LoRA parameters than the other methods. complete. By adding in-context learning, ICM-LoRA's understanding of task scenarios is enhanced compared to COND P-DIFF, implementing the effect of outperforming the original LoRA on some tasks.\nAs shown in Table 3, our method significantly achieves task-specific dataset compression using much less storage than the original LoRA weights with the original dataset. The compression of the visual dataset is achieved through the parameter generation method, which significantly reduces the storage cost. This shows that our approach not only generates task-corresponding LoRAs more accurately, but also enables task-based data compression.\nAs shown in Figure 4, both Model Soup and COND P-DIFF were poorly labeled in the detection, and COND P-DIFF even had a false detection in the complex environment. These phenomena indicate that the reconstructed LoRA and original LoRA parameters of Model Soup and COND P-DIFF are so different that they cannot be successfully adapted to LVLM.\nLanguage Modeling. When it comes to the language modelling task, we set the LoRA rank r = 2. We fine-tuned the Llama-3-8B [Dubey et al., 2024] model across different tasks and proposed the result of five subsets from the Pile corpus, including ArXiv, Books, Ubuntu, Wikipedia, and Gutenberg. As shown in Table 2, compared to other methods, ICM-LORA achieves the lowest perplexity and bits-per-character. This outperformance clearly shows its superiority in these specific tasks of language modelling.\nIn some subtasks, ICM-LORA achieved the same PPL and BPC as the original LoRA and even achieve lower PPL and BPC than the original LoRA. This indicates that even in language tasks, ICM-LORA can reconstruct the LoRA parameters and even reconstruct LoRA with more reasonable parameter distribution in some specific subtasks.\nSince Llama-3 [Dubey et al., 2024] and Florence-2 [Xiao et al., 2024] have different parameter distributions and model constructions, the LoRA construction has a different parameter distribution. We argue that ICM-LORA can adapt both multi-modal and language tasks, and can adapt different models with diverse parameter distributions. Therefore, we conclude that ICM-LORA is highly effective in enhancing the performance of language modeling for parameter generation."}, {"title": "4.3 Ablation Studies", "content": "In this section, we will first discuss the effect of different ranks and parametric quantities of LoRA on the generation of task LORA. Then we discuss the effect of different number of convolutional layers n on model performance during sampling. Finally, we discuss the impact on different task vector generated by CLIP's vision encoder and text encoder.\nImpact of LoRA Rank r and Parameter Number P. We trained the LoRA parameters with rank r of 1,2,4,8 respectively and trained CVAE using these LoRA parameters. The task LoRA parameters were generated based on the task vectors and evaluated on the COCO dataset. Meanwhile, we tested several other methods and discussed the effect of LORA parameter size on the generated LoRA parameters. We selected dogs and cats as examples and reported their MAP50.\nAs shown in Table 4 and Figure 5, as the rank r of LoRA gradually increases, the number of LoRA parameters also increases. The other methods gradually decrease the effect on LORA reconstruction as the number of LoRA parameters increases, which indicates that these methods cannot adapt to the reconstruction of LoRA with a large number of parameters. The detection effect of our method is almost the same as the original LoRA with the increase of LoRA parameters, which indicates that our method is more robust and can adapt to the reconstruction of LoRA weights with different numbers of parameters.\nImpact of Convolutional Layers Number. We evaluate the effect of sampling convolutional layers with different LoRA ranks and number of layers on the model's generation of LORA weights for the task \"Detecting Cats in Pictures\u201d.\nAs shown in Table 5, as the LoRA rank and the number of parameters rise, the effect of sampling progressively decreases the fewer the convolutional layers. This indicates that the deeper the network is, the better the model samples the parameters. However, when the network convolution layer is too deep, it causes the model to fail to learn the parameter distribution characteristics of the fewer parameter LoRA. Therefore, we choose to use 12 convolutional layers to sample the LoRA parameters in our experiments.\nImpact on Text and Vision Task Vector. We generate text task vectors using \"Detect the cat in the picture.\" and \"Detect the dog in the picture.\" by text encoder. Then generate vision task vectors using cat and dog images. Finally we evaluate the reconstructed LoRA parameters on a subset of cats and dog in COCO and report MAP50.\nAs it shown in Table 6, task vectors instructed by vision and task input has equal impact in generate the task vector, So we consider that task vectors of different modalities have equivalent effects on parameter generation. And the simultaneous use of multimodal task vectors is not possible for the process of enhancing parameter generation."}, {"title": "5 Conclusion", "content": "In this paper, we propose ICM-LORA, a novel framework that uses a self-designed parameters generator, Conditional Variational Autoencoder (CVAE), which could generate LoRA parameters to implement model customization. ICM-LORA achieves task vectors and LoRA parameter context modeling by combining in-context learning and meta-learning, which allowing CVAE to learn LoRA parameter distributions more accurately.\nOur method achieves accurate task instruct LoRA parameter generation with only CVAE, eliminating the need for additional training data and storage. The experimental results on both language modeling and object detection tasks have further validated the effectiveness of our approach could apply to different models with different tasks. ICM-LORA can also reduce storage costs and improve computational efficiency. Overall, ICM-LORA represents a significant advancement in parameter generation and large-scale model customization."}]}