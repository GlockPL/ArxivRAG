{"title": "Hallucination Detection in LLMs: Fast and Memory-Efficient Finetuned Models", "authors": ["Gabriel Y. Arteaga", "Thomas B. Sch\u00f6n", "Nicolas Pielawski"], "abstract": "Uncertainty estimation is a necessary component when implementing AI in high-risk settings, such as autonomous cars, medicine, or insurances. Large Language Models (LLMs) have seen a surge in popularity in recent years, but they are subject to hallucinations, which may cause serious harm in high-risk settings. Despite their success, LLMs are expensive to train and run: they need a large amount of computations and memory, preventing the use of ensembling methods in practice. In this work, we present a novel method that allows for fast and memory-friendly training of LLM ensembles. We show that the resulting ensembles can detect hallucinations and are a viable approach in practice as only one GPU is needed for training and inference. Code will be made available upon acceptance.", "sections": [{"title": "1 Introduction", "content": "LLMs have recently grown in popularity, thanks to their ability to interpret natural language and generate answers that resemble human discussions, even surpassing human performance in specific tasks [1]. However, these models face a significant challenge known as hallucination, where outputs that seem plausible may either deviate from instructions or lack factual accuracy. Hallucinations can broadly be categorized into two types [2]: faithfulness hallucinations, where the LLM deviates from provided instructions, and factual hallucinations, where there is a disparity between the generated content and verifiable facts. The risk arises when individuals unaware of these limitations mistakenly treat such outputs as ground-truth, leading to decisions based on erroneous information a concern particularly relevant to safety-critical areas such as healthcare.\nTechniques leveraging natural language inference models and retrieval-based methods to detect hallucinations have shown promise in specific applications like summarization and open-domain question answering [3\u20135]. However, the effectiveness of these methods is typically limited to a narrow set of tasks, which restricts their generalizability across the broader spectrum of LLM applications.\nGiven these limitations, uncertainty estimation methods emerge as a compelling alternative for detecting both types of hallucinations [6]. Unlike task-specific approaches, uncertainty estimation uses the model's own confidence in its predictions to identify if the outputs are unfaithful or factually incorrect. Recent work in uncertainty quantification in LLMs have emerged, with approaches like deep ensembles [7\u201310] and sample-based methods which use stochastic sampling techniques [11\u201315]. However, sample-based methods seldom provide reliable uncertainty estimates as they rely on the distribution of a single model's outputs, which may not fully capture the true uncertainty in the model's predictions. While deep ensembles advertise more robust uncertainty estimates by aggregating predictions from multiple independently trained models, they come with significant computational bottlenecks, especially when applied to larger LLMs, as they require substantial resources for training and inference.\nTo address these limitations, we propose a fast and memory-efficient deep ensemble method which is able to provide reliable uncertainty estimates. Figure 1 describes our proposed method, where low-rank matrices are added on top of a pretrained model and used for fine-tuning. A Low-Rank Adaptation (LORA) matrix allows the whole ensemble to be"}, {"title": "2 Related Work", "content": "We believe that distinguishing between the two types of hallucinations introduced by Huang et al. [2] provides valuable insights into the behavior of LLMs and highlights the distinct challenges associated with each category. Therefore, we adopt this terminology throughout this paper.\nHallucination Detection in LLMS\nVarious approaches have been proposed to identify when an LLM diverges from instructions or deviates from contextual cues in the input. This work is especially critical in tasks like summarization, where adherence to the provided context is crucial for generating accurate summaries. These methods often leverage natural language inference models to compute entailment scores, which are then used to detect instances of unfaithfulness in the generated outputs [3, 5, 17].\nSimilarly, some research already focused on detecting when LLMs produce factual hallucinations, where their generated content deviates from verifiable facts. Some methods have been developed"}, {"title": "3 Method", "content": "Uncertainty Estimation\nTo quantify the uncertainty associated with an LLM\u2019s predictions, we use the predictive entropy of the output distribution, a concept rooted in information theory. Let $x_{<t} = {x_1,...,x_{t-1}}$ represent the preceding tokens, which serve as the input for predicting the target token $x_t$ at time step $t$. The predictive entropy is then defined as:\n$H [P(x_t|x_{<t}; D)] =\\ - \\sum_{x_t}P(x_t|x_{<t}; D) \\log P(x_t|x_{<t}; D)$.\nThe predictive entropy can further be divided into its two subcomponents aleatoric and epistemic uncertainties:\n$I [x_t, \\theta |x_{<t}, D] =$\n$\\underbrace{H [P(x_t|x_{<t}; D)]}_{Predictive} \u2013 \\underbrace{E_{p(\\theta|D)} [H [P(x_t|x_{<t}; D)]]}_{Aleatoric}$.\nEpistemic uncertainty captures the lack of knowledge of a system, which shrinks as more data is made available. Conversely, aleatoric uncertainty represents the noise - the variability of the data and is therefore irreducible [34].\n$P(x_t|x_{<t}; D)$ cannot be directly computed and is approximated with:\n$P(x_t|x_{<t}; D) \\approx \\frac{1}{M} \\sum_{m=1}^{M}P(x_t|x_{<t};\\theta_m)$,\nwhere $\\theta_m$ is the parameters associated to the $m$th model. The estimate of the predictive entropy is an estimate of the predicted uncertainty, and will yield the exact predictive uncertainty as $M \\to \\infty$ [35].\nMemory-Efficient Fine-tuning\nWe employ deep ensembles [25] to approximate Equation (3). However, training those deep ensembles may require prohibitively high computational resources, available to few organizations.\nWe propose using the BatchEnsemble method [28], but rather than pre-training each ensemble member [29], we adapt the method to fine-tune already pre-trained models. BatchEnsemble optimizes memory usage, which is a critical advantage over traditional ensembles. In a standard ensemble, memory requirements grow linearly with the number of ensemble members, with complexity increasing"}, {"title": "4 Design of experiments", "content": "We aim to design experiments that can clearly differentiate between faithfulness and factual halluci-"}, {"title": "5 Results", "content": "Hallucination and OOD Detection\nIn Table 1, we assess the quality of our models\u2019 and baselines\u2019 uncertainty estimates by using them as features for classifiers to predict hallucinations and OOD instances. The uncertainty estimates are fed into five classifiers: k-Nearest Neighbors, logistic regression, decision trees, random forest, and support vector machine. We report the top-1 classification accuracy for each model\u2019s uncertainty estimates. See\nAll methods, including the baselines, demonstrate a relatively high accuracy exceeding 92% in detecting cases where the model faithfully hallucinates.\nIt shows a notable increase in uncertainty when the model encounters an unanswerable question. The uncertainty sharply decreases after the model generates the first token, suggesting that once it commits to a hallucinated token, it becomes more prone to continue hallucinating a phenomenon referred to as the \"snowballing effect\" [40]. More, the high accuracy achieved using the models\u2019 uncertainty estimates supports the idea that LLMs\u2019 internal states possess an inherent understanding of the generated, hallucinated content, a finding similar to that observed by Azaria & Mitchell [41].\nFor detecting factual hallucinations, the LORA Ensemble\u2019s uncertainty estimates result in the highest accuracy. A possible explanation is that the high weight decay strategy applied to the LORA B matrix [30] introduces substantial stochasticity among the ensemble members, leading to improved uncertainty estimates. However, this enhanced expressiveness in uncertainty estimation comes at the cost of predictive performance, which will be discussed further in the next subsection. Additionally, while the sample-based method demonstrates slightly better performance than our proposed approach, this marginal improvement may be attributed to randomness during training or to the task itself. Producing uncertainty estimates for a single token (the choice in a multiple-choice setting) may inherently be easier for the sample-based method.\nAll methods generally performs worse when encountering OOD datapoints. This indicates that, while LLMs are versatile, uncertainty-based ap-"}, {"title": "6 Conclusion", "content": "As LLMs are increasingly used across various fields, minimizing harmful predictions is crucial. This work suggests that deep ensembles are effective for quan-"}, {"title": "A Experimental Details", "content": "In this section, we will outline the detailed training and evaluation splits used for our experiments\nA.1 Factual Hallucination Detection Experiment\nFor the factual hallucination detection experiment, we used the MMLU dataset [39]. We created a training set by combining data from the 'all' category and the 'auxiliary_train' split. The dataset was shuffled with a seed of 42, and the first 42,000 data points were selected. Of these, 40,000 were used for training, while the remaining 2,000 were set aside for validation.\nFor evaluation, we used the 'test' split, shuffled with the same seed of 42. We selected the first 5,000 data points from this split for evaluation.\nA.2 Faithfulness Hallucination Detection Experiment\nFor the faithfulness hallucination detection experiment, we used a modified version of the SQUAD v2 dataset. To create the training and validation splits, we first loaded the SQUAD v2 training set and filtered it into two subsets: answerable and unanswerable questions. Unanswerable questions were relabeled with \"I don't know\" to train the model to differentiate between the answerable and unanswerable questions. We then shuffled both subsets with a seed of 50 and selected 28,000 answerable and 14,000 unanswerable questions to maintain a distribution similar to the original dataset. These subsets were combined into a single dataset and shuffled again to ensure a balanced distribution of answerable and unanswerable questions. From this combined dataset, we selected 40,000 samples for training and 2,000 samples for validation.\nFor evaluation, we processed the SQUAD v2 validation set and filtered to only keep unanswerable questions. The filtered unanswerable questions were shuffled with a seed of 42, and the first 5,000 data points were selected for evaluation.\nA.3 Out-Of-Distribution Detection Experiment\nFor the OOD detection experiment, we trained our models on the original SQUAD dataset and evaluated them on unanswerable questions from the SQUAD v2 validation set.\nWe began by loading the SQUAD training dataset and shuffling it using a seed of 50 to ensure randomness. From this dataset, we selected 40,000 samples for training and 2,000 samples for validation.\nFor evaluation, we utilized the validation set from SQUAD v2, specifically filtering out unanswerable questions. The unanswerable subset was shuffled with a seed of 42, and the first 5,000 data points were selected for the evaluation phase.\nA.4 Predictive Performance Experiments\nTo evaluate predictive performance, we used models trained during the previous experiments on both the MMLU and SQUAD datasets.\nFor the MMLU evaluation, we leveraged the models trained from the factual hallucination detection experiment. We loaded all data points from the MMLU test split, shuffled them with a fixed seed of 42, and selected the first 5,000 samples.\nFor the SQUAD dataset, we used models trained from the OOD detection experiment, as these were specifically trained on the SQUAD dataset. For evaluation, we loaded the SQUAD validation set, shuffled it with a seed of 42, and selected 5,000 samples."}, {"title": "B Detailed Results", "content": "The tables below present the detailed results for each method across five different classifiers: Logistic Regression (LR), Decision Tree (DT) classifier, Support Vector Classifier (SVC), Random Forest (RF), and k-Nearest Neighbors (kNN). All classifiers were implemented using the scikit-learn library [42]. No hyperparameter tuning was performed; instead, we used the default parameters provided by scikit-learn for each classifier.\nFor the features, we used the first token's predictive entropy and aleatoric uncertainty. Additionally, we provided the classifiers with two more features: the average predictive entropy and the average aleatoric"}, {"title": "C Weight Initialization", "content": "We explore two widely-used weight initialization strategies for our fast weights: He initialization [43", "44": ".", "26": "may have used one of these methods, making them natural choices for our approach."}]}