{"title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection", "authors": ["Jinhe Bi", "Yifan Wang", "Danqi Yan", "Xun Xiao", "Artur Hecker", "Volker Tresp", "Yunpu Ma"], "abstract": "Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy, leading to excessive computational costs. Existing data selection methods predominantly rely on proxy models or loss-based metrics, both of which impose substantial computational overheads due to the necessity of model inference and backpropagation. To address this challenge, we propose PRISM, a novel training-free approach for efficient multimodal data selection. Unlike existing methods, PRISM eliminates the reliance on proxy models, warm-up pretraining, and gradient-based optimization. Instead, it leverages Pearson correlation analysis to quantify the intrinsic visual encoding properties of MLLMs, computing a task-specific correlation score to identify high-value instances. This not only enables data-efficient selection, but maintains the model's original performance. Empirical evaluations across multiple MLLMs demonstrate that PRISM reduces the overall time required for visual instruction tuning and data selection to just 30% of conventional methods, while surpassing fully fine-tuned models across eight multimodal and three language understanding benchmarks, achieving a 101.7% relative improvement in final performance.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly transformed artificial intelligence by integrating vision and language processing capabilities (Liu et al., 2024a; Zhu et al., 2023; Dai et al., 2023). Modern MLLMs typically undergo a two-stage training process: (1) large-scale pretraining on web-scale image-text pairs to establish cross-modal alignment, followed by (2) visual instruction tuning on task-specific datasets to enhance instruction-following abilities. While instruction tuning is crucial for achieving strong downstream performance, the exponential growth of low-quality and redundant data (Chen et al., 2024; Wei et al., 2023) in curated datasets poses a major challenge. This proliferation not only increases computational costs but also leads to diminishing returns, highlighting the need for efficient data selection strategies that maximize informativeness while minimizing redundancy.\nAs training on the full dataset becomes increasingly impractical, selecting the most informative samples is essential for maintaining strong performance while reducing computational overhead. Existing data selection approaches can be broadly classified"}, {"title": "2 Visual Instruction Selection", "content": "Visual instruction selection is an approach that can effectively reduce training time of visual instruction tuning by identifying high-value instruction instances. Numerous studies have explored effective methods for selecting such instruction instances while minimizing computational overheads. In this section, we first introduce two fundamental principles for visual instruction selection, which"}, {"title": "3 PRISM", "content": "The PRISM framework establishes a new paradigm for multimodal data selection by directly harnessing the intrinsic representation structures of MLLMs. Unlike existing methods that depend on external heuristics or model-driven proxies, PRISM leverages the model's intrinsic encoding mechanisms to assess data informativeness. Modern MLLMs, such as LLaVA (Liu et al., 2024a), unify visual and textual modalities through a vision encoder and projector, embedding images into the LLM's latent space-where their uniqueness is inherently captured. This approach ultimately enhances performance while reducing training time by 70%. Our initial research (as in Fig. 3) revealed that layer-wise token embeddings inherently capture structural distinctions between informative and redundant samples. Inspired by this, we explored the statistical dependencies within these embeddings to systematically identify high-value data instances. These findings ultimately led to the design of PRISM, a method that selects informative samples without relying on external supervision (e.g., proxy models or gradient-based computations). PRISM formalizes this approach in a three-stage pipeline: feature representation, correlation analysis, and self-pruning selection. As we will see in the performance evaluation, PRISM offers a scalable and computationally efficient solution to multimodal data selection."}, {"title": "3.1 Feature Representation and Correlation Analysis", "content": "Let $D = \\{I_1, I_2, . . ., I_N \\}$ denote the image dataset for target task $T$. For each image $I_i$, the vision encoder (VE) extracts and projects visual embeddings into the LLM's latent space:\n$v_i = VE(I_i) \\in \\mathbb{R}^{d_v}, z_i = Proj(v_i) \\in \\mathbb{R}^d$ (1)\nwhere Proj: $\\mathbb{R}^{d_v} \\rightarrow \\mathbb{R}^d$ is a linear projector. The LLM processes $z_i$ through transformer layers, with averaged token features from layer $l$ computed as:\n$F_i = \\frac{1}{T} \\sum_{t=1}^T LLM^{(l)}(z_i)_t \\in \\mathbb{R}^d$ (2)\nwhere $T$ denotes the number of tokens. We hypothesize that images with divergent feature correlations provide complementary information. This is quantified through Pearson analysis:\n$P_{ij} = \\frac{E[(F_i \u2013 \\mu_i)(F_j \u2013 \\mu_j)]}{\\sigma_i \\sigma_j}, C_i = \\sum_{j=1}^N P_{ij}$ (3)\nwhere $\\mu_i, \\sigma_i$ are mean and standard deviation of $F_i$, and $C_i$ measures alignment with $D$'s feature distribution."}, {"title": "3.2 Self-Pruning Selection", "content": "Images with the lowest $C_i$ values (i.e., those in the bottom $\\tau$% of the sorted correlation scores) are selected as high-value candidates. This selection strategy is guided by three factors:\nReduction of Feature Redundancy: High correlation images (\u2191 $C_i$) exhibit substantial semantic overlap, contributing diminishing returns during training.\nInformation-Theoretic Diversity: Low correlation samples (\u2193 $C_i$) maximize the Shannon entropy"}, {"title": "4 Experiments", "content": "We first present our experimental setup and evaluation benchmarks, followed by comparisons with state-of-the-art methods. Next, we analyze our method's behavior and effectiveness across various dimensions. Additionally, we evaluate the transfer-ability of our strategy to unseen tasks and model architectures. Finally, we conduct ablation studies to assess the contribution of each component."}, {"title": "4.1 Experiment Setup", "content": "Dataset & Model: We evaluate PRISM on the visual instruction tuning dataset LLaVA-665K (Liu et al., 2024a), using LLaVA-1.5-7B (Liu et al., 2024a) as our primary base model. All experiments are conducted for one epoch following the official fine-tuning hyperparameters. To ensure a fair comparison, we maintain a consistent training environment across all evaluations.\nBaselines: We compare PRISM against a comprehensive set of data selection baselines, including Random Selection, Instruction Length, Perplexity (Liu et al., 2024d), GraNd (Paul et al., 2023), EL2N (Paul et al., 2023), InstructionGPT-4 (Wei et al., 2023), SELF-FILTER (Chen et al., 2024), TIVE (Liu et al., 2024d), COINCIDE (Lee et al., 2024), DataTailor (Yu et al., 2024a), and ICONS (Wu et al., 2025). To ensure fair comparisons, we adopt the experimental settings and incorporate results from ICONS (Wu et al., 2025) and TIVE (Liu et al., 2024d).\nBenchmarks: Following the evaluation framework of LLaVA-1.5 (Liu et al., 2024a), we assess the effectiveness of PRISM across a diverse set of multimodal benchmarks designed to test various capabilities of MLLMs. These benchmarks are grouped into three main categories: understanding and reasoning, factual consistency and generalization, and visual conversation and core multimodal skills."}, {"title": "4.2 Main Results", "content": "We present a comprehensive evaluation of PRISM across multiple settings. First, in Table 1, we compare PRISM with the baseline methods selected above on the LLaVA-1.5-7B model (Liu et al., 2024a). This demonstrates its superior performance in multimodal data selection. Next, Table 2 showcases the results of PRISM across different MLLMs, which highlights its generalizability and robustness. Finally, Table 3 focuses on PRISM's text-only capabilities, which provides insights into its effectiveness in uni-modal settings.\nWe further analyze these results as follows:\nSuperior Multimodal Understanding. As shown in Table 1, PRISM achieves the best performance across 11 multimodal benchmarks, surpassing full-dataset fine-tuning by 1.7% in relative performance. Notably, PRISM excels in instruction-sensitive tasks: it outperforms full fine-tuning on MMBench (65.2 vs. 64.3) and MM-Vet (32.0 vs. 31.1), demonstrating its ability to select samples that enhance complex reasoning and visual conversation capabilities. The improvements are particularly significant compared to gradient-based methods like GraNd (62.9 vs. 65.2 on MMBench), highlighting the limitations of loss-driven selection in multimodal contexts.\nHallucination Mitigation. PRISM achieves the highest scores on all POPE subsets (87.7/88.7/85.5), outperforming even specialized hallucination reduction methods like ICONS (87.5). This suggests that low-correlation samples inherently reduce the model's tendency to generate inconsistent facts, as they avoid overfitting to spurious text-visual correlations prevalent in redundant data.\nBalanced Efficiency and Performance. While gradient-based methods like TIVE achieve comparable average performance (100.6% rel.), their total time costs (selection + training) often exceed full fine-tuning due to iterative model updates. In contrast, PRISM achieves higher accuracy (101.7%) and reduces total time by 70%. This efficiency stems from its training-free advantage: feature extraction and correlation computation are executed in a single forward pass and offline batched processing, respectively, with negligible overhead compared to full training cycles. Remarkably, PRISM simultaneously enhances spatial reasoning capabilities (330.0 on MME-C vs. 311.9 for full finetuning), validating that its selection criteria preserve geometrically informative samples often lost in random or length-based pruning."}, {"title": "4.3 Model Behavior Analysis", "content": "Cross-Model Generalization and Scalability. PRISM is designed to identify high-value data that remains effective across different model architectures and scales. To validate this, we assess whether data selected using one model setup can benefit others. While our subset was initially selected with LLaVA-1.5-7B, we further evaluate its effectiveness on two additional model configurations. The detailed architectures of these models are summarized in Appendix 7. The results approve that PRISM captures generally useful training samples rather than those tailored to a specific model.\nAs shown in Table 2, PRISM demonstrates strong cross-architecture and cross-scale generalization capabilities. The subset selected using the 7B model achieves competitive performance across different model sizes and architectures, suggesting that our method captures fundamental visual-language understanding capabilities that are transferable and scalable. This highlights the robustness of PRISM in identifying high-value data points that generalize well across diverse multimodal model configurations.\nLanguage Knowledge Retention. While visual instruction tuning significantly enhances performance on vision-centric tasks, it often leads to a degradation in the model's ability to handle text-only tasks (Zhang et al., 2024). To assess the text-only performance, we evaluate PRISM on a range of benchmarks accordingly, including interdisciplinary knowledge assessments such as MMLU (Hendrycks et al., 2021) and MMLU-PRO (Wang et al., 2024), as well as reasoning tasks like Hellaswag (Zellers et al., 2019). These benchmarks"}, {"title": "4.4 Ablation Study", "content": "To validate the design choices of PRISM, we conduct systematic ablation studies on three key components: LLM layer selection, correlation-based scoring, and token aggregation for image representation.\nInfluence of LLM Layer Selection. We first investigate how different transformer layers impact PRISM's performance by extracting features from three representative layers: Shallow Layer 1, which captures low-level visual patterns such as edges and textures; Middle Layer 16, which balances visual and semantic features; and Deep Layer 32, which encodes high-level semantic abstractions. As shown in Table 5, PRISM achieves the highest performance when using shallow layer features, outperforming deeper layers by 2.8%. This result indicates that early-layer embeddings sufficiently capture the necessary information for redundancy detection, while deeper layers may overfit to task-specific semantics, leading to reduced generalizability.\nImpact of Correlation-based Selection. We evaluate PRISM's correlation-based selection strategy by partitioning the dataset into three groups"}, {"title": "5 Related Work", "content": "Visual Instruction Tuning: Visual instruction tuning is essential for aligning MLLMs with both practical applications and academic benchmarks. Early methods relied on synthetic visual instructions, which performed well in conversations but struggled on rigorous benchmarks. A hybrid approach later emerged, integrating synthetic data with academic datasets to improve training diversity. This advancement has enhanced models like LLaVA (Liu et al., 2024b), InstructBLIP (Dai et al., 2023), and Cambrian (Tong et al., 2024), enabling better visual-linguistic understanding. Beyond task performance, visual instruction tuning improves model alignment with user expectations, ensuring both practical utility and strong academic performance.\nVisual Instruction Selection: Despite the strong performance of MLLMs, the rapid growth of visual instruction datasets has introduced significant redundancy, similar to challenges in LLMs (Zhou et al., 2024; Chen et al., 2023; Xia et al., 2024). State-of-the-art models like BLIP3 (Xue et al., 2024), InternVL2.5 (Chen et al., 2025), and LLaVA-OneVision (Li et al., 2024) rely on billions of instructions to enhance understanding, but their massive scale leads to substantial computational costs, often requiring hundreds to thousands of GPU hours.\nTo address this, various data selection strategies aim to reduce redundancy while preserving performance. TIVE (Liu et al., 2024d) selects valuable data based on gradient similarity but requires additional training on downstream tasks. SELF-FILTER (Chen et al., 2024) uses an auxiliary evaluation model to prioritize high-value samples. COINCIDE (Lee et al., 2024) clusters data by conceptual and skill-based representations, while InstructionGPT-4 (Wei et al., 2023) filters 200 instructions for MiniGPT-4 (Zhu et al., 2023), though it lacks scalability. ICONS (Wu et al., 2025) extends LESS (Xia et al., 2024) by incorporating specialist influence estimation for instruction tuning. DataTailor (Yu et al., 2024a) selects data based on informativeness, uniqueness, and representativeness to retain the most relevant samples."}, {"title": "6 Conclusion", "content": "PRISM leverages MLLMs' intrinsic cross-modal alignment to select high-value samples using Pearson correlations of token embeddings, requiring no proxy models or training. It achieves 70% cost reduction while maintaining performance, setting a new standard for efficient multimodal learning."}, {"title": "Limitations", "content": "A limitation of this work is the static nature of the data selection strategy, which only handles text and image modalities. Extending this approach to include video and sound could introduce challenges due to the temporal and sequential properties of these modalities.\nAdditionally, our method does not incorporate dynamic data selection during training. Adapting the selection process over time could improve model efficiency by focusing on the most relevant data at each stage, particularly for large and diverse datasets."}, {"title": "A Dataset Details", "content": "We present the detailed composition of the PRISM-Instruct-250K dataset, which spans multiple visual question answering (VQA), image understanding, and text-based tasks. This diverse selection ensures a comprehensive representation of multimodal learning challenges. The table below shows the distribution of samples across different data sources."}, {"title": "B Model Architectures", "content": "In our experiments, we assess PRISM's transferability across various model architectures and scales, following the methodology outlined in (Bi et al., 2025). The models tested include LLaVA-Vicuna-7B, LLaVA-Phi2-3B, and LLaVA-Vicuna-13B. Each model consists of a vision encoder, a projector, and a language model. The table below summarizes their configurations."}, {"title": "C Shannon Entropy and Feature Diversity", "content": "Let $F = \\{f_1, f_2, ..., f_v \\}$ denote the set of feature vectors extracted from the dataset D, where each $f_i \\in \\mathbb{R}^d$ corresponds to the averaged token embedding of image $I_i$. The Shannon entropy $H(F)$ of the feature set is defined as:\n$H(F) = - \\sum_{i=1}^N p(f_i) log p(f_i),$ (5)\nwhere $p(f_i)$ is the probability density of feature $f_i$. In practice, $p(f_i)$ can be approximated using kernel density estimation or other non-parametric methods. However, directly maximizing $H(F)$ is computationally infeasible for large datasets. Instead, we use the Pearson correlation matrix $P$ as a proxy for feature diversity.\nThe Pearson correlation matrix $P$ captures pairwise linear dependencies between feature vectors. For a given feature $f_i$, the correlation score is given by:\n$C_i = \\sum_{j=1}^N P_{ij},$ (6)\nwhich quantifies its overall alignment with the dataset. Low $C_i$ values indicate that $f_i$ is weakly correlated with other features, suggesting that it contributes unique information to the dataset.\nLet $F_{selected} = \\{f_i | C_i \\leq Q_{\\tau}(\\{C_j\\}_{j=1}^N) \\}$ denote the subset of features selected by PRISM, where $Q_{\\tau}$ is the $\\tau$-th percentile of correlation scores. The entropy of $F_{selected}$ can be approximated as:\n$H(F_{selected}) \\approx \\sum_{f_i \\in F_{selected}} p (f_i) log p(f_i).$ (7)\nBy selecting features with minimal $C_i$, we implicitly minimize the pairwise dependencies within $F_{selected}$, thereby maximizing the entropy $H(F_{selected})$. This is because low-correlation features are less likely to share redundant information, leading to a more diverse and informative subset. We now formalize this intuition with the following theorem:\nTheorem 1 (Entropy Maximization via Low-Correlation Selection): Let F be a set of feature vectors with correlation matrix P. For any subset $F_{selected} \\subseteq F$, the Shannon entropy $H(F_{selected})$ is maximized when $F_{selected}$ consists of features with minimal row-wise sums:\n$C_i = \\sum_{j=1}^N P_{ij}$ (8)\nProof: The proof follows from the properties of Shannon entropy and the definition of Pearson correlation. Let $F_{selected} = \\{f_i | C_i \\leq"}]}