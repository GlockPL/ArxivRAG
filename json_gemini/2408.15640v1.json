{"title": "GANs Conditioning Methods: A Survey", "authors": ["Anis Bourou", "Auguste Genovesio", "Val\u00e9rie Mezger"], "abstract": "In recent years, Generative Adversarial Networks (GANs) have seen significant advance-\nments, leading to their widespread adoption across various fields. The original GAN ar-\nchitecture enables the generation of images without any specific control over the content,\nmaking it an unconditional generation process. However, many practical applications re-\nquire precise control over the generated output, which has led to the development of condi-\ntional GANs (cGANs) that incorporate explicit conditioning to guide the generation process.\ncGANs extend the original framework by incorporating additional information (conditions),\nenabling the generation of samples that adhere to that specific criteria. Various conditioning\nmethods have been proposed, each differing in how they integrate the conditioning infor-\nmation into both the generator and the discriminator networks. In this work, we review\nthe conditioning methods proposed for GANs, exploring the characteristics of each method\nand highlighting their unique mechanisms and theoretical foundations. Furthermore, we\nconduct a comparative analysis of these methods, evaluating their performance on various\nimage datasets. Through these analyses, we aim to provide insights into the strengths and\nlimitations of various conditioning techniques, guiding future research and application in\ngenerative modeling.", "sections": [{"title": "Introduction", "content": "Generative Adversarial Networks (GANs) Goodfellow et al. (2014) are implicit generative model in which\nthe data distribution is learned by comparing real samples with generated ones. This approach leverages an\nadversarial process between two neural networks: a generator, which produces synthetic data, and a discrim-\ninator, which evaluates the data to distinguish between real and generated (fake) samples. The competition\nbetween these networks drives both to improve, with the generator aiming to create increasingly realistic\ndata while the discriminator becomes better at identifying fakes. Since their introduction, GANs have in-\nspired numerous extensions and enhancements. Notably, the Deep Convolutional GAN (DCGAN) Radford\net al. (2016), marked a significant advancement by employing convolutional layers. DCGAN demonstrated\nthe ability to generate high-quality images and contributed to the robustness of GAN training. To address\nthe inherent training difficulties and instability of GANs, different objectives were proposed Arjovsky et al.\n(2017); Gulrajani et al. (2017); Mroueh et al. (2017a;b); Li et al. (2017), leading to more stable training and\nultimately producing higher quality outputs. Self-Attention GANs (SAGAN) Zhang et al. (2019) enhanced\nGANs' ability to capture global dependencies within images by integrating self-attention mechanisms Vaswani\net al. (2023). BigGAN Brock et al. (2019), scaled up the GAN architecture, achieving spectacular results on\nthe ImageNet dataset through large batch sizes and careful architectural choices, pushing the boundaries of\nwhat GANs can achieve in terms of image quality and diversity. Furthermore, Karras et al. (2018) proposed\nthe Progressive Growing GAN (ProGAN), a method to train GANs starting with low-resolution images and\nincrementally increasing the resolution as training progresses. This approach mitigated training instability"}, {"title": "Discriminator conditioning approaches", "content": "The discriminator in GANs plays a crucial role, by providing feedback on the quality of the generated data\nsamples to the generator. In the conditional setting, the discriminator should be provided with the class\nlabel, the earliest cGANs frameworks fed the class label y to the discriminator by simply concatenating it\nwith the feature vector Mirza & Osindero (2014). Variants of this method have proposed concatenating\nthe class label embedding with the feature vector at different depths in the network Kwak & Zhang (2016);"}, {"title": "Auxiliary-classifier based discriminators", "content": "Concatenating class label information with the input image features can achieve conditioning; however, this\napproach is relatively simplistic and arbitrary, which may hinder GANs from accurately approximating the\ntrue data distribution. In this section, we present a collection of methods that condition the discriminator\nby incorporating an auxiliary classifier."}, {"title": "Auxiliary classifier GAN (AC-GAN)", "content": "The development of conditional Generative Adversarial Networks (cGANs) advanced significantly with the\nintroduction of Auxiliary Classifier GANs (AC-GANs) Odena et al. (2017). This approach integrates an\nauxiliary classifier into the GAN discriminator to predict the class label of the generated image. This design\nshift, motivated by the potential for enhanced performance through multi-task learning, enables the AC-GAN\nto generate higher quality and class-specific images. Unlike prior cGANs, where conditioning information is\ndirectly fed to the discriminator via concatenation, AC-GAN employs a dual objective function. The first\npart, $L_s$ (Eq. 3), focuses on the log-likelihood of correctly identifying real versus fake images, while the\nsecond part, $L_c$ (Eq. 4), concentrates on accurately classifying these images into their respective classes:\n$L_s = E_{x~p(x)} [log D(x)] + E_{z~p(z),y~p_y(y)} [log (1 - D(G(z,y)))]$\n$L_c = -E_{p(x,y)} [log C(x, y)] \u2013 E_{z~p_z(z),y~p_y(y)} [log(C(G(z,y)), y)]$\nWhere C is the introduced auxiliary classifier, By combining $L_s$ and $L_c$ we obtain the AC-GAN loss:\n$min_{G,C} max_{D} L_{AC} (G, D, C) =E_{x~p_z(x)}[log D(x)] + E_{z~p_z(z),y~p_y(y)} [log(1 \u2013 D(G(z,y)))]$\n$-\\lambda_c E_{p(x,y)} [log C(x, y)] \u2013 \\lambda_c E_{z~p_z(z),y~p_y(y)} [log(C(G(z, y), y))]$\nwhere $\\lambda_c$ is a hyperparameter."}, {"title": "Twin Auxiliary Classifier GAN (TAC-GAN)", "content": "To address the limitations of AC-GAN, the Twin Auxiliary Classifier GAN (TAC-GAN) was pro-\nposed by Gong et al. (2019), introducing an additional auxiliary classifier. In Gong et al. (2019), it was\nshown that the absence of the negative conditional entropy term $-H_q(y|x)$ in the objective function of AC-\nGAN can lead to a degenerate solution that causes the generated images to be confined by the decision\nboundaries of the auxiliary classifier. This behavior explains the low intra-class diversity observed in the\nimages synthesized by AC-GAN. To alleviate this issue, the authors proposed to add an additional classifier\nto the AC-GAN that predicts the class of the generated images. This additional auxiliary classifier $C^{mi}$ is\ntrained to compete with the generator, optimizing the following objective function:\n$min_{G} max_{C^{mi}} V(G, C^{mi}) = E_{z~p_z(z),y~p_y(y)} [log (m_i (G(z, y), y)]$\ncombining Eq. 6 with the original AC-GAN objective leads to the total loss of TAC-GAN that reads:\n$min_{G,C} max_{D,C^{mi}} L_{TAC} (G, D,C, C^{mi}) = L_{AC} (G, D, C) + \\lambda_{ac}V(G, C^{mi})$\nIt was shown in Gong et al. (2019), theoretically and experimentally that this new loss function can be\nhelpful in learning an unbiased distribution and generating more diverse images."}, {"title": "Unbiased Auxiliary Classifier GAN (UAC-GAN)", "content": "Similar to Gong et al. (2019), Han et al. (2020) demonstrated that the lack of diversity observed in AC-GAN\nis induced by the absence of $- H_q (y|x)$ in the AC-GAN objective function. Furthermore, it was shown that\nthe TAC-GAN can still converge to a degenerate solution. In addition to that, it was observed that using\nan additional classifier can lead to an unstable training Kocaoglu et al. (2017); Han et al. (2020). Instead\nof using an additional classifier to minimize $-H_q(y|x)$, Han et al. (2020) proposed to estimate the mutual\ninformation $I_q(x; y)$ since:\n$I_q(x; y) = H_q(y) \u2013 H_q(y|x) = H_q(x) \u2013 H_q(x|y)$\nTo estimate $I_q(x; y)$, they employed the Mutual Information Neural Estimator(MINE) Belghazi et al. (2021).\nMINE is built on top of the Donsker and Varadhan bound Donsker & Varadhan (1975), $I_Q$ can be estimated\nusing the following equation:\n$I^{MINE} (x, y) = max_{T} V^{MINE} (G,T)$\nwhere:\n$V^{MINE} (G,T) = E_{z~p(z),y~p(y)}[T(G(z, y), y)] \u2013 log E_{z~p(z),y~q(y)} exp (T(G(z,y),y)$\nTis a scalar-valued function that can be parameterized by a deep neural network. The final objective\nfunction is given by:\n$min_{G,C} max_{D,T}(G, D,C,T) = L_{AC} (G, D,C) + \\lambda_m V^{MINE}(G,T)$\nwhere $\\lambda_m$ is a hyperparameter.\nHan et al. (2020) demonstrated that directly estimating the mutual information effectively addresses the\nlack of diversity in AC-GAN without the need for an additional classifier, which can often lead to unstable\ntraining."}, {"title": "Auxiliary Discriminative Classifier GAN (ADC-GAN)", "content": "In another work, Hou et al. (2021) proposed Auxiliary Discriminative Classifier GAN (ADC-GAN) to\novercome the limitations of AC-GAN. They demonstrated that, for a fixed generator, the optimal classifier\nof AC-GAN is agnostic to the density of the generated distribution q(x). Furthermore, they highlighted\nthat the generators in TAC-GAN and AC-GAN optimize contradictory learning objectives as shown in\nTable 2.1.4.\nTo alleviate these shortcomings, ADC-GAN uses a classifier that is able to classify the real data and the\ngenerated data separately. Using such a classifier $C_d : X \\rightarrow Y^+ \\cup Y^- $ ($Y^+$ for real data and $Y^-$ for generated\ndata), the generator is encouraged to produce classifiable samples that look like the real ones. The objective\nfunctions for the discriminator, the discriminative classifier and the generator are:\n$max_{D,C_d} V_{AC}(G, D) + \\lambda(E_{x,y~p_{z,y}} [log C_d(y^+|x)] + E_{x,y~q_{z,y}} [log C_d(y^-|x)]) $\n$min_{G} V_{AC}(G, D) \u2013 \\lambda(E_{x,y~p_{z,y}} [log C_d(y^+|x)] \u2013 E_{x,y~q_{z,y}} [log C_d(y^-|x)])$\nwhere:\n$C_d(y^+|x) = \\frac{exp(\\phi^+(y) . \\phi(x))}{\\sum_{y} exp(\\phi^+(y)(\\phi(x))+\\sum_{y} exp(\\phi^-(\u1ef9) . \\phi(x))}$ and $C_d(y^-|x) = \\frac{exp(\\phi^-(y) . \\phi(x))}{\\sum_{y} exp(\\phi^+(y)(\\phi(x))+\\sum_{y} exp(\\phi^-(\u1ef9) . \\phi(x))}$\nThe function $\\phi : X \\rightarrow R^d$ serves as a feature extractor, transforming input data X into a d-dimensional\nfeature space. This feature extractor is shared with the original discriminator, which is represented as\nD = $\\sigma \\circ \\psi \\circ \\phi $. Here, $\\psi : R^d \\rightarrow R$ is a linear mapping, and $\\sigma : R \\rightarrow [0, 1]$ is a sigmoid function. Additionally,\n$\\phi^+ : Y \\rightarrow R^d$ and $\\phi^- : Y \\rightarrow R^d$ are learnable embeddings capturing the label representations for real and\ngenerated data, respectively and $V_{AC}$ is the the original loss for AC-GAN.\nThe authors of ADC-GAN proved that for a fixed generator, the optimal discriminative classifier is given as:\n$C(y^+|x) = \\frac{p(x,y)}{p(x)+q(x)}$, $C(y^-|x) = \\frac{q(x,y)}{p(x)+q(x)}$ which shows that the optimal discriminative classifier is aware\nof the real and generated densities. Furthermore, they were able to discard the $-KL(p_x q_x)$ term from the\nobjective of the generator as shown in Table 2.1.4 In addition, they demonstrated that ADC-GAN leads to"}, {"title": "Projection-based discriminators", "content": "In the previous sections, conditioning the discriminator was achieved either by concatenating the class label\nor by adding an auxiliary classifier. Where the former can method can be very naive and sub-optimal in\ncapturing the additional class label information, the latter can make the training more difficult and unstable.\nIn Miyato & Koyama (2018), a new method for conditioning the discriminator was introduced by computing\nthe inner product between the embedded conditional vector y and the feature vector.\nThe design introduced in Miyato & Koyama (2018) presents a novel method for cGANs by employing a\nprojection discriminator, it was proposed by considering the optimal solution for the discriminator's loss\nfunction, Miyato & Koyama (2018) demonstrated that under certain regularity assumptions, the discrimi-\nnator's function can be reparameterized as follows:\n$f(x, y; \\theta) = f_1(x, y; \\theta) + f_2(x; \\theta) = y^T V\\phi(x; \\theta_\\phi) + \\psi(\\phi(x; \\theta_\\phi); \\theta_\\psi)$\nwhere V is the embedding matrix of y, $\\phi(., \\theta_\\phi)$ is a vector output function of x, and $(.,\\theta_\\psi)$ is a scalar\nfunction. The learned parameters $\\theta = {V, \\theta_\\phi, \\theta_\\psi}$ are trained to optimize the adversarial loss.\nThe projection discriminator approach for conditional Generative Adversarial Networks (cGANs), as pro-\nposed in Miyato & Koyama (2018) offers notable improvements over traditional methods like concatenation.\nThis technique enhances inter-class diversity, producing more varied and realistic samples across different\nclasses, which is crucial in many applications. A significant advantage of this method is its avoidance of\nadditional classifiers, leading to a greater training stability. The effectiveness and versatility of this approach\nare further evidenced by its adoption in various advanced GAN architectures, as seen in Brock et al. (2019);\nZhang et al. (2020); Zhao et al. (2020b); Wu et al. (2020); ?.\nBigGAN Brock et al. (2019), was among the pioneering GANs to employ discriminator projection techniques\nfor conditional generation. It brought significant enhancements to the scaling of GAN training, enabling the\ngeneration of images with higher resolutions. A pivotal enhancement in BigGAN's design is the integration of\northogonal regularization, which contributed markedly to its improved performance. Furthermore, BigGAN\ndrew inspiration from the Self-Attention GAN Zhang et al. (2019), particularly its utilization of self-attention\nblocks. These blocks aid both the discriminator and generator in more effectively capturing the global\nstructure of images. Additionally, BigGAN's architecture facilitated the application of the truncation trick,\nwhich allows for nuanced balancing of the fidelity-diversity trade-off in generated images.\nAnother line of work that adopted the projection discriminator is the StyleGAN ?Karras et al. (2020; 2021).\nThe StyleGAN family of models represents a significant advancement in the use of projection discriminators.\nThese models have achieved new state of the art results by incorporating innovative components like the\nMapping Network and AdaIN normalization Huang & Belongie (2017). Moreover, several techniques were\nintroduced to enhance the quality of image generation, even when dealing with limited size data sets Huang\n& Belongie (2017); Zhao et al. (2020a)."}, {"title": "Contrastive learning based discriminators", "content": "Contrastive learning Chen et al. (2020); Jaiswal et al. (2021); Le-Khac et al. (2020) mainly aims to develop\ndeep, meaningful, and robust data representations. At its core, it involves training models to distinguish\nbetween pairs of examples that are either similar or dissimilar. During the training phase, the model is\nencouraged to draw closer the representations of similar items ('positive' pairs) while distancing those of\ndissimilar items ('negative' pairs). This approach not only strengthens the model's capacity to discern un-\nderlying data structures and patterns but also enhances generalization across various tasks. The effectiveness\nof contrastive learning is particularly evident in diverse domains such as computer vision He et al. (2021;\n2020); Addepalli et al. (2022) and text processing Gunel et al. (2021); Chen et al. (2023); Aberdam et al.\n(2020). More recently, its application has been extended to generative models, as explored in Kang & Park"}, {"title": "Contrastive learning GAN (ContraGAN)", "content": "ContraGAN Kang & Park (2021) is a cGAN that achieve conditioning using a contrastive learning strategy\nby capturing the data-to-data relations. Indeed, it was suggested in Kang & Park (2021) that the con-\nditioning in AC-GAN and ProjGAN can only capture data-to-class relations of training examples while\nneglecting the data-to-data relations. To alleviate this, Kang & Park (2021) have proposed the Conditional\nContrastive (2C) loss, a self-supervised learning objective that controls the distances between embedded\nimages depending on their respective labels.\nThe 2C loss can be seen as an adaptation of the NT-Xent loss Chen et al. (2020). Given a minibatch of train-\ning images X = {x1,...,xm}, where x \u2208 RW\u00d7H\u00d73 and their corresponding labels y = {y1,\u2026\u2026\u2026, ym}, an en-\ncoder S(x) \u2208 Rk, a projection layer h : Rk \u2192 Sd that embeds onto a unit hypersphere, the NT-Xent loss con-\nducts random data augmentations T on the training data X, denoted as A = {x1, T(x1), . . .,Xm,T(xm)} =\n{a1,..., a2m}, the loss is given by:\n$l(a_i, a_j; \\tau) = -log(\\frac{exp (l(a_i)^T l(a_j)/\\tau)}{\\sum_{k=1}^{2m} 1_{k\u2260i} exp (l(a_i)^T l(a_k)/\\tau)})$\nwhere, $\\tau$ is the temperature that controls the attraction and repulsion forces.\nIn Kang & Park (2021) the discriminator network before the fully connected layer (D41) is considered as the\nencoder network, an additional multi-layer perceptrons h is used as a projection layer. Instead of using data\naugmentation, the authors used the embeddings of the class labels to capture the data-to-class relations, the\nmodified loss is given as follows:\n$l(x_i, y_i; \\tau)$\n$ - log(\\frac{exp(l(x_i)^T e(y_i)/\\tau)}{\\sum_{j=1}^{m} 1_{k\u2260i} exp(l(x_i)^T l(x_k)/\\tau)})$,\nIn order to ensure that the negative samples having the same label as $y_i$ are not apart, a cosine similarity of\nsuch samples is added to the numerator of Eq. 15 giving rise to the 2C loss:\n$l^{2C}(x_i, y_i; \\tau) = -log(\\frac{exp(l(x_i)^T e(y_i)/\\tau) + \\sum_{k=1}^{m} 1_{y_k=y_i} . exp(l(x_i)^T l(x_k)/\\tau)}{\\sum_{j=1}^{m} 1_{k\u2260i} exp(l(x_i)^T l(x_k)/\\tau)})$,\nwhere 1(xi) is the embedding of the image xi and e(yi) the embedding of the class label yi."}, {"title": "Rebooting Auxiliary Classifier GAN (ReACGAN)", "content": "The introduction of contrastive learning in conditioning GANs paved the way for addressing data-to-data\nrelations, a crucial aspect previously overlooked in previous work, particularly in classifier-based GANs like\nACGAN. Building on this foundation, Rebooting ACGAN (ReACGAN) Kang et al. (2021) introduces the\nData-to-Data Cross-Entropy loss (D2D-CE). This novel approach specifically targets the early training\ncollapse and the generation quality issues inherent in ACGAN. Kang et al. (2021) started by considering the\nempirical cross-entropy loss used in ACGAN, which is given as follows:\n$L_{CE} = \\frac{1}{N} \\sum_{i=1}^{N} log(\\frac{exp(F(x_i)^T w_{y_i})}{\\sum_{l=1}^{C} exp(F(x_i)^T w_j)})$\nwhere $F: X \\rightarrow R^d$ is feature extractor and a single fully connected layer classifier $C : F \\rightarrow R$ which is\nparameterized by $W = [w_1\u2026\u2026\u2026w_c] \\in R^{dxc}$, where c is the number of classes. Kang et al. (2021) found that\nat the early training stage the average norm of ACGAN's input features maps increases. Respectively, the\naverage norm of the gradients dramatically increases at the early training steps and decreases with the high\nclass probabilities of the classifier. In addition, it was observed that as the average norm of the gradients\ndecreases, the FID value of ACGAN does not decrease indicating the collapse of ACGAN.\nKang et al. (2021) found that normalizing the feature embeddings onto a unit hypersphere effectively solves\nthe ACGAN's early-training collapse. Specifically, the authors of ReACGAN introduced a projection layer\nP(F(x))\nPon the feature extractor F and they normalized both the feature embeddings ||P(F(z))|| (denoted as fi\nand the weight vector wy(denoted as vy)\nIn addition to the normalization, Kang et al. (2021) introduced a contrastive loss Data-to-Data Cross-\nEntropy(D2D-CE) to better capture the data-to-data relations as in ContraGAN, furthermore they in-\ntroduced two margin values to the D2D-CE to guarantee inter-class separability and intra class variations.\nThe contrastive D2D-CE loss reads:\n$L_{D2D-CE} \\approx \\frac{1}{N} \\sum_{i=1}^{N} log(\\frac{exp([f_{i}^{T}v_{y_i} - m_p]_- / \\tau)}{exp([f_{i}^{T}v_{y_i} - m_p]_- / \\tau) + \\sum_{j \\in N(i)} exp([f_{i}^{T}f_{j} - m_n]_{+}/ \\tau)})$\nwhere, $\\tau$ is the temperature parameter, and N(i) denotes the set of indices for negative samples with labels\ndifferent from the reference label $v_{y_i}$ in a batch. Margins $m_p$ and $m_n$ are used to manage similarity values for\neasy positives and negatives, respectively. The terms $[.]_-$ and $[.]_+$ correspond to the min(., 0) and max(., 0)\nfunctions.\nThis contrastive loss function proved to be effective in overcoming the limitations of ACGAN, significantly\nenhancing both class consistency and image diversity.\nIn contrast to the 2C loss, the D2D-CE objective does not hold false positives in the denominator, which\ncan cause unexpected repulsion forces. Furthermore, introducing the margins in D2D-CE loss can prevent\nhaving large gradient that can be caused by pulling easy positive samples."}, {"title": "Towards a unified framework for conditioning the discriminator", "content": "As previously discussed, various methods have been introduced to condition the discriminator, either by\nincorporating auxiliary classifiers or by employing alternative approaches. While the inclusion of a classifier\nin ACGAN effectively achieved conditioning, alternative approaches have successfully conditioned the dis-\ncriminator without the need for a classifier. In?, showed that the use of classifiers can benefit conditional\ngeneration. Furthermore, they introduced a unifying framework named Energy-based Conditional Gen-\nerative Adversarial Networks (ECGAN) which explains several cGAN variants. In order to connect\nthe classifier-based and the classifier-free approaches used equivalent formulations of the joint probability\np(x, y).\nFrom a probabilistic perspective, a classifier can be seen as a function that approximates p(y|x), the probabil-\nity that x belongs to y. Similarly, a conditional discriminator can be viewed as a function that approximates\np(xy), the probability that x is real given a class y, the joint probability is given as follows:\n$log p(x, y) = log p(x|y) + log p(y) = log(y|x) + log p(x)$\nIn Eq.19 we observe that the joint probability distribution log p(x, y) can be approached through two distinct\nmethods. The first method involves modeling a conditional discriminator p(xy), while the second focuses\non a classifier p(y|x). By sharing the parameterization between these models, the training process becomes\nmutually beneficial, allowing improvements in the conditional discriminator to enhance the classifier's per-\nformance, and vice versa."}, {"title": "Approaching the joint distribution from the conditional discriminator perspective", "content": "Similar to the energy based models LeCun et al. (2006), log p(x, y) was parameterized in Chen et al. (2021)\nusing a function fo(x), where exp(fo(x)[y]) xp(x, y).\nTherefore, the log-likelihood can be modeled as follows:\n$(log p_\\theta(x|y) = log(\\frac{exp (f_\\theta(x) [y])}{Z_y (\\theta)}) = f_\\theta(x) [y] - log Z_y (\\theta),$\nWhere $Z_y(\\theta) = \\int_{x\u2019} exp (f_\\theta(x\u2019)[y]) dx\u2019$\nOptimizing Eq. 20 is intractable because of the partition function $Z_y (\\theta)$. By introducing the Fenchel duality\nof the partition function $Z_y (\\theta)$ and a trainable generator q(z,y), where $z ~ N(0,1)$, ? showed that the\nmaximum likelihood estimation in Eq.20 is:\n$max_\\theta min_{\\Phi} \\sum_y E_{p_d(x|y)} [f_\\theta(x)[y]] \u2013 E_{p(z)} [f_\\theta(q_\\Phi(z,y))[y]] \u2013 H(q_\\Phi(\u00b7, y))$"}, {"title": "Approaching the joint distribution from the classifier perspective", "content": "As depicted in Eq.19, log p(x, y) can also be approximated using log p(y|x) and log p(x). Using the energy\nfunction introduced earlier fo(x), log(p$\\theta$(y|x) can be expressed as:\np$\\theta$(yx) = $\\frac{p_\\theta(x, y)}{p_\\theta(x)} = \\frac{exp(f_\\theta(x) [y])}{\\sum_y exp(f_\\theta(x) [y\u2019])}$,\nWhich can be written using a softmax as:\n$L_{clf}(x, y; \\theta) = - log (SOFTMAX (f_\\theta(x)) [y])$\nFurthermore, ? showed that by setting h$\\theta$(x) = log $\\sum_y exp(f_\\theta(x), maximizing the log-likelihood of p(x) is\nequivalent to solving the following optimization problem.\n$max_\\theta min_{\\Phi} E_{p_d(x,y)} [h_\\theta(x)] \u2013 E_{p(z)} [h_\\theta(q_\\Phi(z,y))] \u2013 H(q_\\Phi)$\nSimilar to Eq. 21, we can see that the Eq. 25 has the form of the traditional GAN. In this case, the equation\nof the discriminator is given as:\n$L_{d2} (x, z, y; \\theta) = \u2212h_\\theta(x) + h_\\theta(q_\\Phi(z))$\nIn?, two approaches were proposed to estimate the entropy terms in Eq. 21 and Eq. 25. The first approach\ninvolves considering the entropy term to be zero, based on the fact that entropy is always non-negative, the\nconstant zero is a lower bound. The second approach involves estimating a variational lower bound. The\nauthors demonstrated that the 2C loss, introduced in ContraGAN, serves as a lower bound in this context."}, {"title": "Unifying cGANs discriminators", "content": "To unify the classifier-based and classifier-free discriminators, ? proposed the ECGAN discriminator with\nthe following objective:\n$L_D(x, z, y; \\theta) = L_{d1}(x, z, y; \\theta) + \u03b1L_{d2}(x, z, y; \\theta) + \u03bb_c L^{real}_c + \u03bb_{clf}L_{clf}(x, y; \\theta)$\nwhere:\n$L_{d1} (x, z, y; \\theta)$ is designed for the conditional discriminator, adjusting the output specifically for class-\ncorresponding data pairs (x,y). Conversely, $L_{d2}(x, z, y; \\theta)$ addresses the unconditional aspect, updating"}, {"title": "Generator conditioning approaches", "content": "To condition the generator, most approaches typically involve directly integrating the label with the gen-\nerator, either through concatenation or by employing some normalization techniques such as conditional\nbatch normalization and adaptive instance normalization Odena et al. (2017); Gong et al. (2019);\nKang & Park (2021); Kang et al. (2021); Hou et al. (2021). These methods effectively incorporate label\ninformation, enabling the generator to produce outputs that accurately reflect the desired attributes.\nIn this section, we explore the various techniques used for generator conditioning. The majority of popular\nmethods rely on batch normalization, as the basis for conditioning. Initially, batch normalization Ioffe &\nSzegedy (2015) was proposed to make the training of deep learning models faster and more stable. Given\nan input batch x \u2208 RN\u00d7C\u00d7H\u00d7W, each batch normalization layer has two learnable parameters, Ybatch and\nBbatch which shift and scale the normalized input, respectively:\n$Z = Y_{batch}(\\frac{X \u2013 \u00b5_{c}(x)}{\u03c3_{c}(x)}) + B_{batch}$\nwhere $\u00b5_{c}(x) = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} X_{nchw}$ and $\u03c3_{c}(x) \\approx \\sqrt{\\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (X_{nchw} \u2013 \u00b5_{c}(x))^{2}}$.\nInstance normalization, proposed in Ulyanov et al. (2017) as an alternative to batch normalization, was\nmotivated by style transfer applications. In instance normalization, the standard deviation (instance(x))\nand mean ($\u00b5_{instance}(x)$) are computed for each individual instance, which can be considered as contrast\nnormalization, whereas in batch normalization (BN), these statistics are computed across the entire batch.\nIn Ulyanov et al. (2017), it was observed that significant improvements could be achieved using instance\nnormalization.\nIn Dumoulin et al. (2017b) conditional instance normalization was introduced to learn different artistic\nstyles with a single conditional network where it takes a content image and a given style to apply and\nproduces a pastiche corresponding to that style. The authors found that to model a style it is sufficient to\nspecialize scaling and shifting parameters after normalization to each specific style,\nSimilarly, conditional batch normalization was used to condition vision systems on text. For instance,\nit was used in de Vries et al. (2017) as an efficient technique to modulate convolutional feature maps by text\nembeddings.\nIn Huang & Belongie (2017), Adaptive Instance Normalization (AdaIN) was introduced. AdaIN can\nbe seen as an extension of instance normalization, where the shift and the scale are not learnt but computed.\nGiven an input content image x and an input style image y the affine parameters are computed as follows:\n$AdaIN(x, y) = \u03c3_{instance}(y) (\\frac{x \u2212 \u00b5_{instance}(x)}{\u03c3_{instance}(x)}) + \u00b5_{instance}(y)$\nBy computing the the affine transformation, AdaIN aligns the channel-wise mean and variance of the in-\nput image x to match those of the style image y. the authors showed that AdaIN lead to better style"}, {"title": "Experiments", "content": "In this section, we conduct a comparative analysis of various conditioning techniques across multiple datasets.\nTo ensure fairness and consistency, our evaluation of different architectures"}]}