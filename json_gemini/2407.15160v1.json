{"title": "When Can Transformers Count to n?", "authors": ["Gilad Yehudai", "Haim Kaplan", "Asma Ghandeharioun", "Mor Geva", "Amir Globerson"], "abstract": "Large language models based on the transformer architectures can solve highly complex tasks. But are there simple tasks that such models cannot solve? Here we focus on very simple counting tasks, that involve counting how many times a token in the vocabulary have appeared in a string. We show that if the dimension of the transformer state is linear in the context length, this task can be solved. However, the solution we propose does not scale beyond this limit, and we provide theoretical arguments for why it is likely impossible for a size limited transformer to implement this task. Our empirical results demonstrate the same phase-transition in performance, as anticipated by the theoretical argument. Our results demonstrate the importance of understanding how transformers can solve simple tasks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated striking performance on a wide array of tasks, from creative writing to solving complex math problems. Given these successes a key question then arises: what can these models do, and just as importantly what can they not do. There are multiple ways to address this question of expressiveness of LLMs. First, it can be studied empirically by probing LLMs and seeking relatively simple tasks that they cannot perform successfully. Indeed recent work has found several such tasks, including \u201cneedle in haystack\u201d [7, 6] as well as extrapolating tasks to longer sequences [8]. A second, complementary, approach is theoretical studies which chart the computational capabilities of transformers [12, 17].\nIn the current work, we focus on a simple task that transformers often struggle with, and analyze it theoretically. Specifically, we consider a very simple \"Query Counting\u201d task, defined as follows. The model is presented with a sequence of tokens, and is then asked how many times a given query token appears in the sequence. For example:\nConsider the sequence a a b baccda. How many times does the letter \u201ca\u201d appear in the sequence?\nOur interest in this problem is that it seems like a basic module needed for working with long input streams. Indeed, there is a long line of work on sketching and streaming, that studies similar tasks [1, 4]. Furthermore, this task can be viewed as an extension of the \u201cneedle in haystack\" problem which has recently garnered much attention, since it was shown that most LLMs struggle with it for long context. Specifically,"}, {"title": "2 Related Work", "content": "Since the introduction of transformer architectures [16] and the success of LLMs, there has been much work on evaluating their performance on various tasks [e.g., see 14]. In particular, much recent work has explored performance on long context tasks, the dependence of accuracy on context length [8], and the ability of models to extrapolate to lengths beyond those seen at training [2].\nThe fact that models often do not perform well on these evaluations has prompted works that try to pin-point the inherent limitations of transformer models. One line of work is to use computational complexity lower bound approaches to show that transformers of a certain size and architecture cannot implement a given function of interest. For example, Sanford et al. [12] show that a certain functions cannot be implemented without transformer size scaling with input size.\nA related line of work is to relate transformers to known complexity classes. For example it has been shown that transformers are Turing complete [17], and that transformers with bounded percision can only solve problems in uniform TC\u00ba [9]. Chain-of-Thought [18] has also been analyzed from an expressiveness viewpoint, demonstrating it can substantially improve the expressive power of transformers [5].\nOur focus here is not on the general capabilities of transformers but rather on a specific, seemingly simple problem, and on the ability of transformers to solve it."}, {"title": "3 Problem Setup", "content": "We consider inputs that consist of a sequence of n tokens: $x_1,...,x_n$. The desired output for these is denoted by y. We consider the following two counting tasks: Query Count (QC) and Most Frequent Element (MFE) where y is defined as follows:\n\u2022 For the QC task: y is the count of the token $x_n$ in the set $x_1, . . ., x_n$ (i.e., $y \\geq 1$ always).\n\u2022 For the MFE task: y is the count of the most frequent token in $x_1, ..., x_n$.\nWe denote the dictionary size by m, namely $x_i \\in \\{1,...,m\\}$. Furthermore, we use the following notations for model-related quantities:\n\u2022 d: the key-size (i.e. embedding dimension of each head).\n\u2022 h: the number of attention heads.\n\u2022 L: the number of layers\n\u2022 p: the numerical precision of computations: We assume that all arithmetic operations (including the softmax) are performed exactly using registers of p bits.\n\u2022 D: the overall embedding dimension, where $D = d \\times h$\n\u2022 The embedding of token i is $v_i \\in \\mathbb{R}^D$.\n\u2022 The query, key, value matrices for layer i, attention head j will be $Q_{i,j}, K_{i,j}, V_{i,j}$. All are matrices in $\\mathbb{R}^{d_h \\times d}$.\n\u2022 $p_i$: the positional embedding for location i.\n\u2022 $u_{i,j}$: the output of head i in layer j.\nMost of our solutions work with an architecture consisting of a single layer and a single head. When this is the case we omit the indices i and/or j, respectively from our notation. Also note that if h = 1 then $D = d$."}, {"title": "4 The Need for Positional Embeddings", "content": "Before analyzing specific solutions for counting, we remark on a general limitation of transformers for the counting problems we consider.\nTransformers use self attention to average over previous representations. The fact that they average rather than sum leads to an interesting limitation on their ability to count. Specifically, it is easy to see that for variable context size, they cannot perform any counting task without the use of positional embedding.\nConsider the QC task and an input sequence $S_1 = x_1,..., x_n$, where the goal is to return the count of $x_n$ in the sequence. Now consider the length 2n sequence $S_2 = x_1,...,x_n, x_1,...,x_n$. The correct output for this sequence is twice the correct output for $S_1$. However, a transformer without positional embeddings that is applied to $S_1$ will have exactly the same output as the one for $S_2$. This follows because averaging is invariant to input duplication."}, {"title": "5 Analyzing Query Count (QC)", "content": "In this section we focus on the QC problem, and ask which transformer architectures can implement it successfully. We first show in Sec. 5.1 that if $d > 2m$ a one-head one-layer transformer can implement QC. We refer to this as the histogram solution. We then show that the histogram solution stops working if $d < m$. The natural question is then whether there are other solutions for the $d < m$ case. We argue that in this case solutions are likely to require calculating the function $1/x$, and show that this function would require an MLP layer of width $n^2$. This means we cannot expect the transformer to extrapolate to long context sizes, and therefore a one-layer transformer is unlikely to be able to implement QC.", "sections": [{"title": "5.1 A \"Histogram\u201d Solution for d > 2m", "content": "We begin by providing a solution for the case where the model dimension is larger than the vocabulary size.\nTheorem 5.1. For the Query Count problem, if $d > 2m$, there exists a transformer that solves it, which has one layer, one head, and an MLP with d neurons.\nWe provide the construction below (see also Fig. 1a). We begin by describing it as a two head solution, but a one-head implementation is also simple using a residual connection. The idea is to construct a his-togram of all previous tokens (i.e. the number of times each token appears) and then given the query token $x_n$ extract the count for this particular token from the histogram.\nFirst, we assume that the embeddings are orthonormal. Namely:\n$v_i \\cdot v_j = \\delta_{ij} \\forall i, j \\in \\{1, ..., m\\}$  (1)\nwhere $v_i$ is the embedding into $\\mathbb{R}^d$ of the dictionary item i. This is possible because of the assumption $d > m$. For simplicity, we assume that $v_i = e_i$ where $e_i$ is the standard basis in $\\mathbb{R}^d$.\nNext, we construct an attention head whose output at position n is the histogram of the tokens up to and including this token. Let $Q_1 = 0$ (the zero matrix) and $V_1 = I_d$ ($I_d$ is the identity matrix in $\\mathbb{R}^d$). Then the output of this attention head is\n$u_1 = \\sum_{i=1}^{n} e_{x_i} = \\sum_{j=1}^{m} c_j e_j$ (2)\nwhere $c_j$ is the number of occurrences of item j in the context normalized by n, the length of the context. That is $c_j = |\\{i \\in [n] | x_i = j\\}|/n$. In words, $u_1$ is a vector in $\\mathbb{R}^d$ whose $i^{th}$ entry is the number of times that token i appeared in the input $X_1, ..., X_n$.\nThe second head is set to simply copy the input embedding. This is done by setting $Q_2$ and $K_2$ such that $K_2 Q_2 = \\tau I_d$ where $\\tau$ is sufficiently large and $V = I_d$. After this we have:\n$u_2 = e_{x_n}$ (3)"}, {"title": "5.2 The Histogram Solution Breaks for d < m", "content": "Our solution in the previous section uses the fact that if $d > m$ we can embed the dictionary into orthogonal vectors in $\\mathbb{R}^d$. When $d < m$ this is not possible. One may try to extend this solution by embedding the dictionary into a collection of \u201calmost\" orthogonal vectors. However any collection of m, say $m \\geq 2d$, vectors in $\\mathbb{R}^d$, contains a pair of vectors whose inner product is at least $\\Omega(1/\\sqrt{d})$ in absolute value. This is a result of the Welch bounds [19] which provide upper bounds on the minimum dot product between m unit-vectors in $\\mathbb{R}^d$. This implies the following lower bound, which states that counting will fail in this regime.\nTheorem 5.2. Consider the \u201cHistogram\u201d solution for the counting problem presented in Sec. 5.1, and embedding vectors $v_1,..., v_m$. For an input $x = (x_1,...,x_n)$ to the counting problem, denote by $C_{x_n}$ the correct solution and by $hist(x)$ the output of the \u201cHistogram\u201d solution. If $m \\geq 2d$, then for any embedding vectors $v_i$\u02bcs there are inputs to the counting problem for which: $|hist(x) \u2013 C_{x_n}| \\geq \\frac{0.25}{\\sqrt{n}}$.\nProof. Let $v_1,...,v_m \\in \\mathbb{R}^d$ with $m > 2d$, and let $A = max_{i \\neq j}<v_i \\cdot v_j>$. Assume without loss of generality that $A = v_1 \\cdot v_2$. By the Welch bounds [19] for k = 1 we have that $A \\geq \\frac{1}{\\sqrt{2d-1}}$. Consider the input $x_1,...,x_{n}$ to the counting problem where $x_1,..., x_{n-c}$ are equal to the same token which is different from $x_n$ and mapped to the embedding $v_1$, and $x_{n-c}, ..., x_{n}$ are all equal to $x_n$ which is mapped to embedding $v_2$. Then the output for the histogram solution is:\n$|hist(x)| =|( (n \u2013 c)v_1 + cv_2, v_2)|$\n$\\geq c + \\frac{n-c}{\\sqrt{2d-1}}$\nBy choosing $c = 0.5n$ and $n = d$ we have the desired result.\nThe theorem implies that even if the dictionary size is only linear in the embedding dimension, any solution will incur an error that grows with the context size. In practice, the dictionary can contain millions of tokens, while the embedding dimension is at most a few thousands, thus this error can be quite large. Note that picking the embedding vectors at random (e.g. i.i.d Gaussians) will only suffer an even greater error than what is stated in the theorem, since the inner product between each two vectors will be larger (with high probability) than the lower bound we used in the proof."}, {"title": "5.3 The CountAttend solution for all d and m", "content": "In the previous section we considered a histogram based solution, which required $d > m$. Here we provide an alternative approach to solve the counting problem which works for any d and m. However, as we shall see, this solution requires a large MLP, that must scale with the length of the input n. As a result, a transformer with a fixed MLP size will not be able to learn such a solution that will work with arbitrary n values.\nWe first present a high level description of this construction, which uses a 1-layer transformer with a single head. The idea explicitly uses attention to count (hence the name CountAttend) as follows (see also Fig. 1b). Assume that the query token is $x_n = 4$, so that we are seeking the number of elements in $x_1,..., x_n$ that are equal to 4, and assume that the number of these elements is 7. Then the token $x_n$ can attend to all other tokens, such that attention weight is high for all i such that $x_i = 4$ (and same for all these) and near-zero otherwise. Thus, the attention weight will be $\\frac{1}{7}$ for all the $x_i = 4$ tokens, including $x_n$. We next need to extract this number and invert it, in order to get the answer 7.\nExtracting the attention weight can be done by using a single coordinate in the positional embedding that is one for position n and zero otherwise. The value aggregation of self-attention can then extract the number in this coordinate. Finally, to invert this number we need an MLP to implement the function 1/x. If the smallest number we need to invert is 1/n then this can be done with an MLP with 4n neurons.\nThe following proposition, proved in Appendix A, summarizes the properties of this construction.", "sections": [{"title": "5.3.1 Limitations of the CountAttend solution", "content": "The advantage of Proposition 5.3 is that it works for any dimension d, and does not restrict d to be larger than m as in the histogram solution. However, the solution in Proposition 5.3 has two major limitations compared to the histogram solution presented in Sec. 5.1. We discuss these below.\nFirst, Proposition 5.3 has an O(n) sized MLP, which is a result of its internal implementation of the function $x \\rightarrow \\frac{1}{x}$ using an MLP, where $x \\in [\\frac{1}{n}, 1]$, and the desired precision is 0.5 (because we are counting, and can round the result). In the proof of 5.3 we used a naive implementation of this function. It is natural to ask if a smaller implementation exists. The following result shows this is not possible."}]}]}, {"title": "6 Analyzing Most Frequent Element", "content": "In this section we consider the task of finding the most frequent element (MFE) in a given sequence of tokens. This problem is very much related to the counting problem, as intuitively it requires to count every token separately and compute the token that appears the maximum number of times. We show that there are strict bounds on the size of the embedding compared to the size of the dictionary, in order for transformers to be able to perform this task.", "sections": [{"title": "6.1 MFE Models Must have d > \u03a9(m)", "content": "We begin by showing a lower bound on the required size of a 1-layer transformer solving the MFE task. The following results establishes that MFE can be implemented only when $dhp = \u03a9(min\\{m, n\\})$. This means that at a fixed precision p and if n > m, the dimension d must grow linearly in the vocabolary size in order to implement MFE. This is the same trend that we saw for the QC problem."}, {"title": "6.2 MFE Can be Implemented when d = O(m)", "content": "The previous result showed that MFE cannot be implemented with a one layer transformer when d is smaller than m. Here we show that it is possible to implement MFE when $d = O(m)$. This implies that $d = O(m)$"}]}, {"title": "7 Experiments", "content": "Our analysis considers the dependence between the transformer model size d, and its ability to perform counting tasks. Specifically, we show that for vocabulary size m that exceeds d, exact counting is likely to become impossible. In this section we perform experiments that support this observation. We begin with results for training a model from scratch and then also consider results with a pretrained LLM (Gemini 1.5).", "sections": [{"title": "7.1 Training from Scratch", "content": "Tasks: We consider the two counting tasks described in the text: Most Frequent Element (MFE) and Query Count (QC). We generate instances of these by sampling sequences of length n uniformly from a set of m tokens. Denote each such sequence by $x_1,..., x_n$. The expected output y for these is as follows:\n\u2022 For the QC task: y is the count of the token $x_n$ in the set $x_1, . . ., x_n$ (i.e., $y \\geq 1$ always).\n\u2022 For the MFE task: y is the count of the most frequent token in $x_1,..., x_n$.\nDuring training and evaluation we sample batches from the above distribution. Evaluation used 1600 examples in all cases.\nModel: We train a transformer model with the standard architectural components (self attention, MLP, layer norm, etc.). We use two layers and four heads (theoretically we could have used less, but optimization was faster with this architecture). Training uses Adam for optimization, with batch size 16 and step size $10^{-4}$. Training is run for 100K steps. Positional embeddings were optimized. For predicting the count y, we use a linear projection on top of the embedding of the last token in the last layer (i.e., we do this instead of the vocabulary prediction). Training was done via colab and takes about 15 minutes per model with the standard GPU provided therein."}, {"title": "7.2 Evaluation of a pretrained LLM", "content": "Our theoretical results highlight the role of vocabulary size in counting problems. Here we provide an exploration of this role in a trained LLM, Gemini 1.5. We provide the model with the query count task. We then vary m, the number of different tokens used in the sequences (e.g., for m = 5 we'll use a sequence with just numbers 1, . . ., 5), while keeping the expected counts of all elements at a constant c = 10. Namely, for each m we use context length mc. As a baseline for these, we also use the same sequence length, but with binary sequences matched to have expected count c for the query token (which we set to \u201c10\u201d throughout). This allows us to estimate the error attributable to just the vocabulary size and not the sequence length and count. Results are shown in Fig. 2b and it can be seen that increasing vocabulary size indeed has a negative effect on performance. Furthermore, this effect cannot be explained just by increasing the sequence size, since the binary curve is lower."}]}, {"title": "8 Conclusion", "content": "We focus on the basic task of counting using a transformer architecture. When the dimension of the model is sufficiently large, we show that this task can be easily implemented by letting the transformer calculate the histogram of the input sequence. For smaller dimensions, we provide theoretical support suggesting that a one layer transformer cannot implement this function. Our empirical results support this phase transition.\nUnderstanding such limitations of transformers are key to developing new architectures. For example, our results show that in a sense it would be impossible to have transformers count arbitrarily well and for long contexts, without increasing the architecture size considerably. Concretely, this suggests that for counting tasks it may be important to delegate to tools [13] such as code execution that do not have the same limitations."}, {"title": "9 Limitations", "content": "While we provide the first results on upper and lower bounds for counting, these are not yet tight, which would have been the ideal result. Specifically, we show impossibility for $d < m$ for MFE with one layer, but do not show that with more layers (e.g., two), though we conjecture this is true. Proving it would require novel technical tools, as it is not clear that the communication complexity argument is extendible to this case. For QC, we show that the inversion based architecture has inherent limitations for one layer, and here too it would be interesting to prove lower bounds for additional layers. In terms of empirical evaluation, we restricted our training experiments to small architectures, but it would be interesting to explore these tasks for models closer to those used in practice. Additionally, it would be interesting to see how pretrained models perform on these tasks after fine-tuning. Finally, it would be interesting to use a mechanistic in-terpretability approach to check which computation is actually being implemented in trained transformers (either pretrained on language or from scratch on counting)."}, {"title": "A Proofs from Sec. 5.3", "content": "Proof of Proposition 5.3. Here we provide additional information about the CountAttend solution. Recall that the idea is for the last token $x_n$ to attend to earlier tokens such that tokens identical to $x_n$ will eventually receive a weight close to $\\frac{1}{C_{x_n}}$, the count of $x_n$ in the sequence. In what follows, we consider the scale of the logits that will provide this result at sufficient precision.\nThe attention weight of a unit-norm token embedding $v_i$ with itself is $e^{v_i^T v_i} = e^T$, and the attention weight of $v_i$ with $v_j$ is $e^{v_i^T v_j} < e^J$ where J is an upper bound on the dot product between any two vectors in $\\mathbb{R}^d$ among a set of m vectors (e.g., as obtained from analysis of random vectors as in Sec. C).\nNow consider the sum of the attention weights (i.e. the denominator of the softmax in the attention module). Let $C_{x_n}$ denote the number of occurrences of $x_n$ in the context, and let $c'$ denote the number of tokens $x_i$ such that $x_i \\neq x_n$. We get that the sum of the attention weights is $c_{x_n}e^T$ plus a quantity bounded by $c'e^J$. If we divide this by $e^T$ then we get that the normalization factor equals to $c_{x_n}$ plus \u201cnoise\u201d bounded by $c'e^{T(J-1)}$. From this we can recover no if $c'e^{T(J-1)} < \\frac{1}{2}$. We clearly satisfy this inequality if\n$T> \\frac{log(2n)}{1-J}$\nSubstituting the bound we have for J for a random embedding (see Sec. C) we get that we need T such that:\n$T=\\Omega \\bigg(\\frac{log(2n)}{1-\\sqrt{\\frac{log m}{d}}}\\bigg)$\nUsing the above, we obtain that the output of the attention is $\\frac{1}{C_{x_n}}$ to within 0.5 accuracy in the inverse. To get $C_{x_n}$ we need an MLP that inverts $\\frac{1}{x}$. This can be done as follows.\nIt is well known that we can implement a \u201cdelta function\u201d using four ReLU neurons. For example we can approximate a delta function of height h between a and b, by $(max(0, x \u2212 a) \u2013 max(0, x \u2212 (a + \\epsilon) \u2013 max(0, x - b) + max(0, x \u2212 (b + \\epsilon)))$ for some sufficiently small \\epsilon. We use 4 ReLU neurons to implement a \u201cdelta function\u201d between $1/(k \u2212 1/2)$ and $1/(k + 1/2)$ of height k for each k = 1, ..., n."}, {"title": "B Proof from Sec. 6", "content": "Proof of Thm. 6.1. Our proof relies on the following set disjointness lower bound [20]. (It is similar to a lower bound argument in Sanford et al. [12], but simpler since we assume that all arithmetic in the trans-former is performed exactly by registers of p bits.) Alice and Bob are given inputs a,b \u2208 {0,1}n, re-spectively. Their goal is to compute maxi aibi by sending single bit messages to each other in a sequence of communication rounds. The lower bound says that any deterministic protocol for computing maxi aibi must have at least n rounds of communication.\nWe construct a reduction from the set disjointness problem to the MFE task. We assume for ease of notation that the length of the context is 2n, and also assume that m > 3n. If m < 3n then we set the context size to be $n' = \\frac{m}{6}$ and continue the proof as is with n' instead of n. Note that since the lower bound is given by \u03a9(min{m, n}), using n' instead of n will provide a lower bound that depends on m. In fact, min{m, n} can be viewed as the \u201ceffective \u201d dictionary size, which is the maximal number of different tokens that a transformer sees given an input sequence of length n.\nAssume that Alice and Bob received inputs a,b \u2208 {0,1}n. Suppose we have the following distinct tokens in our dictionary (which is possible by our assumption on m): $s_1, . . ., s_n, y_1, ..., y_n, z_1,..., z_n$. We"}, {"title": "C Inner Product of Random Vectors", "content": "Let $v_1, . . ., v_m$ be random unit vectors in $\\mathbb{R}^d$ where each coordinate in any vector is \u00b11/\u221ad with probability 1/2, independently. Hoeffding's inequality (C.1) implies that with probability $1 - \\frac{1}{poly(m)}$, $|v_i \\cdot v_j| = O(\\sqrt{\\frac{log m}{d}})$ for all pairs $i, j \\in [m], i \\neq j$.\nIndeed, $v_i \\cdot v_j$ is a sum of d random variables of values \u00b11/\u221ad so by (4) we have\n$\\text{Pr} (v_i \\cdot v_j \\geq t) \\leq 2e^{-\\frac{dt^2}{2}}$\nand therefore for $t = O(\\sqrt{\\frac{log m}{d}})$ we get that the dot product is large than t with polynomialy small proba-bility for all pairs $v_i, v_j$.\nProposition C.1. Hoeffding's inequality states that if $X_1, . . ., X_n$ are independent random variables such that $a_i \\leq X_i \\leq b_i$ then\n$\\text{Pr} (|S_n - E[S_n]| \\geq t) \\leq 2e^{\\frac{2t^2}{\\sum_{i=1}^n(b_i-a_i)^2}}$ (4)\nwhere $S_n = X_1 + ... + X_n$."}]}