{"title": "Explainability of Deep Learning-Based Plant Disease Classifiers Through Automated Concept Identification", "authors": ["Jihen Amara", "Birgitta K\u00f6nig-Ries", "Sheeba Samuel"], "abstract": "While deep learning has significantly advanced automatic plant disease detection through image-based classification, improving model explainability remains crucial for reliable disease detection. In this study, we apply the Automated Concept-based Explanation (ACE) method to plant disease classification using the widely adopted InceptionV3 model and the Plant Village dataset. ACE automatically identifies the visual concepts found in the image data and provides insights about the critical features influencing the model predictions. This approach reveals both effective disease-related patterns and incidental biases, such as those from background or lighting that can compromise model robustness. Through systematic experiments, ACE helped us to identify relevant features and pinpoint areas for targeted model improvement. Our findings demonstrate the potential of ACE to improve the explainability of plant disease classification based on deep learning, which is essential for producing transparent tools for plant disease management in agriculture.", "sections": [{"title": "1 Introduction", "content": "Agriculture is primordial for human life. It is the foundation for food production and economic stability around the world. With the global population expected to reach 10 billion by 2050, food production must increase by 60% to meet growing demand [26]. To achieve this, it is required to boost crop yield quality and minimise food loss caused by diseases. Hence, efficient and innovative agricultural practices are more important than ever."}, {"title": "2 Related work", "content": "Several explainability methods have been proposed, with the most common approaches falling into two categories: visual-based explanations and concept-based explanations. In this section, we will describe each set of methods and provide examples of their use in the context of plant disease classification.\nVisual-based explanation [32,27,25] generates visual cues highlighting the parts of an input image that contribute most to the model's prediction [22]. These methods produce heat maps or saliency maps that indicate which areas of an image the deep model considers important when making its predictions. Zhou et al. [32] proposed Class activation mapping (CAM), a technique that replaces the fully connected layers at the end of a CNN with a global average pooling layer (GAP), applied on the last convolutional feature maps. The CAM is then computed as a weighted linear sum of these feature maps, where the weights are determined by the output class probabilities of the CNN. This resulted in a heatmap that highlighted the regions of the image that were most strongly associated with the predicted class. Selvaraju et al. [27] proposed Gradient-weighted Class Activation Mapping (Grad-CAM), a generalisation of CAM that can work with any type of CNN to produce local explanations. This is in contrast to CAM, which specifically needs global average pooling. LIME was proposed by Ribeiro et al. [25]. It generates explanations by perturbing different parts of the input image (e.g, removing or altering sections) and observing how these changes affect the model's prediction. This approach produces a visual explanation, typically in the form of a heatmap, that identifies the image regions most influential in the model's decision-making process.\nIn plant disease classification, this form of explanation is particularly useful because it allows researchers and agricultural experts to visually inspect which features of a diseased plant (e.g., spots and discoloration areas) the model is using to make its diagnosis. Kinger et al. [19] presents a review of visual explanation methods and their application in plant leaf disease detection. The authors started by defining the key concept of explainability. They then discussed recent advancements, focusing on popular methods such as LIME and Grad-CAM. Then, using the Plant Village dataset, the authors fine-tuned the VGG16 model and applied Grad-CAM, Grad-CAM++, and LIME to evaluate the effectiveness of each technique in explaining the model's predictions. Their findings suggest that these visualisation techniques can help farmers better understand the models predictions and make more informed decisions.\nAdditionally, in [14], Grad-CAM was used to enhance the explainability of automated grapevine downy mildew disease classification. It helped in visualising the critical image areas that influences the model's decision-making process, such as the symptomatic regions like oil spots on grapevine leaves associated with downy mildew. The approach allowed for a more transparent understanding of how the model identifies disease indicators and accordingly increases confidence in its practical application for field diagnosis. Also, [21] explored a transfer learning approach with three pretrained models EfficientNetV2L, MobileNetV2, and ResNet152V2 to detect plant diseases using the Plant Village dataset. EfficientNetV2L performs best with 99.63% accuracy. They hence used LIME to explain the decisions of the model. However, LIME explanations can be misleading as they may focus on features that are not actually important for the model's prediction. Toda et al. [30] investigated CNNs' predictions for plant disease classification using various visualisation methods. They found that the CNNs can capture the colour and textures of lesions specific to respective diseases. They also found that some layers were not contributing to the inference and then removed them without affecting the classification accuracy. In [4], an alternative approach addressed the explainability problem through a Teacher-student paradigm, trained jointly using multi task learning. The shared representations between the models were used to visualise key image regions critical for classification. This approach produced sharper visualisations compared to other existing methods. However, this method was computationally and time intensive. In another study, [13] presented a new explanation approach by identifying the top-k high-resolution feature maps that contribute most to the model's predictions. They found that the highlighted visual features were closely aligned with those used by experts to assess disease severity. However, this method did not reveal the model's internal mechanisms in detail. Even though these visual based explanation methods have been frequently used in plant disease DL-based classification literature, they suffer from some key disadvantages. One example is the lack of specificity where the method often highlights a large region of the images without clearly indicating why they are essential for the model which leads to subjectivity and differences of interpretation between different users. Another example is their fragility and sensitivity to adversarial perturbations [11]. Where small changes in input can cause the method to focus on entirely different regions which undermine their reliability. Also, other studies have highlighted the potential unreliability of these methods [18]. Since they generate importance maps specific to individual input samples, they provide only local explanations, lacking a comprehensive view of the model's overall behaviour. Additionally, one of their significant drawbacks is the lack of clarity and expressiveness for users. For example, the influence of a single pixel on classification offers little meaningful insight, and the interpretability becomes more complex with a large number of features [23]. This reliance on pixel-level information makes it limiting where higher-level concepts (such as shapes or texture) are more relevant for understanding the model decisions. To overcome these limitations, concept-based approaches were introduced.\nConcept based explanation methods [17,12] offer a more abstract interpretation of what the model is using to make its decision in terms of higher-level human understandable concepts. These concepts could be predefined or learned and range from a simple color to an object or a complex idea [16]. This is suitable for plant disease classification, where it is important to ensure that the model recognizes disease-specific concepts (e.g., leaf spots, discoloration, or texture) rather than irrelevant background features.\nOne of these methods is the TCAV approach, which was proposed by [17]. It explains how a model makes predictions by examining the influence of user-defined concepts on the model's decisions. A key component within TCAV are the concept activation vectors (CAVs) that represent specific concepts (such as \"stripes\" or \"colors\") in the model hidden layers. Using these CAVS, TCAV can measure the importance of the defined concept to the model for the prediction globally. More details about this will be given in Section 3.2.\nOur previous work [2] provided an initial exploration into understanding the semantic concepts that CNNs learn during plant disease diagnosis using TCAV. This study was the first to collect various related disease-based concepts to analyse CNN interpretations in this context.\nEven though TCAV has proven its usefulness in defining and testing specific concepts such as discolorations, disease patterns, and symptoms (i.e., blotchiness, Crackedness, and wrinkledness) [2], it requires users to have a good understanding of which concepts are the most relevant and to have enough resources to collect adequate examples.\nThis manual approach could be a massive burden in plant disease deep learning-based classification, especially when users may not know which concepts are the most relevant for distinguishing between diseases.\nAlso, they may lack sufficient labelled data to define these concepts effectively. This is where Automated Concept-based Explanation (ACE) [12] offers a significant advantage. ACE was proposed in [12] to tackle the problem above. It works on automatically extracting relevant concepts directly from the target class images without the need for manual interference. More details on ACE are provided in Section 3.2.\nThis automatization is especially valuable for plant disease diagnosis, where visual symptoms are not always well-defined. We argue that ACE will help in revealing new concepts that could be important for the prediction but were unnoticed before. This can reduce the problem of mismatch between the concepts a user defines and the actual concepts the model uses to make a decision. This conflict becomes evident in situations where the dataset is imbalanced or contains mislabeled instances, which is often the case in real-world plant disease datasets [3]. Some diseases could be well represented in such cases, while others have few examples. This can make the model focus on features of more frequent diseases while ignoring the rest. For example, while a user defines a concept based on his domain knowledge (e.g., disease symptom or texture), the model could instead only focus on a generic feature that is more common across the majority class to make its decision (e.g., Leaf yellowing).\nIn summary, while significant progress has been made in explainability methods, and their adoption in plant disease classification is increasing, there remains a need for approaches that not only provide visual insights but also uncover high-level concepts and patterns relevant to this domain. This study addresses that need by applying ACE in plant disease classification."}, {"title": "3 Materials and Methods", "content": "Model. The model selected in this study as the basis for our investigation and experimentation is InceptionV3 [29]. This choice is motivated by the wide use of the model in the research community of plant disease classification [9,20,24,28]. Also the inception modules use multiple filters that could capture features at different scales which make it suited to capture different patterns present in the plant diseases image dataset. Moreover, the InceptionV3 structure facilitates an easy extraction of the layer and its associated activations. Besides, in this work our primary goal is to understand how these kinds of deep models work rather than focusing on optimizing the classification performance.\nTo train the network, we used the fine-tuning transfer learning technique [31]. This is based on transferring the knowledge gained from training the model on a larger data set to a smaller one. Hence, in our case the InceptionV3 model was loaded by the pretrained weights from the ImageNet dataset [7]. Next new top layers were added to the model. The new layers consisted of a global average pooling layer and three dense layers with a dropout layer.\nFor training and optimizing the weights on the plant disease dataset, we froze the first 52 convolutional layers and made the rest trainable for InceptionV3. Training optimization was carried out via stochastic gradient descent optimizer with a learning rate of 0.0001 and momentum of 0.9. We used a batch size of 64 and 30 epochs for training.\nWe use data augmentation techniques to increase the dataset size in the training set while including different variations. This is important since if you train a deep learning model with few examples, it will either underfit by not learning the data well or overfit by not generalising well to not previously seen data. In this study, the images are augmented using different transformations such as random rotations, zooms, translations, shears, and flips.\nPlant Village dataset. The Plant Village dataset is a public repository that contains 54,323 images of 14 crops and 38 different types of plant diseases [31]. It has been extensively used by the community of plant disease image classification and made a significant contribution to the advancement of applying computer vision and machine learning in agriculture.\nFigure 1 presents an example image of each disease class alongside its corresponding name. The dataset was initially split into 80% for training and validation and 20% for testing. Then, the training and validation portion was further divided, with 80% used for training and 20% used for validation.\nEvaluation metrics To evaluate the performance of the trained InceptionV3 model, we employed several key evaluation metrics: Accuracy, Precision, Recall, and the F1 Score. These metrics together provide a comprehensive view of the model's performance. The full equations of these metrics can be found in the following works [6,15]."}, {"title": "3.2 Network explanation with ACE", "content": "In their work, Kim et al. [17] presented concept activation vectors (CAVs) which employ directional derivative and linear separability to assess the significance of specific user-defined concepts for the deep model functioning.\nBased on the foundation laid by [17], a subsequent study [12] introduced Automatic based concept explanation. ACE sought to automate the collection of concepts and explore how certain patterns within an image contributed to the model's decision making mechanism.\nThe ACE method consists of three main steps as shown in Figure 2. The first step consists in conducting a multi resolution segmentation of the images. This means dividing the same image into multiple fragments each time with a different resolution. To achieve this, ACE approach employs the SLIC (Simple Linear Iterative Clustering) algorithm [1]. SLIC groups pixels that exhibit similar properties such as colour, texture and intensity into clusters called super-pixels. To obtain concepts of varying complexity (coarse to fine), three levels of segmentations are used by ACE (see Figure 2, Step1).\nIn the second step of ACE, similar segments are gathered to form examples of the same concept. To achieve this, the intermediate activations of the trained CNN model are used as representation to measure perceptual similarity. This is done by first resizing the segments to the image size adequate and expected by the model and then passing each segment through CNN and measuring the Euclidean distance between their activations in the chosen layer.\nUsing k-means clustering, concept patches are gathered into meaningful concept clusters (see Figure 2, Step 2). To ensure concept consistency within each cluster, segments that stand out and have low similarity to the other segments are removed [12].\nFinally, the TCAV scores (see equation 2) for each concept is computed to retrieve the most significant ones for the classification task. In the following, we will describe the TCAV process in detail.\nLet k be a class label, $X_k$ all inputs with this label, I an activation layer from a trained CNN model, C a concept of interest (in our case the concept patches), and $S_{c,k,l}(x)$ the directional derivative.\nFirst, concept patches are fed to the CNN model up to layer I to extract their activations. Second, these activations are then used to train a linear classifier (SVM) to differentiate between the concept and random counterexamples. The vector orthogonal to the decision boundary separating the two classes, i.e., the vector pointing in the direction of the representations of the concept images, is the CAV $v_c$.\nThe dot product between the vector $v_c$ and the output gradient $\\nabla h_{i,k}$, which optimises the prediction of class k is computed to measure the sensitivity $S_{c,k,l}(x)$ (see equation 1) to each concept."}, {"title": "3.3 Experimental setup", "content": "The experiments for both training the model and explainability method were conducted on a server equipped with two NVIDIA Tesla V100 GPUs, each with 16 GB of GPU memory.\nThe server also has 128 GB of system RAM which ensures enough memory for handling the computational demands of training the deep learning model and the explainability task. We implemented the model using the Keras [5] deep learning framework.\nIn this work, we modified ACE method to use it with our trained keras model, incorporating the integration features from the original code. [12]."}, {"title": "4 Results and Discussion", "content": "In this section, we present the results of training and testing a fine-tuned InceptionV3 model on the Plant Village dataset. The model was evaluated based on its ability to classify both healthy and diseased plant leaves across multiple species. The performance metrics include accuracy, precision, recall, and F1-scores (see Section 3.1). After training, our model achieved a training accuracy of 0.98 and a validation accuracy of 0.94, and a testing accuracy of 0.95. These results demonstrate the model's ability to learn effectively from the training data while maintaining high accuracy on unseen data."}, {"title": "4.1 Model training and performance analysis", "content": "Figure 3 presents the precision, recall, F1-score for each class, providing a detailed view of the model's performance across each disease.\nSome classes exhibit perfect performance with precision, recall, and F1-scores close to or at 1.00 (such as Apple Black Rot (C2), Apple Healthy (C4), and Cherry Powdery mildew (C7)).\nHowever, closer examination reveals that certain classes, particularly those with fewer samples or more complex visual patterns, had notably lower recall and F1-scores. For example, the Apple Scab (C1) class had a recall of 0.74 and an F1-score of 0.85, suggesting that the model struggled to correctly identify all instances of this disease.\nSimilarly, Tomato Early blight (C30) showed a slightly lower performance, with a recall of 0.59 and an Fl-score of 0.72. These differences indicate that while the model performs well on many classes, it struggles to generalise to certain underrepresented or visually complex disease categories.\nThis observation motivates the need for explainability methods to better understand the model's decision-making process. The ACE method is particularly essential here. By analysing the most salient concepts the model uses for classification, ACE can verify whether the model is using expert-relevant concepts such as disease symptoms, color or shape. It also could help in uncovering potential biases or unintended features that the model may rely on for classification."}, {"title": "4.2 Experiment 1: Insights into the model: Examples of discovered concepts with high and low TCAV scores", "content": "Figure 4 presents examples of the concepts discovered by the ACE algorithm, organised from the most to the least salient. In each row, the top displays the segmented concepts while the buttom shows the original images from which these segments were extracted.\nEach column corresponds to a different class, and within each cell, the discovered concepts are displayed. For each class, three discovered concepts with the highest TCAV scores and one with the lowest score are shown. These high TCAV scores indicate that these concepts play a significant role in influencing the neural network's predictions for that class while it is the opposite for the lowest one.\nTo present a comprehensive understanding of these concepts, we have included three randomly selected examples for each concept."}, {"title": "4.3 Experiment 2: Identifying concepts shared across the same disease in different plant species.", "content": "Another interesting thing is to investigate the concepts the model used when identifying the same type of disease but in different types of plants. This will help us to understand if the model captured the disease pattern regardless of the leaf type.\nLate Blight diseases affect both potatoes and tomatoes. Infected leaves often display green to brown areas of dead tissue bordered by a pale green or grey edge. Under conditions of high humidity and wet weather, late blight infections may look water-soaked or dark brown, frequently giving the leaves a greasy appearance [10]."}, {"title": "4.4 Experiment 3: Insights into the model from discovered concepts", "content": "Examining the identified concepts that received high TCAV scores provides valuable insights into what the model is focusing on when identifying plant diseases and reveals interesting correlations. Some of these correlations are desirable and in accordance with the expert intuition but others are not and may help in discovering the biases that could be found in the dataset or learned by the model itself. In the following sections, these findings will be studied in detail."}, {"title": "A. Desirable correlations: distinguishing between Healthy and Diseases leaves", "content": "Figure 6 reveals some of the desirable correlations found. In the Black rot disease class, regions of the leaf with disease spots had significantly higher TCAV score than the unaffected parts of the leaf. This means that the disease spots had a big impact on the model decision making process while the unaffected parts had less of an impact. This shows the model's ability to effectively differentiate between healthy and disease patterns and make informed classification.\nIn the healthy blueberry class, we see that the model captured contextually relevant concepts. For instance, the healthy patches of the leaves were identified as significant concepts while the background of the leaf was correctly recognized as not important for the classification task.\nThis could be used as a demonstration for the network's ability in learning the important features associated with the corresponding class and ignoring the rest."}, {"title": "B. Desirable correlations: healthy classes and venation patterns", "content": "Another interesting insight was found when checking most significant extracted concepts (TCAV score: 1.0) for healthy classes of different plants within the dataset (see Figure 7)."}, {"title": "C. Undesirable correlations: Bias detection", "content": "ACE helped to discover some of the undesirable correlations learned by the model and identify the bias within the dataset it was trained on notably concerning background and shadow factors.\nBackground Bias: Figure 8 highlights the presence of background bias.\nThis is a phenomenon where the model mistakenly associates certain background colors or patterns with specific classes.\nInstead of focusing on the main object which is in our case the leaf type and the disease patterns, the model uses the background features as a distinguishing component for classifying the corresponding class.\nThis bias occurs when images of a particular class frequently appear against a specific type of backgrounds. Thus, the model might begin to correlate the background color or texture with that class. ACE has successfully detected this issue by identifying variations in background colors across different images, which may have influenced the model's decision-making process.\nFor example, for the Scab Disease (Apple) the background apears to be light purple and uniform across different images while for Spider mites (Tomato), the background appears darker and grey-brown. These differences in color and brightness if the background across the different classes may be leading the model to unintentionally using it as indicators of the classes. This finding is significant, as it highlights how unintended elements of an image, like the background, can create misleading associations.\nShadow Bias/ Light Bias:\nAnother bias that was identified through the discovered concepts is the shadow of the leaves on the background, which occurred when the leaves were photographed under certain lighting conditions. This could lead the model to incorrectly associating the dark shadows with disease symptoms, particularly those that are marked by black or dark-coloured spots. For example, diseases like black rot (see Figure 9) or other infections that cause dark patches on leaves could become confused with the black shadows in the background. Also this bias seems to not just affect the diseased classes but also the healthy classes (see Figure 9 Tomato and Blueberry healthy examples). The model may have learned to associate the dark shadows in the background as a feature relevant to both diseased and healthy classes, leading to misclassification.\nThis highlights the value of using the ACE method for model inspection. By identifying such biases, researchers can take corrective actions to improve the dataset's representation. These actions may include augmenting the dataset with more diverse samples and refining image capture techniques to eliminate unintended influences like shadows or background variations.\nSuch improvements help ensure that the model focuses on the biological relevant traits and concepts for plant diseases classification which will lead to more accurate and reliable classifications."}, {"title": "4.5 Experiment 4: Lowest recall and F1 score classes discovered concepts", "content": "Another interesting insight appeared when we checked the most salient concepts for classes with low recall and F1-scores. We found that these concepts were mostly related to the background rather than the actual disease features. This suggests that the model became confused and failed to learn the correct patterns associated with these classes, instead focusing more on the background as a distinguishing feature. As a result, this contributed to the poor performance in accurately detecting them.\nThis finding indicates that the classifier may struggle with these specific classes since it doesn't effectively identify the relevant disease features. To address this, incorporating additional training data with more varied and diverse backgrounds could help the model learn to focus on the leaf characteristics rather than the background. Additionally, checking for class imbalance could be beneficial, as imbalanced data may be further affecting the model's ability to correctly classify these classes. For instance, due to dataset constraints, only 31 images were available for testing in the Potao Healthy class. Ensuring balanced representation across all classes, combined with background diversity, could significantly improve the model's performance."}, {"title": "5 Conclusion", "content": "In this work, we present the first study of Automated Concept-based Explanation (ACE) to deep learning-based plant disease classification to enhance the explainability of deep learning in agricultural disease diagnosis. Our objective is to uncover the specific visual concepts that the model uses for its decisions. This is an essential step toward developing transparent and trustworthy tools for plant disease management. Using the well-known InceptionV3 model and Plant Village dataset, we demonstrate how ACE can automatically identify significant visual patterns that influence the model's decision-making.\nThrough a series of experiments, we demonstrated ACE's ability to provide valuable insights into plant disease classification by automatically identifying key visual concepts, such as spots or infected areas, while also revealing potential sources of error, including misleading correlations with background or shadow elements. Furthermore, ACE also highlighted areas where the model struggled, particularly in classes with lower recall and F1-scores, which allows us to identify where targeted improvements could be most beneficial. Hence, this approach holds significant value for various stakeholders. It will increase users' trust, such as plant experts, agriculturists, and farmers, by clarifying why predictions are made. It also supports data scientists and deep learning researchers in diagnosing issues and developing more robust models. In future work, we aim to apply ACE to larger and more diverse datasets that include real-world backgrounds. Also, we would like to test its applicability to other deep learning architectures. Furthermore, integrating ACE into a real-time tool could enable users to explore and validate concept clusters interactively. Such an approach would increase the practicality of ACE and thus support users in refining and improving the model's accuracy and trustworthiness."}], "equations": ["Sc,k,l(x) = lim_{\\epsilon \\to 0} \\frac{h_{i,k}(f_i(x) + \\epsilon v) \u2013 h_{i,k}(f_i(x))}{\\epsilon} = \\nabla h_{i,k}(f_i(x)).v", "TCAVQ_{C,k,l} = \\frac{|x \\in X_k; S_{c,k,l}(x) > 0|}{|X_k|}", "4.1 Model training and performance analysis", "content\": \"Figure 3 presents the precision, recall, F1-score for each class, providing a detailed view of the model's performance across each disease.\nSome classes exhibit perfect performance with precision, recall, and F1-scores close to or at 1.00 (such as Apple Black Rot (C2), Apple Healthy (C4), and Cherry Powdery mildew (C7)).\nHowever, closer examination reveals that certain classes, particularly those with fewer samples or more complex visual patterns, had notably lower recall and F1-scores. For example, the Apple Scab (C1) class had a recall of 0.74 and an F1-score of 0.85, suggesting that the model struggled to correctly identify all instances of this disease.\nSimilarly, Tomato Early blight (C30) showed a slightly lower performance, with a recall of 0.59 and an Fl-score of 0.72. These differences indicate that while the model performs well on many classes, it struggles to generalise to certain underrepresented or visually complex disease categories.\nThis observation motivates the need for explainability methods to better understand the model's decision-making process. The ACE method is particularly essential here. By analysing the most salient concepts the model uses for classification, ACE can verify whether the model is using expert-relevant concepts such as disease symptoms, color or shape. It also could help in uncovering potential biases or unintended features that the model may rely on for classification."]}