{"title": "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation", "authors": ["Xingchen Wan", "Han Zhou", "Ruoxi Sun", "Hootan Nakhost", "Ke Jiang", "Sercan \u00d6. Ar\u0131k"], "abstract": "Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples (\"optimize\u201d) and using them as demonstrations to regenerate new examples (\"generate\") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show that BRIDGE led to significant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation.", "sections": [{"title": "1. Introduction", "content": "Recent advances in large language models (LLMs) have led to the emergence of in-context learning (ICL) as a promising new learning paradigm (Brown et al., 2020). ICL allows LLMs to learn tasks by simply being presented with a few examples within their context window. A key bottleneck for ICL has been the supported context length of LLMs, but with advancements in novel model architectures, computational infrastructures, and efficient serving methods, state-of-the-art models such as Gemini (Anthropic, 2024; Reid et al., 2024) feature context windows of millions of tokens are overcoming this limitation. Such long-context LLMs open unprecedented avenues for the scaling of ICL \u2013 whereas previous LLMs were limited to processing only up to dozens of examples, current LLMs can now accommodate significantly more examples. More importantly, beyond merely supporting a longer context, it has also been shown that scaling more examples led to substantial performance improvements across tasks, creating a new promising paradigm known as many-shot learning (Agarwal et al., 2024; Bertsch et al., 2024).\nDespite these advances, as a nascent paradigm, many-shot ICL still faces several challenges. Long context windows, while powerful, are computationally expensive and introduce significant latency and cost to serving, making it impractical or uneconomical to fully exploit the maximum context length. Some trade-off decisions have to be made under virtually any realistic setting. To leverage the expanded context while controlling the cost and latency under an acceptable limit, existing works typically investigate the experimental setting whereas many examples as costs permit are simply randomly sub-sampled from the pool of all available examples and dumped into the context window. As observed both in prior works (Agarwal et al., 2024) and our investigations (Fig. 1), using the same number of examples but with different combinations of examples as demonstrations can lead to"}, {"title": "2. What Drives Many-Shot In-Context Learning Performance?", "content": "Several previous studies on many-shot ICL (Agarwal et al., 2024; Bertsch et al., 2024) have investigated the presence of performance gains when we scale the number of examples. A key question that remains unanswered, though, is what exactly leads to this improvement. For example, it is unknown whether the benefit is from scaling examples itself due to expanded knowledge in the context via more examples or because including more examples increases the probability of selecting a small subset of disproportionately positive examples, or a combination of the above with some task specificity. We argue that answering this question is critical \u2013 if the benefit comes from expanded knowledge from including more examples, it suggests that scaling and addressing long-context understanding challenges would dominate the end-to-end performance improvements, and future studies should aim to either include"}, {"title": "3. Methodology", "content": "The findings presented above highlight a significant need for improvements that extend beyond simply increasing the number of examples straightforwardly. Instead, identifying the most useful example subset $e^*$ is crucial both for effective cost-performance trade-offs and for better reasoning path generation for more effective examples. Based on these insights, we propose Bayesian Refinement and Iterative Demonstration Generation for Examples, or BRIDGE in short (described in Algorithm 1 and depicted in Fig. 3, an optimization algorithm aiming to enhance many-shot ICL with intelligent example selection and iterative example generation. At a high level, the outer loop of BRIDGE is structured in two alternating steps of \"optimize\" and \"generate\". In the \"optimize\" step, the algorithm focuses on discovering the optimal subset of examples $e^*$ via a carefully designed (for low complexity,"}, {"title": "4. Experiments", "content": "Model and evaluation data. We conduct experiments on an extensive collection of tasks requir- ing a different set of skills task difficulty on two Gemini 1.5 models (gemini-1.5-pro-001 and gemini-1.5-flash-001) while also testing key findings on Mistral family of models: Mistral NeMo (mistral-nemo-12b) and Mistral Large (mistral-large-2407), and Claude 3.5 Sonnet: 1) BIG-Bench Hard (BBH) tasks encompassing a wide range of challenging numerical reasoning, commonsense problem-solving, logical deduction and tabular reasoning tasks \u2013 we particularly focus on the subset of 16 BBH tasks where the model performances have not saturated; 2) Hendryck's MATH (Hendrycks et al., 2021), a challenging numerical reasoning dataset; 3) GSM-Hard (Gao et al., 2022), a more challenging variant of the classical grade-school GSM-8K (Cobbe et al., 2021) with the numbers in the questions replaced with much larger and rarer ones. To further probe the utility of many-shot learning and BRIDGE in coding tasks, we also experiment on 4) BIRD (Li et al., 2024a), a challenging large-scale text-to-SQL generation benchmark where the LLM has to generate SQLite programs from natural language instructions that are executed on real-world databases. For all datasets, when official train-test split is not available, we randomly split the data into train and test splits; unless stated otherwise, a single unified train split is used both for the generation of demonstrations and is reused for validation (i.e., the objective of the optimize step in Algorithm 1; the test splits are held-out and only used for evaluation of the algorithm. We refer the readers to App. B for detailed descriptions, prompt templates, and evaluation protocols used.\nExperimental setup. For all tasks, we run BRIDGE with K = 3 rounds (i.e., the number of outer-loop iterations in Algorithm 1) and within each round, we allow for $n_{eval}$ = 32 evaluations on the validation set (i.e., the number of inner-loop iterations in Algorithm 2) and we report the results at the end of each \"optimize\u201d and \u201cgenerate\" steps to visualize the iteration process. For baselines, we consider 1) using all provided examples and we consider three variants: a) using query-target only without any generated rationales (Direct), b) first prompt the LLM to generate rationales and answers, and use the concatenation of query-rationale-target as demonstrations, regardless of whether the rationale led to the correct answer (CoT), and c) prompting the LLM with both the query and the final, ground-truth"}, {"title": "5. Related work", "content": "Scaling ICL. Before the advent of the long-context LLMs, early efforts in scaling ICL often studied LLMs customized for long context (Li et al., 2023) or required architectural changes assuming white-box model access (Hao et al., 2022). However, the tasks considered are often limited, e.g., to conventional, discriminative tasks like sentiment classification rather than generative tasks as considered in this work. Furthermore, these often study LLMs that are merely capable of handling many examples, but their behavior may differ significantly to modern, natively long-context LLMs that may actively take advantage of the con- text \u2013 indeed, both these works show mixed results, even significant performance deterioration when scaling up the number of examples, a phenomenon not seen in modern long-context LLMs like Gemini and Claude. Recent works like Agarwal et al. (2024) and Bertsch et al. (2024), on the other hand, reported significant gains in scaling ICL to hundreds or more examples and provided important motivation for our work. However, as mentioned in Sec. 2, these works primarily demonstrate the existence of the benefit from scaling but do not focus on investigate the sources of the gain or improving the cost- effectiveness of many-shot ICL. Additionally, there have also been works focusing on applications of many-shot ICL to multi-modalities (Jiang et al., 2024), LLM jail-breaking (Anil et al., 2024), detecting"}, {"title": "6. Conclusion", "content": "This paper focuses on understanding and enhancing the core factors underlying scaling ICL. We first provide an analysis of the nascent paradigm of many-shot ICL in LLMs and show that notwithstanding the long-context abilities of LLMs, the common practice of na\u00efvely dumping as many examples as practically possible into the context can be both inefficient in cost and suboptimal in performance. Instead, the benefit from scaling examples can often be realized by identifying a subset of influential examples, and that subset can be used as demonstrations themselves to re-generate even more examples. Inspired by the findings, we propose BRIDGE by automatically executing the \u201coptimize\u201d and \"generate\" steps iteratively. We demonstrate that BRIDGE perform competitively on a wide range of tasks, significantly outperforming alternatives. We believe that this work builds the foundation for future research in many-shot ICL. First, we mainly focused on the restrictive black-box LLM setup, which is the most general and model-agnostic. However, for a more relaxed, white-box setup with access to LLM weights, it may be possible to perform optimization more efficiently \u2013 for example, it may be possible to take advantage of the internal representations of the model in reducing the cost of iterative optimization. Second, we currently focus on the \u201creinforced ICL\u201d setup typical for reasoning- heavy tasks \u2013 while we have conducted experiments (e.g., low resource translation tasks) beyond this setup, further validations on other types of tasks would be valuable. Lastly, after optimization, the examples generated by BRIDGE are currently static at test time, and it would also be interesting to combine with a mechanism for sample-dependent ICL optimization to further enhance performance"}, {"title": "A. Derivation of the Approximated Importance Score", "content": "In this section, we give detailed derivation of the importance score used in Sec. 2 to rank the examples on a high level, we use a similar approach to Ru et al. (2021) in determining the importance from the GP surrogate: Recalling that we are given a pool of examples $\\mathcal{E}$ with $|\\mathcal{E}| = m$, a collection of T subsets of $e_i$, each represented as a binary vector $e_i \\in \\{0, 1\\}^m$ and their corresponding scores on the validation set $g(\u00b7) : \\{0, 1\\}^m \\rightarrow \\mathbb{R}$, we first fit a GP regression with $e_{1:T} = [e_1, ..., e_T]$ and $g_{1:T} = [g(e_1, ..., g(e_T)]^T$, as presented in Eq. 2, the mean of the posterior GP $\\hat{g}(\u00b7)$ is given by:\n\\[\\mathbb{E}\\left[\\hat{g}(e) | G_\\mathcal{T}\\right] [\\hat{g}(e)] = k_{1:\\mathcal{T}} (K + n^2I)^{-1} g_{1:\\mathcal{T}},\\]\nwhere we define $G_\\mathcal{T}$ as the shorthand of $[e_{1:T}, g_{1:T}]$ to denote that the fitted function $\\hat{g}(e)$ is fitted on the observed input-output pairs; $k_1 = [k(e, e_1), ..., k(e, e_T)]$ and $k(\u00b7, \u00b7)$ is the covariance function of the GP (we use Matern 2.5 by default). As mentioned in Sec. 2, whereas we do not assume any differentiability property from $g(\u00b7)$ on $e$, since the approximated function $\\hat{g}(\u00b7)$ follows a posterior GP, its gradient w.r.t $e$ is analytically available and is itself a GP, given by:\n\\[\\nabla_e \\hat{g} = \\frac{\\partial\\hat{g}(e)}{\\partial e} = \\frac{\\partial k_{1:\\mathcal{T}}}{\\partial e} (K + n^2I)^{-1} g_{1:\\mathcal{T}},\\]\nnoting that the expensive matrix inversion term, $(K + n^2I)^{-1}$ does not have a dependence on $e$ and can be directly cached from Eq. 3 when we compute the posterior mean. The derivative term is essentially a differentiation operation of the covariance function to the input and can be easily computed either analytically for common kernel choices or via automatic differentiation for popular GP or BO packages like gpytorch (Gardner et al., 2018) or botorch (Balandat et al., 2020).\nWith the computed $\\nabla_e \\hat{g} \\in \\mathbb{R}^m$, we can in principle compute the estimated derivative at any $e \\subseteq \\mathcal{E}$. However, in practice, we find the derivative estimate to be more reliable at the training points of the GP (i.e., $[e_1, ..., e_T]$. We then evaluate the derivative at each of the training points, and the final importance score is marginalized by averaging across the training points:\n\\[\\mathcal{M}(e^{(j)}) = \\frac{1}{T} \\sum_{t=1}^T \\nabla_e \\hat{g} |_{\\epsilon = e_t},\\]\nwhere we use the superscript $(j)$ to denote that the estimated importance of the $j$-th individual example (note the regular font $e \\in \\mathcal{E}$ denoting an individual example instead of the bold-face $\\bf{e}$ denoting a set of examples in $\\mathcal{E}$). We then compute the importance score of all examples in $\\mathcal{E}$, which is then used to generate the assigned ranking in the analysis of Sec. 2 such as Fig. 4."}, {"title": "B. Implementation Details", "content": "B.1. Datasets.\nIn the section below, we give detailed implementation details for the availability, data splitting protocol, input prompts, and licensing information of the datasets used.\nBIG-Bench Hard (BBH). BBH is a collection of 26 challenging reasoning tasks and a task is selected if either 1) if it is studied in the seminal work on many-shot ICL (Agarwal et al., 2024) or 2) if the zero-shot performance of gemini-1.5-pro-001 is below 90%, which indicates non-saturation of performance \u2013 these criteria led to a set of 16 tasks that we consider in Sec. 4. For all tasks, we randomize the data points and reserve 40% (usually 100 samples, but some sub-tasks of BBH benchmark have fewer data-points) as held-out sets for testing, whose inputs and labels are not"}, {"title": "C. Additional Experiments and Results", "content": "C.1. Ablation and Sensitivity Studies\nImportance of Bayesian optimization. To ablate BRIDGE, in Table 9 and Table 8, we compare against a simplified variant of BRIDGE with BO replaced with random search consuming the same evaluation budget (32 per stage) \u2013 we find that while random search is a remarkably strong baseline, BO nevertheless outperformed it consistently at all stages of the BRIDGE pipeline."}, {"title": "C.2. Number of examples", "content": "We show the number of examples used for each experiment corresponding to Table 1 in Table 14."}, {"title": "C.3. Additional Visualizations", "content": "In this section, we show analysis similar to Fig. 4 on tasks not represented in the figure of the main text."}, {"title": "C.4. Using BRIDGE for low-resource translation", "content": "While we have primarily considered the reinforced ICL setup suitable for reasoning and general problem-solving tasks, it is worth noting that the BRIDGE framework may also generalize to other practical settings that benefit from many-shot ICL with some modification on the \u201coptimize\u201d and the \"generate\" steps. In this section, we conduct a preliminary analysis of the applicability of BRIDGE in the context of machine translation (MT) for low-resource languages.\nAs noted in Agarwal et al. (2024) and Reid et al. (2024), low-resource machine translation (MT) is one of the task types where many-shot in-context learning (ICL) has demonstrated remarkable performance. In these tasks, there is often a nearly monotonic improvement in translation quality as more source-target language pairs are incorporated into the context \u2013 as a notable exception to our observations in Sec. 2 that primarily involve reasoning tasks, in low resource MT, we often observe"}, {"title": "C.5. Transferring learned demonstrations from GSM-Hard to GSM-8K", "content": "In this section, we investigate whether the BRIDGE-discovered demonstrations can transfer across related but distinct datasets. Specifically, we investigate the extent to which the demonstrations found on GSM-Hard (Table 2) generalize to the original GSM-8K and we show the result in Table 16, where we compare the performance of the demonstrations directly transferred from GSM-Hard at different stages of BRIDGE against directly optimizing on GSM-8K. We find that whereas the demonstrations generated from (iterative) reinforced ICL led to a small deterioration of GSM-8K performance, we found the transferred demonstrations from BRIDGE led to a small improvement even though the Gemini 1.5 Pro performance on GSM-8K has been rather saturated. While optimizing directly on GSM-8K unsurprisingly led to the highest performance given that there is no distribution shift, we also find that the GSM-Hard demonstrations exhibit considerable generalizability."}, {"title": "D. Computational Cost Analysis", "content": "In this section, we provide a computational cost analysis of BRIDGE. In general, since BRIDGE consists of multiple rounds of \"Optimize\u201d and \u201cGenerate\u201d steps, here we analyze each step in detail.\n\u2022 Optimize: The cost of the \u201coptimize\" step depends on the budget allocated ($n_{eval}$ in Line 5 of Algorithm 2), which is user-configurable. If we opt for iterative optimization (such as using Bayesian optimization in the main section of the paper, or random search in App. C.1), each \"optimize\" step thus entails $n_{eval}$ LLM inferences on the validation set. As shown in the App. C.1, it is also possible to use a non-iterative method based on retrieval or embedding diversity, in which case each \u201coptimize\" step entails a single round of LLM inferences on the validation set (or the train set, if we use the dataset for both training and validation).\n\u2022 Generate: The \u201cgenerate\u201d step always involves a single round of LLM inferences on the train set where we simply use the optimized examples from the \u201coptimize\u201d step above as demonstrations and run inference again on the train set."}]}