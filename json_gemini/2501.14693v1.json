{"title": "Rethinking Table Instruction Tuning", "authors": ["Naihao Deng", "Rada Mihalcea"], "abstract": "Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices and lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and reveal significant declines in both out-of-domain table understanding and general capabilities compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the existing table instruction-tuning works, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed a paradigm shift to data-driven methods for table understanding. Researchers have instruction-tuned various LLMs, particularly the open-source models from LLaMA family (Touvron et al., 2023; Dubey et al., 2024) to improve their ability on handling table-related tasks (Chen et al., 2019; Nan et al., 2022), and pushing the state-of-the-art performance on various table benchmarks (Zhang et al., 2024a,b).\nHowever, existing research has been influenced by the lack of transparency in closed-source LLMs, which often claim to be trained on large-scale datasets without revealing the detailed training process. As a result, open-source efforts have tended to follow these closed-source models by focusing primarily on large-scale datasets (Zhang et al., 2024a), while overlooking the crucial influence of hyperparameter choices. In addition, existing works lack the discussion of how these table LLMs perform on out-of-domain table understanding tasks, and how they compromise their general ability when specializing on table tasks. We argue that out-of-domain table understanding is crucial for table LLMs, as it reflects how well these models generalize to unseen table tasks. In addition, the general capabilities of these models are still important for handling table-related tasks. For instance, instruction following is crucial in real-world applications where end-users may request specific input-output formats (e.g., The user may request the model to return the answer in JSON). Additionally, stronger reasoning capabilities and comprehensive general knowledge can enhance these models' ability to handle diverse scenarios, such as interpreting user queries and reasoning over complex data. Therefore, having an understanding of these table LLMs' general capabilities gives us a comprehensive understanding of these models' limitations in our practical usage.\nIn this paper, we first evaluate the existing table LLMs in terms of their out-of-domain table understanding ability and their general abilities. We reveal that existing table LLMs suffer from a significant decline in terms of these abilities compared to their base models. Sometimes, the performance decline on general reasoning benchmarks, such as AI2ARC, can be up to 20 percentage.\nWe then select the latest LLaMA 3.1 8B Instruct model, and proceed to explore how hyperparameter choices influence the model's performance. Our analysis reveals that learning rate plays a crucial role in shaping the model's table understanding ability and influencing the model's general ability. A large learning rate, as seen in the existing table LLMs, compromises the model's general capabilities and leads to suboptimal table understanding performance. On the other hand, a small learning rate, while effectively preserving the model's general capabilities, fails to sufficiently improve its table understanding ability. In addition, we find that it is possible to achieve strong table understanding ability with a much smaller amount of training data - for instance, 2,600 in Section 4. Our training size is significantly smaller compared to the two million instances used by TableLLaMA (Zhang et al., 2024a), and ten times smaller than that of TableBenchLLM (Wu et al., 2024), highlighting the potential to reduce annotation costs in future model development. We also explore the effects of epoch numbers and the task synergy, and discuss our findings in Section 3.\nBased on our findings, we carefully select the hyperparameters and instruction-tune the LLaMA 3.1 8B Instruct model, resulting in TAMA, which demonstrates strong table understanding ability and general capabilities (Figure 1).\nIn summary, our contributions are three folds:\n\u2022 We examine the existing table LLMs and reveal that these table LLMs do not generalize to out-of-domain table tasks and show compromised general capabilities compared to their base model.\n\u2022 We reveal the impacts of the often-ignored hyperparameter selection such as the learning rate, number of training instances, etc. We find that the commonly-adopted learning rate can be too large, and may lead to suboptimal table understanding performance and compromises the model's general capabilities. In addition, we can achieve strong table understanding ability with a much smaller amount of training data compared to the existing works.\n\u2022 Based on our findings, with careful hyperparameter selection, we instruction-tune LLaMA 3.1 8B Instruct model with 2,600 table instruction data. As an 8B size model, our resulting model, TAMA achieves performance on par with, or even exceeding GPT-3.5 in table understanding tasks, and in some cases surpasses GPT-4, while retaining the general capabilities of its base model. Moreover, TAMA exhibits strong out-of-domain table understanding and general capabilities (Figure 1)."}, {"title": "Evaluation of Existing Table LLMs", "content": "Models to Evaluate. Table 1 provides a comprehensive overview of the existing table LLMs. As we do not have access to the closed-source table LLMs, we focus on the evaluation of the open-source ones, including TableLLaMA (Zhang et al., 2024a), TableLLM (Zhang et al., 2024b), and TableBenchLLM (Wu et al., 2024). All of these open models are fine-tuned with all parameters being updated.\nEvaluation Datasets. Table 2 provides the datasets on which we test these table LLMs in terms of their out-of-domain table understanding ability and their general capabilities. We choose Table-Syn (Li et al., 2023) to test these table LLMs' out-of-domain table understanding ability, as none of them has been fine-tuned on this dataset."}, {"title": "Findings", "content": "Existing Table LLMs possess limited out-of-domain table understanding ability. In Table 3, all the existing table LLMs suffer from performance drops on Table-Syn compared to their base models. Though these table LLMs achieve SOTA performance on various benchmarks (Zhang et al., 2024a,b), such a performance decline reveals their limited out-of-domain table understanding capabilities, which aligns with the findings by Zheng et al. (2024).\nExisting Table LLMs demonstrate poor instruction-following ability. In Table 3, both TableLLaMA and TableLLM show significant drops in performance on IFEval (Zhou et al., 2023), with accuracy declines of 5.63 and 17.86, resulting in a score of 25.78 and 30.46, respectively. While TableBenchLLM maintains a similar score to its base model (32.85 compared to 32.13 for LLaMA 3.1-8B), this performance is still limited compared to 83.57 by GPT-4 reported by Zhou et al. (2023).\nAt such low instruction following scores, existing table LLMs cannot consistently follow instructions such as \"return the answer in JSON format\" as shown in Table 6 in Section 4.3 and Tables 17 to 19 in Appendix E, limiting the model's usage if the end users need data extraction that requires certain answer format.\nExisting table instruction tuning compromises models' general capabilities. Existing table instruction-tuning methods lead to significant drops in accuracy on general benchmarks such as MMLU, AI2ARC, GPQA as shown in Table 3. For instance, compared to their base models, TableLLaMA experiences a decline of 13.95 accuracy score on MMLU, while TableLLM and TableBenchLLM lose 8.79 and 9.41, respectively. Appendix B provides further discussion of the model's performance corresponding to each category in MMLU benchmark. On the general reasoning benchmarks such as AI2ARC, the drop can be as large as 20.90 for TableBenchLLM, showing that the existing table instruction tuning hurts their base model's reasoning ability. This limits the existing table LLMs' usage if there are general knowledge or reasoning involved in end users' request."}, {"title": "Hyperparameter Exploration", "content": "Table 1 reports the hyperparameters used in the existing table instruction tuning works. While often overlooked or treated as technical details, hyperparameter selection plays a critical role. The impact of factors such as learning rate, and number of epochs should not be underestimated, as they significantly influence both the table understanding and general ability. In the following subsections, Section 3.1 introduces the model and datasets used in our analysis experiments, Section 3.2 provides the findings and the choices we make that lead to our model in Section 4."}, {"title": "Experimental Setup", "content": "Models. We conduct full parameter table instruction tuning using the 8B version of the LLaMA 3.1 Instruct model (Dubey et al., 2024) because of its superior general capabilities, especially its strong instruction following ability. Appendix C.1 provides detailed reasons for our model choice.\nDatasets. We draw training data from three representative table understanding datasets in this sec-"}, {"title": "Analysis", "content": "Learning Rate. In Figure 2, we fine-tune the LLaMA 3.1 8B Instruct model using instruction data from TabFact, HiTab, and FeTaQA.\nWe find that the learning rate plays a crucial role in determining model performance, as well as how well the model preserves its general capabilities. In general, LLaMA 3.1 8B Instruct achieves the best performance when the learning rate is around 1.0e-6 and 5.0e-7. For instance, on TabFact, LLAMA 3.1 8B Instruct achieves its best performance (73.10) at a learning rate of 1.0e-6 with 1500 examples. Moreover, there is little to no decline in LLaMA 3.1 8B Instruct's performance on MMLU and IFEval with such learning rates. With a smaller learning rate such as 1.0e-7, though the model's performance on MMLU and IFEval can be well-preserved, the model's performance on table tasks such as FEVEROUS is suboptimal under the same setup (66.86 compared to 74.63 at a learning rate of 5.0e-6). In contrast, when the learning rate is too large, such as 1.0e-5, we observe a significant decline in the model's performance on both MMLU and IFEval, suggesting that a larger learning rate may hurt the model's general capabilities. We note that all the existing table LLMs use a large learning rate of 2e-5 (Table 1), which explains their compromised out-of-domain table understanding ability and general capabilities compared to their base models in Table 3.\nNumber of Examples. As the number of training instances increases, we find that there is a period of quick learning followed by a period of marginal performance improvement.\nWe observe in Figure 2 that on table tasks such as FeTaQA and HiTab, there is a period where the model's performance boosts up quickly, typically happening when tuning on the first 200 examples. Later, the performance improvement seems marginal. This aligns with the findings from Zhou et al. (2024) that the foundational LLM's performance can be improved with a limited amount of high-quality data in the instruction tuning stage. We hypothesize that with the first few hundred examples, the model is able to enhance its table reasoning ability quickly. After this point, the model's performance increase may primarily come from fitting the nuanced patterns in these datasets. Therefore, unlike the existing table LLMs which may involve up to two million training instances as seen in Table 1, we choose to train on 200 instances for each dataset in Section 4.\nIn addition, we can achieve competitive or even SOTA performance with limited data. On HiTab, with a learning rate of 1.0e-6 and 1,500 examples, we achieve an accuracy score of 66.29, outperforming the previous SOTA performance of 64.71 by TableLLaMA. On FEVEROUS, with 1,500 examples, we achieve a better score of 74.63 compared to 73.77 by TableLLaMA. Though the credit also comes from the LLaMA 3.1 Instruct model, which is much stronger compared to the LLaMA 2 model that TableLLaMA is tuned from, we highlight that TableLLaMA has used two million data points in its table instruction tuning stage, including the entire training set of TabFact, FeTaQA, and HiTab, while here we use around 7% of the entire training data for HiTab. Our analysis demonstrates that with a strong foundational model and a good choice of learning rate, we can achieve competitive performance on table understanding tasks with limited training instances.\nWe provide analysis on the effects of epochs in Appendix C.2 and multi-task training in Appendix C.3. We do not see significant performance gains when we increase the number of epochs, therefore we choose to train our model for two epochs in Section 4. We find that there are synergy effects on these table tasks, therefore we decide to fine-tune our model on a diverse range of tasks in Section 4. We provide further analysis across LLMs in Appendix C.4, and analysis in terms of LORA and QLoRA in Appendix C.5. We provide further analysis regarding how the data features affect the model's performance degradation on general benchmarks in Appendix C.7."}, {"title": "TAMA", "content": "Based on our findings from Section 3, we start building our general table understanding model, TAMA by instruction tuning the LLaMA 3.1 8B Instruct model."}, {"title": "Experimental Setup", "content": "Hyperparameter Selection. In Section 3, we find that with 200 instruction pairs, the model has already achieved competitive table understanding ability, and the performance gain after such a point is marginal. Moreover, tuning the model at a learning rate of 1.0e-6 for two epochs would enhance the model's table understanding ability while still maintaining its general ability. Therefore, we select 200 instruction pairs in the training set from each of the datasets in Table 4, and train the model at the learning rate of 1.0e-6 for two epochs.\nDataset Splits. As we use FeTaQA, HiTab, TabFact, FEVEROUS, MMLU, and IFEval in Section 3 for hyperparameter selection, we report their scores under the \u201cDev\u201d category. In the test time, we test our model on the additional nine table understanding datasets in Table 4. Moreover, we test our model on the two synthesized table understanding datasets from Table-Syn (Li et al., 2023) and from Wu et al. (2024) (denoted as S1 and S2 in Table 7, respectively) to assess its out-of-domain table understanding ability. To assess the model's general ability, apart from MMLU and IFEval, we test our model on MMLUPro, AI2ARC, and GPQA introduced in Table 2.\nAppendix A provides more details of our experimental setup including the information of GPU server, generation hyperparameters, data processing, and our evaluation setup. Appendix F provides examples from datasets that we evaluate upon."}, {"title": "Results and Analysis", "content": "Table 5 shows TAMA's performance on datasets listed in Table 4. Table 7 shows TAMA's performance on the two out-of-domain table benchmarks and the general benchmarks.\nTAMA demonstrates strong table understanding ability. We notice that there is a significant performance boost for TAMA compared to its base model, LLaMA 3.1 8B Instruct, on almost every dataset. For instance, on Table QA tasks such as HybridQA, TAMA achieves an accuracy of 60.86 compared to LLAMA 3.1 8B Instruct's 32.83. When compared to the commercial closed-source LLMs such as GPT-3.5 and GPT-4, TAMA surpasses the performance of GPT-3.5 model on almost every table task in Table 5 except for KVRET and WikiTQ. And on WikiTQ, the two yields a similar performance (TAMA achieves 52.81 and GPT-3.5 achieves 53.13).\nOn tasks such as WikiSQL, HybridQA, InfoTabs, FEVEROUS, TAMA yields a superior performance than GPT-4. Notably, on two out-of-domain synthesized table understanding datasets in Table 7, TAMA surpasses the performance of GPT-3.5 (on S1, TAMA yields 64.93 while GPT-3.5 yields 54.80, on S2, TAMA yields 28.60 while GPT-3.5 yields 27.75). These two datasets are comprised of diverse table understanding tasks, and the domain distribution is significantly different from all the in-domain training data we use. The competitive performance TAMA demonstrates on these two datasets indicates its strong general table understanding ability.\nThis suggests that while pre-training imparts a foundational understanding of table-related knowledge, table-specific fine-tuning plays a crucial role in further enhancing the model's capability in handling table data.\nTAMA preserves the general capabilities. Table 7 indicates that TAMA preserves the original LLaMA 3.1 8B Instruct's performance on almost every general benchmark. For instance, on MMLU, TAMA yields an accuracy of 66.99 compared to the base model's 66.04; on AI2ARC, TAMA yields an accuracy of 81.23 compared to the base model's 80.89. We leave the discussion of the slight performance improvements on these general benchmarks to Section 4.3. On IFEval, TAMA preserves most of its instruction following ability compared to the base model (74.70 compared to the base model's 79.62). Thanks to the strong instruction following ability of the original LLaMA 3.1 8B Instruct model, TAMA even yields a similar instruction following score on IFEval to GPT-3.5 (74.70 for TAMA compared to 74.80 for GPT-3.5). Table 6 provides two examples from TAMA's predictions versus existing table LLMs' on IFEval and Table-Syn (S1 in Table 5). Existing table LLMs fail to return their answers in JSON formats in most cases, while TAMA returns the correct format.\nTAMA is data efficient. We highlight that for each dataset, we use 200 training instances, which is less than 5% of the size of the original training dataset. For instance, on HiTab, we use 2.67% of the original 7,417 training instances, and on TabFact, we use 0.21% of the original 92,283 training"}, {"title": "Hindsight Analysis", "content": "In hindsight, we want to validate that our selected hyperparameters indeed work the best. Therefore, we run the experiments on the same training set with the learning rate ranging from 1.0e-7 to 1.0e-5, and the number of epochs from one to six. Figure 3 reports part of the results, and Appendix D reports the complete results and provide further discussion."}, {"title": "Related Works", "content": "Table Understanding Methods. The past decade has witnessed a paradigm shift in approaches to table understanding. Before the advent of LLMs, researchers typically adapt model structures to better interpret table data (Lebret et al., 2016; Liu et al., 2018; Yang et al., 2022). As language models demonstrate promising performance on various tasks (Devlin et al., 2019), researchers gradually shift their attention towards data-driven methods for table understanding. For instance, Yin et al. (2020); Herzig et al. (2020) pre-train BERT (Devlin et al., 2019) or BERT-derived model on large-volume of table data from sources such as Wikipedia to acquire better table representations. Xie et al. (2022) reveal the synergy effects of various structured tasks, including many table tasks, laying foundations to build a generalist model for structured data.\nIn the era of LLMs, as LLMs possess innate table-understanding abilities, researchers also explore prompt engineering techniques to optimize LLMs for table tasks (Chang and Fosler-Lussier, 2023; Deng et al., 2024).\nTable Instruction Tuning. Building on the advances in data-driven methods, researchers have increasingly focused on instruction tuning to enhance LLMs' table understanding ability. As demonstrated by Touvron et al. (2023); Dubey et al. (2024); Chung et al. (2024); Su et al. (2024), instruction-tuning can improve model performance and generalization to unseen tasks. Meanwhile, models from the open-source LLaMA family (Touvron et al., 2023) demonstrate strong capabilities, leading researchers to instruction-tune these models for better table understanding. For instance, TableLLaMA (Zhang et al., 2024a) is instruction-tuned from a variant of LLaMA 2 model (Touvron et al., 2023), TableLLM (Zhang et al., 2024b) is instruction-tuned from CodeLLaMA, Wu et al. (2024) instruction-tune various foundational models such as LLaMA 3.1 (Dubey et al., 2024), resulting in their TableBenchLLM model. Moreover, Zheng et al. (2024) treat tables as images and instruction-tune Vicuna (Chiang et al., 2023), a vision model that is originally fine-tuned from the LLaMA model, for table understanding. However, as revealed by Zheng et al. (2024); Deng et al. (2024), treating tables as texts rather than images yields better performance. In this paper, we focus on table instruction tuning with tables fed as texts."}, {"title": "Conclusion", "content": "In this paper, we reveal the limited out-of-domain table understanding ability and general capabilities of the existing table LLMs. From our analysis, we find that the commonly-adopted hyperparameters in existing table LLMs are suboptimal, and hyperparameter choices in table instruction tuning are crucial in shaping the model's capabilities. We select hyperparameters from our analysis, and fine-tune our own model, TAMA. Notably, as an 8B model, TAMA demonstrates strong table understanding ability, outperforming GPT-3.5 on most of the table understanding benchmarks, even achieving performance on par or better than GPT-4. Moreover, TAMA preserves strong general capabilities. We hope our findings and our model TAMA can facilitate future research on structured data."}, {"title": "Limitations", "content": "Due to the space constraint, we provide further analysis across LLMs in Appendix C.4, including Llama 2 7B Instruct (Touvron et al., 2023), QWen 2.5 7B Instruct (Bai et al., 2023), Mistral v0.3 7B Instruct (Jiang et al., 2023), and Phi 3 small 8K Instruct (7B) (Abdin et al., 2024), and analysis in terms of LoRA and QLoRA in Appendix C.5. We provide further analysis regarding how the data features affect the model's performance degradation on general benchmarks in Appendix C.7. However, due to the scope of one study, we cannot exhaust every possible foundational models and every possible dataset.\nWe want to emphasize the contributions of our work in terms of three aspects. First, we provide a systematic examination of the overspecialization issues in existing table LLMs, which is valuable for the community as it highlights the limitations and potential of the existing models. Second, we investigate how different training configurations influence model behavior. Our findings show that with proper setups, we can maintain a model's general capabilities while improving its abilities to deal with table tasks, offering practical guidelines for efficient model development. Our systematic exploration has been largely overlooked in the existing literature, and our paper can fill the existing literature gap. Third, our model is a meaningful addition to the open-source LLM community, providing a strong baseline and facilitating future research in domain-specific instruction tuning."}, {"title": "Ethical Considerations", "content": "In our experiments, the datasets we use are meant to evaluate the models' capabilities on handling tabular data as well as the model's general capabilities. Therefore, we assume there is no ethical consideration within the scope of the datasets. During our exploration and the construction of our own model, we employ foundational models like LLaMA 3.1 8B Instruct (Dubey et al., 2024). These foundational models may be subject to jail breaking (Zou et al., 2023) or other malicious user behaviors. We advocate practitioners to follow the intended usage of these foundational models and the result models after further fine-tuning such as our model introduced in this paper."}]}