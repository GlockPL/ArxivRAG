{"title": "Emotion-Agent: Unsupervised Deep Reinforcement Learning with Distribution-Prototype Reward for Continuous Emotional EEG Analysis", "authors": ["Zhihao Zhou", "Qile Liu", "Jiyuan Wang", "Zhen Liang"], "abstract": "Continuous electroencephalography (EEG) signals are widely used in affective brain-computer interface (aBCI) applications. However, not all continuously collected EEG signals are relevant or meaningful to the task at hand (e.g., wondering thoughts). On the other hand, manually labeling the relevant parts is nearly impossible due to varying engagement patterns across different tasks and individuals. Therefore, effectively and efficiently identifying the important parts from continuous EEG recordings is crucial for downstream BCI tasks, as it directly impacts the accuracy and reliability of the results. In this paper, we propose a novel unsupervised deep reinforcement learning framework, called Emotion-Agent, to automatically identify relevant and informative emotional moments from continuous EEG signals. Specifically, Emotion-Agent involves unsupervised deep reinforcement learning combined with a heuristic algorithm. We first use the heuristic algorithm to perform an initial global search and form prototype representations of the EEG signals, which facilitates the efficient exploration of the signal space and identify potential regions of interest. Then, we design distribution-prototype reward functions to estimate the interactions between samples and prototypes, ensuring that the identified parts are both relevant and representative of the underlying emotional states. Emotion-Agent is trained using Proximal Policy Optimization (PPO) to achieve stable and efficient convergence. Our experiments compare the performance with and without Emotion-Agent. The results demonstrate that selecting relevant and informative emotional parts before inputting them into downstream tasks enhances the accuracy and reliability of aBCI applications.", "sections": [{"title": "Introduction", "content": "Human emotion is a continuous dynamic process, characterized by complex interactions between both internal and external components of the human body (Cowen and Keltner 2017; Horikawa et al. 2020). How to identify task-related emotional segments from continuous EEG signals presents a significant challenge. Electroencephalography (EEG) provides a direct, objective, and scientifically grounded method for assessing emotional states, making it a valuable tool in emotion recognition research (Song et al. 2018). In recent years, the potential of EEG-based emotion recognition has garnered increasing attention from researchers across diverse disciplines (Li, Wang, and Lu 2021; Gong et al. 2023; Liu et al. 2024).\nOne significant limitation of existing research is the reliance on a static labeling approach, where a single, fixed label is assigned to an entire EEG segment. This method fails to capture the dynamic nature of human emotions during EEG-evoked experiments, as emotional states are inherently fluid, constantly shifting in response to both internal cognitive processes and external stimuli (Huang et al. 2014; Liu et al. 2017). Moreover, continuous EEG recordings often include states that are irrelevant to the specific task being studied. These irrelevant states can introduce noise and confounding factors, undermining the accuracy and reliability of emotion recognition models. Current methods face challenges in isolating and identifying the task-related moments within the EEG data that are most relevant to the study. When task-irrelevant EEG segments are included in the training data, they introduce extraneous information that can degrade the model's performance. As a result, the model may mistakenly associate these irrelevant patterns with emotional states, leading to reduced accuracy in emotion recognition by diverting attention from the true task-related emotional dynamics (Li et al. 2019; Zheng and Lu 2015; Zheng 2016). On the other hand, requiring real-time annotation of task-related segments during an experiment is impractical. This is especially true when considering that wandering thoughts or irrelevant mental states are often indistinguishable even to the subject themselves. Thus, developing an artificial intelligence (AI) empowered method that can dynamically adapt to the fluid nature of human emotions and accurately isolate task-relevant EEG segments is essential for improving the precision and effectiveness of emotion recognition models.\nDeep reinforcement learning, with its adaptability and flexibility in uncertain environments, offers a promising solution to this challenge (Vinyals et al. 2019; Kalashnikov et al. 2018). By leveraging a reward-based mechanism, it reduces the dependence on labels and enables unsupervised autonomous exploration of task-relevant information. For example, Zhou (Zhou, Qiao, and Xiang 2018) proposed a Diversity-Representativeness Reward to guide Agent in generating more diverse and representative video summaries. Similarly, AC-SUM-GAN (Apostolidis et al. 2020) used an"}, {"title": "Related Work", "content": ""}, {"title": "Reinforcement Learning", "content": "Reinforcement Learning (RL) is a powerful machine learning paradigm where an intelligent agent learns an optimal decision policy by interacting with its environment (Zoph and Le 2017). Unlike other machine learning methods, RL emphasizes learning through trial and error, with the agent taking actions to maximize cumulative rewards over time. This approach has gained significant traction across various domains due to its ability to handle complex, dynamic environments where the agent's decisions continuously adapt based on new information (He et al. 2016; Yarats, Kostrikov, and Fergus 2021)."}, {"title": "Reinforcement Learning with Heuristics", "content": "Heuristic-Guided Reinforcement Learning (HuRL) was introduced (Cheng, Kolobov, and Swaminathan 2021), aiming to accelerate traditional RL algorithms by incorporating heuristics derived from domain knowledge or offline data. These heuristics guide the RL agent, enabling more informed decisions and speeding up the learning process. HuRL is particularly valuable in environments where the state space is vast, making unguided exploration computationally expensive and time-consuming. Another significant advancement is the introduction of large-state reinforcement learning for hyper-heuristics (Kletzander and Musliu 2023). This approach leverages solution change trajectories from an extensive feature set, integrating them into the RL framework. By incorporating local search principles and introducing a probability distribution within the e-greedy strategy, this method increases the likelihood of sampling high-quality sequences of low-level heuristics. It significantly enhances the efficiency of RL in solving complex optimization problems with exceptionally large state spaces. A novel solution for continuous trajectory generation in urban road networks was also proposed (Jiang et al. 2023), combining a two-stage Generative Adversarial Network (GAN) with A* heuristic search algorithms. This design features discriminators for sequential reward and movement yaw reward, guiding the agent in generating more accurate and efficient trajectories. Building on the foundations of RL, personalized reinforcement learning was introduced (Ivanov and Ben-Porat 2024). Inspired by the classical K-means clustering principle, this approach incorporates the concept of a budget of policies within robust Markov Decision Processes (r-MDPs). The framework enables the RL agent to interact with users through representative policies, efficiently adapting to individual user preferences. An earlier application of RL in the field of education is demonstrated with AgentX (Martin and Arroyo 2004). This intelligent agent was developed to enhance the effectiveness of Intelligent Tutoring Systems (ITS). By clustering personalized group information about students, the RL-based AgentX tailors the learning experience for each group.\nThe advancements in reinforcement learning across various domains: from accelerating traditional RL algorithms with heuristics to personalizing user interactions and improving intelligent tutoring systems\u2014demonstrate the versatility and potential of this machine learning paradigm. The continuous evolution of RL, as seen in large-state optimization and urban trajectory generation, underscores its capacity to tackle increasingly complex challenges. As RL continues to integrate with other AI techniques, such as GANs and heuristic search algorithms, it is positioned to drive significant innovations across a wide array of fields, shaping the future of intelligent systems and autonomous decision-making (Warnell et al. 2018; Ren et al. 2023; Vecerik et al. 2017)."}, {"title": "Methodology", "content": ""}, {"title": "Markov Decision Process", "content": "We model the detection of the most emotionally relevant segments from sequential EEG signals as a sequential decision-making process, formulated as a Markov Decision Process (MDP). An MDP is defined as a tuple M = <S, A,P,R>, where S represents the state space, and A denotes the action space, with A = {0,1} corresponding to the possible actions the agent can take. The transition probability function S \u00d7 A \u00d7 S \u2192 [0, 1] describes the likelihood of transitioning from one state to another given a specific action. The reward function R : S\u00d7A\u00d7S \u2192 R assigns a numerical reward based on the state-action-state transition, providing feedback on the agent's decisions.\nAt each timestep t, the RL agent, upon executing an action at in state st, transitions to a new state st+1 and receives a corresponding reward rt. We define M tuple as transitions. In an episode when interacting with the environment, we collect multiple trajectories consisting of multiple transitions. Through repeated interactions with the environment, the RL algorithm aims to learn an optimal policy \u3160 that maximizes the cumulative reward over time. This optimal policy enables the agent to make decisions that consistently lead to the identification of key emotional segments in the EEG signals."}, {"title": "Prototype Learning", "content": "In the process of extracting key segments from task-related EEG signals, it is crucial to consider not only the intrinsic data distribution characteristics of each segment but also the broader context provided by the global emotion space, which encapsulates the distribution of various emotion categories. To achieve a more effective representation of EEG emotion distribution, we introduce the concept of prototype learning(Zhou et al. 2023). This approach allows us to model each emotion category as a prototype, thereby capturing the globally distributed emotional information with better representation. Prototype learning enables us to integrate global emotion information into the reward structure, ensuring that the reinforcement learning process is informed by a comprehensive understanding of the emotional landscape represented in the EEG data.\nSpecifically, we employ the K-Means clustering algorithm as a heuristic method. This step enables the model to obtain a global perspective on the distribution of emotional information across all subjects' EEG data. By inputting the differential entropy (DE) features of EEG signals from all subjects into the K-Means algorithm, we derive the set of emotion prototypes {C}1, where each C\u00b2 represents a\ncluster center corresponding to a emotion category, N represents the number categories. By clustering the data, we identify the optimal emotion prototypes that serve as representative points in the emotion space. These prototypes are then used to inform the design of the reward function, ensuring that it reflects the global distribution of emotions captured in the EEG signals. The prototype feature vector for a given emotion category c can be calculated by averaging all the sample features that belong to this category. Mathematically, the prototype feature vector \u03bc\u03b5 is given by:\n$\\mu_c = \\frac{1}{|C_i|} \\sum_{x_i \\in C_i} f(x_i),$\nwhere $C_i = \\{(x_i, y_i = c)\\}_{i=1}^N$ represents the set of samples belonging to the emotion category c, and |C| is the number of samples in this category. The centroid \u03bc\u03b5 serves as the average feature vector for the emotion category c.\nThe K-Means algorithm iteratively reclassifies data points and updates cluster centers to minimize the sum of squared errors within the clusters. The objective function of the algorithm is defined as:\n$\\arg \\min_C \\sum_{i=1}^N \\sum_{x \\in C_i} ||x - \\mu_i||^2.$\nTo quantify the variance within each cluster, we use the mean of the sum of squared intra-cluster errors, which reflects the distribution of EEG features within the cluster. The variance of the intra-cluster distribution is indicative of the individual variability of each emotion. The intra-cluster variance \u03c3\u00b2 is calculated using the following formula:\n$\\sigma^2 = \\frac{1}{|C_i|} \\sum_{x_i \\in C_i} ||x_i - \\mu_c||^2,$\nwhere |C | is the number of data points in the cluster, xi represents an individual data point, and \u00b5\u03b5 is the corresponding cluster center.\nTo better represent the distribution of emotions in the EEG data, the prototype learning process integrates the cluster centers as the mean of the data distribution and uses the mean of the sum of squared errors within clusters to describe the variance. This combination of prototype learning and K-Means clustering provides a robust foundation for the reinforcement learning process, enabling the model to effectively navigate and interpret the complex emotional information present in EEG data."}, {"title": "Distribution-Prototype Reward", "content": "We obtain a global distribution of emotional information in an unsupervised manner through prototype learning, and we incorporate this global information into the reward function for our reinforcement learning model. We believe that the clustering centers obtained through heuristic search represent the prototypes of each affective category. These prototypes effectively capture the distribution of sample features across the entire affective category, with other EEG features belonging to the same category clustering around these prototypes.\nFrom a probabilistic perspective, an emotion prototype can be understood as the mean of the emotion sample features, while the variance in this distribution arises from the inherent variability of human emotions and the non-stationarity of EEG signals. We use the mean and variance of each emotion cluster to reflect the individual variability within the distribution of EEG features for each emotion category. The goal of the Actor in our proposed method is to maximize the expected reward over time by selecting key segments that are more closely aligned with the target emotion. To achieve this, we propose two reward functions based on the distributional information derived from prototype learning: center reward and inter-intra reward."}, {"title": "Optimization Process", "content": "We use PPO (Schulman et al. 2017) to train our model, as it searches for emotionally relevant EEG key segments at the trial level. Throughout this process, the model learns to identify emotionally prototypical policies in a trial-and-error manner within a discrete action space. To stabilize the training process, we employ PPO-Clip, which restricts the ratio between old and new policies, reducing oscillations and accelerating convergence. PPO is a policy-based Actor-Critic method. To improve sample efficiency in the On-Policy training process, importance sampling is introduced, allowing the model to reuse trajectories multiple times:\n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)},$\nwhere rt (\u03b8) represents the probability ratio between the current and previous policies.\nTo better estimate cumulative returns, we use Generalized Advantage Estimation (GAE), which provides a more accurate advantage function. The specific expression of GAE is as follows:\n$A_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+1} = \\delta_t^V + \\gamma \\lambda A_{t+1}^{GAE(\\gamma, \\lambda)},$\nwhere y is the discount factor and A is the GAE hyperparameter that controls the trade-off between bias and variance. GAE uses a weighted average of multiple value estimates, and to quickly estimate the advantage at each time step, a recursive calculation is performed, estimating time t from time t + 1. To further stabilize the training process, we apply PPO-Clip, which limits the changes between the old and new policies. The objective function with clipping can be expressed as:\n$L_{policy}(\\theta) = E_t [\\min (r_t(\\theta) A_t, clip (r_t(\\theta), 1 \u2013 \\epsilon,1 + \\epsilon)A_t)],$"}, {"title": "Regularization", "content": "To prevent the Actor from selecting too many keyframes during an episode, we introduce a regularization term that constrains the action probability of the learned policy function. This can be expressed as:\n$L_{prob} = \\frac{1}{T} \\sum_{t=1}^T ||P_t - \\delta||^2,$\nwhere d is a scalar representing the desired proportion of key emotional segments selected. We then use the Adam optimizer to update the parameters @ of the policy function, calculated as:\n$\\theta = \\theta - \\varphi \\nabla_{\\theta} (-L_{policy} + \\beta L_{prob}),$\nwhere is the learning rate, and \u1e9e is a regularization coefficient.\nThe Critic network is responsible for estimating V(St) during the decision process, which serves as a prediction of the actual discounted cumulative reward throughout the process. This estimation guides the Actor network towards converging on the optimal policy. The error function for the Critic is the mean square error (MSE) between the estimated value and the actual discounted cumulative reward, and is expressed as:\n$L(\\theta) = E_{\\pi} [(V(s_t) \u2013 R_t)^2],$\nwhere Rt is the discounted cumulative reward at time t. The Critic's predicted value is then used to calculate the MSE loss relative to the actual discounted reward.\nDuring the training process, the model is divided into two parts: Actor and Critic. The Actor continually interacts with the emotion space during decision-making, iteratively searching for an optimal strategy based on the reward mechanism provided by environmental feedback. The Critic guides this exploration process by estimating the cumulative reward for each state, thereby assisting the Actor in finding the optimal strategy."}, {"title": "Experiments", "content": ""}, {"title": "Datasets and Implementation Details", "content": "Extensive validation experiments are conducted two publicly available datasets, including SEED (Zheng and Lu 2015) and DEAP (Koelstra et al. 2011)). We use DE features as inputs for the model. The details of the dataset and preprocessing will be introduced in the appendix.\nThe network of actor consists of one layer of LSTM where the number of hidden layer nodes is 128 and two fully connected layers where the hidden nodes are from 256 128,128 \u2192 2. The network setup of Critic is one layer of LSTM 128 and two fully connected layers where the hidden layer nodes in the fully connected layers are 256 \u2192 128, 128 \u2192 1. actor is optimised by The optimisation is done by Adam's optimiser and the learning rate"}, {"title": "Evaluation Settings and Metrics", "content": "We use two experimental protocols to evaluate our approach.\n(1) Cross-Subject: Subject-Independent, Subject-Level LOOCV. We use subject-leave-one-out cross-validation to test the performance of our proposed model over cross-subjects. (2) Within-Subject: Subject-Independent, Video-Level LOOCV.Based on the above experimental scheme, we extract the most emotion-related segments on the proposed model, which are then used for subsequent model method analysis.\nWe conducted relevant experiments using the proposed model Emotion-Agent on SEED, DEAP datasets, Emotion-Agent extracted the key segments related to emotions in an unsupervised manner through the guidance of reward function, we used the extracted key segments for downstream task modelling, the experiments compared with and without Emotion-Agent's Accuracy, F1-Scores two evaluation metrics."}, {"title": "Experimental Results", "content": "We compare the proposed Emotion-Agent with the current state-of-the-art methods. The comparison results for the three-classified emotion recognition (positive, neutral, negative) task on SEED are given in Table 1, where the methodology and the experimental protocol used are clearly stated. Overall, a supervised learning based approach yields better"}, {"title": "Discussion", "content": "To further validate our proposed Emotion-Agent model in terms of accuracy and reliability improvement in sentiment analysis, we conducted additional experiments on the SEED dataset. The experiments compared with and without, with i.e., using our proposed method Emotion-Agent extracts the key segments related to emotions and then inputs them into the classifier for the triple categorisation emotion recognition task, without on the other hand, we did not use our proposed method and input them directly into the classifier. On the other hand, we have chosen the traditional method SVM, KNN, and the deep learning supervised method MLP for the classifiers.\nFor emotion recognition with the SVM classifier, the accuracy Pace reaches 56.30%, and Pf reaches 49.53%. After applying our proposed method, Pacc improves by 21.39% and Pf improves by 27.1%. With the KNN classifier, Pacc reaches 42.78%, and Pf reaches 34.93%. Following the implementation of our method, Pacc improves by 19.53% and Pf improves by 26.07%. For the MLP classifier, Pacc reaches 65.70%, and Pf reaches 63.55%. After using our proposed approach, Pacc improves by 12.02% and Pf improves by 13.21%. The experimental results show that the results on both traditional unsupervised, traditional supervised as well as deep learning supervised methods result in significant performance improvement on the SEED emotion recognition triple classification task.This experimental"}, {"title": "Model Optimization Process", "content": "Model Optimization Process In order to have a better exploration of the EEG emotional space based on our well-designed reward function, we used PPO to complete the optimisation of the whole training process. To further study the role of the PPO algorithm in the model training process, we conducted additional experiments to explore the specific circumstances of the training process and the impact of the e in the Clip operation on the model training process.\nWe conducte cross-subject experiments on the SEED dataset, and we compare the cumulative total return from the policy learned by the Agent during the completion of multiple Episodes of exploration and learning with the number of training sessions in the Cross-Subject experiments.Fig.2 depicts the alteration of the return during the cross-subject training of subjects 7 and 8 as the number of interaction episodes escalates. It can be observed that with a meticulously designed reward function, the cumulative benefits acquired by the agent in the task of extracting key EEG segments keep rising, and the agent progressively acquires the optimal action strategy. The right figure demonstrates the influence of the e on the training process during training. It can be noted that when the upper limit of the clipping is lower, the training process is more stable and superior strategies are learned within a certain range. Constraining the ratio of new and old strategies enables the model to converge more steadily and efficiently during training."}, {"title": "Conclusion", "content": "In this paper, we propose a novel unsupervised deep reinforcement learning framework, called Emotion-Agent, to automatically identify relevant and informative emotional moments from continuous EEG signals.Emotion-Agent involves unsupervised deep reinforcement learning combined with a heuristic algorithm. Constructing heuristics for reinforcement learning by constructing prior knowledge for the exploration process can dramatically improve the efficiency of intelligences in the exploration process.The extraction of fragments that have a stronger connection to emotions is more favorable for the following analysis and research.Besides according to the results, we can show the effectiveness of our proposed approach."}, {"title": "Appendix", "content": ""}, {"title": "Datasets", "content": "We perform related experiments on the SEED (Zheng and Lu 2015) and the DEAP (Koelstra et al. 2011) using our proposed method Emotion-Agent. The following is a specific description of the two datasets:\nSEED This dataset was developed by the BCMI laboratory at Shanghai Jiao Tong University. The dataset was acquired using the 62-channel ESI NeuroScan System based on the international 10-20 system, which recorded EEG signals from subjects under different types of video stimuli. The SEED dataset acquires raw EEG signals at a sampling rate of 1000 Hz.Regarding the experimental paradigm of the SEED dataset, specifically, the EEG signals of 15 subjects (7 males and 8 females) were recorded in the SEED dataset under various video stimuli. For each subject, the video clips to be viewed were divided into three different sessions. In each session, 15 different types of film clips were involved, among which there were three types of clips that elicited different emotional states (positive, neutral, and negative moods), and each emotional state comprised five film clips.\nDEAP This dataset utilized a 32-channel Biosemi Active Two device with a sampling frequency of 512 Hz to record the subjects being stimulated by different one-minute-long music videos. Each video in the dataset corresponds to four labels, namely Valence, Arousal, Dominance, and Liking.A total of 32 participants in good physical condition were selected for the trial during the data collection process, consisting of 16 males and 16 females. Each subject was obligated to carry out 40 experiments, and in each of them, a 1-minute music video was watched to induce the relevant EEG. At the end of each experiment, a prompt self-assessment was conducted to rate the current state of the participant (Valence, Arousal, Dominance, and Liking), which was subsequently analysed and quantified comprehensively. Finally, a threshold value is employed to binarize the four labels for each video, thereby obtaining discrete labels for each state."}, {"title": "Preprocessing", "content": "In the preprocessing section, we will respectively introduce the pre-processing of the two datasets and the extraction of the Differential Entropy Feature (DE feature) (Duan, Zhu, and Lu 2013) corresponding to the EEG signals.\nThe raw SEED dataset was initially preprocessed. To be specific, the raw EEG data were initially downsampled to a sampling rate of 200 Hz and filtered through a 1-75 Hz bandpass filter to filter out noise and eliminate artefacts. Next, the preprocessed EEG signals were divided into multiple segments by utilizing a sliding window with a length of 15 to obtain the EEG signals after the preliminary processing. In an effort to obtain features in the EEG signals that"}, {"title": "Implementation Details", "content": "In this section, We will provide a detailed account of the specificities of the two processes, (1) Prototype Learning, and (2) Reinforcement Learning, that are employed in the model when training the model for cross-subject experiments (Subject-Leave-One-Out Cross-Validation). In order to better describe the training details, We define the total number of subjects in the dataset as N."}, {"title": "Prototype Learning", "content": "In the prototype learning stage, with the aim of obtaining a global overview of the data distribution, we clustere the data of N-1 subjects through the utilization of the heuristic algorithm K-Means. In this case, the hyperparameter n_clusters of K-Means was set to the total number of emotion categories (set to 3 for the SEED dataset and 2 for the DEAP dataset). After several iterations of the algorithm, we obtain the emotion prototype. Additionally, we compute the mean of the sum of squared errors in each cluster as the variance"}, {"title": "Reinforcement Learning", "content": "In the reinforcement learning stag, we require Trial-Level EEG data for delineation. The training data consist of the EEG DE features of a subject conducting an experiment to complete an indefinitely long sequence of actions in this manner as a decision-making process, where the action space is the discrete action A = {0,1}.\nThe actor's network comprises one layer of LSTM where the number of hidden layer nodes is 128 and two fully connected layers where the hidden nodes range from 256 \u2192 128, 128 \u2192 2. The network setup of the Critic is one layer of LSTM with 128 nodes and two fully connected layers where the hidden layer nodes in the fully connected layers are 256 \u2192 128, 128 \u2192 1. The actor is optimized by Adam's optimizer and the learning rate is set to 1e-4, while the Critic's learning rate is also optimized by Adam's optimizer and is set to 1e-3. Additionally, in the PPO algorithm, y is set to 0.98, A is set to 0.95, and e is set to 0.2 for the algorithms (refer to the original Eq. (9), Eq. (10), Eq. (11)). All experiments are carried out using PyTorch 1.13.1 on an NVIDIA GeForce RTX 3090 GPU."}, {"title": "Distribution-Prototype Reward", "content": "We obtain information about the global distribution based on the prototype learning stage. We use the mean and variance of each emotion cluster to reflect the individual variability within the distribution of EEG features for each emotion category. We propose two reward functions based on the distributional information derived from prototype learning: center reward and inter-intra reward.\nWe incorporate the relevant theory of Inverse Variance Weighting (Hartung, Knapp, and Sinha 2011) presented in Inter-Intra reward here.\nIf a series of independent measurements of a random variable are represented by yi and possess a variance of of, then the inverse variance weighted average of these measurements is:\n$\\overline{y} = \\frac{\\sum_i y_i / \\sigma_i^2}{\\sum_i 1/\\sigma_i^2},$\nAmong all the methods of weighted averaging, the inverse variance weighted average has the least variance. The expression of its variance is as follows:\n$D^2(\\overline{y}) = \\frac{1}{\\sum_i 1/\\sigma_i^2}.$\nIf the variances of the measurements are equalized, the inverse variance weighted average if the same as the simple average."}, {"title": "Additional Results", "content": "To further explore the EEG key segments extracted by our proposed method Emotion-Agent, we conduct additional experiments on the SEED dataset using the proposed model, visualized the extracted EEG keyfragments using t-SNE, and compared them with the case without.\non all three emotion categories.\nFigure 1 compares the with and without Emotion-Agent models, with SEED DE features as input, and visually compares the original DE features with the data after extracting key segments through t-SNE results. It can be seen that after extracting key segments, the separability of the entire data distribution is improved compared to before, and the same conclusion can be drawn from the improvement in classification accuracy.\nFigure 2 compares the with and without Emotion-Agent models, with the SEED DE features as input and the classification confusion matrix obtained using SVM as the classifier. The experimental results show that compared with without, with achieves higher classification accuracy"}]}