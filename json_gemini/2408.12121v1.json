{"title": "Emotion-Agent: Unsupervised Deep Reinforcement Learning with Distribution-Prototype Reward for Continuous Emotional EEG Analysis", "authors": ["Zhihao Zhou", "Qile Liu", "Jiyuan Wang", "Zhen Liang"], "abstract": "Continuous electroencephalography (EEG) signals are widely used in affective brain-computer interface (aBCI) applications. However, not all continuously collected EEG signals are relevant or meaningful to the task at hand (e.g., wondering thoughts). On the other hand, manually labeling the relevant parts is nearly impossible due to varying engagement patterns across different tasks and individuals. Therefore, effectively and efficiently identifying the important parts from continuous EEG recordings is crucial for downstream BCI tasks, as it directly impacts the accuracy and reliability of the results. In this paper, we propose a novel unsupervised deep reinforcement learning framework, called Emotion-Agent, to automatically identify relevant and informative emotional moments from continuous EEG signals. Specifically, Emotion-Agent involves unsupervised deep reinforcement learning combined with a heuristic algorithm. We first use the heuristic algorithm to perform an initial global search and form prototype representations of the EEG signals, which facilitates the efficient exploration of the signal space and identify potential regions of interest. Then, we design distribution-prototype reward functions to estimate the interactions between samples and prototypes, ensuring that the identified parts are both relevant and representative of the underlying emotional states. Emotion-Agent is trained using Proximal Policy Optimization (PPO) to achieve stable and efficient convergence. Our experiments compare the performance with and without Emotion-Agent. The results demonstrate that selecting relevant and informative emotional parts before inputting them into downstream tasks enhances the accuracy and reliability of aBCI applications.", "sections": [{"title": "Introduction", "content": "Human emotion is a continuous dynamic process, characterized by complex interactions between both internal and external components of the human body (Cowen and Keltner 2017; Horikawa et al. 2020). How to identify task-related emotional segments from continuous EEG signals presents a significant challenge. Electroencephalography (EEG) provides a direct, objective, and scientifically grounded method for assessing emotional states, making it a valuable tool in emotion recognition research (Song et al. 2018). In recent years, the potential of EEG-based emotion recognition has garnered increasing attention from researchers across diverse disciplines (Li, Wang, and Lu 2021; Gong et al. 2023; Liu et al. 2024). One significant limitation of existing research is the reliance on a static labeling approach, where a single, fixed label is assigned to an entire EEG segment. This method fails to capture the dynamic nature of human emotions during EEG-evoked experiments, as emotional states are inherently fluid, constantly shifting in response to both internal cognitive processes and external stimuli (Huang et al. 2014; Liu et al. 2017). Moreover, continuous EEG recordings often include states that are irrelevant to the specific task being studied. These irrelevant states can introduce noise and confounding factors, undermining the accuracy and reliability of emotion recognition models. Current methods face challenges in isolating and identifying the task-related moments within the EEG data that are most relevant to the study. When task-irrelevant EEG segments are included in the training data, they introduce extraneous information that can degrade the model's performance. As a result, the model may mistakenly associate these irrelevant patterns with emotional states, leading to reduced accuracy in emotion recognition by diverting attention from the true task-related emotional dynamics (Li et al. 2019; Zheng and Lu 2015; Zheng 2016). On the other hand, requiring real-time annotation of task-related segments during an experiment is impractical. This is especially true when considering that wandering thoughts or irrelevant mental states are often indistinguishable even to the subject themselves. Thus, developing an artificial intelligence (AI) empowered method that can dynamically adapt to the fluid nature of human emotions and accurately isolate task-relevant EEG segments is essential for improving the precision and effectiveness of emotion recognition models. Deep reinforcement learning, with its adaptability and flexibility in uncertain environments, offers a promising solution to this challenge (Vinyals et al. 2019; Kalashnikov et al. 2018). By leveraging a reward-based mechanism, it reduces the dependence on labels and enables unsupervised autonomous exploration of task-relevant information. For example, Zhou (Zhou, Qiao, and Xiang 2018) proposed a Diversity-Representativeness Reward to guide Agent in generating more diverse and representative video summaries. Similarly, AC-SUM-GAN (Apostolidis et al. 2020) used an"}, {"title": "Related Work", "content": "Reinforcement Learning (RL) is a powerful machine learning paradigm where an intelligent agent learns an optimal decision policy by interacting with its environment (Zoph and Le 2017). Unlike other machine learning methods, RL emphasizes learning through trial and error, with the agent taking actions to maximize cumulative rewards over time. This approach has gained significant traction across various domains due to its ability to handle complex, dynamic environments where the agent's decisions continuously adapt based on new information (He et al. 2016; Yarats, Kostrikov, and Fergus 2021).\nReinforcement Learning with Heuristics\nHeuristic-Guided Reinforcement Learning (HuRL) was introduced (Cheng, Kolobov, and Swaminathan 2021), aiming to accelerate traditional RL algorithms by incorporating heuristics derived from domain knowledge or offline data. These heuristics guide the RL agent, enabling more informed decisions and speeding up the learning process. HuRL is particularly valuable in environments where the state space is vast, making unguided exploration computationally expensive and time-consuming. Another significant advancement is the introduction of large-state reinforcement learning for hyper-heuristics (Kletzander and Musliu 2023). This approach leverages solution change trajectories from an extensive feature set, integrating them into the RL framework. By incorporating local search principles and introducing a probability distribution within the \\( \\epsilon \\)-greedy strategy, this method increases the likelihood of sampling high-quality sequences of low-level heuristics. It significantly enhances the efficiency of RL in solving complex optimization problems with exceptionally large state spaces. A novel solution for continuous trajectory generation in urban road networks was also proposed (Jiang et al. 2023), combining a two-stage Generative Adversarial Network (GAN) with \\( A^* \\) heuristic search algorithms. This design features discriminators for sequential reward and movement yaw reward, guiding the agent in generating more accurate and efficient trajectories. Building on the foundations of RL, personalized reinforcement learning was introduced (Ivanov and Ben-Porat 2024). Inspired by the classical K-means clustering principle, this approach incorporates the concept of a budget of policies within robust Markov Decision Processes (r-MDPs). The framework enables the RL agent to interact with users through representative policies, efficiently adapting to individual user preferences. An earlier application of RL in the field of education is demonstrated with AgentX (Martin and Arroyo 2004). This intelligent agent was developed to enhance the effectiveness of Intelligent Tutoring Systems (ITS). By clustering personalized group information about students, the RL-based AgentX tailors the learning experience for each group. The advancements in reinforcement learning across various domains: from accelerating traditional RL algorithms with heuristics to personalizing user interactions and improving intelligent tutoring systems\u2014demonstrate the versatility and potential of this machine learning paradigm. The continuous evolution of RL, as seen in large-state optimization and urban trajectory generation, underscores its capacity to tackle increasingly complex challenges. As RL continues to integrate with other AI techniques, such as GANs and heuristic search algorithms, it is positioned to drive significant innovations across a wide array of fields, shaping the future of intelligent systems and autonomous decision-making (Warnell et al. 2018; Ren et al. 2023; Vecerik et al. 2017)."}, {"title": "Methodology", "content": "Markov Decision Process\nWe model the detection of the most emotionally relevant segments from sequential EEG signals as a sequential decision-making process, formulated as a Markov Decision Process (MDP). An MDP is defined as a tuple \\( M = <S, A,P,R> \\), where S represents the state space, and A denotes the action space, with A = {0,1} corresponding to the possible actions the agent can take. The transition probability function \\( S \\times A \\times S \\rightarrow [0, 1] \\) describes the likelihood of transitioning from one state to another given a specific action. The reward function \\( R : S \\times A \\times S \\rightarrow \\mathbb{R} \\) assigns a numerical reward based on the state-action-state transition, providing feedback on the agent's decisions. At each timestep t, the RL agent, upon executing an action \\( a_t \\) in state \\( s_t \\), transitions to a new state \\( s_{t+1} \\) and receives a corresponding reward \\( r_t \\). We define \\( M \\) tuple as transitions. In an episode when interacting with the environment, we collect multiple trajectories consisting of multiple transitions. Through repeated interactions with the environment, the RL algorithm aims to learn an optimal policy \\( \\pi \\) that maximizes the cumulative reward over time. This optimal policy enables the agent to make decisions that consistently lead to the identification of key emotional segments in the EEG signals.\nPrototype Learning\nIn the process of extracting key segments from task-related EEG signals, it is crucial to consider not only the intrinsic data distribution characteristics of each segment but also the broader context provided by the global emotion space, which encapsulates the distribution of various emotion categories. To achieve a more effective representation of EEG emotion distribution, we introduce the concept of prototype learning(Zhou et al. 2023). This approach allows us to model each emotion category as a prototype, thereby capturing the globally distributed emotional information with better representation. Prototype learning enables us to integrate global emotion information into the reward structure, ensuring that the reinforcement learning process is informed by a comprehensive understanding of the emotional landscape represented in the EEG data. Specifically, we employ the K-Means clustering algorithm as a heuristic method. This step enables the model to obtain a global perspective on the distribution of emotional information across all subjects' EEG data. By inputting the differential entropy (DE) features of EEG signals from all subjects into the K-Means algorithm, we derive the set of emotion prototypes \\( {C_i}_{i=1}^N \\), where each \\( C_i \\) represents a cluster center corresponding to a emotion category, N represents the number categories. By clustering the data, we identify the optimal emotion prototypes that serve as representative points in the emotion space. These prototypes are then used to inform the design of the reward function, ensuring that it reflects the global distribution of emotions captured in the EEG signals. The prototype feature vector for a given emotion category c can be calculated by averaging all the sample features that belong to this category. Mathematically, the prototype feature vector \\( \\mu_c \\) is given by:\n\\[\\mu_c = \\frac{1}{|C_i|} \\sum_{x_i \\in C_i} f(x_i),\\]\nwhere \\( C_i = {(x_i, y_i = c)}_{i=1}^{|C_i|} \\) represents the set of samples belonging to the emotion category c, and \\( |C_i| \\) is the number of samples in this category. The centroid \\( \\mu_c \\) serves as the average feature vector for the emotion category c. The K-Means algorithm iteratively reclassifies data points and updates cluster centers to minimize the sum of squared errors within the clusters. The objective function of the algorithm is defined as:\n\\[\\arg \\min_C \\sum_{i=1}^N \\sum_{x \\in C_i} ||x - \\mu_c||^2.\\]\nTo quantify the variance within each cluster, we use the mean of the sum of squared intra-cluster errors, which reflects the distribution of EEG features within the cluster. The variance of the intra-cluster distribution is indicative of the individual variability of each emotion. The intra-cluster variance \\( \\sigma_c^2 \\) is calculated using the following formula:\n\\[\\sigma_c^2 = \\frac{1}{|C_i|} \\sum_{x_i \\in C_i} ||x_i - \\mu_c||^2,\\]\nwhere \\( |C_i| \\) is the number of data points in the cluster, \\( x_i \\) represents an individual data point, and \\( \\mu_c \\) is the corresponding cluster center. To better represent the distribution of emotions in the EEG data, the prototype learning process integrates the cluster centers as the mean of the data distribution and uses the mean of the sum of squared errors within clusters to describe the variance. This combination of prototype learning and K-Means clustering provides a robust foundation for the reinforcement learning process, enabling the model to effectively navigate and interpret the complex emotional information present in EEG data.\nDistribution-Prototype Reward\nWe obtain a global distribution of emotional information in an unsupervised manner through prototype learning, and we incorporate this global information into the reward function for our reinforcement learning model. We believe that the clustering centers obtained through heuristic search represent the prototypes of each affective category. These prototypes effectively capture the distribution of sample features across the entire affective category, with other EEG features belonging to the same category clustering around these prototypes. From a probabilistic perspective, an emotion prototype can be understood as the mean of the emotion sample features, while the variance in this distribution arises from the inherent variability of human emotions and the non-stationarity of EEG signals. We use the mean and variance of each emotion cluster to reflect the individual variability within the distribution of EEG features for each emotion category. The goal of the Actor in our proposed method is to maximize the expected reward over time by selecting key segments that are more closely aligned with the target emotion. To achieve this, we propose two reward functions based on the distributional information derived from prototype learning: center reward and inter-intra reward."}, {"title": "Optimization Process", "content": "We use PPO (Schulman et al. 2017) to train our model, as it searches for emotionally relevant EEG key segments at the trial level. Throughout this process, the model learns to identify emotionally prototypical policies in a trial-and-error manner within a discrete action space. To stabilize the training process, we employ PPO-Clip, which restricts the ratio between old and new policies, reducing oscillations and accelerating convergence. PPO is a policy-based Actor-Critic method. To improve sample efficiency in the On-Policy training process, importance sampling is introduced, allowing the model to reuse trajectories multiple times:\n\\[r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)},\\]\nwhere \\( r_t(\\theta) \\) represents the probability ratio between the current and previous policies. To better estimate cumulative returns, we use Generalized Advantage Estimation (GAE), which provides a more accurate advantage function. The specific expression of GAE is as follows:\n\\[A^{GAE(\\lambda)}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} = \\delta_t^V + \\gamma \\lambda A^{GAE(\\lambda)}_{t+1},\\]\nwhere \\( \\gamma \\) is the discount factor and \\( \\lambda \\) is the GAE hyperparameter that controls the trade-off between bias and variance. GAE uses a weighted average of multiple value estimates, and to quickly estimate the advantage at each time step, a recursive calculation is performed, estimating time t from time t + 1. To further stabilize the training process, we apply PPO-Clip, which limits the changes between the old and new policies. The objective function with clipping can be expressed as:\n\\[L_{policy}(\\theta) = E_t [\\min (r_t(\\theta) A_t, clip (r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)A_t)],\\]\nwhere \\( \\epsilon \\) control the update range of the action probability at each iteration by setting upper and lower thresholds on the ratio of the new and old strategies.\nRegularization\nTo prevent the Actor from selecting too many keyframes during an episode, we introduce a regularization term that constrains the action probability of the learned policy function. This can be expressed as:\n\\[L_{prob} = \\frac{1}{T} \\sum_{t=1}^{T} ||p_t - \\delta||^2,\\]\nwhere \\( \\delta \\) is a scalar representing the desired proportion of key emotional segments selected. We then use the Adam optimizer to update the parameters \\( \\theta \\) of the policy function, calculated as:\n\\[\\theta = \\theta - \\alpha \\nabla_{\\theta} (-L_{policy} + \\beta L_{prob}),\\]\nwhere \\( \\alpha \\) is the learning rate, and \\( \\beta \\) is a regularization coefficient. The Critic network is responsible for estimating \\( V(S_t) \\) during the decision process, which serves as a prediction of the actual discounted cumulative reward throughout the process. This estimation guides the Actor network towards converging on the optimal policy. The error function for the Critic is the mean square error (MSE) between the estimated value and the actual discounted cumulative reward, and is expressed as:\n\\[L(\\phi) = E [ (V(s_t) - R_t)^2 ],\\]\nwhere \\( R_t \\) is the discounted cumulative reward at time t. The Critic's predicted value is then used to calculate the MSE loss relative to the actual discounted reward. During the training process, the model is divided into two parts: Actor and Critic. The Actor continually interacts with the emotion space during decision-making, iteratively searching for an optimal strategy based on the reward mechanism provided by environmental feedback. The Critic guides this exploration process by estimating the cumulative reward for each state, thereby assisting the Actor in finding the optimal strategy."}, {"title": "Experiments", "content": "Datasets and Implementation Details\nExtensive validation experiments are conducted two publicly available datasets, including SEED (Zheng and Lu 2015) and DEAP (Koelstra et al. 2011)). We use DE features as inputs for the model. The details of the dataset and preprocessing will be introduced in the appendix. The network of actor consists of one layer of LSTM where the number of hidden layer nodes is 128 and two fully connected layers where the hidden nodes are from 256 \u2192 128,128 \u2192 2. The network setup of Critic is one layer of LSTM 128 and two fully connected layers where the hidden layer nodes in the fully connected layers are 256 \u2192 128, 128 \u2192 1. actor is optimised by the The optimisation is done by Adam's optimiser and the learning rate"}, {"title": "Experimental Results", "content": "We compare the proposed Emotion-Agent with the current state-of-the-art methods. The comparison results for the three-classified emotion recognition (positive, neutral, negative) task on SEED are given in Table 1, where the methodology and the experimental protocol used are clearly stated. Overall, a supervised learning based approach yields better"}, {"title": "Discussion", "content": "To further validate our proposed Emotion-Agent model in terms of accuracy and reliability improvement in sentiment analysis, we conducted additional experiments on the SEED dataset. The experiments compared with and without, with i.e., using our proposed method Emotion-Agent extracts the key segments related to emotions and then inputs them into the classifier for the triple categorisation emotion recognition task, without on the other hand, we did not use our proposed method and input them directly into the classifier. On the other hand, we have chosen the traditional method SVM, KNN, and the deep learning supervised method MLP for the classifiers. For emotion recognition with the SVM classifier, the accuracy \\( P_{acc} \\) reaches 56.30%, and \\( P_f \\) reaches 49.53%. After applying our proposed method, \\( P_{acc} \\) improves by 21.39% and \\( P_f \\) improves by 27.1%. With the KNN classifier, \\( P_{acc} \\) reaches 42.78%, and \\( P_f \\) reaches 34.93%. Following the implementation of our method, \\( P_{acc} \\) improves by 19.53% and \\( P_f \\) improves by 26.07%. For the MLP classifier, \\( P_{acc} \\) reaches 65.70%, and \\( P_f \\) reaches 63.55%. After using our proposed approach, \\( P_{acc} \\) improves by 12.02% and \\( P_f \\) improves by 13.21%. The experimental results show that the results on both traditional unsupervised, traditional supervised as well as deep learning supervised methods result in significant performance improvement on the SEED emotion recognition triple classification task.This experimental"}, {"title": "Conclusion", "content": "In this paper, we propose a novel unsupervised deep reinforcement learning framework, called Emotion-Agent, to automatically identify relevant and informative emotional moments from continuous EEG signals.Emotion-Agent involves unsupervised deep reinforcement learning combined with a heuristic algorithm. Constructing heuristics for reinforcement learning by constructing prior knowledge for the exploration process can dramatically improve the efficiency of intelligences in the exploration process.The extraction of fragments that have a stronger connection to emotions is more favorable for the following analysis and research.Besides according to the results, we can show the effectiveness of our proposed approach."}, {"title": "Appendix", "content": "Datasets\nWe perform related experiments on the SEED (Zheng and Lu 2015) and the DEAP (Koelstra et al. 2011) using our proposed method Emotion-Agent. The following is a specific description of the two datasets: SEED This dataset was developed by the BCMI laboratory at Shanghai Jiao Tong University. The dataset was acquired using the 62-channel ESI NeuroScan System based on the international 10-20 system, which recorded EEG signals from subjects under different types of video stimuli. The SEED dataset acquires raw EEG signals at a sampling rate of 1000 Hz.Regarding the experimental paradigm of the SEED dataset, specifically, the EEG signals of 15 subjects (7 males and 8 females) were recorded in the SEED dataset under various video stimuli. For each subject, the video clips to be viewed were divided into three different sessions. In each session, 15 different types of film clips were involved, among which there were three types of clips that elicited different emotional states (positive, neutral, and negative moods), and each emotional state comprised five film clips. DEAP This dataset utilized a 32-channel Biosemi Active Two device with a sampling frequency of 512 Hz to record the subjects being stimulated by different one-minute-long music videos. Each video in the dataset corresponds to four labels, namely Valence, Arousal, Dominance, and Liking.A total of 32 participants in good physical condition were selected for the trial during the data collection process, consisting of 16 males and 16 females. Each subject was obligated to carry out 40 experiments, and in each of them, a 1-minute music video was watched to induce the relevant EEG. At the end of each experiment, a prompt self-assessment was conducted to rate the current state of the participant (Valence, Arousal, Dominance, and Liking), which was subsequently analysed and quantified comprehensively. Finally, a threshold value is employed to binarize the four labels for each video, thereby obtaining discrete labels for each state. We only use the preprocessed 1-s EEG signals from session 1 for both datasets. When conducting experiments with our model, we use the preprocessed 1-s EEG signals of session 1 for the SEED dataset. For the DEAP dataset, we likewise employ the preprocessed 1-s EEG signals.\nPreprocessing\nIn the preprocessing section, we will respectively introduce the pre-processing of the two datasets and the extraction of the Differential Entropy Feature (DE feature) (Duan, Zhu, and Lu 2013) corresponding to the EEG signals. The raw SEED dataset was initially preprocessed. To be specific, the raw EEG data were initially downsampled to a sampling rate of 200 Hz and filtered through a 1-75 Hz bandpass filter to filter out noise and eliminate artefacts. Next, the preprocessed EEG signals were divided into multiple segments by utilizing a sliding window with a length of 15 to obtain the EEG signals after the preliminary processing. In an effort to obtain features in the EEG signals that are more closely related to the brain state, differential entropy features were extracted for the EEG signals measured in seconds (with a 200 Hz sampling rate corresponding to 200 sampling points) using a band-pass filter (\\( \\delta \\) wave 0.5-4 Hz, \\( \\theta \\) wave: 4-8 Hz, \\( \\alpha \\) wave: 8-13 Hz, \\( \\theta \\) wave: 13-32 Hz, \\( \\gamma \\) wave: 32-50 Hz). The specific expression for calculating the differential entropy of EEG signals is as follows:\n\\[h(X) = \\frac{1}{2} log(2\\pi e \\sigma^2),\\]\nwhere the time series X obeys the Gauss distribution \\( N(\\mu, \\sigma^2) \\). It has been proven that, for a fixed length EEG sequence, DE is equivalent to the logarithm ES in a certain frequency band (Shi, Jiao, and Lu 2013). DE was employed to construct features in five frequency bands mentioned above. After processing the EEG signal per second through differential entropy, the feature dimension changes from (62, 200) to (62, 5), where 62 represents the number of device channels. From the sample features per second, we extracted the features in 5 frequency bands of each channel and flattened them. Thus, the feature dimension of the sample features per second becomes (1, 310). We use the DE features of the EEG signal per second as the input of the model, which corresponds to three emotion labels (negative, neutral, and positive). For the DEAP dataset, the original sampling frequency was 512 Hz. Subsequently, the data was downsampled to 128 Hz, while removing artefacts and deleting the first three seconds of silence in each experiment to obtain the initially processed EEG signal. Similarly for this dataset, to extract DE features (\\( \\delta \\) wave 0.5-4 Hz, \\( \\theta \\) wave: 4-8 Hz, \\( \\alpha \\) wave: 8-14 Hz, \\( \\theta \\) wave: 14-32 Hz, \\( \\gamma \\) wave: 32-50 Hz), the feature dimension per second changes from (32, 128) to (32, 5). We perform a flatten operation on it and the sample feature dimension per second is altered to (1, 160) to obtain the DE features of the EEG. We use the proposed model in Valence and Arousal two labels for experimentation, and both labels correspond to binary classification tasks.\nImplementation Details\nIn this section, We will provide a detailed account of the specificities of the two processes, (1) Prototype Learning, and (2) Reinforcement Learning, that are employed in the model when training the model for cross-subject experiments (Subject-Leave-One-Out Cross-Validation). In order to better describe the training details, We define the total number of subjects in the dataset as N.\nPrototype Learning\nIn the prototype learning stage, with the aim of obtaining a global overview of the data distribution, we clustere the data of N-1 subjects through the utilization of the heuristic algorithm K-Means. In this case, the hyperparameter n_clusters of K-Means was set to the total number of emotion categories (set to 3 for the SEED dataset and 2 for the DEAP dataset). After several iterations of the algorithm, we obtain the emotion prototype. Additionally, we compute the mean of the sum of squared errors in each cluster as the variance of the data distribution within each cluster. We pass the sentiment prototypes \\( \\mu_c \\) for each emotion category and the variance \\( \\sigma^2 \\) within each cluster to the second stage of learning. Additionally, we employ the labels generated during the unsupervised clustering process as semantic information for subsequent EEG features, and thereby we define such a space as Emotional Space.\nReinforcement Learning\nIn the reinforcement learning stag, we require Trial-Level EEG data for delineation. The training data consist of the EEG DE features of a subject conducting an experiment to complete an indefinitely long sequence of actions in this manner as a decision-making process, where the action space is the discrete action A = {0,1}. The actor's network comprises one layer of LSTM where the number of hidden layer nodes is 128 and two fully connected layers where the hidden nodes range from 256 \u2192 128, 128 \u2192 2. The network setup of the Critic is one layer of LSTM with 128 nodes and two fully connected layers where the hidden layer nodes in the fully connected layers are 256 \u2192 128, 128 \u2192 1. The actor is optimized by Adam's optimizer and the learning rate is set to 1e-4, while the Critic's learning rate is also optimized by Adam's optimizer and is set to 1e-3. Additionally, in the PPO algorithm, \\( \\gamma \\) is set to 0.98, \\( \\lambda \\) is set to 0.95, and \\( \\epsilon \\) is set to 0.2 for the algorithms (refer to the original Eq. (9), Eq. (10), Eq. (11)). All experiments are carried out using PyTorch 1.13.1 on an NVIDIA GeForce RTX 3090 GPU.\nDistribution-Prototype Reward\nWe obtain information about the global distribution based on the prototype learning stage. We use the mean and variance of each emotion cluster to reflect the individual variability within the distribution of EEG features for each emotion category. We propose two reward functions based on the distributional information derived from prototype learning: center reward and inter-intra reward. We incorporate the relevant theory of Inverse Variance Weighting (Hartung, Knapp, and Sinha 2011) presented in Inter-Intra reward here. If a series of independent measurements of a random variable are represented by \\( y_i \\) and possess a variance of \\( \\sigma_i^2 \\), then the inverse variance weighted average of these measurements is:\n\\[\\bar{y} = \\frac{\\sum_i y_i / \\sigma_i^2}{\\sum_i 1/ \\sigma_i^2}\\]\nAmong all the methods of weighted averaging, the inverse variance weighted average has the least variance. The expression of its variance is as follows:\n\\[D^2(\\bar{y}) = \\frac{1}{\\sum_i 1/ \\sigma_i^2}\\]\nIf the variances of the measurements are equalized, the inverse variance weighted average if the same as the simple average."}]}