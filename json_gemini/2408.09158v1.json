{"title": "Linear Attention is Enough in Spatial-Temporal Forecasting", "authors": ["Xinyu Ning"], "abstract": "As the most representative scenario of spatial-temporal forecasting tasks, the traffic forecasting task attracted numerous attention from machine learning community due to its intricate correlation both in space and time dimension. Existing methods often treat road networks over time as spatial-temporal graphs, addressing spatial and temporal representations independently. However, these approaches struggle to capture the dynamic topology of road networks, encounter issues with message passing mechanisms and over-smoothing, and face challenges in learning spatial and temporal relationships separately. To address these limitations, we propose treating nodes in road networks at different time steps as independent spatial-temporal tokens and feeding them into a vanilla Transformer to learn complex spatial-temporal patterns, design STformer achieving SOTA. Given its quadratic complexity, we introduce a variant NSTformer based on Nystr\u00f6m method to approximate self-attention with linear complexity but even slightly better than former in a few cases astonishingly. Extensive experimental results on traffic datasets demonstrate that the proposed method achieves state-of-the-art performance at an affordable computational cost. Our code will be made available.", "sections": [{"title": "Introduction", "content": "Learning the representation of space and time is long-time vision of machine learning community. Actually, the Convolutional Neural Network (CNN) exploits the spatial information redundancy (He et al. 2016) while Recurrent Neural Network (RNN) simulates the unidirectionality of time using recurrent structures between neurons.\nTraffic forecasting attracted numerous research interest as the most representative scenario of spatial-temporal forecasting, with its intricate correlation both in space and time while broad application (Wang et al. 2023).\nMost works model the traffic road networks to a graph, where the nodes represent the sensors to record traffic conditions such as speed or capacity while the edges represent the topological relationship of nodes, namely roads or distance in most cases. Further, the traffic flow composed of the graph within a period of time can be regarded as a spatial-temporal graph. And the goal of traffic forecasting is to learn a mapping from the past traffic flow to the future.\nIn the spatial dimension, (Li et al. 2018) used CNN to capture the spatial dependencies. Consider the instinct for gird data rather not topology data of CNN, (Yu, Yin, and Zhu 2018) introduced Graph Convolution Network (GCN) (Kipf and Welling 2016) to traffic forecasting for learning spatial representation. However, a fixed and static graph is unable to represent the ever-changing road networks, (Lin et al. 2023; Shao et al. 2022b; Han et al. 2021) utilize dynamic graph convolution to alleviate the problem. Despite this, the Graph Neural Network (GNN) trends to suffer from over-smoothing problem (Chen et al. 2020), while the message passing mechanism between adjacent nodes leads to a deeper network to connect a pair of remote nodes, which cause the parameters of network become harder to be optimized (Feng et al. 2022).\nIn the temporal dimension, (Li et al. 2018) and (Zhao et al. 2017) captured the temporal dependencies using RNN and LSTM respectively.\nThanks to the advantage of parallel processing,capturing long-range dependencies and so on, Transformer (Vaswani et al. 2017) has become the de-facto standard not only for natural language processing (Devlin et al. 2018), but computer vision (Dosovitskiy et al. 2020; Liu et al. 2021), sequential decision (Chen et al. 2021a), and so on.\nIn the traffic forecasting task, (Guo et al. 2019) proposed an attention-based model. Specifically, it separately used spatial attention and temporal attention first, with GCN in spatial dimension while CNN in temporal dimension behind. And (Cai et al. 2020) only utilized Transformer to capture the continuity and periodicity of time.\nEssentially,all the above works are based on the Spatial-Temporal Graph framework, namely, they modeled the traffic road networks to a graph (We will continue to use the term  in this literature ). We list the inherent drawbacks of the framework here:\n\u2022 First, even using dynamic GNN, it is still hard to capture the spatial dependencies and topological relationship in the complex and ever-changing road networks, needless to say a fixed and static graph.\n\u2022 Second, the GNN trends to suffer from the over-smoothing problem (Chen et al. 2020), and the message passing mechanism between adjacent nodes cause that the neural network needs more layers to connect a pair of remote nodes, namely, it increases the difficulty to train the model and optimize the parameters, which become more unbearable in large-scale road networks.\n\u2022 Third, learning the spatial representation and temporal representation separately requires more layers of neural networks to capture the cross-spatial-temporal dependencies.\nInspired by the breakthrough of Transformer in graph representation (Yun et al. 2019; Chen, O'Bray, and Borgwardt 2022; Kim et al. 2022) and time forecasting (Zhou et al. 2021; Li et al. 2019; Wu et al. 2020a), we study traffic forecasting only using self-attention, namely, we desert any Graph, Convolution and Recurrent module. Obviously, we can immediately overcome the first two out of above problems caused by GNN.\nFirst, we designed a model called STformer (Spatial-Temporal Transformer), in which we regard a sensor of road network at a time-step as an independent token rather than a node of graph. We refer to the kind of token as ST-Token because each token is uniquely determined by a time-step and a spatial location. Then the sequence composed by the tokens from the road networks with a period of time is fed to vanilla Transformer. Though the STformer is a extremely concise model, thanks to its ability to capture the cross-spatial-temporal dependencies, it can learn the spatial-temporal representation dynamically and efficiently, and achieves state-of-the-art performance on two most used public datasets METR-LA and PEMS-BAY.\nGiven the O(N\u00b2) complexity of self-attention, the computational cost of STformer is unbearable under large-scale road networks or long-term forecasting while its performance can be limited. Inspired by Nystr\u00f6mformer (Xiong et al. 2021), which leverages Nystr\u00f6m method to approximate standard self-attention with O(N) complexity, we designed NSTformer (Nystr\u00f6m Spatial-Temporal Transformer) with linear complexity. To our surprise, the performance of NSTformer exceeds that of STformer slightly. Actually, this phenomenon gives a open problem to investigate whether approximate attention has other positive effects such as regularization.\nWe conclude our contributions here:\n\u2022 We investigate the performance of pure self-attention for spatial-temporal forecasting. Our STformer achieves state-of-the-art on METR-LA and PEMS-BAY. We provide a new and extremely concise perspective for spatial-temporal forecasting.\n\u2022 We designed NSTformer, which can achieve state-of-the-art with O(N) complexity. Thanks to the economic linear complexity, we offer the insight that using linear attention to do spatial-temporal forecasting tasks."}, {"title": "Related Work", "content": "We have already discussed the application of deep learning in traffic forecasting task generally in Introduction, particularly the evolution of neural networks for learning the spatial-temporal representation, along with an analysis of the reasons behind it. Here we focus on application of Transformer in traffic forecasting.\n(Guo et al. 2019) proposed an attention-based model. Specifically, they designed a ST block, and several blocks are stacked to form a sequence. In each block, spatial attention and temporal attention separately learn the spatial representation and temporal representation in parallel. Subsequently, further learning is performed by GCN and CNN as the same way. (Xu et al. 2020) alternately set spatial Transformer and temporal Transformer to learn, incorporating GCN in parallel within each spatial Transformer to capture spatial dependencies. (Zheng et al. 2020) combined spatial and temporal attention mechanisms via gated fusion. In summary, these works are still under the Spatial-Temporal Graph framework and learn the spatial representation and temporal representation separately.\n(Jiang et al. 2023) did not use any graph structure but designed an intricate Semantic Spatial Attention, Geographic Spatial Attention with Delay-aware Feature Transformation to capture the spatial dependencies while a parallel Temporal self-attention.\n(Liu et al. 2023) is the most related work with ours. They proposed spatial-temporal adaptive embedding to make vanilla Transformer yield outstanding results, rather than designing complex network structures to obtain marginal performance improvements through arduous efforts. But it still learn the spatial representation and temporal representation separately. Although our models are simple yet effective, we have overcome the problem by learning the real spatial-temporal representation simultaneously, which makes our work surpass their performance with a lower computational cost."}, {"title": "Efficient Transformer", "content": "Transformer has become the de-facto standard in many applications. However, its core module, self-attention mechanism, has O(N\u00b2) space and time complexity, which limits its performance even feasibility when input is large. The research community has long recognized the problem and numerous works have emerged to speed up the calculation of self-attention (Keles, Wijewardena, and Hegde 2023).\nReformer (Kitaev, Kaiser, and Levskaya 2019), Big Bird (Zaheer et al. 2020), Linformer (Wang et al. 2020), Longformer (Beltagy, Peters, and Cohan 2020), and routing transformers (Roy et al. 2021) leveraged a blend of hashing, sparsification, or low-rank approximation to expedite computational processes of the attention scores. Nystr\u00f6mformer (Xiong et al. 2021) and (Katharopoulos et al. 2020) substituted the softmax-based attention with kernel approximations. Performer (Choromanski et al. 2020), Slim (Likhosherstov et al. 2021), RFA (Peng et al. 2021) used random projections to approximate the computation of attention. SOFT (Lu et al. 2021) and Skyformer (Chen et al. 2021b) suggested to replace softmax operations with rapidly evaluable Gaussian kernels.\nAmong them, Nystr\u00f6mformer achieves O(N) complexity. Consider the ample of theoretical groundwork providing analysis and guidance for Nystr\u00f6m method (Kumar, Mohri, and Talwalkar 2009; Gittens and Mahoney 2016; Li, Kwok, and L\u00fc 2010; Kumar, Mohri, and Talwalkar 2012; Si, Hsieh, and Dhillon 2016; Farahat, Ghodsi, and Kamel 2011; Frieze, Kannan, and Vempala 2004; Deshpande et al. 2006), we select Nystr\u00f6mformer as the backbone of NSTformer. Utilizing other sub-quadratic Transformer invariant is an interesting direction for future works."}, {"title": "Problem Setting", "content": "We formally define the traffic forecasting task here.\nDefinition 1 (Road Network). Given the road networks where there is N sensors to capture traffic conditions such as speed. At time-step t, then the traffic condition form a tensor \\(X_t \\in \\mathbb{R}^{N \\times D}\\), where D is feature dimension, generally, D =\n1 in traffic speed forecasting task.\nNote that under Spatial-Temporal Graph framework, the road network is presented by G = (V,E,A), where V =\n{v\u2081,..., v\u0274} presents the nodes, E \u2286 V \u00d7 V presents the edges, and A is the adjacent matrix. In this study, we don't use any graph structure in our models, neither set assumptions about graphs in road network modeling as well.\nDefinition 2 (Traffic Flow). The road network during a period of time T form a traffic flow tensor X\n= (X\u2081, X\u2082,..., X\u1d1b) \u2208\u211d\u1d40\u00d7\u0274\u00d7\u1d05.\nDefinition 3 (Traffic Forecasting). As the essence of machine learning is to learn a hypothesis f from a hypothesis class H (Lu and Lu 2020; Kidger and Lyons 2020; Valiant 1984; Livni, Shalev-Shwartz, and Shamir 2014), the deep learning method for traffic forecasting is to learn a mapping from past T time steps' traffic flow to future T' time steps' traffic flow with neural networks as follow:\n\\[[X_{t-T+1}, ..., X_t] \\rightarrow [X_{t+1}, ..., X_{t+T'}]\\]\nCorrespondingly, the learning of Spatial-Temporal Graph framework is as follow:\n\\[[X_{t-T+1}, ..., X_t; G] \\rightarrow [X_{t+1}, ..., X_{t+T'}]\\]"}, {"title": "Architecture", "content": "We present our pipeline and the architecture of our models in Figure 1. Without any complicated module or data process, we focus on how to capture the complex spatial-temporal relationships. Specifically, instead of regarding the traffic flow as a spatial-temporal graph, we just treat it as a regular 3D tensor and flatten it to a 1D sequence then feed to Transformer or its variant.\nBy this way, we can effectively capture the relationship between any pair of ST-Tokens. Correspondingly, the complexity of STformer is O(N\u00b2T\u00b2), which is unbearable when the input is too large. To overcome it, we desinged NST-former with O(NT) complexity, yielding a powerful and efficient model. And the only one difference between NST-former and STformer is the attention mechanism, where the former with the linear Nystr\u00f6m attention while the latter with the quadratic self-attention."}, {"title": "Embedding Layer & Regression Layer", "content": "As the Figure 1 shows, our models are so concise that it just contains the embedding layers, attention mechanism and regression layers. And the only difference between STformer and NSTformer is their attention mechanism. We first introduce their common module, embedding layers and regression layers and present the models in detail later.\nWe follow (Liu et al. 2023) to set our embedding layers, in which they proposed a spatial-temporal adaptive embedding Ea to capture the intricate spatial-temporal dependency rather than using any graph embedding.\nGiven the traffic flow \\(X \\in \\mathbb{R}^{T \\times N \\times D}\\), where T is the length of the input time-steps and N represents the number of sensors of road networks, the mainstream setting of the feature dimension D is 3 which contains the value of traffic conditions such as speed, the time flag day-of-week from 1 to 7, the time flag timestamps-of-day from 1 to 288. We embed the three features to Ef \u2208 \u211d\u1d40\u00d7\u0274\u00d7\u00b3\u1d48\u1da0, where df is the dimension of the feature embedding. And the simple yet effective Ea \u2208 \u211d\u1d40\u00d7\u0274\u00d7\u1d48\u1d43 to capture intricate spatial-temporal dependencies. After the above embedding, the traffic flow then be X \u2208 \u211d\u1d40\u00d7\u0274\u00d7\u1d48\u02b0, where dh equals to 3df + da, we will introduce our specific setting of the embedding dimensions in Experiment.\nAfter the attention module in STformer and NSTformer, the traffic flow will be mapped to X' \u2208 \u211d\u1d40\u00d7\u0274\u00d7\u1d48\u02b0. Finally, our fully-connected Regression Layer FC yields the prediction Y = FC(X') \u2208 \u211d\u1d40'\u00d7\u0274\u00d7\u1d48, where T' is the length prediction and the d is the dimension of prediction value, which equals to 1 in our traffic forecasting setting."}, {"title": "STformer", "content": "In this paper, we refer to the standard self-attention in Transformer as self-attention, the approximated self-attention in NSTformer as Nystr\u00f6m attention.\nAs Figure 1 shows, we regard each nodes (sensors) at different time-steps as an independent ST-Token, and all ST-Tokens from one traffic flow form a sequence whose length is N\u00d7T to feed to attention. Here we present the traffic flow as \\(X \\in \\mathbb{R}^{NT \\times d_h}\\). We have\n\\[Q = XW_Q, K = XW_K, V = XW_V\\]\nwhere \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d_h \\times d_h}\\).\nThen the score of the self-attention is calculated as:\n\\[S = softmax(\\frac{QK^T}{\\sqrt{d}}) \\in \\mathbb{R}^{NT \\times NT}\\]\nThe final output of the self-attention is:\n\\[SelfAttention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V \\in \\mathbb{R}^{NT \\times d_h}\\]"}, {"title": "NSTformer", "content": "To overcome the above new obstacle, we designed NST-former, in which we adapted Nystr\u00f6mformer to replace the standard Transformer in STformer, yields a linear Nystr\u00f6m attention. Consider the only one difference of the two models is the attention mechanism, we generally introduce Nystr\u00f6mformer (Xiong et al. 2021) and analysis the linear Nystr\u00f6m attention here, we recommend (Xiong et al. 2021) to learn more.\nThe Nystr\u00f6m-like methods approximate a matrix by sampling columns from the matrix. (Xiong et al. 2021) adapted the method to approximate the calculation of the original softmax matrix S in equal (4). The fundamental insight involves utilizing landmarks K and Q derived from key K and query Q to formulate an efficient Nystr\u00f6m approximation without accessing the entire \\(QK^T\\). In cases where the count of landmarks, m, much smaller than the sequence length n, the Nystr\u00f6m approximation exhibits linear scalability concerning both memory and time with respect to the input sequence length.\nDefinition 4 (Xiong et al. 2021). Assume that the selected landmarks for inputs \\(Q = [q_1; ...; q_n]\\) and \\(K = [k_1; ...; k_n]\\) are \\(\\{q_j\\}^m_{j=1}\\) and \\(\\{k_j\\}^m_{j=1}\\) respectively. We denote the matrix form of the corresponding landmarks as\nFor \\(\\{q_j\\}^m_{j=1}\\), \\(Q = [q_1; ...; q_m] \\in \\mathbb{R}^{m \\times d_h}\\)\nFor \\(\\{k_j\\}^m_{j=1}\\), \\(K = [k_1;...; k_m] \\in \\mathbb{R}^{m \\times d_h}\\)\nThen the m \u00d7 m matrix is given by \\(A_S = softmax(K^T)\\).\nAnd the Nystr\u00f6m form of the softmax matrix, \\(\\hat{S} = softmax(\\frac{QK^T}{\\sqrt{d}})\\) is approximated as\n\\[\\hat{S} = softmax(\\frac{QK^T}{\\sqrt{d}}) \\approx softmax(\\frac{QK}{\\sqrt{d}}) A_S^+softmax(\\frac{QK^T}{\\sqrt{d}})\\]\nwhere \\(A_S^+\\) is a Moore-Penrose pseudoinverse of \\(A_S\\).\nLemma 1 (Xiong et al. 2021). For \\(A_S \\in \\mathbb{R}^{m \\times m}\\), the sequence \\(\\{Z_j\\}_{j=0}\\) generated by (Razavi et al. 2014),\n\\[Z_{j+1} = \\frac{1}{2}Z_j(15I - A_SZ_j(7I - A_SZ_j(15I - A_SZ_j(7I - A_SZ_j))))\\]\nconverges to the Moore-Penrose inverse \\(A_S^+\\) in the third order with initial approximation \\(Z_0\\) satisfying \\(\\|A_SA_S^+ - A_SZ_0\\| < 1\\).\nBy \\(Z^*\\) with (6) in Lemma 1 to approximate \\(A_S^+\\), then the Nystr\u00f6m approximation of S becomes\n\\[\\hat{S} = softmax(\\frac{QK}{\\sqrt{d}}) Z^* softmax(\\frac{QK^T}{\\sqrt{d}})\\]\nFinally, we derive the Nystr\u00f6m attention:\n\\[SV = softmax(\\frac{QK}{\\sqrt{d}}) Z^* softmax(\\frac{QK^T}{\\sqrt{d}})V.\\]\nWe present the pipeline for Nystr\u00f6m approximation of softmax matrix in self-attention in Algorithm 2.\nLandmarks selection (Zhang, Tsang, and Kwok 2008; Vyas, Katharopoulos, and Fleuret 2020) used K-Means to select landmark points (Lee et al. 2019). Consider the EM-style updates in K-means is less preferable when using mini-batch training. (Xiong et al. 2021) suggested using Segment-means, which is similar to the local average pooling approach previously utilized in NLP literature (Shen et al. 2018).\nParticularly, for inputs \\(Q = [q_1;...; q_n]\\) and \\(K = [k_1; ...; k_n]\\), n queries are divided into m segments. Assuming n is divisible by m for simplicity, as we can pad inputs to a length divisible by m, let l = n/m. Landmark points for Q and K are then calculated as demonstrated in (7). And the whole process only need a simple scan of inputs, which yields a complexity of O(n).\n\\[\\Bar{q}_j = \\frac{\\sum_{i=(j-1)xl+1}^{(j-1)xl+m} q_i}{m}\\]\n\\[\\Bar{k}_j = \\frac{\\sum_{i=(j-1)xl+1}^{(j-1)xl+m} k_i}{m}\\]"}, {"title": "Revisit Landmarks selection for spatial-temporal forecasting.", "content": "We revisit the landmarks selection for spatial-temporal forecasting. Our insight is that at the same time, the nodes from the same one neighborhood of the road network have similar traffic conditions.\nThe hypothesis is reasonable, as the traffic conditions in a localized area are influenced by similar factors such as road capacity, traffic signals or nearby events. For example, in the downtown area of a city, which typically serves as a hub for business and commercial activities, the traffic patterns during rush hours might be characterized by high congestion due to the influx of commuters. This congestion is likely to spread across multiple nodes within the same vicinity, affecting adjacent streets and intersections. Conversely, in residential neighborhoods, the traffic state may be different, with peak times coinciding with school drop-offs and pick-ups or evening commutes, but generally experiencing lighter traffic compared to commercial districts.\nBased on the understanding of spatial-temporal data, we introduce a landmarks selection algorithm for using Nystr\u00f6m attention in spatial-temporal forecasting tasks, which named after Spatial-Temporal Cluster Sampling (STCS) algorithm. Specifically, the whole nodes of road network are divided into clusters C\u2081 to cs according their distance, and during the T time-steps, each cluster is regarded as a block which we term it ST-block. One can select suitable clustering algorithm to the datasets as long as it can cluster the nodes according their spatial relationships, in our experiment we use the Agglomerative Clustering from scikit-learn (scikit-learn developers 2024). As the above hypothesis, we assume the traffic condition of nodes from the same block follows a same Normal distribution. Then, one can sample a certain number of times to get the average value. Finally, s \u00d7 T landmarks are selected, as presented in Algorithm 1 taking the example of computation of Q, that of K has the same process. In this way, we add our insight for spatial-temporal data to landmarks selection. Note that the clustering operation can be finished before the landmarks selection once the road network is given, then we can immediately figure out that the procession of STCS also only require a simple scan for inputs as Segment-means, with O(n) complexity.\nComplexity analysis We follow (Xiong et al. 2021) to analyze the complexity of Nystr\u00f6m attention, namely \u015cV. The time complexity breakdown is as follows:\n\u2022 Landmark selection using Segment-means takes O(n).\n\u2022 Iterative approximation of the pseudoinverse takes O(m\u00b3) in the worse case.\n\u2022 Matrix multiplication \\(softmax(\\frac{QK}{\\sqrt{d}}) Z^*,softmax(\\frac{QK^T}{\\sqrt{d}})\\), \\((softmax(\\frac{QK}{\\sqrt{d}}) Z^*) \\times V\\), \\((softmax(\\frac{QK^T}{\\sqrt{d}}) \\times V)\\) take O(nm\u00b2 + mndh + m\u00b3 + nmdh).\nThen we have the overall time complexity O(n + m\u00b3 + nm\u00b2 + mndh + m\u00b3 + nmdh).\nThe memory complexity breakdown is as follows:\n\u2022 Storing landmarks matrix Q and K takes O(mdh)."}, {"title": "Experiment", "content": "METR-LA and PEMS-BAY are the two most commonly used datasets in traffic forecasting (Li et al. 2018), we select these and give a brief introduction of the two datasets in Table 2.\nExperimental Setup\nIn our experiments, METR-LA and PEMS-BAY datasets are partitioned into training, validation, and test sets with a ratio of 7:1:2."}, {"title": "Results Analysis", "content": "Table 1 shows our results. If STformer and NSTformer exceed all other models, we marked the results of STformer and NSTformer with underline and bold format respectively. In all cases, bold results are the best while results with asterisk represents the best model except STformer and NSTformer.\nAs it shows, NSTformer and STformer achieve best performance and second best performance respectively on almost all metrics. At the first glance, it could be astonishing that NSTformer can be a bit better than STformer. In a few cases, (Xiong et al. 2021) also discovered that Nystr\u00f6m attention is even slightly better than self-attention in NLP tasks. Based on the discovery, we propose an open problem here: Does approximate attention have any additional positive effect such as regularization ?\nSTAEformer and ST-Mamba are only inferior to NST-former and STformer. Our work with with STAEformer reveal the power of pure attention in spatial-temporal forecasting. Due to learning the spatial and temporal relationships separately and asynchronously, our models exceed it with a big advantage. Though ST-Mamba also has linear complexity, it lags far behind our models."}, {"title": "Model Complexity", "content": "We present the complexity of STformer and NSTformer in Table 3, with the comparison to STAEformer, the parameters are recorded on METR-LA. STformer and NST-former can capture spatial-temporal relationship between nodes with fewer layers thanks to the fully-connected setting, which yields fewer model parameters, indicating their afford cost further."}, {"title": "Conclusion", "content": "We investigated only using attention mechanism in spatial-temporal forecasting tasks to address the problems of existing works. Our STformer earn state-of-the-art performance with a large advantage. Given the quadratic complexity of Transformer and based on our insight to the instinct of spatial-temporal data, we propose NSTformer with linear complexity by adapting Nystr\u00f6mformer to overcome the obstacle, which slightly exceed STformer surprisingly. Thanks to the leading performance and economical cost of NSTformer, we hope our work can offer insight to spatial-temporal forecasting. For future research, one can extend our method to more other spatial-temporal tasks. Another interesting direction is to try other linear attention or Mamba further. As for the cases where approximate attention beat standard self-attention slightly, we get a hypothesis after speculating the discovery: Approximate attention could have additional positive effect such as regularization. We propose the theoretically and practically meaningful open problem to machine learning community."}]}