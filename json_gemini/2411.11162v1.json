{"title": "RPN 2: On Interdependence Function Learning", "authors": ["Jiawei Zhang"], "abstract": "This paper builds upon our previous work on the Reconciled Polynomial Network (RPN) [89]. In our prior research, we introduced RPN as a general model architecture comprising three component functions: data expansion function, parameter reconciliation function, and remainder function. By strategically combining these components functions, we demonstrated RPN's versatility in constructing models for addressing a wide range of function learning tasks on multi-modal data. Furthermore, RPN also unified diverse base models, including PGMs, kernel SVM, MLP, and KAN, into its canonical representation.\n The original RPN model was designed under the assumption of input data independence, presuming the independence among both individual instances within data batches and attributes in each data instance. However, this assumption often proves invalid for function learning tasks involving complex, interdependent data such as language, images, time series, and graphs. Ignoring such data interdependence may inevitably lead to significant performance degradation.\n To overcome these limitations, we introduce the new Reconciled Polynomial Network (version 2), namely RPN 2, in this paper. By incorporating data and structural interdependence functions, RPN 2 explicitly models data interdependence via new component functions in its architecture.", "sections": [{"title": "1 Introduction", "content": "This paper is a follow-up work of our previous study on the Reconciled Polynomial Network (RPN) [89]. In our prior work, we introduced RPN as a general model architecture comprising three component functions: data expansion function, parameter reconciliation function, and remainder function. Inspired by Taylor's Theorem, RPN proposes to disentangle the underlying function to be inferred as the inner product of a data expansion function with a parameter reconciliation function. Together with the remainder function, RPN can accurately approximate the underlying functions that govern data distributions. Our previous research demonstrated RPN's versatility in constructing models with varying complexities, capacities, and levels of completeness, which can also serve as a framework for unifying diverse base models, including PGMs, kernel SVM, MLP, and KAN.\n Meanwhile, the previous RPN model was built on the assumption that data instances in the training batches are independent and identically distributed. Moreover, within each data instance, RPN also presumed the involved attributes to be independent as well, treating them separately in the expansion functions. However, as illustrated in Figure 1 (a)-(d), these assumptions often prove invalid for function learning tasks on complex, and interdependent data such as images, language, time series, and graphs. In such data, strong interdependence relationships typically exist among both instances and attributes. Ignoring these data interdependencies, as the previous RPN model does, will significantly degrade learning performance."}, {"title": "2 Notation System and Background Knowledge", "content": "To ensure the self-containment of this paper, we preface the technical descriptions of the novel RPN 2 model with a concise overview of both the function learning task and the original Reconciled Polynomial Network (RPN) introduced in the previous paper [89] in this section. Before introducing the background knowledge, we will also briefly describe the notations that will be used in this paper."}, {"title": "2.1 Notation System", "content": "In the sequel of this paper, unless otherwise specified, we adopt the following notational conventions: lower-case letters (e.g., x) represent scalars, upper-case letters (e.g., X) represent variables, lower-case bold letters (e.g., x) denote column vectors, boldface upper-case letters (e.g., X) denote matrices and high-order tensors, and upper-case calligraphic letters (e.g., X) denote sets.\n For a vector x, we denote its i-th element as x(i) or $x_i$, which will be used interchangeably. We use $x^T$ to represent the transpose of vector x. For vector x, its $L_p$-norm is defined as $||x||_p = (\\sum_i |x(i)|^p)^{1/p}$. The elementwise product of vectors x and y of the same dimension is denoted by $x \\odot y$, their inner product by $(x, y)$, and their Kronecker product by $x \\otimes y$.\n For a matrix X, we represent its i-th row and j-th column as $X(i, :)$ and $X(:,j)$, respectively. The $(i, j)$-th entry of matrix X is denoted as $X(i, j)$, and its transpose is represented as $X^T$. The elementwise and Kronecker product operations extend to matrices X and Y as $X \\odot Y$ and $X \\otimes Y$, respectively. The Frobenius-norm of matrix X is represented as $||X||_F = (\\sum_{i, j} X(i, j)^2)^{1/2}$, and its infinity-norm is defined as its maximum absolute row sums, i.e., $||X||_{\\infty} = \\max_i (\\sum_j |X(i,j)|)$. The two-to-infinity subordinate vector norm of matrix X is defined as $||X||_{2\\rightarrow\\infty} = \\sup_{||z||_2=1} ||Xz||_{\\infty} = \\max_i ||X(i,:)||_2$.\n For two variables X and Y, we denote their independence as $X \\perp\\!\\!\\!\\perp Y$, and their conditional independence given a condition C as $X \\perp\\!\\!\\!\\perp Y|C$. Conversely, their interdependence and conditional interdependence are represented as $X \\Join Y$ and $X \\Join Y|C$, respectively. If X exhibits direct dependence on Y, we express this as $X \\leftarrow Y$ or $Y \\rightarrow X$."}, {"title": "2.2 Function Learning Task", "content": "As outlined in the previous RPN paper [89], function learning, as the most fundamental task in machine learning, aims to construct a general model comprising a sequence of component functions that infer relationships between inputs and outputs. The term \"function\" in this context refers to not only the mathematical function components constituting the RPN model but also the cognitive function of RPN as an intelligent system generating the desired output responses from input signals.\n In function learning, without prior assumptions about data modalities, the corresponding input and output data can manifest in various forms, including but not limited to continuous numerical values (e.g., continuous functions and time series), discrete categorical features (e.g., images, point clouds and language), probabilistic variables (defining dependency relationships between inputs and outputs), interconnected structures (e.g., grids, graphs and chains) and other forms.\n DEFINITION 1 (Function Learning): Formally, given input and output spaces $\\mathbb{R}^m$ and $\\mathbb{R}^n$, respectively, the underlying mapping governing the data projection between these spaces can be denoted as:\n $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$.\n Function learning aims to construct a model g as a composition of mathematical function sequences $g_1, g_2, \\dots, g_K$ to project data across different vector spaces, which can be represented as:\n $g: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, and $g = g_1 \\circ g_2 \\circ \\dots \\circ g_K$,\n where the $\\circ$ notation denotes component function integration and composition operators. The component functions $g_i$ can be defined on either the input data or the model parameters.\n For input vector $x \\in \\mathbb{R}^m$, if the output generated by the model approximates the desired output, i.e.,\n $g(x|w, \\Theta) \\approx f(x)$,\nthen model g can serve as an approximated of the underlying mapping f for the provided input x.\n Notations $w \\in \\mathbb{R}^l$ and $\\Theta \\in \\mathbb{R}^{l'}$ denote the learnable parameters and hyper-parameters of the function learning model, respectively. By convention, the hyper-parameter vector $\\Theta$ may be omitted from the model representation, which simplifies the model notation to be $g(\\cdot|w)$."}, {"title": "2.3 Reconciled Polynomial Network (RPN) Model", "content": "To address the function learning tasks, our previous paper [89] introduced the Reconciled Polynomial Network (RPN) model as a general framework with versatile architectures. The RPN model comprises three component functions, including data expansion function, parameter reconciliation function, and remainder function. This architecture disentangles input data from model parameters and approximates the target functions as the inner product of the data expansion function with the parameter reconciliation function, subsequently summed with the remainder function.\n Formally, given the underlying data distribution mapping $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, we represent the RPN model proposed to approximate function f as follows:\n $g(x|w) = \\langle \\kappa(x), \\psi(w) \\rangle + \\pi(x)$,\n where\n $\\kappa: \\mathbb{R}^m \\rightarrow \\mathbb{R}^D$ is named as the data expansion function and D is the target expansion space dimension.\n $\\psi: \\mathbb{R}^l \\rightarrow \\mathbb{R}^{n \\times D}$ is named as the parameter reconciliation function, which is defined only on the parameters without any input data.\n $\\pi: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ is named as the remainder function.\n This tripartite set of compositional functions, i.e., data expansion, parameter reconciliation, and remainder functions, serves as the foundation for the RPN model. By strategically selecting and combining these component functions, we will be able to construct a RPN model to address a wide spectrum of learning challenges across diverse function learning tasks. To enhance RPN's modeling capabilities, in the previous RPN paper [89], we introduced both a wide architecture featuring multi-heads and multi-channels (within each layer), and a deep architecture comprising multiple layers. Furthermore, we equipped RPN with a more adaptable and lightweight mechanism for constructing models with comparable capabilities through nested and extended data expansion functions.\n Furthermore, as outlined earlier, the RPN model was based on the assumption that data instances in training batches are independent and identically distributed (i.i.d.). It also assumed that the attributes within each instance were independent, treating them separately within its expansion functions. These restrictive assumptions significantly limit RPN's effectiveness in real-world learning tasks involving complex, interdependent data, such as language, images, time series, and graphs. In the following section, we will delve into the concept of data interdependence, which is explicitly modeled in the newly redesigned RPN 2 model, building upon the foundations of RPN."}, {"title": "3 Data Interdependence", "content": "This section explores the concept of data interdependence in function learning tasks. In practice, interdependence relationships within a data batch can be classified into various granularities, such as attribute interdependence and instance interdependence, among others. By drawing on real-world data, we will illustrate concrete examples of these relationships. Furthermore, we will explore various metrics and methods for quantifying and modeling interdependence, which will serve as the foundation for developing the robust and effective RPN 2 model to address function learning tasks in complex and interdependent datasets."}, {"title": "3.1 What is Data Interdependence?", "content": "Conceptually, the \"Principle of Universal Connection and Development\" discussed in DIALECTICAL MATERIALISM asserts that \u201cNothing in the world stands by itself. Every object is a link in an endless chain and is thus connected with all the other links.\" Data collected from the real-world should inherently reflect such universal and extensive connections. Technically, understanding the data interdependence is also critical for conducting accurate analyses, developing robust models, and making correct decisions in the construction of intelligent function learning systems.\n DEFINITION 2 (Data Interdependence): Formally, data interdependence refers to the relationships and interactions between different individual attributes or the entire data instances within a system. In this context, the state or behavior of one element (either an attribute or an instance) can influence or be influenced by others, creating a network of interdependent relationships.\n Instance Interdependence vs. Attribute Interdependence: Data interdependence may manifest in input data at various granularities, such as attribute interdependence among the attributes within each data instance and instance interdependence among the instances within the data batch. From the perspective of function learning tasks, the distinction between attribute and instance interdependence often becomes blurred, as certain data elements can be regarded as either attributes or instances or even both. For example, image frames may be considered as individual instances in image classification tasks but can serve as attributes in video content understanding tasks. Similar ambiguities arise in various modalities, such as point clouds, languages, graphs, and time series data.\n From a technical implementation perspective, the differences between attribute and instance interdependence are primarily a matter of measuring interdependence across different dimensions of the input data batch (e.g., rows for instance interdependence and columns for attribute interdependence). By leveraging data batch transposition and reshaping techniques, a unified implementation can effectively calculate interdependence across various dimensions of the data batch, enabling consistent modeling of both attribute and instance interdependence.\n To maintain generality, we will illustrate interdependence using examples of general vectors sampled from a vector space below and explore methods to measure these dependencies. In specific applications, these vector variables can represent either attributes or instances within their respective vector spaces, offering a versatile approach to interdependence analysis."}, {"title": "3.2 Data Interdependence Quantitative Measurements", "content": "Data interdependence among vector variables representing attribute or instance vectors (i.e., the columns and rows of the input data batch) can be quantified using various methods, including statistical and numerical approaches. Statistical interdependence measurements, grounded in probability theory and statistics, explicitly model and quantify uncertainty in the interdependence calculation. Conversely, numerical interdependence measurements, based on linear algebra and computational theory, often assume the data interdependence relationships to be deterministic instead. In addition to statistical and numerical approaches, many other quantitative methods also exist, such as topological and geometric measurements, which assess data interdependence relationships from the perspective of topological and geometric structures. These distinct approaches give rise to different categories of quantitative metrics for defining data interdependence functions, all of which will be introduced in detail in the following Section 5."}, {"title": "3.2.1 Statistical Data Interdependence Metrics", "content": "Formally, given variables $X_1, X_2, \\dots, X_k$ representing vectors $x_1, x_2, \\dots, x_k \\in \\mathbb{R}^d$ from a d-dimensional vector space (each representing either an attribute or an instance), statistical interdependence measurements assume each follows certain distributions, such as the multivariate Gaussian distribution:\n $X_i \\sim N(\\mu_i, \\Sigma_i), \\forall i \\in \\{1,2, \\dots, k\\}$,\n where the notation $N(\\mu_i, \\Sigma_i)$ denotes a multivariate Gaussian distribution with mean vector $\\mu_i \\in \\mathbb{R}^d$ and variance matrix $\\Sigma_i \\in \\mathbb{R}^{d \\times d}$.\n The joint distribution of these k variables also follows the multivariate Gaussian distribution, i.e.,\n$\\begin{bmatrix}\nX_1\\\\\nX_2\\\\\n\\vdots\\\\\nX_k\n\\end{bmatrix} \\sim N \\left( \\begin{bmatrix}\n\\mu_1\\\\\n\\mu_2\\\\\n\\vdots\\\\\n\\mu_k\n\\end{bmatrix}, \\begin{bmatrix}\n\\Sigma_1 & \\Sigma_{1,2} & \\dots & \\Sigma_{1,k} \\\\\n\\Sigma_{2,1} & \\Sigma_2 & & \\Sigma_{2,k} \\\\\n\\vdots & & \\ddots & \\vdots \\\\\n\\Sigma_{k,1} & \\Sigma_{k,2} & \\dots & \\Sigma_k\n\\end{bmatrix} \\right)$\n where $\\Sigma_{i,j} = Cov(X_i, X_j)$ denotes the covariance matrix of variables $X_i$ and $X_j$ (for $i,j \\in \\{1,2, \\dots, k\\}$ and $i \\neq j$), and $\\Sigma_{j,i} = Cov(X_j, X_i) = \\Sigma_{i,j}^T$. The diagonal variance matrix $\\Sigma_i$ of variable $X_i$ (for $\\forall i \\in \\{1,2, \\dots, k\\}$) can also be calculated in a similar way as $\\Sigma_i = Cov(X_i, X_i)$, which is symmetric by default.\n Statistically, two variables $X_i$ and $X_j$ are independent if and only if their corresponding covariance matrix is zero, i.e., $\\Sigma_{i,j} = 0$ (or equivalently $\\Sigma_{j,i} = 0$). If variables $X_1, X_2, . . . , X_k$ are statistically jointly independent, all the off-diagonal block matrices of the joint covariance matrix in Equation (6) will be zeros, rendering the joint covariance matrix to be diagonal.\n Based on these above descriptions, various statistical metrics can measure interdependence among the vector variables, such as correlation coefficients and mutual information metrics.\n RV Coefficient based Interdependence Metric: In statistics, the RV coefficient, a multivariate gen- eralization of the squared Pearson correlation coefficient, measures the linear dependence between two variables. It ranges from 0 to 1, with 1 indicating perfect linear dependence (or similarity) and 0 indicating no linear dependence. The RV coefficient for pairs of variables can be directly calculated based on their covariance matrices, as introduced above.\n Given variables $X_i$ and $X_j$, as well as their variance and covariance matrices $\\Sigma_i, \\Sigma_j, \\Sigma_{i,j}$ and $\\Sigma_{j,i}$, their RV-coefficient is defined as:\n $RV(X_i, X_j) = \\frac{tr(\\Sigma_{i,j}\\Sigma_{jj}\\Sigma_{j,i})}{\\sqrt{tr(\\Sigma_i^2) tr(\\Sigma_j^2)}} \\in \\mathbb{R}$,\n where the notation $tr(\\cdot)$ denotes the trace of the input matrix.\n Mutual Information (Gaussian) based Interdependence Metric: In addition to the RV coeffi- cient, mutual information measures the amount of information one random variable contains about another, defining the non-linear interdependence relationships of variables.\n For random variables $X_i$ and $X_j$ following a multivariate Gaussian distribution, based on their variance and covariance matrices, their mutual information metric is calculated as:\n $MI(X_i, X_j) = \\frac{1}{2} \\log(\\frac{det(\\Sigma_i) det(\\Sigma_j)}{det(\\Sigma)}) \\in \\mathbb{R}$,\n where $\\Sigma = \\begin{bmatrix} \\Sigma_i & \\Sigma_{i,j} \\\\ \\Sigma_{j,i} & \\Sigma_j \\end{bmatrix}$ is the covariance matrix of the joint variables $\\begin{bmatrix} X_i\\\\X_j \\end{bmatrix}$ and $det(\\cdot)$ denotes the determinant of the input matrix."}, {"title": "3.2.2 Numerical Data Interdependence Metrics", "content": "Unlike statistical metrics, numerical metrics measure deterministic interdependence relationships among variables without prior assumptions about their distributions. For variables $X_1, X_2,..., X_k$ representing vectors sampled from a vector space (either attribute or instance vector spaces), numerical methods can quantify interdependence using vector similarity or distance metrics. The simplest form of numerical interdependence in vector space is linear interdependence.\n Assuming these vector variables take values $x_1, x_2,...,x_k \\in \\mathbb{R}^d$, respectively, they are linearly interdependent if there exist scalar coefficients $\\alpha_1, \\alpha_2,..., \\alpha_k \\in \\mathbb{R}$, not all zero, such that:\n $\\alpha_1 x_1 + \\alpha_2 x_2 + \\dots + \\alpha_k x_k = 0$,\n where 0 denotes the zero vector of the corresponding vector space. These coefficients $\\alpha_1,\\alpha_2,..., \\alpha_k$ can be calculated using the Gaussian elimination method, which illustrates the lin- ear interdependence relationships among the vectors. Gaussian elimination, also known as row reduction, is a method for solving systems of linear equations. It involves a sequence of row-wise reduction operations performed on the corresponding matrix of coefficients.\n To demonstrate how Gaussian elimination reveals interdependence among vectors, consider an ex- ample with $x_1 = [2, -1, -1]^T, x_2 = [3, -4, -2]$ and $x_3 = [5, -10, -8]^T$. Equation (9) defined on these three vectors can be rewritten as:\n$\\alpha_1\\begin{bmatrix}2\\\\-1\\\\-1\\end{bmatrix} + \\alpha_2\\begin{bmatrix}3\\\\-4\\\\-2\\end{bmatrix} + \\alpha_3\\begin{bmatrix}5\\\\-10\\\\-8\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n This homogeneous system of linear equations can be transformed into its row-reduced form using Gaussian elimination as follows:\n$\\begin{bmatrix}2 & 3 & 5 & 0\\\\-1 & -4 & -10 & 0\\\\-1 & -2 & -8 & 0\\end{bmatrix} \\xrightarrow{row-reduction} \\begin{bmatrix}2 & 3 & 5 & 0\\\\0 & 1 & 3 & 0\\\\0 & 0 & 0 & 0\\end{bmatrix}$.\n From this reduction, we observe that: (1) the first and second columns have pivots, indicating that vectors $x_1$ and $x_2$ are linearly independent, and (2) the third column has no pivot, indicating that vector $x_3$ can be linearly represented by $x_1$ and $x_2$. Furthermore, we can deduce that $2\\alpha_1 + 3\\alpha_2 + 5\\alpha_3 = 0$ and $\\alpha_2 + 3\\alpha_3 = 0$, implying $\\alpha_2 = -3\\alpha_3$ and $\\alpha_1 = \\frac{7}{2}\\alpha_3$. Thus:\n$0 = \\alpha_1 x_1 + \\alpha_2 x_2 + \\alpha_3 x_3 = \\frac{7}{2}\\alpha_3 x_1 - 3\\alpha_3 x_2 + \\alpha_3 x_3$,\nor\n$x_3 = -2x_1 + 3x_2$.\n However, the row-reduced matrix indicates relationships between coefficients and cannot directly define interdependence relationships of data vectors. The row-reduction operation typically has a time complexity of $O(d^3)$, which can be computationally expensive for high-dimensional data vectors. Moreover, such linear interdependence among instances or attributes is rare in real-world data batches. Instead of directly using linear relationship analysis, we propose to compute interdepen- dence metrics based on other operators, such as inner product and bilinear form.\n Inner Product based Interdependence Metric: The inner product, a generalization of the dot product, multiplies vectors to produce a scalar. It quantifies the relationship between two vectors (or variables) by calculating the angle between them in the vector space, capturing both vector magnitudes and relative orientation. Formally, for two variables $X_i = x_i$ and $X_j = x_j$ taking vectors of the same dimension, their inner product is calculated as:\n$I(X_i, X_j) = x_i^T x_j \\in \\mathbb{R}$.\n Bilinear Form based Interdependence Metric: A bilinear form is a function linear in both argu- ments that maps two vectors to a scalar. It measures interdependence by quantifying vector interac- tions or correlations, with properties like symmetry and orthogonality providing additional insights. The inner product metric can also be viewed as a special case of bilinear forms.\n Formally, for variables $X_i$ and $X_j$ taking value vectors $x_i$ and $x_j$, their bilinear form-based interdependence metric can be calculated as follows:\n$B(X_i, X_j) = x_i^T W x_j \\in \\mathbb{R}$"}, {"title": "3.3 Data Interdependence Examples", "content": "In this section, we present examples of real-world data to illustrate the interdependence present in data collected from various sources. These examples span different modalities, including images, language, graphs, and time series. As previously discussed, interdependence relationships can exist at both instance and attribute granularities, with the categorization largely dependent on the data representation format and specific function learning tasks. It is important to note that for the examples discussed below, we present just one potential approach to defining data interdependence relation- ships. Depending on the specific problem and learning settings, other valid methods for defining interdependence relationships within the data batch may exist. Readers are encouraged to select the most appropriate approach for modeling such dependence relationships in their particular contexts."}, {"title": "3.3.1 Image Data Interdependence", "content": "Images can be viewed as a sequence of pixels organized into a square or rectangular shape with specific height and width. In the right plot, we illustrate a colored image of a hummingbird in a square shape with 20 \u00d7 20 pixels, i.e., both the image height and width are 20. For colored images, depending on the encoding method used (such as RGB, YCbCr, and CMYK), each pixel is represented by multiple integers. We show the RGB codes of three randomly picked pixels on the left-hand side of the plot. As demonstrated, it is difficult to interpret the physical meanings of these individual pixel RGB values or their potential contribution to identifying objects in the image.\n The significance of individual image pixels in addressing the target learning task is profoundly influ- enced by their surrounding context, which are normally their nearby pixels. The collective variation patterns of these adjacent pixels provide crucial information about the objects present within the images. On the right-hand side of the illustration, we present a 3 \u00d7 3 pixel segment along with its corresponding RGB color codes. This small-scale representation, when juxtaposed with individual pixels shown on the left, offers a more nuanced perspective. The significant fluctuations in the cen- tral pixel's values compared to its surrounding pixels within this segment indicate its position at the boundary, conveying substantially more information than isolated pixels."}, {"title": "3.3.2 Language Data Interdependence", "content": "Language data, as an important carrier of informa- tion, typically appears in an ordered sequence structure, which may include natural language, program- ming language, and mathematical language. People read and write language data sequentially, which can convey rich semantic information. In the right plot, we illustrate an example sentence \"This section is long and boring.\", and provide its dependency pars- ing tree. In natural language processing, sentences can typically be decomposed into a sequential list of tokens via a tokenizer based on a pre-defined vocabulary set. For this discussion, we will consider tokens as the smallest units to introduce the interdependence of language data.\n In language data, extensive dependence relationships exist among tokens within the same sentence or paragraph (and even across documents). For instance, \u201cthis section\u201d in the above example de- termines the use of \u201cis\u201d rather than \"are\", and the word \u201cis\u201d constrains the following words to be adjectives, e.g., \u201clong\u201d. Furthermore, the conjunction \u201cand\u201d indicates that the two adjectives should have close semantic meanings, i.e., \"long\" and \u201cboring\u201d. The semantic meaning of each word de- pends on its sentence context, which is a crucial factor that should be incorporated into model design and learning."}, {"title": "3.3.3 Time-Series Data Interdependence", "content": "Time series data, such as daily temperature read- ings, stock market prices, and annual GDP growth, provide another representative example of interde- pendence in data. In the right plot, we show the stock price curves of six biotechnology companies AMGN, BIIB, CERN, ILMN, LULU, and MNST over a four-year period. In time series data, data points at later timestamps typically depend on those at previous timestamps. Moreover, for some time se- ries data exhibiting long-term periodic patterns, this interdependence may span a much longer time period, e.g., a month or even several years.\n For the stock price data illustrated in the right plot, these selected stocks belong to the same sector, and the price of one stock may also depend on other correlated stocks. In such stock price time series data, individual data points alone can hardly reveal any information about the underlying price changing patterns. To extract useful features and signals, it may be necessary to include other data points spanning both temporal and sector dimensions."}, {"title": "3.3.4 Graph Data Interdependence", "content": "In addition to images, language and time-series data, graphs are another representative example of data struc- tures with extensive dependence relationships among nodes. Graphs can be represented as a set of nodes con- nected by links. In the right plot, we illustrate an example of the Myricetin molecule (i.\u0435., C15H10O8), a member of the flavonoid class of polyphenolic compounds with antioxidant properties. By treating atoms as nodes and atomic bonds as links, the Myricetin molecule can be rep- resented as a molecular graph structure. Single and double bonds can be represented as different types of links in the graph, rendering it heterogeneous. (Note: The distinction between homoge- neous and heterogeneous graphs is slightly out of the scope of this paper and will not be discussed further.)\n In the molecular graph, it is difficult to infer the roles or functions of individual nodes, such as the central carbon atom in the red dashed circle, based solely on the node itself. We must consider its surrounding nodes on which it depends. Unlike images, nodes' dependence relationships in graphs can span across the entire structure to distant nodes, and local neighbors may not provide sufficient information for inferring their functions in the molecule. For instance, in the plot, we highlight several other carbon nodes with identical surrounding neighbors connected by the same types of links. To infer the functions of the central carbon node, we may also need to consider the functional groups it is involved in, e.g., the one highlighted in the orange background color in the plot."}, {"title": "3.4 Data Interdependence Handling", "content": "The diverse data interdependence relationships illustrated in the above examples play a critical role in the function learning tasks studied in this paper. In this section, we introduce two different ap- proaches for handling such data interdependence relationships: interdependence padding and inter- dependence aggregation. Furthermore, we demonstrate that these two approaches can be unified under a shared representation, expressed as the multiplication of the data batch with an interdepen- dence matrix defined based on the input data."}, {"title": "3.4.1 Interdependence Padding", "content": "Formally, given two variables Z and Y representing the input and output of a data instance in a function learning task, and a set of attribute variables $A_1, A_2,..., A_d$ that Z depends on, we can represent the dependence relationships among these variables with the top plot shown on the right, with notations borrowed from Bayesian networks. The notation Z \u2192 Y denotes the di- rect dependence relationship of variable Y on variable Z; and $(A_1, A_2,..., A_d) \\rightarrow Z$ denotes the direct dependence of Z on variables $A_1, A_2,..., A_d$.\n The interdependence padding approach proposes to introduce one extra new variable $Z'$ to model the information from variables $A_1, A_2,..., A_d$ that Z depends on. The new variable $Z'$ acts as an intermediate bridge between $A_1, A_2, ..., A_d$ and Z. Also, ac- cording to the Bayesian network, the newly created variable $Z'$ renders Z and $A_1, A_2,..., A_d$ to be conditionally independent given $Z'$, i.e., $Z\\bot (A_1, A_2,..., A_d)|Z'$.\n There may exist different ways to define the new variable $Z'$. In this paper, we will define $Z'$ as a concatenation of the dependent variables $A_1, A_2, ..., A_d$, i.e., $Z' = [A_1, A_2, ..., A_d]$. This new variable $Z'$ and the input variable Z together will define the interdependence padding operator as\n $padding(Z|A_1, A_2,\\dots, A_d) = [Z, Z'].$\n For a single variable, the above interdependence padding- based approach may work well, as it includes all the depen- dent information into the padded new variable, which will be used for inferring the desired variable Y. However, in practice, there may exist multiple variables, such as $Z_1, Z_2,..., Z_k$, which may all depend on $A_1, A_2,..., A_d$, as illustrated in the right plot. To make the variables $Z_1, Z_2,..., Z_k$ condi- tionally independent from those in $A_1, A_2, ..., A_d$, redundant interdependence padding can be applied to all the variables $Z_1, Z_2,..., Z_k$ as follows:\n $\\begin{cases}\npadding(Z_1|A_1, A_2,\\dots, A_d) = [Z_1, Z'_1],\\\\\npadding(Z_2|A_1, A_2,\\dots, A_d) = [Z_2, Z'_2],\\\\\n\\vdots\\\\\npadding(Z_k|A_1, A_2,\\dots, A_d) = [Z_k, Z'_k],\n\\end{cases}$\n where the newly created padding variables $Z'_1, Z'_2,..., Z'_k$ are all concatenations of the dependent variables $A_1, A_2,..., A_d$. In other words, multiple duplicated copies of data vectors represented by variables $A_1, A_2,\\cdots, A_d$ will be concatenated to those of $Z_1, Z_2,..., Z_k$, which is actually how the existing models, like convolutional neural networks, handle the data interdependence."}, {"title": "3.4.2 Interdependence Aggregation", "content": "Besides interdependence padding", "follows": "n $aggregation(Z|A_1"}, {"follows": "n$\\begin{cases}\naggregation(Z_1|A_1, A_2,..., A_d) = a_0^1 \\cdot Z_1 + a_1^1 \\cdot A_1 + a_2^1 \\cdot A_2 + \\dots + a_d^1 \\cdot A_d, \\\\\naggregation(Z_2|A_1, A_2,\\dots, A_d) = a_0^2 \\cdot Z_2 + a_1^2 \\cdot A_1 + a_2^2 \\cdot A_2 + \\dots + a_d^2 \\cdot A_d, \\"}]}