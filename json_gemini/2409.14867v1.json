{"title": "A novel agent with formal goal-reaching guarantees: an experimental study with a mobile robot", "authors": ["Grigory Yaremenko", "Dmitrii Dobriborsci", "Roman Zashchitin", "Ruben Contreras Maestre", "Ngoc Quoc Huy Hoang", "Pavel Osinenko"], "abstract": "Reinforcement Learning (RL) has been shown to be effective and convenient for a number of tasks in robotics. However, it requires the exploration of a sufficiently large number of state-action pairs, many of which may be unsafe or unimportant. For instance, online model-free learning can be hazardous and inefficient in the absence of guarantees that a certain set of desired states will be reached during an episode. An increasingly common approach to address safety involves the addition of a shielding system that constrains the RL actions to a safe set of actions. In turn, a difficulty for such frameworks is how to effectively couple RL with the shielding system to make sure the exploration is not excessively restricted. This work presents a novel safe model-free RL agent called Critic As Lyapunov Function (CALF) and showcases how CALF can be used to improve upon control baselines in robotics in an efficient and convenient fashion while ensuring guarantees of stable goal reaching. The latter is a crucial part of safety, as seen generally. With CALF all state-action pairs remain explorable and yet reaching of desired goal states is formally guaranteed. Formal analysis is provided that shows the goal stabilization- ensuring properties of CALF and a set of real-world and numerical experiments with a non-holonomic wheeled mobile robot (WMR) TurtleBot3 Burger confirmed the superiority of CALF over such a well-established RL agent as proximal policy optimization (PPO), and a modified version of SARSA in a few- episode setting in terms of attained total cost.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, model-free Reinforcement Learning (RL) techniques have gained popularity in mobile robotics, espe- cially in environments where optimality and efficiency are critical. Great effort was put into deriving formal guarantees in RL, which still remains one of the greatest challenges. A large number of works within the subject of RL with guarantees has been conducted in the context of reaching a safe set (a goal) or avoiding low-reward areas ([1]-[3]). For instance, [4] combined Robust Control Barrier Func- tions (RCBFs) with RL to ensure exploration in high-reward areas and control in continuous tasks. By integrating a differentiable RCBF safety layer into the Soft Actor-Critic (SAC) framework, this method provides real-time guarantees while optimizing the robot's navigation behavior. The main innovation here is the ability to maintain guarantees during exploration without sacrificing learning performance. This approach ensures that the learned policies balance safety and task performance effectively, however for a general system the design of a control barrier function can be a highly creative and sophisticated procedure.\nA method of Projection-Based Constrained Policy Op- timization (PCPO) that optimizes reward functions while ensuring that certain state constraints are met through a projection step was presented in [5]. By iteratively improving the policy and projecting it back onto the constraint set, PCPO achieves more efficient constraint satisfaction com- pared to therein studied baselines. The work [6] also con- tributed by presenting a method for optimizing policies under the framework of constrained Markov decision processes (CMDPs) using Lyapunov functions for formal guarantees of goal stabilization at every stage of training. By taking advantage of deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO), the approach balances the robot's need to explore while adhering to constraints dur- ing navigation. This technique has been successfully applied in simulations and real-world indoor robot navigation tasks, offering a robust solution to safe and efficient exploration in dynamic environments, however, naturally, it is implied that a Lyapunov function is priorly known.\nIn line with this, [7] proposed an actor-critic RL frame- work that guarantees system stability, a key concern for real-world robotic applications, especially in nonlinear, high- dimensional control tasks. Their method uses Lyapunov's method to ensure the stability of learned policies, enabling the systems to recover when uncertainties interfere.\nSimilarly, [8] introduced a model-free RL method that combines probabilistic reachability analysis and Lyapunov- based techniques to ensure safety. This method constructs a Lyapunov function during policy evaluation to provide guarantees and guide exploration, gradually expanding the set of states the robot can explore without exceeding stated constraints, however this only provides probabilistic con- straints. Further advancements include [9], which presented a model-free neural Lyapunov control approach that combines the flexibility of deep RL with the rigorous guarantees of Lyapunov functions. This proposed method, which co- learns a Twin Neural Lyapunov Function (TNLF) alongside a control policy, provides runtime safety monitoring for robot navigation by guiding robots through collision-free trajectories using raw sensor data such as LiDAR, though it must be pointed out that the approach is not completely risk- free and it is only suitable for a particular set of navigation tasks.\nTwo common patterns can be traced:\n1) No guarantees provided, but no prior knowledge re-"}, {"title": "II. BACKGROUND AND PROBLEM STATEMENT", "content": "Notation: Spaces of class kappa, kappa-infinity, kappa-ell and kappa-ell-infinity functions are denoted K,K\u221e, KL, KL\u221e, respectively. These are scalar monotonically increasing functions, zero at zero, and, additionally, tending to infinity in case of K\u221e.\nA. Problem statement\nConsider the following Markov decision process (MDP):\n(S, A, p, c),\n(1)\nwhere:\n1) S is the state space, assumed as a finite-dimensional Banach space of all states of the given environment;\n2) A is the action space, that is a set of all actions avail- able to the agent, assumed to be a compact topological space;\n3) p:S\u00d7A\u00d7S \u2192 R is the transition probability density function of the environment, that is such a function that p( | st, at) is the probability density of the state St+1 at step t + 1 conditioned on the current state st and current action at;\n4) c:S\u00d7A \u2192 R is the cost function of the problem, that is a function that takes a state st and an action at and returns the immediate cost ct incurred upon the agent if it were to perform action at while in state st.\nLet (\u03a9, \u03a3, P) be a probability space underlying (1), and E be the respective expected value operator. The problem of reinforcement learning is to find a policy \u03c0 from some space of admissible policies II that minimizes\nV\u03c0(s) := E\u03a3\u03b3tc(St, At) | So = s, s\u2208S\n(2)\nfor some \u03b3\u2208 (0,1] called a discount factor. The problem may be concerned with a designated initial state s, a distri- bution thereof, or even the whole state space. The policy \u03c0* that solves the stated problem is commonly referred to as the optimal policy. An agent will be referred to as a finite routine that generates actions from the observed states.\nDefinition 1: A policy \u03c00 \u2208 \u03a0 is said to satisfy the \u03b7- improbable goal reaching property, \u03b7 \u2208 [0, 1), if\n\u2200so \u2208 S P [dG(St) \u2192 0 | At ~ \u03c00(St)] \u2265 1 - \u03b7.\n(3)\nHere dG(St) \u2192 0 denotes that 0 is a limit point of dG(St). That is, for all starting states, \u03c00 ensures that the probability of failing to reach the goal is no greater than \u03b7.\nThe question posed by the present paper is the following one:\nIf one knows a goal reaching policy \u03c00, how does one efficiently improve upon it from data in a way that guarantees to preserve the goal-reaching property?"}, {"title": "III. APPROACH", "content": "In order to solve the aforementioned problem the present paper proposes an approach named Critic As A Lyapunov Function (CALF) a particular implementation of which is described in Algorithm 1.\nIn essence CALF is merely a system of Lyapunov-like constraints for critic updates coupled with a recovery pro- cedure for when no feasible solution could be found during such an update.\nThe idea is that if constraint in line 7: are satisfied, then the resulting Lyapunov-like relation would ensure that G is reached, much akin to how the existence of a Lyapunov function certifies stability.\nOn the other hand if the constraints fail, the provided baseline \u03c00 is invoked, ceasing control until the environment is driven to a narrower area around G, where a critic update is once again feasible. Indeed, it can observed that the constraints in 7: will always be feasible in a sufficiently small neighbourhood of 0, provided that the critic architecture is sufficiently flexible and that Qw\u00b9 (s\u2020, a\u2020) > \u03bd.\nUnlike the shielding approaches available in the literature CALF does not make any state-action pairs unexplorable. Although a naive decision may result in \u03c00 ceasing control immediately after, the outcome of having made the decision can always be explored. The amount of \u201cfreedom\" can also be tuned by modifying . Smaller values of correspond to lighter restrictions on learning and naive exploration. In fact any finite state-action sequence can be explored without interruptions given that is sufficiently small and the critic architecture is reasonable, e. g., if the output layer has a bias or if the critic is at least capable of approximating constant functions locally.\nIt is important to point out that the goal reaching property will be preserved regardless of the values assigned to hyper- parameters (as long as \u03c00 is goal-reaching and \u03bd > 0).\nA. Lyapunov-like constraints and implementation details\nCALF offers a great deal of implementation flexibility. One can choose a custom critic loss, a custom action update routine, a custom exploration policy and even a custom optimizer so long as constraint satisfaction or the absence thereof is being monitored."}, {"title": "IV. ANALYSIS", "content": "Let S\u03c0(so) denote the state trajectory emanating from so under a policy \u03c0\u2208 II, i. e., S0(so) = so and for t > 0 it holds that S\u03c4(so) ~ p(\u2022 | ST\u22121(so), At\u22121(so)), with At\u22121(so) ~ \u03c0\u03c4\u22121(\u2022| ST\u22121(so)).\nThe main result on preservation of the goal-reaching property is formulated in Theorem 1.\nTheorem 1: Consider the problem (2) under the MDP (1). Let \u03c00 \u2208 \u03a00 have the following goal reaching property for G \u2282 S, i. e.,\n\u2200so \u2208 S P [dG(ST(so)) \u2192 0] \u2265 1 \u2212 \u03b7, \u03b7 \u2208 [0, 1).\n(6)\nLet \u03c0t be produced by Algorithm 1 for all t \u2265 0. Then, a similar goal reaching property is preserved under \u03c0t, i. e.,\n\u2200so \u2208 S P [dG(ST(so)) \u2192 0] \u2265 1 \u2013 \u03b7.\n(7)\nProof.\nRecalling Algorithm 1, let us denote:\nQt := Qw\u2020 (st).\n(8)\nNext, we introduce:\nT(w) := {t \u2208 Z>0 : successful critic update},\nThe former set represents the time steps at which the critic succeeds and the corresponding induced action will fire at the next step. The latter set consists of the time steps after each of which the critic first failed, discarding the subsequent failures if any occurred.\nNow, let us define:\nQ\u03c4:={Q\u03c4,t\u2208T,-1,otherwise.\n}\nNext, observe that there are at most\n\u03a4 := max{Q0\u03bd,0} \n(10)\ncritic updates until the critic stops succeeding and hence only \u03c00 is invoked from that moment on. Hence T(w) is a finite set. Notice T was independent of G' and in turn dependent on the initial value of the critic.\nLet Tfinal[w] := supt\u2208T[w]. Now notice that since Atfinal+t+1 ~ \u03c00(STfinal+t(so)), by assumptions of the theorem it holds that\ntfinal+tP [dG(Sat(s)) \u2192 0 | tfinal = \u03c4] = \u03c4\u2265 1 - \u03b7.\n(11)\nNow, evidently by the law of total probability\nlimP [dG(Stfinal+t) \u2192 0]:=\u2211T=0\u221eP [tfinal = \u03c4]\ntfinal+tP [dG(Sat(s)) \u2192 0 | tfinal = \u03c4]=(1\u2212\u03b7)\u2211T=0\u221eP [tfinal = \u03c4]+\u03b7\n(12)\nThis concludes the proof.\nFrom the statement of the theorem the following becomes clear:\nCALF reaches the goal even during online learning."}, {"title": "V. CASE STUDY", "content": "We investigated the performance of CALF by benchmarking against other methods such as a Model Predictive Control (MPC), a modified version of State-action-reward-state-action (SARSA-m), a Proximal Policy Optimization (PPO), and the nominal policy \u03c00 provided to CALF as the baseline. These policies as well as the rest of the computational set up were implemented using Regelum [28], a Python framework for reinforcement learning, control and simulation. The best results of respective algorithms were reproduced on a wheeled mobile robot.\nSARSA-m is an algorithm obtained by removing line 11: from Algorithm 1 [27]. The comparison with this algorithm is necessary to verify the significance of knowledge transfer between \u03c00 and the critic of CALF. We observed no success with plain SARSA, hence the considered modification.\nThe results of experiments can be reproduced from code available via the following link: https://github.com/thd- research/calf-rl-mobile-robot/tree/migrate."}, {"title": "A. Environment description", "content": "The proposed algorithms were tested by using the non- holonomic wheeled mobile robot (WMR) TurtleBot3 Burger by Robotis, schematically depicted in Fig. 1, The correspond- ing testbed with selected result demonstration is shown in Fig. 2. The idealistic differential equations that describe the dynamics of the robot (and do not account for uncertainty) read:\nx\u02d9=vcos(\u03b8),y\u02d9=vsin(\u03b8),\u03b8\u02d9=\u03c9,\n(13)\nwhere x and y represent the coordinates of the robot in the plane, and \u03b8 represents the yaw rotation of the robot relatively to facing the positive horizontal semi-axis, with the respective translational and rotational velocities v, \u03c9 (not to be confused with the sample variable w from Section IV)."}, {"title": "B. Objectives", "content": "The environment has the following three points of interest: the initial state at So := (-1 - 1 0)T, the target G := {(0 0 0)} and the impeding area {(x, y, \u03b8) | (x+0.5)2 + (y +0.5)2 \u2264 0.12}, where the speed of the robot is limited to 0.01 m/s due to, e. g., environmental conditions, irregular terrain or hazards. The cost is designed in a way that penalizes both the displacement from the target and the proximity to the impeding area (the \u03c0 here is not to be confused with the policy):\nc (s, a) = 100x2 + 100y2 + \u03b82 + 100c' (x,y),\n(14)\nwhere\nc' (x, y) =12\u03c00.12exp(\u221212\u239b\u239d(x+0.5)20.12+(y+0.5)20.12\u239e\u23a0),\n(15)"}, {"title": "C. Nominal policy", "content": "The action components of the nominal policy v,\u03c9 were determined using the polar coordinate representation of the WMR as per [29]."}, {"title": "D. Hyper-parameters and pre-training", "content": "Excluding the nominal policy and MPC, the agents of CALF, SARSA-m, and PPO were trained over at least 40 episodes, with each episode composed of 50 time units of operation (at 0.1 s controller sampling period). All of the hyper-parameters used for studied approaches are illustrated in Table I. For the sake of fair comparison, SARSA-m and PPO were pre-trained in such a way that their performance at episode zero approximately matches that of the nominal policy. However, such a pre-training does not guarantee goal reaching afterwards."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "inspecting spot but often displayed an erratic behavior when approaching the goal. Three important observations can be made: both PPO and SARSA-m outperform the nominal baseline upon learning convergence, \u2022 CALF outperforms both SARSA-m and PPO, the performance of CALF is close to MPC with large horizon (MPC).\nSince CALF, PPO and SARSA-m had equal head starts it would be fair to conclude that CALF is superior in terms of sample efficiency in the considered few-episode setting. Notice that even in the first few episodes CALF tends to immediately make use of the information extracted from the nominal policy and get far ahead both the nominal baseline itself and the other RL algorithms considered in the study. Unlike PPO, CALF did not missed the target even once, despite somewhat wider confidence bounds."}, {"title": "VII. CONCLUSION", "content": "This work proposes a novel RL algorithm with guarantees for stochastic environments that combines learning with a system of Lyapunov-like constraints. The resulting approach named Critic As a Lyapunov Function (CALF) possesses goal reaching guarantees that were rigorously formulated and proven. An empirical study with a real non-holonomic WMR was conducted together with a set of experiments that compare the approach to state-of-the-art RL algorithms. Overall the outcomes of the experiments indicate that CALF is superior in terms of sample efficiency in the considered few-episode setting."}]}